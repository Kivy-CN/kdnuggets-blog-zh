- en: 'From Data Collection to Model Deployment: 6 Stages of a Data Science Project'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/01/data-collection-model-deployment-6-stages-data-science-project.html](https://www.kdnuggets.com/2023/01/data-collection-model-deployment-6-stages-data-science-project.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/a0184192af70dc1f4d12c12b7a923884.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Data Science is a growing field. Let me support this with two different pieces
    of research.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The first one is by [Linkedin](https://www.linkedin.com/pulse/linkedin-jobs-rise-2022-25-us-roles-growing-demand-linkedin-news),
    done in 2022\. The study shows the fastest-growing job titles over the past 5
    years. The Machine Learning Engineer title is the fourth fastest-growing job.
    It is a subbranch of Data Science.
  prefs: []
  type: TYPE_NORMAL
- en: As a data scientist, you have to build your own Machine Learning model and put
    it into production. Of course, you should know about web scraping, too, and all
    other stages of a data science project. You can find it all in the following parts
    of the article.
  prefs: []
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/b32e62a3df98e1469088ff8d8d33ba29.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [LinkedIn](https://www.linkedin.com/pulse/linkedin-jobs-rise-2022-25-us-roles-growing-demand-linkedin-news)
  prefs: []
  type: TYPE_NORMAL
- en: The second research comes from [Glassdoor](https://www.glassdoor.com/List/Best-Jobs-in-America-LST_KQ0,20.htm).
    It shows that data science is in the top three of the 50 best jobs in America.
    Also, it’s been the same for the last seven years. Let's see the statistics. 10,071
    job openings and the 4.1/ 5 job satisfaction rate might be the reason behind being
    one of the best jobs. Also, the median base salary is $120,000 per year.
  prefs: []
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/bfc5f014b7ac77e2b4f1ee42e5477ba8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Glassdoor](https://www.glassdoor.com/List/Best-Jobs-in-America-LST_KQ0,20.htm)
  prefs: []
  type: TYPE_NORMAL
- en: Now, it is more evident that data science is a growing field. And also, the
    demand in the industry looks promising.
  prefs: []
  type: TYPE_NORMAL
- en: Due to this demand, the options for learning Data Science are increasing. Online
    courses and websites are popular ways to learn Data Science concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the Right Data Science Project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After mastering theory, doing real projects will make you ready for job interviews.
    It also helps enrich your portfolio.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, the projects that junior developers are doing overlap often. For instance,
    house price prediction, identifying iris plants, or predicting the Titanic survivors.
    Even though they are useful projects, including these in your portfolio might
    not be the best idea.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why?**'
  prefs: []
  type: TYPE_NORMAL
- en: Because the recruiter has very limited time to look at your CV. Here is another
    research that should make you more aware of this. It’s eye-tracking research done
    by [ladders](https://www.theladders.com/static/images/basicSite/pdfs/TheLadders-EyeTracking-StudyC2.pdf).
    This career site shows that recruiters look at your resume for just 6 seconds
    on average.
  prefs: []
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/de1b841efdc773c50929eb8475788dd1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [theladders](https://www.theladders.com/static/images/basicSite/pdfs/TheLadders-EyeTracking-StudyC2.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: Now, I already mentioned Data Science's popularity. Because of its popularity,
    many developers have already turned one step toward it. So that makes the field
    extremely competitive.
  prefs: []
  type: TYPE_NORMAL
- en: To differentiate yourself from others, you must do novel projects that will
    stand out. You must follow certain stages to make the best of these projects.
    That way, you’ll stand out both by doing projects that are different from others
    and by being thorough and systematic. In other words, you’ll attract the recruiter's
    attention and then keep it by showing them you know what you’re doing.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s the overview of the project stages.
  prefs: []
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/93b76527956a350791aba0725f64ffd2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now have a look at each. I’ll also give you links to different projects
    and coding libraries. No need to reinvent the wheel in this case.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Data Collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/e3d31457ed4bbc0f4536645ea801f26f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Data Collection is the process of gathering data from different sources. Over
    the last 10 years, the amount of data created by different sources has been growing.
  prefs: []
  type: TYPE_NORMAL
- en: By 2025 the amount of collected data will have increased to almost three times
    more than today.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the [research](https://www.import.io/wp-content/uploads/2017/04/Seagate-WP-DataAge2025-March-2017.pdf)
    done by IDC, showing the changes in data created by its type.
  prefs: []
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/f2d3f94657ebdb4c72d6a8c3644a6692.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [import.io](https://www.import.io/wp-content/uploads/2017/04/Seagate-WP-DataAge2025-March-2017.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: It means the amount of data collected will continue to increase. This is the
    opportunity for businesses and developers to scrape even more data.
  prefs: []
  type: TYPE_NORMAL
- en: Why do you need more data?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After collecting data, the final stage will be model building and deploying
    it to production. So, increasing this model's performance will be highly important.
    One way to do this is by collecting more data. Here the main thing to focus on
    is how your model's performance can be improved as the amount of data increases.
  prefs: []
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/6ea2086bf12fd37c29574f5011c84c86.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Deeplearning.AI](https://www.deeplearning.ai/)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, many different techniques exist to collect data. Here I will explain 3
    of them: reading from different file sources, web scraping, and API.'
  prefs: []
  type: TYPE_NORMAL
- en: Collecting Data From Different File Sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many websites and companies will present you with data in good shape. Yet reading
    that in your coding environment will need a couple of lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: For example, [this project](https://platform.stratascratch.com/data-projects/marketing-campaign-results)
    aims to analyze the data of people who have huge debt yet have a hard time paying
    them off. As you can see, this project also starts with reading data by using
    the [read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)
    function in pandas.
  prefs: []
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/b348df1194a4153ae091e331b0f388f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Now you have data, you can perform the exploratory data analysis this project
    is asking you.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on how to do it, [here](https://www.youtube.com/watch?v=sVdNVgtpkD4&t=1s)
    is the youtube video.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different websites where you can find open-source data.
  prefs: []
  type: TYPE_NORMAL
- en: Here are 4 of them.
  prefs: []
  type: TYPE_NORMAL
- en: '[Kaggle](https://www.kaggle.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.php)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Google Datasets](https://datasetsearch.research.google.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AWS Datasets](https://registry.opendata.aws/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the data there and read them by using Python functions as has been done
    above. If your data comes in other formats ([HTML](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html),
    [JSON](https://pandas.pydata.org/docs/reference/api/pandas.read_json.html), [excel](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html)),
    you can also read them using [Pandas functions](https://pandas.pydata.org/pandas-docs/stable/reference/io.html).
  prefs: []
  type: TYPE_NORMAL
- en: Web Scraping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Web scraping is a process of using automated tools to scrape through the web
    and gather data. This means using crawlers and scrapers. They map the website
    HTML and then collect data using the given instructions.
  prefs: []
  type: TYPE_NORMAL
- en: The Crawler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Web crawlers look for any information to take from the website. It helps you
    find URLs that contain the information you want to scrape.
  prefs: []
  type: TYPE_NORMAL
- en: The Scraper
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The scraper collects the information that you preset. Every scraper has a selector
    to locate the information you want to take from the website.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generally, they work together. First, the crawler takes the information about
    your topic and finds the URLs. Then the scraper locates the information you need
    by using the selector. In the final step, you scrape this data.
  prefs: []
  type: TYPE_NORMAL
- en: Data Scraping Python Libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Python, you can scrape data by working with different libraries. Here are
    4 of them for you.
  prefs: []
  type: TYPE_NORMAL
- en: '[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Scrapy](https://docs.scrapy.org/en/latest/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Selenium](https://selenium-python.readthedocs.io/) ( This one is not official
    documentation, yet I find that one more readable.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Request](https://requests.readthedocs.io/en/latest/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here is the [video](https://www.youtube.com/watch?v=GmW4F6MHqqs) by Ken Jee.
    You can see his Youtube series of Data Science projects by dividing them into
    different steps. In the [second video](https://www.youtube.com/watch?v=GmW4F6MHqqs),
    he explains the importance of data collection. Then he scrapes data from glassdoor
    by using Selenium to make data analysis of the data scientist's salary.
  prefs: []
  type: TYPE_NORMAL
- en: API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/79ec6c2f36ae9a72bea86bc5dab5251f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: API stands for Application Programming Interface, which is used for communication
    between different programs. It helps two programs or applications to pass information
    between them.
  prefs: []
  type: TYPE_NORMAL
- en: Youtube Scraper API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here is a [Youtube API](https://developers.google.com/youtube/v3) that will
    be used to gather data in the next video. It allows you to get channel statistics
    like the total subscribers and view counts, names of the videos, comments, and
    more.
  prefs: []
  type: TYPE_NORMAL
- en: In this [video](https://www.youtube.com/watch?v=D56_Cx36oGY) on The Vu Data
    Analytics channel, the data is scraped from Youtube using the Youtube API.
  prefs: []
  type: TYPE_NORMAL
- en: First, the data is processed. After that, there’s an exploratory data analysis,
    visualization, and more.
  prefs: []
  type: TYPE_NORMAL
- en: You can see some similarities to the previous video, like cleaning data and
    doing explanatory analysis. This is normal because most data projects go through
    the same stages. But the data you use in these stages and how you use it will
    improve your projects and enrich your portfolio.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Building Data Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/03f453088c4517606431330bfebdcd1b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: It is the phase of turning your unstructured data into a meaningful version.
    The pipeline term here focuses on transformation. Sometimes this phase is also
    called data cleaning, yet visualizing pipelines and assigning them to the stages
    of data makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: After scraping, data generally won't be in the best shape.That’s why changing
    its format will be mandatory. For instance, if you have a small amount of data
    but it contains many [NAs or missing values](https://pandas.pydata.org/docs/user_guide/missing_data.html),
    filling these values into the average of the column will help you to use this
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Or for example, if you work with the [date time column](https://pandas.pydata.org/docs/user_guide/timeseries.html),
    the data type of your column might be an object. That will prevent you from applying
    the date time function to that column. So you should turn this column's data type
    to date-time.
  prefs: []
  type: TYPE_NORMAL
- en: Having more data with better quality helps you build a more effective model.
  prefs: []
  type: TYPE_NORMAL
- en: Let's give an example of the transformation needed in Machine Learning. If you
    build a model, your variables should be numerical. Yet some data has categorical
    information. To turn these variables into numerical, you should do [one hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Data Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/e72eb3447aa3447575ec2393c161ad39.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Knowing your data is really important when doing a project. To do that, you
    must first explore it by using different functions.
  prefs: []
  type: TYPE_NORMAL
- en: Here are some common Pandas functions used in data exploration.
  prefs: []
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/0f932d75b42d474545a2858509ca0191.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: First glance at your data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[head( )](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html)
    function gives you the chance to look at the first several rows of your data.
    Also, the info( ) function will give you a piece of information about your data''s
    columns, such as length and data types.'
  prefs: []
  type: TYPE_NORMAL
- en: '[describe( )](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html)
    function will give a summary of descriptive statistics.'
  prefs: []
  type: TYPE_NORMAL
- en: '[shape( )](https://numpy.org/devdocs/reference/generated/numpy.shape.html)
    function will give information about the dimension of your data, which outputs
    row/column as a tuple, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '[astype( )](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html)
    function helps you to change the format of your columns.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the chance is you won’t be working with a dataset, so merging
    data is also a common operation you’ll use.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Data Visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/bff68f02cc3f6a827c05ad9ac9474c82.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Extracting meaningful information from data becomes easier if you visualize
    it. In Python, there are many libraries you can use to visualize your data.
  prefs: []
  type: TYPE_NORMAL
- en: '[Matplotlib](https://matplotlib.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Seaborn](https://seaborn.pydata.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pandas](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Plotly](https://plotly.com/python/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should use this stage to detect the outliers and correlated predictors.
    If undetected, they will decrease your machine-learning model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Creating graphs makes this detection easier.
  prefs: []
  type: TYPE_NORMAL
- en: Outlier Detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The outliers often happen due to anomalies, and they don’t help your model’s
    predictions. Detecting the outliers is followed by removing them from your data.
  prefs: []
  type: TYPE_NORMAL
- en: '[Here](https://platform.stratascratch.com/data-projects/market-analysis-dublin?tabname=assignment)
    we can see this distribution graph, which shows the maximum prices people are
    willing to pay for a room on Airbnb in Dublin.'
  prefs: []
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/aafdcb3f9222610c61ec43fece37ddb1.png)'
  prefs: []
  type: TYPE_IMG
- en: Most people search for a room below $200/night, and especially around $100/night.
    Yet there are other people who search for rooms between 500-600.
  prefs: []
  type: TYPE_NORMAL
- en: When building a model, filtering it at a certain level will help you to predict
    the behavior more accurately.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation Graph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/afabd605751f6ed1f8f61576b1794f56.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '[Here](https://platform.stratascratch.com/data-projects/delivery-duration-prediction),
    the correlation graph detects correlated predictors, which will decrease your
    model’s performance. You can see the color scale on the right side of the graph,
    which shows that as the color density increases, the correlation increases, too
    for both negative and positive.'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Model Building
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/397417ea5dc20d147f4be2b3fd8f4aa1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Here you can see the different stages of model building.
  prefs: []
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/6f47be405f3ad42ddd5421500c8fdcea.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the Right Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First step in model building is choosing which algorithm type to use. Of course,
    it depends on your topic.
  prefs: []
  type: TYPE_NORMAL
- en: Do you work with numerical data and plan to make predictions? Then your choice
    will be [Regression](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning).
    Or do you want to classify the image with a [Classification](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)
    algorithm? It is up to your project. For instance, anomaly detection is often
    a popular project content. It can be used for credit card fraud detection and
    uses the [Clustering](https://scikit-learn.org/stable/modules/clustering.html#clustering)
    algorithms in the backend. Of course, you can use [Deep Learning](https://www.tensorflow.org/?gclid=Cj0KCQiAkMGcBhCSARIsAIW6d0DKsxizRs0kcr0r3bcwmvwmpCFofj_daKrcjgctLydLxFl9respIxYaAsBFEALw_wcB)
    in your project too.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Splitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One way to evaluate your model is to measure its performance on the data that
    the model is unfamiliar with. Splitting data into training and testing datasets
    helps you achieve that.
  prefs: []
  type: TYPE_NORMAL
- en: You train your model using the training dataset and then test its performance
    using the test set, which contains data the model is unfamiliar with.
  prefs: []
  type: TYPE_NORMAL
- en: In this stage, if you already know which model you’ll use, you can skip using
    the validation set. However, if you want to try different models to see which
    one is best, using the validation set is for you.
  prefs: []
  type: TYPE_NORMAL
- en: A/B testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After choosing the algorithm and splitting your dataset, it is time to do A/B
    testing.
  prefs: []
  type: TYPE_NORMAL
- en: There are different algorithms. How can you be sure which one will be the best
    for your model? One thing to check this is called A/B testing in machine learning.
    A/B testing in Machine Learning means that you will try different models to find
    the best for your project.
  prefs: []
  type: TYPE_NORMAL
- en: Try all possible algorithms for your data and find the best-performing algorithm
    and continue using that one.
  prefs: []
  type: TYPE_NORMAL
- en: Performance Boosting Algorithms and Techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/aab2252ce68e9e3bcc91e9ec51190cab.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The performance of your model often can be enhanced by using different techniques.
    Let’s focus on three of them.
  prefs: []
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/e54b047af38c20ebf99eb8255522323b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality Reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dimensionality reduction is used to find the predictors which will represent
    your data better than others. This technique makes your algorithm work faster,
    and your model predicts better.
  prefs: []
  type: TYPE_NORMAL
- en: '[Here](https://scikit-learn.org/stable/modules/decomposition.html#decompositions)
    you can see the application of the dimensionality reduction technique in the sci-kit
    learn library.'
  prefs: []
  type: TYPE_NORMAL
- en: '**PCA**'
  prefs: []
  type: TYPE_NORMAL
- en: PCA stands for Principal Component Analysis, which helps you determine the number
    of predictors needed to explain a certain percentage of your data set.
  prefs: []
  type: TYPE_NORMAL
- en: Let me explain this in an [ example](https://platform.stratascratch.com/data-projects/delivery-duration-prediction?tabname=assignment),
    the expected time of delivery is predicted. There are 100 predictors, so eliminating
    them will help us to enhance the speed of the algorithm and result to have a better
    working algorithm. That’s why the PCA algorithm is applied.
  prefs: []
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/e377b575feb4102569e5ea58b00718b5.png)'
  prefs: []
  type: TYPE_IMG
- en: PCA shows that at least 60 representative features are needed to explain 80%
    of the dataset. Here is the [code](https://scikit-learn.org/stable/modules/decomposition.html#principal-component-analysis-pca)
    in the sci-kit learn library with an explanation.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When building a machine learning model, there will be a lot of different parameters
    that can be used to make a prediction. Hyperparameters Tuning helps us find the
    best of these parameters according to the needs of our project.
  prefs: []
  type: TYPE_NORMAL
- en: '**Grid Search**'
  prefs: []
  type: TYPE_NORMAL
- en: Grid Search helps you to find the best parameter values to optimize your machine
    learning model.
  prefs: []
  type: TYPE_NORMAL
- en: '[Here](https://scikit-learn.org/stable/modules/grid_search.html#grid-search)
    is the code in the sci-kit learn library, which explains what grid search is and
    gives you the code of implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It helps to scale your variables at the same level, that’s how your algorithm,
    which works for prediction, works faster.
  prefs: []
  type: TYPE_NORMAL
- en: '**Standard Scaler**'
  prefs: []
  type: TYPE_NORMAL
- en: The main idea is to change your predictor’s mean, zero, and standard deviation
    to 1\. [Here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)
    is the sci-kit learn library code to help you calculate that.
  prefs: []
  type: TYPE_NORMAL
- en: Model Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/293329c5b4a3bc6fee9f6ec4c4721ed0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'This step helps us to interpret the model. That’s why there are different evaluation
    metrics for different algorithms. The interpretation of algorithm results will
    differ depending on the problem: to regression, classification, or clustering.'
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is a technique to find relations between numerical variables. Simply, we
    will use regression to predict numeric variables.
  prefs: []
  type: TYPE_NORMAL
- en: Here are the two evaluation metrics of regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: '**MSE**'
  prefs: []
  type: TYPE_NORMAL
- en: Mean squared error is calculated by finding the differences between the predicted
    value and actual value and squaring the result, for every element of your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the formula.
  prefs: []
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/9c56a7b3e3a014a03022d6978ac591ca.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)
    is the implementation of MSE in sci-kit learn with Python.'
  prefs: []
  type: TYPE_NORMAL
- en: '**RMSE**'
  prefs: []
  type: TYPE_NORMAL
- en: RMSE is the root of the MSE.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the formula.
  prefs: []
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/6411a21ade39ae2f90f29e7dcf09d4e9.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)
    is the implementation of RMSE in sci-kit learn with Python.'
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Classification algorithms split your data into different groups and define them
    accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: The classification evaluation process often includes a confusion matrix, which
    includes True Class and predicted class. (True Positive, False Positive, True
    Negative, False Negative)
  prefs: []
  type: TYPE_NORMAL
- en: Here [is](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)
    the implementation of the confusion matrix in sci-kit learn.
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision**'
  prefs: []
  type: TYPE_NORMAL
- en: This is to evaluate the accuracy of positive predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the formula.
  prefs: []
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/5981191f75ba79145efe15d1f690c682.png)'
  prefs: []
  type: TYPE_IMG
- en: TP = True Positive
  prefs: []
  type: TYPE_NORMAL
- en: FP = False Positive
  prefs: []
  type: TYPE_NORMAL
- en: '[Here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)
    is the implementation of precision in Python with the sci-kit learn.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall**'
  prefs: []
  type: TYPE_NORMAL
- en: Recall or sensitivity, it is the ratio of positive examples that are correctly
    classified by your algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the formula.
  prefs: []
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/58235f2d577653e47a155cb7710ff9e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'FN: False Negative'
  prefs: []
  type: TYPE_NORMAL
- en: '[Here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)
    is the implementation of recall in Python with the sci-kit learn.'
  prefs: []
  type: TYPE_NORMAL
- en: '**F1 Score**'
  prefs: []
  type: TYPE_NORMAL
- en: It is the harmonic mean of precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the formula
  prefs: []
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/2ea3e4abbe7d9f2ba32b485d00153dac.png)'
  prefs: []
  type: TYPE_IMG
- en: Clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First step of clustering is grouping the data points as a cluster. The second
    step is assigning them.
  prefs: []
  type: TYPE_NORMAL
- en: If you choose the clustering algorithm, here are two evaluation metrics to interpret
    your model's performance. Also, I added their formula and the link to the sci-kit
    learn library.
  prefs: []
  type: TYPE_NORMAL
- en: '**Purity**'
  prefs: []
  type: TYPE_NORMAL
- en: The purity is the percentage of total data points classified correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the formula
  prefs: []
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/4ba9ee953624299c5d91c329c0ce4fdf.png)'
  prefs: []
  type: TYPE_IMG
- en: To calculate purity, you should first calculate the [confusion matrix.](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)
  prefs: []
  type: TYPE_NORMAL
- en: '**RandIndex**'
  prefs: []
  type: TYPE_NORMAL
- en: It measures the similarity between two clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/16eacb4ecf9802d059bc338bb819e014.png)'
  prefs: []
  type: TYPE_IMG
- en: TP = True Positive
  prefs: []
  type: TYPE_NORMAL
- en: FP = False Positive
  prefs: []
  type: TYPE_NORMAL
- en: TN = True Negative
  prefs: []
  type: TYPE_NORMAL
- en: FN = False Negative
  prefs: []
  type: TYPE_NORMAL
- en: '[Here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.rand_score.html)
    is the implementation of RandIndex in Python with the sci-kit learn.'
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Model in Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/67876eb43df85e03c0ef78405042114b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'After building your machine learning model, it is time to see its performance
    by presenting it to different users using cloud services. Here I will explain
    how to do that by using two different libraries in Python: Flask and Django.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, there are different options to run and host your model, such as [Heroku](https://www.heroku.com/),
    [Digitalocean](https://www.digitalocean.com/), [pythonanywhere.com](https://www.pythonanywhere.com/),
    which you can use for your own project.
  prefs: []
  type: TYPE_NORMAL
- en: Web APP with Flask
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flask is a Python library which will give you the chance to write web applications.
    For more info, please visit the official website [here.](https://flask.palletsprojects.com/en/2.2.x/)
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you developed a [machine learning model](http://gencay.pythonanywhere.com/)
    to predict users' weight using body measurements and age.
  prefs: []
  type: TYPE_NORMAL
- en: To do that, you first need to access the data so you can build Multiple Linear
    Regression. Here you can reach the body fat prediction open-source [dataset](https://www.kaggle.com/datasets/fedesoriano/body-fat-prediction-dataset)
    to create a machine-learning model which will predict the weight.
  prefs: []
  type: TYPE_NORMAL
- en: By using pythonanywhere.com, you can run your model and host it on the website
    at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: API with Django REST
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I already mentioned what API is, and now here I will mention its implementation
    options. You can write your API by using the Django REST framework. Django is
    like a flask; it is a micro web framework. Through using it, you can also code
    the backend and also frontend of the application and also write API.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s talk about the advantages of developing API with Django REST.
  prefs: []
  type: TYPE_NORMAL
- en: The community is active, and the documentation is extensive.
  prefs: []
  type: TYPE_NORMAL
- en: Also, [here](https://www.techrepublic.com/article/build-apis-python/) are the
    other Python frameworks; by using these, you can develop an API.
  prefs: []
  type: TYPE_NORMAL
- en: On top of these six main project stages, here are the bonus two.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Subscription to Your App
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can include a subscription plan in your Web APP. Suppose you developed an
    OCR algorithm which targets to distract information from documents by doing image
    processing, like [Docsumo](https://docsumo.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Here are the different options of their pricing system.
  prefs: []
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/ccfcdeffa095c7ac842b6d3b8a2f6289.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [docsumo.com](https://www.docsumo.com/pricing)
  prefs: []
  type: TYPE_NORMAL
- en: Of course, before turning your web application into a business, many stages
    exist to follow, yet the final destination could be like this one.
  prefs: []
  type: TYPE_NORMAL
- en: To gain income by using API, here is a [website](https://rapidapi.com/) where
    you can upload your API, and it also contains thousands of different APIs.
  prefs: []
  type: TYPE_NORMAL
- en: You can monetize your API by adding different subscription options and gain
    income afterward.
  prefs: []
  type: TYPE_NORMAL
- en: Here is a face recognition [api](https://rapidapi.com/eyerecognize/api/face-detection-and-facial-features/pricing),
    which helps you to turn your text into speech. This API possibly uses CNN in the
    backend to recognize faces.
  prefs: []
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/95373768f894ea7dc690d3b063553f4a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [RapidAPI](https://rapidapi.com/eyerecognize/api/face-detection-and-facial-features/pricing)
  prefs: []
  type: TYPE_NORMAL
- en: If you want to develop an algorithm for face recognition, here you can reach
    a great [tutorial](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78)
    written by Adem Geitgay, which explains the stages of face recognition without
    going deeper and makes it easier by developing his own library.
  prefs: []
  type: TYPE_NORMAL
- en: Model Maintenance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![From Data Collection to Model Deployment: 6 Stages of a Data Science Project](../Images/b1dd8f5e44325ef8e7211f35d8382368.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: After uploading your model into production, it should be regularly maintained.
    Since your model will use your user's information, your algorithm should be updated
    regularly.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say you build a Deep Learning Model which will predict the distance between
    two objects in the user’s photo. When training your model, you possibly use high-quality
    images to do that. Yet, real-life data might not be the quality you’re hoping
    for.
  prefs: []
  type: TYPE_NORMAL
- en: When looking at your evaluation metrics, you can see the drop in your model’s
    performance due to this technical issue. Of course, there are many possible solutions
    to overcome this problem.
  prefs: []
  type: TYPE_NORMAL
- en: One of them is adding noise to the pictures in your model.
  prefs: []
  type: TYPE_NORMAL
- en: That’s how your model will also have the ability to predict low-quality images
    too.
  prefs: []
  type: TYPE_NORMAL
- en: So to keep your model’s performance always high, it needs regular updates like
    that. Also, you should take care of your customer’s feedback regularly to keep
    them satisfied with your work and debug their problems.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating a project often starts with data collection. To differentiate from
    others, use some or all the data collection options I covered in the article.
    Along with that, use different open-source websites where you can reach different
    data sets.
  prefs: []
  type: TYPE_NORMAL
- en: After collecting your data, building a pipeline, and turning your data into
    the right format comes next. After that, it’s time to extract meaningful info
    from it by exploring it and then visualizing it. The next stage is to build a
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Here we examined model building and performance booster algorithms, together
    with the evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there are many options for deploying your models into production,
    of which I covered some.
  prefs: []
  type: TYPE_NORMAL
- en: Following these stages, will help you to ace your project, enrich your portfolio
    and earn possible income.
  prefs: []
  type: TYPE_NORMAL
- en: As you gain experience, feel free to add the stage according to your needs between
    them.
  prefs: []
  type: TYPE_NORMAL
- en: After you finish the project, try mapping these stages into your mind. Because
    explaining them to the interviewers in detail, also will help you to ace the interview
    and land a new job.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Nate Rosidi](https://www.stratascratch.com)** is a data scientist and in
    product strategy. He''s also an adjunct professor teaching analytics, and is the
    founder of [StrataScratch](https://www.stratascratch.com/), a platform helping
    data scientists prepare for their interviews with real interview questions from
    top companies. Connect with him on [Twitter: StrataScratch](https://twitter.com/StrataScratch)
    or [LinkedIn](https://www.linkedin.com/in/nathanrosidi/).'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Top 7 Model Deployment and Serving Tools](https://www.kdnuggets.com/top-7-model-deployment-and-serving-tools)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Full End-to-End Deployment of a Machine Learning Algorithm into a…](https://www.kdnuggets.com/2021/12/deployment-machine-learning-algorithm-live-production-environment.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Back to Basics Week 4: Advanced Topics and Deployment](https://www.kdnuggets.com/back-to-basics-week-4-advanced-topics-and-deployment)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Science Definition Humor: A Collection of Quirky Quotes…](https://www.kdnuggets.com/2022/02/data-science-definition-humor.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets™ News 22:n06, Feb 9: Data Science Programming…](https://www.kdnuggets.com/2022/n06.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, April 6: 8 Free MIT Courses to Learn Data Science…](https://www.kdnuggets.com/2022/n14.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
