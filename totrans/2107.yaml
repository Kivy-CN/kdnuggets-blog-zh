- en: 'Ollama Tutorial: Running LLMs Locally Made Super Simple'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ollama 教程：本地运行 LLMs 变得超级简单
- en: 原文：[https://www.kdnuggets.com/ollama-tutorial-running-llms-locally-made-super-simple](https://www.kdnuggets.com/ollama-tutorial-running-llms-locally-made-super-simple)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/ollama-tutorial-running-llms-locally-made-super-simple](https://www.kdnuggets.com/ollama-tutorial-running-llms-locally-made-super-simple)
- en: '![ollama-tutorial](../Images/1f45326dd049cd0966bfbd8d5988835b.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![ollama-tutorial](../Images/1f45326dd049cd0966bfbd8d5988835b.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Running large language models (LLMs) locally can be super helpful—whether you'd
    like to play around with LLMs or build more powerful apps using them. But configuring
    your working environment and getting LLMs to run on your machine is not trivial.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地运行大型语言模型（LLMs）非常有帮助——无论你是想玩玩 LLMs 还是使用它们构建更强大的应用程序。但是，配置工作环境并让 LLMs 在你的机器上运行并不简单。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织在 IT 方面'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: So how do you run LLMs locally without any of the hassle? **Enter Ollama**,
    a platform that makes local development with open-source large language models
    a breeze. With Ollama, everything you need to run an LLM—model weights and all
    of the config—is packaged into a single Modelfile. **Think Docker for LLMs**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何在没有麻烦的情况下本地运行 LLMs 呢？**Ollama** 就是答案，它使得使用开源大型语言模型的本地开发变得轻而易举。使用 Ollama，你运行
    LLM 所需的一切——模型权重和所有配置——都被打包成一个 Modelfile。**就像 Docker 对 LLMs 一样**。
- en: In this tutorial, we’ll take a look at how to get started with Ollama to run
    large language models locally. So let’s get right into the steps!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将深入了解如何使用 Ollama 在本地运行大型语言模型。所以让我们直接进入步骤！
- en: 'Step 1: Download Ollama to Get Started'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步：下载 Ollama 以开始
- en: 'As a first step, you should download Ollama to your machine. Ollama is supported
    on all major platforms: MacOS, Windows, and Linux.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你应该将 Ollama 下载到你的机器上。Ollama 支持所有主要平台：MacOS、Windows 和 Linux。
- en: To download Ollama, you can either visit the [official GitHub repo](https://github.com/ollama/ollama)
    and follow the download links from there. Or visit the [official website](https://ollama.com/)
    and download the installer if you are on a Mac or a Windows machine.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载 Ollama，你可以访问 [官方 GitHub 仓库](https://github.com/ollama/ollama) 并从那里跟随下载链接。或者访问
    [官方网站](https://ollama.com/) 下载适用于 Mac 或 Windows 机器的安装程序。
- en: 'I’m on Linux: Ubuntu distro. So if you’re a Linux user like me, you can run
    the following command to run the installer script:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用的是 Linux：Ubuntu 发行版。如果你也是像我一样的 Linux 用户，你可以运行以下命令来执行安装脚本：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The installation process typically takes a few minutes. During the installation
    process, any NVIDIA/AMD GPUs will be auto-detected. Make sure you have the drivers
    installed. The CPU-only mode works fine, too. But it may be much slower.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 安装过程通常需要几分钟。在安装过程中，任何 NVIDIA/AMD GPU 将被自动检测。确保你已安装驱动程序。仅 CPU 模式也可以正常工作，但可能会慢很多。
- en: 'Step 2: Get the Model'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二步：获取模型
- en: Next, you can visit the [model library](https://ollama.com/library) to check
    the list of all model families currently supported. The default model downloaded
    is the one with the `latest` tag. On the page for each model, you can get more
    info such as the size and quantization used.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你可以访问 [模型库](https://ollama.com/library) 查看当前支持的所有模型家族列表。默认下载的模型是带有 `latest`
    标签的。在每个模型的页面上，你可以获取更多信息，如大小和量化方式。
- en: You can search through the list of tags to locate the model that you want to
    run. For each model family, there are typically foundational models of different
    sizes and instruction-tuned variants. I’m interested in running the Gemma 2B model
    from the [Gemma family of lightweight models](https://ai.google.dev/gemma) from
    Google DeepMind.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过标签列表搜索以找到你想要运行的模型。对于每个模型系列，通常有不同大小的基础模型和经过指令调优的变体。我对运行来自 Google DeepMind
    的 [Gemma 轻量级模型系列](https://ai.google.dev/gemma) 中的 Gemma 2B 模型感兴趣。
- en: You can run the model using the `ollama run` command to pull and start interacting
    with the model directly. However, you can also pull the model onto your machine
    first and then run it. This is very similar to how you work with Docker images.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 `ollama run` 命令来运行模型，以直接与模型进行交互。不过，你也可以先将模型下载到你的机器上，然后再运行。这与使用 Docker
    镜像的方式非常相似。
- en: 'For Gemma 2B, running the following pull command downloads the model onto your
    machine:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Gemma 2B，运行以下拉取命令将模型下载到你的机器上：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The model is of size 1.7B and the pull should take a minute or two:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的大小为 1.7B，拉取应该需要一两分钟：
- en: '![ollama-pull](../Images/d1a077a8dd047ac39c1a9b9b8e445478.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![ollama-pull](../Images/d1a077a8dd047ac39c1a9b9b8e445478.png)'
- en: 'Step 3: Run the Model'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 3：运行模型
- en: 'Run the model using the `ollama run` command as shown:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用如上所示的 `ollama run` 命令运行模型：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Doing so will start an Ollama REPL at which you can interact with the Gemma
    2B model. Here’s an example:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这样会启动一个 Ollama REPL，你可以在其中与 Gemma 2B 模型进行交互。以下是一个示例：
- en: '![ollama-response](../Images/8644500443780001ff507a342f3ec1f8.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![ollama-response](../Images/8644500443780001ff507a342f3ec1f8.png)'
- en: For a simple question about the Python standard library, the response seems
    pretty okay. And includes most frequently used modules.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于关于 Python 标准库的简单问题，回应似乎相当不错。并且包括了最常用的模块。
- en: 'Step 4: Customize Model Behavior with System Prompts'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 4：通过系统提示自定义模型行为
- en: 'You can customize LLMs by setting system prompts for a specific desired behavior
    like so:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过设置系统提示自定义 LLMs，以实现特定的期望行为：
- en: Set system prompt for desired behavior.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置系统提示以实现期望的行为。
- en: Save the model by giving it a name.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过给模型命名来保存它。
- en: Exit the REPL and run the model you just created.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 退出 REPL 并运行你刚刚创建的模型。
- en: 'Say you want the model to always explain concepts or answer questions in plain
    English with minimal technical jargon as possible. Here’s how you can go about
    doing it:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你希望模型始终用尽可能简单的英语解释概念或回答问题。以下是实现方法：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now run the model you just created:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在运行你刚刚创建的模型：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here’s an example:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例：
- en: '![ipe-response](../Images/5818ee682cda05d60d9c3d1b28fa2057.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![ipe-response](../Images/5818ee682cda05d60d9c3d1b28fa2057.png)'
- en: 'Step 5: Use Ollama with Python'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 5：在 Python 中使用 Ollama
- en: Running the Ollama command-line client and interacting with LLMs locally at
    the Ollama REPL is a good start. But often you would want to use LLMs in your
    applications. You can run Ollama as a server on your machine and run cURL requests.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 Ollama 命令行客户端并在 Ollama REPL 本地与 LLMs 进行交互是一个很好的开始。但通常你会希望在应用程序中使用 LLMs。你可以在你的机器上将
    Ollama 作为服务器运行，并运行 cURL 请求。
- en: 'But there are simpler ways. If you like using Python, you’d want to build LLM
    apps and here are a couple ways you can do it:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 但也有更简单的方法。如果你喜欢使用 Python，你会想要构建 LLM 应用程序，以下是几种方法：
- en: Using the official Ollama Python library
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用官方 Ollama Python 库
- en: Using Ollama with LangChain
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Ollama 与 LangChain
- en: Pull the models you need to use before you run the snippets in the following
    sections.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行以下部分中的代码片段之前，请拉取你需要使用的模型。
- en: Using the Ollama Python Library
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Ollama Python 库
- en: 'To use the Ollama Python library you can install it using pip like so:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Ollama Python 库，你可以像这样使用 pip 安装：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: There is an official JavaScript library too, which you can use if you prefer
    developing with JS.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个官方的 JavaScript 库，如果你更喜欢用 JS 开发，可以使用它。
- en: 'Once you install the Ollama Python library, you can import it in your Python
    application and work with large language models. Here''s the snippet for a simple
    language generation task:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你安装了 Ollama Python 库，你可以在 Python 应用程序中导入它并使用大型语言模型。以下是一个简单语言生成任务的代码片段：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Using LangChain
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 LangChain
- en: Another way to use Ollama with Python is using [LangChain](https://www.langchain.com/).
    If you have existing projects using LangChain it's easy to integrate or switch
    to Ollama.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种在 Python 中使用 Ollama 的方法是使用 [LangChain](https://www.langchain.com/)。如果你有现有的使用
    LangChain 的项目，集成或切换到 Ollama 会很容易。
- en: 'Make sure you have LangChain installed. If not, install it using pip:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你已安装 LangChain。如果没有，请使用 pip 安装：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here''s an example:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示例：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Using LLMs like this in Python apps makes it easier to switch between different
    LLMs depending on the application.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 应用程序中使用 LLM 使得根据应用程序的需要在不同的 LLM 之间切换变得更加容易。
- en: Wrapping Up
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: With Ollama you can run large language models locally and build LLM-powered
    apps with just a few lines of Python code. Here we explored how to interact with
    LLMs at the Ollama REPL as well as from within Python applications.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Ollama，你可以在本地运行大型语言模型，并通过几行 Python 代码构建由 LLM 驱动的应用程序。在这里，我们探讨了如何在 Ollama
    REPL 中以及在 Python 应用程序中与 LLM 进行交互。
- en: Next we'll try building an app using Ollama and Python. Until then, if you’re
    looking to dive deep into LLMs check out [7 Steps to Mastering Large Language
    Models (LLMs)](https://www.kdnuggets.com/7-steps-to-mastering-large-language-models-llms).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将尝试使用 Ollama 和 Python 构建一个应用程序。在那之前，如果你希望深入了解 LLM，请查看 [掌握大型语言模型的 7 个步骤](https://www.kdnuggets.com/7-steps-to-mastering-large-language-models-llms)。
- en: '**[](https://twitter.com/balawc27)**[Bala Priya C](https://www.kdnuggets.com/wp-content/uploads/bala-priya-author-image-update-230821.jpg)****
    is a developer and technical writer from India. She likes working at the intersection
    of math, programming, data science, and content creation. Her areas of interest
    and expertise include DevOps, data science, and natural language processing. She
    enjoys reading, writing, coding, and coffee! Currently, she''s working on learning
    and sharing her knowledge with the developer community by authoring tutorials,
    how-to guides, opinion pieces, and more. Bala also creates engaging resource overviews
    and coding tutorials.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**[](https://twitter.com/balawc27)**[Bala Priya C](https://www.kdnuggets.com/wp-content/uploads/bala-priya-author-image-update-230821.jpg)****
    是来自印度的开发者和技术作家。她喜欢在数学、编程、数据科学和内容创作的交汇点上工作。她的兴趣和专业领域包括 DevOps、数据科学和自然语言处理。她喜欢阅读、写作、编程和喝咖啡！目前，她正在通过撰写教程、操作指南、观点文章等与开发者社区分享她的知识。Bala
    还创建了引人入胜的资源概述和编码教程。'
- en: More On This Topic
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[A Simple Guide to Running LlaMA 2 Locally](https://www.kdnuggets.com/a-simple-guide-to-running-llama-2-locally)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在本地运行 LlaMA 2 的简单指南](https://www.kdnuggets.com/a-simple-guide-to-running-llama-2-locally)'
- en: '[Pydantic Tutorial: Data Validation in Python Made Simple](https://www.kdnuggets.com/pydantic-tutorial-data-validation-in-python-made-simple)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Pydantic 教程：Python 中的数据验证简化版](https://www.kdnuggets.com/pydantic-tutorial-data-validation-in-python-made-simple)'
- en: '[The Easiest Way of Running Llama 3 Locally](https://www.kdnuggets.com/easiest-way-of-running-llama-3-locally)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在本地运行 Llama 3 的最简单方法](https://www.kdnuggets.com/easiest-way-of-running-llama-3-locally)'
- en: '[Combining Pandas DataFrames Made Simple](https://www.kdnuggets.com/2022/09/combining-pandas-dataframes-made-simple.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[简化 Pandas DataFrames 的合并](https://www.kdnuggets.com/2022/09/combining-pandas-dataframes-made-simple.html)'
- en: '[Personalized AI Made Simple: Your No-Code Guide to Adapting GPTs](https://www.kdnuggets.com/personalized-ai-made-simple-your-no-code-guide-to-adapting-gpts)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[简化个性化 AI：适应 GPT 的无代码指南](https://www.kdnuggets.com/personalized-ai-made-simple-your-no-code-guide-to-adapting-gpts)'
- en: '[Machine Learning Made Simple for Data Analysts with BigQuery ML](https://www.kdnuggets.com/machine-learning-made-simple-for-data-analysts-with-bigquery-ml)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 BigQuery ML 为数据分析师简化机器学习](https://www.kdnuggets.com/machine-learning-made-simple-for-data-analysts-with-bigquery-ml)'
