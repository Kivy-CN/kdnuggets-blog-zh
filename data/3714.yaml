- en: The Ultimate Guide To Different Word Embedding Techniques In NLP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP 中不同词嵌入技术的终极指南
- en: 原文：[https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html](https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html](https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html)
- en: '![](../Images/f2707b9df702dfb7da8ae07a7e6d5bdb.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2707b9df702dfb7da8ae07a7e6d5bdb.png)'
- en: '* * *'
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '* * *'
- en: ''
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_BQ
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: ''
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: ''
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: ''
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织 IT'
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '* * *'
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '"You shall know a word by the company it keeps!"'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “你可以通过一个词所处的环境来了解它！”
- en: —John Rupert Firth
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —约翰·鲁珀特·费斯
- en: Wouldn’t it be incredible if computers could start understanding Shakespeare?
    Or write fiction like J.K Rowling? This was unimaginable a few years back. Recent
    advancements in [Natural Language Processing](https://www.ibm.com/cloud/learn/natural-language-processing)
    (NLP) and [Natural Language Generation](https://www.techopedia.com/definition/33012/natural-language-generation-nlg)
    (NLG) have skyrocketed the ability of computers to better understand text-based
    content.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果计算机能开始理解莎士比亚的作品？或者像 J.K. 罗琳那样写小说？这在几年前是不可想象的。近年来，[自然语言处理](https://www.ibm.com/cloud/learn/natural-language-processing)（NLP）和[自然语言生成](https://www.techopedia.com/definition/33012/natural-language-generation-nlg)（NLG）的最新进展使计算机在理解基于文本的内容方面的能力大大提升。
- en: To understand and generate text, NLP-powered systems must be able to recognize
    words, grammar, and a whole lot of language nuances. For computers, this is easier
    said than done because they can only comprehend numbers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解和生成文本，基于 NLP 的系统必须能够识别单词、语法以及大量的语言细微差别。对于计算机来说，这比说起来要困难得多，因为它们只能理解数字。
- en: To bridge the gap, NLP experts developed a technique called *word embeddings*
    that convert words into their numerical representations. Once converted, NLP algorithms
    can easily digest these learned representations to process textual information.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补这一差距，自然语言处理（NLP）专家开发了一种称为*词嵌入*的技术，将单词转换为其数值表示。一旦转换完成，自然语言处理算法可以轻松处理这些学习到的表示，以处理文本信息。
- en: Word embeddings map the words as real-valued numerical vectors. It does so by
    tokenizing each word in a sequence (or sentence) and converting them into a vector
    space. Word embeddings aim to capture the semantic meaning of words in a sequence
    of text. It assigns similar numerical representations to words that have similar
    meanings.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入将单词映射为实值数值向量。它通过对序列（或句子）中的每个单词进行标记化，并将其转换为向量空间来实现。词嵌入旨在捕捉文本序列中单词的语义意义。它为具有相似含义的单词分配类似的数值表示。
- en: Let’s have a look at some of the most promising word embedding techniques in
    NLP.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看 NLP 中一些最有前景的词嵌入技术。
- en: TF-IDF — Term Frequency-Inverse Document Frequency
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TF-IDF — 词频-逆文档频率
- en: 'TF-IDF is a machine learning (ML) algorithm based on a statistical measure
    of finding the relevance of words in the text. The text can be in the form of
    a document or various documents (corpus). It is a combination of two metrics:
    Term Frequency (TF) and Inverse Document Frequency (IDF).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 是一种基于统计度量的机器学习（ML）算法，用于找出文本中单词的相关性。文本可以是一个文档或多个文档（语料库）。它是两种指标的结合：词频（TF）和逆文档频率（IDF）。
- en: TF score is based on the frequency of words in a document. Words are counted
    for their number of occurrences in the documents. TF is calculated by dividing
    the number of occurrences of a word (i) by the total number (N) of words in the
    document (j).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: TF 分数是基于文档中单词的频率。通过计算单词在文档中出现的次数来确定 TF。TF 的计算方法是将单词（i）出现的次数除以文档（j）中的总单词数（N）。
- en: '*TF (i) = log (frequency (i,j)) / log (N (j))*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*TF (i) = log (frequency (i,j)) / log (N (j))*'
- en: IDF score calculates the rarity of the words. It is important because TF gives
    more weightage to words that occur more frequently. However, words that are rarely
    used in the corpus may hold significant information. IDF captures this information.
    It can be calculated by dividing the total number (N) of documents (d) by the
    number of documents containing the word (i).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: IDF 分数计算词语的稀有性。这一点很重要，因为 TF 会给出现频率较高的词语更多的权重。然而，在语料库中很少使用的词语可能包含重要的信息。IDF 捕捉到这些信息。它可以通过将文档总数（N）除以包含该词语的文档数量（i）来计算。
- en: '*IDF (i) = log (N (d) / frequency (d,i))*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*IDF (i) = log (N (d) / frequency (d,i))*'
- en: The *log* is taken in the above formulas to dampen the effect of large scores
    for TF and IDF. The final TF-IDF score is calculated by multiplying TF and IDF
    scores.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述公式中使用 *log* 来减弱 TF 和 IDF 大分数的影响。最终的 TF-IDF 分数通过将 TF 和 IDF 分数相乘来计算。
- en: TF-IDF algorithm is used in solving simpler ML and NLP problems. It is better
    used for information retrieval, keyword extraction, stop words (like ‘a’, ‘the’,
    ‘are’, ‘is’) removal, and basic [text analysis](https://algoscale.com/data-analytics/text-analytics/).
    It cannot capture the semantic meaning of words in a sequence efficiently.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 算法用于解决简单的 ML 和 NLP 问题。它更适合用于信息检索、关键词提取、停用词（如 ‘a’，‘the’，‘are’，‘is’）移除以及基本的
    [文本分析](https://algoscale.com/data-analytics/text-analytics/)。它无法高效捕捉词语序列的语义意义。
- en: Word2Vec — Capturing Semantic Information
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2Vec — 捕捉语义信息
- en: Developed by Tomas Mikolov and other researchers at Google in 2013, [Word2Vec](https://arxiv.org/abs/1301.3781)
    is a word embedding technique for solving advanced NLP problems. It can iterate
    over a large corpus of text to learn associations or dependencies among words.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Tomas Mikolov 和 Google 的其他研究人员于 2013 年开发的 [Word2Vec](https://arxiv.org/abs/1301.3781)
    是一种用于解决高级 NLP 问题的词嵌入技术。它可以遍历大量的文本语料库来学习词语之间的关联或依赖关系。
- en: Word2Vec finds similarities among words by using the [*cosine similarity*](https://www.machinelearningplus.com/nlp/cosine-similarity/)
    metric. If the cosine angle is 1, that means words are overlapping. If the cosine
    angle is 90, that means words are independent or hold no contextual similarity.
    It assigns similar vector representations to similar words.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 通过使用 [*余弦相似度*](https://www.machinelearningplus.com/nlp/cosine-similarity/)
    度量来发现词语之间的相似性。如果余弦角度为 1，意味着词语重叠。如果余弦角度为 90，则意味着词语是独立的或没有上下文相似性。它为相似的词语分配相似的向量表示。
- en: 'Word2Vec offers two [neural network](https://www.ibm.com/cloud/learn/neural-networks)-based
    variants: Continuous Bag of Words (CBOW) and Skip-gram. In CBOW, the neural network
    model takes various words as input and predicts the target word that is closely
    related to the context of the input words. On the other hand, the Skip-gram architecture
    takes one word as input and predicts its closely related context words.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 提供了两种基于 [神经网络](https://www.ibm.com/cloud/learn/neural-networks) 的变体：连续词袋模型（CBOW）和
    Skip-gram。在 CBOW 中，神经网络模型以各种词语作为输入，预测与输入词语上下文紧密相关的目标词语。另一方面，Skip-gram 架构以一个词语作为输入，预测与之紧密相关的上下文词语。
- en: CBOW is quick and finds better numerical representations for frequent words,
    while Skip Gram can efficiently represent rare words. Word2Vec models are good
    at capturing semantic relationships among words. For example, the relationship
    between a country and its capital, like Paris is the capital of France and Berlin
    is the capital of Germany. It is best suited for performing [semantic analysis](https://www.expert.ai/blog/natural-language-process-semantic-analysis-definition/),
    which has application in recommendation systems and knowledge discovery.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: CBOW 速度较快，并为常见词语找到更好的数值表示，而 Skip Gram 能有效地表示稀有词语。Word2Vec 模型擅长捕捉词语之间的语义关系。例如，国家与其首都之间的关系，如巴黎是法国的首都，柏林是德国的首都。它最适合执行
    [语义分析](https://www.expert.ai/blog/natural-language-process-semantic-analysis-definition/)，这在推荐系统和知识发现中有应用。
- en: '![](../Images/9b27c49f170dd269f575b79015ec9c8b.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b27c49f170dd269f575b79015ec9c8b.png)'
- en: 'CBOW & Skip-gram architectures. Image Source: [Word2Vec paper](https://arxiv.org/abs/1301.3781).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: CBOW & Skip-gram 架构。图片来源： [Word2Vec 论文](https://arxiv.org/abs/1301.3781)。
- en: GloVe — Global Vectors for Word Representation
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GloVe — 全局词向量表示
- en: Developed by Jeffery Pennington and other researchers at Stanford, [GloVe](https://nlp.stanford.edu/pubs/glove.pdf)
    extends the work of Word2Vec to capture global contextual information in a text
    corpus by calculating a global [word-word co-occurrence matrix](https://medium.com/swlh/co-occurrence-matrix-9cacc5dd396e).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由Jeffery Pennington及斯坦福大学的其他研究人员开发的，[GloVe](https://nlp.stanford.edu/pubs/glove.pdf)扩展了Word2Vec的工作，通过计算全球[词-词共现矩阵](https://medium.com/swlh/co-occurrence-matrix-9cacc5dd396e)来捕捉文本语料库中的全球上下文信息。
- en: Word2Vec only captures the local context of words. During training, it only
    considers neighboring words to capture the context. GloVe considers the entire
    corpus and creates a large matrix that can capture the co-occurrence of words
    within the corpus.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec仅捕捉词汇的局部上下文。在训练过程中，它仅考虑邻近的词来捕捉上下文。GloVe考虑整个语料库，并创建一个大矩阵，可以捕捉语料库中词汇的共现。
- en: 'GloVe combines the advantages of two-word vector learning methods: matrix factorization
    like [latent semantic analysis](https://blog.marketmuse.com/glossary/latent-semantic-analysis-definition/)
    (LSA) and local context window method like Skip-gram. The GloVe technique has
    a simpler [least square](https://www.investopedia.com/terms/l/least-squares-method.asp)
    cost or error function that reduces the computational cost of training the model.
    The resulting word embeddings are different and improved.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe结合了两种词向量学习方法的优点：矩阵分解方法，如[潜在语义分析](https://blog.marketmuse.com/glossary/latent-semantic-analysis-definition/)（LSA）和局部上下文窗口方法，如Skip-gram。GloVe技术具有更简单的[最小二乘](https://www.investopedia.com/terms/l/least-squares-method.asp)成本或误差函数，从而降低了模型训练的计算成本。生成的词嵌入在某些方面有所不同且有所改进。
- en: GloVe performs significantly better in word analogy and [named entity recognition](https://www.expert.ai/blog/entity-extraction-work/)
    problems. It is better than Word2Vec in some tasks and competes in others. However,
    both techniques are good at capturing semantic information within a corpus.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe在词汇类比和[命名实体识别](https://www.expert.ai/blog/entity-extraction-work/)问题上表现显著更好。在某些任务中，它优于Word2Vec，并在其他任务中与之竞争。然而，两种技术在捕捉语料库中的语义信息方面都表现良好。
- en: '![](../Images/b23719ec023f20703fbec6be04b53e85.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b23719ec023f20703fbec6be04b53e85.png)'
- en: 'GloVe word vectors capturing words with similar semantics. Image Source: [Stanford
    GloVe](https://nlp.stanford.edu/projects/glove/).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe词向量捕捉到语义相似的词。图片来源：[斯坦福GloVe](https://nlp.stanford.edu/projects/glove/)。
- en: BERT — Bidirectional Encoder Representations from Transformers
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERT—来自transformers的双向编码器表示
- en: '[Introduced](https://arxiv.org/abs/1810.04805) by Google in 2019, [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)
    belongs to a class of NLP-based language algorithms known as [transformers](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html).
    BERT is a massive pre-trained deeply bidirectional encoder-based transformer model
    that comes in two variants. BERT-Base has 110 million parameters, and BERT-Large
    has 340 million parameters.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[由Google于2019年介绍](https://arxiv.org/abs/1810.04805)，[BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)属于一种基于NLP的语言算法类别，称为[transformers](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)。BERT是一个大规模预训练的深度双向编码器基础的transformer模型，有两个变体。BERT-Base有1.1亿个参数，而BERT-Large有3.4亿个参数。'
- en: For generating word embeddings, BERT relies on an [*attention mechanism*](https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/).
    It generates high-quality context-aware or contextualized word embeddings. During
    the training process, embeddings are refined by passing through each BERT encoder
    layer. For each word, the attention mechanism captures word associations based
    on the words on the left and the words on the right. Word embeddings are also
    positionally encoded to keep track of the pattern or position of each word in
    a sentence.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成词嵌入时，BERT依赖于[*注意机制*](https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/)。它生成高质量的上下文感知或上下文化的词嵌入。在训练过程中，嵌入通过每一层BERT编码器进行细化。对于每个词，注意机制根据左侧和右侧的词捕捉词汇关联。词嵌入也进行位置编码，以跟踪句子中每个词的模式或位置。
- en: BERT is more advanced than any of the techniques discussed above. It creates
    better word embeddings as the model is pre-trained on massive word corpus and
    Wikipedia datasets. BERT can be improved by fine-tuning the embeddings on task-specific
    datasets.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: BERT比上述讨论的任何技术都更先进。它通过在大规模词汇语料库和维基百科数据集上进行预训练，生成了更好的词嵌入。BERT可以通过在任务特定数据集上微调嵌入来进一步提升。
- en: Though, BERT is most suited for language translation tasks. It has been optimized
    for many other applications and domains.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，BERT最适合语言翻译任务。它已经针对许多其他应用和领域进行了优化。
- en: '![](../Images/6a6c52e356c9b9737bb8e3651c4d79d8.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a6c52e356c9b9737bb8e3651c4d79d8.png)'
- en: 'Bidirectional BERT architecture. Image Source: [Google AI Blog](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 双向BERT架构。图片来源：[Google AI Blog](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)。
- en: Concluding Thoughts
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: With advancements in NLP, word embedding techniques are also improving. There
    are many NLP tasks that don’t require advanced embedding techniques. Many can
    perform equally well with simple word embedding techniques. The selection of a
    word embedding technique must be based on careful experimentations and task-specific
    requirements. Fine-tuning the word embedding models can improve the accuracy significantly.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 随着自然语言处理（NLP）的进步，词嵌入技术也在不断改进。有许多NLP任务并不需要高级的嵌入技术。许多任务使用简单的词嵌入技术同样可以表现良好。选择词嵌入技术必须基于细致的实验和任务特定的需求。微调词嵌入模型可以显著提高准确性。
- en: 'In this article, we have given a high-level overview of various word embedding
    algorithms. Let’s summarize them below:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们对各种词嵌入算法进行了高层次的概述。让我们总结如下：
- en: '| **Word Embedding Technique** | **Main Characteristics** | **Use Cases** |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| **词嵌入技术** | **主要特点** | **应用场景** |'
- en: '| TF-IDF | Statistical method to capture the relevance of words w.r.t the corpus
    of text. It does not capture semantic word associations. | Better for information
    retrieval and keyword extraction in documents. |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| TF-IDF | 统计方法用于捕捉词语相对于文本语料库的相关性。它不捕捉语义词语关联。 | 更适合信息检索和文档中的关键词提取。 |'
- en: '| Word2Vec | Neural network-based CBOW and Skip-gram architectures, better
    at capturing semantic information. | Useful in semantic analysis task. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Word2Vec | 基于神经网络的CBOW和Skip-gram架构，更擅长捕捉语义信息。 | 对语义分析任务有用。 |'
- en: '| GloVe | Matrix factorization based on global word-word co-occurrence. It
    solves the local context limitations of Word2Vec. | Better at word analogy and
    named-entity recognition tasks. Comparable results with Word2Vec in some semantic
    analysis tasks while better in others. |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| GloVe | 基于全局词对共现的矩阵分解方法。它解决了Word2Vec的局部上下文限制。 | 在词语类比和命名实体识别任务中表现更好。在一些语义分析任务中与Word2Vec的结果相当，而在其他任务中则更优。
    |'
- en: '| BERT | Transformer-based attention mechanism to capture high-quality contextual
    information. | Language translation, question-answering system. Deployed in Google
    Search engine to understand search queries. |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| BERT | 基于Transformer的注意力机制，用于捕捉高质量的上下文信息。 | 语言翻译、问答系统。部署在Google搜索引擎中以理解搜索查询。
    |'
- en: References
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[https://www.ibm.com/cloud/learn/natural-language-processing](https://www.ibm.com/cloud/learn/natural-language-processing)'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.ibm.com/cloud/learn/natural-language-processing](https://www.ibm.com/cloud/learn/natural-language-processing)'
- en: '[https://www.techopedia.com/definition/33012/natural-language-generation-nlg](https://www.techopedia.com/definition/33012/natural-language-generation-nlg)'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.techopedia.com/definition/33012/natural-language-generation-nlg](https://www.techopedia.com/definition/33012/natural-language-generation-nlg)'
- en: '[https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)'
- en: '[https://www.machinelearningplus.com/nlp/cosine-similarity/](https://www.machinelearningplus.com/nlp/cosine-similarity/)'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.machinelearningplus.com/nlp/cosine-similarity/](https://www.machinelearningplus.com/nlp/cosine-similarity/)'
- en: '[https://www.ibm.com/cloud/learn/neural-networks](https://www.ibm.com/cloud/learn/neural-networks)'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.ibm.com/cloud/learn/neural-networks](https://www.ibm.com/cloud/learn/neural-networks)'
- en: '[https://www.expert.ai/blog/natural-language-process-semantic-analysis-definition/](https://www.expert.ai/blog/natural-language-process-semantic-analysis-definition/)'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.expert.ai/blog/natural-language-process-semantic-analysis-definition/](https://www.expert.ai/blog/natural-language-process-semantic-analysis-definition/)'
- en: '[https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)'
- en: '[https://medium.com/swlh/co-occurrence-matrix-9cacc5dd396e](https://medium.com/swlh/co-occurrence-matrix-9cacc5dd396e)'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://medium.com/swlh/co-occurrence-matrix-9cacc5dd396e](https://medium.com/swlh/co-occurrence-matrix-9cacc5dd396e)'
- en: '[https://blog.marketmuse.com/glossary/latent-semantic-analysis-definition/](https://blog.marketmuse.com/glossary/latent-semantic-analysis-definition/)'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://blog.marketmuse.com/glossary/latent-semantic-analysis-definition/](https://blog.marketmuse.com/glossary/latent-semantic-analysis-definition/)'
- en: '[https://www.investopedia.com/terms/l/least-squares-method.asp](https://www.investopedia.com/terms/l/least-squares-method.asp)'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.investopedia.com/terms/l/least-squares-method.asp](https://www.investopedia.com/terms/l/least-squares-method.asp)'
- en: '[https://www.expert.ai/blog/entity-extraction-work/](https://www.expert.ai/blog/entity-extraction-work/)'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.expert.ai/blog/entity-extraction-work/](https://www.expert.ai/blog/entity-extraction-work/)'
- en: '[https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)'
- en: '[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)'
- en: '[https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)'
- en: '[https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/](https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/)'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/](https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/)'
- en: '**[Neeraj Agarwal](https://www.linkedin.com/in/neeagl/)** is a founder of Algoscale,
    a data consulting company covering data engineering, applied AI, data science,
    and product engineering. He has over 9 years of experience in the field and has
    helped a wide range of organizations from start-ups to Fortune 100 companies ingest
    and store enormous amounts of raw data in order to translate it into actionable
    insights for better decision-making and faster business value.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Neeraj Agarwal](https://www.linkedin.com/in/neeagl/)** 是 Algoscale 的创始人，Algoscale
    是一家数据咨询公司，涵盖数据工程、应用 AI、数据科学和产品工程。他在该领域拥有超过 9 年的经验，并帮助了从初创企业到财富 100 强公司等各种组织，处理和存储大量原始数据，以将其转化为可操作的见解，帮助做出更好的决策并加快业务价值的实现。'
- en: More On This Topic
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题更多信息
- en: '[The Range of NLP Applications in the Real World: A Different…](https://www.kdnuggets.com/2022/03/different-solution-problem-range-nlp-applications-real-world.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[现实世界中 NLP 应用的范围：一种不同的…](https://www.kdnuggets.com/2022/03/different-solution-problem-range-nlp-applications-real-world.html)'
- en: '[5 Different Ways to Load Data in Python](https://www.kdnuggets.com/2020/08/5-different-ways-load-data-python.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5 种不同的 Python 数据加载方式](https://www.kdnuggets.com/2020/08/5-different-ways-load-data-python.html)'
- en: '[AI-Generated Sports Highlights: Different Approaches](https://www.kdnuggets.com/2022/03/aigenerated-sports-highlights-different-approaches.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AI 生成的体育精彩片段：不同的方法](https://www.kdnuggets.com/2022/03/aigenerated-sports-highlights-different-approaches.html)'
- en: '[How is Data Mining Different from Machine Learning?](https://www.kdnuggets.com/2022/06/data-mining-different-machine-learning.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据挖掘与机器学习有何不同？](https://www.kdnuggets.com/2022/06/data-mining-different-machine-learning.html)'
- en: '[Learn How Different Data Visualizations Work](https://www.kdnuggets.com/2022/09/datacamp-learn-different-data-visualizations-work.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[了解不同数据可视化的工作原理](https://www.kdnuggets.com/2022/09/datacamp-learn-different-data-visualizations-work.html)'
- en: '[Baize: An Open-Source Chat Model (But Different?)](https://www.kdnuggets.com/2023/04/baize-opensource-chat-model-different.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Baize: 一个开源聊天模型（但与众不同？）](https://www.kdnuggets.com/2023/04/baize-opensource-chat-model-different.html)'
