- en: 'Gradient Descent: The Mountain Trekker’s Guide to Optimization with Mathematics'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降：山地行者的数学优化指南
- en: 原文：[https://www.kdnuggets.com/gradient-descent-the-mountain-trekker-guide-to-optimization-with-mathematics](https://www.kdnuggets.com/gradient-descent-the-mountain-trekker-guide-to-optimization-with-mathematics)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/gradient-descent-the-mountain-trekker-guide-to-optimization-with-mathematics](https://www.kdnuggets.com/gradient-descent-the-mountain-trekker-guide-to-optimization-with-mathematics)
- en: 'The Mountain Trekker Analogy:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 山地行者类比：
- en: 'Imagine you''re a mountain trekker, standing somewhere on the slopes of a vast
    mountain range. Your goal is to reach the lowest point in the valley, but there''s
    a catch: you''re blindfolded. Without the ability to see the entire landscape,
    how would you find your way to the bottom?'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你是一个山地行者，站在广阔山脉的某个坡道上。你的目标是到达山谷的最低点，但有一个难题：你被蒙上了眼睛。在无法看到整个地形的情况下，你会如何找到通往山谷底部的道路？
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的IT需求'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Instinctively, you might feel the ground around you with your feet, sensing
    which way is downhill. You'd then take a step in that direction, the steepest
    descent. Repeating this process, you'd gradually move closer to the valley's lowest
    point.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本能地，你可能会用脚感觉周围的地面，判断哪边是下坡。然后你会朝着那个方向迈出一步，即最陡的下降。重复这个过程，你会逐渐接近山谷的最低点。
- en: Translating the Analogy to Gradient Descent
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 翻译类比到梯度下降
- en: 'In the realm of machine learning, this trekker''s journey mirrors the gradient
    descent algorithm. Here''s how:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的领域中，这位行者的旅程类似于梯度下降算法。以下是具体方法：
- en: '**1) The Landscape:** The mountainous terrain represents our cost (or loss)
    function,'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**1) 地形：** 山区地形代表了我们的成本（或损失）函数，'
- en: 'J(θ). This function measures the error or discrepancy between our model''s
    predictions and the actual data. Mathematically, it could be represented as:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: J(θ)。这个函数测量了我们模型的预测与实际数据之间的误差或差异。数学上，它可以表示为：
- en: '![XXXXX](../Images/87b36b4819caf1d17a5cdce74b98dd31.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/87b36b4819caf1d17a5cdce74b98dd31.png)'
- en: where
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: m is the number of data points,hθ(x) is our model's prediction, and
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: m 是数据点的数量，hθ(x) 是我们模型的预测，以及
- en: y is the actual value.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: y 是实际值。
- en: '**2) The Trekker''s Position:** Your current position on the mountain corresponds
    to the current values of the model''s parameters,θ. As you move, these values
    change, altering the model''s predictions.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**2) 行者的位置：** 你在山上的当前位置对应于模型参数θ的当前值。随着你移动，这些值会改变，从而改变模型的预测。'
- en: '**3) Feeling the Ground:** Just as you sense the steepest descent with your
    feet, in gradient descent, we compute the gradient,'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**3) 感知地面：** 就像你用脚感知最陡的下降一样，在梯度下降中，我们计算梯度，'
- en: '∇J(θ). This gradient tells us the direction of the steepest increase in our
    cost function. To minimise the cost, we move in the opposite direction. The gradient
    is given by:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ∇J(θ)。这个梯度告诉我们成本函数最陡的上升方向。为了最小化成本，我们沿着相反方向移动。梯度由以下公式给出：
- en: '![XXXXX](../Images/6b8736e98e39f3e0413ebbc51ffbd1ad.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/6b8736e98e39f3e0413ebbc51ffbd1ad.png)'
- en: 'Where:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: m is the number of training examples.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: m 是训练示例的数量。
- en: '![XXXXX](../Images/4b0f4a119372345146cb1d808255c80e.png)is the prediction for
    the ith'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![XXXXX](../Images/4b0f4a119372345146cb1d808255c80e.png)是第i个训练示例的预测值。'
- en: training example.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 训练示例。
- en: '![XXXXX](../Images/5f09549cae3c9f28907642b109fb2b76.png)is the jth feature
    value for the ith training example.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '![XXXXX](../Images/5f09549cae3c9f28907642b109fb2b76.png)是第i个训练示例的第j个特征值。'
- en: '![XXXXX](../Images/d3fbf4747f530f0a39da2a64c432f9e5.png)is the actual output
    for the ith'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![XXXXX](../Images/d3fbf4747f530f0a39da2a64c432f9e5.png)是第i个训练示例的实际输出。'
- en: training example.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 训练示例。
- en: '**4) Steps**: The size of the steps you take is analogous to the learning rate
    in gradient descent, denoted by ?. A large step might help you descend faster
    but risks overshooting the valley''s bottom. A smaller step is more cautious but
    might take longer to reach the minimum. The update rule is:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**4) 步骤**：你采取的步长类似于梯度下降中的学习率，用?表示。大的步长可能帮助你更快地下降，但风险是可能超越谷底。较小的步长则更为谨慎，但可能需要更长时间才能达到最小值。更新规则是：'
- en: '![XXXXX](../Images/fe31d944307e27583ff81cbb44feca6f.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/fe31d944307e27583ff81cbb44feca6f.png)'
- en: '**5) Reaching the Bottom:** The iterative process continues until you reach
    a point where you feel no significant descent in any direction. In gradient descent,
    this is when the change in the cost function becomes negligible, indicating that
    the algorithm has (hopefully) found the minimum.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**5) 达到底部：** 迭代过程将持续到你到达一个点，在任何方向上都感受不到显著的下降。在梯度下降中，这意味着成本函数的变化变得微不足道，表明算法已经（希望）找到了最小值。'
- en: In Conclusion
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Gradient descent is a methodical and iterative process, much like our blindfolded
    trekker trying to find the valley's lowest point. By combining intuition with
    mathematical rigor, we can better understand how machine learning models learn,
    adjust their parameters, and improve their predictions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一个有条理的迭代过程，就像我们带着眼罩的旅行者试图找到谷底的最低点一样。通过将直觉与数学严格性结合，我们可以更好地理解机器学习模型如何学习、调整其参数并改进预测。
- en: Batch Gradient Descent
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量梯度下降
- en: Batch Gradient Descent computes the gradient using the entire dataset. This
    method provides a stable convergence and consistent error gradient but can be
    computationally expensive and slow for large datasets.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 批量梯度下降使用整个数据集来计算梯度。该方法提供了稳定的收敛性和一致的误差梯度，但对于大数据集而言可能计算开销较大且速度较慢。
- en: Stochastic Gradient Descent (SGD)
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机梯度下降（SGD）
- en: SGD estimates the gradient using a single, randomly chosen data point. While
    it can be faster and is capable of escaping local minima, it has a more erratic
    convergence pattern due to its inherent randomness, potentially leading to oscillations
    in the cost function.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: SGD使用单个随机选择的数据点来估计梯度。尽管它可以更快并且能够逃离局部最小值，但由于其固有的随机性，它的收敛模式更为不稳定，可能导致成本函数的波动。
- en: Mini-Batch Gradient Descent
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小批量梯度下降
- en: Mini-Batch Gradient Descent strikes a balance between the two aforementioned
    methods. It computes the gradient using a subset (or "mini-batch") of the dataset.
    This method accelerates convergence by benefiting from the computational advantages
    of matrix operations and offers a compromise between the stability of Batch Gradient
    Descent and the speed of SGD.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量梯度下降在上述两种方法之间取得平衡。它使用数据集的一个子集（或“迷你批量”）来计算梯度。该方法通过利用矩阵运算的计算优势加快了收敛速度，并在批量梯度下降的稳定性和SGD的速度之间提供了折衷。
- en: Challenges and Solutions
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战与解决方案
- en: Local Minima
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局部最小值
- en: Gradient descent can sometimes converge to a local minimum, which is not the
    optimal solution for the entire function. This is particularly problematic in
    complex landscapes with multiple valleys. To overcome this, incorporating momentum
    helps the algorithm navigate through valleys without getting stuck. Additionally,
    advanced optimization algorithms like Adam combine the benefits of momentum and
    adaptive learning rates to ensure more robust convergence to global minima.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降有时会收敛到局部最小值，而这不是整个函数的最佳解。这在复杂的景观中，尤其是有多个谷底的情况，尤其成问题。为了解决这个问题，加入动量帮助算法穿越谷底而不被困住。此外，像Adam这样的高级优化算法结合了动量和自适应学习率的优点，以确保更稳健地收敛到全局最小值。
- en: Vanishing & Exploding Gradients
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消失梯度与爆炸梯度
- en: In deep neural networks, as gradients are back-propagated, they can diminish
    to near zero (vanish) or grow exponentially (explode). Vanishing gradients slow
    down training, making it hard for the network to learn, while exploding gradients
    can cause the model to diverge. To mitigate these issues, gradient clipping sets
    a threshold value to prevent gradients from becoming too large. On the other hand,
    normalized initialization techniques, like He or Xavier initialization, ensure
    that the weights are set to optimal values at the start, reducing the risk of
    these challenges.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度神经网络中，随着梯度的反向传播，它们可能会逐渐减小到接近零（消失）或指数增长（爆炸）。消失的梯度会减慢训练，使网络难以学习，而爆炸的梯度可能导致模型发散。为了解决这些问题，梯度裁剪设置一个阈值，以防止梯度变得过大。另一方面，像He初始化或Xavier初始化这样的规范化初始化技术，确保权重在开始时设置为最佳值，从而降低这些挑战的风险。
- en: Example Code for Gradient Descent Algorithm
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降算法示例代码
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code provides a basic gradient descent algorithm for linear regression.
    The function gradient_descent takes in the feature matrix X, target vector y,
    a learning rate, and the number of iterations. It returns the optimized parameters
    (theta) and the history of the cost function over the iterations.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码提供了一个用于线性回归的基本梯度下降算法。函数gradient_descent接收特征矩阵X、目标向量y、学习率和迭代次数。它返回优化后的参数（theta）以及迭代过程中成本函数的历史记录。
- en: '![XXXXX](../Images/1faaf30b7df2bf9a5eae00b999587f0a.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/1faaf30b7df2bf9a5eae00b999587f0a.png)'
- en: The left subplot shows the cost function decreasing over iterations.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧子图显示了成本函数在迭代过程中下降的情况。
- en: The right subplot shows the data points and the line of best fit obtained from
    gradient descent.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧子图显示了数据点和通过梯度下降法获得的最佳拟合线。
- en: '![XXXXX](../Images/b837159ff31149508a57141142be7399.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/b837159ff31149508a57141142be7399.png)'
- en: a 3D plot of the function ![XXXXX](../Images/67f53670604e23d20236abde223fb91e.png)
    and overlay the path taken by gradient descent in red. The gradient descent starts
    from a random point and moves towards the minimum of the function.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一个函数的三维图 ![XXXXX](../Images/67f53670604e23d20236abde223fb91e.png) 并用红色标记了梯度下降路径。梯度下降从一个随机点开始，移动到函数的最小值处。
- en: Applications
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用
- en: Stock Price Prediction
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 股票价格预测
- en: Financial analysts use gradient descent in conjunction with algorithms like
    linear regression to predict future stock prices based on historical data. By
    minimising the error between the predicted and actual stock prices, they can refine
    their models to make more accurate predictions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 财务分析师使用梯度下降法结合线性回归等算法，根据历史数据预测未来的股票价格。通过最小化预测值和实际股票价格之间的误差，他们可以改进模型，以做出更准确的预测。
- en: Image Recognition
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像识别
- en: Deep learning models, especially Convolutional Neural Networks (CNNs), employ
    gradient descent to optimise weights while training on vast datasets of images.
    For instance, platforms like Facebook use such models to automatically tag individuals
    in photos by recognizing facial features. The optimization of these models ensures
    accurate and efficient facial recognition.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型，尤其是卷积神经网络（CNN），利用梯度下降法在大规模图像数据集上优化权重。例如，像Facebook这样的平台使用这些模型自动标记照片中的个人，通过识别面部特征来实现。对这些模型的优化确保了准确和高效的面部识别。
- en: Sentiment Analysis
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 情感分析
- en: Companies use gradient descent to train models that analyse customer feedback,
    reviews, or social media mentions to determine public sentiment about their products
    or services. By minimising the difference between predicted and actual sentiments,
    these models can accurately classify feedback as positive, negative, or neutral,
    helping businesses gauge customer satisfaction and tailor their strategies accordingly.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 公司使用梯度下降法训练模型，分析客户反馈、评论或社交媒体提及，以确定公众对其产品或服务的情感。通过最小化预测情感和实际情感之间的差异，这些模型可以准确地将反馈分类为积极、消极或中立，从而帮助企业评估客户满意度，并相应地调整其策略。
- en: '**[Arun](https://www.linkedin.com/in/arunchandramouli/)** is a seasoned Senior
    Data Scientist with over 8 years of experience in harnessing the power of data
    to drive impactful business solutions. He excels in leveraging advanced analytics,
    predictive modelling, and machine learning to transform complex data into actionable
    insights and strategic narratives. Holding a PGP in Machine Learning and Artificial
    Intelligence from a renowned institution, Arun''s expertise spans a broad spectrum
    of technical and strategic domains, making him a valuable asset in any data-driven
    endeavour.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**[阿伦](https://www.linkedin.com/in/arunchandramouli/)** 是一位经验丰富的高级数据科学家，拥有超过8年的经验，擅长利用数据的力量推动具有影响力的商业解决方案。他精通运用高级分析、预测建模和机器学习，将复杂数据转化为可操作的见解和战略叙事。阿伦持有著名机构颁发的机器学习和人工智能的PGP学位，其专业知识涵盖了广泛的技术和战略领域，使他在任何数据驱动的项目中都成为宝贵的资产。'
- en: More On This Topic
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[5 Concepts You Should Know About Gradient Descent and Cost Function](https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你应该了解的5个梯度下降和成本函数的概念](https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html)'
- en: '[Back To Basics, Part Dos: Gradient Descent](https://www.kdnuggets.com/2023/03/back-basics-part-dos-gradient-descent.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[回到基础知识，第二部分：梯度下降](https://www.kdnuggets.com/2023/03/back-basics-part-dos-gradient-descent.html)'
- en: '[September 26-30: SIAM Conference on Mathematics of Data Science (Hybrid)](https://www.kdnuggets.com/2022/08/siam-conference-mathematics-data-science-hybrid.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9月26-30日：SIAM数据科学数学会议（混合形式）](https://www.kdnuggets.com/2022/08/siam-conference-mathematics-data-science-hybrid.html)'
- en: '[Inside DeepMind’s New Efforts to Use Deep Learning to Advance Mathematics](https://www.kdnuggets.com/2021/12/inside-deepmind-new-efforts-deep-learning-advance-mathematics.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深入了解DeepMind在利用深度学习推动数学发展的新努力](https://www.kdnuggets.com/2021/12/inside-deepmind-new-efforts-deep-learning-advance-mathematics.html)'
- en: '[Mathematics for Machine Learning: The Free eBook](https://www.kdnuggets.com/2020/04/mathematics-machine-learning-book.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中的数学：免费电子书](https://www.kdnuggets.com/2020/04/mathematics-machine-learning-book.html)'
- en: '[Vanishing Gradient Problem: Causes, Consequences, and Solutions](https://www.kdnuggets.com/2022/02/vanishing-gradient-problem.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[梯度消失问题：原因、后果及解决方案](https://www.kdnuggets.com/2022/02/vanishing-gradient-problem.html)'
