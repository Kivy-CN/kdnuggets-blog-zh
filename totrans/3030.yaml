- en: Four Approaches to Explaining AI and Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释AI和机器学习的四种方法
- en: 原文：[https://www.kdnuggets.com/2018/12/four-approaches-ai-machine-learning.html](https://www.kdnuggets.com/2018/12/four-approaches-ai-machine-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/12/four-approaches-ai-machine-learning.html](https://www.kdnuggets.com/2018/12/four-approaches-ai-machine-learning.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Jay Budzik](https://www.linkedin.com/in/jaybudzik), Chief Technology
    Officer, ZestFinance**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：[Jay Budzik](https://www.linkedin.com/in/jaybudzik)，首席技术官，ZestFinance**'
- en: Advanced machine learning (ML) is a subset of AI that uses more data and sophisticated math
    to make better predictions and decisions. It’s what powers self-driving cars,
    Netflix recommendations, and a lot of bank fraud detection. Banks and lenders
    could make a lot more money using ML on top of legacy credit scoring techniques
    to find better borrowers and reject more bad ones. But adoption of ML has been
    held back by the technology’s “black-box” nature. ML models are exceedingly complex.
    You can’t run a credit model safely or accurately if you can’t explain its decisions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 高级机器学习（ML）是人工智能（AI）的一个子集，使用更多的数据和复杂的数学方法来做出更好的预测和决策。它驱动了自动驾驶汽车、Netflix推荐系统以及大量的银行欺诈检测。银行和贷款机构可以利用ML与传统的信用评分技术结合，找到更好的借款人，并拒绝更多不良借款人，从而赚取更多的钱。但由于技术的“黑箱”特性，ML的采用受到了阻碍。ML模型极其复杂。如果无法解释其决策，你无法安全或准确地运行信用模型。
- en: 'What we really want from AI explainability are three things: consistency, accuracy,
    and performance. A handful of techniques are being promoted in the market claiming
    to solve this “black-box” problem. Caveat emptor. The techniques are often inconsistent,
    inaccurate, computationally expensive, and/or fail to spot unacceptable outcomes
    such as race- and gender-based discrimination. Some of these techniques have been
    in use for a long time, and are quite effective on old modeling approaches such
    as logistic regression. But they don’t work well with ML.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真正希望从AI可解释性中得到三样东西：一致性、准确性和性能。市场上有一些技术被宣传为解决这种“黑箱”问题。谨防上当。这些技术往往不一致、不准确、计算成本高，和/或未能发现不可接受的结果，例如基于种族和性别的歧视。其中一些技术已经使用了很长时间，对于旧的建模方法，如逻辑回归，非常有效。但它们在ML上效果不佳。
- en: At ZestFinance, we’ve built new kinds of explainability math into our ZAML software
    tools that quickly render the inner workings of ML models transparent from creation
    through deployment. You can use these tools to monitor model health in run-time.
    You can trust the results are fair and accurate. And we’ve automated the reporting
    so all you have to do is push a button and produce all the documentation required
    to comply with regulations.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在ZestFinance，我们在ZAML软件工具中构建了新的可解释性数学方法，这些方法可以快速地将ML模型的内部工作从创建到部署变得透明。你可以使用这些工具实时监控模型健康状况。你可以相信结果是公平和准确的。我们还自动化了报告生成，所以你只需按一下按钮即可生成所有符合规定所需的文档。
- en: In this paper, we compare a few popular explainability techniques (including
    our ZAML technology), describe their relative strengths and weaknesses, and show
    how each technique stacks up on a simple modeling problem. The test we ran showed
    that ZAML is more accurate, consistent, and faster than any of these other approaches.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，我们比较了几种流行的可解释性技术（包括我们的ZAML技术），描述了它们的相对优缺点，并展示了每种技术在一个简单建模问题上的表现。我们进行的测试显示，ZAML比这些其他方法更准确、一致和快速。
- en: Four Kinds of Explainability Techniques, Explained
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 四种可解释性技术的解释
- en: Three common ML explainability techniques being championed today are known as
    LOCO, permutation impact, and LIME. LOCO, which stands for “leave one column out,”
    substitutes “missing” for a variable and recomputes the model’s prediction. The
    idea is that if the score changes a lot, the variable that was left out must be
    really important. Permutation impact (PI), also called permutation importance,
    substitutes a variable with a randomly selected value and recomputes the model’s
    prediction. As with LOCO, the idea is that if the score changes a lot, the variable
    that was scrambled must be really important. LIME, which stands for local interpretable
    model-agnostic explanations, fits a new linear model in a local neighborhood around
    a given applicant’s real data values and the real model’s score for those synthetic
    values. It then uses this new, linear approximation of the actual model to explain
    how the more complex model behaves. Essentially, you’re taking a very complex
    model and pretending it’s simple so you can explain it.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 目前推广的三种常见机器学习可解释性技术分别是LOCO、置换影响和LIME。LOCO，代表“留一列”的意思，通过将一个变量“遗漏”并重新计算模型的预测值。其思路是，如果分数变化很大，那么被遗漏的变量一定非常重要。置换影响（PI），也称为置换重要性，用一个随机选择的值替代一个变量，并重新计算模型的预测值。与LOCO类似，其思路是，如果分数变化很大，那么被扰乱的变量一定非常重要。LIME，代表局部可解释的模型无关解释，拟合一个新的线性模型在给定申请人的真实数据值和真实模型对这些合成值的得分周围的局部邻域内。然后使用这个新的线性近似模型来解释更复杂的模型的行为。本质上，你是将一个非常复杂的模型假装成简单的，以便进行解释。
- en: ZestFinance’s ZAML software uses a proprietary explanation method derived from
    game theory and multivariate calculus that works on the actual underlying model.
    ZAML explainability determines the relative importance of each variable to the
    final score by looking at how it interacts with other variables. The checklist
    below compares these four explainability techniques on seven important capabilities.
    We also added scores for each of the techniques from a test we ran on their accuracy,
    consistency and speed at explaining two common ML models. Accuracy is measured
    by average rank correlation, or how far off the explanations are from the ground
    truth baselines. Consistency is measured by variance, or how much the difference
    in explanations varies across all applicants. Speed was the time it took to generate
    explanations for all the rows in the dataset.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ZestFinance的ZAML软件使用了一种来自博弈论和多元微积分的专有解释方法，适用于实际的基础模型。ZAML可解释性通过查看每个变量与其他变量的交互，来确定每个变量对最终得分的相对重要性。下方的清单比较了这四种可解释性技术在七个重要能力上的表现。我们还为每种技术添加了在解释两个常见机器学习模型时的准确性、一致性和速度的测试评分。准确性通过平均排名相关性来衡量，或者说解释与实际基准的偏差有多大。一致性通过方差来衡量，即解释的差异在所有申请人中的变化程度。速度是生成数据集中所有行解释所需的时间。
- en: '![Four approaches to AI and Machine Learning Figure 1](../Images/d842b8f7e82e89788c1821c3365c5329.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![人工智能和机器学习的四种方法 图1](../Images/d842b8f7e82e89788c1821c3365c5329.png)'
- en: Let’s Unpack This Checklist
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 让我们解读一下这个清单
- en: Explanations of ML models have to meet a variety of requirements to be trustworthy
    for high stakes business applications such as credit risk modeling, chief among
    them accuracy, consistency, and performance, i.e., prediction speed. The chart
    above summarizes how the four techniques you may be hearing about stack up on
    an extremely simple model. For real-world explainability problems, such as those
    endemic to credit risk modeling, the performance differences can be even more
    pronounced. We used a simple model to demonstrate these differences to make a
    point that even a model with just two variables can’t be explained by LOCO, PI,
    or LIME as accurately as ZAML explainability can. If an explainability technique
    has a hard time explaining the simplest of “toy” models, it may be unwise to trust
    it on a real-world application where large dollar amounts are at stake.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于高风险业务应用（如信用风险建模），机器学习模型的解释必须满足多种要求，以便其具有可信性，其中最重要的是准确性、一致性和性能，即预测速度。上面的图表总结了你可能听说的四种技术在一个极其简单的模型上的表现。对于现实世界中的可解释性问题，例如信用风险建模中普遍存在的问题，性能差异可能更为明显。我们使用了一个简单的模型来展示这些差异，旨在说明即使是只有两个变量的模型，也无法像ZAML可解释性那样准确地用LOCO、PI或LIME解释。如果一种可解释性技术在解释最简单的“玩具”模型时都很困难，那么在涉及大量资金的现实世界应用中信任它可能是不明智的。
- en: LOCO, LIME, and PI all have shortcomings with respect to accuracy, consistency,
    or speed. LOCO and PI work by analyzing one feature one at a time. This means
    as the number of model variables goes up, the algorithms become slower and more
    expensive to run. LOCO, LIME, and PI look only at the inputs and outputs of the
    model, which means they have access to much less information than explainers such
    as ZAML’s that look at the internal structure of the model. Probing the model
    externally (i.e. the inputs and outputs) is an imperfect process leading to potential
    mistakes and inaccuracies. So is analyzing refitted and/or proxy models, as LOCO
    and LIME require, instead of analyzing your final model. We believe that analyzing
    the real model is really important - if you don’t you’re opening yourself up to
    lots of risk.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LOCO、LIME 和 PI 在准确性、一致性或速度方面都有缺陷。LOCO 和 PI 通过一次分析一个特征来工作。这意味着随着模型变量数量的增加，算法运行变得更慢、更昂贵。LOCO、LIME
    和 PI 只关注模型的输入和输出，这意味着它们获取的信息比像 ZAML 这样查看模型内部结构的解释器少得多。外部探测模型（即输入和输出）是一个不完美的过程，容易导致潜在的错误和不准确性。分析重新拟合和/或代理模型，如
    LOCO 和 LIME 所要求的，而不是分析你的最终模型，也存在问题。我们认为分析真实模型非常重要——如果不这样做，你将面临很多风险。
- en: When attempting to explain advanced ML models, it’s important that your explanations
    capture the effect of a feature holistically, or in relation to other features.
    Univariate analysis, as is typically employed with LOCO and PI, will not properly
    capture these feature interactions and correlation effects. Again, accuracy suffers.
    Explainers should also calculate feature explanations from a global, and not just
    local perspective. Let’s say you built a model to understand why Kobe Bryant was
    a great basketball player. Explaining it using only local feature importance will
    suggest that his height was not that big of a deal because, from the model’s perspective,
    all the NBA players clustered around Kobe are also very tall. But you can’t deny
    that Kobe’s 6’6” height has nothing to do with why he’s so good at basketball.
    Again, inaccuracies crop up.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试解释高级机器学习模型时，重要的是你的解释能全面捕捉特征的影响，或与其他特征的关系。单变量分析，如 LOCO 和 PI 通常采用的，将无法正确捕捉这些特征交互和相关效应。准确性再次受到影响。解释器还应从全局而不仅仅是局部角度计算特征解释。假设你建立了一个模型来理解为什么科比·布莱恩特是一位伟大的篮球运动员。仅仅使用局部特征重要性进行解释将表明他的身高并不是问题，因为从模型的角度看，所有与科比接近的
    NBA 球员也都很高。但你不能否认科比的 6 英尺 6 英寸的身高与他为何打得这么好有关系。再一次，不准确性出现了。
- en: Most modeling techniques also have tuning knobs known as hyperparameters that
    can be automatically dialed in to make a model more accurate. There is no practical
    equivalent to this “automated tuning” operation for explainers. This makes it
    really difficult to be confident in the results you get from an explainer like
    LIME, because there’s no way to tell whether you correctly set a hyperparameter
    such as the data-area size. Set it too small and the data will be super sparse
    in the high-dimensional space of all the features in the model. Set it too wide
    and you will violate the local linear assumption of LIME.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数建模技术也有称为超参数的调节旋钮，可以自动调整以使模型更准确。对于解释器来说，没有实际等同于这种“自动调优”操作的东西。这使得对从解释器如 LIME
    获得的结果充满信心变得非常困难，因为无法确定你是否正确设置了像数据区域大小这样的超参数。设置得过小，数据会在模型的高维空间中变得非常稀疏。设置得过宽，你将违反
    LIME 的局部线性假设。
- en: A consistent explainer uses real data. LIME and PI probe the model with non-real
    data. Let’s say you built a model with income, debt, and debt to income ratio.
    PI will shuffle each column one at a time independently. This means when PI analyzes
    debt to income ratio by shuffling, it will no longer be the debt feature divided
    by the income feature. Since all of the data in the training data maintained this
    relationship, it should be considered undefined behavior to query a model this
    way.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一致的解释器使用真实数据。LIME 和 PI 使用非真实数据来探测模型。假设你建立了一个包含收入、债务和债务收入比率的模型。PI 会独立地逐列打乱。这意味着当
    PI 通过打乱分析债务收入比率时，它将不再是债务特征除以收入特征。由于训练数据中的所有数据保持了这种关系，因此以这种方式查询模型应该被视为未定义的行为。
- en: Even slight inaccuracies in explanations can lead to devastating outcomes, like
    releasing models that discriminate based on age, gender, race, or ethnicity, or
    releasing models that are unstable and produce high default rates that can cause
    lending businesses to hemorrhage money. A model explainability technique should
    provide the same answer given the same inputs and model. Inaccurate or inconsistent
    model explainability techniques might appear to provide some assurance of risk
    management, but if the results aren’t accurate, they provide little value or assurance
    to business stakeholders and regulators.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 即使解释中的轻微不准确也可能导致灾难性的结果，例如发布基于年龄、性别、种族或民族的歧视模型，或发布不稳定且产生高违约率的模型，这可能导致借贷业务的资金流失。一个模型可解释性技术应该在给定相同的输入和模型时提供相同的答案。不准确或不一致的模型可解释性技术可能看似提供了风险管理的一些保证，但如果结果不准确，它们对业务利益相关者和监管机构几乎没有价值或保证。
- en: 'Speed and efficiency are also important. Model explainability should be used
    often during the model build process and as the model is applied in order to generate
    explanations for each model-based decision. Slow and inefficient techniques limit
    the usefulness of explainability: If you have to wait overnight for results, you’re
    going to run the analysis less often, and get less frequent insights about how
    your model is working. That means less time working on better models.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 速度和效率也很重要。在模型构建过程中以及模型应用时，模型可解释性应被频繁使用，以便为每个基于模型的决策生成解释。缓慢和低效的技术限制了可解释性的实用性：如果你需要等待一整夜才能获得结果，你会减少分析的频率，从而获得关于模型运行的见解也会减少。这意味着你在改进模型上投入的时间会减少。
- en: Putting Explainability Techniques To The Test
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对可解释性技术进行测试
- en: We took our comparison of these four explainability techniques one step further
    and ran an experiment evaluating how these approaches performed in relation to
    ground truth on a simple modeling problem. For this test, we used as ground truth
    a reference value computed using math derived from competitive game theory. The
    reference value measures the average marginal benefit introduced by a given model’s
    input variables. It does this by exhaustively enumerating all possible combinations
    of variables. Computing the reference value requires creating an exponential number
    of model variations, each with a different combination of variables included.
    By understanding the comparative performance of the models, we can quantify the
    value of each variable used in the model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对这四种可解释性技术的比较更进一步，并进行了一个实验，评估这些方法在简单建模问题上相对于真实值的表现。在这个测试中，我们使用了一个基于竞争博弈理论计算出的参考值作为真实值。参考值衡量了给定模型输入变量引入的平均边际效益。它通过穷举所有可能的变量组合来实现这一点。计算参考值需要创建指数数量的模型变体，每个变体包含不同的变量组合。通过了解模型的比较表现，我们可以量化模型中每个变量的价值。
- en: For our experiment, we evaluated the performance of the four techniques at explaining
    two ML models and the “moons” dataset. This is a simple, two variable data set
    with 100,000 rows, and is often used in undergraduate machine learning courses
    to demonstrate the limitations of linear models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们评估了四种技术在解释两个机器学习模型和“月亮”数据集方面的表现。这是一个简单的、包含 100,000 行的双变量数据集，通常用于本科机器学习课程中，以演示线性模型的局限性。
- en: '![Four approaches to AI and Machine Learning Figure 2](../Images/bb69e27a3ee22fae7b322c65e5588589.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![人工智能和机器学习的四种方法 图2](../Images/bb69e27a3ee22fae7b322c65e5588589.png)'
- en: The moons classification task is to build a model that separates the light orange
    data points from the dark orange data points using only the X and Y coordinates.
    A linear model simply can’t do this very well.  To see this, just draw any straight
    line through the above diagram and notice that on either side of the line you
    have both yellow and green data points. There is no straight line that can separate
    the orange sections. The moons dataset provides a simple example showing that
    machine learning methods are sensitive to variable interactions and capture the
    real shape of the data better than old methods like logistic regression that can
    only create linear models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: “月亮”分类任务是建立一个模型，使用仅 X 和 Y 坐标将浅橙色数据点与深橙色数据点分开。线性模型无法很好地完成这一任务。要查看这一点，只需在上图中画一条直线，注意到直线两侧都有黄色和绿色数据点。没有一条直线可以分开橙色区域。月亮数据集提供了一个简单的例子，展示了机器学习方法对变量交互的敏感性，并且比只能创建线性模型的旧方法如逻辑回归更好地捕捉数据的真实形状。
- en: We built two machine learning classification models trained on this data set.
    One using XGBoost, a popular tree ensemble induction algorithm, and Tensorflow,
    a popular neural network package. At the time of writing, both were considered
    best in class machine learning modeling techniques. We then applied the various
    explainability approaches to the resulting tree and neural network models. The
    results are provided below.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了两个机器学习分类模型，分别使用了XGBoost，一个流行的树集合诱导算法，以及Tensorflow，一个流行的神经网络包。在撰写本文时，这两种方法都被认为是最顶尖的机器学习建模技术。随后，我们将各种可解释性方法应用于生成的树和神经网络模型。结果如下所示。
- en: '![Four approaches to AI and Machine Learning Figure 3](../Images/9119c34b660588dd669548b8aad462fe.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![AI和机器学习的四种方法 图3](../Images/9119c34b660588dd669548b8aad462fe.png)'
- en: For gradient boosted trees, LIME, PI, and LOCO all produce high error rates
    and much higher variance even on a very simple modeling problem, when compared
    with ZAML. Also important to note is that LIME is much slower to compute. While
    PI and LOCO appear to be more reasonable in this test on a two-variable model,
    these methods get much slower very quickly as the number of rows and columns increase.
    For larger datasets such as those we see every day in our credit risk modeling
    work, they become impractical. We’ve waited hours, even days, for explanations
    from LOCO, PI, and LIME. In our experience, the ZAML approach doesn’t take more
    than a few seconds to compute on a single laptop or modest computer, even for
    models with thousands of variables.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于梯度提升树，LIME、PI和LOCO在面对非常简单的建模问题时产生的错误率较高且方差更大，甚至与ZAML相比。还要注意的是，LIME的计算速度要慢得多。虽然PI和LOCO在这种两变量模型测试中看起来更合理，但随着行列数的增加，这些方法的速度很快变得非常慢。对于像我们在信用风险建模工作中每天遇到的大型数据集，它们变得不切实际。我们曾经等待了几个小时甚至几天才获得LOCO、PI和LIME的解释。根据我们的经验，ZAML方法在单台笔记本电脑或普通计算机上计算时间不超过几秒，即使是对具有数千个变量的模型也是如此。
- en: '![Four approaches to AI and Machine Learning Figure 4](../Images/5d8f242ef7a504c3ae61e94ee0c93435.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![AI和机器学习的四种方法 图4](../Images/5d8f242ef7a504c3ae61e94ee0c93435.png)'
- en: For neural networks, LIME, PI and LOCO also generate much higher error rates
    and higher variance. Consistent with its performance on gradient boosted trees,
    LIME is very slow. For those interested, the scatter plots showing the comparison
    of each method with the reference values are included in the appendix below.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于神经网络，LIME、PI和LOCO也产生了更高的错误率和方差。与在梯度提升树上的表现一致，LIME的速度非常慢。对感兴趣的人来说，比较每种方法与参考值的散点图已包含在下面的附录中。
- en: Conclusion
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: Not all explainability techniques are created equal. Even on a simple modeling
    problem like the two-dimensional “moons” dataset, with only 100,000 rows, we have
    shown that ZAML explainability is more accurate, consistent, and often faster
    than the other approaches. When applying machine learning on high-stakes business
    problems like credit risk modeling, it’s critical to get the core explainability
    method right so that you know what your model is doing and can manage the risk
    associated with its application. If the core explainability math you use generates
    the wrong answer, you might get your business into serious trouble.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有可解释性技术都是一样的。即使在像二维“月亮”数据集这样简单的建模问题上，只有100,000行数据，我们也展示了ZAML的可解释性比其他方法更准确、更一致，且通常更快。在处理如信用风险建模等高风险业务问题时，确保核心可解释性方法正确至关重要，以便你了解模型的工作情况并能够管理其应用所带来的风险。如果你使用的核心可解释性数学产生了错误的答案，可能会让你的业务陷入严重麻烦。
- en: '**Bio**: [Jay Budzik](https://www.linkedin.com/in/jaybudzik) is a product and
    technology leader with track record of driving exceptional revenue growth by applying
    AI and ML to create new products and services.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介**：[Jay Budzik](https://www.linkedin.com/in/jaybudzik) 是一位产品和技术领导者，有通过应用AI和ML创建新产品和服务的优秀收入增长记录。'
- en: '[Original](https://www.zestfinance.com/blog/explainability-snake-oil). Reposted
    with permission.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://www.zestfinance.com/blog/explainability-snake-oil)。转载已获许可。'
- en: '**Resources:**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：**'
- en: '[On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning
    education](https://www.kdnuggets.com/education/online.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在线和基于网络的：分析、数据挖掘、数据科学、机器学习教育](https://www.kdnuggets.com/education/online.html)'
- en: '[Software for Analytics, Data Science, Data Mining, and Machine Learning](https://www.kdnuggets.com/software/index.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分析、数据科学、数据挖掘和机器学习的软件](https://www.kdnuggets.com/software/index.html)'
- en: '**Related:**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Learning Machine Learning vs Learning Data Science](https://www.kdnuggets.com/2018/12/learning-machine-learning-data-science.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习机器学习与学习数据科学](https://www.kdnuggets.com/2018/12/learning-machine-learning-data-science.html)'
- en: '[Math for Machine Learning](https://www.kdnuggets.com/2018/12/rhan-math-machine-learning-ebook.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习数学](https://www.kdnuggets.com/2018/12/rhan-math-machine-learning-ebook.html)'
- en: '[Interpretability is crucial for trusting AI and machine learning](https://www.kdnuggets.com/2018/11/interpretability-trust-ai-machine-learning.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[可解释性对信任AI和机器学习至关重要](https://www.kdnuggets.com/2018/11/interpretability-trust-ai-machine-learning.html)'
- en: '* * *'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的快车道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力。'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT需求。'
- en: '* * *'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Data Analytics: The Four Approaches to Analyzing Data and How To…](https://www.kdnuggets.com/2023/04/data-analytics-four-approaches-analyzing-data-effectively.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据分析：四种数据分析方法及其有效应用](https://www.kdnuggets.com/2023/04/data-analytics-four-approaches-analyzing-data-effectively.html)'
- en: '[Master the Power of Data Analytics: The Four Approaches to Analyzing Data](https://www.kdnuggets.com/2023/03/master-power-data-analytics-four-approaches-analyzing-data.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握数据分析的力量：四种数据分析方法](https://www.kdnuggets.com/2023/03/master-power-data-analytics-four-approaches-analyzing-data.html)'
- en: '[Data Labeling for Machine Learning: Market Overview, Approaches, and Tools](https://www.kdnuggets.com/2021/12/data-labeling-ml-overview-and-tools.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习的数据标注：市场概况、方法和工具](https://www.kdnuggets.com/2021/12/data-labeling-ml-overview-and-tools.html)'
- en: '[Machine Learning’s Sweet Spot: Pure Approaches in NLP and Document Analysis](https://www.kdnuggets.com/2022/05/machine-learning-sweet-spot-pure-approaches-nlp-document-analysis.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习的甜点：NLP和文档分析中的纯方法](https://www.kdnuggets.com/2022/05/machine-learning-sweet-spot-pure-approaches-nlp-document-analysis.html)'
- en: '[Diffusion and Denoising: Explaining Text-to-Image Generative AI](https://www.kdnuggets.com/diffusion-and-denoising-explaining-text-to-image-generative-ai)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[扩散与去噪：解释文本到图像的生成AI](https://www.kdnuggets.com/diffusion-and-denoising-explaining-text-to-image-generative-ai)'
- en: '[Automated Machine Learning with Python: A Comparison of Different…](https://www.kdnuggets.com/2023/03/automated-machine-learning-python-comparison-different-approaches.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Python的自动化机器学习：不同方法的比较…](https://www.kdnuggets.com/2023/03/automated-machine-learning-python-comparison-different-approaches.html)'
