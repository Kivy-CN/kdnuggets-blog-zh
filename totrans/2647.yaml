- en: All Machine Learning Algorithms You Should Know in 2021
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你应该了解的所有机器学习算法（2021）
- en: 原文：[https://www.kdnuggets.com/2021/01/machine-learning-algorithms-2021.html](https://www.kdnuggets.com/2021/01/machine-learning-algorithms-2021.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/01/machine-learning-algorithms-2021.html](https://www.kdnuggets.com/2021/01/machine-learning-algorithms-2021.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '![](../Images/8eaa944a2ca4608d78159c07d9972a2f.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8eaa944a2ca4608d78159c07d9972a2f.png)'
- en: '*Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/machine-learning?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText).*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*照片由 [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来自 [Unsplash](https://unsplash.com/s/photos/machine-learning?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)。*'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织在IT领域'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: As my knowledge in machine learning grows, so does the number of machine learning
    algorithms! This article will cover machine learning algorithms that are commonly
    used in the data science community.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我在机器学习领域知识的增长，机器学习算法的数量也在增加！本文将介绍数据科学社区中常用的机器学习算法。
- en: Keep in mind that I’ll be elaborating on some algorithms more than others simply
    because this article would be as long as a book if I thoroughly explained every
    algorithm! I’m also going to try to minimize the amount of math in this article
    because I know it can be pretty daunting for those who aren’t mathematically savvy.
    Instead, I’ll try to give a concise summary of each and point out some of the
    key features.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我将会对某些算法进行更多的详细阐述，因为如果我详细解释每个算法，这篇文章将会和一本书一样长！我也会尽量减少文章中的数学内容，因为我知道对于那些不擅长数学的人来说，这可能会相当吓人。相反，我会尽量给出每个算法的简明总结，并指出一些关键特征。
- en: With that in mind, I’m going to start with some of the more fundamental algorithms
    and then dive into some newer algorithms like CatBoost, Gradient Boost, and XGBoost.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有鉴于此，我将从一些基本的算法开始，然后深入探讨一些较新的算法，如CatBoost、Gradient Boost和XGBoost。
- en: Linear Regression
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性回归
- en: Linear Regression is one of the most fundamental algorithms used to model relationships
    between a dependent variable and one or more independent variables. In simpler
    terms, it involves finding the ‘line of best fit’ that represents two or more
    variables.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是用于建模因变量与一个或多个自变量之间关系的最基本算法之一。简单来说，它涉及找到代表两个或更多变量的“最佳拟合线”。
- en: The line of best fit is found by minimizing the squared distances between the
    points and the line of best fit — this is known as minimizing the sum of squared
    residuals. A residual is simply equal to the predicted value minus the actual
    value.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最小化点与最佳拟合线之间的平方距离来找到最佳拟合线——这被称为最小化平方残差之和。残差等于预测值减去实际值。
- en: '![](../Images/5e05d6134a52fd2abe790ba257d5cb2e.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e05d6134a52fd2abe790ba257d5cb2e.png)'
- en: '*Image created by the author.*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片由作者创建。*'
- en: In case it doesn’t make sense yet, consider the image above. Comparing the green
    line of best fit to the red line, notice how the vertical lines (the residuals)
    are much bigger for the green line than the red line. This makes sense because
    the green line is so far away from the points that it isn’t a good representation
    of the data at all!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果现在还不明白，可以考虑上面的图片。比较绿色的最佳拟合线和红色的线，注意垂直线（残差）对于绿色线来说明显大于红色线。这是因为绿色线离数据点太远，根本不能很好地代表数据！
- en: If you want to learn more about the math behind linear regression, I would start
    off with [Brilliant’s explanation](https://brilliant.org/wiki/linear-regression/#:~:text=Linear%20regression%20is%20a%20technique,a%20linear%20relationship%20between%20them.&text=The%20best%2Dfitting%20linear%20relationship%20between%20the%20variables%20x%20and%20y.).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于线性回归背后的数学知识，我建议你从[Brilliant的解释](https://brilliant.org/wiki/linear-regression/#:~:text=Linear%20regression%20is%20a%20technique,a%20linear%20relationship%20between%20them.&text=The%20best%2Dfitting%20linear%20relationship%20between%20the%20variables%20x%20and%20y.)开始。
- en: Logistic Regression
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Logistic regression is similar to linear regression but is used to model the
    probability of a discrete number of outcomes, typically two. At a glance, logistic
    regression sounds much more complicated than linear regression but really only
    has one extra step.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归类似于线性回归，但用于建模离散数量结果的概率，通常是两个。乍看之下，逻辑回归听起来比线性回归复杂得多，但实际上只多了一个步骤。
- en: First, you calculate a score using an equation similar to the equation for the
    line of best fit for linear regression.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你使用一个类似于线性回归最佳拟合线方程的方程来计算得分。
- en: '![](../Images/6fd0167a72ce61116b19064a3222332f.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6fd0167a72ce61116b19064a3222332f.png)'
- en: The extra step is feeding the score that you previously calculated in the sigmoid
    function below so that you get a probability in return. This probability can then
    be converted to a binary output, either 1 or 0.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的步骤是将你之前计算的得分输入到下面的Sigmoid函数中，以便获得一个概率。这个概率然后可以转化为二进制输出，0或1。
- en: '![](../Images/f67b60747da4919edc1b698dccb894e9.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f67b60747da4919edc1b698dccb894e9.png)'
- en: To find the weights of the initial equation and calculate the score, methods
    like gradient descent or maximum likelihood are used. Since it’s beyond the scope
    of this article, I won’t go into much more detail, but now you know how it works!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到初始方程的权重并计算得分，可以使用梯度下降法或最大似然法等方法。由于这超出了本文的范围，我不会详细讨论，但现在你知道它是如何工作的了！
- en: K-Nearest Neighbors
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K-最近邻
- en: '![](../Images/31cb440d1b26ee334db8d8f57ba0a71c.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31cb440d1b26ee334db8d8f57ba0a71c.png)'
- en: '*Image created by the author.*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*图像由作者创建。*'
- en: K-nearest neighbors is a simple idea. First, you start off with data that is
    already classified (i.e., the red and blue data points). Then when you add a new
    data point, you classify it by looking at the k nearest classified points. Whichever
    class gets the most votes determines what the new point gets classified as.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: K-最近邻是一个简单的概念。首先，你从已经分类的数据（即红色和蓝色的数据点）开始。然后，当你添加一个新的数据点时，你通过查看k个最近的已分类点来对其进行分类。得到最多投票的类别决定了新点的分类。
- en: In this case, if we set *k*=1, we can see that the first nearest point to the
    grey sample is a red data point. Therefore, the point would be classified as red.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，如果我们设置*k*=1，我们可以看到灰色样本的第一个最近点是一个红色数据点。因此，这个点会被分类为红色。
- en: Something to keep in mind is that if the value of *k* is set too low, it can
    be subject to outliers. On the other hand, if the value of *k* is set too high,
    then it might overlook classes with only a few samples.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，如果*k*的值设置得太低，它可能会受到离群值的影响。另一方面，如果*k*的值设置得太高，那么它可能会忽略只有少数样本的类别。
- en: Naive Bayes
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: '[Naive Bayes](https://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html)
    is a classification algorithm. This means that Naive Bayes is used when the output
    variable is discrete.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[朴素贝叶斯](https://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html)是一种分类算法。这意味着，当输出变量是离散的时，使用朴素贝叶斯算法。'
- en: 'Naive Bayes can seem like a daunting algorithm because it requires preliminary
    mathematical knowledge in conditional probability and Bayes Theorem, but it’s
    an extremely simple and ‘naive’ concept, which I’ll do my best to explain with
    an example:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯算法可能看起来很复杂，因为它需要条件概率和贝叶斯定理的初步数学知识，但它是一个非常简单和“朴素”的概念，我会尽力通过一个例子来解释：
- en: '![](../Images/b255beeaffd37abf899784964331ddf7.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b255beeaffd37abf899784964331ddf7.png)'
- en: '*Image created by the author.*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*图像由作者创建。*'
- en: Suppose we have input data on the characteristics of the weather (outlook, temperature,
    humidity, windy) and whether you played golf or not (i.e., last column).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有关于天气特征（如展望、温度、湿度、风力）的输入数据，以及你是否打过高尔夫的数据（即最后一列）。
- en: What Naive Bayes essentially does is compare the **proportion** between each
    input variable and the categories in the output variable. This can be shown in
    the table below.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯算法基本上是比较每个输入变量与输出变量类别之间的**比例**。这可以在下面的表格中显示。
- en: '![](../Images/da6017b20498fbfb145852c1b8c36fc6.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da6017b20498fbfb145852c1b8c36fc6.png)'
- en: '*Image created by the author.*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片由作者创建。*'
- en: To give an example to help you read this, in the **temperature** section, it
    was hot for two days out of the nine days that you played golf (i.e., yes).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助你理解这一点，在**温度**部分，你在你打高尔夫的九天中有两天是炎热的（即，打高尔夫）。
- en: In mathematical terms, you can write this as **the probability of it being hot
    GIVEN that you played golf. **The mathematical notation is P(hot|yes). This is
    known as conditional probability and is essential to understand the rest of what
    I’m about to say.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，你可以将其写为**在你打高尔夫的情况下天气炎热的概率。** 数学符号是P(hot|yes)。这是条件概率的定义，对理解接下来要讲的内容至关重要。
- en: Once you have this, then you can predict whether you’ll play golf or not for
    any combination of weather characteristics.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了这些数据，你就可以预测在任何天气特征组合下你是否会打高尔夫。
- en: 'Imagine that we have a new day with the following characteristics:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下我们有一个新的一天，其特征如下：
- en: 'Outlook: sunny'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天气：晴朗
- en: 'Temperature: mild'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 温度：适中
- en: 'Humidity: normal'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 湿度：正常
- en: 'Windy: false'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有风：假
- en: First, we’ll calculate the probability that you will play golf given X, P(yes|X),
    followed by the probability that you won’t play golf given X, P(no|X).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将计算在给定X的情况下你打高尔夫的概率P(yes|X)，然后是你不打高尔夫的概率P(no|X)。
- en: 'Using the chart above, we can get the following information:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上面的图表，我们可以得到以下信息：
- en: '![](../Images/0dfa3242d96bc275ad2811180c495a56.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0dfa3242d96bc275ad2811180c495a56.png)'
- en: 'Now we can simply input this information into the following formula:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将这些信息简单地输入到以下公式中：
- en: '![](../Images/2e144ad25596ebfdb821104880152173.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e144ad25596ebfdb821104880152173.png)'
- en: Similarly, you would complete the same sequence of steps for P(no|X).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，你需要对P(no|X)执行相同的步骤。
- en: '![](../Images/ca6c2ec598ed7fee6ff411c3ee4efd7e.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca6c2ec598ed7fee6ff411c3ee4efd7e.png)'
- en: Because P(yes|X) > P(no|X), then you can predict that this person would play
    golf given that the outlook is sunny, the temperature is mild, the humidity is
    normal, and it’s not windy.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因为P(yes|X) > P(no|X)，所以你可以预测在天气晴朗、温度适中、湿度正常且无风的情况下，这个人会打高尔夫。
- en: This is the essence of Naive Bayes!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是朴素贝叶斯的本质！
- en: Support Vector Machines
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持向量机
- en: '![](../Images/7de55836d123b70c6f6dd0e9f7f818fe.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7de55836d123b70c6f6dd0e9f7f818fe.png)'
- en: '*Image created by the author.*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片由作者创建。*'
- en: ASupport Vector Machine is a supervised classification technique that can actually
    get pretty complicated but is pretty intuitive at the most fundamental level.
    For the sake of this article, we’ll keep it pretty high level.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机是一种监督分类技术，从本质上来看非常直观，但实际上可能会变得相当复杂。为了本文的目的，我们将保持在较高层次的讨论。
- en: Let’s assume that there are two classes of data. A support vector machine will
    find a** hyperplane **or a boundary between the two classes of data that maximizes
    the margin between the two classes (see above). There are many planes that can
    separate the two classes, but only one plane can maximize the margin or distance
    between the classes.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数据有两个类别。支持向量机将找到一个**超平面**或两个数据类别之间的边界，该边界最大化两个类别之间的间隔（见上图）。虽然有许多平面可以分隔这两个类别，但只有一个平面可以最大化两个类别之间的间隔或距离。
- en: If you want to get into the math behind support vector machines, check out this [series
    of articles](https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入了解支持向量机的数学背景，请查看这个[系列文章](https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/)。
- en: Decision Tree
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: '![](../Images/ac0429fc3849c937de001e97421a9d47.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac0429fc3849c937de001e97421a9d47.png)'
- en: '*Image created by the author.*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片由作者创建。*'
- en: Random Forest
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: 'Before understanding random forests, there are a couple of terms that you’ll
    need to know:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解随机森林之前，你需要了解几个术语：
- en: '**Ensemble learning**is a method where multiple learning algorithms are used
    in conjunction. The purpose of doing so is that it allows you to achieve higher
    predictive performance than if you were to use an individual algorithm by itself.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成学习**是一种将多个学习算法结合使用的方法。这样做的目的是使你能够实现比单独使用任何一个算法更高的预测性能。'
- en: '**Bootstrap sampling**is a resampling method that uses random sampling with
    replacement. It sounds complicated but trust me when I say it’s REALLY simple
    — [read more about it here](https://towardsdatascience.com/what-is-bootstrap-sampling-in-machine-learning-and-why-is-it-important-a5bb90cbd89a).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自助法采样**是一种使用带替换的随机采样的重采样方法。这听起来很复杂，但相信我，当我说它真的很简单时——[在这里](https://towardsdatascience.com/what-is-bootstrap-sampling-in-machine-learning-and-why-is-it-important-a5bb90cbd89a)阅读更多。'
- en: '**Bagging**when you use the aggregate of the bootstrapped datasets to make
    a decision — I dedicated an article to this topic, so feel free to check it out [here](https://towardsdatascience.com/ensemble-learning-bagging-and-boosting-explained-in-3-minutes-2e6d2240ae21) if
    this doesn’t make complete sense.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**袋装**当你使用自助法数据集的聚合来做决策时——我专门写了一篇关于这个主题的文章，所以如果这让你感到困惑，可以[在这里](https://towardsdatascience.com/ensemble-learning-bagging-and-boosting-explained-in-3-minutes-2e6d2240ae21)查看一下。'
- en: Now that you understand these terms let’s dive into it.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了这些术语，让我们深入探讨一下。
- en: Random forests are an ensemble learning technique that builds off of decision
    trees. Random forests involve creating multiple decision trees using bootstrapped
    datasets of the original data and randomly selecting a subset of variables at
    each step of the decision tree. The model then selects the mode of all of the
    predictions of each decision tree (bagging). What’s the point of this? By relying
    on a “majority wins” model, it reduces the risk of error from an individual tree.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一种集成学习技术，建立在决策树的基础上。随机森林涉及使用原始数据的自助法数据集创建多个决策树，并在每一步决策树中随机选择一个变量子集。然后模型选择每棵决策树预测的众数（袋装）。这样做的意义是什么？通过依赖于“多数决胜”的模型，降低了单棵树的错误风险。
- en: '![](../Images/18580c1ee180ad6d725b4911003ab70c.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18580c1ee180ad6d725b4911003ab70c.png)'
- en: '*Image created by the author.*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*图像由作者创作。*'
- en: For example, if we created one decision tree, the third one, it would predict
    0\. But if we relied on the mode of all 4 decision trees, the predicted value
    would be 1\. This is the power of random forests!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们创建了一棵决策树，即第三棵树，它的预测结果将是 0。如果我们依赖于所有 4 棵决策树的众数，那么预测值将是 1。这就是随机森林的强大之处！
- en: AdaBoost
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AdaBoost
- en: AdaBoost, or Adaptive Boost, is also an ensemble algorithm that leverages bagging
    and boostingmethods to develop an enhanced predictor.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost，即自适应增强，也是一个集成算法，利用袋装和提升方法来开发一个增强型预测器。
- en: 'AdaBoost is similar to Random Forests in the sense that the predictions are
    taken from many decision trees. However, there are three main differences that
    make AdaBoost unique:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 与随机森林类似，都是通过多个决策树来进行预测。然而，有三个主要的区别使得 AdaBoost 独特：
- en: '![](../Images/4383db4a9efe4c4de296ff65beaac8f1.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4383db4a9efe4c4de296ff65beaac8f1.png)'
- en: '*Example of a stump.*'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*弱分类器的示例。*'
- en: First, AdaBoost creates a forest of stumps rather than trees. A stump is a tree
    that is made of only one node and two leaves (like the image above).
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，AdaBoost 创建的是弱分类器的森林，而不是树。弱分类器是一棵仅由一个节点和两个叶子组成的树（如上图所示）。
- en: Second, the stumps that are created are not equally weighted in the final decision
    (final prediction). Stumps that create more errors will have less say in the final
    decision.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其次，创建的弱分类器在最终决策（最终预测）中并不是等权重的。产生更多错误的弱分类器在最终决策中的影响力会更小。
- en: Lastly, the order in which the stumps are made is important because each stump
    aims to reduce the errors that the previous stump(s) made.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，弱分类器的创建顺序很重要，因为每个弱分类器旨在减少前一个弱分类器所犯的错误。
- en: In essence, AdaBoost takes a more iterative approach in the sense that it seeks
    to iteratively improve from the mistakes that the previous stump(s) made.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，AdaBoost 采取了一种更迭代的方法，旨在通过不断改进先前弱分类器所犯的错误来逐步提升模型。
- en: '[*If you want to learn more about the underlying math behind AdaBoost, check
    out my article ‘A Mathematical Explanation of AdaBoost in 5 Minutes’.*](https://towardsdatascience.com/a-mathematical-explanation-of-adaboost-4b0c20ce4382)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[*如果你想了解更多关于 AdaBoost 背后的数学原理，请查看我的文章《5 分钟数学解释 AdaBoost》。*](https://towardsdatascience.com/a-mathematical-explanation-of-adaboost-4b0c20ce4382)'
- en: Gradient Boost
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度提升
- en: 'It’s no surprise that Gradient **Boost**is also an ensemble algorithm that
    uses boostingmethods to develop an enhanced predictor. In many ways, Gradient
    Boost is similar to AdaBoost, but there are a couple of key differences:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度**提升**也是一个集成算法，使用提升方法来开发增强型预测器，这一点也不奇怪。在许多方面，梯度提升与 AdaBoost 相似，但也有几个关键区别：
- en: Unlike AdaBoost, which builds stumps, Gradient Boost builds trees with usually
    8–32 leaves.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与构建桩的 AdaBoost 不同，梯度提升构建通常具有 8–32 个叶子的树。
- en: Gradient Boost views the boosting problem as an optimization problem, where
    it uses a loss function and tries to minimize the error. This is why it’s called ***Gradient***boost,
    as it’s inspired by gradient descent.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升将提升问题视为优化问题，其中使用损失函数并尝试最小化误差。这就是为什么它被称为 ***梯度***提升，因为它受到梯度下降的启发。
- en: Lastly, the trees are used to predict the residuals of the samples (predicted
    minus actual).
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，树用于预测样本的残差（预测值减去实际值）。
- en: While the last point may have been confusing, all that you need to know is that
    Gradient Boost starts by building one tree to try to fit the data, and the subsequent
    trees built after aim to reduce the residuals (error). It does this by concentrating
    on the areas where the existing learners performed poorly, similar to AdaBoost.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最后一点可能令人困惑，但你只需要知道的是，梯度提升开始时通过构建一棵树来拟合数据，随后构建的树旨在减少残差（误差）。它通过集中关注现有学习器表现不佳的区域来实现这一点，类似于
    AdaBoost。
- en: XGBoost
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XGBoost
- en: 'XGBoost is one of the most popular and widely used algorithms today because
    it is simply so powerful. It is similar to Gradient Boost but has a few extra
    features that make it that much stronger including:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 是目前最流行和广泛使用的算法之一，因为它非常强大。它类似于梯度提升，但具有一些额外的特性，使其更强大，包括：
- en: '**A proportional shrinking of leaf nodes (pruning) **— used to improve the
    generalization of the model'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**叶节点的比例收缩（修剪）** — 用于提高模型的泛化能力'
- en: '**Newton Boosting**— provides a direct route to the minima than gradient descent,
    making it much faster'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**牛顿提升** — 提供比梯度下降更直接的最小值路径，使其更快'
- en: '**An extra randomization parameter **— reduces the correlation between trees,
    ultimately improving the strength of the ensemble'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**额外的随机化参数** — 减少树之间的相关性，最终增强集成的强度'
- en: '**Unique penalization of trees**'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独特的树惩罚机制**'
- en: I strongly recommend that you watch [StatQuest’s video](https://www.youtube.com/watch?v=OtD8wVaFm6E&t=3s) to
    understand how the algorithm works in greater detail.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈推荐你观看 [StatQuest 的视频](https://www.youtube.com/watch?v=OtD8wVaFm6E&t=3s)，以更详细地了解算法的工作原理。
- en: LightGBM
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LightGBM
- en: If you thought XGBoost was the best algorithm out there, think again. LightGBM
    is another type of boosting algorithm that has shown to be faster and sometimes
    more accurate than XGBoost.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为 XGBoost 是最好的算法，那么请再考虑一下。LightGBM 是另一种提升算法，它在某些情况下比 XGBoost 更快且更准确。
- en: What makes LightGBM different is that it uses a unique technique called Gradient-based
    One-Side Sampling (GOSS) to filter out the data instances to find a split value.
    This is different than XGBoost, which uses pre-sorted and histogram-based algorithms
    to find the best split.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 的独特之处在于它使用一种叫做**基于梯度的单边采样（GOSS）**的独特技术来筛选数据实例，以找到分裂值。这与 XGBoost 不同，XGBoost
    使用预排序和基于直方图的算法来找到最佳分裂。
- en: '[*Read more about Light GBM vs. XGBoost here*](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db)*!*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[*了解更多关于 Light GBM 和 XGBoost 的信息请点击这里*](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db)*!*'
- en: CatBoost
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CatBoost
- en: 'CatBoost is another algorithm based on Gradient Descent that has a few subtle
    differences that make it unique:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: CatBoost 是另一种基于梯度下降的算法，具有一些微妙的差异，使其独特：
- en: CatBoost implements symmetric trees, which help in decreasing prediction time,
    and it also has a shallower tree-depth by default (six)
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CatBoost 实现了对称树，这有助于减少预测时间，同时默认的树深度较浅（六）
- en: CatBoost leverages random permutations similar to the way XGBoost has a randomization
    parameter
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CatBoost 利用随机排列，类似于 XGBoost 的随机化参数
- en: Unlike XGBoost however, CatBoost handles categorical features more elegantly,
    using concepts like ordered boosting and response coding
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，与 XGBoost 不同，CatBoost 更优雅地处理分类特征，使用如有序提升和响应编码等概念
- en: Overall, what makes CatBoost so powerful is its low latency requirements, which
    translates to it being around **eight times faster** than XGBoost.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，使 CatBoost 如此强大的原因是其低延迟要求，这使得其速度约为 XGBoost 的 **八倍**。
- en: '*If you want to read about CatBoost in greater detail, check out this *[*article*](https://medium.com/@hanishsidhu/whats-so-special-about-catboost-335d64d754ae)*.*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你想更详细地了解 CatBoost，请查看这篇* [*文章*](https://medium.com/@hanishsidhu/whats-so-special-about-catboost-335d64d754ae)*。*'
- en: '**Thanks for Reading!**'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**感谢阅读！**'
- en: If you made it to the end, congrats! You should now have a better idea of all
    of the different machine learning algorithms out there.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你坚持到了最后，恭喜你！你现在应该对所有不同的机器学习算法有了更好的了解。
- en: Don’t feel discouraged if you had a harder time understanding the last few algorithms
    — not only are they more complex, but they’re also relatively new! So stay tuned
    for more resources that will go into these algorithms in greater depth.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在理解最后几个算法时遇到困难，不要气馁——这些算法不仅更复杂，而且相对较新！因此，敬请关注更多将深入探讨这些算法的资源。
- en: '[Original](https://towardsdatascience.com/all-machine-learning-algorithms-you-should-know-in-2021-2e357dd494c7).
    Reposted with permission.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/all-machine-learning-algorithms-you-should-know-in-2021-2e357dd494c7)。经授权转载。'
- en: '**Related:**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Key Data Science Algorithms Explained: From k-means to k-medoids clustering](https://www.kdnuggets.com/2020/12/algorithms-explained-k-means-k-medoids-clustering.html)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关键数据科学算法解释：从k-means到k-medoids聚类](https://www.kdnuggets.com/2020/12/algorithms-explained-k-means-k-medoids-clustering.html)'
- en: '[A Top Machine Learning Algorithm Explained: Support Vector Machines (SVM)](https://www.kdnuggets.com/2020/03/machine-learning-algorithm-svm-explained.html)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一种顶级机器学习算法解释：支持向量机（SVM）](https://www.kdnuggets.com/2020/03/machine-learning-algorithm-svm-explained.html)'
- en: '[Decision Tree Algorithm, Explained](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[决策树算法解释](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)'
- en: More On This Topic
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[建立一个稳固的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用管道编写干净的Python代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每位数据科学家都应该知道的三个R库（即使你使用Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2021年最佳ETL工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每位初学者数据科学家都应该掌握的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，并通过寻找目标来…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
