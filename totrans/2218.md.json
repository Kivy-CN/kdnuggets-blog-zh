["```py\nimport torch\nx = torch.rand(3, 3)\nprint(x)\n```", "```py\nimport lightning\nprint(lightning.__version__)\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\n```", "```py\nimport lightning as pl\n\nclass LitModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = Net()\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.forward(x)\n        loss = F.cross_entropy(y_hat, y)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=0.02)\n\nmodel = LitModel()\n```", "```py\ntrainer = pl.Trainer()\ntrainer.fit(model, train_dataloader, val_dataloader)\n```", "```py\nresult = trainer.test(model, test_dataloader)\nprint(result)\n```", "```py\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\n# Assume Net class and train_dataloader, val_dataloader, test_dataloader are defined\n\nclass Net(torch.nn.Module):\n    # Define your network architecture here\n    pass\n\n# Initialize model and optimizer\nmodel = Net()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n\n# Training Loop\nfor epoch in range(10):  # Number of epochs\n    for batch_idx, (x, y) in enumerate(train_dataloader):\n        optimizer.zero_grad()\n        y_hat = model(x)\n        loss = F.cross_entropy(y_hat, y)\n        loss.backward()\n        optimizer.step()\n\n# Validation Loop\nmodel.eval()\nwith torch.no_grad():\n    for x, y in val_dataloader:\n        y_hat = model(x)\n\n# Testing Loop and Evaluate\nmodel.eval()\ntest_loss = 0\nwith torch.no_grad():\n    for x, y in test_dataloader:\n        y_hat = model(x)\n        test_loss += F.cross_entropy(y_hat, y, reduction='sum').item()\ntest_loss /= len(test_dataloader.dataset)\nprint(f\"Test loss: {test_loss}\") \n```", "```py\ntuner = pl.Tuner(trainer)\ntuner.fit(model, train_dataloader)\nprint(tuner.results)\n```", "```py\nmodel = LitModel()\nmodel.add_module('dropout', nn.Dropout(0.2)) # Regularization\n\ntrainer = pl.Trainer(early_stop_callback=True) # Early stopping\n```", "```py\n# Save\ntrainer.save_checkpoint(\"model.ckpt\") \n\n# Load\nmodel = LitModel.load_from_checkpoint(checkpoint_path=\"model.ckpt\")\n```"]