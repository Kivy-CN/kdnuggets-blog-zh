- en: The Essential Guide to Transformers, the Key to Modern SOTA AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《变压器的基本指南：现代最先进人工智能的关键》
- en: 原文：[https://www.kdnuggets.com/2021/06/essential-guide-transformers-key-modern-sota-ai.html](https://www.kdnuggets.com/2021/06/essential-guide-transformers-key-modern-sota-ai.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/06/essential-guide-transformers-key-modern-sota-ai.html](https://www.kdnuggets.com/2021/06/essential-guide-transformers-key-modern-sota-ai.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: Are you overwhelmed by the vast array of X-formers?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否被各种X-formers的浩繁数量所压倒？
- en: No, they aren't another new team of mutants from Marvel in the tradition of
    the X-Men, X-Factor, and X-Force. X-formers are the name being given to the wide
    array of Transformer variants that have been implemented or proposed. You likely
    know Transformers from their recent spate of success stories in natural language
    processing, computer vision, and other areas of artificial intelligence, but are
    familiar with all of the X-formers? More importantly, do you know the differences,
    and why you might use one over another?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 不，它们不是像X战警、X因素和X力量那样来自漫威的新变种团队。X-formers是指各种变压器变体的名称，这些变体已经被实现或提议。你可能通过最近在自然语言处理、计算机视觉和其他人工智能领域的成功故事了解了变压器，但你是否熟悉所有的X-formers？更重要的是，你知道它们之间的区别，以及为什么你可能会选择其中一个而不是另一个吗？
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三名课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织在IT方面'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**[A Survey of Transformers](https://arxiv.org/abs/2106.04554)**, by Tianyang
    Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu, has been written to help interested
    readers in this regard.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**[变压器调查](https://arxiv.org/abs/2106.04554)**，由Tianyang Lin, Yuxin Wang, Xiangyang
    Liu 和 Xipeng Qiu编写，旨在帮助对此感兴趣的读者。'
- en: 'From the abstract:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要：
- en: '[A] great variety of Transformer variants (a.k.a. X-formers) have been proposed,
    however, a systematic and comprehensive literature review on these Transformer
    variants is still missing. In this survey, we provide a comprehensive review of
    various X-formers. We first briefly introduce the vanilla Transformer and then
    propose a new taxonomy of X-formers. Next, we introduce the various X-formers
    from three perspectives: architectural modification, pre-training, and applications.
    Finally, we outline some potential directions for future research.'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[A] 提出了各种各样的变压器变体（即X-formers），然而，系统全面的文献综述仍然缺失。在这篇调查中，我们提供了各种X-formers的综合综述。我们首先简要介绍了原始变压器，然后提出了一种新的X-formers分类法。接下来，我们从三个方面介绍了各种X-formers：架构修改、预训练和应用。最后，我们概述了一些未来研究的潜在方向。'
- en: That's the 30,000 foot view of what this survey papers covers; let's have a
    closer look.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是本调查论文所涵盖的三万英尺高度的视角；让我们更详细地看看。
- en: '![Vanilla Transformer architecture](../Images/6f98b90273d2f7d0533a2c4a294c024e.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![原始变压器架构](../Images/6f98b90273d2f7d0533a2c4a294c024e.png)'
- en: 'Figure 1\. Categorization of Transformer variants (source: [A Survey of Transformers](https://arxiv.org/abs/2106.04554))'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 变压器变体的分类（来源：[变压器调查](https://arxiv.org/abs/2106.04554)）
- en: 'After introducing the Transformer, the paper establishes the idea of numerous
    subsequent Transformer variants, the aforementioned X-formers, and notes that
    these different architectures have all attempted to improve on the original from
    one of these perspectives: model efficiency, model generalization, and model adaptation.
    The paper will go on to note these perspectives of change as they relate to the
    various X-formers subsequently brought up.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍变压器之后，论文建立了许多后续变压器变体的概念，即上述的X-formers，并指出这些不同的架构都试图从以下一个方面改进原始变压器：模型效率、模型泛化和模型适应。论文将继续讨论这些变化的观点，以及它们如何与随后提出的各种X-formers相关。
- en: Next, the paper goes into deeper technical detail of the vanilla Transformer
    architecture (see Figure 2), including its usage and performance analysis.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，论文深入探讨了普通Transformer架构的技术细节（见图2），包括其使用情况和性能分析。
- en: 'Before getting into specific variants, the authors compare and contrast the
    Transformer with other architectures. This discussion includes an analysis of
    both the self-attention mechanism and inductive bias. Herein, Transformers are
    compared to the architectures of: fully-connected deep neural networks, convolutional
    networks, and recurrent networks, with regard to per-layer complexity, minimum
    number of sequential operations and maximum path lengths.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在具体变体之前，作者对Transformer与其他架构进行了比较和对比。这一讨论包括对自注意力机制和归纳偏差的分析。在这里，Transformer与以下架构进行比较：全连接深度神经网络、卷积网络和递归网络，比较了每层的复杂性、最小的序列操作数量和最大路径长度。
- en: 'A taxonomy of Transformers is then presented (see Figure 1), noting that a
    variety of proposed models attempt to improve on the vanilla Transformer from
    three perspectives: types of architecture modification, pre-training methods,
    and applications.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后展示了一个Transformer的分类（见图1），指出各种提出的模型试图从三个方面改进普通Transformer：架构修改类型、预训练方法和应用。
- en: '![Categorization of Transformer variants](../Images/786f7c58f8c64287607c976d949eef28.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![Transformer变体的分类](../Images/786f7c58f8c64287607c976d949eef28.png)'
- en: Figure 2\. Vanilla Transformer architecture (source:[A Survey of Transformers](https://arxiv.org/abs/2106.04554))
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 普通Transformer架构（来源：[A Survey of Transformers](https://arxiv.org/abs/2106.04554)）
- en: 'The real meat of the survey comes in the following sections, with the first
    of these focusing on proposed improvements to the attention mechanism, discussing
    in detail a number of these above-mentioned specific architectures and laying
    out a taxonomy of these Transformers based on these improvements. Several distinct
    directions of development have been taken in order to facilitate improvements
    and these are all discussed, including:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 调查的核心内容出现在以下几节中，其中第一节专注于对注意力机制的改进，详细讨论了上述几个特定架构，并基于这些改进提出了Transformer的分类。为了促进改进，已经采取了几种不同的发展方向，并讨论了这些方向，包括：
- en: Sparse Attention
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏注意力
- en: Linearized Attention
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性化注意力
- en: Query Prototyping and Memory Compression
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询原型和记忆压缩
- en: Low-rank Self-Attention
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低秩自注意力
- en: Attention with Prior
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有先验的注意力
- en: Improved Multi-Head Mechanism
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进的多头机制
- en: The authors then turn their attention to other X-former module-level modifications,
    in contrast to architecture level modifications (discussed in a subsequent section).
    These modifications include position representations, layer normalization, and
    position-wise feed-forward network layers. Each of these are discussed in detail,
    connecting how and why each are important to Transfomer architecture, and what
    various improvements have proposed.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 作者随后转向其他X-former模块级的修改，与架构级的修改（在后续部分讨论）相对。这些修改包括位置表示、层归一化和位置-wise前馈网络层。详细讨论了这些修改，阐明了它们为何对Transformer架构重要，以及各种改进的提案。
- en: 'Then comes architecture level X-former variants, discussing modifications beyond
    the module level, including:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是架构级的X-former变体，讨论了模块级之外的修改，包括：
- en: Adapting Transformer to Be Lightweight
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使Transformer轻量化
- en: Strengthening Cross-Block Connectivity
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化跨块连接
- en: Adaptive Computation Time
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自适应计算时间
- en: Transformers with Divide-and-Conquer Strategies
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有分治策略的Transformers
- en: Exploring Alternative Architecture
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索替代架构
- en: The last modification type, Exploring Alternative Architecture, notes that "several
    studies have explored alternative architectures for Transformer" and discusses
    where some of this research might be heading.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一种修改类型，探索替代架构，提到“几项研究探索了Transformer的替代架构”，并讨论了这些研究可能的未来发展方向。
- en: 'The next section discusses pre-trained Transformers, while noting a potential
    problem with Transformers:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分讨论了预训练的Transformers，同时指出了Transformers的潜在问题：
- en: Transformer does not make any assumption about how the data is structured. On
    the one hand, this effectively makes Transformer a very universal architecture
    that has the potential of capturing dependencies of different ranges. On the other
    hand, this makes Transformer prone to overfitting when the data is limited. One
    way to alleviate this issue is to introduce inductive bias into the model.
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Transformer 不对数据结构做任何假设。一方面，这使得 Transformer 成为一种非常通用的架构，具有捕捉不同范围依赖关系的潜力。另一方面，这使得
    Transformer 在数据有限时容易过拟合。缓解这一问题的一种方法是将归纳偏差引入模型中。
- en: A brief overview of encoder-only (including the BERT family), decoder-only (including
    the GPT family), and encoder-decoder (including BART, an extended approach to
    BERT) architectures as relates to inductive bias is presented.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍了仅编码器（包括 BERT 系列）、仅解码器（包括 GPT 系列）和编码器-解码器（包括 BART，一种扩展的 BERT 方法）架构的简要概述，涉及归纳偏差。
- en: 'The paper follows with a summary of the applications of Transformers — which,
    if you have made it this far, you are probably already aware of — including how
    it is used in NLP, CV, audio, and multimodal applications. Finally, the authors
    finish off with conclusions and future directions for X-former related research,
    noting potential further developments in these directions:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 论文接着总结了 Transformers 的应用——如果你已经看到这一步，你可能已经了解——包括它在 NLP、CV、音频和多模态应用中的使用。最后，作者总结了
    X-former 相关研究的结论和未来方向，指出了这些方向的潜在进一步发展：
- en: Theoretical Analysis
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理论分析
- en: Better Global Interaction Mechanism beyond Attention
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超越注意力的更佳全局交互机制
- en: Unified Framework for Multimodal Data
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多模态数据的统一框架
- en: The authors have a done a great job in what they set out to do in this paper.
    If you are interested in sorting out the specific relationship between the various
    X-formers out there, I suggest that you, too, give [A Survey of Transformers](https://arxiv.org/abs/2106.04554)
    by Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu a read.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在论文中做得非常出色。如果你有兴趣理清各种 X-formers 之间的具体关系，我建议你阅读 Tianyang Lin、Yuxin Wang、Xiangyang
    Liu 和 Xipeng Qiu 的 [Transformers 调查](https://arxiv.org/abs/2106.04554)。
- en: '**Related**:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关**：'
- en: '[Learn Neural Networks for Natural Language Processing Now](/2021/04/learn-neural-networks-natural-language-processing-now.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[现在学习自然语言处理的神经网络](/2021/04/learn-neural-networks-natural-language-processing-now.html)'
- en: '[Great New Resource for Natural Language Processing Research and Applications](/2021/05/great-new-resource-natural-language-processing-research-applications.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理研究和应用的新资源](/2021/05/great-new-resource-natural-language-processing-research-applications.html)'
- en: '[Getting Started with 5 Essential Natural Language Processing Libraries](/2021/02/getting-started-5-essential-nlp-libraries.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[入门 5 个必备自然语言处理库](/2021/02/getting-started-5-essential-nlp-libraries.html)'
- en: More On This Topic
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Introducing MetaGPT''s Data Interpreter: SOTA Open Source LLM-based…](https://www.kdnuggets.com/metagpt-data-interpreter-open-source-llm-based-data-solutions)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍 MetaGPT 的数据解释器：SOTA 开源 LLM 基于…](https://www.kdnuggets.com/metagpt-data-interpreter-open-source-llm-based-data-solutions)'
- en: '[Essential Machine Learning Algorithms: A Beginner''s Guide](https://www.kdnuggets.com/2021/05/essential-machine-learning-algorithms-beginners.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习算法基础：初学者指南](https://www.kdnuggets.com/2021/05/essential-machine-learning-algorithms-beginners.html)'
- en: '[A Data Scientist’s Essential Guide to Exploratory Data Analysis](https://www.kdnuggets.com/2023/06/data-scientist-essential-guide-exploratory-data-analysis.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学家的探索性数据分析基本指南](https://www.kdnuggets.com/2023/06/data-scientist-essential-guide-exploratory-data-analysis.html)'
- en: '[The Essential Guide to SQL’s Execution Order](https://www.kdnuggets.com/the-essential-guide-to-sql-execution-order)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQL 执行顺序的基本指南](https://www.kdnuggets.com/the-essential-guide-to-sql-execution-order)'
- en: '[A Comprehensive Guide to Essential Tools for Data Analysts](https://www.kdnuggets.com/a-comprehensive-guide-to-essential-tools-for-data-analysts)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据分析师必备工具的综合指南](https://www.kdnuggets.com/a-comprehensive-guide-to-essential-tools-for-data-analysts)'
- en: '[10 Modern Data Engineering Tools](https://www.kdnuggets.com/2022/07/10-modern-data-engineering-tools.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10 种现代数据工程工具](https://www.kdnuggets.com/2022/07/10-modern-data-engineering-tools.html)'
