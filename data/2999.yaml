- en: Machine Learning Security
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习安全
- en: 原文：[https://www.kdnuggets.com/2019/01/machine-learning-security.html](https://www.kdnuggets.com/2019/01/machine-learning-security.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/01/machine-learning-security.html](https://www.kdnuggets.com/2019/01/machine-learning-security.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Zak Jost](https://blog.zakjost.com), Research Scientist at Amazon Web
    Services**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：[Zak Jost](https://blog.zakjost.com)，亚马逊网络服务研究科学家**。'
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍
- en: '*As more and more systems leverage ML models in their decision-making processes,
    it will become increasingly important to consider how malicious actors might exploit
    these models, and how to design defenses against those attacks. The purpose of
    this post is to share some of my recent learnings on this topic.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*随着越来越多的系统在其决策过程中利用机器学习模型，考虑恶意行为者如何利用这些模型以及如何设计防御措施将变得越来越重要。本文的目的是分享我在这一主题上的一些最新学习。*'
- en: ML Everywhere
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习无处不在
- en: '*The explosion of available data, processing power, and innovation in the ML
    space have resulted in ML ubiquity. It’s actually quiet easy to build these models
    given the proliferation of open source frameworks and data ([this tutorial](https://machinelearningmastery.com/machine-learning-in-python-step-by-step/)
    takes someone from zero ML/programming knowledge to 6 ML models in about 5-10
    minutes). Further, the ongoing trend from cloud providers to offer ML as a service
    is enabling customers to build solutions without needing to ever write code or
    understand how it works under the hood.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*可用数据、处理能力和机器学习领域创新的爆炸性增长导致了机器学习的普及。由于开源框架和数据的广泛存在，构建这些模型实际上相当简单（[这个教程](https://machinelearningmastery.com/machine-learning-in-python-step-by-step/)可以将一个零机器学习/编程知识的人带到6个机器学习模型中，时间约为5-10分钟）。此外，云服务提供商持续推出机器学习服务，使客户能够构建解决方案而无需编写代码或了解其底层工作原理。*'
- en: Alexa can purchase on our behalf using voice commands. Models identify pornography
    and help make internet platforms safer for our kids. They’re driving cars on our
    roadways and protecting us from scammers and malware. They monitor our credit
    card transactions and internet usage to look for suspicious anomalies.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*Alexa可以通过语音命令代我们进行购买。模型可以识别色情内容，并帮助使互联网平台对我们的孩子更安全。它们正在我们的道路上驾驶汽车，并保护我们免受诈骗和恶意软件的侵害。它们监控我们的信用卡交易和互联网使用，以寻找可疑的异常。*'
- en: “Alexa, buy my cat guacamole”
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: “Alexa，给我的猫买鳄梨酱”
- en: '*The benefit of ML is clear–it just isn’t possible to have a human manually
    review every credit card transaction, every Facebook image, every YouTube video…etc.
    What about the risks?'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*机器学习的好处是显而易见的——不可能让人类手动审查每一笔信用卡交易、每一张Facebook图片、每一个YouTube视频……等等。那么风险呢？*'
- en: It doesn’t take much imagination to understand the possible harm of an ML algorithm
    making mistakes when navigating a driverless car. The common argument is often,
    “as long as it makes fewer mistakes than humans, it’s a net benefit”.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*理解机器学习算法在自动驾驶汽车中出现错误可能带来的潜在危害并不需要过多的想象。常见的论点是，“只要它比人类犯的错误少，就是一种净收益”。*'
- en: 'But what about cases where malicious actors are *actively trying to deceive
    models*? Labsix, a student group from MIT, 3D printed a turtle that’s reliably
    classified as “rifle” by Google’s InceptionV3 image classifier for any camera
    angle^([1](#fn:1)). For speech-to-text systems, Carlini and Wagner found^([2](#fn:2)):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*但当恶意行为者*主动试图欺骗模型*时情况又如何？来自MIT的学生小组Labsix 3D打印了一个在任何摄像角度下都被Google的InceptionV3图像分类器可靠地分类为“步枪”的乌龟^([1](#fn:1))。对于语音转文本系统，Carlini和Wagner发现^([2](#fn:2))：*'
- en: Given any audio waveform, we can produce another that is over 99.9% similar,
    but transcribes as any phrase we choose….Our attack works with 100% success, regardless
    of the desired transcription or initial source audio sample.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*给定任何音频波形，我们可以生成另一种与之99.9%相似的波形，但可以转录成我们选择的任何短语……我们的攻击成功率为100%，无论所需的转录内容或初始音频样本是什么。*'
- en: Papernot, et al. showed that adding a single line of code to malware in a targeted
    way could trick state-of-the-art malware detection models in >60% of the cases^([3](#fn:3)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Papernot等人表明，有针对性地向恶意软件添加一行代码可能会使最先进的恶意软件检测模型在超过60%的情况下受到欺骗^([3](#fn:3))。*
- en: 'By using fairly simple techniques, a bad actor can make even the most performant
    and impressive models wrong in pretty much any way they desire. This image, for
    instance, fools the Google model into deciding with almost 100% confidence that
    it’s a picture of guacamole^([1](#fn:1)):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用相当简单的技术，一个坏演员可以让即使是最具性能和令人印象深刻的模型也以几乎任何他们想要的方式出错。例如，这张图片使谷歌模型几乎以100%的置信度判断它是一张鳄梨酱的图片^([1](#fn:1))：
- en: '![Cat or Guacamole?](../Images/13da1e4c597e08e1adf446dd299c65bf.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![猫还是鳄梨酱？](../Images/13da1e4c597e08e1adf446dd299c65bf.png)'
- en: 'This is an image of a real-life stop sign that was manipulated so that computer
    vision models were tricked 100% of the time in drive-by experiments into thinking
    it was a “Speed Limit 45 MPH” sign^([4](#fn:4)):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一张真实世界中交通标志的图片，该标志被操控，以致于在经过实验中计算机视觉模型被100%地欺骗，误以为这是一个“限速45英里/小时”的标志^([4](#fn:4))：
- en: '![Adversarial Stop Sign](../Images/1a4f4bf5719865394091eb9d6b92b2cc.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![对抗性停车标志](../Images/1a4f4bf5719865394091eb9d6b92b2cc.png)'
- en: How so good, but *so bad*?
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 怎么这么好，却*这么糟*？
- en: '*In all of these examples, the basic idea is to perturb an input in a way that
    maximizes the change to the model’s output. These are known as “adversarial examples”.
    With this framework, you can figure out how to most efficiently tweak the cat
    image so that the model thinks it’s guacamole. This is sort of like getting all
    the little errors to line up and point in the same direction, so that snowflakes
    turn into an avalanche. Technically, this reduces to finding the gradient of the
    output with respect to the input–something ML practitioners are well-equipped
    to do!'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*在所有这些例子中，基本的思想是以最大化模型输出变化的方式来扰动输入。这些被称为“对抗性示例”。利用这个框架，你可以找出如何最有效地调整猫的图像，使得模型认为它是鳄梨酱。这有点像让所有的小错误排列成一行并指向相同的方向，从而使雪花变成雪崩。从技术上讲，这等同于找到输出对输入的梯度——这是机器学习从业者很擅长的事情！*'
- en: It’s worth stressing that these changes are mostly imperceptible. For instance,
    listen to [these audio samples](https://nicholas.carlini.com/code/audio_adversarial_examples/).
    Despite sounding identical to my ears, one translates to “without the dataset
    the article is useless” and the other to “okay google, browse to evil.com”. It’s
    further worth stressing that real malicious users are not always constrained to
    making imperceptible changes, so we should assume this is a lower-bound estimate
    on security vulnerability.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 值得强调的是，这些变化大多是难以察觉的。例如，听听 [这些音频样本](https://nicholas.carlini.com/code/audio_adversarial_examples/)。尽管听起来对我来说是相同的，但一个翻译为“没有数据集这篇文章就没用”，另一个翻译为“好的谷歌，浏览到evil.com”。更值得强调的是，真正的恶意用户并不总是受限于做出难以察觉的变化，因此我们应该假设这是对安全漏洞的一个下限估计。
- en: But do *I* have to worry?
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 那么*我*需要担心吗？
- en: '*OK, so there’s a problem with the robustness of these models that makes them
    fairly easy to exploit. But unless you’re Google or Facebook, you’re probably
    not building huge neural networks in production systems, so you don’t have to
    worry…right? *Right!?*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*好吧，所以这些模型的鲁棒性存在问题，使得它们相对容易被利用。但是除非你是谷歌或脸书，你可能不会在生产系统中构建大型神经网络，所以你不必担心……对吧？*
    对吧!？'
- en: Wrong. This problem is not unique to neural networks. In fact, adversarial examples
    found to fool one model often fool other models, even if they were trained using
    a different architecture, dataset, or even algorithm. This means that even if
    you were to ensemble models of different types, you’re *still not safe*^([5](#fn:5)).
    If you’re exposing a model to the world, even indirectly, where someone can send
    an input to it and get a response, you’re at risk. The history of this field started
    with exposing the vulnerability of *linear models* and was only later re-kindled
    in the context of deep networks^([6](#fn:6)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 错了。这个问题并不是神经网络所独有的。实际上，发现能欺骗一种模型的对抗性示例往往也能欺骗其他模型，即使它们使用了不同的架构、数据集，甚至算法。这意味着即使你组合了不同类型的模型，你*仍然不安全*^([5](#fn:5))。如果你将模型暴露给世界，即使是间接地，让别人能向它发送输入并得到响应，你就有风险。这个领域的历史从暴露*线性模型*的脆弱性开始，后来才在深度网络的背景下重新点燃^([6](#fn:6))。
- en: How do we stop it?
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们怎么阻止它？
- en: '*There’s a continual arms race between attacks and defenses. This recent [“best
    paper” of ICML 2018](https://nicholas.carlini.com/papers/2018_icml_obfuscatedgradients.pdf)
    “broke” 7 of the 9 defenses presented in the same year’s conference papers. It’s
    not likely that this trend will stop any time soon.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*攻击和防御之间存在持续的军备竞赛。最近的 [ICML 2018的“最佳论文”](https://nicholas.carlini.com/papers/2018_icml_obfuscatedgradients.pdf)
    “击破”了同年会议论文中提出的9种防御中的7种。这个趋势不太可能在短期内停止。*'
- en: So what’s an average ML practitioner to do, who likely doesn’t have time to
    stay on the very cutting of ML security literature, much less endlessly incorporate
    new defenses to all outward-facing production models? In my judgment, the only
    sane approach is to design systems that have multiple sources of intelligence,
    such that a single point of failure does not destroy the efficacy of the entire
    system. This means that you assume an individual model can be broken, and you
    design your systems to be robust against that scenario.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，普通的机器学习从业者该怎么办呢？他们可能没有时间跟进最新的机器学习安全文献，更不用说不断地将新防御措施整合到所有面向外部的生产模型中。在我看来，唯一理智的办法是设计多个信息来源的系统，以便单点故障不会破坏整个系统的效能。这意味着你假设单一模型可能会被攻破，并设计你的系统以抵御这种情况。
- en: For instance, it’s likely a very dangerous idea to have driverless cars entirely
    navigated by computer vision ML systems (for more reasons than just security).
    Redundant measurements of the environment that use orthogonal information like
    LIDAR, GPS, and historic records might help refute an adversarial vision result.
    This naturally presumes the system is designed to integrate these signals to make
    a final judgment.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，完全由计算机视觉机器学习系统导航的无人驾驶汽车可能是一个非常危险的想法（原因不仅仅是安全问题）。使用激光雷达、GPS 和历史记录等正交信息的环境冗余测量可能有助于反驳对抗性视觉结果。这自然假设系统被设计成整合这些信号以做出最终判断。
- en: 'The larger point is that we need to recognize that model security is a substantial
    and pervasive risk that will only increase with time as ML is incorporated more
    and more into our lives. As such, we will need to build the muscle as ML practitioners
    to think about these risks and design systems robust against them. In the same
    way that we take precautions in our web apps to protect our systems against malicious
    users, we should also be proactive with model security risk. Just as institutions
    have Application Security Review groups that do e.g. penetration testing of software,
    we will need to build Model Security Review groups that serve a similar function.
    One thing is for sure: this problem won’t be going away any time soon, and will
    likely grow in relevance.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 更大的问题是，我们需要认识到模型安全是一个重大的、普遍存在的风险，随着机器学习越来越多地融入我们的生活，这一风险只会随着时间的推移而增加。因此，我们需要作为机器学习从业者培养思考这些风险和设计抗风险系统的能力。正如我们在网络应用中采取预防措施以保护系统免受恶意用户的攻击一样，我们也应该对模型安全风险采取积极措施。正如机构有应用安全审查小组进行例如渗透测试，我们也需要建立类似功能的模型安全审查小组。可以肯定的是：这个问题不会很快消失，而且可能会变得越来越重要。
- en: '***If you’d like to learn more about this topic, Biggio and Roli’s paper gives
    a wonderful review and history of the field, including totally different attack
    methods not mentioned here (e.g. data poisoning).^([6](#fn:6))***'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '***如果你想了解更多关于这个话题的信息，Biggio 和 Roli 的论文提供了对该领域的精彩回顾和历史，包括一些这里未提及的完全不同的攻击方法（例如数据中毒）。^([6](#fn:6))***'
- en: References
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*1.  “Fooling Neural Networks in the Physical World.” Labsix, 31 Oct. 2017,
    [www.labsix.org/physical-objects-that-fool-neural-nets](https://www.labsix.org/physical-objects-that-fool-neural-nets/).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*1. “在物理世界中欺骗神经网络。” Labsix，2017 年 10 月 31 日，[www.labsix.org/physical-objects-that-fool-neural-nets](https://www.labsix.org/physical-objects-that-fool-neural-nets/)。'
- en: '[^([return])](#fnref:1)'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[^([返回])](#fnref:1)'
- en: 'N. Carlini, D. Wagner. “Audio Adversarial Examples: Targeted Attacks on Speech-to-Text.”
    arXiv preprint [arXiv:1801.01944](https://arxiv.org/abs/1801.01944), 2018.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: N. Carlini, D. Wagner. “音频对抗样本：对语音转文本的有针对性攻击。” arXiv 预印本 [arXiv:1801.01944](https://arxiv.org/abs/1801.01944)，2018。
- en: '[^([return])](#fnref:2)'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[^([返回])](#fnref:2)'
- en: 'K. Grosse, N. Papernot, P. Manoharan, M. Backes, P. McDaniel (2017) [Adversarial
    Examples for Malware Detection](http://www.patrickmcdaniel.org/pubs/esorics17.pdf).
    In: Foley S., Gollmann D., Snekkenes E. (eds) Computer Security – ESORICS 2017\.
    ESORICS 2017\. Lecture Notes in Computer Science, vol 10493\. Springer, Cham.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K. Grosse, N. Papernot, P. Manoharan, M. Backes, P. McDaniel (2017) [恶意软件检测中的对抗样本](http://www.patrickmcdaniel.org/pubs/esorics17.pdf)。在：Foley
    S., Gollmann D., Snekkenes E.（编辑）《计算机安全 – ESORICS 2017》。ESORICS 2017。计算机科学讲义，卷
    10493。Springer，Cham。
- en: '[^([return])](#fnref:3)'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[^([返回])](#fnref:3)'
- en: K. Eykhold, I. Evtimov, et al. “Robust Physical-World Attacks on Deep Learning
    Visual Classification”. arXiv preprint [arXiv:1707.08945](https://arxiv.org/abs/1707.08945),
    2017.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K. Eykhold, I. Evtimov 等。“对深度学习视觉分类的稳健物理世界攻击”。arXiv 预印本 [arXiv:1707.08945](https://arxiv.org/abs/1707.08945)，2017。
- en: '[^([return])](#fnref:4)'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[^([返回])](#fnref:4)'
- en: 'N. Papernot, P. McDaniel, I.J. Goodfellow. “Transferability in Machine Learning:
    from Phenomena to Black-Box Attacks using Adversarial Samples”. arXiv preprint
    [arXiv:1605.07277](https://arxiv.org/abs/1605.07277), 2016.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'N. Papernot, P. McDaniel, I.J. Goodfellow. “Transferability in Machine Learning:
    from Phenomena to Black-Box Attacks using Adversarial Samples”. arXiv 预印本 [arXiv:1605.07277](https://arxiv.org/abs/1605.07277),
    2016.'
- en: '[^([return])](#fnref:5)'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[^([返回])](#fnref:5)'
- en: 'B. Biggio, F. Roli. “Wild Patterns: Ten Years After the Rise of Adversarial
    Machine Learning.” arXiv preprint [arXiv:1712.03141](https://arxiv.org/abs/1712.03141),
    2018.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'B. Biggio, F. Roli. “Wild Patterns: Ten Years After the Rise of Adversarial
    Machine Learning.” arXiv 预印本 [arXiv:1712.03141](https://arxiv.org/abs/1712.03141),
    2018.'
- en: '[^([return])](#fnref:6)'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[^([返回])](#fnref:6)'
- en: '[Original](https://blog.zakjost.com/post/model-security/). Reposted with permission.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://blog.zakjost.com/post/model-security/)。经许可转载。'
- en: '**Bio**: [Zak Jost](https://blog.zakjost.com) is a builder and Machine Learning
    Scientist.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介**: [Zak Jost](https://blog.zakjost.com) 是一名构建者和机器学习科学家。'
- en: '**Resources:**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源:**'
- en: '[On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning
    education](https://www.kdnuggets.com/education/online.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在线和基于网页的：分析、数据挖掘、数据科学、机器学习教育](https://www.kdnuggets.com/education/online.html)'
- en: '[Software for Analytics, Data Science, Data Mining, and Machine Learning](https://www.kdnuggets.com/software/index.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分析、数据科学、数据挖掘和机器学习的软件](https://www.kdnuggets.com/software/index.html)'
- en: '**Related:**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关:**'
- en: '[How To Fine Tune Your Machine Learning Models To Improve Forecasting Accuracy](https://www.kdnuggets.com/2019/01/fine-tune-machine-learning-models-forecasting.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何微调机器学习模型以提高预测准确性](https://www.kdnuggets.com/2019/01/fine-tune-machine-learning-models-forecasting.html)'
- en: '[What were the most significant machine learning/AI advances in 2018?](https://www.kdnuggets.com/2019/01/machine-learning-ai-advances-2018.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2018年最重要的机器学习/AI进展是什么？](https://www.kdnuggets.com/2019/01/machine-learning-ai-advances-2018.html)'
- en: '[How to Monitor Machine Learning Models in Real-Time](https://www.kdnuggets.com/2019/01/monitor-machine-learning-real-time.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何实时监控机器学习模型](https://www.kdnuggets.com/2019/01/monitor-machine-learning-real-time.html)'
- en: '* * *'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析水平'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的IT'
- en: '* * *'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[How Metadata Improves Security, Quality, and Transparency](https://www.kdnuggets.com/2022/04/metadata-improves-security-quality-transparency.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[元数据如何改善安全性、质量和透明度](https://www.kdnuggets.com/2022/04/metadata-improves-security-quality-transparency.html)'
- en: '[The World Needs More Cyber Security Analysts!](https://www.kdnuggets.com/the-world-needs-more-cyber-security-analysts)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[世界需要更多的网络安全分析师！](https://www.kdnuggets.com/the-world-needs-more-cyber-security-analysts)'
- en: '[5 Machine Learning Skills Every Machine Learning Engineer Should…](https://www.kdnuggets.com/2023/03/5-machine-learning-skills-every-machine-learning-engineer-know-2023.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每位机器学习工程师应掌握的5项机器学习技能…](https://www.kdnuggets.com/2023/03/5-machine-learning-skills-every-machine-learning-engineer-know-2023.html)'
- en: '[KDnuggets News, December 14: 3 Free Machine Learning Courses for…](https://www.kdnuggets.com/2022/n48.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets新闻，12月14日：3门免费机器学习课程…](https://www.kdnuggets.com/2022/n48.html)'
- en: '[A Solid Plan for Learning Data Science, Machine Learning, and Deep Learning](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学、机器学习和深度学习的可靠计划](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
- en: '[KDnuggets News, November 2: The Current State of Data Science…](https://www.kdnuggets.com/2022/n43.html)*******'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets新闻，11月2日：数据科学的现状…](https://www.kdnuggets.com/2022/n43.html)'
