- en: 'Introducing TPU v4: Googles Cutting Edge Supercomputer for Large Language Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '介绍 TPU v4: 谷歌前沿超级计算机用于大型语言模型'
- en: 原文：[https://www.kdnuggets.com/2023/04/introducing-tpu-v4-googles-cutting-edge-supercomputer-large-language-models.html](https://www.kdnuggets.com/2023/04/introducing-tpu-v4-googles-cutting-edge-supercomputer-large-language-models.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/04/introducing-tpu-v4-googles-cutting-edge-supercomputer-large-language-models.html](https://www.kdnuggets.com/2023/04/introducing-tpu-v4-googles-cutting-edge-supercomputer-large-language-models.html)
- en: '![Introducing TPU v4: Googles Cutting Edge Supercomputer for Large Language
    Models](../Images/0dad33ae6b45aeffa128995ae7739c43.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![介绍 TPU v4: 谷歌前沿超级计算机用于大型语言模型](../Images/0dad33ae6b45aeffa128995ae7739c43.png)'
- en: Image by Editor
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：编辑
- en: Machine learning and artificial intelligence seem to be growing at a rapid rate
    that some of us can even keep up with. As these machine-learning models get better
    at what they do, they will require better infrastructure and hardware support
    to keep them going. The advancement of machine learning has a direct lead to scaling
    computing performance. So let’s learn more about TPU v4.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习和人工智能似乎在以非常快的速度增长，以至于有些人甚至跟不上。随着这些机器学习模型的不断进步，它们将需要更好的基础设施和硬件支持来维持运行。机器学习的进步直接推动了计算性能的扩展。让我们进一步了解
    TPU v4。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: What is TPU v4?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 TPU v4？
- en: TPU stands for Tensor Processing Unit and they were designed for machine learning
    and deep learning applications. TPU was invented by Google and was constructed
    in a way that it has the ability to be able to handle the high computational needs
    of machine learning and artificial intelligence.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: TPU 代表张量处理单元，它们被设计用于机器学习和深度学习应用。TPU 是谷歌发明的，构造方式使其能够处理机器学习和人工智能的高计算需求。
- en: When Google designed the TPU, they created it as a domain-specific architecture,
    which means they designed it as a matrix processor, instead of it being a general-purpose
    processor so that it specializes in neural network workloads. This solves Google's
    issue of memory access problem which slows down GPUs and CPUs, causing them to
    use more processing power.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当谷歌设计 TPU 时，他们将其设计为特定领域架构，这意味着它被设计为矩阵处理器，而不是通用处理器，从而专注于神经网络工作负载。这解决了谷歌的内存访问问题，这一问题使得
    GPU 和 CPU 的速度减慢，导致它们使用更多的处理能力。
- en: So there’s been TPU v2, v3, and now v4\. So what’s v2 all about?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，TPU v2、v3，现在是 v4。v2 到底是什么？
- en: 'The TPU v2 chip contains two TensorCores, four MXUs, a vector unit, and a scalar
    unit. See the image below:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: TPU v2 芯片包含两个 TensorCores、四个 MXUs、一个矢量单元和一个标量单元。见下图：
- en: '![Introducing TPU v4: Googles Cutting Edge Supercomputer for Large Language
    Models](../Images/259c8fa83f027f061868b432cfacae63.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![介绍 TPU v4: 谷歌前沿超级计算机用于大型语言模型](../Images/259c8fa83f027f061868b432cfacae63.png)'
- en: Image by [Google](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Google](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm)
- en: Optical Circuit Switches (OCSes)
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 光学电路交换机（OCSes）
- en: TPU v4 is the first supercomputer to deploy reconfigurable optical circuit switches.
    Optical circuit switches (OCS) are considered to be more effective. They reduce
    congestion found in previous networks because they are transmitted as they occur.
    OCS improves scalability, availability, modularity, deployment, security, power,
    performance, and more.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: TPU v4 是首个部署可重构光学电路交换机的超级计算机。光学电路交换机（OCS）被认为更有效。它们减少了以前网络中的拥堵，因为它们在发生时进行传输。OCS
    提高了可扩展性、可用性、模块化、部署、安全性、功率、性能等。
- en: OCSes and other optical components in TPU v4 make up less than 5% of TPU v4’s
    system cost and less than 5% of system power.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: TPU v4 中的 OCS 和其他光学组件占 TPU v4 系统成本的不到 5%，系统功耗的不到 5%。
- en: SparseCores
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SparseCores
- en: TPU v4 also is the first supercomputer with hardware support for embedding.
    Neural networks train well on dense vectors, and embeddings are the most effective
    way to transform categorical feature values into dense vectors. TPU v4 include
    third-generation SparseCores, which are dataflow processes that accelerate machine
    learning models that are reliant on embedding.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: TPU v4 也是首个具有硬件支持嵌入的超级计算机。神经网络在密集向量上训练效果良好，而嵌入是将类别特征值转换为密集向量的最有效方式。TPU v4 包含第三代
    SparseCores，这些数据流处理器加速了依赖于嵌入的机器学习模型。
- en: For example, the embedding function can translate a word in English, which would
    be considered a large categorical space into a smaller dense space of a 100-vector
    representation of each word. Embedding is a key element to Deep Learning Recommendation
    Models (DLRMs), which are part of our everyday lives and are used in advertising,
    search ranking, YouTube, and more.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，嵌入函数可以将英文单词翻译为较小的密集空间，每个单词表示为 100 维向量。嵌入是深度学习推荐模型（DLRMs）的关键元素，它们存在于我们日常生活中，用于广告、搜索排名、YouTube
    等。
- en: The image below shows the performance of recommendation models on CPUs, TPU
    v3, TPU v4 (using SparseCore), and TPU v4 with embeddings in CPU memory (not using
    SparseCore). As you can see the TPU v4 SparseCore is 3X faster than TPU v3 on
    recommendation models, and 5–30X faster than systems using CPUs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了在 CPU、TPU v3、TPU v4（使用 SparseCore）和 TPU v4（在 CPU 内存中使用嵌入，不使用 SparseCore）上的推荐模型性能。可以看到，TPU
    v4 SparseCore 在推荐模型上比 TPU v3 快 3 倍，比使用 CPU 的系统快 5 到 30 倍。
- en: '![Introducing TPU v4: Googles Cutting Edge Supercomputer for Large Language
    Models](../Images/70bd04dcc3d9db52513a04d61349bb72.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![介绍 TPU v4：Google 的前沿超级计算机，用于大型语言模型](../Images/70bd04dcc3d9db52513a04d61349bb72.png)'
- en: Image by [Google](https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Google](https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains)
    提供
- en: Performance
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能
- en: TPU v4 outperforms TPU v3 by 2.1x and has an improved performance/Watt by 2.7x.
    TPU v4 is 4x larger at 4096 chips, making it 10x faster. The implementation and
    flexibility of OCS are also major help for large language models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: TPU v4 的性能比 TPU v3 高 2.1 倍，性能/瓦特比提高了 2.7 倍。TPU v4 体积增大了 4 倍，达到 4096 个芯片，使其速度提高了
    10 倍。OCS 的实现和灵活性也大大助力于大型语言模型。
- en: The performance and availability of TPU v4 supercomputers are being heavily
    considered to improve large language models such as LaMDA, MUM, and PaLM. PaLM,
    the 540B-parameter model was trained on TPU v4 for over 50 days and had a remarkable
    57.8% hardware floating point performance.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: TPU v4 超级计算机的性能和可用性正被广泛考虑用于改进大型语言模型，如 LaMDA、MUM 和 PaLM。540B 参数的 PaLM 模型在 TPU
    v4 上训练了超过 50 天，硬件浮点性能达到了令人惊叹的 57.8%。
- en: TPU v4 also has multidimensional model-partitioning techniques that enable low-latency,
    high-throughput inference for large language models.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: TPU v4 还具有多维模型分区技术，使大型语言模型的低延迟、高吞吐量推理成为可能。
- en: Energy Efficiency
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 能效
- en: With more laws and regulations being put in place for companies globally to
    do better to improve their overall energy efficiency, TPU v4 is doing a decent
    job. TPU v4s inside Google Cloud use ~2-6x less energy and produce ~20x less CO2e
    than contemporary DSAs in typical on-premise data centres.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 随着全球公司为提升整体能效而制定更多法律法规，TPU v4 表现不俗。Google Cloud 中的 TPU v4 使用的能量比当代 DSA 少 ~2-6
    倍，CO2e 排放量比典型的本地数据中心少 ~20 倍。
- en: Machine Learning Workloads Changes
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习工作负载的变化
- en: So now you know a bit more about TPU v4, you might be wondering how fast machine
    learning workloads actually change with TPU v4.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你对 TPU v4 了解更多了，你可能想知道机器学习工作负载在 TPU v4 上实际变化有多快。
- en: The below table shows the workload by deep neural network model type and the
    % TPUs used. Over 90% of training at Google is on TPUs, and this table shows the
    fast change in production workloads at Google.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了按深度神经网络模型类型的工作负载和使用的 TPU 百分比。Google 90%以上的训练在 TPU 上进行，这张表显示了 Google 生产工作负载的快速变化。
- en: There is a drop in recurrent neural networks (RNN), as this is because RNNs
    process the input all at once rather than sequentially, in comparison to transforms
    which are known for natural language translation and text summarization.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络（RNN）的应用有所下降，因为 RNN 是一次性处理输入，而与之相比，变换模型在自然语言翻译和文本摘要方面更为出色。
- en: '![Introducing TPU v4: Googles Cutting Edge Supercomputer for Large Language
    Models](../Images/22ec13f5b8689ecfcfc1bbf86ee8bf38.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![介绍 TPU v4: 谷歌前沿超级计算机用于大型语言模型](../Images/22ec13f5b8689ecfcfc1bbf86ee8bf38.png)'
- en: 'To learn more about TPU v4 capabilities, read the research paper [TPU v4: An
    Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support
    for Embeddings](https://arxiv.org/pdf/2304.01433.pdf).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '要了解更多关于 TPU v4 能力的信息，请阅读研究论文 [TPU v4: 一种用于机器学习的光学可重构超级计算机，支持嵌入的硬件](https://arxiv.org/pdf/2304.01433.pdf)。'
- en: Wrapping it up
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Last year, TPU v4 supercomputers were available to AI researchers and developers
    at Google Cloud’s ML cluster in Oklahoma. The author of this [paper](https://arxiv.org/pdf/2304.01433.pdf)
    claims that the TPU v4 is faster and uses less power than Nvidia A100\. However,
    they have not been able to compare the TPU v4 to the newer Nvidia H100 GPUs due
    to their limited availability and its 4nm architecture, whereas TPU v4 has a 7nm
    architecture.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 去年，TPU v4 超级计算机在谷歌云的机器学习集群中向 AI 研究人员和开发者开放。该[论文](https://arxiv.org/pdf/2304.01433.pdf)的作者声称
    TPU v4 比 Nvidia A100 更快且功耗更低。然而，由于 Nvidia H100 GPU 的有限供应和其 4nm 架构，他们尚未能将 TPU v4
    与更新的 Nvidia H100 GPU 进行比较，而 TPU v4 采用的是 7nm 架构。
- en: What do you think TPU v4 is capable of, its limitations, and is it better than
    Nvidia A100 GPU?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为 TPU v4 的能力是什么，它的局限性如何，它是否比 Nvidia A100 GPU 更好？
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist,
    Freelance Technical Writer and Community Manager at KDnuggets. She is particularly
    interested in providing Data Science career advice or tutorials and theory based
    knowledge around Data Science. She also wishes to explore the different ways Artificial
    Intelligence is/can benefit the longevity of human life. A keen learner, seeking
    to broaden her tech knowledge and writing skills, whilst helping guide others.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**[尼莎·阿亚](https://www.linkedin.com/in/nisha-arya-ahmed/)** 是一名数据科学家、自由职业技术写作者及
    KDnuggets 的社区经理。她特别关注于提供数据科学职业建议或教程，以及围绕数据科学的理论知识。她还希望探索人工智能如何在延长人类寿命方面发挥作用。作为一个热衷学习者，她寻求拓宽技术知识和写作技能，同时帮助指导他人。'
- en: More On This Topic
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解更多相关话题
- en: '[Introducing Healthcare-Specific Large Language Models from John Snow Labs](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍 John Snow Labs 的医疗保健专用大型语言模型](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
- en: '[Cutting Down Implementation Time by Integrating Jupyter and KNIME](https://www.kdnuggets.com/2021/12/cutting-implementation-time-integrating-jupyter-knime.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过整合 Jupyter 和 KNIME 缩短实施时间](https://www.kdnuggets.com/2021/12/cutting-implementation-time-integrating-jupyter-knime.html)'
- en: '[Top Open Source Large Language Models](https://www.kdnuggets.com/2022/09/john-snow-top-open-source-large-language-models.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[顶级开源大型语言模型](https://www.kdnuggets.com/2022/09/john-snow-top-open-source-large-language-models.html)'
- en: '[Learn About Large Language Models](https://www.kdnuggets.com/2023/03/learn-large-language-models.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[了解大型语言模型](https://www.kdnuggets.com/2023/03/learn-large-language-models.html)'
- en: '[What are Large Language Models and How Do They Work?](https://www.kdnuggets.com/2023/05/large-language-models-work.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是大型语言模型，它们是如何工作的？](https://www.kdnuggets.com/2023/05/large-language-models-work.html)'
- en: '[AI: Large Language & Visual Models](https://www.kdnuggets.com/2023/06/ai-large-language-visual-models.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AI: 大型语言和视觉模型](https://www.kdnuggets.com/2023/06/ai-large-language-visual-models.html)'
