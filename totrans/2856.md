# 停止为高风险决策解释黑箱机器学习模型，改用可解释的模型

> 原文：[https://www.kdnuggets.com/2019/11/stop-explaining-black-box-models.html](https://www.kdnuggets.com/2019/11/stop-explaining-black-box-models.html)

[评论](#comments)

[停止为高风险决策解释黑箱机器学习模型，改用可解释的模型](https://arxiv.org/abs/1811.10154) 由Rudin等人，*arXiv 2019*

感谢Glyn Normington指出了这篇论文。

从标题中可以很清楚地看出辛西娅·鲁丁希望我们做什么！这篇论文混合了技术性和哲学性的论点，对我来说有两个主要的收获：首先，加深了我对可解释性与解释性的区别的理解，以及为什么前者可能存在问题；其次，提供了一些很好的方法来创建真正可解释的模型。

> 在医疗保健和刑事司法领域，越来越多地利用机器学习（ML）进行高风险预测应用，这对人类生活产生深远影响……预测模型的透明性和问责制的缺乏可能会（并且已经）带来严重后果……

### 定义术语

模型可能成为**黑箱**有两个原因：（a）模型计算的函数过于复杂，以至于任何人都无法理解，或者（b）模型实际上可能很简单，但其细节是专有的，无法进行检查。

在**可解释的机器学习**中，我们使用一个复杂的黑箱模型（例如，DNN）进行预测，并使用第二个（事后）模型来解释第一个模型的行为。一个经典的例子是[LIME](https://blog.acolyer.org/2016/09/22/why-should-i-trust-you-explaining-the-predictions-of-any-classifier/)，它探讨了复杂模型的局部区域以揭示决策边界。

**可解释的模型**是一个用于预测的模型，可以被人类专家直接检查和解释。

> 可解释性是一个特定领域的概念，因此没有一个通用的定义。然而，通常来说，可解释的机器学习模型是**在模型形式上受限的**，以便它要么对某人有用，要么遵循领域的结构性知识，如单调性，或来自领域知识的物理约束。

### 解释并没有真正解释

目前已经有大量研究致力于为黑箱模型的输出生成解释。鲁丁认为这种方法从根本上是有缺陷的。她论点的根源在于观察到临时解释实际上只是“猜测”（我选择的词）黑箱模型在做什么：

> 解释一定是错误的。它们不能与原始模型完美一致。如果解释完全忠实于原始模型的计算，解释将等同于原始模型，那么我们根本不需要原始模型，只需要解释。

甚至“解释”这个词也是有问题的，因为我们并没有真正描述原始模型实际做了什么。COMPAS（替代制裁的刑事犯罪管理分析）这一例子生动地体现了这种区别。ProPublica 创建的一个线性解释模型与种族相关，被用来指责 COMPAS（这是一个黑箱）依赖于种族。但我们不知道 COMPAS 是否将种族作为一个特征（尽管它可能确实有相关的变量）。

> 让我们停止称对黑箱模型预测的近似为**解释**。对于一个没有明确使用种族的模型，自动生成的解释“这个模型预测你会因为是黑人而被逮捕”并不能反映模型实际在做什么，这会让法官、律师或被告感到困惑。

在图像空间中，显著性图展示了网络关注的区域，但即使是这些图也无法告诉我们它真正关注的内容。不同类别的显著性图可能非常相似。在下面的例子中，模型认为图像是哈士奇的显著性“解释”和认为图像是长笛的显著性“解释”看起来非常相似！

![](../Images/e66f228dd34d0b1544bfcb140b069b74.png)

由于解释并没有真正解释，识别和排查黑箱模型的问题可能非常困难

### 针对可解释模型的争论

鉴于黑箱模型和解释的问题，为什么黑箱模型如此流行？虽然很难对深度学习模型的巨大近期成功提出异议，但我们不应从中得出更复杂的模型总是更好的结论。

> 有一种广泛的观点认为更复杂的模型更为准确，这意味着为了获得顶级的预测性能，需要一个复杂的黑箱。然而，这通常并不成立，特别是当数据是结构化的，并且在自然有意义的特征方面有很好的表示时。

由于相信复杂的东西更好，也有一种普遍的误解，即如果你想要好的性能，你必须牺牲可解释性：

![](../Images/d21235ffd41cfe8d022b1c6d55b7c07d.png)

> 认为准确性和可解释性之间总是存在权衡的信念使许多研究人员放弃了**尝试**产生可解释模型。这个问题由于研究人员现在受过深度学习训练，但没有受过可解释机器学习训练而变得更加复杂…

*Rashomon 集* 说，如果我们尝试的话，我们通常有可能找到一个可解释的模型：鉴于数据允许存在一大组合理准确的预测模型，它通常至少包含一个可解释的模型。

这使我想到一个有趣的方法：首先进行相对较快的尝试，即使用深度学习方法而不进行任何特征工程等。如果产生了合理的结果，我们就知道数据允许存在合理准确的预测模型，我们可以投入时间去寻找一个*可解释的模型*。

> 对于那些没有混杂、完整且干净的数据，使用黑箱机器学习方法要比解决计算难题要容易得多。然而，对于高风险决策来说，分析师时间和计算时间的成本比存在一个有缺陷或过于复杂的模型的成本要低。

### 创建可解释的模型

论文的第5节讨论了在寻找可解释机器学习模型时常遇到的三个常见挑战：构建最佳逻辑模型、构建最佳（稀疏）评分系统，以及在特定领域中定义可解释性可能意味着什么。

**逻辑模型**

逻辑模型只是一些if-then-else语句！这些语句已经手工制作了很长时间。理想的逻辑模型应具有给定准确性水平下最少的分支数。[CORELS](https://corels.eecs.harvard.edu/index.html) 是一个旨在寻找这种最佳逻辑模型的机器学习系统。以下是一个输出模型的示例，它在来自佛罗里达州布劳沃德县的数据上具有与黑箱COMPAS模型类似的准确性：

![](../Images/fa82ab8b6e0fc0e660f3f07f7e1e33c0.png)

请注意，图例称其为“机器学习模型”。我认为这个术语不太准确。它是一个机器*学习*模型，而CORELS是一个生成它的机器学习模型，但IF-THEN-ELSE语句本身不是一个机器学习模型。不过，CORELS看起来很有趣，我们将在下一期《晨报》中对其进行深入探讨。

**评分系统**

评分系统在医学中被广泛使用。我们对作为机器学习模型输出的最佳评分系统感兴趣，但*看起来像是由人类产生的*。例如：

![](../Images/bf08ff918280541f56732be46d22cb9c.png)

这个模型实际上是由 [RiskSLIM](https://www.kdd.org/kdd2017/papers/view/optimized-risk-scores) 产生的，即风险超稀疏线性整数模型算法（我们将在本周晚些时候对此进行更深入的探讨）。

对于CORELS和RiskSLIM模型，关键在于尽管它们看起来简单且高度可解释，但它们的结果具有高度竞争的准确性。让这些东西看起来如此简单并不容易！我当然知道如果有选择的话，我更愿意部署和故障排除哪些模型。

**在特定领域设计可解释性**

> …即使是在经典的机器学习领域，其中数据的潜在表示需要被构建，也可能存在与黑箱模型一样准确的可解释模型。

关键是要在模型设计本身中考虑可解释性。例如，如果一个专家要解释为什么他们以某种方式对图像进行分类，他们可能会指出图像中的不同部分，这些部分在他们的推理过程中是重要的（有点像显著性），并*解释原因*。将这一理念引入网络设计，[Chen, Li 等人](https://arxiv.org/abs/1806.10574) 构建了一个模型，在训练过程中学习作为类别原型的图像部分，然后在测试过程中寻找与已学到的原型相似的测试图像部分。

> 这些解释是模型的实际计算过程，而不是事后解释。该网络被称为“这看起来像那”，因为它的推理过程考虑了图像的“这”部分是否像“那”原型。

![](../Images/db78028091ac65304cab1c502dba973b.png)

### 解释、解释性和政策

论文第4节讨论了鼓励优先使用（或甚至在高风险情况下要求使用）可解释模型的潜在政策变化。

> 让我们考虑一个可能的要求，即对于某些高风险决策，**如果存在一个性能水平相同的可解释模型，则不应部署黑箱模型**。

这听起来是一个值得追求的目标，但按这种措辞，很难证明不存在可解释模型。因此，或许公司需要被要求能够提供证据，证明他们已经进行了适当级别的尽职调查来寻找可解释模型……

> 考虑第二个提议，它比上述提议要弱一些，但可能会有类似的效果。我们考虑一下这样的可能性：引入黑箱模型的组织应该被要求报告可解释建模方法的准确性。

如果按照这个过程进行，我们很可能会看到实际应用中部署的黑箱机器学习模型大大减少，如果作者的经验有参考价值的话：

> 可能存在一些应用领域，对于高风险决策需要完全的黑箱模型。尽管我在医疗保健、刑事司法、能源可靠性和金融风险评估等多个领域工作过，但至今尚未遇到这样的应用。

### 最后一段

> 如果这篇评论能够稍微转移一下大多数可解释机器学习工作的基本假设——即黑箱模型对于准确预测是必需的——我们将认为这份文件是成功的……如果我们没有成功[让政策制定者意识到当前可解释机器学习的挑战](https://www.kdnuggets.com/google-data-analytics)，黑箱模型可能会继续被允许使用，而这些模型在不安全的情况下仍然被使用。

[原文](https://blog.acolyer.org/2019/10/28/interpretable-models/)。已获许可转载。

**相关：**

+   [选择机器学习模型](/2019/10/choosing-machine-learning-model.html)

+   [“请解释。”机器学习模型的可解释性](/2019/05/interpretability-machine-learning-models.html)

+   [可解释机器学习的Python库](/2019/09/python-libraries-interpretable-machine-learning.html)

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织IT

* * *

### 更多相关话题

+   [停止学习数据科学来寻找目标，找到目标后…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)

+   [停止在数据科学项目中硬编码 - 改用配置文件](https://www.kdnuggets.com/2023/06/stop-hard-coding-data-science-project-config-files-instead.html)

+   [每个数据科学家都应该知道的三大R库（即使你使用Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)

+   [学习数据科学统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)

+   [90亿美元的AI失败，详细审查](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)

+   [成功的数据科学家的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)
