- en: Decision Trees vs Random Forests, Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树与随机森林的解释
- en: 原文：[https://www.kdnuggets.com/2022/08/decision-trees-random-forests-explained.html](https://www.kdnuggets.com/2022/08/decision-trees-random-forests-explained.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/08/decision-trees-random-forests-explained.html](https://www.kdnuggets.com/2022/08/decision-trees-random-forests-explained.html)
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行IT支持'
- en: '* * *'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Decision trees and random forests are two of the most popular predictive models
    for supervised learning. These models can be used for both classification and
    regression problems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树和随机森林是两种最受欢迎的监督学习预测模型。这些模型可用于分类和回归问题。
- en: 'In this article, I will explain the difference between decision trees and random
    forests. By the end of the article, you should be familiar with the following
    concepts:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将解释决策树和随机森林之间的区别。到文章结束时，你应该熟悉以下概念：
- en: How does the decision tree algorithm work?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树算法是如何工作的？
- en: Components of a decision tree
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树的组成部分
- en: Pros and cons of the decision tree algorithm?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树算法的优缺点是什么？
- en: What does bagging mean, and how does the random forest algorithm work?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是袋装（bagging），随机森林算法是如何工作的？
- en: Which algorithm is better in terms of speed and performance
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪种算法在速度和性能方面更好？
- en: Decision Trees
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: Decision trees are highly interpretable machine learning models that allow us
    to stratify or segment data. They allow us to continuously split data based on
    specific parameters until a final decision is made.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是高度可解释的机器学习模型，允许我们对数据进行分层或分段。它们允许我们根据特定参数不断拆分数据，直到做出最终决策。
- en: How does the decision tree algorithm work?
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树算法是如何工作的？
- en: 'Take a look at the following table:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下下面的表格：
- en: '![Decision Trees vs Random Forests, Explained](../Images/30839068c44d7adef3ee93bb66180fcf.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![决策树与随机森林的解释](../Images/30839068c44d7adef3ee93bb66180fcf.png)'
- en: This dataset consists of only four variables?—?“Day”, ‘‘Temperature’’, ‘‘Wind’’,
    and ‘‘Play?’’. Depending on the temperature and wind on any given day, the outcome
    is binary - either to go out and play or stay home.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集仅包含四个变量——“Day”、“Temperature”、“Wind”和“Play？”。根据任意一天的温度和风速，结果是二元的——要么出去玩，要么待在家里。
- en: 'Let’s build a decision tree using this data:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这些数据来构建一个决策树：
- en: '![Decision Trees vs Random Forests, Explained](../Images/3d32f94954dd9e6333a894782b060c9a.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![决策树与随机森林的解释](../Images/3d32f94954dd9e6333a894782b060c9a.png)'
- en: The example above is one that is simple, but encapsulates exactly how a decision
    tree splits on different data points until an outcome is obtained.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的示例虽然简单，但准确地概括了决策树如何在不同的数据点上拆分，直到获得结果。
- en: From the visualization above, notice that the decision tree splits on the variable
    “Temperature” first. It also stops splitting when the temperature is extreme,
    and says that we should not go out to play.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的可视化中可以看出，决策树首先在“Temperature”变量上进行拆分。当温度极端时，它也会停止拆分，并表示我们不应该出去玩。
- en: It is only if the temperature is mild that it starts to split on the second
    variable.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当温度适中时，它才会开始对第二个变量进行拆分。
- en: 'These observations lead to the following questions:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些观察结果引出了以下问题：
- en: How does a decision tree decide on the first variable to split on?
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决策树如何决定第一个要拆分的变量？
- en: How to decide when to stop splitting?
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何决定何时停止拆分？
- en: What is the order used to construct a decision tree?
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建决策树时使用的顺序是什么？
- en: Entropy
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 熵
- en: Entropy is a metric that measures the impurity of a split in a decision tree.
    It determines how the decision tree chooses to partition data. Entropy values
    range from 0 to 1\. A value of 0 indicates a pure split and a value of 1 indicates
    an impure split.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 熵是一个衡量决策树中分裂不纯度的指标。它决定了决策树如何选择划分数据。熵值范围从0到1。0的值表示纯净的分裂，1的值表示不纯的分裂。
- en: 'In the decision tree above, recall the tree had stopped splitting when the
    temperature was extreme:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的决策树中，回顾一下树在温度极端时已经停止分裂：
- en: '![Decision Trees vs Random Forests, Explained](../Images/b47e25071ea03307a5e5fcacbf4525fd.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![决策树与随机森林的解释](../Images/b47e25071ea03307a5e5fcacbf4525fd.png)'
- en: This is because when the temperature is extreme, the outcome of “Play?” is always
    “No.” This means that we have a pure split with 100% of data points in a single
    class. The entropy value of this split is 0, and the decision tree will stop splitting
    on this node.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为当温度极端时，“是否玩？”的结果总是“否”。这意味着我们有一个纯净的分裂，100%的数据点属于一个单一类别。这个分裂的熵值为0，决策树将在这个节点停止分裂。
- en: Decision trees will always select the feature with the **lowest entropy** to
    be the first node.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树总是选择**最低熵**的特征作为第一个节点。
- en: In this case, since the variable “Temperature” had a lower entropy value than
    “Wind”, this was the first split of the tree.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，由于“温度”变量的熵值低于“风速”，因此这是树的第一个分裂。
- en: Watch [this](https://www.youtube.com/watch?v=1IQOtJ4NI_0) YouTube video to learn
    more about how entropy is calculated and how it is used in decision trees.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 观看[这个](https://www.youtube.com/watch?v=1IQOtJ4NI_0)YouTube视频，了解更多关于如何计算熵及其在决策树中使用的内容。
- en: Information Gain
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信息增益
- en: Information gain measures the reduction in entropy when building a decision
    tree.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益测量构建决策树时熵的减少量。
- en: Decision trees can be constructed in a number of different ways. The tree needs
    to find a feature to split on first, second, third, etc. Information gain is a
    metric that tells us the best possible tree that can be constructed to minimize
    entropy.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可以通过多种不同的方式构建。树需要首先找到一个特征进行分裂，然后是第二个、第三个等。信息增益是一个指标，它告诉我们构建能够最小化熵的最佳决策树。
- en: The best tree is one with the highest information gain.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的树是信息增益最高的树。
- en: If you’d like to learn more about how to calculate information gain and use
    it to build the best possible decision tree, you can watch [this](https://www.youtube.com/watch?v=FuTRucXB9rA)
    YouTube video.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于如何计算信息增益并利用它来构建最佳决策树的内容，你可以观看[这个](https://www.youtube.com/watch?v=FuTRucXB9rA)YouTube视频。
- en: 'Components of a decision tree:'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树的组成部分：
- en: '![Decision Trees vs Random Forests, Explained](../Images/97538c4659bdec844d102b9fc636cc74.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![决策树与随机森林的解释](../Images/97538c4659bdec844d102b9fc636cc74.png)'
- en: 'Root node: A root node is at the top of the decision tree, and is the variable
    from which the dataset starts dividing. The root node is the feature that provides
    us with the best split of data.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根节点：根节点位于决策树的顶部，是数据集开始划分的变量。根节点是为我们提供最佳数据分裂的特征。
- en: 'Internal nodes: These are the nodes that split the data after the root node.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内部节点：这些是根节点之后分裂数据的节点。
- en: 'Leaf nodes: Finally, these are nodes at the bottom of the decision tree after
    which no further splits are possible.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 叶节点：这些是决策树底部的节点，之后没有进一步的分裂可能。
- en: 'Branches: Branches connect one node to another and are used to represent the
    outcome of a test.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分支：分支连接一个节点到另一个节点，并用于表示测试的结果。
- en: 'Pros and Cons of the Decision Tree Algorithm:'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树算法的优缺点：
- en: Now that you understand how decision trees work, let’s take a look at some advantages
    and disadvantages of the algorithm.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你了解了决策树的工作原理，让我们来看看该算法的一些优缺点。
- en: Pros
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优点
- en: Decision trees are simple and easy to interpret.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树简单且易于解释。
- en: They can be used for classification and regression problems.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以用于分类和回归问题。
- en: They can partition data that isn’t linearly separable.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以对非线性可分的数据进行划分。
- en: Cons
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺点
- en: Decision trees are prone to overfitting.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树容易出现过拟合。
- en: Even a small change in the training dataset can make a huge difference in the
    logic of decision trees.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使训练数据集的一个小变化也会对决策树的逻辑产生巨大影响。
- en: Random Forests
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: One of the biggest drawbacks of the decision tree algorithm is that it is prone
    to overfitting. This means that the model is overly complex and has **high variance.**
    A model like this will have high training accuracy but will not generalize well
    to other datasets.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法的一个最大缺点是容易过拟合。这意味着模型过于复杂，并且具有**高方差**。这样的模型会有很高的训练准确率，但对其他数据集的泛化能力较差。
- en: How does the random forest algorithm work?
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林算法如何工作？
- en: The random forest algorithm solves the above challenge by combining the predictions
    made by multiple decision trees and returning a single output. This is done using
    an extension of a technique called **bagging**, or **bootstrap aggregation**.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林算法通过结合多个决策树的预测结果并返回一个单一的输出，解决了上述挑战。这是通过扩展一种称为**bagging**或**自助聚合**的技术来实现的。
- en: Bagging is a procedure that is applied to reduce the variance of machine learning
    models. It works by averaging a set of observations to reduce variance.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging 是一种用于减少机器学习模型方差的过程。它通过对一组观察结果取平均来减少方差。
- en: 'Here is how bagging works:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 bagging 的工作原理：
- en: Bootstrap
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自助法
- en: If we had more than one training dataset, we could train multiple decision trees
    on each dataset and average the results.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有多个训练数据集，我们可以在每个数据集上训练多棵决策树，并对结果取平均。
- en: However, since we usually only have one training dataset in most real-world
    scenarios, a statistical technique called **bootstrap** is used to sample the
    dataset with replacement.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于在大多数实际场景中我们通常只有一个训练数据集，因此使用了一种称为**自助法**的统计技术来对数据集进行有放回的抽样。
- en: 'Then, multiple decision trees are created, and each tree is trained on a different
    data sample:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，创建多棵决策树，每棵树在不同的数据样本上进行训练：
- en: '![Decision Trees vs Random Forests, Explained](../Images/586e5360990898829b981920a41c3645.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![决策树与随机森林，解释](../Images/586e5360990898829b981920a41c3645.png)'
- en: Notice that three bootstrap samples have been created from the training dataset
    above. The random forest algorithm goes a step further than bagging and randomly
    samples features so only a subset of variables are used to build each tree.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，以上的训练数据集创建了三个 bootstrap 样本。随机森林算法在 bagging 的基础上进一步随机抽取特征，因此每棵树只使用部分变量进行构建。
- en: Each decision tree will render different predictions based on the data sample
    they were trained on.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 每棵决策树根据其训练的数据样本会产生不同的预测结果。
- en: Aggregation
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚合
- en: In this step, the prediction of each decision tree will be combined to come
    up with a single output.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步骤中，将结合每棵决策树的预测结果，得出一个单一的输出。
- en: 'In the case of a classification problem, a majority class prediction is made:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中，会做出一个多数类预测：
- en: '![Decision Trees vs Random Forests, Explained](../Images/6a2800f970f006e6206a7a4e57de325f.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![决策树与随机森林，解释](../Images/6a2800f970f006e6206a7a4e57de325f.png)'
- en: In regression problems, the predictions of all decision trees will be averaged
    to come up with a single value.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归问题中，所有决策树的预测结果会被平均，得出一个单一值。
- en: Why do we randomly sample variables in the random forest algorithm?
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么在随机森林算法中我们要随机抽取变量？
- en: In the random forest algorithm, it is not only rows that are randomly sampled,
    but variables too.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机森林算法中，不仅行会被随机抽样，变量也会被随机抽样。
- en: This is because if we were to build multiple decision trees with the same features,
    every tree will be similar and highly correlated with each other, potentially
    yielding the same result. This will again lead to the issue of high variance.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为如果我们用相同的特征构建多棵决策树，每棵树都会相似并高度相关，可能会产生相同的结果。这会导致高方差的问题。
- en: Decision Trees vs. Random Forests - Which One Is Better and Why?
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树与随机森林 - 哪个更好，为什么？
- en: 'Random forests typically perform better than decision trees due to the following
    reasons:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林通常比决策树表现更好，原因如下：
- en: Random forests solve the problem of overfitting because they combine the output
    of multiple decision trees to come up with a final prediction.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林解决了过拟合问题，因为它们将多个决策树的输出结合起来，以得出最终预测结果。
- en: When you build a decision tree, a small change in data leads to a huge difference
    in the model’s prediction. With a random forest, this problem does not arise since
    the data is sampled many times before generating a prediction.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你构建一棵决策树时，数据的微小变化会导致模型预测的巨大差异。使用随机森林，这个问题不会出现，因为数据在生成预测之前会被多次抽样。
- en: In terms of speed, however, the random forests are slower since more time is
    taken to construct multiple decision trees. Adding more trees to a random forest
    model will improve its accuracy to a certain extent, but also increases computation
    time.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，就速度而言，随机森林较慢，因为需要更多时间来构建多个决策树。将更多树添加到随机森林模型中会在一定程度上提高其准确性，但也会增加计算时间。
- en: Finally, decision trees are also easier to interpret than random forests since
    they are straightforward. It is easy to visualize a decision tree and understand
    how the algorithm reached its outcome. A random forest is harder to deconstruct
    since it is more complex and combines the output of multiple decision trees to
    make a prediction.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，决策树相比于随机森林更易于解释，因为它们结构简单。可以轻松地可视化决策树并理解算法如何得出结果。随机森林更难以拆解，因为它更加复杂，并结合了多个决策树的输出进行预测。
- en: '**[Natassha Selvaraj](https://www.natasshaselvaraj.com/)** is a self-taught
    data scientist with a passion for writing. You can connect with her on [LinkedIn](https://www.linkedin.com/in/natassha-selvaraj-33430717a/).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Natassha Selvaraj](https://www.natasshaselvaraj.com/)** 是一位自学成才的数据科学家，对写作充满热情。你可以通过[LinkedIn](https://www.linkedin.com/in/natassha-selvaraj-33430717a/)与她联系。'
- en: More On This Topic
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解更多此主题
- en: '[Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集成学习技术：Python中随机森林的操作指南](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
- en: '[Machine Learning from Scratch: Decision Trees](https://www.kdnuggets.com/2022/11/machine-learning-scratch-decision-trees.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从零开始的机器学习：决策树](https://www.kdnuggets.com/2022/11/machine-learning-scratch-decision-trees.html)'
- en: '[Generalized and Scalable Optimal Sparse Decision Trees(GOSDT)](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通用且可扩展的最优稀疏决策树(GOSDT)](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
- en: '[Demystifying Decision Trees for the Real World](https://www.kdnuggets.com/demystifying-decision-trees-for-the-real-world)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[揭开决策树在现实世界中的神秘面纱](https://www.kdnuggets.com/demystifying-decision-trees-for-the-real-world)'
- en: '[Random Forest vs Decision Tree: Key Differences](https://www.kdnuggets.com/2022/02/random-forest-decision-tree-key-differences.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[随机森林与决策树：关键区别](https://www.kdnuggets.com/2022/02/random-forest-decision-tree-key-differences.html)'
- en: '[Decision Tree Algorithm, Explained](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[决策树算法详解](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)'
