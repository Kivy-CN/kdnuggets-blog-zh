["```py\n# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')# data visualisation and manipulationimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\nimport missingno as msno#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)#import the necessary modelling algos.\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score,precision_score\nfrom sklearn.model_selection import GridSearchCV#preprocess.\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\n```", "```py\ntrain=pd.read_csv(\"../RandomForest/voice.csv\")df=train.copy()\n```", "```py\ndf.head(10)\n```", "```py\ndf.shape\n```", "```py\n*# check for null values.*\ndf.isnull().any()\n```", "```py\ndef calc_limits(feature):\n    q1,q3=df[feature].quantile([0.25,0.75])\n    iqr=q3-q1\n    rang=1.5*iqr\n    return(q1-rang,q3+rang)\n\ndef plot(feature):\n    fig,axes=plt.subplots(1,2)\n    sns.boxplot(data=df,x=feature,ax=axes[0])\n    sns.distplot(a=df[feature],ax=axes[1],color='#ff4125')\n    fig.set_size_inches(15,5)\n\n    lower,upper = calc_limits(feature)\n    l=[df[feature] for i in df[feature] if i>lower and i<upper] \n    print(\"Number of data points remaining if outliers removed : \",len(l))\n```", "```py\nplot('meanfreq')\n```", "```py\nsns.countplot(data=df,x='label')\ndf['label'].value_counts()\n```", "```py\ntemp = []\nfor i in df.label:\n    if i == 'male':\n        temp.append(1)\n    else:\n        temp.append(0)\ndf['label'] = temp\n#corelation matrix.\ncor_mat= df[:].corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)] = False\nfig=plt.gcf()\nfig.set_size_inches(23,9)\nsns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)\n```", "```py\ndf.drop('centroid',axis=1,inplace=True)\n```", "```py\n*# removal of any data point which is an outlier for any fetaure.*\nfor col in df.columns:\n    lower,upper=calc_limits(col)\n    df = df[(df[col] >lower) & (df[col]<upper)]df.shape\n```", "```py\ntemp_df=df.copy()temp_df.drop(['skew','kurt','mindom','maxdom'],axis=1,inplace=True) # only one of maxdom and dfrange.\ntemp_df.head(10)\n```", "```py\ntemp_df['meanfreq']=temp_df['meanfreq'].apply(lambda x:x*2)\ntemp_df['median']=temp_df['meanfreq']+temp_df['mode']\ntemp_df['median']=temp_df['median'].apply(lambda x:x/3)sns.boxplot(data=temp_df,y='median',x='label') # seeing the new 'median' against the 'label'\n```", "```py\ntemp_df['pear_skew']=temp_df['meanfreq']-temp_df['mode']\ntemp_df['pear_skew']=temp_df['pear_skew']/temp_df['sd']\ntemp_df.head(10)sns.boxplot(data=temp_df,y='pear_skew',x='label')\n```", "```py\nscaler=StandardScaler()\nscaled_df=scaler.fit_transform(temp_df.drop('label',axis=1))\nX=scaled_df\nY=df['label'].as_matrix()\n```", "```py\nx_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.20,random_state=42)\n```", "```py\nmodels=[RandomForestClassifier(), DecisionTreeClassifier()]model_names=['RandomForestClassifier','DecisionTree']acc=[]\nd={}for model in range(len(models)):\n    clf=models[model]\n    clf.fit(x_train,y_train)\n    pred=clf.predict(x_test)\n    acc.append(accuracy_score(pred,y_test))\n\nd={'Modelling Algo':model_names,'Accuracy':acc}\n```", "```py\nacc_frame=pd.DataFrame(d)\nacc_frame\n```", "```py\nparam_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\nCV_rfc = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid, scoring='accuracy', cv= 5)\nCV_rfc.fit(x_train, y_train)\n```", "```py\nprint(\"Best score : \",CV_rfc.best_score_)\nprint(\"Best Parameters : \",CV_rfc.best_params_)\nprint(\"Precision Score : \", precision_score(CV_rfc.predict(x_test),y_test))\n```", "```py\ndf1 = pd.DataFrame.from_records(x_train)     \ntmp = pd.DataFrame({'Feature': df1.columns, 'Feature importance': clf_rf.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (7,4))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()\n```"]