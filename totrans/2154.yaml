- en: 'Introduction to NExT-GPT: Any-to-Any Multimodal Large Language Model'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/introduction-to-nextgpt-anytoany-multimodal-large-language-model](https://www.kdnuggets.com/introduction-to-nextgpt-anytoany-multimodal-large-language-model)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Introduction to NExT-GPT: Any-to-Any Multimodal Large Language Model](../Images/759e062253b4a9e3c294ac44bcd62830.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Editor
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, Generative AI research has evolved in a way that has changed
    how we work. From developing content, planning our work, and finding answers to
    creating artwork, it’s all possible now with Generative AI. However, each model
    usually works for certain use cases, e.g., GPT for text-to-text, Stable Diffusion
    for text-to-image, and many others.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The model capable of performing multiple tasks is called the multimodal model.
    Much state-of-the-art research is moving in the multimodal direction as it’s proven
    useful in many conditions. This is why one of the exciting research regarding
    multimodal people need to know is the [NExT-GPT](https://arxiv.org/pdf/2309.05519.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: NExT-GPT is a multimodal model that could transform anything into anything.
    So, how does it work? Let’s explore it further.
  prefs: []
  type: TYPE_NORMAL
- en: NExT-GPT Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NExT-GPT is an any-to-any multimodal LLM that can handle four different kinds
    of input and output: text, images, videos, and audio. The research was initiated
    by the research group called [NExT++ of the National University of Singapore](https://www.nextcenter.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: The overall representation of the NExT-GPT model is shown in the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to NExT-GPT: Any-to-Any Multimodal Large Language Model](../Images/5f3b1d0fe21edbaefa094c994bb7be0e.png)'
  prefs: []
  type: TYPE_IMG
- en: NExT-GPT LLM Model ([Wu *et al*. (2023)](https://arxiv.org/pdf/2309.05519.pdf))
  prefs: []
  type: TYPE_NORMAL
- en: 'NExT-GPT model consists of three parts of works:'
  prefs: []
  type: TYPE_NORMAL
- en: Establish encoders for input from various modalities and represent them into
    a language-like input that LLM could accept,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Utilizing the open-source LLM as the core to process the input for both semantic
    understanding and reasoning with additional unique modality signal,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide multimodal signal into different encoders and generate the result to
    the appropriate modalities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An example of the NExT-GPT inferences process can be seen in the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to NExT-GPT: Any-to-Any Multimodal Large Language Model](../Images/36bfc2d94328d91f1f314f6955d82673.png)'
  prefs: []
  type: TYPE_IMG
- en: NExT-GPT inference Process ([Wu *et al*. (2023)](https://arxiv.org/pdf/2309.05519.pdf))
  prefs: []
  type: TYPE_NORMAL
- en: We can see in the image above that depending on the tasks that we want, the
    encoder and decoder would switch to the appropriate modalities. This process can
    only happen because NExT-GPT utilizes a concept called modality-switching instruction
    tuning so the model can conform with the user's intention.
  prefs: []
  type: TYPE_NORMAL
- en: The researchers have tried to experiment with various combinations of modalities.
    Overall, the NExT-GPT performance can be summarized in the graph below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to NExT-GPT: Any-to-Any Multimodal Large Language Model](../Images/ddbf67c258e36190111962e38b0edf0c.png)'
  prefs: []
  type: TYPE_IMG
- en: NExT-GPT Overall Performance Result ([Wu *et al*. (2023)](https://arxiv.org/pdf/2309.05519.pdf))
  prefs: []
  type: TYPE_NORMAL
- en: NExT-GPT's best performance is the Text and Audio input to produce Images, followed
    by the Text, Audio, and Image input to produce Image results. The least performing
    action is the Text and Video input to produce Video output.
  prefs: []
  type: TYPE_NORMAL
- en: An example of the NExT-GPT capability is shown in the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to NExT-GPT: Any-to-Any Multimodal Large Language Model](../Images/635190c69b67ec01e1b628ac9c1d7d4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Text-to-Text+Image+Audio from NExT-GPT (Source: [NExT-GPT web](https://next-gpt.github.io/))'
  prefs: []
  type: TYPE_NORMAL
- en: The result above shows that interacting with the NExT-GPT can produce Audio,
    Text, and Images appropriate to the user's intention. It’s shown that NExT-GPT
    can act quite well and is pretty reliable.
  prefs: []
  type: TYPE_NORMAL
- en: Another example of NExT-GPT is shown in the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to NExT-GPT: Any-to-Any Multimodal Large Language Model](../Images/4b238e3be3fc5a3039d9c824af04e7a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Text+Imaget-to-Text+Audio from NExT-GPT (Source: [NExT-GPT web](https://next-gpt.github.io/))'
  prefs: []
  type: TYPE_NORMAL
- en: The image above shows that NExT-GPT can handle two kinds of modalities to produce
    Text and Audio output. It’s shown how the model is versatile enough.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to try the model, you can set up the model and environment from
    their [GitHub page](https://github.com/NExT-GPT/NExT-GPT). Additionally, you can
    try out the demo on the following [page](https://4c5b3bd137f4eec297.gradio.live/).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NExT-GPT is a multimodal model that accepts input data and produces output in
    text, image, audio, and video. This model works by utilizing a specific encoder
    for the modalities and switching to appropriate modalities according to the user's
    intention. The performance experiment result shows a good result and promising
    work that can be used in many applications.
  prefs: []
  type: TYPE_NORMAL
- en: '**[](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**[Cornellius Yudha
    Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**** is a data science
    assistant manager and data writer. While working full-time at Allianz Indonesia,
    he loves to share Python and data tips via social media and writing media. Cornellius
    writes on a variety of AI and machine learning topics.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Multimodal Grounded Learning with Vision and Language](https://www.kdnuggets.com/2022/11/multimodal-grounded-learning-vision-language.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Multimodal Models Explained](https://www.kdnuggets.com/2023/03/multimodal-models-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Ultimate Open-Source Large Language Model Ecosystem](https://www.kdnuggets.com/2023/05/ultimate-opensource-large-language-model-ecosystem.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Exploring the Zephyr 7B: A Comprehensive Guide to the Latest Large…](https://www.kdnuggets.com/exploring-the-zephyr-7b-a-comprehensive-guide-to-the-latest-large-language-model)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Free Mastery Course: Become a Large Language Model Expert](https://www.kdnuggets.com/ree-mastery-course-become-a-large-language-model-expert)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Steps to Mastering Large Language Model Fine-tuning](https://www.kdnuggets.com/7-steps-to-mastering-large-language-model-fine-tuning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
