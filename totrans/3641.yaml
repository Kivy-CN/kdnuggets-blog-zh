- en: Extractive Summarization with LLM using BERT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/extractive-summarization-with-llm-using-bert](https://www.kdnuggets.com/extractive-summarization-with-llm-using-bert)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Extractive Summarization with LLM using BERT](../Images/c1bd380b5cea926954e185cf7890779f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Editor
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In today's fast-paced world, we're bombarded with more information than we can
    handle. We’re increasingly getting used to receiving more information in less
    time, leading to frustration when having to read extensive documents or books.
    That's where extractive summarization steps in. To the heart of a text, the process
    pulls out key sentences from an article, piece, or page to give us a snapshot
    of its most important points.
  prefs: []
  type: TYPE_NORMAL
- en: For anyone needing to understand big documents without reading every word, this
    is a game changer.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we delve into the fundamentals and applications of extractive
    summarization. We'll examine the role of Large Language Models, especially [BERT](https://www.exxactcorp.com/blog/Deep-Learning/how-do-bert-transformers-work) (Bidirectional
    Encoder Representations from Transformers),  in enhancing the process. The article
    will also include a hands-on tutorial on using BERT for extractive summarization,
    showcasing its practicality in condensing large text volumes into informative
    summaries.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Extractive Summarization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Extractive summarization is a prominent technique in the field of natural language
    processing (NLP) and text analysis. With it, key sentences or phrases are carefully
    selected from the original text and combined to create a concise and informative
    summary. This involves meticulously sifting through text to identify the most
    crucial elements and central ideas or arguments presented in the selected piece.
  prefs: []
  type: TYPE_NORMAL
- en: Where abstractive summarization involves generating entirely new sentences often
    not present in the source material, extractive summarization sticks to the original
    text. It doesn’t alter or paraphrase but instead extracts the sentences exactly
    as they appear, maintaining the original wording and structure. This way the summary
    stays true to the source material's tone and content. The technique of extractive
    summarization is extremely beneficial in instances where the accuracy of the information
    and the preservation of the author's original intent are a priority.
  prefs: []
  type: TYPE_NORMAL
- en: It has many different uses, like summarizing news articles, academic papers,
    or lengthy reports. The process effectively conveys the original content's message
    without potential biases or reinterpretations that can occur with paraphrasing.
  prefs: []
  type: TYPE_NORMAL
- en: How Does Extractive Summarization Use LLMs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 1\. Text Parsing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This initial step involves breaking down the text into its essential elements,
    primarily sentences, and phrases. The goal is to identify the basic units (sentences,
    in this context) the algorithm will later evaluate to include in a summary, like
    dissecting a text to understand its structure and individual components.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the model would analyze a four-sentence paragraph by breaking
    it down into the following four-sentence components.
  prefs: []
  type: TYPE_NORMAL
- en: The Pyramids of Giza, built in ancient Egypt, stood magnificently for millennia.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They were constructed as tombs for pharaohs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Great Pyramids are the most famous.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These structures symbolize architectural intelligence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. Feature Extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this stage, the algorithm analyzes each sentence to identify characteristics
    or 'features' that might indicate what their significance is to the overall text.
    Common features include the frequency and repeated use of keywords and phrases,
    the length of sentences, their position in the text and its implications, and
    the presence of specific keywords or phrases that are central to the text's main
    topic.
  prefs: []
  type: TYPE_NORMAL
- en: Below is an example of how the LLM would do feature extraction for the first
    sentence “The Pyramids of Giza, built in ancient Egypt, stand magnificently for
    millennia.”
  prefs: []
  type: TYPE_NORMAL
- en: '| Attributes | Text |'
  prefs: []
  type: TYPE_TB
- en: '| Frequency | “Pyramids of Giza”, “Ancient Egypt”, “Millennia” |'
  prefs: []
  type: TYPE_TB
- en: '| Sentence Length | Moderate |'
  prefs: []
  type: TYPE_TB
- en: '| Position in Text | Introductory, sets the topic |'
  prefs: []
  type: TYPE_TB
- en: '| Specific Keywords | “Pyramids of Giza”, “ancient Egypt” |'
  prefs: []
  type: TYPE_TB
- en: 3\. Scoring Sentences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each sentence is assigned a score based on its content. This score reflects
    a sentence’s perceived importance in the context of the entire text. Sentences
    that score higher are deemed to carry more weight or relevance.
  prefs: []
  type: TYPE_NORMAL
- en: Simply put, this process rates each sentence for its potential significance
    to a summary of the entire text.
  prefs: []
  type: TYPE_NORMAL
- en: '| Sentence Score | Sentence | Explanation |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | The Pyramids of Giza, built in ancient Egypt, stood magnificently for
    millennia. | It''s the introductory sentence that sets the topic and context,
    containing key terms like “Pyramids of Giza” and “Ancient Egypt. |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | They were constructed as tombs for pharaohs. | This sentence provides
    critical historical information about the Pyramids and may identify them as significant
    for understanding their purpose and significance. |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | The Great Pyramids are the most famous. | Although this sentence adds
    specific information about the Great Pyramids, it is considered less crucial in
    the broader context of summarizing the overall significance of the Pyramids. |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | These structures symbolize architectural intelligence. | This summarizing
    statement captures the overarching significance of the Pyramids. |'
  prefs: []
  type: TYPE_TB
- en: 4\. Selection and Aggregation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final phase involves selecting the highest-scoring sentences and compiling
    them into a summary. When carefully done, this ensures the summary remains coherent
    and an aggregately representative of the main ideas and themes of the original
    text.
  prefs: []
  type: TYPE_NORMAL
- en: To create an effective summary, the algorithm must balance the need to include
    important sentences that are concise, avoid redundancy, and ensure that the selected
    sentences provide a clear and comprehensive overview of the entire original text.
  prefs: []
  type: TYPE_NORMAL
- en: The Pyramids of Giza, built in Ancient Egypt, stood magnificently for millennia.
    They were constructed as tombs for pharaohs. These structures symbolize architectural
    brilliance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This example is extremely basic, extracting 3 of the total 4 sentences for the
    best overall summarization. Reading an extra sentence doesn't hurt, but what happens
    when the text is longer? Let's say, 3 paragraphs?
  prefs: []
  type: TYPE_NORMAL
- en: How to Run Extractive Summarization with BERT LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Step 1: Installing and Importing Necessary Packages'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be leveraging the pre-trained BERT model. However, we won't be using
    just any BERT model; instead, we'll focus on the BERT Extractive Summarizer. This
    particular model has been finely tuned for specialized tasks in extractive summarization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: Introduce Summarizer Function'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Summarizer() function imported from the summarizer in Python is an extractive
    text summarization tool. It uses the BERT model to analyze and extract key sentences
    from a larger text. This function aims to retain the most important information,
    providing a condensed version of the original content. It's commonly used to summarize
    lengthy documents efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: Importing our Text'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we will import any piece of text that we would like to test our model
    on. To test our extractive summary model we generated text using ChatGPT 3.5 with
    the prompt: “Provide a 3-paragraph summary of the history of GPUs and how they
    are used today.”'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 4: Performing Extractive Summarization'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, we''ll execute our summarization function. This function requires
    two inputs: the text to be summarized and the desired number of sentences for
    the summary. After processing, it will generate an extractive summary, which we
    will then display.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Extractive Summary Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The history of Graphics Processing Units (GPUs) dates back to the early 1980s
    when companies like IBM and Texas Instruments developed specialized graphics accelerators
    for rendering images and improving overall graphical performance. NVIDIA''s GeForce
    256, released in 1999, is often considered the first GPU, as it integrated both
    2D and 3D acceleration on a single chip. Today, GPUs have evolved far beyond their
    original graphics-centric purpose, now widely used for parallel processing tasks
    in various fields, such as scientific simulations, artificial intelligence, and
    machine learning. As technology progresses, GPUs are expected to play an even
    more critical role in shaping the future of computing.*'
  prefs: []
  type: TYPE_NORMAL
- en: Our model pulled the 4 most important sentences from our large corpus of text
    to generate this summary!
  prefs: []
  type: TYPE_NORMAL
- en: Challenges of Extractive Summarization Using LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Contextual Understanding Limitations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While LLMs are proficient in processing and generating language, their understanding
    of context, especially in longer texts, is limited. LLMs can miss subtle nuances
    or fail to recognize critical aspects of the text leading to less accurate or
    relevant summaries. The more advanced the language model the better the summary
    will be.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Bias in Training Data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LLMs learn from vast datasets compiled from various sources, including the internet.
    These datasets can contain biases, which the models might inadvertently learn
    and replicate in their summaries leading to skewed or unfair representations.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Handling Specialized or Technical Language
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While LLMs are generally trained on a wide range of general texts, they may
    not accurately capture specialized or technical language in fields like law, medicine,
    or other highly technical fields. This can be alleviated by feeding it more specialized
    and technical text. Lack of training in specialized jargon can affect the quality
    of summaries when used in these fields.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's clear that extractive summarization is more than just a handy tool; it's
    a growing necessity in our information-saturated age where we’re inundated with
    walls of text every day. By harnessing the power of technologies like BERT, we
    can see how complex texts can be distilled into digestible summaries, saving us
    time and helping us to further comprehend the texts being summarized.
  prefs: []
  type: TYPE_NORMAL
- en: Whether for academic research, business insights, or just staying informed in
    a technologically advanced world, extractive summarization is a practical way
    to navigate the sea of information we’re surrounded by. As natural language processing
    continues to evolve, tools like extractive summarization will become even more
    essential, helping us to quickly find and understand the information that matters
    most in a world where every minute counts.
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://www.exxactcorp.com/blog/deep-learning/extractive-summarization-with-llm-using-bert).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Kevin Vu](https://blog.exxactcorp.com/)** manages [Exxact Corp blog](https://blog.exxactcorp.com/)
    and works with many of its talented authors who write about different aspects
    of Deep Learning.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Classifying Long Text Documents Using BERT](https://www.kdnuggets.com/2022/02/classifying-long-text-documents-bert.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Approaches to Text Summarization: An Overview](https://www.kdnuggets.com/2019/01/approaches-text-summarization-overview.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Getting Started with Automated Text Summarization](https://www.kdnuggets.com/2019/11/getting-started-automated-text-summarization.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Summarization with GPT-3](https://www.kdnuggets.com/2022/04/packt-summarization-gpt3.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Text Summarization Development: A Python Tutorial with GPT-3.5](https://www.kdnuggets.com/2023/04/text-summarization-development-python-tutorial-gpt35.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unlocking GPT-4 Summarization with Chain of Density Prompting](https://www.kdnuggets.com/unlocking-gpt-4-summarization-with-chain-of-density-prompting)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
