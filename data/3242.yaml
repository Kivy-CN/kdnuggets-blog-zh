- en: K-Nearest Neighbors – the Laziest Machine Learning Technique
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-最近邻——最懒的机器学习技术
- en: 原文：[https://www.kdnuggets.com/2017/09/rapidminer-k-nearest-neighbors-laziest-machine-learning-technique.html](https://www.kdnuggets.com/2017/09/rapidminer-k-nearest-neighbors-laziest-machine-learning-technique.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/09/rapidminer-k-nearest-neighbors-laziest-machine-learning-technique.html](https://www.kdnuggets.com/2017/09/rapidminer-k-nearest-neighbors-laziest-machine-learning-technique.html)
- en: '**By Rapidminer** Sponsored Post.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**由Rapidminer** 赞助的帖子。'
- en: K-Nearest Neighbors (K-NN) is one of the simplest machine learning algorithms. Like
    other machine learning techniques, it was inspired by human reasoning. For example,
    when something significant happens in your life, you memorize that experience
    and use it as a guideline for future decisions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: K-最近邻（K-NN）是最简单的机器学习算法之一。像其他机器学习技术一样，它的灵感来源于人类推理。例如，当生活中发生重大事件时，你会记住这个经历，并将其作为未来决策的指南。
- en: Let me give you a scenario of a person dropping a glass. While the glass is
    falling, you’ve made the prediction that the glass will break when it hits the
    ground. But how can you do this? You never have seen this glass break before,
    right?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我给你举一个掉落玻璃的场景。当玻璃正在下落时，你预测玻璃会在撞击地面时破碎。但你是怎么做到的呢？你之前从未见过这个玻璃破碎，对吧？
- en: No, indeed not. But, you have seen similar glasses drop to the floor before. And
    while the situation might not be exactly the same, you know that a glass dropping
    from 5-feet onto a concrete floor usually breaks, giving you a high level of confidence
    that will be the outcome.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，不是的。但你之前见过类似的玻璃掉落在地上。虽然情况可能并不完全相同，但你知道从5英尺高的地方掉落到混凝土地板上的玻璃通常会破碎，这使你对结果有很高的信心。
- en: But what about dropping a glass from a short distance onto a soft carpet? Have
    you experienced this as well? We can see that the distance and ground surface
    both play a role in the outcome.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，掉落玻璃从较短的距离落在柔软的地毯上呢？你也经历过这种情况吗？我们可以看到，距离和地面表面都会对结果产生影响。
- en: 'This is the same reasoning the K-NN algorithm is using. When a new situation
    occurs, it scans through all past experiences and looks up the k closest experiences. Those
    experiences (or: data points) are what we call the *k nearest neighbors*.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是K-NN算法使用的推理方式。当新的情况发生时，它会扫描所有过去的经历，并查找k个最接近的经历。这些经历（或称数据点）就是我们所谓的*k个最近邻*。
- en: If you have a classification task, for example you want to predict if the glass
    will break, you take the majority vote of all k neighbors.  If k=5 and in 3 or
    more of your most similar experiences the glass broke, it will go with the prediction
    “yes, it will break”.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个分类任务，例如你想预测玻璃是否会破碎，你会根据所有k个邻居的多数投票来做出预测。如果k=5，并且在你最相似的经历中有3次或更多次玻璃破碎，那么它会预测“是的，它会破碎”。
- en: Let’s now assume that you want to predict the number of pieces a glass will
    break into. In this case, you want to predict a number which we call “regression”. 
    You take the average value of your k neighbors’ numbers of glass pieces as a prediction
    or score. If k=5 and the numbers of pieces are 1 (did not break), 4, 8, 2, and
    10 you will end up with the prediction of 5.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设你想预测玻璃会破成多少片。在这种情况下，你想预测一个数字，我们称之为“回归”。你将k个邻居的玻璃碎片数量的平均值作为预测或评分。如果k=5，碎片数量分别是1（未破碎）、4、8、2和10，那么你的预测结果将是5。
- en: '![Image 1](../Images/b133c96278078528875b7573035a61a3.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片 1](../Images/b133c96278078528875b7573035a61a3.png)'
- en: We have blue and orange data points.  For a new data point (green), we can determine
    the most likely class by looking up the classes of the nearest neighbors.  Here,
    the decision would be “blue”, because that is the majority of the neighbors.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有蓝色和橙色的数据点。对于一个新的数据点（绿色），我们可以通过查看最近邻居的类别来确定最可能的类别。在这里，决定将是“蓝色”，因为这是大多数邻居的类别。
- en: So why is this algorithm “lazy”? That’s because it doesn’t do any training when
    you supply the training data. At training time, all it is doing is storing the
    data set. It doesn’t do any calculations at this point. Nor does it try to derive
    a more compact model from the data which it could use for scoring.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么这个算法被称为“懒惰”的呢？这是因为当你提供训练数据时，它不会进行任何训练。在训练时，它只是存储数据集，并不会在此时进行任何计算。它也不会尝试从数据中导出更紧凑的模型来进行评分。
- en: Theory
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理论
- en: All the computation happens during scoring, i.e. when you apply the model on
    unseen data points. You need to determine which k data points out of our training
    set are closest to the data point we want to get a prediction for.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所有计算都发生在评分过程中，即当你将模型应用于未见过的数据点时。你需要确定在我们的训练集中哪些k个数据点最接近我们要预测的数据点。
- en: 'Let’s say that our data points look like the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的数据点如下所示：
- en: '![Image 3](../Images/26b69ffabb4f16ef558ec40647ddf6b7.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![Image 3](../Images/26b69ffabb4f16ef558ec40647ddf6b7.png)'
- en: We have a table of *n* rows and *m+1* columns where the first *m* columns are
    the attributes we use to predict the remaining label column (also known as “target”). For
    now, let’s also assume that all attribute values *x* are numerical, while the
    label values for *y* are categorical, i.e. we have a classification problem.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个包含*n*行和*m+1*列的表格，其中前*m*列是我们用来预测剩余标签列（也称为“目标”）的属性。现在，假设所有属性值*x*都是数值型的，而标签值*y*是分类的，即我们有一个分类问题。
- en: We can now define a distance function which calculates the distance between
    data points. It should find the closest data points from our training data for
    any new point. The [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) is
    often a good choice for a distance function if the data is numerical. If our new
    data point has attribute values *s**1* to *s**m*, we can calculate the distance *d(s,
    x**j**)* between point *s* to any data point *x**j* by
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以定义一个距离函数来计算数据点之间的距离。它应该为任何新点找到我们训练数据中最接近的数据点。如果数据是数值型的，**欧几里得距离**通常是一个不错的距离函数选择。如果我们的新数据点具有属性值*s**1*到*s**m*，我们可以通过以下公式计算点*s*到任何数据点*x**j*的距离*d(s,
    x**j***：
- en: '![Image 2](../Images/2b252eb7e0ab1cac7587e110fb382969.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![Image 2](../Images/2b252eb7e0ab1cac7587e110fb382969.png)'
- en: The k data points with the closest value for this distance become our k neighbors. For
    a classification task, we now use the most frequent of all values *y*from our
    k neighbors. For regression tasks, where *y* is numerical, we use the average
    of all values *y* from our k neighbors.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 距离最近的k个数据点成为我们的k个邻居。对于分类任务，我们现在使用k个邻居中所有值*y*中最频繁的那个。对于回归任务，当*y*是数值型时，我们使用k个邻居中所有值*y*的平均值。
- en: But what if our attributes are not numerical and don’t consist of numerical
    and categorical attributes? Then you can use any other distance measure which
    can handle this type of data.  [This article](https://dzone.com/articles/machine-learning-measuring) discusses
    some frequent choices.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们的属性不是数值型的，且不包含数值和分类属性呢？那你可以使用其他能够处理这种数据类型的距离度量。[这篇文章](https://dzone.com/articles/machine-learning-measuring)讨论了一些常见的选择。
- en: By the way, K-NN models with k=1 are the reason why calculating training errors
    are completely pointless.  Can you see why?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，k=1的K-NN模型是为什么计算训练错误完全没有意义的原因。你能看出为什么吗？
- en: Practical Usage
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实际应用
- en: K-NN should be a standard tool in your toolbox. It is fast, easy to understand,
    and easy to tune to different kinds of predictive problems.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: K-NN应该是你工具箱中的一个标准工具。它快速、易于理解，并且容易调整以适应不同类型的预测问题。
- en: '**Data Preparation**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据准备**'
- en: There are some things to consider with K-NN. We have seen that the key part
    of the algorithm is the definition of a distance measure, and the Euclidean distance
    is often used. This distance measure treats all data columns in the same way though.
    It subtracts the values for each dimension before it sums up the squares of those
    distances. And that means that columns with a wider data range have a larger influence
    on the distance than columns with a smaller data range.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用K-NN时需要考虑一些问题。我们已经看到算法的关键部分是距离度量的定义，而**欧几里得距离**通常被使用。然而，这种距离度量将所有数据列视为相同。它在累加这些距离的平方之前，会减去每个维度的值。这意味着数据范围较大的列对距离的影响大于数据范围较小的列。
- en: So, you need to normalize the data set so that all columns are roughly on the
    same scale. There are two common ways of normalization. First, you could bring
    all values of a column into a range between 0 and 1\. Or you could change the
    values of each column so that the column has a mean 0 with a standard deviation
    of 1 afterwards. We call this type of normalization [z-transformation or standard
    score](https://en.wikipedia.org/wiki/Standard_score).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你需要对数据集进行归一化，以使所有列的大致尺度相同。归一化有两种常见方法。首先，你可以将某列的所有值调整到0到1之间。或者你可以改变每列的值，使得该列在调整后具有均值为0和标准差为1。我们称这种归一化方法为[z-变换或标准分数](https://en.wikipedia.org/wiki/Standard_score)。
- en: 'Tip: Whenever you know that the machine learning algorithm is making use of
    a distance measure, you should normalize the data. Another famous example would
    be k-Means clustering.'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 提示：每当你知道机器学习算法在使用距离度量时，你应该对数据进行归一化。另一个著名的例子是k-均值聚类。
- en: '**Parameters to Tune**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**调整参数**'
- en: The most important parameter you need to tune is k, the number of neighbors
    used to make the class decision. The minimum value is 1 in which case you only
    look at the closest neighbor for each prediction to make your decision. In theory,
    you could use a value for k which is as large as your total training set. This
    would make no sense though, since in this case you would always predict the majority
    class of the complete training set.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要调整的最重要参数是k，即用于做出分类决策的邻居数量。最小值为1，在这种情况下，你只查看每个预测的最近邻居以做出决策。理论上，你可以使用一个与训练集总量相同的k值。然而这样做没有意义，因为在这种情况下你总是会预测完整训练集的多数类别。
- en: Here is a good way to interpret the meaning behind k. Small numbers indicate
    “local” models, which can be non-linear and the decision boundary between the
    classes wiggle a lot. If the number grows, the wiggling gets less until you almost
    end up with a linear decision boundary.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个好的方式来解释k的含义。小的k值表示“局部”模型，这些模型可以是非线性的，并且类别之间的决策边界会出现很多波动。如果k值增大，波动会减少，直到你几乎得到一个线性的决策边界。
- en: '![Image 4](../Images/2333f9c70bb9393a0cfa603bd82d1f56.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![Image 4](../Images/2333f9c70bb9393a0cfa603bd82d1f56.png)'
- en: We see a data set in two dimensions on the left.  In general, the top right
    is red and the bottom left is the blue class.  But there are also some local groups
    inside of both areas.  Small values for k lead to more wiggly decision boundaries. 
    For larger values the decision boundary becomes smoother, almost linear in this
    case.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到左侧的一个二维数据集。通常，右上方是红色类，左下方是蓝色类。但在两个区域内部也有一些局部群体。k值小会导致更为弯曲的决策边界。对于较大的k值，决策边界变得更平滑，这种情况下几乎是线性的。
- en: Good values for k depend on the data you have and if the problem is non-linear. You
    should try a couple of values between 1 and about 10% of the size of the training
    data set size to see if there is a promising area worth the further optimization
    of k.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 合适的k值取决于你拥有的数据以及问题是否非线性。你应该尝试几个在1到训练数据集大小的约10%之间的值，以查看是否有值得进一步优化k的有希望的区域。
- en: The second parameter you might want to consider is the type of distance function
    you are using.  For numerical values, Euclidean distance is a good choice.  You
    might want to try [Manhattan distance](https://en.wikipedia.org/wiki/Taxicab_geometry) which
    is sometimes used as well.  For text analytics, [cosine distance](https://en.wikipedia.org/wiki/Cosine_similarity) can
    be another good alternative worth trying.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还需要考虑的第二个参数是你使用的距离函数类型。对于数值数据，欧氏距离是一个不错的选择。你也可以尝试一下[曼哈顿距离](https://en.wikipedia.org/wiki/Taxicab_geometry)，它有时也会被使用。对于文本分析，[余弦距离](https://en.wikipedia.org/wiki/Cosine_similarity)也可以是一个值得尝试的好替代方案。
- en: '**Memory Usage & Runtimes**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**内存使用情况与运行时间**'
- en: Please note that all this algorithm is doing is storing the complete training
    data.  So, the memory needs grow linearly with the number of data points you provide
    for training.  Smarter implementations of this algorithm might choose to store
    the data in a more compact fashion. But in a worst-case scenario you still end
    up with a lot of memory usage.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个算法所做的只是存储完整的训练数据。因此，内存需求随着你提供的训练数据点数量线性增长。这个算法的更智能实现可能会选择以更紧凑的方式存储数据。但在最坏情况下，你仍然会面临大量的内存使用。
- en: For training, the runtime is as good as it gets.  The algorithm is doing no
    calculations at all besides storing the data which is fast.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，运行时间已经是最好的。算法除了存储数据外没有进行任何计算，而存储是快速的。
- en: The runtime for scoring though can be large though which is unusual in the world
    of machine learning.  All calculations happen during model application. Hence,
    the scoring runtime scales linearly with the number of data columns m and the
    number of training points n. If you need to score fast and the number of training
    data points is large, then K-NN is not a good choice.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，评分的运行时间可能会很长，这在机器学习领域是不常见的。所有计算都发生在模型应用期间。因此，评分运行时间与数据列数m和训练点数n线性增长。如果你需要快速评分而训练数据点数量又很大，那么K-NN不是一个好的选择。
- en: '**RapidMiner Processes**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**RapidMiner流程**'
- en: Want to build this machine learning model yourself? [Download RapidMiner](http://go.rapidminer.com/l/32612/2017-09-05/8243cb)
    and use the processes below*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 想自己构建这个机器学习模型？ [下载RapidMiner](http://go.rapidminer.com/l/32612/2017-09-05/8243cb)并使用下面的过程*。
- en: Train and apply a K-NN model: [knn_training_scoring](https://ingomierswacom.files.wordpress.com/2017/05/knn_training_scoring.zip)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和应用K-NN模型： [knn_training_scoring](https://ingomierswacom.files.wordpress.com/2017/05/knn_training_scoring.zip)
- en: Optimize the value for k: [knn_optimize_parameter](https://ingomierswacom.files.wordpress.com/2017/05/knn_optimize_parameter.zip)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化k值： [knn_optimize_parameter](https://ingomierswacom.files.wordpress.com/2017/05/knn_optimize_parameter.zip)
- en: '*Download the Zip-file and extract its content.  The result will be an .rmp
    file which can be loaded into RapidMiner via “File” -> “Import Process”.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*下载Zip文件并提取其内容。结果将是一个.rmp文件，可以通过“文件”->“导入过程”在RapidMiner中加载。*'
- en: '* * *'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织进行IT工作'
- en: '* * *'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题更多信息
- en: '[From Theory to Practice: Building a k-Nearest Neighbors Classifier](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从理论到实践：构建一个k-Nearest Neighbors分类器](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)'
- en: '[Nearest Neighbors for Classification](https://www.kdnuggets.com/2022/04/nearest-neighbors-classification.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分类的最近邻方法](https://www.kdnuggets.com/2022/04/nearest-neighbors-classification.html)'
- en: '[K-nearest Neighbors in Scikit-learn](https://www.kdnuggets.com/2022/07/knearest-neighbors-scikitlearn.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Scikit-learn中的K-Nearest Neighbors](https://www.kdnuggets.com/2022/07/knearest-neighbors-scikitlearn.html)'
- en: '[Parallel Processing in Prompt Engineering: The Skeleton-of-Thought…](https://www.kdnuggets.com/parallel-processing-in-prompt-engineering-the-skeleton-of-thought-technique)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提示工程中的并行处理：思想框架…](https://www.kdnuggets.com/parallel-processing-in-prompt-engineering-the-skeleton-of-thought-technique)'
- en: '[5 Machine Learning Skills Every Machine Learning Engineer Should…](https://www.kdnuggets.com/2023/03/5-machine-learning-skills-every-machine-learning-engineer-know-2023.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个机器学习工程师都应该具备的5项机器学习技能…](https://www.kdnuggets.com/2023/03/5-machine-learning-skills-every-machine-learning-engineer-know-2023.html)'
- en: '[KDnuggets News, December 14: 3 Free Machine Learning Courses for…](https://www.kdnuggets.com/2022/n48.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets新闻，12月14日：3个免费机器学习课程…](https://www.kdnuggets.com/2022/n48.html)'
