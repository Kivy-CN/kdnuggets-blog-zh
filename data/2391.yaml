- en: What are the Assumptions of XGBoost?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost 的假设是什么？
- en: 原文：[https://www.kdnuggets.com/2022/08/assumptions-xgboost.html](https://www.kdnuggets.com/2022/08/assumptions-xgboost.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/08/assumptions-xgboost.html](https://www.kdnuggets.com/2022/08/assumptions-xgboost.html)
- en: '![What are the Assumptions of XGBoost?](../Images/640a6c40bc036eae9847f9795f190915.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![XGBoost 的假设是什么？](../Images/640a6c40bc036eae9847f9795f190915.png)'
- en: '[Faye Cornish](https://unsplash.com/@fcornish) via Unsplash'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Faye Cornish](https://unsplash.com/@fcornish) 通过 Unsplash'
- en: Before we get into the assumptions of XGBoost, I will do an overview of the
    algorithm.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨 XGBoost 的假设之前，我将对该算法进行概述。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在的组织进行 IT 工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: XGBoost stands for Extreme Gradient Boosting and is a supervised learning algorithm
    and falls under the gradient-boosted decision tree (GBDT) family of machine learning
    algorithms.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 代表极端梯度提升，是一种监督学习算法，属于梯度提升决策树（GBDT）家族的机器学习算法。
- en: Let’s dive into boosting first.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先深入了解提升方法。
- en: From Boosting to XGBoost
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从提升到 XGBoost
- en: Boosting is the method of combining a set of weak learners into a strong learner
    to reduce the level of training errors. Boosting helps to deal with bias-variance
    trade-off making it more effective. There are different types of boosting algorithms,
    such as AdaBoost (Adaptive Boosting), Gradient Boosting, XGBoost, and more.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 提升方法是将一组弱学习器结合成一个强学习器，以减少训练误差的水平。提升方法有助于处理偏差-方差权衡，使其更有效。有不同类型的提升算法，如 AdaBoost（自适应提升）、梯度提升、XGBoost
    等。
- en: Now let’s go into XGBoost
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入了解 XGBoost
- en: As mentioned before, XGBoost is an extension to gradient boosted decision trees
    (GBM) and is well known for its speed and performance.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，XGBoost 是对梯度提升决策树（GBM）的扩展，以其速度和性能而闻名。
- en: Predictions are made based on combining a set of simpler, weaker models - these
    models are decision trees that are created in sequential form. These models make
    predictions based on evaluating other decision trees through if-then-else true/false
    feature questions, using these to assess and estimate the probability of producing
    a correct decision.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 预测是基于结合一组更简单、更弱的模型进行的——这些模型是以序列形式创建的决策树。这些模型通过 if-then-else 的真假特征问题评估其他决策树，以此来评估和估计产生正确决策的概率。
- en: 'It consists of these three elements:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 它包含以下三个要素：
- en: A loss function that is to be optimized.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个需要优化的损失函数。
- en: A weak learner to make predictions.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个弱学习器用来进行预测。
- en: An additive model to add to the weak learners to reduce errors
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一种加性模型，用于增加到弱学习器中以减少错误
- en: Features of XGBoost
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost 的特点
- en: 'There are 3 features of XGBoost:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 有三个特点：
- en: 1\. Gradient Tree Boosting
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 梯度树提升
- en: The tree ensemble model needs to be trained in an additive manner. Meaning that
    it is an iterative and sequential process where decision trees are added one step
    at a time. There are a fixed number of trees added and with each iteration, there
    should be a reduction in loss function value.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 树集合模型需要以加性方式进行训练。这意味着这是一个迭代和顺序的过程，其中决策树一步步地增加。添加的树数量是固定的，并且每次迭代时，损失函数值应该减少。
- en: 2\. Regularized Learning
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 正则化学习
- en: Regularized Learning helps to minimize the loss function and prevent overfitting
    or underfitting from occurring - helping to smooth out the final learnt weight.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化学习有助于最小化损失函数，并防止过拟合或欠拟合的发生——帮助平滑最终学习到的权重。
- en: 3\. Shrinkage and Feature Subsampling
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 收缩和特征子采样
- en: These two techniques are used to further prevent overfitting.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种技术用于进一步防止过拟合。
- en: Shrinkage reduces the level of influence each tree has on the overall model
    and allows room for future trees to potentially improve it.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 收缩减少了每棵树对整体模型的影响程度，并为未来的树提供了改进的空间。
- en: Feature Subsampling is something that you may have seen in the Random Forest
    algorithm. The features are in the column section of the data and not only does
    it prevent overfitting, but it also speeds up computations of the parallel algorithm.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 特征子采样是你可能在随机森林算法中见过的。特征位于数据的列部分，它不仅能防止过拟合，还能加快并行算法的计算速度。
- en: XGBoost Hyperparameters
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost 超参数
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'XGBoost hyperparameters are divided into 4 groups:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 超参数分为 4 个组：
- en: General parameters
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一般参数
- en: Booster parameters
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增强器参数
- en: Learning task parameters
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习任务参数
- en: Command line parameters
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 命令行参数
- en: General parameters, Booster parameters and Task parameters are set before running
    the XGBoost model. The Command line parameters are only used in the console version
    of XGBoost.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一般参数、增强器参数和任务参数在运行 XGBoost 模型之前设置。命令行参数仅在 XGBoost 的控制台版本中使用。
- en: If the parameters are not tuned properly, it can easily lead to overfitting.
    However, it is difficult to tune the parameters of an XGBoost model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果参数未正确调整，可能会导致过拟合。然而，调优 XGBoost 模型的参数是困难的。
- en: Stay tuned for an upcoming article about Tuning XGBoost Hyperparameters.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 敬请期待关于调优 XGBoost 超参数的即将发布的文章。
- en: What Are the Assumptions of XGBoost?
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost 的假设是什么？
- en: 'The main assumptions of XGBoost are:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 的主要假设是：
- en: XGBoost may assume that encoded integer values for each input variable have
    an ordinal relationship
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost 可能假设每个输入变量的编码整数值具有顺序关系
- en: XGBoost assume that your data may not be complete (i.e. it can deal with missing
    values)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost 假设你的数据可能不完整（即它可以处理缺失值）
- en: 'As it DOES NOT assume that all values are present, the algorithm can handle
    missing values by default. When working with tree based algorithms, missing values
    are learned during the training phase. This then leads to the fact that:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于算法**不假设所有值都存在**，它可以默认处理缺失值。在处理基于树的算法时，缺失值在训练阶段会被学习。这导致：
- en: XGBoost can handle sparsity
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost 能够处理稀疏性
- en: XGBoost manages only numeric vectors, therefore if you have categorical variables
    they will need to be converted into numeric variables.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 仅管理数值向量，因此如果你有分类变量，它们需要转换为数值变量。
- en: You will have to transform a dense dataframe with few zeroes in the matrix to
    a very sparse matrix which has lots of zero in the matrix. This means that XGBoost
    has the ability to convert variables into a sparse matrix format as an input.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你将不得不将一个密集的数据框（矩阵中几乎没有零）转换为一个稀疏矩阵（矩阵中有大量零）。这意味着 XGBoost 能够将变量转换为稀疏矩阵格式作为输入。
- en: Conclusion
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this blog, you have come to understand: how boosting relates to XGBoost;
    the features of XGBoost; how it reduces the loss function value and overfitting.
    We have briefly gone over the 4 hyperparameters which will be followed up with
    an article which solely focuses on Tuning XGBoost Hyperparameters.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个博客中，你已经了解了：提升方法与 XGBoost 的关系；XGBoost 的特点；它如何减少损失函数值和过拟合。我们简要介绍了 4 个超参数，接下来将发布一篇专注于调优
    XGBoost 超参数的文章。
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and Freelance Technical Writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** 是一位数据科学家和自由技术作家。她特别关注提供数据科学职业建议或教程以及数据科学的理论知识。她还希望探索人工智能在延长人类生命方面的不同方式。作为一名热心学习者，她希望扩大她的技术知识和写作技能，同时帮助指导他人。'
- en: More On This Topic
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[How to Speed Up XGBoost Model Training](https://www.kdnuggets.com/2021/12/speed-xgboost-model-training.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何加快 XGBoost 模型训练](https://www.kdnuggets.com/2021/12/speed-xgboost-model-training.html)'
- en: '[Tuning XGBoost Hyperparameters](https://www.kdnuggets.com/2022/08/tuning-xgboost-hyperparameters.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[调优 XGBoost 超参数](https://www.kdnuggets.com/2022/08/tuning-xgboost-hyperparameters.html)'
- en: '[Leveraging XGBoost for Time-Series Forecasting](https://www.kdnuggets.com/2023/08/leveraging-xgboost-timeseries-forecasting.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用 XGBoost 进行时间序列预测](https://www.kdnuggets.com/2023/08/leveraging-xgboost-timeseries-forecasting.html)'
- en: '[WTF is the Difference Between GBM and XGBoost?](https://www.kdnuggets.com/wtf-is-the-difference-between-gbm-and-xgboost)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GBM 和 XGBoost 之间的区别是什么？](https://www.kdnuggets.com/wtf-is-the-difference-between-gbm-and-xgboost)'
