- en: Free resources to learn Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2018/09/free-resources-natural-language-processing.html](https://www.kdnuggets.com/2018/09/free-resources-natural-language-processing.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By [Muktabh Mayank](https://twitter.com/muktabh), [ParallelDots](https://www.paralleldots.com/)**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fb0dc19ad05061fdc63ed485a5b17f2a.png)'
  prefs: []
  type: TYPE_IMG
- en: Natural Language Processing (NLP) is the ability of a computer system to understand
    human language. Natural Langauge Processing is a subset of Artificial Intelligence
    (AI). There are multiple resources available online which can help you develop
    expertise in Natural Language Processing.
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, we list resources for the beginners and intermediate level
    learners.
  prefs: []
  type: TYPE_NORMAL
- en: '**Natural Language Resources for Beginners**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/d873ca29f1691333fbfa3d861d733b30.png)'
  prefs: []
  type: TYPE_IMG
- en: A beginner can follow two methods i.e. Traditional Machine Learning and Deep
    Learning to get started with Natural language processing. These two methods are
    very different from each other. For the inquisitive, [here’s](https://towardsdatascience.com/why-deep-learning-is-needed-over-traditional-machine-learning-1b6a99177063) the
    difference between these two.
  prefs: []
  type: TYPE_NORMAL
- en: '**Traditional Machine Learning**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Traditional machine learning algorithms are complex and often not easy to understand.
    Here are a few resources which will help you get started in learning NLP using
    machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Speech and Language Processing by Jurafsky and Martin is the popularly acclaimed
    bible for traditional Natural Language Processing. You can access it [here](https://web.stanford.edu/~jurafsky/slp3/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a more practical approach, you can try out [Natural Language Toolkit.](http://www.nltk.org/book/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep Learning**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Deep learning is a subfield of machine learning and is far better than traditional
    machine learning due to the introduction of Artificial Neural Networks. Beginners
    can start with the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: CS 224n: This is the best course to get started with using Deep Learning for
    Natural Language Processing. This course is hosted by Stanford and can be accessed [here](http://web.stanford.edu/class/cs224n/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yoav Golberg’s free and paid books are great resources to get started with Deep
    Learning in Natural Language Processing. The free version can be accessed [here](https://u.cs.biu.ac.il/~yogo/nnlp.pdf)and
    the full version is available [here](https://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies/dp/1627052984).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A very thorough coverage of all algorithms can be found in Jacob Einsenstein’s
    notes from GATECH’s NLP class which deals in almost all NLP methods. You can access
    the notes on GitHub [here](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural Language Resources for Practitioners**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/0509064d98e73366d835bb6b6c84c51e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you are a practicing Data Scientist, you will need three types of resources:'
  prefs: []
  type: TYPE_NORMAL
- en: Quick Getting Started guides / Knowing about what is hot and new
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Problem-Specific Surveys of Methods
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Blogs to follow regularly
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Quick Getting Started guides / Knowing about what is hot and new
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One can start with Otter et al.’s Deep Learning for Natural Language Processing
    survey. You can access it [here](https://arxiv.org/abs/1807.10854).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A survey paper by Young et al tries to summarize everything hip in Deep Learning
    based Natural Language Processing, and is recommended to get started with Natural
    Language Processing for practitioners. You can access the paper [here](https://arxiv.org/abs/1708.02709).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can refer to [this](https://arxiv.org/abs/1808.03314)article to understand
    the basics of LSTMs and RNNs, which are used in Natural Language Processing a
    lot. Another much more cited (and highly reputed) survey of LSTMs is [here](https://arxiv.org/abs/1503.04069).
    This is an interesting paper to understand how the hidden states of RNNs work.
    It is an enjoyable read and can be accessed [here](https://github.com/locuslab/TCN).
    I always recommend the following two blog posts anyone who hasn’t read them:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: http://colah.github.io/posts/2015-08-Understanding-LSTMs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: https://distill.pub/2016/augmented-rnns/
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convolutional Neural Networks (Convnets) can be used to make sense of Natural
    Language. You can visualize how Convnets work in NLP by reading this paper [here](https://arxiv.org/abs/1801.06287).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How Convnets and RNNs compare with each other has been highlighted in [this](https://arxiv.org/abs/1803.01271)paper
    by Bai et al.. All its pytorch (I have stopped or reduced, to a large extent,
    reading deep learning code not written in pytorch  ) code is open sourced [here](https://github.com/locuslab/TCN) and
    gives you a feel of Godzilla v/s King Kong or Ford Mustang vs Chevy Camaro(if
    you enjoy(ed) that type of thing). Who will win! .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problem-specific Surveys of Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another type of resources practitioners need is answers to questions of the
    type: “I have to train an algorithm to do X, what is the coolest (and easily accessible)
    thing I can apply?”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what you will need for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TEXT CLASSIFICATION**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: What’s the first problem people solve? Text Classification, mostly. Text Classification
    can be in the form of categorizing text into different categories or detecting
    sentiment/emotion within the text.
  prefs: []
  type: TYPE_NORMAL
- en: I would like to highlight an easy read about different surveys of Sentiment
    Analysis described in this ParallelDots [blog](https://blog.paralleldots.com/data-science/breakthrough-research-papers-and-models-for-sentiment-analysis/).
    Though the survey is for sentiment analysis technologies, it can be extended to
    most text classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: Our (ParallelDots) surveys are slightly less technical and aim to direct you
    to cool resources to understand a concept. The Arxiv survey papers I point you
    to will be very technical and will need you to read other important papers to
    deeply understand a topic. Our suggested way is to use our links to get familiar
    and have fun with a topic but then to be sure to read the thorough guides we point
    to. (Dr. Oakley’s [course](https://www.coursera.org/learn/learning-how-to-learn) talks
    about chunking, where you first try to get small bits here and there before you
    jump deep). Remember, it is great to have fun but unless you understand the techniques
    in detail, it will be hard to apply concepts in a new situation.
  prefs: []
  type: TYPE_NORMAL
- en: Another survey of Sentiment Analysis algorithms (by people at Linked University
    and UIUC) is [here](https://arxiv.org/abs/1801.07883).
  prefs: []
  type: TYPE_NORMAL
- en: The Transfer Learning revolution has already hit Deep Learning. Just like in
    images where a model trained on ImageNet classification can be fine-tuned for
    any classification task, NLP models trained for language modeling on Wikipedia
    can now transfer learn text classification on a relatively lesser amount of data.
    Here are two papers from OpenAI and Ruder, and Howard which deal with these techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://arxiv.org/abs/1801.06146](https://arxiv.org/abs/1801.06146)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fast.ai has a more friendly documentation to apply these methods [here](http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html).
  prefs: []
  type: TYPE_NORMAL
- en: If you are transfer learning two different tasks (not transferring from Wikipedia
    language modeling task), tricks to use Convnets are mentioned [here](https://arxiv.org/abs/1801.06480).
  prefs: []
  type: TYPE_NORMAL
- en: IMHO, such approaches will slowly take up on all other classification methods
    (simple extrapolation from what has happened in vision). We also released our
    work on [Zero Shot Text classification](https://arxiv.org/abs/1712.05972) which
    gets good accuracy without any training on a dataset and are working on its next
    generation. We have built our custom text classification API commonly called the
    Custom Classifier in which you can define your own categories. You can check out
    the free [here](https://www.paralleldots.com/custom-classifier).
  prefs: []
  type: TYPE_NORMAL
- en: SEQUENCE LABELING
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sequence Labeling is a task which labels words with different attributes. These
    include part-of-speech tagging, Named Entity Recognition, Keyword Tagging etc.
  prefs: []
  type: TYPE_NORMAL
- en: We wrote a fun review of methods to tasks like these [here](https://blog.paralleldots.com/data-science/named-entity-recognition-milestone-models-papers-and-technologies/).
  prefs: []
  type: TYPE_NORMAL
- en: An excellent resource for such problems is the research paper from this year’s
    COLING which gives optimal guidelines to train Sequence labeling algorithms. You
    can access it [here](https://arxiv.org/abs/1806.04470).
  prefs: []
  type: TYPE_NORMAL
- en: MACHINE TRANSLATION
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the biggest advances in NLP in recent days has been the discovery of
    algorithms that can translate text from one language to another. Google’s system
    is an insane 16 layered LSTM (which requires no dropout because they have tons
    of data to train on) and gives state-of-the-art translation results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Media experts blew the hype out of proportion with hyperbole reports claiming
    “Facebook had to shut down AI which invented its own language”. Here are some
    of these.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://gadgets.ndtv.com/social-networking/news/facebook-shuts-ai-system-after-bots-create-own-language-1731309](https://gadgets.ndtv.com/social-networking/news/facebook-shuts-ai-system-after-bots-create-own-language-1731309)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.forbes.com/sites/tonybradley/2017/07/31/facebook-ai-creates-its-own-language-in-creepy-preview-of-our-potential-future/#1d1ca041292c](https://www.forbes.com/sites/tonybradley/2017/07/31/facebook-ai-creates-its-own-language-in-creepy-preview-of-our-potential-future/#1d1ca041292c)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For an extensive tutorial on Machine Translation, refer to Philip Koehn’s research
    paper [here](https://arxiv.org/abs/1709.07809). A specific review to use Deep
    Learning for Machine Translation (which we call NMT or Neural Machine Translation)
    is [here](https://arxiv.org/abs/1707.07631).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A couple of my favorite papers are here –
  prefs: []
  type: TYPE_NORMAL
- en: This [paper by Google](https://arxiv.org/abs/1609.08144) tells you how to solve
    a problem end-to-end when you have a lot of money and data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facebook’s [Convolutional NMT system](https://arxiv.org/abs/1705.03122)(just
    because of its cool convolutional approach) and its code is released as a library [here](https://github.com/facebookresearch/fairseq).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://marian-nmt.github.io/](https://marian-nmt.github.io/)is a framework
    for fast translation in C++[http://www.aclweb.org/anthology/P18-4020](http://www.aclweb.org/anthology/P18-4020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://opennmt.net/](http://opennmt.net/)enables everyone to train their NMT
    systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: QUESTION ANSWERING
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: IMHO this is going to be the next “Machine Translation”. There are many different
    types of Question Answering tasks. Choosing from options, selecting answers from
    a paragraph or a knowledge graph and answering questions based on an image (also
    known as Visual Question Answering) and there are different datasets to get to
    know the state of the art method.
  prefs: []
  type: TYPE_NORMAL
- en: '[SQuAD dataset](https://rajpurkar.github.io/SQuAD-explorer/)is a question answering
    datasets which tests an algorithm’s ability to read comprehensions and answer
    questions. Microsoft published a paper earlier this year claiming they have reached
    human-level accuracy for the [task](https://blogs.microsoft.com/ai/microsoft-creates-ai-can-read-document-answer-questions-well-person/).
    The paper can be found [here](https://www.ailab.microsoft.com/experiments/ef90706b-e822-4686-bbc4-94fd0bca5fc5).
    Another important algorithm (which I feel is the coolest) is Allen AI’s [BIDAF](https://allenai.github.io/bi-att-flow/) and
    its improvements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another important set of algorithms is Visual Question Answering which answers
    questions about images. Teney et al.’s [paper](https://allenai.github.io/bi-att-flow/)from
    VQA 2017 challenge is an excellent resource to get started. You can also find
    its implementations on Github [here](https://github.com/markdtw/vqa-winner-cvprw-2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extractive Question Answering on large documents (like how Google Highlights
    answer to your queries in first few results) in real life can be done using Transfer
    Learning (thus with few annotations) as shown in this ETH paper [here](https://arxiv.org/abs/1804.07097). A
    very good paper criticizing the “understanding” of Question Answering algorithms
    is [here](https://arxiv.org/abs/1808.04926). Must read if you are working in this
    field.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PARAPHRASE, SENTENCE SIMILARITY OR INFERENCE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The task of comparing sentences. NLP has three different tasks: Sentence Similarity,
    Paraphrase detection and Natural Language Inference (NLI) for this, each requiring
    more semantic understanding than the last. [MultiNLI](https://www.nyu.edu/projects/bowman/multinli/) and
    its subset Stanford NLI are the most well-known benchmarks datasets for NLI and
    have become the focus of research lately. There are also MS Paraphrase Corpus
    and Quora Corpus for paraphrase detection, and a SemEval Dataset for STS (Semantic
    Text Similarity). A good survey for advanced models in this domain can be found [here](https://arxiv.org/abs/1806.04330).
    Applied NLI in the clinical domain is very important. (Finding out about right
    medical procedures, side effects and cross effects of drugs etc. ). [This tutorial](https://arxiv.org/abs/1808.06752) from
    applied NLI in the medical domain is a good read if you are looking to apply the
    tech in a specific domain.'
  prefs: []
  type: TYPE_NORMAL
- en: Here is a list of my favorite papers in this domain
  prefs: []
  type: TYPE_NORMAL
- en: Natural Language Inference over Interaction Space – It highlights a very clever
    approach for putting a DenseNet (Convolutional Neural Network on Sentence representations).
    The fact that this was the outcome of an internship project makes it even cooler!
    You can read the paper [here](https://arxiv.org/pdf/1709.04348.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This research [paper](https://homes.cs.washington.edu/~roysch/papers/artifacts/artifacts_poster.pdf)from
    Omar Levy’s group shows that even simple algorithms can perform the task. This
    is because algorithms are still not learning “inference”.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BiMPM is a cool model to predict paraphrases and can be accessed [here](https://arxiv.org/abs/1702.03814).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have a new work for Paraphrase detection too which applies Relation Networks
    on top of sentence representations and has been accepted at this year’s AINL conference.
    You can read it [here](https://peerj.com/preprints/26847).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OTHER FIELDS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here are some of the more detailed survey papers to get information about research
    for other tasks you might encounter making an NLP system.
  prefs: []
  type: TYPE_NORMAL
- en: '**Language Modelling(LM) – **Language Modelling is the task of learning an
    unsupervised representation of a language. This is done by predicting the (n+1)th
    word of a sentence given the first N words. These models have two important real-world
    uses, autocomplete and acting as a base model for transfer learning for text classification
    as mentioned above. A detailed survey is [here](https://arxiv.org/abs/1708.07252).
    If you are interested in learning how to autocomplete LSTMs in cellphones/search
    engines work based on search history, [here](https://arxiv.org/abs/1804.09661)is
    a cool paper you should read.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relation Extraction – **Relation extraction is the task of extracting relations
    between entities present in a sentence. A given sentence “A is related as r to
    B”, gives the triplet (A,r, B). A survey of the research work in the field is [here](https://arxiv.org/abs/1712.05191). [Here](https://arxiv.org/abs/1706.04115)is
    a research paper that I found to be really interesting. It uses BIDAFs for Zero
    Shot Relation extraction (that is, it can recognize relations it was not even
    trained to recognize).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dialog Systems – **With the onset of the chatbot revolution, Dialog systems
    are now the rage. Many people (including us) make dialog systems as a combination
    of models such as intent detection, keyword detection, question answering etc,
    while others try to model it end-to-end. A detailed survey of dialog system models
    by the team at JD.com is [here](https://arxiv.org/abs/1711.01731). I would also
    like to mention Parl.ai, a framework by Facebook AI for the purpose.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text Summarization – **Text Summarization is used to get a condensed text
    from a document (paragraph/news article etc.). There are two ways to do this:
    extractive and abstractive summarization. While extractive summarization gives
    out sentences from the article with the highest information content (and what
    has been available for decades), abstractive summarization aims to write a summary
    just like a human would. This [demo](https://www.salesforce.com/products/einstein/ai-research/tl-dr-reinforced-model-abstractive-summarization/)from
    Eintein AI brought abstractive summarization into mainstream research. There is
    an extensive survey of techniques [here](https://arxiv.org/abs/1804.04589).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural Language Generation (NLG) – **Natural Language Generation is the
    research where the computer aims to write like a human would. This could be stories,
    poetries, image captions etc. Out of these, current research has been able to
    do very well on image captions where LSTMs and attention mechanism combined has
    given outputs usable in real life. A survey of techniques is available [here](https://arxiv.org/abs/1703.09902).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blogs to follow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here’s a list of blogs which we highly recommend for anyone interested in keeping
    track of what’s new in NLP research.
  prefs: []
  type: TYPE_NORMAL
- en: Einstein AI – [ https://einstein.ai/research](https://einstein.ai/research)
  prefs: []
  type: TYPE_NORMAL
- en: Google AI blog – [https://ai.googleblog.com/](https://ai.googleblog.com/)
  prefs: []
  type: TYPE_NORMAL
- en: WildML – [http://www.wildml.com/](http://www.wildml.com/)
  prefs: []
  type: TYPE_NORMAL
- en: DistillPub – [https://distill.pub/](https://distill.pub/) (distillpub is unique,
    blog and publication both)
  prefs: []
  type: TYPE_NORMAL
- en: Sebastian Ruder – [http://ruder.io/](http://ruder.io/)
  prefs: []
  type: TYPE_NORMAL
- en: If you liked this article, you must follow our blog. We come up with resource
    lists quite frequently here.
  prefs: []
  type: TYPE_NORMAL
- en: That’s all folks! Enjoy making neural nets understand language. You can also
    try on text analysis APIs based on Natural Language Processing [here](https://www.paralleldots.com/text-analysis-apis).
  prefs: []
  type: TYPE_NORMAL
- en: You can also read about Machine Learning algorithms you should know to become
    a Data Scientist [here](https://blog.paralleldots.com/data-science/machine-learning/ten-machine-learning-algorithms-know-become-data-scientist/).
  prefs: []
  type: TYPE_NORMAL
- en: You can also check out free demos of ParallelDots AI APIs [here](https://www.paralleldots.com/text-analysis-apis).
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://blog.paralleldots.com/data-science/nlp/free-natural-language-processing-resources/).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio**: [Muktabh Mayank](https://twitter.com/muktabh) is cofounder at [ParallelDots](https://www.paralleldots.com/),
    a company that is "Simplifying Machine Learning For Everyone". Muktabh is a Data
    Scientist, entrepreneur , sociologist, not the classic nerd.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[The Data Science of “Someone Like You” or Sentiment Analysis of Adele’s Songs](https://www.kdnuggets.com/2018/09/sentiment-analysis-adele-songs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning for Text Classification Using SpaCy in Python](https://www.kdnuggets.com/2018/09/machine-learning-text-classification-using-spacy-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Learning for NLP: An Overview of Recent Trends](https://www.kdnuggets.com/2018/09/deep-learning-nlp-overview-recent-trends.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[N-gram Language Modeling in Natural Language Processing](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Free Books on Natural Language Processing to Read in 2023](https://www.kdnuggets.com/2023/06/5-free-books-natural-language-processing-read-2023.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25 Free Books to Master SQL, Python, Data Science, Machine…](https://www.kdnuggets.com/25-free-books-to-master-sql-python-data-science-machine-learning-and-natural-language-processing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Free Courses to Master Natural Language Processing](https://www.kdnuggets.com/5-free-courses-to-master-natural-language-processing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Transfer Learning for Image Recognition and Natural Language Processing](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Start Using Natural Language Processing With PyTorch](https://www.kdnuggets.com/2022/04/start-natural-language-processing-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
