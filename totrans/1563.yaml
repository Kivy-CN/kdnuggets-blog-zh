- en: What Is Dimension Reduction In Data Science?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/01/dimension-reduction-data-science.html](https://www.kdnuggets.com/2019/01/dimension-reduction-data-science.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: We have access to a large amounts of data now. The large amount of data can
    lead us to situations where by we take every possible data that is available to
    us and feed it into a forecasting model to predict our target variable. This article
    aims to explain the common issues associated with introduction of large set of
    features and provides solutions which we can utilise to resolve those problems.
  prefs: []
  type: TYPE_NORMAL
- en: '*It is crucial for every data scientist and machine learning expert to understand
    what dimension reduction techniques are and when to use them.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/006d7d77736e6ecab546ceb5e7acf823.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Photo by [Sergi Kabrera](https://unsplash.com/@skabrera?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Let’s Understand The Issues Better**'
  prefs: []
  type: TYPE_NORMAL
- en: Occasionally we gather data for our data science project and end up gathering
    a large set of features. Some of these features (known as variables) are not as
    important as others. Sometimes the features themselves are correlated with each
    other. And occasionally we end up over-fitting the problem by introducing too
    many features. The large number of features make the data set sparse.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, it takes a much larger space to store a data set with a large number
    of features. Moreover, it can get very difficult to analyse and visualize a data
    set with a large number of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '*Dimension reduction can reduce the time that is required to train our machine
    learning model and it can also benefit in eliminating over-fitting.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This article outlines the techniques which we can follow to compress our data
    set onto a new feature subspace of lower dimensionality. I will also be providing
    details of important dimension reduction techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '*Please read FinTechExplained d*[*isclaimer*](https://medium.com/p/87dba77241c7?source=your_stories_page---------------------------)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**How Do I Define Dimension Reduction?**'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you want to e-mail a large set of files to your friend. Uploading and
    sending the files might take a longer time. You can speed up the process of uploading
    of the files by zipping the files and e-mail the zipped file instead. Zipping
    the file compresses large quantity of data into smaller equivalent sets.
  prefs: []
  type: TYPE_NORMAL
- en: '*Dimension reduction is the same principal as zipping the data.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Dimension reduction compresses large set of features onto a new feature subspace
    of lower dimensional without losing the important information.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Although the slight difference is that dimension reduction techniques will lose
    some of the information when the dimensions are reduced.
  prefs: []
  type: TYPE_NORMAL
- en: It is harder to visualise a large set of dimensions. Dimension reduction techniques
    can be employed to make a 20+ dimension feature space into 2 or 3 dimension subspace.
  prefs: []
  type: TYPE_NORMAL
- en: '**What Are Different Dimension Reduction Techniques?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we take a deep dive into the key techniques, let’s quickly understand
    the two main areas of machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised — when the results of the training set are known
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unsupervised — when the final outcome is not known
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you want to get a better understanding of machine learning then have a look
    at my article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Machine Learning in 8 Minutes](https://medium.com/fintechexplained/introduction-to-machine-learning-4b2d7c57613b)'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a large number of techniques to reduce the dimensions such as forward/backward
    feature selection or combining the dimensions together by calculating weighted
    average of the correlated features. However in this article I will explore two
    of the main techniques of dimension reduction:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear Discriminant Analysis (LDA):'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LDA is used for compressing supervised data
  prefs: []
  type: TYPE_NORMAL
- en: When we have a large set of features (classes), and our data is normally distributed
    and the features are not correlated with each other then we can use LDA to reduce
    the number of dimensions. LDA is a generalised version of Fisher’s linear discriminant.
  prefs: []
  type: TYPE_NORMAL
- en: '*Calculate z-score to normalise the features that are highly skewed.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to understand how to enrich features and calculate z-score then
    have a look at this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Processing Data To Improve Machine Learning Models Accuracy](https://medium.com/fintechexplained/processing-data-to-improve-machine-learning-models-accuracy-de17c655dc8e)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sci-kit learn offers easy to use LDA tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This code will result in producing three LDA components for the entire data
    set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a9b51b9491e5ee73e742e88d09cb8150.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Photo by [NASA](https://unsplash.com/@nasa?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Principal component analysis (PCA):**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: They are mainly used for compressing unsupervised data.
  prefs: []
  type: TYPE_NORMAL
- en: PCA is a very useful technique that can help de-noise and detect patterns in
    data. PCA is used in reducing dimensions in images, textual contents and in speech
    recognition systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sci-kit learn library offers a powerful PCA component classifier. This code
    snippet illustrates how to create PCA components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**It is wise to understand how PCA works.**'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding PCA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This section of the article provides an overview of the process:'
  prefs: []
  type: TYPE_NORMAL
- en: PCA technique analyses the entire data set and then finds the points with maximum
    variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It creates new variables such that there is a linear relationship between the
    new and original variables such that the variance is maximised.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Covariance matrix is then created for the features to understand their multi-collinearity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the variance-covariance matrix is computed, PCA then uses the gathered
    information to reduce the dimensions. It computes orthogonal axes from the original
    feature axes. These are the axes of directions with maximum variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Firstly the eigenvectors of the variance-covariance matrix are calculated. The
    vector represents the directions of maximum variance which are known as the principal
    components. The eigenvalues are then created that define magnitude of the principal
    components.
  prefs: []
  type: TYPE_NORMAL
- en: '*The eigenvalues are the PCA components.*'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, for N dimensions, there will be a NxN variance-covariance matrix
    and as a result, we will have a eigen vector of N values and N eigen values matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use following python modules to create the components:'
  prefs: []
  type: TYPE_NORMAL
- en: Use linalg.eig to create eigen vectors
  prefs: []
  type: TYPE_NORMAL
- en: Use numpy.cov to compute variance-covariance matrix
  prefs: []
  type: TYPE_NORMAL
- en: We need to take the eigen vectors that represent the our data set best. These
    are the vectors which we have highest eigenvalues.
  prefs: []
  type: TYPE_NORMAL
- en: '*Take eigen vectors that capture about 70% of the variance.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Remember eigenvectors with largest eigenvalues are the ones with highest variance
    and they are closest to the original data set. Also larger the number of eigenvectors,
    slower the computation performance.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*I normally take 2–3 top eigen vectors to represent the data set.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to keep sci-kit learn to give us all of the PCA components so that
    we can assess the variance then initialise PCA with None components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7cf98875a6bedcf8791460f87c4e2e29.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Photo by [Chang Qing](https://unsplash.com/@lee0201?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)**'
  prefs: []
  type: TYPE_NORMAL
- en: '*It is important to normalise/standardise the data before performing PCA because
    PCA is sensitive to the scale of the data in the features.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kernel principal component analysis (KDA):'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*They are used for Nonlinear dimensionality reduction*'
  prefs: []
  type: TYPE_NORMAL
- en: When we have non-linear features then we can project them onto a larger feature
    set to remove their correlations and to make them linear.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, non-linear data is mapped and transformed onto a higher-dimensional
    space. Then PCA is used to reduce the dimensions. However, one downside of this
    approach is that it is computationally very expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Just like in PCA, we first compute variance-covariance matrix and then eigen
    vectors and eigen values are prepared with the highest variance to compute principal
    components.
  prefs: []
  type: TYPE_NORMAL
- en: We then compute kernel matrix. This requires us to construct a similarity matrix.
    The matrix is then decomposed via creating eigen values and eigen vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sci-Kit learn offers Kernal PCA modules. To use Kernal PCA, we can use following
    snippet of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Gamma is a tuning parameter of the RBF kernel.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cb6d57451b3efb60222c26792b1d2e47.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Photo by [Billy Huynh](https://unsplash.com/@billy_huy?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)**'
  prefs: []
  type: TYPE_NORMAL
- en: Benefits Of Dimension Reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section briefly outlines the core benefits of reducing dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: We have access to a large set of data now. When we are building forecasting
    models that are trained on images, sound and/or textual contents then the input
    feature sets can end up having a large set of features. It increases space, further
    adds over-fitting and slows down the time to train the models. Occasionally features
    are introduced that end up adding more noise than expected.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key methodologies to improve efficiency in computational intensive
    tasks is to reduce the dimensions after ensuring most of the key information is
    maintained. It also eliminates features with strong correlation between them and
    reduces over-fitting.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This article provided an overview of the techniques which we can follow to compress
    our data set onto a new feature subspace of lower dimensionality. It also provided
    details of important dimension reduction techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly the benefits of dimension reductions were summarised.
  prefs: []
  type: TYPE_NORMAL
- en: Please let me know if there are any questions.
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/fintechexplained/what-is-dimension-reduction-in-data-science-2aa5547f4d29).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resources:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning
    education](https://www.kdnuggets.com/education/online.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Software for Analytics, Data Science, Data Mining, and Machine Learning](https://www.kdnuggets.com/software/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[The Data Science Gold Rush: Top Jobs in Data Science and How to Secure Them](https://www.kdnuggets.com/2019/01/top-jobs-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Dimensionality Reduction : Does PCA really improve classification outcome?](https://www.kdnuggets.com/2018/07/dimensionality-reduction-pca-improve-classification-results.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Must-Know: What is the curse of dimensionality?](https://www.kdnuggets.com/2017/04/must-know-curse-dimensionality.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Dimensionality Reduction Techniques in Data Science](https://www.kdnuggets.com/2022/09/dimensionality-reduction-techniques-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Science Minimum: 10 Essential Skills You Need to Know to Start…](https://www.kdnuggets.com/2020/10/data-science-minimum-10-essential-skills.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets™ News 22:n06, Feb 9: Data Science Programming…](https://www.kdnuggets.com/2022/n06.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Science Definition Humor: A Collection of Quirky Quotes…](https://www.kdnuggets.com/2022/02/data-science-definition-humor.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Data Science Projects to Learn 5 Critical Data Science Skills](https://www.kdnuggets.com/2022/03/5-data-science-projects-learn-5-critical-data-science-skills.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
