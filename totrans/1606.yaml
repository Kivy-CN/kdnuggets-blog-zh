- en: Using Confusion Matrices to Quantify the Cost of Being Wrong
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2018/10/confusion-matrices-quantify-cost-being-wrong.html](https://www.kdnuggets.com/2018/10/confusion-matrices-quantify-cost-being-wrong.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: There are so many confusing and sometimes even counter-intuitive concepts in
    statistics.  I mean, come on…even explaining the differences between Null Hypothesis
    and Alternative Hypothesis can be an ordeal.  All I want to do is to understand
    and quantify the cost of my analytical models being wrong.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s say that I’m a shepherd who has bad eyesight and have a
    hard time distinguishing between a wolf and a sheep dog.  That’s obviously a bad
    trait, because the costs of being wrong are very expensive:'
  prefs: []
  type: TYPE_NORMAL
- en: There are times when I confuse a wolf for a sheep dog and take no action, which
    results in loss of sheep.  The incident costs me $2,000 per occurrence and happens
    10% of the time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatively, there are times when I confuse a sheep dog for a wolf and accidently
    kill the sheep dog, which results in the flock being unprotected. The incident
    costs me $5,000 per occurrence and happens 5% of the time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Okay, so I’m not a very good shepherd, but I am a very sophisticated shepherd
    and I’ve build a Neural Network application to distinguish a sheep dog from a
    wolf. Through much training of the “Wolf Detection” neural network, I now have
    a tool that can correctly distinguish a sheep dog from a wolf with 95% accuracy
    (see Figure 1).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/d97bc8d1ec1359a4ca9b25acc6abec52.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 1: Source: “Why Deep Learning Is Suddenly Changing Your Life”** [**(http://fortune.com/ai-artificial-intelligence-deep-machine-learning/?xid=for_em_sh)**](http://fortune.com/ai-artificial-intelligence-deep-machine-learning/?xid=for_em_sh)'
  prefs: []
  type: TYPE_NORMAL
- en: Okay, that seems pretty great, but is 95% accuracy *good enough* given the costs
    of False Positives and False Negatives? Shouldn’t I invest more time and effort
    to improve that accuracy percentage to ensure that my model is “profitable;” quantify
    that 5% inaccuracy which is making my analytical model wrong?
  prefs: []
  type: TYPE_NORMAL
- en: Enter the Confusion Matrix (if there was ever an accurate description of something,
    this name nails it).
  prefs: []
  type: TYPE_NORMAL
- en: '**Understanding the Confusion Matrix**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So how does one go about quantifying the costs of being wrong using the Confusion
    Matrix? That is, determining if a model that correctly predicts with, for instance,
    95% accuracy is *good enough* given the business situation and the costs associated
    with being wrong.
  prefs: []
  type: TYPE_NORMAL
- en: The terms ‘true condition’ (‘positive outcome’) and ‘predicted condition’ (‘negative
    outcome’) are used when discussing Confusion Matrices.  This means that you need
    to understand the differences (and eventually the costs associated) with Type
    I and Type II Errors.
  prefs: []
  type: TYPE_NORMAL
- en: Type I Error (or False Positive) is a result that indicates that a given condition
    is present when it actually is not present. In our shepherd example, that would
    be incorrectly identifying the animal as a wolf when in reality it is a dog.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Type II Error (or False Negative) is a result that indicates that a given condition
    is not present when it actually is present.  In our shepherd example, that would
    be incorrectly identifying the animal as a dog when in reality it is a wolf.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, let’s set up our Confusion Matrix for testing the condition: “Is that
    animal in the grove a wolf?” The Positive Condition is “The Animal is a Wolf”
    in which case I’d take the appropriate action (probably wouldn’t try to pet it).
     Below is the 2x2 Confusion Matrix for our use case.'
  prefs: []
  type: TYPE_NORMAL
- en: '| True Condition |'
  prefs: []
  type: TYPE_TB
- en: '| Predicted Condition |  | True (Wolf) | False (Dog) |'
  prefs: []
  type: TYPE_TB
- en: '| True (Wolf) | TP | FP |'
  prefs: []
  type: TYPE_TB
- en: '| False (Dog) | FN | TN |'
  prefs: []
  type: TYPE_TB
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: True Positive (TP) where the True Condition was wolf and the model accurately
    predicted a wolf.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: True Negative (TN) where the True Condition was a dog and the model accurately
    predicted a dog.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: False Positive (FP) which is a Type I Error where the True Condition was a dog
    and the model inaccurately predicted a wolf (so that I accidently shoot the dog
    protecting the sheep).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: False Negative (FN) which is a Type II Error where the True Condition is a wolf
    but the model inaccurately predicted a dog (so that I ignore the wolf and it feasts
    on the sheep smorgasbord).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So once the neural network model produces the Confusion Model that covers all
    four of the above conditions in the 2x2 matrix, we can calculate goodness of fit
    and effectiveness measures, such as model Precision, Sensitivity and Specificity.
  prefs: []
  type: TYPE_NORMAL
- en: '| True Condition |'
  prefs: []
  type: TYPE_TB
- en: '| Predicted Condition | Cell Probabilities | True (Wolf) | False (Dog) |'
  prefs: []
  type: TYPE_TB
- en: '| True (Wolf) | Precision TP / (TP + FP) | FP / (TP + FP) |'
  prefs: []
  type: TYPE_TB
- en: '| False (Dog) | FN /(TN + FN) | TN / (TN + FN) |'
  prefs: []
  type: TYPE_TB
- en: '|  | Recall / Sensitivity TP / (TP + FN) | Specificity TN / (FP + TN) |'
  prefs: []
  type: TYPE_TB
- en: The Confusion Matrix can then be used to create the following measures of goodness
    of fit and model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Precision = TP / (TP + FP)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall or Sensitivity = TP / (TP + FN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specificity = TN / (FP + TN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy = (TP + TN) / (TP + FP + TN + FN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Putting the Confusion Matrix to Work**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now let’s get back to our shepherd example.  We want to determine the costs
    of the model being wrong, or the savings the neural network provides.  We need
    to determine if the there is sufficient improvement in what the model provides
    over what the shepherd already does himself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before the Wolf Detection application, I (as the shepherd) had the following
    Confusion Matrix where:'
  prefs: []
  type: TYPE_NORMAL
- en: False Positives 10% of the time, where he mistakes a wolf for a sheep dog, took
    no action, and the wolf causes $2,000 of damage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: False Negatives 5% of the time, where he mistakes a sheep dog for a wolf, accidently
    kill the sheep dog and leaves the flock unprotected which $5,000 of damage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Without Wolf Detection Application |'
  prefs: []
  type: TYPE_TB
- en: '| True Condition |'
  prefs: []
  type: TYPE_TB
- en: '| Predicted Condition | 5,000 Observations | True (Wolf) | False (Dog) |'
  prefs: []
  type: TYPE_TB
- en: '| True (Wolf) | True Positive = 75% | False Positive = 10% |'
  prefs: []
  type: TYPE_TB
- en: '| False (Dog) | False Negative = 5% | True Negative = 10% |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'By the previous definitions, the corresponding metrics without using the Wolf
    Detection Application are:'
  prefs: []
  type: TYPE_NORMAL
- en: Precision = 88%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall / Sensitivity = 94%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specificity = 50%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy = 85%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now using the Wolf Detection application, we get the below Confusion Matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '| With Wolf Detection Application |'
  prefs: []
  type: TYPE_TB
- en: '| True Condition |'
  prefs: []
  type: TYPE_TB
- en: '| Predicted Condition | 5,000 Observations | True (Wolf) | False (Dog) |'
  prefs: []
  type: TYPE_TB
- en: '| True (Wolf) | True Positive = 4,000 No cost'
  prefs: []
  type: TYPE_NORMAL
- en: 4000 / 5000 = 80% | False Positive = 200 Cost per occurrence = $2,000
  prefs: []
  type: TYPE_NORMAL
- en: 200 / 5000 = 4% |
  prefs: []
  type: TYPE_NORMAL
- en: '| False (Dog) | False Negative = 50 Cost per occurrence = $5,000'
  prefs: []
  type: TYPE_NORMAL
- en: 50 / 5000 = 1% | True Negative = 750 No cost
  prefs: []
  type: TYPE_NORMAL
- en: 750 / 5000 = 15% |
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Confusion Matrix metrics with using the Wolf Detection Application are:'
  prefs: []
  type: TYPE_NORMAL
- en: Precision = 95%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall/Sensitivity = 99%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specificity = 79%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy = 95%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bringing this all together into a single table:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Without Wolf Detection App | With Wolf Detection App | Improvement | %
    Improvement |'
  prefs: []
  type: TYPE_TB
- en: '| Precision | 88% | 95% | 7 points | 8.0% |'
  prefs: []
  type: TYPE_TB
- en: '| Recall/Sensitivity | 94% | 99% | 5 points | 5.3% |'
  prefs: []
  type: TYPE_TB
- en: '| Specificity | 50% | 79% | 29 points | 58.0% |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | 85% | 95% | 10 points | 11.8% |'
  prefs: []
  type: TYPE_TB
- en: 'Return on Investment then equals:'
  prefs: []
  type: TYPE_NORMAL
- en: Reduction in False Positives from 10% to 4% (which saves $2,000 per occurrence)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduction in False Negatives from 5% to 1% (which saves $5,000 per occurrence)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Finally, the Expected Value Per Prediction (EvP) =**'
  prefs: []
  type: TYPE_NORMAL
- en: = ($2000 * Change in FP%) + ($5000 * Change in FN%)
  prefs: []
  type: TYPE_NORMAL
- en: = ($2000*.06) + ($5000*.04)
  prefs: []
  type: TYPE_NORMAL
- en: '**= $320 average savings per night**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Not all Type I and Type II errors are of equal value.  One needs to invest the
    time to understand the costs of Type I and Type II errors in relationship to your
    specific case.  The real challenge is determining whether the improvement in performance
    from the analytic model is “good enough.” The Confusion Matrix can help us make
    that determination.
  prefs: []
  type: TYPE_NORMAL
- en: And if folks are still struggling with the concept of Type I and Type II errors,
    I hope the below image can help to clarify the difference.  Hehehe
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/218e6f208abb196df259082becadc01f.png)'
  prefs: []
  type: TYPE_IMG
- en: Special thanks to Larry Berk, one of my Senior Data Scientists, for his guidance
    on this blog. He still understands the use of Confusion Matrices much better than
    me!
  prefs: []
  type: TYPE_NORMAL
- en: '**Sources:**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: “[Simple Guide to Confusion Matrix Terminology](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)”
  prefs: []
  type: TYPE_NORMAL
- en: “[Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix)” from Wikipedia
    (by the way, I did make a [donation to Wikipedia](https://donate.wikimedia.org/w/index.php?title=Special:LandingPage&country=XX&uselang=en&utm_medium=sidebar&utm_source=donate&utm_campaign=C13_en.wikipedia.org).
    They are a valuable source of information for these sorts of topics).
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[A Winning Game Plan For Building Your Data Science Team](/2018/09/winning-game-plan-building-data-science-team.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What’s the Difference Between Data Integration and Data Engineering?](/2018/06/difference-between-data-integration-data-engineering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Great Data Scientists Don’t Just Think Outside the Box, They Redefine the
    Box](/2018/03/great-data-scientists-think-outside-redefine-box.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Idiot''s Guide to Precision, Recall, and Confusion Matrix](https://www.kdnuggets.com/2020/01/guide-precision-recall-confusion-matrix.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Visualizing Your Confusion Matrix in Scikit-learn](https://www.kdnuggets.com/2022/09/visualizing-confusion-matrix-scikitlearn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Confusion Matrix, Precision, and Recall Explained](https://www.kdnuggets.com/2022/11/confusion-matrix-precision-recall-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, November 16: How LinkedIn Uses Machine Learning •…](https://www.kdnuggets.com/2022/n45.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How I Got 4 Data Science Offers and Doubled My Income 2 Months…](https://www.kdnuggets.com/2021/01/data-science-offers-doubled-income-2-months.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Challenges of Being a Data Scientist](https://www.kdnuggets.com/2022/02/data-scientist-challenges.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
