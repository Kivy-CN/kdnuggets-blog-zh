- en: Interpretability is crucial for trusting AI and machine learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释性对于信任AI和机器学习至关重要
- en: 原文：[https://www.kdnuggets.com/2018/11/interpretability-trust-ai-machine-learning.html](https://www.kdnuggets.com/2018/11/interpretability-trust-ai-machine-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/11/interpretability-trust-ai-machine-learning.html](https://www.kdnuggets.com/2018/11/interpretability-trust-ai-machine-learning.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Ilknur Kaynar Kabul](https://blogs.sas.com/content/author/ilknurkaynarkabul/),
    SAS**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：[Ilknur Kaynar Kabul](https://blogs.sas.com/content/author/ilknurkaynarkabul/)，SAS**。'
- en: As [machine learning](https://www.sas.com/en_us/webinars/implementing-ai-systems.html) takes
    its place in many recent advances in science and technology, the interpretability
    of machine learning models grows in importance.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 随着[machine learning](https://www.sas.com/en_us/webinars/implementing-ai-systems.html)在科学和技术领域的许多最新进展中占据了重要地位，机器学习模型的可解释性变得越来越重要。
- en: 'We are surrounded with applications powered by machine learning, and we’re
    personally affected by the decisions made by machines more and more every day.
    From the mundane to the lifesaving, we ask machine learning models for answers
    to questions like:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们周围充斥着由机器学习驱动的应用程序，机器做出的决定每天都越来越多地影响着我们个人。从平凡到挽救生命，我们向机器学习模型提出诸如：
- en: What song will I enjoy?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我会喜欢哪首歌？
- en: Will I be able to get a loan?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我能获得贷款吗？
- en: Who should I hire?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我应该雇用谁？
- en: What is the probability of me having cancer?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我得癌症的概率是多少？
- en: These and many other questions are answered by predictive models that most users
    know very little about. Data scientists often put emphasis on the prediction accuracy
    of their models – not on understanding *how* those predictions are actually made.
    With machine learning, the models just do it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这些以及许多其他问题由大多数用户了解甚少的预测模型来回答。数据科学家通常强调模型的预测准确性——而不是理解*如何*这些预测实际上是如何做出的。使用机器学习，模型只是这样做的。
- en: '**Complex models are harder to understand**'
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**复杂模型更难以理解**'
- en: Some machine learning models are simple and easy to understand. We know how
    changing the inputs will affect the predicted outcome and can make justification
    for each prediction. However, with the recent advances in machine learning and [artificial
    intelligence](https://www.sas.com/en_us/insights/analytics/what-is-artificial-intelligence.html),
    models have become very complex, including complex deep neural networks and ensembles
    of different models. We refer to these complex models as black box models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习模型简单且易于理解。我们知道改变输入将如何影响预测结果，并且可以对每个预测做出解释。然而，随着机器学习和[人工智能](https://www.sas.com/en_us/insights/analytics/what-is-artificial-intelligence.html)的最新进展，模型变得非常复杂，包括复杂的深度神经网络和不同模型的集成。我们将这些复杂模型称为黑箱模型。
- en: Unfortunately, the complexity that gives extraordinary predictive abilities
    to black box models also makes them very difficult to understand and trust. The
    algorithms inside the black box models do not expose their secrets. They don't,
    in general, provide a clear explanation of why they made a certain prediction.
    They just give us a probability, and they are opaque and hard to interpret. Sometimes
    there are thousands (even millions) of model parameters, there’s no one-to-one
    relationship between input features and parameters, and often combinations of
    multiple models using many parameters affect the prediction. Some of them are
    also data-hungry. They need enormous amounts of data to achieve high accuracy.
    It’s hard to figure out what they learned from those data sets and which of those
    data points have more influence on the outcome than the others.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，使黑箱模型具备卓越预测能力的复杂性也使它们非常难以理解和信任。黑箱模型内部的算法不会透露其秘密。它们通常不会提供明确的解释说明为何做出某个预测。它们只是给出一个概率，而且这些模型不透明且难以解释。有时模型参数多达数千（甚至数百万），输入特征与参数之间没有一一对应的关系，多个模型的组合以及许多参数常常影响预测结果。有些模型也对数据需求量大，需要大量数据才能达到高准确度。很难弄清楚它们从这些数据集中学到了什么，以及哪些数据点对结果的影响大于其他数据点。
- en: '![](../Images/27cac00146774e160aab7f1e91db0585.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27cac00146774e160aab7f1e91db0585.png)'
- en: Due to all those reasons, it’s very difficult to understand the process and
    the outcomes from those techniques. It’s also difficult to figure out whether
    we can trust the models and whether we can make fair decisions when using them.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，很难理解这些技术的过程和结果。弄清楚我们是否可以信任这些模型以及在使用它们时是否能够做出公平的决策也很困难。
- en: What happens if they learn the wrong thing? What happens if they are not ready
    for deployment? There is a risk of misrepresentation, oversimplification or overfitting.
    Thus, we need to be careful when we use them, and we'd better understand how those
    models work.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果他们学到错误的东西会发生什么？如果他们还没有准备好部署会发生什么？存在错误表述、过度简化或过拟合的风险。因此，我们在使用这些模型时需要小心，我们最好了解这些模型如何工作。
- en: '**Why accuracy is not enough**'
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**为什么准确性不够**'
- en: In machine learning, accuracy is measured by comparing the output of a machine
    learning model to the known actual values from the input data set.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，准确性是通过将机器学习模型的输出与输入数据集中的已知实际值进行比较来衡量的。
- en: A model can achieve high accuracy by memorizing the unimportant features or
    patterns in your data set. If there is a bias in your input data set, this can
    also affect your model. In addition, the data in the training environment may
    not be a good representation of the data in the production environment in which
    the model is deployed. Even if it is sufficiently representative initially, if
    we consider that the data in the production environment is not stationary, it
    can become outdated very quickly.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型可以通过记住数据集中的不重要特征或模式来实现高准确性。如果输入数据集存在偏差，也会影响模型。此外，训练环境中的数据可能无法很好地代表模型部署后生产环境中的数据。即使最初足够具有代表性，如果考虑到生产环境中的数据不是静态的，它也可能很快变得过时。
- en: Thus, we cannot rely only on the prediction accuracy achieved for a specific
    data set. We need to know more. We need to demystify the black box machine learning
    models and improve transparency and interpretability to make them more trustworthy
    and reliable.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们不能仅仅依赖于特定数据集的预测准确性。我们需要了解更多。我们需要揭开黑箱机器学习模型的神秘面纱，提高透明度和解释性，使其更加可信赖和可靠。
- en: '**What is interpretability?**'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**什么是解释性？**'
- en: 'Interpretability means giving explanations to the end users for a particular
    decision or process. More specifically, it entails:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 解释性意味着为最终用户提供特定决策或过程的解释。更具体来说，它包括：
- en: Understanding the main tasks that affect the outcomes.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解影响结果的主要任务。
- en: Explaining the decisions that are made by an algorithm.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释算法做出的决策。
- en: Finding out the patterns/rules/features that are learned by an algorithm.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现算法学习到的模式/规则/特征。
- en: Being critical about the results.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对结果进行批判性分析。
- en: Exploring the unknown unknowns for your algorithm.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索算法的未知未知。
- en: It is not about understanding every detail about how a model works for each
    data point in the training data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是关于理解模型如何处理训练数据中每一个数据点的每一个细节。
- en: '**Why do we need interpretability?**'
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**我们为什么需要解释性？**'
- en: 'Interpretability is important to different people for different reasons:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 解释性对不同的人有不同的重要性：
- en: '![](../Images/9d4d150f43117f3b6601f2a86b4bc243.png)**Data scientists** want
    to build models with high accuracy. They want to understand the details to find
    out how they can pick the best model and improve that model. They also want to
    get insights from the model so that they can communicate their findings to their
    target audience.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/9d4d150f43117f3b6601f2a86b4bc243.png)**数据科学家**希望建立具有高准确性的模型。他们希望了解细节，以找出如何选择最佳模型并改进该模型。他们还希望从模型中获得见解，以便将他们的发现传达给目标受众。'
- en: '**End users** want to know why a model gives a certain prediction. They want
    to know how they will be affected by those decisions. They want to know whether
    they are being treated fairly and whether they need to object to any decision.
    They want to have a certain measure of trust when they are shopping online, or
    clicking ads on the web.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**最终用户**想知道模型为什么给出某个预测。他们想了解这些决策将如何影响他们。他们想知道自己是否被公平对待，以及是否需要对任何决策提出异议。他们希望在网上购物或点击网页广告时有一定程度的信任。'
- en: '**Regulators** and lawmakers want to make the system fair and transparent.
    They want to protect consumers. With the inevitable rise of machine learning algorithms,
    they are becoming more concerned about the decisions made by models.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**监管者**和立法者希望使系统公平和透明。他们希望保护消费者。随着机器学习算法的不可避免的崛起，他们对模型做出的决策越来越感到担忧。'
- en: All those users want similar things from the black box models. They want them
    to be transparent, trustworthy and explainable.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些用户对黑箱模型有类似的期望。他们希望模型是透明的、可信赖的并且可以解释的。
- en: '**Transparent**: The system can explain how it works and/or why it gives certain
    predictions'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**透明**：系统可以解释它是如何工作的和/或为什么给出某些预测。'
- en: '**Trustworthy**: The system can handle different scenarios in the real world
    without continuous control.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**可信赖性**：系统能够在现实世界的不同场景中处理问题，而无需持续控制。'
- en: '**Explainable**: The system can convey useful information about its inner workings,
    for the patterns that it learns and for the results that it gives.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**可解释性**：系统能够传达有关其内部工作原理的有用信息，包括其学习的模式和给出的结果。'
- en: In a typical machine learning pipeline, we have control over the data set used
    to train the model, we have control over the model that we use, and we have control
    over how we assess and deploy those models.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的机器学习流程中，我们对用于训练模型的数据集有控制权，对我们使用的模型有控制权，并且对如何评估和部署这些模型有控制权。
- en: '**When is interpretability needed?**'
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**何时需要可解释性？**'
- en: 'If you need interpretability, first you need to ask yourself why you need it.
    In which stage of this process do you need interpretability? It may not be necessary
    to understand how a model makes its predictions for every application. However,
    you may need to know it if those predictions are used for high-stakes decisions.
    After you define your purpose, you should focus on what techniques you need in
    which stage of the process:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要可解释性，首先需要问自己为什么需要它。在这个过程中，你在哪个阶段需要可解释性？并不是每种应用都需要理解模型如何做出预测。然而，如果这些预测用于高风险决策，你可能需要了解。如果你定义了目的，你应该关注在过程的哪个阶段需要哪些技术：
- en: '**Interpretability in pre-modeling (interpretability of model inputs):**Understanding
    your data set is very important before you start building models. You can use
    different exploratory data analysis and visualization techniques to have a better
    understanding of your data set. This can include summarizing the main characteristics
    of your data set, finding representative or critical points in your data set,
    and finding the relevant features from your data set. After you have an overall
    understanding of your data set, you need to think about which features you are
    going to use in modeling. If you want to explain the input-output relationship
    after you do modeling, you need to start with meaningful features. While highly
    engineered features (such as those obtained from t-sne, random projections, etc.)
    can boost the accuracy of your model, they will not be interpretable when you
    put the model to use.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**建模前的可解释性（模型输入的可解释性）**：在开始构建模型之前，理解数据集非常重要。你可以使用不同的探索性数据分析和可视化技术，以更好地理解数据集。这可能包括总结数据集的主要特征、找到数据集中的代表性或关键点，以及从数据集中找出相关特征。在对数据集有了整体理解后，你需要考虑在建模时将使用哪些特征。如果你想在建模后解释输入输出关系，你需要从有意义的特征开始。虽然高度工程化的特征（例如，从
    t-sne、随机投影等获得的特征）可以提高模型的准确性，但在模型实际应用时，它们可能不可解释。'
- en: '**Interpretability in modeling:**We can categorize models as white box (transparent)
    and black box (opaque) models based on their simplicity, transparency and explainability.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**建模中的可解释性**：我们可以根据模型的简单性、透明度和解释性将模型分为白箱（透明）模型和黑箱（不透明）模型。'
- en: '**White box (transparent) models**: Decision trees, rule-lists, and regression
    algorithms are usually considered in this category. These models are easy to understand
    when used with few predictors. They use interpretable transformations and give
    you more intuition about how things work, which helps you understand what’s going
    on in the model. You can explain them to a technical audience. But of course,
    if you have hundreds of features and you build a very deep, large decision tree,
    things can still become complicated and uninterpretable.'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**白箱（透明）模型**：决策树、规则列表和回归算法通常被认为属于这一类别。这些模型在使用少量预测变量时易于理解。它们使用可解释的转换，并能提供关于如何工作的更多直观感受，这有助于你理解模型中的情况。你可以向技术观众解释它们。但当然，如果你有数百个特征并构建了一个非常深且大的决策树，事情仍然可能变得复杂和难以解释。'
- en: '**Black box (opaque) models:**Deep neural networks, random forests, and gradient
    boosting machines can be considered in this category. They usually use many predictors
    and complex transformations. Some of them have many parameters. It’s usually hard
    to visualize and understand what is going on inside these models. They’re harder
    to communicate to a target audience. However, their prediction accuracy can be
    much better than other models. Recent research in this area hopes to make these
    models more transparent. Some of that research includes techniques that are part
    of the training process. Generating explanations in addition to the predictions
    is one way to improve transparency in these models. Another improvement is to
    include visualization of features after the training process.'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**黑箱（不透明）模型：** 深度神经网络、随机森林和梯度提升机可以被视为这一类别的模型。它们通常使用许多预测变量和复杂的转换，有些模型有很多参数。通常很难可视化和理解这些模型内部的工作情况，也更难以向目标受众进行沟通。然而，它们的预测准确性通常比其他模型要好。近期在这一领域的研究希望使这些模型更加透明。一些研究包括作为训练过程一部分的技术。在这些模型中生成解释是提高透明度的一种方式。另一种改进是包含训练过程后的特征可视化。'
- en: '**Interpretability in post-modeling (post hoc interpretability):**Interpretability
    in the model predictions helps us to inspect the dynamics between input features
    and output predictions. Some post-modeling activities are model-specific, while
    the others are model-agnostic. Adding interpretability at this phase can help
    us to understand the most important features for a model, how those features affect
    the predictions, how each feature contributes to the prediction and how sensitive
    your model is to certain features. There are model-agnostic techniques such as
    Partial Dependence (PD) plots, Individual Conditional Expectation (ICE) plots
    and Local Interpretable Model-Agnostic Explanations (LIME), in addition to the
    model-specific techniques, such as variable importance output from Random Forest.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**后建模可解释性（post hoc interpretability）：** 模型预测中的可解释性帮助我们检查输入特征与输出预测之间的动态关系。某些后建模活动是特定于模型的，而其他则是模型无关的。在这个阶段添加可解释性可以帮助我们理解模型中最重要的特征，这些特征如何影响预测，每个特征对预测的贡献，以及模型对某些特征的敏感性。除了模型特定的技术，如随机森林的变量重要性输出，还有模型无关的技术，例如部分依赖（PD）图、个体条件期望（ICE）图和局部可解释模型无关解释（LIME）。'
- en: Stay tuned for more posts on this topic. [In this blog series](https://blogs.sas.com/content/tag/interpretability/),
    we’ll go into more detail about some of the interpretability techniques. We’ll
    explain how you can use model-agnostic interpretability techniques to understand
    black box models. We’ll also cover some of the recent advances in interpretability.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 敬请关注关于这个话题的更多帖子。[在这系列博客中](https://blogs.sas.com/content/tag/interpretability/)，我们将详细介绍一些可解释性技术。我们将解释如何使用模型无关的可解释性技术来理解黑箱模型。我们还会涵盖一些最新的可解释性进展。
- en: '**Bio**: [Ilknur Kaynar Kabul](https://blogs.sas.com/content/author/ilknurkaynarkabul/)
    is a Senior Manager in the SAS Advanced Analytics division, where she leads the
    SAS R&D team that focuses on machine learning algorithms and applications.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介**：[Ilknur Kaynar Kabul](https://blogs.sas.com/content/author/ilknurkaynarkabul/)
    是SAS高级分析部门的高级经理，她领导着SAS研发团队，专注于机器学习算法和应用。'
- en: '[Original](https://blogs.sas.com/content/subconsciousmusings/2017/12/18/interpretability-crucial-trusting-ai-machine-learning/).
    Reposted with permission.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://blogs.sas.com/content/subconsciousmusings/2017/12/18/interpretability-crucial-trusting-ai-machine-learning/)。经许可转载。'
- en: '**Resources:**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：**'
- en: '[On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning
    education](https://www.kdnuggets.com/education/online.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在线和基于网络的：分析、数据挖掘、数据科学、机器学习教育](https://www.kdnuggets.com/education/online.html)'
- en: '[Software for Analytics, Data Science, Data Mining, and Machine Learning](https://www.kdnuggets.com/software/index.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分析、数据科学、数据挖掘和机器学习的软件](https://www.kdnuggets.com/software/index.html)'
- en: '**Related:**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Using Uncertainty to Interpret your Model](https://www.kdnuggets.com/2018/11/using-uncertainty-interpret-model.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用不确定性来解释你的模型](https://www.kdnuggets.com/2018/11/using-uncertainty-interpret-model.html)'
- en: '[5 Machine Learning Projects You Should Not Overlook, June 2018](https://www.kdnuggets.com/2018/06/5-machine-learning-projects-overlook-jun-2018.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5 个你不应忽视的机器学习项目，2018年6月](https://www.kdnuggets.com/2018/06/5-machine-learning-projects-overlook-jun-2018.html)'
- en: '[Interpreting Machine Learning Models: An Overview](https://www.kdnuggets.com/2017/11/interpreting-machine-learning-models-overview.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习模型的解释：概述](https://www.kdnuggets.com/2017/11/interpreting-machine-learning-models-overview.html)'
- en: '* * *'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT 工作'
- en: '* * *'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[3 Crucial Challenges in Conversational AI Development and How to Avoid Them](https://www.kdnuggets.com/3-crucial-challenges-in-conversational-ai-development-and-how-to-avoid-them)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[对话式 AI 开发中的 3 个关键挑战及如何避免它们](https://www.kdnuggets.com/3-crucial-challenges-in-conversational-ai-development-and-how-to-avoid-them)'
- en: '[Big Data Analytics: Why Is It So Crucial For Business Intelligence?](https://www.kdnuggets.com/2023/06/big-data-analytics-crucial-business-intelligence.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大数据分析：为什么它对商业智能如此关键？](https://www.kdnuggets.com/2023/06/big-data-analytics-crucial-business-intelligence.html)'
- en: '[5 Crucial Steps to Develop an Effective Coding Routine](https://www.kdnuggets.com/2023/08/5-crucial-steps-develop-effective-coding-routine.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[制定有效编码例程的 5 个关键步骤](https://www.kdnuggets.com/2023/08/5-crucial-steps-develop-effective-coding-routine.html)'
- en: '[Using SHAP Values for Model Interpretability in Machine Learning](https://www.kdnuggets.com/2023/08/shap-values-model-interpretability-machine-learning.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 SHAP 值进行机器学习模型可解释性分析](https://www.kdnuggets.com/2023/08/shap-values-model-interpretability-machine-learning.html)'
- en: '[Simplifying Decision Tree Interpretability with Python & Scikit-learn](https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 和 Scikit-learn 简化决策树的可解释性](https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html)'
- en: '[A Solid Plan for Learning Data Science, Machine Learning, and Deep Learning](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学、机器学习和深度学习的可靠计划](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
