- en: How to Package and Distribute Machine Learning Models with MLFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/08/package-distribute-machine-learning-models-mlflow.html](https://www.kdnuggets.com/2022/08/package-distribute-machine-learning-models-mlflow.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: One of the fundamental activities during each stage of the ML model life cycle
    development is collaboration. Taking an ML model from its conception to deployment
    requires participation and interaction between different roles involved in constructing
    the model. In addition, the nature of ML model development involves experimentation,
    tracking of artifacts and metrics, model versions, etc., which demands an effective
    organization for the correct maintenance of the ML model life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there are tools for developing and maintaining a model’s life cycle,
    such as [MLflow](https://mlflow.org/). In this article, we will break down MLflow,
    its main components, and its characteristics. We’ll also offer examples showing
    how MLflow works in practice.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: What is MLflow?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLflow is an open-source tool for the development, maintenance, and collaboration
    in each phase of the life cycle of an ML model. Furthermore, MLflow is a framework-agnostic
    tool, so any ML / DL framework can quickly adapt to the ecosystem that MLflow
    proposes.
  prefs: []
  type: TYPE_NORMAL
- en: MLflow emerges as a platform that offers tools for tracking metrics, artifacts,
    and metadata. It also provides standard formats for packaging, distributing, and
    deploying models and projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'MLflow also offers tools for managing model versions. These tools are encapsulated
    in its four main components:'
  prefs: []
  type: TYPE_NORMAL
- en: MLflow Tracking,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLflow Projects,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLflow Models and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLflow Registry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLflow Tracking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[MLflow Tracking](https://mlflow.org/docs/latest/tracking.html) is an API-based
    tool for logging metrics, parameters, model versions, code versions, and files.
    MLflow Tracking is integrated with a UI for visualizing and managing artifacts,
    models, files, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: Each MLflow Tracking session is organized and managed under the concept of runs.
    A run refers to the execution of code where the artifact log is performed explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: MLflow Tracking allows you to generate runs through MLflow’s [Python](https://www.mlflow.org/docs/latest/python_api/index.html),
    [R](https://www.mlflow.org/docs/latest/R-api.html), [Java](https://www.mlflow.org/docs/latest/java_api/index.html),
    and [REST APIs](https://www.mlflow.org/docs/latest/rest-api.html). By default,
    the runs are stored in the directory where the code session is executed. However,
    MLflow also allows storing artifacts on a local or remote server.
  prefs: []
  type: TYPE_NORMAL
- en: MLflow Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[MLflow Models](https://mlflow.org/docs/latest/models.html) allow packaging
    machine learning models in a standard format to be consumed directly through different
    services such as REST API, Microsoft Azure ML, Amazon SageMaker, or Apache Spark.
    One of the advantages of the MLflow Models convention is that the packaging is
    multi-language or multi-flavor.'
  prefs: []
  type: TYPE_NORMAL
- en: For packaging, MLflow generates a directory with two files, the model and a
    file that specifies the packaging and loading details of the model. For example,
    the following code snippet shows an MLmodel file where the flavor loader is specified
    as well as the `conda.yaml` file  that defines the environment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: MLflow Project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[MLflow Projects](https://www.mlflow.org/docs/latest/projects.html) provides
    a standard format for packaging, sharing, and reusing machine learning projects.
    Each project can be a remote repository or a local directory. Unlike MLflow Models,
    MLflow Projects aims at the portability and distribution of machine learning projects.'
  prefs: []
  type: TYPE_NORMAL
- en: An MLflow Project is defined by a YAML manifest called `MLProject`, where the
    project’s specifications are exposed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key features for the implementation of the model are specified in the MLProject
    file. These include:'
  prefs: []
  type: TYPE_NORMAL
- en: the input parameters that the model receives,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the data type of the parameters,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the command for executing the model, and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the environment in which the project runs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following code snippet shows an example of an MLProject file where the model
    to implement is a decision tree whose only parameter is the depth of the tree
    and whose default value is 2.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Likewise, MLflow provides a CLI to run projects located on a local server or
    a remote repository. The following code snippet shows an example of how a project
    is run from a local server or a remote repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In both examples, the environment will be generated based on the `MLProject
    file` specification. The command that triggers the model will be executed under
    the arguments passed on the command line. Since the model allows input parameters,
    these are assigned through the `-P` flag. In both examples, the model parameter
    refers to the maximum depth of the decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: By default, a run like the one shown in the example will store the artifacts
    in the `.mlruns` directory.
  prefs: []
  type: TYPE_NORMAL
- en: How to Store Artifacts in an MLflow Server?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most common use cases when implementing MLflow is using MLflow Server
    to log metrics and artifacts. The MLflow Server is responsible for managing the
    artifacts and files generated by an MLflow Client. These artifacts can be stored
    in different schemes, from a file directory to a remote database. For example,
    to run an MLflow Server locally, we type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The above command will start an MLflow service through the IP address http://127.0.0.1:5000/.
    To store artifacts and metrics, the tracking URI of the server is defined in a
    client session
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code snippet, we will see the basic implementation of artifact
    storage in an MLflow Server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `MLflow.set_tracking_uri ()` command sets the location of the server.
  prefs: []
  type: TYPE_NORMAL
- en: How to Run Authentication in an MLflow Server?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Exposing a  server with no authentication can be risky. Therefore, it is convenient
    to add authentication. Authentication will depend on the ecosystem in which you
    will deploy the server:'
  prefs: []
  type: TYPE_NORMAL
- en: on a local server, it is enough to add a basic authentication based on *user*
    and *password*,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: on a remote server, credentials must be adjusted coupled with respective proxies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For illustration, let's look at an example of an MLflow Server deployed with
    basic authentication (*username* and *password*). We will also see how to configure
    a client to make use of this server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: MLflow Server authentication'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we apply basic user and password authentication to the MLflow
    Server through an Nginx reverse proxy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the installation of Nginx, which we can do in the following
    way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: For Windows OS, you have to use the native Win32 API. Please follow the detailed
    instructions [here](https://nginx.org/en/docs/windows.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once installed, we will proceed to generate a user with its respective password
    using the `htpasswd` command, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The above command generates credentials for the user `mlflow-user` in the `.htpasswd`
    file of the nginx service. Later, to define the proxy under the created user credentials,
    the configuration file `/usr/local/etc/nginx/nginx.conf` is modified, which by
    default has the following content: :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'which has to look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We are defining an authentication proxy for localhost through port 5000\. This
    is the IP address and port number where MLflow Server is deployed by default.
    When using a cloud provider, you must configure the credentials and proxies necessary
    for the implementation. Now initialize the MLflow server as shown in the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: When trying to access http://localhost in the browser, authentication will be
    requested through the username and password created.
  prefs: []
  type: TYPE_NORMAL
- en: '![Login](../Images/75f7e933ab09a0f7fb49dc4d8a1bcbf1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Login
  prefs: []
  type: TYPE_NORMAL
- en: Once you have entered the credentials, you will be directed to the MLflow Server
    UI.
  prefs: []
  type: TYPE_NORMAL
- en: '![MLflow Server UI](../Images/ee8980fc7b14bbdc2fb10610bfc24070.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. MLflow Server UI
  prefs: []
  type: TYPE_NORMAL
- en: 'To store data in MLflow Server from a client, you have to:'
  prefs: []
  type: TYPE_NORMAL
- en: define the environment variables that will contain the credentials to access
    the server and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: set the URI where the artifacts will be stored.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, for the credentials, we are going to export the following environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Once you have defined the environment variables, you only need to define the
    server URI for the artifact storage.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: When executing the code snippet above, we can see the test metric and parameter
    reflect on the server.
  prefs: []
  type: TYPE_NORMAL
- en: '![Metrics and parameters stored from a client service with authentication on
    the server.](../Images/27b97fd9e33290425f2e3d54daec50bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Metrics and parameters stored from a client service with authentication
    on the server.
  prefs: []
  type: TYPE_NORMAL
- en: How to Register an MLflow Model?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the everyday needs when developing machine learning models is to maintain
    order in the versions of the models. For this, MLflow offers the [MLflow Registry](https://mlflow.org/docs/latest/model-registry.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The MLflow Registry is an extension that helps to:'
  prefs: []
  type: TYPE_NORMAL
- en: manage versions of each MLModel and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'record the evolution of each model in three different phases: archive, *staging,*
    and *production. It is* very similar to the git version system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are four alternatives for registering a model:'
  prefs: []
  type: TYPE_NORMAL
- en: through the UI,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as an argument to `MLflow.<flavor> .log_model()`,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: with the `MLflow.register_model()` method or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: with the `create_registered_model()` client API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following example, the model is registered using the `MLflow.<flavor>
    .log_model()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: If it is a new model, MLFlow will initialize it as *Version 1*. If the model
    is already versioned, it will be initialized as *Version 2* (or subsequent version).
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, when registering a model, the assigned status is none. To assign
    a status to a registered model, we can do it in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the above code snippet, *version 2* of the *Decision Tree* model is assigned
    to the *Staging* state. In the server UI, we can see the states as shown in Figure
    4:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Registered Models](../Images/a520e6d37fe1a626f8761a15d3e6ee51.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Registered Models
  prefs: []
  type: TYPE_NORMAL
- en: 'To serve the model we will use the MLflow CLI, for this we only need the server
    URI, the model name, and the model status, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Model is Served and POST Eequests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the previous code snippet, a POST request is made to the address where the
    model is served. An array that contains five elements has been passed in the request,
    which is what the model expects as input data for the inference. The prediction,
    in this case, turned out to be 1.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is important to mention that MLFlow allows defining the data structure
    for inferring in the `MLmodel` file through the implementation of [signatures](https://www.mlflow.org/docs/latest/_modules/mlflow/models/signature.html).
    Likewise, the data passed through the request can be of different types, which
    can be consulted [here](https://www.mlflow.org/docs/latest/_modules/mlflow/models/signature.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The full implementation of the previous example can be found here: [https://github.com/FernandoLpz/MLFlow-example](https://github.com/FernandoLpz/MLFlow-example)'
  prefs: []
  type: TYPE_NORMAL
- en: MLflow Plugins
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Due to the framework-agnostic nature of MLflow, [MLflow Plugins](https://mlflow.org/docs/latest/plugins.html#writing-your-own-mlflow-plugins)
    emerged.  Its primary function is to extend the functionalities of MLflow in an
    adaptive way to different frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: MLflow Plugins allow customization and adaptation of the deployment and storage
    of artifacts for specific platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, there are plugins for a platform-specific deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[MLflow-redisai](https://github.com/RedisAI/mlflow-redisai): which allows the
    creation of deployments to [RedisAI](https://oss.redislabs.com/redisai/) from
    models created and managed in MLFlow,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MLflow-torchserve](https://github.com/mlflow/mlflow-torchserve): which enables
    [PyTorch](https://pytorch.org/) models to be deployed directly to [TorchServe](https://github.com/pytorch/serve),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MLflow-algorithmia](https://github.com/algorithmiaio/mlflow-algorithmia):
    that allows deployment of  models created and managed with MLFlow, to the [Algorithmia](https://algorithmia.com/)
    infrastructure, and'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MLflow-ray-serve](https://github.com/ray-project/mlflow-ray-serve): that supports
    deployment of MLFlow models to the [Ray](https://docs.ray.io/en/master/serve/)
    infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, for the management of MLflow Projects, we have [MLflow-yarn](https://github.com/criteo/mlflow-yarn),
    a plugin for managing MLProjects under a [Hadoop](https://hadoop.apache.org/)
    / [Yarn](https://yarnpkg.com/) backed. For the customization of MLflow Tracking,
    we have [MLflow-elasticsearchstore](https://github.com/criteo/mlflow-elasticsearchstore),
    which allows the management of the MLFlow Tracking extension under an [Elasticsearch](https://www.elastic.co/)
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Likewise, specific plugins are offered for deployment in *AWS* and *Azure*.
    They are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*MLflow.sagemaker*](https://www.mlflow.org/docs/latest/python_api/mlflow.sagemaker.html)
    and'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*MLflow.azureml*](https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is essential to mention that MLflow provides the ability to create and [customize
    plugins](https://www.mlflow.org/docs/latest/plugins.html#writing-your-own-mlflow-plugins)
    according to needs.
  prefs: []
  type: TYPE_NORMAL
- en: MLflow vs. Kubeflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Due to the increasing demand for tools to develop and maintain the life cycle
    of machine learning models, different alternatives such as MLflow and [KubeFlow](https://www.kubeflow.org/)
    have emerged.
  prefs: []
  type: TYPE_NORMAL
- en: As we have already seen throughout this article, MLflow is a tool that allows
    collaboration in developing the life cycle of machine learning models, mainly
    focused on tracking artifacts (MLflow Tracking), collaboration, maintenance, and
    versioning of the project.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, there is KubeFlow, which, like MLflow, is a tool for developing
    machine learning models with some specific differences.
  prefs: []
  type: TYPE_NORMAL
- en: Kubeflow is a platform that works on a [Kubernetes](https://kubernetes.io/)
    cluster; that is, KubeFlow takes advantage of the containerization nature of Kubernetes.
    Also, KubeFlow provides tools such as [KubeFlow Pipelines](https://www.kubeflow.org/docs/components/pipelines/overview/pipelines-overview/),
    which aim to generate and automate pipelines (DAGs) through an SDK extension.
  prefs: []
  type: TYPE_NORMAL
- en: KubeFlow also offers [Katib](https://www.kubeflow.org/docs/components/katib/overview/),
    a tool for optimizing hyperparameters on a large scale and provides a service
    for management and collaboration from Jupyter notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: 'SEO Link: Kubernetes and Kubeflow guides'
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, MLflow is a tool focused on management and collaboration for the
    development of machine learning projects. On the other hand, Kubeflow is a platform
    focused on developing, training, and deploying models through a Kubernetes cluster
    and the use of containers.
  prefs: []
  type: TYPE_NORMAL
- en: Both platforms offer significant advantages and are alternatives for developing,
    maintaining, and deploying machine learning models. However, it is vital to consider
    the barrier to entry for the use, implementation, and integration of these technologies
    in development teams.
  prefs: []
  type: TYPE_NORMAL
- en: Since Kubeflow is linked to a Kubernetes cluster for its implementation and
    integration, it is advisable to have an expert for managing this technology. Likewise,
    developing and configuring pipeline automation is also a challenge that demands
    a learning curve, which under specific circumstances may not be beneficial for
    companies.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, MLflow and Kubeflow are platforms focused on specific stages
    of the life cycle of machine learning models. MLflow is a tool with a collaboration
    orientation, and Kubeflow is more oriented to take advantage of a Kubernetes cluster
    to generate machine learning tasks. However, Kubeflow requires experience in the
    MLOps part. One needs to know about the deployment of services in Kubernetes,
    which can be an issue to consider when trying to approach Kubeflow.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Fernando López](https://www.linkedin.com/in/ferneutron/)** ([GitHub](https://github.com/FernandoLpz))
    is Head of Data Science at Hitch leading a data science team for the development
    and deployment of artificial intelligence models throughout the organization for
    video interview evaluation, candidate profiling and evaluation pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Distribute and Run LLMs with llamafile in 5 Simple Steps](https://www.kdnuggets.com/distribute-and-run-llms-with-llamafile-in-5-simple-steps)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Why Do Machine Learning Models Die In Silence?](https://www.kdnuggets.com/2022/01/machine-learning-models-die-silence.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Working With Sparse Features In Machine Learning Models](https://www.kdnuggets.com/2021/01/sparse-features-machine-learning-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Models Are Rarely Deployed: An Industry-wide Failure in Machine…](https://www.kdnuggets.com/2022/01/models-rarely-deployed-industrywide-failure-machine-learning-leadership.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Advanced Feature Selection Techniques for Machine Learning Models](https://www.kdnuggets.com/2023/06/advanced-feature-selection-techniques-machine-learning-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Machine Learning Models Explained in 5 Minutes](https://www.kdnuggets.com/5-machine-learning-models-explained-in-5-minutes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
