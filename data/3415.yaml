- en: 'Deep Learning Reading Group: Deep Residual Learning for Image Recognition'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习阅读组：图像识别中的深度残差学习
- en: 原文：[https://www.kdnuggets.com/2016/09/deep-learning-reading-group-deep-residual-learning-image-recognition.html](https://www.kdnuggets.com/2016/09/deep-learning-reading-group-deep-residual-learning-image-recognition.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/09/deep-learning-reading-group-deep-residual-learning-image-recognition.html](https://www.kdnuggets.com/2016/09/deep-learning-reading-group-deep-residual-learning-image-recognition.html)
- en: '![](../Images/7de88231876e55a10fbd949387f67285.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7de88231876e55a10fbd949387f67285.png)'
- en: 'Today’s [paper offers a new architecture for Convolution Networks](https://arxiv.org/abs/1512.03385).
    It was written by He, Zhang, Ren, and Sun from Microsoft Research. I’ll warn you
    before we start: this paper is ancient. It was published in the dark ages of deep
    learning sometime at the end of 2015, which I’m pretty sure means its original
    format was papyrus; thankfully someone scanned it so that future generations could
    read it. But it is still worth blowing off the dust and flipping through it because
    the architecture it proposes has been used time and time again, including in [some
    of the papers we have previously read](https://gab41.lab41.org/lab41-reading-group-deep-networks-with-stochastic-depth-564321956729):[Deep
    Networks with Stochastic Depth](https://arxiv.org/abs/1603.09382).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的[论文提出了一种新的卷积网络架构](https://arxiv.org/abs/1512.03385)。这篇论文由微软研究院的He、Zhang、Ren和Sun撰写。我要在开始之前警告你：这篇论文非常古老。它发布于2015年底深度学习的黑暗时代，我相当确定它的原始格式是莎草纸；幸运的是，有人扫描了它，以便未来几代人可以阅读。然而，它仍然值得抹去尘土并翻阅，因为它提出的架构已经被反复使用，包括在[我们之前阅读的一些论文中](https://gab41.lab41.org/lab41-reading-group-deep-networks-with-stochastic-depth-564321956729)：[具有随机深度的深度网络](https://arxiv.org/abs/1603.09382)。
- en: 'He *et al.* begin by noting a seemingly paradoxical situation: very deep networks
    perform more poorly than moderately deep networks, that is, that while adding
    layers to a network generally improves the performance, after some point the new
    layers begin to hinder the network. They refer to this effect as network degradation.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: He *et al.* 开始时指出了一个看似矛盾的情况：非常深的网络表现比中等深度的网络更差，也就是说，尽管增加网络的层数通常会提高性能，但在某些点之后，新层开始阻碍网络。他们将这种效应称为网络降级。
- en: If you have been following our [previous posts](https://gab41.lab41.org/lab41-reading-group-deep-networks-with-stochastic-depth-564321956729) this
    won’t surprise you; training issues like vanishing gradients become worse as networks
    get deeper so you would expect more layers to make the network worse after some
    point. But the authors anticipate this line of reasoning and state that several
    other deep learning methods, like [batch normalization](https://arxiv.org/abs/1502.03167) (see [our
    post](https://gab41.lab41.org/batch-normalization-what-the-hey-d480039a9e3b)for
    a summary), essentially have solved these training issues, and yet the networks
    still perform increasingly poorly as their depth increases. For example, they
    compare 20- and 56-layer networks and find the 56-layer network performs far worse;
    see the image below from their paper.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一直关注我们的[之前的帖子](https://gab41.lab41.org/lab41-reading-group-deep-networks-with-stochastic-depth-564321956729)，这不会让你感到惊讶；随着网络变得更深，诸如梯度消失等训练问题会变得更严重，因此你会期望更多的层会在某个点之后使网络表现变差。但作者预见了这种推理，并指出几种其他深度学习方法，如[批量归一化](https://arxiv.org/abs/1502.03167)（参见[我们的帖子](https://gab41.lab41.org/batch-normalization-what-the-hey-d480039a9e3b)的总结），基本上解决了这些训练问题，但网络的表现仍然随着深度的增加而变得越来越差。例如，他们比较了20层和56层的网络，并发现56层网络表现远不如20层；见下图。
- en: '[![CIFAR Nets](../Images/fa20235ad1477a7fa79c4b093a35095e.png)](https://cdn-images-1.medium.com/max/1500/1*UbNuXfVNVfTq1WkMZyJ1QA.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[![CIFAR Nets](../Images/fa20235ad1477a7fa79c4b093a35095e.png)](https://cdn-images-1.medium.com/max/1500/1*UbNuXfVNVfTq1WkMZyJ1QA.png)'
- en: '*Comparison of 20- and 56-layer networks on CIFAR-10\. Note that the 56-layer
    network performs more poorly in both training and testing.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*20层和56层网络在CIFAR-10上的比较。注意56层网络在训练和测试中的表现都较差。*'
- en: 'The authors then set up a thought experiment (or [gedankenexperiment](https://en.wiktionary.org/wiki/gedankenexperiment) if
    you’re a recovering physicist like me) to demonstrate that deeper networks should
    always perform better. Their argument is as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然后作者设立了一个思想实验（或者说[思想实验](https://en.wiktionary.org/wiki/gedankenexperiment)如果你像我一样是一个恢复中的物理学家）来证明更深的网络应该总是表现更好。他们的论点如下：
- en: Start with a network that performs well;
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个表现良好的网络开始；
- en: Add additional layers that are forced to be the identity function, that is,
    they simply pass along whatever information arrives at them without change;
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加强制为恒等函数的额外层，即，它们只是传递到达它们的任何信息而不进行更改；
- en: This network is deeper, but must have the same performance as the original network
    by construction since the new layers do not do anything;
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个网络更深，但必须与原始网络具有相同的性能，因为新层没有做任何事情。
- en: Layers in a network can learn the identity function, so they should be able
    to exactly replicate the performance of this deep network if it is optimal.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络中的层可以学习恒等函数，因此如果它是最优的，它们应该能够准确复制这个深层网络的性能。
- en: This thought experiment leads them to propose their deep residual learning architecture.
    They construct their network of what they call residual building blocks. The image
    below shows one such block. These blocks have become known as ResBlocks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个思想实验促使他们提出了深度残差学习架构。他们构建了他们所谓的残差构建块网络。下图展示了其中一个这样的块。这些块被称为ResBlocks。
- en: '[![ResBlock](../Images/0798dc5151f41fd870ef7235d82b7976.png)](https://cdn-images-1.medium.com/max/1500/1*4sO3vjCdUlYQZcRq-GNqyw.png)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[![ResBlock](../Images/0798dc5151f41fd870ef7235d82b7976.png)](https://cdn-images-1.medium.com/max/1500/1*4sO3vjCdUlYQZcRq-GNqyw.png)'
- en: '*A ResBlock; a residual function f(x) is learned on the top and information
    is passed along the bottom unchanged. Image modified from [Huang et al.’s Stochastic
    Depth paper](https://arxiv.org/abs/1603.09382).*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*一个ResBlock；顶部学习一个残差函数 f(x)，而底部的信息保持不变。图像改编自[黄等人的随机深度论文](https://arxiv.org/abs/1603.09382)。*'
- en: The ResBlock is constructed out of normal network layers connected with [rectified
    linear units](https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29) (ReLUs)
    and a pass-through below that feeds through the information from previous layers
    unchanged. The network part of the ResBlock can consist of an arbitrary number
    of layers, but the simplest is two.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ResBlock由普通网络层构建，这些层通过[线性整流单元](https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29)（ReLUs）连接，并且在下面有一个通过的连接，保持来自先前层的信息不变。ResBlock的网络部分可以包含任意数量的层，但最简单的情况是两层。
- en: 'To get a little into the math behind the ResBlock: let us assume that a set
    of layers would perform best if they learned a specific function, *h(x)*. The
    authors note that the residual, *f(x) = h(x)*−*x*, can be learned instead and
    combined with the original input such that we recover *h(x)* as follows: *h(x)
    = f(x) + x.* This can be accomplished by adding a *+x* component to the network,
    which, thinking back to our thought experiment, is simply the identity function.
    The authors hope that adding this “pass-through” to their layers will aid in training.
    As with most deep learning, there is only this intuition backing up the method
    and not any deeper understanding. However, as the authors show, it works, and
    in that end that’s the only thing many of us practitioners care about.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解ResBlock背后的数学：假设一组层如果学习一个特定的函数*h(x)*，将表现最好。作者指出，可以学习残差*f(x) = h(x) − x*，然后将其与原始输入结合，以恢复*h(x)*，如下所示：*h(x)
    = f(x) + x*。这可以通过向网络中添加一个*+x*组件来实现，回想一下我们的思想实验，这只是恒等函数。作者希望将这种“传递”添加到他们的层中将有助于训练。与大多数深度学习一样，这种方法只有直观的支持，而没有更深层次的理解。然而，正如作者所展示的，它确实有效，而这正是许多从业者所关心的唯一问题。
- en: The paper also explores a few modifications to the ResBlock. The first is creating
    bottleneck blocks with three layers where the middle layer constricts the information
    flow by using fewer inputs and outputs. The second is testing different types
    of pass-through connections including learning a full projection matrix. Although
    the more complicated pass-throughs perform better, they do so only slightly and
    at the cost of training time.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 论文还探讨了对ResBlock的一些修改。第一个是创建具有三层的瓶颈块，其中中间层通过使用较少的输入和输出来限制信息流。第二个是测试不同类型的传递连接，包括学习完整的投影矩阵。尽管更复杂的传递连接性能更好，但只是略微提高，而且训练时间成本更高。
- en: The rest of the paper tests the performance of the network. The authors find
    that their networks perform better than identical networks without the pass-through;
    see the image below for their plot showing this. They also find that they can
    train far deeper networks and still show improved performance, culminating in
    training a 152-layer ResNet that outperforms shallower networks. They even train
    a 1202-layer network to prove that it is feasible, but find that its performance
    is worse than the other networks examined in the paper.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的其余部分测试了网络的性能。作者发现，他们的网络在使用通行功能后表现优于没有通行功能的相同网络；请参见下图的绘图以了解这一点。他们还发现，能够训练更深层的网络，并且仍然表现出改进的性能，最终训练了一个152层的ResNet，表现优于较浅的网络。他们甚至训练了一个1202层的网络以证明其可行性，但发现其性能不如论文中检查的其他网络。
- en: '[![](../Images/a10ad9ecf70715537142a85783583456.png)](https://cdn-images-1.medium.com/max/1500/1*f8TRIszXY6xGqtcMBWvD3g.png)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/a10ad9ecf70715537142a85783583456.png)](https://cdn-images-1.medium.com/max/1500/1*f8TRIszXY6xGqtcMBWvD3g.png)'
- en: '*A comparison of the performance of two networks: the ones on the left do not
    use ResBlocks, while the ones on the right do. Notice the the 34-layer network
    performs better than the 18-layer network, but only when using ResBlocks.*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*两个网络性能的比较：左侧的网络不使用ResBlocks，而右侧的网络使用了。注意34层的网络比18层的网络表现更好，但仅在使用ResBlocks时。*'
- en: So that’s it! He *et al.* proposed a new architecture motivated by thought experiments
    and the hope that it will work better than previous ones. They construct several
    networks, including a few very deep ones, and find that their new architecture
    does indeed improve performance of the networks. Although we don’t gain any further
    understanding of the underlying principles of deep learning, we do get a new method
    of making our networks work better, and in the end maybe that’s good enough.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！他 *et al.* 提出了一个新的架构，受到思想实验的启发，并希望它能比以前的架构表现更好。他们构建了几个网络，包括一些非常深的网络，发现他们的新架构确实改善了网络的性能。虽然我们对深度学习的基本原理没有获得进一步的理解，但我们确实获得了一种使网络表现更好的新方法，最终也许这已经足够好。
- en: '**[Alexander Gude](https://twitter.com/alex_gude)** is currently a data scientist
    at Lab41 working on investigating recommender system algorithms. He holds a BA
    in physics from University of California, Berkeley, and a PhD in Elementary Particle
    Physics from University of Minnesota-Twin Cities.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**[亚历山大·古德](https://twitter.com/alex_gude)** 目前是Lab41的数据科学家，研究推荐系统算法。他拥有加州大学伯克利分校的物理学学士学位和明尼苏达大学双城分校的基础粒子物理学博士学位。'
- en: '**[Lab41](http://www.lab41.org)** is a “challenge lab” where the U.S. Intelligence
    Community comes together with their counterparts in academia, industry, and In-Q-Tel
    to tackle big data. It allows participants from diverse backgrounds to gain access
    to ideas, talent, and technology to explore what works and what doesn’t in data
    analytics. An open, collaborative environment, Lab41 fosters valuable relationships
    between participants.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Lab41](http://www.lab41.org)** 是一个“挑战实验室”，美国情报界与学术界、工业界以及In-Q-Tel的同行共同合作，解决大数据问题。它允许来自不同背景的参与者接触思想、人才和技术，以探索数据分析中有效和无效的方面。作为一个开放、协作的环境，Lab41促进了参与者之间的宝贵关系。'
- en: '[Original](https://gab41.lab41.org/lab41-reading-group-deep-residual-learning-for-image-recognition-ffeb94745a1f).
    Reposted with permission.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://gab41.lab41.org/lab41-reading-group-deep-residual-learning-for-image-recognition-ffeb94745a1f)。经许可转载。'
- en: '**Related:**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[In Deep Learning, Architecture Engineering is the New Feature Engineering](/2016/07/deep-learning-architecture-engineering-feature-engineering.html)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在深度学习中，架构工程是新的特征工程](/2016/07/deep-learning-architecture-engineering-feature-engineering.html)'
- en: '[Up to Speed on Deep Learning: July Update](/2016/08/deep-learning-july-update.html)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习进展：七月更新](/2016/08/deep-learning-july-update.html)'
- en: '[Why Do Deep Learning Networks Scale?](/2016/07/deep-learning-networks-scale.html)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习网络为何会扩展？](/2016/07/deep-learning-networks-scale.html)'
- en: '* * *'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速入门网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT需求'
- en: '* * *'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Transfer Learning for Image Recognition and Natural Language Processing](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用于图像识别和自然语言处理的迁移学习](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
- en: '[A Step by Step Guide to Reading and Understanding SQL Queries](https://www.kdnuggets.com/a-step-by-step-guide-to-reading-and-understanding-sql-queries)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[逐步指南：阅读和理解SQL查询](https://www.kdnuggets.com/a-step-by-step-guide-to-reading-and-understanding-sql-queries)'
- en: '[Reading Minds with AI: Researchers Translate Brain Waves to Images](https://www.kdnuggets.com/2023/03/reading-minds-ai-researchers-translate-brain-waves-images.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用AI读取思维：研究人员将脑电波转换为图像](https://www.kdnuggets.com/2023/03/reading-minds-ai-researchers-translate-brain-waves-images.html)'
- en: '[2024 Reading List: 5 Essential Reads on Artificial Intelligence](https://www.kdnuggets.com/2024-reading-list-5-essential-reads-on-artificial-intelligence)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2024年阅读清单：5本关于人工智能的必读书籍](https://www.kdnuggets.com/2024-reading-list-5-essential-reads-on-artificial-intelligence)'
- en: '[The Evolution of Speech Recognition Metrics](https://www.kdnuggets.com/2022/10/evolution-speech-recognition-metrics.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[语音识别指标的发展](https://www.kdnuggets.com/2022/10/evolution-speech-recognition-metrics.html)'
- en: '[5 IT Jobs That Are High in Demand But Don’t Get Enough Recognition](https://www.kdnuggets.com/5-it-jobs-that-are-high-in-demand-but-dont-get-enough-recognition)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5个需求量大但不被足够认可的IT职位](https://www.kdnuggets.com/5-it-jobs-that-are-high-in-demand-but-dont-get-enough-recognition)'
