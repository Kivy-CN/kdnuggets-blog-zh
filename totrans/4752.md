# Python 中的文本预处理：步骤、工具和示例

> 原文：[https://www.kdnuggets.com/2018/11/text-preprocessing-python.html](https://www.kdnuggets.com/2018/11/text-preprocessing-python.html)

![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](/2018/11/text-preprocessing-python.html?page=2#comments)

**由 Olga Davydova，[数据怪物](https://datamonsters.com)**。

获取文本后，我们开始进行文本规范化。文本规范化包括：

+   将所有字母转换为小写或大写

+   将数字转换为单词或移除数字

+   移除标点符号、重音符号和其他变音符号

+   移除空白字符

+   扩展缩写

+   移除停用词、稀疏词项和特定单词

+   文本规范化

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持组织的 IT 需求

* * *

我们将在下面详细描述文本规范化步骤。

![](../Images/1b57451b11f6a00cd631ccd21101e282.png)

### **将文本转换为小写**

**示例 1\. 转换文本为小写**

**Python 代码：**

```py

input_str = ”The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.”
input_str = input_str.lower()
print(input_str)

```

**输出：**

```py

the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.

```

### 移除数字

如果数字与分析无关，可以移除它们。通常使用正则表达式来移除数字。

**示例 2\. 数字移除**

**Python 代码：**

```py

import re
input_str = ’Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.’
result = re.sub(r’\d+’, ‘’, input_str)
print(result)

```

**输出：**

```py

Box A contains red and white balls, while Box B contains red and blue balls.

```

### 移除标点符号

以下代码移除这组符号 [!”#$%&’()*+,-./:;<=>?@[\]^_`{|}~]：

**示例 3\. 标点符号移除**

**Python 代码：**

```py

import string
input_str = “This &is [an] example? {of} string. with.? punctuation!!!!” # Sample string
result = input_str.translate(string.maketrans(“”,””), string.punctuation)
print(result)

```

**输出：**

```py

This is an example of string with punctuation

```

### 移除空白字符

要移除前导和尾部空格，可以使用 *strip()* 函数：

**示例 4\. 空白字符移除**

**Python 代码：**

```py

input_str = “ \t a string example\t “
input_str = input_str.strip()
input_str

```

**输出：**

```py

‘a string example’

```

### 分词

分词是将给定文本拆分成更小的部分称为标记的过程。单词、数字、标点符号等都可以视为标记。在[这个表格](https://docs.google.com/spreadsheets/d/1-9rMhfcmxFv2V2Q5ZWn1FfLDZZYsuwb1eoSp9CiEEOg/edit?usp=sharing)（“分词”工作表）中描述了几个实现分词的工具。

表 1: **分词工具**

| **名称、开发者、首次发布** | **特性** | **编程语言** | **许可证** |
| --- | --- | --- | --- |
| [**自然语言工具包 (NLTK)**，宾夕法尼亚大学，2001](http://www.nltk.org/index.html) | Mac/Unix/Windows 支持 | Python | [Apache 许可证第 2.0 版。](http://www.apache.org/licenses/LICENSE-2.0) |
| [包含多个语料库、玩具语法、训练模型等 [1]。](http://www.nltk.org/index.html) |
| [**TextBlob**, Steven Loria, 2013](http://textblob.readthedocs.io/en/dev/) | 将文本拆分为单词和句子 | Python | [http://textblob.readthedocs.io/en/dev/license.html](http://textblob.readthedocs.io/en/dev/license.html) |
| [WordNet集成 [2]](http://textblob.readthedocs.io/en/dev/) |
| [**Spacy**, Explosion AI, 2016](https://spacy.io/) | 运行于Unix/Linux、MacOS/OS X和Windows。 | Python | [MIT许可证](https://github.com/explosion/spaCy/blob/master/LICENSE) |
| 神经网络模型 |
| [多语言支持 [3]](https://spacy.io/usage/facts-figures) |
| [**Gensim**, RaRe Technologies, 2009](https://radimrehurek.com/gensim/) | 能处理大规模网络语料库 | Python | [GNU LGPLv2.1许可证](https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html) |
| 运行于Linux、Windows和OS X |
| [向量空间建模和主题建模 [4]](https://radimrehurek.com/gensim/) |
| [**Apache OpenNLP**, Apache软件基金会, 2004](https://opennlp.apache.org/) | 包含大量预构建的多语言模型 | Java | [Apache许可证 2.0版](https://www.apache.org/licenses/LICENSE-2.0) |
| [包含注释文本资源 [5]](https://opennlp.apache.org/) |
| [**OpenNMT**, Yoon Kim, harvardnlp, 2016](http://opennmt.net/) | 是一个通用的深度学习框架，主要专注于序列到序列模型 | Python | [MIT许可证](https://github.com/OpenNMT/OpenNMT/blob/master/LICENSE.md) |
| [可通过命令行应用程序、客户端-服务器或库使用。[6]](http://opennmt.net/) | Lua |
| 目前有3种主要实现（OpenNMT-lua, OpenNMT-py, OpenNMT-tf） |   |
| [**文本工程通用架构（GATE）**, GATE研究团队, 谢菲尔德大学, 1995](https://gate.ac.uk/) | 包含信息提取系统 | Java | [GNU许可证及其他](http://www.gnu.org/licenses/) |
| 支持多种语言 |
| [接受多种格式的输入 [7]](https://gate.ac.uk/) |
| [**Apache UIMA**, IBM, Apache软件基金会, 2006](https://uima.apache.org/) | [包含附加组件和沙盒](https://uima.apache.org/sandbox.html) | Java, C++ | [Apache许可证 2.0](https://www.apache.org/licenses/LICENSE-2.0) |
| 跨平台 |
| [REST请求支持 [8]](https://uima.apache.org/) |
| [**基于记忆的浅层解析器（MBSP）**, Vincent Van Asch, Tom De Smedt, 2010](https://www.clips.uantwerpen.be/pages/MBSP#tokenizer) | 客户端-服务器架构 | Python | [GPL](http://www.gnu.org/licenses/gpl.html) |
| 包含预编译的二进制文件（TiMBL, MBT和MBLEM）适用于Mac OS X |
| [Cygwin在Windows上的使用 [9]](https://www.clips.uantwerpen.be/pages/MBSP#tokenizer) |
| [**RapidMiner**, RapidMiner, 2006](https://rapidminer.com/) | 统一平台 | RapidMiner提供图形界面来设计和执行分析工作流 | [AGPL](https://en.wikipedia.org/wiki/Affero_General_Public_License) |
| 视觉化工作流设计 |
| 功能广泛 |
| [广泛的连接性 [10]](https://rapidminer.com/) |
| [**语言处理工具包 (MALLET)**，安德鲁·卡基特斯·麦卡勒姆，马萨诸塞大学阿默斯特分校，2002](http://mallet.cs.umass.edu/) | 包含用于文档分类和序列标注的复杂工具 | Java | [通用公共许可证](https://opensource.org/licenses/cpl1.0.php) |
| [支持一般图模型推理 [11]](http://mallet.cs.umass.edu/) |
| [**Pattern**，T. De Smedt 和 W. Daeleman，2012](https://www.clips.uantwerpen.be/pages/pattern-en#parser) | 网络挖掘模块 | Python | [BSD](http://www.linfo.org/bsdlicense.html) |
| 兼容 Windows、Mac 和 Linux |
| [多语言支持 [12]](https://www.clips.uantwerpen.be/pages/pattern) |
| [**斯坦福分词器**，斯坦福自然语言处理小组，2010](https://nlp.stanford.edu/software/tokenizer.html) | [分词器未单独分发，但包含在多个软件下载中；](https://nlp.stanford.edu/software/) | Java | [GNU 通用公共许可证](http://www.gnu.org/licenses/gpl-2.0.html) |
| 每秒约 1,000,000 个标记 |
| [有许多选项影响标记化的执行方式 [13]](https://nlp.stanford.edu/software/tokenizer.html#About) |
| [**FreeLing**，TALP 研究中心，加泰罗尼亚理工大学](http://nlp.lsi.upc.edu/freeling/) | 提供语言分析功能 | C++ | [Affero GNU 通用公共许可证](http://www.gnu.org/licenses/agpl.html) |
| 支持多种语言 |
| 提供命令行前端 |
| [输出格式：XML，JSON，CoNLL [45]](http://nlp.lsi.upc.edu/freeling/) |

### 删除停用词

“停用词”是语言中最常见的词汇，如“the”、“a”、“on”、“is”、“all”。这些词没有重要的意义，通常会从文本中删除。可以使用 [自然语言工具包 (NLTK)](http://www.nltk.org/)，这是一个用于符号和统计自然语言处理的库和程序套件，来移除停用词。

**示例 7. 停用词移除**

**代码：**

```py

input_str = “NLTK is a leading platform for building Python programs to work with human language data.”
stop_words = set(stopwords.words(‘english’))
from nltk.tokenize import word_tokenize
tokens = word_tokenize(input_str)
result = [i for i in tokens if not i in stop_words]
print (result)

```

**输出：**

```py

[‘NLTK’, ‘leading’, ‘platform’, ‘building’, ‘Python’, ‘programs’, ‘work’, ‘human’, ‘language’, ‘data’, ‘.’]

```

一个 [scikit-learn](http://scikit-learn.org/stable/) 工具还提供了一个停用词列表：

```py

from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS

```

也可以使用 [spaCy](https://spacy.io/)，这是一个免费的开源库：

```py

from spacy.lang.en.stop_words import STOP_WORDS

```

### 删除稀疏词汇和特定词汇

在某些情况下，需要从文本中删除稀疏词汇或特定词汇。这可以通过停用词移除技术来完成，考虑到任何词组都可以作为停用词。

### 词干提取

词干提取是将单词减少到其词干、基础或根形式的过程（例如，books — book，looked — look）。主要的两个算法是 [Porter 词干提取算法](https://tartarus.org/martin/PorterStemmer/)（移除单词中的常见形态和屈折词尾 [[14])](https://tartarus.org/martin/PorterStemmer/) 和 [Lancaster 词干提取算法](http://web.archive.org/web/20140827005744/http:/www.comp.lancs.ac.uk/computing/research/stemming/index.htm)（一个更具侵略性的词干提取算法）。在 [“词干提取”表格](https://docs.google.com/spreadsheets/d/1-9rMhfcmxFv2V2Q5ZWn1FfLDZZYsuwb1eoSp9CiEEOg/edit?usp=sharing) 中描述了一些词干提取器。

| **名称，开发者，首次发布** | **特点** | **编程语言** | **许可证** |
| --- | --- | --- | --- |
| [**自然语言工具包 (NLTK)**，宾夕法尼亚大学，2001](http://www.nltk.org/index.html) | 支持 Mac/Unix/Windows | Python | [Apache 许可证 2.0 版本](http://www.apache.org/licenses/LICENSE-2.0) |
| [包含许多语料库、玩具语法、训练模型等 [1].](http://www.nltk.org/index.html) |
| [**TextBlob**，Steven Loria，2013](http://textblob.readthedocs.io/en/dev/) | 将文本分割成单词和句子 | Python | [http://textblob.readthedocs.io/en/dev/license.html](http://textblob.readthedocs.io/en/dev/license.html) |
| [WordNet 集成 [2]](http://textblob.readthedocs.io/en/dev/) |
| [**Spacy**，Explosion AI，2016](https://spacy.io/) | 适用于 Unix/Linux、MacOS/OS X 和 Windows。 | Python | [MIT 许可证](https://github.com/explosion/spaCy/blob/master/LICENSE) |
| 神经网络模型 |
| [多语言支持 [3]](https://spacy.io/usage/facts-figures) |
| [**Gensim**，RaRe Technologies，2009](https://radimrehurek.com/gensim/) | 可以处理大型的网络规模语料库 | Python | [GNU LGPLv2.1 许可证](https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html) |
| 适用于 Linux、Windows 和 OS X |
| [向量空间建模和主题建模 [4]](https://radimrehurek.com/gensim/) |
| [**Apache OpenNLP**，Apache 软件基金会，2004](https://opennlp.apache.org/) | 包含大量预构建的模型，支持多种语言 | Java | [Apache 许可证，2.0 版本](https://www.apache.org/licenses/LICENSE-2.0) |
| [包含带注释的文本资源 [5]](https://opennlp.apache.org/) |
| [**OpenNMT**，Yoon Kim，harvardnlp，2016](http://opennmt.net/) | 是一个通用深度学习框架，主要专注于序列到序列模型 | Python | [MIT 许可证](https://github.com/OpenNMT/OpenNMT/blob/master/LICENSE.md) |
| [可以通过命令行应用程序、客户端-服务器或库使用 [6]](http://opennmt.net/) | Lua |
| 目前有 3 个主要实现（OpenNMT-lua、OpenNMT-py、OpenNMT-tf） |  |
| [**General Architecture for Text Engineering (GATE)**，GATE 研究团队，谢菲尔德大学，1995](https://gate.ac.uk/) | 包含信息提取系统 | Java | [GNU 许可证及其他](http://www.gnu.org/licenses/) |
| 多语言支持 |
| [接受多种格式的输入 [7]](https://gate.ac.uk/) |
| [**Apache UIMA**，IBM，Apache 软件基金会，2006](https://uima.apache.org/) | [包含附加组件和沙箱](https://uima.apache.org/sandbox.html) | Java，C++ | [Apache 2.0 许可证](https://www.apache.org/licenses/LICENSE-2.0) |
| 跨平台 |
| [支持 REST 请求 [8]](https://uima.apache.org/) |
| [**基于记忆的浅层解析器 (MBSP)**，Vincent Van Asch，Tom De Smedt，2010](https://www.clips.uantwerpen.be/pages/MBSP#tokenizer) | 客户端-服务器架构 | Python | [GPL](http://www.gnu.org/licenses/gpl.html) |
| 包含预编译的二进制文件（TiMBL、MBT 和 MBLEM）适用于 Mac OS X |
| [Windows 下的 Cygwin 使用 [9]](https://www.clips.uantwerpen.be/pages/MBSP#tokenizer) |
| [**RapidMiner**，RapidMiner，2006](https://rapidminer.com/) | 统一平台 | RapidMiner 提供一个 GUI 设计和执行分析工作流 | [AGPL](https://en.wikipedia.org/wiki/Affero_General_Public_License) |
| 视觉工作流设计 |
| 功能广泛 |
| [广泛的连接性 [10]](https://rapidminer.com/) |
| [**MAchine Learning for LanguagE Toolkit (MALLET)**，Andrew Kachites McCallum，马萨诸塞大学阿默斯特分校，2002](http://mallet.cs.umass.edu/) | 包含文档分类和序列标记的高级工具 | Java | [通用公共许可证](https://opensource.org/licenses/cpl1.0.php) |
| [支持一般图模型中的推理 [11]](http://mallet.cs.umass.edu/) |
| [**Pattern**，T. De Smedt & W. Daeleman，2012](https://www.clips.uantwerpen.be/pages/pattern-en#parser) | 网络挖掘模块 | Python | [BSD](http://www.linfo.org/bsdlicense.html) |
| 运行于 Windows、Mac 和 Linux |
| [多语言支持 [12]](https://www.clips.uantwerpen.be/pages/pattern) |
| [**斯坦福分词器**，斯坦福自然语言处理小组，2010](https://nlp.stanford.edu/software/tokenizer.html) | [分词器没有单独发布，而是包含在多个软件下载中；](https://nlp.stanford.edu/software/) | Java | [GNU 通用公共许可证](http://www.gnu.org/licenses/gpl-2.0.html) |
| 每秒约 1,000,000 个标记 |
| [有多种选项影响分词的执行方式 [13]](https://nlp.stanford.edu/software/tokenizer.html#About) |
| [**FreeLing**，TALP 研究中心，加泰罗尼亚理工大学](http://nlp.lsi.upc.edu/freeling/) | 提供语言分析功能 | C++ | [Affero GNU 通用公共许可证](http://www.gnu.org/licenses/agpl.html) |
| 支持多种语言 |
| 提供命令行前端 |
| [输出格式：XML，JSON，CoNLL [45]](http://nlp.lsi.upc.edu/freeling/) |

**词干提取工具**

**示例 8\. 使用 NLTK 进行词干提取：**

**代码：**

```py

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
stemmer= PorterStemmer()
input_str=”There are several types of stemming algorithms.”
input_str=word_tokenize(input_str)
for word in input_str:
    print(stemmer.stem(word))

```

**输出：**

```py

There are sever type of stem algorithm.

```

### 词形还原

词形还原的目标与词干提取类似，都是将屈折形式简化为共同的基本形式。与词干提取不同，词形还原并不会简单地剪切屈折形式。相反，它使用词汇知识库来获取单词的正确基本形式。

词形还原工具包括上文描述的库：[NLTK (WordNet Lemmatizer)](http://www.nltk.org/_modules/nltk/stem/wordnet.html)，[spaCy](https://spacy.io/api/lemmatizer)，[TextBlob](http://textblob.readthedocs.io/en/dev/quickstart.html#words-inflection-and-lemmatization)，[Pattern](https://www.clips.uantwerpen.be/pages/pattern-en#conjugation)，[gensim](https://radimrehurek.com/gensim/utils.html)，[Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/simple.html)，[Memory-Based Shallow Parser (MBSP)](https://www.clips.uantwerpen.be/pages/MBSP#lemmatizer)，[Apache OpenNLP](https://opennlp.apache.org/docs/1.8.4/manual/opennlp.html#tools.lemmatizer.tagging.cmdline)，[Apache Lucene](http://lucene.apache.org/core/)，[General Architecture for Text Engineering (GATE)](https://gate.ac.uk/)，[Illinois Lemmatizer](https://cogcomp.org/page/software_view/illinois-lemmatizer) 和 [DKPro Core](https://dkpro.github.io/dkpro-core/releases/1.8.0/docs/component-reference.html#_lemmatizer)。

**示例 9\. 使用 NLTK 进行词形还原：**

**代码：**

```py

from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
lemmatizer=WordNetLemmatizer()
input_str=”been had done languages cities mice”
input_str=word_tokenize(input_str)
for word in input_str:
    print(lemmatizer.lemmatize(word))

```

**输出：**

```py

be have do language city mouse

```

### 词性标注（POS）

词性标注旨在根据每个单词的定义和上下文将其标记为不同的词性（如名词、动词、形容词等）。许多工具包含词性标注器，包括 [NLTK](http://www.nltk.org/book/ch05.html)，[spaCy](https://spacy.io/usage/linguistic-features)，[TextBlob](http://textblob.readthedocs.io/en/dev/quickstart.html#part-of-speech-tagging)，[Pattern](https://www.clips.uantwerpen.be/pages/pattern-en#parser)，[Stanford CoreNLP](https://nlp.stanford.edu/software/tagger.shtml)，[Memory-Based Shallow Parser (MBSP)](https://www.clips.uantwerpen.be/pages/MBSP#parser)，[Apache OpenNLP](https://opennlp.apache.org/docs/1.8.4/manual/opennlp.html#tools.postagger.tagging)，[Apache Lucene](https://lucene.apache.org/core/)，[General Architecture for Text Engineering (GATE)](https://gate.ac.uk/)，[FreeLing](http://nlp.lsi.upc.edu/freeling/)，[Illinois Part of Speech Tagger](https://cogcomp.org/page/software_view/POS) 和 [DKPro Core](https://dkpro.github.io/dkpro-core/releases/1.9.0/docs/component-reference.html#_part_of_speech_tagger)。

**示例 10\. 使用 TextBlob 进行词性标注：**

**代码：**

```py

input_str=”Parts of speech examples: an article, to write, interesting, easily, and, of”
from textblob import TextBlob
result = TextBlob(input_str)
print(result.tags)

```

**输出：**

```py

[(‘Parts’, u’NNS’), (‘of’, u’IN’), (‘speech’, u’NN’), (‘examples’, u’NNS’), (‘an’, u’DT’), (‘article’, u’NN’), (‘to’, u’TO’), (‘write’, u’VB’), (‘interesting’, u’VBG’), (‘easily’, u’RB’), (‘and’, u’CC’), (‘of’, u’IN’)]

```

### 词组分析（浅层解析）

分块是自然语言处理的一种过程，它识别句子的组成部分（名词、动词、形容词等），并将其连接到具有离散语法意义的更高级单元（名词组或短语、动词组等）[[23]](https://en.wikipedia.org/wiki/Shallow_parsing)。 分块工具： [NLTK](http://www.nltk.org/book/ch07.html)， [TreeTagger分块器](http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/)， [Apache OpenNLP](https://opennlp.apache.org/docs/1.8.4/manual/opennlp.html#tools.parser.chunking)， [通用文本工程架构（GATE）](https://gate.ac.uk/)， [FreeLing](http://nlp.lsi.upc.edu/freeling/)。

**示例 11\. 使用NLTK的分块：**

第一步是确定每个词的词性：

**代码：**

```py

input_str=”A black television and a white stove were bought for the new apartment of John.”
from textblob import TextBlob
result = TextBlob(input_str)
print(result.tags)

```

**输出：**

```py

[(‘A’, u’DT’), (‘black’, u’JJ’), (‘television’, u’NN’), (‘and’, u’CC’), (‘a’, u’DT’), (‘white’, u’JJ’), (‘stove’, u’NN’), (‘were’, u’VBD’), (‘bought’, u’VBN’), (‘for’, u’IN’), (‘the’, u’DT’), (‘new’, u’JJ’), (‘apartment’, u’NN’), (‘of’, u’IN’), (‘John’, u’NNP’)]

```

第二步是分块：

**代码：**

```py

reg_exp = “NP: {?*}”
rp = nltk.RegexpParser(reg_exp)
result = rp.parse(result.tags)
print(result)

```

**输出：**

```py

(S (NP A/DT black/JJ television/NN) and/CC (NP a/DT white/JJ stove/NN) were/VBD bought/VBN for/IN (NP the/DT new/JJ apartment/NN)
of/IN John/NNP)

```

也可以使用代码 result.draw()绘制句子树结构。

![](../Images/decd12c76c9568338ba6cc4043f64bfa.png)

### 命名实体识别

命名实体识别（NER）的目标是从文本中查找命名实体，并将其分类为预定义类别（个人名字、地点、组织、时间等）。

命名实体识别工具： [NLTK](http://www.nltk.org/book/ch07.html)， [spaCy](https://spacy.io/usage/linguistic-features#section-named-entities)， [通用文本工程架构（GATE）— ANNIE](https://gate.ac.uk/sale/tao/splitch6.html#chap:annie)， [Apache OpenNLP](https://opennlp.apache.org/docs/1.8.4/manual/opennlp.html#tools.namefind.recognition)， [斯坦福CoreNLP](https://nlp.stanford.edu/software/CRF-NER.shtml)， [DKPro Core](https://dkpro.github.io/dkpro-core/releases/1.9.0/docs/component-reference.html#_named_entity_recognizer)， [MITIE](https://github.com/mit-nlp/MITIE)， [Watson自然语言理解](https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1/#entities)， [TextRazor](https://www.textrazor.com/)， [FreeLing](http://nlp.lsi.upc.edu/freeling/)在 [“NER”表格](https://docs.google.com/spreadsheets/d/1-9rMhfcmxFv2V2Q5ZWn1FfLDZZYsuwb1eoSp9CiEEOg/edit?usp=sharing)中有描述。

| **名称，开发者，初始发布** | **特性** | **编程语言** | **许可证** |
| --- | --- | --- | --- |
| [**Baleen**，国防科学与技术实验室（Dstl），2014](https://github.com/dstl/baleen) | 处理非结构化和半结构化数据源 | Java | [Apache许可证2.0](https://github.com/dstl/baleen/blob/master/LICENSE.txt) |
| 包含内置服务器 |
| [[25]](https://github.com/dstl/baleen/) |
| [**CogComp NER Tagger（伊利诺伊命名实体标记器）**，L. Ratinov，D. Roth，认知计算组，2009](https://github.com/CogComp/cogcomp-nlp/tree/master/ner) | 用命名实体标记纯文本 | Java | [许可协议](http://cogcomp.org/page/download_view/NETagger) |
| 4标签类型集（人/组织/地点/其他） |
| [18-label 类型集合（基于 OntoNotes 语料库） [26]](https://github.com/CogComp/cogcomp-nlp/tree/master/ner) |
| [**最小化命名实体识别器 (MER)**, LaSIGE, Faculdade de Ciências, Universidade de Lisboa, 葡萄牙, 2017](https://github.com/lasigeBioTM/MER) | 返回文本中识别出的术语列表，包括其确切位置（注释） | GNU awk | - |
| [仅需要包含感兴趣实体术语列表的词典（文本文件）RESTful 网络服务](http://labs.rd.ciencias.ulisboa.pt/mer/) |
| [[27]](https://github.com/lasigeBioTM/MER) |
| [**ParallelDots**, ParallelDots](https://www.paralleldots.com/named-entity-recognition) | 使用深度学习技术确定字符分组的表示 | [excel 插件](https://www.paralleldots.com/excel-docs) | [定价](https://www.paralleldots.com/pricing) |
| 发现文本内容中最相关的实体 | [AI APIs](https://www.paralleldots.com/) |
| 准确、实时、可定制 |
| [[28]](https://blog.paralleldots.com/product/dig-relevant-text-elements-entity-extraction-api/) |  |
| [演示](https://www.paralleldots.com/text-analysis-apis#named-entity-recognition) |  |
| [**Open Calais**, Thomson Reuters Corporation](http://www.opencalais.com/about-open-calais/) | [提取实体（公司、人物、地点、产品等）、关系、事实、事件、主题 [29]](http://www.opencalais.com/about-open-calais/) | [API](http://www.opencalais.com/opencalais-api/) | [服务条款](http://www.opencalais.com/open-calais-terms-of-service/) |
| [**LingPipe**, Breck Baldwin, 1999](http://alias-i.com/lingpipe/index.html) | 查找人物、组织或地点的名称 | Java | [许可矩阵](http://alias-i.com/lingpipe/web/download.html) |
| 源代码和单元测试 |
| [多语言、多领域、多类型模型 [30]](http://alias-i.com/lingpipe/index.html) |
| [**命名实体识别工具**, Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer, 2016](https://github.com/glample/tagger) | 一种神经架构 | Python | [Apache 许可证 2.0](https://github.com/glample/tagger/blob/master/LICENSE.md) |
| [在 4 个 CoNLL 数据集（英语、西班牙语、德语和荷兰语）上实现了最先进的命名实体识别性能，无需依赖任何语言特定的知识或资源，如地名词典 [31]](https://github.com/glample/tagger) |
| [**MinorThird**, William W. Cohen, Carnegie Mellon University, 2004](http://minorthird.sourceforge.net/old/doc/) | 结合了用于注释和可视化文本的工具与最先进的学习方法 | Java | [BSD 许可](https://opensource.org/licenses/bsd-license.php) |
| 支持主动学习和在线学习 |
| [[32]](http://minorthird.sourceforge.net/old/doc/) |
| [**Watson 实体识别标注器**，IBM](https://www.ibm.com/support/knowledgecenter/en/SS8NLW_10.0.0/com.ibm.watson.wex.aac.doc/aac-tasystemt.html) | 人物、地点和组织标注器 | Python SDK | [定价](https://www.ibm.com/watson-analytics/pricing) |
| 英语、中文、法语、德语、日语、西班牙语 | Node SDK |
| [添加条目的可能性 [33]](https://www.ibm.com/support/knowledgecenter/en/SS8NLW_10.0.0/com.ibm.watson.wex.aac.doc/aac-tasystemt.html) | Swift SDK |
|  | Java SDK |
|  | Unity SDK |
|  | .NET 标准库 |
| [**PoolParty 语义套件**，语义网公司，2009](https://www.poolparty.biz/) | 模块化和灵活性 | 数据被转换成 RDF 图，并可以使用 SPARQL 查询 | [价格概览](https://www.poolparty.biz/priceoverview/) |
| 使用 W3C 定义的标准技术 |
| 通过有价值的元数据丰富信息 |
| [[34]](https://www.poolparty.biz/) |
| [**Rosette 实体提取器**，Basis Technology，1995](https://www.basistech.com/text-analytics/rosette/entity-extractor/) | 支持 20 种语言 | 绑定：cURL、Python、PHP、Java、R、Ruby、C#、Node.js | - |
| 检测到 18 种实体类型 |
| 筛选关键实体 |
| [每个结果的置信度评分 [35]](https://www.basistech.com/text-analytics/rosette/entity-extractor/) |

**NER 工具**

### 更多相关内容

+   [在 Pandas 中清理和预处理文本数据以进行 NLP 任务](https://www.kdnuggets.com/cleaning-and-preprocessing-text-data-in-pandas-for-nlp-tasks)

+   [掌握数据清理和预处理技术的 7 个步骤](https://www.kdnuggets.com/2023/08/7-steps-mastering-data-cleaning-preprocessing-techniques.html)

+   [SQL LIKE 操作符示例](https://www.kdnuggets.com/2022/09/sql-like-operator-examples.html)

+   [带有示例的集成学习](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)

+   [挑选示例以理解机器学习模型](https://www.kdnuggets.com/2022/11/picking-examples-understand-machine-learning-model.html)

+   [Python 数据预处理简易指南](https://www.kdnuggets.com/2020/07/easy-guide-data-preprocessing-python.html)
