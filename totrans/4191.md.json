["```py\n!wget http://www.gutenberg.org/cache/epub/16457/pg16457.txt\n```", "```py\n!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n```", "```py\n!unzip wikitext-103-raw-v1.zip\n```", "```py\n## importing the tokenizer and subword BPE trainer\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE, Unigram, WordLevel, WordPiece\nfrom tokenizers.trainers import BpeTrainer, WordLevelTrainer, \\\n                                WordPieceTrainer, UnigramTrainer\n\n## a pretokenizer to segment the text into words\nfrom tokenizers.pre_tokenizers import Whitespace\n```", "```py\nunk_token = \"<UNK>\"  # token for unknown words\nspl_tokens = [\"<UNK>\", \"<SEP>\", \"<MASK>\", \"<CLS>\"]  # special tokens\n\ndef prepare_tokenizer_trainer(alg):\n    \"\"\"\n    Prepares the tokenizer and trainer with unknown & special tokens.\n    \"\"\"\n    if alg == 'BPE':\n        tokenizer = Tokenizer(BPE(unk_token = unk_token))\n        trainer = BpeTrainer(special_tokens = spl_tokens)\n    elif alg == 'UNI':\n        tokenizer = Tokenizer(Unigram())\n        trainer = UnigramTrainer(unk_token= unk_token, special_tokens = spl_tokens)\n    elif alg == 'WPC':\n        tokenizer = Tokenizer(WordPiece(unk_token = unk_token))\n        trainer = WordPieceTrainer(special_tokens = spl_tokens)\n    else:\n        tokenizer = Tokenizer(WordLevel(unk_token = unk_token))\n        trainer = WordLevelTrainer(special_tokens = spl_tokens)\n\n    tokenizer.pre_tokenizer = Whitespace()\n    return tokenizer, trainer\n```", "```py\ndef train_tokenizer(files, alg='WLV'):\n    \"\"\"\n    Takes the files and trains the tokenizer.\n    \"\"\"\n    tokenizer, trainer = prepare_tokenizer_trainer(alg)\n    tokenizer.train(files, trainer) # training the tokenzier\n    tokenizer.save(\"./tokenizer-trained.json\")\n    tokenizer = Tokenizer.from_file(\"./tokenizer-trained.json\")\n    return tokenizer\n```", "```py\nsmall_file = ['pg16457.txt']\nlarge_files = [f\"./wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n\nfor files in [small_file, large_files]:\n    print(f\"========Using vocabulary from {files}=======\")\n    for alg in ['WLV', 'BPE', 'UNI', 'WPC']:\n        trained_tokenizer = train_tokenizer(files, alg)\n        input_string = \"This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!????\"\n        output = tokenize(input_string, trained_tokenizer)\n        tokens_dict[alg] = output.tokens\n        print(\"----\", alg, \"----\")\n        print(output.tokens, \"->\", len(output.tokens))\n```", "```py\nimport pandas as pd\n\nmax_len = max(len(tokens_dict['UNI']), len(tokens_dict['WPC']), len(tokens_dict['BPE']))\ndiff_bpe = max_len - len(tokens_dict['BPE'])\ndiff_wpc = max_len - len(tokens_dict['WPC'])\n\ntokens_dict['BPE'] = tokens_dict['BPE'] + ['<PAD>']*diff_bpe\ntokens_dict['WPC'] = tokens_dict['WPC'] + ['<PAD>']*diff_wpc\n\ndel tokens_dict['WLV']\n\ndf = pd.DataFrame(tokens_dict)\ndf.head(10)\n```"]