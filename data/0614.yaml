- en: Real Time Image Segmentation Using 5 Lines of Code
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时图像分割使用5行代码
- en: 原文：[https://www.kdnuggets.com/2021/10/real-time-image-segmentation-5-lines-code.html](https://www.kdnuggets.com/2021/10/real-time-image-segmentation-5-lines-code.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/10/real-time-image-segmentation-5-lines-code.html](https://www.kdnuggets.com/2021/10/real-time-image-segmentation-5-lines-code.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Ayoola Olafenwa](https://www.linkedin.com/in/ayoola-olafenwa-003b901a9/),
    Machine Learning Engineer**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由[Ayoola Olafenwa](https://www.linkedin.com/in/ayoola-olafenwa-003b901a9/)，机器学习工程师**'
- en: '**Demand for Real Time Image Segmentation Applications**'
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**对实时图像分割应用的需求**'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的IT需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Image segmentation is an aspect of computer vision that deals with segmenting
    the contents of objects visualized by a computer into different categories for
    better analysis. The contributions of image segmentation in solving a lot of computer
    vision problems such as analysis of medical images, background editing, vision
    in self driving cars and analysis of satellite images make it an invaluable field
    in computer vision. One of the greatest challenges in computer vision is keeping
    the space between accuracy and speed performance for real time applications. In
    the field of computer vision there is this dilemma of a computer vision solution
    either being more accurate and slow or less accurate and faster.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割是计算机视觉的一个方面，涉及将计算机可视化的对象内容分割成不同类别以便更好地分析。图像分割在解决许多计算机视觉问题中的贡献，如医学图像分析、背景编辑、自动驾驶汽车视觉和卫星图像分析，使其在计算机视觉领域中具有不可替代的价值。计算机视觉中的一个重大挑战是保持实时应用的准确性和速度性能之间的平衡。在计算机视觉领域，存在一个两难困境，即计算机视觉解决方案要么更准确但速度较慢，要么准确性较低但速度较快。
- en: PixelLib Library is a library created to allow easy integration of object segmentation
    in images and videos using few lines of python code. The previous version of PixelLib
    uses Tensorflow deep learning as its backend which employs Mask R-CNN to perform
    instance segmentation. Mask R-CNN is a great object segmentation architecture,
    but it fails to balance between the accuracy and speed performance for real time
    applications.  PixelLib provides support for PyTorch backend to perform faster,
    more accurate segmentation and extraction of objects in images and videos using ***PointRend ***segmentation
    architecture.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: PixelLib库是一个允许通过几行python代码轻松集成图像和视频中的对象分割的库。PixelLib的早期版本使用Tensorflow深度学习作为其后端，采用Mask
    R-CNN进行实例分割。Mask R-CNN是一个优秀的对象分割架构，但它未能在实时应用中平衡准确性和速度性能。PixelLib提供对PyTorch后端的支持，以更快、更准确地分割和提取图像和视频中的对象，使用***PointRend***分割架构。
- en: '***PointRend*** by [Alexander Kirillov et al](https://arxiv.org/abs/1912.08193) is
    used to replace Mask R-CNN for performing instance segmentation of objects. ***PointRend*** is
    an excellent state of the art neural network for implementing object segmentation.
    It generates accurate segmentation masks and run at high inference speed that
    matches the increasing demand for an accurate and real time computer vision applications.
    I integrated PixelLib with the python implementation of PointRend by Detectron2
    which supports only Linux OS. I made modifications to the original Detectron2
    PointRend implementation to support Windows OS. PointRend implementation used
    for PixelLib supports both Linux and Windows OS.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '***PointRend*** 由 [Alexander Kirillov et al](https://arxiv.org/abs/1912.08193)
    提供，用于替代 Mask R-CNN 执行对象实例分割。***PointRend*** 是一种出色的最先进神经网络，用于实现对象分割。它生成准确的分割掩膜，并以高推理速度运行，满足了对准确和实时计算机视觉应用的日益增长的需求。我将
    PixelLib 与仅支持 Linux 操作系统的 Detectron2 的 Python 实现集成。我对原始 Detectron2 PointRend 实现进行了修改，以支持
    Windows 操作系统。用于 PixelLib 的 PointRend 实现支持 Linux 和 Windows 操作系统。'
- en: '**Note:** This article is based on performing instance segmentation using PyTorch
    and ***PointRend.*** If you want to learn how to perform instance segmentation
    with Tensorflow and Mask R-CNN read this [article](https://towardsdatascience.com/image-segmentation-with-six-lines-0f-code-acb870a462e8).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 本文基于使用 PyTorch 和 ***PointRend*** 进行实例分割。如果你想了解如何使用 Tensorflow 和 Mask
    R-CNN 进行实例分割，请阅读这篇 [文章](https://towardsdatascience.com/image-segmentation-with-six-lines-0f-code-acb870a462e8)。'
- en: '![Figure](../Images/22bc7c780b4afe84799ac42f72f0cfd6.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/22bc7c780b4afe84799ac42f72f0cfd6.png)'
- en: '[Original Image Source](https://unsplash.com/photos/6UWqw25wfLI) (left:MASK
    R-CNN, right:PointRend)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[原始图像来源](https://unsplash.com/photos/6UWqw25wfLI)（左：MASK R-CNN，右：PointRend）'
- en: '![Figure](../Images/cb489b9285153f06bae3e2511f8f1e0f.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/cb489b9285153f06bae3e2511f8f1e0f.png)'
- en: '[Original Image Source](https://unsplash.com/photos/rrI02QQ9GSQ) (left:MASK
    R-CNN, right:PointRend)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[原始图像来源](https://unsplash.com/photos/rrI02QQ9GSQ)（左：MASK R-CNN，右：PointRend）'
- en: The images labelled **PointRend** are obviously better segmentation results
    than **Mask R-CNN**.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 标签为 **PointRend** 的图像显然比 **Mask R-CNN** 的分割结果更好。
- en: Download & Installation
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 下载与安装
- en: '**Download Python**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**下载 Python**'
- en: PixelLib PyTorch supports python version 3.7 and above. Download a compatible
    [python version](https://www.python.org/).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: PixelLib PyTorch 支持 Python 版本 3.7 及以上。下载兼容的 [python 版本](https://www.python.org/)。
- en: '**Install PixelLib and its dependencies**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装 PixelLib 及其依赖项**'
- en: '**Install PyTorch**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装 PyTorch**'
- en: PixelLib PyTorch version supports these versions of PyTorch(***1.6.0,1.7.1,1.8.0***
    and ***1.90)***. PyTorch ***1.7.0*** is not supported and do not use any PyTorch
    version less than 1.6.0\. Install a compatible [PyTorch version](https://PyTorch.org/).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: PixelLib PyTorch 版本支持这些 PyTorch 版本（***1.6.0, 1.7.1, 1.8.0*** 和 ***1.90***）。PyTorch
    ***1.7.0*** 不受支持，请不要使用任何低于 1.6.0 的 PyTorch 版本。安装兼容的 [PyTorch 版本](https://PyTorch.org/)。
- en: '**Install Pycocotools**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装 Pycocotools**'
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Install PixelLib**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装 PixelLib**'
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**If installed, upgrade to the latest version using:**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果已安装，请使用以下命令升级到最新版本：**'
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Image Segmentation**'
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**图像分割**'
- en: PixelLib uses five lines of python code for performing object segmentation in
    images and videos with PointRend model. Download the [PointRend model](https://github.com/ayoolaolafenwa/PixelLib/releases/download/0.2.0/pointrend_resnet50.pkl).
    This is the code for image segmentation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: PixelLib 使用五行 Python 代码与 PointRend 模型在图像和视频中执行对象分割。下载 [PointRend 模型](https://github.com/ayoolaolafenwa/PixelLib/releases/download/0.2.0/pointrend_resnet50.pkl)。这是图像分割的代码。
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Line 1-4:** PixelLib package was imported and we also imported the class
    ***instanceSegmentation*** from the module ***pixellib.torchbackend.instance***
    (importing instance segmentation class from PyTorch support). We created an instance
    of the class and finally loaded the ***PointRend*** model we have downloaded.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 1-4 行：** 导入了 PixelLib 包，我们还从模块 ***pixellib.torchbackend.instance*** 导入了类
    ***instanceSegmentation***（从 PyTorch 支持中导入实例分割类）。我们创建了该类的一个实例，并最终加载了我们下载的 ***PointRend***
    模型。'
- en: '**Line 5:** We called the function ***segmentImage*** to perform segmentation
    of objects in images and added the following parameters to the function:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 5 行：** 我们调用了函数 ***segmentImage*** 来执行图像中的对象分割，并向函数添加了以下参数：'
- en: '***Image_path:*** This is the path to the image to be segmented.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Image_path:*** 这是待分割图像的路径。'
- en: '***Show_bbox:*** This is an optional parameter to show the segmented results
    with bounding boxes.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Show_bbox:*** 这是一个可选参数，用于显示带有边界框的分割结果。'
- en: '***Output_image_name:*** This is the name of the saved segmented image.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Output_image_name:*** 这是保存的分割图像的名称。'
- en: '**Sample Image for Segmentation**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**分割的示例图像**'
- en: '![Figure](../Images/a8a8466af953d96d9291e39b63072e43.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/a8a8466af953d96d9291e39b63072e43.png)'
- en: '[Original Image Source](https://commons.wikimedia.org/wiki/File:Carspotters.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[原始图像来源](https://commons.wikimedia.org/wiki/File:Carspotters.jpg)'
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Image After Segmentation**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**分割后的图像**'
- en: '![Image](../Images/03a6cceedbc5db33a5d577d457ae424b.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/03a6cceedbc5db33a5d577d457ae424b.png)'
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This log above may appear if you are running the segmentation code. It is not
    an error and the code will work fine.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在运行分割代码，上面的日志可能会出现。这不是错误，代码将正常运行。
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The segmentation results return a dictionary with values associated with the
    objects segmented in the image. The results printed will be in the following format:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 分割结果返回一个字典，其中包含与图像中分割物体相关的值。打印的结果将如下格式：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Detection Threshold**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**检测阈值**'
- en: PixelLib makes it possible to determine the detection threshold of object segmentation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: PixelLib使得确定物体分割的检测阈值成为可能。
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**confidence:** This is a new parameter introduced in the ***load_model***
    function and it is set to ***0.3*** to threshold the detections by ***30%.***
    The default value I set for detection threshold is ***0.5*** and it can be increased
    or decreased using the ***confidence*** parameter.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**confidence:** 这是在***load_model***函数中引入的新参数，设置为***0.3***以通过***30%***的阈值进行检测。我设置的检测阈值默认值为***0.5***，可以通过***confidence***参数进行增加或减少。'
- en: '**Speed Records**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**速度记录**'
- en: PixelLib makes it possible to perform real time object segmentation and added
    the ability to adjust the inference speed to suit real time predictions.  The
    default inference speed for processing a single image using Nvidia GPU with 4GB
    capacity is about ***0.26 seconds***.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: PixelLib 使得实时物体分割成为可能，并增加了调整推理速度以适应实时预测的能力。使用4GB容量的Nvidia GPU处理单张图像的默认推理速度约为***0.26秒***。
- en: '**Speed Adjustments**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**速度调整**'
- en: 'PixelLib supports speed adjustments and there are two types of speed adjustment
    modes which are ***fast*** and ***rapid*** modes:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: PixelLib支持速度调整，有两种速度调整模式：***fast***和***rapid***模式。
- en: '**1\. Fast Mode**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 快速模式**'
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the ***load_model*** function, we added the parameter ***detection_speed***
    and set the value to ***fast***. The fast mode achieves ***0.20 seconds*** for
    processing a single image.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在***load_model***函数中，我们添加了参数***detection_speed***并将其值设置为***fast***。快速模式实现了***0.20秒***的单张图像处理时间。
- en: '**Full Code for Fast Mode Detection**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**快速模式检测的完整代码**'
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**2\. Rapid Mode**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 快速模式**'
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the ***load_model*** function, we added the parameter ***detection_speed***
    and set the value to ***rapid***. The **rapid** mode achieves ***0.15 seconds***
    for processing a single image.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在***load_model***函数中，我们添加了参数***detection_speed***并将其值设置为***rapid***。**rapid**模式实现了***0.15秒***的单张图像处理时间。
- en: '**Full Code for Rapid Mode Detection**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**快速模式检测的完整代码**'
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**PointRend Models**'
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**PointRend模型**'
- en: There are two types of PointRend models used for object segmentation and they
    are of ***resnet50 variant*** and ***resnet101 variant***. The ***resnet50 variant***
    is used throughout this article because it is faster and of good accuracy. The
    ***resnet101 variant*** is more accurate but it is slower than ***resnet50 variant***.
    According to the [official reports](https://github.com/facebookresearch/detectron2/tree/main/projects/PointRend)
    of the models on Detectron2 the ***resnet50 variant*** achieves ***38.3 mAP***
    on COCO and ***resnet101 variant*** achieves ***40.1 mAP*** on COCO.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的 PointRend 模型用于物体分割，它们是***resnet50变体***和***resnet101变体***。本文章中使用的是***resnet50变体***，因为它速度更快且准确度较高。***resnet101变体***则更准确，但比***resnet50变体***慢。根据
    [官方报告](https://github.com/facebookresearch/detectron2/tree/main/projects/PointRend)，***resnet50变体***在COCO上实现了***38.3
    mAP***，而***resnet101变体***在COCO上实现了***40.1 mAP***。
- en: '**Speed Records for Resnet101:** The default speed for segmentation is ***0.5
    seconds***, fast mode is ***0.3 seconds*** while the rapid mode is ***0.25 seconds***.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**Resnet101的速度记录：** 分割的默认速度为***0.5秒***，快速模式为***0.3秒***，而快速模式为***0.25秒***。'
- en: '**Code for Resnet101 variant**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**Resnet101变体的代码**'
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The code for performing inference with the resnet101 model is the same, except
    we loaded the ***PointRend resnet101 model*** in the ***load_model*** function.
    Download the resnet101 model from [here](https://github.com/ayoolaolafenwa/PixelLib/releases/download/0.2.0/pointrend_resnet101.pkl).
    We added an extra parameter ***network_backbone*** in the ***load_model*** function
    and set the value to ***resnet101***.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 resnet101 模型进行推理的代码是相同的，只是我们在 ***load_model*** 函数中加载了 ***PointRend resnet101
    模型***。从 [这里](https://github.com/ayoolaolafenwa/PixelLib/releases/download/0.2.0/pointrend_resnet101.pkl)
    下载 resnet101 模型。我们在 ***load_model*** 函数中添加了一个额外的参数 ***network_backbone*** 并将其值设置为
    ***resnet101***。
- en: '**Note:** If you want to achieve high inference speed and good accuracy, use
    **PointRend** ***resnet50 variant***, but if you are more concerned about accuracy,
    use the **PointRend** ***resnet101 variant***. All these inference reports are
    based on using Nvidia GPU with 4GB capacity.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 如果你希望实现高推理速度和良好准确度，请使用**PointRend** ***resnet50变体***；但如果你更关注准确度，请使用**PointRend**
    ***resnet101变体***。所有这些推理报告均基于使用4GB容量的Nvidia GPU。'
- en: '**Custom Object Detection in Image Segmentation**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**图像分割中的自定义对象检测**'
- en: 'The PointRend model used is a pretrained COCO model which supports 80 classes
    of objects. PixelLib supports custom object detection which makes it possible
    to filter detections and ensure segmentation of target objects. We can choose
    out of the 80 classes of objects supported to match our target goal. These are
    the 80 classes of objects supported:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的 PointRend 模型是一个预训练的 COCO 模型，支持80类对象。PixelLib 支持自定义对象检测，可以过滤检测结果并确保目标对象的分割。我们可以从支持的80类对象中选择，以匹配我们的目标。这是支持的80类对象：
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Code for Segmentation of Target Classes**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标类别分割的代码**'
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The function ***select_target_classes*** was called to select the target objects
    to be segmented. The function ***segmentImage*** got a new parameter ***segment_target_classes***
    to choose from the target classes and filter the detections based on them. We
    filter the detections to detect only person in the image.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 ***select_target_classes*** 被调用以选择要分割的目标对象。函数 ***segmentImage*** 新增了一个参数
    ***segment_target_classes*** 用于从目标类别中进行选择，并根据这些类别过滤检测结果。我们过滤检测结果以仅检测图像中的人。
- en: '![Image](../Images/03a6cceedbc5db33a5d577d457ae424b.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../Images/03a6cceedbc5db33a5d577d457ae424b.png)'
- en: '**Object Extractions in Images**'
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**图像中的对象提取**'
- en: PixelLib makes it possible to extract and analyse objects segmented in an image.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: PixelLib 使得提取和分析图像中分割对象成为可能。
- en: '**Code for Object Extraction**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**对象提取的代码**'
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The code for image segmentation is the same, except we added extra parameters
    ***extract_segmented_objects*** and ***save_extracted_objects*** to extract segmented
    object and save the extracted objects respectively.  Each of the segmented objects
    will be saved as ***segmented_object_index*** e.g ***segmented_object_1***. The
    objects are saved based in the order in which they are extracted.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割的代码是相同的，只是我们新增了额外的参数 ***extract_segmented_objects*** 和 ***save_extracted_objects***
    分别用于提取分割对象和保存提取的对象。每个分割出的对象将保存为 ***segmented_object_index*** 例如 ***segmented_object_1***。对象的保存顺序与其提取顺序一致。
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Figure](../Images/8b1a301199f1b4ca6df20a01cd384dcc.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/8b1a301199f1b4ca6df20a01cd384dcc.png)'
- en: Note:  All the objects in the image are extracted and I chose to display only
    three of them.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：图像中的所有对象均已提取，我选择仅显示其中三个。
- en: '**Extraction of Object from Bounding Box Coordinates**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**从边界框坐标提取对象**'
- en: '[PRE18]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We introduced a new parameter ***extract_from_box*** to extract the objects
    segmented from their bounding boxes coordinates. Each of the extracted objects
    will be saved as ***object_extract_index*** e.g ***object_extract_1***. The objects
    are saved in the order in which they are extracted.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了一个新的参数 ***extract_from_box*** 用于从其边界框坐标中提取分割出的对象。每个提取的对象将保存为 ***object_extract_index***
    例如 ***object_extract_1***。对象的保存顺序与其提取顺序一致。
- en: '![Figure](../Images/948881319af9a109839b6ad02bbf7278.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/948881319af9a109839b6ad02bbf7278.png)'
- en: Extracts from Bounding Box Coordinates
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从边界框坐标提取
- en: '**Image Segmentation Output Visualization**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**图像分割输出可视化**'
- en: PixelLib makes it possible to regulate the visualization of images according
    to their resolutions.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: PixelLib 使得根据图像分辨率调节可视化成为可能。
- en: '[PRE19]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Figure](../Images/2eff7a7adc25c50baf6aac1db642d3ee.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/2eff7a7adc25c50baf6aac1db642d3ee.png)'
- en: '[Original Image Source](https://unsplash.com/photos/UiVe5QvOhao)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[原始图像来源](https://unsplash.com/photos/UiVe5QvOhao)'
- en: The visualization wasn’t visible because the ***text size***, and ***box thickness***
    are too slim. We can regulate the ***text size***, ***text thickness***, and ***box
    thickness*** to regulate the visualizations.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化效果不明显，因为 ***text size*** 和 ***box thickness*** 太细。我们可以调节 ***text size***、***text
    thickness*** 和 ***box thickness*** 来调整可视化效果。
- en: '**Modifications for Better Visualization**.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**为了更好的可视化效果**。'
- en: '[PRE20]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The segmentImage function accepted new parameters that regulate the thickness
    of texts and bounding boxes.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: segmentImage 函数接受了新的参数，用于调节文本和边框的厚度。
- en: '***text_size:*** The default text size is ***0.6*** and it is okay with images
    with moderate resolutions. It will be too samll for images with high resolutions.
    I increased it to 5.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***text_size:*** 默认文本大小为 ***0.6***，适用于中等分辨率的图像。对于高分辨率的图像来说太小。我将其增加到 5。'
- en: '***text_thickness:*** The default text thickness is 1\. I increased it to 4
    to match the image resolution.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***text_thickness:*** 默认文本厚度为 1。我将其增加到 4，以匹配图像分辨率。'
- en: '***box_thickness:*** The default box thickness is 2 and I changed it to 10
    to match the image resolution.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***box_thickness:*** 默认框厚度为 2，我将其更改为 10，以匹配图像分辨率。'
- en: '**Output Image with A Better Visualization**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**更好的可视化效果的输出图像**'
- en: '![Image](../Images/0841dec48cb891e893d87abb233a4a2d.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../Images/0841dec48cb891e893d87abb233a4a2d.png)'
- en: '**Note:** Regulate the parameters according to the resolutions of your images.
    The values I used for this sample image whose resolution is ***5760 x 3840*** might
    be too large if your image resolution is lower. You can increase the values of
    the parameters beyond the ones I set in this sample code if you have images whose
    resolutions are very high.***text_thickness*** and ***box_thickness*** parameters’
    values must be in integers and do not express their values in floating point numbers. ***text_size*** value
    can be expressed in both integers and floating point numbers.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 根据图像的分辨率调节参数。我为这张分辨率为 ***5760 x 3840*** 的样本图像使用的值可能对于分辨率较低的图像来说太大。如果您的图像分辨率非常高，可以将参数值增加到我在此示例代码中设置的值以上。***text_thickness***
    和 ***box_thickness*** 参数的值必须是整数，不要用浮点数表示。***text_size*** 值可以用整数和浮点数表示。'
- en: We discussed in detail in this article how to perform accurate and fast image
    segmentation and extraction of objects in images. We also described the upgrade
    added to PixelLib using PointRend that makes it possible for the library to match
    the increasing demand to balance between accuracy and speed performance in computer
    vision.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这篇文章中详细讨论了如何准确快速地进行图像分割和提取图像中的对象。我们还描述了使用 PointRend 对 PixelLib 进行的升级，使得库能够满足计算机视觉中准确性和速度性能之间的平衡需求。
- en: '**Note:** [Read the full tutorial](https://towardsdatascience.com/real-time-image-segmentation-using-5-lines-of-code-7c480abdb835)
    that includes how to perform object segmentation on a batch of images, videos
    and live camera feeds using PixelLib.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** [阅读完整教程](https://towardsdatascience.com/real-time-image-segmentation-using-5-lines-of-code-7c480abdb835)
    包括如何使用 PixelLib 对一批图像、视频和实时摄像头视频进行对象分割。'
- en: '**Bio: [Ayoola Olafenwa](https://www.linkedin.com/in/ayoola-olafenwa-003b901a9/)**
    is a self-taught programmer, technical writer, and a deep learning practitioner.
    Ayoola has developed two open source computer vision projects that are used by
    many developers across the globe, and presently works as a Machine Learning Engineer
    at DeepQuest AI building and deploying machine learning applications in the cloud.
    Ayoola''s areas of expertise are in computer vision and machine learning. She
    has experience working on machine learning systems, using deep learning libraries
    like PyTorch and Tensorflow to build and deploy machine learning models in production
    on cloud computing platforms like Azure using DevOp tools such as Docker, Pulumi
    and Kubernetes. Ayoola also works on deploying machine learning models on edge
    devices like Nvidia Jetson Nano and Raspberry PI devices using efficient frameworks
    like PyTorchMobile, TensorflowLite and ONNX Runtime.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**简历：[Ayoola Olafenwa](https://www.linkedin.com/in/ayoola-olafenwa-003b901a9/)**
    是一位自学成才的程序员、技术作家和深度学习实践者。Ayoola 开发了两个开源计算机视觉项目，这些项目被全球许多开发者使用，目前在 DeepQuest AI
    担任机器学习工程师，负责在云端构建和部署机器学习应用程序。Ayoola 的专长领域包括计算机视觉和机器学习。她有使用深度学习库如 PyTorch 和 Tensorflow
    在生产环境中构建和部署机器学习模型的经验，并在 Azure 云计算平台上使用 Docker、Pulumi 和 Kubernetes 等 DevOps 工具。Ayoola
    还在边缘设备如 Nvidia Jetson Nano 和 Raspberry PI 上使用 PyTorchMobile、TensorflowLite 和 ONNX
    Runtime 等高效框架部署机器学习模型。'
- en: '**Related:**'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关信息：**'
- en: '[Extraction of Objects In Images and Videos Using 5 Lines of Code](/2021/03/extraction-objects-images-videos-5-lines-code.html)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用5行代码提取图像和视频中的对象](/2021/03/extraction-objects-images-videos-5-lines-code.html)'
- en: '[Change the Background of Any Image with 5 Lines of Code](/2020/11/change-background-image-5-lines-code.html)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用5行代码更改任何图像的背景](/2020/11/change-background-image-5-lines-code.html)'
- en: '[Change the Background of Any Video with 5 Lines of Code](/2020/12/change-background-video-5-lines-code.html)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用5行代码更改任何视频的背景](/2020/12/change-background-video-5-lines-code.html)'
- en: More On This Topic
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Multi-modal deep learning in less than 15 lines of code](https://www.kdnuggets.com/2023/01/predibase-multi-modal-deep-learning-less-15-lines-code.html)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[少于15行代码的多模态深度学习](https://www.kdnuggets.com/2023/01/predibase-multi-modal-deep-learning-less-15-lines-code.html)'
- en: '[How to Build a Real-Time Recommendation Engine Using Graph Databases](https://www.kdnuggets.com/2023/08/build-realtime-recommendation-engine-graph-databases.html)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用图数据库构建实时推荐引擎](https://www.kdnuggets.com/2023/08/build-realtime-recommendation-engine-graph-databases.html)'
- en: '[Segment Anything Model: Foundation Model for Image Segmentation](https://www.kdnuggets.com/2023/07/segment-anything-model-foundation-model-image-segmentation.html)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Segment Anything Model：图像分割的基础模型](https://www.kdnuggets.com/2023/07/segment-anything-model-foundation-model-image-segmentation.html)'
- en: '[Feature Stores for Real-time AI & Machine Learning](https://www.kdnuggets.com/2022/03/feature-stores-realtime-ai-machine-learning.html)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实时AI和机器学习的特征存储](https://www.kdnuggets.com/2022/03/feature-stores-realtime-ai-machine-learning.html)'
- en: '[Real-time Translations with AI](https://www.kdnuggets.com/2022/07/realtime-translations-ai.html)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实时翻译与AI](https://www.kdnuggets.com/2022/07/realtime-translations-ai.html)'
- en: '[How Big Data Is Saving Lives in Real Time: IoV Data Analytics Helps…](https://www.kdnuggets.com/how-big-data-is-saving-lives-in-real-time-iov-data-analytics-helps-prevent-accidents)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大数据如何实时拯救生命：IoV数据分析帮助…](https://www.kdnuggets.com/how-big-data-is-saving-lives-in-real-time-iov-data-analytics-helps-prevent-accidents)'
