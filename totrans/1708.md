# 数据科学家需要掌握的10个统计技术

> 原文：[https://www.kdnuggets.com/2017/11/10-statistical-techniques-data-scientists-need-master.html](https://www.kdnuggets.com/2017/11/10-statistical-techniques-data-scientists-need-master.html)

[标题图片](../Images/192409ea180b5be09e1a283158eb89e2.png)

不管你对数据科学的吸引程度如何，无法忽视数据的持续重要性和我们分析、组织和情境化数据的能力。Glassdoor根据他们的大量就业数据和员工反馈，把数据科学家排名为美国最佳职位的第一名。所以这个职位是会长留的，但毫无疑问，数据科学家的具体工作内容将会发生变化。随着机器学习等技术越来越普及，以及深度学习等新兴领域在研究人员和工程师以及雇佣他们的公司中获得显著的发展，数据科学家继续乘坐着令人难以置信的创新和技术进步的浪潮。

* * *

## 我们的前3个课程推荐

[谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 开启网络安全职业的快车道。

[谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

[谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的IT组织

* * *

拥有较强的编码能力固然重要，但数据科学并不全是关于软件工程（事实上，熟悉Python就足够了）。数据科学家生活在编码、统计学和批判性思维的交叉点上。正如Josh Wills所说，“数据科学家是一个统计学家比任何程序员更擅长统计分析，比任何统计学家更擅长编写程序的人。”我个人知道太多软件工程师希望转行成为数据科学家，盲目地使用诸如TensorFlow或Apache Spark等机器学习框架，却对其背后的统计理论缺乏全面的了解。这就引申出了对统计学习的研究，这是一个从统计学和函数分析领域汲取的机器学习的理论框架。

**为什么学习统计学习？**理解各种技术背后的思想很重要，以便知道何时以及如何使用它们。必须先理解更简单的方法，才能掌握更复杂的方法。准确评估方法的性能很重要，以了解它的工作效果如何。此外，这是一个充满激情的研究领域，在科学、工业和金融等方面具有重要应用。最终，统计学习是培养现代数据科学家的基本要素。统计学习问题的例子包括：

+   识别前列腺癌的风险因素。

+   基于对频谱图进行对数周期图分类一个记录下来的音素。

+   根据人口统计、饮食和临床测量预测某人是否会发生心脏病发作。

+   自定义电子邮件垃圾检测系统。

+   识别手写邮政编码中的数字。

+   将组织样本分类为多种癌症类型之一。

+   在人口调查数据中建立薪水和人口统计变量之间的关系。

在我上大学的最后一个学期，我进行了一项有关数据挖掘的自主研究。课程涵盖了来自3本书的广泛材料：[《统计学习导论》](http://www-bcf.usc.edu/~gareth/ISL/) (Hastie, Tibshirani, Witten, James)、[《贝叶斯数据分析》](https://sites.google.com/site/doingbayesiandataanalysis/) (Kruschke) 和[《时间序列分析与应用》](http://www.stat.pitt.edu/stoffer/tsa4/) (Shumway, Stoffer)。我们做了很多关于贝叶斯分析、马尔可夫链蒙特卡洛、层次建模、有监督和无监督学习的练习。这次经历加深了我对数据挖掘学术领域的兴趣，并让我决定进一步专业化。最近，我完成了在斯坦福大学 Lagunita 平台上的[统计学习在线课程](https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about)，该课程涵盖了我在自主研究中阅读的[**《统计学习导论》这本书**](https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370)的所有内容。现在有两次接触这些内容后，我想分享书中的十个统计技术，我相信任何数据科学家都应该学习这些技术以更有效地处理大数据集。

在介绍这10种技术之前，我想区分统计学习和机器学习。我之前写过[一篇关于机器学习的最受欢迎的 Medium 文章](https://gab41.lab41.org/the-10-algorithms-machine-learning-engineers-need-to-know-f4bb63f5b2fa)，所以我有足够的专业知识来解释这些区别：

+   机器学习作为人工智能的一个子领域而出现。

+   统计学习作为统计学的一个子领域而出现。

+   机器学习更加强调大规模应用和预测准确性。

+   统计学习强调模型及其可解释性、精确性和不确定性。

+   但是区分变得越来越模糊，并且有很多“交叉授粉”。

+   机器学习在市场营销中占据优势！

****1 — 线性回归(Linear Regression)****:

在统计学中，线性回归是通过拟合因变量和自变量之间的*最佳线性关系*来预测目标变量的方法。*最佳拟合*是通过确保在每个点上形状与实际观测之间的所有距离之和尽可能小来实现的。在选择形状的情况下，最佳拟合的形状意味着没有其他位置会产生更小的误差。线性回归的两种主要类型是*简单线性回归(Simple Linear Regression)*和*多元线性回归(Multiple Linear Regression)*。*简单线性回归*使用单个自变量来预测因变量，通过拟合最佳线性关系。*多元线性回归*使用多个自变量来预测因变量，通过拟合最佳线性关系。

![](../Images/ef9c3a012f80c3ae290f8a8106b0e553.png)

从你日常生活中选择任意2样与之相关的东西。比如，我有过去3年的月度支出、月收入和每月旅行次数的数据。现在我需要回答以下问题：

+   我明年的每月支出会是多少？

+   月收入和每月旅行次数中哪个因素对于决定我的月度支出更重要？

+   月收入和每月旅行次数与月度消费有关系吗？

****2 — 分类(Classification)****:

分类是一种数据挖掘技术，它将数据集合分配到不同的类别，以帮助进行更准确的预测和分析。有时也称为决策树，分类是几种旨在使大规模数据集分析有效的方法之一。两种主要的分类技术突出了出来：*逻辑回归(Logistic Regression)*和*判别分析(Discriminant Analysis)*。

*逻辑回归(Logistic Regression)*是适用于因变量是二元（二进制）的回归分析。与所有回归分析一样，逻辑回归是一种预测分析。逻辑回归用于描述数据，并解释一个因变的二元变量与一个或多个名义、序数、区间或比例级自变量之间的关系。逻辑回归可以回答以下类型的问题：

+   每多一磅超重和每天吸一包香烟，患肺癌（是或否）的概率会如何变化？

+   体重摄入热量、脂肪摄入和参与者的年龄是否对心脏病发作（是或否）有影响？

![](../Images/b938a7f44237a2491b90de2f19a3c6c7.png)

在**判别分析**中，2个或更多个群体或群集或种群是先验已知的，并且1个或更多个新的观察结果是基于测量特征分类到已知群体中的。判别分析对预测变量X分别在响应类别中建模，然后使用贝叶斯定理将其转换为响应类别给定X值的概率估计。这样的模型可以是*线性*或*二次***。**

+   **线性判别分析** 计算每个观察结果的“判别分数”以分类响应变量类别。这些分数是通过找到独立变量的线性组合获得的。它假设每个类别内的观察值都是从多元高斯分布中抽取的，并且预测变量的协方差在响应变量Y的所有k级别上都是公共的。

+   **二次判别分析** 提供了另一种方法。与LDA一样，QDA假设来自Y的每个类别的观测值都是从高斯分布中抽取的。然而，与LDA不同，QDA假设每个类别都有自己的协方差矩阵。换句话说，预测变量不假设在Y的每个k级别上具有共同的方差。

****3 — 重新采样方法****

重新采样是一种从原始数据样本中重复抽取样本的方法。这是一种非参数统计推断方法。换句话说，重新采样方法不涉及利用通用分布表来计算近似p概率值。

重新采样会基于实际数据生成一个独特的采样分布。它使用实验方法而不是分析方法来生成独特的采样分布。因为它基于研究人员研究的所有可能结果的无偏样本，所以它产生无偏估计。为了理解重新采样的概念，您应该了解*自助法*和*交叉验证*这些术语：

![](../Images/13226e7dc419f46ee9646e6742c25c15.png)

+   **自助法** 是一种有助于许多情况的技术，如验证预测模型性能，集成方法，估计模型的偏差和方差。它通过从原始数据中进行替换抽样，并将“*未选择*”的数据点作为测试用例。我们可以多次进行这个过程，并计算平均得分作为我们模型性能的估计。

+   另一方面，**交叉验证** 是一种用于验证模型性能的技术，通过将训练数据分成k个部分来完成。我们把k-1个部分作为训练集，将“*留出*”部分作为测试集。我们以不同方式重复k次。最后，我们将k个得分的平均值作为我们的性能估计。

通常对于线性模型，最小二乘法是将其拟合到数据中所考虑的主要标准。接下来的3种方法是可提供更好的预测精度和模型可解释性的拟合线性模型的替代方法。

****4 — 子集选择:****

这种方法确定了我们认为与响应相关的 *p* 个预测变量的一个子集。然后，我们使用该子集特征的最小二乘法拟合模型。

![](../Images/6379988a88899a4e3fce74341f2ba5a0.png)

+   **最佳子集选择:** 在这里，我们为 *p* 个预测变量的每个可能的组合拟合一个单独的OLS回归，然后查看得到的模型拟合。算法分为2个阶段：(1) 拟合包含 *k* 个预测变量的所有模型，其中 *k* 是模型长度的最大值，(2) 使用交叉验证的预测误差选择一个单一模型。使用 *测试* 或 *验证误差* 来评估模型拟合，而不是训练误差，因为RSS和R²随着更多变量的增加而单调增加。最好的方法是交叉验证，并选择在测试误差估计中R²最高、RSS最低的模型。

+   **前向逐步选择** 考虑了一个更小的预测子集。它从一个不包含预测变量的模型开始，然后逐个将预测变量添加到模型中，直到所有的预测变量都在模型中。被添加的变量的顺序是能够最大程度改善拟合的变量，直到没有更多的变量通过交叉验证的预测误差来改善模型拟合。

+   **后向逐步选择** 从包含所有 *p* 预测变量的模型开始，然后逐步地一个一个去除最不重要的预测变量。

+   **混合方法** 遵循前向逐步的方法，然而，在添加每个新变量之后，该方法也可能去除对模型拟合没有贡献的变量。

****5 — 收缩:****

此方法拟合涉及所有 *p* 个预测变量的模型，然而，相对于最小二乘估计，估计系数被收缩至零。这种收缩，也称为 *正则化* ，能够减少方差。根据进行的收缩类型，有些系数可能被估计为确切的零。因此，此方法还执行变量选择。将系数估计值收缩至零的两种最知名的技术是 *岭回归* 和 *套索*。

![](../Images/1b75295dced1b4ca5439ddd13fa90199.png)

+   **岭回归**类似于最小二乘法，只是系数的估计通过最小化稍有不同的量来进行。岭回归和OLS一样，寻求减少RSS的系数估计，但当系数接近零时也有收缩惩罚。这种惩罚的效果是将系数估计收缩至零。不详细说明数学问题，但要知道，岭回归会将方差最小的特征收缩。如在主成分分析中，岭回归将数据投影到*d*的方向空间，然后比高方差分量更多地收缩低方差分量的系数，这相当于最大和最小主成分。

+   岭回归至少有一个缺点；它在最终模型中包括了所有的*p*个预测因子。惩罚项会将其中的许多因子设置得接近于零，但从不是*完全的*零。这通常不会影响预测的准确性，但会使模型更难以解释结果。**Lasso**克服了这个缺点，并有能力将其中某些系数强制设置为零，前提是*s*足够小。当*s* = 1时，结果为正常的OLS回归，当*s*接近0时，系数趋向于零。因此，Lasso回归也可以进行变量选择。

### 更多关于此主题的内容

+   [每个初学者数据科学家都应精通的六种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)

+   [构建稳固的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)

+   [使用管道编写整洁的Python代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)

+   [成为一名优秀数据科学家所需的五个关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)

+   [2021年最佳ETL工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)

+   [停止学习数据科学，找到目标并寻找目标…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)
