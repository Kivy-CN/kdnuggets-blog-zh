- en: 'Many Heads Are Better Than One: The Case For Ensemble Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多人智慧胜过单人：集成学习的案例
- en: 原文：[https://www.kdnuggets.com/2019/09/ensemble-learning.html](https://www.kdnuggets.com/2019/09/ensemble-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/09/ensemble-learning.html](https://www.kdnuggets.com/2019/09/ensemble-learning.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Jay Budzik](https://www.linkedin.com/in/jaybudzik/), ZestFinance**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：[Jay Budzik](https://www.linkedin.com/in/jaybudzik/)，ZestFinance**。'
- en: “The interests of truth require a diversity of opinions.” —J. S. Mill
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “真理的利益需要多样的意见。” —J. S. Mill
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行IT管理'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Banks and lenders are increasingly turning to AI and machine learning to automate
    their core functions and make more accurate predictions in credit underwriting
    and fraud detection. ML practitioners can take advantage of a growing number of
    modeling algorithms, such as simple decision trees, random forests, gradient boosting
    machines, deep neural networks, and support vector machines. Each method has its
    strengths and weaknesses, which is why it often makes sense to combine ML algorithms
    to provide even greater predictive performance than any single ML method could
    provide on its own. (This is our standard practice at ZestFinance in every project.)
    This method of combining algorithms is known as ensembling.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 银行和贷款机构越来越多地转向AI和机器学习，以自动化其核心功能，并在信贷承保和欺诈检测中做出更准确的预测。ML从业者可以利用越来越多的建模算法，例如简单决策树、随机森林、梯度提升机、深度神经网络和支持向量机。每种方法都有其优缺点，这也是为什么将ML算法结合起来往往能提供比任何单一ML方法更出色的预测性能。（这是我们在ZestFinance每个项目中的标准做法。）这种结合算法的方法被称为集成。
- en: Ensembles improve generalization performance in many scenarios, including classification,
    regression, and class probability estimation. Ensemble methods have set numerous
    world records on challenging datasets. An ensemble model won [the Netflix Prize](https://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf)
    and [international data science competitions](https://mlwave.com/kaggle-ensembling-guide/)
    in almost every domain, including predicting [credit risk](https://nycdatascience.com/blog/student-works/kaggle-predict-consumer-credit-default/)
    and [classifying videos](https://www.kaggle.com/c/youtube8m-2018/discussion/62781).
    While ensembles are generally understood to perform better than single-model predictive
    functions, they are notoriously hard to set up, operate, and explain. These challenges
    are falling away with the invention of better modeling, explainability and monitoring
    tools, which we will touch on at the end of this post.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法在许多场景下提高了泛化性能，包括分类、回归和类别概率估计。集成方法在具有挑战性的数据集上创造了众多世界纪录。一个集成模型赢得了[Netflix奖](https://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf)和[国际数据科学竞赛](https://mlwave.com/kaggle-ensembling-guide/)，几乎涵盖了所有领域，包括[信用风险预测](https://nycdatascience.com/blog/student-works/kaggle-predict-consumer-credit-default/)和[视频分类](https://www.kaggle.com/c/youtube8m-2018/discussion/62781)。虽然集成方法通常被认为比单一模型的预测功能表现更好，但它们
    notoriously 难以设置、操作和解释。随着更好的建模、可解释性和监控工具的发明，这些挑战正在减少，我们将在这篇文章的最后讨论这些工具。
- en: How ensemble models achieve better performance
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成模型如何实现更好的性能
- en: Just as diversity in nature contributes to more robust biological systems, ensembles
    of ML models produce stronger results by combining the strengths (and compensating
    for the weaknesses) of multiple submodels. Neural networks require explicit handling
    of missing values prior to modeling, but gradient boosted trees handle them automatically.
    Modelers can introduce bias and errors in the act of making choices about how
    to handle missing data for neural networks. Error and bias can also enter in from
    the choices the modeling method makes in the case of gradient boosted trees. Combining
    different methods (for instance, by averaging or blending the scores) can improve
    predictions. More specifically, ensembles reduce bias and variance by incorporating
    different estimators with different patterns of error, diminishing the impact
    of a single source of error.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 就像自然界中的多样性有助于更强健的生物系统一样，机器学习模型的集成通过结合多个子模型的优势（并弥补其不足）产生更强的结果。神经网络需要在建模之前显式处理缺失值，而梯度提升树则自动处理这些缺失值。建模者在处理神经网络的缺失数据时，可能会引入偏差和错误。梯度提升树的方法选择也可能引入错误和偏差。通过结合不同的方法（例如，通过平均或混合分数），可以改善预测。更具体地说，集成通过结合具有不同错误模式的不同估计器来减少偏差和方差，从而降低单一错误来源的影响。
- en: There are countless ways to apply ensemble techniques. Submodels can work on
    different raw input data, and you can even use submodels to generate features
    for another model to consume. For example, you could train a model on each segment
    of the data, such as different income levels, and combine the results.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 应用集成技术的方法有无数种。子模型可以处理不同的原始输入数据，你甚至可以使用子模型生成特征供另一个模型使用。例如，你可以对数据的每个段（如不同收入水平）训练一个模型，然后将结果结合起来。
- en: Types of ensembles and how they work
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成方法的类型及其工作原理
- en: 'There are four major flavors of ensemble learning:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习有四种主要类型：
- en: In **bagging**, we use [bootstrap sampling](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))
    to obtain subsets of data for training a set of base models. Bootstrap sampling
    is the process of using increasingly large random samples until you achieve diminishing
    returns in predictive accuracy. Each sample is used to train a separate decision
    tree, and the results of each model are aggregated. For classification tasks,
    each model votes on an outcome. In regression tasks, the model result is averaged.
    Base models with low bias but high variance are well-suited for bagging. [Random
    forests](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf), which are
    bagged combinations of decision trees, are the canonical example of this approach.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**装袋（bagging）**中，我们使用[自助采样](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))来获取用于训练一组基础模型的数据子集。自助采样是使用逐渐增大的随机样本，直到预测准确性达到递减回报。每个样本用于训练一个单独的决策树，并对每个模型的结果进行聚合。对于分类任务，每个模型对结果进行投票。在回归任务中，模型结果取平均。具有低偏差但高方差的基础模型非常适合用于装袋。[随机森林](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)，即装袋的决策树组合，是这一方法的经典例子。
- en: '![](../Images/e9e7fadf6c3c4ac89ad29189f35eb079.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9e7fadf6c3c4ac89ad29189f35eb079.png)'
- en: In **boosting**, we improve performance by concentrating modeling efforts on
    the data that results in more errors (i.e., focus on the hard stuff). We train
    a sequence of models where more weight is given to examples that were misclassified
    by earlier iterations. As with bagging, classification tasks are solved through
    a weighted majority vote, and regression tasks are solved with a weighted sum
    to produce the final prediction. Base models with a low variance but high bias
    are well-adapted for boosting. [Gradient boosting](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf)
    is a famous example of this approach.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**提升（boosting）**中，我们通过将建模努力集中于导致更多错误的数据（即关注困难的数据）来提高性能。我们训练一系列模型，对早期迭代中被错误分类的示例给予更多权重。与装袋（bagging）一样，分类任务通过加权多数投票来解决，回归任务则通过加权求和来得出最终预测。具有低方差但高偏差的基础模型非常适合用于提升。[梯度提升](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf)是这一方法的一个著名例子。
- en: '![](../Images/095036a5951d8ca57b5d4fb86307e42e.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/095036a5951d8ca57b5d4fb86307e42e.png)'
- en: In **stacking**, we create an ensemble function that combines the outputs from
    multiple base models into a single score. The base-level models are trained based
    on a complete dataset, and then their outputs are used as input features to train
    an ensemble function. Usually, the ensemble function is a simple linear combination
    of the base model scores.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**堆叠**中，我们创建一个集成函数，将多个基础模型的输出组合成一个单一的分数。基础级模型基于完整的数据集进行训练，然后其输出作为输入特征来训练集成函数。通常，集成函数是基础模型分数的简单线性组合。
- en: '![](../Images/db8f766123f78a6b59bb8d51927afefd.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db8f766123f78a6b59bb8d51927afefd.png)'
- en: At ZestFinance, we prefer an even more powerful approach called **deep stacking**,
    which uses stacked ensembles with nonlinear ensemble functions that take both
    model scores as inputs and the input data itself. These are useful in uncovering
    the deeper relationships between submodels and produce greater accuracy than simple
    linear stacked ensembles by learning when to apply each submodel based on its
    strengths. Deep stacking allows the model to select the right submodel weights
    based on specific input variables (like a product segment, income band, or marketing
    channel) to boost performance even further.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在ZestFinance，我们更喜欢一种更强大的方法，称为**深度堆叠**，它使用堆叠集成与非线性集成函数，这些函数同时将模型分数和输入数据作为输入。这些方法有助于揭示子模型之间更深层次的关系，并通过根据每个子模型的优势来学习何时应用每个子模型，从而比简单的线性堆叠集成具有更高的准确性。深度堆叠允许模型根据特定的输入变量（如产品细分、收入带或营销渠道）选择正确的子模型权重，以进一步提高性能。
- en: '![](../Images/346c03a047c218c3521c41cbdb0c29a1.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/346c03a047c218c3521c41cbdb0c29a1.png)'
- en: Results on a real-world credit risk model
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 真实世界信用风险模型的结果
- en: To demonstrate the superior performance of ensemble learning, we built a series
    of binary classification models to predict defaults from a database of auto loans
    made over three recent years. The loan data included more than 100,000 borrowers
    and more than 1,100 features.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示集成学习的卓越表现，我们建立了一系列二元分类模型，以预测从过去三年内发放的汽车贷款数据库中的违约情况。贷款数据包括超过100,000名借款人和超过1,100个特征。
- en: 'The competition was between six base machine learning models: four XGBoost
    models and two neural network models built using features from different sets
    of credit bureau data, and a combined ensemble model stacking these six base models
    using a neural network.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 竞争是在六个基础机器学习模型之间进行的：四个XGBoost模型和两个神经网络模型，这些模型使用来自不同信用局数据集的特征构建，并结合一个集成模型，该模型使用神经网络堆叠这六个基础模型。
- en: To evaluate the accuracy of the models, we measured their [AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve)
    and [KS](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) scores
    on the validation data. AUC (area under the receiver operating curve) is used
    to measure a model’s false positive and false negative rates, while KS (short
    for Kolmogorov–Smirnov test) is used to compare data distributions. Higher numbers
    correlate to better business results.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型的准确性，我们测量了它们在验证数据上的[AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve)和[KS](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test)分数。AUC（接收器工作特征曲线下面积）用于衡量模型的假阳性和假阴性率，而KS（Kolmogorov–Smirnov检验的简称）用于比较数据分布。较高的数值与更好的业务结果相关。
- en: Below are the results of how each model performs in predictive accuracy and
    dollars saved through lower losses compared to [a logistic regression baseline
    model](https://www.celent.com/system/media_documents/documents/794/651/760/original/557191332.pdf).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是每个模型在预测准确性和通过减少损失节省的美元方面的表现，与[逻辑回归基准模型](https://www.celent.com/system/media_documents/documents/794/651/760/original/557191332.pdf)相比。
- en: '| **Model Type** | **AUC** | **KS** | **Est. Dollars Saved** |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| **Model Type** | **AUC** | **KS** | **Est. Dollars Saved** |'
- en: '| **Ensemble** | 0.803 | 0.446 | $21M |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| **Ensemble** | 0.803 | 0.446 | $21M |'
- en: '| **XGB 1** | 0.791 (2%) | 0.420 (6%) | $18M (14%) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| **XGB 1** | 0.791 (2%) | 0.420 (6%) | $18M (14%) |'
- en: '| **XGB 2** | 0.791 (2%) | 0.428 (4%) | $18M (12%) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| **XGB 2** | 0.791 (2%) | 0.428 (4%) | $18M (12%) |'
- en: '| **XGB 3** | 0.781 (3%) | 0.411 (9%) | $17M (16%) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| **XGB 3** | 0.781 (3%) | 0.411 (9%) | $17M (16%) |'
- en: '| **XGB 4** | 0.782 (3%) | 0.413 (8%) | $17M (16%) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| **XGB 4** | 0.782 (3%) | 0.413 (8%) | $17M (16%) |'
- en: '| **ANN 1** | 0.750 (7%) | 0.376 (19%) | $16M (19%) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **ANN 1** | 0.750 (7%) | 0.376 (19%) | $16M (19%) |'
- en: '| **ANN 2** | 0.786 (2%) | 0.430 (4%) | $18M (13%) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| **ANN 2** | 0.786 (2%) | 0.430 (4%) | $18M (13%) |'
- en: Our ensemble model produces the highest AUC (0.803), which means that 80% of
    the time our ensemble ranks a random good applicant more highly than a random
    bad one. The ensemble’s AUC was 2% better than the best XGBoost and neural network
    models, which might not sound like much, but AUC is in log scale, so even small
    increases are more impactful than they appear (similar to the Richter scale for
    earthquakes). This $500 million-dollar loan business would save $3 million more
    per year by using the ensemble compared to a single XGBoost or neural network.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的集成模型产生了最高的AUC（0.803），这意味着80%的情况下，我们的集成模型将随机的优质申请人排在随机的劣质申请人之上。集成模型的AUC比最佳的XGBoost和神经网络模型高出2%，这听起来可能不多，但AUC是对数刻度的，所以即使是小幅度的增加也比它们看起来的影响要大（类似于地震的里氏震中）。这个5亿美元的贷款业务通过使用集成模型相比于单一的XGBoost或神经网络模型每年将节省300万美元。
- en: The advantages of ensembling extend beyond predictive accuracy. They also benefit
    the business by improving stability over time. To verify that, using the validation
    set, we computed the daily AUC score over six months for the ensemble and the
    submodels on their own. The ensemble model has a 3% lower AUC variance across
    that time period than the best-performing neural network model and a 21% lower
    AUC variance than the best-performing XGBoost model. Better stability leads to
    more predictable results for the business over time.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法的优势不仅限于预测准确性。它们还通过随着时间的推移提高稳定性来惠及业务。为了验证这一点，我们使用验证集计算了集成模型和子模型在六个月期间的每日AUC分数。集成模型在这段时间内的AUC方差比表现最佳的神经网络模型低3%，比表现最佳的XGBoost模型低21%。更好的稳定性导致业务结果的可预测性更高。
- en: Challenges with ensembles
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成方法的挑战
- en: We’ve seen above that ensembles produce better economics and better stability
    in a real-world credit risk modeling problem. However, using complex ensembles
    in production can be tricky.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上面看到，集成方法在实际的信用风险建模问题中提供了更好的经济效益和更好的稳定性。然而，在生产中使用复杂的集成模型可能会很棘手。
- en: '***Engineering complexity***'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '***工程复杂性***'
- en: The  we mentioned above? [It was too complex for even Netflix to put into production.](https://www.wired.com/2012/04/netflix-prize-costs/)
    Technical dependencies, runtime performance, and model verification are all practical
    challenges associated with using these kinds of models in production. We designed
    ZAML software tools from the ground up to manage these engineering complexities
    and we are proud to have many customers with world-class ensembles operating in
    production.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们上面提到的那个？[即使是Netflix也无法投入生产。](https://www.wired.com/2012/04/netflix-prize-costs/)
    技术依赖、运行时性能和模型验证都是在生产中使用这些模型的实际挑战。我们从头开始设计了ZAML软件工具来管理这些工程复杂性，我们很自豪地拥有许多世界级的集成模型在生产中运行。
- en: Producing accurate explanations for the results of ensemble models is particularly
    challenging. Many explainability methods are model-dependent and do not [make
    problematic assumptions.](https://www.zestfinance.com/blog/part-deux-why-not-just-use-shap)
    Deeply stacked ensembles exacerbate the situation, as each model may be continuous
    or discrete, and the combination of these two distinct types of functions is challenging
    for even the most advanced analysis techniques.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为集成模型结果提供准确解释尤其具有挑战性。许多可解释性方法依赖于模型，并且不[做出有问题的假设。](https://www.zestfinance.com/blog/part-deux-why-not-just-use-shap)
    深度堆叠的集成模型使情况更加复杂，因为每个模型可能是连续的或离散的，这两种不同类型的函数的组合对于即使是最先进的分析技术也是一个挑战。
- en: In prior posts, we showed how commonly used techniques like [LOCO, PI, and LIME
    generate inaccurate explanations](https://www.zestfinance.com/snake-oil-explainability)
    even for simple toy models. They are inaccurate and slow to run on simple models
    and, as expected, the situation gets worse with more complex models. [Integrated
    Gradients (IG)](https://arxiv.org/abs/1703.01365), an explainer technique based
    on the [Aumann-Shapley value](https://en.wikipedia.org/wiki/Shapley_value#Aumann%E2%80%93Shapley_value),
    works on neural networks and other continuous functions. But, you can’t combine
    a neural network with a tree-based model like [XGBoost](https://xgboost.readthedocs.io/en/latest/).
    If you do, IG is rendered useless.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的帖子中，我们展示了像 [LOCO, PI 和 LIME 生成不准确解释](https://www.zestfinance.com/snake-oil-explainability)
    等常用技术，即使对于简单的玩具模型也是如此。它们在简单模型上不准确且运行缓慢，随着模型复杂性的增加，情况变得更糟。[集成梯度（IG）](https://arxiv.org/abs/1703.01365)，一种基于
    [奥曼-沙普利值](https://en.wikipedia.org/wiki/Shapley_value#Aumann%E2%80%93Shapley_value)
    的解释技术，适用于神经网络和其他连续函数。但是，你不能将神经网络与基于树的模型如 [XGBoost](https://xgboost.readthedocs.io/en/latest/)
    结合。如果这样做，IG 就会失效。
- en: '[SHAP](https://github.com/slundberg/shap) TreeExplainer works only on decision
    trees, not neural networks. And while some ML engineers have started to use [SHAP](https://github.com/slundberg/shap)
    KernelExplainer, which claims to work on any model, it makes some problematic
    assumptions about variable independence and whether averages can be substituted
    for missing values.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[SHAP](https://github.com/slundberg/shap) TreeExplainer 仅对决策树有效，不适用于神经网络。尽管一些机器学习工程师已开始使用
    [SHAP](https://github.com/slundberg/shap) KernelExplainer，它声称适用于任何模型，但它对变量独立性和是否可以用平均值替代缺失值做了一些有问题的假设。'
- en: What’s more, you’re often not just trying to understand what drives a model’s
    score.  Instead, you’re trying to understand a model-based decision. Understanding
    a model-based decision means taking into account how a model’s score is applied. 
    Before a decision is made based on a model, the model’s score usually goes through
    some transformations, like putting it on a 0-1 scale and making scores greater
    than 90% correspond to the lowest decile of the model score. These kinds of transformations
    must be carefully considered when generating an explanation. Unfortunately, this
    is where many open source tools, and even the smartest data scientists, can run
    into challenges.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，你通常不仅仅是试图理解模型得分的驱动因素。相反，你试图理解基于模型的决策。理解基于模型的决策意味着考虑模型得分的应用方式。在基于模型做出决策之前，模型的得分通常会经过一些转换，例如将其放在
    0-1 范围内，并使得高于 90% 的得分对应于模型得分的最低十分位。这些转换在生成解释时必须仔细考虑。不幸的是，这也是许多开源工具，甚至最聪明的数据科学家，可能会遇到挑战的地方。
- en: ZAML customers benefit from an explainability method that does not suffer from
    such limitations. It can provide accurate explanations for virtually any possible
    ensemble of machine learning functions. ZAML makes this better kind of model safe
    to use.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ZAML 客户可以从一种不受这些限制的可解释性方法中受益。它可以为几乎任何可能的机器学习函数集提供准确的解释。ZAML 使这种更好的模型使用起来更安全。
- en: '***Compliance***'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '***合规性***'
- en: Unlike movie recommendation systems, credit risk models must undergo intensive
    analysis and scrutiny to comply with consumer lending laws and regulations. An
    ensemble credit underwriting model using thousands of variables adds additional
    complexity to the task of verifying that the model will perform as expected under
    a broad variety of conditions. Having the right explainability math is only part
    of the solution. It is also important to document the model build and analysis
    so that it is possible for another practitioner to reproduce the work, produce
    intelligible reasons why an applicant was denied (adverse action), perform fair
    lending analysis, search for less biased models, and provide adequate monitoring
    so you know when your model needs to be refreshed. All of these tasks become more
    complex when you have ensembles of multiple submodels, each using a different
    feature space and model target. Fortunately, ZAML tools automate large swaths
    of this process.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与电影推荐系统不同，信用风险模型必须经过严格分析和审查，以遵守消费者贷款法律和规定。使用数千个变量的集成信用承保模型增加了验证模型在各种条件下是否按预期表现的复杂性。拥有正确的可解释性数学仅仅是解决方案的一部分。记录模型构建和分析也很重要，以便其他从业者能够复制工作，提供为什么申请人被拒绝的清晰理由（不利行动），执行公平贷款分析，寻找较少偏见的模型，并提供足够的监控以了解模型何时需要更新。当你有多个子模型时，每个子模型使用不同的特征空间和模型目标，所有这些任务会变得更加复杂。幸运的是，ZAML
    工具可以自动化大量这一过程。
- en: Conclusion
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: We have seen above how ensembling produces more accurate and stable predictions,
    which translates into more profitable business results. Those benefits come with
    additional complexity, but lenders seeking the very best predictive accuracy and
    stability now have tools like ZAML to help manage the task. ZAML-powered ensemble
    models have been helping U.S. lenders beat their competition for years. Diversity
    is a powerful tool in evolution and political debate — and makes for far better
    underwriting results, too.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，集成学习如何产生更准确和稳定的预测，从而转化为更有利可图的商业结果。这些好处带来了额外的复杂性，但寻求最佳预测准确性和稳定性的贷款机构现在有像
    ZAML 这样的工具来帮助管理任务。使用 ZAML 驱动的集成模型已经帮助美国贷款机构多年战胜竞争对手。多样性是进化和政治辩论中的强大工具 —— 同样，它也能带来更好的承保结果。
- en: '**Bio: ****[Jay Budzik](https://www.linkedin.com/in/jaybudzik/) **is the CTO
    of ZestFinance with track record of driving revenue growth by applying AI and
    ML to create new products and services that gain market share. As a computer scientist
    earning a Ph.D. from Northwestern with a background in AI, ML, big data, and NLP,
    Jay has been an inventor and entrepreneur with experience in finance and web-scale
    media and advertising: by developing and deploying successful products and building
    and cultivating the great teams behind them.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：** [Jay Budzik](https://www.linkedin.com/in/jaybudzik/) 是 ZestFinance
    的首席技术官，拥有通过应用 AI 和 ML 来推动收入增长的成功记录。他在西北大学获得计算机科学博士学位，背景涉及 AI、ML、大数据和 NLP，Jay 曾是一名发明家和企业家，拥有金融和网络规模媒体及广告的经验：通过开发和部署成功的产品以及建设和培养优秀团队来取得成果。'
- en: '**Related:**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Ensemble Learning: 5 Main Approaches](https://www.kdnuggets.com/2019/01/ensemble-learning-5-main-approaches.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集成学习：5 种主要方法](https://www.kdnuggets.com/2019/01/ensemble-learning-5-main-approaches.html)'
- en: '[Intuitive Ensemble Learning Guide with Gradient Boosting](https://www.kdnuggets.com/2018/07/intuitive-ensemble-learning-guide-gradient-boosting.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[直观的集成学习指南与梯度提升](https://www.kdnuggets.com/2018/07/intuitive-ensemble-learning-guide-gradient-boosting.html)'
- en: '[What is the difference between Bagging and Boosting?](https://www.kdnuggets.com/2017/11/difference-bagging-boosting.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bagging 和 Boosting 有什么区别？](https://www.kdnuggets.com/2017/11/difference-bagging-boosting.html)'
- en: More On This Topic
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Why You Need To Learn More Than One Programming Language!](https://www.kdnuggets.com/2022/06/need-learn-one-programming-language.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么你需要学习多于一种编程语言！](https://www.kdnuggets.com/2022/06/need-learn-one-programming-language.html)'
- en: '[IMPACT: The Data Observability Summit is back November 8th and the…](https://www.kdnuggets.com/2023/10/monte-carlo-impact-the-data-observability-summit-is-back)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[IMPACT：数据可观察性峰会将于 11 月 8 日回归](https://www.kdnuggets.com/2023/10/monte-carlo-impact-the-data-observability-summit-is-back)'
- en: '[Ensemble Learning with Examples](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集成学习实例](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
- en: '[Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集成学习技术：使用 Python 中的随机森林的详细介绍](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
- en: '[When Would Ensemble Techniques be a Good Choice?](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么时候集成技术是一个好的选择？](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
- en: '[Automated Machine Learning with Python: A Case Study](https://www.kdnuggets.com/2023/04/automated-machine-learning-python-case-study.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 的自动化机器学习：一个案例研究](https://www.kdnuggets.com/2023/04/automated-machine-learning-python-case-study.html)'
