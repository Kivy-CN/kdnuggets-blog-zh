- en: 'ChatGPT Dethroned: How Claude Became the New AI Leader'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ChatGPT 被取代：Claude 如何成为新的 AI 领导者
- en: 原文：[https://www.kdnuggets.com/2023/07/chatgpt-dethroned-claude-became-new-ai-leader.html](https://www.kdnuggets.com/2023/07/chatgpt-dethroned-claude-became-new-ai-leader.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/07/chatgpt-dethroned-claude-became-new-ai-leader.html](https://www.kdnuggets.com/2023/07/chatgpt-dethroned-claude-became-new-ai-leader.html)
- en: '![ChatGPT Dethroned: How Claude Became the New AI Leader](../Images/7ca8fce82348ced32998b6dde9532ea4.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![ChatGPT 被取代：Claude 如何成为新的 AI 领导者](../Images/7ca8fce82348ced32998b6dde9532ea4.png)'
- en: '“The great AI race”. Source: Author with Diffusion model in the style of **Tiago
    Hoisel**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: “伟大的 AI 竞赛”。来源：作者使用**Tiago Hoisel**风格的扩散模型
- en: We’ve grown accustomed to continuous breakthroughs in AI over the last few months.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几个月里，我们已经习惯了 AI 的持续突破。
- en: But not record-breaking announcements that set the new bar at 10 times the one
    before, which is precisely what Anthropic has done with its newest version of
    its chatbot Claude, ChatGPT’s biggest competitor.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 但没有创纪录的公告将新标准提高到之前的 10 倍，这正是 Anthropic 通过其最新版本的聊天机器人 Claude（ChatGPT 最大的竞争对手）所做的。
- en: It literally puts to shame everyone around.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实让周围的所有人感到羞愧。
- en: Now, you’ll soon be turning hours of text and information searches into seconds, **evolving
    Generative AI chatbots from simple conversation agents to truly game-changing
    tools for your life and those around you.**
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你很快就能将数小时的文本和信息搜索缩短到几秒钟，**使生成式 AI 聊天机器人从简单的对话代理发展成为真正改变生活的工具**。
- en: A Chatbot on Steroids, and Focused on Doing Good
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一款超强版的聊天机器人，专注于做好事
- en: As you know, with GenAI we’ve opened a window for AI to generate stuff, like
    text or images, which is awesome.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所知道的，通过生成式 AI，我们为 AI 打开了生成文本或图像等内容的窗口，这非常棒。
- en: But as with anything in technology, it comes with a trade-off, in that GenAI
    models lack awareness or judgment of what’s *‘good’* or *‘bad’.*
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但正如技术中的任何事物一样，这也带来了一个权衡问题，即生成式 AI 模型缺乏对*‘好’*或*‘坏’*的意识或判断。
- en: Actually, they’ve achieved the capacity to generate text by imitating data generated
    by humans that, most often than not, **hide debatable biases and dubious content.**
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，他们已经具备了通过模仿人类生成的数据来生成文本的能力，而这些数据大多数时候**隐藏了有争议的偏见和可疑的内容**。
- en: Sadly, as these models get better as they grow bigger, the incentive to simply
    give it any possible text you can find, **no matter the content**, is particularly
    enticing.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 遗憾的是，由于这些模型随着规模的增大而变得更为出色，因此有理由将**不管内容如何**的任何文本都投入其中，这种诱惑尤为强烈。
- en: And that causes huge risks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 而这带来了巨大的风险。
- en: The alignment problem
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对齐问题
- en: Due to their lack of judgment, base Large Language Models, or base LLMs as they
    are commonly referred to, are particularly dangerous, as they are very susceptible
    to learning the biases their training data hides because they reenact those same
    behaviors.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缺乏判断力，基础的大型语言模型（通常称为基础 LLMs）特别危险，因为它们非常容易学习其训练数据隐藏的偏见，因为它们会重演那些相同的行为。
- en: For instance, if the data is biased toward racism, these LLMs become the living
    embodiment of it. And same applies to homophobia and any other sort of discrimination
    you can imagine.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果数据存在种族偏见，这些 LLMs 就会成为这种偏见的具象化体现。同样的道理也适用于恐同症和你能想象的任何其他歧视。
- en: Thus, considering that many people see the Internet as the perfect playground
    to test their limits of unethicality and immorality, the fact that LLMs have been
    trained with pretty much all the Internet with no guardrails whatsoever says it
    all about the potential risks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，考虑到许多人将互联网视为测试自己不道德和不伦理极限的完美游乐场，LLMs 在没有任何保护措施的情况下用几乎所有互联网数据进行训练，这本身就揭示了潜在的风险。
- en: Thankfully, models like ChatGPT are an evolution of these base models achieved
    by aligning their responses to what humans consider as *‘appropriate’*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，像 ChatGPT 这样的模型是基础模型的进化，通过将其响应对齐到人类认为*‘合适’*的标准来实现的。
- en: This was done using a reward mechanism described as *Reinforcement Learning
    for Human Feedback*, or RLHF.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点是通过一种称为*人类反馈的强化学习*（Reinforcement Learning for Human Feedback，RLHF）的奖励机制完成的。
- en: Particularly, ChatGPT was filtered through the commanding judgment of OpenAI’s
    engineers that transformed a very dangerous model into something not only much
    less biased, but also much more useful and great at following instructions.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，ChatGPT 经过了 OpenAI 工程师的严格筛选，这些工程师将一个非常危险的模型转变为不仅偏见更少，而且在执行指令方面更加有用和出色的模型。
- en: Unsurprisingly, these LLMs are generally called Instruction-tuned Language Models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 不出所料，这些 LLMs 通常被称为指令调优语言模型。
- en: Of course, OpenAI engineers shouldn’t be in charge of deciding what’s good or
    bad for the rest of the world, as they also have their fair share of biases (cultural,
    ethnical, etc.).
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当然，OpenAI的工程师不应负责决定什么对世界其他地方的好坏，因为他们也有自己的一份偏见（文化、民族等）。
- en: ''
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: At the end of the day, **even the most virtuous of humans have biases.**
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 归根结底，**即使是最善良的人类也有偏见。**
- en: Needless to say, this procedure isn’t perfect.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不用说，这个过程并不完美。
- en: We’ve seen in several cases where these models, despite their alleged alignment,
    have acted in a sketchy, almost vile way towards their users, as suffered by many
    with Bing, forcing Microsoft to limit the context of the interaction to just a
    few messages before things went sideways.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在多个案例中看到这些模型，尽管声称对齐，但对用户表现得不靠谱，甚至有些卑劣，这些情况被许多使用Bing的人所经历，迫使微软将互动的上下文限制在几条消息内，然后事情开始变得不对劲。
- en: Considering all of this, when two ex-OpenAI researchers founded Anthropic, they
    had another idea in mind…** they would align their models using AI instead of
    humans, with the completely revolutionary concept of self-alignment.**
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些，当两位前OpenAI研究人员创办了Anthropic时，他们有了另一个想法……**他们计划用AI而不是人类来对齐他们的模型，提出了自我对齐这一完全革命性的概念。**
- en: From Massachusetts to AI
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从马萨诸塞州到人工智能
- en: First, the team drafted a Constitution that included the likes of the Universal
    Declaration of Human Rights, or Apple’s terms of service.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，团队起草了一部宪法，包含了《世界人权宣言》这样的内容，或者是苹果的服务条款。
- en: This way, the model not only was taught to predict the next word in a sentence
    (like any other language model) but it also had to take into account, in each
    and every response it gave, **a Constitution that determined what it could say
    or not.**
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，模型不仅被教会预测句子中的下一个单词（就像任何其他语言模型一样），而且还必须在每一个响应中考虑到**一部决定它能说什么或不能说什么的宪法**。
- en: Next, instead of humans, the actual AI is in charge of aligning the model, potentially
    liberating it from human bias.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，由AI而不是人类负责对齐模型，有可能将其从人类偏见中解放出来。
- en: But the crucial news that Anthropic has released recently isn’t the concept
    of aligning their models to something humans can tolerate and utilize with AI, **but
    a recent announcement that has turned Claude into the unwavering dominant player
    in the GenAI war.**
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 但Anthropic最近发布的关键消息并不是将它们的模型对齐到人类可以容忍和利用的AI的概念，**而是一个最近的公告，使Claude成为了GenAI战争中坚定的主导者。**
- en: Specifically, **it has increased its context window from 9,000 tokens to 100,000**.
    An unprecedented improvement that has incomparable implications.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，**它将上下文窗口从9,000个标记增加到了100,000个**。这是一次前所未有的改进，具有不可比拟的影响。
- en: '*But what does that mean and what are these implications?*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*但这意味着什么，这些影响是什么？*'
- en: It’s all about tokens
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一切都关乎标记
- en: Let me be clear that the importance of this concept of *‘token’ *can’t be neglected,
    as despite what many people may tell you, **LLMs don’t predict the next word in
    a sequence… at least not literally.**
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我明确指出，这个*‘标记’*概念的重要性不可忽视，因为尽管许多人可能告诉你，**大型语言模型并不是预测序列中的下一个单词……至少不是字面意义上的。**
- en: When generating their response, LLMs predict the next token, which usually represents
    between 3 and 4 characters, not the next word.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成响应时，大型语言模型预测下一个标记，这通常代表3到4个字符，而不是下一个单词。
- en: Naturally, these tokens may represent a word, or words can be composed of several
    of them (for reference, 100 tokens represent around 75 words).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，这些标记可能代表一个词，或者词可以由多个标记组成（作为参考，100个标记大约代表75个词）。
- en: When running an inference, models like ChatGPT break the text you gave them
    into parts, and perform a series of matrix calculations,** a concept defined as *self-attention***,
    that combine all the different tokens in the text to learn how each token impacts
    the rest.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行推理时，像ChatGPT这样的模型会将你给它的文本拆分成部分，并执行一系列矩阵计算，**这一概念被定义为*自注意力***，结合文本中的所有不同标记以学习每个标记如何影响其他标记。
- en: That way, the model *“learns” *the meaning and context of the text and, that
    way, can then proceed to respond.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，模型*“学习”*文本的意义和上下文，然后才能进行响应。
- en: The issue is that this process is very computationally intensive for the model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，这个过程对模型来说计算量非常大。
- en: To be precise, the computation requirements are quadratic to the input length,
    so the longer the text you give it, described as the context window, the more
    expensive is to run the model both in training and in inference time.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 精确来说，计算要求与输入长度的平方成正比，因此你给它的文本越长（被描述为上下文窗口），运行模型的成本在训练和推理时就越高。
- en: These forced researchers to considerably limit the allowed size of the input
    given to these models to around a standard proportion between 2,000 to 8,000 tokens,
    the latter of which is around 6,000 words.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这迫使研究人员大幅限制模型输入的允许大小，通常在2,000到8,000个标记之间，后者大约是6,000字。
- en: Predictably, constraining the context window has severely crippled the capacity
    of LLMs to impact our lives, leaving them as a funny tool that can help you with
    a handful of things.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 可预测的是，限制上下文窗口严重削弱了LLM对我们生活的影响，使它们成为一个只能帮你做少量事情的有趣工具。
- en: '*But why does increasing this context window unlock LLMs'' greatest potential?*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*但为什么增加这个上下文窗口能够解锁LLM最大的潜力？*'
- en: Well, simple, because it unlocks LLMs' most powerful feature, *in-context learning*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这很简单，因为它解锁了LLM最强大的功能——*上下文学习*。
- en: Learning without training
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无需训练的学习
- en: Put simply, LLMs have a rare capability that enables them to learn *‘on the
    go’*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，LLM具备一种罕见的能力，允许它们*“随时学习”*。
- en: As you know, training LLMs is both expensive and dangerous, specifically because
    to train them you have to hand them your data, which isn’t the best option if
    you want to protect your privacy.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，训练LLM既昂贵又危险，特别是因为训练它们需要你提供数据，而这不是保护隐私的最佳选择。
- en: Also, new data appears every day, so if you had to fine-tune — further train
    — your model constantly, the business case for LLMs would be absolutely demolished.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，每天都有新数据出现，因此如果你需要不断对模型进行微调——进一步训练——LLM的商业前景将会被彻底打破。
- en: Luckily, LLMs are great at this concept described as *in-context learning*,
    which is their capacity to learn without actually modifying the weights of the
    model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，LLM在被称为*上下文学习*的概念上表现出色，这种学习能力是不需要实际修改模型权重的。
- en: In other words, they can learn to answer your query by simply giving them the
    data they need at the same time you’re requesting whatever you need from them…
    without actually having to train the model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，它们可以通过简单地提供所需的数据来学习回答你的查询，而不需要实际训练模型。
- en: This concept, also known as *zero-shot* learning or *few-shot* learning (depending
    on how many times it needs to see the data to learn), **is the capacity of LLMs
    to accurately respond to a given request using data they haven’t seen before until
    that point in time.**
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念，也被称为*零样本*学习或*少样本*学习（具体取决于需要多少次看到数据才能学习），**是LLM（大型语言模型）使用之前未见过的数据准确回应特定请求的能力。**
- en: Consequently, the bigger the context window, the more data you can give them
    and, thus, **the more complex queries it can answer.**
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因此，上下文窗口越大，你可以提供的数据就越多，**模型能够回答的复杂查询也就越多。**
- en: Therefore, although small context windows were okay-ish for chatting and other
    simpler tasks, they were completely incapable of handling truly powerful tasks…
    until now.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，尽管小的上下文窗口对于聊天和其他简单任务来说还算可以，但它们完全无法处理真正强大的任务……直到现在。
- en: The Star Wars Saga in Seconds
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《星球大战》系列的秒级速览
- en: I’ll get to the point.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我会直入主题。
- en: As I mentioned earlier, the newest version of Claude, version 1.3, can ingest
    in one go 100,000 tokens, or around 75,000 words.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，最新版本的Claude，版本1.3，可以一次性处理100,000个标记，或大约75,000字。
- en: But that doesn’t tell you a lot, *does it?*
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并没有告诉你很多，*对吧？*
- en: Let me give you a better idea of what fits inside 75,000 words.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我给你一个更清晰的概念，75,000字的内容大致是什么样的。
- en: From Frankenstein to Anakin
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从《科学怪人》到《安纳金》
- en: The article you’re reading right now is below 2,000 words, which is more than
    37.5 times less than what Claude is now capable of ingesting in one sitting.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在正在阅读的文章不到2,000字，这比Claude目前一次性处理的能力少了37.5倍以上。
- en: '*But what are comparable-size examples? *Well, to be more specific, 75,000
    words represent:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*但类似大小的例子有哪些？*更具体地说，75,000字代表：'
- en: Around the total length of Mary Shelley’s Frankenstein book
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大约相当于玛丽·雪莱的《科学怪人》全书的长度
- en: The entire *Harry Potter and the Philosopher’s Stone* book, which sits at 76,944
    words
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整本*《哈利·波特与魔法石》*，共有76,944字
- en: Any of the Chronicles of Narnia books, as all have smaller word counts
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何一部《纳尼亚传奇》书籍，因为它们的字数都较少
- en: And the most impressive number of all, **it’s enough to include the dialogs
    from up to 8 of the Star Wars films… combined**
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 而最令人印象深刻的数字是，**这些对话的总量足以涵盖最多8部《星球大战》电影的对话……综合起来。**
- en: Now, think about a chatbot that can, in a matter of seconds, give you the power
    to ask it ***anything*** you want about any given text.
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在，想象一下一个聊天机器人，它可以在几秒钟内让你询问关于任何给定文本的***任何问题***。
- en: For instance, I recently saw a video where they gave Claude a five-hour long
    podcast of John Cormack, and the model was capable of not only summarizing the
    overall podcast in just a few words, **it was capable of pointing out particular
    stuff said at one precise moment in time over a five-hour long speaking session.**
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，我最近看到一个视频，他们给 Claude 提供了一个长达五小时的 John Cormack 播客，模型不仅能够用几个词总结整个播客，**它还能够指出在五小时长的演讲过程中某个特定时刻说的具体内容。**
- en: It’s unfathomable to think that not only this model is capable of doing this
    with a 75,000-word transcript, **but the mind-blowing thing is that it’s also
    working with data it could be seeing for the first time.**
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 难以想象，不仅这个模型能够处理 75,000 字的转录文本，**更令人震惊的是，它还能够处理它可能第一次看到的数据。**
- en: Undoubtedly, this is the pinnacle solution for students, lawyers, research scientists,
    and basically anyone that must go through lots of data simultaneously.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，这对学生、律师、研究科学家以及任何需要同时处理大量数据的人来说都是终极解决方案。
- en: To me, this is a paradigm shift in AI like few we’ve seen.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对我而言，这是一种在人工智能领域中少见的范式转变。
- en: Undoubtedly, the door to truly disruptive innovation has been opened for LLMs.
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 毫无疑问，真正颠覆性创新的大门已经为 LLMs 打开。
- en: It’s incredible how AI has changed in just a few months, and how rapidly is
    changing every week. And the only thing we know is that it’s changing… one token
    at a time.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能在短短几个月内的变化令人难以置信，每周变化的速度更是惊人。我们唯一知道的是，它在变化……一步步来。
- en: '**[Ignacio de Gregorio Noblejas](https://www.linkedin.com/in/ignacio-de-gregorio-noblejas/)**
    has more than five years of comprehensive experience in the technology sector,
    and currently holds a position as a Management Consulting Manager in a top-tier
    consulting company, where he has developed a robust background in offering strategic
    guidance on technology adoption and digital transformation initiatives. His expertise
    is not limited to work in consulting but in his free time he also shares his profound
    insights with a broader audience. He actively educates and inspires others about
    the latest advancements in Artificial Intelligence (AI) through his writing on
    [Medium](https://medium.com/@ignacio.de.gregorio.noblejas) and his weekly newsletter,
    [TheTechOasis](https://thetechoasis.beehiiv.com/subscribe), which have an engaged
    audience of over 11,000 and 3,000 people respectively.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Ignacio de Gregorio Noblejas](https://www.linkedin.com/in/ignacio-de-gregorio-noblejas/)**
    在技术领域拥有超过五年的全面经验，目前担任顶级咨询公司的一名管理咨询经理，在技术采纳和数字化转型方面提供战略指导，积累了丰富的背景。他的专业知识不仅限于咨询工作，在空闲时间，他还通过在
    [Medium](https://medium.com/@ignacio.de.gregorio.noblejas) 上的写作和每周通讯 [TheTechOasis](https://thetechoasis.beehiiv.com/subscribe)
    向更广泛的受众分享他对人工智能（AI）最新进展的深刻见解，这些平台分别拥有超过 11,000 和 3,000 名活跃读者。'
- en: '[Original](https://medium.com/@ignacio.de.gregorio.noblejas/chatgpt-dethroned-how-claude-became-the-new-ai-leader-e5aae2284d6d).
    Reposted with permission.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/@ignacio.de.gregorio.noblejas/chatgpt-dethroned-how-claude-became-the-new-ai-leader-e5aae2284d6d)。经许可转载。'
- en: More On This Topic
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Top 10 Tools for Detecting ChatGPT, GPT-4, Bard, and Claude](https://www.kdnuggets.com/2023/05/top-10-tools-detecting-chatgpt-gpt4-bard-llms.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[检测 ChatGPT、GPT-4、Bard 和 Claude 的十大工具](https://www.kdnuggets.com/2023/05/top-10-tools-detecting-chatgpt-gpt4-bard-llms.html)'
- en: '[Meet Gorilla: UC Berkeley and Microsoft’s API-Augmented LLM…](https://www.kdnuggets.com/2023/06/meet-gorilla-uc-berkeley-microsoft-apiaugmented-llm-outperforms-gpt4-chatgpt-claude.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[认识 Gorilla: UC Berkeley 和微软的 API 增强 LLM…](https://www.kdnuggets.com/2023/06/meet-gorilla-uc-berkeley-microsoft-apiaugmented-llm-outperforms-gpt4-chatgpt-claude.html)'
- en: '[3 Ways to Access Claude AI for Free](https://www.kdnuggets.com/2023/06/3-ways-access-claude-ai-free.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[免费访问 Claude AI 的三种方法](https://www.kdnuggets.com/2023/06/3-ways-access-claude-ai-free.html)'
- en: '[Getting Started with Claude 2 API](https://www.kdnuggets.com/getting-started-with-claude-2-api)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[开始使用 Claude 2 API](https://www.kdnuggets.com/getting-started-with-claude-2-api)'
- en: '[Getting Started With Claude 3 Opus That Just Destroyed GPT-4 and Gemini](https://www.kdnuggets.com/getting-started-with-claude-3-opus-that-just-destroyed-gpt-4-and-gemini)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[开始使用刚刚摧毁 GPT-4 和 Gemini 的 Claude 3 Opus](https://www.kdnuggets.com/getting-started-with-claude-3-opus-that-just-destroyed-gpt-4-and-gemini)'
- en: '[Visual ChatGPT: Microsoft Combine ChatGPT and VFMs](https://www.kdnuggets.com/2023/03/visual-chatgpt-microsoft-combine-chatgpt-vfms.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Visual ChatGPT: 微软结合 ChatGPT 和 VFM](https://www.kdnuggets.com/2023/03/visual-chatgpt-microsoft-combine-chatgpt-vfms.html)'
