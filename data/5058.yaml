- en: Implementing Your Own k-Nearest Neighbor Algorithm Using Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Python 实现你自己的 k-最近邻算法
- en: 原文：[https://www.kdnuggets.com/2016/01/implementing-your-own-knn-using-python.html](https://www.kdnuggets.com/2016/01/implementing-your-own-knn-using-python.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/01/implementing-your-own-knn-using-python.html](https://www.kdnuggets.com/2016/01/implementing-your-own-knn-using-python.html)
- en: '**By Natasha Latysheva**.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**由娜塔莎·拉季谢娃**。'
- en: '*Editor''s note*: Natasha is active in the [Cambridge Coding Academy](https://cambridgecoding.com/),
    which is holding an upcoming [Data Science Bootcamp in Python](https://cambridgecoding.com/datascience-bootcamp)
    on 20-21 February 2016, where you can learn state-of-the-art machine learning
    techniques for real-world problems.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*编辑者注*：娜塔莎活跃于[剑桥编码学院](https://cambridgecoding.com/)，该学院将在2016年2月20日至21日举办[Python
    数据科学训练营](https://cambridgecoding.com/datascience-bootcamp)，在这里你可以学习针对现实世界问题的最先进机器学习技术。'
- en: 'In machine learning, you may often wish to build predictors that allows to
    classify things into categories based on some set of associated values. For example,
    it is possible to provide a diagnosis to a patient based on data from previous
    patients. Classification can involve constructing highly non-linear boundaries
    between classes, as in the case of the red, green and blue classes [below](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm):'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，你可能经常希望构建能够根据一组相关值将事物分类的预测器。例如，可以根据以前患者的数据为患者提供诊断。分类可能涉及在类别之间构建高度非线性的边界，就像[下图](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)中红色、绿色和蓝色类别的情况一样：
- en: '![kNN boundaries](../Images/4322b5f9ae66be35375b65bec3b6c05f.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![kNN 边界](../Images/4322b5f9ae66be35375b65bec3b6c05f.png)'
- en: 'Many algorithms have been developed for automated classification, and common
    ones include random forests, support vector machines, Naïve Bayes classifiers,
    and many types of neural networks. To get a feel for how classification works,
    we take a simple example of a classification algorithm - k-Nearest Neighbours
    (kNN) - and build it from scratch in Python 2\. You can use a mostly imperative
    [style of coding](http://latentflip.com/imperative-vs-declarative/), rather than
    a declarative/functional one with [lambda functions](http://www.secnetix.de/olli/Python/lambda_functions.hawk)
    and [list comprehensions](http://www.secnetix.de/olli/Python/list_comprehensions.hawk)
    to keep things simple if you are starting with Python. Here, we will provide an
    introduction to the latter approach. kNN classifies new instances by grouping
    them together with the most similar cases. Here, you will use kNN on the popular
    (if idealized) iris dataset, which consists of flower measurements for three species
    of iris flower. Our task is to predict the species labels of a set of flowers
    based on their flower measurements. Since you’ll be building a predictor based
    on a set of known correct classifications, kNN is a type of supervised machine
    learning (though somewhat confusingly, in kNN there is no explicit training phase;
    see [lazy learning](https://en.wikipedia.org/wiki/Lazy_learning)). The kNN task
    can be broken down into writing 3 primary functions:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 已经开发了许多用于自动分类的算法，其中常见的包括随机森林、支持向量机、朴素贝叶斯分类器和多种神经网络。为了了解分类是如何工作的，我们以一个简单的分类算法——k-最近邻（kNN）——为例，并在
    Python 2 中从头开始构建它。如果你刚开始接触 Python，你可以使用主要的[命令式编码风格](http://latentflip.com/imperative-vs-declarative/)，而不是带有[lambda
    函数](http://www.secnetix.de/olli/Python/lambda_functions.hawk)和[列表推导](http://www.secnetix.de/olli/Python/list_comprehensions.hawk)的声明式/函数式风格，以保持简单。在这里，我们将介绍后者的方法。kNN
    通过将新实例与最相似的情况分组来进行分类。在这里，你将使用 kNN 在流行（虽然理想化的）鸢尾花数据集上进行分类，该数据集包含三种鸢尾花的花朵测量数据。我们的任务是根据花朵测量数据预测一组花朵的物种标签。由于你将根据一组已知正确的分类构建预测器，kNN
    是一种监督式机器学习（尽管有些混淆地说，在 kNN 中没有明确的训练阶段；见[懒惰学习](https://en.wikipedia.org/wiki/Lazy_learning)）。kNN
    任务可以分解为编写 3 个主要函数：
- en: 1\. Calculate the distance between any two points
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 计算任何两点之间的距离
- en: 2\. Find the nearest neighbours based on these pairwise distances
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 根据这些成对距离找到最近的邻居
- en: 3\. Majority vote on a class labels based on the nearest neighbour list
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 基于最近邻列表对类别标签进行多数投票
- en: The steps in the following diagram provide a high-level overview of the tasks
    you'll need to accomplish in your code.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 下图中的步骤提供了你在代码中需要完成的任务的高级概述。
- en: '[![kNN algorithm](../Images/39dbb98036b866c0b324bcb95cf9762c.png)](https://cambridgecoding.files.wordpress.com/2016/01/knn2.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[![kNN 算法](../Images/39dbb98036b866c0b324bcb95cf9762c.png)](https://cambridgecoding.files.wordpress.com/2016/01/knn2.jpg)'
- en: '**The algorithm**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**算法**'
- en: Briefly, you would like to build a script that, for each input that needs classification,
    searches through the entire training set for the k-most similar instances. The
    class labels of the most similar instances should then be summarised by majority
    voting and returned as predictions for the test cases.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，你希望构建一个脚本，对于每一个需要分类的输入，搜索整个训练集中的 k 个最相似实例。最相似实例的类别标签应通过多数投票进行总结，并作为对测试案例的预测返回。
- en: The complete code is at the end of the post. Now, let's go through the different
    parts separately and explain what they do.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码在文章末尾。现在，让我们分别查看不同的部分并解释它们的功能。
- en: 'Loading the data and splitting into train and test sets To get up and running,
    you’ll use some helper functions: although we can [download the iris data](https://archive.ics.uci.edu/ml/datasets/Iris)
    ourselves and use [csv.reader](https://docs.python.org/2/library/csv.html) to
    load it in, you can also quickly fetch the iris data straight from scikit-learn.
    Further, you can do a 60/40 train/test split using the train_test_split function,
    but you could have also randomly assigned the rows yourself (see this type of
    implementation here). In machine learning, the train/test split is used in order
    to reduce overfitting - training models on the full dataset tends to lead to the
    model being overfitted to the noise and peculiarities of the data, rather than
    the real, underlying trend. You do any sort of model tuning (e.g. picking the
    number of neighbours, k) on the training set only - the test set acts as a stand-alone,
    untouched dataset that you use to test your final model performance on.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据并分割成训练集和测试集 为了开始，你将使用一些辅助函数：虽然我们可以自己[下载鸢尾花数据](https://archive.ics.uci.edu/ml/datasets/Iris)并使用[csv.reader](https://docs.python.org/2/library/csv.html)加载它，但你也可以直接从
    scikit-learn 快速获取鸢尾花数据。此外，你可以使用 train_test_split 函数进行 60/40 的训练/测试分割，但你也可以自己随机分配行（见此类型的实现）。在机器学习中，训练/测试分割用于减少过拟合——在整个数据集上训练模型往往会导致模型对数据的噪声和特性进行过度拟合，而不是对真实的、潜在的趋势进行拟合。你仅在训练集上进行任何模型调优（例如选择邻居数量
    k）——测试集作为一个独立的、未经触及的数据集，用于测试最终模型的性能。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here is an overview of the iris dataset, the data split, and a quick guide to
    the indexing.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是鸢尾花数据集的概述、数据分割以及索引的快速指南。
- en: '[![Splitting the iris dataset](../Images/8099def34ac1f68142c2841020df09ca.png)](https://cambridgecoding.files.wordpress.com/2016/01/knn3.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[![分割鸢尾花数据集](../Images/8099def34ac1f68142c2841020df09ca.png)](https://cambridgecoding.files.wordpress.com/2016/01/knn3.jpg)'
- en: More On This Topic
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[From Theory to Practice: Building a k-Nearest Neighbors Classifier](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从理论到实践：构建 k 最近邻分类器](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)'
- en: '[Nearest Neighbors for Classification](https://www.kdnuggets.com/2022/04/nearest-neighbors-classification.html)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分类的最近邻](https://www.kdnuggets.com/2022/04/nearest-neighbors-classification.html)'
- en: '[K-nearest Neighbors in Scikit-learn](https://www.kdnuggets.com/2022/07/knearest-neighbors-scikitlearn.html)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Scikit-learn 中的 K 最近邻](https://www.kdnuggets.com/2022/07/knearest-neighbors-scikitlearn.html)'
- en: '[LangChain 101: Build Your Own GPT-Powered Applications](https://www.kdnuggets.com/2023/04/langchain-101-build-gptpowered-applications.html)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LangChain 101: 构建你自己的 GPT 驱动应用](https://www.kdnuggets.com/2023/04/langchain-101-build-gptpowered-applications.html)'
- en: '[Build Your Own PandasAI with LlamaIndex](https://www.kdnuggets.com/build-your-own-pandasai-with-llamaindex)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 LlamaIndex 构建你自己的 PandasAI](https://www.kdnuggets.com/build-your-own-pandasai-with-llamaindex)'
- en: '[Make Your Own GPTs with ChatGPT''s GPTs!](https://www.kdnuggets.com/make-your-own-gpts-with-chatgpts-gpts)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 ChatGPT 的 GPTs 创建你自己的 GPT](https://www.kdnuggets.com/make-your-own-gpts-with-chatgpts-gpts)'
