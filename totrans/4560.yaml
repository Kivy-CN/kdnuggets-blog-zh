- en: Data Cleaning and Preprocessing for Beginners
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/11/data-cleaning-preprocessing-beginners.html](https://www.kdnuggets.com/2019/11/data-cleaning-preprocessing-beginners.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Sciforce](https://sciforce.solutions).**'
  prefs: []
  type: TYPE_NORMAL
- en: When our team’s [project](https://arxiv.org/abs/1908.02505) scored first in
    the text subtask of this year’s CALL Shared Task challenge, one of the key components
    of our success was careful preparation and cleaning of data. Data cleaning and
    preparation is the most critical first step in any AI project. As evidence shows, [most
    data scientists spend most of their time — up to **70%** — on cleaning data](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#77d304176f63).
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, we’ll guide you through these initial steps of data cleaning
    and preprocessing in Python, starting from importing the most popular libraries
    to actual encoding of features.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_BQ
  - PREF_H2
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '* * *'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***Data cleansing**** or **data cleaning** is the process of detecting and
    correcting (or removing) corrupt or inaccurate records from a record set, table,
    or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant
    parts of the data and then replacing, modifying, or deleting the dirty or coarse
    data. //[**Wikipedia**](https://en.wikipedia.org/wiki/Data_cleansing)*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Step 1\. Loading the data set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/ad30e2684164aac3dcbd2a80f07f2552.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Importing libraries**'
  prefs: []
  type: TYPE_NORMAL
- en: The absolutely first thing you need to do is to import libraries for data preprocessing.
    There are lots of libraries available, but the most popular and important Python
    libraries for working on data are Numpy, Matplotlib, and Pandas. **Numpy** is
    the library used for all mathematical things. **Pandas** is the best tool available
    for importing and managing datasets. **Matplotlib** (Matplotlib.pyplot) is the
    library to make charts.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make it easier for future use, you can import these libraries with a shortcut
    alias:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Loading data into pandas**'
  prefs: []
  type: TYPE_NORMAL
- en: Once you downloaded your data set and named it as a .csv file, you need to load
    it into a pandas DataFrame to explore it and perform some basic cleaning tasks
    removing information you don’t need that will make data processing slower.
  prefs: []
  type: TYPE_NORMAL
- en: 'Usually, such tasks include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Removing the first line: it contains extraneous text instead of the column
    titles. This text prevents the data set from being parsed properly by the pandas
    library:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Removing columns with text explanations that we won’t need, url columns and
    other unnecessary columns:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Removing all columns with only one value, or have more than 50% missing values
    to work faster (if your data set is large enough that it will still be meaningful):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: It’s also a good practice to name the filtered data set differently to keep
    it separate from the raw data. This makes sure you still have the original data
    in case you need to go back to it.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2\. Exploring the data set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/a236f09b6fbe813c2f84daf5c39f23b4.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Understanding the data**'
  prefs: []
  type: TYPE_NORMAL
- en: Now you have got your data set up, but you still should spend some time exploring
    it and understanding what feature each column represents. Such a manual review
    of the data set is important, to avoid mistakes in the data analysis and the modeling
    process.
  prefs: []
  type: TYPE_NORMAL
- en: To make the process easier, you can create a DataFrame with the names of the
    columns, data types, the first row’s values, and description from the data dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you explore the features, you can pay attention to any column that:'
  prefs: []
  type: TYPE_NORMAL
- en: is formatted poorly,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: requires more data or a lot of pre-processing to turn into useful a feature,
    or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: contains redundant information,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: since these things can hurt your analysis if handled incorrectly.
  prefs: []
  type: TYPE_NORMAL
- en: '***You should also pay attention to data leakage***, which can cause the model
    to overfit. This is because the model will be also learning from features that
    won’t be available when we’re using it to make predictions. We need to be sure
    our model is trained using only the data it would have at the point of a loan
    application.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deciding on a target column**'
  prefs: []
  type: TYPE_NORMAL
- en: With a filtered data set explored, you need to create a matrix of dependent
    variables and a vector of independent variables. At first, you should decide on
    the appropriate column to use as a target column for modeling based on the question
    you want to answer. For example, if you're going to predict the development of
    cancer, or the chance the credit will be approved, you need to find a column with
    the status of the disease or loan granting ad use it as the target column.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if the target column is the last one, you can create the matrix
    of dependent variables by typing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'That first colon (**:**) means that we want to take all the lines in our dataset. **:
    -1** means that we want to take all of the columns of data except the last one.
    The .**values** on the end means that we want all of the values.'
  prefs: []
  type: TYPE_NORMAL
- en: To have a vector of independent variables with only the data from the last column,
    you can type
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Step 3\. Preparing the Features for Machine Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/0ffa2ab48587877da91d164b39e5af97.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, it’s time to do the preparatory work to feed the features for ML algorithms.
    To clean the data set, you need to **handle missing values and categorical features**,
    because the mathematics underlying most machine learning models assumes that the
    data is numerical and contains no missing values. Moreover, the **scikit-learn** library
    returns an error if you try to train a model like linear regression and logistic
    regression using data that contain missing or non-numeric values.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dealing with Missing Values**'
  prefs: []
  type: TYPE_NORMAL
- en: Missing data is perhaps the most common trait of unclean data. These values
    usually take the form of NaN or None.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are several causes of missing values: sometimes values are missing because
    they do not exist, or because of improper collection of data or poor data entry.
    For example, if someone is underage, and the question applies to people over 18,
    then the question will contain a missing value. In such cases, it would be wrong
    to fill in a value for that question.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several ways to fill up missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: you can remove the lines with the data if you have your data set is big enough
    and the percentage of missing values is high (over 50%, for example);
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: you can fill all null variables with 0 is dealing with numerical values;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: you can use the Imputerclass from the scikit-learn library to fill in missing
    values with the data’s (mean, median, most_frequent)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: you can also decide to fill up missing values with whatever value comes directly
    after it in the same column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These decisions depend on the type of data, what you want to do with the data,
    and the cause of values missing. In reality, just because something is popular
    doesn’t necessarily make it the right choice. The most common strategy is to use
    the mean value, but depending on your data, you may come up with a totally different
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: '**Handling categorical data**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Machine learning uses only numeric values (float or int data type). However,
    data sets often contain the object data type than needs to be transformed into
    numeric. In most cases, categorical values are discrete and can be encoded as
    dummy variables, assigning a number for each category. The simplest way is to
    use One Hot Encoder, specifying the index of the column you want to work on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Dealing with inconsistent data entry**'
  prefs: []
  type: TYPE_NORMAL
- en: Inconsistency occurs, for example, when there are different unique values in
    a column that are meant to be the same. You can think of different approaches
    to capitalization, simple misprints and inconsistent formats to form an idea.
    One of the ways to remove data inconsistencies is by to remove whitespaces before
    or after entry names and by converting all cases to lower cases.
  prefs: []
  type: TYPE_NORMAL
- en: If there is a large number of inconsistent unique entries, however, it is impossible
    to manually check for the closest matches. You can use the [Fuzzy Wuzzy ](https://github.com/seatgeek/fuzzywuzzy)package
    to identify which strings are most likely to be the same. It takes in two strings
    and returns a ratio. The closer the ratio is to 100, the more likely you will
    unify the strings.
  prefs: []
  type: TYPE_NORMAL
- en: '**Handling Dates and Times**'
  prefs: []
  type: TYPE_NORMAL
- en: A specific type of data inconsistency is the inconsistent format of dates, such
    as dd/mm/yy and mm/dd/yy in the same columns. Your date values might not be in
    the right data type, and this will not allow you effectively perform manipulations
    and get insight from it. This time you can use the [datetime](https://docs.python.org/2/library/datetime.html) package
    to fix the type of the date.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scaling and Normalization**'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling is important if you need to specify that a change in one quantity is
    not equal to another change in another. With the help of scaling you ensure that
    just because some features are big they won’t be used as the main predictor. For
    example, if you use the age and the salary of a person in prediction, some algorithms
    will pay attention to the salary more because it is bigger, which does not make
    any sense.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization involves transforming or converting your dataset into a normal
    distribution. Some algorithms like SVM converge far faster on normalized data,
    so it makes sense to normalize your data to get better results.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many ways to perform feature scaling. In a nutshell, we put all of
    our features into the same scale so that none are dominated by another. For example,
    you can use the [StandardScaler](https://scikit-learn.org/stable/modules/preprocessing.html) class
    from the sklearn.preprocessing package to fit and transform your data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Save to CSV**'
  prefs: []
  type: TYPE_NORMAL
- en: To be sure that you still have the raw data, it is a good practice to store
    the final output of each section or stage of your workflow in a separate CSV file.
    In this way, you’ll be able to make changes in your data processing flow without
    having to recalculate everything.
  prefs: []
  type: TYPE_NORMAL
- en: As we did previously, you can store your DataFrame as a .csv using the pandas *to_csv()* function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These are the very basic steps required to work through a large data set, cleaning
    and preparing the data for any Data Science project. There are other forms of
    data cleaning that you might find useful. But for now we want you to understand
    that you need to properly arrange and tidy up your data before the formulation
    of any model. Better and cleaner data outperforms the best algorithms. If you
    use a very simple algorithm on the cleanest data, you will get very impressive
    results. And, what is more, it is not that difficult to perform basic preprocessing!
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/sciforce/data-cleaning-and-preprocessing-for-beginners-25748ee00743).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: **[SciForce ](https://sciforce.solutions)is a Ukraine-based IT company
    specialized in development of software solutions based on science-driven information
    technologies. We have wide-ranging expertise in many key AI technologies, including
    Data Mining, Digital Signal Processing, Natural Language Processing, Machine Learning,
    Image Processing and Computer Vision.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[7 Steps to Mastering Data Preparation for Machine Learning with Python — 2019
    Edition](https://www.kdnuggets.com/2019/06/7-steps-mastering-data-preparation-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Simple Yet Practical Data Cleaning Codes](https://www.kdnuggets.com/2019/02/simple-yet-practical-data-cleaning-codes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Notes on Feature Preprocessing: The What, the Why, and the How](https://www.kdnuggets.com/2018/10/notes-feature-preprocessing-what-why-how.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Learn Data Cleaning and Preprocessing for Data Science with This Free eBook](https://www.kdnuggets.com/2023/08/learn-data-cleaning-preprocessing-data-science-free-ebook.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Steps to Mastering Data Cleaning and Preprocessing Techniques](https://www.kdnuggets.com/2023/08/7-steps-mastering-data-cleaning-preprocessing-techniques.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Harnessing ChatGPT for Automated Data Cleaning and Preprocessing](https://www.kdnuggets.com/2023/08/harnessing-chatgpt-automated-data-cleaning-preprocessing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cleaning and Preprocessing Text Data in Pandas for NLP Tasks](https://www.kdnuggets.com/cleaning-and-preprocessing-text-data-in-pandas-for-nlp-tasks)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Easy Guide To Data Preprocessing In Python](https://www.kdnuggets.com/2020/07/easy-guide-data-preprocessing-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, June 29: 20 Basic Linux Commands for Data Science…](https://www.kdnuggets.com/2022/n26.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
