["```py\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\ndf = pd.DataFrame(dataset['train'])\n```", "```py\nfrom transformers import GPT2Tokenizer\n\n# Loading the dataset to train our model\ndataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize_function(examples):\n   return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n```", "```py\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n```", "```py\nfrom transformers import GPT2ForSequenceClassification\n\nmodel = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=3)\n```", "```py\nimport evaluate\n\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n   logits, labels = eval_pred\n   predictions = np.argmax(logits, axis=-1)\n   return metric.compute(predictions=predictions, references=labels)\n```", "```py\nfrom transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n   output_dir=\"test_trainer\",\n   #evaluation_strategy=\"epoch\",\n   per_device_train_batch_size=1,  # Reduce batch size here\n   per_device_eval_batch_size=1,    # Optionally, reduce for evaluation as well\n   gradient_accumulation_steps=4\n   )\n\ntrainer = Trainer(\n   model=model,\n   args=training_args,\n   train_dataset=small_train_dataset,\n   eval_dataset=small_eval_dataset,\n   compute_metrics=compute_metrics,\n\n)\n\ntrainer.train()\n```", "```py\nimport evaluate\n\ntrainer.evaluate()\n```"]