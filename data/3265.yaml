- en: 'Train your Deep Learning Faster: FreezeOut'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让你的深度学习训练更快：FreezeOut
- en: 原文：[https://www.kdnuggets.com/2017/08/train-deep-learning-faster-freezeout.html](https://www.kdnuggets.com/2017/08/train-deep-learning-faster-freezeout.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/08/train-deep-learning-faster-freezeout.html](https://www.kdnuggets.com/2017/08/train-deep-learning-faster-freezeout.html)
- en: 'Deep neural networks have many, many learnable parameters that are used to
    make inferences. Often, this poses a problem in two ways: Sometimes, the model
    does not make very accurate predictions. It also takes a long time to train them.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络有许多可学习的参数用于进行推断。这通常在两个方面构成问题：有时，模型的预测不够准确，同时训练所需时间也很长。
- en: 'In a previous post, we covered [Train your Deep Learning model faster and sharper:
    Snapshot Ensembling — M models for the cost of 1](/2017/08/train-deep-learning-faster-snapshot-ensembling.html).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的文章中，我们介绍了 [让你的深度学习模型训练更快更精准：Snapshot Ensembling — 用1的成本实现M个模型](/2017/08/train-deep-learning-faster-snapshot-ensembling.html)。
- en: This post talks about reducing training time with little or no impact on accuracy
    using a novel method.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本文讨论了使用一种新颖的方法减少训练时间，对准确性几乎没有影响。
- en: '[FreezeOut — Training Acceleration by Progressively Freezing Layers](https://arxiv.org/pdf/1706.04983.pdf)'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[FreezeOut — 通过逐步冻结层加速训练](https://arxiv.org/pdf/1706.04983.pdf)'
- en: The authors of this paper propose a method to increase training speed by freezing
    layers. They experiment with a few different ways of freezing the layers, and
    demonstrate the training speed up with little(or none) effect on accuracy.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本文作者提出了一种通过冻结层来提高训练速度的方法。他们尝试了几种不同的冻结层的方法，并展示了训练速度的提高对准确性影响很小（或没有影响）。
- en: What does Freezing a Layer mean?
  id: totrans-7
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 冻结一层是什么意思？
- en: Freezing a layer prevents its weights from being modified. This technique is
    often used in **transfer learning, **where the base model(trained on some other
    dataset)is frozen.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 冻结一层可以防止其权重被修改。这种技术常用于**迁移学习**，其中基础模型（在其他数据集上训练过的）被冻结。
- en: '**How does freezing affect the speed of the model?**'
  id: totrans-9
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**冻结如何影响模型的速度？**'
- en: If you don’t want to modify the weights of a layer, the **backward pass** to
    that layer can be** completely avoided**, resulting in a significant **speed boost**.
    For e.g. if half your model is frozen, and you try to train the model, it will
    take about half the time compared to a fully trainable model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想修改某一层的权重，可以**完全避免**对该层的**反向传播**，从而显著**提高速度**。例如，如果你冻结了模型的一半，训练模型将只需比完全可训练模型少一半的时间。
- en: On the other hand, you still need to train the model, so if you freeze it too **early**,
    it will give **inaccurate** predictions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，你仍然需要训练模型，因此如果冻结得太**早**，将会得到**不准确**的预测。
- en: What is the ‘novel’ approach?
  id: totrans-12
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 什么是‘新颖’的方法？
- en: The authors demonstrated a way to **freeze** the layers one by one as soon as
    possible, resulting in fewer and fewer backward passes, which in turn lowers training
    time.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 作者展示了一种**逐层冻结**的方式，尽可能快地冻结每一层，从而减少反向传播的次数，从而降低训练时间。
- en: At first, the entire model is trainable (exactly like a regular model). After
    a few iterations the first layer is frozen, and the rest of the model is continued
    to train. After another few iterations , the next layer is frozen, and so on.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，整个模型是可训练的（和普通模型完全一样）。经过几次迭代后，第一层被冻结，其余模型继续训练。再经过几次迭代，下一层被冻结，以此类推。
- en: '**Learning Rate Annealing**'
  id: totrans-15
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**学习率退火**'
- en: 'The authors used learning rate annealing to govern the learning rate of the
    model. The notably different technique they used was to **change** the learning
    rate **layer by layer** instead of the whole model. They used the following equation:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们使用了学习率退火来控制模型的学习率。他们使用的显著不同的技术是**逐层**而不是整个模型来**改变**学习率。他们使用了以下方程：
- en: '![learning rate layer by layer](../Images/3ee3d24d4a8960a68c1ed13706be64d5.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![学习率逐层调整](../Images/3ee3d24d4a8960a68c1ed13706be64d5.png)'
- en: Equation 2.0: **α** is the learning rate. **t** is the iteration number. ***i*** denotes
    the ith layer of the model
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.0：**α**是学习率。**t**是迭代次数。***i***表示模型的第i层。
- en: Equation 2.0 Explanation
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 方程 2.0 解释
- en: The sub *i* denotes the ith layer. So *α* sub *i* denotes the learning rate
    for the ith layer. Similarly , *t[i]* denotes the number of iterations the ith
    layer has been trained on. *t* denotes the total number of iterations for the
    whole model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 子*i*表示第i层。因此*α*子*i*表示第i层的学习率。类似地，*t[i]*表示第i层训练的迭代次数。*t*表示整个模型的总迭代次数。
- en: '![alpha](../Images/7b2da0a9a0e9eb8a88a5b558cfe03f31.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![alpha](../Images/7b2da0a9a0e9eb8a88a5b558cfe03f31.png)'
- en: This denotes the initial learning rate for the ith layer.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示第 i 层的初始学习率。
- en: The authors experimented with different values for Equation 2.1
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们尝试了方程 2.1 的不同值
- en: Initial learning rate for Equation 2.1
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 方程式 2.1 的初始学习率
- en: The authors tried scaling the initial learning rate so that each layer was trained
    for an equal amount of time.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们尝试了缩放初始学习率，使每层被训练的时间相等。
- en: Remember that because the first layer of the model would be stopped first, it
    would be otherwise trained for the least amount of time. To remedy that, they
    scaled the the learning rate for each layer.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，由于模型的第一层会首先停止训练，因此它会被训练最少的时间。为了解决这个问题，他们对每层的学习率进行了缩放。
- en: '![linear schedule](../Images/b36a7082533ab3cf8168df5b0e18f6e9.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![线性计划](../Images/b36a7082533ab3cf8168df5b0e18f6e9.png)'
- en: The scaling was done to ensure all the layers’ weights moved equally in the
    weight space, i.e. the layers that were being trained the longest(the later layers),
    had a lower learning rate.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 进行缩放是为了确保所有层的权重在权重空间中均匀移动，即被训练时间最长的层（后面的层）具有较低的学习率。
- en: The authors also played with **cubic scaling**, where the value of t sub i is
    replaced by its own cube.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们还尝试了**立方缩放**，即将 t 子 i 的值替换为其立方。
- en: '![Performance vs Error on DenseNet](../Images/eee55845ce5249ed089173cdc233e562.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![DenseNet 上的性能与误差](../Images/eee55845ce5249ed089173cdc233e562.png)'
- en: 'Fig. 2.1: Performance vs Error on DenseNet'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：DenseNet 上的性能与误差
- en: The authors have included more benchmarks, and their method increases a training
    speedup of about **20%** at only **3% accuracy** drop, and **15%** at **no drop** in
    accuracy.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们增加了更多的基准测试，他们的方法在仅**3%**的准确率下降下提高了约**20%**的训练速度，而在**无下降**的情况下提高了**15%**。
- en: Their method does not work very well for models that do not utilize skip connections(such
    as VGG-16). Neither accuracy not speedups were noticeably different in such networks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不利用跳跃连接的模型（例如 VGG-16），他们的方法效果不是很好。在这些网络中，准确率和加速并没有明显差异。
- en: '* * *'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: My Bonus Trick
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我的额外技巧
- en: The authors are progressively stopping each layer from being trained, which
    they then don’t calculate the backward passes for. They seemed to have missed to
    exploit **precomputing layer activations**. By doing so, you can even prevent
    calculating the **forward pass.**
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们正在逐步停止每层的训练，并且不再计算反向传递。他们似乎遗漏了利用**预计算层激活**。通过这样做，你甚至可以防止计算**前向传递**。
- en: '**What is precomputation**'
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**什么是预计算**'
- en: This is a trick used in transfer learning. This is the general workflow.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这是转移学习中使用的技巧。这是一般工作流程。
- en: Freeze the layers you don’t want to modify
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结你不想修改的层
- en: Calculate the activations the last layer from the frozen layers(for your entire
    dataset)
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算从冻结层到最后一层的激活（对你的整个数据集）
- en: Save those activations to disk
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些激活保存到磁盘
- en: Use those activations as the input of your trainable layers
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些激活作为可训练层的输入
- en: Since the layers are frozen progressively, the new model can now be seen as
    a standalone model(a smaller model) , that just takes the input of whatever the
    last layer outputs. This can be done over and over again as each layer is frozen.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于层是逐步冻结的，新的模型现在可以视为一个独立模型（一个更小的模型），它只接收最后一层的输出作为输入。每冻结一层，可以重复这个过程。
- en: Doing this along with **FreezeOut **will result in a further substantial reduction
    in training time while not affecting other metrics(like accuracy) in any way.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做加上**FreezeOut**将会大幅减少训练时间，同时不会以任何方式影响其他指标（如准确率）。
- en: Conclusion
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: I demonstrated 2(and half of my own) very recent and novel techniques to improve
    accuracy and lower training time by fine tuning learning rates. By also adding
    pre computation whenever possible, a significant speed boost can be possible using
    my own proposed method.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我展示了 2 种（以及我自己的一半）非常近期和新颖的技术，通过微调学习率来提高准确率和降低训练时间。同时，通过尽可能添加预计算，使用我自己提出的方法可以显著加快速度。
- en: '[Original](https://hackernoon.com/training-your-deep-model-faster-and-sharper-e85076c3b047).
    Reposted with permission.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://hackernoon.com/training-your-deep-model-faster-and-sharper-e85076c3b047)。经授权转载。'
- en: '**Bio:** Harshvardhan Gupta writes at [HackerNoon](https://hackernoon.com/).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：** Harshvardhan Gupta 在 [HackerNoon](https://hackernoon.com/) 上撰写文章。'
- en: '**Related:**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Train your Deep Learning model faster and sharper: Snapshot Ensembling — M
    models for the cost of 1](/2017/08/train-deep-learning-faster-snapshot-ensembling.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[让你的深度学习模型更快更精准：Snapshot Ensembling — 用1的成本获得M个模型](/2017/08/train-deep-learning-faster-snapshot-ensembling.html)'
- en: '[DeepSense: A unified deep learning framework for time-series mobile sensing
    data processing](/2017/08/deepsense-unified-deep-learning-framework-time-series-mobile.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DeepSense：用于时间序列移动传感数据处理的统一深度学习框架](/2017/08/deepsense-unified-deep-learning-framework-time-series-mobile.html)'
- en: '[How to squeeze the most from your training data](/2017/07/squeeze-most-from-training-data.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何从训练数据中挤出更多价值](/2017/07/squeeze-most-from-training-data.html)'
- en: '* * *'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的 IT 需求'
- en: '* * *'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主题更多
- en: '[Learn Machine Learning 4X Faster by Participating in Competitions](https://www.kdnuggets.com/2022/01/learn-machine-learning-4x-faster-participating-competitions.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过参与比赛，4倍速学习机器学习](https://www.kdnuggets.com/2022/01/learn-machine-learning-4x-faster-participating-competitions.html)'
- en: '[Why we will always need humans to train AI — sometimes in real-time](https://www.kdnuggets.com/2021/12/why-we-need-humans-training-ai.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么我们总是需要人类来训练 AI——有时是实时的](https://www.kdnuggets.com/2021/12/why-we-need-humans-training-ai.html)'
- en: '[A Guide to Train an Image Classification Model Using Tensorflow](https://www.kdnuggets.com/2022/12/guide-train-image-classification-model-tensorflow.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Tensorflow 训练图像分类模型的指南](https://www.kdnuggets.com/2022/12/guide-train-image-classification-model-tensorflow.html)'
- en: '[How to Build and Train a Transformer Model from Scratch with…](https://www.kdnuggets.com/how-to-build-and-train-a-transformer-model-from-scratch-with-hugging-face-transformers)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何从头开始构建和训练 Transformer 模型…](https://www.kdnuggets.com/how-to-build-and-train-a-transformer-model-from-scratch-with-hugging-face-transformers)'
- en: '[A Faster Way to Prepare Time-Series Data with the AI & Analytics Engine](https://www.kdnuggets.com/2021/12/piexchange-faster-way-prepare-timeseries-data-ai-analytics-engine.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用 AI 和分析引擎更快地准备时间序列数据](https://www.kdnuggets.com/2021/12/piexchange-faster-way-prepare-timeseries-data-ai-analytics-engine.html)'
- en: '[oBERT: Compound Sparsification Delivers Faster Accurate Models for NLP](https://www.kdnuggets.com/2022/05/obert-compound-sparsification-delivers-faster-accurate-models-nlp.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[oBERT：复合稀疏化提供更快、更准确的 NLP 模型](https://www.kdnuggets.com/2022/05/obert-compound-sparsification-delivers-faster-accurate-models-nlp.html)'
