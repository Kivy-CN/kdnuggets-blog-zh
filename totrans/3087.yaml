- en: 'Reinforcement Learning: The Business Use Case, Part 1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2018/08/reinforcement-learning-business-use-case-part-1.html](https://www.kdnuggets.com/2018/08/reinforcement-learning-business-use-case-part-1.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By [Aishwarya Srinivasan](https://www.linkedin.com/in/aishwarya-srinivasan/),
    Deep Learning Researcher**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d3186ef6bb45edf185fd49ae8531fc0.png)'
  prefs: []
  type: TYPE_IMG
- en: The whirl of reinforcement learning started with the advent of AlphaGo by DeepMind,
    the AI system built to play the game Go. Since then, various companies have invested
    a great deal of time, energy, and research, and today reinforcement learning is
    one of the hot topics within Deep Learning. That said, most businesses are struggling
    to find use cases for reinforcement learning or ways to encompass it within their
    business logic. That shouldn’t surprise us. So far, it’s been studied only in
    risk-free, observed, environments that are easy to simulate, which means that
    industries like finance, health, insurance, tech-consultancies are reluctant to
    risk their own money to explore its applications. What’s more, the aspect of “risk
    factoring” within reinforcement learning puts a high strain on systems. Andrew
    Ng, the co-chair and cofounder of Coursera has said that “reinforcement learning
    is a type of machine learning whose hunger for data is even greater than supervised
    learning. It is really difficult to get enough data for reinforcement learning
    algorithms. There’s more work to be done to translate this to businesses and practice.”
  prefs: []
  type: TYPE_NORMAL
- en: With this somewhat pessimistic view in mind, let’s use Part 1 of this blog to
    dig a little deeper into the technical aspect of reinforcement learning. In Part
    2, we’ll look at some possible applications in business. At base, RL is a complex
    algorithm for mapping observed entities and measures into some set of actions,
    while optimizing for a long-term or short-term reward. The RL agent interacts
    with the environment and tries to learn policies, which are sequences of decisions
    or actionstoward obtaining the reward. In fact, RL considers immediate rewards
    and delayed rewards as it drives its interactions with the agent.
  prefs: []
  type: TYPE_NORMAL
- en: A Reinforcement learning model consists of an agent which infers an action which
    then acts on the environment to make a change, and the significance of the action
    is reflected using a reward function. This reward is optimized for the agent and
    the feedback is passed to the agent so that it can assess the next best action
    to take. The system learns from the previous action by recalling the best action
    to take in similar circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc56fb7c6dd7211a6e8198322d49ba7b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 1: The Reinforcement Learning Model'
  prefs: []
  type: TYPE_NORMAL
- en: 'From a mathematics perspective, we can look at reinforcement learning as a
    state model, specifically a fully observable [Markov Decision process](https://en.wikipedia.org/wiki/Markov_decision_process)(MDP).
    To understand the probabilistic theory behind the MDP, we need to know the Markov
    property:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“The future is independent of the past given the present”*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Markov Property is used in situations where the probabilities of different
    outcomes are not dependent on past states; therefore, it requires only the current
    state. Some people use the term “memoryless” to describe the property. In situations
    where you need previous states to inform the outcome, a Markov Decision Process
    won’t work.
  prefs: []
  type: TYPE_NORMAL
- en: The environment of the model is a stochastic finite state machine, with take
    actions given out from the agent as inputs and where the rewards/feedback sent
    from environment to the agent are outputs. The overall reward function consists
    of immediate rewards and delayed rewards. The immediate reward is the quantitative
    impact of the action on the state environment. The delayed reward is the effect
    of the action on future states of the environment. The delayed reward is accounted
    for using the *‘discount factor (*γ*)’ *parameter, 0< γ *<1. *A higher value for
    the discount factor tips the system toward far-sighted rewards, while a lower
    value tips the system toward immediate rewards. *X(t) *is the representation of
    the environment state at time ‘*t’. A(t) *is action taken by the agent at time *‘t’.*
  prefs: []
  type: TYPE_NORMAL
- en: '**State transition function**: From one state to another in the environment
    as a result of the actions given out by the agent.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/24a69e2d466cc46f4c8b0aec565d4f33.png)'
  prefs: []
  type: TYPE_IMG
- en: The agent is also modeled as a stochastic finite state machine, where the rewards
    sent from the environment is the inputs and the actions sent to the environment
    for the next time step is the output. *S(t)*is the current state of the agent
    at time *‘t’ *after receiving the feedback from the environment applied from the
    environment at *‘t-1 *after the action *A(t)*is prepresents the policy being built
    using model learning by overall reward optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b0a1910a2d6e85f55d95bc0ef5e14f7e.png)'
  prefs: []
  type: TYPE_IMG
- en: '**State transition function**: From one state to another in the agent as a
    result of the rewards given out by the environment.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95215638d0b466de150decf7b7738cc4.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Policy function**: The policy/output function from the agent to give out
    the action based on the optimization of the reward function.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c220cddc57d1846a5dc09b1a9df5e05.png)'
  prefs: []
  type: TYPE_IMG
- en: The goal of the agent is to find the policy P(pi), which maximizes the overall
    expected reward with the discount factor.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9a7f2bcea68aad3db6ff48c2f2f61a10.png)'
  prefs: []
  type: TYPE_IMG
- en: The agent trained with the MDP tries to get the most expected sum of rewards
    from the present state. Hence, the optimal value function needs to be obtained.
    The Bellman equation is used for the value function, decomposed into present reward
    and the discounted value of the next state value.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/13c2d168962f7b7e9056ae04783b9efb.png)'
  prefs: []
  type: TYPE_IMG
- en: I hope you got a view of the technical aspects of Reinforcement Learning by
    now!!
  prefs: []
  type: TYPE_NORMAL
- en: In the next part of this post-series, we’ll look at a real-world application
    as a business use-case of financial industry, which would be stock trading.
  prefs: []
  type: TYPE_NORMAL
- en: Keep Learning Deep!
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Aishwarya Srinivasan](https://www.linkedin.com/in/aishwarya-srinivasan/)**:
    MS Data Science - Columbia University || IBM - Data Science Elite || Unicorn in
    Data Science || Scikit-Learn Contributor || Deep Learning Researcher'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/inside-machine-learning/reinforcement-learning-the-business-use-case-part-1-65976c745319).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[5 Things You Need to Know about Reinforcement Learning](/2018/03/5-things-reinforcement-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Explaining Reinforcement Learning: Active vs Passive](/2018/06/explaining-reinforcement-learning-active-passive.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[When reinforcement learning should not be used?](/2017/12/when-reinforcement-learning-not-used.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Hands-on Reinforcement Learning Course Part 3: SARSA](https://www.kdnuggets.com/2022/01/handson-reinforcement-learning-course-part-3-sarsa.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hands-On Reinforcement Learning Course, Part 1](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-course-part-1.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hands-On Reinforcement Learning Course, Part 2](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-part-2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Automated Machine Learning with Python: A Case Study](https://www.kdnuggets.com/2023/04/automated-machine-learning-python-case-study.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chip Huyen shares frameworks and case studies for implementing ML systems](https://www.kdnuggets.com/2023/02/sphere-chip-huyen-shares-frameworks-case-studies-implementing-ml-systems.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Case of Homegrown Large Language Models](https://www.kdnuggets.com/the-case-of-homegrown-large-language-models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
