- en: Why Machine Learning is vulnerable to adversarial attacks and how to fix it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/06/machine-learning-adversarial-attacks.html](https://www.kdnuggets.com/2019/06/machine-learning-adversarial-attacks.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: The AI Safety debate rages following the lead from top researchers in the field
    alongside many of today’s technological visionaries, such as those propping up
    the [Future of Life Institute](http://futureoflife.org/team) and the [Machine
    Intelligence Research Institute](http://intelligence.org/team). Through the media,
    this conversation may appear to sit in a cloud of worry about speculative future-bots
    that will wipe out humanity.
  prefs: []
  type: TYPE_NORMAL
- en: However, real inklings of how we can easily lose mastery over our AI creations
    [are observed](https://arxiv.org/pdf/1606.06565.pdf) in practical problems related
    to unintended behaviors from poorly designed machine learning systems. Among these
    potential “AI accidents” is the case of [adversarial techniques](https://www.kdnuggets.com/2018/10/adversarial-examples-explained.html).
    This approach takes, for instance, a trained classifier model that performs well
    with identifying inputs compared to how a person would classify. Then, a new input
    comes along that includes subtle yet maliciously crafted data that causes the
    model to behave very poorly. What is troublesome is that the type of poor behavior
    is not a reduction in the *statistical performance* of the model. With such a
    manipulated input, the model may still classify the object with very high confidence,
    but the classification is *no longer* what a human anticipates.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Top-performing and competition-winning image classifiers have been fooled by
    [adversarial attacks](https://www.kdnuggets.com/2019/03/breaking-neural-networks-adversarial-attacks.html).
    If you look closely at these scenarios, you may find an interesting catch. The
    offending noise is based on the cost function of the model suggesting that a malicious
    attacker needs to have the model in hand to figure out how to build the adversarial
    input. Simply protecting the model from attackers could be a straightforward solution
    to the problem.
  prefs: []
  type: TYPE_NORMAL
- en: Not so fast.
  prefs: []
  type: TYPE_NORMAL
- en: In 2017, researchers at Penn State, including Nicolas Papernot and Goodfellow,
    [showed](https://arxiv.org/pdf/1602.02697.pdf) that remote adversarial attacks
    could be performed on ML algorithms considered as a “black-box” – in other words,
    without access to the model’s training data or its code. This approach uses the
    [idea of transferability](https://arxiv.org/pdf/1605.07277.pdf) where a sample
    that is trained to mislead one model will likely be able to mislead another model
    (i.e., one targeted for attack).
  prefs: []
  type: TYPE_NORMAL
- en: Papernot’s team created local image classifier models and trained them to see
    how modified samples would result in misclassifications. They then took these
    same adversarial samples (images of stop signs modified in ways imperceptible
    to human vision) and sent them off to remotely hosted deep neural networks accessible
    through APIs from Amazon and Google. The same tricky images tricked the remote
    models as well. They even reported misleading remotely hosted models that included
    adversarial defense strategies.
  prefs: []
  type: TYPE_NORMAL
- en: (Want to try to break your models with adversarial examples? Try out the [cleverhans](https://github.com/tensorflow/cleverhans)
    library developed by Papernot et al.)
  prefs: []
  type: TYPE_NORMAL
- en: All this is a big deal, [so what should we do about it](https://www.kdnuggets.com/2019/01/machine-learning-security.html)?
    If an attacker can control (or confuse) the classification results of, say, a
    computer vision model driving your autonomous vehicle to work without needing
    direct access to the code, then a stop sign may be easily classified as a yield
    sign. So much for getting to work on time or alive. While a clear problem that
    AI developers must appreciate, this still represents a *defined challenge* that
    should be tractable and solvable. It’s just one more way *humans* ingeniously
    hack into other *human* systems to cause *human* issues against which *humans*
    must defend.
  prefs: []
  type: TYPE_NORMAL
- en: What if this case of adversarial samples is an iceberg tip of something more?
    Something inherent in the way machines process information? Something beyond human
    perception?
  prefs: []
  type: TYPE_NORMAL
- en: Just recently, a group from MIT posted [a study](https://arxiv.org/pdf/1905.02175.pdf)
    hypothesizing that previously published observations of the vulnerability of machine
    learning to adversarial techniques are the direct consequence of inherent patterns
    within standard data sets. While incomprehensible to humans, these exist as natural
    features that are fundamentally used by supervised learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, Andrew Ilyas et al. describe a notion of robust and non-robust
    features existing within a data set that *both* provide useful signals for classification.
    In hindsight, this might not be surprising when we recall that classifiers typically
    are trained only to maximize accuracy. Which features a model uses to obtain the
    highest accuracy – as determined by a human’s supervisory labeling – is left to
    the model to sort through with brute force. What looks to us like a person’s nose
    in an input image is just another combination of 0s and 1s compared to a chin
    from the perspective of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate how these human-perceptible and non-perceptible features support
    accurate classification, the MIT team constructed a framework to separate the
    features into distinct training data sets. Both are run through the training and
    are sufficient to classify as expected. The models are seen, as illustrated in
    Figure 1, to use non-robust features in *meaningful ways* just as the robust features,
    even though they look like garbled information to us.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e3a143e102045c2966404c87fe678a73.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 1.** An image of a frog is separated into “robust” features used to
    train a classifier to accurately identify it as a frog and “non-robust” features
    that are also used to train a classifier to identify frog accurately, but not
    when adversarial perturbations are included in the input. These non-robust features
    used by the model are *inherent* to the image and not manipulated or inserted
    by an attacker [from [Ilyas et al.](https://arxiv.org/pdf/1905.02175.pdf) with
    permission].'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s be clear on this point again.
  prefs: []
  type: TYPE_NORMAL
- en: Legitimate data is comprised of human-imperceptible features that impact training.
    These weird features that don’t appear to connect to the appropriate human classification
    are not artifacts or overfittings but are naturally occurring patterns that are
    sufficient *on their own* to result in a learned classifier.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, patterns useful for classification that we as humans expect
    to see might be found by the algorithm. However, *other* patterns are being identified
    that make no sense to us – but they are still patterns that support the algorithm
    in making good classifications.
  prefs: []
  type: TYPE_NORMAL
- en: Dimitris Tsipras from the MIT team is excited about their result as “it convincingly
    demonstrates that models can indeed learn imperceptible well-generalizing features.”
  prefs: []
  type: TYPE_NORMAL
- en: While these patterns don’t look like anything meaningful to us, the ML classifier
    is finding them and using them.
  prefs: []
  type: TYPE_NORMAL
- en: When facing adversarial inputs (e.g., small perturbations in the new data),
    this same set of nonsensical features breaks down leading to incorrect classification.
    Now, it’s not only some third-party attacker inserting misleading data into a
    sample, but it’s a consequence of the non-robust features existing in the real
    data that makes the classifier vulnerable to an adversary’s taunting.
  prefs: []
  type: TYPE_NORMAL
- en: Recalling the notion of transferability mentioned above from Papernot et al.,
    it’s also these non-robust features existing across datasets that make them sensitive
    to adversarial examples trained on other models. An adversary trained to break
    one model will likely break another because it’s messing with the inherent non-robust
    features found in both models.
  prefs: []
  type: TYPE_NORMAL
- en: So, the notion that supervised machine learning is vulnerable to adversarial
    examples is a direct consequence of the inherent properties of how algorithms
    look at the data. The algorithm is performing just fine chugging away at whichever
    patterns of features lead it to the best accuracy. If an adversary messes with
    its classification result, then the model still thinks it’s performing quite well.
    The fact that the result is not what we as humans would perceive is our problem.
  prefs: []
  type: TYPE_NORMAL
- en: To have a bit more human touch, Tsipras said that since “models might rely on
    all sorts of features of the data, we need to explicitly enforce priors on what
    we want the model to learn.”
  prefs: []
  type: TYPE_NORMAL
- en: In other words, if we want to create models that are *interpretable* as far
    as we are concerned, then we need to encode our expected perceptions into the
    training process and not leave the algorithm to its own, non-human, devices.
  prefs: []
  type: TYPE_NORMAL
- en: Well, so what if the algorithm is seeing things *differently* than we do as
    humans? Why is this unexpected? Is it scary? Is it something we don’t want to
    see happening?
  prefs: []
  type: TYPE_NORMAL
- en: We know ML processes our world differently than us. Just compare to the foundational
    notion in NLP where the meanings of words can be extracted not from a memorized
    dictionary (as we humans do) but from exploring the nearby terms in a human-written
    text (such as in [Word2vec](https://www.tensorflow.org/tutorials/representation/word2vec)).
    It should already be appreciated that algorithms running on silicon see information
    differently than synapses firing in brains.
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s this potential for an alternate, although equally effective, viewpoint
    that can lead AI down different paths of intention than humans. It’s what might
    make us lose our control.
  prefs: []
  type: TYPE_NORMAL
- en: From the work of Ilyas et al., a key focus, then, should be to embed human biases
    into ML algorithms to always nudge them into the direction we expect from *our*
    viewpoint. For their future work, Tsipras said, “we are actively working on utilizing
    robustness as a tool to bias models towards learning features that are more meaningful
    to humans.”
  prefs: []
  type: TYPE_NORMAL
- en: Building our models to be more human-aligned by including priors during development
    and training may be the crucial starting point to ensuring we don’t lose our controlling
    grasp on the future of AI.
  prefs: []
  type: TYPE_NORMAL
- en: So, consider keeping a little touch of “human” in your next machine learning
    project.
  prefs: []
  type: TYPE_NORMAL
- en: …
  prefs: []
  type: TYPE_NORMAL
- en: 'Resources cited in this article:'
  prefs: []
  type: TYPE_NORMAL
- en: Amodei, D. et al. “Concrete Problems in AI Safety.” [arXiv.org](https://arxiv.org/abs/1606.06565),
    June 2016.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tanay, T. “Adversarial Examples, Explained.” [KDnuggets](https://www.kdnuggets.com/2018/10/adversarial-examples-explained.html),
    October 2018.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Jain, A. “Breaking neural networks with adversarial attacks.” [KDnuggets](https://www.kdnuggets.com/2019/03/breaking-neural-networks-adversarial-attacks.html),
    March 2019.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Papernot, N. et al. “Practical Black-Box Attacks against Machine Learning.”
    [arXiv.org](https://arxiv.org/abs/1602.02697), February 2016.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Papernot, N. et al. “Transferability in Machine Learning: from Phenomena to
    Black-Box Attacks using Adversarial Samples.” [arXiv.org](https://arxiv.org/abs/1605.07277),
    May 2016.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Jost, Z. “Machine Learning Security.” [KDnuggets](https://www.kdnuggets.com/2019/01/machine-learning-security.html),
    January 2019.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ilyas, A. et al. “Adversarial Examples Are Not Bugs, They Are Features.” [arXiv.org](https://arxiv.org/abs/1905.02175),
    May 2019.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[3 Big Problems with Big Data and How to Solve Them](https://www.kdnuggets.com/2019/04/3-big-problems-big-data.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning Meets Humans – Insights from HUML 2016](https://www.kdnuggets.com/2017/01/machine-learning-humans-huml-2016.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Key Takeaways from AI Conference SF, Day 2: AI and Security, Adversarial Examples,
    Innovation](https://www.kdnuggets.com/2018/10/key-takeaways-aiconf-san-francisco-day2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[GPT-4 is Vulnerable to Prompt Injection Attacks on Causing Misinformation](https://www.kdnuggets.com/2023/05/gpt4-vulnerable-prompt-injection-attacks-causing-misinformation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10 Most Common Data Quality Issues and How to Fix Them](https://www.kdnuggets.com/2022/11/10-common-data-quality-issues-fix.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What is Adversarial Machine Learning?](https://www.kdnuggets.com/2022/03/adversarial-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning Algorithms - What, Why, and How?](https://www.kdnuggets.com/2022/09/machine-learning-algorithms.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Why Do Machine Learning Models Die In Silence?](https://www.kdnuggets.com/2022/01/machine-learning-models-die-silence.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine learning does not produce value for my business. Why?](https://www.kdnuggets.com/2021/12/machine-learning-produce-value-business.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
