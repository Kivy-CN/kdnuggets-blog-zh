- en: 'From Theory to Practice: Building a k-Nearest Neighbors Classifier'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Another day, another classic algorithm: *k*-nearest neighbors. Like the [naive
    Bayes classifier](https://towardsdatascience.com/learning-by-implementing-gaussian-naive-bayes-3f0e3d2c01b2),
    it’s a rather simple method to solve classification problems. The algorithm is
    intuitive and has an unbeatable training time, which makes it a great candidate
    to learn when you just start off your machine learning career. Having said this,
    making predictions is painfully slow, especially for large datasets. The performance
    for datasets with many features might also not be overwhelming, due to the [curse
    of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality).
  prefs: []
  type: TYPE_NORMAL
- en: In this article, you will learn
  prefs: []
  type: TYPE_NORMAL
- en: how the *k*-nearest neighbors classifier works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: why it was designed like this
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: why it has these severe disadvantages and, of course,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: how to implement it in Python using NumPy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: As we will implement the classifier in a scikit learn-conform way, it’s also
    worthwhile to check out my article [Build your own custom scikit-learn Regression](https://towardsdatascience.com/build-your-own-custom-scikit-learn-regression-5d0d718f289).
    However, the scikit-learn overhead is quite small and you should be able to follow
    along anyway.
  prefs: []
  type: TYPE_NORMAL
- en: '*You can find the code on *[*my Github*](https://github.com/Garve/TDS/blob/main/TDS%20-%20KNN.ipynb)*.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main idea of this classifier is stunningly simple. It is directly derived
    from the fundamental question of classifying:'
  prefs: []
  type: TYPE_NORMAL
- en: Given a data point x, what is the probability of x belonging to some class c?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the language of mathematics, we search for the conditional probability *p*(*c*|*x*).
    While the [naive Bayes classifier](https://towardsdatascience.com/learning-by-implementing-gaussian-naive-bayes-3f0e3d2c01b2) tries
    to model this probability directly using some assumptions, there is another intuitive
    way to compute this probability — the frequentist view of probability.
  prefs: []
  type: TYPE_NORMAL
- en: The naive View on Probabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ok, but what does this mean now? Let us consider the following simple example:
    You roll a six-sided, possibly rigged die and want to compute the probability
    of rolling a six, i.e. *p*(roll number 6). How to do this? Well, you roll the
    die *n* times and write down how often it showed a six. If you have seen the number
    six *k* times, you say the probability of seeing a six is **around *k*/*n***.
    Nothing new or fancy here, right?'
  prefs: []
  type: TYPE_NORMAL
- en: Now, imagine that we want to compute a conditional probability, for example
  prefs: []
  type: TYPE_NORMAL
- en: p(roll number 6 | roll an even number)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'You don’t need [Bayes’ theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) to
    solve this. Just roll the die again and ignore all the rolls with an odd number.
    That’s what conditioning does: filtering results. If you rolled the die *n* times,
    have seen *m* even numbers and *k* of them were a six, the probability above is **around *k*/*m*** instead
    of *k*/*n*.'
  prefs: []
  type: TYPE_NORMAL
- en: Motivating k-Nearest Neighbors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Back to our problem. We want to compute *p*(*c*|*x*) where *x* is a vector containing
    the features and *c* is some class. As in the die example, we
  prefs: []
  type: TYPE_NORMAL
- en: need a lot of data points,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: filter out the ones with features *x *and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: check how often these data points belong to class *c*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The relative frequency is our guess for the probability *p*(*c*|*x*).
  prefs: []
  type: TYPE_NORMAL
- en: Do you see the problem here?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Usually, we don’t have many data points with the same features.** Often only
    one, maybe two. For example, imagine a dataset with two features: the height (in
    cm) and the weight (in kg) of persons. The labels are *male* or *female*. Thus, *x=*(*x*?, *x*?)
    where *x*? is the height and *x*? is the weight, and *c* can take the values male
    and female. Let us look at some dummy data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![From Theory to Practice: Building a k-Nearest Neighbors Classifier](../Images/1302b46785fe121717ca1c386938eb79.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author.
  prefs: []
  type: TYPE_NORMAL
- en: Since these two features are continuous, the probability of having two, let
    alone several hundred, data points is negligible.
  prefs: []
  type: TYPE_NORMAL
- en: '**Another problem:** what happens if we want to predict the gender for a data
    point with features we have never seen before, such as (190.1, 85.2)? That is
    what prediction is actually all about. That’s why this naive approach does not
    work. What the *k*-nearest neighbor algorithm does instead is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It tries to approximate the probability *p*(*c*|*x*) not with data points that
    have **exactly** the features *x,* but with data points with features close to *x*.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**It is less strict, in a sense.** Instead of waiting for a lot of persons
    with height=182.4 and weight=92.6, and checking their gender, *k*-nearest neighbors
    allows considering people *close* to having these characteristics. The *k* in
    the algorithm is the number of people we consider, it is a *hyperparameter*.'
  prefs: []
  type: TYPE_NORMAL
- en: These are parameters that we or a hyperparameter optimization algorithm such
    as grid search have to choose. They are not directly optimized by the learning
    algorithm.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![From Theory to Practice: Building a k-Nearest Neighbors Classifier](../Images/e1fe20c23528526a64dddeaa28fec6e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author.
  prefs: []
  type: TYPE_NORMAL
- en: The Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have everything we need now to describe the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '**Training:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Organize** the training data in some way. During prediction time, this order
    should make it possible to give us the *k* closest points for any given data point *x*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That’s it already! ????
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prediction:**'
  prefs: []
  type: TYPE_NORMAL
- en: For a new data point *x*, find the *k* **closest neighbors** in the organized
    training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Aggregate** the labels of these *k* neighbors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Output the label/probabilities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can’t implement this so far, because there are a lot of blanks we have to
    fill in.
  prefs: []
  type: TYPE_NORMAL
- en: What does organizing mean?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we measure proximity?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to aggregate?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Besides the value for *k*, these are all things we can choose, and different
    decisions give us different algorithms. Let’s make it easy and answer the questions
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Organizing = just save the training dataset as it is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proximity = Euclidean distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregation = Averaging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This calls for an example. Let us check the picture with the person data again.
  prefs: []
  type: TYPE_NORMAL
- en: '![From Theory to Practice: Building a k-Nearest Neighbors Classifier](../Images/34e027df8209b2c1c7f0f222a6dc59e4.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the *k*=5 closest data points to the black one have 4 male labels
    and one female label. So we could output that the person belonging to the black
    point is, in fact, 4/5=80% male and 1/5=20% female. If we prefer a single class
    as output, we would return male. No problem!
  prefs: []
  type: TYPE_NORMAL
- en: Now, let us implement it.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The hardest part is to find the nearest neighbors to a point.
  prefs: []
  type: TYPE_NORMAL
- en: A quick primer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us do a small example of how you can do this in Python. We start with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![From Theory to Practice: Building a k-Nearest Neighbors Classifier](../Images/41908a9b680f91b82e59ea679196a47e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author.
  prefs: []
  type: TYPE_NORMAL
- en: We have created a small dataset consisting of four data points, as well as another
    point. Which are the closest points? And should the new point have the label 0
    or 1? Let us find out. Typing in
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: gives us the four values distances=[4, 4, 1, 5], which is the **squared** Euclidean
    distance from `new_point` to all other points in `features` . Sweet! We can see
    that point number three is the closest, followed by points number one and two.
    The fourth point is furthest away.
  prefs: []
  type: TYPE_NORMAL
- en: How can we extract the closest points now from the array [4, 4, 1, 5]? A `distances.argsort()` helps.
    The result is [2, 0, 1, 3] which tells us that the data point with index 2 is
    the smallest (out point number three), then the point with index 0, then with
    index 1, and finally the point with index 3 is the largest.
  prefs: []
  type: TYPE_NORMAL
- en: Note that `argsort` put the first 4 in `distances` before the second 4\. Depending
    on the sorting algorithm, this could also be the other way around, but let’s not
    go into these details for this introductory article.
  prefs: []
  type: TYPE_NORMAL
- en: How if we want the three closest neighbors, for example, we could get them via
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: and the labels correspond to these closest points via
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We get [1, 0, 0], where 1 is the label of the closest point to (1, 4), and the
    zeroes are the labels belonging to the next two closest points.
  prefs: []
  type: TYPE_NORMAL
- en: That’s all we need, let’s get to the real deal.
  prefs: []
  type: TYPE_NORMAL
- en: The final Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You should be quite familiar with the code. The only new function is `np.bincount` which
    counts the occurrences of the labels. Note that I implemented a `predict_proba` method
    first to compute probabilities. The method `predict` just calls this method and
    returns the index (=class) with the highest probability using an `argmax`function.
    The class awaits classes from 0 to *C*-1, where *C* is the number of classes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Disclaimer:** This code is not optimized, it only serves an educational purpose.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it! We can do a small test and see if it agrees with the scikit-learn *k*-nearest
    neighbor classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us create another small dataset for testing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'It looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![From Theory to Practice: Building a k-Nearest Neighbors Classifier](../Images/a0cae9fde0a784977f82d7fa65069e57.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author.
  prefs: []
  type: TYPE_NORMAL
- en: Using our classifier with *k *= 3
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: we get
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Read the output as follows: **The first point is 100% belonging to class
    1 the second point lies in each class equally with 33%, and the third point is
    about 67% class 2 and 33% class 3.'
  prefs: []
  type: TYPE_NORMAL
- en: If you want concrete labels, try
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: which outputs [0, 0, 1]. Note that in the case of a tie, the model as we implemented
    it outputs the lower class, that’s why the point (0, 5) gets classified as belonging
    to class 0.
  prefs: []
  type: TYPE_NORMAL
- en: If you check the picture, it makes sense. But let’s reassure ourselves with
    the help of scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Phew! Everything looks good. Let us check the decision boundaries of the algorithm,
    just because it’s pretty.
  prefs: []
  type: TYPE_NORMAL
- en: '![From Theory to Practice: Building a k-Nearest Neighbors Classifier](../Images/f810d3b226d2e92e4e86383316bf5cb0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author.
  prefs: []
  type: TYPE_NORMAL
- en: Again, the top black point is not 100% blue. It’s 33% blue, red and yellow,
    but the algorithm decides deterministically for the lowest class, which is blue.
  prefs: []
  type: TYPE_NORMAL
- en: We can also check the decision boundaries for different values of *k*.
  prefs: []
  type: TYPE_NORMAL
- en: '![From Theory to Practice: Building a k-Nearest Neighbors Classifier](../Images/17d410874d49ad049e585e5aa428e780.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the blue region gets bigger in the end, because of this preferential
    treatment of this class. We can also see that for *k*=1 the boundaries are a mess:
    the model is **overfitting**. On the other side of the extreme, *k* is as large
    as the size of the dataset, and all points are used for the aggregation step.
    Thus, each data point gets the same prediction: the majority class. The model
    is **underfitting** in this case. The sweet spot is somewhere in between and can
    be found using hyperparameter optimization techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: Before we get to the end, let us see which problems this algorithm has.
  prefs: []
  type: TYPE_NORMAL
- en: Drawbacks of k-Nearest Neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The problems are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Finding the nearest neighbors takes a lot of time, especially with our naive
    implementation. If we want to predict the class of a new data point, we have to
    check it against every other point in our dataset, which is slow. There are better
    ways to organize the data using advanced data structures, but the problem still
    persists.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Following problem number 1: Usually, you train models on faster, stronger computers
    and can deploy the model on weaker machines afterward. Think about deep learning,
    for example. But for *k*-nearest neighbors, the training time is easy and the
    heavy work is done during prediction time, which is not what we want.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What happens if the nearest neighbors are not close at all? Then they don’t
    mean anything. This can already happen in datasets with a small number of features,
    but with even more features the chance of encountering this problem increases
    drastically. This is also what people refer to as the curse of dimensionality.
    A nice visualization can be found in [this post of Cassie Kozyrkov](https://towardsdatascience.com/the-curse-of-dimensionality-minus-the-curse-of-jargon-520da109fc87).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![From Theory to Practice: Building a k-Nearest Neighbors Classifier](../Images/836b112f026ada91e568cc1fbe1bbc5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Especially because of problem 2, you don’t see the *k*-nearest neighbor classifier
    in the wild too often. It’s still a nice algorithm you should know, and you can
    also use it for small datasets, nothing wrong with that. But if you have millions
    of data points with thousands of features, things get rough.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we discussed how the *k*-nearest neighbor classifier works
    and why its design makes sense. It tries to estimate the probability of a data
    point *x* belonging to a class *c* as well as possible using the closest data
    points to *x*. It is a very natural approach, and therefore this algorithm is
    usually taught at the beginning of machine learning courses.
  prefs: []
  type: TYPE_NORMAL
- en: Note that it is really simple to build a *k*-nearest neighbor **regressor**,
    too. Instead of counting occurrences of classes, just average the labels of the
    nearest neighbors to get a prediction. You could implement this on your own, it’s
    just a small change!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We then implemented it in a straightforward way, mimicking the scikit-learn
    API. This means that you can also use this estimator in scikit-learn’s pipelines
    and grid searches. This is a great benefit since we even have the hyperparameter *k* that
    you can optimize using grid search, random search, or Bayesian optimization.
  prefs: []
  type: TYPE_NORMAL
- en: However, this algorithm has some serious issues. It is not suitable for large
    datasets and it can’t be deployed on weaker machines for making predictions. Together
    with the susceptibility to the curse of dimensionality, it is an algorithm that
    is theoretically nice but can only be used for smaller data sets.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Dr. Robert Kübler](https://www.linkedin.com/in/robert-kuebler/)** is a Data
    Scientist at METRO.digital and Author at Towards Data Science.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/understanding-by-implementing-k-nearest-neighbors-469d6f84b8a9).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Nearest Neighbors for Classification](https://www.kdnuggets.com/2022/04/nearest-neighbors-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[K-nearest Neighbors in Scikit-learn](https://www.kdnuggets.com/2022/07/knearest-neighbors-scikitlearn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Visualization: Theory and Techniques](https://www.kdnuggets.com/data-visualization-theory-and-techniques)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Statistics in Data Science: Theory and Overview](https://www.kdnuggets.com/statistics-in-data-science-theory-and-overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding Supervised Learning: Theory and Overview](https://www.kdnuggets.com/understanding-supervised-learning-theory-and-overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning Evaluation Metrics: Theory and Overview](https://www.kdnuggets.com/machine-learning-evaluation-metrics-theory-and-overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
