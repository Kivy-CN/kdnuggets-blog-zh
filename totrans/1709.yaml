- en: The 10 Statistical Techniques Data Scientists Need to Master
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学家需要掌握的10种统计技术
- en: 原文：[https://www.kdnuggets.com/2017/11/10-statistical-techniques-data-scientists-need-master.html/2](https://www.kdnuggets.com/2017/11/10-statistical-techniques-data-scientists-need-master.html/2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/11/10-statistical-techniques-data-scientists-need-master.html/2](https://www.kdnuggets.com/2017/11/10-statistical-techniques-data-scientists-need-master.html/2)
- en: '****6 — Dimension Reduction:****'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '****6 — 降维：****'
- en: Dimension reduction reduces the problem of estimating *p + 1* coefficients to
    the simple problem of *M + 1* coefficients, where *M < p. *This is attained by
    computing *M* different *linear combinations,* or *projections,* of the variables.
    Then these *M* projections are used as predictors to fit a linear regression model
    by least squares. 2 approaches for this task are *principal component regression*
    and *partial least squares.*
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 降维将估计 *p + 1* 个系数的问题简化为仅需要 *M + 1* 个系数的简单问题，其中 *M < p*。这是通过计算变量的 *M* 个不同的线性组合或投影来实现的。然后，将这些
    *M* 个投影用作预测变量，通过最小二乘法拟合线性回归模型。这个任务可以采用2种方法：*主成分回归*和*部分最小二乘*。
- en: '![](../Images/aaf5c3c81c82d9b597e23931df90d35b.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aaf5c3c81c82d9b597e23931df90d35b.png)'
- en: One can describe **Principal Components Regression** as an approach for deriving
    a low-dimensional set of features from a large set of variables. The *first*principal
    component direction of the data is along which the observations vary the most.
    In other words, the first PC is a line that fits as close as possible to the data.
    One can fit *p* distinct principal components. The second PC is a linear combination
    of the variables that is uncorrelated with the first PC, and has the largest variance
    subject to this constraint. The idea is that the principal components capture
    the most variance in the data using linear combinations of the data in subsequently
    orthogonal directions. In this way, we can also combine the effects of correlated
    variables to get more information out of the available data, whereas in regular
    least squares we would have to discard one of the correlated variables.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个可以描述为从一组大量变量中提取一个低维特征集的方法是**主成分回归**。数据的第一个主成分方向是数据变化最大的方向。换句话说，第一个主成分是一个尽可能紧密贴合数据的直线。可以拟合
    *p* 个不同的主成分。第二个主成分是满足以下约束条件的与第一个主成分不相关的变量的线性组合，并且具有最大的方差。其思想是主成分使用数据的线性组合在相互垂直的方向上捕捉最大的变化。通过这种方式，我们还可以将相关变量的效应组合在一起，以充分利用可用数据的信息，而在普通的最小二乘法中，我们必须舍弃一个相关变量。
- en: The PCR method that we described above involves identifying linear combinations
    of *X* that best represent the predictors. These combinations (*directions*) are
    identified in an unsupervised way, since the response *Y* is not used to help
    determine the principal component directions. That is, the response *Y* does not *supervise* the
    identification of the principal components, thus there is no guarantee that the
    directions that best explain the predictors also are the best for predicting the
    response (even though that is often assumed). **Partial least square**s (PLS)
    are a *supervised* alternative to PCR. Like PCR, PLS is a dimension reduction
    method, which first identifies a new smaller set of features that are linear combinations
    of the original features, then fits a linear model via least squares to the new *M* features.
    Yet, unlike PCR, PLS makes use of the response variable in order to identify the
    new features.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上述我们描述的 PCR 方法涉及识别能最好地表示预测变量的线性组合 *X*。这些线性组合（*方向*）是通过无监督方式确定的，因为响应变量 *Y* 并未用于帮助确定主成分方向。也就是说，响应变量
    *Y* 并未对主成分的确定产生 *监督*，因此不能保证最好解释预测变量的方向也是最好预测响应变量的方向（尽管通常是这样认为的）。**部分最小二乘**（PLS）是一种与
    PCR 相比的 *监督* 方法。与 PCR 类似，PLS 是一种降维方法，首先确定一组新的较小特征，这些特征是原始特征的线性组合，然后通过最小二乘拟合线性模型到新的
    *M* 个特征中。然而，与 PCR 不同，PLS 利用响应变量来确定新特征。
- en: '****7 — Nonlinear Models:****'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '****7 — 非线性模型：****'
- en: 'In statistics, nonlinear regression is a form of regression analysis in which
    observational data are modeled by a function which is a nonlinear combination
    of the model parameters and depends on one or more independent variables. The
    data are fitted by a method of successive approximations. Below are a couple of
    important techniques to deal with nonlinear models:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学中，非线性回归是一种回归分析形式，其观测数据被建模为一个非线性模型参数的组合，并依赖于一个或多个自变量。这些数据通过逐步逼近的方法进行拟合。以下是处理非线性模型的几种重要技术：
- en: A function on the real numbers is called a **step function** if it can be written
    as a finite linear combination of indicator functions of intervals. Informally
    speaking, a step function is a piecewise constant function having only finitely
    many pieces.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个实数函数能够被写成有限数量区间指示函数的线性组合，那么它被称为**阶跃函数**。非正式地说，阶跃函数是一个分段常数函数，只有有限多个片段。
- en: A **piecewise function** is a function which is defined by multiple sub-functions,
    each sub-function applying to a certain interval of the main function’s domain.
    Piecewise is actually a way of expressing the function, rather than a characteristic
    of the function itself, but with additional qualification, it can describe the
    nature of the function. For example, a **piecewise polynomial** function is a
    function that is a polynomial on each of its sub-domains, but possibly a different
    one on each.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分段函数**是由多个子函数定义的函数，每个子函数适用于主函数定义域的某个区间。分段实际上是一种表示函数的方式，而不是函数本身的特征，但通过额外的限定词，它可以描述函数的性质。例如，**分段多项式**函数是一个在每个子域上都是多项式函数的函数，但在每个子域上可能是不同的多项式函数。'
- en: '![](../Images/594bd23c68d30b0aa895e2e2b83540c7.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/594bd23c68d30b0aa895e2e2b83540c7.png)'
- en: A **spline** is a special function defined piecewise by polynomials. In computer
    graphics, spline refers to a piecewise polynomial parametric curve. Splines are
    popular curves because of the simplicity of their construction, their ease and
    accuracy of evaluation, and their capacity to approximate complex shapes through
    curve fitting and interactive curve design.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样条**是一种由多项式分段定义的特殊函数。在计算机图形学中，样条指的是分段多项式参数曲线。由于它们的构造简单，评估容易而准确，以及通过曲线拟合和交互式曲线设计来近似复杂形状的能力，样条是流行的曲线。'
- en: A **generalized additive model**is a generalized linear model in which the linear
    predictor depends linearly on unknown smooth functions of some predictor variables,
    and interest focuses on inference about these smooth functions.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**广义可加模型**是一种广义线性模型，其中线性预测子线性地依赖于某些预测变量的未知平滑函数，研究重点是关于这些平滑函数的推断。'
- en: '****8 — Tree-Based Methods:****'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '****8 — 树基方法：****'
- en: Tree-based methods can be used for both regression and classification problems.
    These involve stratifying or segmenting the predictor space into a number of simple
    regions. Since the set of splitting rules used to segment the predictor space
    can be summarized in a tree, these types of approaches are known as **decision-tree** methods.
    The methods below grow multiple trees which are then combined to yield a single
    consensus prediction.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 树基方法可用于回归和分类问题。这些方法将预测空间分成若干简单区域。由于用于分割预测空间的分割规则可以在树中总结，所以这些方法被称为**决策树**方法。下面的方法会生成多个树，然后将它们组合起来得到一个单一的一致性预测。
- en: '**Bagging** is the way decrease the variance of your prediction by generating
    additional data for training from your original dataset using combinations with
    repetitions to produce multistep of the same carnality/size as your original data.
    By increasing the size of your training set you can’t improve the model predictive
    force, but just decrease the variance, narrowly tuning the prediction to expected
    outcome.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**装袋法**是通过使用重复组合从原始数据集生成用于训练的额外数据的方法，以减少预测的方差。通过增加训练集的大小，无法改善模型的预测力，而只能减小方差，狭义地调整预测以适应预期结果。'
- en: '**Boosting** is an approach to calculate the output using several different
    models and then average the result using a weighted average approach. By combining
    the advantages and pitfalls of these approaches by varying your weighting formula
    you can come up with a good predictive force for a wider range of input data,
    using different narrowly tuned models.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升**是一种通过使用几个不同的模型计算输出，然后使用加权平均的方法来得到结果的方法。通过通过改变加权公式来结合这些方法的优点与缺点，可以得到对更广泛的输入数据具有较好预测力的模型。'
- en: '![](../Images/8564dca67fc55bd7baf8ab9137185528.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8564dca67fc55bd7baf8ab9137185528.png)'
- en: The **random forest** algorithm is actually very similar to bagging. Also here,
    you draw random bootstrap samples of your training set. However, in addition to
    the bootstrap samples, you also draw a random subset of features for training
    the individual trees; in bagging, you give each tree the full set of features.
    Due to the random feature selection, you make the trees more independent of each
    other compared to regular bagging, which often results in better predictive performance
    (due to better variance-bias trade-offs) and it’s also faster, because each tree
    learns only from a subset of features.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林算法实际上与装袋算法非常相似。在这里，你除了使用训练集的随机自助样本，还对每个树训练的特征进行了随机子集选择；而在装袋算法中，你给每棵树提供了完整的特征集。由于随机特征选择，每棵树相对于常规装袋更加独立，这通常会导致更好的预测性能（由于更好的方差-偏差权衡），而且速度更快，因为每棵树只从特征的子集中进行学习。
- en: '****9 — Support Vector Machines:****'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '****9 — 支持向量机:****'
- en: '![](../Images/cce76979f7f43d567a5ca6b735d0d831.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cce76979f7f43d567a5ca6b735d0d831.png)'
- en: SVM is a classification technique that is listed under supervised learning models
    in Machine Learning. In layman’s terms, it involves finding the hyperplane (line
    in 2D, plane in 3D and hyperplane in higher dimensions. More formally, a hyperplane
    is n-1 dimensional subspace of an n-dimensional space) that best separates two
    classes of points with the maximum margin. Essentially, it is a constrained optimization
    problem where the margin is maximized subject to the constraint that it perfectly
    classifies the data (hard margin).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: SVM是一种分类技术，属于机器学习中的监督学习模型。通俗地说，它涉及找到最大间隔（二维中的直线，三维中的平面，更高维中的超平面）来最佳分离两类点。更正式地说，超平面是n维空间中n-1维子空间。本质上，它是一个约束优化问题，在满足完美分类数据（硬间隔）的约束条件下，最大化间隔。
- en: The data points that kind of “support” this hyperplane on either sides are called
    the “support vectors”. In the above picture, the filled blue circle and the two
    filled squares are the support vectors. For cases where the two classes of data
    are not linearly separable, the points are projected to an exploded (higher dimensional)
    space where linear separation may be possible. A problem involving multiple classes
    can be broken down into multiple one-versus-one or one-versus-rest binary classification
    problems.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在上方图片中，双边支持这个超平面的数据点称为“支持向量”。填充的蓝色圆圈和两个填充的正方形是支持向量。对于数据两类不是线性可分的情况，这些点被投影到一个膨胀的（更高维）空间，线性分离可能是可能的。涉及多类的问题可以分解为多个一对一或一对多二元分类问题。
- en: '****10 — Unsupervised Learning:****'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '****10 — 无监督学习:****'
- en: 'So far, we only have discussed supervised learning techniques, in which the
    groups are known and the experience provided to the algorithm is the relationship
    between actual entities and the group they belong to. Another set of techniques
    can be used when the groups (categories) of data are not known. They are called
    unsupervised as it is left on the learning algorithm to figure out patterns in
    the data provided. Clustering is an example of unsupervised learning in which
    different data sets are clustered into groups of closely related items. Below
    is the list of most widely used unsupervised learning algorithms:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只讨论了监督学习技术，其中组别是已知的，算法提供给算法的经验是实体与其所属组别之间的关系。当数据的组别（类别）未知时，可以使用另一组技术。它们被称为无监督学习，因为学习算法自己来发现数据中的模式。聚类是无监督学习的一个例子，它将不同的数据集聚类成密切相关的项目组。下面是最常用的无监督学习算法列表：
- en: '![](../Images/61f3908308615b6f22c03bc7ae8be331.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61f3908308615b6f22c03bc7ae8be331.png)'
- en: '**Principal Component Analysis** helps in producing low dimensional representation
    of the dataset by identifying a set of linear combination of features which have
    maximum variance and are mutually un-correlated. This linear dimensionality technique
    could be helpful in understanding latent interaction between the variable in an
    unsupervised setting.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析**通过识别一组具有最大方差且相互不相关的特征的线性组合来产生数据集的低维表示。这种线性维数技术在无监督环境中有助于理解变量之间的潜在相互作用。'
- en: '**k-Means clustering**: partitions data into k distinct clusters based on distance
    to the centroid of a cluster.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**k-均值聚类**：根据与聚类中心的距离将数据分为k组不同的簇。'
- en: '**Hierarchical clustering**: builds a multilevel hierarchy of clusters by creating
    a cluster tree.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层次聚类（Hierarchical clustering）**：通过创建一棵聚类树来构建一个多级层次的聚类。'
- en: This was a basic run-down of some basic statistical techniques that can help
    a data science program manager and or executive have a better understanding of
    what is running underneath the hood of their data science teams. Truthfully, some
    data science teams purely run algorithms through python and R libraries. Most
    of them don’t even have to think about the math that is underlying. However, being
    able to understand the basics of statistical analysis gives your teams a better
    approach. Have insight into the smallest parts allows for easier manipulation
    and abstraction. I hope this basic data science statistical guide gives you a
    decent understanding!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一些基本统计技术的基本介绍，可以帮助数据科学项目经理和或高管更好地了解其数据科学团队背后的运行机制。事实上，一些数据科学团队纯粹通过 Python
    和 R 库运行算法。他们中的大多数甚至不必考虑底层的数学。然而，能够理解统计分析的基础知识可以使您的团队更加有效。对最小部分的洞察力有助于更容易地进行操作和抽象。希望这个基本数据科学统计指南能给你一个不错的理解！
- en: '*P.S: You can get all the lecture slides and RStudio sessions from *[*my GitHub
    source code here*](https://github.com/khanhnamle1994/statistical-learning)*. Thanks
    for the overwhelming response!*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*附言：你可以从[*我的 GitHub 源代码*](https://github.com/khanhnamle1994/statistical-learning)中获取所有的讲座幻灯片和
    RStudio 会议。感谢大家的反响！*'
- en: '**Bio: [James Le](https://www.linkedin.com/in/khanhnamle94/)** is currently
    applying for Master of Science Computer Science programs in the US for the Fall
    2018 admission. His intended research will focus on Machine Learning and Data
    Mining. In the mean time, he is working as a freelance full-stack web developer.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介：[James Le](https://www.linkedin.com/in/khanhnamle94/)** 目前正在申请美国的计算机科学硕士专业，准备于2018年秋季入学。他的研究方向将集中在机器学习和数据挖掘。同时，他还兼职担任全栈网络开发人员。'
- en: '[Original](https://towardsdatascience.com/the-10-statistical-techniques-data-scientists-need-to-master-1ef6dbd531f7).
    Reposted with permission.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文（Original）](https://towardsdatascience.com/the-10-statistical-techniques-data-scientists-need-to-master-1ef6dbd531f7)。经允许转贴。'
- en: '**Related:**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[The 10 Algorithms Machine Learning Engineers Need to Know](/2016/08/10-algorithms-machine-learning-engineers.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习工程师需要了解的10种算法](/2016/08/10-algorithms-machine-learning-engineers.html)'
- en: '[Top 10 Data Mining Algorithms, Explained](/2015/05/top-10-data-mining-algorithms-explained.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[解释的前十种数据挖掘算法](/2015/05/top-10-data-mining-algorithms-explained.html)'
- en: '[Top 10 Machine Learning Algorithms for Beginners](/2017/10/top-10-machine-learning-algorithms-beginners.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[初学者数据科学家需要掌握的前10种机器学习算法](/2017/10/top-10-machine-learning-algorithms-beginners.html)'
- en: More On This Topic
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[初学数据科学的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[构建一个稳固的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用管道编写清晰的Python代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成为优秀数据科学家所需的5个关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2021年最佳ETL工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学，寻找目标并寻找目标...](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
