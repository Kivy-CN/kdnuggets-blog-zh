- en: 'Implementing Deep Learning Methods and Feature Engineering for Text Data: FastText'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现深度学习方法和文本数据的特征工程：FastText
- en: 原文：[https://www.kdnuggets.com/2018/05/implementing-deep-learning-methods-feature-engineering-text-data-fasttext.html](https://www.kdnuggets.com/2018/05/implementing-deep-learning-methods-feature-engineering-text-data-fasttext.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/05/implementing-deep-learning-methods-feature-engineering-text-data-fasttext.html](https://www.kdnuggets.com/2018/05/implementing-deep-learning-methods-feature-engineering-text-data-fasttext.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**Editor''s note:** This post is only one part of a far more thorough and in-depth
    original, [found here](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa),
    which covers much more than what is included here.'
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**编辑注释：** 这篇文章只是一个更全面、深入的原文的一部分，[原文链接](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)，涵盖了比这里更多的内容。'
- en: The FastText Model
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FastText模型
- en: The [**FastText**](https://fasttext.cc/) model was first introduced by Facebook
    in 2016 as an extension and supposedly improvement of the vanilla Word2Vec model.
    Based on the original paper titled [*‘Enriching Word Vectors with Subword Information’ *by
    Mikolov et al.](https://arxiv.org/pdf/1607.04606.pdf) which is an excellent read
    to gain an in-depth understanding of how this model works. Overall, FastText is
    a framework for learning word representations and also performing robust, fast
    and accurate text classification. The framework is open-sourced by [Facebook](https://www.facebook.com/) on [**GitHub**](https://github.com/facebookresearch/fastText) and
    claims to have the following.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[**FastText**](https://fasttext.cc/)模型首次由Facebook在2016年介绍，作为对普通Word2Vec模型的扩展和改进。基于原始论文[*‘Enriching
    Word Vectors with Subword Information’* by Mikolov et al.](https://arxiv.org/pdf/1607.04606.pdf)，这是深入了解该模型如何工作的绝佳读物。总体来说，FastText是一个学习词表示以及进行稳健、快速、准确的文本分类的框架。该框架由[Facebook](https://www.facebook.com/)在[**GitHub**](https://github.com/facebookresearch/fastText)上开源，并声称具备以下功能。'
- en: Recent state-of-the-art [English word vectors](https://fasttext.cc/docs/en/english-vectors.html).
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最新的[英语词向量](https://fasttext.cc/docs/en/english-vectors.html)。
- en: Word vectors for [157 languages trained on Wikipedia and Crawl](https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md).
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 157种语言的词向量训练于维基百科和Crawl，[链接](https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md)。
- en: Models for [language identification](https://fasttext.cc/docs/en/language-identification.html#content) and [various
    supervised tasks](https://fasttext.cc/docs/en/supervised-models.html#content).
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于[语言识别](https://fasttext.cc/docs/en/language-identification.html#content)和[各种监督任务](https://fasttext.cc/docs/en/supervised-models.html#content)的模型。
- en: Though I haven’t implemented this model from scratch, based on the research
    paper, following is what I learnt about how the model works. In general, predictive
    models like the Word2Vec model typically considers each word as a distinct entity
    (e.g. ***where***) and generates a dense embedding for the word. However this
    poses to be a serious limitation with languages having massive vocabularies and
    many rare words which may not occur a lot in different corpora. The Word2Vec model
    typically ignores the morphological structure of each word and considers a word
    as a single entity. The FastText model *considers each word as a Bag of Character
    n-grams.* This is also called as a subword model in the paper.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我还未从头实现该模型，但根据研究论文，以下是我对模型工作的理解。一般来说，像Word2Vec这样的预测模型通常将每个词视为一个独立的实体（例如，***where***）并为该词生成一个密集的嵌入。然而，这对于具有大量词汇和许多稀有词的语言来说是一个严重的限制，这些稀有词在不同语料中可能不会出现很多。Word2Vec模型通常忽略每个词的形态结构，将词视为一个单一的实体。FastText模型*将每个词视为一个字符n-gram的袋子。*
    这在论文中也称为子词模型。
- en: We add special boundary symbols **< **and **>** at the beginning and end of
    words. This enables us to distinguish prefixes and suffixes from other character
    sequences. We also include the word ***w ***itself in the set of its n-grams,
    to learn a representation for each word (in addition to its character n-grams).
    Taking the word ***where ***and ***n=3*** (tri-grams) as an example, it will be
    represented by the character n-grams: ***<wh, whe, her, ere, re> ***and the special
    sequence ***<where>*** representing the whole word. Note that the sequence , corresponding
    to the word ***<her> ***is different from the tri-gram ***her ***from the word ***where***.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在单词的开头和结尾添加特殊的边界符号 **<** 和 **>**。这使我们能够区分前缀和后缀与其他字符序列。我们还将单词 ***w*** 本身包含在其
    n-gram 集中，以学习每个单词的表示（除了它的字符 n-gram）。以单词 ***where*** 和 ***n=3***（三元组）为例，它将由字符 n-gram
    表示：***<wh, whe, her, ere, re>*** 以及表示整个单词的特殊序列 ***<where>***。请注意，序列 ***<her>***
    对应于单词 ***her*** 与单词 ***where*** 的三元组 ***her*** 是不同的。
- en: In practice, the paper recommends in extracting all the n-grams for ***n ≥*** ***3*** and ***n
    ≤*** ***6***. This is a very simple approach, and different sets of n-grams could
    be considered, for example taking all prefixes and suffixes. We typically associate
    a vector representation (embedding) to each n-gram for a word. Thus, we can represent
    a word by the sum of the vector representations of its n-grams or the average
    of the embedding of these n-grams. Thus, due to this effect of leveraging n-grams
    from individual words based on their characters, there is a higher chance for
    rare words to get a good representation since their character based n-grams should
    occur across other words of the corpus.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，论文建议提取所有的 n-gram，***n ≥*** ***3*** 和 ***n ≤*** ***6***。这是一种非常简单的方法，可以考虑不同的
    n-gram 集，例如提取所有前缀和后缀。我们通常将每个 n-gram 关联一个向量表示（嵌入）。因此，我们可以通过 n-gram 向量表示的总和或这些 n-gram
    嵌入的平均值来表示一个单词。因此，由于这种基于字符的 n-gram 的作用，稀有单词获得良好表示的机会更高，因为它们的基于字符的 n-gram 应该会出现在语料库中的其他单词中。
- en: Applying FastText features for Machine Learning Tasks
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用 FastText 特性于机器学习任务
- en: The `gensim` package has nice wrappers providing us interfaces to leverage the
    FastText model available under the `gensim.models.fasttext` module. Let’s apply
    this once again on our ***Bible corpus*** and look at our words of interest and
    their most similar words.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`gensim` 包提供了很好的封装，给我们提供了接口来利用在 `gensim.models.fasttext` 模块下可用的 FastText 模型。让我们再次在我们的***圣经语料库***上应用这个，并查看我们感兴趣的单词及其最相似的单词。'
- en: '![](../Images/999728a20ab50605ab08b779dee4097f.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/999728a20ab50605ab08b779dee4097f.png)'
- en: You can see a lot of similarity in the results with our Word2Vec model with
    relevant similar words for each of our words of interest. Do you notice any interesting
    associations and similarities?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，与我们的 Word2Vec 模型相比，结果中存在许多相似之处，每个感兴趣的单词都有相关的相似单词。你注意到有什么有趣的关联和相似之处吗？
- en: '![](../Images/b3b49b9c52202eed979cbd9f1a64f398.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3b49b9c52202eed979cbd9f1a64f398.png)'
- en: Moses, his brother Aaron and the Tabernacle of Moses
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 摩西、他的兄弟亚伦和摩西的帐幕
- en: '**Note:** Running this model is computationally expensive and usually takes
    more time as compared to the skip-gram model since it considers n-grams for each
    word. This works better if trained using a GPU or a good CPU. I trained this on
    an AWS `***p2.x***` instance and it took me around 10 minutes as compared to over
    2–3 hours on a regular system.'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 运行这个模型计算开销大，通常比 skip-gram 模型花费更多时间，因为它考虑了每个单词的 n-gram。如果使用 GPU 或较好的
    CPU 进行训练效果更好。我在 AWS `***p2.x***` 实例上进行了训练，花了大约 10 分钟，而在普通系统上则需要超过 2 到 3 小时。'
- en: Let’s now use [***Principal Component Analysis (PCA)***](https://en.wikipedia.org/wiki/Principal_component_analysis) to
    reduce the word embedding dimensions to 2-D and then visualize the same.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用[***主成分分析 (PCA)***](https://en.wikipedia.org/wiki/Principal_component_analysis)来将单词嵌入维度降至
    2D，然后可视化。
- en: '![](../Images/cd3a3180e4b5a37fc702bbb58d372a6b.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd3a3180e4b5a37fc702bbb58d372a6b.png)'
- en: Visualizing FastTest word embeddings on our Bible corpus
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的圣经语料库中可视化 FastText 单词嵌入
- en: We can see a lot of interesting patterns! *Noah*, his son *Shem *and grandfather *Methuselah *are
    close to each other. We also see *God *associated with *Moses *and *Egypt *where
    it endured the Biblical plagues including *famine *and *pestilence*. Also *Jesus *and
    some of his *disciples *are associated close to each other.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到很多有趣的模式！*诺亚*、他的儿子*Shem*和祖父*Methuselah*彼此接近。我们还看到*上帝*与*Moses*和*埃及*相关，那里经历了包括*饥荒*和*瘟疫*在内的圣经灾难。此外，*耶稣*和一些他的*门徒*也彼此接近。
- en: To access any of the word embeddings you can just index the model with the word
    as follows.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问任何词嵌入，你可以按如下方式用词索引模型。
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Having these embeddings, we can perform some interesting natural language tasks.
    One of these would be to find out similarity between different words (entities).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这些嵌入，我们可以执行一些有趣的自然语言任务。其中之一是找出不同词（实体）之间的相似性。
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We can see that ***‘god’ ***is more closely associated with ***‘jesus’*** rather
    than ***‘satan’*** based on the text in our Bible corpus. Quite relevant!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到***‘god’***与***‘jesus’***的关联比与***‘satan’***的关联更为紧密，基于我们《圣经》语料库中的文本。相当相关！
- en: Considering word embeddings being present, we can even find out odd words from
    a bunch of words as follows.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到词嵌入的存在，我们甚至可以从一堆词中找出奇异词。
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Interesting and relevant results in both cases for the odd entity amongst the
    other words!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，奇异实体与其他词汇之间都有有趣且相关的结果！
- en: Conclusion
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: These examples should give you a good idea about newer and efficient strategies
    around leveraging deep learning language models to extract features from text
    data and also address problems like word semantics, context and data sparsity.
    Next up will be detailed strategies on leveraging deep learning models for feature
    engineering on image data. Stay tuned!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例应该能让你对利用深度学习语言模型提取文本数据特征以及解决诸如词汇语义、上下文和数据稀疏性等问题的新颖高效策略有一个很好的了解。接下来将详细介绍利用深度学习模型进行图像数据特征工程的策略，敬请期待！
- en: To read about feature engineering strategies for continuous numeric data, check
    out [**Part 1**](https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b) of
    this series!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解连续数值数据的特征工程策略，请查看本系列的 [**第1部分**](https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b)！
- en: To read about feature engineering strategies for discrete categoricial data,
    check out [**Part 2**](https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63) of
    this series!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解离散分类数据的特征工程策略，请查看本系列的 [**第2部分**](https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63)！
- en: To read about traditional feature engineering strategies for unstructured text
    data, check out [**Part 3**](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41) of
    this series!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解非结构化文本数据的传统特征工程策略，请查看本系列的 [**第3部分**](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41)！
- en: All the code and datasets used in this article can be accessed from my [**GitHub**](https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/bonus%20content/feature%20engineering%20text%20data)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 本文使用的所有代码和数据集可以从我的 [**GitHub**](https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/bonus%20content/feature%20engineering%20text%20data) 访问。
- en: The code is also available as a [**Jupyter notebook**](https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%20content/feature%20engineering%20text%20data/Feature%20Engineering%20Text%20Data%20-%20Advanced%20Deep%20Learning%20Strategies.ipynb)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 代码也可以作为 [**Jupyter notebook**](https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%20content/feature%20engineering%20text%20data/Feature%20Engineering%20Text%20Data%20-%20Advanced%20Deep%20Learning%20Strategies.ipynb) 获取。
- en: Architecture diagrams unless explicitly cited are my copyright. Feel free to
    use them, but please do remember to cite the source if you want to use them in
    your own work.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 除非明确注明，否则架构图属于我的版权。请随意使用，但如果你想在自己的工作中使用它们，请记得注明来源。
- en: If you have any feedback, comments or interesting insights to share about my
    article or data science in general, feel free to reach out to me on my LinkedIn
    social media channel.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对我的文章或数据科学有任何反馈、评论或有趣的见解，欢迎通过我的LinkedIn社交媒体渠道联系我。
- en: '**Bio: [Dipanjan Sarkar](https://www.linkedin.com/in/dipanzan)** is a Data
    Scientist @Intel, an author, a mentor @Springboard, a writer, and a sports and
    sitcom addict.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介： [Dipanjan Sarkar](https://www.linkedin.com/in/dipanzan)** 是英特尔的数据科学家、作者、Springboard的导师、作家以及体育和情景喜剧迷。'
- en: '[Original](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa).
    Reposted with permission.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)。经授权转载。'
- en: '**Related:**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Text Data Preprocessing: A Walkthrough in Python](/2018/03/text-data-preprocessing-walkthrough-python.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本数据预处理：Python 实践指南](/2018/03/text-data-preprocessing-walkthrough-python.html)'
- en: '[A General Approach to Preprocessing Text Data](/2017/12/general-approach-preprocessing-text-data.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[预处理文本数据的一般方法](/2017/12/general-approach-preprocessing-text-data.html)'
- en: '[A Framework for Approaching Textual Data Science Tasks](/2017/11/framework-approaching-textual-data-tasks.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[处理文本数据科学任务的框架](/2017/11/framework-approaching-textual-data-tasks.html)'
- en: '* * *'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三名课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的 IT 部门'
- en: '* * *'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Feature Store Summit 2022: A free conference on Feature Engineering](https://www.kdnuggets.com/2022/10/hopsworks-feature-store-summit-2022-free-conference-feature-engineering.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2022 特征工程峰会：关于特征工程的免费会议](https://www.kdnuggets.com/2022/10/hopsworks-feature-store-summit-2022-free-conference-feature-engineering.html)'
- en: '[Alternative Feature Selection Methods in Machine Learning](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中的替代特征选择方法](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
- en: '[A Practical Approach To Feature Engineering In Machine Learning](https://www.kdnuggets.com/2023/07/practical-approach-feature-engineering-machine-learning.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中的特征工程实用方法](https://www.kdnuggets.com/2023/07/practical-approach-feature-engineering-machine-learning.html)'
- en: '[Building a Tractable, Feature Engineering Pipeline for Multivariate…](https://www.kdnuggets.com/2022/03/building-tractable-feature-engineering-pipeline-multivariate-time-series.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为多变量时间序列建立可处理的特征工程管道](https://www.kdnuggets.com/2022/03/building-tractable-feature-engineering-pipeline-multivariate-time-series.html)'
- en: '[Using RAPIDS cuDF to Leverage GPU in Feature Engineering](https://www.kdnuggets.com/2023/06/rapids-cudf-leverage-gpu-feature-engineering.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 RAPIDS cuDF 在特征工程中利用 GPU](https://www.kdnuggets.com/2023/06/rapids-cudf-leverage-gpu-feature-engineering.html)'
- en: '[Feature Engineering for Beginners](https://www.kdnuggets.com/feature-engineering-for-beginners)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[初学者的特征工程](https://www.kdnuggets.com/feature-engineering-for-beginners)'
