- en: The Intuitions Behind Bayesian Optimization with Gaussian Processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2018/10/intuitions-behind-bayesian-optimization-gaussian-processes.html](https://www.kdnuggets.com/2018/10/intuitions-behind-bayesian-optimization-gaussian-processes.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By [Charles Brecque](https://www.linkedin.com/in/charles-brecque-96768397/),
    Mind Foundry**'
  prefs: []
  type: TYPE_NORMAL
- en: In certain applications the objective function is expensive or difficult to
    evaluate. In these situations, a general approach consists in creating a simpler
    surrogate model of the objective function which is cheaper to evaluate and will
    be used instead to solve the optimization problem. Moreover, due to the high cost
    of evaluating the objective function, an iterative approach is often recommended.
    Iterative optimizers work by iteratively requesting evaluations of the function
    at a sequence of points in the domain. Bayesian Optimization adds a Bayesian methodology
    to the iterative optimizer paradigm by incorporating a prior model on the space
    of possible target functions. This article introduces the basic concepts and intuitions
    behind Bayesian Optimization with Gaussian Processes and introduces [OPTaaS](https://mindfoundry.ai/optaas),
    an [API](https://optaas.mindfoundry.ai/) for Bayesian Optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimization**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Optimization methods try to locate an input*** x**** in a domain*** ????*** to
    a function ***f***which maximizes (or minimizes) the value of this function over*** ????***:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b59b492614c9a5bca2ae7228fdf70de5.png)'
  prefs: []
  type: TYPE_IMG
- en: The general Optimization framework
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the function *f *represents the outcome of a process that is required
    to be optimized, such as the overall profitability of a trading strategy, quality
    control metrics on a factory production line, or the performance of a data science
    pipeline with many parameters and hyper-parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The input domain*** ????*** represents the valid parameter choices for the process
    needing optimization. These could be market predictors used in a trading strategy,
    quantities of raw materials used in a factory process, or the parameters of ML
    models in a data science pipeline. It is the description of the input domain***,????***,
    together with the properties of the function, ***f***, that characterize the optimization
    problem. The valid inputs to the process domain,*** ????***, may be discrete,
    continuous, constrained, or any combination of these. Similarly, the outcome function, ***f***,
    may be convex, differentiable, multi- modal, noisy, slowly changing, or have many
    other important properties.
  prefs: []
  type: TYPE_NORMAL
- en: In certain applications the objective function is expensive to evaluate (computationally
    or economically), difficult to evaluate (chemical experiments, oil drilling).
    In these situations, a general approach consists in creating a simpler surrogate
    model ***f ̂***of the objective function ***f***which is cheaper to evaluate and
    will be used instead to solve the optimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, due to the high cost of evaluating the objective function, an iterative
    approach is often recommended. Iterative optimizers work by iteratively requesting
    evaluations of the function *f*at a sequence of points in the domain *x*1, *x*2, . . .
    ∈*** ????***. Through these evaluations the optimizer is able to build up a picture
    of the function ***f***. For gradient descent algorithms this picture is local
    but for surrogate model approaches this picture is global. At any time, or at
    the end of a pre-allocated budget of function evaluations, the iterative optimizer
    will be able to state its best approximation to the true value of ***x****.
  prefs: []
  type: TYPE_NORMAL
- en: The surrogate model is trained using ***N***known evaluations of ***f***: ***F
    =(f1, f2,…,fN )*** at ***XN =(x1,x2,…,xN)***. There are many approaches used for
    building the surrogate model such as polynomial interpolation, neural networks,
    support vector machines, random forests and Gaussian processes. At Mind Foundry,
    our method of choice is regression using Gaussian Processes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Gaussian Processes**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gaussian Processes (GPs) provide a rich and flexible class of non-parametric
    statistical models over function spaces with domains that can be continuous, discrete,
    mixed, or even hierarchical in nature. Furthermore, the GP provides not just information
    about the likely value of ***f***, but importantly also about the uncertainty
    around that value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea behind Gaussian Process Regression is for a set of observed values ***FN***at
    some points ***XN***we assume that these values correspond to the realisation
    of a multivariate Gaussian Process with a prior distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5303443f71691ebbf61550e8347bd162.png)'
  prefs: []
  type: TYPE_IMG
- en: where ***KN***is a ***N*x*N***covariance matrix and its coefficients are expressed
    in terms of a correlation function (or kernel) ***Kmn =K(xm,xn,θ)***. The hyper-
    parameters ***θ***of the kernel are calibrated according to a maximum likelihood
    principle. ***KN***is chosen to reflect a prior assumption of the function and
    therefore the choice of the kernel will have a significant impact on the correctness
    of the regression. An illustration of several covariance functions is given in
    Figure 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Through mathematical transformations and using the conditional probability
    rule it is possible to estimate the posterior distribution *p*( *f N+1*|*FN *, *XN+1 *)
    and express ***f ̂N+1***as a function of KN and FN with an uncertainty. This allows
    us to construct a probabilistic surrogate from our observations as illustrated
    in Figure 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b2c70b77d4c10ce68363611c7cb8429.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](../Images/2336ca1258c09ad25c08b8459c5bce70.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Bayesian Optimization**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Bayesian Optimization is a class of iterative optimization methods that focuses
    on the general optimization setting, where a description of ???? is available,
    but knowledge of the properties of *f *is limited. Bayesian Optimization methods
    are characterized by two features:'
  prefs: []
  type: TYPE_NORMAL
- en: the ***surrogate model f ̂***, for the function *f*,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and an ***acquisition function ***computed from the surrogate and used for guiding
    the selection of the next evaluation point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BO adds a Bayesian methodology to the iterative optimizer paradigm by incorporating
    a ***prior model ***on the space of possible target functions, *f*. By updating
    this model every time time a function evaluation is reported, a Bayesian optimization
    routine keeps a posterior model of the target function *f*. This posterior model
    is the surrogate ***f ̂*** for the function *f*. The pseudo code for a Bayesian
    Optimization routine with a GP prior is:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initialisation**:'
  prefs: []
  type: TYPE_NORMAL
- en: Place a Gaussian process prior on *f*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observe *f *at *n*0 points according to an initial space-filling experimental
    design.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set *n *at *n*0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**While ***n *≤ *N ***do:**'
  prefs: []
  type: TYPE_NORMAL
- en: Update the posterior probability distribution on f using all available data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify the maximiser ***xn***of the acquisition function over ????, where
    the acquisition function is calculated using the current posterior distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observe *yn *= *f *(*xn*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increment *n*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**End while**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Return **either the point evaluated with the largest *f *(*x*) or the point
    with the largest posterior mean.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of a standard acquisition function is the ***Expected Improvement
    Criterion ***(EI), which, for any given point in *x *∈ ???? is the expected improvement
    in the value of *f *at *x *over the best value of ***f***yet seen, given that
    the function ***f***at *x *is indeed higher than the best value of ***f***yet
    seen: so if we are finding the maxima of ***f***, EI can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '***E I *(*x*) = ????(max( *f *(*x*) − *f **,0))**'
  prefs: []
  type: TYPE_NORMAL
- en: where *f ** is the maximum value of *f *seen so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additional examples of acquisition functions are:'
  prefs: []
  type: TYPE_NORMAL
- en: Entropy search which consists in seeking to minimise the uncertainty we have
    in the location of the optimal value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upper confidence bound
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expected loss criterion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 3 illustrates the evolution of the surrogate and its interactions with
    the acquisition function as it improves its knowledge after each iteration of
    the underlying function it is trying to minimize.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ea46119fdccc1852d95bce04f6ddc78.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Operationalizing BO with OPTaaS**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OPTaaS is a general purpose Bayesian optimizer which provides optimal parameter
    configurations via web-services. It can handle any parameter type and does not
    need to know the underlying process, models, or data. It requires the client to
    specify the parameters and their domains, and to post back the accuracy scores
    for each of OPTaaS’ recommended parameter configurations. OPTaaS uses these scores
    to model the underlying system and search for optimal configurations faster.
  prefs: []
  type: TYPE_NORMAL
- en: Mind Foundry have implemented an ensemble of surrogate models and acquisition
    functions within OPTaaS that it will automatically select and configure based
    on the nature and number of parameters it is provided as described in the Figure
    4\. This selection is based on thorough scientific testing and research so that
    OPTaaS always makes the most appropriate choices. Moreover, Mind Foundry has the
    ability to design custom covariance functions for a client’s specific problem
    which will significantly increase the speed and accuracy of the optimization process.
    Most users of OPTaaS require the optimization of complex processes, which are
    expensive to run, and give limited feedback. For this reason, OPTaaS focuses its
    API on providing a simple iterative optimizer interface. However, if more information
    is available about the process being optimized, it can always be leveraged for
    faster convergence towards the optimum. Therefore, OPTaaS also supports the communication
    of information about the domain ????, such as constraints on inputs, and about
    the evaluations of the function *f*, such as noise or gradients or partially complete
    evaluations. Furthermore, clients are often able to leverage local infrastructure
    to distribute the optimization search, and OPTaaS can also be requested for batches
    to evaluate together.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ca0fff42e26280d2942c825cbbd4c56.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The optimization process is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: OPTaaS recommends a configuration to the customer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The customer evaluates the configuration on their machines
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The customer sends back a score (accuracy, Sharpe ratio, Return on investment,…)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OPTaaS uses the score to update its surrogate model and the cycle repeats until
    the optimal configuration has been reached.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During the whole process, OPTaaS does not access the underlying data or models.
    More information about OPTaaS can be found at the end of this page.
  prefs: []
  type: TYPE_NORMAL
- en: '**Team and Resources**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mind Foundry is an Oxford University spin-out founded by Professors Stephen
    Roberts and Michael Osborne who have 35 person years in data analytics. The Mind
    Foundry team is composed of over 30 world class Machine Learning researchers and
    elite software engineers, many former post-docs from the University of Oxford.
    Moreover, Mind Foundry has a privileged access to over 30 Oxford University Machine
    Learning PhDs through its spin-out status. Mind Foundry is a portfolio company
    of the University of Oxford and its investors include [Oxford Sciences Innovation](https://www.oxfordsciencesinnovation.com/),
    the [Oxford Technology and Innovations Fund,](http://www.oxfordtechnology.com/) the [University
    of Oxford Innovation Fund](https://innovation.ox.ac.uk/award-details/university-oxford-isis-fund-uoif/) and [Parkwalk
    Advisors](http://parkwalkadvisors.com/).
  prefs: []
  type: TYPE_NORMAL
- en: '**Documentation**'
  prefs: []
  type: TYPE_NORMAL
- en: Tutorials: [https://tutorial.optaas.mindfoundry.ai](https://tutorial.optaas.mindfoundry.ai/)
  prefs: []
  type: TYPE_NORMAL
- en: API documentation: [https://optaas.mindfoundry.ai](https://optaas.mindfoundry.ai/)
  prefs: []
  type: TYPE_NORMAL
- en: '**Research**'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.robots.ox.ac.uk/~mosb/projects/project/2009/01/01/bayesopt/](http://www.robots.ox.ac.uk/~mosb/projects/project/2009/01/01/bayesopt/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs: []
  type: TYPE_NORMAL
- en: Osborne, M.A. (2010). Bayesian Gaussian Processes for Sequential Prediction,
    Optimisation and Quadrature (PhD thesis). PhD thesis, University of Oxford.
  prefs: []
  type: TYPE_NORMAL
- en: '**Demo: **charles.brecque@mindfoundry.ai'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Charles Brecque](https://www.linkedin.com/in/charles-brecque-96768397/)**
    is the Product Manager at Mind Foundry for OPTaaS, a general purpose Bayesian
    Optimizer deployed via web-services. Mind Foundry is an Oxford University spin-out
    founded by Professors Stephen Roberts and Michael Osborne who have over 35 person
    years experience in advanced data analytics.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/the-intuitions-behind-bayesian-optimization-with-gaussian-processes-7e00fcc898a0).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Unfolding Naive Bayes From Scratch](/2018/09/unfolding-naive-bayes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Optimization 101 for Data Scientists](/2018/08/optimization-101-data-scientists.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Beginners Ask “How Many Hidden Layers/Neurons to Use in Artificial Neural
    Networks?”](/2018/07/beginners-ask-how-many-hidden-layers-neurons-neural-networks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Gaussian Naive Bayes, Explained](https://www.kdnuggets.com/2023/03/gaussian-naive-bayes-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How ChatGPT Works: The Model Behind The Bot](https://www.kdnuggets.com/2023/04/chatgpt-works-model-behind-bot.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stable Diffusion: Basic Intuition Behind Generative AI](https://www.kdnuggets.com/2023/06/stable-diffusion-basic-intuition-behind-generative-ai.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Getting Started with LLMOps: The Secret Sauce Behind Seamless Interactions](https://www.kdnuggets.com/getting-started-with-llmops-the-secret-sauce-behind-seamless-interactions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Growth Behind LLM-based Autonomous Agents](https://www.kdnuggets.com/the-growth-behind-llmbased-autonomous-agents)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Neural Network Optimization with AIMET](https://www.kdnuggets.com/2022/04/qualcomm-neural-network-optimization-aimet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
