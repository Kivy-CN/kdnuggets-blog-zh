# 机器学习研究人员需要学习的8种神经网络架构

> 原文：[https://www.kdnuggets.com/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html](https://www.kdnuggets.com/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html)

[评论](/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html?page=2#comments)

![标题图像](../Images/ce396eda700c6c69f296bdb37596e40f.png)

**为什么我们需要机器学习？**

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业道路。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你所在组织的IT

* * *

机器学习对于那些过于复杂而无法直接由人类编码的任务是必要的。有些任务如此复杂，以至于对人类来说不切实际，甚至不可能逐一解决所有细节并明确编写代码。因此，我们向机器学习算法提供大量数据，让算法通过探索这些数据并寻找一个能够实现程序员设定目标的模型来解决问题。

让我们来看这两个例子：

+   编写能够解决如在混乱场景中从新光照下的视角识别三维物体的问题的程序非常困难。我们不知道该写什么程序，因为我们不了解大脑是如何处理这些问题的。即使我们对如何做到这一点有一个很好的想法，这个程序也可能会极其复杂。

+   编写一个程序来计算信用卡交易是否欺诈是很困难的。可能没有既简单又可靠的规则。我们需要结合大量的弱规则。欺诈是一个不断变化的目标，但程序需要不断变化。

然后出现了**机器学习方法**：我们不再手动为每个特定任务编写程序，而是收集大量示例，指定给定输入的正确输出。机器学习算法然后利用这些示例生成一个完成工作的程序。学习算法生成的程序可能与典型的手写程序大相径庭。它可能包含数百万个数字。如果我们做对了，程序不仅能处理我们训练时的案例，也能处理新案例。如果数据发生变化，程序也可以通过对新数据进行训练来改变。你应该注意到，大量的计算现在比支付某人编写特定任务的程序要便宜。

鉴于此，一些由机器学习最好解决的任务包括：

+   识别模式：真实场景中的物体，面部身份或面部表情，口语词汇

+   识别异常：不寻常的信用卡交易序列，核电站传感器读数的不寻常模式

+   预测：未来的股票价格或货币汇率，个人喜欢哪些电影

**什么是神经网络？**

神经网络是一般机器学习文献中的一类模型。例如，如果你参加了Coursera上的机器学习课程，神经网络可能会被覆盖。神经网络是一组特定的算法，已经彻底改变了机器学习领域。它们受到生物神经网络的启发，当前所谓的深度神经网络已经证明了其效果非常好。神经网络本身是通用函数逼近器，这就是为什么它们可以应用于几乎任何涉及从输入到输出空间的复杂映射的机器学习问题。

这里有3个理由说服你学习神经计算：

+   要了解大脑如何实际工作：它非常大且复杂，由在被触碰时会死亡的物质组成。因此，我们需要使用计算机模拟。

+   理解一种受神经元及其自适应连接启发的并行计算风格：这与顺序计算风格非常不同。

+   通过使用受大脑启发的新型学习算法解决实际问题：即使学习算法的工作方式与大脑实际工作方式不同，它们也可以非常有用。

在完成著名的 [Andrew Ng的机器学习Coursera课程](https://github.com/khanhnamle1994/machine-learning) 后，我开始对神经网络和深度学习产生兴趣。因此，我开始寻找最好的在线资源来学习这些主题，并发现了 [Geoffrey Hinton的机器学习神经网络课程](https://github.com/khanhnamle1994/neural-nets)。如果你是深度学习从业者或希望进入深度学习/机器学习领域的人，你真的应该参加这个课程。Geoffrey Hinton 无疑是深度学习领域的教父。他在这个课程中确实提供了非凡的内容。在这篇博客文章中，我想分享我认为任何机器学习研究人员应该熟悉的课程中的**8种神经网络架构**，以推进他们的工作。

![](../Images/39c93b981f680576756eed859830f9e8.png)

通常，这些架构可以分为3个具体类别：

**1 — 前馈神经网络**

这些是实际应用中最常见的神经网络类型。第一层是输入层，最后一层是输出层。如果有多个隐藏层，我们称之为“深度”神经网络。它们计算一系列变换，改变样本之间的相似性。每一层中神经元的活动是下层活动的非线性函数。

**2 — 递归网络**

这些网络在其连接图中存在有向环路。这意味着有时你可以通过沿箭头的方向回到起点。它们可能具有复杂的动态行为，这使得它们的训练非常困难。它们在生物学上更为真实。

当前对寻找高效训练递归网络的方法有很大兴趣。递归神经网络是一种非常自然的建模序列数据的方式。它们等同于每个时间片一个隐藏层的非常深的网络；只不过它们在每个时间片使用相同的权重，并在每个时间片接收输入。它们有能力在其隐藏状态中长时间记住信息，但很难训练它们充分利用这一潜力。

**3 — 对称连接网络**

这些网络类似于递归网络，但单元之间的连接是对称的（它们在两个方向上具有相同的权重）。对称网络比递归网络更容易分析。由于它们遵循能量函数，因此它们在功能上也更加受限。没有隐藏单元的对称连接网络称为“霍普菲尔德网络”。具有隐藏单元的对称连接网络称为“玻尔兹曼机”。

### 1 — 感知器

被认为是神经网络的第一代，**感知器**只是单个神经元的计算模型。它们在20世纪60年代初由[弗兰克·罗森布拉特](https://blogs.umass.edu/comphon/2017/06/15/did-frank-rosenblatt-invent-deep-learning-in-1962/)推广。它们似乎具有非常强大的学习算法，并且对其能够学习做的事情提出了许多宏大的主张。1969年，明斯基和佩普斯出版了一本名为[《感知器》](https://mitpress.mit.edu/books/perceptrons)的书，分析了它们的功能并展示了它们的局限性。许多人认为这些局限性适用于所有神经网络模型。然而，感知器学习程序今天仍然广泛用于处理包含数百万特征的庞大特征向量的任务。

![](../Images/4d94d59e6e4b8eae6bd7be64cc25f945.png)

在统计模式识别的标准范式中，我们首先将原始输入向量转换为特征激活向量。然后，我们使用基于常识的手写程序来定义特征。接下来，我们学习如何对每个特征激活进行加权，以得到一个标量量。如果这个量超过某个阈值，我们就认为输入向量是目标类别的正例。

标准感知机架构遵循前馈模型，这意味着输入被送入神经元，经过处理后产生输出。在下面的图示中，这意味着网络是自下而上的：输入从底部进入，输出从顶部出去。

![](../Images/2b2a45c42c8d6cd62cb8bcdb0870fd89.png)

然而，感知机确实存在限制：如果允许手动选择特征，并且使用足够多的特征，你几乎可以做任何事情。对于二进制输入向量，我们可以为每个指数级多的二进制向量设置一个单独的特征单元，因此我们可以对二进制输入向量进行任何可能的区分。但是一旦确定了手工编码的特征，感知机所能学习的东西就有很大的限制。

这一结果对感知机来说是毁灭性的，因为模式识别的核心在于尽管存在平移等变换，仍能识别模式。明斯基和帕珀特的“群不变性定理”指出，如果变换形成一个群，那么感知机中学习的部分就无法学会这样做。为了处理这些变换，感知机需要使用多个特征单元来识别信息子模式的变换。因此，模式识别中的棘手部分必须由手工编码的特征检测器解决，而不是学习过程。

没有隐藏单元的网络在输入-输出映射方面的学习能力非常有限。更多的线性单元层没有帮助。它仍然是线性的。固定输出非线性是不够的。因此，我们需要多个层次的自适应非线性隐藏单元。但我们如何训练这样的网络？我们需要一种有效的方式来调整所有权重，而不仅仅是最后一层。这很困难。学习输入到隐藏单元的权重等同于学习特征。这很困难，因为没有人直接告诉我们隐藏单元应该做什么。

### 2 — 卷积神经网络

随着时间的推移，机器学习研究已经广泛关注于对象检测问题。识别对象的难点有很多：

+   分割：真实场景中杂乱无章，难以判断哪些部分属于同一对象的组成部分。物体的部分可能被其他物体遮挡。

+   照明：像素的强度既由照明条件决定，也由物体决定。

+   变形：物体可以以各种非仿射方式变形。例如，手写字母“o”可以有一个大圈或只是一个尖点。

+   可用性：对象类别通常通过它们的使用方式来定义。例如，椅子是为了坐的而设计的，因此它们有各种各样的物理形状。

+   视角：视角变化导致图像变化，而标准学习方法无法应对。这些信息在输入维度（即像素）之间跳跃。

+   想象一个医疗数据库，其中患者的年龄有时会跳到通常用于编码体重的输入维度！为了应用机器学习，我们首先要消除这种维度跳跃。

![](../Images/5b43460563dfae211f0ceeee551229b6.png)

目前，复制特征的方法是神经网络解决目标检测问题的主要方法。它使用许多位置不同的相同特征检测器的副本。它还可以在尺度和方向上进行复制，这既棘手又昂贵。复制大大减少了需要学习的自由参数的数量。它使用几种不同的特征类型，每种特征都有自己复制的检测器图。它还允许每个图像的补丁以多种方式表示。

那么，复制特征检测器有什么成效呢？

+   等效活动：复制的特征并不会使神经活动对平移不变。这些活动是等变的。

+   不变知识：如果某个特征在训练中的某些位置有用，那么在测试时该特征的检测器将在所有位置可用。

1998年，Yann LeCun及其合作者开发了一种非常好的手写数字识别器，叫做[LeNet](http://yann.lecun.com/exdb/lenet/)。它在具有许多隐藏层的前馈网络中使用了反向传播，许多层中都有复制单元的图，每层复制单元的输出进行了池化，一个可以处理多个字符（即使它们重叠）的宽网络，以及一种训练完整系统的巧妙方法，而不仅仅是识别器。后来，它被正式命名为**卷积神经网络**。有趣的是：这个网络用于读取北美约10%的支票。

![](../Images/5d555581239934e2d31a32647a7003a6.png)

卷积神经网络可以用于与目标识别相关的所有工作，从手写数字到三维物体。然而，在从网络下载的彩色照片中识别真实物体比识别手写数字复杂得多。类别数量是前者的百倍（1000对10），像素数量也是前者的百倍（256 x 256 彩色对 28 x 28 灰度），还有二维图像的三维场景、需要分割的杂乱场景，以及每张图像中的多个物体。相同类型的卷积神经网络会有效吗？

随后是[ILSVRC-2012比赛](http://www.image-net.org/challenges/LSVRC/2012/)，该比赛基于**ImageNet**数据集，包含大约120万张高分辨率的训练图像。测试图像在没有初始注释（没有分割或标签）的情况下呈现，算法必须生成标签，指定图像中存在的对象。一些现有的最佳计算机视觉方法已被来自牛津、INRIA、XRCE等领先计算机视觉团队在此数据集上进行尝试。通常，计算机视觉系统使用复杂的多阶段系统，早期阶段通常通过优化少量参数进行手动调整。

![](../Images/10ef5959d0e91a8e208616591820caa9.png)

比赛的获胜者，[Alex Krizhevsky (NIPS 2012)](http://www.image-net.org/challenges/LSVRC/2012/supervision.pdf)，开发了一种非常深的卷积神经网络，这种网络类型是由Yann LeCun开创的。其架构包括7个隐藏层，不包括一些最大池化层。早期层是卷积层，而最后两个层是全连接层。每个隐藏层的激活函数是修正线性单元。这些单元的训练速度更快，比逻辑单元更具表达能力。此外，它还使用了竞争性归一化，以在附近单元活动更强时抑制隐藏活动。这有助于处理强度变化。

有几个技术技巧显著提高了神经网络的泛化能力：

1.  在256 x 256的图像中，随机提取224 x 224的图像块以获取更多数据，并使用图像的左右反射。在测试时，结合来自10个不同图像块的意见：四个224 x 224的角落图像块加上中央的224 x 224图像块，以及这5个图像块的反射。

1.  使用“dropout”对全连接层的权重进行正则化（这些层包含大部分参数）。Dropout意味着在每个训练样本中，随机去除一半的隐藏单元。这防止了隐藏单元过度依赖其他隐藏单元。

![](../Images/adcd19bf34e1806f4d14c72e8c826e70.png)

在硬件要求方面，Alex使用了非常高效的卷积网络实现，运行在2块Nvidia GTX 580 GPU上（超过1000个快速的小核心）。这些GPU非常适合矩阵乘法，并且具有非常高的内存带宽。这使得他能够在一周内训练网络，并且在测试时快速结合来自10个图像块的结果。如果我们能快速地通信状态，我们可以将网络分布到许多核心上。随着核心变得便宜，数据集变得更大，大型神经网络将比传统计算机视觉系统改进得更快。

### 3 — 递归神经网络

![](../Images/165765dffd0351593bc95483a6eccae1.png)

要理解 RNNs，我们需要对序列建模有一个简要的概述。在将机器学习应用于序列时，我们通常希望将输入序列转换为位于不同领域的输出序列；例如，将一系列声音压力转换为一系列单词身份。当没有单独的目标序列时，我们可以通过尝试预测输入序列中的下一个项来获得教学信号。目标输出序列是输入序列的前进 1 步。这似乎比尝试从其他像素预测图像中的一个像素，或从图像的其余部分预测图像中的一个补丁自然得多。预测序列中的下一个项模糊了监督学习和无监督学习之间的区别。它使用了为监督学习设计的方法，但不需要单独的教学信号。

**无记忆模型** 是处理这一任务的标准方法。特别是，自回归模型可以从固定数量的前一个项中预测序列中的下一个项，使用“延迟抽头”；前馈神经网络是广义的自回归模型，使用一个或多个层的非线性隐藏单元。然而，如果我们给生成模型一些隐藏状态，并且如果我们赋予这个隐藏状态自己的内部动态，我们就会得到一种更有趣的模型：它可以在其隐藏状态中存储信息很长时间。如果这些动态是嘈杂的，并且它们从隐藏状态生成输出的方式也是嘈杂的，我们永远无法知道其确切的隐藏状态。我们能做的最好的是推断隐藏状态向量空间上的概率分布。这种推断仅对两种类型的隐藏状态模型是可行的。

**递归神经网络** 非常强大，因为它们结合了两个特性：1) 分布式隐藏状态，使它们能够高效地存储大量关于过去的信息，以及 2) 非线性动态，使它们能够以复杂的方式更新隐藏状态。只要有足够的神经元和时间，RNNs 可以计算出任何计算机能计算的东西。那么 RNNs 可以表现出什么样的行为呢？它们可以发生振荡，可以收敛到点吸引子，可以表现得混乱不堪。它们还有可能学习实现许多小程序，每个程序捕捉一个知识点并并行运行，相互作用产生非常复杂的效果。

![](../Images/e6043e3e88d5c61a2b88f6a281218d60.png)

然而，RNN的计算能力使得它们非常难以训练。由于梯度爆炸或消失问题，训练RNN相当困难。当我们通过多层进行反向传播时，梯度的幅度会发生什么？如果权重很小，梯度会指数级缩小。如果权重很大，梯度会指数级增长。典型的前馈神经网络可以应对这些指数效应，因为它们只有少量隐藏层。另一方面，在训练长序列的RNN中，梯度很容易爆炸或消失。即使初始权重良好，也很难检测到当前目标输出依赖于许多时间步之前的输入，因此RNN在处理长距离依赖时会遇到困难。

学习RNN的有效方法有4种：

+   **长短期记忆**：将RNN构建为旨在长期记忆的模块。

+   **Hessian Free Optimization**：通过使用一种可以检测到微小梯度但曲率更小的复杂优化器来处理梯度消失问题。

+   **回声状态网络**：非常仔细地初始化输入->隐藏层和隐藏层->隐藏层以及输出->隐藏层的连接，使得隐藏状态拥有一个巨大的、弱耦合的振荡器储备，这些振荡器可以通过输入选择性地驱动。

+   **良好的动量初始化**：像在回声状态网络中那样初始化，然后使用动量学习所有的连接。

### 4 — 长短期记忆网络

![](../Images/d6dbe07ea1caf153d300783588d959d4.png)

[Hochreiter & Schmidhuber (1997)](http://www.bioinf.jku.at/publications/older/2604.pdf) 通过建立被称为**长短期记忆网络**的模型解决了使RNN长期记忆的难题（如数百个时间步骤）。他们设计了一个使用逻辑单元和线性单元以及乘法交互的记忆单元。当“写入”门开启时，信息会进入单元。信息会在“保持”门开启时留在单元中。通过打开“读取”门，可以从单元中读取信息。

阅读草书是一项RNN的自然任务。输入是笔尖的（x，y，p）坐标序列，其中p表示笔是上还是下。输出是字符序列。[Graves & Schmidhuber (2009)](http://people.idsia.ch/~juergen/nips2009.pdf) 显示带有LSTM的RNN目前是阅读草书的最佳系统。简言之，他们使用了一系列小图像作为输入，而不是笔坐标。

![](../Images/0de05190c7dbf55ac81c791d617a527e.png)

### 更多相关内容

+   [成为优秀数据科学家所需的5项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)

+   [每个初学者数据科学家应该掌握的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)

+   [2021年最佳ETL工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)

+   [停止学习数据科学以寻找目标，并找到目标以…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)

+   [数据科学学习统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)

+   [成功数据科学家的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)
