- en: Top 10 Machine Learning Algorithms for Beginners
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/10/top-10-machine-learning-algorithms-beginners.html](https://www.kdnuggets.com/2017/10/top-10-machine-learning-algorithms-beginners.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/c9c27633e28613cfa8923640c7a1149f.png)'
  prefs: []
  type: TYPE_IMG
- en: I. Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The study of [ML algorithms](https://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html)
    has gained immense traction post the Harvard Business Review [article](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century)
    terming a ‘Data Scientist’ as the ‘Sexiest job of the 21st century’. So, for those
    starting out in the field of ML, we decided to do a reboot of our immensely popular
    Gold blog [The 10 Algorithms Machine Learning Engineers need to know](/2016/08/10-algorithms-machine-learning-engineers.html)
    - albeit this post is targetted towards beginners.
  prefs: []
  type: TYPE_NORMAL
- en: ML algorithms are those that can learn from data and improve from experience,
    without human intervention. Learning tasks may include learning the function that
    maps the input to the output, learning the hidden structure in unlabeled data;
    or ‘instance-based learning’, where a class label is produced for a new instance
    by comparing the new instance (row) to instances from the training data, which
    were stored in memory. ‘Instance-based learning’ does not create an abstraction
    from specific instances.
  prefs: []
  type: TYPE_NORMAL
- en: II. Types of ML algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are 3 types of ML algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Supervised learning:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised learning can be explained as follows: use labeled training data
    to learn the mapping function from the input variables (X) to the output variable
    (Y).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Y = f (X)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised learning problems can be of two types:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a. Classification*: To predict the outcome of a given sample where the output
    variable is in the form of categories. Examples include labels such as male and
    female, sick and healthy.'
  prefs: []
  type: TYPE_NORMAL
- en: '*b. Regression*: To predict the outcome of a given sample where the output
    variable is in the form of real values. Examples include real-valued labels denoting
    the amount of rainfall, the height of a person.'
  prefs: []
  type: TYPE_NORMAL
- en: The 1st 5 algorithms that we cover in this blog– Linear Regression, Logistic
    Regression, CART, Naïve Bayes, KNN are examples of supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Ensembling is a type of supervised learning. It means combining the predictions
    of multiple different weak ML models to predict on a new sample. Algorithms 9-10
    that we cover– Bagging with Random Forests, Boosting with XGBoost are examples
    of ensemble techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Unsupervised learning:**'
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning problems possess only the input variables (X) but no corresponding
    output variables. It uses unlabeled training data to model the underlying structure
    of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsupervised learning problems can be of two types:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a. Association*: To discover the probability of the co-occurrence of items
    in a collection. It is extensively used in market-basket analysis. Example: If
    a customer purchases bread, he is 80% likely to also purchase eggs.'
  prefs: []
  type: TYPE_NORMAL
- en: '*b. Clustering*: To group samples such that objects within the same cluster
    are more similar to each other than to the objects from another cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: '*c. Dimensionality Reduction*: True to its name, Dimensionality Reduction means
    reducing the number of variables of a dataset while ensuring that important information
    is still conveyed. Dimensionality Reduction can be done using Feature Extraction
    methods and Feature Selection methods. Feature Selection selects a subset of the
    original variables. Feature Extraction performs data transformation from a high-dimensional
    space to a low-dimensional space. Example: PCA algorithm is a Feature Extraction
    approach.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms 6-8 that we cover here - Apriori, K-means, PCA are examples of unsupervised
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Reinforcement learning:**'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning is a type of machine learning algorithm that allows the
    agent to decide the best next action based on its current state, by learning behaviours
    that will maximize the reward.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement algorithms usually learn optimal actions through trial and error.
    They are typically used in robotics – where a robot can learn to avoid collisions
    by receiving negative feedback after bumping into obstacles, and in video games
    – where trial and error reveals specific movements that can shoot up a player’s
    rewards. The agent can then use these rewards to understand the optimal state
    of game play and choose the next action.
  prefs: []
  type: TYPE_NORMAL
- en: III. Quantifying the popularity of ML algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Survey papers [such as these](http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf)
    have quantified the 10 most popular data mining algorithms. However, such lists
    are subjective and as in the case of the quoted paper, the sample size of the
    polled participants is very narrow and consists of advanced practitioners of data
    mining. The persons polled were the winners of the ACM KDD Innovation Award, the
    IEEE ICDM Research Contributions Award; the Program Committee members of the KDD-06,
    ICDM’06 and SDM’06; and the 145 attendees of the ICDM’06.
  prefs: []
  type: TYPE_NORMAL
- en: The Top 10 algorithms in this blog are meant for beginners and are primarily
    those that I learnt from the ‘Data Warehousing and Mining’ (DWM) course during
    my Bachelor’s degree in Computer Engineering at the University of Mumbai. The
    DWM course is a great introduction to the field of ML algorithms. I have especially
    included the last 2 algorithms (ensemble methods) based on their [prevalence to
    win Kaggle competitions](http://www.datasciencecentral.com/profiles/blogs/want-to-win-at-kaggle-pay-attention-to-your-ensembles)
    . Hope you enjoy the article!
  prefs: []
  type: TYPE_NORMAL
- en: IV. Supervised learning algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**1\. Linear Regression**'
  prefs: []
  type: TYPE_NORMAL
- en: In ML, we have a set of input variables (x) that are used to determine the output
    variable (y). A relationship exists between the input variables and the output
    variable. The goal of ML is to quantify this relationship.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a1ae8980334e4a697589004f968e799e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Linear Regression is represented as a line in the form of y = a +
    bx. [Source](http://bhagyeshvikani.blogspot.ca/2015/10/linear-regression.html)In
    Linear Regression, the relationship between the input variables (x) and output
    variable (y) is expressed as an equation of the form y = a + bx. Thus, the goal
    of linear regression is to find out the values of coefficients a and b. Here,
    a is the intercept and b is the slope of the line.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1 shows the plotted x and y values for a dataset. The goal is to fit
    a line that is nearest to most of the points. This would reduce the distance (‘error’)
    between the y value of a data point and the line.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Logistic Regression**'
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression predictions are continuous values (rainfall in cm),logistic
    regression predictions are discrete values (whether a student passed/failed) after
    applying a transformation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic regression is best suited for binary classification (datasets where
    y = 0 or 1, where 1 denotes the default class. Example: In predicting whether
    an event will occur or not, the event that it occurs is classified as 1\. In predicting
    whether a person will be sick or not, the sick instances are denoted as 1). It
    is named after the transformation function used in it, called the logistic function
    h(x)= 1/ (1 + e^x), which is an S-shaped curve.'
  prefs: []
  type: TYPE_NORMAL
- en: In logistic regression, the output is in the form of probabilities of the default
    class (unlike linear regression, where the output is directly produced). As it
    is a probability, the output lies in the range of 0-1\. The output (y-value) is
    generated by log transforming the x-value, using the logistic function h(x)= 1/
    (1 + e^ -x) . A threshold is then applied to force this probability into a binary
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4c145ea1c19f29d1a5af2933df8d96f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Logistic Regression to determine if a tumour is malignant or benign.
    Classified as malignant if the probability h(x)>= 0.5\. [Source](https://athemathmo.github.io/2016/03/07/rusty-machine.html)In
    Figure 2, to determine whether a tumour is malignant or not, the default variable
    is y=1 (tumour= malignant) ; the x variable could be a measurement of the tumour,
    such as the size of the tumour. As shown in the figure, the logistic function
    transforms the x-value of the various instances of the dataset, into the range
    of 0 to 1\. If the probability crosses the threshold of 0.5 (shown by the horizontal
    line), the tumour is classified as malignant.'
  prefs: []
  type: TYPE_NORMAL
- en: The logistic regression equation *P(x) = e ^ (b0 +b1*x) / (1 + e^(b0 + b1*x))*
    can be transformed into *ln(p(x) / 1-p(x)) = b0 + b1*x*.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of logistic regression is to use the training data to find the values
    of coefficients b0 and b1 such that it will minimize the error between the predicted
    outcome and the actual outcome. These coefficients are estimated using the technique
    of Maximum Likelihood Estimation.
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. CART**'
  prefs: []
  type: TYPE_NORMAL
- en: Classification and Regression Trees (CART) is an implementation of Decision
    Trees, among others such as ID3, C4.5.
  prefs: []
  type: TYPE_NORMAL
- en: 'The non-terminal nodes are the root node and the internal node. The terminal
    nodes are the leaf nodes. Each non-terminal node represents a single input variable
    (x) and a splitting point on that variable; the leaf nodes represent the output
    variable (y). The model is used as follows to make predictions: walk the splits
    of the tree to arrive at a leaf node and output the value present at the leaf
    node.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The decision tree in Figure3 classifies whether a person will buy a sports
    car or a minivan depending on their age and marital status. If the person is over
    30 years and is not married, we walk the tree as follows : ‘over 30 years?’ ->
    yes -> ’married?’ -> no. Hence, the model outputs a sportscar.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b6949b5859022506961fc0ced7a7b482.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Parts of a decision tree. [Source](http://www.hypertextbookshop.com/dataminingbook/public_version/contents/chapters/chapter001/section002/green/page001.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Naïve Bayes**'
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the probability that an event will occur, given that another event
    has already occurred, we use Bayes’ Theorem. To calculate the probability of an
    outcome given the value of some variable, that is, to calculate the probability
    of a hypothesis(h) being true, given our prior knowledge(d), we use Bayes’ Theorem
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(h|d)= (P(d|h) * P(h)) / P(d)*'
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: P(h|d) = Posterior probability. The probability of hypothesis h being true,
    given the data d, where P(h|d)= P(d1| h)* P(d2| h)*....*P(dn| h)* P(d)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(d|h) = Likelihood. The probability of data d given that the hypothesis h was
    true.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(h) = Class prior probability. The probability of hypothesis h being true (irrespective
    of the data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(d) = Predictor prior probability. Probability of the data (irrespective of
    the hypothesis)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This algorithm is called ‘naive’ because it assumes that all the variables are
    independent of each other, which is a naive assumption to make in real-world examples.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f2dc03a423162be40bd2e9876237bb27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Using Naive Bayes to predict the status of ‘play’ using the variable
    ‘weather’.Using Figure 4 as an example, what is the outcome if weather=’sunny’?'
  prefs: []
  type: TYPE_NORMAL
- en: To determine the outcome play= ‘yes’ or ‘no’ given the value of variable weather=’sunny’,
    calculate P(yes|sunny) and P(no|sunny) and choose the outcome with higher probability.
  prefs: []
  type: TYPE_NORMAL
- en: ->P(yes|sunny)= (P(sunny|yes) * P(yes)) /  P(sunny)
  prefs: []
  type: TYPE_NORMAL
- en: = (3/9  * 9/14 ) / (5/14)
  prefs: []
  type: TYPE_NORMAL
- en: = 0.60
  prefs: []
  type: TYPE_NORMAL
- en: -> P(no|sunny)=  (P(sunny|no) * P(no)) /  P(sunny)
  prefs: []
  type: TYPE_NORMAL
- en: = (2/5  * 5/14 ) / (5/14)
  prefs: []
  type: TYPE_NORMAL
- en: = 0.40
  prefs: []
  type: TYPE_NORMAL
- en: Thus, if the weather =’sunny’, the outcome is play= ‘yes’.
  prefs: []
  type: TYPE_NORMAL
- en: '**5\. KNN**'
  prefs: []
  type: TYPE_NORMAL
- en: The k-nearest neighbours algorithm uses the entire dataset as the training set,
    rather than splitting the dataset into a trainingset and testset.
  prefs: []
  type: TYPE_NORMAL
- en: When an outcome is required for a new data instance, the KNN algorithm goes
    through the entire dataset to find the k-nearest instances to the new instance,
    or the k number of instances most similar to the new record, and then outputs
    the mean of the outcomes (for a regression problem) or the mode (most frequent
    class) for a classification problem. The value of k is user-specified.
  prefs: []
  type: TYPE_NORMAL
- en: The similarity between instances is calculated using measures such as Euclidean
    distance and Hamming distance.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
