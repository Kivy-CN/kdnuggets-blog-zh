- en: 'Introducing MPT-7B: A New Open-Source LLM'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/05/introducing-mpt7b-new-opensource-llm.html](https://www.kdnuggets.com/2023/05/introducing-mpt7b-new-opensource-llm.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Introducing MPT-7B: A New Open-Source LLM](../Images/e8dc98d8b617d12aa091933c049049a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The Large language models (LLM) are going crazy at the moment. However, as an
    organization, if you do not have the right resources, it can be challenging to
    jump on the large language model wave. Training and deploying large language models
    can be difficult, and you suddenly feel left out. Open-source LLMs, such as the
    LLaMA series from Meta have allowed for LLM resources to be available.
  prefs: []
  type: TYPE_NORMAL
- en: And to add to the open-source collection is [MosaicML Foundations](https://www.mosaicml.com/)'
    latest addition to their series - [MPT-7B](https://huggingface.co/mosaicml/mpt-7b).
  prefs: []
  type: TYPE_NORMAL
- en: What is MPT-7B?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MPT stands for MosaicML Pretrained Transformer. MPT models are GPT-style decoder-only
    transformers that come with many improvements:'
  prefs: []
  type: TYPE_NORMAL
- en: Performance-optimized layer implementations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Greater training stability due to architecture changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No context length limitations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MPT-7B is a transformer model that has been trained from scratch using 1T tokens
    of text and code. Yes, 1 TRILLION! It was trained on the MosaicML platform, with
    a time frame of 9.5 days with zero human intervention. Costing MosaicML ~$200k.
  prefs: []
  type: TYPE_NORMAL
- en: It is open-source, making it available for commercial use and the tool will
    be a game changer on how businesses and organizations work with their predictive
    analytics and decision-making process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main features of MPT-7B are:'
  prefs: []
  type: TYPE_NORMAL
- en: Licensed for commercial use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trained on a large amount of data (1T tokens)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can handle extremely long inputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimized for fast training and inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly efficient open-source training code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MPT-7B is the base model and has been shown to outperform other open-source
    7B - 20B models. The quality of MPT-7B matches LLaMA-7B. To evaluate the quality
    of MPT-7B, MosaicML Foundation put together 11 open-source benchmarks and evaluated
    them using the industry-standard manner.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing MPT-7B: A New Open-Source LLM](../Images/05981dd7e854b430900b0d303b0aa9b6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [MosaicML Foundation](https://www.mosaicml.com/blog/mpt-7b)
  prefs: []
  type: TYPE_NORMAL
- en: 'MosaicML foundations are also releasing three additional fine-tuned models:'
  prefs: []
  type: TYPE_NORMAL
- en: MPT-7B-Instruct
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MPT-7B-Chat
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MPT-7B-StoryWriter-65k+
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MPT-7B-Instruct
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [MPT-7B-Instruct](https://huggingface.co/mosaicml/mpt-7b-instruct) model
    is for short-form instruction following. With 26,834 dated the 14th of May, MPT-7B-Instruct
    allows you to ask quick and short questions and provides you with an instant response.
    Have a question, and you just want a simple answer - use MPT-7B-Instruct.
  prefs: []
  type: TYPE_NORMAL
- en: Why is this so great? Typically LLMs are taught to continue generating text
    based on the input that was provided. However, some are looking for LLMs that
    treat their input as an instruction. Instruction finetuning allows LLMs to perform
    instruction-following outputs.
  prefs: []
  type: TYPE_NORMAL
- en: MPT-7B-Chat
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Yes, we have another chatbot. [MPT-7B-Chat](https://huggingface.co/mosaicml/mpt-7b-chat)
    generates dialogue. For example, if you want the chatbot to generate a speech,
    giving it context it will generate a text in a conversational manner. Or maybe
    you want to write a tweet which paraphrases a paragraph from an article, it can
    generate the dialogue for you!
  prefs: []
  type: TYPE_NORMAL
- en: Why is this so great? MPT-7B Chat is ready and well-equipped for a variety of
    conversational tasks, delivering more seamless, engaging multi-turn interactions
    for users.
  prefs: []
  type: TYPE_NORMAL
- en: MPT-7B-StoryWriter-65k+
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is for the story writers! For those who want to write stories that have
    a long context, [MPT-7B-StoryWriter-65k+](https://huggingface.co/mosaicml/mpt-7b-storywriter)
    is a model designed for exactly that. The model was built by fine-tuning MPT-7B
    with a **context length of 65k tokens**, and it can extrapolate beyond 65k tokens.
    MosaicML Foundation has been able to generate 84k tokens on a single node of A100-80GB
    GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Why is this so great? This is because most open-source LLMs can only handle
    sequences with up to a few thousand tokens. But just by using a single node of
    8xA100-80GB on the MosaicML platform, you can finetune MPT-7B to handle context
    lengths up to 65k!
  prefs: []
  type: TYPE_NORMAL
- en: More on How MPT-7B was Built
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The MosaicML team built these models in only a few weeks. In only a few weeks
    they dealt with the data preparation, training, finetuning, and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: The data was sourced from a variety of sources, which all had a billion tokens
    available in each source. The number of effective tokens still got a billion in
    each source! The team used [EleutherAI’s](https://www.eleuther.ai/), [GPT-NeoX](https://aclanthology.org/2022.bigscience-1.9/),
    and [20B tokenizer](https://huggingface.co/docs/transformers/model_doc/gpt_neox),
    allowing them to train on a diverse mix of data, apply consistent space delimitation,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: All the MPT-7B models were trained on the [MosaicML platform](https://www.mosaicml.com/training),
    using A100-40GB and A100-80GB GPUs from Oracle Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you would like to know more about the tools and costs of MPT-7B, have a
    read of the: [MPT-7B Blog](https://www.mosaicml.com/blog/mpt-7b).'
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping it up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The MosaicML platform can be considered as the best starting point for organisations,
    if it be private, commercial or community related to build custom LLMs. Having
    this open-source resource available will allow organisations to feel freer about
    using these tools to improve the current organisational challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Customers are able to train LLMs on any computing provider, or data source,
    whilst being able to maintain efficiency, privacy and cost transparency.
  prefs: []
  type: TYPE_NORMAL
- en: What do you think you will be using MPT-7B for? Let us know in the comments
    below
  prefs: []
  type: TYPE_NORMAL
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist,
    Freelance Technical Writer and Community Manager at KDnuggets. She is particularly
    interested in providing Data Science career advice or tutorials and theory based
    knowledge around Data Science. She also wishes to explore the different ways Artificial
    Intelligence is/can benefit the longevity of human life. A keen learner, seeking
    to broaden her tech knowledge and writing skills, whilst helping guide others.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Introducing MetaGPT''s Data Interpreter: SOTA Open Source LLM-based…](https://www.kdnuggets.com/metagpt-data-interpreter-open-source-llm-based-data-solutions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Web LLM: Bring LLM Chatbots to the Browser](https://www.kdnuggets.com/2023/05/webllm-bring-llm-chatbots-browser.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introducing the Testing Library for Natural Language Processing](https://www.kdnuggets.com/2023/04/introducing-testing-library-natural-language-processing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introducing Healthcare-Specific Large Language Models from John Snow Labs](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introducing OpenChat: The Free & Simple Platform for Building…](https://www.kdnuggets.com/2023/06/introducing-openchat-free-simple-platform-building-custom-chatbots-minutes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introducing OpenLLM: Open Source Library for LLMs](https://www.kdnuggets.com/2023/07/introducing-openllm-open-source-library-llms.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
