- en: The 5 Classification Evaluation Metrics Every Data Scientist Must Know
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/10/5-classification-evaluation-metrics-every-data-scientist-must-know.html](https://www.kdnuggets.com/2019/10/5-classification-evaluation-metrics-every-data-scientist-must-know.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '***What do we want to optimize for? ***Most of the businesses fail to answer
    this simple question.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Every business problem is a little different, and it should be optimized
    differently.***'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: We all have created classification models. A lot of time we try to increase
    evaluate our models on accuracy.*** But do we really want accuracy as a metric
    of our model performance?***
  prefs: []
  type: TYPE_NORMAL
- en: '***What if we are predicting the number of asteroids that will hit the earth.***'
  prefs: []
  type: TYPE_NORMAL
- en: Just say zero all the time. And you will be 99% accurate. My model can be reasonably
    accurate, but not at all valuable. What should we do in such cases?
  prefs: []
  type: TYPE_NORMAL
- en: '*Designing a Data Science project is much more important than the modeling
    itself.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***This post is about various evaluation metrics and how and when to use them.***'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Accuracy, Precision, and Recall:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/fe2bc86aaaf0cf9b6c68d2768751a9ed.png)'
  prefs: []
  type: TYPE_IMG
- en: A. Accuracy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Accuracy is the quintessential classification metric. It is pretty easy to understand.
    And easily suited for binary as well as a multiclass classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: '`Accuracy = (TP+TN)/(TP+FP+FN+TN)`'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy is the proportion of true results among the total number of cases examined.
  prefs: []
  type: TYPE_NORMAL
- en: '**When to use?**'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy is a valid choice of evaluation for classification problems which are
    well balanced and not skewed or No class imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Caveats**'
  prefs: []
  type: TYPE_NORMAL
- en: Let us say that our target class is very sparse. Do we want accuracy as a metric
    of our model performance? ***What if we are predicting if an asteroid will hit
    the earth? ***Just say `No` all the time. And you will be 99% accurate. My model
    can be reasonably accurate, but not at all valuable.
  prefs: []
  type: TYPE_NORMAL
- en: B. Precision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s start with *precision*, which answers the following question: what proportion
    of **predicted Positives **is truly Positive?'
  prefs: []
  type: TYPE_NORMAL
- en: '`Precision = (TP)/(TP+FP)`'
  prefs: []
  type: TYPE_NORMAL
- en: In the asteroid prediction problem, we never predicted a true positive.
  prefs: []
  type: TYPE_NORMAL
- en: And thus precision = 0
  prefs: []
  type: TYPE_NORMAL
- en: '**When to use?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Precision is a valid choice of evaluation metric when we want to be very sure
    of our prediction. For example: If we are building a system to predict if we should
    decrease the credit limit on a particular account, we want to be very sure about
    our prediction or it may result in customer dissatisfaction.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Caveats**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Being very precise means our model will leave a lot of credit defaulters untouched
    and hence lose money.*'
  prefs: []
  type: TYPE_NORMAL
- en: C. Recall
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another very useful measure is* recall*, which answers a different question:
    what proportion of **actual Positives **is correctly classified?'
  prefs: []
  type: TYPE_NORMAL
- en: '`Recall = (TP)/(TP+FN)`'
  prefs: []
  type: TYPE_NORMAL
- en: In the asteroid prediction problem, we never predicted a true positive.
  prefs: []
  type: TYPE_NORMAL
- en: And thus recall is also equal to 0.
  prefs: []
  type: TYPE_NORMAL
- en: '**When to use?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall is a valid choice of evaluation metric when we want to capture as many
    positives as possible. For example: If we are building a system to predict if
    a person has cancer or not, we want to capture the disease even if we are not
    very sure.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Caveats**'
  prefs: []
  type: TYPE_NORMAL
- en: '***Recall is 1 if we predict 1 for all examples.***'
  prefs: []
  type: TYPE_NORMAL
- en: And thus comes the idea of utilizing tradeoff of precision vs. recall — ***F1
    Score***.
  prefs: []
  type: TYPE_NORMAL
- en: '2\. F1 Score:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is my ***favorite evaluation metric*** and I tend to use this a lot in
    my classification projects.
  prefs: []
  type: TYPE_NORMAL
- en: The F1 score is a number between 0 and 1 and is the harmonic mean of precision
    and recall.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d13bce6c7f3757567b2860521e727525.png)'
  prefs: []
  type: TYPE_IMG
- en: Let us start with a binary prediction problem. ***We are predicting if an asteroid
    will hit the earth or not.***
  prefs: []
  type: TYPE_NORMAL
- en: So if we say “No” for the whole training set. Our precision here is 0\. What
    is the recall of our positive class? It is zero. What is the accuracy? It is more
    than 99%.
  prefs: []
  type: TYPE_NORMAL
- en: And hence the F1 score is also 0\. And thus we get to know that the classifier
    that has an accuracy of 99% is basically worthless for our case. And hence it
    solves our problem.
  prefs: []
  type: TYPE_NORMAL
- en: '**When to use?**'
  prefs: []
  type: TYPE_NORMAL
- en: We want to have a model with both good precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/588de63fd8749e1cfea448c9144f6dbe.png)'
  prefs: []
  type: TYPE_IMG
- en: Precision-Recall Tradeoff
  prefs: []
  type: TYPE_NORMAL
- en: Simply stated the ***F1 score sort of maintains a balance between the precision
    and recall for your classifier***. If your precision is low, the F1 is low and
    if the recall is low again your F1 score is low.
  prefs: []
  type: TYPE_NORMAL
- en: If you are a police inspector and you want to catch criminals, you want to be
    sure that the person you catch is a criminal (Precision) and you also want to
    capture as many criminals (Recall) as possible. The F1 score manages this tradeoff.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**How to Use?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can calculate the F1 score for binary prediction problems using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This is one of my functions which I use to get the best threshold for maximizing
    F1 score for binary predictions. The below function iterates through possible
    threshold values to find the one that gives the best F1 score.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Caveats**'
  prefs: []
  type: TYPE_NORMAL
- en: The main problem with the F1 score is that it gives equal weight to precision
    and recall. We might sometimes need to include domain knowledge in our evaluation
    where we want to have more recall or more precision.
  prefs: []
  type: TYPE_NORMAL
- en: To solve this, we can do this by creating a weighted F1 metric as below where
    beta manages the tradeoff between precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/642e02997e7d35b1a68750374c0e6542.png)'
  prefs: []
  type: TYPE_IMG
- en: Here we give β times as much importance to recall as precision.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: F1 Score can also be used for Multiclass problems. See [this](https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1) awesome
    blog post by [Boaz Shmueli](https://medium.com/u/57ee515c83c5?source=post_page-----aa97784ff226----------------------) for
    details.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Log Loss/Binary Cross-entropy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Log loss is a pretty good evaluation metric for binary classifiers and it is
    sometimes the optimization objective as well in case of Logistic regression and
    Neural Networks.
  prefs: []
  type: TYPE_NORMAL
- en: Binary Log loss for an example is given by the below formula where p is the
    probability of predicting 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/589f6701243bff934d5bab0dac0f4793.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](../Images/5eb3b742cca4ae0ee23b80a85487d7f6.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see the log loss decreases as we are fairly certain in our prediction
    of 1 and the true label is 1.
  prefs: []
  type: TYPE_NORMAL
- en: '**When to Use?**'
  prefs: []
  type: TYPE_NORMAL
- en: '*When the output of a classifier is prediction probabilities. ****Log Loss
    takes into account the uncertainty of your prediction based on how much it varies
    from the actual label. ***This gives us a more nuanced view of the performance
    of our model. In general, minimizing Log Loss gives greater accuracy for the classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: '**How to Use?**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Caveats**'
  prefs: []
  type: TYPE_NORMAL
- en: It is susceptible in case of [imbalanced](https://towardsdatascience.com/the-5-sampling-algorithms-every-data-scientist-need-to-know-43c7bc11d17c) datasets.
    You might have to introduce class weights to penalize minority errors more or
    you may use this after balancing your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Categorical Cross-entropy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The log loss also generalizes to the multiclass problem. The classifier in
    a multiclass setting must assign a probability to each class for all examples.
    If there are N samples belonging to M classes, then the *Categorical Cross-entropy* is
    the summation of -`ylogp` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/607c37a49856b1fa9722078f40518ccb.png)'
  prefs: []
  type: TYPE_IMG
- en: '`y_ij` is 1 if the sample `i` belongs to class `j` else 0'
  prefs: []
  type: TYPE_NORMAL
- en: '`p_ij` is the probability our classifier predicts of sample `i` belonging to
    class `j`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**When to Use?**'
  prefs: []
  type: TYPE_NORMAL
- en: '*When the output of a classifier is multiclass prediction probabilities. We
    generally use Categorical Crossentropy in case of Neural Nets. *In general, minimizing
    Categorical cross-entropy gives greater accuracy for the classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: '**How to Use?**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Caveats:**'
  prefs: []
  type: TYPE_NORMAL
- en: It is susceptible in case of [imbalanced](https://towardsdatascience.com/the-5-sampling-algorithms-every-data-scientist-need-to-know-43c7bc11d17c) datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. AUC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AUC is the area under the ROC curve.
  prefs: []
  type: TYPE_NORMAL
- en: '***AUC ROC indicates how well the probabilities from the positive classes are
    separated from the negative classes***'
  prefs: []
  type: TYPE_NORMAL
- en: '***What is the ROC curve?***'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe2bc86aaaf0cf9b6c68d2768751a9ed.png)'
  prefs: []
  type: TYPE_IMG
- en: We have got the probabilities from our classifier. We can use various threshold
    values to plot our sensitivity(TPR) and (1-specificity)(FPR) on the cure and we
    will have a ROC curve.
  prefs: []
  type: TYPE_NORMAL
- en: Where True positive rate or TPR is just the proportion of trues we are capturing
    using our algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '`Sensitivty = TPR(True Positive Rate)= Recall = TP/(TP+FN)`'
  prefs: []
  type: TYPE_NORMAL
- en: and False positive rate or FPR is just the proportion of false we are capturing
    using our algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '`1- Specificity = FPR(False Positive Rate)= FP/(TN+FP)`'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/c1344e1d0fb96baffeb0c4d68b4b55aa.png)'
  prefs: []
  type: TYPE_IMG
- en: ROC Curve
  prefs: []
  type: TYPE_NORMAL
- en: Here we can use the ROC curves to decide on a Threshold value.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of threshold value will also depend on how the classifier is intended
    to be used.
  prefs: []
  type: TYPE_NORMAL
- en: If it is a cancer classification application you don’t want your threshold to
    be as big as 0.5\. Even if a patient has a 0.3 probability of having cancer you
    would classify him to be 1.
  prefs: []
  type: TYPE_NORMAL
- en: Otherwise, in an application for reducing the limits on the credit card, you
    don’t want your threshold to be as less as 0.5\. You are here a little worried
    about the negative effect of decreasing limits on customer satisfaction.
  prefs: []
  type: TYPE_NORMAL
- en: '**When to Use?**'
  prefs: []
  type: TYPE_NORMAL
- en: AUC is **scale-invariant**. It measures how well predictions are ranked, rather
    than their absolute values. So, for example, if you as a marketer want to find
    a list of users who will respond to a marketing campaign. AUC is a good metric
    to use since the predictions ranked by probability is the order in which you will
    create a list of users to send the marketing campaign.
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit of using AUC is that it is **classification-threshold-invariant **like
    log loss. It measures the quality of the model’s predictions irrespective of what
    classification threshold is chosen, unlike F1 score or accuracy which depend on
    the choice of threshold.
  prefs: []
  type: TYPE_NORMAL
- en: '**How to Use?**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Caveats**'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes we will need well-calibrated probability outputs from our models and
    AUC doesn’t help with that.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An important step while creating our [machine learning pipeline](https://towardsdatascience.com/6-important-steps-to-build-a-machine-learning-system-d75e3b83686) is
    evaluating our different models against each other. A bad choice of an evaluation
    metric could wreak havoc to your whole system.
  prefs: []
  type: TYPE_NORMAL
- en: '***So, always be watchful of what you are predicting and how the choice of
    evaluation metric might affect/alter your final predictions.***'
  prefs: []
  type: TYPE_NORMAL
- en: Also, the choice of an evaluation metric should be well aligned with the business
    objective and hence it is a bit subjective. And you can come up with your own
    evaluation metric as well.
  prefs: []
  type: TYPE_NORMAL
- en: Continue Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to [learn](https://towardsdatascience.com/how-did-i-start-with-data-science-3f4de6b501b0?source=---------8------------------) more
    about how to structure a Machine Learning project and the best practices, I would
    like to call out his awesome [third course](https://click.linksynergy.com/link?id=lVarvwc5BD0&offerid=467035.11421702016&type=2&murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Fmachine-learning-projects) named
    Structuring Machine learning projects in the Coursera [Deep Learning Specialization](https://click.linksynergy.com/deeplink?id=lVarvwc5BD0&mid=40328&murl=https%3A%2F%2Fwww.coursera.org%2Fspecializations%2Fdeep-learning).
    Do check it out. It talks about the pitfalls and a lot of basic ideas to improve
    your models.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for the read. I am going to be writing more beginner-friendly posts in
    the future too. Follow me up at [**Medium**](https://medium.com/@rahul_agarwal?source=post_page---------------------------) or
    Subscribe to my [**blog**](http://eepurl.com/dbQnuX?source=post_page---------------------------) to
    be informed about them. As always, I welcome feedback and constructive criticism
    and can be reached on Twitter [@mlwhiz](https://twitter.com/MLWhiz?source=post_page---------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Also, a small disclaimer — There might be some affiliate links in this post
    to relevant resources as sharing knowledge is never a bad idea.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Rahul Agarwal](https://www.linkedin.com/in/rahulagwl/)** is Senior
    Statistical Analyst at WalmartLabs. Follow him on Twitter [@mlwhiz](https://twitter.com/MLWhiz).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[The 5 Graph Algorithms That Data Scientists Should Know](/2019/09/5-graph-algorithms-data-scientists-know.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Hitchhiker’s Guide to Feature Extraction](/2019/06/hitchhikers-guide-feature-extraction.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 bits of advice for Data Scientists](/2019/09/advice-data-scientists.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[More Performance Evaluation Metrics for Classification Problems You…](https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning Evaluation Metrics: Theory and Overview](https://www.kdnuggets.com/machine-learning-evaluation-metrics-theory-and-overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Classification Metrics Walkthrough: Logistic Regression with…](https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding Classification Metrics: Your Guide to Assessing Model…](https://www.kdnuggets.com/understanding-classification-metrics-your-guide-to-assessing-model-accuracy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, May 25: The 6 Python Machine Learning Tools Every…](https://www.kdnuggets.com/2022/n21.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 6 Python Machine Learning Tools Every Data Scientist Should Know About](https://www.kdnuggets.com/2022/05/6-python-machine-learning-tools-every-data-scientist-know.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
