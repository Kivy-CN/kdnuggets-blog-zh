# CLIP 模型初学者指南

> 原文：[https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html](https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html)

[评论](#comments)

**作者 [Matthew Brems](https://www.linkedin.com/in/matthewbrems/)，Roboflow 的增长经理**

你可能听说过 [OpenAI 的 CLIP 模型](https://openai.com/blog/clip/)。如果你查阅过，你会看到 CLIP 代表“对比语言-图像预训练”。这对我来说并不立即明白，所以我 [阅读了论文](https://arxiv.org/pdf/2103.00020.pdf) 以及 [相应的博客文章](https://openai.com/blog/clip/)。

* * *

## 我们的三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业道路。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你组织的 IT 工作

* * *

我在这里是为了将 CLIP 进行详细讲解，期望让你能轻松有趣地阅读！在这篇文章中，我将涵盖：

+   CLIP 是什么，

+   CLIP 的工作原理，以及

+   CLIP 的酷炫之处。

### 什么是 CLIP？

CLIP 是第一个多模态（在此案例中为视觉和文本）模型，旨在解决计算机视觉问题，最近由 [OpenAI](https://openai.com/blog/clip/) 于 2021 年 1 月 5 日发布。从 [OpenAI CLIP 仓库](https://github.com/openai/CLIP) 中可以看到，“CLIP（对比语言-图像预训练）是一个在多种（图像，文本）对上进行训练的神经网络。它可以用自然语言指令预测最相关的文本片段，而不直接针对任务进行优化，类似于 GPT-2 和 GPT-3 的零样本能力。”

根据你的背景，这 *可能* 有意义——但其中有很多内容可能对你来说比较陌生。我们来解读一下。

+   CLIP 是一个神经网络模型。

+   它在 400,000,000 对（图像，文本）数据上进行训练。一个（图像，文本）对可能是一张图片及其标题。也就是说，有 400,000,000 张图片及其标题进行了匹配，这些数据用于训练 CLIP 模型。

+   *“它可以根据给定的图像预测最相关的文本片段。”* 你可以将图像输入到 CLIP 模型中，它会为你返回该图像最可能的标题或摘要。

+   *“不直接优化任务，类似于GPT-2和GPT-3的零样本能力。”* 大多数机器学习模型学习特定任务。例如，一个训练用于分类狗和猫的图像分类器，期望在我们给定的任务上表现良好：分类狗和猫。我们通常不会期望一个训练过狗和猫的机器学习模型在检测浣熊方面表现得很好。然而，一些模型——包括CLIP、GPT-2和GPT-3——在未直接训练的任务上往往表现出色，这被称为“零样本学习”。

+   “零样本学习”是指模型尝试预测在训练数据中从未见过的类别。因此，例如，使用一个只在猫和狗上训练过的模型来检测浣熊。像CLIP这样的模型，由于它在（图像，文本）对中利用文本信息的方式，通常在零样本学习方面表现得非常出色——即使你看到的图像与训练图像非常不同，你的CLIP模型仍然能够为该图像提供一个不错的标题预测。

综合来看，CLIP模型是：

+   一个建立在数亿张图像和标题上的神经网络模型，

+   可以在给定图像的情况下返回最佳标题，并且

+   具有令人印象深刻的“零样本”能力，使其能够准确预测它从未见过的整个类别！

当我写我的[计算机视觉简介](https://blog.roboflow.com/intro-to-computer-vision/)文章时，我将计算机视觉描述为“计算机以类似于人类的方式看到并理解它所见的能力。”

[当我教授自然语言处理时](https://github.com/matthewbrems/nlp-fundamentals-python)，我以类似的方式描述了NLP：“计算机以类似于人类的方式理解语言的能力。”

**CLIP是计算机视觉和自然语言处理之间的桥梁。**

它不仅仅是计算机视觉和自然语言处理之间的桥梁——它是一个**非常强大的桥梁**，在这两者之间具有很大的灵活性和应用潜力。

### CLIP是如何工作的？

为了使图像和文本彼此关联，它们必须都被[嵌入](https://en.wikipedia.org/wiki/Embedding)。即使你没有这样考虑过，你也曾经使用过嵌入。我们来举一个例子。假设你有一只猫和两只狗。你可以将其表示为图表上的一个点，如下所示：

![](../Images/da688b366c7e92a9420424b780e0ca7a.png)

“1只猫，2只狗”的嵌入。 ([Source](https://www.wolframalpha.com/input/?i=%281%2C2%29).)

这可能看起来不那么令人兴奋，但我们刚刚将信息嵌入到你可能在初中学到的X-Y坐标系中（正式称为[欧几里得空间](https://en.wikipedia.org/wiki/Euclidean_space)）。你也可以将朋友的宠物信息嵌入到同一个图表中，或者你可以选择多种不同的方法来表示这些信息（例如，将狗放在猫之前，或为浣熊添加第三维度）。

我喜欢把嵌入看作是将信息压缩到数学空间中的一种方式。我们刚刚把关于狗和猫的信息压缩到数学空间中。***我们可以用同样的方法处理文本和图像！***

**CLIP模型由两个子模型组成，称为编码器：**

+   一个文本编码器将把文本嵌入（压缩）到数学空间中。

+   一个图像编码器将把图像嵌入（压缩）到数学空间中。

每当你训练一个[监督学习模型](https://towardsdatascience.com/a-brief-introduction-to-supervised-learning-54a3e3932590)时，你必须找到某种方法来衡量该模型的“好坏”——目标是训练一个“最好”和“最少坏”的模型。

CLIP模型也不例外：文本编码器和图像编码器旨在最大化好处并最小化坏处。

**那么，我们如何衡量“好坏”？**

在下面的图像中，你会看到一组紫色文本卡片输入到文本编码器中。每张卡片的输出将是一系列数字。例如，最上面的卡片，`pepper the aussie pup`，将进入文本编码器——即将其压缩到数学空间中的东西——并输出一系列数字，如(0, 0.2, 0.8)。

图像将会发生完全相同的情况：每张图像都会输入到图像编码器中，每张图像的输出也将是一系列数字。比如，假设是澳大利亚小狗Pepper的图片，输出可能是(0.05, 0.25, 0.7)。

![](../Images/e6be4d3d47d9e227f3975927e4c436de.png)

预训练阶段。（[来源](https://openai.com/blog/clip/)）

**我们模型的“好处”**

在理想的情况下，文本“pepper the aussie pup”的数字序列应该非常接近（相同于）对应图像的数字序列。*事实上，这应该是普遍适用的*：文本的数字序列应与对应图像的数字序列非常接近。衡量我们模型的“好处”的一种方法是每个文本的嵌入表示（数字序列）与每个图像的嵌入表示之间的接近程度。

有一种方便的方法来计算两个数字序列之间的相似性：[余弦相似性](https://www.machinelearningplus.com/nlp/cosine-similarity/)。我们在这里不会深入探讨这个公式的内部原理，但可以放心，它是一种经过验证的方法，用于查看两个向量或数字序列之间的相似度。（尽管这不是唯一的方法！）

在上面的图像中，浅蓝色方块表示文本和图像的重合位置。例如，T1 是第一个文本的嵌入表示；I1 是第一个图像的嵌入表示。我们希望 I1 和 T1 的余弦相似度尽可能高。对 I2 和 T2 也希望如此，其他浅蓝色方块也是如此。**这些余弦相似度越高，我们的模型就越“好ness”！**

**我们模型的“坏ness”**

在最大化每一个蓝色方块的余弦相似度的同时，也有很多灰色方块显示文本和图像的不对齐。例如，T1 是文本“pepper the aussie pup”，但也许 I2 是 [浣熊的图片](https://public.roboflow.com/object-detection/raccoon)。

![](../Images/733d91cf5addc3335394fbde3eff9d2e.png)

带有边界框注释的浣熊图片。 ([来源](https://public.roboflow.com/object-detection/raccoon).)

尽管这个浣熊很可爱，我们希望这张图像 (I2) 和文本 `pepper the aussie pup` 之间的余弦相似度相当小，因为这不是 Pepper the Aussie pup！

尽管我们希望蓝色方块具有高余弦相似度（因为这测量了“好ness”），我们希望所有灰色方块的余弦相似度都很低，因为这测量了“坏ness”。

![](../Images/5daa99f9af00dfc14023717b1561fad6.png)

最大化蓝色方块的余弦相似度；最小化灰色方块的余弦相似度。 ([来源](https://openai.com/blog/clip/).)

**文本和图像编码器如何适配？**

文本编码器和图像编码器同时通过最大化这些蓝色方块的余弦相似度和最小化灰色方块的余弦相似度来进行适配，这涵盖了我们所有的文本+图像对。

> 注意：根据数据的大小，这可能需要很长时间。CLIP 模型在 400,000,000 张标记图像上进行了训练。训练过程在 [592 个 V100 GPU](https://www.nvidia.com/en-us/data-center/v100/) 上耗时 30 天。这在 AWS 按需实例上训练将花费 1,000,000 美元！

一旦模型训练完成，你可以将图像传递给图像编码器，以检索最符合图像的文本描述——或者，反之，你可以将文本描述传递给模型以检索图像，如下文中的一些应用所示！

**CLIP 是计算机视觉和自然语言处理之间的桥梁。**

### 为什么 CLIP 很酷？

通过建立计算机视觉和自然语言处理之间的桥梁，CLIP 具有许多优势和酷炫的应用。我们将重点关注这些应用，但也有几个优势需要提及：

+   泛化能力：模型通常非常脆弱，只能知道你训练它做的非常具体的事情。CLIP 通过利用文本中的语义信息，扩展了分类模型对更多事物的知识。标准分类模型完全忽略了类别标签的语义含义，只是在幕后枚举数字类别；CLIP 通过理解类别的含义来工作。

+   比以往更好地连接文本和图像：考虑到速度和准确性，CLIP 可以说是“世界上最好的标题撰写者”。

+   已标注的数据：CLIP 基于已创建的图像和标题；而其他最先进的计算机视觉算法需要大量额外的人工时间来标注。

> 为什么 [@OpenAI](https://twitter.com/OpenAI?ref_src=twsrc%5Etfw) 的 CLIP 模型很重要？[https://t.co/X7bnSgZ0or](https://t.co/X7bnSgZ0or)
> 
> — Joseph Nelson (@josephofiowa) [2021年1月6日](https://twitter.com/josephofiowa/status/1346639942571712521?ref_src=twsrc%5Etfw)

迄今为止 CLIP 的一些用途：

+   [CLIP 已被用于对 Unsplash 上的照片进行索引](https://twitter.com/metasemantic/status/1349446585952989186?s=20)。

+   一位推特用户使用 CLIP 和 StyleGAN 将名人（包括 Elvis Presley、Beyoncé 和 Billie Eilish）[生成了“我的小马驹”风格的肖像](https://twitter.com/metasemantic/status/1368713208429764616)。

+   你玩过 Pictionary 吗？现在你可以在 [paint.wtf, where you'll be judged by CLIP](https://paint.wtf/) 在线玩这个游戏。

+   CLIP 可能被用来轻松改善 NSFW 过滤器！

+   [找到匹配某种情绪的照片——例如，通过一段诗歌](https://twitter.com/metasemantic/status/1349446585952989186)。

+   OpenAI 还创建了 [DALL-E, which creates images from text](https://openai.com/blog/dall-e/)。

我们希望你能查看以上内容，或创建你自己的项目！我们为你准备了一个 [CLIP 教程](https://blog.roboflow.com/how-to-use-openai-clip/)。如果你使用了它，[请告诉我们，以便我们将其添加到以上列表中](https://roboflow.com/contact)！

需要注意的是，CLIP 是计算机视觉和自然语言处理之间的 *一个* 桥梁。CLIP 并不是它们之间唯一的桥梁。你可以用非常不同的方式构建这些文本和图像编码器，或者找到其他连接两者的方法。**然而，CLIP 迄今为止是一种极具创新性的技术，推动了显著的额外创新。**

我们期待看到你用 CLIP 构建的作品，以及在其基础上取得的进展！

**简介：[Matthew Brems](https://www.linkedin.com/in/matthewbrems/)** 是 Roboflow 的增长经理。

[原文](https://blog.roboflow.com/clip-model-eli5-beginner-guide/)。经许可转载。

**相关内容：**

+   [OpenAI 发布了两种 Transformer 模型，将语言和计算机视觉神奇地连接起来](/2021/01/openai-transformer-models-link-language-computer-vision.html)

+   [使用均值平均精度评估对象检测模型](/2021/03/evaluating-object-detection-models-using-mean-average-precision.html)

+   [通过 SRU++ 降低训练 NLP 模型的高成本](/2021/03/reducing-high-cost-training-nlp-models-sru.html)

### 更多相关内容

+   [端到端机器学习的初学者指南](https://www.kdnuggets.com/2021/12/beginner-guide-end-end-machine-learning.html)

+   [必备机器学习算法：初学者指南](https://www.kdnuggets.com/2021/05/essential-machine-learning-algorithms-beginners.html)

+   [Q 学习的初学者指南](https://www.kdnuggets.com/2022/06/beginner-guide-q-learning.html)

+   [使用 Python 的网页抓取初学者指南](https://www.kdnuggets.com/2022/10/beginner-guide-web-scraping-python.html)

+   [云计算初学者指南](https://www.kdnuggets.com/2023/01/beginner-guide-cloud-computing.html)

+   [数据科学中异常检测技术的初学者指南](https://www.kdnuggets.com/2023/05/beginner-guide-anomaly-detection-techniques-data-science.html)
