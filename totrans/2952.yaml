- en: “Please, explain.” Interpretability of machine learning models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/05/interpretability-machine-learning-models.html](https://www.kdnuggets.com/2019/05/interpretability-machine-learning-models.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: '**By [Olga Mierzwa-Sulima](https://www.linkedin.com/in/olga-mierzwa-sulima-84843539/),
    Appsilon**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://appsilon.com/please-explain-black-box/). Reposted with permission.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: In February 2019 Polish government added an amendment to a banking law that
    gives a customer a right to receive an explanation in case of a negative credit
    decision. It’s one of the direct consequences of implementing GDPR in EU. This
    means that a bank needs to be able to explain why the loan wasn’t granted if the
    decision process was automatic.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: In October 2018 world headlines reported about [Amazon AI recruiting tool](https://www.theguardian.com/technology/2018/oct/10/amazon-hiring-ai-gender-bias-recruiting-engine) that
    favored men. Amazon’s model was trained on biased data that were skewed towards
    male candidates. It has built rules that penalized resumes that included the word
    “women’s”.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '**Consequences of not understanding models’ predictions**'
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What is common for the two examples above is that both models in the banking
    industry and the one built by Amazon are very complex tools, so-called black-box
    classifiers, that don’t offer straightforward and human-interpretable decision
    rules.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Financial institutions will have to invest in model interpretability research
    if they want to continue using ML-based solutions. And they probably will, because
    such algorithms are more accurate in predicting credit risk. Amazon on the other
    hand, could have saved a lot of money and bad press if the model was properly
    validated and understood.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '**Why now? Trends in data modeling.**'
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Machine learning has continued to stay on the top of Gartner’s Hype Cycle since
    2014, to be replaced by the Deep Learning (a form of ML) in 2018 suggesting the
    adoption hasn’t reached its peak yet.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57e65fff7ae21083675f67d428b48500.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
- en: '[**Source**](https://www.gartner.com/smarterwithgartner/5-trends-emerge-in-gartner-hype-cycle-for-emerging-technologies-2018/)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning growth is predicted to further accelerate. Based on the [report](http://www.univa.com/resources/univa-machine-learning-survey.php) by
    Univa 96% of the companies are expected to use ML in production in the next 2
    years.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'The reasons behind this are: widespread data collection, availability of vast
    computation resources and active open-source community. ML adoption growth is
    accompanied by the increase in ML-interpretability research driven by regulations
    like GDPR, EU’s “right to explain”, concerns about safety (medicine, autonomous
    vehicles), reproducibility and bias or end-users expectations (debug the model
    to improve it or learn something new about the studied subject).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d41678e8798ddf49f1a5a07c61346c62.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: '**[Source](http://people.csail.mit.edu/beenkim/papers/BeenK_FinaleDV_ICML2017_tutorial.pdf)**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '**Black-box algorithms interpretability possibilities**'
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As data scientists, we should be able to provide an explanation to end users
    about how a model works. However, this not necessarily means understanding every
    piece of the model or generating a set of decision rules.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'There could also be a case where this is not required:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: problem is well studied,
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: model results has no consequences,
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: understanding the model by the end-user could pose a risk of gaming the system.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we look at the results from the [Kaggle’s Machine Learning and Data Science
    Survey](https://www.kaggle.com/sudhirnl7/data-science-survey-2018) from 2018,
     around 60% of respondents think they could explain most of machine learning models
    (some models were still hard to explain for them). The most common approach used
    to ML understanding is analyzing model features by looking at feature importance
    and feature correlations.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature importance analysis** offers first good insights into what the model
    is learning and what factors might be important. However, this technique can be
    unreliable if features are correlated. It can provide good insights only if model
    variables are interpretable. For many [GBMs](https://towardsdatascience.com/boosting-algorithm-gbm-97737c63daa3) libraries
    it’s fairly easy to generate [feature importance plots](https://www.r-bloggers.com/variable-importance-plot-and-variable-selection/).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: In the case of **Deep Learning** situation is much more complicated. When using
    neural networks you could look at weights, as they contain the information about
    the input, but the information is compressed. What’s more, you can only analyze
    the connections on the first level, since on further levels it’s too complicated.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: No wonder that when in 2016 [**LIME **(Local Interpretable Model-Interpretable
    Explanations) ](https://arxiv.org/abs/1602.04938)paper was presented at NIPS conference
    it had a huge impact. The idea behind LIME is to locally approximate a black-box
    model with an easier to understand white-box model constructed on interpretable
    input data. It has proven great results providing [interpretation for image classification](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime) and [text](https://christophm.github.io/interpretable-ml-book/lime.html#lime-for-text).
    However, for tabular data, it’s difficult to find interpretable features and their
    local interpretation might be misleading.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: LIME is implemented in Python ([lime](https://github.com/marcotcr/lime) and [Skater](https://github.com/datascienceinc/Skater))
    and R ([lime package](https://cran.r-project.org/web/packages/lime/index.html) and [iml
    package](https://cran.r-project.org/web/packages/iml/index.html), [live package](https://cloud.r-project.org/web/packages/live/index.html))
    and is very easy to use.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Another promising idea is [SHAP (Shapley Additive Explanations)](https://arxiv.org/abs/1705.07874).
    It’s based on game theory. It assumes that features are players, models are coalitions
    and Shapley values tell how to fairly distribute the “payout” among the features.
    This technique distributes the effects fairly, is easy to use and offers visually
    compelling implementation.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[**DALEX** package](https://github.com/pbiecek/DALEX) (Descriptive Machine
    Learning Explanations) available in R offers a set of tools that help to understand
    how complex models are working. Using DALEX you can create model explainer and
    inspect it visually e.g. breakdown plots. You might also be interested in [DrWhy.Ai](https://github.com/ModelOriented/DrWhy/blob/master/README.md) which
    is developed by the same group of researchers as DALEX.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '**Practical use cases**'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Detecting objects on the pictures**'
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Image recognition** is already widely used, among others in autonomous cars
    to detect if cars, traffic lights etc. are on the picture, in wildlife conservation
    to detect if a certain animal is in the picture or in the insurance to detect
    flooding of crops.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: We will use the “Husky vs Wolf example” from the original LIME paper to illustrate
    the importance of model interpretation. The classifier task was to identify if
    a wolf was on the picture or not. It falsely misclassified Siberian Husky as a
    wolf. Thanks to LIME researchers were able to identify what areas of the pictures
    were important for the model. It turned out that if the picture contains snow
    it is classified as a wolf.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f4faf0e9878b6d177fc2001ead3de57.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: The algorithm was using the background of the picture and totally ignoring animal
    characteristics. The model should look at the animal eyes instead. Thanks to this
    discovery it was possible to fix the model and extend the training examples to
    prevent the reasoning snow = wolf.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification as decision support system**'
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Intensive Care Unit of Amsterdam UMC [wants predict the probabilities of patient’s
    readmission and/or mortality at the moment of discharge](https://medium.com/@Pacmedhealth/ai-for-health-care-tackling-the-issue-of-interpretability-868be42aaf50).
    The goal is to help doctors pick the right moment to move the patient from ICU.
    If the doctor understands what the model is doing is more likely to use it’s recommendation
    in making the final judgement.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: In order to demonstrate how such model can be interpreted using LIME, we can
    have a look at the example from [another study](https://www.researchgate.net/publication/309551203_Machine_Learning_Model_Interpretability_for_Precision_Medicine) that
    aims to do early prediction of the mortality at the ICU. Random Forest model (a
    black-box model) is used to predict mortality status and lime package is used
    to locally explain the prediction score for every patient.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f78b78b2133323e495c6249e5d6381b4.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: '[**Source**](https://www.researchgate.net/publication/309551203_Machine_Learning_Model_Interpretability_for_Precision_Medicine)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: A patient from the selected example has high death probability (78%). The model
    features that contribute to mortality are higher counts of atrial fibrillation
    and higher lactate level, which is consistent with current medical understanding.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '**Humans and machines – a perfect match**'
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to achieve success in building an interpretable AI we need to combine
    data science knowledge, algorithms and end users expertise. Data science work
    doesn’t finish after creating the model. It’s an iterative, usually long process
    with feedback loops provided by the experts, making sure the outcome is solid
    and understandable by humans.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'We strongly believe that by combining humans expertise and machines performance
    we can obtain the best conclusion: improve machine results and overcome human
    gut-feel bias.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio**: [Olga Mierzwa-Sulima](https://www.linkedin.com/in/olga-mierzwa-sulima-84843539/) is
    a Senior Data Scientist and Project Leader at Appsilon.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '**Resources:**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning
    education](https://www.kdnuggets.com/education/online.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Software for Analytics, Data Science, Data Mining, and Machine Learning](https://www.kdnuggets.com/software/index.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Related:**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[Are BERT Features InterBERTible?](https://www.kdnuggets.com/2019/02/bert-features-interbertible.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Artificial Intelligence and Data Science Advances in 2018 and Trends for 2019](https://www.kdnuggets.com/2019/02/ai-data-science-advances-trends.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The year in AI/Machine Learning advances: Xavier Amatriain 2018 Roundup](https://www.kdnuggets.com/2019/01/xamat-ai-machine-learning-roundup.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Explain NLP Models with LIME](https://www.kdnuggets.com/2022/01/explain-nlp-models-lime.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using SHAP Values for Model Interpretability in Machine Learning](https://www.kdnuggets.com/2023/08/shap-values-model-interpretability-machine-learning.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Simplifying Decision Tree Interpretability with Python & Scikit-learn](https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SHAP: Explain Any Machine Learning Model in Python](https://www.kdnuggets.com/2022/11/shap-explain-machine-learning-model-python.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Why Do Machine Learning Models Die In Silence?](https://www.kdnuggets.com/2022/01/machine-learning-models-die-silence.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么机器学习模型在沉默中消亡？](https://www.kdnuggets.com/2022/01/machine-learning-models-die-silence.html)'
- en: '[Working With Sparse Features In Machine Learning Models](https://www.kdnuggets.com/2021/01/sparse-features-machine-learning-models.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在机器学习模型中处理稀疏特征](https://www.kdnuggets.com/2021/01/sparse-features-machine-learning-models.html)'
