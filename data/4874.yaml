- en: Creating a simple text classifier using Google CoLaboratory
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用谷歌 CoLaboratory 创建一个简单的文本分类器
- en: 原文：[https://www.kdnuggets.com/2018/03/simple-text-classifier-google-colaboratory.html](https://www.kdnuggets.com/2018/03/simple-text-classifier-google-colaboratory.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/03/simple-text-classifier-google-colaboratory.html](https://www.kdnuggets.com/2018/03/simple-text-classifier-google-colaboratory.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Sudipto Dasgupta](https://www.linkedin.com/in/dsudipto/), Flipkart.**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Sudipto Dasgupta](https://www.linkedin.com/in/dsudipto/), Flipkart.**'
- en: '**The Platform**: Google has scored another hit with CoLaboratory, its in-house
    data science platform that is freely available for anyone to use. It gives several
    benefits of Jupyter, free GPU time, easy code sharing and storage, no requirement
    of software installation, coding using a chrome browser and compatibility with
    the Python language and access to modules such as scikit-learn. This is a truly
    great step in making AI and data accessible to all.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**平台**：谷歌凭借 CoLaboratory 这一内部数据科学平台再次取得成功，该平台对任何人免费开放。它提供了 Jupyter 的几个好处，包括免费的
    GPU 时间、便捷的代码共享和存储、不需要安装软件、使用 Chrome 浏览器编码，并兼容 Python 语言和访问 scikit-learn 等模块。这是在使人工智能和数据对所有人可及方面迈出的真正伟大的一步。'
- en: '**The Context**: I work for an e-commerce organization, where mis-shipments
    are ubiquitous in the business. When the information regarding a mis-shipment
    is received in the system, a team of experts read the comments generated by the
    customer against every case, to determine how to investigate it. As open text
    fields are difficult to control, customers are free to post messages which may
    not be actionable, or sometimes even understandable. Reading comments take up
    a considerable amount of time, and given previous labelling information, the junk
    comments can be easily labelled by text classification algorithms. Given below
    is a simple classifier which can generate labels with a high level of accuracy
    given sufficient training data and balanced label distribution.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**背景**：我在一家电子商务公司工作，在业务中误发货是普遍现象。当系统收到关于误发货的信息时，一组专家会阅读客户对每个案件生成的评论，以确定如何调查。由于开放文本字段难以控制，客户可以自由发布可能无法采取行动或有时甚至无法理解的消息。阅读评论需要相当长的时间，鉴于以前的标记信息，垃圾评论可以通过文本分类算法轻松标记。以下是一个简单的分类器，它可以在有足够的训练数据和均衡标签分布的情况下生成高准确度的标签。'
- en: '![Binary Text Classifier](../Images/b19924393dc974c34e1cfb8aad65a7d5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![二元文本分类器](../Images/b19924393dc974c34e1cfb8aad65a7d5.png)'
- en: '**Loading the corpus**: The training data consists of two columns, the first
    containing comments and the second, the labels (0 and 1). First, we load the data
    onto the colab environment with the following code -'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**加载语料库**：训练数据包括两列，第一列包含评论，第二列包含标签（0 和 1）。首先，我们使用以下代码将数据加载到 colab 环境中 -'
- en: '**from** google**.**colab **import** files'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**从** google**.**colab **导入** files'
- en: '**import** pandas **as** pd'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**导入** pandas **作为** pd'
- en: '**import** io'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**导入** io'
- en: uploaded = files.upload()
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: uploaded = files.upload()
- en: df = pd.read_csv(io.StringIO(uploaded['data_train.csv'].decode('utf-8')),header=None)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: df = pd.read_csv(io.StringIO(uploaded['data_train.csv'].decode('utf-8')),header=None)
- en: Executing this block (known as cell in colab) generates an upload widget, by
    which the training data needs to be uploaded. Once this operation is complete,
    columns are given name references –
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码块（在 colab 中称为单元格）会生成一个上传小部件，通过它需要上传训练数据。操作完成后，列将被赋予名称引用 –
- en: raw_text = df[0]
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: raw_text = df[0]
- en: y = df[1]
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: y = df[1]
- en: '**Pre-Processing**:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**预处理**：'
- en: Though there are several different methods to classify, the one I have used
    involve the NLTK python package.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有几种不同的方法进行分类，但我使用的方法涉及 NLTK python 包。
- en: Stemming involves reducing a derived word to its base form. For example, the
    word ‘fish’ is a root for words such as ‘fishing’, ‘fished’, and ‘fisher’. Martin
    Porter’s algorithm is a popular stemming tool, which can be found in NLTK. Stopwords
    are words that do not add much meaning to a sentence from a feature extraction
    point of view. Words such as ‘after’, ‘few’, ‘right’ etc. are frequently ignored
    by search engines. A list of common stopwords can be found [HERE](https://www.webconfs.com/stop-words.php).
    I have imported the ‘PorterStemmer’ and ‘Stopwords’ from NLTK using the following
    commands.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取涉及将派生词还原为其基本形式。例如，‘fish’是‘fishing’、‘fished’和‘fisher’等词的词根。Martin Porter的算法是一个流行的词干提取工具，可以在NLTK中找到。停用词是从特征提取角度来看，对句子意义贡献不大的词。像‘after’、‘few’、‘right’等词经常被搜索引擎忽略。常见停用词的列表可以在[此处](https://www.webconfs.com/stop-words.php)找到。我通过以下命令从NLTK导入了‘PorterStemmer’和‘Stopwords’。
- en: '**import** nltk'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**import** nltk'
- en: nltk**.**download**(**'stopwords'**),**nltk**.**download**(**'porter_test'**)**
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: nltk**.**download**(**'stopwords'**),**nltk**.**download**(**'porter_test'**)**
- en: Another pre-processing step was conducted using the [Regular Expressions](https://docs.python.org/3/howto/regex.html)
    or ‘re’ module. This involved removing whitespaces, tabs, punctuations and finally
    converting all text in lowercase. This step is commonly known as normalization.
    A function named ‘pre_process’ was created to implement all these steps in a single
    line to any text or block of text.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个预处理步骤是使用[正则表达式](https://docs.python.org/3/howto/regex.html)或‘re’模块。这包括去除空格、制表符、标点符号，最后将所有文本转换为小写。此步骤通常称为规范化。创建了一个名为‘pre_process’的函数来将所有这些步骤实现为一行代码，适用于任何文本或文本块。
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now let us look at what this function does to text using the code on the first
    5 comments in the corpus.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看这个函数对文本的作用，使用代码对语料库中的前5条评论进行处理。
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Running the cell generates the following output in which you can compare lines
    to see the combined effect of pre-processing steps.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 运行单元格会生成以下输出，你可以比较这些行以查看预处理步骤的综合效果。
- en: '##Original Comment###'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '##原始评论###'
- en: Item is not same as shown in pciture
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 项目与显示的图片不符
- en: cmbissue :- cust called for the return and ...
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: cmbissue :- 客户打电话要求退货和...
- en: Different item
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的项目
- en: Dial color is white  I have ordered blackItem ...
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 表盘颜色是白色，我订购的是黑色项目...
- en: issue with quality and price tag missing
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 质量问题且价格标签缺失
- en: '###Transformed Comment###'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '###转换后的评论###'
- en: item shown pcitur
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 显示的项目图片
- en: cmbissucust call return refund due discript ...
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: cmbissucust call return refund due discript ...
- en: differ item
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的项目
- en: dial color white order blackitem receivvari...
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: dial color white order blackitem receivvari...
- en: issuqualiti price tag miss
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: issuqualiti price tag miss
- en: '**Tokenization:** Let’s consider the sentence- ‘How are you?’. Obviously, programs
    don’t understand words, they only understand characters. So, if a bag of words
    model is brought into play, the sentence ‘How are you?’ and ‘are How you?’ are
    same. However, the bigrams for the sentences would be different. Bigrams are subset
    of n-grams, which is a collection of base pairs, syllables or words. N-grams are
    highly popular not only in NLP, but also in other fields such as DNA sequencing!
    The bigrams are:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**分词：** 让我们考虑句子——‘你好吗？’。显然，程序无法理解单词，它们只理解字符。因此，如果采用词袋模型，句子‘你好吗？’和‘好吗 你’是相同的。然而，句子的二元组将会不同。二元组是n-grams的子集，n-grams是由基本对、音节或单词组成的集合。n-grams在自然语言处理和其他领域如DNA测序中都非常流行！二元组是：'
- en: ‘How are you?’  ----  ‘How are’ , ‘are you’
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ‘你好吗？’ ---- ‘你好吗’ , ‘好吗 你’
- en: ‘are How you?’  ----  ‘are How’ , ‘How you’
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ‘你好吗？’ ---- ‘你好吗’ , ‘好吗 你’
- en: 'The bigram generation code is:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 二元组生成代码是：
- en: Creating Unigram & Bigram Vectors
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建单字法和二字法向量
- en: '**from** sklearn**.**feature_extraction**.**text **import** TfidfVectorizer'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**from** sklearn**.**feature_extraction**.**text **import** TfidfVectorizer'
- en: vectorizer **=**TfidfVectorizer**(**ngram_range**=(**1**,**2**))**
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: vectorizer **=**TfidfVectorizer**(**ngram_range**=(**1**,**2**))**
- en: X_ngrams**=**vectorizer**.**fit_transform**(**processed**)**
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: X_ngrams**=**vectorizer**.**fit_transform**(**processed**)**
- en: The term frequency (tf) measures the occurrences of each n-gram for each training
    example. This is down weighted with the inverse document frequency (idf), ensuring
    that words/word pairs distinctive to either class have higher weights, while common
    grams have lower weights.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 词频（tf）衡量每个n-gram在每个训练示例中的出现次数。这个值会通过逆文档频率（idf）进行加权，以确保对每个类别具有独特性的词或词对具有更高的权重，而常见的n-grams具有较低的权重。
- en: '**Creating the classifier**: Once the features are generated by the previous
    block of code, the next step is to fit the model using the data. The data is split
    in 80/20 ratio, and modelled using (Binary) Logistic Regression.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**创建分类器**：一旦特征由前一段代码生成，下一步就是使用数据拟合模型。数据按 80/20 的比例分割，并使用（二元）逻辑回归进行建模。'
- en: '#Train/Test Split'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '# 训练/测试拆分'
- en: '**from** sklearn**.**model_selection **import** train_test_split'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**from** sklearn**.**model_selection **import** train_test_split'
- en: X_train**,**X_test**,**y_train**,**y_test**=**train_test_split**(**X_ngrams**,**y**,**test_size**=**0.2**,**stratify**=**y**)**
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: X_train**,**X_test**,**y_train**,**y_test**=**train_test_split**(**X_ngrams**,**y**,**test_size**=**0.2**,**stratify**=**y**)**
- en: Running the Classsifier
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行分类器
- en: '**from** sklearn**.**linear_model **import** LogisticRegression'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**from** sklearn**.**linear_model **import** 逻辑回归'
- en: clf**=**LogisticRegression**()**
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: clf**=**逻辑回归**()**
- en: clf**.**fit**(**X_train**,**y_train**)**
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: clf**.**fit**(**X_train**,**y_train**)**
- en: At this point one can use a metric such as F1 score to evaluate model performance.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，可以使用如 F1 分数这样的指标来评估模型性能。
- en: The machine has learnt!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 机器已经学会了！
- en: '![Machine learning robot](../Images/65cdcf419067bd98e8d68da1c84c8f30.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习机器人](../Images/65cdcf419067bd98e8d68da1c84c8f30.png)'
- en: Next, we need a wrapper function to separate the pearlsfrom the junk.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个包装函数来将珍珠与垃圾分开。
- en: '**def** pearl_or_junk**(**message**):**# Wrapper Function'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**def** pearl_or_junk**(**message**):**# 包装函数'
- en: '**if** clf**.**predict**(**vectorizer**.**transform**([**pre_process**(**message**)])):**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**if** clf**.**predict**(**vectorizer**.**transform**([**pre_process**(**message**)])):**'
- en: '**return** ''pearl'''
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**return** ''珍珠'''
- en: '**else****:**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**else****:**'
- en: '**return** ''junk'''
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**return** ''垃圾'''
- en: 'That’s it! The wrapper function can process comment columns to classify whether
    a comment is useful or not. An example file can be passed using the following
    code:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！包装函数可以处理评论列，以分类评论是否有用。可以使用以下代码传递示例文件：
- en: uploaded **=**files**.**upload**()**
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 上传 **=**文件**.**upload**()**
- en: test_data**=**pd**.**read_csv**(**io**.**StringIO**(**uploaded**[**'test_file.csv'**].**decode**(**'utf-8'**)),**header**=****None****)**
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据**=**pd**.**read_csv**(**io**.**StringIO**(**uploaded**[**'test_file.csv'**].**decode**(**'utf-8'**)),**header**=****None****)**
- en: test_data**[**1**]=**test_data**[**0**].**apply**(**pearl_or_junk**)**
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据**[**1**]=**测试数据**[**0**].**apply**(**pearl_or_junk**)**
- en: test_data**.**to_csv**(**'newfile.csv'**,** index **=****None****,** header
    **=****False****)**
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据**.**to_csv**(**'newfile.csv'**,** index **=****None****,** header **=****False****)**
- en: files**.**download**(**'newfile.csv'**)**
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 文件**.**下载**(**'newfile.csv'**)**
- en: The code will create a column with labels ‘pearl’ and ‘junk’ against every line
    of comment which you will find in the file named ‘newfile’ that will be automatically
    downloaded to your system.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码将为每行评论创建一个带有‘珍珠’和‘垃圾’标签的列，您会在名为‘newfile’的文件中找到这些列，该文件将自动下载到您的系统中。
- en: '**What next?** As I stated earlier, this is a simple piece of code, and has
    a lot of improvement areas. If there is class imbalance in the training data,
    this model will predict the dominating class, and to address that a resampling
    technique such as SMOTE may help.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**接下来做什么？** 如我之前所述，这是一段简单的代码，还有很多改进的空间。如果训练数据中存在类别不平衡，该模型将预测占主导地位的类别，为了解决这个问题，可能需要使用如
    SMOTE 这样的重采样技术。'
- en: There are several NLP techniques such as lemmatization, that can boost the pre-processing.
    Usually increasing the n-gram to a 3 or 4 combination increases the load on the
    machine, and in my case, it just gave me a message that it is too tired to handle
    such complicated stuff. FYI, the bigram generated over 36,000 features in a sparse
    matrix. Imagine what will happen if you increase N.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种 NLP 技术如词形还原，可以提升预处理效果。通常，将 n-gram 增加到 3 或 4 的组合会增加机器的负担，而在我的情况下，它只是给了我一个信息，说它太累了，无法处理如此复杂的内容。仅供参考，二元组在稀疏矩阵中生成了超过
    36,000 个特征。想象一下，如果你增加 N 会发生什么。
- en: The modelling was super simple. Model stacking may help increase the accuracy,
    and other algorithms such as Naïve Bayes or SVC are known to handle such stuff
    better. I did not try a neural network, but that may also give better results,
    if the training data is sufficiently large. Google colab does provide free GPU
    also, so that may be worth a try.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 建模非常简单。模型堆叠可能有助于提高准确性，而 Naïve Bayes 或 SVC 等其他算法在处理这些问题时表现更好。我没有尝试神经网络，但如果训练数据足够大，它也可能给出更好的结果。谷歌
    Colab 也提供了免费的 GPU，所以值得一试。
- en: '![Google colab notebook](../Images/7bb1889dead5dd2c3101c7a50bb01ac5.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![谷歌 Colab 笔记本](../Images/7bb1889dead5dd2c3101c7a50bb01ac5.png)'
- en: Though I created this code in Jupyter, moving it to coLaboratory was a breeze.
    Thank you, Google!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我在 Jupyter 中创建了这段代码，但将其迁移到 Colaboratory 是非常轻松的。感谢你，谷歌！
- en: 'Tags: NLP; Machine Learning; Regression;'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 标签：NLP；机器学习；回归；
- en: 'Bio:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 简介：
- en: '**Sudipto Dasgupta **is currently working as a Specialist – Process Design
    for Flipkart India Pvt. Ltd., the largest e-commerce organization in India. He
    has 15+ years of experience in Business Analytics in domains such as software,
    market research, education and supply chain. He is an experienced Six Sigma Master
    Black Belt and project management professional (PMP) with an educational background
    in Mathematics and Statistics. He has an active interest in the Data Sciences.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sudipto Dasgupta** 目前在 Flipkart India Pvt. Ltd. 担任流程设计专员，这是印度最大的电子商务组织。他在软件、市场研究、教育和供应链等领域有超过
    15 年的业务分析经验。他是经验丰富的六西格玛大师黑带和项目管理专业人士（PMP），拥有数学和统计学的教育背景。他对数据科学有着积极的兴趣。'
- en: '**Related:**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[3 Essential Google Colaboratory Tips & Tricks](https://www.kdnuggets.com/2018/02/essential-google-colaboratory-tips-tricks.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3 个必备的 Google Colaboratory 小贴士](https://www.kdnuggets.com/2018/02/essential-google-colaboratory-tips-tricks.html)'
- en: '[Automated Text Classification Using Machine Learning](https://www.kdnuggets.com/2018/01/automated-text-classification-machine-learning.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用机器学习进行自动化文本分类](https://www.kdnuggets.com/2018/01/automated-text-classification-machine-learning.html)'
- en: '[Deep Learning Development with Google Colab, TensorFlow, Keras & PyTorch](https://www.kdnuggets.com/2018/02/google-colab-free-gpu-tutorial-tensorflow-keras-pytorch.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Google Colab、TensorFlow、Keras 和 PyTorch 进行深度学习开发](https://www.kdnuggets.com/2018/02/google-colab-free-gpu-tutorial-tensorflow-keras-pytorch.html)'
- en: '* * *'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的快车道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您所在组织的 IT 工作'
- en: '* * *'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题
- en: '[Creating A Simple Docker Data Science Image](https://www.kdnuggets.com/2023/08/simple-docker-data-science-image.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[创建一个简单的 Docker 数据科学镜像](https://www.kdnuggets.com/2023/08/simple-docker-data-science-image.html)'
- en: '[From Theory to Practice: Building a k-Nearest Neighbors Classifier](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从理论到实践：构建 k-近邻分类器](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)'
- en: '[Generate Music From Text Using Google MusicLM](https://www.kdnuggets.com/2023/06/generate-music-text-google-musiclm.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Google MusicLM 从文本生成音乐](https://www.kdnuggets.com/2023/06/generate-music-text-google-musiclm.html)'
- en: '[5 Simple Steps Series: Master Python, SQL, Scikit-learn, PyTorch &…](https://www.kdnuggets.com/5-simple-steps-series-master-python-sql-scikit-learn-pytorch-google-cloud)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5 个简单步骤系列：掌握 Python、SQL、Scikit-learn、PyTorch 和…](https://www.kdnuggets.com/5-simple-steps-series-master-python-sql-scikit-learn-pytorch-google-cloud)'
- en: '[Winning The Room: Creating and Delivering an Effective Data-Driven…](https://www.kdnuggets.com/2022/04/franks-winning-room-creating-delivering-effective-data-driven-presentation.html)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[赢得房间：创建和呈现有效的数据驱动…](https://www.kdnuggets.com/2022/04/franks-winning-room-creating-delivering-effective-data-driven-presentation.html)'
- en: '[Best Practices for Creating Domain-Specific AI Models](https://www.kdnuggets.com/2022/07/best-practices-creating-domainspecific-ai-models.html)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[创建特定领域 AI 模型的最佳实践](https://www.kdnuggets.com/2022/07/best-practices-creating-domainspecific-ai-models.html)'
