- en: 'Comparing Clustering Techniques: A Concise Technical Overview'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较聚类技术：简明技术概述
- en: 原文：[https://www.kdnuggets.com/2016/09/comparing-clustering-techniques-concise-technical-overview.html](https://www.kdnuggets.com/2016/09/comparing-clustering-techniques-concise-technical-overview.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/09/comparing-clustering-techniques-concise-technical-overview.html](https://www.kdnuggets.com/2016/09/comparing-clustering-techniques-concise-technical-overview.html)
- en: Clustering is used for analyzing data which does not include pre-labeled classes.
    Data instances are grouped together using the concept of maximizing intraclass
    similarity and minimizing the similarity between differing classes. This translates
    to the clustering algorithm identifying and grouping instances which are very
    similar, as opposed to ungrouped instances which are much less-similar to one
    another. As clustering does not require the pre-labeling of classes, it is a form
    of unsupervised learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类用于分析未标记类别的数据。数据实例被分组在一起，使用最大化类内相似度和最小化不同类别之间相似度的概念。这意味着聚类算法识别并分组非常相似的实例，而不是那些彼此相似度较低的未分组实例。由于聚类不需要预先标记类别，它是一种无监督学习。
- en: '![Clustering](../Images/42a053780bfea2e58ce12ff265cb28cb.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![Clustering](../Images/42a053780bfea2e58ce12ff265cb28cb.png)'
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*k*-means Clustering is perhaps the most well-known example of a clustering
    algorithm. However, it is not the only one. Completely different schemes exist,
    such hierarchical clustering, fuzzy clustering, and density clustering, as do
    different takes on centroid-style clustering, such as using the median (as opposed
    to the mean), or ensuring that the centroid is a cluster member (read on for some
    context).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* k *-均值聚类可能是最著名的聚类算法示例。然而，它并不是唯一的。完全不同的方案存在，如层次聚类、模糊聚类和密度聚类，还有不同的质心风格聚类方法，例如使用中位数（而不是均值），或确保质心是簇成员（请继续阅读以了解更多背景）。'
- en: Below is a brief technical overview of the *k*-means clustering algorithm, as
    well as Expectation-Maximization (EM), a Gaussian distribution-based clustering
    method. Both approaches attempt to maximize intraclass similarity and minimize
    inter-class similarity, yet take different approaches in doing so.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是* k *-均值聚类算法的简要技术概述，以及基于高斯分布的期望最大化（EM）聚类方法。这两种方法都试图最大化类内相似度和最小化类间相似度，但采用了不同的方法来实现。
- en: '***k*-means Clustering**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '***k*-均值聚类**'
- en: '*k*-means is a simple, yet often effective, approach to clustering. *k* points
    are randomly chosen as cluster centers, or centroids, and all training instances
    are plotted and added to the closest cluster. After all instances have been added
    to clusters, the centroids, representing the mean of the instances of each cluster
    are re-calculated, with these re-calculated centroids becoming the new centers
    of their respective clusters.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '* k *-均值是一种简单但通常有效的聚类方法。* k *个点被随机选择作为簇中心或质心，所有训练实例被绘制并添加到最近的簇中。在所有实例被添加到簇中后，代表每个簇实例均值的质心会重新计算，这些重新计算的质心将成为各自簇的新中心。'
- en: At this point, all cluster membership is reset, and all instances of the training
    set are re-plotted and -added to their closest, possibly re-centered, cluster.
    This iterative process continues until there is no change to the centroids or
    their membership, and the clusters are considered settled.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，所有的簇成员资格被重置，所有训练集中的实例被重新绘制并-添加到其最近的、可能重新中心化的簇中。这个迭代过程会持续进行，直到质心或其成员资格没有变化，簇被认为是稳定的。
- en: '![k-means Algorithm](../Images/826561ae313b869f1d4d0b7041d2d089.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![k-means Algorithm](../Images/826561ae313b869f1d4d0b7041d2d089.png)'
- en: '**Fig.1: *k*-means Clustering Algorithm.**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1: *k*-均值聚类算法。**'
- en: 'Convergence is achieved once the re-calculated centroids match the previous
    iteration’s centroids, or are within some preset margin. The measure of distance
    is generally Euclidean in *k*-means, which, given 2 points in the form of *(x,
    y)*, can be represented as:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 收敛是在重新计算的中心点与上一次迭代的中心点匹配，或在某个预设的范围内时实现的。在*k*-均值算法中，距离度量通常是欧几里得的，对于形式为*(x, y)*的2个点，可以表示为：
- en: '![Equation](../Images/5496763be70b3ffcee728278cd3945e7.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![公式](../Images/5496763be70b3ffcee728278cd3945e7.png)'
- en: Of technical note, especially in the era of parallel computing, iterative clustering
    in *k*-means is serial in nature; however, the distance calculations within an
    iteration need not be. Therefore, for sets of a significant size, distance calculations
    are a worthy target for parallelization in the *k*-means clustering algorithm.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 技术上特别要注意的是，特别是在并行计算时代，*k*-均值中的迭代聚类本质上是串行的；然而，迭代中的距离计算不必是串行的。因此，对于大规模数据集，距离计算是*k*-均值聚类算法中值得并行化的目标。
- en: '**Expectation-Maximization**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**期望最大化**'
- en: Probabilistic clustering aims to determine the most likely set of clusters,
    given a set of data. EM is a probabilistic clustering algorithm, and, as such,
    involves determining the probabilities that instances belong to particular clusters.
    EM ”approaches maximum likelihood or maximum a posteriori estimates of parameters
    in statistical models” ([Han, Kamber & Pei](http://hanj.cs.illinois.edu/bk3/)).
    The EM process begins with a set of parameters, iterating until clustering is
    maximized, with respect to k clusters.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 概率聚类旨在在给定数据集的情况下确定最可能的聚类集。EM 是一种概率聚类算法，因此涉及确定实例属于特定聚类的概率。EM “接近统计模型中参数的最大似然或最大后验估计”（[Han,
    Kamber & Pei](http://hanj.cs.illinois.edu/bk3/)）。EM 过程以一组参数开始，迭代直到在 k 个聚类的情况下最大化聚类。
- en: 'As its name suggests, EM consists of 2 distinct steps: expectation and maximization.
    The expectation step (E-step) assigns particular objects to clusters based on
    parameters. This can also be referred to as the cluster probability calculation
    step, the cluster probabilities being the ”expected” class values. The maximization
    step (M-step) calculates the distribution parameters, maximizing expected likelihood.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，EM 包括两个不同的步骤：期望和最大化。期望步骤（E 步骤）根据参数将特定对象分配到聚类中。这也可以称为聚类概率计算步骤，聚类概率是“期望的”类别值。最大化步骤（M
    步骤）计算分布参数，最大化期望的似然。
- en: The parameter estimation equations express the fact that cluster probabilities
    are known for each cluster, as opposed to the clusters themselves. The mean for
    a cluster is calculated as
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 参数估计方程表达了每个聚类的概率是已知的，而不是聚类本身。聚类的均值计算为
- en: '![Equation](../Images/e5eedf278af22418280caf14e82c84ee.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![公式](../Images/e5eedf278af22418280caf14e82c84ee.png)'
- en: and the standard deviation of a cluster is determined by
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的标准差由以下公式确定
- en: '![Equation](../Images/c95f31494efd75e7934b9cde5f7e0af3.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![公式](../Images/c95f31494efd75e7934b9cde5f7e0af3.png)'
- en: where *w[i]* is the probability an instance *i* is a member of cluster *C*,
    and *x[i]* are all of the dataset’s instances.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *w[i]* 是实例 *i* 属于聚类 *C* 的概率，而 *x[i]* 是数据集中的所有实例。
- en: When given a new instance to cluster, its cluster membership probability is
    calculated and compared to each cluster. The instance becomes a member of the
    cluster with the highest membership probability. These steps are repeated until
    the inter-cluster delta is below a predefined threshold. In practice, iteration
    should continue until the log-likelihood increase is negligible, and the log-likelihood
    typically increases dramatically during the first number of iterations and converges
    to this negligible point quite quickly.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当给定一个新的实例进行聚类时，其聚类成员概率被计算并与每个聚类进行比较。该实例成为具有最高成员概率的聚类的成员。这些步骤会重复进行，直到聚类间的变化低于预定的阈值。实际上，迭代应继续进行，直到对数似然增加微不足道，并且对数似然在前几次迭代期间通常会急剧增加，并迅速收敛到这个微不足道的点。
- en: It should be noted that EM can also be used for fuzzy clustering, as opposed
    to probabilistic clustering.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，EM 也可以用于模糊聚类，而不是概率聚类。
- en: While it is clear that *k*-means and Expectation-maximization take different
    approaches to getting there, they are both clustering techniques. This difference
    should be a good hint to the variance which exists between the wide array of clustering
    techniques which are available [and in use today](/2016/09/poll-algorithms-used-data-scientists.html).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然* k *均值和期望最大化采用了不同的方法，但它们都是聚类技术。这种差异应该是对广泛存在的各种聚类技术之间差异的良好提示，[这些技术在今天仍在使用](/2016/09/poll-algorithms-used-data-scientists.html)。
- en: '**Related**:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关**：'
- en: '[MDL Clustering: Unsupervised Attribute Ranking, Discretization, and Clustering](/2016/08/mdl-clustering-unsupervised-attribute-ranking-discretization-clustering.html)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MDL聚类：无监督属性排序、离散化和聚类](/2016/08/mdl-clustering-unsupervised-attribute-ranking-discretization-clustering.html)'
- en: '[A Tutorial on the Expectation Maximization (EM) Algorithm](/2016/08/tutorial-expectation-maximization-algorithm.html)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[期望最大化（EM）算法教程](/2016/08/tutorial-expectation-maximization-algorithm.html)'
- en: '[Support Vector Machines: A Concise Technical Overview](/2016/09/support-vector-machines-concise-technical-overview.html)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[支持向量机：简明技术概述](/2016/09/support-vector-machines-concise-technical-overview.html)'
- en: More On This Topic
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Clustering Unleashed: Understanding K-Means Clustering](https://www.kdnuggets.com/2023/07/clustering-unleashed-understanding-kmeans-clustering.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[解锁聚类：理解K均值聚类](https://www.kdnuggets.com/2023/07/clustering-unleashed-understanding-kmeans-clustering.html)'
- en: '[Comparing Natural Language Processing Techniques: RNNs, Transformers, BERT](https://www.kdnuggets.com/comparing-natural-language-processing-techniques-rnns-transformers-bert)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[比较自然语言处理技术：RNN、Transformers、BERT](https://www.kdnuggets.com/comparing-natural-language-processing-techniques-rnns-transformers-bert)'
- en: '[How to Write Engaging Technical Blogs](https://www.kdnuggets.com/2022/04/write-engaging-technical-blogs.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何撰写吸引人的技术博客](https://www.kdnuggets.com/2022/04/write-engaging-technical-blogs.html)'
- en: '[With Data Privacy learn to implement technical privacy solutions…](https://www.kdnuggets.com/2022/04/manning-data-privacy-learn-implement-technical-privacy-solutions-tools-scale.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在数据隐私中学习实施技术隐私解决方案…](https://www.kdnuggets.com/2022/04/manning-data-privacy-learn-implement-technical-privacy-solutions-tools-scale.html)'
- en: '[Hidden Technical Debts Every AI Practitioner Should be Aware of](https://www.kdnuggets.com/2022/07/hidden-technical-debts-every-ai-practitioner-aware.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每位AI从业者应了解的隐性技术债务](https://www.kdnuggets.com/2022/07/hidden-technical-debts-every-ai-practitioner-aware.html)'
- en: '[ChatGPT vs Google Bard: A Comparison of the Technical Differences](https://www.kdnuggets.com/2023/03/chatgpt-google-bard-comparison-technical-differences.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatGPT与Google Bard：技术差异比较](https://www.kdnuggets.com/2023/03/chatgpt-google-bard-comparison-technical-differences.html)'
