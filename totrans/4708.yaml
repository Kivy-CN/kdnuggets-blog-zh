- en: 'Neural Networks with Numpy for Absolute Beginners: Introduction'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Numpy 进行绝对初学者的神经网络：简介
- en: 原文：[https://www.kdnuggets.com/2019/03/neural-networks-numpy-absolute-beginners-introduction.html](https://www.kdnuggets.com/2019/03/neural-networks-numpy-absolute-beginners-introduction.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/03/neural-networks-numpy-absolute-beginners-introduction.html](https://www.kdnuggets.com/2019/03/neural-networks-numpy-absolute-beginners-introduction.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Suraj Donthi](https://www.surajdonthi.com/), Computer Vision Consultant
    & Course Instructor at DataCamp**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由[Suraj Donthi](https://www.surajdonthi.com/)，计算机视觉顾问及DataCamp课程讲师**'
- en: Artificial Intelligence has become one of the hottest fields in the current
    day and most of us willing to dive into this field start off with Neural Networks!!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能已成为当今最热门的领域之一，我们大多数希望进入这一领域的人都从神经网络开始!!
- en: But on confronting the math intensive concepts of Neural Networks we just end
    up learning a few frameworks like Tensorflow, Pytorch etc., for implementing Deep
    Learning Models.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，当面对神经网络中的数学密集概念时，我们最终只会学习一些框架，如 Tensorflow、Pytorch 等，用于实现深度学习模型。
- en: Moreover, just learning these frameworks and not understanding the underlying
    concepts is like playing with a black box.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，仅仅学习这些框架而不理解其基础概念就像在玩一个黑箱。
- en: Whether you want to work in the industry or academia, you will be working, tweaking
    and playing with the models for which you need to have a clear understanding.
    Both the industry and the academia expect you to have full clarity of these concepts
    including the math.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是想在行业还是学术界工作，你都将涉及到模型的工作、调整和使用，这要求你对这些概念有清晰的理解。行业和学术界都期望你对这些概念，包括数学，有充分的了解。
- en: In this series of tutorials, I’ll make it extremely simple to understand Neural
    Networks by providing step by step explanation. Also, the math you’ll need will
    be the level of high school.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一系列教程中，我将通过逐步讲解让神经网络变得非常简单易懂。此外，你需要的数学知识将是高中水平。
- en: Let us start with the inception of artificial neural networks and gain some
    inspiration as to how it evolved.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从人工神经网络的起源开始，并获得一些关于其演变的灵感。
- en: A little bit into the history of how Neural Networks evolved
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于神经网络如何发展的简要历史
- en: It must be noted that most of the Algorithms for Neural Networks that were developed
    during the period 1950–2000 and now existing, are highly inspired by the working
    of our brain, the neurons, their structure and how they learn and transfer data.
    The most popular works include the Perceptron(1958) and the Neocognitron(1980).
    These papers were extremely instrumental in unwiring the brain code. They try
    to mathematically formulate a model of the neural networks in our brain.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 必须指出的是，大多数在1950–2000年间开发的神经网络算法，及其现存算法，都高度受到我们大脑、神经元的工作方式、其结构以及它们如何学习和传递数据的启发。最著名的作品包括感知机（1958年）和新认知网络（1980年）。这些论文在解开大脑代码方面极为重要。它们试图数学化地构建我们大脑中的神经网络模型。
- en: And everything changed after the God Father of AI Geoffrey Hinton formulated
    the back-propagation algorithm in 1986(That’s right! what you are learning is
    more than 30 years old!).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都在1986年由人工智能教父 Geoffrey Hinton 公式化反向传播算法之后发生了变化（没错，你学习的内容已有30多年历史！）。
- en: A biological Neuron
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个生物神经元
- en: '![figure-name](../Images/e3a2dbe5d4d4c8051fca72a379720e58.png)The figure above
    shows a biological neuron. It has *dendrites* that receive information from neurons.
    The received information is passed on to the *cell body or the nucleus* of the
    neuron. The *nucleus* is where the information is processed and passed on to the
    next layer of neurons through *axons*.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '![figure-name](../Images/e3a2dbe5d4d4c8051fca72a379720e58.png) 上图展示了一个生物神经元。它有*树突*接收来自其他神经元的信息。接收到的信息会传递到神经元的*细胞体或细胞核*。*细胞核*是处理信息的地方，并通过*轴突*将信息传递给下一个神经元层。'
- en: Our brain consists of about 100 billion such neurons which communicate through
    electrochemical signals. Each neuron is connected to 100s and 1000s of other neurons
    which constantly transmit and receive signals.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大脑包含大约1000亿个这样的神经元，它们通过电化学信号进行通信。每个神经元都连接到数百甚至数千个其他神经元，这些神经元不断地传递和接收信号。
- en: But how can our brain process so much information just by sending electrochemical
    signals? How can the neurons understand which signal is important and which isn't?
    How do the neurons know what information to pass forward?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们的脑袋是如何仅通过发送电化学信号来处理如此多的信息的？神经元如何理解哪个信号是重要的，哪个不是？神经元如何知道要传递什么信息？
- en: The electrochemical signals consist of strong and weak signals. The strong signals
    are the ones to dominate which information is important. So only the strong signal
    or a combination of them pass through the nuclues (the CPU of neurons) and are
    transmitted to the next set of neurons through the axons.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 电化学信号由强信号和弱信号组成。强信号决定了哪些信息是重要的。因此，只有强信号或它们的组合通过核（神经元的CPU）传递，并通过轴突传输到下一组神经元。
- en: But how are some signals strong and some signals week?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么有些信号强而有些信号弱呢？
- en: Well, through millions of years of evolution, the neurons have become sensitive
    to certain kinds of signals. When the neuron encounters a specific pattern, they
    get triggered(activated) and as a consequence send strong signals to other neurons
    and hence the information is transmitted.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通过数百万年的进化，神经元已经变得对某些类型的信号敏感。当神经元遇到特定模式时，它们会被触发（激活），从而向其他神经元发送强信号，因此信息得以传递。
- en: Most of us also know that different regions of our brain are activated (or receptive)
    for different actions like seeing, hearing, creative thinking and so on. This
    is because the neurons belonging to a specific region in the brain are trained
    to process a certain kind of information better and hence get activated only when
    certain kinds of information is being sent. The figure below gives us a better
    understanding of the different receptive regions of the brain.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们大多数人也知道，大脑的不同区域在执行诸如视觉、听觉、创造性思维等不同动作时会被激活（或接收）。这是因为大脑中特定区域的神经元被训练以更好地处理某种信息，因此只有在特定信息传递时才会被激活。下图帮助我们更好地理解大脑的不同接收区域。
- en: '![alternate text](../Images/35b55fb1a389baa89cde9023eb63fc32.png)If that is
    so… can the neurons be made sensitive to a different pattern(i.e., if they have
    truly become sensitive based on some patterns)?'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![alternate text](../Images/35b55fb1a389baa89cde9023eb63fc32.png)如果是这样的话… 神经元能否对不同的模式变得敏感（即，如果它们真的基于某些模式变得敏感）？'
- en: It has been shown through Neuroplasticity that the different regions of the
    brain can be rewired to perform totally different tasks. Such as the neurons responsible
    for touch sensing can be rewired to become sensitive to smell. Check out this
    great TEDx video below to know more about neuroplasticity.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通过神经可塑性研究已显示，大脑的不同区域可以被重新连接以执行完全不同的任务。例如，负责触觉的神经元可以被重新连接以对嗅觉变得敏感。查看下面这个精彩的TEDx视频，以了解更多关于神经可塑性的内容。
- en: TEDx video on Neuroplasticity [[Source]](https://www.youtube.com/watch?v=xzbHtIrb14s)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: TEDx 视频关于神经可塑性 [[Source]](https://www.youtube.com/watch?v=xzbHtIrb14s)
- en: But what is the mechanism by which the neurons become sensitive?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，神经元如何变得敏感呢？
- en: Unfortunately, neuroscientists are still trying to figure that out!!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，神经科学家们仍在努力弄清楚这一点！！
- en: But fortunately enough, god father of AI Geff has saved the day by inventing
    back propagation which accomplishes the same task for our Artificial Neurons,
    i.e., sensitizing them to certain patterns.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 但幸运的是，人工智能的奠基人Geff发明了反向传播，完成了对我们人工神经元的相同任务，即使它们对某些模式变得敏感。
- en: In the next section, we’ll explore the working of a perceptron and also gain
    a mathematical intuition.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分中，我们将探讨感知机的工作原理，并获得数学直觉。
- en: Perceptron/Artificial Neuron
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 感知机/人工神经元
- en: '![](../Images/bbd835297009a0c24b5acc73e39a6d00.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bbd835297009a0c24b5acc73e39a6d00.png)'
- en: From the figure, you can observe that the perceptron is a reflection of the
    biological neuron. The inputs combined with the weights(*wᵢ*) are analogous to
    dendrites. These values are summed and passed through an activation function (like
    the thresholding function as shown in fig.). This is analogous to the nucleus.
    Finally, the activated value is transmitted to the next neuron/perceptron which
    is analogous to the axons.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以观察到，感知机是生物神经元的反映。与权重(*wᵢ*)结合的输入类似于树突。这些值被求和并通过激活函数（如图中所示的阈值函数）传递。这类似于核。最后，激活值传递到下一个神经元/感知机，这类似于轴突。
- en: The latent weights(*wᵢ*) multiplied with each input(*xᵢ*) depicts the significance(strength)
    of the respective input signal. Hence, larger the value of a weight, more important
    is the feature.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在的权重（*wᵢ*）与每个输入（*xᵢ*）相乘，表示各自输入信号的重要性（强度）。因此，权重值越大，特征就越重要。
- en: You can infer from this architecture that the weights are what is learned in
    a perceptron so as to arrive at the required result. An additional bias(*b*, here
    *w₀*) is also learned.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个架构中你可以推断出，权重是感知器中学习到的内容，以便得到所需的结果。一个额外的偏置（*b*，这里是 *w₀*）也是学习到的。
- en: 'Hence, when there are multiple inputs (say *n*), the equation can be generalized
    as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当输入有多个（假设为 *n*）时，方程可以推广为：
- en: '![Equation](../Images/379910f15ba571c347905c8b1b3342ec.png)![Equation](../Images/68bac74c2701bd3d5270a73e2d19398f.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![Equation](../Images/379910f15ba571c347905c8b1b3342ec.png)![Equation](../Images/68bac74c2701bd3d5270a73e2d19398f.png)'
- en: Finally, the output of summation (assume as *z*) is fed to the *thresholding
    activation function*, where the function outputs
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，求和的输出（假设为 *z*）被送入 *阈值激活函数*，该函数输出
- en: An Example
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个例子
- en: '![logic-gates](../Images/72bff0e49b84cadc2e36e8077e41cc42.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![logic-gates](../Images/72bff0e49b84cadc2e36e8077e41cc42.png)'
- en: Let us consider our perceptron to perform as *logic gates* to gain more intuition.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑将感知器作为*逻辑门*来获得更多直观的理解。
- en: 'Let’s choose an *AND Gate*. The Truth Table for the gate is shown below:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们选择一个 *AND Gate*。该门的真值表如下所示：
- en: The perceptron for the *AND Gate* can be formed as shown in the figure. It is
    clear that the perceptron has two inputs (here *x₁ = A* and *x₂ = B*).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*AND Gate* 的感知器可以如图所示。很明显，感知器有两个输入（这里是 *x₁ = A* 和 *x₂ = B*）。'
- en: '![threshold-function](../Images/ba979f442dfea6e1392af3988d3a6e18.png)![threshold-function-eq](../Images/5caacf2f8f14fbcac3a7ab4b807dc330.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![threshold-function](../Images/ba979f442dfea6e1392af3988d3a6e18.png)![threshold-function-eq](../Images/5caacf2f8f14fbcac3a7ab4b807dc330.png)'
- en: We can see that for inputs *x₁, x₂* and *x₀* = 1, setting their weights as
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，对于输入 *x₁, x₂* 和 *x₀* = 1，将它们的权重设置为
- en: '![Equation](../Images/87dbdc6f996cdfd46e54f905db58c6a1.png)![Equation](../Images/1cf563cb01c06d75daf731d1c46db107.png)![Equation](../Images/05dee933fdf09eee757a5d693178c757.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![Equation](../Images/87dbdc6f996cdfd46e54f905db58c6a1.png)![Equation](../Images/1cf563cb01c06d75daf731d1c46db107.png)![Equation](../Images/05dee933fdf09eee757a5d693178c757.png)'
- en: respectively and keeping the *Threshold function* as the activation function
    we can arrive at the *AND Gate*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 分别设置权重并保持 *阈值函数* 作为激活函数，我们可以得到 *AND Gate*。
- en: Now, let’s get our hands dirty and codify this and test it out!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们动手实践，将这些内容编码并进行测试吧！
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Similarly for *NOR Gate* the Truth Table is,
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，*NOR Gate* 的真值表是，
- en: '![nor-gate-truth-table](../Images/7a93c890f845f018f767a3c1036c0433.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![nor-gate-truth-table](../Images/7a93c890f845f018f767a3c1036c0433.png)'
- en: 'The perceptron for *NOR Gate* will be as below:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*NOR Gate* 的感知器将如下所示：'
- en: '![nor-gate=perceptron](../Images/f00b0279cec3976ab8f92b7359aee1e4.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![nor-gate=perceptron](../Images/f00b0279cec3976ab8f92b7359aee1e4.png)'
- en: You can set the weights as
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将权重设置为
- en: '![Equation](../Images/009579f99c16b1c77b7d43fafd76c5ce.png)![Equation](../Images/c14c9c9e6dab4255694305da0f1d7dce.png)![Equation](../Images/7665e03d35078c41cf61daae80a25b50.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![Equation](../Images/009579f99c16b1c77b7d43fafd76c5ce.png)![Equation](../Images/c14c9c9e6dab4255694305da0f1d7dce.png)![Equation](../Images/7665e03d35078c41cf61daae80a25b50.png)'
- en: so that you obtain a *NOR Gate.*
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以便你能得到一个 *NOR Gate*。
- en: 'You can go ahead and implement this in code as:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以继续将其实现为代码，如下所示：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: What you are actually calculating…
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实际上你在计算的内容是…
- en: If you analyse what you were trying to do in the above examples, you will realize
    that you were actually trying to adjust the values of the weights to obtain the
    required output.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你分析一下你在上述示例中所做的事情，你会发现你实际上是在调整权重值以获得所需的输出。
- en: Lets consider the *NOR Gate* example and break it down to very miniscule steps
    to gain more understanding.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑 *NOR Gate* 示例，并将其分解为非常细小的步骤以获得更好的理解。
- en: What you would usually do first is to simply set some values to the weights
    and observe the result, say
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通常你会首先设置一些权重值并观察结果，比如
- en: '![Equation](../Images/1664f5c911dd3834b15ae94a0f4adc01.png)![Equation](../Images/fdad0fc3d15c9f516e3eaecd32f765c5.png)![Equation](../Images/e7f0d60adce140c228e6f813e0c73a84.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![Equation](../Images/1664f5c911dd3834b15ae94a0f4adc01.png)![Equation](../Images/fdad0fc3d15c9f516e3eaecd32f765c5.png)![Equation](../Images/e7f0d60adce140c228e6f813e0c73a84.png)'
- en: 'Then the output will be as shown in below table:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后输出将如下面的表格所示：
- en: '![figure-name](../Images/6c9985311aad0f3841fd23651f371576.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![figure-name](../Images/6c9985311aad0f3841fd23651f371576.png)'
- en: So, how can you fix the values of weights so that you get the right output?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你如何调整权重值以获得正确的输出呢？
- en: By intuition, you can easily observe that *w₀* must be increased and *w₁* and
    *w₀* must be reduced or rather made negative so that you obtain the actual output.
    But if you breakdown this intuition, you will observe that you are actually finding
    the difference between the actual output and the predicted output and finally
    reflecting that on the weights…
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 根据直觉，你可以很容易地观察到，*w₀* 必须增加，而 *w₁* 和 *w₀* 必须减少或变为负值，以获得实际输出。但如果你深入分析这种直觉，你会发现你实际上是在找到实际输出和预测输出之间的差异，并最终将其反映到权重上……
- en: This is a very important concept that you will be digging deeper and will be
    the core to formulate the ideas behind *gradient descent* and also *backward propagation*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常重要的概念，你将深入挖掘，并将成为形成*梯度下降*和*反向传播*背后理念的核心。
- en: What did you learn?
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你学到了什么？
- en: Neurons must be made sensitive to a pattern in order to recognize it.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元必须对模式变得敏感才能识别它。
- en: So, similarly, in our perceptron/artificial neuron, **the weights are what is
    to be learnt**.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，同样地，在我们的感知器/人工神经元中，**需要学习的是权重**。
- en: In the later articles you’ll fully understand how the weights are trained to
    recognize patterns and also the different techniques that exist.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续文章中，你将充分理解权重是如何被训练以识别模式的，以及存在的不同技术。
- en: As you’ll see later, the neural networks are very similar to the structure of
    biological neural networks.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你稍后会看到的，神经网络与生物神经网络的结构非常相似。
- en: While it is true that we learnt only a few small concepts (although very crucial)
    in this first part of the article, they will serve as the strong foundation for
    implementing Neural Networks. Moreover, I’m keeping this article short and sweet
    so that too much is information is not dumped at once and will help absorb more!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在本文的第一部分我们只学习了几个小概念（尽管非常重要），但它们将成为实现神经网络的坚实基础。此外，我保持这篇文章简短而精炼，以免一次性提供过多信息，有助于更好地吸收！
- en: In the next tutorial, you will learn about **Linear Regression** (which can
    otherwise be called a perceptron with linear activation function) in detail and
    also implement them. The **Gradient Descent algorithm which helps learn the weights**
    are described and implemented in detail. Lastly, you’ll be able to **predict the
    outcome of an event** with the help of Linear Regression. So, head on to the next
    article to implement it!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个教程中，你将详细学习 **线性回归**（也可以称为具有线性激活函数的感知器），并进行实现。**帮助学习权重的梯度下降算法**将被详细描述和实现。最后，你将能够**利用线性回归预测事件的结果**。所以，前往下一篇文章进行实现吧！
- en: 'You can checkout the next part of the article here:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里查看文章的下一部分：
- en: '[**Neural Networks with Numpy for Absolute Beginners — Part 2: Linear Regression**](https://medium.com/@surajdonthi95/neural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[**使用 Numpy 的神经网络绝对初学者 — 第 2 部分：线性回归**](https://medium.com/@surajdonthi95/neural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a)'
- en: '**Bio**: [Suraj Donthi](https://www.surajdonthi.com/) is a Computer Vision
    Consultant, Author, Machine Learning and Deep Learning Trainer.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**简历**： [Suraj Donthi](https://www.surajdonthi.com/) 是一位计算机视觉顾问、作者、机器学习和深度学习培训师。'
- en: '[Original](https://towardsdatascience.com/neural-networks-with-numpy-for-absolute-beginners-introduction-c1394639edb2).
    Reposted with permission.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/neural-networks-with-numpy-for-absolute-beginners-introduction-c1394639edb2).
    经许可转载。'
- en: '**Related:**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Neural Networks – an Intuition](/2019/02/neural-networks-intuition.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[神经网络 – 直观理解](/2019/02/neural-networks-intuition.html)'
- en: '[The Backpropagation Algorithm Demystified](/2019/01/backpropagation-algorithm-demystified.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[反向传播算法揭秘](/2019/01/backpropagation-algorithm-demystified.html)'
- en: '[Mastering the Learning Rate to Speed Up Deep Learning](/2018/11/mastering-learning-rate-speed-up-deep-learning.html)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握学习率以加速深度学习](/2018/11/mastering-learning-rate-speed-up-deep-learning.html)'
- en: '* * *'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速入门网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的IT工作'
- en: '* * *'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[The Absolute Basics of MLOps](https://www.kdnuggets.com/2022/09/absolute-basics-mlops.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MLOps的基础知识](https://www.kdnuggets.com/2022/09/absolute-basics-mlops.html)'
- en: '[10 Simple Things to Try Before Neural Networks](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[神经网络之前可以尝试的10件简单事情](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[可解释的神经网络与PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
- en: '[Deep Neural Networks Don''t Lead Us Towards AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度神经网络不会引领我们走向AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
- en: '[Explainable Forecasting and Nowcasting with State-of-the-art Deep…](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[可解释的前瞻性和即时性预测的最前沿深度学习](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
- en: '[Image Classification with Convolutional Neural Networks (CNNs)](https://www.kdnuggets.com/2022/05/image-classification-convolutional-neural-networks-cnns.html)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[卷积神经网络（CNNs）进行图像分类](https://www.kdnuggets.com/2022/05/image-classification-convolutional-neural-networks-cnns.html)'
