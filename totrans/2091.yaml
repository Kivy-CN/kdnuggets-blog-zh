- en: How to Use the Hugging Face Tokenizers Library to Preprocess Text Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/how-to-use-the-hugging-face-tokenizers-library-to-preprocess-text-data](https://www.kdnuggets.com/how-to-use-the-hugging-face-tokenizers-library-to-preprocess-text-data)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![ Hugging Face Tokenizers Library](../Images/23c326950f8c1dfd30d486cd87509ab3.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: If you have studied NLP, you might have heard about the term "tokenization."
    It is an important step in text preprocessing, where we transform our textual
    data into something that machines can understand. It does so by breaking down
    the sentence into smaller chunks, known as tokens. These tokens can be words,
    subwords, or even characters, depending on the tokenization algorithm being used.
    In this article, we will see how to use the Hugging Face Tokenizers Library to
    preprocess our textual data.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up Hugging Face Tokenizers Library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To start using the Hugging Face Tokenizers library, you''ll need to install
    it first. You can do this using pip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The Hugging Face library supports various tokenization algorithms, but the
    three main types are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Byte-Pair Encoding (BPE):** Merges the most frequent pairs of characters
    or subwords iteratively, creating a compact vocabulary. It is used by models like
    GPT-2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**WordPiece:** Similar to BPE but focuses on probabilistic merges (doesn''t
    choose the pair that is the most frequent but the one that will maximize the likelihood
    of the corpus once merged), commonly used by models like BERT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SentencePiece:** A more flexible tokenizer that can handle different languages
    and scripts, often used with models like ALBERT, XLNet, or the Marian framework.
    It treats spaces as characters rather than word separators.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Hugging Face Transformers library provides an `AutoTokenizer` class that
    can automatically select the best tokenizer for a given pre-trained model. This
    is a convenient way to use the correct tokenizer for a specific model and can
    be imported from the `transformers` library. However, for the sake of our discussion
    regarding the Tokenizers library, we will not follow this approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the pre-trained `BERT-base-uncased` tokenizer. This tokenizer was
    trained on the same data and using the same techniques as the `BERT-base-uncased`
    model, which means it can be used to preprocess text data compatible with BERT
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Single Sentence Tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s encode a simple sentence using this tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To ensure correctness, let''s decode the tokenized input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this output, you can see two special tokens. `[CLS]` marks the start of the
    input sequence, and `[SEP]` marks the end, indicating a single sequence of text.
  prefs: []
  type: TYPE_NORMAL
- en: Batch Tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s tokenize a corpus of text instead of a single sentence using `batch_encode_plus`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For better understanding, let's decode the batch-encoded corpus as we did incase
    of single sentence. This will provide the original sentences, tokenized appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Padding and Truncation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When preparing data for machine learning models, ensuring all input sequences
    have the same length is often necessary. Two methods to accomplish this are:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Padding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Padding works by adding the special token `[PAD]` at the end of the shorter
    sequences to match the length of the longest sequence in the batch or max length
    supported by the model if `max_length` is defined. You can do this by:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can see that extra 0s are placed, but for better understanding, let''s
    decode to see where the tokenizer has placed the `[PAD]` tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Truncation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many NLP models have a maximum input length sequence, and truncation works by
    chopping off the end of the longer sequence to meet this maximum length. It reduces
    memory usage and prevents the model from being overwhelmed by very large input
    sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can also use the `batch_decode` method, but for better understanding,
    let''s print this information in a different way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This article is part of our amazing series on Hugging Face. If you want to
    explore more about this topic, here are some references to help you out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Hugging Face Tokenizer Documentation](https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer#tokenizer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hugging Face Transformers Course: Preprocessing Data](https://huggingface.co/docs/transformers/v4.14.1/en/preprocessing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[HuggingFace: Summary of Different Tokenizers](https://huggingface.co/docs/transformers/en/tokenizer_summary)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[](https://www.linkedin.com/in/kanwal-mehreen1/)**[Kanwal Mehreen](https://www.linkedin.com/in/kanwal-mehreen1/)****
    Kanwal is a machine learning engineer and a technical writer with a profound passion
    for data science and the intersection of AI with medicine. She co-authored the
    ebook "Maximizing Productivity with ChatGPT". As a Google Generation Scholar 2022
    for APAC, she champions diversity and academic excellence. She''s also recognized
    as a Teradata Diversity in Tech Scholar, Mitacs Globalink Research Scholar, and
    Harvard WeCode Scholar. Kanwal is an ardent advocate for change, having founded
    FEMCodes to empower women in STEM fields.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How to Use Hugging Face’s Datasets Library for Efficient Data Loading](https://www.kdnuggets.com/how-to-use-hugging-faces-datasets-library-for-efficient-data-loading)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using Hugging Face Transformers for Emotion Detection in Text](https://www.kdnuggets.com/using-hugging-face-transformers-for-emotion-detection-in-text)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Use Hugging Face AutoTrain to Fine-tune LLMs](https://www.kdnuggets.com/how-to-use-hugging-face-autotrain-to-finetune-llms)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Use GPT for Generating Creative Content with Hugging Face…](https://www.kdnuggets.com/how-to-use-gpt-for-generating-creative-content-with-hugging-face-transformers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A community developing a Hugging Face for customer data modeling](https://www.kdnuggets.com/2022/08/objectiv-community-developing-hugging-face-customer-data-modeling.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top 10 Machine Learning Demos: Hugging Face Spaces Edition](https://www.kdnuggets.com/2022/05/top-10-machine-learning-demos-hugging-face-spaces-edition.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
