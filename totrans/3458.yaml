- en: A Visual Explanation of the Back Propagation Algorithm for Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2016/06/visual-explanation-backpropagation-algorithm-neural-networks.html](https://www.kdnuggets.com/2016/06/visual-explanation-backpropagation-algorithm-neural-networks.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let's assume we are really into mountain climbing, and to add a little extra
    challenge, we cover eyes this time so that we can't see where we are and when
    we accomplished our "objective," that is, reaching the top of the mountain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we can''t see the path upfront, we let our intuition guide us: assuming
    that the mountain top is the "highest" point of the mountain, we think that the
    steepest path leads us to the top most efficiently.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We approach this challenge by iteratively "feeling" around you and taking a
    step into the direction of the steepest ascent -- let''s call it "gradient ascent."
    But what do we do if we reach a point where we can''t ascent any further? I.e.,
    each direction leads downwards? At this point, we may have already reached the
    mountain''s top, but we could just have reached a smaller plateau ... we don''t
    know. Essentially, this is just an analogy of gradient ascent optimization (basically
    the counterpart of minimizing a cost function via gradient descent). However,
    this is not specific to backpropagation but just one way to minimize a convex
    cost function (if there is only a global minima) or non-convex cost function (which
    has local minima like the "plateaus" that let us think we reached the mountain''s
    top). Using a little visual aid, we could picture a non-convex cost function with
    only one parameter (where the blue ball is our current location) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![Nonconvex cost](../Images/b7cb1df71cfdd2e82bb55a3beeeb8c30.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation/nonconvex-cost.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, backpropagation is just back-propagating the cost over multiple "levels"
    (or layers). E.g., if we have a multi-layer perceptron, we can picture forward
    propagation (passing the input signal through a network while multiplying it by
    the respective weights to compute an output) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![Forward propagation](../Images/9c05363841f1cd37e9b476c669cb53fe.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation/forward-propagation.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'And in backpropagation, we "simply" backpropagate the error (the "cost" that
    we compute by comparing the calculated output and the known, correct target output,
    which we then use to update the model parameters):'
  prefs: []
  type: TYPE_NORMAL
- en: '[![Backpropagation](../Images/026031521802271f37a0fdb30bc165f3.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation/backpropagation.png)'
  prefs: []
  type: TYPE_NORMAL
- en: It may be some time ago since pre-calc, but it's essentially all based on the
    simple chain-rule that we use for nested functions
  prefs: []
  type: TYPE_NORMAL
- en: '[![Chain rule](../Images/e05c3db54124e253d0e75c7cff434527.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation/chain_rule_1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[![Chain rule](../Images/f9c99e2855716c634e5701100e8ad8f9.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation/chain_rule_2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of doing this "manually" we can use computational tools (called "automatic
    differentiation"), and backpropagation is basically the "reverse" mode of this
    auto-differentiation. Why reverse and not forward? Because it is computationally
    cheaper! If we'd do it forward-wise, we'd successively multiply large matrices
    for each layer until we multiply a large matrix by a vector in the output layer.
    However, if we start backwards, that is, we start by multiplying a matrix by a
    vector, we get another vector, and so forth. So, I'd say the beauty in backpropagation
    is that we are doing more efficient matrix-vector multiplications instead of matrix-matrix
    multiplications.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Sebastian Raschka](https://twitter.com/rasbt)** is a ''Data Scientist''
    and Machine Learning enthusiast with a big passion for Python & open source. Author
    of ''[Python Machine Learning](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning)''.
    Michigan State University.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation.md).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[When Does Deep Learning Work Better Than SVMs or Random Forests?](/2016/04/deep-learning-vs-svm-random-forest.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Development of Classification as a Learning Machine](/2016/04/development-classification-learning-machine.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Why Implement Machine Learning Algorithms From Scratch?](/2016/05/implement-machine-learning-algorithms-scratch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Linear vs Logistic Regression: A Succinct Explanation](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News 22:n12, March 23: Best Data Science Books for…](https://www.kdnuggets.com/2022/n12.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[An Intuitive Explanation of Collaborative Filtering](https://www.kdnuggets.com/2022/09/intuitive-explanation-collaborative-filtering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Thought Propagation: An Analogical Approach to Complex Reasoning…](https://www.kdnuggets.com/thought-propagation-an-analogical-approach-to-complex-reasoning-with-large-language-models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10 Simple Things to Try Before Neural Networks](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
