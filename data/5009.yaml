- en: How to Rank 10% in Your First Kaggle Competition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在你的第一个Kaggle竞赛中排名前10%
- en: 原文：[https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/4](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/4)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/4](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/4)
- en: Home Depot Search Relevance
  id: totrans-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Home Depot 搜索相关性
- en: In this section I will share my solution in [Home Depot Search Relevance Competition](https://www.kaggle.com/c/home-depot-product-search-relevance) and
    what I learned from top teams after the competition.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我将分享我在[Home Depot 搜索相关性竞赛](https://www.kaggle.com/c/home-depot-product-search-relevance)中的解决方案，以及我从竞赛后顶尖团队中学到的东西。
- en: The task in this competition is to predict how relevant a result is for a search
    term on Home Depot website. The relevance is an average score from three human
    evaluators and ranges between 1 ~ 3\. Therefore it’s a regression task. The datasets
    contains search terms, product titles / descriptions and some attributes like
    brand, size and color. The metric is[RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本次竞赛的任务是预测搜索词在Home Depot网站上的结果相关性。相关性是三名人工评估者的平均分，范围在1~3之间。因此这是一个回归任务。数据集包含搜索词、产品标题/描述以及品牌、尺寸和颜色等属性。评估指标是[RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation)。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行IT管理'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: This is much like [Crowdflower Search Results Relevance](https://www.kaggle.com/c/crowdflower-search-relevance).
    The difference is that [Quadratic Weighted Kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa#Weighted_kappa) is
    used in Crowdflower competition and therefore complicated the final cutoff of
    regression scores. Also there were no attributes provided in Crowdflower.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这很像[Crowdflower 搜索结果相关性](https://www.kaggle.com/c/crowdflower-search-relevance)。不同之处在于[Cohen's
    Kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa#Weighted_kappa)在Crowdflower竞赛中被使用，这使得回归分数的最终截止点更加复杂。此外，Crowdflower没有提供属性。
- en: '**EDA**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**EDA**'
- en: 'There were several quite good EDAs by the time I joined the competition, especially [this
    one](https://www.kaggle.com/briantc/home-depot-product-search-relevance/homedepot-first-dataexploreation-k).
    I learned that:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我参加竞赛时，有几个相当好的EDA，特别是[这个](https://www.kaggle.com/briantc/home-depot-product-search-relevance/homedepot-first-dataexploreation-k)。我了解到：
- en: Many search terms / products appeared several times.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多搜索词/产品出现了多次。
- en: Text similarities are great features.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本相似性是很好的特征。
- en: Many products don’t have attributes features. Would this be a problem?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多产品没有属性特征。这会是一个问题吗？
- en: Product ID seems to have strong predictive power. However the overlap of product
    ID between the training set and the testing set is not very high. Would this contribute
    to overfitting?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品ID似乎具有很强的预测能力。然而，训练集和测试集之间的产品ID重叠度不是很高。这会导致过拟合吗？
- en: '**Preprocessing**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**预处理**'
- en: 'You can find how I did preprocessing and feature engineering [on GitHub](https://github.com/dnc1994/Kaggle-Playground/blob/master/home-depot/Preprocess.ipynb).
    I’ll only give a brief summary here:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[GitHub](https://github.com/dnc1994/Kaggle-Playground/blob/master/home-depot/Preprocess.ipynb)上找到我如何进行预处理和特征工程的详细信息。我这里只给出一个简要总结：
- en: Use [typo dictionary](https://www.kaggle.com/steubk/home-depot-product-search-relevance/fixing-typos) posted
    in the forum to correct typos in search terms.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用[错别字词典](https://www.kaggle.com/steubk/home-depot-product-search-relevance/fixing-typos)来修正搜索词中的错别字。
- en: Count attributes. Find those frequent and easily exploited ones.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 统计属性。找出那些频繁出现且容易被利用的属性。
- en: Join the training set with the testing set. This is important because otherwise
    you’ll have to do feature transformation twice.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练集与测试集合并。这很重要，否则你将不得不进行两次特征变换。
- en: Do **[stemming](https://en.wikipedia.org/wiki/Stemming)** and **[tokenizing](https://en.wikipedia.org/wiki/Text_segmentation#Word_segmentation)** for
    all the text fields. Some **normalization** (with digits and units) and **synonym
    substitutions** are performed manually.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对所有文本字段进行**[词干提取](https://en.wikipedia.org/wiki/Stemming)**和**[分词](https://en.wikipedia.org/wiki/Text_segmentation#Word_segmentation)**。一些**归一化**（包括数字和单位）和**同义词替换**是手动执行的。
- en: '**Feature**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征**'
- en: '*Attribute Features'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*属性特征'
- en: Whether the product contains a certain attribute (brand, size, color, weight,
    indoor/outdoor, energy star certified …)
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品是否包含某个属性（品牌、尺寸、颜色、重量、室内/室外、能源之星认证等）
- en: Whether a certain attribute matches with the search term
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 某个属性是否与搜索词匹配
- en: Meta Features
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元特征
- en: Length of each text field
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个文本字段的长度
- en: Whether the product contains attribute fields
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品是否包含属性字段
- en: Brand (encoded as integers)
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 品牌（编码为整数）
- en: Product ID
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品ID
- en: Matching
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 匹配
- en: Whether search term appears in product title / description / attributes
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索词是否出现在产品标题/描述/属性中
- en: Count and ratio of search term’s appearance in product title / description /
    attributes
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索词在产品标题/描述/属性中的出现次数和比例
- en: '*Whether the i-th word of search term appears in product title / description
    / attributes'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*搜索词的第 i 个单词是否出现在产品标题/描述/属性中'
- en: Text similarities between search term and product title/description/attributes
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索词与产品标题/描述/属性之间的文本相似度
- en: '[BOW](https://en.wikipedia.org/wiki/Bag-of-words_model) [Cosine Similairty](https://en.wikipedia.org/wiki/Cosine_similarity)'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BOW](https://en.wikipedia.org/wiki/Bag-of-words_model) [余弦相似度](https://en.wikipedia.org/wiki/Cosine_similarity)'
- en: '[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) Cosine Similarity'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) 余弦相似度'
- en: '[Jaccard Similarity](https://en.wikipedia.org/wiki/Jaccard_index)'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Jaccard 相似度](https://en.wikipedia.org/wiki/Jaccard_index)'
- en: '*[Edit Distance](https://en.wikipedia.org/wiki/Edit_distance)'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*[编辑距离](https://en.wikipedia.org/wiki/Edit_distance)'
- en: '[Word2Vec](https://en.wikipedia.org/wiki/Word2vec) Distance (I didn’t include
    this because of its poor performance and slow calculation. Yet it seems that I
    was using it wrong.)'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Word2Vec](https://en.wikipedia.org/wiki/Word2vec) 距离（我没有包括这个，因为它的表现差且计算缓慢。不过，似乎我使用得不对。）'
- en: '**[Latent Semantic Indexing](https://en.wikipedia.org/wiki/Latent_semantic_indexing):
    By performing [SVD decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) to
    the matrix obtained from BOW/TF-IDF Vectorization, we get a latent representation
    of different search term / product groups. This enables our model to distinguish
    between groups and assign different weights to features, therefore solving the
    issue of dependent data and products lacking some features (to an extent).**'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[潜在语义分析](https://en.wikipedia.org/wiki/Latent_semantic_indexing)：通过对从BOW/TF-IDF向量化获得的矩阵进行[SVD分解](https://en.wikipedia.org/wiki/Singular_value_decomposition)，我们获得了不同搜索词/产品组的潜在表示。这使我们的模型能够区分不同组并为特征分配不同的权重，从而在一定程度上解决了数据依赖和产品缺乏某些特征的问题。**'
- en: Note that features listed above with `*` are the last batch of features I added.
    The problem is that the model trained on data that included these features performed
    worse than the previous ones. At first I thought that the increase in number of
    features would require re-tuning of model parameters. However, after wasting much
    CPU time on grid search, I still could not beat the old model. I think it might
    be the issue of **feature correlation**mentioned above. I actually knew a solution
    that might work, which is to **combine models trained on different version of
    features by stacking**. Unfortunately I didn’t have enough time to try it. **As
    a matter of fact, most of top teams regard the ensemble of models trained with
    different preprocessing and feature engineering pipelines as a key to success**.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上面列出的带`*`的特征是我最后添加的一批特征。问题是，训练了这些特征的数据模型的表现比之前的模型差。起初我认为特征数量的增加会要求重新调整模型参数。然而，在浪费了大量CPU时间进行网格搜索后，我仍然无法超越旧模型。我认为这可能是上述**特征相关性**的问题。我实际上知道一个可能有效的解决方案，即**通过堆叠将不同特征版本训练的模型进行组合**。不幸的是，我没有足够的时间尝试它。**事实上，大多数顶尖团队认为使用不同预处理和特征工程管道训练的模型的集成是成功的关键**。
- en: '**Model**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型**'
- en: At first I was using `RandomForestRegressor` to build my model. Then I tried**Xgboost** and
    it turned out to be more than twice as fast as Sklearn. From that on what I do
    everyday is basically running grid search on my work station while working on
    features on my laptop.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 起初我使用`RandomForestRegressor`来构建模型。然后我尝试了**Xgboost**，结果发现它比Sklearn快了两倍以上。从那时起，我每天做的基本上就是在工作站上运行网格搜索，同时在笔记本上处理特征。
- en: Dataset in this competition is not trivial to validate. It’s not i.i.d. and
    many records are dependent. Many times I used better features / parameters only
    to end with worse LB scores. As repeatedly stated by many accomplished Kagglers,
    you have to trust your own CV score under such situation. Therefore I decided
    to use 10-fold instead of 5-fold in cross validation and ignore the LB score in
    the following attempts.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本次比赛的数据集验证并不简单。数据不是独立同分布的，许多记录是相关的。我多次使用更好的特征/参数，结果LB分数却更差。正如许多成功的Kaggle选手所反复强调的那样，在这种情况下，你必须相信自己的CV分数。因此，我决定在交叉验证中使用10折而不是5折，并在接下来的尝试中忽略LB分数。
- en: '**Ensemble**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**集成**'
- en: 'My final model is an ensemble consisting of 4 base models:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我的最终模型是由4个基础模型组成的集成模型：
- en: '`RandomForestRegressor`'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomForestRegressor`'
- en: '`ExtraTreesRegressor`'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ExtraTreesRegressor`'
- en: '`GradientBoostingRegressor`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GradientBoostingRegressor`'
- en: '`XGBRegressor`'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`XGBRegressor`'
- en: The stacker is also a `XGBRegressor`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠模型也是一个`XGBRegressor`。
- en: The problem is that all my base models are highly correlated (with a lowest
    correlation of 0.9). I thought of including linear regression, SVM regression
    and `XGBRegressor`with linear booster into the ensemble, but these models had
    RMSE scores that are 0.02 higher (this accounts for a gap of hundreds of places
    on the leaderboard) than the 4 models I finally used. Therefore I decided not
    to use more models although they would have brought much more diversity.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于我所有的基础模型高度相关（最低相关系数为0.9）。我考虑将线性回归、SVM 回归和`XGBRegressor`（使用线性提升器）纳入集成模型，但这些模型的
    RMSE 分数比我最终使用的4个模型高出0.02（这在排行榜上相当于数百个名次）。因此，我决定不再使用更多模型，尽管它们会带来更多的多样性。
- en: The good news is that, despite base models being highly correlated, stacking
    still bumps up my score a lot. **What’s more, my CV score and LB score are in
    complete sync after I started stacking.**
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，尽管基础模型高度相关，堆叠模型仍然大大提升了我的分数。**更重要的是，自从开始使用堆叠模型后，我的CV分数和LB分数完全同步。**
- en: During the last two days of the competition, I did one more thing: **use 20
    or so different random seeds to generate the ensemble and take a weighted average
    of them as the final submission**. This is actually a kind of **bagging**. It
    makes sense in theory because in stacking I used 80% of the data to train base
    models in each iteration, whereas 100% of the data is used to train the stacker.
    Therefore it’s less clean. Making multiple runs with different seeds makes sure
    that **different 80% of the data are used each time**, thus reducing the risk
    of information leak. Yet by doing this I only achieved an increase of `0.0004`,
    which might be just due to randomness.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在比赛的最后两天，我还做了一件事：**使用大约20个不同的随机种子生成集成模型，并将它们的加权平均作为最终提交**。这实际上是一种**袋装法（bagging）**。理论上是有意义的，因为在堆叠中，我使用80%的数据来训练每次迭代中的基础模型，而100%的数据用于训练堆叠模型。因此它的效果不够干净。进行多次不同种子的运行可以确保**每次使用不同的80%数据**，从而降低信息泄露的风险。然而，通过这种方式，我只取得了`0.0004`的提升，这可能仅仅是由于随机性造成的。
- en: After the competition, I found out that my best single model scores `0.46378` on
    the private leaderboard, whereas my best stacking ensemble scores `0.45849`. That
    was the difference between the 174th place and the 98th place. In other words,
    feature engineering and model tuning got me into 10%, whereas stacking got me
    into 5%.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 比赛结束后，我发现我最好的单一模型在私人排行榜上的分数是`0.46378`，而我最好的堆叠集成模型的分数是`0.45849`。这代表了174名和98名之间的差距。换句话说，特征工程和模型调优让我进入了前10%，而堆叠模型让我进入了前5%。
- en: '**Lessons Learned**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**经验教训**'
- en: 'There’s much to learn from the solutions shared by top teams:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从顶级团队分享的解决方案中有很多值得学习的地方：
- en: There’s a pattern in the product title. For example, whether a product is accompanied
    by a certain accessory will be indicated by `With/Without XXX` at the end of the
    title.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品标题中有一种模式。例如，产品是否附带某种配件将通过标题末尾的`With/Without XXX`来指示。
- en: Use external data. For example use [WordNet](https://wordnet.princeton.edu/) or [Reddit
    Comments Dataset](https://www.kaggle.com/reddit/reddit-comments-may-2015) to train
    synonyms and [hypernyms](https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy).
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用外部数据。例如使用[WordNet](https://wordnet.princeton.edu/)或[Reddit 评论数据集](https://www.kaggle.com/reddit/reddit-comments-may-2015)来训练同义词和[上位词](https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy)。
- en: Some features based on **letters** instead of **words**. At first I was rather
    confused by this. But it makes perfect sense if you consider it. For example,
    the team that won the 3rd place took the number of letters matched into consideration
    when computing text similarity. They argued that **longer words are more specific
    and thus more likely to be assigned high relevance scores by human**. They also
    used char-by-char sequence comparison (`difflib.SequenceMatcher`) to measure **visual
    similarity**, which they claimed to be important for human.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些基于**字母**而不是**词语**的特征。一开始我对此感到困惑。但如果考虑到这一点，就会有意义。例如，获得第三名的团队在计算文本相似性时考虑了匹配的字母数量。他们认为**较长的词语更为具体，因此更有可能被人类赋予高相关性分数**。他们还使用逐字比较
    (`difflib.SequenceMatcher`) 来测量**视觉相似性**，他们认为这对人类很重要。
- en: POS-tag words and find **[head](https://en.wikipedia.org/wiki/Head_(linguistics))** in
    phrases and use them when computing various distance metrics.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对词语进行词性标注，找到**[头词](https://en.wikipedia.org/wiki/Head_(linguistics))**并在计算各种距离度量时使用它们。
- en: Extract top-ranking trigrams from the TF-IDF of product title / description
    field and compute the ratio of word from search terms that appear in these trigrams.
    Vice versa. This is like computing latent indexes from another point of view.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从产品标题/描述字段的 TF-IDF 中提取排名靠前的三元组，并计算搜索词出现在这些三元组中的比率。反之亦然。这就像从另一个角度计算潜在的索引。
- en: Some novel distance metrics like [Word Movers Distance](http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些新颖的距离度量方法，如[Word Movers Distance](http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf)。
- en: Apart from SVD, some used [NMF](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization).
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了 SVD，部分人使用了[NMF](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization)。
- en: Generate **pairwise polynomial interactions** between top-ranking features.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成**配对多项式交互**在排名靠前的特征之间。
- en: '**For CV, construct splits in which product IDs do not overlap between training
    set and testing set, and splits in which IDs do. Then we can use these with corresponding
    ratio to approximate the impact of public/private LB split in our local CV.**'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对于交叉验证，构建训练集和测试集中产品 ID 不重叠的分割，以及 ID 重叠的分割。然后我们可以使用这些分割和相应的比率来估算公共/私人 LB 分割在本地交叉验证中的影响。**'
- en: Summary
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: '**Takeaways**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**要点**'
- en: It was a good call to **start doing ensembles early in the competition**. As
    it turned out, I was still playing with features during the very last days.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在竞赛早期开始进行集成**是一个明智的选择。结果是，我在最后几天仍在处理特征。'
- en: It’s of high priority that I build a pipeline capable of automatic model training
    and recording best parameters.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立一个能够自动训练模型和记录最佳参数的管道是优先事项。
- en: '**Features matter the most!** I didn’t spend enough time on features in this
    competition.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征最为重要！** 我在这次竞赛中没有花足够的时间在特征上。'
- en: If possible, spend some time to manually inspect raw data for patterns.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果可能，花一些时间手动检查原始数据中的模式。
- en: '**Issues Raised**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**提出的问题**'
- en: Several issues I encountered in this competitions are of high research values.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这次竞赛中遇到的几个问题具有很高的研究价值。
- en: How to do reliable CV with dependent data.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何在相关数据下进行可靠的交叉验证。
- en: How to quantify **the trade-off between diversity and accuracy** in ensemble
    learning.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何量化**多样性与准确性之间的权衡**在集成学习中。
- en: How to deal with feature interaction which harms the model’s performance. And**how
    to determine whether new features are effective in such situations**.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何处理对模型性能有害的特征交互。以及**如何确定新特征在这种情况下是否有效**。
- en: '**Beginner Tips**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**初学者建议**'
- en: Choose a competition you’re interested in. **It would be better if you’ve already
    have some insights about the problem domain.**
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个你感兴趣的竞赛。**如果你对问题领域已有一些见解会更好。**
- en: Following my approach or somebody else’s, start exploring, understanding and
    modeling data.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 无论是遵循我的方法还是别人的方法，开始探索、理解和建模数据。
- en: Learn from forum and scripts. See how others interpret data and construct features.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从论坛和脚本中学习。查看其他人如何解释数据和构建特征。
- en: '**Find winner interviews / blog posts of previous competitions. They’re extremely
    helpful, especially if from competitions that share some similarities with that
    one you’re working on.**'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**查找之前比赛的获胜者访谈/博客文章。这些非常有帮助，特别是如果来自与你正在进行的比赛有相似之处的比赛。**'
- en: Start doing ensemble after you have reached a pretty good score (e.g. 10% ~
    20%) or you feel that there isn’t much room for new features (which, sadly, always
    turns out to be false).
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你达到相当好的分数（例如 10% ~ 20%）后，或者你觉得没有太多新特征的空间时（这通常是错误的），开始使用集成方法。
- en: If you think you may have a chance to win the prize, try teaming up!
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你认为自己有机会赢得奖项，试着组队合作！
- en: '**Don’t give up until the end of the competition. At least try something new
    every day.**'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**不要在比赛结束前放弃。至少每天尝试一些新东西。**'
- en: Learn from the sharings of top teams after the competition. Reflect on your
    approaches. **If possible, spend some time verifying what you learn.**
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从比赛后的顶级团队分享中学习。反思你的方法。**如果可能，花些时间验证你所学到的东西。**
- en: Get some rest!
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 休息一下吧！
- en: Reference
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Beating Kaggle the Easy Way - Dong Ying](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiPxZHewLbMAhVKv5QKHb3PCGwQFggcMAA&url=http%3A%2F%2Fwww.ke.tu-darmstadt.de%2Flehre%2Farbeiten%2Fstudien%2F2015%2FDong_Ying.pdf&usg=AFQjCNE9o2BcEkqdnu_-lQ3EFD3eRAFWiw&sig2=oiU8TCEH57EYF9v9l6Scrw&bvm=bv.121070826,d.dGo)'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[轻松战胜 Kaggle - 董颖](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiPxZHewLbMAhVKv5QKHb3PCGwQFggcMAA&url=http%3A%2F%2Fwww.ke.tu-darmstadt.de%2Flehre%2Farbeiten%2Fstudien%2F2015%2FDong_Ying.pdf&usg=AFQjCNE9o2BcEkqdnu_-lQ3EFD3eRAFWiw&sig2=oiU8TCEH57EYF9v9l6Scrw&bvm=bv.121070826,d.dGo)'
- en: '[Search Results Relevance Winner’s Interview: 1st place, Chenglong Chen](https://github.com/ChenglongChen/Kaggle_CrowdFlower/blob/master/BlogPost/BlogPost.md)'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[搜索结果相关性获胜者访谈：第一名，程龙陈](https://github.com/ChenglongChen/Kaggle_CrowdFlower/blob/master/BlogPost/BlogPost.md)'
- en: '[(Chinese) Solution for Prudential Life Insurance Assessment - Nutastray](http://rstudio-pubs-static.s3.amazonaws.com/158725_5d2f977f4004490e9b095c0ef9357c6b.html)'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[(中文) 保诚人寿保险评估解决方案 - Nutastray](http://rstudio-pubs-static.s3.amazonaws.com/158725_5d2f977f4004490e9b095c0ef9357c6b.html)'
- en: '![Linghao Zhang](../Images/47effce75b02d75ca99360d2793970ea.png)**Bio: [Linghao
    Zhang](https://www.linkedin.com/in/linghaozh)** is a senior year Computer Science
    student at Fudan University and Data Mining Engineer at Strikingly. His interests
    include machine learning, data mining, natural language processing, knowledge
    graphs, and big data analytics.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '![Linghao Zhang](../Images/47effce75b02d75ca99360d2793970ea.png)**简介：[Linghao
    Zhang](https://www.linkedin.com/in/linghaozh)** 是复旦大学计算机科学大四学生和 Strikingly 的数据挖掘工程师。他的兴趣包括机器学习、数据挖掘、自然语言处理、知识图谱和大数据分析。'
- en: '[Original](https://dnc1994.com/2016/05/rank-10-percent-in-first-kaggle-competition-en/).
    Reposted with permission.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://dnc1994.com/2016/05/rank-10-percent-in-first-kaggle-competition-en/)。经授权转载。'
- en: '**Related:**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Approaching (Almost) Any Machine Learning Problem](/2016/08/approaching-almost-any-machine-learning-problem.html)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[接近（几乎）任何机器学习问题](/2016/08/approaching-almost-any-machine-learning-problem.html)'
- en: '[Automated Data Science & Machine Learning: An Interview with the Auto-sklearn
    Team](/2016/10/interview-auto-sklearn-automated-data-science-machine-learning-team.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动化数据科学与机器学习：与 Auto-sklearn 团队的访谈](/2016/10/interview-auto-sklearn-automated-data-science-machine-learning-team.html)'
- en: '[Data Science Basics: An Introduction to Ensemble Learners](/2016/11/data-science-basics-intro-ensemble-learners.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学基础：集成学习者简介](/2016/11/data-science-basics-intro-ensemble-learners.html)'
- en: More On This Topic
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[How LinkedIn Uses Machine Learning To Rank Your Feed](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LinkedIn 如何利用机器学习对你的动态进行排名](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)'
- en: '[How to Get Your First Job in Data Science without Any Work Experience](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在没有任何工作经验的情况下获得数据科学的第一份工作](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)'
- en: '[Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 TensorFlow 和 Keras 构建和训练你的第一个神经网络](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
- en: '[It''s alive! Build your first robots with Python and some cheap,…](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[它活了！使用 Python 和一些便宜的组件构建你的第一个机器人](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
- en: '[From Zero to Hero: Create Your First ML Model with PyTorch](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从零到英雄：使用 PyTorch 创建你的第一个机器学习模型](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)'
- en: '[Deploying Your First Machine Learning Model](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[部署你的第一个机器学习模型](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)'
