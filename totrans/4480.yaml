- en: Hyperparameter Optimization for Machine Learning Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/05/hyperparameter-optimization-machine-learning-models.html](https://www.kdnuggets.com/2020/05/hyperparameter-optimization-machine-learning-models.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)![Figure](../Images/e60360ab2907502e8bfd8adc536d14bf.png)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Credits](https://swisscognitive.ch/2020/01/13/machine-learning-in-adversity/)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Model optimization is one of the toughest challenges in the implementation of
    machine learning solutions. Entire branches of machine learning and deep learning
    theory have been dedicated to the optimization of models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter optimization in machine learning intends to find the hyperparameters
    of a given machine learning algorithm that deliver the best performance as measured
    on a validation set. Hyperparameters, in contrast to model parameters, are set
    by the machine learning engineer before training. The number of trees in a random
    forest is a hyperparameter while the weights in a neural network are model parameters
    learned during training. I like to think of hyperparameters as the model settings
    to be tuned so that the model can optimally solve the machine learning problem.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples of model hyperparameters include:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: The `learning rate` for training a neural network.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `**C**` and `**????**` hyperparameters for support vector machines.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `**k**` in k-nearest neighbors.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter optimization finds a combination of hyperparameters that returns
    an optimal model which reduces a predefined loss function and in turn increases
    the accuracy on given independent data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/1a76857e56c4740636dc8f28879c8971.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: '[Classification models with their respective hyperparameters.](https://www.computer.org/csdl/journal/ts/2018/06/07990590/13rRUx0gerA)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter Optimization methods
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Hyperparameters can have a direct impact on the training of machine learning
    algorithms. Thus, to achieve maximal performance, it is important to understand
    how to optimize them. Here are some common strategies for optimizing hyperparameters:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Manual Hyperparameter Tuning**'
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Traditionally, hyperparameters were tuned manually by trial and error. This **is** still
    commonly done, and experienced engineers can “guess” parameter values that will
    deliver very high accuracy for ML models. However, there isa continual search
    for better, faster, and more automatic methods to optimize hyperparameters.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Grid Search
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Grid search is arguably the most basic hyperparameter tuning method. With this
    technique, we simply build a model for each possible combination of all of the
    hyperparameter values provided, evaluating each model, and selecting the architecture
    which produces the best results.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d65f74594b1f825f12beb47db397245.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: Grid-search does NOT only apply to one model type but can be applied across
    machine learning to calculate the best parameters to use for any given model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a typical soft-margin SVM classifier equipped with an RBF kernel
    has at least two hyperparameters that need to be optimized for good performance
    on unseen data: a regularization constant *C* and a kernel hyperparameter γ. Both
    parameters are continuous, so to perform grid search, one selects a finite set
    of “reasonable” values for each, let’s say'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e78be57e455f5dfa0eac00be4c135791.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: Grid search then trains an SVM with each pair (*C*, γ) in the c[artesian product](https://www.wikiwand.com/en/Cartesian_product) of
    these two sets and evaluates their performance on a held-out validation set (or
    by internal cross-validation on the training set, in which case multiple SVMs
    are trained per pair). Finally, the grid search algorithm outputs the settings
    that achieved the highest score in the validation procedure.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '**How does it work in python?**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here’s a python implementation of grid search using `[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)` from
    the `sklearn` library.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Fitting the Grid Search:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Methods to Run on Grid-Search:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We then use the best set of hyperparameter values chosen in the grid search,
    in the actual model as shown above.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: One of the **drawbacks** of grid search is that when it comes to dimensionality,
    it suffers when evaluating the number of hyperparameters grows exponentially.
    However, there is no guarantee that the search will produce the perfect solution,
    as it usually finds one by aliasing around the right set.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Random Search
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Often some of the hyperparameters matter much more than others. Performing random
    search rather than grid search allows a much more precise discovery of good values
    for the important ones.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8292a0cb03eeaa9f5cf4f473a2581516.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: Random Search sets up a grid of hyperparameter values and selects random combinations
    to train the model and score. This allows you to explicitly control the number
    of parameter combinations that are attempted. The number of search iterations
    is set based on time or resources. Scikit Learn offers the `RandomizedSearchCV` function
    for this process.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: While it’s possible that `RandomizedSearchCV` will not find as accurate a result
    as `GridSearchCV`, it surprisingly picks the best result more often than not and
    in a *fraction* of the time it takes `GridSearchCV` would have taken. Given the
    same resources, Randomized Search can even outperform Grid Search. This can be
    visualized in the graphic below when continuous parameters are used.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: The chances of finding the optimal parameter are comparatively higher in random
    search because of the random search pattern where the model might end up being
    trained on the optimized parameters without any aliasing. Random search works
    best for lower dimensional data since the time taken to find the right set is
    less with less number of iterations. Random search is the best parameter search
    technique when there is less number of dimensions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: In the case of deep learning algorithms, it outperforms the grid search.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/b1b1600a34c9a27779b36bbb9d9a02cc.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: '[Credits](https://community.alteryx.com/t5/Data-Science-Blog/Hyperparameter-Tuning-Black-Magic/ba-p/449289)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: In the above figure, yay that you have two parameters, with 5x6 grid search
    you check only 5 different parameter values from each of the parameters (six rows
    and five columns on the plot on the left), while with the random search you check
    14 different parameter values of each of the parameters.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '**How does it work in python?**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here’s a python implementation of grid search using `RandomizedSearchCV` of
    the `sklearn` library.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Fitting the Random Search:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Methods to Run on Grid-Search:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 4\. Bayesian Optimization
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The previous two methods performed individual experiments building models with
    various hyperparameter values and recording the model performance for each. Because
    each experiment was performed in isolation, it’s very easy to parallelize this
    process. However, because each experiment was performed in isolation, we’re not
    able to use the information from one experiment to improve the next experiment.
    Bayesian optimization belongs to a class of *sequential model-based optimization* (SMBO)
    algorithms that allow for one to use the results of our previous iteration to
    improve our sampling method of the next experiment.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: This, in turn, limits the number of times a model needs to be trained for validation
    as solely those settings that are expected to generate a higher validation score
    are passed through for evaluation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian optimization works by constructing a posterior distribution of functions
    (Gaussian process) that best describes the function you want to optimize. As the
    number of observations grows, the posterior distribution improves, and the algorithm
    becomes more certain of which regions in parameter space are worth exploring and
    which are not.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see this in the image below:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/23d29016155406c3b70ff0644e409dd3.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: Source: [bayesian-optimization](https://github.com/fmfn/BayesianOptimization)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: As you iterate over and over, the algorithm balances its needs of exploration
    and exploitation taking into account what it knows about the target function.
    At each step, a Gaussian Process is fitted to the known samples (points previously
    explored), and the posterior distribution, combined with an exploration strategy
    (such as UCB (Upper Confidence Bound), or EI (Expected Improvement)), is used
    to determine the next point that should be explored.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Using Bayesian Optimization, we can explore the parameter space more smartly,
    and thus reduce the time required to do this process.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check the python implementation of Bayesian optimization below:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[**thuijskens/bayesian-optimization**](https://github.com/thuijskens/bayesian-optimization/blob/master/ipython-notebooks/svm-optimization.ipynb)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Gradient-based Optimization
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is specially used in the case of Neural Networks. It computes the gradient
    with respect to hyperparameters and optimizes them using the gradient descent
    algorithm.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: The calculation of the gradient is the least of problems. At least in times
    of advanced [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) software.
    (**Implementing this in a general way for all sklearn-classifiers, of course,
    is not easy**).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/43a7177eaea30f7a4c2b619f758eb304.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: '[Credits](https://www.mathworks.com/matlabcentral/fileexchange/27631-derivative-based-optimization)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'And while there are works of people who used this kind of idea, they only did
    this for some specific and well-formulated problem (e.g. SVM-tuning). Furthermore,
    there probably were a lot of assumptions because:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Why is this not a good idea?
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**1\. Hyperparameter optimization is in general non-smooth**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: GD really likes smooth functions as a gradient of zero is not helpful
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each hyper-parameter which is defined by some discrete-set (e.g. choice of l1
    vs. l2 penalization) introduces non-smooth surfaces.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2\. Hyperparameter optimization is in general non-convex**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: The whole convergence-theory of gradient descent assumes, that the underlying
    problem is convex.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Good-case: you obtain some local-minimum (can be arbitrarily bad).'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Worst-case: gradient descent is not even converging to some local-minimum.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To get python implementation and more about the Gradient Descent Optimization
    algorithm [click here.](https://hackernoon.com/implementing-different-variants-of-gradient-descent-optimization-algorithm-in-python-using-numpy-809e7ab3bab4)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '**6\. Evolutionary Optimization**'
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Evolutionary optimization follows a process inspired by the biological concept
    of evolution and since natural evolution is a dynamic process in a changing environment,
    they are also well suited to dynamic optimization problems.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/801484956ac7dd0a3414aecce35df824.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: Evolutionary algorithms are often used to find good approximate solutions that
    cannot be easily solved by other techniques. Optimization problems often don’t
    have an exact solution as it may be too time-consuming and computationally intensive
    to find an optimal solution. However, evolutionary algorithms are ideal in such
    situations as they can be used to find a near-optimal solution which is often
    sufficient.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: One advantage of evolutionary algorithms is that they develop solutions free
    of any human misconceptions or biases, which means they can produce surprising
    ideas which we might never generate ourselves.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: You can learn more about evolutionary algorithms [here](https://en.wikipedia.org/wiki/Evolutionary_algorithm).
    You can also check python implementation [here](https://github.com/MorvanZhou/Evolutionary-Algorithm).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: As a general rule of thumb, any time you want to optimize tuning hyperparameters,
    think Grid Search and Randomized Search!
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this article, we’ve learned that finding the right values for hyperparameters
    can be a frustrating task and can lead to underfitting or overfitting machine
    learning models. We saw how this hurdle can be overcome by using Grid Search &
    Randomized Search and other algorithms — which optimize tuning of hyperparameters
    to save time and eliminate the chance of overfitting or underfitting by random
    guessing.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Well, this concludes this article**. **I hope you guys have enjoyed reading
    it, feel free to share your comments/thoughts/feedback in the comment section.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading !!!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Nagesh Singh Chauhan](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)**
    is a Data Science enthusiast. Interested in Big Data, Python, Machine Learning.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/swlh/hyperparameter-optimization-for-machine-learning-models-12582f00ae52).
    Reposted with permission.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[How to Do Hyperparameter Tuning on Any Python Script in 3 Easy Steps](/2020/04/hyperparameter-tuning-python.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Coronavirus COVID-19 Genome Analysis using Biopython](/2020/04/coronavirus-covid-19-genome-analysis-biopython.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Practical Hyperparameter Optimization](/2020/02/practical-hyperparameter-optimization.html)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Hyperparameter Optimization: 10 Top Python Libraries](https://www.kdnuggets.com/2023/01/hyperparameter-optimization-10-top-python-libraries.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning Pipeline Optimization with TPOT](https://www.kdnuggets.com/2021/05/machine-learning-pipeline-optimization-tpot.html)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hyperparameter Tuning Using Grid Search and Random Search in Python](https://www.kdnuggets.com/2022/10/hyperparameter-tuning-grid-search-random-search-python.html)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hyperparameter Tuning: GridSearchCV and RandomizedSearchCV, Explained](https://www.kdnuggets.com/hyperparameter-tuning-gridsearchcv-and-randomizedsearchcv-explained)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SQL Query Optimization Techniques](https://www.kdnuggets.com/2023/03/sql-query-optimization-techniques.html)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Database Optimization: Exploring Indexes in SQL](https://www.kdnuggets.com/2023/07/database-optimization-exploring-indexes-sql.html)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据库优化：探索 SQL 中的索引](https://www.kdnuggets.com/2023/07/database-optimization-exploring-indexes-sql.html)'
