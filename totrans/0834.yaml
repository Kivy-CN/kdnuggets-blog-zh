- en: 7 Steps to Mastering Data Cleaning and Preprocessing Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 掌握数据清理和预处理技术的7个步骤
- en: 原文：[https://www.kdnuggets.com/2023/08/7-steps-mastering-data-cleaning-preprocessing-techniques.html](https://www.kdnuggets.com/2023/08/7-steps-mastering-data-cleaning-preprocessing-techniques.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/08/7-steps-mastering-data-cleaning-preprocessing-techniques.html](https://www.kdnuggets.com/2023/08/7-steps-mastering-data-cleaning-preprocessing-techniques.html)
- en: '![7 Steps to Mastering Data Cleaning and Preprocessing Techniques](../Images/b1ac81474d1360b7d4a1b880bdd3551d.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![掌握数据清理和预处理技术的7个步骤](../Images/b1ac81474d1360b7d4a1b880bdd3551d.png)'
- en: Illustration by Author. Inspired by MEME of Dr. Angshuman Ghosh
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 插图作者。灵感来源于 Dr. Angshuman Ghosh 的 MEME
- en: Mastering Data Cleaning and Preprocessing Techniques is  fundamental for solving
    a lot of data science projects. A simple demonstration of how important can be
    found in the [meme](https://www.quora.com/Why-are-the-career-expectations-vs-the-reality-of-being-data-scientists)
    about the expectations of a student studying data science before working, compared
    with the reality of the data scientist job.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 掌握数据清理和预处理技术对解决许多数据科学项目至关重要。一个简单的演示可以在 [meme](https://www.quora.com/Why-are-the-career-expectations-vs-the-reality-of-being-data-scientists)
    中找到，该meme展示了学生在进入数据科学领域之前对工作的期望与数据科学家的实际工作的对比。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: We tend to idealise the job position before having a concrete experience, but
    the reality is that it’s always different from what we really expect. When working
    with a real-world problem, there is no documentation of the data and the dataset
    is very dirty. First, you have to dig deep in the problem, understand what clues
    you are missing and what information you can extract.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们往往会在没有具体经验之前对职位进行理想化，但现实总是与我们真正的期待不同。当处理实际问题时，数据没有文档，并且数据集非常脏乱。首先，你必须深入挖掘问题，理解你缺少了什么线索以及可以提取哪些信息。
- en: After understanding the problem, you need to prepare the dataset for your machine
    learning model since the data in its initial condition is never enough. In this
    article, I am going to show seven steps that can help you on pre-processing and
    cleaning your dataset.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解问题后，你需要为你的机器学习模型准备数据集，因为数据在初始状态下从来是不够的。在这篇文章中，我将展示七个步骤，这些步骤可以帮助你进行数据集的预处理和清理。
- en: 'Step 1: Exploratory Data Analysis'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一步：探索性数据分析
- en: The first step in a data science project is the exploratory analysis, that helps
    in understanding the problem and taking decisions in the next steps. It tends
    to be skipped, but it’s the worst error because you’ll lose a lot of time later
    to find the reason why the model gives errors or didn’t perform as expected.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学项目的第一步是探索性分析，这有助于理解问题并在下一步做出决策。这一步往往被跳过，但这是最糟糕的错误，因为你之后会花费很多时间去找出模型为何出现错误或未达到预期的原因。
- en: 'Based on my experience as data scientist, I would divide the exploratory analysis
    into three parts:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我作为数据科学家的经验，我将探索性分析分为三个部分：
- en: Check the structure of the dataset, the statistics, the missing values, the
    duplicates, the unique values of the categorical variables
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查数据集的结构、统计信息、缺失值、重复值以及分类变量的唯一值
- en: Understand the meaning and the distribution of the variables
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 了解变量的含义和分布
- en: Study the relationships between variables
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 研究变量之间的关系
- en: 'To analyse how the dataset is organised, there are the following Pandas methods
    that can help you:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分析数据集的组织方式，可以使用以下 Pandas 方法：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When trying to understand the variables, it’s useful to split the analysis
    into two further parts: numerical features and categorical features. First, we
    can focus on the numerical features that can be visualised through histograms
    and boxplots. After, it’s the turn for the categorical variables. In case it’s
    a binary problem, it’s better to start by looking if the classes are balanced.
    After our attention can be focused on the remaining categorical variables using
    the bar plots. In the end, we can finally check the correlation between each pair
    of numerical variables. Other useful data visualisations can be the scatter plots
    and boxplots to observe the relations between a numerical and a categorical variable.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在试图理解变量时，将分析分为两个部分：数值特征和分类特征是有用的。首先，我们可以关注那些可以通过直方图和箱线图可视化的数值特征。之后，轮到分类变量。如果这是一个二分类问题，最好先查看类别是否平衡。接下来，我们可以集中关注其余的分类变量，使用条形图进行分析。最后，我们可以检查每对数值变量之间的相关性。其他有用的数据可视化方法可以是散点图和箱线图，用于观察数值变量和分类变量之间的关系。
- en: 'Step 2: Deal with Missings'
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 2：处理缺失值
- en: In the first step, we have already investigated if there are missings in each
    variable. In case there are missing values, we need to understand how to handle
    the issue. The easiest way would be to remove the variables or the rows that contain
    NaN values, but we would prefer to avoid it because we risk losing useful information
    that can help our machine learning model on solving the problem.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，我们已经检查了每个变量是否存在缺失值。如果存在缺失值，我们需要了解如何处理这些问题。最简单的方法是删除包含 NaN 值的变量或行，但我们最好避免这样做，因为我们可能会丢失有助于机器学习模型解决问题的有用信息。
- en: 'If we are dealing with a numerical variable, there are several approaches to
    fill it. The most popular method consists in filling the missing values with the
    mean/median of that feature:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们处理的是数值变量，有几种方法可以填补这些空值。最常用的方法是用该特征的均值/中位数来填补缺失值：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Another way is to substitute the blanks with group by imputations:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是用按组插补的方法来替代空值：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It can be a better option in case there is a strong relationship between a numerical
    feature and a categorical feature.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数值特征和分类特征之间存在强关系，这可能是一个更好的选择。
- en: 'In the same way, we can fill the missing values of categorical based on the
    mode of that variable:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以根据该变量的众数来填补分类特征的缺失值：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Step 3: Deal with Duplicates and Outliers'
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 3：处理重复项和异常值
- en: 'If there are duplicates within the dataset, it’s better to delete the duplicated
    rows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集中存在重复项，最好删除重复的行：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: While deciding how to handle duplicates is simple, dealing with outliers can
    be challenging. You need to ask yourself “Drop or not Drop Outliers?”.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 决定如何处理重复项比较简单，但处理异常值可能会具有挑战性。你需要问自己“是否删除异常值？”。
- en: Outliers should be deleted if you are sure that they provide only noisy information.
    For example, the dataset contains two people with 200 years, while the range of
    the age is between 0 and 90\. In that case, it’s better to remove these two data
    points.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你确定异常值只提供噪声信息，则应该删除异常值。例如，数据集中包含两个人的年龄为 200 年，而年龄范围在 0 到 90 之间。在这种情况下，最好删除这两个数据点。
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Unfortunately, most of the time removing outliers can lead to losing important
    information. The most efficient way is to apply the logarithm transformation to
    the numerical feature.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，大多数时候，删除异常值可能会导致丢失重要的信息。最有效的方法是对数值特征应用对数变换。
- en: Another technique that I discovered during my last experience is the clipping
    method. In this technique, you choose the upper and the lower bound, that can
    be the 0.1 percentile and the 0.9 percentile. The values of the feature below
    the lower bound will be substituted with the lower bound value, while the values
    of the variable above the upper bound will be replaced with the upper bound value.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我在最近的经验中发现的另一种技术是剪辑方法。在这种技术中，你选择上界和下界，这可以是第 0.1 百分位数和第 0.9 百分位数。低于下界的特征值将被替换为下界值，而高于上界的变量值将被替换为上界值。
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Step 4: Encode Categorical Features'
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 4：对分类特征进行编码
- en: The next phase is to convert the categorical features into numerical features.
    Indeed, the machine learning model is able only to work with numbers, not strings.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 下一阶段是将分类特征转换为数值特征。实际上，机器学习模型只能处理数字，不能处理字符串。
- en: 'Before going further, you should distinguish between two types of categorical
    variables: non-ordinal variables and ordinal variables.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在进一步讨论之前，你应该区分两种类别变量：非序数变量和序数变量。
- en: Examples of non-ordinal variables are the gender, the marital status, the type
    of job. So, it’s non-ordinal if the variable doesn’t follow an order, differently
    from the ordinal features. An example of ordinal variables can be the education
    with values “childhood”, “primary”, “secondary” and “tertiary", and the income
    with levels “low”, “medium” and “high”.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 非序数变量的例子包括性别、婚姻状况、工作类型。因此，如果变量没有遵循顺序，则为非序数变量，与序数特征不同。序数变量的一个例子可以是教育水平，如“童年”、“小学”、“中学”和“大学”，以及收入水平，如“低”、“中”等“高”。
- en: When we are dealing with non-ordinal variables, One-Hot Encoding is the most
    popular technique taken into account to convert these variables into numerical.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理非序数变量时，One-Hot编码是最常用的技术，用于将这些变量转换为数值型。
- en: In this method, we create a new binary variable for each level of the categorical
    feature. The value of each binary variable is 1 when the name of the level coincides
    with the value of the level, 0 otherwise.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们为每个类别特征的水平创建一个新的二进制变量。当水平名称与变量值相同时，该二进制变量的值为1，否则为0。
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: When the variable is ordinal, the most common technique used is the Ordinal
    Encoding, which consists in converting the unique values of the categorical variable
    into integers that follow an order. For example, the levels “low”, “Medium” and
    “High” of income will be encoded respectively as 0,1 and 2.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当变量是序数时，最常用的技术是序数编码，它将类别变量的唯一值转换为遵循顺序的整数。例如，收入的水平“低”、“中”和“高”将分别编码为0、1和2。
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: There are other possible encoding techniques if you want to explore here. You
    can take a look [here](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02)
    in case you are interested in alternatives.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想进一步探索其他可能的编码技术，可以查看[这里](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02)，以了解替代方案。
- en: 'Step 5: Split dataset into training and test set'
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五步：将数据集拆分为训练集和测试集
- en: 'It’s time to divide the dataset into three fixed subsets: the most common choice
    is to use 60% for training, 20% for validation and 20% for testing. As the quantity
    of data grows, the percentage for training increases and the percentage for validation
    and testing decreases.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候将数据集划分为三个固定的子集：最常见的选择是将60%用于训练，20%用于验证，20%用于测试。随着数据量的增长，训练集的比例增加，而验证集和测试集的比例减少。
- en: It’s important to have three subsets because the training set is used to train
    the model, while the validation and the test sets can be useful to understand
    how the model is performing on new data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有三个子集很重要，因为训练集用于训练模型，而验证集和测试集可以用来了解模型在新数据上的表现。
- en: 'To split the dataset, we can use the train_test_split of scikit-learn:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要拆分数据集，我们可以使用scikit-learn的train_test_split：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In case we are dealing with a classification problem and the classes are not
    balanced, it’s better to set up the stratify argument to be sure that there is
    the same proportion of classes in training, validation and test sets.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们处理的是分类问题且类别不平衡，最好设置分层参数，以确保训练集、验证集和测试集中类别的比例相同。
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This stratified cross-validation also helps to ensure that there is the same
    percentage of the target variable in the three subsets and give more accurate
    performances of the model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分层交叉验证还帮助确保三个子集中的目标变量百分比相同，从而提供更准确的模型性能。
- en: 'Step 6: Feature Scaling'
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六步：特征缩放
- en: There are machine learning models, like Linear Regression, Logistic Regression,
    KNN, Support Vector Machine and Neural Networks, that require scaling features.
    The feature scaling only helps the variables be in the same range, without changing
    the distribution.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习模型，如线性回归、逻辑回归、KNN、支持向量机和神经网络，需要特征缩放。特征缩放只是帮助变量处于相同范围内，并不改变分布。
- en: There are three most popular feature scaling techniques are Normalization, Standardization
    and Robust scaling.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最流行的三种特征缩放技术是归一化、标准化和鲁棒缩放。
- en: Normalization, aso called min-max scaling, consists of mapping the value of
    a variable into a range between 0 and 1\. This is possible by subtracting the
    minimum of the feature from the feature value and, then, dividing by the difference
    between the maximum and the minimum of that feature.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**归一化**，也称为最小-最大缩放，将变量的值映射到 0 和 1 之间。这是通过从特征值中减去特征的最小值，然后除以该特征的最大值和最小值之间的差值来实现的。'
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Another common approach is Standardization, that rescales the values of a column
    to respect the properties of a standard normal distribution, which is characterised
    by mean equal to 0 and variance equal to 1.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见方法是**标准化**，它将列的值重新缩放以符合标准正态分布的特性，即均值为 0，方差为 1。
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If the feature contains outliers that cannot be removed, a preferable method
    is the Robust Scaling, that rescales the values of a feature based on robust statistics,
    the median, the first and the third quartile. The rescaled value is obtained by
    subtracting the median from the original value and, then, dividing by the Interquartile
    Range, which is the difference between the 75th and 25th quartile of the feature.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征包含无法移除的离群值，建议使用**鲁棒缩放**方法，该方法根据鲁棒统计量（中位数、第一四分位数和第三四分位数）对特征值进行缩放。缩放值通过从原始值中减去中位数，然后除以四分位距（即特征的第
    75 和第 25 四分位数之间的差异）来获得。
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In general, it’s preferable to calculate the statistics based on the training
    set and then use them to rescale the values on both training, validation and test
    sets. This is because we suppose that we only have the training data and, later,
    we want to test our model on new data, which should have a similar distribution
    than the training set.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，最好基于训练集计算统计数据，然后用这些统计数据来缩放训练、验证和测试集上的值。这是因为我们假设只有训练数据，然后希望在新数据上测试我们的模型，新数据应具有与训练集相似的分布。
- en: 'Step 7: Deal with Imbalanced Data'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 7 步：处理不平衡数据
- en: '![7 Steps to Mastering Data Cleaning and Preprocessing Techniques](../Images/33d67baeb895e22926fa8545da187b47.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![掌握数据清洗和预处理技术的 7 个步骤](../Images/33d67baeb895e22926fa8545da187b47.png)'
- en: This step is only included when we are working in a classification problem and
    we have found that the classes are imbalanced.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步骤仅在我们处理分类问题并发现类别不平衡时才包含。
- en: In case there is a slight difference between the classes, for example class
    1 contains 40% of the observations and class 2 contains the remaining 60%, we
    don’t need to apply oversampling or undersampling techniques to alter the number
    of samples in one of the classes. We can just avoid looking at accuracy since
    it’s a good measure only when the dataset is balanced and we should care only
    about evaluation measures, like precision, recall and f1-score.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果类别之间存在细微差异，例如类别 1 包含 40% 的观察数据，类别 2 包含剩余的 60%，我们无需应用过采样或欠采样技术来改变某一类别的样本数量。我们只需避免关注准确率，因为准确率只有在数据集平衡时才是一个良好的衡量指标，我们应该只关注评估指标，如**精确度**、**召回率**和**F1
    分数**。
- en: But it can happen that the positive class has a very low proportion of data
    points (0.2) compared to the negative class (0.8). The machine learning may not
    perform well with the class with less observations, leading to failing on solving
    the task.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 但有可能正类的数据点比例非常低（0.2），与负类（0.8）相比。机器学习可能在样本较少的类别上表现不佳，从而导致无法解决任务。
- en: 'To overcome this issue, there are two possibilities: undersampling the majority
    class and oversampling the minority class. Undersampling consists in reducing
    the number of samples by randomly removing some data points from the majority
    class, while Oversampling increases the number of observations in the minority
    class by adding randomly data points from the less frequent class. There is the
    imblearn that allows to balance the dataset with few lines of code:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个问题，有两种可能性：对多数类进行欠采样和对少数类进行过采样。欠采样通过随机移除一些数据点来减少样本数量，而过采样通过从较少出现的类别中随机添加数据点来增加少数类的观察数量。imblearn
    可以用少量代码实现数据集平衡：
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: However, removing or duplicating some of the observations can be ineffective
    sometimes in improving the performance of the model. It would be better to create
    new artificial data points in the minority class. A technique proposed to solve
    this issue is SMOTE, which is known for generating synthetic records in the class
    less represented. Like KNN, the idea is to identify k nearest neighbors of observations
    belonging to the minority class, based on a particular distance, like t.  After
    a new point is generated at a random location between these k nearest neighbors.
    This process will keep creating new points until the dataset is completely balanced.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时删除或重复一些观察可能在提高模型性能方面效果不佳。更好的方法是创建新的人工数据点在少数类中。为解决此问题，提出了一种技术 SMOTE，它以生成在代表性较少的类别中的合成记录而闻名。像
    KNN 一样，其思路是基于特定距离 t 确定属于少数类的观察的 k 个最近邻。然后在这些 k 个最近邻之间的随机位置生成新点。这个过程将持续创建新点，直到数据集完全平衡。
- en: '[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: I should highlight that these approaches should be applied only to resample
    the training set. We want that our machine model learns in a robust way and, then,
    we can apply it to make predictions on new data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该强调，这些方法应仅应用于重新采样训练集。我们希望我们的机器模型以稳健的方式学习，然后，我们可以将其应用于对新数据进行预测。
- en: Final Thoughts
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终想法
- en: I hope you have found this comprehensive tutorial useful. It can be hard to
    start our first data science project without being aware of all these techniques.
    You can find all my code [here](https://www.kaggle.com/code/eugeniaanello/7-steps-to-mastering-preprocessing#7.-Deal-with-Imbalanced-Data).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你发现这个全面的教程对你有帮助。开始我们的第一个数据科学项目时，如果不掌握所有这些技巧，可能会很困难。你可以在 [这里](https://www.kaggle.com/code/eugeniaanello/7-steps-to-mastering-preprocessing#7.-Deal-with-Imbalanced-Data)
    找到我的所有代码。
- en: There are surely other methods I didn’t cover in the article, but I preferred
    to focus on the most popular and known ones. Do you have other suggestions? Drop
    them in the comments if you have insightful suggestions.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，还有其他方法我在文章中没有涵盖，但我更愿意专注于最受欢迎和知名的方法。你还有其他建议吗？如果有有见地的建议，请在评论中留言。
- en: '**Useful resources:**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**有用的资源：**'
- en: '[A Practical Guide for Exploratory Data Analysis](https://towardsdatascience.com/a-practical-guide-for-exploratory-data-analysis-5ab14d9a5f24)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[探索性数据分析的实用指南](https://towardsdatascience.com/a-practical-guide-for-exploratory-data-analysis-5ab14d9a5f24)'
- en: '[Which models require normalized data?](https://www.yourdatateacher.com/2022/06/13/which-models-require-normalized-data/)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[哪些模型需要规范化数据？](https://www.yourdatateacher.com/2022/06/13/which-models-require-normalized-data/)'
- en: '[Random Oversampling and Undersampling for Imbalanced Classification](https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[不平衡分类的随机过采样和欠采样](https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/)'
- en: '**[Eugenia Anello](https://www.linkedin.com/in/eugenia-anello/)** is currently
    a research fellow at the Department of Information Engineering of the University
    of Padova, Italy. Her research project is focused on Continual Learning combined
    with Anomaly Detection.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**[尤金尼亚·安内洛](https://www.linkedin.com/in/eugenia-anello/)** 目前是意大利帕多瓦大学信息工程系的研究员。她的研究项目集中在持续学习与异常检测的结合上。'
- en: More On This Topic
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Learn Data Cleaning and Preprocessing for Data Science with This Free eBook](https://www.kdnuggets.com/2023/08/learn-data-cleaning-preprocessing-data-science-free-ebook.html)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过这本免费电子书学习数据清理和预处理](https://www.kdnuggets.com/2023/08/learn-data-cleaning-preprocessing-data-science-free-ebook.html)'
- en: '[Harnessing ChatGPT for Automated Data Cleaning and Preprocessing](https://www.kdnuggets.com/2023/08/harnessing-chatgpt-automated-data-cleaning-preprocessing.html)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用 ChatGPT 进行自动化数据清理和预处理](https://www.kdnuggets.com/2023/08/harnessing-chatgpt-automated-data-cleaning-preprocessing.html)'
- en: '[Cleaning and Preprocessing Text Data in Pandas for NLP Tasks](https://www.kdnuggets.com/cleaning-and-preprocessing-text-data-in-pandas-for-nlp-tasks)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Pandas 中清理和预处理文本数据以用于 NLP 任务](https://www.kdnuggets.com/cleaning-and-preprocessing-text-data-in-pandas-for-nlp-tasks)'
- en: '[7 Steps to Mastering Data Cleaning with Python and Pandas](https://www.kdnuggets.com/7-steps-to-mastering-data-cleaning-with-python-and-pandas)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握 Python 和 Pandas 数据清理的 7 个步骤](https://www.kdnuggets.com/7-steps-to-mastering-data-cleaning-with-python-and-pandas)'
- en: '[Collection of Guides on Mastering SQL, Python, Data Cleaning, Data…](https://www.kdnuggets.com/collection-of-guides-on-mastering-sql-python-data-cleaning-data-wrangling-and-exploratory-data-analysis)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握 SQL、Python、数据清理、数据处理等的指南合集](https://www.kdnuggets.com/collection-of-guides-on-mastering-sql-python-data-cleaning-data-wrangling-and-exploratory-data-analysis)'
- en: '[Exploring Data Cleaning Techniques With Python](https://www.kdnuggets.com/2023/04/exploring-data-cleaning-techniques-python.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[探索使用 Python 的数据清洗技术](https://www.kdnuggets.com/2023/04/exploring-data-cleaning-techniques-python.html)'
