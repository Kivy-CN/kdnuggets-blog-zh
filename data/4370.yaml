- en: Algorithms for Advanced Hyper-Parameter Optimization/Tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级超参数优化/调优的算法
- en: 原文：[https://www.kdnuggets.com/2020/11/algorithms-for-advanced-hyper-parameter-optimization-tuning.html](https://www.kdnuggets.com/2020/11/algorithms-for-advanced-hyper-parameter-optimization-tuning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/11/algorithms-for-advanced-hyper-parameter-optimization-tuning.html](https://www.kdnuggets.com/2020/11/algorithms-for-advanced-hyper-parameter-optimization-tuning.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Gowrisankar JG](https://www.linkedin.com/in/gowrisankar-jg/), Software
    Developer at Hexaware**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Gowrisankar JG](https://www.linkedin.com/in/gowrisankar-jg/)，Hexaware的软件开发人员**'
- en: Most Professional Machine Learning practitioners follow the ML Pipeline as a
    standard, to keep their work efficient and to keep the flow of work. A pipeline
    is created to allow data flow from its raw format to some useful information.
    All sub-fields in this pipeline’s modules are equally important for us to produce
    quality results, and one of them is *Hyper-Parameter Tuning.*
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数专业的机器学习从业者遵循机器学习管道作为标准，以保持工作的高效性和流畅性。一个管道的创建是为了让数据从原始格式流向一些有用的信息。管道中所有子领域的模块对我们产生高质量的结果同样重要，其中之一是
    *超参数调优*。
- en: '![Figure](../Images/7a860a576205a10b864f38d935c21a35.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7a860a576205a10b864f38d935c21a35.png)'
- en: '**A Generalized Machine Learning Pipeline**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**一个通用的机器学习管道**'
- en: '* * *'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行IT管理'
- en: '* * *'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Most of us know the best way to proceed with Hyper-Parameter Tuning is to use
    the [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)
    or [RandomSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)
    from the sklearn module. But apart from these algorithms, there are many other
    Advanced methods for Hyper-Parameter Tuning. This is what the article is all about,
    Introduction to Advanced Hyper-Parameter Optimization, Transfer Learning and when
    & how to use these algorithms to make out the best of them.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们大多数人知道，进行超参数调优的最佳方法是使用 [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)
    或 [RandomSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)
    来自sklearn模块。但除了这些算法，还有许多其他高级的超参数调优方法。这正是本文的重点，介绍高级超参数优化、迁移学习以及何时和如何使用这些算法来最大限度地发挥它们的作用。
- en: Both of the algorithms, Grid-Search and Random-Search are instances of Uninformed
    Search. Now, let’s dive deep !!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 两种算法，网格搜索和随机搜索，都是非信息搜索的实例。现在，让我们深入了解吧！！
- en: Uninformed search
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非信息搜索
- en: Here in these algorithms, each iteration of the Hyper-parameter tuning does
    not learn from the previous iterations. This is what allows us to parallelize
    our work. But, this isn’t very efficient and costs a lot of computational power.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些算法中，每次超参数调优的迭代不会从之前的迭代中学习。这使我们能够并行化工作。然而，这并不高效，且消耗大量计算资源。
- en: Random search tries out a bunch of hyperparameters from a uniform distribution
    randomly over the preset list/hyperparameter search space (the number iterations
    is defined). It is good in testing a wide range of values and normally reaches
    to a very good combination very fastly, but the problem is that, it doesn’t guarantee
    to give the best parameter’s combination.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索从均匀分布中随机尝试一组超参数，遍历预设的列表/超参数搜索空间（迭代次数是定义的）。它适合测试广泛的值范围，通常能很快找到一个非常好的组合，但问题是，它不能保证给出最佳参数组合。
- en: On the other hand, Grid search will give the best combination, but it can takes
    a lot of time and the computational cost is high.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，网格搜索将提供最佳组合，但可能需要很多时间，且计算成本很高。
- en: '![Figure](../Images/589a454c302e0cf0015566b12ca8832f.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/589a454c302e0cf0015566b12ca8832f.png)'
- en: '**Searching Pattern of Grid and Random Search**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**网格和随机搜索的搜索模式**'
- en: It may look like grid search is the better option, compared to the random one,
    but bare in mind that when the dimensionality is high, the number of combinations
    we have to search is enormous. For example, to grid-search ten boolean (yes/no)
    parameters you will have to test 1024 (2¹⁰) different combinations. This is the
    reason, why random search is sometimes combined with clever heuristics, is often
    used.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 看似网格搜索比随机搜索更好，但要记住，当维度很高时，我们需要搜索的组合数量是巨大的。例如，要对十个布尔（是/否）参数进行网格搜索，你将不得不测试1024（2¹⁰）种不同的组合。这就是为什么有时会将随机搜索与聪明的启发式方法结合使用的原因。
- en: Why bring Randomness in Grid Search? [Mathematical Explanation]
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么在网格搜索中引入随机性？[数学解释]
- en: Random search is more of a stochastic search/optimization perspective — the
    reason we introduce noise (or some form of stochasticity) into the process is
    to potentially *bounce out* of poor local minima. While this is more typically
    used to explain the intuition in general optimization (like stochastic gradient
    descent for updating parameters, or learning temperature-based models), we can
    think of humans looking through the meta-parameter space as simply a higher-level
    optimization problem. Since most would agree that these dimensional spaces (reasonably
    high) lead to a non-convex form of optimization, we humans, armed even with some
    clever heuristics from the previous research, can get stuck in the local optima.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索更多是从随机搜索/优化的角度来看——我们在过程中引入噪声（或某种形式的随机性）的原因是为了可能*摆脱*糟糕的局部最小值。虽然这通常用于解释一般优化中的直觉（如更新参数的随机梯度下降，或基于温度的模型），我们可以将人类在元参数空间中的搜索视为一个更高级的优化问题。由于大多数人同意这些维度空间（相当高）会导致非凸优化形式，即使我们拥有之前研究中一些聪明的启发式方法，人类仍然可能会陷入局部最优解中。
- en: Therefore, Randomly exploring the search space might give us better coverage,
    and more importantly, it might help us find better local optima.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，随机探索搜索空间可能会提供更好的覆盖率，更重要的是，它可能帮助我们找到更好的局部最优解。
- en: Sofar in Grid and Random Search Algorithms, we have been creating all the models
    at once and combining their scores before deciding the best model at the end.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，在网格和随机搜索算法中，我们一直是在一次创建所有模型并在最后决定最佳模型之前合并它们的评分。
- en: An alternative approach would be to build models sequentially, learning from
    each iteration. This approach is termed as ***Informed Search***.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一种替代方法是按顺序构建模型，从每次迭代中学习。这种方法被称为***信息化搜索***。
- en: 'Informed Method: Coarse to Fine Tuning'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信息化方法：粗到精调优
- en: A basic informed search methodology.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一种基本的信息化搜索方法。
- en: '**The process follows:**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**过程如下：**'
- en: Random search
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机搜索
- en: Find promising areas in the search space
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在搜索空间中找到有前景的区域
- en: Grid search in the smaller area
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在较小的区域进行网格搜索
- en: Continue until optimal score is obtained
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续直到获得最佳分数
- en: You could substitute (3) with random searches before the grid search.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在网格搜索之前用随机搜索来替代（3）。
- en: Why Coarse to Fine?
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么粗到精？
- en: '*Coarse to Fine* *tuning* optimizes and uses the advantages of both grid and
    random search.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*粗到精* *调优* 优化并利用了网格搜索和随机搜索的优势。'
- en: Wide searching capabilities of random search
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机搜索的广泛搜索能力
- en: Deeper search once you know where a good spot is likely to be
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦知道一个好的位置可能出现，进行更深入的搜索
- en: No need to waste time on search spaces that are not giving good results !! Therefore,
    this better utilizes the spending of time and computational efforts, i.e we can
    iterate quickly, also there is boost in the performance.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 无需浪费时间在没有良好结果的搜索空间上！！因此，这样可以更好地利用时间和计算资源，即我们可以快速迭代，同时性能也有所提升。
- en: 'Informed Method: Bayesian Statistics'
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信息化方法：贝叶斯统计
- en: The most popular informed search method is Bayesian Optimization. Bayesian Optimization
    was originally designed to optimize black-box functions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的信息化搜索方法是贝叶斯优化。贝叶斯优化最初是为了优化黑箱函数而设计的。
- en: This is a basic theorem or rule from *Probability Theory and Statistics*, in
    case if you want to brush up and get refreshed with the terms used here, refer [this](https://towardsdatascience.com/basic-probability-theory-and-statistics-3105ab637213).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个基本的定理或规则，来自于*概率论与统计学*，如果你想复习和刷新这里使用的术语，可以参考 [this](https://towardsdatascience.com/basic-probability-theory-and-statistics-3105ab637213)。
- en: '***Bayes Rule | Theorem***'
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***贝叶斯规则 | 定理***'
- en: A statistical method of using *new evidence* to iteratively update our beliefs
    about some *outcome*. In simpler words, it is used to calculate the probability
    of an event based on its association with another event.
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一种使用*新证据*来迭代更新我们对某些*outcome*的信念的统计方法。简单来说，它用于根据事件与另一事件的关联来计算事件的概率。
- en: '![Figure](../Images/adeebe6c343bec80ed4c3044e8073132.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/adeebe6c343bec80ed4c3044e8073132.png)'
- en: Source: [Bayes Theorem in Data Science](https://luminousmen.com/post/data-science-bayes-theorem)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[数据科学中的贝叶斯定理](https://luminousmen.com/post/data-science-bayes-theorem)
- en: LHS is the probability of A, given B has occurred. B is some new evidence. This
    is known as the ‘posterior’.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LHS 是在发生 B 的情况下 A 的概率。B 是一些新的证据。这被称为‘后验’。
- en: RHS is how we calculate this.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RHS 是我们计算这个的方法。
- en: P(A) is the ‘prior’. The initial hypothesis about the event. It is different
    to P(A|B), the P(A|B) is the probability given new evidence.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(A) 是‘先验’，是关于事件的初步假设。它不同于 P(A|B)，P(A|B) 是在新证据给定的情况下的概率。
- en: P(B) is the ‘marginal likelihood’ and it is the probability of observing this
    new evidence.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(B) 是‘边际似然’，即观察到这一新证据的概率。
- en: P(B|A) is the ‘likelihood’ which is the probability of observing the evidence,
    given the event we care about.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(B|A) 是‘似然’，即在我们关心的事件发生的情况下观察证据的概率。
- en: '**Applying the logic of Bayes rule to hyperparameter tuning:**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**将贝叶斯规则的逻辑应用于超参数调优：**'
- en: Pick a hyperparameter combination
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个超参数组合
- en: Build a model
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个模型
- en: Get new evidence (i.e the score of the model)
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取新证据（即模型的得分）
- en: Update our beliefs and chose better hyperparameters next round
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新我们的信念，并在下一轮选择更好的超参数
- en: '*Bayesian hyperparameter tuning is quite new but is very popular for larger
    and more complex hyperparameter tuning tasks as they work well to find optimal *hyperparameter
    combinations in these kinds of situations.'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*贝叶斯超参数调优虽然相对较新，但在较大和更复杂的超参数调优任务中非常受欢迎，因为它们能在这种情况下很好地找到最佳*超参数组合。'
- en: Note
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more complex cases you might want to dig a bit deeper and explore all the
    details about Bayesian optimization. Bayesian optimization can only work on continuous
    hyper-parameters, and not categorical ones.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的情况，你可能需要深入探讨所有关于贝叶斯优化的细节。贝叶斯优化只能在连续的超参数上工作，而不能在分类超参数上工作。
- en: Bayesian Hyper-parameter Tuning with HyperOpt
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 HyperOpt 进行贝叶斯超参数调优
- en: HyperOpt package, uses a form of Bayesian optimization for parameter tuning
    that allows us to get the best parameters for a given model. It can optimize a
    model with hundreds of parameters on a very large scale.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: HyperOpt 包使用贝叶斯优化的一种形式进行参数调优，允许我们为给定的模型获取最佳参数。它可以在非常大规模上优化具有数百个参数的模型。
- en: '![Figure](../Images/8e2d799ec43e4a30d5c1c2c297ec1e17.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8e2d799ec43e4a30d5c1c2c297ec1e17.png)'
- en: '[HyperOpt](http://hyperopt.github.io/hyperopt/): [**Distributed Hyper-parameter
    Optimization**](https://github.com/hyperopt/hyperopt)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[HyperOpt](http://hyperopt.github.io/hyperopt/): [**分布式超参数优化**](https://github.com/hyperopt/hyperopt)'
- en: To know more about this library and the parameters of HyperOpt library feel
    free to visit [*here*](https://www.analyticsvidhya.com/blog/2020/09/alternative-hyperparameter-optimization-technique-you-need-to-know-hyperopt/).
    And visit [*here*](https://machinelearningmastery.com/hyperopt-for-automated-machine-learning-with-scikit-learn/)for
    a quick tutorial with adequate explanation on how to use HyperOpt for Regression
    and Classification.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于这个库和 HyperOpt 库的参数，可以随时访问 [*这里*](https://www.analyticsvidhya.com/blog/2020/09/alternative-hyperparameter-optimization-technique-you-need-to-know-hyperopt/)。另外可以访问
    [*这里*](https://machinelearningmastery.com/hyperopt-for-automated-machine-learning-with-scikit-learn/)
    进行快速教程，了解如何使用 HyperOpt 进行回归和分类。
- en: Introducing the HyperOpt package.
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍 HyperOpt 包。
- en: 'To undertake Bayesian hyperparameter tuning we need to:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行贝叶斯超参数调优，我们需要：
- en: 'Set the Domain: Our Grid i.e. search space (with a bit of a twist)'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置领域：我们的网格，即搜索空间（略有变化）
- en: 'Set the Optimization algorithm (default: TPE)'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置优化算法（默认：TPE）
- en: 'Objective function to minimize: we use “*1-Accuracy”*'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标函数以最小化：“*1-准确率*”
- en: Know more about the Optimization Algorithm used, Original Paper of TPE (Tree
    of Parzen Estimators)
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 了解使用的优化算法，TPE（Parzen 估计树）的原始论文
- en: Sample Code for using HyperOpt [ Random Forest ]
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 HyperOpt 的示例代码 [ 随机森林 ]
- en: HyperOpt does not use point values on the grid but instead, each point represents
    probabilities for each hyperparameter value. Here, simple uniform distribution
    is used, but there are many more if you check the [documentation](http://hyperopt.github.io/hyperopt/getting-started/search_spaces/).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: HyperOpt 不使用网格上的点值，而是每个点代表每个超参数值的概率。在这里使用了简单的均匀分布，但如果你查看[文档](http://hyperopt.github.io/hyperopt/getting-started/search_spaces/)，你会发现还有更多选择。
- en: '**HyperOpt implemented on Random Forest**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**HyperOpt 在随机森林上的实现**'
- en: To really see this in action !!* try on a larger search space, with more trials,
    more CVs and a larger dataset size.*
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 要真正看到这一点的实际效果，*尝试在更大的搜索空间中进行更多试验、更多交叉验证以及更大的数据集*。
- en: '![Image for post](../Images/f20c865d8a22dd6693482e7cdad8e6b4.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![Image for post](../Images/f20c865d8a22dd6693482e7cdad8e6b4.png)'
- en: 'For practical implementation of HyperOpt refer:'
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有关 HyperOpt 的实际实现，请参考：
- en: '[1] [Hyperopt Bayesian Optimization for Xgboost and Neural network](https://medium.com/analytics-vidhya/hyperparameter-tuning-hyperopt-bayesian-optimization-for-xgboost-and-neural-network-8aedf278a1c9)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [Xgboost 和神经网络的 Hyperopt 贝叶斯优化](https://medium.com/analytics-vidhya/hyperparameter-tuning-hyperopt-bayesian-optimization-for-xgboost-and-neural-network-8aedf278a1c9)'
- en: '[2] [Tuning using HyperOpt in python](http://www.17bigdata.com/python%E8%B0%83%E5%8F%82%E7%A5%9E%E5%99%A8hyperopt/)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [在 Python 中使用 HyperOpt 调优](http://www.17bigdata.com/python%E8%B0%83%E5%8F%82%E7%A5%9E%E5%99%A8hyperopt/)'
- en: Curious to know why XGBoost has high potential in winning competitions ?? Read
    the below article to expand your knowledge !!
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 好奇为何 XGBoost 在比赛中有很高的潜力吗？阅读下面的文章扩展你的知识！
- en: '[**XGBoost — Queen of Boosting Algorithms?**](https://levelup.gitconnected.com/xgboost-queens-of-boosting-algorithms-f270894c6aa5)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[**XGBoost — 提升算法之皇后？**](https://levelup.gitconnected.com/xgboost-queens-of-boosting-algorithms-f270894c6aa5)'
- en: Kaggler’s Favo Algorithm | Understanding How & Why XGBoost is used to win Kaggle
    competitions
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggler 的 Favo 算法 | 了解 XGBoost 如何以及为何用于赢得 Kaggle 比赛
- en: 'Informed Method: Genetic Algorithms'
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 知识方法：遗传算法
- en: '***Why does this work well?***'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '***为什么这效果这么好？***'
- en: It allows us to learn from previous iterations, just like Bayesian hyperparameter
    tuning.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它允许我们从之前的迭代中学习，就像贝叶斯超参数调整一样。
- en: It has the additional advantage of some *randomness*
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它还有一个额外的优点，即具有一定的*随机性*。
- en: TPOT will automate the most tedious part of machine learning by intelligently
    exploring thousands of possible pipelines to find the best one for your data.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TPOT 将通过智能探索成千上万种可能的管道来自动化机器学习中最乏味的部分，以找到最适合你数据的管道。
- en: '**A useful library for genetic hyperparameter tuning: TPOT**'
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**一个用于遗传超参数调整的有用库：TPOT**'
- en: '**TPOT is a Python Automated Machine Learning tool that optimizes machine learning
    pipelines using genetic programming.**'
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**TPOT 是一个 Python 自动化机器学习工具，通过遗传编程优化机器学习管道。**'
- en: ''
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Consider TPOT your **Data Science Assistant** for advanced optimization.
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将 TPOT 视为你的**数据科学助手**，用于高级优化。
- en: Pipelines not only include the model (or multiple models) but also work on features
    and other aspects of the process. Plus it returns the Python code of the pipeline
    for you! TPOT is designed to run for many hours to find the best model. You should
    have a much larger population and offspring size as well as hundreds of more generations
    to find a good model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 管道不仅包括模型（或多个模型），还涉及特征和过程的其他方面。除此之外，它还会返回管道的 Python 代码给你！TPOT 设计为运行许多小时，以找到最佳模型。你应该拥有更大的种群和后代规模，并且需要更多的代数来找到一个好的模型。
- en: TPOT Components ( Key Arguments )
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TPOT 组件（关键参数）
- en: '***generations ***— Iterations to run training for'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***generations*** — 进行训练的迭代次数'
- en: '***population_size*** — The number of models to keep after each iteration'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***population_size*** — 每次迭代后保留的模型数量'
- en: '***offspring_size*** — Number of models to produce in each iteration'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***offspring_size*** — 每次迭代中生成的模型数量'
- en: '***mutation_rate*** — The proportion of pipelines to apply randomness to'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***mutation_rate*** — 施加随机性的管道比例'
- en: '***crossover_rate*** — The proportion of pipelines to breed each iteration'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***crossover_rate*** — 每次迭代中交配的管道比例'
- en: '***scoring* **— The function to determine the best models'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***scoring*** — 确定最佳模型的函数'
- en: '***cv*** — Cross-validation strategy to use'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***cv*** — 要使用的交叉验证策略'
- en: TPOT Classifier
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: TPOT 分类器
- en: We will keep default values for ***mutation_rate* **and ***crossover_rate* **as
    they are best left to the default without deeper knowledge on genetic programming.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保持默认值***mutation_rate*** 和 ***crossover_rate***，因为在没有深入了解遗传编程的情况下，默认值是最好的选择。
- en: 'Notice: No algorithm-specific hyperparameters?'
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意：没有特定于算法的超参数？
- en: Since TPOT is an open-source library for performing *AutoML *in Python.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 TPOT 是一个开源库，用于在 Python 中执行*AutoML*。
- en: '***AutoML ??***'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '***AutoML ??***'
- en: Automated Machine Learning (AutoML) refers to techniques for automatically discovering
    well-performing models for predictive modeling tasks with very little user involvement.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化机器学习（AutoML）指的是自动发现性能良好的模型用于预测建模任务的技术，几乎无需用户参与。
- en: '![Figure](../Images/2fa3dc19bdd1bc023f5d5bb7aaef5d69.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/2fa3dc19bdd1bc023f5d5bb7aaef5d69.png)'
- en: '**Output for the above code snippet**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**上述代码片段的输出**'
- en: TPOT is quite unstable when not run for a reasonable amount of time. The below
    code snippets shows the instability of TPOT. Here, only the random state has been
    changed in the below three codes, but the Output shows major differences in choosing
    the pipeline, i.e. model and it’s hyperparameters.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当 TPOT 运行时间不足时，会显得相当不稳定。以下代码片段展示了 TPOT 的不稳定性。这里仅在下面的三段代码中更改了随机状态，但输出结果在选择管道，即模型及其超参数方面显示了显著差异。
- en: You can see in the output the score produced by the chosen model (in this case
    a version of Naive Bayes) over each generation, and then the final accuracy score
    with the hyperparameters chosen for the final model. This is a great first example
    of using TPOT for automated hyperparameter tuning. You can now extend this on
    your own and build great machine learning models!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在输出中看到所选模型（在这种情况下是朴素贝叶斯的一个版本）在每一代中的得分，然后是最终模型选择的超参数的最终准确度。这是使用 TPOT 进行自动超参数调整的一个很好的初步示例。你现在可以在自己的项目中扩展这一点，构建出优秀的机器学习模型！
- en: '**To understand more about TPOT:**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**要了解更多关于 TPOT 的信息：**'
- en: '[1] [TPOT for Automated Machine Learning in Python](https://machinelearningmastery.com/tpot-for-automated-machine-learning-in-python/)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [TPOT 在 Python 中的自动化机器学习](https://machinelearningmastery.com/tpot-for-automated-machine-learning-in-python/)'
- en: '[2] For more information in using TPOT, visit the [documentation](http://epistasislab.github.io/tpot/using/).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 要获取更多关于 TPOT 的信息，请访问 [文档](http://epistasislab.github.io/tpot/using/)。'
- en: Summary
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: In informed search, each iteration learns from the last, whereas in Grid and
    Random, modelling is all done at once and then the best is picked. In case for
    small datasets, GridSearch or RandomSearch would be fast and sufficient.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在已知搜索中，每次迭代都从上一次学习，而在 Grid 和 Random 中，建模是一次完成的，然后选择最佳的。对于小数据集，GridSearch 或 RandomSearch
    会更快且足够。
- en: AutoML approaches provide a neat solution to properly select the required hyperparameters
    that improve the model’s performance.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML 方法提供了一个整洁的解决方案，以正确选择能够提高模型性能的超参数。
- en: '**Informed methods explored were:**'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**探索的已知方法包括：**'
- en: ‘Coarse to Fine’ (Iterative random then grid search).
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ‘粗到细’（迭代随机然后网格搜索）。
- en: Bayesian hyperparameter tuning, updating beliefs using evidence on model performance
    (HyperOpt).
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 贝叶斯超参数调整，通过模型性能的证据更新信念（HyperOpt）。
- en: Genetic algorithms, evolving your models over generations (TPOT).
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遗传算法，通过世代演化模型（TPOT）。
- en: I hope you’ve learned some useful methodologies for your future work undertaking
    hyperparameter tuning in Python!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你在进行 Python 超参数调整的未来工作中学到了一些有用的方法！
- en: '[**Create REST API in Minutes With Go / Golang**](https://medium.com/swlh/create-rest-api-in-minutes-with-go-golang-c4a2c6279721)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[**用 Go / Golang 在几分钟内创建 REST API**](https://medium.com/swlh/create-rest-api-in-minutes-with-go-golang-c4a2c6279721)'
- en: In this article, there is a short comparison between different routers and then
    the walk-through to create a REST API…
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，有一个简短的路由器比较，然后是创建 REST API 的详细步骤……
- en: If you are curious to know about Golang’s Routers and want to try out a simple
    web development project using Go, I suggest to read the above article.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对 Golang 的路由器感到好奇，并且想尝试一个简单的 Web 开发项目，我建议阅读上面的文章。
- en: '*For more informative articles from me, follow me on *[*medium*](https://medium.com/@jggowrisankar)*.*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*欲了解更多我的信息文章，请在*[*medium*](https://medium.com/@jggowrisankar)*上关注我。*'
- en: And if you’re passionate about Data Science/Machine Learning, feel free to add
    me on [*LinkedIn*](http://linkedin.com/in/gowrisankar-jg/)*.*
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对数据科学/机器学习充满热情，可以在[*LinkedIn*](http://linkedin.com/in/gowrisankar-jg/)上添加我为好友。*
- en: '![Image for post](../Images/3013180f2562a1fd674e3f5231c77bc4.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![帖子图片](../Images/3013180f2562a1fd674e3f5231c77bc4.png)'
- en: '**References**'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[1] [Bayesian Hyperparameter Optimization — A Primer](https://www.wandb.com/articles/bayesian-hyperparameter-optimization-a-primer)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [贝叶斯超参数优化 — 入门指南](https://www.wandb.com/articles/bayesian-hyperparameter-optimization-a-primer)'
- en: '[2] [How To Make Deep Learning Models That Don’t Suck](https://nanonets.com/blog/hyperparameter-optimization/)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [如何制作不差劲的深度学习模型](https://nanonets.com/blog/hyperparameter-optimization/)'
- en: '[3] [Algorithms for Hyper-Parameter Optimization](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [超参数优化算法](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)'
- en: '[4] [Grid Search and Bayesian Hyperparameter Optimization](https://www.r-bloggers.com/2020/03/grid-search-and-bayesian-hyperparameter-optimization-using-tune-and-caret-packages/)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [Grid Search 和贝叶斯超参数优化](https://www.r-bloggers.com/2020/03/grid-search-and-bayesian-hyperparameter-optimization-using-tune-and-caret-packages/)'
- en: '[5] [Tree-structured Parzen Estimator](https://optunity.readthedocs.io/en/latest/user/solvers/TPE.html)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [树结构 Parzen 估计器](https://optunity.readthedocs.io/en/latest/user/solvers/TPE.html)'
- en: '[6] [Informed Search - Hyperparameter Tuning](https://learn.datacamp.com/courses/hyperparameter-tuning-in-python)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [知情搜索 - 超参数调优](https://learn.datacamp.com/courses/hyperparameter-tuning-in-python)'
- en: '**Bio: [Gowrisankar JG](https://www.linkedin.com/in/gowrisankar-jg/)** (**[@jg_gowrisankar](https://twitter.com/jg_gowrisankar)**)
    is passionate about Data Science, Data Analytics, Machine Learning and #NLP, and
    is a Software Developer at Hexaware.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介：[Gowrisankar JG](https://www.linkedin.com/in/gowrisankar-jg/)** (**[@jg_gowrisankar](https://twitter.com/jg_gowrisankar)**)
    对数据科学、数据分析、机器学习和#NLP充满热情，并在Hexaware担任软件开发人员。'
- en: '[Original](https://medium.com/analytics-vidhya/algorithms-for-advanced-hyper-parameter-optimization-tuning-cebea0baa5d6).
    Reposted with permission.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/analytics-vidhya/algorithms-for-advanced-hyper-parameter-optimization-tuning-cebea0baa5d6)。经许可转载。'
- en: '**Related:**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Automated Machine Learning: The Free eBook](/2020/05/automated-machine-learning-free-ebook.html)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动化机器学习：免费电子书](/2020/05/automated-machine-learning-free-ebook.html)'
- en: '[Top Python Libraries for Data Science, Data Visualization & Machine Learning](/2020/11/top-python-libraries-data-science-data-visualization-machine-learning.html)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学、数据可视化和机器学习的顶级 Python 库](/2020/11/top-python-libraries-data-science-data-visualization-machine-learning.html)'
- en: '[Build Your Own AutoML Using PyCaret 2.0](/2020/08/build-automl-pycaret.html)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 PyCaret 2.0 构建你自己的 AutoML](/2020/08/build-automl-pycaret.html)'
- en: More On This Topic
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Hyperparameter Optimization: 10 Top Python Libraries](https://www.kdnuggets.com/2023/01/hyperparameter-optimization-10-top-python-libraries.html)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[超参数优化：10 个顶级 Python 库](https://www.kdnuggets.com/2023/01/hyperparameter-optimization-10-top-python-libraries.html)'
- en: '[Hyperparameter Tuning Using Grid Search and Random Search in Python](https://www.kdnuggets.com/2022/10/hyperparameter-tuning-grid-search-random-search-python.html)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Grid Search 和 Random Search 进行超参数调优](https://www.kdnuggets.com/2022/10/hyperparameter-tuning-grid-search-random-search-python.html)'
- en: '[Hyperparameter Tuning: GridSearchCV and RandomizedSearchCV, Explained](https://www.kdnuggets.com/hyperparameter-tuning-gridsearchcv-and-randomizedsearchcv-explained)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[超参数调优：GridSearchCV 和 RandomizedSearchCV 解析](https://www.kdnuggets.com/hyperparameter-tuning-gridsearchcv-and-randomizedsearchcv-explained)'
- en: '[3 Research-Driven Advanced Prompting Techniques for LLM Efficiency…](https://www.kdnuggets.com/3-research-driven-advanced-prompting-techniques-for-llm-efficiency-and-speed-optimization)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3 种研究驱动的高级提示技术，以提高 LLM 效率…](https://www.kdnuggets.com/3-research-driven-advanced-prompting-techniques-for-llm-efficiency-and-speed-optimization)'
- en: '[Machine Learning Pipeline Optimization with TPOT](https://www.kdnuggets.com/2021/05/machine-learning-pipeline-optimization-tpot.html)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 TPOT 进行机器学习管道优化](https://www.kdnuggets.com/2021/05/machine-learning-pipeline-optimization-tpot.html)'
- en: '[SQL Query Optimization Techniques](https://www.kdnuggets.com/2023/03/sql-query-optimization-techniques.html)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQL 查询优化技术](https://www.kdnuggets.com/2023/03/sql-query-optimization-techniques.html)'
