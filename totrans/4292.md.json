["```py\nfrom urllib.request import urlopen\nimport pandas as pd\n\ninput_data = urlopen('https://featurelabs-static.s3.amazonaws.com/spam_text_messages_modified.csv')\ndata = pd.read_csv(input_data)\nX = data.drop(['Category'], axis=1)\ny = data['Category']display(X.head())\n```", "```py\ny.value_counts().plot.pie(figsize=(10,10))\n```", "```py\nimport woodwork as ww\nX = ww.DataTable(X)\n\n# Note: We could have also manually set the Message column to \n# natural language if Woodwork had not automatically detected\nfrom evalml.utils import infer_feature_types\nX = infer_feature_types(X, {'Message': 'NaturalLanguage'})\n```", "```py\ny = ww.DataColumn(y)\n```", "```py\nfrom evalml.preprocessing import split_data\n\nX_train, X_holdout, y_train, y_holdout = split_data(X, y, problem_type='binary', test_size=0.2)\n```", "```py\nautoml = AutoMLSearch(X_train=X_train, y_train=y_train, problem_type='binary')\n```", "```py\nautoml.search()\n```", "```py\n# rankings are ordered from best to worst, \n# so 0th index is the best pipeline\n\nbest_pipeline_id = automl.rankings.iloc[0][\"id\"])\nautoml.describe_pipeline(best_pipeline_id)\n```", "```py\n# We can also grab the best performing pipeline like this\nautoml.best_pipeline\nautoml.graph(automl.best_pipeline)\n```", "```py\n>>> scores = best_pipeline.score(X_holdout, y_holdout,  objectives=evalml.objectives.get_core_objectives('binary') + ['recall'])\n>>> scores\nOrderedDict([('MCC Binary', 0.9278003804626707),\n\t\t\t ('Log Loss Binary', 0.1137465525638786),\n\t\t\t ('AUC', 0.9823022077397945),\n             ('Precision', 0.9716312056737588),\n             ('F1', 0.9448275862068964),\n             ('Balanced Accuracy Binary', 0.9552772006397513),\n             ('Accuracy Binary', 0.9732441471571907),\n             ('Recall', 0.9194630872483222)])\n```", "```py\nfrom evalml.utils import infer_feature_types\n\n# manually set \"Message\" feature as categorical \nX = infer_feature_types(X, {'Message': 'Categorical'}) \n\nX_train, X_holdout, y_train, y_holdout = split_data(X, y, problem_type='binary', test_size=0.2, random_seed=0)\n\nautoml_no_text = AutoMLSearch(X_train=X_train, y_train=y_train,                              problem_type='binary')\nautoml_no_text.search()\n```", "```py\n>>> best_pipeline_no_text = automl_no_text.best_pipeline\n>>> scores = best_pipeline_no_text.score(X_holdout, y_holdout,\nobjectives=evalml.objectives.get_core_objectives('binary') + ['recall'])\n>>> scores\n\nOrderedDict([('MCC Binary', 0.0710465299061946),\n\t\t\t ('Log Loss Binary', 0.5576891229036224),\n\t\t\t ('AUC', 0.5066740407467751),\n             ('Precision', 1.0),\n             ('F1', 0.013333333333333332),\n             ('Balanced Accuracy Binary', 0.5033557046979866),\n             ('Accuracy Binary', 0.7525083612040134),\n             ('Recall', 0.006711409395973154)])\n```", "```py\nautoml_no_text.graph(best_pipeline_no_text)\n```"]