["```py\n# Download the TensorFlow Serving Docker image and repo\ndocker pull tensorflow/serving\n\ngit clone https://github.com/tensorflow/serving\n# Location of demo models\nTESTDATA=\"$(pwd)/serving/tensorflow_serving/servables/tensorflow/testdata\"\n\n# Start TensorFlow Serving container and open the REST API port\ndocker run -t --rm -p 8501:8501 \\\n    -v \"$TESTDATA/saved_model_half_plus_two_cpu:/models/half_plus_two\" \\\n    -e MODEL_NAME=half_plus_two \\\n    tensorflow/serving &\n\n# Query the model using the predict API\ncurl -d '{\"instances\": [1.0, 2.0, 5.0]}' \\\n    -X POST http://localhost:8501/v1/models/half_plus_two:predict\n\n# Returns => { \"predictions\": [2.5, 3.0, 4.5] }\n```", "```py\ndocker run -t --rm -p 8501:8501 -v \"$(pwd)/mobilenet_v2_test:/models/mobilenet_v2_test\" -e MODEL_NAME=mobilenet_v2_test tensorflow/serving &\n\n```", "```py\nimport json\nimport requests\nimport base64\n\ndata = {}\nwith open('../../Downloads/imagenes-osos-panda.jpg', mode='rb') as file:\n    img = file.read()\ndata = {\"inputs\":[{\"b64\":base64.encodebytes(img).decode(\"utf-8\")}]}\n\n# Making the request\nr = requests.post(\"http://localhost:8501/v1/models/mobilenet_v2_test:predict\", data=json.dumps(data))\nr.content\n# And returns:\n# b'{\\n    \"outputs\": [\\n        \"giant panda\"\\n    ]\\n}'\n```"]