- en: 'From Zero to Hero: Create Your First ML Model with PyTorch'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![From Zero to Hero: Create Your First ML Model with PyTorch](../Images/aac9a7fec562df82c6895dad92e6871c.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch is the most widely used Python-based Deep Learning framework. It provides
    tremendous support for all machine learning architectures and data pipelines.
    In this article, we go through all the framework basics to get you started with
    implementing your algorithms.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'All machine learning implementations have 4 major steps:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Handling**'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Architecture**'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training Loop**'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation**'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We go through all these steps while implementing our own MNIST image classification
    model in PyTorch. This will familiarize you with the general flow of a machine-learning
    project.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Imports
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: torch.nn module provides support for neural network architectures and has built-in
    implementations for popular layers such as Dense Layers, Convolutional Neural
    Networks, and many more.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: torch.optim provides implementations for optimizers such as Stochastic Gradient
    Descent and Adam.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Other utility modules are available for data handling support and transformations.
    We will go through each in more detail later.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Declare Hyperparameters
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each hyperparameter will be explained further where appropriate. However, it
    is a best practice to declare them at the top of our file for ease of change and
    understanding.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Data Loading and Transforms
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: MNIST is a popular image classification dataset, provided by default in PyTorch.
    It consists of grayscale images of 10 hand-written digits from 0 to 9\. Each image
    is of size 28 pixels by 28 pixels, and the dataset contains 60000 training and
    10000 testing images.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: We load the training and testing dataset separately, denoted by the train argument
    in the MNIST initialization function. The root argument declares the directory
    in which the dataset is to be downloaded.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we also pass an additional transform argument. For PyTorch, all inputs
    and outputs are supposed to be in Torch.Tensor format. This is equivalent to a
    numpy.ndarray in numpy. This tensor format provides additional support for data
    manipulation. However, the MNIST data we load from is in the PIL.Image format.
    We need to transform the images into PyTorch-compatible tensors. Accordingly,
    we pass the following transforms:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们还传递了一个额外的转换参数。对于 PyTorch，所有输入和输出应该是 Torch.Tensor 格式。这相当于 numpy 中的 numpy.ndarray。这个张量格式提供了额外的数据操作支持。然而，我们加载的
    MNIST 数据是 PIL.Image 格式。我们需要将图像转换为 PyTorch 兼容的张量。因此，我们传递了以下转换：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The ToTensor() transform converts images to tensor format. Next, we pass an
    additional Lambda transform. The Lambda function allows us to implement custom
    transforms. Here we declare a function to flatten the input. The images are of
    size 28x28, but, we flatten them i.e. convert them to a single-dimensional array
    of size 28x28 or 784\. This will be important later when we implement our model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ToTensor() 转换将图像转换为张量格式。接下来，我们传递一个额外的 Lambda 转换。Lambda 函数允许我们实现自定义转换。在这里，我们声明一个函数来展平输入。图像的大小为
    28x28，但我们将其展平，即转换为大小为 28x28 或 784 的一维数组。稍后在实现模型时，这一点将很重要。
- en: The Compose function sequentially combines all the transforms. Firstly, the
    data is converted to tensor format and then flattened to a one-dimensional array.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Compose 函数按顺序组合所有转换。首先，数据被转换为张量格式，然后展平为一维数组。
- en: Dividing our Data into Batches
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据划分为批次
- en: For computational and training purposes, we can not pass the complete dataset
    into the model at once. We need to divide our dataset into mini-batches that will
    be fed to the model in sequential order. This allows faster training and adds
    randomness to our dataset, which can assist in stable training.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 出于计算和训练目的，我们不能一次将整个数据集传递到模型中。我们需要将数据集划分为小批量，按顺序喂给模型。这可以加快训练速度，并为我们的数据集添加随机性，这有助于稳定训练。
- en: PyTorch provides built-in support for batching our data. The DataLoader class
    from torch. utils module can create batches of data, given a torch dataset module.
    As above, we already have the dataset loaded.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 提供了对数据批处理的内置支持。来自 torch.utils 模块的 DataLoader 类可以创建数据批次，给定一个 torch 数据集模块。如上所述，我们已经加载了数据集。
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We pass the dataset to our dataloader, and our batch_size hyperparameter as
    initialization arguments. This creates an iterable data loader, so we can easily
    iterate over each batch using a simple for loop.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据集传递给我们的数据加载器，并将 batch_size 超参数作为初始化参数。这创建了一个可迭代的数据加载器，因此我们可以使用简单的 for 循环轻松遍历每个批次。
- en: Our initial image was of size (784, ) with a single associated label. The batching
    then combines different images and labels in a batch. For example, if we have
    a batch size of 64, the input size in a batch will become (64, 784) and we will
    have 64 associated labels for each batch.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初始图像的大小为 (784, )，带有一个相关标签。批处理将不同的图像和标签组合在一起。例如，如果我们有一个批量大小为 64，那么批次中的输入大小将变为
    (64, 784)，并且我们将为每个批次有 64 个相关标签。
- en: We also shuffle the training batch, which changes the images within a batch
    for each epoch. It allows for stable training and faster convergence of our model
    parameters.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还会打乱训练批次，这会在每个 epoch 更改批次中的图像。这有助于稳定训练并加快模型参数的收敛。
- en: Defining our Classification Model
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义我们的分类模型
- en: We use a simple implementation consisting of 3 hidden layers. Although simple,
    this can give you a general understanding of combining different layers for more
    complex implementations.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个由 3 个隐藏层组成的简单实现。虽然简单，但这可以让你对将不同层组合在一起以实现更复杂的实现有一个大致了解。
- en: As described above, we have an input tensor of size (784, ) and 10 different
    output classes, one for each digit from 0-9.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，我们有一个大小为 (784, ) 的输入张量和 10 个不同的输出类，每个类对应一个从 0 到 9 的数字。
- en: '**** For model implementation, we can ignore the batch dimension.**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**** 对于模型实现，我们可以忽略批次维度。**'
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Firstly, the model must inherit from the torch.nn.Module class. This provides
    basic functionality for neural network architectures. We then must implement two
    methods, __init__ and forward.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，模型必须继承自 torch.nn.Module 类。这为神经网络架构提供了基本功能。然后，我们必须实现两个方法，__init__ 和 forward。
- en: In the __init__ method, we declare all layers the model will use. We use Linear
    (also called Dense) layers provided by PyTorch. The first layer maps the input
    to 512 neurons. We can pass input_size as a model parameter, so we can later use
    it for input of different sizes as well. The second layer maps the 512 neurons
    to 256\. The third hidden layer maps the 256 neurons from the previous layer to
    128\. The final layer then finally reduces to the output size. Our output size
    will be a tensor of size (10, ) because we are predicting ten different numbers.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![From Zero to Hero: Create Your First ML Model with PyTorch](../Images/892683a58550b21fc5cd9d81645de015.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we initialize a ReLU activation layer for non-linearity in our model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: The forward function receives images and we provide code for processing the
    input. We use the layers declared and sequentially pass our input through each
    layer, with an intermediate ReLU activation layer.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: In our main code, we can then initialize the model providing it with the input
    and output size for our dataset.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once initialized, we change the model device (which can be either CUDA GPU or
    CPU). We checked for our device when we initialized the hyperparameters. Now,
    we have to manually change the device for our tensors and model layers.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Training Loop
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Firstly, we must declare our loss function and optimizer that will be used to
    optimize our model parameters.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Firstly, we must declare our loss function and optimizer that will be used to
    optimize our model parameters.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: We use the Cross-Entropy Loss that is primarily used for multi-label classification
    models. It first applies softmax to the predictions and calculates the given target
    labels and predicted values.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Adam optimizer is the most-used optimizer function that allows stable gradient
    descent toward convergence. It is the default optimizer choice nowadays and provides
    satisfactory results. **We pass our model parameters as an argument that denotes
    the weights that will be optimized.**
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: For our training loop, we build step-by-step and fill in missing portions as
    we gain an understanding.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'As a starting point, we iterate over the complete dataset multiple times (called
    epoch), and optimize our model each time. However, we have divided our data into
    batches. Then, for every epoch, we must iterate over each batch as well. The code
    for this will look as below:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, we can train the model given a single input batch. Our batch consists of
    images and labels. Firstly, we must separate each of these. Our model only requires
    images as input to make predictions. We then compare the predictions with the
    true labels, to estimate our model’s performance.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We pass the batch of images directly to the model that will be processed by
    the forward function defined within the model. Once we have our predictions, we
    can optimize our model weights.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimization code looks as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Using the above code, we can compute all the backpropagation gradients and optimize
    the model weights using the Adam optimizer. All the above codes combined can train
    our model toward convergence.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'Complete training loop looks as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The loss gradually decreases and reaches close to 0\. Then, we can evaluate
    the model on the test dataset we declared initially.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating our Model Performace
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Similar to the training loop, we iterate over each batch in the test dataset
    for evaluation. We generate predictions for the inputs. However, for evaluation,
    we only need the label with the highest probability. The argmax function provides
    this functionality to obtain the index of the value with the highest value in
    our predictions array.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: For the accuracy score, we can then compare if the predicted label matches the
    true target label. We then compute the accuracy of the number of correct labels
    divided by the total predicted labels.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Results
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I only trained the model for five epochs and achieved a test accuracy of over
    96 percent, as compared to 10 percent accuracy before training. The image below
    shows the model predictions after training five epochs.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![From Zero to Hero: Create Your First ML Model with PyTorch](../Images/eb5127ca41a590e3e22753e37e78bab3.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: There you have it. You have now implemented a model from scratch that can differentiate
    hand-written digits just from image pixel values.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: This in no way is a comprehensive guide to PyTorch but it does provide you with
    a general understanding of structure and data flow in a machine learning project.
    This is nonetheless sufficient knowledge to get you started with implementing
    state-of-the-art architectures in deep learning.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Complete Code
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The complete code is as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '**model.py:**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**main.py**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**[Muhammad Arham](https://www.linkedin.com/in/muhammad-arham-a5b1b1237/)**
    is a Deep Learning Engineer working in Computer Vision and Natural Language Processing.
    He has worked on the deployment and optimizations of several generative AI applications
    that reached the global top charts at Vyro.AI. He is interested in building and
    optimizing machine learning models for intelligent systems and believes in continual
    improvement.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Breaking the Data Barrier: How Zero-Shot, One-Shot, and Few-Shot…](https://www.kdnuggets.com/2023/08/breaking-data-barrier-zeroshot-oneshot-fewshot-learning-transforming-machine-learning.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Zero-shot Learning, Explained](https://www.kdnuggets.com/2022/12/zeroshot-learning-explained.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Bug That Can Make You a Data Science Hero](https://www.kdnuggets.com/2022/03/bug-make-data-science-hero.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deploying Your First Machine Learning Model](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Step-by-Step Tutorial to Building Your First Machine Learning Model](https://www.kdnuggets.com/step-by-step-tutorial-to-building-your-first-machine-learning-model)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PyTorch Tips to Boost Your Productivity](https://www.kdnuggets.com/2023/08/pytorch-tips-boost-productivity.html)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提升生产力的 PyTorch 技巧](https://www.kdnuggets.com/2023/08/pytorch-tips-boost-productivity.html)'
