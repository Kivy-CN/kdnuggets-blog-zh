- en: Extractive Summarization with LLM using BERT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 BERT 的抽取式摘要
- en: 原文：[https://www.kdnuggets.com/extractive-summarization-with-llm-using-bert](https://www.kdnuggets.com/extractive-summarization-with-llm-using-bert)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/extractive-summarization-with-llm-using-bert](https://www.kdnuggets.com/extractive-summarization-with-llm-using-bert)
- en: '![Extractive Summarization with LLM using BERT](../Images/c1bd380b5cea926954e185cf7890779f.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![使用 BERT 的抽取式摘要](../Images/c1bd380b5cea926954e185cf7890779f.png)'
- en: Image by Editor
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由编辑提供
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In today's fast-paced world, we're bombarded with more information than we can
    handle. We’re increasingly getting used to receiving more information in less
    time, leading to frustration when having to read extensive documents or books.
    That's where extractive summarization steps in. To the heart of a text, the process
    pulls out key sentences from an article, piece, or page to give us a snapshot
    of its most important points.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今快节奏的世界中，我们被大量信息轰炸。我们越来越习惯于在更短的时间内接收更多信息，当需要阅读大量文档或书籍时，这会导致沮丧。此时，抽取式摘要就发挥了作用。这个过程从文章、片段或页面中提取关键句子，为我们提供其最重要点的快照。
- en: For anyone needing to understand big documents without reading every word, this
    is a game changer.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何需要在不逐字阅读每一个单词的情况下理解大型文档的人来说，这是一项颠覆性的改变。
- en: In this article, we delve into the fundamentals and applications of extractive
    summarization. We'll examine the role of Large Language Models, especially [BERT](https://www.exxactcorp.com/blog/Deep-Learning/how-do-bert-transformers-work) (Bidirectional
    Encoder Representations from Transformers),  in enhancing the process. The article
    will also include a hands-on tutorial on using BERT for extractive summarization,
    showcasing its practicality in condensing large text volumes into informative
    summaries.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将深入探讨抽取式摘要的基础和应用。我们将审视大型语言模型，特别是 [BERT](https://www.exxactcorp.com/blog/Deep-Learning/how-do-bert-transformers-work) （双向编码器表示模型）在提升这一过程中的作用。文章还将包括一个关于使用
    BERT 进行抽取式摘要的实践教程，展示其在将大量文本浓缩为信息摘要中的实用性。
- en: Understanding Extractive Summarization
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解抽取式摘要
- en: Extractive summarization is a prominent technique in the field of natural language
    processing (NLP) and text analysis. With it, key sentences or phrases are carefully
    selected from the original text and combined to create a concise and informative
    summary. This involves meticulously sifting through text to identify the most
    crucial elements and central ideas or arguments presented in the selected piece.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 抽取式摘要是自然语言处理（NLP）和文本分析领域的一个重要技术。通过这种方法，从原始文本中精心选择关键句子或短语，并将其组合成简洁且信息丰富的摘要。这涉及仔细筛选文本，以识别所选内容中最关键的元素和中心思想或论点。
- en: Where abstractive summarization involves generating entirely new sentences often
    not present in the source material, extractive summarization sticks to the original
    text. It doesn’t alter or paraphrase but instead extracts the sentences exactly
    as they appear, maintaining the original wording and structure. This way the summary
    stays true to the source material's tone and content. The technique of extractive
    summarization is extremely beneficial in instances where the accuracy of the information
    and the preservation of the author's original intent are a priority.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 抽取式摘要涉及从原始文本中提取句子，保持原文的措辞和结构，而不改变或改写。这样，摘要保持了源材料的语气和内容。这种方法在准确性和保留作者原意的情况下极为有用。
- en: It has many different uses, like summarizing news articles, academic papers,
    or lengthy reports. The process effectively conveys the original content's message
    without potential biases or reinterpretations that can occur with paraphrasing.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 它有许多不同的用途，比如总结新闻文章、学术论文或冗长的报告。该过程有效地传达了原始内容的信息，而不会出现可能的偏见或重新解释，这些情况可能在改述中发生。
- en: How Does Extractive Summarization Use LLMs?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取性总结如何使用 LLM？
- en: 1\. Text Parsing
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 文本解析
- en: This initial step involves breaking down the text into its essential elements,
    primarily sentences, and phrases. The goal is to identify the basic units (sentences,
    in this context) the algorithm will later evaluate to include in a summary, like
    dissecting a text to understand its structure and individual components.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步骤涉及将文本分解为其基本元素，主要是句子和短语。目标是识别算法随后将评估以纳入摘要的基本单位（在此上下文中为句子），类似于剖析文本以理解其结构和各个组成部分。
- en: For instance, the model would analyze a four-sentence paragraph by breaking
    it down into the following four-sentence components.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，该模型将通过将四句段落拆分为以下四句组件来分析它。
- en: The Pyramids of Giza, built in ancient Egypt, stood magnificently for millennia.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 吉萨金字塔，建于古埃及，巍峨屹立了千年。
- en: They were constructed as tombs for pharaohs.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们被建作为法老的墓。
- en: The Great Pyramids are the most famous.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大金字塔是最著名的。
- en: These structures symbolize architectural intelligence.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些结构象征着建筑智慧。
- en: 2\. Feature Extraction
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 特征提取
- en: In this stage, the algorithm analyzes each sentence to identify characteristics
    or 'features' that might indicate what their significance is to the overall text.
    Common features include the frequency and repeated use of keywords and phrases,
    the length of sentences, their position in the text and its implications, and
    the presence of specific keywords or phrases that are central to the text's main
    topic.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一阶段，算法分析每个句子，以识别可能指示其对整体文本重要性的特征或“特征”。常见的特征包括关键词和短语的频率和重复使用、句子的长度、它们在文本中的位置及其含义，以及文本主要主题的特定关键词或短语的存在。
- en: Below is an example of how the LLM would do feature extraction for the first
    sentence “The Pyramids of Giza, built in ancient Egypt, stand magnificently for
    millennia.”
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 LLM 如何对第一个句子“吉萨金字塔，建于古埃及，巍峨屹立了千年”进行特征提取的示例。
- en: '| Attributes | Text |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 属性 | 文本 |'
- en: '| Frequency | “Pyramids of Giza”, “Ancient Egypt”, “Millennia” |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 频率 | “吉萨金字塔”，“古埃及”，“千年” |'
- en: '| Sentence Length | Moderate |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 句子长度 | 中等 |'
- en: '| Position in Text | Introductory, sets the topic |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 位置在文本中 | 引言，设置主题 |'
- en: '| Specific Keywords | “Pyramids of Giza”, “ancient Egypt” |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 特定关键词 | “吉萨金字塔”，“古埃及” |'
- en: 3\. Scoring Sentences
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 句子评分
- en: Each sentence is assigned a score based on its content. This score reflects
    a sentence’s perceived importance in the context of the entire text. Sentences
    that score higher are deemed to carry more weight or relevance.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 每个句子根据其内容分配一个得分。这个得分反映了句子在整个文本中的重要性。得分较高的句子被认为在整体文本中更有分量或相关性。
- en: Simply put, this process rates each sentence for its potential significance
    to a summary of the entire text.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，该过程对每个句子的潜在重要性进行评分，以总结整个文本。
- en: '| Sentence Score | Sentence | Explanation |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 句子得分 | 句子 | 解释 |'
- en: '| 9 | The Pyramids of Giza, built in ancient Egypt, stood magnificently for
    millennia. | It''s the introductory sentence that sets the topic and context,
    containing key terms like “Pyramids of Giza” and “Ancient Egypt. |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 吉萨金字塔，建于古埃及，巍峨屹立了千年。 | 这是引言句，设置了主题和背景，包含了诸如“吉萨金字塔”和“古埃及”这样的关键词。 |'
- en: '| 8 | They were constructed as tombs for pharaohs. | This sentence provides
    critical historical information about the Pyramids and may identify them as significant
    for understanding their purpose and significance. |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 它们被建作为法老的墓。 | 这句话提供了关于金字塔的关键历史信息，可能表明它们在理解其目的和重要性方面的意义。 |'
- en: '| 3 | The Great Pyramids are the most famous. | Although this sentence adds
    specific information about the Great Pyramids, it is considered less crucial in
    the broader context of summarizing the overall significance of the Pyramids. |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 大金字塔是最著名的。 | 尽管这句话提供了关于大金字塔的具体信息，但在总结金字塔总体重要性的广泛背景下，它被认为不那么关键。 |'
- en: '| 7 | These structures symbolize architectural intelligence. | This summarizing
    statement captures the overarching significance of the Pyramids. |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 这些结构象征着建筑智慧。 | 这个总结陈述捕捉了金字塔的整体重要性。 |'
- en: 4\. Selection and Aggregation
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 选择和聚合
- en: The final phase involves selecting the highest-scoring sentences and compiling
    them into a summary. When carefully done, this ensures the summary remains coherent
    and an aggregately representative of the main ideas and themes of the original
    text.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后阶段涉及选择得分最高的句子并将其编入总结。经过精心处理，这确保了总结保持连贯，并且全面代表了原文的主要思想和主题。
- en: To create an effective summary, the algorithm must balance the need to include
    important sentences that are concise, avoid redundancy, and ensure that the selected
    sentences provide a clear and comprehensive overview of the entire original text.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建有效的总结，算法必须平衡包括简洁的重要句子，避免冗余，并确保所选的句子提供对整个原文的清晰和全面的概述。
- en: The Pyramids of Giza, built in Ancient Egypt, stood magnificently for millennia.
    They were constructed as tombs for pharaohs. These structures symbolize architectural
    brilliance.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建于古埃及的吉萨金字塔雄伟地矗立了数千年。它们是作为法老的陵墓而建造的。这些结构象征着建筑才华。
- en: This example is extremely basic, extracting 3 of the total 4 sentences for the
    best overall summarization. Reading an extra sentence doesn't hurt, but what happens
    when the text is longer? Let's say, 3 paragraphs?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子非常基础，从总共4个句子中提取了3个句子以获得最佳的整体总结。多读一个句子也无妨，但当文本更长时会怎样呢？假设是3段文字呢？
- en: How to Run Extractive Summarization with BERT LLMs
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用BERT LLMs运行抽取式总结
- en: 'Step 1: Installing and Importing Necessary Packages'
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤1：安装和导入必要的包
- en: We will be leveraging the pre-trained BERT model. However, we won't be using
    just any BERT model; instead, we'll focus on the BERT Extractive Summarizer. This
    particular model has been finely tuned for specialized tasks in extractive summarization.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用预训练的BERT模型。然而，我们不会使用任何BERT模型，而是专注于BERT Extractive Summarizer。这个特定的模型已经为抽取式总结的专业任务进行了精细调优。
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Step 2: Introduce Summarizer Function'
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤2：引入总结函数
- en: The Summarizer() function imported from the summarizer in Python is an extractive
    text summarization tool. It uses the BERT model to analyze and extract key sentences
    from a larger text. This function aims to retain the most important information,
    providing a condensed version of the original content. It's commonly used to summarize
    lengthy documents efficiently.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从Python的summarizer中导入的Summarizer()函数是一个抽取式文本总结工具。它使用BERT模型来分析和提取较大文本中的关键句子。该函数旨在保留最重要的信息，提供原始内容的精简版本。它通常用于高效地总结冗长的文档。
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Step 3: Importing our Text'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤3：导入我们的文本
- en: 'Here, we will import any piece of text that we would like to test our model
    on. To test our extractive summary model we generated text using ChatGPT 3.5 with
    the prompt: “Provide a 3-paragraph summary of the history of GPUs and how they
    are used today.”'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将导入任何我们想要测试模型的文本。为了测试我们的抽取式总结模型，我们使用ChatGPT 3.5生成了以下文本，提示是：“提供一个关于GPU历史及其今日应用的3段总结。”
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Step 4: Performing Extractive Summarization'
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤4：执行抽取式总结
- en: 'Finally, we''ll execute our summarization function. This function requires
    two inputs: the text to be summarized and the desired number of sentences for
    the summary. After processing, it will generate an extractive summary, which we
    will then display.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将执行我们的总结函数。该函数需要两个输入：要总结的文本和所需的总结句子数量。处理后，它将生成一个抽取式总结，我们将显示出来。
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Extractive Summary Output:**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**抽取式总结输出：**'
- en: '*The history of Graphics Processing Units (GPUs) dates back to the early 1980s
    when companies like IBM and Texas Instruments developed specialized graphics accelerators
    for rendering images and improving overall graphical performance. NVIDIA''s GeForce
    256, released in 1999, is often considered the first GPU, as it integrated both
    2D and 3D acceleration on a single chip. Today, GPUs have evolved far beyond their
    original graphics-centric purpose, now widely used for parallel processing tasks
    in various fields, such as scientific simulations, artificial intelligence, and
    machine learning. As technology progresses, GPUs are expected to play an even
    more critical role in shaping the future of computing.*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*图形处理单元（GPU）的历史可以追溯到1980年代初期，当时像IBM和德州仪器这样的公司开发了专用的图形加速器，用于渲染图像和提高整体图形性能。NVIDIA的GeForce
    256，发布于1999年，常被认为是第一款GPU，因为它将2D和3D加速集成在一个芯片上。如今，GPU已经远远超出了其最初的图形中心用途，现在广泛用于各种领域的并行处理任务，如科学模拟、人工智能和机器学习。随着技术的发展，GPU预计将在塑造计算未来中发挥更为关键的作用。*'
- en: Our model pulled the 4 most important sentences from our large corpus of text
    to generate this summary!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型从大量文本中提取了4个最重要的句子来生成这个摘要！
- en: Challenges of Extractive Summarization Using LLMs
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LLM进行提取式总结的挑战
- en: Contextual Understanding Limitations
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上下文理解的局限性
- en: While LLMs are proficient in processing and generating language, their understanding
    of context, especially in longer texts, is limited. LLMs can miss subtle nuances
    or fail to recognize critical aspects of the text leading to less accurate or
    relevant summaries. The more advanced the language model the better the summary
    will be.
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然LLM在处理和生成语言方面表现出色，但它们对上下文的理解，尤其是在较长文本中的理解，仍然有限。LLM可能会遗漏细微的差别或未能识别文本中的关键方面，导致总结不够准确或相关。语言模型越先进，总结的质量就越好。
- en: Bias in Training Data
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练数据中的偏差
- en: LLMs learn from vast datasets compiled from various sources, including the internet.
    These datasets can contain biases, which the models might inadvertently learn
    and replicate in their summaries leading to skewed or unfair representations.
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）从包括互联网在内的各种来源编译的大量数据集中学习。这些数据集可能包含偏差，模型可能无意中学习并在总结中复制这些偏差，导致扭曲或不公平的表现。
- en: Handling Specialized or Technical Language
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理专业或技术语言
- en: While LLMs are generally trained on a wide range of general texts, they may
    not accurately capture specialized or technical language in fields like law, medicine,
    or other highly technical fields. This can be alleviated by feeding it more specialized
    and technical text. Lack of training in specialized jargon can affect the quality
    of summaries when used in these fields.
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然LLM通常在各种普通文本上进行训练，但它们可能无法准确捕捉到法律、医学或其他高度专业领域的专业或技术语言。这可以通过提供更多专业和技术文本来缓解。缺乏对专业术语的训练可能会影响这些领域中总结的质量。
- en: Conclusion
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: It's clear that extractive summarization is more than just a handy tool; it's
    a growing necessity in our information-saturated age where we’re inundated with
    walls of text every day. By harnessing the power of technologies like BERT, we
    can see how complex texts can be distilled into digestible summaries, saving us
    time and helping us to further comprehend the texts being summarized.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，提取式总结不仅仅是一个方便的工具；在我们每天被大量文本淹没的信息时代，它变得越来越必要。通过利用像BERT这样的技术，我们可以看到复杂的文本如何被提炼成易于理解的摘要，从而节省时间，并帮助我们更好地理解被总结的文本。
- en: Whether for academic research, business insights, or just staying informed in
    a technologically advanced world, extractive summarization is a practical way
    to navigate the sea of information we’re surrounded by. As natural language processing
    continues to evolve, tools like extractive summarization will become even more
    essential, helping us to quickly find and understand the information that matters
    most in a world where every minute counts.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是用于学术研究、商业洞察，还是仅仅为了在技术先进的世界中保持信息更新，提取式总结都是在我们被信息海洋包围的世界中导航的实用方式。随着自然语言处理技术的不断发展，像提取式总结这样的工具将变得更加重要，帮助我们快速找到并理解在每分钟都很关键的世界中最重要的信息。
- en: '[Original](https://www.exxactcorp.com/blog/deep-learning/extractive-summarization-with-llm-using-bert).
    Reposted with permission.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://www.exxactcorp.com/blog/deep-learning/extractive-summarization-with-llm-using-bert)。经许可转载。'
- en: '**[Kevin Vu](https://blog.exxactcorp.com/)** manages [Exxact Corp blog](https://blog.exxactcorp.com/)
    and works with many of its talented authors who write about different aspects
    of Deep Learning.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Kevin Vu](https://blog.exxactcorp.com/)** 负责管理 [Exxact Corp 博客](https://blog.exxactcorp.com/)，并与许多才华横溢的作者合作，这些作者撰写关于深度学习不同方面的内容。'
- en: More On This Topic
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解更多主题
- en: '[Classifying Long Text Documents Using BERT](https://www.kdnuggets.com/2022/02/classifying-long-text-documents-bert.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 BERT 对长文本文档进行分类](https://www.kdnuggets.com/2022/02/classifying-long-text-documents-bert.html)'
- en: '[Approaches to Text Summarization: An Overview](https://www.kdnuggets.com/2019/01/approaches-text-summarization-overview.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本摘要方法概述](https://www.kdnuggets.com/2019/01/approaches-text-summarization-overview.html)'
- en: '[Getting Started with Automated Text Summarization](https://www.kdnuggets.com/2019/11/getting-started-automated-text-summarization.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[开始使用自动化文本摘要](https://www.kdnuggets.com/2019/11/getting-started-automated-text-summarization.html)'
- en: '[Summarization with GPT-3](https://www.kdnuggets.com/2022/04/packt-summarization-gpt3.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 GPT-3 进行摘要](https://www.kdnuggets.com/2022/04/packt-summarization-gpt3.html)'
- en: '[Text Summarization Development: A Python Tutorial with GPT-3.5](https://www.kdnuggets.com/2023/04/text-summarization-development-python-tutorial-gpt35.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本摘要开发：使用 GPT-3.5 的 Python 教程](https://www.kdnuggets.com/2023/04/text-summarization-development-python-tutorial-gpt35.html)'
- en: '[Unlocking GPT-4 Summarization with Chain of Density Prompting](https://www.kdnuggets.com/unlocking-gpt-4-summarization-with-chain-of-density-prompting)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过密度链提示解锁 GPT-4 摘要](https://www.kdnuggets.com/unlocking-gpt-4-summarization-with-chain-of-density-prompting)'
