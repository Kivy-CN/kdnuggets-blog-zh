["```py\nimport pandas as pd\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/'\n                 'machine-learning-databases/wine/wine.data',\n                  header=None)\nprint(df.head)\n\n```", "```py\nX = df.iloc[:, 1:] #all rows, columns starting from 1 till end\ny = df.iloc[:, 0] #all rows, only 0th column\n\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=0)\n\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_std = sc.fit_transform(X_train)\nX_test_std = sc.transform(X_test)\n\n```", "```py\nfrom sklearn.decomposition import PCA\n#pca = PCA()\n\n```", "```py\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train_std)\nprint(X_train_pca.shape)\n\n```", "```py\npca.explained_variance_ratio_\n\n```", "```py\ncolors = ['r','b','g']\nmarkers = ['s','x','o']\nfor l,c,m in zip(np.unique(y_train), colors, markers):\n    plt.scatter(X_train_pca[y_train==l,0],\n                X_train_pca[y_train==l,1],\n                c=c, label=l, marker=m)\nplt.title(\"Compressed Data\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n```", "```py\npca1 = PCA(n_components=0.8)\nX_train_pca = pca1.fit_transform(X_train_std)\nX_test_pca = pca1.transform(X_test_std)\n\n```", "```py\nX_train_pca.shape\n>>> (142,5)\n\n```", "```py\nX_train_pca[:6, :]\n\n```", "```py\npca1.explained_variance_ratio_\n\n```", "```py\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n# we have already preprocessed the dataset in PCA, we will use the same dataset\n\n```", "```py\nlda = LDA()\nX_train_lda = lda.fit_transform(X_train_std, y_train)\nX_test_lda = lda.transform(X_test_std)\n\n```", "```py\nX_train_lda.shape\n>>> (124, 2)\n\n```", "```py\ncolors = ['r','b','g']\nmarkers = ['s','x','o']\nfor l,c,m in zip(np.unique(y_train), colors, markers):\n    plt.scatter(X_train_lda[y_train==l,0],\n                X_train_lda[y_train==l,1],\n                c=c, label=l, marker=m)\nplt.title(\"Compressed Data\")\nplt.xlabel(\"LD 1\")\nplt.ylabel(\"LD 2\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n```", "```py\nfrom sklearn.datasets import make_moons #non-linear dataset\nfrom sklearn.decomposition import KernelPCA\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n```", "```py\nX, y = make_moons(n_samples=100)\nprint(X.shape)\n\n```", "```py\nprint(\"Number of Unique Classes\", len(np.unique(y)), \"\\nUnique Values are\", np.unique(y))\n\n```", "```py\n#plotting the first class i.e y=0\nplt.scatter(X[y==0,0], X[y==0,1],\n            color = 'red', marker='^',\n            alpha= 0.5)\n#plotting the 2nd class i.e y=1\nplt.scatter(X[y==1,0], X[y==1,1],\n            color = 'blue', marker='o',\n            alpha= 0.5)\n#showing the plot\nplt.xlabel(\"Axis 1\")\nplt.ylabel(\"Axis 2\")\nplt.tight_layout()\nplt.show()\n\n```", "```py\nkpca = KernelPCA(n_components=2, kernel='rbf', gamma=15) #defining the KenrelPCA object\n\n```", "```py\nX_kpca = kpca.fit_transform(X) #transforming our dataset\n\n```", "```py\nplt.scatter(X_kpca[y==0,0], X_kpca[y==0,1],\n            color = 'red', marker='^',\n            alpha= 0.5)\nplt.scatter(X_kpca[y==1,0], X_kpca[y==1,1],\n            color = 'blue', marker='o',\n            alpha= 0.5)\nplt.xlabel(\"PCA 1\")\nplt.ylabel(\"PCA 2\")\nplt.tight_layout()\nplt.show()\n\n```", "```py\nkpca = KernelPCA(n_components=2)\nX_train_kpca = kpca.fit_transform(X_train_std)\nX_test_kpca = kpca.transform(X_test_std)\n\n```"]