# PySpark SQL 备忘单：Python 中的大数据

> 原文：[https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html](https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html)

**作者：Karlijn Willems，[DataCamp](https://www.datacamp.com/)。**

### 使用 Python 进行大数据分析

大数据无处不在，传统上由三个 V 特征来描述：速度、种类和体量。大数据是快速的、多样的，且体量庞大。作为数据科学家、数据工程师、数据架构师，...或在数据科学行业中承担的任何角色，你迟早都会接触到大数据，因为公司现在在各个领域收集了大量的数据。

* * *

## 我们的三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速开启网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的 IT 部门

* * *

数据并不总是意味着信息，这就是你，数据科学爱好者，可以发挥作用的地方。你可以利用 Apache Spark，“一个用于大规模数据处理的快速且通用的引擎”来开始应对大数据给你和你所在公司带来的挑战。

尽管如此，当你使用 Spark 时，总是可能会出现疑问，这时可以查看 DataCamp 的 [Apache Spark 教程：使用 PySpark 进行机器学习](https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning) 或免费下载 [备忘单](https://www.datacamp.com/community/blog/pyspark-sql-cheat-sheet)！

接下来，我们将深入探讨备忘单的结构和内容。

### PySpark 备忘单

PySpark 是 Spark 的 Python API，将 Spark 编程模型暴露给 Python。Spark SQL 是 PySpark 的一个模块，允许你使用 DataFrames 形式的结构化数据。这与通常用于处理非结构化数据的 RDDs 相对立。

![PySpark 备忘单](../Images/f31b5ce6452a08f76098ca13c56b14cb.png)

**提示：** 如果你想了解 RDDs 和 DataFrames 之间的区别，还想了解 Spark DataFrames 与 pandas DataFrames 的不同之处，务必查看 [Python 中的 Apache Spark：初学者指南](https://www.datacamp.com/community/tutorials/apache-spark-python)。

### 初始化 SparkSession

如果你想开始使用 PySpark 的 Spark SQL，你需要首先启动一个 SparkSession：你可以用它来创建 DataFrames、将 DataFrames 注册为表、在表上执行 SQL 查询以及读取 parquet 文件。如果这一切对你来说都很陌生，不用担心 - 你将在本文后续部分了解到更多信息！

你通过首先从 pyspark 包的 sql 模块中导入 SparkSession 来启动 SparkSession。接下来，你可以初始化一个变量 spark，例如，不仅构建 SparkSession，还可以为应用程序命名、设置配置，然后使用 getOrCreate() 方法来获取当前运行的 SparkSession（如果已经有一个），或者创建一个新的 SparkSession（如果尚未存在）！最后这个方法会非常有用，特别是在未来的参考中，因为它可以防止同时运行多个 SparkSessions！

很酷，对吧？

![PySpark cheat sheet](../Images/3c86b85673b094ab3ed9a4afa5142260.png)

### 数据检查

当你导入了数据后，使用一些内置的属性和方法来检查 Spark DataFrame。这样，你可以在开始操作 DataFrames 之前更好地了解你的数据。

现在，你可能已经了解了本节中提到的大部分方法和属性，这些方法和属性来自于使用 pandas DataFrames 或 NumPy 的经验，比如 dtypes、head()、describe()、count() 等。还有一些方法可能对你来说是新的，比如 take() 或 printSchema() 方法，或者 schema 属性。

![PySpark cheat sheet](../Images/2b87b5681e4fc864b638d73298521c50.png)

尽管如此，你会发现 ramp-up 的过程相当温和，因为你可以最大限度地利用之前在数据科学包中的知识。

### 重复值

在检查数据时，你可能会发现存在一些重复值。为了处理这些问题，你可以使用 dropDuplicates() 方法，例如，用于删除 Spark DataFrame 中的重复值。

### 查询

你可能还记得，使用 Spark DataFrames 的主要原因之一是你可以以更结构化的方式处理数据 - 查询就是处理数据的一种更结构化的方式，无论你是在关系型数据库中使用 SQL、在 No-SQL 数据库中使用类似 SQL 的语言，还是在 Pandas DataFrames 上使用 [query()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html?highlight=query#pandas.DataFrame.query) 方法，等等。

![PySpark cheat sheet](../Images/8a57af6b8937cff2c56c8dfbf857efe5.png)

现在我们谈论的是 Pandas DataFrames，你会注意到 Spark DataFrames 遵循类似的原则：你使用方法来更好地了解你的数据。不过，在这种情况下，你不会使用 query()。相反，你需要利用其他方法来获得你想要的结果：首先，select() 和 show() 将是你获取 DataFrames 信息时的好帮手。

在这些方法中，你可以构建你的查询。与标准 SQL 一样，你可以在 select() 中指定你到底想要获取哪些列。你可以使用普通字符串或通过 DataFrame 本身指定列名，如以下示例所示：df.lastName 或 df[“firstName”]。

注意，后一种方法在你想要指定你究竟想要检索哪些信息时，给予了你更多的自由，例如可以使用 isin() 或 startswith() 等附加函数。

除了 select()，你还可以利用 PySpark SQL 中的函数模块来指定例如查询中的 when 子句，如下表所示：

![PySpark 备忘单](../Images/d5234f75f8ad26d04f778d4b0bbf5d52.png)

总的来说，你可以看到，通过使用 select()、help() 和函数模块的功能，有很多可能性来处理和深入了解你的数据。

### 添加、更新和删除列

你可能还想查看如何向 Spark DataFrame 中添加、更新或删除一些列。你可以通过 withColumn()、withColumnRenamed() 和 drop() 方法轻松实现。你现在可能知道在使用 Pandas DataFrames 时，你也可以使用 drop() 方法。你可以在[这里](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html?highlight=drop#pandas.DataFrame.drop)阅读更多信息。

![PySpark 备忘单](../Images/751902d5d047e7357ce04767eb6020aa.png)

### GroupBy

再次提醒，就像使用 Pandas DataFrames 一样，你可能会想按某些值进行分组并汇总一些值 - 下面的示例展示了如何使用 groupBy() 方法，结合 count() 和 show() 来检索和显示按年龄分组的数据，以及有多少人拥有那个特定年龄。

![PySpark 备忘单](../Images/283cb11c84f4170ae04c5534ce466532.png)

### 更多信息

+   [PySpark 用于数据科学](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)

+   [使用 Pandera 进行 PySpark 应用程序的数据验证](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)

+   [使用 Python 进行数据清理的备忘单](https://www.kdnuggets.com/2023/02/data-cleaning-python-cheat-sheet.html)

+   [构建生成 AI 应用程序的最佳 Python 工具备忘单](https://www.kdnuggets.com/2023/08/best-python-tools-generative-ai-cheat-sheet.html)

+   [Python 控制流备忘单](https://www.kdnuggets.com/2022/11/python-control-flow-cheatsheet.html)

+   [KDnuggets 新闻，7月5日：一个糟糕的数据科学项目 • 10个人工智能…](https://www.kdnuggets.com/2023/n24.html)
