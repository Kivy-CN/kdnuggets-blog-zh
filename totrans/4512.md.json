["```py\nimport pandas as pd\nimport numpy as np\nfrom numpy import argmax\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport librosa\nimport librosa.display\nimport IPython.display\nimport random\nimport warnings\nimport os\nfrom PIL import Image\nimport pathlib\nimport csv\n# sklearn Preprocessing\nfrom sklearn.model_selection import train_test_split\n#Keras\nimport keras\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom keras import layers\nfrom keras.layers import Activation, Dense, Dropout, Conv2D, Flatten, MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling1D, AveragePooling2D, Input, Add\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD\n```", "```py\n genres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split()\nfor g in genres:\n    pathlib.Path(f'img_data/{g}').mkdir(parents=True, exist_ok=True)\n    for filename in os.listdir(f'./drive/My Drive/genres/{g}'):\n        songname = f'./drive/My Drive/genres/{g}/{filename}'\n        y, sr = librosa.load(songname, mono=True, duration=5)\n        print(y.shape)\n        plt.specgram(y, NFFT=2048, Fs=2, Fc=0, noverlap=128, cmap=cmap, sides='default', mode='default', scale='dB');\n        plt.axis('off');\n        plt.savefig(f'img_data/{g}/{filename[:-3].replace(\".\", \"\")}.png')\n        plt.clf()\n```", "```py\npip install split-folders\n```", "```py\nimport split-folders\n# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\nsplit-folders.ratio('./img_data/', output=\"./data\", seed=1337, ratio=(.8, .2)) # default values\n```", "```py\nfrom keras.preprocessing.image import ImageDataGenerator\ntrain_datagen = ImageDataGenerator(\n        rescale=1./255, # rescale all pixel values from 0-255, so aftre this step all our pixel values are in range (0,1)\n        shear_range=0.2, #to apply some random tranfromations\n        zoom_range=0.2, #to apply zoom\n        horizontal_flip=True) # image will be flipper horiztest_datagen = ImageDataGenerator(rescale=1./255)\n```", "```py\ntraining_set = train_datagen.flow_from_directory(\n        './data/train',\n        target_size=(64, 64),\n        batch_size=32,\n        class_mode='categorical',\n        shuffle = False)test_set = test_datagen.flow_from_directory(\n        './data/val',\n        target_size=(64, 64),\n        batch_size=32,\n        class_mode='categorical',\n        shuffle = False )\n```", "```py\nmodel = Sequential()\ninput_shape=(64, 64, 3)#1st hidden layer\nmodel.add(Conv2D(32, (3, 3), strides=(2, 2), input_shape=input_shape))\nmodel.add(AveragePooling2D((2, 2), strides=(2,2)))\nmodel.add(Activation('relu'))#2nd hidden layer\nmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\nmodel.add(AveragePooling2D((2, 2), strides=(2,2)))\nmodel.add(Activation('relu'))#3rd hidden layer\nmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\nmodel.add(AveragePooling2D((2, 2), strides=(2,2)))\nmodel.add(Activation('relu'))#Flatten\nmodel.add(Flatten())\nmodel.add(Dropout(rate=0.5))#Add fully connected layer.\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(rate=0.5))#Output layer\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))model.summary()\n```", "```py\nepochs = 200\nbatch_size = 8\nlearning_rate = 0.01\ndecay_rate = learning_rate / epochs\nmomentum = 0.9\nsgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\nmodel.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n```", "```py\nmodel.fit_generator(\n        training_set,\n        steps_per_epoch=100,\n        epochs=50,\n        validation_data=test_set,\n        validation_steps=200)\n```", "```py\n#Model Evaluation\nmodel.evaluate_generator(generator=test_set, steps=50)#OUTPUT\n[1.704445120342617, 0.33798882681564246]\n```", "```py\ntest_set.reset()\npred = model.predict_generator(test_set, steps=50, verbose=1)\n```", "```py\npredicted_class_indices=np.argmax(pred,axis=1)\n\nlabels = (training_set.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]\npredictions = predictions[:200]\nfilenames=test_set.filenames\n```", "```py\nprint(len(filename, len(predictions)))\n# (200, 200)\n```", "```py\nresults=pd.DataFrame({\"Filename\":filenames,\n                      \"Predictions\":predictions},orient='index')\nresults.to_csv(\"prediction_results.csv\",index=False)\n```"]