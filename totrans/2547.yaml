- en: Building Machine Learning Pipelines using Snowflake and Dask
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Snowflake和Dask构建机器学习管道
- en: 原文：[https://www.kdnuggets.com/2021/07/building-machine-learning-pipelines-snowflake-dask.html](https://www.kdnuggets.com/2021/07/building-machine-learning-pipelines-snowflake-dask.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/07/building-machine-learning-pipelines-snowflake-dask.html](https://www.kdnuggets.com/2021/07/building-machine-learning-pipelines-snowflake-dask.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Daniel Foley](https://www.linkedin.com/in/daniel-foley-1ab904a2/), Data
    Scientist**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：[Daniel Foley](https://www.linkedin.com/in/daniel-foley-1ab904a2/)，数据科学家**'
- en: '![Image](../Images/9a765e838d415cad554c4eb10c45ec3e.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../Images/9a765e838d415cad554c4eb10c45ec3e.png)'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: Recently I have been trying to find better ways to improve my workflow as a
    data scientist. I tend to spend a decent chunk of my time modelling and building
    ETLs in my job. This has meant that more and more I need to rely on tools to reliably
    and efficiently handle large datasets. I quickly realised that using pandas for
    manipulating these datasets is not always a good approach and this prompted me
    to look into other alternatives.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，我一直在尝试找到更好的方法来改善作为数据科学家的工作流程。我发现自己在工作中花了相当多的时间进行建模和构建ETL。这意味着我越来越需要依赖工具来可靠且高效地处理大型数据集。我很快意识到，使用pandas来操作这些数据集并不总是一个好方法，这促使我寻找其他的替代方案。
- en: In this post, I want to share some of the tools that I have been exploring recently
    and show you how I use them and how they helped improve the efficiency of my workflow.
    The two I will talk about in particular are Snowflake and Dask. Two very different
    tools but ones that complement each other well especially as part of the ML Lifecycle.
    My hope is that after reading this post you will have a good understanding of
    what Snowflake and Dask are, how they can be used effectively and be able to get
    up and running with your own use cases.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我想分享一些我最近探索的工具，并展示我如何使用它们以及它们如何帮助提高工作效率。我特别要谈的是Snowflake和Dask。这两者是非常不同的工具，但它们在机器学习生命周期中相互补充。我的希望是，在阅读这篇文章后，你将对Snowflake和Dask有一个良好的理解，知道如何有效使用它们，并能够快速上手自己的用例。
- en: More specifically, I want to show you how you can build an ETL pipeline using
    Snowflake and Python to generate training data for a machine learning task. I
    then want to introduce Dask and [Saturn Cloud](https://www.saturncloud.io/s/?utm_source=daniel-foley) and
    show you how you can take advantage of parallel processing in the cloud to really
    speed up the ML training process so you can increase your productivity as a data
    scientist.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我想展示如何使用Snowflake和Python构建ETL管道，为机器学习任务生成训练数据。接着，我会介绍Dask和[Saturn Cloud](https://www.saturncloud.io/s/?utm_source=daniel-foley)，并展示如何利用云中的并行处理来真正加速机器学习训练过程，从而提升你作为数据科学家的生产力。
- en: Building ETLs in Snowflake and Python
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Snowflake和Python中构建ETL
- en: Before we jump into coding I better briefly explain what Snowflake is. This
    is a question I recently asked when my team decided to start using it. At a high
    level, it is a data warehouse in the cloud. After playing around with it for a
    while I realised how powerful it was. I think for me, one of the most useful features
    is the virtual warehouses that you can use. A virtual warehouse gives you access
    to the same data but is completely independent of other virtual warehouses so
    compute resources are not shared across teams. This has proven very useful as
    it removes any potential for performance issues caused by other users executing
    queries throughout the day. This has resulted in less frustration and time wasted
    waiting for queries to run.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始编码之前，我最好简要解释一下 Snowflake 是什么。这是我最近在团队决定开始使用它时问的一个问题。从高层次来看，它是一个云中的数据仓库。在玩了一段时间后，我意识到它的强大功能。我认为对我来说，最有用的功能之一是你可以使用的虚拟仓库。虚拟仓库使你可以访问相同的数据，但与其他虚拟仓库完全独立，因此计算资源不会在团队之间共享。这被证明非常有用，因为它消除了由于其他用户在一天中执行查询而引起的性能问题的潜在可能。这减少了因查询运行而产生的挫败感和等待时间。
- en: 'Since we are going to be using Snowflake I will briefly outline how you can
    set it up and start experimenting with it yourself. We need to do the following:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用 Snowflake，我将简要概述如何设置它并开始自己进行实验。我们需要做以下几件事：
- en: '*Get a Snowflake account set up*'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*设置一个 Snowflake 账户*'
- en: '*Get our data into Snowflake*'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将我们的数据导入 Snowflake*'
- en: '*Write and test our queries using SQL and the Snowflake UI*'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用 SQL 和 Snowflake UI 编写并测试我们的查询*'
- en: '*Write a Python class that can execute our queries to generate our final dataset
    for modelling*'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*编写一个 Python 类，以执行我们的查询来生成最终的数据集用于建模*'
- en: Setting up an account is as easy as signing up for a free trial on their [website](http://.com/?_ga=2.18641021.553593417.1621861037-941570141.1610013295).
    Once you have done that you can download the snowsql CLI [here](https://docs.snowflake.com/en/user-guide/snowsql-install-config.html#installing-snowsql-on-macos-using-the-installer).
    This will make it straightforward to add data to Snowflake. After following these
    steps we can try and connect to Snowflake using our credentials and the command
    line.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 设置账户就像在他们的 [网站](http://.com/?_ga=2.18641021.553593417.1621861037-941570141.1610013295)
    上注册一个免费试用一样简单。完成后，你可以从 [这里](https://docs.snowflake.com/en/user-guide/snowsql-install-config.html#installing-snowsql-on-macos-using-the-installer)
    下载 snowsql CLI。这将使将数据添加到 Snowflake 变得简单。在完成这些步骤后，我们可以尝试使用我们的凭据和命令行连接到 Snowflake。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can find your account name in the URL when you log in to the Snowflake
    UI. It should look something like this: xxxxx.europe-west2.gcp. Ok, let’s move
    onto the next step and get our data into Snowflake. There are a few steps we need
    to follow here namely:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当你登录 Snowflake UI 时，你可以在 URL 中找到你的账户名。它应该类似于这样：xxxxx.europe-west2.gcp。好了，让我们进入下一步，把数据导入
    Snowflake。我们需要遵循以下几个步骤：
- en: '*Create our virtual warehouse*'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*创建我们的虚拟仓库*'
- en: '*Create a database*'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*创建一个数据库*'
- en: '*Define and Create our tables*'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*定义和创建我们的表*'
- en: '*Create a staging table for our CSV files*'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*为我们的 CSV 文件创建一个临时表*'
- en: '*Copying the data into our tables*'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将数据复制到我们的表中*'
- en: Luckily this isn’t too difficult and we can do this entirely using the snowsql
    CLI. For this project, I will be using a smaller dataset than I would like but
    unfortunately, I cannot use any of my company’s data and it can be pretty difficult
    to find large suitable datasets online. I did however find some transaction data
    from Dunnhumby which is freely available on [Kaggle](https://www.kaggle.com/frtgnn/dunnhumby-the-complete-journey).
    Just for kicks though I create a much larger synthetic dataset using this data
    to test how well Dask handles the challenge compared to sklearn.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这并不难，我们可以完全使用 snowsql CLI 来完成。对于这个项目，我将使用一个比我希望的小的数据集，但不幸的是，我不能使用公司中的任何数据，而且在网上找到大规模适合的数据集也相当困难。不过，我确实找到了来自
    Dunnhumby 的一些交易数据，这些数据可以在 [Kaggle](https://www.kaggle.com/frtgnn/dunnhumby-the-complete-journey)
    上免费获取。为了好玩，我使用这些数据创建了一个更大的合成数据集，以测试 Dask 相比于 sklearn 处理挑战的效果。
- en: First of all, we need to set up a virtual warehouse and a database using the
    following commands in the Snowflake UI.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要在 Snowflake UI 中使用以下命令设置一个虚拟仓库和一个数据库。
- en: '**create** **or** **replace** warehouse analytics_wh **with**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**create** **or** **replace** **warehouse** analytics_wh **with**'
- en: warehouse_size=”X-SMALL”
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: warehouse_size=”X-SMALL”
- en: auto_suspend=180
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: auto_suspend=180
- en: auto_resume=true
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: auto_resume=true
- en: initially_suspended=true;
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: initially_suspended=true;
- en: '**create** **or** **replace** **database** dunnhumby;'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**create** **or** **replace** **database** dunnhumby;'
- en: Our data consists of 6 CSVs which we will convert into 6 tables. I won’t spend
    too much time going over the dataset as this post is more about using Snowflake
    and Dask rather than interpreting data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据由 6 个 CSV 文件组成，我们将其转换为 6 个表格。我不会花太多时间讲解数据集，因为这篇文章更多的是关于使用 Snowflake 和 Dask，而不是解释数据。
- en: Below are the commands we can use to create our tables. All you will need to
    know in advance is what columns and data types you will be working with.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们可以用来创建表格的命令。你需要提前了解的只是你将处理的列和数据类型。
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we have our tables created we can start thinking about how to get data
    into them. For this, we will need to stage our CSV files. This is basically just
    an intermediary step so Snowflake can directly load the files from our stage into
    our tables. We can use the **PUT** command to put local files in our stage and
    then the **COPY INTO** command to instruct Snowflake where to put this data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了表格，可以开始考虑如何将数据导入这些表格。为此，我们需要对 CSV 文件进行分阶段处理。这基本上只是一个中间步骤，以便 Snowflake
    可以直接从我们的阶段加载文件到表格中。我们可以使用**PUT**命令将本地文件放入阶段，然后使用**COPY INTO**命令指示 Snowflake 将数据放置到哪里。
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As a quick check, you can run this command to check what is in the staging area.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 作为快速检查，你可以运行这个命令来检查阶段区域中的内容。
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now we just need to copy the data into our tables using the queries below. You
    can execute these either in the Snowflake UI or in the command line after logging
    into Snowflake.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们只需使用下面的查询将数据复制到我们的表格中。你可以在 Snowflake UI 或登录 Snowflake 后在命令行中执行这些查询。
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Ok great, with any luck we have our data in our tables first try. Oh, if only
    it was that simple, this whole process took me a few tries to get right (beware
    of spelling things wrong). Hopefully, you can follow along with this and be good
    to go. We are getting closer to the interesting stuff but the steps above are
    a vital part of the process so make sure you understand each of these steps.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，如果运气好的话，我们第一次尝试时数据就会在表格中。哦，真希望这么简单，这整个过程我尝试了几次才搞对（注意拼写错误）。希望你能跟上这些步骤，并顺利完成。我们离有趣的部分越来越近，但上述步骤是过程中的关键部分，所以一定要理解每一步。
- en: Writing our Pipeline in SQL
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用 SQL 编写我们的管道
- en: In this next step, we will be writing the queries to generate our target, our
    features and then finally produce a training data set. One approach to creating
    a dataset for modelling is to read this data into memory and use pandas to create
    new features and join all the data frames together. This is typically the approach
    you see on Kaggle and in other online tutorials. The issue with this is that it
    is not very efficient, particularly when you are working with any reasonably sized
    datasets. For this reason, it is a much better idea to outsource the heavy lifting
    to something like Snowflake which handles massive datasets extremely well and
    will likely save you a huge amount of time. I won’t be spending much time diving
    into the specifics of our dataset here as it isn’t really vital for what I am
    trying to show. In general, though, you would want to spend a considerable amount
    of time exploring and understanding your data before you start modelling. The
    goal of these queries will be to preprocess the data and create some simple features
    which we can later use in our models.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将编写查询以生成我们的目标、特征，最后产生一个训练数据集。创建建模数据集的一种方法是将数据读入内存，使用 pandas 创建新特征并将所有数据框连接在一起。这通常是在
    Kaggle 和其他在线教程中看到的方法。这样做的问题是效率不是很高，特别是当你处理任何合理大小的数据集时。因此，最好将繁重的工作外包给像 Snowflake
    这样的工具，它非常擅长处理大规模数据集，并且可能会节省大量时间。我不会花太多时间深入探讨我们的数据集，因为这并不真正影响我想展示的内容。总的来说，你应该花相当多的时间来探索和理解你的数据，然后再开始建模。这些查询的目标是对数据进行预处理并创建一些简单的特征，我们可以在模型中使用这些特征。
- en: Target Definition
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标定义
- en: Obviously, a vital component of supervised machine learning is defining an appropriate
    target to predict. For our use case, we will be predicting churn by calculating
    whether or not a user makes another visit within two weeks after a cutoff week.
    The choice of 2 weeks is pretty arbitrary and will depend on the specific problem
    we are trying to solve but let’s just assume that it is fine for this project.
    In general, you would want to carefully analyse your customers to understand the
    distribution in gaps between visits to arrive at a suitable definition of churn.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，监督机器学习的一个关键组成部分是定义一个合适的目标进行预测。对于我们的使用案例，我们将通过计算用户在截止周后两周内是否再次访问来预测流失。选择两周是相当随意的，具体取决于我们试图解决的具体问题，但我们就假设这个项目中这样做是合适的。一般来说，你会想要仔细分析你的客户，以了解访问之间的间隔分布，从而得出一个合适的流失定义。
- en: The main idea here is that for each table we want to have one row per household_key
    containing values for each of our features.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的主要思想是，对于每个表，我们希望每个 household_key 具有每个特征的值的一行。
- en: Campaign Features
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动特征
- en: Transaction Features
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交易特征
- en: Below we create some simple metrics based on aggregate statistics such as the
    average, the max and standard deviation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们基于汇总统计信息（如平均值、最大值和标准差）创建一些简单的指标。
- en: Demographic Features
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人口统计特征
- en: This dataset has lots of missing data so I decided to use imputation here. There
    are plenty of techniques out there for missing data from dropping the missing
    data, to advanced imputation methods. I have just made life easy for myself here
    and replaced missing values with the mode. I wouldn’t necessarily recommend taking
    this approach in general as understanding why this data is missing is really important
    in deciding how to deal with it but for the purposes of this example, I will go
    ahead and take the easy approach. We first compute the mode for each of our features
    and then use coalesce to replace each row with the mode if data is missing.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集有很多缺失数据，所以我决定在这里使用插补。对于缺失数据，有很多技术，从丢弃缺失数据到高级插补方法。我在这里让自己简化了操作，用众数替换缺失值。我不会普遍推荐这种方法，因为理解数据缺失的原因对于决定如何处理它非常重要，但为了这个例子的目的，我会继续采用简单的方法。我们首先计算每个特征的众数，然后使用
    coalesce 来替换缺失的数据。
- en: Training Data
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练数据
- en: Finally, we build a query for our training data by joining our main tables together
    and end up with a table containing our target, our campaign, transactions and
    demographic features which we can use to build a model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过将主要表连接起来，构建一个用于训练数据的查询，并最终得到一个包含我们的目标、我们的活动、交易和人口统计特征的表，我们可以用来构建模型。
- en: As a brief aside, for those interested in learning more about the features and
    nuances of Snowflake I would recommend the following book: [**Snowflake Cookbook**](https://www.amazon.co.uk/gp/product/1800560613/ref=as_li_tl?ie=UTF8&camp=1634&creative=6738&creativeASIN=1800560613&linkCode=as2&tag=mediumdanny05-21&linkId=b016babb9c48e7f068b4a6bfa70c403c).
    I started reading this book and it is full of really helpful information on how
    to use Snowflake and goes into far more detail than I do here.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，对于那些有兴趣了解 Snowflake 的更多功能和细节的人，我推荐以下书籍：[**Snowflake Cookbook**](https://www.amazon.co.uk/gp/product/1800560613/ref=as_li_tl?ie=UTF8&camp=1634&creative=6738&creativeASIN=1800560613&linkCode=as2&tag=mediumdanny05-21&linkId=b016babb9c48e7f068b4a6bfa70c403c)。我开始阅读这本书，它包含了如何使用
    Snowflake 的非常有用的信息，并且详细程度远超我在这里所述的。
- en: Python Code for ETL
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python 代码用于 ETL
- en: The final piece we require for this ETL is to write a script to execute it.
    Now, this is only really required if you plan on running an ETL like this regularly
    but this is good practice and makes it much easier to run the ETL as and when
    needed.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个 ETL，最终需要写一个脚本来执行它。现在，如果你打算定期运行这样的 ETL，这确实是必要的，但这是一种良好的实践，并且使得在需要时运行 ETL
    更加容易。
- en: Let’s briefly discuss the main components of our EtlTraining class. Our class
    takes one input which is the cutoff week. This is due to the way data is defined
    in our dataset but ordinarily, this would be in a date format that corresponds
    to the cutoff date we want to choose for generating training data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 简要讨论一下我们 EtlTraining 类的主要组件。我们的类接受一个输入，即截止周。这是由于数据在数据集中被定义的方式，但通常，这将是一个与我们选择的生成训练数据的截止日期相对应的日期格式。
- en: We initialise a list of our queries so we can easily loop through these and
    execute them. We also create a dictionary containing our parameters which we pass
    to our Snowflake connection. Here we use environment variables that we set up
    in Saturn Cloud. [Here](https://saturncloud.io/docs/using-saturn-cloud/credentials/) is
    a guide on how to do this. It is not too difficult to connect to Snowflake, all
    we need to do is use the Snowflake connector and pass in our dictionary of credentials.
    We implement this in the Snowflake connect method and return this connection as
    an attribute.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们初始化了一个查询列表，以便可以轻松地循环遍历这些查询并执行它们。我们还创建了一个包含我们参数的字典，并将其传递给我们的Snowflake连接。在这里，我们使用了在Saturn
    Cloud中设置的环境变量。[这里](https://saturncloud.io/docs/using-saturn-cloud/credentials/)是关于如何做到这一点的指南。连接Snowflake并不太困难，我们只需要使用Snowflake连接器并传入我们的凭据字典即可。我们在Snowflake连接方法中实现了这一点，并将此连接作为属性返回。
- en: To make these queries a little bit easier to run I save each query as a python
    string variable in the ml_query_pipeline.py file. The execute_etl method does
    exactly what it says on the tin. We loop through each query, format it, execute
    it and finish off by closing the Snowflake connection.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这些查询更容易运行，我将每个查询保存为`python`字符串变量在ml_query_pipeline.py文件中。execute_etl方法正如其名，我们循环遍历每个查询，对其进行格式化，执行它，并最后关闭Snowflake连接。
- en: To run this ETL we can simply type the commands below into the terminal. (where
    ml_pipeline is the name of the script above.)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行这个ETL，我们可以简单地在终端中输入以下命令。（其中ml_pipeline是上面脚本的名称。）
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As a brief aside, you will probably want to run an ETL like this at regular
    intervals. For example, if you want to make daily predictions then you will need
    to generate a dataset like this each day to pass to your model so you can identify
    which of your customers are likely to churn. I won’t go into this in detail here
    but in my job, we use Airflow to orchestrate our ETLs so I would recommend checking
    it out if you are interested. In fact, I recently bought a book ‘[Data Pipelines
    with Apache Airflow](https://www.amazon.co.uk/gp/product/1617296902/ref=as_li_tl?ie=UTF8&camp=1634&creative=6738&creativeASIN=1617296902&linkCode=as2&tag=mediumdanny05-21&linkId=1ad3a1bf79e65482860570c3a484a73c)’
    which I think is great and really gives some solid examples and advice on how
    to use airflow.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，你可能希望定期运行像这样的ETL。例如，如果你想进行每日预测，那么你将需要每天生成一个这样的数据集以传递给你的模型，从而识别哪些客户可能会流失。我不会在这里详细讲解，但在我的工作中，我们使用Airflow来编排我们的ETL，因此如果你感兴趣，我建议你去了解一下。实际上，我最近买了一本书‘[Data
    Pipelines with Apache Airflow](https://www.amazon.co.uk/gp/product/1617296902/ref=as_li_tl?ie=UTF8&camp=1634&creative=6738&creativeASIN=1617296902&linkCode=as2&tag=mediumdanny05-21&linkId=1ad3a1bf79e65482860570c3a484a73c)’，我认为它非常棒，提供了一些很好的示例和关于如何使用Airflow的建议。
- en: Dask and Modeling
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dask和建模
- en: Now that we have our data pipeline built, we can begin to think about modelling.
    The other main goal I have for this post is to highlight the advantages of using
    Dask as part of the ML development process and show you guys how easy it is to
    use.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了数据管道，我们可以开始考虑建模。我这篇文章的另一个主要目标是突出使用**Dask**作为机器学习开发过程的一部分的优势，并向大家展示它的易用性。
- en: For this part of the project, I also used [Saturn Cloud](https://www.saturncloud.io/s/?utm_source=daniel-foley) which
    is a really nice tool I came across recently that allows us to harness the power
    of Dask across a cluster of computers in the cloud. The main advantages of using
    Saturn for me are that it is really easy to share your work, super simple to scale
    up your compute as and when you need it and it has a free tier option. Model development
    in general is a really good use case for Dask as we usually want to train a bunch
    of different models and see what works best. The faster we can do this the better
    as we have more time to focus on other important aspects of model development.
    Similar to Snowflake you just need to sign up [here](https://www.saturncloud.io/s/?utm_source=daniel-foley) and
    you can very quickly spin up an instance of Jupyter lab and start experimenting
    with it yourself.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目的部分，我还使用了[Saturn Cloud](https://www.saturncloud.io/s/?utm_source=daniel-foley)，这是我最近遇到的一个非常好的工具，它允许我们在云中通过计算机集群利用Dask的力量。对我来说，使用Saturn的主要优势是非常容易共享你的工作、在需要时简单地扩展计算资源，并且它有一个免费的选项。模型开发通常是Dask的一个很好的应用场景，因为我们通常想要训练一组不同的模型，看看哪个效果最好。我们能越快做到这一点越好，因为我们可以有更多时间专注于模型开发的其他重要方面。类似于Snowflake，你只需要[在这里](https://www.saturncloud.io/s/?utm_source=daniel-foley)注册，你可以非常快速地启动一个Jupyter
    lab实例并开始自己动手实验。
- en: Now, I realise at this point I have mentioned Dask a few times but have never
    really explained what it is. So let me take a moment to give you a very high-level
    overview of Dask and why I think it is awesome. Very simply, Dask is a python
    library that takes advantage of parallel computing to allow you to process and
    perform operations on very large datasets. And, the best part is, if you are already
    familiar with Python, then Dask should be very straightforward as the syntax is
    very similar.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我意识到我在这里提到 Dask 几次，但从未真正解释过它是什么。所以让我花点时间给你一个关于 Dask 的高层次概述，以及为什么我认为它很棒。简单来说，Dask
    是一个 Python 库，利用并行计算来处理和执行非常大的数据集上的操作。而且，最棒的是，如果你已经熟悉 Python，那么 Dask 应该非常直接，因为其语法非常相似。
- en: The graph below highlights the main components of Dask.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 下图突出显示了 Dask 的主要组件。
- en: '![](../Images/1a682f60d9ac33bfbfc70cbb981a4bf0.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a682f60d9ac33bfbfc70cbb981a4bf0.png)'
- en: Source: [Dask Documentation](https://docs.dask.org/en/latest/)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 来源: [Dask 文档](https://docs.dask.org/en/latest/)
- en: Collections allow us to create a graph of tasks which can then be executed across
    multiple computers. Some of these data structures probably sound pretty familiar
    such as arrays and data frames and they are similar to what you would find in
    python but with some important differences. For example, you can think of a Dask
    data frame as a bunch of pandas data frames built in such a way that allows us
    to perform operations in parallel.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Collections 允许我们创建一个任务图，这些任务图可以在多个计算机上执行。这些数据结构中有些可能听起来很熟悉，比如数组和数据框，它们类似于你在
    Python 中会遇到的，但有一些重要的不同之处。例如，你可以把 Dask 数据框看作是一个由 pandas 数据框组成的集合，这些数据框以一种可以让我们并行执行操作的方式构建。
- en: Moving on from collections we have the scheduler. Once we create the task graph
    the scheduler handles the rest for us. It manages the workflow and sends these
    tasks to either a single machine or distributes them across a cluster. Hopefully,
    that gives you a very brief overview of how Dask works. For more info, I suggest
    checking out the [documentation](https://docs.dask.org/en/latest/) or this [book](https://www.amazon.co.uk/gp/product/1617295604/ref=as_li_tl?ie=UTF8&camp=1634&creative=6738&creativeASIN=1617295604&linkCode=as2&tag=mediumdanny05-21&linkId=fbcbf83b35fe6ce3c909ba9ece7001af).
    Both are very good resources to dig deeper into this topic.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从 collections 说到调度器。一旦我们创建了任务图，调度器就会处理剩下的工作。它管理工作流程，并将这些任务发送到单台机器或分布到集群中。希望这能给你一个关于
    Dask 工作原理的简要概述。欲了解更多信息，我建议你查看 [文档](https://docs.dask.org/en/latest/) 或这本 [书](https://www.amazon.co.uk/gp/product/1617295604/ref=as_li_tl?ie=UTF8&camp=1634&creative=6738&creativeASIN=1617295604&linkCode=as2&tag=mediumdanny05-21&linkId=fbcbf83b35fe6ce3c909ba9ece7001af)。这两者都是深入了解该主题的非常好资源。
- en: Python Code for Modelling
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python 建模代码
- en: When modelling, I tend to have a small number of go-to algorithms that I will
    always try out first. This will generally give me a good idea of what might be
    suited to the specific problem I have. These models are Logistic Regression, Random
    Forest and GradientBoosting. In my experience, when working with tabular data
    these algorithms will usually give you pretty good results. Below we build a sklearn
    modelling pipeline using these 3 models. The exact models we use here are not
    really important as the pipeline should work for any sklearn classification model,
    this is just my preference.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在建模时，我倾向于使用少量的常用算法，这些算法是我总是会首先尝试的。这通常会让我对可能适合我具体问题的模型有一个很好的了解。这些模型包括 Logistic
    Regression、Random Forest 和 GradientBoosting。在我的经验中，处理表格数据时，这些算法通常会给出相当不错的结果。下面我们使用这三种模型构建一个
    sklearn 建模管道。我们在这里使用的具体模型并不是特别重要，因为该管道应该适用于任何 sklearn 分类模型，这只是我的偏好。
- en: Without further ado, let’s dive into the code. Luckily we outsourced most of
    our preprocessing to Snowflake so we don’t have to mess around with our training
    data too much here but we will add a few additional steps using sklearn pipelines.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 不再废话，让我们直接进入代码。幸运的是，我们将大部分预处理工作外包给了 Snowflake，因此在这里我们不需要过多处理训练数据，但我们将使用 sklearn
    管道添加一些额外的步骤。
- en: The first code snippet below shows the pipeline when using sklearn. Notice our
    dataset is a plain old pandas data frame and our preprocessing steps are all carried
    out using sklearn methods. There is nothing particularly out of the ordinary going
    on here. We are reading in our data from the table produced by our Snowflake ETL
    and passing this into a sklearn pipeline. The usual modelling steps apply here.
    We split the dataset into train and test and do some preprocessing, namely impute
    missing values using the median, scale the data and one-hot encode our categorical
    data. I am a big fan of sklearn pipelines and basically use them whenever I develop
    models nowadays, they really facilitate clean and concise code.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 下方的第一个代码片段展示了使用 sklearn 的管道。注意我们的数据集是一个普通的 pandas 数据框，我们的预处理步骤都是通过 sklearn 方法完成的。这里没有特别不同的地方。我们从
    Snowflake ETL 生成的表中读取数据，并将其传递到 sklearn 管道中。这里应用了常规的建模步骤。我们将数据集拆分为训练集和测试集，并进行一些预处理，即使用中位数填补缺失值，缩放数据并对分类数据进行独热编码。我非常喜欢
    sklearn 管道，并且在开发模型时基本上都会使用它们，它们确实有助于编写干净简洁的代码。
- en: How does this pipeline perform on a dataset with about 2 million rows? Well,
    running this model without any hyperparameter tuning takes about 34 minutes. Ouch,
    kinda slow. You can imagine how prohibitively long this would take if we wanted
    to do any type of hyperparameter tuning. Ok, so not ideal but let’s see how Dask
    handles the challenge.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个管道在一个大约有 200 万行的数据集上的表现如何？好吧，不进行任何超参数调优的情况下运行这个模型大约需要 34 分钟。哎，有点慢。如果我们想进行任何类型的超参数调优，你可以想象这将花费多么漫长的时间。好的，所以并不理想，但让我们看看
    Dask 如何应对这个挑战。
- en: Dask ML Python Code
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dask ML Python 代码
- en: Our goal here is to see if we can beat the sklearn pipeline above, spoiler alert,
    we definitely can. The cool thing about Dask is that the barrier to entry when
    you are already familiar with python is pretty low. We can get this pipeline up
    and running in Dask with only a few changes.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是看看是否可以超越上述 sklearn 管道，剧透一下，我们绝对可以。Dask 的酷炫之处在于，当你已经熟悉 Python 时，上手的门槛相对较低。我们只需进行几处更改，就可以在
    Dask 中启动并运行这个管道。
- en: The first change you probably will notice is that we have some different imports.
    One of the key differences between this pipeline and the previous one is that
    we will be using a Dask data frame instead of a pandas data frame to train our
    model. You can think of a Dask data frame as a bunch of pandas data frames where
    we can perform computations on each one at the same time. This is the core of
    Dask’s parallelism and is what is going to reduce the training time for this pipeline.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到的第一个变化是我们有一些不同的导入。这条管道与之前的主要区别之一是我们将使用 Dask 数据框而不是 pandas 数据框来训练我们的模型。你可以把
    Dask 数据框想象成一堆 pandas 数据框，我们可以同时在每一个上执行计算。这是 Dask 并行性的核心，也是减少这个管道训练时间的关键所在。
- en: Notice we use ***@dask.delayed ***as a decorator to our ***load_training_data* **function.
    This instructs Dask to parallelise this function for us.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们使用 ***@dask.delayed*** 作为装饰器来装饰我们的 ***load_training_data*** 函数。这指示 Dask
    为我们并行化这个函数。
- en: We are also going to import some preprocessing and pipeline methods from Dask
    and most importantly, we will need to import SaturnCluster which will allow us
    to create a cluster for training our models. Another key difference with this
    code is that we use ***dask.persist ***after our train test split. Before this
    point, none of our functions has actually been computed due to Dask’s lazy evaluation.
    Once we use the persist method though we are telling Dask to send our data to
    the workers and execute the tasks we have created up until this point and leave
    these objects on the cluster.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将从 Dask 导入一些预处理和管道方法，更重要的是，我们需要导入 **SaturnCluster**，它将允许我们创建一个集群来训练我们的模型。另一个关键的不同点是，在我们的训练测试拆分之后，我们使用了
    ***dask.persist***。在此之前，由于 Dask 的延迟评估，我们的函数实际上并没有被计算。 一旦我们使用 persist 方法，我们就在告诉
    Dask 将数据发送到工作节点，执行我们到目前为止创建的任务，并将这些对象保留在集群上。
- en: 'Finally, we train our models using the delayed method. Again, this enables
    us to create our pipeline in a lazy way. The pipeline is not executed until we
    reach this code:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用延迟方法训练我们的模型。这样，我们能够以懒惰的方式创建管道。管道不会被执行，直到我们到达这段代码：
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This time it only took us around 10 minutes to run this pipeline on the exact
    same dataset. That is a speedup by a factor of 3.4, not too shabby. Now, if we
    wanted to, we could speed this up even more by scaling up our compute resources
    at the touch of a button in Saturn.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们只花了大约 `10 minutes` 就在完全相同的数据集上运行了这个管道。这是提高了 3.4 倍的速度，表现不错。如果我们愿意的话，我们还可以通过在
    Saturn 中一键扩展计算资源进一步加速。
- en: Deploying our Pipeline
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署我们的管道
- en: 'I mentioned earlier that you will probably want to run a pipeline like this
    quite regularly using something like airflow. It just so happens that if you don’t
    want the initial hassle of setting everything up for airflow Saturn Cloud offers
    a simple alternative with Jobs. Jobs allow us to package up our code and run it
    at regular intervals or as needed. All you need to do is go to an existing project
    and click on create a job. Once we do that, it should look like the following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前提到过，你可能会想要定期运行这样的管道，使用类似 airflow 的工具。恰好的是，如果你不想经历设置 airflow 的初始麻烦，Saturn
    Cloud 提供了一个简单的替代方案，即 Jobs。Jobs 允许我们打包代码，并按需或在固定间隔内运行。你只需进入现有项目并点击创建作业。一旦我们这样做，它应该会像以下这样：
- en: '![](../Images/8f159fecc5ad6c10629cf007cf4c029d.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f159fecc5ad6c10629cf007cf4c029d.png)'
- en: Source: [Saturn](https://saturncloud.io/docs/using-saturn-cloud/jobs_and_deployments/)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 来源: [Saturn](https://saturncloud.io/docs/using-saturn-cloud/jobs_and_deployments/)
- en: From here, all we need to do is make sure our python files above are in the
    directory in the image and we can enter our python command above
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们需要确保上面的 Python 文件在图像中的目录中，然后可以输入上面的 Python 命令。
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can also set up a schedule using cron syntax to run the ETL on a daily basis
    if we like. For those interested, here is a [Tutorial](https://saturncloud.io/docs/using-saturn-cloud/jobs_and_deployments/) that
    goes into all the nitty-gritty.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，我们还可以使用 cron 语法设置日常 ETL 任务。对那些感兴趣的人，这里有一个 [教程](https://saturncloud.io/docs/using-saturn-cloud/jobs_and_deployments/) 详细讲解所有细节。
- en: Conclusions and Takeaways
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论和收获
- en: Well, we have reached the end of our project at this point. Now obviously I
    have left out some key parts of the ML development cycle such as hyperparameter
    tuning and deploying our model but perhaps I will leave that for another day.
    Do I think you should try Dask? I am no expert by any means but from what I have
    seen so far it certainly seems really useful and I am super excited to experiment
    more with it and find more opportunities to incorporate it into my daily work
    as a data scientist. Hopefully, you found this useful and you too can see some
    of the advantages of Snowflake and Dask and you will start experimenting with
    them on your own.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，我们现在已经到了项目的最后阶段。显然，我省略了一些 ML 开发周期的关键部分，如超参数调优和模型部署，但也许我会留到另一天。我认为你应该尝试 Dask
    吗？我并不是专家，但从我目前看到的情况来看，它确实非常有用，我非常期待进一步实验，并寻找更多将其融入我作为数据科学家的日常工作的机会。希望你觉得这有用，并且你也能看到
    Snowflake 和 Dask 的一些优点，开始自己动手尝试。
- en: Resources
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 资源
- en: '[Data Pipelines with Apache Airflow](https://www.amazon.co.uk/gp/product/1617296902/ref=as_li_tl?ie=UTF8&camp=1634&creative=6738&creativeASIN=1617296902&linkCode=as2&tag=mediumdanny05-21&linkId=1ad3a1bf79e65482860570c3a484a73c)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Apache Airflow 的数据管道](https://www.amazon.co.uk/gp/product/1617296902/ref=as_li_tl?ie=UTF8&camp=1634&creative=6738&creativeASIN=1617296902&linkCode=as2&tag=mediumdanny05-21&linkId=1ad3a1bf79e65482860570c3a484a73c)'
- en: '[Snowflake Cookbook](https://www.amazon.co.uk/gp/product/1800560613/ref=as_li_tl?ie=UTF8&camp=1634&creative=6738&creativeASIN=1800560613&linkCode=as2&tag=mediumdanny05-21&linkId=b016babb9c48e7f068b4a6bfa70c403c)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Snowflake 食谱](https://www.amazon.co.uk/gp/product/1800560613/ref=as_li_tl?ie=UTF8&camp=1634&creative=6738&creativeASIN=1800560613&linkCode=as2&tag=mediumdanny05-21&linkId=b016babb9c48e7f068b4a6bfa70c403c)'
- en: '[Data Science at Scale with Python and Dask](https://www.amazon.co.uk/gp/product/1617295604/ref=as_li_tl?ie=UTF8&camp=1634&creative=6738&creativeASIN=1617295604&linkCode=as2&tag=mediumdanny05-21&linkId=fbcbf83b35fe6ce3c909ba9ece7001af)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 和 Dask 进行大规模数据科学](https://www.amazon.co.uk/gp/product/1617295604/ref=as_li_tl?ie=UTF8&camp=1634&creative=6738&creativeASIN=1617295604&linkCode=as2&tag=mediumdanny05-21&linkId=fbcbf83b35fe6ce3c909ba9ece7001af)'
- en: '[Coursera: SQL for Data Science](https://click.linksynergy.com/deeplink?id=z2stMJEP3T4&mid=40328&murl=https%3A%2F%2Fwww.coursera.org%2Fspecializations%2Flearn-sql-basics-data-science%23courses)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Coursera: 数据科学中的 SQL](https://click.linksynergy.com/deeplink?id=z2stMJEP3T4&mid=40328&murl=https%3A%2F%2Fwww.coursera.org%2Fspecializations%2Flearn-sql-basics-data-science%23courses)'
- en: Some of my other posts you may find interesting
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你可能会发现我其他的一些文章很有趣
- en: '[**Let’s Build a Streaming Data Pipeline**](https://towardsdatascience.com/lets-build-a-streaming-data-pipeline-e873d671fc57)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[**让我们构建一个流式数据管道**](https://towardsdatascience.com/lets-build-a-streaming-data-pipeline-e873d671fc57)'
- en: '[**Gaussian Mixture Modelling (GMM)**](https://towardsdatascience.com/gaussian-mixture-modelling-gmm-833c88587c7f)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[**高斯混合模型 (GMM)**](https://towardsdatascience.com/gaussian-mixture-modelling-gmm-833c88587c7f)'
- en: '[**A Bayesian Approach to Time Series Forecasting**](https://towardsdatascience.com/a-bayesian-approach-to-time-series-forecasting-d97dd4168cb7)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[**一种贝叶斯时间序列预测方法**](https://towardsdatascience.com/a-bayesian-approach-to-time-series-forecasting-d97dd4168cb7)'
- en: '*Note: Some of the links in this post are affiliate links.*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：本文中的部分链接为附属链接。*'
- en: '**Bio: [Daniel Foley](https://www.linkedin.com/in/daniel-foley-1ab904a2/)**
    is a former Economist turned Data Scientist working in the mobile gaming industry.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介： [丹尼尔·福伊](https://www.linkedin.com/in/daniel-foley-1ab904a2/)** 是一位曾经的经济学家，现转行成为从事移动游戏行业的数据科学家。'
- en: '[Original](https://towardsdatascience.com/building-machine-learning-pipelines-using-snowflake-and-dask-10ae5e7fff0f).
    Reposted with permission.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/building-machine-learning-pipelines-using-snowflake-and-dask-10ae5e7fff0f)。经授权转载。'
- en: '**Related:**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[BigQuery vs Snowflake: A Comparison of Data Warehouse Giants](/2021/06/bigquery-snowflake-comparison-data-warehouse-giants.html)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BigQuery 与 Snowflake：数据仓库巨头的比较](/2021/06/bigquery-snowflake-comparison-data-warehouse-giants.html)'
- en: '[Pandas not enough? Here are a few good alternatives to processing larger and
    faster data in Python](/2021/07/pandas-alternatives-processing-larger-faster-data-python.html)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Pandas 不够用？这里有几个处理更大更快数据的 Python 备选方案](/2021/07/pandas-alternatives-processing-larger-faster-data-python.html)'
- en: '[Are You Still Using Pandas to Process Big Data in 2021? Here are two better
    options](/2021/03/pandas-big-data-better-options.html)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你还在用 Pandas 处理 2021 年的大数据吗？这里有两个更好的选择](/2021/03/pandas-big-data-better-options.html)'
- en: More On This Topic
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Sky''s the Limit: Learn how JetBlue uses Monte Carlo and Snowflake…](https://www.kdnuggets.com/2022/12/monte-carlo-jetblue-snowflake-build-trust-improve-model-accuracy.html)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[天空才是极限：了解 JetBlue 如何利用 Monte Carlo 和 Snowflake…](https://www.kdnuggets.com/2022/12/monte-carlo-jetblue-snowflake-build-trust-improve-model-accuracy.html)'
- en: '[Building Data Science Pipelines Using Pandas](https://www.kdnuggets.com/building-data-science-pipelines-using-pandas)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Pandas 构建数据科学管道](https://www.kdnuggets.com/building-data-science-pipelines-using-pandas)'
- en: '[Data Warehousing with Snowflake for Beginners](https://www.kdnuggets.com/2022/02/data-warehousing-snowflake-beginners.html)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[初学者的 Snowflake 数据仓库](https://www.kdnuggets.com/2022/02/data-warehousing-snowflake-beginners.html)'
- en: '[Top 6 Tools to Improve Your Productivity on Snowflake](https://www.kdnuggets.com/2023/08/top-6-tools-improve-productivity-snowflake.html)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提高在 Snowflake 上工作效率的 6 大工具](https://www.kdnuggets.com/2023/08/top-6-tools-improve-productivity-snowflake.html)'
- en: '[How to Build a Streaming Semi-structured Analytics Platform on Snowflake](https://www.kdnuggets.com/2023/07/build-streaming-semistructured-analytics-platform-snowflake.html)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在 Snowflake 上构建流式半结构化分析平台](https://www.kdnuggets.com/2023/07/build-streaming-semistructured-analytics-platform-snowflake.html)'
- en: '[Building Data Pipelines to Create Apps with Large Language Models](https://www.kdnuggets.com/building-data-pipelines-to-create-apps-with-large-language-models)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[构建数据管道以创建大型语言模型应用](https://www.kdnuggets.com/building-data-pipelines-to-create-apps-with-large-language-models)'
