- en: A Quick Introduction to Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2016/11/quick-introduction-neural-networks.html/3](https://www.kdnuggets.com/2016/11/quick-introduction-neural-networks.html/3)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Multi Layer Perceptron shown in Figure 5 (adapted from Sebastian Raschka's
    [excellent visual explanation of the backpropagation algorithm](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation.md))
    has two nodes in the input layer (apart from the Bias node) which take the inputs
    'Hours Studied' and 'Mid Term Marks'. It also has a hidden layer with two nodes
    (apart from the Bias node). The output layer has two nodes as well - the upper
    node outputs the probability of 'Pass' while the lower node outputs the probability
    of 'Fail'.
  prefs: []
  type: TYPE_NORMAL
- en: In classification tasks, we generally use a [Softmax function](http://cs231n.github.io/linear-classify/#softmax)
    as the Activation Function in the Output layer of the Multi Layer Perceptron to
    ensure that the outputs are probabilities and they add up to 1\. The Softmax function
    takes a vector of arbitrary real-valued scores and squashes it to a vector of
    values between zero and one that sum to one. So, in this case,
  prefs: []
  type: TYPE_NORMAL
- en: Probability (Pass) + Probability (Fail) = 1
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Forward Propagation**'
  prefs: []
  type: TYPE_NORMAL
- en: All weights in the network are randomly assigned. Lets consider the hidden layer
    node marked **V** in Figure 5 below. Assume the weights of the connections from
    the inputs to that node are w1, w2 and w3 (as shown).
  prefs: []
  type: TYPE_NORMAL
- en: The network then takes the first training example as input (we know that for
    inputs 35 and 67, the probability of Pass is 1).
  prefs: []
  type: TYPE_NORMAL
- en: Input to the network = [35, 67]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Desired output from the network (target) = [1, 0]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then output V from the node in consideration can be calculated as below (*****f*****
    is an activation function such as sigmoid):'
  prefs: []
  type: TYPE_NORMAL
- en: V = ***f*** (1*w1 + 35*w2 + 67*w3)
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, outputs from the other node in the hidden layer is also calculated.
    The outputs of the two nodes in the hidden layer act as inputs to the two nodes
    in the output layer. This enables us to calculate output probabilities from the
    two nodes in output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the output probabilities from the two nodes in the output layer are
    0.4 and 0.6 respectively (since the weights are randomly assigned, outputs will
    also be random). We can see that the calculated probabilities (0.4 and 0.6) are
    very far from the desired probabilities (1 and 0 respectively), hence the network
    in Figure 5 is said to have an 'Incorrect Output'.
  prefs: []
  type: TYPE_NORMAL
- en: '![Screen Shot 2016-08-09 at 11.52.57 PM.png](../Images/03b9ea028c3e2f38889c14e169255473.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: forward propagation step in a multi layer perceptron'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Step 2: Back Propagation and Weight Updating**'
  prefs: []
  type: TYPE_NORMAL
- en: We calculate the total error at the output nodes and propagate these errors
    back through the network using Backpropagation to calculate the *gradients*. Then
    we use an optimization method such as *Gradient Descent* to 'adjust' **all** weights
    in the network with an aim of reducing the error at the output layer. This is
    shown in the Figure 6 below (ignore the mathematical equations in the figure for
    now).
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that the new weights associated with the node in consideration are w4,
    w5 and w6 (after Backpropagation and adjusting weights).
  prefs: []
  type: TYPE_NORMAL
- en: '![Screen Shot 2016-08-09 at 11.53.06 PM.png](../Images/658b72bb9d8a4b1f43dc9feaf05e91ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: backward propagation and weight updation step in a multi layer perceptron'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If we now input the same example to the network again, the network should perform
    better than before since the weights have now been adjusted to minimize the error
    in prediction. As shown in Figure 7, the errors at the output nodes now reduce
    to [0.2, -0.2] as compared to [0.6, -0.4] earlier. This means that our network
    has learned to correctly classify our first training example.
  prefs: []
  type: TYPE_NORMAL
- en: '![Screen Shot 2016-08-09 at 11.53.15 PM.png](../Images/b303562a02f5729aa8178e2433fb4808.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: the MLP network now performs better on the same input'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We repeat this process with all other training examples in our dataset. Then,
    our network is said to have *learned* those examples.
  prefs: []
  type: TYPE_NORMAL
- en: If we now want to predict whether a student studying 25 hours and having 70
    marks in the mid term will pass the final term, we go through the forward propagation
    step and find the output probabilities for Pass and Fail.
  prefs: []
  type: TYPE_NORMAL
- en: I have avoided mathematical equations and explanation of concepts such as 'Gradient
    Descent' here and have rather tried to develop an intuition for the algorithm.
    For a more mathematically involved discussion of the Backpropagation algorithm,
    refer to [this link](http://home.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html).
  prefs: []
  type: TYPE_NORMAL
- en: 3d Visualization of a Multi Layer Perceptron
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Adam Harley has created a [3d visualization](http://scs.ryerson.ca/~aharley/vis/fc/)
    of a Multi Layer Perceptron which has already been trained (using Backpropagation)
    on the MNIST Database of handwritten digits.
  prefs: []
  type: TYPE_NORMAL
- en: The network takes 784 numeric pixel values as inputs from a 28 x 28 image of
    a handwritten digit (it has 784 nodes in the Input Layer corresponding to pixels).
    The network has 300 nodes in the first hidden layer, 100 nodes in the second hidden
    layer, and 10 nodes in the output layer (corresponding to the 10 digits) [15].
  prefs: []
  type: TYPE_NORMAL
- en: Although the network described here is much larger (uses more hidden layers
    and nodes) compared to the one we discussed in the previous section, all computations
    in the forward propagation step and backpropagation step are done in the same
    way (at each node) as discussed before.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8 shows the network when the input is the digit '5'.
  prefs: []
  type: TYPE_NORMAL
- en: '![Screen Shot 2016-08-09 at 5.45.34 PM.png](../Images/c07f23238abc74838a1b91b796fb316c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: visualizing the network for an input of ''5'''
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A node which has a higher output value than others is represented by a brighter
    color. In the Input layer, the bright nodes are those which receive higher numerical
    pixel values as input. Notice how in the output layer, the only bright node corresponds
    to the digit 5 (it has an output probability of 1, which is higher than the other
    nine nodes which have an output probability of 0). This indicates that the MLP
    has correctly classified the input digit. I highly recommend playing around with
    this visualization and observing connections between nodes of different layers.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Neural Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[What is the difference between deep learning and usual machine learning?](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning.md)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[What is the difference between a neural network and a deep neural network?](http://stats.stackexchange.com/questions/182734/what-is-the-difference-between-a-neural-network-and-a-deep-neural-network?rq=1)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[How is deep learning different from multilayer perceptron?](https://www.quora.com/How-is-deep-learning-different-from-multilayer-perceptron)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: I have skipped important details of some of the concepts discussed in this post
    to facilitate understanding. I would recommend going through [Part1](http://cs231n.github.io/neural-networks-1/),
    [Part2](http://cs231n.github.io/neural-networks-2/), [Part3](http://cs231n.github.io/neural-networks-3/)
    and [Case Study](http://cs231n.github.io/neural-networks-case-study/) from Stanford's
    Neural Network tutorial for a thorough understanding of Multi Layer Perceptrons.
  prefs: []
  type: TYPE_NORMAL
- en: Let me know in the comments below if you have any questions or suggestions!
  prefs: []
  type: TYPE_NORMAL
- en: 'Bio: [![ujjwal-karn-150](../Images/a2ca7f20cf4747d5e6567bb0a4741fcc.png)Ujjwal
    Karn](https://ujjwalkarn.me/) has 3 years of industry and research experience
    in machine learning and is interested in practical applications of deep learning
    to language and vision understanding.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Artificial Neuron Models](https://www.willamette.edu/~gorr/classes/cs449/ann-overview.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Neural Networks Part 1: Setting up the Architecture (Stanford CNN Tutorial)](http://cs231n.github.io/neural-networks-1/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Wikipedia article on Feed Forward Neural Network](https://en.wikipedia.org/wiki/Feedforward_neural_network)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Wikipedia article on Perceptron](https://en.wikipedia.org/wiki/Perceptron)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Single-layer Neural Networks (Perceptrons)](http://computing.dcu.ie/~humphrys/Notes/Neural/single.neural.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Single Layer Perceptrons](http://www.cs.stir.ac.uk/courses/ITNP4B/lectures/kms/2-Perceptrons.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Weighted Networks – The Perceptron](http://page.mi.fu-berlin.de/rojas/neural/chapter/K3.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Neural network models (supervised) (scikit learn documentation)](http://scikit-learn.org/dev/modules/neural_networks_supervised.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[What does the hidden layer in a neural network compute?](http://stats.stackexchange.com/a/63163/53914)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[How to choose the number of hidden layers and nodes in a feedforward neural
    network?](http://stats.stackexchange.com/a/1097/53914)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Crash Introduction to Artificial Neural Networks](http://ulcar.uml.edu/~iag/CS/Intro-to-ANN.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Why the BIAS is necessary in ANN? Should we have separate BIAS for each layer?](http://stackoverflow.com/questions/7175099/why-the-bias-is-necessary-in-ann-should-we-have-separate-bias-for-each-layer)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Basic Neural Network Tutorial – Theory](https://takinginitiative.wordpress.com/2008/04/03/basic-neural-network-tutorial-theory/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Neural Networks Demystified (Video Series): Part 1, Welch Labs @ MLconf SF](https://www.youtube.com/watch?v=5MXp9UUkSmc)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. W. Harley, "An Interactive Node-Link Visualization of Convolutional Neural
    Networks," in ISVC, pages 867-877, 2015 ([link](http://scs.ryerson.ca/~aharley/vis/harley_vis_isvc15.pdf))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Original](https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/).'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Quick Data Science Tips and Tricks to Learn SAS](https://www.kdnuggets.com/2022/05/sas-quick-data-science-tips-tricks-learn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Pandas Plotting Functions for Quick Data Visualization](https://www.kdnuggets.com/7-pandas-plotting-functions-for-quick-data-visualization)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Quick Overview of Voronoi Diagrams](https://www.kdnuggets.com/2022/11/quick-overview-voronoi-diagrams.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Quick Guide to Find the Right Minds for Annotation](https://www.kdnuggets.com/2022/04/quick-guide-find-right-minds-annotation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Neural Networks Don''t Lead Us Towards AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
