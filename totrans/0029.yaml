- en: The Promise of Edge AI and Approaches for Effective Adoption
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/the-promise-of-edge-ai-and-approaches-for-effective-adoption](https://www.kdnuggets.com/the-promise-of-edge-ai-and-approaches-for-effective-adoption)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![The Promise of Edge AI and Approaches for Effective Adoption](../Images/366302b2c027adc5b084a96d267af716.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Editor
  prefs: []
  type: TYPE_NORMAL
- en: The current technological landscape is experiencing a pivotal shift towards
    edge computing, spurred by rapid advancements in generative AI (GenAI) and traditional
    AI workloads. Historically reliant on cloud computing, these AI workloads are
    now encountering the limits of cloud-based AI, including concerns over data security,
    sovereignty, and network connectivity.
  prefs: []
  type: TYPE_NORMAL
- en: Working around these limitations of cloud-based AI, organizations are looking
    to embrace edge computing. Edge computing’s ability to enable real-time analysis
    and responses at the point where data is created and consumed is why organizations
    see it as critical for AI innovation and business growth.
  prefs: []
  type: TYPE_NORMAL
- en: With its promise of faster processing with zero-to-minimal latency, edge AI
    can dramatically transform emerging applications. While the edge device computing
    capabilities are increasingly getting better, there are still limitations that
    can make implementing highly accurate AI models difficult. Technologies and approaches
    such as model quantization, imitation learning, distributed inferencing and distributed
    data management can help remove the barriers to more efficient and cost-effective
    edge AI deployments so organizations can tap into their true potential.
  prefs: []
  type: TYPE_NORMAL
- en: A “Cloud-Only” Approach to AI Won’t Meet The Needs of Next-Gen Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI inference in the cloud is often impacted by latency issues, causing delays
    in data movement between devices and cloud environments. Organizations are realizing
    the cost of moving data across regions, into the cloud, and back and forth from
    the cloud to the edge. It can hinder applications that require extremely fast,
    real-time responses, such as financial transactions or industrial safety systems.
    Additionally, when organizations must run AI-powered applications in remote locations
    where network connectivity is unreliable, the cloud isn’t always in reach.
  prefs: []
  type: TYPE_NORMAL
- en: The limitations of a "cloud-only" AI strategy are becoming increasingly evident,
    especially for next-generation AI-powered applications that demand fast, real-time
    responses. Issues such as network latency can slow insights and reasoning that
    can be delivered to the application in the cloud, leading to delays and increased
    costs associated with data transmission between the cloud and edge environments.
    This is particularly problematic for real-time applications, especially in remote
    areas with intermittent network connectivity. As AI takes center stage in decision-making
    and reasoning, the physics of moving data around can be extremely costly with
    a negative impact on business outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: '[Gartner](https://www.gartner.com/en/newsroom/press-releases/2023-08-01-gartner-identifies-top-trends-shaping-future-of-data-science-and-machine-learning.html)
    predicts that more than 55% of all data analysis by deep neural networks will
    occur at the point of capture in an edge system by 2025, up from less than 10%
    in 2021\. Edge computing helps alleviate latency, scalability, data security,
    connectivity and more challenges, reshaping the way data processing is handled
    and, in turn, accelerating AI adoption. Developing applications with an offline-first
    approach will be critical for the success of agile applications.'
  prefs: []
  type: TYPE_NORMAL
- en: With an effective edge strategy, organizations can get more value from their
    applications and make business decisions faster.
  prefs: []
  type: TYPE_NORMAL
- en: Edge AI Made Possible With Evolving Technologies, Approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As AI models become increasingly sophisticated and application architectures
    grow more complex, the challenge of deploying these models on edge devices with
    computational constraints becomes more pronounced. However, advancements in technology
    and evolving methodologies are paving the way for the efficient integration of
    powerful AI models within the edge computing framework ranging from:'
  prefs: []
  type: TYPE_NORMAL
- en: Model Compression and Quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Techniques such as model pruning and quantization are crucial for reducing the
    size of AI models without significantly compromising their accuracy. Model pruning
    eliminates redundant or non-critical information from the model, while quantization
    reduces the precision of the numbers used in the model's parameters, making the
    models lighter and faster to run on resource-constrained devices. Model Quantization
    is a technique that involves compressing large AI models to improve portability
    and reduce model size, making models more lightweight and suitable for edge deployments.
    Using fine-tuning techniques, including Generalized Post-Training Quantization
    (GPTQ), Low-Rank Adaptation (LoRA) and Quantized LoRA (QLoRA), model quantization
    lowers the numerical precision of model parameters, making models more efficient
    and accessible for edge devices like tablets, edge gateways and mobile phones.
  prefs: []
  type: TYPE_NORMAL
- en: Edge-Specific AI Frameworks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The development of AI frameworks and libraries specifically designed for edge
    computing can simplify the process of deploying edge AI workloads. These frameworks
    are optimized for the computational limitations of edge hardware and support efficient
    model execution with minimal performance overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Databases with Distributed Data Management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With capabilities such as vector search and real-time analytics, help meet the
    edge’s operational requirements and support local data processing, handling various
    data types, such as audio, images and sensor data. This is especially important
    in real-time applications like autonomous vehicle software, where diverse data
    types are constantly being collected and must be analyzed in real-time.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Inferencing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Which places models or workloads across multiple edge devices with local data
    samples without actual data exchange can mitigate potential compliance and data
    privacy issues. For applications, such as smart cities and industrial IoT, that
    involve many edge and IoT devices, distributing inferencing is crucial to take
    into account.
  prefs: []
  type: TYPE_NORMAL
- en: Balancing the Placement of AI Workloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While AI has been predominantly processed in the cloud, finding a balance with
    edge will be critical to accelerating AI initiatives. Most, if not all, industries
    have recognized AI and GenAI as a competitive advantage, which is why gathering,
    analyzing and quickly gaining insights at the edge will be increasingly important.
    As organizations evolve their AI use, implementing model quantization, multimodal
    capabilities, data platforms and other edge strategies will help drive real-time,
    meaningful business outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: '**[](https://www.linkedin.com/in/pradhanrahul/)**[Rahul Pradhan](https://www.linkedin.com/in/pradhanrahul/)****
    is VP of Product and Strategy at Couchbase (NASDAQ: BASE), provider of a leading
    modern database for enterprise applications that 30% of the Fortune 100 depend
    on. Rahul has over 20 years of experience leading and managing both engineering
    and product teams focusing on databases, storage, networking, and security technologies
    in the cloud. Before Couchbase, he led the Product Management and Business Strategy
    team for Dell EMC''s Emerging Technologies and Midrange Storage Divisions to bring
    all flash NVMe, Cloud, and SDS products to market.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Cloud Storage Adoption is the Need of the Hour for Business](https://www.kdnuggets.com/2022/02/cloud-storage-adoption-need-hour-business.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How Has the Adoption of AI in Algorithmic Trading Affected the…](https://www.kdnuggets.com/2022/04/adoption-ai-algorithmic-trading-affected-finance-industry.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How ML Model Explainability Accelerates the AI Adoption Journey for…](https://www.kdnuggets.com/2022/07/ml-model-explainability-accelerates-ai-adoption-journey-financial-services.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Labeling for Machine Learning: Market Overview, Approaches, and Tools](https://www.kdnuggets.com/2021/12/data-labeling-ml-overview-and-tools.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning’s Sweet Spot: Pure Approaches in NLP and Document Analysis](https://www.kdnuggets.com/2022/05/machine-learning-sweet-spot-pure-approaches-nlp-document-analysis.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Multi-label NLP: An Analysis of Class Imbalance and Loss Function…](https://www.kdnuggets.com/2023/03/multilabel-nlp-analysis-class-imbalance-loss-function-approaches.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
