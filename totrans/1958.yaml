- en: Building Data Pipeline with Prefect
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/building-data-pipeline-with-prefect](https://www.kdnuggets.com/building-data-pipeline-with-prefect)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Building Data Pipeline with Prefect](../Images/4d13b082e59808e5824b4489d6e1fbe1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author | Canva
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we will learn about Prefect, a modern workflow orchestration
    tool. We will start by building a data pipeline with Pandas and then compare it
    with a Prefect workflow to gain a better understanding. In the end, we will deploy
    our workflow and view run logs on the dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: What is Prefect?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Prefect](https://www.prefect.io/) is a workflow management system designed
    to orchestrate and manage complex data workflows, including machine learning (ML)
    pipelines. It provides a framework for building, scheduling, and monitoring workflows,
    making it an essential tool for managing ML operations (MLOps).'
  prefs: []
  type: TYPE_NORMAL
- en: Prefect offers task and flow management, allowing users to define dependencies
    and execute workflows efficiently. With features like state management and observability,
    Prefect provides insights into task status and history, aiding debugging and optimization.
    It comes with a highly interactive dashboard that lets you schedule, monitor,
    and integrate various other features that will improve your workflow for the MLOps
    pipeline. You can even set up notifications and integrate other ML frameworks
    with a few clicks.
  prefs: []
  type: TYPE_NORMAL
- en: Prefect is available as an open-source framework and a managed cloud service,
    simplifying your workflow even more.
  prefs: []
  type: TYPE_NORMAL
- en: Building Data Pipeline with Pandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will replicate the data pipeline that I used in the previous tutorials ([Building
    Data Science Pipelines Using Pandas—KDnuggets](/building-data-science-pipelines-using-pandas))
    to give you an idea of how each task works in the pipeline and how to combine
    them. I am mentioning it here so that you can clearly compare how perfect data
    pipelines are different from normal pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: When we run the above code, each task will run sequentially and generate the
    data visualization. Apart from that, it doesn't do anything. We can schedule it,
    view the run logs, or even integrate third party tools for notification or monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Data Pipeline with Prefect](../Images/79beb4560737e97df467c8f9d7ff23a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Building Data Pipeline with Prefect
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we will build the same pipeline with the same dataset [Online Sales Dataset
    - Popular Marketplace Data](https://www.kaggle.com/datasets/shreyanshverma27/online-sales-dataset-popular-marketplace-data)
    but with Prefect. We will first install the PRefect library by using the PIP command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you review the code below, you will notice that nothing has really changed.
    The functions are the same, but with the addition of the Python decorators. Each
    step in the pipeline has the `@task` decorator, and the pipeline combining these
    steps has the `@flow` decorator. Additionally, we are saving the generated figure
    too.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We will run our data pipeline by providing the CSV file location. It will perform
    all the steps in sequence and generate logs with the run states.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the end, you will get the transformed data frame and visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Data Pipeline with Prefect](../Images/c74d76a8f60b64e2a696a39dcd248852.png)'
  prefs: []
  type: TYPE_IMG
- en: Deploying the Prefect Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to deploy the Prefect pipeline, we need to start by moving our codebase
    to the Python file `data_pipe.py`. After that, we will modify how we run our pipeline.
    We will use the `.server` function to deploy the pipeline and pass the CSV file
    as an argument to the function.
  prefs: []
  type: TYPE_NORMAL
- en: '**data_pipe.py:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the Python file, we will receive the message saying that to run
    the deployed pipeline, we have to use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Data Pipeline with Prefect](../Images/7de84a30f6d0c14c52f3ba144649446f.png)'
  prefs: []
  type: TYPE_IMG
- en: Launch a new Terminal window and type the command to trigger the run for this
    flow.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, flow runs have initiated, meaning the pipeline is running in
    the background. We can always go back to the first Terminal window to view the
    logs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Data Pipeline with Prefect](../Images/50e3e5c9ca50ab412878a272c357108a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To view the logs in the dashboard, we have to launch the Prefect dashboard
    by typing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Click on the dashboard link to launch the dashboard in your web browser.
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Data Pipeline with Prefect](../Images/a3adf6f824b5460e04567e9d3adb3aff.png)'
  prefs: []
  type: TYPE_IMG
- en: The dashboard consists of various tabs and information related to your pipeline,
    workflow, and runs. To view the current run, navigate to the “Flow Runs” tab and
    select the most recent flow run.
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Data Pipeline with Prefect](../Images/071f403e8756b9a9161de8310b94c8d2.png)'
  prefs: []
  type: TYPE_IMG
- en: All the source code, data, and information are available at the [Kingabzpro/Data-Pipeline-with-Prefect](https://github.com/kingabzpro/Data-Pipeline-with-Prefect)
    GitHub repository. Please don't forget to star ⭐ it.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building a pipeline using the proper tools is necessary for you to scale your
    data workflow and avoid unnecessary hiccups. By using Prefect, you can schedule
    your runs, debug the pipeline, and integrate it with multiple third-party tools
    that you are already using. It is easy to use and comes with tons of features
    that you will love. If you are new to Prefect, I highly recommend checking out
    Prefect Cloud. They offer free hours for users to experience the cloud platform
    and become familiar with the workflow management system.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.polywork.com/kingabzpro)****[Abid Ali Awan](https://www.polywork.com/kingabzpro)****
    ([@1abidaliawan](https://www.linkedin.com/in/1abidaliawan)) is a certified data
    scientist professional who loves building machine learning models. Currently,
    he is focusing on content creation and writing technical blogs on machine learning
    and data science technologies. Abid holds a Master''s degree in technology management
    and a bachelor''s degree in telecommunication engineering. His vision is to build
    an AI product using a graph neural network for students struggling with mental
    illness.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Orchestrate a Data Science Project in Python With Prefect](https://www.kdnuggets.com/2022/02/orchestrate-data-science-project-python-prefect.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a Formula 1 Streaming Data Pipeline With Kafka and Risingwave](https://www.kdnuggets.com/building-a-formula-1-streaming-data-pipeline-with-kafka-and-risingwave)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a Tractable, Feature Engineering Pipeline for Multivariate…](https://www.kdnuggets.com/2022/03/building-tractable-feature-engineering-pipeline-multivariate-time-series.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building Your First ETL Pipeline with Bash](https://www.kdnuggets.com/building-your-first-etl-pipeline-with-bash)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Data Science Technologies You Need to Build Your Supply Chain Pipeline](https://www.kdnuggets.com/2022/01/6-data-science-technologies-need-build-supply-chain-pipeline.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Simplify Data Processing with Pandas Pipeline](https://www.kdnuggets.com/2022/08/simplify-data-processing-pandas-pipeline.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
