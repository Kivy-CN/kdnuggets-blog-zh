- en: 'Key Machine Learning Technique: Nested Cross-Validation, Why and How, with
    Python code'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键机器学习技术：嵌套交叉验证，为什么以及如何使用Python代码
- en: 原文：[https://www.kdnuggets.com/2020/10/nested-cross-validation-python.html](https://www.kdnuggets.com/2020/10/nested-cross-validation-python.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/10/nested-cross-validation-python.html](https://www.kdnuggets.com/2020/10/nested-cross-validation-python.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Omar Martinez](https://www.linkedin.com/in/omarmartinez182/), Arcalea**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Omar Martinez](https://www.linkedin.com/in/omarmartinez182/), Arcalea**。'
- en: In this article, we’ll briefly discuss and implement a technique that, in the
    grand scheme of things, might be getting less attention than it deserves. The
    previous statement comes from the observation that it’s a well-known issue that
    some models have a tendency to underperform in production compared to the performance
    in the model building stage. While there is an abundance of potential culprits
    for this issue, a common cause could lie in the model selection process.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将简要讨论并实现一种技术，这种技术在宏观上可能没有得到应有的关注。之前的陈述源于这样一个观察：有一个众所周知的问题，即某些模型在生产中的表现往往不如在模型构建阶段的表现。虽然有很多潜在的罪魁祸首，这个问题的一个常见原因可能在于模型选择过程。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析水平'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'A standard model selection process will usually include a hyperparameter optimization
    phase, in which, through the use of a validation technique, such as k-fold cross-validation
    (CV), an “optimal” model will be selected based on the results of a validation
    test. However, this process is vulnerable to a form of selection bias, which makes
    it unreliable in many applications. This is discussed in detail on a paper from
    Gavin Cawley & Nicola Talbot, in which we can find the following nugget of information:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的模型选择过程通常包括一个超参数优化阶段，在此阶段，通过使用验证技术，如k折交叉验证（CV），将基于验证测试的结果选择一个“最佳”模型。然而，这个过程容易受到选择偏差的影响，使得它在许多应用中不可靠。这在Gavin
    Cawley和Nicola Talbot的一篇论文中有详细讨论，在其中我们可以找到以下信息：
- en: '*“In a biased evaluation protocol, occasionally observed in machine learning
    studies, an initial model selection step is performed using all of the available
    data, often interactively as part of a “preliminary study.” The data are then
    repeatedly re-partitioned to form one or more pairs of random, disjoint design
    and test sets. These are then used for performance evaluation using the same fixed
    set of hyper-parameter values. This practice may seem at first glance to be fairly
    innocuous, however the test data are no longer statistically pure, as they have
    been “seen” by the models in tuning the hyperparameters.”*'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“在机器学习研究中偶尔观察到的偏倚评估协议中，初步模型选择步骤是使用所有可用数据进行的，通常作为“初步研究”的一部分。数据随后被反复重新分区，以形成一个或多个随机、互不重叠的设计和测试集。然后使用相同的固定超参数值进行性能评估。这个做法乍一看似乎相当无害，但测试数据已经不再是统计上纯粹的，因为它们在调整超参数时已经被模型‘看到’。”*'
- en: ''
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- [On Over-fitting in Model Selection and Subsequent Selection Bias in Performance
    Evaluation](https://jmlr.csail.mit.edu/papers/volume11/cawley10a/cawley10a.pdf),
    2010.'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- [关于模型选择中的过拟合及后续性能评估中的选择偏差](https://jmlr.csail.mit.edu/papers/volume11/cawley10a/cawley10a.pdf)，2010年。'
- en: To illustrate why this is happening, let’s use an example. Suppose that we are
    working on a machine learning task, in which we are selecting a model based on
    *n* rounds of hyperparameter optimization, and we do this by using a grid search
    and cross-validation. Now, if we are using the same train and test data at each
    one of the *nth* iterations, this means that information of performance on the
    test set is being incorporated into the training data via the choice of hyperparameters.
    At each iteration, this information can be capitalized by the process to find
    the best performing hyperparameters, and that leads to a test dataset that is
    no longer pure for performance evaluation. If *n* is large at some point, then
    we might consider the test set as a secondary training set (loosely speaking).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明为什么会这样，我们来用一个例子。假设我们在进行一个机器学习任务，其中我们基于*n*轮超参数优化来选择模型，我们通过网格搜索和交叉验证来实现。如果我们在每一轮的*n*次迭代中使用相同的训练和测试数据，这意味着测试集的性能信息通过超参数选择被纳入训练数据中。在每次迭代中，这些信息可以被过程利用，以找到表现最佳的超参数，这会导致测试数据集不再纯粹用于性能评估。如果*n*在某些时候很大，那么我们可能会将测试集视为次要的训练集（宽泛地说）。
- en: On the bright side, there are some techniques that can help us tackle this problem.
    One consists of having a train set, a test set, and also a validation set, and
    then tuning hyperparameters based on performance on the validation set.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从积极的一面来看，有一些技术可以帮助我们解决这个问题。一种方法是使用训练集、测试集和验证集，并根据验证集的表现调整超参数。
- en: The other strategy, and the focus of this article, is **nested cross-validation**,
    which coincides with one of the proposed solutions from the paper above, by treating
    the hyperparameter optimization as a part of the model fitting itself and evaluating
    it with a different validation set that is part of an outer layer of cross-validation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种策略，也是本文的重点，是**嵌套交叉验证**，它与上述论文中提出的解决方案之一相一致，通过将超参数优化视为模型拟合的一部分，并用不同的验证集来评估它，这些验证集是交叉验证的外层的一部分。
- en: Put simply, fitting, including the fitting of the hyperparameters, that in itself
    includes an inner CV process, is just like any other part of the model process
    and not a tool to evaluate model performance for that particular fitting approach.
    To evaluate performance, you use the outer cross-validation process. In practice,
    you do this by letting grid-search (or any other object you use for optimization)
    handle the inner cross-validation and then use cross_val_score to estimate generalization
    error in the outer loop. Thus, the final score will be obtained by averaging test
    set scores over several splits, as a regular CV process.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，拟合，包括超参数的拟合，其中本身包含内部交叉验证过程，就像模型过程中的任何其他部分一样，而不是评估特定拟合方法模型性能的工具。为了评估性能，你需要使用外部交叉验证过程。在实践中，你可以通过让网格搜索（或任何其他用于优化的对象）处理内部交叉验证，然后使用cross_val_score来估计外部循环中的泛化误差。因此，最终得分将通过对多个分割的测试集得分取平均值来获得，就像常规交叉验证过程一样。
- en: Here’s a simplified overview of the approach. This illustration is by no means
    technically rigorous but is to provide some intuition behind the whole process.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这是该方法的简化概述。这一示意图并不是技术上严格的，但旨在提供整个过程的直观理解。
- en: '![](../Images/de4aa608e4442ce1f06da7ffe8109d41.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de4aa608e4442ce1f06da7ffe8109d41.png)'
- en: '*Simplified Illustration of the Nested Cross-Validation Process.*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*嵌套交叉验证过程的简化示意图。*'
- en: Nested Cross-validation in Python
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python中的嵌套交叉验证
- en: Implementing nested CV in python, thanks to [scikit-learn](https://scikit-learn.org/),
    is relatively straightforward.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢[scikit-learn](https://scikit-learn.org/)，在Python中实现嵌套交叉验证相对简单。
- en: Let’s look at an example. We’ll start by loading the [wine dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine)
    from sklearn.datasets and all of the necessary modules.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子。我们将开始加载[葡萄酒数据集](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine)以及所有必要的模块。
- en: Now, we proceed to instantiate the classifier and then specify the number of
    rounds/trials we would like to run, in other words, how many times we’ll perform
    the whole process of going through both the inner and outer loops. Because this
    is computationally expensive and we are doing this only for demonstration purposes,
    let's choose 20\. Remember, the number of rounds represents how many times you’ll
    split your dataset differently on each of the CV processes.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们开始实例化分类器，并指定我们希望运行的回合/试验次数，换句话说，就是我们将执行多少次完整的内外循环过程。由于这在计算上比较昂贵，我们仅用于演示目的，选择20次。请记住，回合数表示每次交叉验证过程将如何不同地拆分您的数据集。
- en: The next step involves creating a dictionary that will establish the hyperparameter
    space that we are going to explore in each round, and we also create two empty
    arrays to store the results from the nested and the non-nested processes.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建一个字典，以建立我们将在每轮中探索的超参数空间，并创建两个空数组来存储嵌套和非嵌套过程的结果。
- en: The next step is crucial, we are going to create a for loop that will iterate
    for the number of rounds we've specified, and that will contain two different
    cross-validation objects.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步至关重要，我们将创建一个for循环，该循环会迭代我们指定的回合数，并包含两个不同的交叉验证对象。
- en: For this example, we'll use 5-fold cross-validation for both the outer and inner
    loops, and we use the value of each round (i) as the random_state for both CV
    objects.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们将使用5折交叉验证作为外层和内层循环，并使用每轮的值（i）作为两个交叉验证对象的random_state。
- en: Then, we proceed to create and configure the object to perform the hyperparameter
    optimization. In this case, we'll use grid-search. Notice that we pass to grid-search
    the 'inner_cv' object as the cross-validation method.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们创建并配置用于执行超参数优化的对象。在这种情况下，我们将使用网格搜索。请注意，我们将'inner_cv'对象作为交叉验证方法传递给网格搜索。
- en: Subsequently, notice that for the final scores of the "nested" cross-validation
    process, we use the cross_val_score function and feed it the classifier object
    'clf' (includes its own CV process), which is the object we used to perform the
    hyperparameter optimization, and also the 'outer_cv' cross-validation object.
    In this process, we also fit the data and then store the results of each process
    inside the empty arrays we created before.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，请注意，对于“嵌套”交叉验证过程的最终分数，我们使用cross_val_score函数，并将分类器对象'clf'（包括其自身的交叉验证过程）传递给它，这个对象是我们用于执行超参数优化的对象，同时还传递'outer_cv'交叉验证对象。在此过程中，我们还将数据拟合，并将每个过程的结果存储在之前创建的空数组中。
- en: We can now calculate the difference in the accuracy scores of both the "simple"
    cross-validation and the nested cross-validation processes to see how much they
    disagree with each other on average.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以计算“简单”交叉验证和嵌套交叉验证过程的准确性分数差异，以查看它们在平均上有多大分歧。
- en: In this case, by nested cross-validation scores, we mean the scores of the nested
    process (not to be confused with the inner cross-validation process), and we compare
    them with the scores of the regular process (non-nested).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，通过嵌套交叉验证分数，我们指的是嵌套过程的分数（不要与内部交叉验证过程混淆），并将其与常规过程（非嵌套）的分数进行比较。
- en: '**Output:**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As we can see, the non-nested scores, on average, are more optimistic. Thus,
    relying solely on the information of that process might result in a biased model
    selection.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，非嵌套分数平均来看更为乐观。因此，仅依赖该过程的信息可能会导致模型选择偏差。
- en: We can also plot the scores of each iteration and create a plot to get a visual
    comparison of how both processes behaved.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以绘制每次迭代的分数，并创建图表以获取两个过程行为的可视化比较。
- en: '**Output:**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '![](../Images/4fd278f5460b4e7fb02cff5c757ca4a7.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fd278f5460b4e7fb02cff5c757ca4a7.png)'
- en: Finally, we can also plot the difference in each round for both CV processes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还可以绘制两个交叉验证过程在每轮中的差异。
- en: '**Output:**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '![](../Images/2866efccbead0cc67410ff0a538a951f.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2866efccbead0cc67410ff0a538a951f.png)'
- en: Final thoughts
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最终想法
- en: While cross-validation is an industry-standard to assess generalization, it’s
    important to consider the problem at hand and potentially implement a more rigorous
    process to avoid a selection bias while choosing the final model. From personal
    experience, this is significatively more important when you’re dealing with small
    datasets and when the system is built to support non-trivial decisions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管交叉验证是评估泛化能力的行业标准，但考虑到具体问题，重要的是实施更严格的过程，以避免在选择最终模型时产生选择偏差。从个人经验来看，当处理小型数据集时，尤其是在系统用于支持复杂决策时，这一点尤为重要。
- en: Nested cross-validation is not a perfect process, it is **computationally expensive**,
    and it’s definitely not a panacea for poor model performance in production. However,
    in most scenarios, the process will allow you to get a more realistic view of
    the generalization capacity of each model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌套交叉验证不是一个完美的过程，它是**计算开销很大的**，而且它绝对不是解决生产中模型性能差的万能药。然而，在大多数情况下，这一过程将使你能够更现实地了解每个模型的泛化能力。
- en: You can find the notebook with all of the code from the article on [GitHub](https://github.com/omartinez182/data-science-notebooks/blob/master/Nested_Cross_Validation_in_Python.ipynb).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [GitHub](https://github.com/omartinez182/data-science-notebooks/blob/master/Nested_Cross_Validation_in_Python.ipynb)
    上找到包含本文所有代码的笔记本。
- en: '**Bio:** [Eduardo Martinez](https://www.linkedin.com/in/omarmartinez182/) is
    a marketer with a postgraduate degree in business analytics. Currently at [Arcalea](https://arcalea.com/),
    Eduardo consolidates data science frameworks with marketing processes.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介：** [爱德华多·马丁内斯](https://www.linkedin.com/in/omarmartinez182/) 是一位拥有商业分析硕士学位的市场营销人员。目前在
    [Arcalea](https://arcalea.com/) 工作，爱德华多将数据科学框架与营销流程整合在一起。'
- en: '**Related:**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[How (and Why) to Create a Good Validation Set](https://www.kdnuggets.com/2017/11/create-good-validation-set.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何（以及为什么）创建一个好的验证集](https://www.kdnuggets.com/2017/11/create-good-validation-set.html)'
- en: '[5 Reasons Why You Should Use Cross-Validation in Your Data Science Projects](https://www.kdnuggets.com/2018/10/5-reasons-cross-validation-data-science-projects.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你应该在数据科学项目中使用交叉验证的 5 个理由](https://www.kdnuggets.com/2018/10/5-reasons-cross-validation-data-science-projects.html)'
- en: '[Building Reliable Machine Learning Models with Cross-validation](https://www.kdnuggets.com/2018/08/building-reliable-machine-learning-models-cross-validation.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用交叉验证构建可靠的机器学习模型](https://www.kdnuggets.com/2018/08/building-reliable-machine-learning-models-cross-validation.html)'
- en: More On This Topic
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Parallel Processing in Prompt Engineering: The Skeleton-of-Thought…](https://www.kdnuggets.com/parallel-processing-in-prompt-engineering-the-skeleton-of-thought-technique)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提示工程中的并行处理：思维骨架……](https://www.kdnuggets.com/parallel-processing-in-prompt-engineering-the-skeleton-of-thought-technique)'
- en: '[AI, Analytics, Machine Learning, Data Science, Deep Learning…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能、分析、机器学习、数据科学、深度学习……](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
- en: '[Announcing PyCaret 3.0: Open-source, Low-code Machine Learning in Python](https://www.kdnuggets.com/2023/03/announcing-pycaret-30-opensource-lowcode-machine-learning-python.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[宣布 PyCaret 3.0：开源、低代码的 Python 机器学习工具](https://www.kdnuggets.com/2023/03/announcing-pycaret-30-opensource-lowcode-machine-learning-python.html)'
- en: '[Optimizing Python Code Performance: A Deep Dive into Python Profilers](https://www.kdnuggets.com/2023/02/optimizing-python-code-performance-deep-dive-python-profilers.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化 Python 代码性能：深入了解 Python 性能分析器](https://www.kdnuggets.com/2023/02/optimizing-python-code-performance-deep-dive-python-profilers.html)'
- en: '[Main 2021 Developments and Key 2022 Trends in AI, Data Science,…](https://www.kdnuggets.com/2021/12/trends-ai-data-science-ml-technology.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2021 年主要发展和 2022 年人工智能、数据科学的关键趋势……](https://www.kdnuggets.com/2021/12/trends-ai-data-science-ml-technology.html)'
- en: '[Key Data Science, Machine Learning, AI and Analytics Developments of 2022](https://www.kdnuggets.com/2022/12/key-data-science-machine-learning-ai-analytics-developments-2022.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2022 年的数据科学、机器学习、人工智能和分析领域的关键发展](https://www.kdnuggets.com/2022/12/key-data-science-machine-learning-ai-analytics-developments-2022.html)'
