- en: Building a Formula 1 Streaming Data Pipeline With Kafka and Risingwave
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建 Formula 1 流式数据管道与 Kafka 和 Risingwave
- en: 原文：[https://www.kdnuggets.com/building-a-formula-1-streaming-data-pipeline-with-kafka-and-risingwave](https://www.kdnuggets.com/building-a-formula-1-streaming-data-pipeline-with-kafka-and-risingwave)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/building-a-formula-1-streaming-data-pipeline-with-kafka-and-risingwave](https://www.kdnuggets.com/building-a-formula-1-streaming-data-pipeline-with-kafka-and-risingwave)
- en: '![Building a Formula 1 Streaming Data Pipeline With Kafka and Risingwave](../Images/61bb9011d7231856a58a2c22fc62f979.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![构建 Formula 1 流式数据管道与 Kafka 和 Risingwave](../Images/61bb9011d7231856a58a2c22fc62f979.png)'
- en: Real-time data has arrived and is here to stay. There’s no doubt that every
    day the amount of streaming data increases exponentially and we need to find the
    best way to extract, process, and visualize it. For instance, each Formula 1 car
    produces around 1.5 terabytes of data through a race weekend ([source](https://www.racecar-engineering.com/articles/data-analytics-managing-f1s-digital-gold/#:~:text=Each%20Formula%201%20car%20carries,data%20throughout%20a%20race%20weekend.)).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 实时数据已经到来并且会持续存在。毫无疑问，每天流式数据的数量都在指数级增长，我们需要找到提取、处理和可视化数据的最佳方法。例如，每辆 Formula 1
    汽车在一个比赛周末产生约 1.5 TB 的数据（[来源](https://www.racecar-engineering.com/articles/data-analytics-managing-f1s-digital-gold/#:~:text=Each%20Formula%201%20car%20carries,data%20throughout%20a%20race%20weekend.)）。
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的 IT 工作'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In this article, we are not going to stream the car’s data, but we will be streaming,
    processing, and visualizing the race’s data simulating we’re live on a Formula
    1 race. Before we get started, it’s important to mention that this article will
    not be focused on what each technology is, but on how to implement them in a streaming
    data pipeline, so some knowledge about Python, Kafka, SQL, and data visualization
    is expected.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们不会流式传输汽车的数据，而是会流式传输、处理和可视化比赛的数据，模拟我们在 Formula 1 比赛中实时进行。在开始之前，重要的是要提到这篇文章不会关注每种技术是什么，而是如何在流式数据管道中实现它们，因此需要对
    Python、Kafka、SQL 和数据可视化有一定了解。
- en: Prerequisites
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前提条件
- en: '**F1 Source Data**: The Formula 1 data used in this data streaming pipeline
    was downloaded from Kaggle and can be found as [Formula 1 World Championship (1950
    - 2023)](https://www.kaggle.com/datasets/rohanrao/formula-1-world-championship-1950-2020?select=lap_times.csv).'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F1 源数据**：在这个数据流管道中使用的 Formula 1 数据是从 Kaggle 下载的，可以在 [Formula 1 世界锦标赛（1950
    - 2023）](https://www.kaggle.com/datasets/rohanrao/formula-1-world-championship-1950-2020?select=lap_times.csv)
    中找到。'
- en: '**Python:** Python 3.9 was used to build this pipeline, but any version greater
    than 3.0 should work. Further details on how to download and install Python can
    be found on the [official Python website](https://www.python.org/downloads/).'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Python：** 这个管道是用 Python 3.9 构建的，但任何高于 3.0 的版本都应该可以使用。有关如何下载和安装 Python 的更多详细信息，可以在
    [官方 Python 网站](https://www.python.org/downloads/) 上找到。'
- en: '**Kafka:** Kafka is one of the main technologies used in this streaming pipeline,
    so it’s important to have it installed before you get started. This streaming
    pipeline was built on MacOS, so brew was used to install Kafka. More details can
    be found on the [official brew website](https://formulae.brew.sh/formula/kafka).
    We also need a Python library to use Kafka with Python. This pipeline uses kafka-python.
    Installation details can be found on their [official website](https://kafka-python.readthedocs.io/en/master/).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kafka：** Kafka 是这个流式数据管道中使用的主要技术之一，因此在开始之前安装它是很重要的。这个流式数据管道是在 MacOS 上构建的，因此使用了
    brew 来安装 Kafka。更多详细信息可以在 [官方 brew 网站](https://formulae.brew.sh/formula/kafka)
    上找到。我们还需要一个 Python 库来将 Kafka 与 Python 一起使用。这个管道使用了 kafka-python。安装详细信息可以在它们的 [官方网站](https://kafka-python.readthedocs.io/en/master/)
    上找到。'
- en: '**RisingWave (Streaming Database):** There are multiple streaming databases
    available in the market, but the one used in this article and one of the best
    is RisingWave. Getting started with RisingWave is pretty simple and it only takes
    a couple of minutes. A full tutorial on how to get started can be found on their
    [official website](https://www.risingwave.dev/docs/current/get-started/).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RisingWave（流数据库）：** 市场上有多种流数据库，但本文使用的且最佳的之一是 RisingWave。开始使用 RisingWave 非常简单，只需几分钟。有关如何入门的完整教程可以在他们的[官方网站](https://www.risingwave.dev/docs/current/get-started/)上找到。'
- en: '**Grafana Dashboard:** Grafana was used in this streaming pipeline to visualize
    the Formula 1 data in real time. Details on how to get started can be found on
    [this website](https://www.risingwave.dev/docs/current/grafana-integration/).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Grafana 仪表盘：** 在这个流媒体管道中使用了 Grafana 来实时可视化 Formula 1 数据。有关如何开始使用的详细信息可以在[这个网站](https://www.risingwave.dev/docs/current/grafana-integration/)上找到。'
- en: Streaming the Source Data
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流数据源
- en: Now that we have all the prerequisites, it’s time to start building the Formula
    1 data streaming pipeline. The source data is stored in a JSON file, so we have
    to extract it and send it through a Kafka topic. To do so, we will be using the
    below Python script.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了所有先决条件，是时候开始构建 Formula 1 数据流管道了。源数据存储在一个 JSON 文件中，所以我们需要提取它并通过 Kafka
    主题发送。为此，我们将使用以下 Python 脚本。
- en: Code by Author
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的代码
- en: Setting up Kafka
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 Kafka
- en: The Python script to stream the data is all set to start streaming the data,
    but the Kafka topic F1Topic is not created yet, so let’s create it. First, we
    need to initialize Kafka. To do so, we have to start Zookeper, then start Kafka,
    and finally create the topic with the below commands. Remember that Zookeper and
    Kafka should be running in a separate terminal.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 用于流数据的 Python 脚本已准备好开始流数据，但 Kafka 主题 F1Topic 尚未创建，所以我们需要创建它。首先，我们需要初始化 Kafka。为此，我们必须启动
    Zookeper，然后启动 Kafka，最后使用以下命令创建主题。请记住，Zookeper 和 Kafka 应在不同的终端中运行。
- en: Code by Author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的代码
- en: '![Building a Formula 1 Streaming Data Pipeline With Kafka and Risingwave](../Images/1d8a37dabe5fded7b0985f49b670a78b.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Kafka 和 Risingwave 构建 Formula 1 流数据管道](../Images/1d8a37dabe5fded7b0985f49b670a78b.png)'
- en: Setting up the Streaming Database RisingWave
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置流数据库 RisingWave
- en: Once RisingWave is installed, it’s very easy to start it. First, we need to
    initialize the database and then we have to connect to it via the Postgres interactive
    terminal psql. To initialize the streaming database RisingWave, we have to execute
    the below command.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装了 RisingWave，就很容易启动它。首先，我们需要初始化数据库，然后通过 Postgres 交互式终端 psql 连接它。要初始化流数据库
    RisingWave，我们必须执行以下命令。
- en: Code by Author
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的代码
- en: The above command launches RisingWave in playground mode, where data is temporarily
    stored in memory. The service is designed to automatically terminate after 30
    minutes of inactivity, and any data stored will be deleted upon termination. This
    method is recommended for tests only, [RisingWave Cloud](https://www.risingwave.dev/cloud/intro/)
    should be used for production environments.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令在游乐场模式下启动 RisingWave，其中数据暂时存储在内存中。该服务设计为在 30 分钟不活动后自动终止，所有存储的数据在终止时将被删除。此方法仅推荐用于测试，[RisingWave
    Cloud](https://www.risingwave.dev/cloud/intro/) 应用于生产环境。
- en: After RisingWave is up and running, it’s time to connect to it in a new terminal
    via the Postgress interactive terminal with the below command.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当 RisingWave 启动并运行后，就可以通过以下命令在新的终端中通过 Postgress 交互式终端连接它。
- en: Code by Author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的代码
- en: '![Building a Formula 1 Streaming Data Pipeline With Kafka and Risingwave](../Images/c816d777ef84b8c406f0b7c4868ed4b8.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Kafka 和 Risingwave 构建 Formula 1 流数据管道](../Images/c816d777ef84b8c406f0b7c4868ed4b8.png)'
- en: With the connection established, it’s time to start pulling the data from the
    Kafka topic. In order to get the streaming data into RisingWave we need to create
    a source. This source will establish the communication between the Kafka topic
    and RisingWave, so let’s execute the below command.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦建立了连接，就可以开始从 Kafka 主题中提取数据。为了将流数据导入 RisingWave，我们需要创建一个源。这个源将建立 Kafka 主题和
    RisingWave 之间的通信，所以让我们执行以下命令。
- en: Code by Author
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的代码
- en: '![Building a Formula 1 Streaming Data Pipeline With Kafka and Risingwave](../Images/9090340484e1b06428324dea97d381e3.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Kafka 和 Risingwave 构建 Formula 1 流数据管道](../Images/9090340484e1b06428324dea97d381e3.png)'
- en: If the command runs successfully, then we can see the message “CREATE SOURCE”
    and the source has been created. It’s important to highlight that once the source
    is created, the data is not ingested into RisingWave automatically. We need to
    create a materialized view to start the data movement. This materialized view
    will also help us to create the Grafana dashboard in the next step.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果命令成功运行，我们将看到消息“CREATE SOURCE”，并且源已经创建。需要强调的是，一旦源创建完成，数据不会自动导入到 RisingWave
    中。我们需要创建一个物化视图来启动数据的流动。这个物化视图还将帮助我们在下一步中创建 Grafana 仪表板。
- en: Let’s create the materialized view with the same schema as the source data with
    the following command.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用以下命令创建具有与源数据相同架构的物化视图。
- en: Code by Author
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的代码
- en: '![Building a Formula 1 Streaming Data Pipeline With Kafka and Risingwave](../Images/9b0e745277388c695722c81310b769dd.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Kafka 和 RisingWave 构建 F1 流数据管道](../Images/9b0e745277388c695722c81310b769dd.png)'
- en: If the command runs successfully, then we can see the message “CREATE MATERIALIZED_VIEW”
    and the materialized view has been created and now it’s time to test it out!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果命令成功运行，我们将看到消息“CREATE MATERIALIZED_VIEW”，并且物化视图已创建，现在可以进行测试了！
- en: Execute the Python script to start streaming the data and in the RisingWave
    terminal query the data in real time. RisingWave is a Postgres-compatible SQL
    database, so if you are familiar with PostgreSQL or any other SQL database everything
    will flow smoothly to query your streaming data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 执行 Python 脚本以开始流式传输数据，并在 RisingWave 终端实时查询数据。RisingWave 是一个兼容 Postgres 的 SQL
    数据库，因此如果你熟悉 PostgreSQL 或任何其他 SQL 数据库，一切将会顺利进行，你可以轻松查询流数据。
- en: '![Building a Formula 1 Streaming Data Pipeline With Kafka and Risingwave](../Images/3d14ebdc7fb5cb2792d10859af3e6839.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Kafka 和 RisingWave 构建 F1 流数据管道](../Images/3d14ebdc7fb5cb2792d10859af3e6839.png)'
- en: As you can see the streaming pipeline is now up and running, but we are not
    taking all the advantages of the streaming database RisingWave. We can add more
    tables to join data in real time and build a fully functional application.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，流式处理管道现在已经启动并运行，但我们还没有充分利用流数据库 RisingWave 的所有优势。我们可以添加更多的表来实时连接数据，构建一个功能完善的应用程序。
- en: Let’s create the races table so we can join the streaming data with the race
    table and get the actual name of the race instead of the race id.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建比赛表，以便我们可以将流数据与比赛表连接，并获取比赛的实际名称，而不是比赛 id。
- en: Code by Author
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的代码
- en: '![Building a Formula 1 Streaming Data Pipeline With Kafka and Risingwave](../Images/18711795a7b00d830f27307bec3cb759.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Kafka 和 RisingWave 构建 F1 流数据管道](../Images/18711795a7b00d830f27307bec3cb759.png)'
- en: Now, let’s insert the data for the specific race id that we need.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为我们需要的特定比赛 id 插入数据。
- en: Code by Author
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的代码
- en: '![Building a Formula 1 Streaming Data Pipeline With Kafka and Risingwave](../Images/dde90853d3bc601a175c38bf103bfbdc.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Kafka 和 RisingWave 构建 F1 流数据管道](../Images/dde90853d3bc601a175c38bf103bfbdc.png)'
- en: Let’s follow the same procedure but with the driver’s table.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照相同的程序进行，但这次用司机的表格。
- en: Code by Author
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的代码
- en: '![Building a Formula 1 Streaming Data Pipeline With Kafka and Risingwave](../Images/84278fe3a968267f841a7def11070f9a.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Kafka 和 RisingWave 构建 F1 流数据管道](../Images/84278fe3a968267f841a7def11070f9a.png)'
- en: And finally, let’s insert the driver’s data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们插入司机的数据。
- en: Code by Author
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的代码
- en: We have the tables ready to start joining the streaming data, but we need the
    materialized view where all the magic will happen. Let’s create a materialized
    view where we can see the top 3 positions in real-time, joining the driver id
    and the race id to get the actual names.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好表格来开始连接流数据，但我们需要一个物化视图，所有的魔法都将在这里发生。让我们创建一个物化视图，在实时中查看前 3 名的位置，连接司机 id
    和比赛 id 以获取实际名称。
- en: Code by Author
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的代码
- en: Last, but not least let’s create the last materialized view to see how many
    times a driver got the position number one during the whole race.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，让我们创建最后一个物化视图，以查看一个司机在整个比赛中获得第一名的次数。
- en: Code by Author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的代码
- en: And now, it’s time to build the Grafana dashboard and see all the joined data
    in real-time thanks to the materialized views.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候构建 Grafana 仪表板，并通过物化视图实时查看所有连接的数据了。
- en: Setting up the Grafana Dashboard
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 Grafana 仪表板
- en: The final step in this streaming data pipeline is visualizing the streaming
    data in a real-time dashboard. Before we create the Gafana dashboard, we need
    to create a data source to establish the connection between Grafana and our streaming
    database RisingWave following the below steps.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 流数据管道的最后一步是实时仪表板中的数据可视化。在创建 Grafana 仪表板之前，我们需要创建一个数据源，以建立 Grafana 和我们的流数据库 RisingWave
    之间的连接，按照以下步骤进行。
- en: Go to Configuration > Data sources.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转到 配置 > 数据源。
- en: Click the Add data source button.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击添加数据源按钮。
- en: Select PostgreSQL from the list of supported databases.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从支持的数据库列表中选择 PostgreSQL。
- en: 'Fill in the PostgreSQL Connection fields like so:'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填写 PostgreSQL 连接字段，如下所示：
- en: '![Building a Formula 1 Streaming Data Pipeline With Kafka and Risingwave](../Images/951ebdcdd9cc0e872a4735fd6afdd662.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![构建一个 Formula 1 流数据管道与 Kafka 和 Risingwave](../Images/951ebdcdd9cc0e872a4735fd6afdd662.png)'
- en: Scroll down and click on the save and test button. The database connection is
    now established.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 向下滚动并点击保存和测试按钮。数据库连接现在已建立。
- en: '![Building a Formula 1 Streaming Data Pipeline With Kafka and Risingwave](../Images/bf607d178e9e24a6b6ea5ae8f54e2d54.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![构建一个 Formula 1 流数据管道与 Kafka 和 Risingwave](../Images/bf607d178e9e24a6b6ea5ae8f54e2d54.png)'
- en: Now go to dashboards in the left panel, click on the new dashboard option, and
    add a new panel. Select the table visualization, switch to the code tab, and let’s
    query the materialized view live_positions where we can see the joined data for
    the top 3 positions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在转到左侧面板中的仪表板，点击新建仪表板选项，添加一个新面板。选择表格可视化，切换到代码选项卡，查询物化视图 live_positions，我们可以看到前
    3 名位置的连接数据。
- en: Code by Author
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 作者代码
- en: '![Building a Formula 1 Streaming Data Pipeline With Kafka and Risingwave](../Images/4c0575c2a81230a7fe1eaac0ed549871.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![构建一个 Formula 1 流数据管道与 Kafka 和 Risingwave](../Images/4c0575c2a81230a7fe1eaac0ed549871.png)'
- en: Let’s add another panel to visualize the current lap. Select the gauge visualization
    and in the code tab query the max lap available in the streaming data. Gauge customization
    is up to you.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们添加另一个面板以可视化当前圈数。选择仪表可视化，在代码选项卡中查询流数据中可用的最大圈数。仪表的自定义由你决定。
- en: Code by Author
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 作者代码
- en: '![Building a Formula 1 Streaming Data Pipeline With Kafka and Risingwave](../Images/b7ea9fca41c7a2e9a5638a52437d0ca8.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![构建一个 Formula 1 流数据管道与 Kafka 和 Risingwave](../Images/b7ea9fca41c7a2e9a5638a52437d0ca8.png)'
- en: Finally, let’s add another panel to query the materialized view times_in_position_one
    and see in real-time how many times a driver got the number one position during
    the whole race.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们添加另一个面板以查询物化视图 times_in_position_one，并实时查看在整个比赛过程中车手获得第一名的位置次数。
- en: Code by Author
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 作者代码
- en: '![Building a Formula 1 Streaming Data Pipeline With Kafka and Risingwave](../Images/447ca46324e2b01fbae5bfec41b27aab.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![构建一个 Formula 1 流数据管道与 Kafka 和 Risingwave](../Images/447ca46324e2b01fbae5bfec41b27aab.png)'
- en: Visualizing the Results
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果可视化
- en: Finally, all the components for the streaming data pipeline are up and running.
    The Python script has been executed to start streaming the data through the Kafka
    topic, the streaming database RisingWave is reading, processing, and joining the
    data in real-time. The materialized view f1_lap_times reads the data from the
    Kafka topic and each panel in the Grafana dashboard is a different materialized
    view joining data in real-time to show detailed data thanks to the joins done
    by the materialized views to the races and drivers tables. The Grafana dashboard
    queries the materialized views and all the processing has been simplified thanks
    to the materialized views processed in the streaming database RisingWave.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，所有流数据管道的组件都已启动并运行。Python 脚本已经执行，开始通过 Kafka 主题流式传输数据，流式数据库 RisingWave 正在实时读取、处理和连接数据。物化视图
    f1_lap_times 从 Kafka 主题中读取数据，Grafana 仪表板中的每个面板都是不同的物化视图，这些视图实时连接数据，通过物化视图对比赛和车手表进行的连接来显示详细数据。Grafana
    仪表板查询物化视图，所有处理过程都因为在流式数据库 RisingWave 中处理的物化视图而得到简化。
- en: '![Building a Formula 1 Streaming Data Pipeline With Kafka and Risingwave](../Images/8b9e238e85d295a6099097f4cbdfb25d.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![构建一个 Formula 1 流数据管道与 Kafka 和 Risingwave](../Images/8b9e238e85d295a6099097f4cbdfb25d.png)'
- en: '**[Javier Granados](https://medium.com/@JavierGr)** is a Senior Data Engineer
    who likes to read and write about data pipelines. He specialize in cloud pipelines
    mainly on AWS, but he is always exploring new technologies and new trends. You
    can find him in Medium at https://medium.com/@JavierGr'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**[哈维尔·格拉纳多斯](https://medium.com/@JavierGr)** 是一名高级数据工程师，他喜欢阅读和写作关于数据管道的内容。他专注于云管道，主要是在
    AWS 上，但他总是探索新技术和新趋势。你可以在 Medium 上找到他，网址是 https://medium.com/@JavierGr'
- en: More On This Topic
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[How to Build a Scalable Data Architecture with Apache Kafka](https://www.kdnuggets.com/2023/04/build-scalable-data-architecture-apache-kafka.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 Apache Kafka 构建可扩展的数据架构](https://www.kdnuggets.com/2023/04/build-scalable-data-architecture-apache-kafka.html)'
- en: '[Building Data Pipeline with Prefect](https://www.kdnuggets.com/building-data-pipeline-with-prefect)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Prefect 构建数据管道](https://www.kdnuggets.com/building-data-pipeline-with-prefect)'
- en: '[Building a Tractable, Feature Engineering Pipeline for Multivariate…](https://www.kdnuggets.com/2022/03/building-tractable-feature-engineering-pipeline-multivariate-time-series.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[构建一个可处理的特征工程管道用于多变量时间序列…](https://www.kdnuggets.com/2022/03/building-tractable-feature-engineering-pipeline-multivariate-time-series.html)'
- en: '[Building Your First ETL Pipeline with Bash](https://www.kdnuggets.com/building-your-first-etl-pipeline-with-bash)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Bash 构建你的第一个 ETL 管道](https://www.kdnuggets.com/building-your-first-etl-pipeline-with-bash)'
- en: '[Simple and Fast Data Streaming for Machine Learning Projects](https://www.kdnuggets.com/2022/11/simple-fast-data-streaming-machine-learning-projects.html)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[简单且快速的数据流处理，用于机器学习项目](https://www.kdnuggets.com/2022/11/simple-fast-data-streaming-machine-learning-projects.html)'
- en: '[Python in Finance: Real Time Data Streaming within Jupyter Notebook](https://www.kdnuggets.com/python-in-finance-real-time-data-streaming-within-jupyter-notebook)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[金融中的 Python：在 Jupyter Notebook 中进行实时数据流处理](https://www.kdnuggets.com/python-in-finance-real-time-data-streaming-within-jupyter-notebook)'
