- en: The Evolution of Speech Recognition Metrics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语音识别指标的发展
- en: 原文：[https://www.kdnuggets.com/2022/10/evolution-speech-recognition-metrics.html](https://www.kdnuggets.com/2022/10/evolution-speech-recognition-metrics.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/10/evolution-speech-recognition-metrics.html](https://www.kdnuggets.com/2022/10/evolution-speech-recognition-metrics.html)
- en: '![The Evolution of Speech Recognition Metrics](../Images/42ae84473b30fd40757f23e592be8f48.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![语音识别指标的发展](../Images/42ae84473b30fd40757f23e592be8f48.png)'
- en: '[Image by pch.vector](https://www.freepik.com/free-vector/tiny-people-near-phone-with-voice-assistant-screen-man-woman-using-ai-speaking-into-speaker-recording-voice-messages-digital-devices-flat-vector-illustration-technology-software-concept_28480879.htm#query=speech%20recognition&position=19&from_view=search&track=sph)
    on Freepik'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[图片由 pch.vector 提供](https://www.freepik.com/free-vector/tiny-people-near-phone-with-voice-assistant-screen-man-woman-using-ai-speaking-into-speaker-recording-voice-messages-digital-devices-flat-vector-illustration-technology-software-concept_28480879.htm#query=speech%20recognition&position=19&from_view=search&track=sph)
    自 Freepik'
- en: With the widespread adoption of deep learning, progress in Automatic Speech
    Recognition (ASR) accuracy has accelerated in the last few years. Researchers
    have been quick to claim human parity on specific measures. But is Speech Recognition
    really a solved problem today? If not, what we focus on measuring today is going
    to impact the direction ASR takes next.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习的广泛应用，自动语音识别（ASR）准确率在过去几年中得到了加速进展。研究人员迅速宣称在特定测量上达到了人类平等。然而，语音识别今天真的已经解决了吗？如果没有，我们目前关注的测量指标将会影响ASR未来的发展方向。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速入门网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析水平'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织在 IT 方面'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Human Parity was the End Goal, but now we move the Goalpost!
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人类平等曾是最终目标，但现在我们要移动目标！
- en: ASR systems have for a long time focused on [word error rate (WER)](https://en.wikipedia.org/wiki/Word_error_rate)
    to measure accuracy. This metric makes sense. WER measures how many words the
    system gets wrong for every hundred words it recognizes. Because system performance
    can vary significantly depending on the scenario, audio quality, accent, etc.,
    it is hard to put a single number as the end goal. (Of course, 0% would be nice).
    So, instead, we have typically used WER to compare two systems. And as the system
    started to get a lot better, we started comparing its WER with that of humans.
    Human parity was originally thought of as a distant goal, then deep learning accelerated
    things and we ended up “achieving” that a lot sooner.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ASR 系统长期以来一直专注于 [词错率（WER）](https://en.wikipedia.org/wiki/Word_error_rate) 来衡量准确性。这个指标是有意义的。WER
    衡量系统每识别一百个词中出错多少个词。由于系统性能可能因场景、音质、口音等因素显著变化，因此很难给出一个单一的最终目标（当然，0% 是理想的）。因此，我们通常使用
    WER 来比较两个系统。当系统开始变得越来越好时，我们开始将其 WER 与人类的 WER 进行比较。人类平等最初被认为是一个遥远的目标，但深度学习加速了进程，我们“实现”得要早得多。
- en: If we are Comparing Machines Against Humans, Who are we Comparing Humans Against?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果我们在比较机器与人类，那我们又在比较人类与什么呢？
- en: Human judges are first asked to transcribe audio. To generate transcription
    references, different judges do multiple passes of listening to the audio and
    editing to get more accurate transcriptions. Transcription is considered clean
    when multiple judges reach a consensus. Human baseline WER is measured by comparing
    a new judge’s transcription to that of the reference transcription. One judge
    can still be incorrect at times, when compared to a reference, in fact, typically
    getting one out of twenty words incorrect. This is equivalent to a WER of 5%.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 人工评审首先被要求转录音频。为了生成转录参考，不同的评审会多次听取音频并编辑以获得更准确的转录。当多个评审达成一致时，转录被认为是干净的。通过将新评审的转录与参考转录进行比较来测量人类基准WER。与参考相比，一个评审有时仍可能出错，实际上，通常会有每二十个单词中有一个单词出错。这相当于5%的WER。
- en: To claim human parity, we typically compare the system against one judge. An
    interesting aspect of this is that ASR systems do try to do multiple passes themselves.
    Then is it fair to compare against one-person performance? It is indeed fair because
    we assume that the person also had enough time to replay the audio as needed when
    transcribing, effectively doing multiple passes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要声称达到人类水平，我们通常将系统与一个评审进行比较。有趣的一点是，ASR系统确实尝试自己做多次处理。那么，将系统与一个人进行比较是否公平？确实公平，因为我们假设该人也有足够的时间在转录时重复播放音频，实际上也做了多次处理。
- en: Once ASR reached human parity (En-US) for certain Speech Recognition tasks,
    it quickly became apparent that for formally written form, getting one word wrong
    out of every twenty words is still a terrible experience. One way dictation products
    have tried tackling this issue is by trying to figure out which recognized words
    are low confidence and surfacing viable alternatives for them. We can observe
    this ‘correction’ experience in many commercial products today, including Microsoft
    Office Dictation and Google Docs (Fig. 1 and Fig. 2). However, there was another
    glaring issue with WER.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦ASR在某些语音识别任务中达到了人类水平（美式英语），很快就显现出对于正式书写文本而言，每二十个单词中有一个单词出错仍然是糟糕的体验。一种解决这个问题的方法是尝试找出识别的单词中哪些是低置信度的，并为它们提供可行的替代选项。我们可以在许多商业产品中观察到这种‘修正’体验，包括微软Office
    Dictation和Google Docs（图1和图2）。然而，WER还有另一个显而易见的问题。
- en: '![The Evolution of Speech Recognition Metrics](../Images/67cdee93a8bc65ebd54e004e7d1fd893.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![语音识别指标的发展](../Images/67cdee93a8bc65ebd54e004e7d1fd893.png)'
- en: 'Figure 1: Microsoft Word Dictation showing alternates for dictated text![The
    Evolution of Speech Recognition Metrics](../Images/205f0e2fdabddec80c9716f2e2467002.png)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：Microsoft Word Dictation展示了为听写文本提供的替代选项![语音识别指标的发展](../Images/205f0e2fdabddec80c9716f2e2467002.png)
- en: 'Figure 2: Google Docs showing alternates for dictated text'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：Google Docs展示了为听写文本提供的替代选项
- en: To keep things simple, traditionally WER wasn’t calculated on the final written
    form text. One could argue that the primary task of the ASR system was just to
    recognize the words correctly and not format entities like date, time, currency,
    email, etc. correctly. So instead of properly formatted text, the spoken form
    version was used for WER calculation. It canceled out any specific formatting
    differences, punctuation, capitalization, etc., and purely focused on the spoken
    words. This is an acceptable assumption if the use case is voice search, where
    task completion matters much more than text formatting. Searching with a voice
    could be made to work with just the ‘spoken form’. However, with a different use
    case like voice assistant, things started changing. Now the spoken form “wake
    me up at eight thirty-seven am” was much harder to handle compared to the written
    form “wake me up at 8:37 am”. The written form here is easier for the assistant
    to parse and turn into action.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化问题，传统上WER并没有计算最终书面形式的文本。可以说，ASR系统的主要任务只是正确识别单词，而不是正确格式化日期、时间、货币、电子邮件等实体。因此，WER计算使用的是口语形式版本，而不是正确格式化的文本。这消除了任何特定的格式差异、标点符号、大写等，只专注于口语单词。如果使用场景是语音搜索，这种假设是可以接受的，因为任务完成比文本格式更重要。通过语音搜索可以只使用‘口语形式’来进行。然而，对于像语音助手这样的不同使用场景，情况开始发生变化。现在，口语形式“wake
    me up at eight thirty-seven am”比书面形式“wake me up at 8:37 am”更难处理。书面形式在这里更容易被助手解析并转化为动作。
- en: Voice search and voice assistants are examples of what we call one-shot dictation
    use cases. As the ASR system became more reliable with one-shot dictation use
    cases, attention turned to dictation and conversation scenarios. These are both
    long-form speech recognition tasks. It was easy to ignore punctuation for voice
    search or voice assistant, but for any long-form dictation or conversation, not
    being able to punctuate is a dealbreaker. Since automatic punctuation models weren’t
    as good yet, one route that dictation took was to support ‘explicit punctuation’.
    You could explicitly say ‘period’ or ‘question mark’ and the system would do the
    right thing. It gave control to the users and ‘unblocked’ them to use dictation
    for writing emails or documents. Other aspects like capitalization or disfluency
    handling also started gaining importance for dictation scenarios. If we continued
    to rely on WER as the primary metric, we would be falsely painting the picture
    that our system has solved it. Our metric needed to evolve with the evolving use
    cases for Speech Recognition.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 语音搜索和语音助手是我们所说的“一次性听写”用例的例子。随着 ASR 系统在一次性听写用例中变得更加可靠，注意力转向了听写和对话场景。这些都是长篇语音识别任务。对于语音搜索或语音助手，忽略标点符号很容易，但对于任何长篇听写或对话，无法标点是一个障碍。由于自动标点模型尚未足够好，听写采用的一条途径是支持“明确标点”。你可以明确说“句号”或“问号”，系统会做出正确的处理。这使用户能够控制并“解锁”他们使用听写来写电子邮件或文档。大小写或流畅性处理等其他方面也开始在听写场景中变得重要。如果我们继续依赖
    WER 作为主要指标，我们会错误地描绘出我们的系统已经解决了这个问题。我们的指标需要随着语音识别用例的发展而演变。
- en: An obvious successor of Word Error Rate (WER) is Token Error Rate (TER). For
    TER, we try to consider all aspects of the written form like capitalization, punctuation,
    dysfluency, etc., and try to calculate a single metric just like WER (see Table
    1). On the same sets where ASR had reached human parity on WER when re-evaluated
    with TER, ASR lost the human parity battle again. Our goalpost has moved, but
    this time it does feel like a real goalpost because the outcome is going to be
    much closer to the written form that is widely used.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Token Error Rate (TER) 是 Word Error Rate (WER) 的明显继任者。对于 TER，我们尝试考虑书面形式的所有方面，如大小写、标点符号、语言流畅性等，并尝试像
    WER 一样计算一个单一指标（见表 1）。在 ASR 在 WER 上达到人类平等的同一数据集上，当用 TER 重新评估时，ASR 再次失去了人类平等。我们的目标已发生变化，但这次感觉像一个真正的目标，因为结果将更接近广泛使用的书面形式。
- en: '|  | Recognition | Reference | Metric |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|  | 识别 | 参考 | 指标 |'
- en: '| Spoken form | wake me pat eight thirty five a m | wake me up at eight twenty
    five a m | WER = 3/9 = 33.3% |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 口语形式 | wake me pat eight thirty five a m | wake me up at eight twenty five
    a m | WER = 3/9 = 33.3% |'
- en: '| Written form | Wake me pat 8:35 AM. | Wake me up at 8:25 AM. | TER = 3/7
    = 42.9%*(Punctuation is counted separately)* |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 书面形式 | 叫我在 8:35 AM 起床。 | 叫我在 8:25 AM 起床。 | TER = 3/7 = 42.9%*(标点符号单独计算)*
    |'
- en: 'Table 1: How does spoken form vs written form impact metrics computation?'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：口语形式与书面形式如何影响指标计算？
- en: The Problem with these Aggregate Metrics
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这些综合指标的问题
- en: TER is a good overall metric to look at, but it hides within all the crucial
    details. There are now many categories contributing to this one number, and it
    isn’t about just getting the words right. So, this metric by itself is less actionable.
    To make it more actionable, we need to figure out which category is contributing
    the most to it and decide to focus on improving that. There is a class imbalance
    issue as well. Less than 2% of all tokens would contain any numbers and number-related
    formats like time, currency, date, etc. Even if we get this category completely
    wrong it would still have a limited impact on TER depending on the TER baseline.
    But getting this 2% of the cases even wrong half the time would be a terrible
    experience for the users. For this reason, the TER metric alone cannot guide our
    research investments. I’d argue that we need to figure out important categories
    for our users and measure and improve more targeted metrics like category-[F1](https://en.wikipedia.org/wiki/F-score).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: TER是一个很好的总体度量标准，但它掩盖了所有关键细节。现在有许多类别影响这个数字，这不仅仅是关于准确获得单词。因此，这个度量标准本身的可操作性较差。为了提高可操作性，我们需要弄清楚哪些类别对它的影响最大，并决定专注于改进那些类别。同时也存在类别不平衡的问题。不到2%的所有标记包含任何数字以及相关格式，如时间、货币、日期等。即使我们完全搞错了这一类别，它对TER的影响也会有限，具体取决于TER的基准。但即使是这2%的错误出现一半的时间，也会给用户带来糟糕的体验。因此，单靠TER度量标准无法指导我们的研究投资。我认为，我们需要找出对用户重要的类别，并测量和改进更具针对性的度量标准，如类别-[F1](https://en.wikipedia.org/wiki/F-score)。
- en: How to Figure Out What’s Important for our Users?
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何确定对我们的用户来说什么是重要的？
- en: Aha! This is the most important question for ASR. WER, TER, or category-F1 are
    all metrics for scientists to validate their progress, but still can be quite
    far off from what’s important for the users. So, what is important? To answer
    this question, we’ll have to go back to why users need an ASR system in the first
    place. This, of course, depends on the scenario. Let’s take the case of dictation
    first. The sole purpose of dictation is to be able to replace typing. Don’t we
    already have a metric for that? Words per minute (WPM) is already a well-established
    metric to measure typing efficiency. I’d argue this is the perfect metric for
    Dictation. If dictation users can achieve much higher WPM by dictating than typing,
    the ASR system has done its job. Of course, WPM here takes well into account how
    users need to go back and correct things that can slow them down. There would
    be some errors that are a must-fix for users, and then there are some errors that
    are acceptable. This naturally assigns a higher weight to what matters and may
    even disagree with TER which gives equal weight to all errors. Beautiful!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 啊哈！这是ASR（自动语音识别）中最重要的问题。词错误率（WER）、翻译错误率（TER）或类别-F1都是科学家用来验证进展的度量标准，但仍可能与用户真正关心的内容相距甚远。那么，什么才是重要的呢？为了回答这个问题，我们需要回到用户最初需要ASR系统的原因。这当然取决于具体场景。首先以听写为例。听写的唯一目的是替代打字。我们难道没有一个衡量这个的标准吗？每分钟字数（WPM）已经是一个衡量打字效率的成熟标准。我认为这是听写的完美标准。如果听写用户能够通过听写实现比打字更高的WPM，那么ASR系统就完成了它的任务。当然，这里的WPM充分考虑了用户需要回头纠正错误的情况，这可能会拖慢他们的速度。某些错误是用户必须修复的，而有些错误是可以接受的。这自然赋予了重要内容更高的权重，甚至可能与TER对所有错误赋予相等权重的做法有所不同。太棒了！
- en: Can we use the same Logic for Conversation or Meeting Transcription?
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们能否将相同的逻辑应用于对话或会议记录？
- en: The meeting transcription case is quite different from dictation, in its objective.
    Dictation is a human2machine use-case and meeting transcription is a human2human
    use-case. WPM stops being the right metric. However, if there is a purpose to
    generating transcriptions, the right metric lies around that purpose. For instance,
    the goal of a broadcast meeting might be to generate human-readable transcriptions
    at the end, so how many edits a human annotator have to make on top of the machine-generated
    transcript becomes a metric. This is like TER, except it doesn’t matter if some
    sections are misrecognized, what matters is that the result is coherent and flows
    well.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 会议记录与听写在目标上有很大不同。听写是人机交互的用例，而会议记录则是人际间的用例。每分钟字数（WPM）不再是合适的度量标准。然而，如果生成记录有其目的，那么合适的度量标准就是围绕这个目的而来的。例如，广播会议的目标可能是在最后生成可读的记录，因此人工标注员需要对机器生成的记录进行多少次编辑就成为了一个度量标准。这类似于TER，只是部分内容是否被误识别并不重要，重要的是结果是否连贯且流畅。
- en: Another purpose could be to extract actionable insights from transcriptions
    or generate summaries. These are harder to measure to connect back to recognition
    accuracy. However, human interactions can still be measured, and a task completion
    or engagement-type metric is more suitable.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个目的可能是从转录中提取可操作的见解或生成摘要。这些更难以衡量与识别准确率的关系。然而，人类交互仍然可以被衡量，任务完成或参与度类型的指标更为合适。
- en: The metrics we discussed so far can be categorized as ‘online’ metrics and ‘offline’
    metrics, as summarized in Table 2\. In an ideal world, they move together. I’d
    argue that while offline metrics could be a good indicator of potential improvements,
    ‘online’ metrics are the real measure of success. When improved ASR models are
    ready, it is important to first measure the right offline metrics. Shipping them
    is only half the job done. The real test is passed when these models improve the
    ‘online’ metrics for the customers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止讨论的指标可以被归类为“在线”指标和“离线”指标，如表2所总结的那样。在理想的世界中，它们应该是同步的。我认为，虽然离线指标可能是潜在改进的良好指示，但“在线”指标才是真正的成功衡量标准。当改进的
    ASR 模型准备好时，首先测量正确的离线指标非常重要。发布这些模型只是完成了一半的工作。真正的考验是这些模型是否能改善客户的“在线”指标。
- en: '|  | Offline Metrics | Online Metrics |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | 离线指标 | 在线指标 |'
- en: '| Voice Search | Spoken-form Word Error Rate (WER) | Successful Click on relevant
    search result |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 语音搜索 | 口语形式的单词错误率（WER） | 成功点击相关搜索结果 |'
- en: '| Voice Assistant | Token Error Rate (TER),Formatting F1 for time, date, phone
    numbers, etc. | Task Completion rate |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 语音助手 | 令牌错误率（TER），时间、日期、电话号码等的格式化F1 | 任务完成率 |'
- en: '| Voice Typing (Dictation) | Token Error Rate (TER),Punctuation-F1, Capitalization-F1,
    | Words per minute (WPM), Edit rate,User retention/engagement |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 语音输入（听写） | 令牌错误率（TER），标点符号F1，大小写F1 | 每分钟单词数（WPM），编辑率，用户保留/参与度 |'
- en: '| Meeting (Conversation) | Token Error Rate (TER),Punctuation-F1, Capitalization-F1,
    Disfluency-F1 | Transcription edit rate,User engagement with Transcription UI
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 会议（对话） | 令牌错误率（TER），标点符号F1，大小写F1，流畅性F1 | 转录编辑率，用户与转录UI的互动 |'
- en: 'Table 2: Offline and Online metrics for Speech Recognition'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：语音识别的离线和在线指标
- en: References
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '"Achieving Human Parity in Conversational Speech Recognition - arXiv." 17 Oct.
    2016, [https://arxiv.org/abs/1610.05256](https://arxiv.org/abs/1610.05256).'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “实现对话语音识别中的人类水平 - arXiv。” 2016年10月17日，[https://arxiv.org/abs/1610.05256](https://arxiv.org/abs/1610.05256).
- en: Word Error Rate [https://en.wikipedia.org/wiki/Word_error_rate](https://en.wikipedia.org/wiki/Word_error_rate)
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单词错误率 [https://en.wikipedia.org/wiki/Word_error_rate](https://en.wikipedia.org/wiki/Word_error_rate)
- en: F1 score [https://en.wikipedia.org/wiki/F-score](https://en.wikipedia.org/wiki/F-score)
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: F1 分数 [https://en.wikipedia.org/wiki/F-score](https://en.wikipedia.org/wiki/F-score)
- en: Words per minute (WPM) [https://en.wikipedia.org/wiki/Words_per_minute](https://en.wikipedia.org/wiki/Words_per_minute)
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每分钟单词数（WPM） [https://en.wikipedia.org/wiki/Words_per_minute](https://en.wikipedia.org/wiki/Words_per_minute)
- en: '**[Piyush Behre](https://www.linkedin.com/in/piyushbehre/)** is a Principal
    Applied Scientist at Microsoft working on Speech Recognition/Natural Language
    Processing. He received his Bachelor of Technology degree in Computer Science
    and Engineering from the Indian Institute of Technology, Roorkee.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Piyush Behre](https://www.linkedin.com/in/piyushbehre/)** 是微软的首席应用科学家，专注于语音识别/自然语言处理。他获得了印度理工学院鲁尔基分校的计算机科学与工程学士学位。'
- en: More On This Topic
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解更多相关信息
- en: '[Build a Text-to-Speech Converter with Python in 5 Minutes](https://www.kdnuggets.com/2022/09/build-texttospeech-converter-python-5-minutes.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用Python在5分钟内构建一个文本转语音转换器](https://www.kdnuggets.com/2022/09/build-texttospeech-converter-python-5-minutes.html)'
- en: '[Transfer Learning for Image Recognition and Natural Language Processing](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用于图像识别和自然语言处理的迁移学习](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
- en: '[5 IT Jobs That Are High in Demand But Don’t Get Enough Recognition](https://www.kdnuggets.com/5-it-jobs-that-are-high-in-demand-but-dont-get-enough-recognition)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5个需求量大但未得到足够认可的IT职位](https://www.kdnuggets.com/5-it-jobs-that-are-high-in-demand-but-dont-get-enough-recognition)'
- en: '[From Oracle to Databases for AI: The Evolution of Data Storage](https://www.kdnuggets.com/2022/02/oracle-databases-ai-evolution-data-storage.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从Oracle到AI数据库：数据存储的演变](https://www.kdnuggets.com/2022/02/oracle-databases-ai-evolution-data-storage.html)'
- en: '[Analyzing the Probability of Future Success with Intelligence…](https://www.kdnuggets.com/2022/02/analyzing-probability-future-success-intelligence-node-attributes-evolution-model.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分析未来成功的概率与智能…](https://www.kdnuggets.com/2022/02/analyzing-probability-future-success-intelligence-node-attributes-evolution-model.html)'
- en: '[The Evolution From Artificial Intelligence to Machine Learning to…](https://www.kdnuggets.com/2022/08/evolution-artificial-intelligence-machine-learning-data-science.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从人工智能到机器学习再到…](https://www.kdnuggets.com/2022/08/evolution-artificial-intelligence-machine-learning-data-science.html)'
