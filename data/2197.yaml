- en: WTF is the Difference Between GBM and XGBoost?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GBM和XGBoost之间的区别是什么？
- en: 原文：[https://www.kdnuggets.com/wtf-is-the-difference-between-gbm-and-xgboost](https://www.kdnuggets.com/wtf-is-the-difference-between-gbm-and-xgboost)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/wtf-is-the-difference-between-gbm-and-xgboost](https://www.kdnuggets.com/wtf-is-the-difference-between-gbm-and-xgboost)
- en: '![WTF is the Difference Between GBM and XGBoost?](../Images/ad561fe1a6e273df29c1f7e74542575f.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![GBM和XGBoost之间的区别是什么？](../Images/ad561fe1a6e273df29c1f7e74542575f.png)'
- en: Image by [upklyak](https://www.freepik.com/free-vector/business-startup-project-launch-successful-idea_29222778.htm#query=boost&position=4&from_view=search&track=sph&uuid=b8330f52-58e3-4628-830f-2673f9ab16e6)
    on [Freepik](https://www.freepik.com/)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[upklyak](https://www.freepik.com/free-vector/business-startup-project-launch-successful-idea_29222778.htm#query=boost&position=4&from_view=search&track=sph&uuid=b8330f52-58e3-4628-830f-2673f9ab16e6)提供，来源于[Freepik](https://www.freepik.com/)。
- en: I am sure everyone knows about the algorithms GBM and XGBoost. They are go-to
    algorithms for many real-world use cases and competition because the metric output
    is often better than the other models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信每个人都知道GBM和XGBoost这两个算法。它们是许多实际应用和竞赛中的首选算法，因为它们的度量输出通常优于其他模型。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: For those who don’t know about GBM and XGBoost, GBM (Gradient Boosting Machine)
    and XGBoost (eXtreme Gradient Boosting) are ensemble learning methods. Ensemble
    learning is a machine learning technique where multiple “weak” models (often decision
    trees) are trained and combined for further purposes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些不了解GBM和XGBoost的人，GBM（梯度提升机）和XGBoost（极端梯度提升）是集成学习方法。集成学习是一种机器学习技术，通过训练和组合多个“弱”模型（通常是决策树）来实现进一步的目的。
- en: The algorithm was based on the ensemble learning boosting technique shown in
    their name. Boosting techniques is a method that tries to combine multiple weak
    learners sequentially, with each one correcting its predecessor.  Each learner
    would learn from their previous mistakes and correct the errors of the previous
    models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法基于其名称所示的集成学习提升技术。提升技术是一种方法，试图顺序地结合多个弱学习器，每一个都修正其前任。每个学习器都会从之前的错误中学习并纠正前一个模型的错误。
- en: That’s the fundamental similarity between GBM and XGB, but how about the differences?
    We will discuss that in this article, so let’s get into it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是GBM和XGB之间的基本相似之处，但差异如何呢？我们将在本文中讨论这一点，让我们深入了解一下吧。
- en: GBM (Gradient Boosting Machine)
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GBM（梯度提升机）
- en: As mentioned above, GBM is based on boosting, which tries sequentially iterating
    the weak learner to learn from the error and develop a robust model. GBM developed
    a better model for each iteration by minimizing the loss function using gradient
    descent. Gradient descent is a concept to find the minimum function with each
    iteration, such as the loss function. The iteration would keep going until it
    achieves the stopping criterion.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，GBM基于提升方法，尝试通过顺序迭代弱学习器以从错误中学习并开发出一个稳健的模型。GBM通过使用梯度下降法来最小化损失函数，从而在每次迭代中开发出更好的模型。梯度下降是一个在每次迭代中寻找最小函数（例如损失函数）的概念。迭代将持续进行，直到达到停止标准。
- en: For the GBM concepts, you can see it in the image below.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 关于GBM的概念，你可以在下图中查看。
- en: '![WTF is the Difference Between GBM and XGBoost?](../Images/967eac9e5ae21617bb2b8b2302c000f8.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![GBM和XGBoost之间的区别是什么？](../Images/967eac9e5ae21617bb2b8b2302c000f8.png)'
- en: GBM Model Concept ([Chhetri *et al.* (2022)](https://www.researchgate.net/publication/358924681_A_Combined_System_Metrics_Approach_to_Cloud_Service_Reliability_Using_Artificial_Intelligence))
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: GBM模型概念（[Chhetri *et al.* (2022)](https://www.researchgate.net/publication/358924681_A_Combined_System_Metrics_Approach_to_Cloud_Service_Reliability_Using_Artificial_Intelligence)）
- en: You can see in the image above that for each iteration, the model tries to minimize
    the loss function and learn from the previous mistake. The final model would be
    the whole weak learner that sums up all the predictions from the model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在上面的图像中看到，对于每次迭代，模型试图最小化损失函数并从之前的错误中学习。最终模型将是所有弱学习器的总和，汇总了模型的所有预测。
- en: XGB (eXtreme Gradient Boosting) and How It Is Different From GBM
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGB（eXtreme Gradient Boosting）及其与 GBM 的不同
- en: XGBoost or eXtreme Gradient Boosting is a machine-learning algorithm based on
    the gradient boosting algorithm developed by [Tiangqi Chen and Carlos Guestrin
    in 2016](https://arxiv.org/pdf/1603.02754.pdf). At a basic level, the algorithm
    still follows a sequential strategy to improve the next model based on gradient
    descent. However, a few differences of XGBoost push this model as one of the best
    in terms of performance and speed.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 或 eXtreme Gradient Boosting 是一种基于梯度提升算法的机器学习算法，由 [Tiangqi Chen 和 Carlos
    Guestrin 于 2016 年开发](https://arxiv.org/pdf/1603.02754.pdf)。从基本层面来看，该算法仍遵循一种顺序策略，以基于梯度下降改进下一个模型。然而，XGBoost
    的一些差异使其在性能和速度方面成为最优秀的模型之一。
- en: 1\. Regularization
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 正则化
- en: Regularization is a technique in machine learning to avoid overfitting. It’s
    a collection of methods to constrain the model to become overcomplicated and have
    bad generalization power. It’s become an important technique as many models fit
    the training data too well.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是一种机器学习技术，用于避免过拟合。它是一组约束模型过度复杂化并具有差劲泛化能力的方法。由于许多模型对训练数据的拟合过好，正则化成为了一种重要的技术。
- en: GBM doesn’t implement Regularization in their algorithm, which makes the algorithm
    only focus on achieving minimum loss functions. Compared to the GBM, XGBoost implements
    the regularization methods to penalize the overfitting model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: GBM 的算法中没有实现正则化，这使得算法只关注于达到最小损失函数。与 GBM 相比，XGBoost 实施了正则化方法来惩罚过拟合的模型。
- en: 'There are two kinds of regularization that XGBoost could apply: L1 Regularization
    (Lasso) and L2 Regularization (Ridge). L1 Regularization tries to minimize the
    feature weights or coefficients to zero (effectively becoming a feature selection),
    while L2 Regularization tries to shrink the coefficient evenly (help to deal with
    multicollinearity).  By implementing both regularizations, XGBoost could avoid
    overfitting better than the GBM.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 可以应用两种正则化：L1 正则化（Lasso）和 L2 正则化（Ridge）。L1 正则化试图将特征权重或系数最小化为零（有效地进行特征选择），而
    L2 正则化则试图均匀缩小系数（帮助处理多重共线性）。通过实施这两种正则化，XGBoost 可以比 GBM 更好地避免过拟合。
- en: 2\. Parallelization
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 并行化
- en: GBM tends to have a slower training time than the XGBoost because the latter
    algorithm implements parallelization during the training process. The boosting
    technique might be sequential, but parallelization could still be done within
    the XGBoost process.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: GBM 的训练时间通常比 XGBoost 慢，因为后者在训练过程中实现了并行化。尽管提升技术可能是顺序的，但并行化仍然可以在 XGBoost 过程中进行。
- en: The parallelization aims to speed up the tree-building process, mainly during
    the splitting event. By utilizing all the available processing cores, the XGBoost
    training time can be shortened.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 并行化旨在加速树构建过程，主要是在拆分事件期间。通过利用所有可用的处理核心，XGBoost 的训练时间可以缩短。
- en: Speaking of speeding up the XGBoost process, the developer also preprocessed
    the data into their developed data format, DMatrix, for memory efficiency and
    improved training speed.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 说到加速 XGBoost 过程，开发者还将数据预处理为他们开发的数据格式 DMatrix，以提高内存效率和训练速度。
- en: 3\. Missing Data Handling
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 缺失数据处理
- en: Our training dataset could contain missing data, which we must explicitly handle
    before passing them into the algorithm. However, XGBoost has its own in-built
    missing data handler, whereas GBM doesn’t.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练数据集可能包含缺失数据，我们必须在将其传递给算法之前明确处理。然而，XGBoost 具有内置的缺失数据处理器，而 GBM 没有。
- en: XGBoost implemented their technique to handle missing data, called Sparsity-aware
    Split Finding. For any sparsities data that XGBoost encounters (Missing Data,
    Dense Zero, OHE), the model would learn from these data and find the most optimum
    split. The model would assign where the missing data should be placed during splitting
    and see which direction minimizes the loss.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 实现了处理缺失数据的技术，称为“感知稀疏拆分”。对于 XGBoost 遇到的任何稀疏数据（缺失数据、密集零值、OHE），模型会从这些数据中学习，并找到最优的拆分点。模型会在拆分过程中确定缺失数据应放置的位置，并查看哪个方向可以最小化损失。
- en: 4\. Tree Pruning
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 树修剪
- en: The growth strategy for the GBM is to stop splitting after the algorithm arrives
    at the negative loss in the split. The strategy could lead to suboptimal results
    because it’s only based on local optimization and might neglect the overall picture.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: GBM的增长策略是在算法达到负损失时停止分裂。这个策略可能导致次优结果，因为它仅基于局部优化，可能忽视了整体情况。
- en: XGBoost tries to avoid the GBM strategy and grows the tree until the set parameter
    max depth starts pruning backward. The split with negative loss is pruned, but
    there is a case when the negative loss split was not removed. When the split arrives
    at a negative loss, but the further split is positive, it would still be retained
    if the overall split is positive.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost试图避免GBM策略，继续增长树直到设置参数最大深度开始向后剪枝。负损失的分裂会被剪枝，但在某些情况下，负损失分裂未被移除。当分裂达到负损失，但进一步分裂为正时，如果总体分裂为正，仍会保留该分裂。
- en: 5\. In-Built Cross-Validation
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 内置交叉验证
- en: Cross-validation is a technique to assess our model generalization and robustness
    ability by splitting the data systematically during several iterations. Collectively,
    their result would show if the model is overfitting or not.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证是一种评估模型泛化能力和鲁棒性的技术，通过在多个迭代过程中系统地拆分数据来实现。综合结果将显示模型是否过拟合。
- en: Normally, the machine algorithm would require external help to implement the
    Cross-Validation, but XGBoost has an in-built Cross-Validation that could be used
    during the training session. The Cross-Validation would be performed at each boosting
    iteration and ensure the produce tree is robust.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，机器算法需要外部帮助来实现交叉验证，但XGBoost具有内置的交叉验证，可以在训练过程中使用。交叉验证将在每次提升迭代中执行，并确保生成的树是鲁棒的。
- en: Conclusion
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: GBM and XGBoost are popular algorithms in many real-world cases and competitions.
    Conceptually, both a boosting algorithms that use weak learners to achieve better
    models. However, they contain few differences in their algorithm implementation.
    XGBoost enhances the algorithm by embedding regularization, performing parallelization,
    better-missing data handling, different tree pruning strategies, and in-built
    cross-validation techniques.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: GBM和XGBoost是许多实际案例和竞赛中的热门算法。从概念上讲，这两者都是使用弱学习器来实现更好模型的提升算法。然而，它们在算法实现上有一些差异。XGBoost通过嵌入正则化、执行并行化、改进缺失数据处理、不同的树剪枝策略以及内置交叉验证技术来增强算法。
- en: '**[](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**[Cornellius Yudha
    Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**** is a data science
    assistant manager and data writer. While working full-time at Allianz Indonesia,
    he loves to share Python and data tips via social media and writing media. Cornellius
    writes on a variety of AI and machine learning topics.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Cornellius Yudha Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**是一名数据科学助理经理和数据撰稿人。在全职工作于Allianz
    Indonesia期间，他喜欢通过社交媒体和写作媒体分享Python和数据技巧。Cornellius在各种AI和机器学习话题上撰写文章。'
- en: More On This Topic
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[What Is the Difference Between SQL and Object-Relational Mapping (ORM)?](https://www.kdnuggets.com/2022/02/difference-sql-object-relational-mapping-orm.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQL与对象关系映射（ORM）之间的区别是什么？](https://www.kdnuggets.com/2022/02/difference-sql-object-relational-mapping-orm.html)'
- en: '[What’s the Difference Between Data Analysts and Data Scientists?](https://www.kdnuggets.com/2022/03/difference-data-analysts-data-scientists.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据分析师和数据科学家之间的区别是什么？](https://www.kdnuggets.com/2022/03/difference-data-analysts-data-scientists.html)'
- en: '[Efficiency Spells the Difference Between Biological Neurons and…](https://www.kdnuggets.com/2022/11/efficiency-spells-difference-biological-neurons-artificial-counterparts.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[效率决定了生物神经元和……之间的区别](https://www.kdnuggets.com/2022/11/efficiency-spells-difference-biological-neurons-artificial-counterparts.html)'
- en: '[The Difference Between L1 and L2 Regularization](https://www.kdnuggets.com/2022/08/difference-l1-l2-regularization.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[L1和L2正则化之间的区别](https://www.kdnuggets.com/2022/08/difference-l1-l2-regularization.html)'
- en: '[The Difference Between Training and Testing Data in Machine Learning](https://www.kdnuggets.com/2022/08/difference-training-testing-data-machine-learning.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中训练数据和测试数据之间的区别](https://www.kdnuggets.com/2022/08/difference-training-testing-data-machine-learning.html)'
- en: '[WTF is Regularization and What is it For?](https://www.kdnuggets.com/wtf-is-regularization-and-what-is-it-for)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[正则化是什么？它的用途是什么？](https://www.kdnuggets.com/wtf-is-regularization-and-what-is-it-for)'
