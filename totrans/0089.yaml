- en: 'Effective Small Language Models: Microsoft’s 1.3 Billion Parameter phi-1.5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/effective-small-language-models-microsoft-phi-15](https://www.kdnuggets.com/effective-small-language-models-microsoft-phi-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Effective Small Language Models: Microsoft’s 1.3 Billion Parameter phi-1.5](../Images/b849647a0bb3e8b48392d4773f0f727e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: When you thought you had heard enough news about Large Language Models (LLMs),
    Microsoft Research has come out to disturb the market again. In June 2023, Microsoft
    Research released a paper called “[Textbooks is All You Need](https://arxiv.org/abs/2306.11644),”
    where they introduced [phi-1](https://huggingface.co/microsoft/phi-1), a new large
    language model for code. phi-1 is a transformer-based model with 1.3B parameters,
    which was trained for 4 days on 8 A100s GPUs, which used a selection of “textbook
    quality” data from the web.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: It seems like LLMs are getting smaller and smaller.
  prefs: []
  type: TYPE_NORMAL
- en: What is phi-1.5?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now Microsoft Research introduces to you [phi-1.5](https://huggingface.co/microsoft/phi-1_5),
    a Transformer with 1.3B parameters, which was trained using the same data sources
    as phi-1\. As stated above, phi-1 was trained on high-quality textbook data, whereas
    phi-1.5 was trained on synthetic data only.
  prefs: []
  type: TYPE_NORMAL
- en: phi-1.5 used 32xA100-40G GPUs and was successfully trained in 8 days. The aim
    behind phi-1.5 was to craft an open-source model that can play a role in the research
    community using a non-restricted small model which allows you to explore the different
    safety challenges with LLMs, such as reducing toxicity, enhancing controllability,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: By using the ‘Synthetic Data Generation’ approach, phi-1.5 performance is equivalent
    to models that are 5x larger on natural language tests and has been shown to outperform
    most LLMs on more difficult reasoning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Pretty impressive right?
  prefs: []
  type: TYPE_NORMAL
- en: The model’s learning journey is very interesting. It draws data from a variety
    of sources, including Python code snippets from StackOverflow, synthetic Python
    textbooks as well exercises that were generated by GPT-3.5-turbo-0301.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing Toxicity and Biases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the major challenges with LLMs is toxicity and biased content. Microsoft
    Research aimed to overcome this ongoing challenge of harmful/offensive content
    and content that promotes a specific ideology.
  prefs: []
  type: TYPE_NORMAL
- en: 'The synthetic data used to train the model generated responses with a lower
    propensity for generating toxic content in comparison to other LLMs such as Falcon-7B
    and Llama 2–7B, as shown in the image below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Effective Small Language Models: Microsoft’s 1.3 Billion Parameter phi-1.5](../Images/d214eb2af724ebdc4c9347a1b547e1e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image via [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/pdf/2309.05463.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The image below shows how phi-1.5 performed slightly better than state-of-the-art
    models, such as Llama 2–7B, Llama-7B, and Falcon-RW-1.3B) on 3 benchmarks: common
    sense reasoning, language skills, and multi-step reasoning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Effective Small Language Models: Microsoft’s 1.3 Billion Parameter phi-1.5](../Images/44c24b924f8ef1c49cdb6710bc245a03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image via [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/pdf/2309.05463.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: How was this done?
  prefs: []
  type: TYPE_NORMAL
- en: The use of textbook-like data differentiated the use of such data in LLMs in
    comparison to data extracted from the internet. To further assess how the model
    deals with toxic content, ToxiGen was used as well 86 prompts were designed and
    manually labeled ‘pass’, ‘fail’ or ‘did not understand’ to get a better understanding
    of the model's limitations.
  prefs: []
  type: TYPE_NORMAL
- en: With this being said, phi-1.5 passed 47 prompts, failed 34 prompts and did not
    understand 4 prompts. The HumanEval approach to assess the models generates responses
    showing that phi-1.5 scored higher in comparison to other well-known models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key Takeaways:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are the major talking points you should take away from here regarding
    phi-1.5:'
  prefs: []
  type: TYPE_NORMAL
- en: Is a transformer-based model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is a LLM that focuses on next-word prediction objectives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Was trained on 30 billion tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Used 32xA100-40G GPUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Was successfully trained in 8 days
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist,
    Freelance Technical Writer and Community Manager at KDnuggets. She is particularly
    interested in providing Data Science career advice or tutorials and theory based
    knowledge around Data Science. She also wishes to explore the different ways Artificial
    Intelligence is/can benefit the longevity of human life. A keen learner, seeking
    to broaden her tech knowledge and writing skills, whilst helping guide others.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Overview of PEFT: State-of-the-art Parameter-Efficient Fine-Tuning](https://www.kdnuggets.com/overview-of-peft-stateoftheart-parameterefficient-finetuning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Digest 15 Billion Logs Per Day and Keep Big Queries Within 1 Second](https://www.kdnuggets.com/how-to-digest-15-billion-logs-per-day-and-keep-big-queries-within-1-second)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Steps to Running a Small Language Model on a Local CPU](https://www.kdnuggets.com/7-steps-to-running-a-small-language-model-on-a-local-cpu)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[N-gram Language Modeling in Natural Language Processing](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Open Source Large Language Models](https://www.kdnuggets.com/2022/09/john-snow-top-open-source-large-language-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AI: Large Language & Visual Models](https://www.kdnuggets.com/2023/06/ai-large-language-visual-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
