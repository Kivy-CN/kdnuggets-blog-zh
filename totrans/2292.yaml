- en: Machine Learning Algorithms Explained in Less Than 1 Minute Each
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习算法解释，少于 1 分钟
- en: 原文：[https://www.kdnuggets.com/2022/07/machine-learning-algorithms-explained-less-1-minute.html](https://www.kdnuggets.com/2022/07/machine-learning-algorithms-explained-less-1-minute.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/07/machine-learning-algorithms-explained-less-1-minute.html](https://www.kdnuggets.com/2022/07/machine-learning-algorithms-explained-less-1-minute.html)
- en: '![Machine Learning Algorithms Explained in Less Than 1 Minute Each](../Images/513b4a116bd442cb812272e0c636791c.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习算法解释，少于 1 分钟](../Images/513b4a116bd442cb812272e0c636791c.png)'
- en: Image by [pch.vector](https://www.freepik.com/free-vector/man-digital-era-algorithm-ai-social-system-21st-century-workforce-challenge-flat-vector-illustration-smart-business-process-human-resources-automation-artificial-intelligence-concept_22343897.htm#query=Algorithms&position=23&from_view=search&track=sph)
    on Freepik
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [pch.vector](https://www.freepik.com/free-vector/man-digital-era-algorithm-ai-social-system-21st-century-workforce-challenge-flat-vector-illustration-smart-business-process-human-resources-automation-artificial-intelligence-concept_22343897.htm#query=Algorithms&position=23&from_view=search&track=sph)
    在 Freepik 提供
- en: This article will explain some of the most well known machine learning algorithms
    in less than a minute - helping everyone to understand them!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章将在不到一分钟的时间里解释一些最著名的机器学习算法——帮助每个人理解它们！
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三名课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Linear Regression
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归
- en: One of the simplest Machine learning algorithms out there, Linear Regression
    is used to make predictions on continuous dependent variables with knowledge from
    independent variables. A dependent variable is the effect, in which its value
    depends on changes in the independent variable.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是最简单的机器学习算法之一，用于对连续的因变量进行预测，基于对自变量的知识。因变量是效果，其值取决于自变量的变化。
- en: You may remember the line of best fit from school - this is what Linear Regression
    produces. A simple example is predicting one's weight depending on their height.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得学校中的最佳拟合线——这就是线性回归所产生的。一个简单的例子是根据身高预测体重。
- en: Logistic Regression
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Logistic Regression, similar to Linear Regression, is used to make predictions
    on categorical dependent variables with knowledge of independent variables. A
    categorical variable has two or more categories. Logistic Regression classifies
    outputs that can only be between 0 and 1.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归与线性回归类似，用于对分类因变量进行预测，基于自变量的知识。分类变量有两个或更多类别。逻辑回归将输出分类为只能在0和1之间的值。
- en: For example, you can use Logistic Regression to determine whether a student
    will be admitted or not to a particular college depending on their grades - either
    Yes or No, or 0 or 1.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以使用逻辑回归来确定学生是否会被某个特定大学录取，取决于他们的成绩——结果可以是“是”或“否”，或者是0或1。
- en: Decision Trees
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: Decision Trees (DTs) is a probability tree-like structure model that continuously
    splits data to categorize or make predictions based on the previous set of questions
    that were answered. The model learns the features of the data and answers questions
    to help you make better decisions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树（DTs）是一种树状结构的概率模型，它不断分裂数据，以根据先前回答的问题进行分类或预测。该模型学习数据的特征并回答问题，以帮助你做出更好的决策。
- en: For example, you can use a decision tree using the answers Yes or No to determine
    a specific species of bird using data features such as feathers, ability to fly
    or swim, beak type, etc.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以使用决策树，通过“是”或“否”来确定某种特定鸟类，使用的特征数据包括羽毛、飞行或游泳能力、喙的类型等。
- en: Random Forest
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: Similar to Decision Trees, Random Forest is also a tree-based algorithm. Where
    Decision Tree consists of one tree, Random forest uses multiple decision trees
    for making decisions - a forest of trees.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于决策树，随机森林也是一种基于树的算法。决策树由一棵树组成，而随机森林使用多棵决策树进行决策——即一片树木的森林。
- en: It combines multiple models to make predictions and can be used in Classification
    and Regression tasks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 它结合了多个模型来进行预测，可用于分类和回归任务。
- en: K-Nearest Neighbors
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-最近邻
- en: K-Nearest Neighbors uses the statistical knowledge of how close a data point
    is to another data point and determines if these data points can be grouped together.
    The closeness in the data points reflects the similarities in one another.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: K-最近邻利用数据点之间的距离来决定这些数据点是否可以分组。数据点之间的接近度反映了它们之间的相似性。
- en: For example, if we had a graph which had a group of data points that were close
    to one another called Group A and another group of data points that were in close
    proximity to one another called Group B. When we input a new data point, depending
    which group the new data point is nearer to - that will be their new classified
    group.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有一个图表，其中有一组彼此靠近的数据点称为组A，另一组彼此靠近的数据点称为组B。当输入一个新的数据点时，根据新数据点更接近哪个组——这将是它的新分类组。
- en: Support Vector Machines
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Similar to Nearest Neighbor, Support Vector Machines performs classification,
    regression and outlier detection tasks. It does this by drawing a hyperplane (a
    straight line) to separate the classes. The data points that are located on one
    side of the line will be labeled as Group A, whilst the points on the other side
    will be labeled as Group B.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于最近邻，支持向量机执行分类、回归和异常值检测任务。它通过绘制一个超平面（直线）来分隔类别。位于直线一侧的数据点将被标记为组A，而另一侧的数据点将被标记为组B。
- en: For example, when a new data point is inputted, depending on which side of the
    hyperplane and its location within the margin it is - this will determine which
    group the data point belongs to.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当输入一个新的数据点时，依据该数据点位于超平面哪一侧及其在边界内的位置——这将决定数据点属于哪个类别。
- en: Naive Bayes
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Naive Bayes is based on Bayes’ Theorem which is a mathematical formula used
    for calculating conditional probabilities. Conditional probability is the chance
    of an outcome occurring given that another event has also occurred.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯基于贝叶斯定理，这是一种用于计算条件概率的数学公式。条件概率是指在另一个事件发生的情况下，某一结果发生的机会。
- en: It predicts that the probabilities for each class belongs to a particular class
    and that the class with the highest probability is considered the most likely
    class.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 它预测每个类别的概率，归属于某个特定类别，概率最高的类别被认为是最可能的类别。
- en: k-means Clustering
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k均值聚类
- en: K-means clustering, similar to nearest neighbors but uses the method of clustering
    to group similar items/data points in clusters. The number of groups is referred
    to as K. You do this by selecting the k value, initializing the centroids and
    then selecting the group and finding the average.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: K均值聚类，类似于最近邻，但使用聚类的方法将相似的项/数据点分组在簇中。簇的数量被称为K。你通过选择k值、初始化质心，然后选择组并计算平均值来完成这一过程。
- en: For example, if there are 3 clusters present and a new data point is inputted,
    depending on which cluster it falls in - that is the cluster they belong to.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果存在3个簇，且输入一个新的数据点，根据它属于哪个簇——这就是它所属的簇。
- en: Bagging
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 袋装
- en: Bagging is also known as Bootstrap aggregating and is an ensemble learning technique.
    Bagging is used in both regression and classification models and aims to avoid
    overfitting of data and reduce the variance in the predictions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装也被称为自助聚合（Bootstrap aggregating），是一种集成学习技术。袋装用于回归和分类模型，旨在避免数据的过拟合，并减少预测中的方差。
- en: Overfitting is when a model fits exactly against its training data - basically
    not teaching us anything and can be due to various reasons. Random Forest is an
    example of Bagging.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是指模型对训练数据的拟合过于精确——基本上没有提供新的信息，这可能由各种原因造成。随机森林是袋装（Bagging）的一个例子。
- en: Boosting
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升
- en: The overall aim of Boosting is to convert weak learners to strong learners.
    Weak learners are found by applying base learning algorithms which then generates
    a new weak prediction rule. A  random sample of data is inputted in a model and
    then trained sequentially, aiming to train the weak learners and trying to correct
    its predecessor
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Boosting 的总体目标是将弱学习者转化为强学习者。弱学习者是通过应用基本学习算法找到的，这些算法生成新的弱预测规则。数据的随机样本被输入到模型中，然后进行顺序训练，旨在训练弱学习者并尝试纠正其前身。
- en: XGBoost, which stands for Extreme Gradient Boosting, is used in Boosting.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost，代表极端梯度提升，被用于 Boosting。
- en: Dimensionality Reduction
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: Dimensionality reduction is used to reduce the number of input variables in
    the training data, by reducing the dimension of your feature set. When a model
    has a high number of features, it is naturally more complex leading to a higher
    chance of overfitting and decrease in accuracy.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 降维用于减少训练数据中输入变量的数量，通过降低特征集的维度。当模型具有大量特征时，模型自然更复杂，这导致过拟合的机会增加和准确性的下降。
- en: For example, if you had a dataset with a hundred columns, dimensionality reduction
    will reduce the number of columns down to twenty. However, you will need Feature
    Selection to select relevant features and Feature Engineering to generate new
    features from existing features.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你有一个包含一百列的数据集，降维将把列数减少到二十。然而，你还需要特征选择来选择相关特征，并需要特征工程从现有特征中生成新的特征。
- en: The Principal Component Analysis (PCA) technique is a type of Dimensionality
    Reduction.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）技术是一种降维方法。
- en: Conclusion
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The aim of this article was to help you understand Machine Learning algorithms
    in the most simplest terms. If you would like some more in depth understanding
    on each of them, have a read of this [Popular Machine Learning Algorithms](/2022/05/popular-machine-learning-algorithms.html).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目的是帮助你以最简单的术语理解机器学习算法。如果你想更深入地了解每一个算法，可以阅读这个[流行的机器学习算法](/2022/05/popular-machine-learning-algorithms.html)。
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and Freelance Technical Writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** 是一名数据科学家和自由技术撰稿人。她特别感兴趣于提供数据科学职业建议或教程以及数据科学的理论知识。她还希望探索人工智能如何能促进人类寿命的不同方式。作为一个热衷学习者，她寻求拓宽自己的技术知识和写作技能，同时帮助指导他人。'
- en: More On This Topic
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[KDnuggets News, July 20: Machine Learning Algorithms Explained in…](https://www.kdnuggets.com/2022/n29.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，7月20日：机器学习算法的解释…](https://www.kdnuggets.com/2022/n29.html)'
- en: '[Multi-modal deep learning in less than 15 lines of code](https://www.kdnuggets.com/2023/01/predibase-multi-modal-deep-learning-less-15-lines-code.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在少于 15 行代码中进行多模态深度学习](https://www.kdnuggets.com/2023/01/predibase-multi-modal-deep-learning-less-15-lines-code.html)'
- en: '[Become a Business Intelligence Analyst in Less Than 6 Months](https://www.kdnuggets.com/become-a-business-intelligence-analyst-in-less-than-6-months)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在不到 6 个月的时间内成为商业智能分析师](https://www.kdnuggets.com/become-a-business-intelligence-analyst-in-less-than-6-months)'
- en: '[Unlock the Secrets of LLMs in 60-Minute with Andrej Karpathy](https://www.kdnuggets.com/unlock-the-secrets-of-llms-in-a-60-minute-with-andrej-karpathy)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[与 Andrej Karpathy 一起在 60 分钟内解锁 LLM 的秘密](https://www.kdnuggets.com/unlock-the-secrets-of-llms-in-a-60-minute-with-andrej-karpathy)'
- en: '[The Range of NLP Applications in the Real World: A Different…](https://www.kdnuggets.com/2022/03/different-solution-problem-range-nlp-applications-real-world.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NLP 应用在现实世界中的范围：不同的解决方案…](https://www.kdnuggets.com/2022/03/different-solution-problem-range-nlp-applications-real-world.html)'
- en: '[Black Friday Deal - Master Machine Learning for Less with DataCamp](https://www.kdnuggets.com/2022/11/datacamp-black-friday-deal-master-machine-learning-less-datacamp.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[黑色星期五特惠 - 通过 DataCamp 以更低的价格掌握机器学习](https://www.kdnuggets.com/2022/11/datacamp-black-friday-deal-master-machine-learning-less-datacamp.html)'
