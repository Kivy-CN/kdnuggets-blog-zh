- en: 7 Types of Artificial Neural Networks for Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/10/7-types-artificial-neural-networks-natural-language-processing.html/2](https://www.kdnuggets.com/2017/10/7-types-artificial-neural-networks-natural-language-processing.html/2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 3\. Recursive neural network (RNN)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/bce9a470631fe036e9e9f0becdd3aa48.png)'
  prefs: []
  type: TYPE_IMG
- en: A simple recursive neural network architecture ([https://upload.wikimedia.org/wikipedia/commons/6/60/Simple_recursive_neural_network.svg](https://upload.wikimedia.org/wikipedia/commons/6/60/Simple_recursive_neural_network.svg))
  prefs: []
  type: TYPE_NORMAL
- en: A recursive neural network (RNN) is a type of deep neural network formed by
    applying the same set of weights recursively over a structure to make a structured
    prediction over variable-size input structures, or a scalar prediction on it,
    by traversing a given structure in topological order [[6]](https://en.wikipedia.org/wiki/Recursive_neural_network).
    In the simplest architecture, a nonlinearity such as tanh, and a weight matrix
    that is shared across the whole network are used to combine nodes into parents.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Recurrent neural network (RNN)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A recurrent neural network (RNN), unlike a [feedforward neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network),
    is a variant of a recursive artificial neural network in which connections between
    neurons make a directed cycle. It means that output depends not only on the present
    inputs but also on the previous step’s neuron state. This memory lets users solve
    NLP problems like connected handwriting recognition or speech recognition. In
    a paper, [Natural Language Generation, Paraphrasing and Summarization of User
    Reviews with Recurrent Neural Networks](http://www.meanotek.ru/files/TarasovDS%282%292015-Dialogue.pdf),
    authors demonstrate a recurrent neural network (RNN) model that can generate novel
    sentences and document summaries [[7]](http://www.meanotek.ru/files/TarasovDS%282%292015-Dialogue.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao created a recurrent convolutional
    neural network for text classification without human-designed features and described
    it in [Recurrent Convolutional Neural Networks for Text Classification](https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745/9552).
    Their model was compared to existing text classification methods like Bag of Words,
    Bigrams + LR, SVM, LDA, Tree Kernels, Recursive neural network, and CNN. It was
    shown that their model outperforms traditional methods for all used data sets [[8]](https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745/9552).
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Long short-term memory (LSTM)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/fbc3b4f78b901a963f9dcd7f21f5233b.png)'
  prefs: []
  type: TYPE_IMG
- en: A peephole LSTM block with input, output, and forget gates. ([https://upload.wikimedia.org/wikipedia/commons/5/53/Peephole_Long_Short-Term_Memory.svg](https://upload.wikimedia.org/wikipedia/commons/5/53/Peephole_Long_Short-Term_Memory.svg))
  prefs: []
  type: TYPE_NORMAL
- en: Long Short-Term Memory (LSTM) is a specific recurrent neural network (RNN) architecture
    that was designed to model temporal sequences and their long-range dependencies
    more accurately than conventional RNNs [[9]](https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenTerm1201415/sak2.pdf).
    LSTM does not use activation function within its recurrent components, the stored
    values are not modified, and the gradient does not tend to vanish during training.
    Usually, LSTM units are implemented in “blocks” with several units. These blocks
    have three or four “gates” (for example, input gate, forget gate, output gate)
    that control information flow drawing on the logistic function.
  prefs: []
  type: TYPE_NORMAL
- en: In [Long Short-Term Memory Recurrent Neural Network Architectures for Large
    Scale Acoustic Modeling](https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenTerm1201415/sak2.pdf),
    Hasim Sak, Andrew Senior, and Françoise Beaufays showed that the deep LSTM RNN
    architectures achieve state-of-the-art performance for large scale acoustic modeling.
  prefs: []
  type: TYPE_NORMAL
- en: In the work, [Part-of-Speech Tagging with Bidirectional Long Short-Term Memory
    Recurrent Neural Network](https://arxiv.org/pdf/1510.06168.pdf) by Peilu Wang,
    Yao Qian, Frank K. Soong, Lei He, and Hai Zhao, a model for [part-of-speech (POS)
    tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging) was presented [[10]](https://arxiv.org/pdf/1510.06168.pdf).
    The model achieved a performance of 97.40% tagging accuracy. Apple, Amazon, Google,
    Microsoft and other companies incorporated LSTM as a fundamental element into
    their products.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Sequence-to-sequence models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Usually, a sequence-to-sequence model consists of two recurrent neural networks:
    an [encoder](https://en.wikipedia.org/wiki/Artificial_neural_network#Encoder.E2.80.93decoder_networks) that
    processes the input and a [decoder](https://en.wikipedia.org/wiki/Artificial_neural_network#Encoder.E2.80.93decoder_networks) that
    produces the output. Encoder and decoder can use the same or different sets of
    parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: Sequence-to-Sequence models are mainly used in question answering systems, chatbots,
    and machine translation. Such multi-layer cells have been successfully used in
    sequence-to-sequence models for translation in [Sequence to Sequence Learning
    with Neural Networks study](https://arxiv.org/pdf/1409.3215.pdf) [[11]](https://arxiv.org/pdf/1409.3215.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: In [Paraphrase Detection Using Recursive Autoencoder](https://nlp.stanford.edu/courses/cs224n/2011/reports/ehhuang.pdf),
    a novel recursive autoencoder architecture is presented. The representations are
    vectors in an n-dimensional semantic space where phrases with similar meanings
    are close to each other [[12]](https://nlp.stanford.edu/courses/cs224n/2011/reports/ehhuang.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Shallow neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Besides deep neural networks, shallow models are also popular and useful tools.
    For example, [word2vec](https://ru.wikipedia.org/wiki/Word2vec) is a group of
    shallow two-layer models that are used for producing [word embeddings](https://en.wikipedia.org/wiki/Word_embedding).
    Presented in [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf),
    word2vec takes a large corpus of text as its input and produces a vector space [[13]](https://arxiv.org/pdf/1301.3781.pdf).
    Every word in the corpus obtains the corresponding vector in this space. The distinctive
    feature is that words from common contexts in the corpus are located close to
    one another in the vector space.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this paper, we described different variants of artificial neural networks,
    such as deep multilayer perceptron (MLP), convolutional neural network (CNN),
    recursive neural network (RNN), recurrent neural network (RNN), long short-term
    memory (LSTM), sequence-to-sequence model, and shallow neural networks including
    word2vec for word embeddings. We showed how these networks function and how different
    types of them are used in natural language processing tasks. We demonstrated that
    convolutional neural networks are primarily utilized for text classification tasks
    while recurrent neural networks are commonly used for natural language generation
    or machine translation. In the next part of this series, we will study existing
    tools and libraries for the discussed neural network types.
  prefs: []
  type: TYPE_NORMAL
- en: '**Resources**'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.aclweb.org/anthology/D14-1181](http://www.aclweb.org/anthology/D14-1181)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/pdf/1502.01710.pdf](https://arxiv.org/pdf/1502.01710.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[http://www.aclweb.org/anthology/P15-1128](http://www.aclweb.org/anthology/P15-1128)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.aclweb.org/anthology/K15-1013](https://www.aclweb.org/anthology/K15-1013)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/CNN_ASLPTrans2-14.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/CNN_ASLPTrans2-14.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Recursive_neural_network](https://en.wikipedia.org/wiki/Recursive_neural_network)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[http://www.meanotek.ru/files/TarasovDS(2)2015-Dialogue.pdf](http://www.meanotek.ru/files/TarasovDS%282%292015-Dialogue.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745/9552](https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745/9552)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenTerm1201415/sak2.pdf](https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenTerm1201415/sak2.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/pdf/1510.06168.pdf](https://arxiv.org/pdf/1510.06168.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/pdf/1409.3215.pdf](https://arxiv.org/pdf/1409.3215.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://nlp.stanford.edu/courses/cs224n/2011/reports/ehhuang.pdf](https://nlp.stanford.edu/courses/cs224n/2011/reports/ehhuang.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**[Data Monsters](https://datamonsters.co/)** helps corporations and funded
    startups research, design, and develop real-time intelligent software to improve
    their business with data technologies.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/@datamonsters/artificial-neural-networks-for-natural-language-processing-part-1-64ca9ebfa3b2).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Machine Learning Translation and the Google Translate Algorithm](/2017/09/machine-learning-translation-google-translate-algorithm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Free Resources for Getting Started with Deep Learning for Natural Language
    Processing](/2017/07/5-free-resources-getting-started-deep-learning-nlp.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How I started with learning AI in the last 2 months](/2017/10/how-started-learning-ai.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
