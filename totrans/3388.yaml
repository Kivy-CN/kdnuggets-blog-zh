- en: A Quick Introduction to Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络快速入门
- en: 原文：[https://www.kdnuggets.com/2016/11/quick-introduction-neural-networks.html/2](https://www.kdnuggets.com/2016/11/quick-introduction-neural-networks.html/2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/11/quick-introduction-neural-networks.html/2](https://www.kdnuggets.com/2016/11/quick-introduction-neural-networks.html/2)
- en: 'A feedforward neural network can consist of three types of nodes:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络可以由三种类型的节点组成：
- en: '**Input Nodes -** The Input nodes provide information from the outside world
    to the network and are together referred to as the "Input Layer". No computation
    is performed in any of the Input nodes - they just pass on the information to
    the hidden nodes.'
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输入节点 -** 输入节点将来自外部世界的信息传递给网络，被统称为“输入层”。输入节点中不进行任何计算——它们只是将信息传递给隐藏节点。'
- en: '**Hidden Nodes -** The Hidden nodes have no direct connection with the outside
    world (hence the name "hidden"). They perform computations and transfer information
    from the input nodes to the output nodes. A collection of hidden nodes forms a
    "Hidden Layer". While a feedforward network will only have a single input layer
    and a single output layer, it can have zero or multiple Hidden Layers.'
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**隐藏节点 -** 隐藏节点与外部世界没有直接连接（因此称为“隐藏”）。它们执行计算，并将信息从输入节点传递到输出节点。隐藏节点的集合形成了一个“隐藏层”。虽然前馈网络只有一个输入层和一个输出层，但它可以有零个或多个隐藏层。'
- en: '**Output Nodes -** The Output nodes are collectively referred to as the "Output
    Layer" and are responsible for computations and transferring information from
    the network to the outside world.'
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输出节点 -** 输出节点统称为“输出层”，负责计算并将信息从网络传递到外部世界。'
- en: In a feedforward network, the information moves in only one direction - forward
    - from the input nodes, through the hidden nodes (if any) and to the output nodes.
    There are no cycles or loops in the network [3] (this property of feed forward
    networks is different from Recurrent Neural Networks in which the connections
    between the nodes form a cycle).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在前馈网络中，信息仅朝一个方向移动——前进——从输入节点，通过隐藏节点（如有）到达输出节点。网络中没有循环或回路[3]（这一前馈网络的特性不同于递归神经网络，其中节点之间的连接形成循环）。
- en: '* * *'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业之路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的IT需求'
- en: '* * *'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Two examples of feedforward networks are given below:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 下方给出了两个前馈网络的例子：
- en: '**Single Layer Perceptron** - This is the simplest feedforward neural network
    [4] and does not contain any hidden layer. You can learn more about Single Layer
    Perceptrons in [4], [5], [6], [7].'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**单层感知器** - 这是最简单的前馈神经网络[4]，不包含任何隐藏层。您可以在[4]，[5]，[6]，[7]中了解更多关于单层感知器的内容。'
- en: '**Multi Layer Perceptron** - A Multi Layer Perceptron has one or more hidden
    layers. We will only discuss Multi Layer Perceptrons below since they are more
    useful than Single Layer Perceptons for practical applications today.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**多层感知器** - 多层感知器具有一个或多个隐藏层。我们将仅讨论多层感知器，因为它们在实际应用中比单层感知器更有用。'
- en: Multi Layer Perceptron
  id: totrans-16
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多层感知器
- en: A Multi Layer Perceptron (MLP) contains one or more hidden layers (apart from
    one input and one output layer). While a single layer perceptron can only learn
    linear functions, a multi layer perceptron can also learn non - linear functions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器（MLP）包含一个或多个隐藏层（除了一个输入层和一个输出层）。虽然单层感知器只能学习线性函数，但多层感知器也可以学习非线性函数。
- en: Figure 4 shows a multi layer perceptron with a single hidden layer. Note that
    all connections have weights associated with them, but only three weights (w0,
    w1, w2) are shown in the figure.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图4显示了一个具有单一隐藏层的多层感知器。注意，所有连接都有权重，但图中仅显示了三个权重（w0, w1, w2）。
- en: '**Input Layer:** The Input layer has three nodes. The Bias node has a value
    of 1\. The other two nodes take X1 and X2 as external inputs (which are numerical
    values depending upon the input dataset). As discussed above, no computation is
    performed in the Input layer, so the outputs from nodes in the Input layer are
    1, X1 and X2 respectively, which are fed into the Hidden Layer.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入层：** 输入层有三个节点。偏置节点的值为1。其他两个节点接受X1和X2作为外部输入（这些值取决于输入数据集的数值）。如上所述，输入层不进行计算，因此输入层的节点输出为1、X1和X2，它们被送入隐藏层。'
- en: '**Hidden Layer:** The Hidden layer also has three nodes with the Bias node
    having an output of 1\. The output of the other two nodes in the Hidden layer
    depends on the outputs from the Input layer (1, X1, X2) as well as the weights
    associated with the connections (edges). Figure 4 shows the output calculation
    for one of the hidden nodes (highlighted). Similarly, the output from other hidden
    node can be calculated. Remember that ***f*** refers to the activation function.
    These outputs are then fed to the nodes in the Output layer.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐藏层：** 隐藏层也有三个节点，偏置节点的输出为1。隐藏层中其他两个节点的输出取决于输入层的输出（1、X1、X2）以及与连接（边）相关的权重。图4显示了一个隐藏节点（高亮显示）的输出计算。同样，其他隐藏节点的输出也可以计算。请记住，***f***
    指的是激活函数。这些输出随后被送入输出层的节点。'
- en: '![ds.png](../Images/483b085219ab03df0c51baad5b5a06a9.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![ds.png](../Images/483b085219ab03df0c51baad5b5a06a9.png)'
- en: 'Figure 4: a multi layer perceptron having one hidden layer'
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4：具有一个隐藏层的多层感知器
- en: '**Output Layer:** The Output layer has two nodes which take inputs from the
    Hidden layer and perform similar computations as shown for the highlighted hidden
    node. The values calculated (Y1 and Y2) as a result of these computations act
    as outputs of the Multi Layer Perceptron.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出层：** 输出层有两个节点，这些节点从隐藏层接受输入，并执行与高亮隐藏节点所示类似的计算。这些计算得到的值（Y1和Y2）作为多层感知器的输出。'
- en: Given a set of features **X = (x1, x2, ...)** and a target **y**, a Multi Layer
    Perceptron can learn the relationship between the features and the target, for
    either classification or regression.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组特征**X = (x1, x2, ...)**和一个目标**y**，多层感知器可以学习特征与目标之间的关系，用于分类或回归。
- en: 'Lets take an example to understand Multi Layer Perceptrons better. Suppose
    we have the following student-marks dataset:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子更好地理解多层感知器。假设我们有以下学生-成绩数据集：
- en: '![train.png](../Images/b98a6c22ecf31c8b881eac0ea5c2df13.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![train.png](../Images/b98a6c22ecf31c8b881eac0ea5c2df13.png)'
- en: The two input columns show the number of hours the student has studied and the
    mid term marks obtained by the student. The Final Result column can have two values
    1 or 0 indicating whether the student passed in the final term. For example, we
    can see that if the student studied 35 hours and had obtained 67 marks in the
    mid term, he / she ended up passing the final term.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 两个输入列显示了学生学习的小时数和学生在期中考试中获得的分数。期末结果列可以有两个值1或0，表示学生是否通过了期末考试。例如，我们可以看到，如果学生学习了35小时并在期中考试中获得了67分，他/她最终通过了期末考试。
- en: Now, suppose, we want to predict whether a student studying 25 hours and having
    70 marks in the mid term will pass the final term.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们想预测一个学习了25小时并在期中考试中得了70分的学生是否能通过期末考试。
- en: '![test.png](../Images/adb31843a47ea31455f9bfda07e7e2f3.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![test.png](../Images/adb31843a47ea31455f9bfda07e7e2f3.png)'
- en: This is a binary classification problem where a multi layer perceptron can learn
    from the given examples (training data) and make an informed prediction given
    a new data point. We will see below how a multi layer perceptron learns such relationships.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个二分类问题，其中多层感知器可以从给定的示例（训练数据）中学习，并在给定新的数据点时做出有根据的预测。我们将看到多层感知器如何学习这种关系。
- en: 'Training our MLP: The Back-Propagation Algorithm'
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 训练我们的MLP：反向传播算法
- en: The process by which a Multi Layer Perceptron learns is called the Backpropagation
    algorithm. I would recommend reading [this Quora answer by Hemanth Kumar](https://www.quora.com/How-do-you-explain-back-propagation-algorithm-to-a-beginner-in-neural-network/answer/Hemanth-Kumar-Mantri)
    (quoted below) which explains Backpropagation clearly.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器学习的过程称为反向传播算法。我建议阅读[Hemanth Kumar在Quora上的回答](https://www.quora.com/How-do-you-explain-back-propagation-algorithm-to-a-beginner-in-neural-network/answer/Hemanth-Kumar-Mantri)（下文引用），它清楚地解释了反向传播。
- en: '**Backward Propagation of Errors,** often abbreviated as BackProp is one of
    the several ways in which an artificial neural network (ANN) can be trained. It
    is a supervised training scheme, which means, it learns from labeled training
    data (there is a supervisor, to guide its learning).'
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**反向误差传播**，通常缩写为BackProp，是训练人工神经网络（ANN）的几种方法之一。它是一种监督训练方案，这意味着它从标记的训练数据中学习（有监督者来指导其学习）。'
- en: ''
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To put in simple terms, BackProp is like "**learning from mistakes**". The supervisor
    ***corrects*** the ANN whenever it makes mistakes.
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 简而言之，BackProp就像是“**从错误中学习**”。监督者在ANN犯错时会进行***纠正***。
- en: ''
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An ANN consists of nodes in different layers; input layer, intermediate hidden
    layer(s) and the output layer. The connections between nodes of adjacent layers
    have "weights" associated with them. The goal of learning is to assign correct
    weights for these edges. Given an input vector, these weights determine what the
    output vector is.
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个ANN由不同层中的节点组成：输入层、隐藏层和输出层。相邻层节点之间的连接有“权重”与之相关。学习的目标是为这些边分配正确的权重。给定一个输入向量，这些权重决定了输出向量。
- en: ''
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In supervised learning, the training set is labeled. This means, for some given
    inputs, we know the desired/expected output (label).
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在监督学习中，训练集是标记的。这意味着，对于一些给定的输入，我们知道期望的输出（标签）。
- en: ''
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**BackProp Algorithm:**'
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**反向传播算法：**'
- en: Initially all the edge weights are randomly assigned. For every input in the
    training dataset, the ANN is activated and its output is observed. This output
    is compared with the desired output that we already know, and the error is "propagated"
    back to the previous layer. This error is noted and the weights are "adjusted"
    accordingly. This process is repeated until the output error is below a predetermined
    threshold.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 最初，所有边权重都是随机分配的。对于训练数据集中的每个输入，激活ANN并观察其输出。这个输出与我们已经知道的期望输出进行比较，并且误差被“传播”回上一层。记录这个误差并相应地“调整”权重。这个过程会重复进行，直到输出误差低于预定的阈值。
- en: ''
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Once the above algorithm terminates, we have a "learned" ANN which, we consider
    is ready to work with "new" inputs. This ANN is said to have learned from several
    examples (labeled data) and from its mistakes (error propagation).
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一旦上述算法终止，我们就拥有一个“学习过”的ANN，我们认为它准备好处理“新”输入了。这个ANN被认为从多个示例（标记数据）和它的错误（误差传播）中学习了。
- en: Now that we have an idea of how Backpropagation works, lets come back to our
    student-marks dataset shown above.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对反向传播的工作原理有了了解，让我们回到上面的学生分数数据集。
- en: More On This Topic
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Quick Data Science Tips and Tricks to Learn SAS](https://www.kdnuggets.com/2022/05/sas-quick-data-science-tips-tricks-learn.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[快速数据科学提示和技巧以学习SAS](https://www.kdnuggets.com/2022/05/sas-quick-data-science-tips-tricks-learn.html)'
- en: '[7 Pandas Plotting Functions for Quick Data Visualization](https://www.kdnuggets.com/7-pandas-plotting-functions-for-quick-data-visualization)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7个用于快速数据可视化的Pandas绘图函数](https://www.kdnuggets.com/7-pandas-plotting-functions-for-quick-data-visualization)'
- en: '[A Quick Overview of Voronoi Diagrams](https://www.kdnuggets.com/2022/11/quick-overview-voronoi-diagrams.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Voronoi图的快速概述](https://www.kdnuggets.com/2022/11/quick-overview-voronoi-diagrams.html)'
- en: '[A Quick Guide to Find the Right Minds for Annotation](https://www.kdnuggets.com/2022/04/quick-guide-find-right-minds-annotation.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[找到适合注释的合适人才的快速指南](https://www.kdnuggets.com/2022/04/quick-guide-find-right-minds-annotation.html)'
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用PyTorch解释神经网络](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
- en: '[Deep Neural Networks Don''t Lead Us Towards AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度神经网络不会引导我们走向AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
