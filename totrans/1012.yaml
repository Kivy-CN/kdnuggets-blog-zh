- en: 'Essential Math for Data Science: Eigenvectors and Application to PCA'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学的基础数学：特征向量及其在 PCA 中的应用
- en: 原文：[https://www.kdnuggets.com/2022/06/essential-math-data-science-eigenvectors-application-pca.html](https://www.kdnuggets.com/2022/06/essential-math-data-science-eigenvectors-application-pca.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/06/essential-math-data-science-eigenvectors-application-pca.html](https://www.kdnuggets.com/2022/06/essential-math-data-science-eigenvectors-application-pca.html)
- en: '*Matrix decomposition*, also called *matrix factorization* is the process of
    splitting a matrix into multiple pieces. In the context of data science, you can
    for instance use it to select parts of the data, aimed at reducing dimensionality
    without losing much information (as for instance in Principal Component Analysis,
    as you’ll later in this post). Some operations are also more easily computed on
    the matrices resulting from the decomposition.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*矩阵分解*，也叫做*矩阵因式分解*，是将一个矩阵拆分成多个部分的过程。在数据科学的背景下，你可以例如用它来选择数据的部分，旨在减少维度而不丢失太多信息（例如在主成分分析中，你将在这篇文章的后面看到）。一些操作也可以更容易地在分解后的矩阵上进行计算。'
- en: In this article, you’ll learn about the eigendecomposition of a matrix. One
    way to understand it is to consider it as a special change of basis (more details
    about change of basis in [my last post](https://hadrienj.github.io/posts/Essential-Math-for-Data-Science-Change-of-Basis/)).
    You’ll first learn about eigenvectors and eigenvalues and then you’ll see how
    it can be applied to Principal Component Analysis (PCA). The main idea is to consider
    the eigendecomposition of a matrix ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png)
    as a change of basis where the new basis vectors are the eigenvectors.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，你将学习矩阵的特征分解。理解它的一种方法是将其视为一种特殊的基变换（有关基变换的更多细节，请参阅[我上一篇文章](https://hadrienj.github.io/posts/Essential-Math-for-Data-Science-Change-of-Basis/)）。你将首先了解特征向量和特征值，然后看到它如何应用于主成分分析（PCA）。主要思想是将矩阵 ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) 的特征分解视为基变换，其中新的基向量是特征向量。
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的 IT'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Eigenvectors and Eigenvalues
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征向量和特征值
- en: As you can see in Chapter 7 of [Essential Math for Data Science](https://www.essentialmathfordatascience.com/?utm_source=hadrienj&utm_medium=blog&utm_campaign=hadrienj_2021-02-01-Essential-Math-for-Data-Science-Eigendecomposition.md) you
    can consider matrices as linear transformations. This means that if you take any
    vector ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) and apply the
    matrix ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) to it, you
    obtain a transformed vector ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在[数据科学的基础数学](https://www.essentialmathfordatascience.com/?utm_source=hadrienj&utm_medium=blog&utm_campaign=hadrienj_2021-02-01-Essential-Math-for-Data-Science-Eigendecomposition.md)第七章中看到的，你可以将矩阵视为线性变换。这意味着如果你取任意一个向量 ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) 并对其应用矩阵 ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png)，你会得到一个变换后的向量 ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png)。
- en: 'Take the example of:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以以下例子为例：
- en: '![Equation](../Images/dee23a1453606cccba63318b471db7cd.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![Equation](../Images/dee23a1453606cccba63318b471db7cd.png)'
- en: and
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![Equation](../Images/ffd25749845bb0ffe9bb83c6cf3555ea.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![Equation](../Images/ffd25749845bb0ffe9bb83c6cf3555ea.png)'
- en: 'If you apply ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) to
    the vector ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) (with the
    matrix-vector product), you get a new vector:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将 ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) 应用于向量 ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) （使用矩阵-向量乘积），你会得到一个新向量：
- en: '![Equation](../Images/ee1a6407bd3de954cc6b370389f22b49.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![Equation](../Images/ee1a6407bd3de954cc6b370389f22b49.png)'
- en: '![Equation](../Images/6043576e294a8d22f397ca24b1e957fd.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![Equation](../Images/6043576e294a8d22f397ca24b1e957fd.png)'
- en: '![Equation](../Images/20a8b29fa654acfac64968c9d78f3cdc.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![Equation](../Images/20a8b29fa654acfac64968c9d78f3cdc.png)'
- en: '![Equation](../Images/f61d37b006830115c275a9ef5faa4c42.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![Equation](../Images/f61d37b006830115c275a9ef5faa4c42.png)'
- en: 'Let’s draw the initial and transformed vectors:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制初始向量和变换后的向量：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Essential Math for Data Science: Eigenvectors and Application to PCA](../Images/d9ef15de870b38c85005ee8a32e0ea9e.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![数据科学基础数学：特征向量及其在PCA中的应用](../Images/d9ef15de870b38c85005ee8a32e0ea9e.png)'
- en: 'Figure 1: Transformation of the vector ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) by
    the matrix ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) into the
    vector ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：矩阵 ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) 对向量 ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) 的变换成向量 ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png)。
- en: Note that, as you can expect, the transformed vector ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png) doesn’t
    run in the same direction as the initial vector ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png).
    This change of direction characterizes most of the vectors you can transform by ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如你所料，变换后的向量 ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png) 并不与初始向量 ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) 在同一方向上。这种方向的变化是你可以用 ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) 变换的大多数向量的特征。
- en: 'However, take the following vector:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，考虑以下向量：
- en: '![Equation](../Images/f2e88aceae6e8e7f7b10526bb8bd06b6.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![Equation](../Images/f2e88aceae6e8e7f7b10526bb8bd06b6.png)'
- en: 'Let’s apply the matrix ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) to
    the vector ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png) to obtain
    a vector ![Equation](../Images/ea571e13bda6431149977a7da4e30a98.png):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将矩阵 ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) 应用于向量 ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png) 以获得一个向量 ![Equation](../Images/ea571e13bda6431149977a7da4e30a98.png)：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Essential Math for Data Science: Eigenvectors and Application to PCA](../Images/aaa03eb844fe9221ed7a97da30f230d6.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![数据科学基础数学：特征向量及其在PCA中的应用](../Images/aaa03eb844fe9221ed7a97da30f230d6.png)'
- en: 'Figure 2: Transformation of the special vector ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png) by
    the matrix ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：矩阵 ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) 对特殊向量 ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png) 的变换。
- en: 'You can see in Figure 2 that the vector ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png) has
    a special relationship with the matrix ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png):
    it is rescaled (with a negative value), but both the initial vector ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png) and
    the transformed vector ![Equation](../Images/ea571e13bda6431149977a7da4e30a98.png) are
    on the same line.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从图2中可以看出，向量 ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png) 与矩阵 ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) 具有特殊关系：它被缩放（带有负值），但初始向量 ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png) 和变换后的向量 ![Equation](../Images/ea571e13bda6431149977a7da4e30a98.png) 都在同一条直线上。
- en: The vector ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png) is an *eigenvector* of ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png).
    It is only scaled by a value, which is called an *eigenvalue* of the matrix ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png).
    An eigenvector of the matrix ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) is
    a vector that is contracted or elongated when transformed by the matrix. The eigenvalue
    is the scaling factor by which the vector is contracted or elongated.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 向量 ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png) 是 ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) 的*特征向量*。它仅被一个值缩放，这个值称为矩阵 ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) 的*特征值*。矩阵 ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) 的特征向量是在矩阵变换时被缩短或拉长的向量。特征值是缩短或拉长向量的缩放因子。
- en: 'Mathematically, the vector ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png) is
    an eigenvector of ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) if:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，向量 ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png) 是 ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) 的特征向量，如果：
- en: '![Equation](../Images/ec7ce5865031a7580f2590d4162f88b9.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![Equation](../Images/ec7ce5865031a7580f2590d4162f88b9.png)'
- en: with ![Equation](../Images/ef17aed938e5576b0a5e042246b4ac47.png) (pronounced
    “lambda”) being the eigenvalue corresponding to the eigenvector ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![Equation](../Images/ef17aed938e5576b0a5e042246b4ac47.png)（读作“lambda”）是与特征向量 ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png) 对应的特征值。
- en: '**Eigenvectors**'
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**特征向量**'
- en: ''
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Eigenvectors of a matrix are nonzero vectors that are only rescaled when the
    matrix is applied to them. If the scaling factor is positive, the directions of
    the initial and the transformed vectors are the same, if it is negative, their
    directions are reversed.
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 矩阵的特征向量是非零向量，当矩阵作用于它们时只是被重新缩放。如果缩放因子为正，初始向量和变换后的向量的方向相同；如果为负，则方向相反。
- en: '**Number of eigenvectors**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征向量的数量**'
- en: 'An ![Equation](../Images/9a60fefb21d8c288f3b817f86bef589d.png)-by-![Equation](../Images/9a60fefb21d8c288f3b817f86bef589d.png) matrix
    has, at most, ![Equation](../Images/9a60fefb21d8c288f3b817f86bef589d.png) linearly
    independent eigenvectors. However, each eigenvector multiplied by a nonzero scalar
    is also an eigenvector. If you have:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一个![方程](../Images/9a60fefb21d8c288f3b817f86bef589d.png)-by-![方程](../Images/9a60fefb21d8c288f3b817f86bef589d.png)矩阵最多具有![方程](../Images/9a60fefb21d8c288f3b817f86bef589d.png)个线性独立的特征向量。然而，每个特征向量乘以一个非零标量也是一个特征向量。如果你有：
- en: '![Equation](../Images/5184f65ed2fd4fc2c8ca10dfc0147dcd.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![方程](../Images/5184f65ed2fd4fc2c8ca10dfc0147dcd.png)'
- en: 'Then:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然后：
- en: '![Equation](../Images/450cc7ad9f7b7c3b13e31d23d61da926.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![方程](../Images/450cc7ad9f7b7c3b13e31d23d61da926.png)'
- en: with ![Equation](../Images/d571d7685ad3d9f4acf544844e771d21.png) any nonzero
    value.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 具有![方程](../Images/d571d7685ad3d9f4acf544844e771d21.png)任何非零值。
- en: This excludes the zero vector as eigenvector, since you would have
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这排除了零向量作为特征向量，因为你会有
- en: '![Equation](../Images/31c9d00058c36ce6f668e66d7d1179a5.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![方程](../Images/31c9d00058c36ce6f668e66d7d1179a5.png)'
- en: In this case, every scalar would be an eigenvalue and thus would be undefined.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，每个标量将是一个特征值，因此会变得未定义。
- en: 'Hands-On Project: Principal Component Analysis'
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践项目：主成分分析
- en: '*Principal Component Analysis*, or PCA, is an algorithm that you can use to
    reduce the dimensionality of a dataset. It is useful, for instance, to reduce
    computation time, compress data, or avoid what is called *the curse of dimensionality*.
    It is also useful for visualization purposes: high dimensional data is hard to
    visualize and it can be useful to decrease the number of dimensions to plot your
    data.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*主成分分析*（PCA）是一种算法，用于减少数据集的维度。例如，它在减少计算时间、压缩数据或避免所谓的*维数灾难*方面很有用。它也对可视化很有帮助：高维数据很难可视化，减少维数可以有助于绘制数据。'
- en: In this hands-on project, you’ll use various concepts that you can learn in
    the book [Essential Math for Data Science](https://www.essentialmathfordatascience.com/?utm_source=hadrienj&utm_medium=blog&utm_campaign=hadrienj_2021-02-01-Essential-Math-for-Data-Science-Eigendecomposition.md),
    as change of basis (Sections 7.5 and 9.2, some samples [here](https://hadrienj.github.io/posts/Essential-Math-for-Data-Science-Change-of-Basis/)),
    eigendecomposition (Chapter 9) or covariance matrices (Section 2.1.3) to understand
    how PCA is working.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实践项目中，你将使用你可以在[数据科学基础数学](https://www.essentialmathfordatascience.com/?utm_source=hadrienj&utm_medium=blog&utm_campaign=hadrienj_2021-02-01-Essential-Math-for-Data-Science-Eigendecomposition.md)一书中学习的各种概念，如基变换（第7.5节和9.2节，一些示例[在这里](https://hadrienj.github.io/posts/Essential-Math-for-Data-Science-Change-of-Basis/)）、特征分解（第9章）或协方差矩阵（第2.1.3节），以理解PCA是如何工作的。
- en: In the first part, you’ll learn about the relationship between projections,
    explained variance and error minimization, first with a bit of theory, and then
    by coding a PCA on the beer dataset (consumption of beer as a function of temperature).
    Note that you’ll also find another example in [Essential Math for Data Science](https://www.essentialmathfordatascience.com/?utm_source=hadrienj&utm_medium=blog&utm_campaign=hadrienj_2021-02-01-Essential-Math-for-Data-Science-Eigendecomposition.md) where
    you’ll use Sklearn to use PCA on audio data to visualize audio samples according
    to their category, and then to compress these audio samples.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分，你将学习投影、解释方差和误差最小化之间的关系，首先通过一点理论，然后通过对啤酒数据集（啤酒消费与温度的关系）进行PCA编码。请注意，你还会在[数据科学基础数学](https://www.essentialmathfordatascience.com/?utm_source=hadrienj&utm_medium=blog&utm_campaign=hadrienj_2021-02-01-Essential-Math-for-Data-Science-Eigendecomposition.md)中找到另一个示例，在那里你将使用Sklearn对音频数据进行PCA，以根据类别可视化音频样本，然后压缩这些音频样本。
- en: Under the Hood
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入分析
- en: Theoretical context
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理论背景
- en: The goal of PCA is to project data onto a lower dimensional space while keeping
    as much of the information contained in the data as possible. The problem can
    be seen as a *perpendicular least squares* problem also called *orthogonal regression*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的目标是将数据投影到较低维度的空间，同时尽可能保留数据中包含的所有信息。这个问题可以被视为一个*垂直最小二乘*问题，也称为*正交回归*。
- en: You’ll see here that the error of the orthogonal projections is minimized when
    the projection line corresponds to the direction where the variance of the data
    is maximal.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里你会看到，当投影线与数据方差最大的方向相符时，正交投影的误差被最小化。
- en: Variance and Projections
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 方差和投影
- en: It is first important to understand that, when the features of your dataset
    are not completely uncorrelated, some directions are associated with a larger
    variance than others.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先需要理解的是，当你的数据集的特征不完全不相关时，一些方向的方差大于其他方向。
- en: '![Essential Math for Data Science: Eigenvectors and Application to PCA](../Images/8d12f3afe57864e2400e68d5d2be0c74.png)>'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![数据科学中的基本数学：特征值和主成分分析的应用](../Images/8d12f3afe57864e2400e68d5d2be0c74.png)>'
- en: 'Figure 3: The variance of the data in the direction of the vector ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) (red)
    is larger than in the direction of the vector ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png) (green).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：在向量![方程](../Images/a1bb06b27d7ad44981ed7b8aae646595.png)（红色）方向上的数据方差大于在向量![方程](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png)（绿色）方向上的数据方差。
- en: 'Projecting data to a lower-dimensional space means that you might lose some
    information. In Figure 3, if you project two-dimensional data onto a line, the
    variance of the projected data tells you how much information you lose. For instance,
    if the variance of the projected data is near zero, it means that the data points
    will be projected to very close positions: you lose a lot of information.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据投影到低维空间意味着你可能会丢失一些信息。在图3中，如果将二维数据投影到一条线上的话，投影数据的方差告诉你你丢失了多少信息。例如，如果投影数据的方差接近零，这意味着数据点会被投影到非常接近的位置：你丢失了大量信息。
- en: For this reason, the goal of the PCA is to change the basis of the data matrix
    such that the direction with the maximum variance (![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) in
    Figure 3) becomes the first *principal component*. The second component is the
    direction with the maximum variance which is orthogonal to the first one, and
    so on.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，PCA的目标是改变数据矩阵的基，使得具有最大方差的方向（图3中的![方程](../Images/a1bb06b27d7ad44981ed7b8aae646595.png)）成为第一个*主成分*。第二个成分是与第一个成分正交的具有最大方差的方向，以此类推。
- en: When you have found the components of the PCA, you change the basis of your
    data such that the components are the new basis vectors. This transformed dataset
    has new features, which are the components and which are linear combinations of
    the initial features. Reducing the dimensionality is done by selecting some of
    the components only.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当你找到PCA的组件时，你会改变数据的基，使得这些组件成为新的基向量。这个变换后的数据集具有新的特征，这些特征是组件，它们是初始特征的线性组合。通过仅选择一些组件来减少维度。
- en: '![Essential Math for Data Science: Eigenvectors and Application to PCA](../Images/bf56f430ef597ae9d416efcb74b7a827.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![数据科学中的基本数学：特征值和主成分分析的应用](../Images/bf56f430ef597ae9d416efcb74b7a827.png)'
- en: 'Figure 4: Change of basis such that the maximum variance is in the ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png)-axis.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：基变换，使得最大方差位于![方程](../Images/18d793480e7f7fb6e486026b18729633.png)-轴。
- en: 'As an illustration, Figure 4 shows the data after a change of basis: the maximum
    variance is now associated with the ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png)-axis.
    You can for instance keep only this first dimension.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 作为说明，图4显示了基变换后的数据：最大方差现在与![方程](../Images/18d793480e7f7fb6e486026b18729633.png)-轴相关。例如，你可以只保留这一维度。
- en: In other words, expressing the PCA in terms of change of basis, its goal is
    to find a new basis (which is a linear combination of the initial basis) in which
    the variance of the data is maximized along the first dimensions.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，将主成分分析（PCA）表达为基变换，其目标是找到一个新的基（该基是初始基的线性组合），使得数据的方差在第一个维度上达到最大。
- en: Minimizing the Error
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最小化误差
- en: Finding the directions that maximize the variance is similar as minimizing the
    error between the data and its projection.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 找到最大化方差的方向类似于最小化数据与其投影之间的误差。
- en: '![Essential Math for Data Science: Eigenvectors and Application to PCA](../Images/0a186ddc1f3ae7e09e131edc60cc8444.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![数据科学中的基本数学：特征值和主成分分析的应用](../Images/0a186ddc1f3ae7e09e131edc60cc8444.png)'
- en: 'Figure 5: The direction that maximizes the variance is also the one associated
    with the smallest error (represented in gray).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：最大化方差的方向也是与最小误差（用灰色表示）相关的方向。
- en: You can see in Figure 5 that lower errors are shown in the left figure. Since
    projections are orthogonal, the variance associated to the direction of the line
    on which you project doesn’t impact the error.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在图5中你可以看到，左图中显示了较低的误差。由于投影是正交的，投影方向上的方差不会影响误差。
- en: Finding the Best Directions
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 寻找最佳方向
- en: 'After changing the basis of the dataset, you should have a covariance between
    features close to zero (as for instance in Figure 4). In other terms, you want
    that the transformed dataset has a diagonal covariance matrix: the covariance
    between each pair of principal components is equal to zero.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在改变数据集的基之后，你应该有一个特征之间的协方差接近零（例如图4）。换句话说，你希望转换后的数据集具有对角协方差矩阵：每对主成分之间的协方差为零。
- en: You can see in Chapter 9 of [Essential Math for Data Science](https://www.essentialmathfordatascience.com/?utm_source=hadrienj&utm_medium=blog&utm_campaign=hadrienj_2021-02-01-Essential-Math-for-Data-Science-Eigendecomposition.md),
    that you can use eigendecomposition to diagonalize a matrix (make the matrix diagonal).
    Thus, you can calculate the eigenvectors of the covariance matrix of the dataset.
    They will give you the directions of the new basis in which the covariance matrix
    is diagonal.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [数据科学的基础数学](https://www.essentialmathfordatascience.com/?utm_source=hadrienj&utm_medium=blog&utm_campaign=hadrienj_2021-02-01-Essential-Math-for-Data-Science-Eigendecomposition.md) 第9章中看到，你可以使用特征分解将矩阵对角化（使矩阵变为对角矩阵）。因此，你可以计算数据集的协方差矩阵的特征向量。它们将给你协方差矩阵对角化的新基方向。
- en: To summarize, the principal components are calculated as the eigenvectors of
    the covariance matrix of the dataset. In addition, the eigenvalues give you the
    explained variance of the corresponding eigenvector. Thus, by sorting the eigenvectors
    in the decreasing order according to their eigenvalues, you can sort the principal
    components by importance order, and eventually remove the ones associated with
    a small variance.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，主成分是计算为数据集协方差矩阵的特征向量。此外，特征值提供了相应特征向量的解释方差。因此，通过根据特征值的降序对特征向量进行排序，你可以按重要性顺序对主成分进行排序，并最终去除与小方差相关的主成分。
- en: Calculating the PCA
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算PCA
- en: '**Dataset**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据集**'
- en: Let’s illustrate how PCA is working with the beer dataset showing the beer consumption
    and the temperature in São Paulo, Brazil for the year 2015.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用2015年在巴西圣保罗的啤酒消费和温度数据集来说明PCA是如何工作的。
- en: 'Let’s load the data and plot the consumption as a function of the temperature:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载数据，并绘制温度作为消费的函数：
- en: '[PRE3]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Essential Math for Data Science: Eigenvectors and Application to PCA](../Images/768a9d226e65a2449abd32e1520240cc.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![数据科学的基础数学：特征向量与PCA应用](../Images/768a9d226e65a2449abd32e1520240cc.png)'
- en: 'Figure 6: Consumption of beer as a function of temperature.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：温度作为啤酒消费的函数。
- en: 'Now, let’s create the data matrix ![Equation](../Images/fad5207092e118bbda7de8e70211a6f4.png) with
    the two variables: temperatures and consumption.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建数据矩阵 ![Equation](../Images/fad5207092e118bbda7de8e70211a6f4.png) ，包含两个变量：温度和消费。
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The matrix ![Equation](../Images/fad5207092e118bbda7de8e70211a6f4.png) has 365
    rows and two columns (the two variables).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 ![Equation](../Images/fad5207092e118bbda7de8e70211a6f4.png) 有365行和两列（这两个变量）。
- en: '**Eigendecomposition of the Covariance Matrix**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**协方差矩阵的特征分解**'
- en: 'As you saw, the first step is to compute the covariance matrix of the dataset:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，第一步是计算数据集的协方差矩阵：
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Remember that you can read it as follows: the diagonal values are respectively
    the variances of the first and the second variable. The covariance between the
    two variables is around 12.2.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，你可以这样理解：对角线值分别是第一个和第二个变量的方差。这两个变量之间的协方差大约为12.2。
- en: 'Now, you will calculate the eigenvectors and eigenvalues of this covariance
    matrix:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你将计算该协方差矩阵的特征向量和特征值：
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can store the eigenvectors as two vectors ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) and ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将特征向量存储为两个向量 ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) 和 ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png)。
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Let’s plot the eigenvectors with the data (note that you should use centered
    data because it is the data used to calculate the covariance matrix).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用数据绘制特征向量（注意你应该使用中心化的数据，因为它是计算协方差矩阵时使用的数据）。
- en: 'You can scale the eigenvectors by their corresponding eigenvalues, which is
    the explained variance. For visualization purpose, let’s use a vector length of
    three standard deviations (equal to three times the square root of the explained
    variance):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以按对应的特征值缩放特征向量，这些特征值是解释的方差。为了可视化目的，我们使用三个标准差的向量长度（等于三倍的解释方差的平方根）：
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![Essential Math for Data Science: Eigenvectors and Application to PCA](../Images/7e866c41618f3917728a398331a0f01a.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![数据科学的基本数学：特征向量及其在 PCA 中的应用](../Images/7e866c41618f3917728a398331a0f01a.png)'
- en: 'Figure 7: The eigenvectors ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) (in
    gray) and ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png) (in red)
    scaled according to the explained variance.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：特征向量 ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) （灰色）和 ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png) （红色），按解释的方差进行缩放。
- en: You can see in Figure 7 that the eigenvectors of the covariance matrix give
    you the important directions of the data. The vector ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png) in
    red is associated with the largest eigenvalue and thus corresponds to the direction
    with the largest variance. The vector ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) in
    gray is orthogonal to ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png) and
    is the second principal component.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在图7中，你可以看到协方差矩阵的特征向量为你提供了数据的重要方向。红色的向量 ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png) 与最大的特征值相关联，因此对应于具有最大方差的方向。灰色的向量 ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) 与 ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png) 正交，是第二主成分。
- en: 'Then, you just need to change the basis of the data using the eigenvectors
    as the new basis vectors. But first, you can sort the eigenvectors with respect
    to the eigenvalues in decreasing order:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你只需使用特征向量作为新的基准向量来改变数据的基准。但首先，你可以根据特征值按降序排序特征向量：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now that your eigenvectors are sorted, let’s change the basis of the data:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你的特征向量已经排序好了，我们来改变数据的基准：
- en: '[PRE14]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You can plot the transformed data to check that the principal components are
    now uncorrelated:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以绘制变换后的数据，以检查主成分现在是否不相关：
- en: '[PRE15]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![Essential Math for Data Science: Eigenvectors and Application to PCA](../Images/f9725bfa0d344f604d5cc1d8dc8003fb.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![数据科学的基本数学：特征向量及其在 PCA 中的应用](../Images/f9725bfa0d344f604d5cc1d8dc8003fb.png)'
- en: 'Figure 8: The dataset in the new basis.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：新基准下的数据集。
- en: Figure 8 shows the data samples in the new basis. You can see that the first
    dimension (the ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png)-axis)
    corresponds to the direction with the largest variance.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图8显示了新基准下的数据样本。你可以看到第一个维度（ ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png)-轴）对应于具有最大方差的方向。
- en: You can keep only the first component of the data in this new basis without
    losing too much information.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这个新基准下只保留数据的第一个主成分，而不会丢失太多信息。
- en: '**Covariance matrix or Singular Value Decomposition?**'
  id: totrans-116
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**协方差矩阵还是奇异值分解？**'
- en: ''
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: One caveat of using the covariance matrix to calculate the PCA is that it can
    be hard to compute when there are many features (as with audio data, like in the
    second part of this hands-on). For this reason, it is usually preferred to use
    the Singular Value Decomposition (SVD) to calculate the PCA.
  id: totrans-118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用协方差矩阵计算 PCA 的一个警告是，当特征较多时（如在本动手操作的第二部分中的音频数据），计算可能会很困难。因此，通常更倾向于使用奇异值分解（SVD）来计算
    PCA。
- en: '**[Hadrien Jean](https://hadrienj.github.io/)** is a machine learning scientist.
    He owns a Ph.D in cognitive science from the Ecole Normale Superieure, Paris,
    where he did research on auditory perception using behavioral and electrophysiological
    data. He previously worked in industry where he built deep learning pipelines
    for speech processing. At the corner of data science and environment, he works
    on projects about biodiversity assessement using deep learning applied to audio
    recordings. He also periodically creates content and teaches at Le Wagon (data
    science Bootcamp), and writes articles in his blog ([hadrienj.github.io](http://hadrienj.github.io)).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Hadrien Jean](https://hadrienj.github.io/)** 是一位机器学习科学家。他拥有巴黎高等师范学院的认知科学博士学位，在那里他使用行为和电生理数据进行听觉感知研究。他曾在工业界工作，建立了用于语音处理的深度学习管道。在数据科学与环境交汇处，他从事关于生物多样性评估的项目，利用应用于音频录音的深度学习。他还定期在
    Le Wagon（数据科学训练营）创建内容并教授课程，并在他的博客上撰写文章（[hadrienj.github.io](http://hadrienj.github.io)）。'
- en: '[Original](https://hadrienj.github.io/posts/Essential-Math-for-Data-Science-Eigendecomposition/).
    Reposted with permission.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://hadrienj.github.io/posts/Essential-Math-for-Data-Science-Eigendecomposition/)。经许可转载。'
- en: More On This Topic
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[How To Overcome The Fear of Math and Learn Math For Data Science](https://www.kdnuggets.com/2021/03/overcome-fear-learn-math-data-science.html)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何克服对数学的恐惧并学习数据科学中的数学](https://www.kdnuggets.com/2021/03/overcome-fear-learn-math-data-science.html)'
- en: '[Principal Component Analysis (PCA) with Scikit-Learn](https://www.kdnuggets.com/2023/05/principal-component-analysis-pca-scikitlearn.html)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Scikit-Learn 进行主成分分析 (PCA)](https://www.kdnuggets.com/2023/05/principal-component-analysis-pca-scikitlearn.html)'
- en: '[Essential Math for Data Science: Visual Introduction to Singular…](https://www.kdnuggets.com/2022/06/essential-math-data-science-visual-introduction-singular-value-decomposition.html)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学的基础数学：奇异值分解的可视化介绍…](https://www.kdnuggets.com/2022/06/essential-math-data-science-visual-introduction-singular-value-decomposition.html)'
- en: '[Building a Geospatial Application in Python with Google Earth…](https://www.kdnuggets.com/2022/03/building-geospatial-application-python-google-earth-engine-greppo.html)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Google Earth 构建 Python 地理空间应用程序…](https://www.kdnuggets.com/2022/03/building-geospatial-application-python-google-earth-engine-greppo.html)'
- en: '[Creating a Web Application to Extract Topics from Audio with Python](https://www.kdnuggets.com/2023/01/creating-web-application-extract-topics-audio-python.html)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[创建一个 Web 应用程序以提取音频中的主题](https://www.kdnuggets.com/2023/01/creating-web-application-extract-topics-audio-python.html)'
- en: '[8 Ways to Improve Your Search Application this Week](https://www.kdnuggets.com/2022/09/corise-8-ways-improve-search-application-week.html)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[本周改进搜索应用程序的 8 种方法](https://www.kdnuggets.com/2022/09/corise-8-ways-improve-search-application-week.html)'
