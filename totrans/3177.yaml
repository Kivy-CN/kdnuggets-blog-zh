- en: Learning Curves for Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的学习曲线
- en: 原文：[https://www.kdnuggets.com/2018/01/learning-curves-machine-learning.html/2](https://www.kdnuggets.com/2018/01/learning-curves-machine-learning.html/2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/01/learning-curves-machine-learning.html/2](https://www.kdnuggets.com/2018/01/learning-curves-machine-learning.html/2)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](/2018/01/learning-curves-machine-learning.html?page=2#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](/2018/01/learning-curves-machine-learning.html?page=2#comments)'
- en: The learning_curve() function from scikit-learn
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`scikit-learn`中的`learning_curve()`函数'
- en: We'll use the `learning_curve()` [function](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html) from
    the scikit-learn library to generate a learning curve for a regression model.
    There's no need on our part to put aside a validation set because `learning_curve()` will
    take care of that.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`learning_curve()` [函数](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html)
    从`scikit-learn`库生成回归模型的学习曲线。我们不需要单独划分验证集，因为`learning_curve()`会处理这一点。
- en: 'In the code cell below, we:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码单元格中，我们：
- en: Do the required imports from `sklearn`.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从`sklearn`中进行必要的导入。
- en: Declare the features and the target.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 声明特征和目标。
- en: 'Use `learning_curve()` to generate the data needed to plot a learning curve.
    The function returns a tuple containing three elements: the training set sizes,
    and the error scores on both the validation sets and the training sets. Inside
    the function, we use the following parameters:'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`learning_curve()`生成绘制学习曲线所需的数据。该函数返回一个包含三个元素的元组：训练集大小，以及验证集和训练集上的错误评分。在函数内部，我们使用以下参数：
- en: '`estimator` — indicates the learning algorithm we use to estimate the true
    model;'
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`estimator`— 指示我们用来估计真实模型的学习算法；'
- en: '`X` — the data containing the features;'
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X`— 包含特征的数据；'
- en: '`y` — the data containing the target;'
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y`— 包含目标的数据；'
- en: '`train_sizes` — specifies the training set sizes to be used;'
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_sizes`— 指定要使用的训练集大小；'
- en: '`cv` — determines the cross-validation splitting strategy (we''ll discuss this
    immediately);'
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cv`— 确定交叉验证拆分策略（我们将立即讨论这一点）；'
- en: '`scoring` — indicates the error metric to use; the intention is to use the
    mean squared error (MSE) metric, but that''s not a possible parameter for `scoring`;
    we''ll use the nearest proxy, negative MSE, and we''ll just have to flip signs
    later on.'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scoring`— 指定使用的错误度量；目标是使用均方误差（MSE）度量，但`scoring`中没有这个参数；我们将使用最接近的代理，即负MSE，稍后需要翻转符号。'
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We already know what''s in `train_sizes`. Let''s inspect the other two variables
    to see what `learning_curve()` returned:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道了`train_sizes`中的内容。现在让我们检查另外两个变量，看看`learning_curve()`返回了什么：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Since we specified six training set sizes, you might have expected six values
    for each kind of score. Instead, we got six rows for each, and every row has five
    error scores.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们指定了六个训练集大小，你可能期望每种评分都有六个值。相反，我们得到了六行数据，每行有五个错误评分。
- en: This happens because `learning_curve()` runs a `k`-fold cross-validation under
    the hood, where the value of `k` is given by what we specify for the `cv` parameter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 之所以会发生这种情况，是因为`learning_curve()`在后台执行了`k`-折交叉验证，其中`k`的值由我们为`cv`参数指定的值决定。
- en: 'In our case, `cv = 5`, so there will be five splits. For each split, an estimator
    is trained for every training set size specified. Each column in the two arrays
    above designates a split, and each row corresponds to a test size. Below is a
    table for the training error scores to help you understand the process better:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，`cv = 5`，所以会有五次切分。对于每个切分，都会为每个指定的训练集大小训练一个估计器。上面两个数组中的每列表示一个切分，每行对应一个测试大小。下面是一个用于训练误差评分的表格，以帮助你更好地理解过程：
- en: '| TRAINING SET SIZE (INDEX) | SPLIT1 | SPLIT2 | SPLIT3 | SPLIT4 | SPLIT5 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 训练集大小（索引） | 切分1 | 切分2 | 切分3 | 切分4 | 切分5 |'
- en: '| :-- | :-- | :-- | :-- | :-- | :-- |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- | :-- | :-- | :-- | :-- |'
- en: '| 1 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 | 0 | 0 | 0 |'
- en: '| 100 | -19.71230701 | -18.31492642 | -18.31492642 | -18.31492642 | -18.31492642
    |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 100 | -19.71230701 | -18.31492642 | -18.31492642 | -18.31492642 | -18.31492642
    |'
- en: '| 500 | -18.14420459 | -19.63885072 | -19.63885072 | -19.63885072 | -19.63885072
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 500 | -18.14420459 | -19.63885072 | -19.63885072 | -19.63885072 | -19.63885072
    |'
- en: '| 2000 | -21.53603444 | -20.18568787 | -19.98317419 | -19.98317419 | -19.98317419
    |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 2000 | -21.53603444 | -20.18568787 | -19.98317419 | -19.98317419 | -19.98317419
    |'
- en: '| 5000 | -20.47708899 | -19.93364211 | -20.56091569 | -20.4150839 | -20.4150839
    |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 5000 | -20.47708899 | -19.93364211 | -20.56091569 | -20.4150839 | -20.4150839
    |'
- en: '| 7654 | -20.98565335 | -20.63006094 | -21.04384703 | -20.63526811 | -20.52955609
    |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 7654 | -20.98565335 | -20.63006094 | -21.04384703 | -20.63526811 | -20.52955609
    |'
- en: To plot the learning curves, we need only a single error score per training
    set size, not 5\. For this reason, in the next code cell we take the mean value
    of each row and also flip the signs of the error scores (as discussed above).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要绘制学习曲线，我们只需要每个训练集大小的一个误差评分，而不是5个。因此，在下一个代码单元格中，我们取每一行的平均值，并翻转误差评分的符号（如上所述）。
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now we have all the data we need to plot the learning curves.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们拥有绘制学习曲线所需的所有数据。
- en: Before doing the plotting, however, we need to stop and make an important observation.
    You might have noticed that some error scores on the *training* sets are the same.
    For the row corresponding to training set size of 1, this is expected, but what
    about other rows? With the exception of the last row, we have a lot of identical
    values. For instance, take the second row where we have identical values from
    the second split onward. Why is that so?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在进行绘图之前，我们需要暂停并做一个重要的观察。你可能注意到一些*训练*集上的误差评分是相同的。对于训练集大小为1的行，这是预期的，但其他行呢？除了最后一行，我们有很多相同的值。例如，取第二行，从第二次拆分开始的值是相同的。为什么会这样？
- en: This is caused by not randomizing the *training* data for each split. Let's
    walk through a single example with the aid of the diagram below. When the training
    size is 500 the first 500 instances in the training set are selected. For the
    first split, these 500 instances will be taken from the second chunk. From the
    second split onward, these 500 instances will be taken from the first chunk. Because
    we don't randomize the training set, the 500 instances used for training are the
    same for the second split onward. This explains the identical values from the
    second split onward for the 500 training instances case.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这是由于没有为每次拆分随机化*训练*数据。我们通过下面的图示来逐步了解一个示例。当训练集的大小为500时，前500个实例被选中。对于第一次拆分，这500个实例将从第二个数据块中选取。从第二次拆分开始，这500个实例将从第一个数据块中选取。因为我们没有随机化训练集，所以第二次拆分之后用于训练的500个实例是相同的。这解释了500个训练实例的情况下，从第二次拆分开始相同的值。
- en: An identical reasoning applies to the 100 instances case, and a similar reasoning
    applies to the other cases.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的推理适用于100个实例的情况，类似的推理适用于其他情况。
- en: '![splits](../Images/edfe52b4e23dc00451643d1c2c6ac0a8.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![splits](../Images/edfe52b4e23dc00451643d1c2c6ac0a8.png)'
- en: 'To stop this behavior, we need to set the `shuffle` parameter to `True` in
    the `learning_curve()` function. This will randomize the indices for the *training* data
    for each split. We haven''t randomized above for two reasons:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要停止这种行为，我们需要将`shuffle`参数设置为`True`，以便在`learning_curve()`函数中对*训练*数据的每次拆分进行随机化。我们之前没有进行随机化有两个原因：
- en: The data comes pre-shuffled five times (as mentioned in the [documentation](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant))
    so there's no need to randomize anymore.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据已经预先洗牌了五次（如[文档](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)中提到的），所以不需要再随机化了。
- en: I wanted to make you aware about this quirk in case you stumble upon it in practice.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我想让你知道这个怪癖，以防你在实践中遇到它。
- en: Finally, let's do the plotting.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们进行绘图。
- en: Learning curves - high bias and low variance
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习曲线 - 高偏差和低方差
- en: 'We plot the learning curves using a regular matplotlib workflow:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用常规的matplotlib工作流绘制学习曲线：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Learning_curves_12_1](../Images/d08412056c4965911fcf9e9506f74f34.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![Learning_curves_12_1](../Images/d08412056c4965911fcf9e9506f74f34.png)'
- en: There's a lot of information we can extract from this plot. Let's proceed granularly.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个图中我们可以提取很多信息。让我们详细地进行分析。
- en: When the training set size is 1, we can see that the MSE for the training set
    is 0\. This is normal behavior, since the model has no problem fitting perfectly
    a single data point. So when tested upon the same data point, the prediction is
    perfect.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练集大小为1时，我们可以看到训练集的MSE为0。这是正常行为，因为模型对一个数据点的拟合没有问题。因此，当在同一个数据点上进行测试时，预测是完美的。
- en: But when tested on the validation set (which has 1914 instances), the MSE rockets
    up to roughly 423.4\. This relatively high value is the reason we restrict the
    y-axis range between 0 and 40\. This enables us to read most MSE values with precision.
    Such a high value is expected, since it's extremely unlikely that a model trained
    on a single data point can generalize accurately to 1914 new instances it hasn't
    seen in training.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 但在验证集（包含1914个实例）上测试时，均方误差（MSE）飙升至大约423.4。这一相对较高的值是我们将y轴范围限制在0到40之间的原因。这使我们能够更精确地读取大多数MSE值。如此高的值是可以预期的，因为模型在单一数据点上训练，不太可能对1914个在训练中未见过的新实例做出准确的泛化。
- en: When the training set size increases to 100, the training MSE increases sharply,
    while the validation MSE decreases likewise. The linear regression model doesn't
    predict all 100 training points perfectly, so the training MSE is greater than
    0\. However, the model performs much better now on the validation set because
    it's estimated with more data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练集大小增加到100时，训练MSE急剧上升，而验证MSE也同样下降。线性回归模型没有完美预测所有100个训练点，因此训练MSE大于0。然而，模型在验证集上的表现现在好多了，因为它使用了更多的数据进行估计。
- en: 'From 500 training data points onward, the validation MSE stays roughly the
    same. This tells us something extremely important: adding more training data points
    won''t lead to significantly better models. So instead of wasting time (and possibly
    money) with collecting more data, we need to try something else, like switching
    to an algorithm that can build more complex models.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从500个训练数据点开始，验证MSE大致保持不变。这告诉我们一个极其重要的事情：添加更多的训练数据点不会显著改善模型。因此，我们需要尝试其他方法，比如切换到可以构建更复杂模型的算法，而不是浪费时间（可能还会浪费金钱）去收集更多数据。
- en: '![add_data](../Images/a32f9b2090fa9861adde627057f852a2.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![add_data](../Images/a32f9b2090fa9861adde627057f852a2.png)'
- en: To avoid a misconception here, it's important to notice that what really won't
    help is adding more *instances* (rows) to the training data. Adding more features,
    however, is a different thing and is very likely to help because it will increase
    the complexity of our current model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免误解，重要的是注意到真正没有帮助的是在训练数据中添加更多的*实例*（行）。然而，添加更多的特征是另一回事，很可能会有所帮助，因为这会增加我们当前模型的复杂性。
- en: Let's now move to diagnosing bias and variance. The main indicator of a bias
    problem is a high validation error. In our case, the validation MSE stagnates
    at a value of approximately 20\. But how good is that? We'd benefit from some
    domain knowledge (perhaps physics or engineering in this case) to answer this,
    but let's give it a try.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来诊断偏差和方差。偏差问题的主要指标是高验证错误。在我们的案例中，验证MSE停滞在大约20的值。但这有多好呢？我们需要一些领域知识（在这个情况下可能是物理学或工程学）来回答这个问题，但我们还是尝试一下。
- en: Technically, that value of 20 has MW22 (megawatts squared) as units (the units
    get squared as well [when we compute the MSE](https://en.wikipedia.org/wiki/Mean_squared_error#Predictor)).
    But the values in our target column are in MW (according to the [documentation](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)).
    Taking the square root of 20 MW22 results in approximately 4.5 MW. Each target
    value represents net *hourly* electrical energy output. So for each hour our model
    is off by 4.5 MW on average. According to [this Quora answer](https://www.quora.com/How-can-I-get-an-intuitive-understanding-of-what-a-Kw-Mw-Gw-of-electricity-equates-to-in-real-life-terms),
    4.5 MW is equivalent to the heat power produced by 4500 handheld hair dryers.
    And this would add up if we tried to predict the total energy output for one day
    or a longer period.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，20的值单位是MW²（兆瓦平方）（单位也会平方[当我们计算MSE时](https://en.wikipedia.org/wiki/Mean_squared_error#Predictor)）。但我们目标列中的值是以MW为单位（根据[文档](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)）。取20
    MW²的平方根约为4.5 MW。每个目标值代表净*小时*电力输出。因此，每小时我们的模型平均偏差为4.5 MW。根据[这个Quora回答](https://www.quora.com/How-can-I-get-an-intuitive-understanding-of-what-a-Kw-Mw-Gw-of-electricity-equates-to-in-real-life-terms)，4.5
    MW相当于4500个手持吹风机所产生的热功率。如果我们尝试预测一天或更长时间的总能量输出，这个值将会累积。
- en: We can conclude that the an MSE of 20 MW22 is quite large. So our model has
    a bias problem. But is it a *low* bias problem or a *high* bias problem?
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以得出结论，20 MW²的MSE相当大。因此，我们的模型存在偏差问题。但是这是一个*低*偏差问题还是一个*高*偏差问题？
- en: To find the answer, we need to look at the training error. If the training error
    is very low, it means that the training data is fitted very well by the estimated
    model. If the model fits the training data very well, it means it has *low* bias
    with respect to that set of data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到答案，我们需要查看训练误差。如果训练误差非常低，这意味着估计模型对训练数据的拟合非常好。如果模型对训练数据的拟合非常好，这意味着它对该数据集的偏差很*低*。
- en: If the training error is high, it means that the training data is not fitted
    well enough by the estimated model. If the model fails to fit the training data
    well, it means it has *high* bias with respect to that set of data.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练误差很高，这意味着估计模型对训练数据的拟合不够好。如果模型无法很好地拟合训练数据，这意味着它对该数据集的偏差很*高*。
- en: '![low_high_bias](../Images/2b7be8d34b94b5b41f0bd0156ff9d9a1.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![low_high_bias](../Images/2b7be8d34b94b5b41f0bd0156ff9d9a1.png)'
- en: In our particular case, the training MSE plateaus at a value of roughly 20 MW22\.
    As we've already established, this is a high error score. Because the validation
    MSE is high, and the training MSE is high as well, our model has a high bias problem.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们特定的案例中，训练 MSE 平稳在大约 20 MW22 的值。正如我们已经确定的，这是一个高误差分数。由于验证 MSE 很高，而训练 MSE 也很高，我们的模型存在高偏差问题。
- en: 'Now let''s move with diagnosing eventual variance problems. Estimating variance
    can be done in at least two ways:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续诊断最终的方差问题。估计方差至少有两种方法：
- en: By examining the gap between the validation learning curve and training learning
    curve.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过检查验证学习曲线和训练学习曲线之间的差距。
- en: 'By examining the training error: its value and its evolution as the training
    set sizes increase.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过检查训练误差：其值及随着训练集大小增加的演变。
- en: '![lc_regression](../Images/6a3d18a5b15c8d34fd28790435933b1a.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![lc_regression](../Images/6a3d18a5b15c8d34fd28790435933b1a.png)'
- en: 'A narrow gap indicates low variance. Generally, the more narrow the gap, the
    lower the variance. The opposite is also true: the wider the gap, the greater
    the variance. Let''s now explain why this is the case.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 窄的差距表示低方差。一般来说，差距越窄，方差越低。反之亦然：差距越宽，方差越大。现在让我们解释为什么会这样。
- en: As we've discussed earlier, if the variance is high, then the model fits training
    data too well. When training data is fitted too well, the model will have trouble
    generalizing on data that hasn't seen in training. When such a model is tested
    on its training set, and then on a validation set, the training error will be
    low and the validation error will generally be high. As we change training set
    sizes, this pattern continues, and the differences between training and validation
    errors will determine that gap between the two learning curves.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，如果方差很高，则模型对训练数据拟合得过好。当训练数据拟合得过好时，模型将难以对未在训练中见过的数据进行泛化。当这样的模型在其训练集上进行测试，然后在验证集上进行测试时，训练误差将很低，而验证误差通常会很高。随着训练集大小的变化，这种模式会持续下去，训练误差和验证误差之间的差异将决定两条学习曲线之间的差距。
- en: 'The relationship between the training and validation error, and the gap can
    be summarized this way:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 训练误差和验证误差之间的关系可以总结如下：
- en: gap=validation error−training errorgap=validation error−training error
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: gap=验证误差−训练误差gap=验证误差−训练误差
- en: So the bigger the difference between the two errors, the bigger the gap. The
    bigger the gap, the bigger the variance.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，两种误差之间的差异越大，差距也越大。差距越大，方差也越大。
- en: In our case, the gap is very narrow, so we can safely conclude that the variance
    is low.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，差距非常小，因此我们可以放心地得出方差很低的结论。
- en: High *training* MSE scores are also a quick way to detect low variance. If the
    variance of a learning algorithm is low, then the algorithm will come up with
    simplistic and similar models as we change the training sets. Because the models
    are overly simplified, they cannot even fit the training data well (they *underfit* the
    data). So we should expect high training MSEs. Hence, high training MSEs can be
    used as indicators of low variance.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 高*训练* MSE 分数也是检测低方差的快捷方式。如果学习算法的方差很低，那么随着训练集的变化，算法会产生简化且类似的模型。由于模型过于简化，它们甚至无法很好地拟合训练数据（它们*欠拟合*了数据）。因此，我们应该预期高训练
    MSE。因此，高训练 MSE 可以用作低方差的指示器。
- en: '![low_high_var](../Images/31c509512a7d66c68d59887ac5c7830a.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![low_high_var](../Images/31c509512a7d66c68d59887ac5c7830a.png)'
- en: In our case, the training MSE plateaus at around 20, and we've already concluded
    that's a high value. So besides the narrow gap, we now have another confirmation
    that we have a low variance problem.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，训练的均方误差（MSE）在大约20处趋于平稳，我们已经得出结论这是一个较高的值。因此，除了狭窄的差距，我们现在还有另一个确认表明我们有一个低方差问题。
- en: 'So far, we can conclude that:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们可以得出结论：
- en: Our learning algorithm suffers from high bias and low variance, underfitting
    the training data.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的学习算法存在高偏差和低方差的问题，对训练数据进行欠拟合。
- en: Adding more instances (rows) to the training data is hugely unlikely to lead
    to better models under the current learning algorithm.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在当前学习算法下，向训练数据中添加更多实例（行）极不可能导致更好的模型。
- en: One solution at this point is to change to a more complex learning algorithm.
    This should decrease the bias and increase the variance. A mistake would be to
    try to increase the number of training instances.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此时的一个解决方案是更改为更复杂的学习算法。这应该能减少偏差并增加方差。错误的做法是尝试增加训练实例的数量。
- en: 'Generally, these other two fixes also work when dealing with a high bias and
    low variance problem:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这另外两种修复方法也适用于处理高偏差和低方差问题：
- en: Training the current learning algorithm on more features (to avoid *collecting* new
    data, you can generate easily [polynomial features](http://scikit-learn.org/stable/modules/preprocessing.html)).
    This should lower the bias by increasing the model's complexity.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在更多特征上训练当前学习算法（为了避免*收集*新数据，你可以轻松生成[多项式特征](http://scikit-learn.org/stable/modules/preprocessing.html)）。这应该通过增加模型的复杂性来降低偏差。
- en: Decreasing the [regularization](https://www.quora.com/What-is-regularization-in-machine-learning) of
    the current learning algorithm, if that's the case. In a nutshell, regularization
    prevents the algorithm from fitting the training data too well. If we decrease
    regularization, the model will fit training data better, and, as a consequence,
    the variance will increase and the bias will decrease.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少当前学习算法的[正则化](https://www.quora.com/What-is-regularization-in-machine-learning)，如果是这种情况。简而言之，正则化防止算法对训练数据过于拟合。如果我们减少正则化，模型将更好地拟合训练数据，因此方差将增加，偏差将减少。
- en: Learning curves - low bias and high variance
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习曲线 - 低偏差和高方差
- en: Let's see how an unregularized Random Forest regressor fares here. We'll generate
    the learning curves using the same workflow as above. This time we'll bundle everything
    into a function so we can use it for later. For comparison, we'll also display
    the learning curves for the linear regression model above.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个未正则化的随机森林回归器在这里的表现。我们将使用与上述相同的工作流程生成学习曲线。这次我们将所有内容打包成一个函数，以便以后使用。为了进行比较，我们还将展示线性回归模型的学习曲线。
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Learning_curves_15_0](../Images/543485dfb23baba95c68632d5c624a0a.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![学习曲线_15_0](../Images/543485dfb23baba95c68632d5c624a0a.png)'
- en: Now let's try to apply what we've just learned. It'd be a good idea to pause
    reading at this point and try to interpret the new learning curves yourself.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试应用我们刚刚学到的知识。此时暂停阅读并尝试自己解释新的学习曲线会是个好主意。
- en: Looking at the validation curve, we can see that we've managed to decrease bias.
    There still is some significant bias, but not that much as before. Looking at
    the training curve, we can deduce that this time there's a *low* bias problem.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看验证曲线，我们可以看到我们已经设法减少了偏差。虽然仍然存在一些显著的偏差，但比之前少了很多。通过查看训练曲线，我们可以推测这次存在一个*低*偏差问题。
- en: The new gap between the two learning curves suggests a substantial increase
    in variance. The low training MSEs corroborate this diagnosis of high variance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 两条学习曲线之间的新差距表明方差显著增加。低的训练均方误差（MSE）证实了这一高方差的诊断。
- en: The large gap and the low training error also indicates an overfitting problem.
    Overfitting happens when the model performs well on the training set, but far
    poorer on the test (or validation) set.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 大的差距和低的训练误差也表明存在过拟合问题。过拟合发生在模型在训练集上表现良好，但在测试（或验证）集上表现较差时。
- en: One more important observation we can make here is that *adding new training
    instances* is very likely to lead to better models. The validation curve doesn't
    plateau at the maximum training set size used. It still has potential to decrease
    and converge toward the training curve, similar to the convergence we see in the
    linear regression case.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在这里做出一个更重要的观察，即*添加新的训练实例*很可能会导致更好的模型。验证曲线没有在使用的最大训练集大小处趋于平稳。它仍然有潜力降低并趋近于训练曲线，类似于我们在线性回归情况下看到的收敛情况。
- en: 'So far, we can conclude that:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们可以得出结论：
- en: Our learning algorithm (random forests) suffers from high variance and quite
    a low bias, overfitting the training data.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的学习算法（随机森林）在训练数据上过拟合，表现出较高的方差和较低的偏差。
- en: Adding more training instances is very likely to lead to better models under
    the current learning algorithm.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在当前学习算法下，增加更多的训练实例很可能会导致更好的模型。
- en: 'At this point, here are a couple of things we could do to improve our model:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们可以做几件事情来改进我们的模型：
- en: Adding more training instances.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加更多的训练实例。
- en: Increase the regularization for our current learning algorithm. This should
    decrease the variance and increase the bias.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加我们当前学习算法的正则化。这应该会减少方差并增加偏差。
- en: Reducing the numbers of features in the training data we currently use. The
    algorithm will still fit the training data very well, but due to the decreased
    number of features, it will build less complex models. This should increase the
    bias and decrease the variance.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少我们当前使用的训练数据中的特征数量。算法仍然会很好地拟合训练数据，但由于特征数量的减少，它会构建较简单的模型。这应该会增加偏差并减少方差。
- en: In our case, we don't have any other readily available data. We could go into
    the power plant and take some measurements, but we'll save this for another post
    (just kidding).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们没有其他现成的数据。我们可以去电厂进行一些测量，但我们将留到另一个帖子中（开个玩笑）。
- en: Let's rather try to regularize our random forests algorithm. One way to do that
    is to adjust the maximum number of leaf nodes in each decision tree. This can
    be done by using the `max_leaf_nodes` parameter of `RandomForestRegressor()`.
    It's not necessarily for you to understand this regularization technique. For
    our purpose here, what you need to focus on is the effect of this regularization
    on the learning curves.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 不如尝试对我们的随机森林算法进行正则化。实现这一点的一种方法是调整每棵决策树中的最大叶节点数。可以通过使用 `RandomForestRegressor()`
    的 `max_leaf_nodes` 参数来完成。你不必理解这种正则化技术。对于我们目前的目的，你需要关注的是这种正则化对学习曲线的影响。
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Learning_curves_17_0](../Images/e2668882d10fe739e9a86d9f5d803409.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![Learning_curves_17_0](../Images/e2668882d10fe739e9a86d9f5d803409.png)'
- en: Not bad! The gap is now more narrow, so there's less variance. The bias seems
    to have increased just a bit, which is what we wanted.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 不错！现在差距缩小了，因此方差减少了。偏差似乎稍微增加了一点，这正是我们想要的。
- en: 'But our work is far from over! The validation MSE still shows a lot of potential
    to decrease. Some steps you can take toward this goal include:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们的工作还远未结束！验证 MSE 仍显示出很大的下降潜力。你可以采取的一些步骤包括：
- en: Adding more training instances.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加更多的训练实例。
- en: Adding more features.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加更多的特征。
- en: Feature selection.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征选择。
- en: Hyperparameter optimization.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数优化。
- en: The ideal learning curves and the irreducible error
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理想的学习曲线和不可减少的误差
- en: Learning curves constitute a great tool to do a quick check on our models at
    every point in our machine learning workflow. But how do we know when to stop?
    How do we recognize the perfect learning curves?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线是一个很好的工具，可以在机器学习工作流中的每一个点上快速检查我们的模型。但是我们如何知道何时停止？我们如何识别完美的学习曲线？
- en: For our regression case before, you might think that the perfect scenario is
    when both curves converge toward an MSE of 0\. That's a perfect scenario, indeed,
    but, unfortunately, it's not possible. Neither in practice, neither in theory.
    And this is because of something called *irreducible error*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们之前的回归案例，你可能会认为完美的情境是当两条曲线都收敛到 MSE 为 0\. 这确实是一个完美的情境，但不幸的是，这在实践中和理论上都不可能。原因是所谓的*不可减少的误差*。
- en: 'When we build a model to map the relationship between the features ***X*** and
    the target ***Y***, we assume that there is such a relationship in the first place.
    Provided the assumption is true, there is a true model ![Equation](../Images/0124e78268c96059dbff35809fb67ffe.png) that
    describes perfectly the relationship between ***X*** and ***Y***, like so:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们构建一个模型来映射特征***X***和目标***Y***之间的关系时，我们首先假设存在这样的关系。如果假设是正确的，那么确实存在一个完美描述***X***和***Y***之间关系的真实模型
    ![Equation](../Images/0124e78268c96059dbff35809fb67ffe.png)，如下面所示：
- en: '![Equation](../Images/0617d3c146a95ad60f2d2561bd93af9b.png)     (1)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![Equation](../Images/0617d3c146a95ad60f2d2561bd93af9b.png)     (1)'
- en: But why is there an error?! Haven't we just said that ![Equation](../Images/0124e78268c96059dbff35809fb67ffe.png) describes
    the relationship between X and Y perfectly?!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么会有错误呢？！我们不是刚刚说过 ![Equation](../Images/0124e78268c96059dbff35809fb67ffe.png)
    完美地描述了 X 和 Y 之间的关系吗？！
- en: There's an error there because ***Y*** is not only a function of our limited
    number of features ***X***. There could be many other features that influence
    the value of ***Y***. Features we don't have. It might also be the case that ***X*** contains
    measurement errors. So, besides ***X***, ***Y*** is also a function of *irreducible error*.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个错误，因为***Y***不仅是我们有限特征***X***的函数。可能还有许多其他特征影响***Y***的值。我们没有这些特征。也可能***X***包含测量误差。因此，除了***X***之外，***Y***也是*不可约误差*的函数。
- en: 'Now let''s explain why this error is *irreducible*. When we estimate ***f(X)*** with
    a model ![Equation](../Images/e95a915c8e775f7ecfcf46075ea77dec.png), we introduce
    another kind of error, called *reducible* error:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们解释为什么这种误差是*不可约的*。当我们用模型![Equation](../Images/e95a915c8e775f7ecfcf46075ea77dec.png)估计***f(X)***时，我们引入了另一种误差，称为*可约误差*：
- en: '![Equation](../Images/d87907e57e1aca8903ee05fe75b060c0.png)     (2)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![Equation](../Images/d87907e57e1aca8903ee05fe75b060c0.png)     (2)'
- en: 'Replacing ***f(X)*** in (1) we get:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在（1）中替换***f(X)***，我们得到：
- en: '![Equation](../Images/f4958e321c98d383fbf90c05a31e2410.png)     (3)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![Equation](../Images/f4958e321c98d383fbf90c05a31e2410.png)     (3)'
- en: Error that is reducible can be reduced by building better models. Looking at
    equation (2) we can see that if the reducible errorreducible error is 0, our estimated
    model ![Equation](../Images/e95a915c8e775f7ecfcf46075ea77dec.png) is equal to
    the true model ***f(X)***. However, from (3) we can see that *irreducible error* remains
    in the equation even if *reducible error* is 0\. From here we deduce that no matter
    how good our model estimate is, generally there still is some error we cannot
    reduce. And that's why this error is considered *irreducible*.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 可约误差可以通过构建更好的模型来减少。从方程（2）中我们可以看到，如果可约误差为0，我们估计的模型![Equation](../Images/e95a915c8e775f7ecfcf46075ea77dec.png)等于真实模型***f(X)***。然而，从（3）中我们可以看到，即使*可约误差*为0，*不可约误差*仍然存在于方程中。因此，我们推断无论我们的模型估计有多好，通常仍然存在一些我们无法减少的误差。这就是为什么这种误差被认为是*不可约*的原因。
- en: This tells us that that in practice the best possible learning curves we can
    see are those which converge to the value of some irreducible error, not toward
    some ideal error value (for MSE, the ideal error score is 0; we'll see immediately
    that other error metrics have different ideal error values).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，在实际操作中，我们看到的最佳学习曲线是那些收敛到某种不可约误差值的曲线，而不是收敛到某种理想误差值（对于均方误差，理想的误差值是0；我们会立即看到其他误差度量有不同的理想误差值）。
- en: '![irr_error](../Images/511282f6512b6f0ce0e5e0d5af5d4a23.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![irr_error](../Images/511282f6512b6f0ce0e5e0d5af5d4a23.png)'
- en: 'In practice, the exact value of the irreducible error is almost always unknown.
    We also assume that the irreducible error is independent of ***X***. This means
    that we cannot use ***X*** to find the true irreducible error. Expressing the
    same thing in the more precise language of mathematics, there''s no function ***g*** to
    map ***X*** to the true value of the irreducible error:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，无法准确知道不可约误差的值。我们还假设不可约误差与***X***无关。这意味着我们不能使用***X***来找到不可约误差的真实值。用更精确的数学语言表达，就是没有函数***g***可以将***X***映射到不可约误差的真实值：
- en: '*irreducible error ≠ g(X)*'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*不可约误差 ≠ g(X)*'
- en: So there's no way to know the true value of the irreducible error based on the
    data we have. In practice, a good workaround is to try to lower the error score
    as much as possible, while keeping in mind that the limit is given by some irreducible
    error.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据我们拥有的数据无法知道不可约误差的真实值。在实践中，一个好的方法是尽可能降低误差分数，同时牢记限制由某种不可约误差决定。
- en: What about classification?
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 那么分类呢？
- en: 'So far, we''ve learned about learning curves in a regression setting. For classification
    tasks, the workflow is almost identical. The main difference is that we''ll have
    to choose another error metric - one that is suitable for evaluating the performance
    of a classifier. Let''s see an example:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们了解了回归设置中的学习曲线。对于分类任务，工作流程几乎相同。主要的区别在于我们需要选择另一种误差度量——一种适合评估分类器性能的度量。我们来看一个例子：
- en: '![classification](../Images/f0f1d038900ad67bb1b9b5c16f88d42a.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![classification](../Images/f0f1d038900ad67bb1b9b5c16f88d42a.png)'
- en: Source: [scikit-learn documentation](http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[scikit-learn documentation](http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html)
- en: Unlike what we've seen so far, notice that the learning curve for the training
    error is above the one for the validation error. This is because the score used, *accuracy*,
    describes how good the model is. The higher the accuracy, the better. The MSE,
    on the other side, describes how bad a model is. The lower the MSE, the better.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们迄今为止看到的不同，注意训练误差的学习曲线高于验证误差的曲线。这是因为使用的分数，*准确性*，描述了模型的好坏。准确性越高，模型越好。而均方误差（MSE）则描述了模型的差劲程度。均方误差越低，模型越好。
- en: 'This has implications for the irreducible error as well. For error metrics
    that describe how bad a model is, the irreducible error gives a lower bound: you
    cannot get lower than that. For error metrics that describe how good a model is,
    the irreducible error gives an upper bound: you cannot get higher than that.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这也对不可减少的错误有影响。对于描述模型有多差的误差度量，不可减少的错误提供了一个下界：你不能低于这个值。对于描述模型有多好的误差度量，不可减少的错误提供了一个上界：你不能高于这个值。
- en: As a side note here, in more technical writings the term [*Bayes error rate*](https://en.wikipedia.org/wiki/Bayes_error_rate) is
    what's usually used to refer to the best possible error score of a classifier.
    The concept is analogous to the irreducible error.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这里补充一点，在更技术性的写作中，通常使用[*贝叶斯误差率*](https://en.wikipedia.org/wiki/Bayes_error_rate)来指代分类器的最佳可能错误分数。这个概念类似于不可减少的错误。
- en: Next steps
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 下一步
- en: Learning curves constitute a great tool to diagnose bias and variance in any
    supervised learning algorithm. We've learned how to generate them using scikit-learn
    and matplotlib, and how to use them to diagnose bias and variance in our models.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线是诊断任何监督学习算法中的偏差和方差的绝佳工具。我们已经学习了如何使用scikit-learn和matplotlib生成它们，并如何利用它们来诊断我们模型中的偏差和方差。
- en: 'To reinforce what you''ve learned, these are some next steps to consider:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了巩固你所学到的内容，以下是一些值得考虑的下一步：
- en: Generate learning curves for a regression task using a different data set.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的数据集生成回归任务的学习曲线。
- en: Generate learning curves for a classification task.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成分类任务的学习曲线。
- en: Generate learning curves for a supervised learning task by coding everything
    from scratch (don't use `learning_curve()` from scikit-learn). Using cross-validation
    is optional.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过从头开始编写代码（不要使用`learning_curve()`来自scikit-learn），生成监督学习任务的学习曲线。使用交叉验证是可选的。
- en: Compare learning curves obtained without cross-validating with curves obtained
    using cross-validation. The two kinds of curves should be for the same learning
    algorithm.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较没有进行交叉验证和使用交叉验证获得的学习曲线。这两种曲线应针对相同的学习算法。
- en: '**Bio: [Alex Olteanu](https://www.dataquest.io/blog/author/alex-olteanu/)**
    is a Student Success Specialist at Dataquest.io. He enjoys learning and sharing
    knowledge, and is getting ready for the new AI revolution.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介: [亚历克斯·奥尔特亚努](https://www.dataquest.io/blog/author/alex-olteanu/)** 是Dataquest.io的学生成功专家。他喜欢学习和分享知识，并为新的AI革命做好准备。'
- en: '[Original](https://www.dataquest.io/blog/learning-curves-machine-learning/?utm_source=kdnuggets&utm_medium=blog).
    Reposted with permission.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://www.dataquest.io/blog/learning-curves-machine-learning/?utm_source=kdnuggets&utm_medium=blog)。经授权转载。'
- en: '**Related:**'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容:**'
- en: '[Regularization in Machine Learning](/2018/01/regularization-machine-learning.html)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中的正则化](/2018/01/regularization-machine-learning.html)'
- en: '[How to Generate FiveThirtyEight Graphs in Python](/2017/12/generate-fivethirtyeight-graphs-python.html)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在Python中生成FiveThirtyEight图表](/2017/12/generate-fivethirtyeight-graphs-python.html)'
- en: '[Training Sets, Test Sets, and 10-fold Cross-validation](/2018/01/training-test-sets-cross-validation.html)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[训练集、测试集和10折交叉验证](/2018/01/training-test-sets-cross-validation.html)'
- en: '* * *'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT工作'
- en: '* * *'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[KDnuggets News, December 14: 3 Free Machine Learning Courses for…](https://www.kdnuggets.com/2022/n48.html)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，12月14日：3 个免费机器学习课程](https://www.kdnuggets.com/2022/n48.html)'
- en: '[5 Machine Learning Skills Every Machine Learning Engineer Should…](https://www.kdnuggets.com/2023/03/5-machine-learning-skills-every-machine-learning-engineer-know-2023.html)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个机器学习工程师都应该掌握的5个机器学习技能…](https://www.kdnuggets.com/2023/03/5-machine-learning-skills-every-machine-learning-engineer-know-2023.html)'
- en: '[A Solid Plan for Learning Data Science, Machine Learning, and Deep Learning](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学、机器学习和深度学习的稳固计划](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
- en: '[AI, Analytics, Machine Learning, Data Science, Deep Learning…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能、分析、机器学习、数据科学、深度学习…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
- en: '[Breaking the Data Barrier: How Zero-Shot, One-Shot, and Few-Shot…](https://www.kdnuggets.com/2023/08/breaking-data-barrier-zeroshot-oneshot-fewshot-learning-transforming-machine-learning.html)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[突破数据障碍：如何通过零样本、单样本和少样本…](https://www.kdnuggets.com/2023/08/breaking-data-barrier-zeroshot-oneshot-fewshot-learning-transforming-machine-learning.html)'
- en: '[Federated Learning: Collaborative Machine Learning with a Tutorial…](https://www.kdnuggets.com/2021/12/federated-learning-collaborative-machine-learning-tutorial-get-started.html)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[联邦学习：带教程的协作机器学习入门…](https://www.kdnuggets.com/2021/12/federated-learning-collaborative-machine-learning-tutorial-get-started.html)'
