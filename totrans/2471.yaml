- en: Transfer Learning for Image Recognition and Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转移学习用于图像识别和自然语言处理
- en: 原文：[https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)
- en: If you had the chance to read Part 1 of this article, you will remember that
    Transfer Learning is a machine learning method where the application of knowledge
    obtained from a model used in one task, can be reused as a foundation point for
    another task.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有机会阅读了本文的第一部分，你会记得转移学习是一种机器学习方法，其中从一个任务中获得的模型的知识应用可以作为另一个任务的基础点被重用。
- en: If you did not get the chance and don’t have a great understanding of Transfer
    Learning, give it a read it will help you understand this article much better.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有机会深入了解转移学习，读一读它将帮助你更好地理解这篇文章。
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的快车道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织在IT方面'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Image Recognition
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像识别
- en: So let's first go through what Image Recognition is. Image Recognition is the
    task assigned to computer technology to be able to detect and analyse an object
    or a feature in an image or video. It is the major area where deep neural networks
    work their magic as they are designed to recognise patterns.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们了解一下什么是图像识别。图像识别是指计算机技术能够检测和分析图像或视频中的物体或特征的任务。这是深度神经网络施展魔力的主要领域，因为它们被设计用来识别模式。
- en: If you would like to see how neural networks analyse and grasp their understanding
    of images, take a look [here](https://distill.pub/2017/feature-visualization/).
    This shows you how lower-level layers concentrate on learning low-level features
    and how the higher-level layers adapt to learn higher-level features.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解神经网络如何分析和掌握图像的理解，请查看[这里](https://distill.pub/2017/feature-visualization/)。这将向你展示低层次的层如何集中学习低级特征，以及高层次的层如何适应学习高级特征。
- en: We vaguely spoke about training the network on cats and dogs in part 1 of this
    article, let's speak more on that. The human brain can recognise and distinguish
    the difference between objects. Image recognition aims to also have a similar
    ability to the human brain, also being able to identify and detect the difference
    between objects in an image.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本文第一部分中模糊地谈到了训练网络识别猫和狗的内容，让我们更详细地讨论一下。人脑可以识别和区分物体。图像识别也旨在拥有类似于人脑的能力，能够识别和检测图像中物体之间的差异。
- en: The algorithm used by image recognition is an image classifier that receives
    an input image and outputs what the image contains. This algorithm has to be trained
    to learn the differences between the objects it detects. For the image classifier
    to identify cats and dogs, the image classifier needs to be trained with thousands
    of images of cats and dogs along with images that also do not contain cats and
    dogs. Therefore the data obtained from this image classifier can then be implemented
    and used in the detection of cats and dogs in other tasks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图像识别所使用的算法是一个图像分类器，它接收输入图像并输出图像包含的内容。这个算法必须经过训练，以学习它检测的对象之间的差异。为了让图像分类器识别猫和狗，需要用成千上万的猫和狗的图像，以及不包含猫和狗的图像来训练图像分类器。因此，从这个图像分类器获得的数据可以在其他任务中用于猫和狗的检测。
- en: So where can you find pre-trained image models?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 那么你可以在哪里找到预训练的图像模型呢？
- en: There are a lot of pre-trained models out there, so finding the right one to
    solve your problem will require a little bit of research. I’m going to do a bit
    of the research for you. I will start by talking about [Keras](https://keras.io/about/).
    Keras offers a wide range of pre-trained models that can be used for transfer
    learning, feature extraction, fine-tuning, and prediction. You can find a list
    of them [here](https://keras.io/api/applications/).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有很多预训练模型，因此找到合适的模型来解决你的问题需要一些研究。我将为你做一点研究。我将首先讨论 [Keras](https://keras.io/about/)。Keras
    提供了广泛的预训练模型，可用于迁移学习、特征提取、微调和预测。你可以在 [这里](https://keras.io/api/applications/) 找到它们的列表。
- en: An example of a Keras application is the Xception architecture, which I spoke
    about briefly further up. This is how you can initialise Xception on ImageNet.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Keras 应用的例子是 Xception 架构，我在上面简要提到过。以下是如何在 ImageNet 上初始化 Xception。
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As we speak about Keras, let’s not forget about TensorFlow Hub. TensorFlow is
    an open-source library developed by Google, which supports machine learning and
    deep learning. TensorFlow Hub has a variety of pre-trained, ready-to-deploy models
    such as image, text, video, and audio  classification.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 说到 Keras，我们也不要忘记 TensorFlow Hub。TensorFlow 是一个由 Google 开发的开源库，支持机器学习和深度学习。TensorFlow
    Hub 提供了多种预训练的、可部署的模型，如图像、文本、视频和音频分类。
- en: A Sequential model is good to use for a plain stack of layers, this entails
    when each layer has one input and one output tensor.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 序列模型适合用于简单的层叠堆叠，这意味着每一层都有一个输入和一个输出张量。
- en: '**Example 1:**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例 1：**'
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Example 2:**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例 2：**'
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Within deep learning, some strategies can allow one to gain maximum benefit
    from pre-trained models.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，一些策略可以使我们最大限度地利用预训练模型。
- en: Feature Extraction
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征提取
- en: The first strategy I will speak on is Feature Extraction. You essentially run
    the pre-trained model as a fixed feature extractor, using the desired features
    to train a new classifier. Convolutional layers nearer to the input layer of the
    model learn low-level features such as edges and lines in image recognition. As
    the layers go on, they start to learn more complex features to then eventually
    be able to interpret specific features closer to the output, which is the classification
    task at hand. We can use the different layer levels as a separate feature extraction
    integration by using portions of the model to classify our task.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我将讨论的第一个策略是特征提取。你基本上是将预训练模型作为固定的特征提取器来运行，使用所需的特征来训练新的分类器。接近模型输入层的卷积层学习低级特征，如图像识别中的边缘和线条。随着层次的增加，它们开始学习更复杂的特征，最终能够解释接近输出的特定特征，即当前的分类任务。我们可以通过使用模型的不同层级作为单独的特征提取整合来分类我们的任务。
- en: For example, if the task differs from classifying objects in an image which
    the pre-trained model does, using the output after the pre-trained model has gone
    through a few layers would be more appropriate. Therefore, the new output of the
    desired layer will be fed as the input to a new model has gained essential features
    to pass into the new classifier and be able to solve the task at hand.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果任务与对图像中物体的分类不同（这是预训练模型所做的），那么使用预训练模型经过几层处理后的输出会更为合适。因此，所需层的新输出将作为输入提供给新模型，新模型已经获得了必要的特征，可以传递给新的分类器，并能够解决当前任务。
- en: Overall, the idea is to leverage the pre-trained model using feature extraction
    to determine which features are useful to solve your task, but you will not use
    the output of the network as it is too task-specific. You will then have to build
    the last part of the model to be specific to your task.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这个想法是利用特征提取来利用预训练模型，以确定哪些特征对解决你的任务有用，但你不会使用网络的输出，因为它过于特定于任务。然后你将需要构建模型的最后部分，以适应你的任务。
- en: Fine-tuning
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调
- en: The second strategy is Fine-tuning, or what I like to call network surgery.
    We spoke about Fine-tuning above, where it is built on making “fine” adjustments
    to a process to obtain the desired output to further improve performance. Fine-tuning
    is also seen as a further step to Feature Extraction. In fine-tuning we freeze
    certain layers and selectively re-train some to improve its accuracy, gaining
    a higher performance at a lower learning rate and requiring less training time.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种策略是微调，或者我喜欢称之为网络手术。我们在上面讨论过微调，它基于对过程进行“精细”调整，以获得所需的输出，从而进一步提高性能。微调也被视为特征提取的进一步步骤。在微调中，我们冻结某些层并选择性地重新训练一些层，以提高其准确性，从而在较低的学习率下获得更高的性能，并减少训练时间。
- en: Fine-tuning is vital if you want to be able to obtain efficient feature representation
    from the base model, thus being more catered to your task at hand.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望从基本模型中获得有效的特征表示，微调是至关重要的，这样可以更好地适应你的任务。
- en: 'Example: How to use a pre-trained image model in Keras'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例：如何在Keras中使用预训练的图像模型
- en: So now let’s look at an example. Since we’ve been speaking so much about cats
    and dogs, let’s do an Image recognition example on them.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一个例子。由于我们一直在讨论猫和狗，让我们对它们做一个图像识别的示例。
- en: Load the data
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载数据
- en: So let's first make sure we import the libraries that we need and load the Data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所以首先让我们确保导入所需的库并加载数据。
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: It is always good to print out your sample size for your training and test to
    understand how much data you are working with and have a gander with the images,
    so you can have a look at what data you’re working with.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出训练和测试的样本量总是好的，以了解你正在处理的数据量，并查看图像，这样你可以了解你正在处理的数据。
- en: '![Transfer Learning for Image Recognition and Natural Language Processing](../Images/b3fd27648714f720ac95c80bac98cf59.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图像识别和自然语言处理的迁移学习](../Images/b3fd27648714f720ac95c80bac98cf59.png)'
- en: Due to images varying in size, it is a good approach to standardise to fixed
    image size, as it's a good consistent input for the neural network.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于图像大小各异，将图像标准化为固定大小是一种良好的方法，因为这是神经网络的一个一致输入。
- en: Data pre-processing
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Now let’s go into Data Augmentation. When working with a smaller dataset, it
    is good practice to apply random transformation to the training images, such as
    horizontal flipping. Flipping means rotating an image on a vertical or horizontal
    axis. This will help expose the model to different angles and aspects of the training
    data, reducing overfitting.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进入数据增强的部分。当处理较小的数据集时，应用随机变换到训练图像上是一个好的做法，例如水平翻转。翻转意味着在垂直或水平轴上旋转图像。这将帮助模型接触到训练数据的不同角度和方面，从而减少过拟合。
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Creating a base_model from chosen architecture (Xception)
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从选择的架构（Xception）创建一个基本模型
- en: The next step is creating the model. We are first going to start with instantiating
    a base model, [Xception](https://keras.io/api/applications/xception/), and loading
    pre-trained weights into it. In this example, we are using ImageNet. We then move
    on to freezing all the layers in the base model. Freezing helps to prevent the
    weights from being updated during training.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建模型。我们首先将实例化一个基本模型，[Xception](https://keras.io/api/applications/xception/)，并将预训练的权重加载到其中。在这个例子中，我们使用的是ImageNet。然后，我们冻结基本模型中的所有层。冻结有助于防止在训练过程中更新权重。
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Train the top layer
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练顶层
- en: The next step is creating a new layer on top of the frozen layers, which will
    learn its knowledge of the old features and use that to determine predictions
    on the new dataset. This is beneficial as explained further up in the steps of
    transfer learning, the likelihood that the current output on the pre-trained model
    and the output you want from your model will be different is high, therefore adding
    new layers will improve the model overall.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是在冻结的层上创建一个新层，该层将学习旧特征的知识并利用这些知识对新数据集进行预测。正如前面迁移学习步骤中进一步解释的那样，预训练模型的当前输出与模型希望得到的输出之间的差异可能很大，因此添加新层将整体上提高模型的性能。
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Transfer Learning for Image Recognition and Natural Language Processing](../Images/f61240edecb4a96c89b8cd19a90f78a3.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图像识别和自然语言处理的迁移学习](../Images/f61240edecb4a96c89b8cd19a90f78a3.png)'
- en: Fine-tuning
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调
- en: So after we have run the model with frozen layers, we need to run the model
    with the base models unfrozen, which essentially improves the model, with a lower
    learning rate. You want to reduce the amount of overfitting, so let’s take this
    step slowly and unfreeze the base model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now it’s time to compile the model again.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Transfer Learning for Image Recognition and Natural Language Processing](../Images/5cb503590a101193a082457b4d4319d9.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: If you would like to have a further gander, you can do so by clicking [here](https://colab.research.google.com/drive/10WqNas4nwDeBqpWzDgC098Fkzu8sbzLB?usp=sharing)
    to view the Colab notebook.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Natural Language Processing
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve spoken quite a bit about image classification, let’s now look into Natural
    Language Processing (NLP). So what is NLP? NLP is the ability of a computer to
    be able to detect and understand human language, through speech and text just
    the way we humans can.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: The human language contains many ambiguities that cause difficulty in being
    able to create software that can accurately detect speech and text. For example,
    sarcasm, grammar, and homophones, however, these are just a few bumps in learning
    the human language.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Examples of NLP are Speech Recognition and Sentiment Analysis. Use cases of
    NLP are things like spam detection. I know, you may have never thought of NLP
    being implemented into spam detection, but it is. NLP’s text classification has
    the ability to scan emails and detect language that indicates that it could be
    spam or phishing. It does this by analysing the overuse of bad grammar, threats,
    overuse of financial terminology, and more.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: So how does Transfer Learning work in NLP? Well, it basically works the same
    with Image Recognition. Training a natural language model can be quite hefty in
    price, it requires a lot of data and takes up a lot of training time on fancy
    hardware. But the good news is that, just like image recognition, you can download
    these pre-trained models for free along with fine-tuning them to work on your
    specific dataset.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Where can you find these pre-trained models, you ask?
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Keras does not only offer pre-trained models for Image recognition, it also
    provides architectures for NLP. You can have a gander [here](https://keras.io/examples/nlp/).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: HuggingFace
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s look into [HuggingFace](https://huggingface.co/). HuggingFace is an open-source
    provider of natural language processing (NLP) which has done an amazing job to
    make it user-friendly.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Their Transformers library is a python-based library that provides architectures
    such as BERT, that perform NLP tasks such as text classification and question
    answering. All you have to do is load their pre-trained models with just a few
    lines of code and you are ready to start experimenting. To get started with Transformers,
    you will need to install it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Below is an example of using sentiment analysis, which is the ability to be
    able to identify an opinion expressed in a piece of text.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Output:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Improving your pre-trained model with fine-tuning is an option with HuggingFace,
    using the Trainer API. Transformers prived a Trainer class which aids in fine-tuning
    your pre-trained model. This step comes after processing your data and requires
    you to import TrainingArguments
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 HuggingFace 对预训练模型进行微调是一种选择，可以使用 Trainer API。Transformers 提供了一个 Trainer 类来帮助微调你的预训练模型。这一步骤在处理数据之后进行，并且需要导入
    TrainingArguments。
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: To find out more about HuggingFace, you can follow this [link](https://huggingface.co/course/chapter3/3?fw=pt).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 HuggingFace 的信息，你可以访问这个[链接](https://huggingface.co/course/chapter3/3?fw=pt)。
- en: Word Embedding
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词嵌入
- en: How about Word Embedding? What’s that? Word embedding is a representation in
    which similar words have a similar encoding. It can be able to detect a word in
    a document and link its relation to other words.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是什么呢？词嵌入是一种表示方式，其中相似的词具有相似的编码。它可以在文档中检测到一个词，并将其与其他词的关系联系起来。
- en: An example of this is Word2Vec, which is a two-layer neural network that processes
    texts by vector encoding.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个例子是 Word2Vec，它是一个通过向量编码处理文本的两层神经网络。
- en: 'Word2Vec used two methods to learn word representation:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 使用了两种方法来学习词的表示：
- en: '**Continuous Bag-of-Words** - is a model architecture that tries to predict
    the target word (middle word) based on the source words (neighbouring words).
    The order of the words in the context is not necessarily important, the reason
    the architecture is called ‘bag-of-words'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**连续词袋模型（Continuous Bag-of-Words）** - 是一种模型架构，它试图基于源词（邻近词）预测目标词（中间词）。上下文中的词的顺序不一定重要，因此这个架构被称为“词袋模型”。'
- en: '**Continuous Skip-gram** - is a model architecture essentially the opposite
    of Bag-of-Words. It tries to predict the source words (neighbouring words) given
    a target word (middle word)'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**连续跳字模型（Continuous Skip-gram）** - 是一种模型架构，本质上是词袋模型的对立面。它试图给定目标词（中间词）来预测源词（邻近词）。'
- en: Feature extraction is a strategy used in Word Embedding, where it detects and
    produces feature representations that are useful for the type of NLP task you
    are trying to work on. In feature extraction, parts (sentences or words) are extracted
    to a matrix that has a representation of every word given of every word. The weights
    do not change and only the top layer of the model is used.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取是一种在词嵌入中使用的策略，它检测并生成对你正在处理的 NLP 任务有用的特征表示。在特征提取中，将部分（句子或词）提取到一个矩阵中，该矩阵包含每个词的表示。权重不会改变，仅使用模型的顶层。
- en: However, fine-tuning tweaks the pre-trained model’s weights, which can be a
    detriment as the method can cause a loss in words in the adjustment phase, due
    to the weights being changed and what was once learned, is no longer in the memory.
    This is known as ‘catastrophic interference’.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，微调会调整预训练模型的权重，这可能会带来不利影响，因为该方法在调整阶段可能导致词汇的丢失，原因在于权重被更改，曾经学到的内容不再存在于记忆中。这被称为“灾难性遗忘”。
- en: 'Example: How to use a pre-trained NLP model in Keras'
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例：如何在 Keras 中使用预训练的 NLP 模型
- en: 'We were just speaking on Word Embedding, so let’s look at an example. To quickly
    recap what Word Embedding is: it is a representation in which similar words have
    a similar encoding. It can be able to detect a word in a document and link its
    relation to other words. So let’s see how this works then.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚讨论了词嵌入（Word Embedding），让我们看一个例子。快速回顾一下词嵌入的定义：它是一种表示方式，其中相似的词具有相似的编码。它可以在文档中检测到一个词，并将其与其他词的关系联系起来。那么我们来看看它是如何工作的吧。
- en: Load the Data
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据
- en: In this example, we are going to look at movie reviews. If you remember the
    example we did above, using sentiment analysis, produced us an output determining
    how positive it was. With these movie reviews, we have positive and negative,
    therefore this is binary classification.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将查看电影评论。如果你还记得我们之前做的例子，使用情感分析得出了一个结果来确定其积极性。对于这些电影评论，我们有积极和消极的，因此这是二元分类。
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can see the distinction in the below image, 0 being negative and 1 being
    positive.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在下面的图像中看到区别，0 表示负面，1 表示正面。
- en: '![Transfer Learning for Image Recognition and Natural Language Processing](../Images/4657f2679672895c02e8d53998547640.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图像识别与自然语言处理的迁移学习](../Images/4657f2679672895c02e8d53998547640.png)'
- en: I then moved on to using Embedding Layer from Keras, to turn positive integers
    into dense vectors with fixed sizes. The weights for the embedding are randomly
    initialised and are adjusted during backpropagation. Once the knowledge is learnt,
    the word embeddings will encode similarities between words they have learnt and
    words they had learnt before.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我转向使用Keras的Embedding Layer，将正整数转换为固定大小的稠密向量。嵌入的权重是随机初始化的，并在反向传播过程中进行调整。一旦知识学会了，词嵌入将编码它们所学过的词与之前学过的词之间的相似性。
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Text pre-processing
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本预处理
- en: During the text pre-processing stage, we are going to initialize a TextVectorization
    layer with the desired parameters to vectorize movie reviews. The text vectorization
    layer will help split and map strings from the movie reviews into integers.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本预处理阶段，我们将初始化一个TextVectorization层，以期望的参数对电影评论进行向量化。文本向量化层将帮助拆分和映射来自电影评论的字符串到整数。
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Create a model
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建模型
- en: The vectorize_layer from the above pre-processing will be implemented as the
    first layer into the model, as it has transformed the strings into vocabulary
    indices. It will feed the transformed strings into the Embedding layer.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 上述预处理中的vectorize_layer将作为模型的第一层实现，因为它已经将字符串转换为词汇索引。它将把转换后的字符串输入到Embedding层中。
- en: The Embedding layer then takes these vocabulary indices and scans through the
    vector for each word index, where they are learning as the model trains.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，Embedding层会接受这些词汇索引，并扫描每个词索引的向量，在模型训练时进行学习。
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In this example, we are going to use [TensorBoard](https://www.tensorflow.org/tensorboard/get_started),
    which is a tool for providing visualisations showing machine learning workflow,
    for example loss and accuracy.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用[TensorBoard](https://www.tensorflow.org/tensorboard/get_started)，这是一个提供机器学习工作流可视化的工具，例如损失和准确率。
- en: Compile the Model
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编译模型
- en: We are going to compile the model using the Adam optimizer and BinaryCrossentropy
    loss.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Adam优化器和BinaryCrossentropy损失函数来编译模型。
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Transfer Learning for Image Recognition and Natural Language Processing](../Images/3752ccb3a402ba4dafec117e9e1222de.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图像识别和自然语言处理的迁移学习](../Images/3752ccb3a402ba4dafec117e9e1222de.png)'
- en: I mentioned the TensorBoard, which visualises the models metrics and you can
    see this below.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我提到了TensorBoard，它可视化模型指标，你可以在下面看到。
- en: '![Transfer Learning for Image Recognition and Natural Language Processing](../Images/ee91bb8de47eca756eb55dae91a67005.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图像识别和自然语言处理的迁移学习](../Images/ee91bb8de47eca756eb55dae91a67005.png)'
- en: You have seen how to train and test an NLP model using a pre-trained word embedding
    model. If you would like to have a further look at the Colab notebook, you can
    do so [here.](https://colab.research.google.com/drive/1Du5GMWWh8-a5UEXtdbaaDgHy71joSG-K?usp=sharing)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解了如何使用预训练的词嵌入模型训练和测试NLP模型。如果你想进一步查看Colab笔记本，你可以[在这里](https://colab.research.google.com/drive/1Du5GMWWh8-a5UEXtdbaaDgHy71joSG-K?usp=sharing)查看。
- en: What are the best practices with transfer learning?
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移学习的最佳实践是什么？
- en: '**Pre-trained models** - Take advantage of these pre-trained open-source models
    which can help you solve your target task as they cover different fields. It can
    save you a lot of time building a model from scratch.'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预训练模型** - 利用这些预训练的开源模型，它们可以帮助你解决目标任务，因为它们涵盖了不同的领域。这可以节省你从头开始构建模型的大量时间。'
- en: '**Source model** - finding a model that is compatible with both your source
    task and target task is important. If your source model is too far off from your
    target and little knowledge can get transferred, it will be more time-consuming
    as the target model will take longer to achieve.'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**源模型** - 找到一个与源任务和目标任务都兼容的模型是很重要的。如果源模型与你的目标差距过大，并且知识转移有限，目标模型将需要更长的时间来实现。'
- en: '**Overfitting** - Target tasks with a small data sample, where the source task
    and target task are too similar, this can lead to overfitting. Freezing layers
    and tuning the learning rate in the course model can help you reduce overfitting
    in your model.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**过拟合** - 如果目标任务的数据样本很小，且源任务与目标任务过于相似，这可能会导致过拟合。冻结层并调整课程模型中的学习率可以帮助你减少模型的过拟合。'
- en: I hope this article has given you a better understanding of how to implement
    transfer learning through types of Machine Learning. Try it out for yourself!
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章让你对如何通过机器学习的不同类型实现迁移学习有了更好的理解。试试看吧！
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and freelance Technical writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**[尼莎·阿雅](https://www.linkedin.com/in/nisha-arya-ahmed/)** 是一名数据科学家和自由技术撰稿人。她特别感兴趣于提供数据科学职业建议或教程以及围绕数据科学的理论知识。她还希望探讨人工智能如何以及如何能够提升人类生命的持久性。她是一个热衷学习者，寻求拓宽自己的技术知识和写作技能，同时帮助指导他人。'
- en: More On This Topic
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题
- en: '[N-gram Language Modeling in Natural Language Processing](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理中的 N-gram 语言建模](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
- en: '[25 Free Books to Master SQL, Python, Data Science, Machine…](https://www.kdnuggets.com/25-free-books-to-master-sql-python-data-science-machine-learning-and-natural-language-processing)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25 本免费书籍掌握 SQL、Python、数据科学、机器学习…](https://www.kdnuggets.com/25-free-books-to-master-sql-python-data-science-machine-learning-and-natural-language-processing)'
- en: '[Natural Language Processing Key Terms, Explained](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理关键术语解析](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
- en: '[Data Representation for Natural Language Processing Tasks](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理任务的数据表示](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
- en: '[How to Start Using Natural Language Processing With PyTorch](https://www.kdnuggets.com/2022/04/start-natural-language-processing-pytorch.html)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何开始使用 PyTorch 进行自然语言处理](https://www.kdnuggets.com/2022/04/start-natural-language-processing-pytorch.html)'
- en: '[A Gentle Introduction to Natural Language Processing](https://www.kdnuggets.com/2022/06/gentle-introduction-natural-language-processing.html)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理的温和入门](https://www.kdnuggets.com/2022/06/gentle-introduction-natural-language-processing.html)'
