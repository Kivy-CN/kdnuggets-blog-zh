- en: There is No Free Lunch in Data Science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/09/no-free-lunch-data-science.html](https://www.kdnuggets.com/2019/09/no-free-lunch-data-science.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Sydney Firmin](https://www.linkedin.com/in/sydney-firmin-4369a65b/),
    Alteryx**.'
  prefs: []
  type: TYPE_NORMAL
- en: During your adventures in machine learning, you may have already come across
    the “No Free Lunch” Theorem. Borrowing its name from the adage "[there ain’t no
    such thing as a free lunch](https://en.wikipedia.org/wiki/There_ain%27t_no_such_thing_as_a_free_lunch)," the [mathematical
    folklore theorem](https://en.wikipedia.org/wiki/Mathematical_folklore) describes
    the phenomena that there is no single algorithm that is best suited for all possible
    scenarios and data sets.
  prefs: []
  type: TYPE_NORMAL
- en: The No Free Lunch Theorem(s)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are, generally speaking, two No Free Lunch (NFL) theorems: one for machine
    learning and one for search and optimization. These two theorems are related and
    tend to be bundled into one general axiom (the folklore theorem).'
  prefs: []
  type: TYPE_NORMAL
- en: Although many different researchers have contributed to the collective publications
    on the No Free Lunch theorems, the most prevalent name associated with these works
    is David Wolpert.
  prefs: []
  type: TYPE_NORMAL
- en: '**No Free Lunch for Supervised Machine Learning**'
  prefs: []
  type: TYPE_NORMAL
- en: In his 1996 paper [The Lack of A Priori Distinctions Between Learning Algorithms](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.390.9412&rep=rep1&type=pdf),
    David Wolpert examines if it is possible to get useful theoretical results with
    a training data set and a learning algorithm without making any assumptions about
    the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: What Wolpert proves in his 1996 paper (through a series of mathematical proofs)
    is that given a noise-free data set (i.e., no random variation, only trend) and
    a machine learning algorithm where the cost function is the error rate, all machine
    learning algorithms are equivalent when assessed with a [generalization error
    rate](https://en.wikipedia.org/wiki/Generalization_error) (the model’s error rate
    on a validation data set).
  prefs: []
  type: TYPE_NORMAL
- en: Extending this logic (again, Wolpert does it with math equations), he demonstrates
    that for any two algorithms, A and B, there are as many scenarios where A will
    perform worse than B as there are where A will outperform B. This even holds true
    when one of the given algorithms is random guessing. Wolpert proved that for all
    possible domains (all possible problem instances drawn from a uniform probability
    distribution), the average performance for algorithms A and B is the same.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c6033a7e2b0fc288269ff8f6338bda7.png)'
  prefs: []
  type: TYPE_IMG
- en: This is because nearly all ([non-rote](https://en.wikipedia.org/wiki/Rote_learning#In_computer_science))
    machine learning algorithms make some assumptions (known as [inductive or learning
    bias](https://en.wikipedia.org/wiki/Inductive_bias)) about the relationships between
    the predictor and target variables, introducing [bias](https://en.wikipedia.org/wiki/Bias) into
    the model. The assumptions made by machine learning algorithms mean that some
    algorithms will fit certain data sets better than others. It also (by definition)
    means that there will be as many data sets that a given algorithm will not be
    able to model effectively. How effective a model will be is directly dependent
    on how well the assumptions made by the model fit the true nature of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Coming back to the lunch of it all, you can’t get good machine learning “for
    free.” You must use knowledge about your data and the context of the world we
    live in (or the world your data lives in) to select an appropriate machine learning
    model. There is no such thing as a single, universally-best machine learning algorithm,
    and there are no context or usage-independent ([a priori](https://en.wikipedia.org/wiki/A_priori_and_a_posteriori))
    reasons to favor one algorithm over all others.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact, this 1996 paper is a mathematical formalization of the work of the
    philosopher [David Hume](https://en.wikipedia.org/wiki/David_Hume). I promise
    I will circle back to this a little later.
  prefs: []
  type: TYPE_NORMAL
- en: '**No Free Lunch for Search and Optimization Either**'
  prefs: []
  type: TYPE_NORMAL
- en: Wolpert and Macready’s 1997 paper [No Free Lunch Theorems for Optimization](https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf) demonstrates
    the application of similar “no free lunch” logic to the mathematical fields of
    search and optimization.
  prefs: []
  type: TYPE_NORMAL
- en: The No Free Lunch theorems for search and optimization demonstrate that for
    search/optimization problems in a limited search space, where the points being
    searched through are *not* resampled, the performance of any two given search
    algorithms over all possible problems is the same.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, consider that you are on foot, looking for the highest point
    (extremum) on the earth. To find it, you would likely always make a choice to
    go in the direction that elevation increased, acting as a “hill-climber.”
  prefs: []
  type: TYPE_NORMAL
- en: This would work fine on Earth, where the highest point is a part of a very tall
    mountain range.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e1de396b0d18eb1387393ab0e1dbfdf1.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, imagine a world where the highest point on the planet is surrounded by
    the lowest points on the planet.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3da553d6b50555fa7eb0ae68a5a9a464.png)'
  prefs: []
  type: TYPE_IMG
- en: '*This guy is headed for disappointment.*'
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, a search where you always go down in elevation would perform
    better.
  prefs: []
  type: TYPE_NORMAL
- en: Neither hill climbing nor hill descending performs better on both planets, meaning
    that a random search would be just as good (on average). Without being able to
    assume that the highest point on a planet will occur next to other high points
    (a smooth surface), we can’t know that hill climbing will perform better than
    descending.
  prefs: []
  type: TYPE_NORMAL
- en: 'What this all boils down to: if no prior assumptions about the optimization
    program can be made, no optimization strategy can be expected to perform better
    than any other strategy (including random searching). A general purpose universal
    optimization strategy is theoretically impossible, and the only way one strategy
    can outperform another is for it to be specialized for the specific problem at
    hand. Sound familiar? There is no free efficient search optimization either.'
  prefs: []
  type: TYPE_NORMAL
- en: In his 2012 paper [What the No Free Lunch Theorems Really mean; How to Improve
    Search Algorithms](https://sfi-edu.s3.amazonaws.com/sfi-edu/production/uploads/sfi-com/dev/uploads/filer/33/44/33440e97-fe46-4827-a1eb-a27196e1c49a/12-10-017.pdf),
    Wolpert discusses the close relationship between search and supervised learning
    and its implications to the No Free Lunch theorems. He demonstrates that, in the
    context of the No Free Lunch theorem, supervised learning is closely analogous
    to search/optimization. For these two theorems, “search algorithm” is seemingly
    interchangeable with “learning algorithm” and “objective function” with “target
    function.”
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are interested in knowing more about each of the No Free Lunch theorems,
    there is a great summary resource on [http://www.no-free-lunch.org/](http://www.no-free-lunch.org/),
    including a [powerpoint deck](http://www.no-free-lunch.org/coev.pdf) from David
    Wolpert on an overview of the No Free Lunch Theorems. For a more philosophical
    explanation of the No Free Lunch Theorem(s) give the note [Notice: No Free Lunches
    for Anyone, Bayesians Included](http://www.no-free-lunch.org/Fors99.pdf) a read.'
  prefs: []
  type: TYPE_NORMAL
- en: Are We Sure About No Free Lunch?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The No Free Lunch theorems have sparked a lot of research and academic publications.
    These publications are not always in favor of No Free Lunch (because who doesn’t
    want a free lunch)?
  prefs: []
  type: TYPE_NORMAL
- en: The “all possible problems” component of the no free lunch theorems is the first
    sticking point. All possible problems don’t reflect the conditions of the real
    world. In fact, many would argue that problems taken on by machine learning and
    optimization, “real world” problems, are nothing like many of the domains included
    in all problems. The real world tends to exhibit patterns and structures, where
    the “all possible problems” space includes scenarios that are entirely random
    or chaotic.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers also argue [against taking a uniform average](https://pdfs.semanticscholar.org/83cd/86c2c7e507e8ebba9563a9efaba7c966a1b3.pdf) of
    performance on philosophical grounds. If you have one hand in boiling water, and
    one hand froze in an ice block, on average the temperature of your hands is fine.
    Averaging performance of algorithms might cause trouble where one algorithm performs
    well on most problems but epically fails on a few edge cases.
  prefs: []
  type: TYPE_NORMAL
- en: In empirical research, it has been demonstrated that some algorithms consistently
    perform better than others.  In the 2014 paper [Do we Need Hundreds of Classifiers
    to Solve Real-World Classification Problems](http://jmlr.org/papers/volume15/delgado14a/delgado14a.pdf),
    researchers found that for all of the data sets in the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.html?format=&task=&att=&area=life&numAtt=&numIns=&type=&sort=nameUp&view=table) (121
    data sets), the family of classifiers that perform best includes random forest,
    support vector machines (SVM), neural networks, and boosting ensembles.
  prefs: []
  type: TYPE_NORMAL
- en: This is an important consideration, as it does seem that some algorithms seem
    to consistently top the Kaggle leaderboards or get implemented into production
    environments more than others. However, the No Free Lunch theorem is an important
    reminder about the assumptions each model makes, and the (often subconscious)
    assumptions we make about data and the environment it was gathered in.
  prefs: []
  type: TYPE_NORMAL
- en: '**No Free Lunch and David Hume**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember when (back about 800 words) I promised I would circle back to the old
    dead philosopher? This is that section.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8e5ea3f3b34dde006bceb3c1d54f818f.png)'
  prefs: []
  type: TYPE_IMG
- en: '*This guy…*'
  prefs: []
  type: TYPE_NORMAL
- en: '[David Hume](https://en.wikipedia.org/wiki/David_Hume) was an 18^(th)-century philosopher
    from Edinburgh, particularly interested in skepticism and empiricism. He did a
    lot of thinking about the [problem of induction](https://en.wikipedia.org/wiki/Problem_of_induction),
    which falls under the field of epistemology (the study of knowledge).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get into the problem of induction, let’s first split the world into two
    different types of knowledge: a priori and a posteriori (this division is going
    back to the work of [Immanuel Kant](https://en.wikipedia.org/wiki/Immanuel_Kant)).'
  prefs: []
  type: TYPE_NORMAL
- en: A priori knowledge is knowledge that exists independently of experience. This
    is the type of knowledge that is based on the relation of ideas, for example,
    2 + 2 = 4, or an octagon has eight sides. This type of knowledge will be true
    regardless of what the world is like outside.
  prefs: []
  type: TYPE_NORMAL
- en: A posteriori knowledge or “matters of fact” are types of knowledge that require
    experience or empirical evidence. This is the type of knowledge that makes up
    most personal and scientific knowledge. A posteriori knowledge requires observation.
  prefs: []
  type: TYPE_NORMAL
- en: '[Inductive reasoning](https://en.wikipedia.org/wiki/Inductive_reasoning) is
    a method of reasoning for a posteriori knowledge in which we apply observed regular
    patterns to instances we haven’t directly observed; e.g., all rain I have observed
    is wet, therefore, all rain I haven’t observed must be wet, too.'
  prefs: []
  type: TYPE_NORMAL
- en: With induction, we are implicitly assuming the [uniformity of nature](https://en.wikipedia.org/wiki/Uniformitarianism) (sometimes
    called the Uniformity Principle or Resemblance Principle) and that the future
    will always resemble the past.
  prefs: []
  type: TYPE_NORMAL
- en: Science is entirely based on inductive reasoning. Without assumptions like the
    uniformity of nature, no algorithm (including high-level algorithms like the [scientific
    method](https://en.wikipedia.org/wiki/Scientific_method) or [cross-validation
    routines](https://en.wikipedia.org/wiki/Cross-validation_(statistics))) can be *proven* to
    perform better than random guessing.
  prefs: []
  type: TYPE_NORMAL
- en: Hume argued that inductive reasoning and the belief in causality *cannot* be
    rationally justified. There is no reasoning behind assuming uniformity. According
    to Hume, induction is instinctive for humans, like how dogs seem to instinctively
    chase rabbits and squirrels. Hume doesn’t really suggest a solution or justification
    for induction -- he kind of just shrugs and says, we can’t really help the way
    we are, so let’s not worry about it. (I am imagining the shrugging.)
  prefs: []
  type: TYPE_NORMAL
- en: 'David Wolpert’s work on the No Free Lunch theorem for supervised machine learning
    is a formalization of Hume. Wolpert even opens his 1996 paper with this quote
    from David Hume:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Even after the observation of the frequent conjunction of objects, we have
    no reason to draw any inference concerning any object beyond those of which we
    have had experience.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- David Hume, in A Treatise of Human Nature, Book I, part 3, Section 12.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Science (and machine learning models) inherently believe that under the same
    conditions everything always happens the same way. There is no reason to assume
    that because a model worked well on one data set, it will work well on other data
    sets. Additionally, science, statistics, and machine learning models cannot give
    guarantees about future experiments based on the results of previous experiments.
    No system can give guarantees about prediction, control, or observation in any
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/445f511b5797a6bc7b1279c8f02b7d0d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you’ve enjoyed the philosophy detour, and want more information on Hume
    and the problem of induction, there is a great video on Khan Academy called [Hume:
    Skepticism and Induction, Part 2](https://www.khanacademy.org/partner-content/wi-phi/wiphi-history/wiphi-hume/v/humes-skepticism-and-induction-part-2) ([Part
    1](https://www.khanacademy.org/partner-content/wi-phi/wiphi-history/wiphi-hume/v/humes-skepticism-part-1) is
    about a priori vs. a posteriori knowledge).'
  prefs: []
  type: TYPE_NORMAL
- en: The No Free Lunch Theorem and You
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you’ve stuck with me this long, you’re probably looking for an explanation
    of what this all means to you. The two most important things to take away from
    the No Free Lunch theorems are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Always check your assumptions before relying on a model or search algorithm.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**There is no “super algorithm” that will work perfectly for all datasets.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The No Free Lunch theorems were not written to tell you what to do in different
    scenarios. The No Free Lunch theorems were specifically written to counter claims
    along the lines of:'
  prefs: []
  type: TYPE_NORMAL
- en: My machine learning algorithm/optimization strategy is *the best, always and
    forever*, for all the scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Models are simplifications of a specific component of reality (observed with
    data). To simplify reality, a machine learning algorithm or statistical model
    needs to make assumptions and introduce bias (known specifically as [inductive
    or learning bias](https://en.wikipedia.org/wiki/Inductive_bias)). Bias-free learning
    is futile because a learner that makes no a priori assumptions will have no rational
    basis for creating estimates when provided new, unseen input data. The assumptions
    of an algorithm will work for some data sets but fail for others. This phenomenon
    is important to understand the concepts of [underfitting](https://en.wikipedia.org/wiki/Overfitting#Underfitting) and [the
    bias/variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff).
  prefs: []
  type: TYPE_NORMAL
- en: The combination of your data and a randomly selected machine learning model
    are not enough to make accurate or meaningful predictions about the future or
    unknown outcomes. You, the human, will need to make assumptions about the nature
    of your data and the world we live in. Playing an active role in making assumptions
    will only strengthen your models and make them more useful, [even if they are
    wrong](https://community.alteryx.com/t5/Data-Science-Blog/All-Models-Are-Wrong/ba-p/348080).
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://community.alteryx.com/t5/Data-Science-Blog/There-is-No-Free-Lunch-in-Data-Science/ba-p/347402).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: **A geographer by training and a data geek at heart, [Sydney Firmin](https://www.linkedin.com/in/sydney-firmin-4369a65b/)
    strongly believes that data and knowledge are most valuable when they can be clearly
    communicated and understood. In her current role as a Sr. Data Science Content
    Engineer, she gets to spend her days doing what she loves best; transforming technical
    knowledge and research into engaging, creative, and fun content for the Alteryx
    Community.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[10 Gradient Descent Optimisation Algorithms + Cheat Sheet](https://www.kdnuggets.com/2019/06/gradient-descent-algorithms-cheat-sheet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How Optimization Works](https://www.kdnuggets.com/2019/04/how-optimization-works.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Supervised vs. Unsupervised Learning](https://www.kdnuggets.com/2018/04/supervised-vs-unsupervised-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
