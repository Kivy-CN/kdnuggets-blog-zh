- en: How to Create Unbiased Machine Learning Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何创建无偏机器学习模型
- en: 原文：[https://www.kdnuggets.com/2021/07/create-unbiased-machine-learning-models.html](https://www.kdnuggets.com/2021/07/create-unbiased-machine-learning-models.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/07/create-unbiased-machine-learning-models.html](https://www.kdnuggets.com/2021/07/create-unbiased-machine-learning-models.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By Philip Tannor, Co-Founder & CEO of [Deepchecks](https://deepchecks.com/)**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：Philip Tannor，[Deepchecks](https://deepchecks.com/)的联合创始人兼首席执行官**。'
- en: '![Figure](../Images/f109257a81bf17a819ae1929ce210afe.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/f109257a81bf17a819ae1929ce210afe.png)'
- en: '*Image by* [*Clker-Free-Vector-Images*](https://pixabay.com/users/clker-free-vector-images-3736/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=307248)
    *from* [*Pixabay*](https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=307248)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片由* [*Clker-Free-Vector-Images*](https://pixabay.com/users/clker-free-vector-images-3736/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=307248)
    *提供，来源于* [*Pixabay*](https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=307248)'
- en: '* * *'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的IT需求'
- en: '* * *'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: AI systems are becoming increasingly popular and central in many industries.
    They decide who might get a loan from the bank, whether an individual should be
    convicted, and we may even entrust them with our lives when using systems such
    as autonomous vehicles in the near future. Thus, there is a growing need for mechanisms
    to harness and control these systems so that we may ensure that they behave as
    desired.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: AI系统在许多行业中变得越来越流行和核心。它们决定谁可能从银行获得贷款，个人是否应被定罪，甚至在不久的将来，我们可能会将生命托付给使用自动驾驶系统等技术。因此，迫切需要机制来驾驭和控制这些系统，以确保它们按预期行为。
- en: One important issue that has been gaining popularity in the last few years is
    *fairness*. While usually ML models are evaluated based on metrics such as accuracy,
    the idea of fairness is that we must ensure that our models are unbiased with
    regard to attributes such as gender, race and other selected attributes.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个近年来越来越受到关注的重要问题是*公平性*。虽然通常机器学习模型的评估基于准确性等指标，但公平性的观点是，我们必须确保我们的模型在性别、种族及其他选定属性上不带有偏见。
- en: A classic example of an episode regarding racial bias in AI systems, is the
    COMPAS software system, developed by Northpointe, which aims to assist US courts
    with assessing the likelihood of a defendant becoming a recidivist. Propublica
    published an [article](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)
    which claims that this system is biased against blacks, giving them higher risk
    ratings.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经典的关于AI系统种族偏见的例子是COMPAS软件系统，该系统由Northpointe开发，旨在协助美国法院评估被告成为再犯的可能性。Propublica发布了一篇[文章](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)，称该系统对黑人有偏见，给他们更高的风险评级。
- en: '![Figure](../Images/286f789c76493394720b3c8cbe64ca84.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/286f789c76493394720b3c8cbe64ca84.png)'
- en: '*ML system bias against African Americans? (*[*source*](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)*)*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*机器学习系统对非裔美国人的偏见？ (*[*来源*](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)*)*'
- en: In this post we will try to understand where biases in ML models originate,
    and explore methods for creating unbiased models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将尝试理解机器学习模型中的偏见从何而来，并探索创建无偏模型的方法。
- en: Where Does Bias Come From?
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 偏见从何而来？
- en: '"Humans are the weakest link"'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “人类是最薄弱的环节”
- en: —Bruce Schneier
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —布鲁斯·施奈尔
- en: In the field of Cybersecurity it is often said that “humans are the weakest
    link” (Schneier). This idea applies in our case as well. Biases are in fact introduced
    into ML models by humans unintentionally.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络安全领域，常常说“人是最弱的环节”（Schneier）。这个观点在我们的案例中同样适用。实际上，偏见是由人类无意中引入到机器学习模型中的。
- en: Remember, an ML model can only be as good as the data it’s trained on, and thus
    if the training data contains biases, we can expect our model to mimic those same
    biases. Some representative examples for this can be found in the field of word
    embeddings in NLP. Word embeddings are learned dense vector representations of
    words, that are meant to capture semantic information of a word, which can then
    be fed to ML models for different downstream tasks. Thus, embeddings of words
    with similar meanings are expected to be “close” to each other.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，机器学习模型的好坏取决于其训练数据，因此如果训练数据包含偏见，我们可以预期我们的模型也会模仿这些偏见。在自然语言处理（NLP）领域的词嵌入（word
    embeddings）中可以找到一些具有代表性的例子。词嵌入是学习得到的词的稠密向量表示，旨在捕捉一个词的语义信息，然后可以将这些信息输入到机器学习模型中用于不同的下游任务。因此，具有相似含义的词的嵌入预计应该是“接近”的。
- en: '![Figure](../Images/226ab20e7f0444a5cf42625469fff5d4.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/226ab20e7f0444a5cf42625469fff5d4.png)'
- en: '*Word embeddings can capture the semantic meaning of words. (*[*source*](https://www.ibm.com/blogs/research/2018/11/word-movers-embedding/)*)*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*词嵌入可以捕捉词的语义含义。 (*[*来源*](https://www.ibm.com/blogs/research/2018/11/word-movers-embedding/)*)*'
- en: It turns out that the embedded space can be used to extract relations between
    words, and to find analogies as well. A classic example for this is the [well
    known](https://www.technologyreview.com/2015/09/17/166211/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/)
    king-man+woman=queen equation. However, if we substitute the word “doctor” for
    the word “king” we get “nurse” as the female equivalent of the “doctor”. This
    undesired result simply reflects existing gender biases in our society and history.
    If in most available texts doctors are generally male and nurses are generally
    female, that’s what our model will understand.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，嵌入空间可以用来提取词与词之间的关系，也可以用来找到类比。例如著名的[king-man+woman=queen](https://www.technologyreview.com/2015/09/17/166211/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/)方程。如果我们将“doctor”替换为“king”，我们会得到“nurse”作为“doctor”的女性等价词。这一不良结果简单地反映了我们社会和历史中的性别偏见。如果在大多数现有文本中，医生通常是男性，护士通常是女性，那么我们的模型也会这样理解。
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Code example:* *man* *is to* *doctor* *as* *woman* *is to* *nurse* *according
    to gensim word2vec* *(*[*source*](https://colab.research.google.com/drive/165qN7RfKByFDlWB6m-E5gRvcSEEodGV0?usp=sharing)*)*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*代码示例：* *man* *对* *doctor* *的关系就像* *woman* *对* *nurse* *的关系* *（根据 gensim word2vec）*
    *(*[*来源*](https://colab.research.google.com/drive/165qN7RfKByFDlWB6m-E5gRvcSEEodGV0?usp=sharing)*)*'
- en: Culture Specific Tendencies
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文化特定倾向
- en: Currently, the most used language on the internet is [English](https://www.statista.com/statistics/262946/share-of-the-most-common-languages-on-the-internet/#:~:text=As%20of%20January%202020%2C%20English,percent%20of%20global%20internet%20users.).
    Much of the research and products in the field of Data Science and ML is done
    in English as well. Thus, many of the “natural” datasets that are used to create
    huge language models tend to match American thought and culture, and may be biased
    towards other nationalities and cultures.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，互联网上使用最多的语言是[英语](https://www.statista.com/statistics/262946/share-of-the-most-common-languages-on-the-internet/#:~:text=As%20of%20January%202020%2C%20English,percent%20of%20global%20internet%20users.)。数据科学和机器学习领域的大部分研究和产品也是用英语进行的。因此，许多用于创建大型语言模型的“自然”数据集往往反映了美国的思想和文化，并且可能对其他国籍和文化存在偏见。
- en: '![Figure](../Images/3a6bd2ee7625df70901f1fe33257b7ca.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/3a6bd2ee7625df70901f1fe33257b7ca.png)'
- en: '*Cultural bias: GPT-2 needs active steering in order to produce a positive
    paragraph with the given prompt. (*[*source*](https://blog.einstein.ai/gedi/)*)*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*文化偏见：GPT-2 需要积极引导才能在给定的提示下生成正面的段落。 (*[*来源*](https://blog.einstein.ai/gedi/)*)*'
- en: Synthetic Datasets
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 合成数据集
- en: Some biases in the data may be created unintentionally in the process of the
    dataset’s construction. During construction and evaluation people are more likely
    to notice and pay attention to details they are familiar with. A well known example
    for an image classification mistake, is when Google Photos [misclassified black
    people as gorillas](https://www.wsj.com/articles/BL-DGB-42522). While a single
    misclassification of this sort may not have a strong impact on the overall evaluation
    metrics, it is a sensitive issue, and could have a large impact on the product
    and the way customers relate to it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中的一些偏见可能是在数据集构建过程中无意中产生的。在构建和评估过程中，人们更可能注意到他们熟悉的细节。一个众所周知的图像分类错误例子是谷歌照片 [将黑人误分类为猩猩](https://www.wsj.com/articles/BL-DGB-42522)。虽然这种单一的误分类可能对整体评估指标没有强烈影响，但这是一个敏感问题，并可能对产品及客户对产品的看法产生重大影响。
- en: '![Figure](../Images/05cedf7c97a4e086de2b0fa7ba670e26.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/05cedf7c97a4e086de2b0fa7ba670e26.png)'
- en: '*Racist AI algorithm? Misclassification of black people as gorillas. (*[*source*](https://www.wsj.com/articles/BL-DGB-42522)*)*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*种族主义AI算法？将黑人误分类为猩猩。(*[*source*](https://www.wsj.com/articles/BL-DGB-42522)*)*'
- en: In conclusion, no dataset is perfect. Whether a dataset is handcrafted or “natural”,
    it is likely to reflect the biases of it’s creators, and thus the resulting model
    will contain the same biases as well.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，没有哪个数据集是完美的。无论数据集是手工制作的还是“自然”的，它都可能反映其创建者的偏见，因此结果模型也会包含相同的偏见。
- en: Creating fair ML models
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建公平的机器学习模型
- en: There are multiple proposed methods for creating fair ML models, which generally
    fall into one of the following stages.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 创建公平的机器学习模型有多种方法，这些方法通常属于以下阶段之一。
- en: Preprocessing
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理
- en: A naive approach to creating ML models that are unbiased with respect to sensitive
    attributes is to simply remove these attributes from the data, so that the model
    cannot use them for its prediction. However, it is not always straightforward
    to divide attributes into clear cut categories. For example, a person’s name may
    be correlated with their gender or ethnicity, nevertheless we would not necessarily
    want to regard this attribute as sensitive. More sophisticated approaches attempt
    to use dimensionality reduction techniques in order to eliminate sensitive attributes.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 创建对敏感属性不偏见的机器学习模型的一个简单方法是从数据中去除这些属性，以便模型无法利用这些属性进行预测。然而，将属性划分为明确的类别并不总是很简单。例如，一个人的名字可能与其性别或种族相关，但我们不一定希望将这一属性视为敏感属性。更复杂的方法尝试使用降维技术来消除敏感属性。
- en: At Training Time
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在训练时
- en: An elegant method for [creating unbiased ML models](https://deepchecks.com/how-to-create-unbiased-ml-models/)
    is using adversarial debiasing. In this method we simultaneously train two models.
    The adversary model is trained to predict the protected attributes given the predictors
    prediction or hidden representation. The predictor is trained to succeed on the
    original task while making the adversary fail, thus minimizing the bias.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[创建无偏见的机器学习模型](https://deepchecks.com/how-to-create-unbiased-ml-models/)的一个优雅方法是使用对抗去偏见。在这种方法中，我们同时训练两个模型。对抗模型被训练以预测受保护的属性，而预测器被训练以在原始任务上取得成功，同时使对抗模型失败，从而最小化偏见。'
- en: '![Figure](../Images/ac04940d304eced71c619b269d1c2468.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/ac04940d304eced71c619b269d1c2468.png)'
- en: '*Adversarial debiasing illustration: The predictor loss function consists of
    two terms, the predictor loss, and the adversarial loss. (*[*source*](https://arxiv.org/pdf/1801.07593.pdf)*)*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*对抗去偏见的示意图：预测器损失函数由两个部分组成，即预测器损失和对抗损失。(*[*source*](https://arxiv.org/pdf/1801.07593.pdf)*)*'
- en: This method can achieve great results for debiasing models without having to
    “throw away” the input data, however, it may suffer from difficulties that arise
    in general when training adversarial networks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以在不“丢弃”输入数据的情况下取得很好的去偏见效果，但它可能会遇到在训练对抗网络时通常出现的困难。
- en: Post Processing
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 后处理
- en: In the post processing stage we get the model’s predictions as probabilities,
    but we can still choose how to act based on these outputs, for example we can
    move the decision threshold for different groups in order to meet our fairness
    requirements.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在后处理阶段，我们获得模型的预测结果作为概率，但我们仍然可以根据这些输出选择如何行动，例如，我们可以为不同的群体调整决策阈值，以满足我们的公平性要求。
- en: One way to ensure model fairness in the post processing stage is to look at
    the intersection of the area under the ROC curve for all groups. The intersection
    represents TPRs and FPRs that can be achieved for all classes simultaneously.
    Note that in order to satisfy the desired result of  equal TPRs and FPRs for all
    classes one might need to purposefully choose to get less good results on some
    of the classes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 确保模型公平性的一种后处理方法是查看所有组的ROC曲线下的交集。这个交集代表了所有类别可以同时实现的TPR和FPR。请注意，为了满足所有类别的TPR和FPR相等的期望结果，可能需要故意选择在某些类别上得到较差的结果。
- en: '![Figure](../Images/1bd7e256c87e70aaceaf57dc23972269.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/1bd7e256c87e70aaceaf57dc23972269.png)'
- en: '*The colored region is what’s achievable while fulfilling the separability
    criterion for fairness. (*[*source*](https://fairmlbook.org/classification.html)*)*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*彩色区域是实现公平性分离标准时所能达到的范围。 (*[*来源*](https://fairmlbook.org/classification.html)*)*'
- en: 'Another method for debiasing a model in the post processing stage involves
    *calibrating* the predictions for each class independently. *Calibration* is a
    method for ensuring that the probability outputs of a classification model indeed
    reflect the matching ratio of positive labels. Formally, a classification model
    is calibrated if for each value of r:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种在后处理阶段去偏差的方法涉及*独立校准*每个类别的预测。*校准*是一种确保分类模型的概率输出确实反映正标签匹配比例的方法。形式上，如果对于每个r值：
- en: '![Equation](../Images/4261c701bf4517a4b0357b5fd7d1157d.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![方程](../Images/4261c701bf4517a4b0357b5fd7d1157d.png)'
- en: When a model is properly calibrated error rates will be similar across the different
    values of protected attributes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型经过适当校准后，不同保护属性值的误差率会相似。
- en: Conclusion
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: To sum it up, we have discussed the concepts of *bias* and *fairness* in the
    ML world, we have seen that model biases often reflect existing biases in society.
    There are various ways in which we could enforce and test for fairness in our
    models, and hopefully, using these methods will lead to more just decision making
    in AI assisted systems around the world.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们讨论了机器学习世界中的*偏差*和*公平性*概念，我们看到模型的偏差通常反映了社会中的现有偏差。我们可以采取多种方法来强制执行和测试模型的公平性，希望使用这些方法能在全球范围内的人工智能辅助系统中实现更公正的决策。
- en: Further Reading
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[Gender bias in word embeddings](https://towardsdatascience.com/gender-bias-word-embeddings-76d9806a0e17)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[词嵌入中的性别偏差](https://towardsdatascience.com/gender-bias-word-embeddings-76d9806a0e17)'
- en: '[Propublica article](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[Propublica 文章](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)'
- en: Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Langford, & Hanna Wallach.
    (2018). A Reductions Approach to Fair Classification.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Langford, & Hanna Wallach.
    (2018). 公平分类的减少方法。
- en: Brian Hu Zhang, Blake Lemoine, & Margaret Mitchell. (2018). Mitigating Unwanted
    Biases with Adversarial Learning.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Brian Hu Zhang, Blake Lemoine, & Margaret Mitchell. (2018). 使用对抗学习减轻不必要的偏差。
- en: Solon Barocas, Moritz Hardt, & Arvind Narayanan (2019). *Fairness and Machine
    Learning*. fairmlbook.org.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Solon Barocas, Moritz Hardt, & Arvind Narayanan (2019). *公平性与机器学习*。fairmlbook.org。
- en: Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, & Aram Galstyan.
    (2019). A Survey on Bias and Fairness in Machine Learning.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, & Aram Galstyan.
    (2019). 关于机器学习中的偏差和公平性的调查。
- en: '**Bio: Philip Tannor** is Co-Founder & CEO of [Deepchecks](https://deepchecks.com/).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介：Philip Tannor** 是 [Deepchecks](https://deepchecks.com/) 的联合创始人兼首席执行官。'
- en: '**Related:**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Data Protection Techniques Needed to Guarantee Privacy](/2020/10/data-protection-techniques-guarantee-privacy.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[保证隐私所需的数据保护技术](/2020/10/data-protection-techniques-guarantee-privacy.html)'
- en: '[What Makes AI Trustworthy?](/2021/05/what-makes-ai-trustworthy.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么使人工智能值得信赖？](/2021/05/what-makes-ai-trustworthy.html)'
- en: '[Ethics, Fairness, and Bias in AI](/2021/06/ethics-fairness-ai.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能中的伦理、公平性与偏差](/2021/06/ethics-fairness-ai.html)'
- en: More On This Topic
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Building Data Pipelines to Create Apps with Large Language Models](https://www.kdnuggets.com/building-data-pipelines-to-create-apps-with-large-language-models)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[构建数据管道以创建大型语言模型应用](https://www.kdnuggets.com/building-data-pipelines-to-create-apps-with-large-language-models)'
- en: '[How to Create a Dataset for Machine Learning](https://www.kdnuggets.com/2022/02/create-dataset-machine-learning.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何为机器学习创建数据集](https://www.kdnuggets.com/2022/02/create-dataset-machine-learning.html)'
- en: '[How to Create a Sampling Plan for Your Data Project](https://www.kdnuggets.com/2022/11/create-sampling-plan-data-project.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何为你的数据项目制定采样计划](https://www.kdnuggets.com/2022/11/create-sampling-plan-data-project.html)'
- en: '[Create Efficient Combined Data Sources with Tableau](https://www.kdnuggets.com/2022/05/create-efficient-combined-data-sources-tableau.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Tableau 创建高效的组合数据源](https://www.kdnuggets.com/2022/05/create-efficient-combined-data-sources-tableau.html)'
- en: '[Use your Data Science Skills to Create 5 Streams of Income](https://www.kdnuggets.com/2023/03/data-science-skills-create-5-streams-income.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用你的数据科学技能创建 5 个收入来源](https://www.kdnuggets.com/2023/03/data-science-skills-create-5-streams-income.html)'
- en: '[Create a Time Series Ratio Analysis Dashboard](https://www.kdnuggets.com/2023/06/wolfer-create-time-series-ratio-analysis-dashboard.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[创建时间序列比率分析仪表盘](https://www.kdnuggets.com/2023/06/wolfer-create-time-series-ratio-analysis-dashboard.html)'
