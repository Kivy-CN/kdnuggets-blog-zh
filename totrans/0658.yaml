- en: A 2019 Guide to Object Detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/08/2019-guide-object-detection.html](https://www.kdnuggets.com/2019/08/2019-guide-object-detection.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)![Figure](../Images/f8b967b1fdf10c9ed86a20319f500993.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Photo by [Fernando @cferdo](https://unsplash.com/@dearferdo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/search/photos/objects?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Object detection is a [computer vision](https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b) technique
    whose aim is to detect objects such as cars, buildings, and human beings, just
    to mention a few. The objects can generally be identified from [either pictures
    or video feeds](https://heartbeat.fritz.ai/detecting-objects-in-videos-and-camera-feeds-using-keras-opencv-and-imageai-c869fe1ebcdb).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[Object detection](https://www.fritz.ai/features/object-detection.html) has
    been applied widely in video surveillance, self-driving cars, and object/[people
    tracking](https://heartbeat.fritz.ai/real-time-person-tracking-on-the-edge-with-a-raspberry-pi-93ae636af9fa).
    In this piece, we’ll look at the basics of object detection and review some of
    the most commonly-used algorithms and a few brand new approaches, as well.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: How Object Detection Works
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Object detection locates the presence of an object in an image and draws a
    bounding box around that object. This usually involves two processes; classifying
    and object’s type, and then drawing a box around that object. We’ve [covered image
    classification before](https://heartbeat.fritz.ai/a-beginners-guide-to-convolutional-neural-networks-cnn-cf26c5ee17ed),
    so let’s now review some of the common model architectures used for object detection:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[R-CNN](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3#5401)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Fast R-CNN](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3#62df)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Faster R-CNN](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3#6c29)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mask R-CNN](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3#8300)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SSD (Single Shot MultiBox Defender)](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3#b49b)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[YOLO (You Only Look Once)](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3#3837)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[YOLO（你只看一次）](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3#3837)'
- en: '[Objects as Points](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3#047e)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[将对象视为点](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3#047e)'
- en: '[Data Augmentation Strategies for Object Detection](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3#2836)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用于目标检测的数据增强策略](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3#2836)'
- en: R-CNN Model
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: R-CNN模型
- en: 'This technique combines two main approaches: applying high-capacity convolutional
    neural networks to bottom-up region proposals so as to localize and segment objects;
    and supervised pre-training for auxiliary tasks.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术结合了两种主要方法：对自底向上的区域提议应用高容量卷积神经网络以定位和分割对象；以及对辅助任务的监督预训练。
- en: '**[Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524?source=post_page---------------------------)**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**[用于准确目标检测和语义分割的丰富特征层次结构](https://arxiv.org/abs/1311.2524?source=post_page---------------------------)**'
- en: Object detection performance, as measured on the canonical PASCAL VOC dataset,
    has plateaued in the last few years. The...
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典的PASCAL VOC数据集上测量的目标检测性能在过去几年已趋于平稳。...
- en: This is followed by domain-specific fine-tuning that yields a high-performance
    boost. The authors of this paper named the algorithm R-CNN (Regions with CNN features)
    because it combines regional proposals with [convolutional neural networks](https://heartbeat.fritz.ai/a-beginners-guide-to-convolutional-neural-networks-cnn-cf26c5ee17ed).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接着进行领域特定的微调，以获得显著的性能提升。本文作者将算法命名为R-CNN（具有CNN特征的区域），因为它将区域提议与[卷积神经网络](https://heartbeat.fritz.ai/a-beginners-guide-to-convolutional-neural-networks-cnn-cf26c5ee17ed)结合起来。
- en: '![Figure](../Images/59b06d6c6e92c9bbfae706f2f1222ebd.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/59b06d6c6e92c9bbfae706f2f1222ebd.png)'
- en: '[source](https://arxiv.org/pdf/1311.2524.pdf)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1311.2524.pdf)'
- en: This model takes an image and extracts about 2000 bottom-up region proposals.
    It then computes the features for each proposal using a large CNN. Thereafter,
    it classifies each region using class-specific linear Support Vector Machines
    (SVMs). This model achieves a mean average precision of 53.7% on [PASCAL VOC 2010](http://host.robots.ox.ac.uk/pascal/VOC/voc2010/index.html).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型接收一张图像，并提取大约2000个自底向上的区域提议。然后，它使用大型CNN计算每个提议的特征。之后，它使用特定类别的线性支持向量机（SVMs）对每个区域进行分类。这个模型在[PASCAL
    VOC 2010](http://host.robots.ox.ac.uk/pascal/VOC/voc2010/index.html)上实现了53.7%的平均精度。
- en: The object detection system in this model has three modules. The first one is
    responsible for generating category-independent regional proposals that define
    the set of candidate detectors available to the model’s detector. The second module
    is a large convolutional neural network responsible for extracting a fixed-length
    feature vector from each region. The third module consists of a class of support
    vector machines.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的目标检测系统有三个模块。第一个模块负责生成类别无关的区域提议，定义模型检测器可用的候选检测器集合。第二个模块是一个大型卷积神经网络，负责从每个区域提取固定长度的特征向量。第三个模块由一类支持向量机组成。
- en: '![Figure](../Images/d24a28d097e1712f433b14bf995bb069.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/d24a28d097e1712f433b14bf995bb069.png)'
- en: '[source](https://arxiv.org/pdf/1311.2524.pdf)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1311.2524.pdf)'
- en: This model uses selective search to generate regional categories. Selective
    search groups regions that are similar based on color, texture, shape, and size.
    For feature extraction, the model uses a 4096-dimensional feature vector by applying
    the Caffe CNN implementation on each regional proposal. Forward propagating a
    227 × 227 RGB image through five convolutional layers and two fully connected
    layers computes the features. The model explained in this paper achieves a 30%
    relative improvement over the previous results on PASCAL VOC 2012.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型使用选择性搜索生成区域类别。选择性搜索根据颜色、纹理、形状和大小对相似的区域进行分组。对于特征提取，模型通过在每个区域提议上应用Caffe CNN实现，使用4096维特征向量。通过五个卷积层和两个全连接层前向传播一个227
    × 227的RGB图像来计算特征。本文中解释的模型在PASCAL VOC 2012上的相对改进达到30%。
- en: 'Some of the drawbacks of R-CNN are:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN的一些缺点包括：
- en: '**Training is a multi-stage pipeline.** Tuning a convolutional neural network
    on object proposals, fitting SVMs to the ConvNet features, and finally learning
    bounding box regressors.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练是一个多阶段管道。** 调整卷积神经网络以适应目标提议，将SVMs拟合到ConvNet特征，最后学习边界框回归器。'
- en: '**Training is expensive in space and time** because of deep networks such as
    VGG16, which take up huge amounts of space.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Object detection is slow** because it performs a ConvNet forward pass for
    each object proposal.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast R-CNN
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this paper, a Fast Region-based Convolutional Network method (Fast R-CNN)
    for object detection is proposed.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '**[Fast R-CNN](https://arxiv.org/abs/1504.08083?source=post_page---------------------------)**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN)
    for object detection. Fast R-CNN...
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: It’s implemented in [Python and in C++ using Caffe](https://github.com/rbgirshick/fast-rcnn).
    This model achieves a mean average precision of 66% on PASCAL VOC 2012, versus
    62% for R-CNN.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/e68ab3603e1ce7abf96c668a6a6462d8.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: '[https://arxiv.org/pdf/1504.08083.pdf](https://arxiv.org/pdf/1504.08083.pdf)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: In comparison to the R-CNN, Fast R-CNN has a higher mean average precision,
    single stage training, training that updates all network layers, and disk storage
    isn’t required for feature caching.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: In its architecture, a Fast R-CNN, takes an image as input as well as a set
    of object proposals. It then processes the image with convolutional and max-pooling
    layers to produce a convolutional feature map. A fixed-layer feature vector is
    then extracted from each feature map by a region of interest pooling layer for
    each region proposal.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: The feature vectors are then fed to fully connected layers. These then branch
    into two output layers. One produces softmax probability estimates over several
    object classes, while the other produces four real-value numbers for each of the
    object classes. These 4 numbers represent the position of the bounding box for
    each of the objects.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Faster R-CNN
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**[Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497?source=post_page---------------------------)**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: State-of-the-art object detection networks depend on region proposal algorithms
    to hypothesize object locations...
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: This paper proposes a training mechanism that alternates fine-tuning for regional
    proposal tasks and fine-tuning for object detection.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/64825171bc64b80b110181d76c89b17c.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1506.01497.pdf)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'The Faster R-CNN model is comprised of two modules: a deep convolutional network
    responsible for proposing the regions, and a Fast R-CNN detector that uses the
    regions. The Region Proposal Network takes an image as input and generates an
    output of rectangular object proposals. Each of the rectangles has an objectness
    score.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/1f7a4a99e387bb4ca9393c40f6b4b3e9.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1506.01497.pdf)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Mask R-CNN
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**[Mask R-CNN](https://arxiv.org/abs/1703.06870?source=post_page---------------------------)**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: We present a conceptually simple, flexible, and general framework for object
    instance segmentation. Our approach...
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: The model presented in this paper is an extension of the Faster R-CNN architecture
    described above. It also allows for the estimation of human poses.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/d59e7349c9d3b2c88fcc68bf08fd32f6.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1703.06870.pdf)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: In this model, objects are classified and localized using a bounding box and
    semantic segmentation that classifies each pixel into a set of categories. This
    model extends Faster R-CNN by adding the prediction of segmentation masks on each
    Region of Interest. The Mask R-CNN produces two outputs; a class label and a bounding
    box.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'SSD: Single Shot MultiBox Detector'
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**[SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325?source=post_page---------------------------)**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: We present a method for detecting objects in images using a single deep neural
    network. Our approach, named SSD...
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: This paper presents a model to predict objects in images using a single deep
    neural network. The network generates scores for the presence of each object category
    using small convolutional filters applied to feature maps.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/d37869ee91bde630ec338c0824790c64.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1512.02325.pdf)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: This approach uses a feed-forward convolutional neural network that produces
    a collection of bounding boxes and scores for the presence of certain objects.
    Convolutional feature layers are added to allow for feature detection at multiple
    scales. In this model, each feature map cell is linked to a set of default bounding
    boxes. The figure below shows how SSD512 performs on animals, vehicles, and furniture.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/ce5c9b600e729d6e72190673cacbbd2f.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1512.02325.pdf)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '**You Only Look Once (YOLO)**'
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This paper proposes a single neural network to predict bounding boxes and class
    probabilities from an image in a single evaluation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[**You Only Look Once: Unified, Real-Time Object Detection**](https://arxiv.org/abs/1506.02640?source=post_page---------------------------)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: We present YOLO, a new approach to object detection. Prior work on object detection
    repurposes classifiers to perform...
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: The YOLO models process 45 frames per second in real-time. YOLO views image
    detection as a regression problem, which makes its pipeline quite simple. It’s
    extremely fast because of this simple pipeline.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: It can process a streaming video in real-time with a latency of less than 25
    seconds. During the training process, YOLO sees the entire image and is, therefore,
    able to include the context in object detection.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/29be92e8209c12e5a7016dfcc778304b.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1506.02640.pdf)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: In YOLO, each bounding box is predicted by features from the entire image. Each
    bounding box has 5 predictions; x, y, w, h, and confidence. (x, y) represents
    the center of the bounding box relative to the bounds of the grid cell. *w* and *h* are
    the predicted width and height of the whole image.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: This model is implemented as a convolutional neural network and evaluated on
    the PASCAL VOC detection dataset. The convolutional layers of the network are
    responsible for extracting the features, while the fully connected layers predict
    the coordinates and output probabilities.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型实现为卷积神经网络，并在 PASCAL VOC 检测数据集上进行评估。网络的卷积层负责提取特征，而全连接层则预测坐标并输出概率。
- en: '![](../Images/d3e8667f8e1f27accd075c933af61233.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3e8667f8e1f27accd075c933af61233.png)'
- en: The network architecture for this model is inspired by the [GoogLeNet](https://ai.google/research/pubs/pub43022) model
    for image classification. The network has 24 convolutional layers and 2 fully-connected
    layers. The main challenges of this model are that it can only predict one class,
    and it doesn’t perform well on small objects such as birds.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的网络架构灵感来源于 [GoogLeNet](https://ai.google/research/pubs/pub43022) 图像分类模型。网络有
    24 层卷积层和 2 层全连接层。该模型的主要挑战是它只能预测一个类别，并且在小物体如鸟类上的表现不佳。
- en: '![Figure](../Images/90aae58fca30371d8f0f137ea908d3ff.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/90aae58fca30371d8f0f137ea908d3ff.png)'
- en: '[source](https://arxiv.org/pdf/1506.02640.pdf)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[source](https://arxiv.org/pdf/1506.02640.pdf)'
- en: This model achieves a mean average precision of 52.7%, it is, however, able
    to go up to 63.4%.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型实现了 52.7% 的平均精度，但可以达到 63.4%。
- en: '![](../Images/fc635dc346ab80d50a6b142487614d29.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc635dc346ab80d50a6b142487614d29.png)'
- en: Objects as Points
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将对象视为点
- en: This paper proposes modeling an object as a single point. It uses key point
    estimation to find center points and regresses to all other object properties.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出将对象建模为一个单一的点。它使用关键点估计来找到中心点，并对所有其他对象属性进行回归。
- en: '[**Objects as Points**](https://arxiv.org/abs/1904.07850v2?source=post_page---------------------------)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[**将对象视为点**](https://arxiv.org/abs/1904.07850v2?source=post_page---------------------------)'
- en: Detection identifies objects as axis-aligned boxes in an image. Most successful
    object detectors enumerate a nearly...
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 检测将图像中的对象识别为轴对齐的框。大多数成功的目标检测器枚举了几乎...
- en: These properties include 3D location, pose orientation, and size. It uses CenterNet,
    a center point based approach that’s faster and more accurate compared to other
    bounding box detectors.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这些属性包括 3D 位置、姿态方向和大小。它使用 CenterNet，这是一种基于中心点的方法，比其他边界框检测器更快、更准确。
- en: '![](../Images/6fd2f8d7c60edc2a759f78d0c7a68a13.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6fd2f8d7c60edc2a759f78d0c7a68a13.png)'
- en: Properties such as object size and pose are regressed from features of the image
    at the center location. In this model, an image is fed to a convolutional neural
    network which generates a heatmap. The maximum values in these heatmaps represent
    the centers of the objects in the image. In order to estimate human poses, the
    model examines 2D joint locations and regresses them at the center point location.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 属性如对象大小和姿态是从图像中心位置的特征中回归得到的。在此模型中，图像被输入到卷积神经网络中，该网络生成热图。这些热图中的最大值代表图像中对象的中心。为了估计人体姿势，该模型检查
    2D 关节位置，并在中心点位置进行回归。
- en: This model achieves 45.1% COCO average precision at 1.4 frames per second. The
    figure below shows how this compared with the results obtained in other research
    papers.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在每秒 1.4 帧的速度下实现了 45.1% 的 COCO 平均精度。下图显示了与其他研究论文中获得的结果的比较。
- en: '![](../Images/f3b0114b26c4cef0400b77c7f10e8ade.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3b0114b26c4cef0400b77c7f10e8ade.png)'
- en: '**Learning Data Augmentation Strategies for Object Detection**'
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**学习数据增强策略用于目标检测**'
- en: Data augmentation involves the process of creating new image data by manipulating
    the original image by, for example, rotating and resizing.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强涉及通过操控原始图像（例如旋转和缩放）来创建新的图像数据。
- en: '[**Learning Data Augmentation Strategies for Object Detection**](https://arxiv.org/abs/1906.11172v1?source=post_page---------------------------)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[**学习数据增强策略用于目标检测**](https://arxiv.org/abs/1906.11172v1?source=post_page---------------------------)'
- en: Data augmentation is a critical component of training deep learning models.
    Although data augmentation has been shown...
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强是训练深度学习模型的关键组成部分。虽然数据增强已经被证明...
- en: While this isn’t itself a model architecture, this paper proposes the creation
    of transformations that can be applied to object detection datasets that can be
    transferred to other objection detection datasets. The transformations are usually
    applied at training time.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这本身不是一个模型架构，但本文提出了创建可以应用于目标检测数据集的变换，这些变换可以转移到其他目标检测数据集中。这些变换通常在训练时应用。
- en: '![](../Images/29cf759a0f56409a6ffb612bf5e18499.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29cf759a0f56409a6ffb612bf5e18499.png)'
- en: In this model, an augmentation policy is defined as a set of n policies that
    are selected at random during the training process. Some of the operations that
    have been applied in this model include distorting color channels, distorting
    the images geometrically, and distorting only the pixel content found in the bounding
    box annotations.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，增强策略被定义为在训练过程中随机选择的 n 个策略的集合。在此模型中应用的一些操作包括扭曲颜色通道、几何扭曲图像，以及仅扭曲边界框注释中的像素内容。
- en: Experimentation on the COCO dataset has shown that optimizing a data augmentation
    policy is able to improve the accuracy of detection by more than +2.3 mean average
    precision. This allows a single inference model to achieve an accuracy of 50.7
    mean average precision.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对 COCO 数据集的实验表明，优化数据增强策略可以将检测准确率提高超过 +2.3 平均精度。这使得单个推理模型可以达到 50.7 的平均精度。
- en: '**Conclusion**'
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**结论**'
- en: We should now be up to speed on some of the most common—and a couple of recent—techniques
    for performing object detection in a variety of contexts.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在应该对一些最常见的——以及几种最近的——对象检测技术有了了解。
- en: The papers/abstracts mentioned and linked to above also contain links to their
    code implementations. We’d be happy to see the kind of results you obtain after
    testing them.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 上述提到和链接的论文/摘要也包含了它们的代码实现链接。我们很乐意看到你在测试后获得的结果。
- en: Let’s not limit ourselves. Object detection can also live inside your smartphone. [Learn
    how](https://fritz.ai/?utm_campaign=feature2&utm_source=heartbeat) Fritz can teach
    mobile apps to see, hear, sense, and think.
  id: totrans-109
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 不要局限于此。对象检测也可以存在于你的智能手机中。[了解如何](https://fritz.ai/?utm_campaign=feature2&utm_source=heartbeat)
    Fritz 可以教会移动应用程序看、听、感知和思考。
- en: '**Bio: [Derrick Mwiti](https://derrickmwiti.com/)** is a data analyst, a writer,
    and a mentor. He is driven by delivering great results in every task, and is a
    mentor at Lapid Leaders Africa.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介: [德里克·穆伊提](https://derrickmwiti.com/)** 是一位数据分析师、作家和导师。他致力于在每项任务中取得出色成果，并且是
    Lapid Leaders Africa 的导师。'
- en: '[Original](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3).
    Reposted with permission.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3)。经许可转载。'
- en: '**Related:**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关:**'
- en: '[Object Detection with Luminoth](/2019/03/object-detection-luminoth.html)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Luminoth 进行对象检测](/2019/03/object-detection-luminoth.html)'
- en: '[An Overview of Human Pose Estimation with Deep Learning](/2019/06/human-pose-estimation-deep-learning.html)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基于深度学习的人体姿态估计概述](/2019/06/human-pose-estimation-deep-learning.html)'
- en: '[Large-Scale Evolution of Image Classifiers](/2019/05/large-scale-evolution-image-classifiers.html)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大规模图像分类器的演变](/2019/05/large-scale-evolution-image-classifiers.html)'
- en: More On This Topic
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[What Is the Difference Between SQL and Object-Relational Mapping (ORM)?](https://www.kdnuggets.com/2022/02/difference-sql-object-relational-mapping-orm.html)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQL 与对象关系映射 (ORM) 有什么区别？](https://www.kdnuggets.com/2022/02/difference-sql-object-relational-mapping-orm.html)'
- en: '[KDnuggets™ News 22:n09, Mar 2: Telling a Great Data Story: A…](https://www.kdnuggets.com/2022/n09.html)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets™ 新闻 22:n09, 3 月 2 日: 讲述一个精彩的数据故事: A…](https://www.kdnuggets.com/2022/n09.html)'
- en: '[A Beginner''s Guide to Anomaly Detection Techniques in Data Science](https://www.kdnuggets.com/2023/05/beginner-guide-anomaly-detection-techniques-data-science.html)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中异常检测技术的初学者指南](https://www.kdnuggets.com/2023/05/beginner-guide-anomaly-detection-techniques-data-science.html)'
- en: '[How to Perform Motion Detection Using Python](https://www.kdnuggets.com/2022/08/perform-motion-detection-python.html)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 Python 执行运动检测](https://www.kdnuggets.com/2022/08/perform-motion-detection-python.html)'
- en: '[Using Hugging Face Transformers for Emotion Detection in Text](https://www.kdnuggets.com/using-hugging-face-transformers-for-emotion-detection-in-text)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Hugging Face Transformers 进行文本情感检测](https://www.kdnuggets.com/using-hugging-face-transformers-for-emotion-detection-in-text)'
- en: '[KDnuggets News, August 17: How to Perform Motion Detection Using…](https://www.kdnuggets.com/2022/n33.html)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，8 月 17 日: 如何执行运动检测](https://www.kdnuggets.com/2022/n33.html)'
