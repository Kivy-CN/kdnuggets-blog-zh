- en: Generative AI Key Terms Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式 AI 关键术语解析
- en: 原文：[https://www.kdnuggets.com/generative-ai-key-terms-explained](https://www.kdnuggets.com/generative-ai-key-terms-explained)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/generative-ai-key-terms-explained](https://www.kdnuggets.com/generative-ai-key-terms-explained)
- en: '![Generative AI Key Terms Explained](../Images/1faf4eb7416fa0192ebcca3069047be5.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![生成式 AI 关键术语解析](../Images/1faf4eb7416fa0192ebcca3069047be5.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: In the past few years, and especially since the appearance of ChatGPT just over
    12 months ago, generative AI models for creating realistic synthetic text, images,
    video, and audio have emerged and have been rapidly advancing since. What began
    as humble research quickly developed into systems with the capacity to generate
    high-quality, human-like outputs across the various mediums mentioned above. Propelled
    in particular by key innovations in neural networks and massive increases in computational
    power, more and more companies now offer free and/or paid access to these models
    that increase in ability at a remarkable pace.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年，尤其是自 ChatGPT 出现以来，生成式 AI 模型用于创建逼真的合成文本、图像、视频和音频不断涌现，并迅速发展。最初仅是微不足道的研究，但很快发展成能够在上述各种媒介中生成高质量、类人输出的系统。特别是在神经网络的关键创新和计算能力的大幅提升的推动下，越来越多的公司现在提供这些模型的免费和/或付费访问，这些模型的能力以惊人的速度增长。
- en: Generative AI isn't all rainbows and puppy dogs, however. While holding great
    promise to augment human creativity in a wide variety of applications, concerns
    remain about how to properly evaluate, test, and responsibly deploy these generative
    systems. There is particular unease related to the spread of misinformation, along
    with concerns of bias, truthfulness, and social impacts introduced by this technology.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，生成式 AI 并非全是美好愿景。尽管它在多种应用中极具潜力，能够增强人类创造力，但如何恰当地评估、测试和负责任地部署这些生成系统仍然存在诸多担忧。尤其是对虚假信息传播的担忧，以及由这项技术带来的偏见、真实性和社会影响的忧虑。
- en: However, the first thing to do with any new technology is to attempt to understand
    it before we either harness or criticize it. Getting a start at doing so is what
    we have planned for this article. We intend to lay out some key generative AI
    terms and do our best to make them understandable at an intuitive level for beginners,
    in order to provide an elementary foundation and pave the way for more in-depth
    learning ahead. In that vein, for each key term below you will find links to related
    material to begin to investigate further as desired.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，处理任何新技术的首要任务是尝试理解它，然后再决定是利用还是批评它。我们计划在本文中着手进行这一工作。我们打算列出一些关键的生成式 AI 术语，并尽力以直观的方式使初学者能够理解，从而提供一个基础框架，为进一步深入学习铺平道路。为此，在下文中的每个关键术语下，你将找到相关材料的链接，便于根据需要进一步调查。
- en: Now let's get started.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始吧。
- en: Natural Language Processing
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: '[Natural Language Processing](https://www.kdnuggets.com/tag/natural-language-processing)
    (NLP) is an AI subfield focusing on enabling machines to understand, interpret,
    and generate human language, by programmatically providing these machines with
    the tools required to do so. NLP bridges the gap between human communication and
    computer understanding. [NLP first employed](https://www.kdnuggets.com/2018/10/main-approaches-natural-language-processing-tasks.html)
    rule-based methods, followed by "traditional" machine learning approaches, while
    most cutting edge NLP today relies on a variety of neural network techniques.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[自然语言处理](https://www.kdnuggets.com/tag/natural-language-processing)（NLP）是一个专注于使机器理解、解释和生成自然语言的
    AI 子领域，通过编程为这些机器提供所需的工具。NLP 构建了人类沟通与计算机理解之间的桥梁。[NLP 首先使用](https://www.kdnuggets.com/2018/10/main-approaches-natural-language-processing-tasks.html)基于规则的方法，其后采用了“传统”的机器学习方法，而如今最前沿的
    NLP 则依赖于各种神经网络技术。'
- en: Neural Networks
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: '[Neural networks](https://www.kdnuggets.com/a-brief-history-of-the-neural-networks)
    are machine learning computational models *inspired by* (not *replicas of*) the
    human brain, used for learning from data. Neural networks consist of layers (many
    layers = deep learning) of artificial neurons processing and transmitting small
    individual pieces of data, fitting this data to function, and repetitively updating
    the weights associated with the processing neurons in an attempt to "better fit"
    the data to the function. Neural networks are essential for the learning and decision-making
    capabilities of today''s AI. Without the deep learning revolution started a little
    over a decade ago, much of what we refer to as AI would not have been possible.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[神经网络](https://www.kdnuggets.com/a-brief-history-of-the-neural-networks)是受（而非*复制自*）人脑启发的机器学习计算模型，用于从数据中学习。神经网络由层（许多层=深度学习）人工神经元组成，处理和传输小的个体数据，将这些数据适配到函数中，并重复更新与处理神经元相关的权重，以“更好地适配”数据到函数中。神经网络对于现代人工智能的学习和决策能力至关重要。没有十多年前开始的深度学习革命，我们所称之为人工智能的许多东西将不可能存在。'
- en: Generative AI
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成式人工智能
- en: '[Generative AI](https://www.kdnuggets.com/?s=Generative+AI) is a category of
    artificial intelligence, powered by neural networks, which is focused on the creation
    of new content. This content can take many forms, from text to images to audio
    and beyond. This differs from "traditional" types of AI which focus on classifying
    or analyzing existing data, embodying the capability to "imagine" and produce
    novel content based on training data.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[生成式人工智能](https://www.kdnuggets.com/?s=Generative+AI)是一类由神经网络驱动的人工智能，专注于创造新的内容。这些内容可以以多种形式出现，从文本到图像，再到音频等。这不同于专注于分类或分析现有数据的“传统”人工智能类型，它体现了“想象”并基于训练数据生成新内容的能力。'
- en: Content Generation
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内容生成
- en: Content generation is the actual process where trained generative models generate
    synthetic text, images, video, and audio, doing so with learned patterns from
    their training data, producing contextually relevant output in response to user
    input or prompts. These prompts can be in any of these mentioned forms as well.
    For example, text could be used as a prompt to generate more text, or to generate
    an image based on the text description, or a piece of audio or video instead.
    Likewise, an image could be used as a prompt to generate another image, or text,
    or video, etc. Multi-modal prompting is also possible, in which, for example,
    text and an image could be used to generate audio.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 内容生成是经过训练的生成模型实际生成合成文本、图像、视频和音频的过程，这些生成是基于从训练数据中学到的模式，对用户输入或提示进行上下文相关的输出。这些提示也可以是上述提到的任何形式。例如，文本可以作为提示生成更多的文本，或根据文本描述生成图像，或生成音频或视频。类似地，图像可以作为提示生成另一张图像、文本或视频等。多模态提示也是可能的，例如，可以使用文本和图像生成音频。
- en: Large Language Models
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型
- en: '[Large Language Models](https://www.kdnuggets.com/?s=Large+Language+Models)
    (LLMs) are specialized machine learning models which are tailored to process and
    "understand" human language. LLMs are trained on vast amounts of text data, which
    enables them to analyze and replicate complex language structures, nuances, and
    contexts. Regardless of the exact LLM model and techniques being used, the entire
    essence of these models is to learn and predict what the next word, or token (group
    of letters) follows the current, and so on. LLMs are essentially incredibly complex
    "next word guessers," and improving the next word guess is a very hot research
    topic at the moment, as you have likely heard.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[大型语言模型](https://www.kdnuggets.com/?s=Large+Language+Models)（LLMs）是专门处理和“理解”人类语言的机器学习模型。LLMs通过大量的文本数据进行训练，使其能够分析和复制复杂的语言结构、细微差别和语境。不论使用的具体LLM模型和技术是什么，这些模型的核心本质都是学习并预测当前词汇或标记（字母组合）之后的下一个词汇。LLMs本质上是非常复杂的“下一个词汇猜测器”，而提升下一个词汇的猜测能力是目前一个非常热门的研究话题。'
- en: Foundation Models
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础模型
- en: '[Foundational models](https://www.kdnuggets.com/?s=Foundation+Models) are the
    AI systems that have been designed with broad capabilities that can then be adapted
    for a variety of specific tasks. Foundational models provide a base for building
    more specialized applications, such as tweaking a general language model for specific
    chatbot, assistant, or additional generative functionalities. Foundational models
    are not limited to language models, however, and exist for generation tasks such
    as image and video as well. Examples of well-known and relied-upon foundational
    models include GPT, BERT, and Stable Diffusion.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[基础模型](https://www.kdnuggets.com/?s=Foundation+Models)是被设计为具有广泛能力的AI系统，这些能力可以适应多种特定任务。基础模型提供了构建更专业化应用的基础，例如将通用语言模型调整为特定的聊天机器人、助手或其他生成性功能。基础模型不仅限于语言模型，还存在于生成图像和视频等任务中。知名且被广泛依赖的基础模型包括GPT、BERT和稳定扩散。'
- en: Parameters
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数
- en: In this context, parameters are numerical values that define a model's structure,
    operational behavior, and capacity for learning and predicting. For example, the
    billions of parameters in OpenAI's GPT-4 influence its word prediction and dialogue
    creation abilities. More technically, connections between each neuron in a neural
    network carry weights (mentioned above), with each of these weights being a single
    model parameter. The more neurons → the more weights → the more parameters → the
    more capacity for a (well-trained) network to learn and predict.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，参数是定义模型结构、操作行为和学习与预测能力的数值。例如，OpenAI的GPT-4中的数十亿个参数影响其单词预测和对话生成能力。更技术性地说，神经网络中每个神经元之间的连接承载权重（如上所述），每个权重都是一个单一的模型参数。神经元越多
    → 权重越多 → 参数越多 → 网络的学习和预测能力越强。
- en: Word Embeddings
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词嵌入
- en: '[Word embeddings](https://www.kdnuggets.com/?s=word+embeddings) are a technique
    in which words or phrases are converted into numerical vectors of a predetermined
    number of dimensions, in an attempt to capture their meaning and contextual relationships
    in a multidimensional space of a size much smaller than what would be required
    to one-hot encode each word (or phrase) in a vocabulary. If you were to create
    a matrix of 500,000 words where each row was created for a single word, and every
    column in that row was set to "0" except for a single column representing the
    word in question, the matrix would be 500,000 x 500,000 rows x columns, and be
    incredibly sparse. This would be a disaster for both storage and performance.
    By setting columns to various fractional values between 0 and 1, and reducing
    the number of columns to, say, 300 (dimensions), we have a much more focused storage
    structure, and inherently increase operation performance. As a side effect, by
    having these dimensional embedding values learned by a a neural network, like
    terms will be "closer" in dimensional values than unlike terms, providing us with
    insights into relative word meanings.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[词嵌入](https://www.kdnuggets.com/?s=word+embeddings)是一种技术，通过将单词或短语转换为预定维度数量的数值向量，试图在远小于对词汇表中的每个单词（或短语）进行独热编码所需的空间中捕捉其含义和上下文关系。如果你创建一个500,000个单词的矩阵，其中每一行代表一个单词，每一行中的每一列都设置为“0”，除了一个表示该单词的列，这个矩阵将是500,000
    x 500,000行 x 列，并且非常稀疏。这将是存储和性能上的灾难。通过将列设置为0到1之间的各种分数值，并将列数减少到例如300（维度），我们获得了一个更为集中存储的结构，并本质上提高了操作性能。作为副作用，通过让神经网络学习这些维度嵌入值，相似的术语在维度值上会“更接近”，从而为我们提供关于相对单词含义的见解。'
- en: Transformer Models
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变换器模型
- en: '[Transformer models](https://www.kdnuggets.com/?s=transformer) are AI architectures
    that simultaneously process entire sentences, which is crucial for grasping language
    context and long-term associations. They excel in detecting relationships between
    words and phrases, even when far apart in a sentence. For example, when "she"
    is established early in a chunk of text as a noun and/or pronoun referencing a
    particular individual, transformers are able to "remember" this relationship.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[变换器模型](https://www.kdnuggets.com/?s=transformer)是同时处理整句话的AI架构，这对于掌握语言上下文和长期关联至关重要。它们在检测单词和短语之间的关系方面表现优异，即使它们在句子中相隔很远。例如，当“她”在一段文本中早期被确立为指代特定个体的名词和/或代词时，变换器能够“记住”这种关系。'
- en: Positional Encoding
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位置编码
- en: Positional encoding refers to a method in transformer models that helps to maintain
    the sequential order of words. This is a crucial component for understanding the
    context within a sentence and between sentences.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码是指变换器模型中的一种方法，帮助保持词汇的顺序。这是理解句子及句子之间上下文的关键组件。
- en: Reinforcement Learning From Human Feedback
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从人类反馈中学习的强化学习
- en: Reinforcement learning from human feedback (RLHF) refers to a method of training
    LLMs. Like traditional reinforcment learning (RL), RLHF trains and uses a reward
    model, though this one comes directly from human feedback. The reward model is
    then used as a reward function in the training of the LLM by use of an optimization
    algorithm. This model explicitly keeps humans in the loop during model training,
    with the hopes that human feedback can provide essential and perhaps otherwise
    unattainable feedback required for optimized LLMs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从人类反馈中学习的强化学习（RLHF）指的是一种训练大型语言模型（LLMs）的方法。与传统的强化学习（RL）类似，RLHF训练并使用一个奖励模型，不过这个奖励模型直接来源于人类反馈。然后，该奖励模型作为奖励函数用于LLM的训练，借助优化算法。这个模型明确地将人类纳入模型训练的过程中，期望人类反馈能提供必要的、可能否则无法获得的反馈，以优化LLM。
- en: Emergent Behavior
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 突现行为
- en: '[Emergent behavior](https://arxiv.org/abs/2206.07682) refers to the unexpected
    skills displayed by large and complex language models, skills which are not displayed
    in simpler models. These unexpected skills can include abilities like coding,
    musical composition, and fiction writing. These skills are not explicitly programmed
    into the models but emerge from their complex architectures. The quesiton of emergent
    abilities can go beyond these more common skills, however; for example, is [theory
    of mind](https://arxiv.org/abs/2302.02083) an emergent behavior?'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[突现行为](https://arxiv.org/abs/2206.07682)指的是大型和复杂语言模型展示出的意外技能，这些技能在较简单的模型中并不存在。这些意外的技能可能包括编程、音乐创作和小说写作等能力。这些技能并没有被明确地编程到模型中，而是从其复杂的架构中自然出现的。然而，突现能力的问题可能超越这些更常见的技能；例如，[心智理论](https://arxiv.org/abs/2302.02083)是否是一种突现行为？'
- en: Hallucinations
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 幻觉
- en: '[Hallucinations](https://www.kdnuggets.com/?s=hallucinations) is the term given
    to when LLMs produce factually incorrect or illogical responses due to constraints
    in data and architecture. Despite whatever advanced capabilities the model possesses,
    these errors can still occur both when queries are encountered that have no grounding
    in the model''s training data, and when a model''s training data consists of incorrect
    or nonfactual information.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[幻觉](https://www.kdnuggets.com/?s=hallucinations)是指大型语言模型由于数据和架构的限制而产生事实错误或不合逻辑的响应。尽管模型可能具备先进的能力，但这些错误仍然可能发生，无论是当遇到与模型训练数据无关的查询，还是当模型的训练数据包含错误或虚假的信息时。'
- en: Anthropomorphism
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拟人化
- en: Anthropomorphism is the tendency to attribute human-like qualities to AI systems.
    It is important to note that, despite their ability to mimic human emotions or
    speech and our instinct to think of the models or as "he" or a "she" (or any other
    pronoun) as opposed to an "it," AI systems do not possess feelings or consciousness.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 拟人化是指将类人特质归于人工智能系统的倾向。需要注意的是，尽管人工智能系统能够模仿人类情感或语言，以及我们本能地将模型视为“他”或“她”（或其他代词）而非“它”，人工智能系统并不具备情感或意识。
- en: Bias
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏见
- en: Bias is a loaded term in AI research, and can refer to a number of different
    things. In our context, bias refers to the errors in AI outputs caused by skewed
    training data, leading to inaccurate, offensive, or misleading predictions. Bias
    arises when algorithms prioritize irrelevant data traits over meaningful patterns,
    or lack meaningful patterns altogether.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见在人工智能研究中是一个含义丰富的术语，可以指代多种不同的事物。在我们的上下文中，偏见指的是由于训练数据偏斜而导致的人工智能输出错误，进而导致不准确、冒犯或误导的预测。偏见的出现是因为算法将无关数据特征置于有意义的模式之上，或完全缺乏有意义的模式。
- en: '[](https://www.linkedin.com/in/mattmayo13/)****[Matthew Mayo](https://www.kdnuggets.com/wp-content/uploads/./profile-pic.jpg)****
    ([**@mattmayo13**](https://twitter.com/mattmayo13)) holds a master''s degree in
    computer science and a graduate diploma in data mining. As managing editor of
    [KDnuggets](https://www.kdnuggets.com/) & [Statology](https://www.statology.org/),
    and contributing editor at [Machine Learning Mastery](https://machinelearningmastery.com/),
    Matthew aims to make complex data science concepts accessible. His professional
    interests include natural language processing, language models, machine learning
    algorithms, and exploring emerging AI. He is driven by a mission to democratize
    knowledge in the data science community. Matthew has been coding since he was
    6 years old.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.linkedin.com/in/mattmayo13/)****[Matthew Mayo](https://www.kdnuggets.com/wp-content/uploads/./profile-pic.jpg)****
    ([**@mattmayo13**](https://twitter.com/mattmayo13)) 拥有计算机科学硕士学位和数据挖掘研究生文凭。作为[KDnuggets](https://www.kdnuggets.com/)和[Statology](https://www.statology.org/)的总编辑，以及[Machine
    Learning Mastery](https://machinelearningmastery.com/)的特约编辑，Matthew 致力于使复杂的数据科学概念变得易于理解。他的专业兴趣包括自然语言处理、语言模型、机器学习算法以及探索新兴的人工智能。他的终极目标是将数据科学领域的知识普及化。Matthew
    从6岁开始编程。'
- en: More On This Topic
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Database Key Terms, Explained](https://www.kdnuggets.com/2016/07/database-key-terms-explained.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据库关键术语详解](https://www.kdnuggets.com/2016/07/database-key-terms-explained.html)'
- en: '[Descriptive Statistics Key Terms, Explained](https://www.kdnuggets.com/2017/05/descriptive-statistics-key-terms-explained.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[描述性统计关键术语详解](https://www.kdnuggets.com/2017/05/descriptive-statistics-key-terms-explained.html)'
- en: '[Machine Learning Key Terms, Explained](https://www.kdnuggets.com/2016/05/machine-learning-key-terms-explained.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习关键术语详解](https://www.kdnuggets.com/2016/05/machine-learning-key-terms-explained.html)'
- en: '[Deep Learning Key Terms, Explained](https://www.kdnuggets.com/2016/10/deep-learning-key-terms-explained.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习关键术语详解](https://www.kdnuggets.com/2016/10/deep-learning-key-terms-explained.html)'
- en: '[Natural Language Processing Key Terms, Explained](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理关键术语详解](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
- en: '[Genetic Algorithm Key Terms, Explained](https://www.kdnuggets.com/2018/04/genetic-algorithm-key-terms-explained.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[遗传算法关键术语详解](https://www.kdnuggets.com/2018/04/genetic-algorithm-key-terms-explained.html)'
