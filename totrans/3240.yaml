- en: Machine Learning Translation and the Google Translate Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/09/machine-learning-translation-google-translate-algorithm.html](https://www.kdnuggets.com/2017/09/machine-learning-translation-google-translate-algorithm.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**By Daniil Korbut, [Statsbot](https://statsbot.co/).**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad62fc7c2ba75d845ae4835dea932f43.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Google Machine Translation](https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Every day we use different technologies without even knowing how exactly they
    work. In fact, it’s not very easy to understand engines powered by machine learning.
    The [Statsbot](http://statsbot.co/?utm_source=kdnuggets) team wants to make machine
    learning clear by telling data stories in this blog. Today, we’ve decided to explore
    machine translators and explain how the Google Translate algorithm works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Years ago, it was very time consuming to translate the text from an unknown
    language. Using simple vocabularies with word-for-word translation was hard for
    two reasons: 1) the reader had to know the grammar rules and 2) needed to keep
    in mind all language versions while translating the whole sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we don’t need to struggle so much– we can translate phrases, sentences,
    and even large texts just by putting them in Google Translate. But most people
    don’t actually care how the engine of machine learning translation works. This
    post is for those who do care.
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep learning translation problems**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the Google Translate engine tried to kept the translations for even short
    sentences, it wouldn’t work because of the huge number of possible variations.
    The best idea can be to teach the computer sets of grammar rules and translate
    the sentences according to them. If only it were as easy as it sounds.
  prefs: []
  type: TYPE_NORMAL
- en: If you have ever tried learning a foreign language, you know that there are
    always a lot of exceptions to rules. When we try to capture all these rules, exceptions
    and exceptions to the exceptions in the program, the quality of translation breaks
    down.
  prefs: []
  type: TYPE_NORMAL
- en: '*Modern machine translation systems use a different approach: they allocate
    the rules from text by analyzing a huge set of documents.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Creating your own simple machine translator would be [**a great project for
    any data science resume**](https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6?utm_source=kdnuggets).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to investigate what hides in the “black boxes” that we call machine
    translators. Deep neural networks can achieve excellent results in very complicated
    tasks (speech/visual object recognition), but despite their flexibility, they
    can be applied only for tasks where the input and target have fixed dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: '**Recurrent Neural Networks**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here is where Long Short-Term Memory networks (LSTMs) come into play, helping
    us to work with sequences whose length we can’t know a priori.
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs are a special kind of recurrent neural network (RNN), capable of learning
    long-term dependencies. All RNNs look like a chain of repeating modules.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6de3d1247fc0c6dd19b6126f28c69ff1.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Unrolled recurrent neural network](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  prefs: []
  type: TYPE_NORMAL
- en: So the LSTM transmits data from module to module and, for example, for generating
    Ht we use not only Xt, but all previous input values X. To learn more about structure
    and mathematical models of LSTM, you can read the great article “[Understanding
    LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).”
  prefs: []
  type: TYPE_NORMAL
- en: '**Bidirectional RNNs**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our next step is bidirectional recurrent neural networks (BRNNs). What a BRNN
    does, is split the neurons of a regular RNN into two directions. One direction
    is for positive time, or forward states. The other direction is for negative time,
    or backward states. The output of these two states are not connected to inputs
    of the opposite direction states.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c632cea579cb40f348be6066dbe1e821.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Bidirectional recurrent neural networks](https://www.semanticscholar.org/paper/Hybrid-speech-recognition-with-Deep-Bidirectional-Graves-Jaitly/5807664af8e63d5207f59fb263c9e7bd3673be79)'
  prefs: []
  type: TYPE_NORMAL
- en: To understand why BRNNs can work better than a simple RNN, imagine that we have
    a sentence of 9 words and we want to predict the 5th word. We can make it know
    either only the first 4 words, or the first 4 words and last 4 words. Of course,
    the quality in the second case would be better.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sequence to sequence**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we’re ready to move to sequence to sequence models (also called seq2seq).
    The basic seq2seq model consist of two RNNs: an encoder network that processes
    the input and a decoder network that generates the output.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4a829f9839b0211cdd633e5c69373cec.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Sequence to sequence model](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can make our first machine translator!
  prefs: []
  type: TYPE_NORMAL
- en: However, let’s think about one trick. Google Translate [currently supports 103
    languages](https://www.newscientist.com/article/2114748-google-translate-ai-invents-its-own-language-to-translate-with/),
    so we should have 103x102 different models for each pair of languages. Of course,
    the quality of these models varies according to the popularity of languages and
    the amount of documents needed for training this network. The best that we can
    do is to make one NN to take any language as input and translate into any language.
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Translate**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: That very idea was [realized by Google engineers at the end of 2016](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html).
    Architecture of NN was build on the seq2seq model, which we have already studied.
  prefs: []
  type: TYPE_NORMAL
- en: The only exception is that between the encoder and decoder there are 8 layers
    of LSTM-RNN that have residual connections between layers with some tweaks for
    accuracy and speed. If you want to go deeper with that, take a look at the article [Google’s
    Neural Machine Translation System](https://arxiv.org/abs/1609.08144).
  prefs: []
  type: TYPE_NORMAL
- en: The main thing about this approach is that now the Google Translate algorithm
    uses only one system instead of a huge set for every pair of languages.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The system requires a “token” at the beginning of the input sentence which specifies
    the language you’re trying to translate the phrase into.
  prefs: []
  type: TYPE_NORMAL
- en: This improves translation quality and enables translations even between two
    languages which the system hasn’t seen yet, a method termed “Zero-Shot Translation.”
  prefs: []
  type: TYPE_NORMAL
- en: '**What means better translation?**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we’re talking about improvements and better results from Google Translate
    algorithms, how can we correctly evaluate that the first candidate for translation
    is better than the second?
  prefs: []
  type: TYPE_NORMAL
- en: It’s not a trivial problem, because for some commonly used sentences we have
    the sets of reference translations from the professional translators, that have,
    of course, some differences.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a lot of approaches that partly solve this problem, but the most
    popular and effective metric is [BLEU](https://en.wikipedia.org/wiki/BLEU) (bilingual
    evaluation understudy). Imagine, we have two candidates from machine translators:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Candidate 1: Statsbot makes it easy for companies to closely monitor data from
    various analytical platforms via natural language.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Candidate 2: Statsbot uses natural language to accurately analyze businesses’
    metrics from different analytical platforms.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/a812122e97cb432b115267962b8ead35.png)'
  prefs: []
  type: TYPE_IMG
- en: Although they have the same meaning they differ in quality and have different
    structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at two human translations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reference 1: Statsbot helps companies closely monitor their data from different
    analytical platforms via natural language.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Reference 2: Statsbot allows companies to carefully monitor data from various
    analytics platforms by using natural language.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Obviously, Candidate 1 is better, sharing more words and phrases compared to
    Candidate 2\. This is a key idea of the simple BLEU approach. We can compare [n-grams](https://en.wikipedia.org/wiki/N-gram) of
    the candidate with n-grams of the reference translation and count the number of
    matches (independent from their position). We use only n-gram precisions, because
    calculating recall is difficult with multiple refs and the result is the geometric
    average of n-gram scores.
  prefs: []
  type: TYPE_NORMAL
- en: Now you can evaluate the complex engine of machine learning translation. Next
    time when you translate something with Google Translate, imagine how many millions
    of documents it analyzed before giving you the best language version.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Daniil Korbut](https://medium.com/@daniilkorbut)** is a Junior Data
    Scientist at [Statsbot](https://statsbot.co/).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Attention and Memory in Deep Learning and NLP](/2016/01/attention-memory-deep-learning-nlp.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top /r/MachineLearning Posts, May: Deep Image Analogy; Stylized Facial Animations;
    Google Open Sources Sketch-RNN](/2017/06/top-reddit-machine-learning-posts-may.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Free Resources for Getting Started with Deep Learning for Natural Language
    Processing](/2017/07/5-free-resources-getting-started-deep-learning-nlp.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[OpenAI’s Whisper API for Transcription and Translation](https://www.kdnuggets.com/2023/06/openai-whisper-api-transcription-translation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Translate Languages with MarianMT and Hugging Face Transformers](https://www.kdnuggets.com/how-to-translate-languages-with-marianmt-and-hugging-face-transformers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Reading Minds with AI: Researchers Translate Brain Waves to Images](https://www.kdnuggets.com/2023/03/reading-minds-ai-researchers-translate-brain-waves-images.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Full End-to-End Deployment of a Machine Learning Algorithm into a…](https://www.kdnuggets.com/2021/12/deployment-machine-learning-algorithm-live-production-environment.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unlock the Secrets to Choosing the Perfect Machine Learning Algorithm!](https://www.kdnuggets.com/2023/07/ml-algorithm-choose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[DBSCAN Clustering Algorithm in Machine Learning](https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
