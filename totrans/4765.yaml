- en: 'Notes on Feature Preprocessing: The What, the Why, and the How'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征预处理的笔记：什么，为什么，怎么做
- en: 原文：[https://www.kdnuggets.com/2018/10/notes-feature-preprocessing-what-why-how.html](https://www.kdnuggets.com/2018/10/notes-feature-preprocessing-what-why-how.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/10/notes-feature-preprocessing-what-why-how.html](https://www.kdnuggets.com/2018/10/notes-feature-preprocessing-what-why-how.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)![Image](../Images/936e98582a437454bc4a6a7480f46cbf.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)![图片](../Images/936e98582a437454bc4a6a7480f46cbf.png)'
- en: 'Original vs shifted vs shifted & scaled data (Source: [Artificial Intelligence
    GitBook](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/feature_scaling.html))'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据与移动数据与移动&缩放数据（来源：[人工智能GitBook](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/feature_scaling.html)）
- en: This article covers a few important points related to the preprocessing of numeric
    data, focusing on the scaling of feature values, and the broad question of dealing
    with outliers.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本文涵盖了一些与数值数据预处理相关的重要点，重点讨论特征值的缩放和处理异常值的广泛问题。
- en: '**Effects of Feature Scaling**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**特征缩放的影响**'
- en: '[Feature scaling](https://en.wikipedia.org/wiki/Feature_scaling) can be defined
    as "a method used to standardize the range of independent variables or features
    of data." Feature scaling is one of the main components of data preprocessing,
    and can be applied to all types of data one might come across.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[特征缩放](https://en.wikipedia.org/wiki/Feature_scaling)可以定义为“用来标准化独立变量或数据特征范围的方法。”
    特征缩放是数据预处理的主要组成部分之一，可以应用于所有类型的数据。'
- en: 'Some models are influenced by features scaling, while others are not. Non-tree-based
    models can be easily influenced by feature scaling:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型受特征缩放的影响，而另一些则不受影响。非树基模型容易受特征缩放的影响：
- en: Linear models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性模型
- en: Nearest neighbor classifiers
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近邻分类器
- en: Neural networks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'Conversely, tree-based models are not influenced by feature scaling. It should
    be obvious, but examples of tree-based modeling algorithms include:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，树基模型不受特征缩放的影响。这应该很明显，但树基建模算法的例子包括：
- en: Plain old decision trees
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 普通决策树
- en: Random forests
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: Gradient boosted trees
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升树
- en: '**Why is this?**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么会这样？**'
- en: Imagine we multiply the scale of a particular feature's values (and only that
    feature's values) by 100\. If we think in terms of the positioning of data points
    along an axis corresponding to this feature, there will be a clear effect on a
    model such as a nearest neighbors classifier, for instance. Linear models, nearest
    neighbors, and neural networks are all sensitive to the relative positioning of
    data point values to one another; in fact, that's how the explicit premise of
    K-nearest neighbors (kNN).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们将特定特征值的尺度乘以100。如果我们考虑沿着与该特征对应的轴的数据点定位，这将对如最近邻分类器等模型产生明显影响。例如，线性模型、最近邻和神经网络都对数据点值之间的相对位置敏感；实际上，这就是K最近邻（kNN）的明确前提。
- en: Since tree-based models search for the best split positions along a feature's
    data axis, this relative positioning is far less important. If the determined
    split position is between points a and b, this will remain constant if both points
    are multiplied or divided by some constant.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于树基模型在特征数据轴上寻找最佳划分位置，这种相对位置的重要性要小得多。如果确定的划分位置在点a和b之间，那么如果这两个点都被某个常数乘或除，这一点将保持不变。
- en: '![Image](../Images/666fca2f624b4f62bddf531313ba86a3.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/666fca2f624b4f62bddf531313ba86a3.png)'
- en: Resulting models from algorithms such as kNN on the left are susceptible to
    feature scaling; tree-based models such as those on the right are not susceptible
    to feature scaling
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的算法，如kNN，生成的模型易受特征缩放的影响；右侧的树基模型则不受特征缩放的影响
- en: A potential use of feature scaling beyond the obvious is testing feature importance.
    For kNN, for example, the larger a given feature's values are, the more impact
    they will have on a model. This fact can be taken advantage of by intentionally
    boosting the scale of a feature or features which we may believe to be of greater
    importance, and see if slight deviations in these larger values impact the resulting
    model. Conversely, multiplying a feature's data point values by zero will result
    in stacked data points along that axis, rendering this feature useless in the
    modeling process.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 特征缩放的一个潜在用途是测试特征的重要性。例如，对于 kNN 来说，给定特征的值越大，它对模型的影响越大。我们可以通过有意放大一个或多个我们认为更重要的特征的尺度，来利用这一事实，观察这些更大值的轻微变化是否会影响结果模型。相反，将特征的数据点值乘以零会导致沿该轴的数据点堆叠，使该特征在建模过程中无用。
- en: 'This covers linear models, but what about neural networks? It turns out that
    neural networks are greatly affected by scale as well: the impact of [regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics))
    is proportional to feature scale. Gradient descent methods can have trouble without
    proper scaling.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这涵盖了线性模型，但神经网络呢？事实证明，神经网络也受到尺度的重大影响：[正则化](https://en.wikipedia.org/wiki/Regularization_(mathematics))的影响与特征尺度成正比。如果没有适当的缩放，梯度下降方法可能会遇到问题。
- en: 'Finally, to provide the simplest example of why feature scaling is important:
    consider k-means clustering. If your data has 4 columns, with 3 of the columns''
    values being scaled between 0 and 1, and the fourth possessing values between
    0 and 1,000, it is relatively easy to determine which of these features will be
    the sole determinant of clustering results.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，提供一个最简单的特征缩放重要性示例：考虑 k-means 聚类。如果你的数据有4列，其中3列的值在0和1之间缩放，而第四列的值在0和1,000之间，那么相对容易确定这些特征中的哪一个将是聚类结果的唯一决定因素。
- en: '**Feature Scaling Methods**'
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**特征缩放方法**'
- en: '**Min/Max Scaling**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**最小/最大缩放**'
- en: '![Formula](../Images/8ec67a92d352f40501d4e8c926490369.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![公式](../Images/8ec67a92d352f40501d4e8c926490369.png)'
- en: This method scales values between 0 and 1 ???? [0, 1]
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法将值缩放在0和1之间 ???? [0, 1]
- en: Sets minimum equal to zero, maximum equal to one, and all other values scaled
    accordingly in between
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将最小值设置为零，最大值设置为一，所有其他值相应地缩放在两者之间
- en: The Scikit-learn [`sklearn.preprocessing.MinMaxScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)
    module is such an implementation
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-learn [`sklearn.preprocessing.MinMaxScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)
    模块就是这样一个实现
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Standard Scaling**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**标准缩放**'
- en: '![Formula](../Images/20e63d6c5f250e7fd176da67e0a378ba.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![公式](../Images/20e63d6c5f250e7fd176da67e0a378ba.png)'
- en: This method scales by setting the mean of the data to 0, and the standard deviation
    to 1
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法通过将数据的均值设置为0，将标准差设置为1来进行缩放
- en: Subtracts the mean value from feature value, and divides by standard deviation
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从特征值中减去均值，并除以标准差
- en: This results in the standardized distribution
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这导致了标准化分布
- en: The Scikit-learn [`sklearn.preprocessing.StandardScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)
    module is such an implementation
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-learn [`sklearn.preprocessing.StandardScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)
    模块就是这样一个实现
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: What is the difference between these (and other) scaling methods in practice?
    This is dependent on the specific dataset they are applied to, but in practice
    the results are often *roughly* similar.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这些（及其他）缩放方法在实践中的区别是什么？这取决于应用的具体数据集，但实际上结果通常是*大致*相似的。
- en: Dealing with Outliers
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理异常值
- en: People often push back hard against talk of "removing outliers." The reality
    is that outliers can unhelpfully skew a model. The real issue is not that removing
    outliers is, in itself, an issue; however, you need to understand outliers as
    they relate to the data, and make an informed judgement as to whether or not they
    should be removed, and exactly why this is the case. An alternative to removing
    outliers is using some form of scaling to manage them.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 人们经常强烈反对“去除异常值”的讨论。现实情况是，异常值可能会不利地扭曲模型。真正的问题不是去除异常值本身，而是你需要理解异常值与数据的关系，并做出是否应去除它们以及原因的知情判断。去除异常值的替代方法是使用某种形式的缩放来管理它们。
- en: Outliers can be an issue not just for features, but also potentially for target
    values. If all target values are between 0 and 1, for example, and a new data
    point is introduced with a target value of 100,000, a resulting regression model
    would start predicting artificially high values for the rest of our data points
    in the model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 离群值不仅对特征可能是个问题，对目标值也可能如此。例如，如果所有目标值都在0到1之间，而引入了一个目标值为100,000的新数据点，那么生成的回归模型会开始对模型中的其他数据点进行不真实的高预测。
- en: Keep in mind that outliers can often represent NaNs, especially if they fall
    outside of upper and lower bounds and are tightly clustered (all NaNs may have
    been converted to -999, for example). This type of outlier skews distribution
    artificially, and some approach to dealing with them is absolutely necessary.
    Domain knowledge and data inspection are the most useful tools for deciding how
    to deal with outliers.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，离群值通常代表NaNs，特别是当它们超出上下界且紧密聚集时（例如，所有NaNs可能已经转换为-999）。这种类型的离群值会人为地扭曲分布，因此绝对需要一些处理方法。领域知识和数据检查是决定如何处理离群值的最有用工具。
- en: '**Clipping**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**裁剪**'
- en: One approach to fix outliers for linear models is to keep values between upper
    and lower bounds. How do we choose these bounds? A reasonable approach is using
    percentiles -- keep the values between, say, the first and 99th percentiles only.
    This form of clipping is referred to as [winsorization](https://en.wikipedia.org/wiki/Winsorizing).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 修正线性模型中的离群值的一种方法是将值保持在上下界之间。我们如何选择这些界限？一种合理的方法是使用百分位数——仅保持在第一个和第99个百分位数之间的值。这种裁剪形式被称为[温莎化](https://en.wikipedia.org/wiki/Winsorizing)。
- en: Refer to [`scipy.stats.mstats.winsorize`](https://docs.scipy.org/doc/scipy-0.13.0/reference/generated/scipy.stats.mstats.winsorize.html)
    for more info on winsorization.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有关温莎化的更多信息，请参考[`scipy.stats.mstats.winsorize`](https://docs.scipy.org/doc/scipy-0.13.0/reference/generated/scipy.stats.mstats.winsorize.html)。
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Rank Transform**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**排名变换**'
- en: 'Rank transform removes the relative distance between feature values and replaces
    them with a consistent interval representing feature value ranking (think: first,
    second, third...). This moves outliers closer to other feature values (at a regular
    interval), and can be a valid approach for linear models, kNN, and neural networks.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 排名变换去除特征值之间的相对距离，并用表示特征值排名的一致间隔替代（例如：第一、第二、第三...）。这将离群值移动到其他特征值附近（在一个规律的间隔内），并且可以作为线性模型、kNN和神经网络的有效方法。
- en: Refer to [`scipy.stats.rankdata`](https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.rankdata.html)
    for more on rank transform.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 有关排名变换的更多信息，请参考[`scipy.stats.rankdata`](https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.rankdata.html)。
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Log Transform**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**日志变换**'
- en: Log transforms can be useful for non-tree-based models. They can be used to
    drive larger feature values away from extremes and closer to the mean feature
    values. It also has the effect of making values closer to zero more distinguishable.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非树模型，日志变换可能非常有用。它们可以将较大的特征值从极端值驱离，靠近特征均值。它还有使接近零的值更易区分的效果。
- en: See [`numpy.log`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.log.html)
    and [`numpy.sqrt`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.sqrt.html)
    for more information on implementing these transforms.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有关实施这些变换的更多信息，请参见[`numpy.log`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.log.html)和[`numpy.sqrt`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.sqrt.html)。
- en: It may be useful to combine a variety of preprocessing methods together into
    one model, by preprocessing percentages of your dataset instances using different
    methods, and concatenating the results into a single training set. Alternatively,
    mixing models which were trained on differently preprocessed data (compare with
    the former method description) via ensembling could also prove useful.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 将各种预处理方法结合到一个模型中可能会很有用，通过使用不同的方法预处理数据集实例的百分比，然后将结果合并到一个训练集中。或者，通过集成在不同预处理数据上训练的模型（与前述方法描述比较）也可能是有益的。
- en: Preprocessing is not a "one size fits all" situation, and your willingness to
    experiment with alternative approaches based on sound mathematical principles
    can perhaps lead to increased model robustness.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理不是“万能”的情况，你愿意根据可靠的数学原理尝试不同的方法，可能会增加模型的稳健性。
- en: '**References:**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: '[How to Win a Data Science Competition: Learn from Top Kagglers](https://www.coursera.org/learn/competitive-data-science),
    National Research University Higher School of Economics (Coursera)'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[如何赢得数据科学竞赛：向顶尖 Kagglers 学习](https://www.coursera.org/learn/competitive-data-science)，国家研究大学高等经济学院（Coursera）'
- en: '**Related:**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[7 Steps to Mastering Data Preparation with Python](/2017/06/7-steps-mastering-data-preparation-python.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握数据准备的 7 个步骤，使用 Python](/2017/06/7-steps-mastering-data-preparation-python.html)'
- en: '[Text Data Preprocessing: A Walkthrough in Python](/2018/03/text-data-preprocessing-walkthrough-python.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本数据预处理：Python 实践指南](/2018/03/text-data-preprocessing-walkthrough-python.html)'
- en: '[Data Preparation Tips, Tricks, and Tools: An Interview with the Insiders](/2016/10/data-preparation-tips-tricks-tools.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据准备技巧、窍门和工具：与内部人士的访谈](/2016/10/data-preparation-tips-tricks-tools.html)'
- en: '* * *'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三名课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行 IT 管理'
- en: '* * *'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[SQL Notes for Professionals: The Free eBook Review](https://www.kdnuggets.com/2022/05/sql-notes-professionals-free-ebook-review.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQL 专业笔记：免费电子书评审](https://www.kdnuggets.com/2022/05/sql-notes-professionals-free-ebook-review.html)'
- en: '[KDnuggets News, May 11: SQL Notes for Professionals; How To…](https://www.kdnuggets.com/2022/n19.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，5 月 11 日：SQL 专业笔记；如何...](https://www.kdnuggets.com/2022/n19.html)'
- en: '[Feature Store Summit 2022: A free conference on Feature Engineering](https://www.kdnuggets.com/2022/10/hopsworks-feature-store-summit-2022-free-conference-feature-engineering.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[特征存储峰会 2022：关于特征工程的免费会议](https://www.kdnuggets.com/2022/10/hopsworks-feature-store-summit-2022-free-conference-feature-engineering.html)'
- en: '[7 Steps to Mastering Data Cleaning and Preprocessing Techniques](https://www.kdnuggets.com/2023/08/7-steps-mastering-data-cleaning-preprocessing-techniques.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握数据清理和预处理技巧的 7 个步骤](https://www.kdnuggets.com/2023/08/7-steps-mastering-data-cleaning-preprocessing-techniques.html)'
- en: '[Harnessing ChatGPT for Automated Data Cleaning and Preprocessing](https://www.kdnuggets.com/2023/08/harnessing-chatgpt-automated-data-cleaning-preprocessing.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用 ChatGPT 实现自动化数据清理和预处理](https://www.kdnuggets.com/2023/08/harnessing-chatgpt-automated-data-cleaning-preprocessing.html)'
- en: '[Cleaning and Preprocessing Text Data in Pandas for NLP Tasks](https://www.kdnuggets.com/cleaning-and-preprocessing-text-data-in-pandas-for-nlp-tasks)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Pandas 中清理和预处理文本数据用于 NLP 任务](https://www.kdnuggets.com/cleaning-and-preprocessing-text-data-in-pandas-for-nlp-tasks)'
