- en: Deep Learning for Virtual Try On Clothes – Challenges and Opportunities
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟试衣服装的深度学习 – 挑战与机遇。
- en: 原文：[https://www.kdnuggets.com/2020/10/deep-learning-virtual-try-clothes.html](https://www.kdnuggets.com/2020/10/deep-learning-virtual-try-clothes.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/10/deep-learning-virtual-try-clothes.html](https://www.kdnuggets.com/2020/10/deep-learning-virtual-try-clothes.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Maksym Tatariants](https://www.linkedin.com/in/maksym-tatariants/), Data
    Science Engineer at [MobiDev](https://mobidev.biz/services/machine-learning-consulting)**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：[Maksym Tatariants](https://www.linkedin.com/in/maksym-tatariants/)，MobiDev
    的数据科学工程师**。'
- en: The research described below was held by MobiDev as a part of an investigation
    on bringing [AR & AI technologies for virtual fitting room development](https://mobidev.biz/blog/ar-ai-technologies-virtual-fitting-room-development).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 下述研究由 MobiDev 进行，作为将[AR 和 AI 技术用于虚拟试衣间开发](https://mobidev.biz/blog/ar-ai-technologies-virtual-fitting-room-development)的调查的一部分。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的快车道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能。'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的IT需求。'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Exploring 2D Cloth Transfer onto an Image of a Person
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索将 2D 服装转移到人物图像上。
- en: When working on virtual fitting room apps, we conducted a series of experiments
    with virtual try on clothes and found out that the proper rendering of a 3D clothes
    model on a person still remains a challenge. For a convincing AR experience, the
    deep learning model should detect not only the basic set of keypoints corresponding
    to the joints of the human body. It should also identify the body's actual shape
    in three dimensions so that the clothing could be appropriately fitted to the
    body.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发虚拟试衣间应用程序时，我们进行了系列实验，尝试了虚拟试衣，并发现 3D 服装模型在人体上的适当渲染仍然是一个挑战。为了实现令人信服的 AR 体验，深度学习模型不仅应该检测到与人体关节对应的基本关键点集合，还应该识别身体的实际三维形状，以便衣物能够适当地贴合身体。
- en: For an example of this model type, we can look at the [DensePose](https://github.com/facebookresearch/Densepose)
    by the Facebook research team (**Fig. 1**). However, this approach is not accurate,
    slow for mobile, and expensive.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种模型类型的一个示例，我们可以查看 Facebook 研究团队的[DensePose](https://github.com/facebookresearch/Densepose)（**图
    1**）。然而，这种方法不够准确、对移动设备而言速度较慢且成本高昂。
- en: '![](../Images/c454a7b8aa2c0b2598991b23e2054bb1.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c454a7b8aa2c0b2598991b23e2054bb1.png)'
- en: '***Figure 1**: Body mesh detection using DensePose ([source](https://www.skylab.ai/product/product-pose-recognition)).*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '***图 1**：使用 DensePose 进行身体网格检测 ([来源](https://www.skylab.ai/product/product-pose-recognition))。*'
- en: So, it is required to search for simpler alternatives to virtual clothing try-on
    techniques.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，需要寻找虚拟服装试穿技术的更简单替代方案。
- en: A popular option here is, instead of going for fitting 3D clothing items, working
    with 2D clothing items and 2D person silhouettes. It is exactly what [Zeekit](https://zeekit.me/)
    company does, giving users a possibility to apply several clothing types (dresses,
    pants, shirts, etc.) to their photo.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一个受欢迎的选择是，与其使用 3D 服装进行试穿，不如使用 2D 服装和 2D 人体轮廓。这正是[Zeekit](https://zeekit.me/)公司所做的，为用户提供了将多种服装类型（如连衣裙、裤子、衬衫等）应用到他们的照片上的可能性。
- en: '![](../Images/43af6ef9d0a9b7ecb05a7ea14e0c3f24.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43af6ef9d0a9b7ecb05a7ea14e0c3f24.png)'
- en: '***Figure 2**: 2D clothing try-on, Zeekit ([source](https://www.youtube.com/watch?v=IXIbeBQwgDA),
    0:29 - 0:39).*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '***图 2**：2D 服装试穿，Zeekit ([来源](https://www.youtube.com/watch?v=IXIbeBQwgDA),
    0:29 - 0:39)。*'
- en: Since the cloth transferring techniques used by the company have not been revealed
    besides incorporating deep learning models, let's refer to scientific articles
    on the topic. Upon reviewing several of the most recent works ([source 1](https://arxiv.org/abs/1807.07688),
    [source 2](https://arxiv.org/abs/2003.05863), [source 3](https://arxiv.org/abs/1912.06324)),
    the predominant approach to the problem is to use Generative Adversarial Networks
    ([GANs](https://towardsdatascience.com/generative-adversarial-networks-explained-34472718707a))
    in combination with Pose Estimation and [Human Parsing](http://sysu-hcp.net/lip/index.php)
    models. The utilization of the last two models helps identify the areas in the
    image corresponding to specific body parts and determine the position of body
    parts. The use of Generative Models helps produce a warped image of the transferred
    clothing and apply it to the image of the person so as to minimize the number
    of produced artifacts.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于公司使用的布料转移技术除了结合深度学习模型之外尚未披露，我们可以参考相关的科学文章。在审阅了几篇最新的文章（[source 1](https://arxiv.org/abs/1807.07688),
    [source 2](https://arxiv.org/abs/2003.05863), [source 3](https://arxiv.org/abs/1912.06324)）后，发现主要方法是使用生成对抗网络（[GANs](https://towardsdatascience.com/generative-adversarial-networks-explained-34472718707a)）结合姿态估计和[人体解析](http://sysu-hcp.net/lip/index.php)模型。利用后两者模型可以帮助识别图像中与特定身体部位对应的区域，并确定身体部位的位置。生成模型的使用有助于生成转移衣物的变形图像并将其应用于人的图像，以减少产生的伪影数量。
- en: Selected Model and Research Plan
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选定模型和研究计划
- en: For this research, we chose the Adaptive Content Generating and Preserving Network
    (ACGPN) model described in the “[Towards Photo-Realistic Virtual Try-On by Adaptively
    GeneratingPreserving](https://arxiv.org/abs/2003.05863v1)[↔](https://arxiv.org/abs/2003.05863v1)[Image
    Content](https://arxiv.org/abs/2003.05863v1)” paper. In order to explain how ACGPN
    works, let’s review its architecture shown in **Fig. 3**.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这项研究，我们选择了在“[Towards Photo-Realistic Virtual Try-On by Adaptively GeneratingPreserving](https://arxiv.org/abs/2003.05863v1)[↔](https://arxiv.org/abs/2003.05863v1)[Image
    Content](https://arxiv.org/abs/2003.05863v1)”论文中描述的自适应内容生成与保存网络（ACGPN）模型。为了说明ACGPN的工作原理，让我们查看其架构如**图3**所示。
- en: '![](../Images/896bc1e05c38321ad1b5c84d7231b686.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/896bc1e05c38321ad1b5c84d7231b686.png)'
- en: '***Figure 3**: Architecture of the ACGPN model (credit: [Yang et al.,2020](https://arxiv.org/abs/2003.05863)).*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '***图3**：ACGPN模型的架构（致谢： [Yang et al.,2020](https://arxiv.org/abs/2003.05863)）。*'
- en: 'The model consists of three main modules: Semantic Generation, Clothes Warping,
    and Content Fusion.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型包括三个主要模块：语义生成、衣物变形和内容融合。
- en: The Semantic Generation module receives the image of a target clothing and its
    mask, data on the person's pose, a segmentation map with all the body parts (hands
    are especially important), and clothing items identified.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 语义生成模块接收目标衣物的图像及其掩膜、人的姿态数据、包含所有身体部位的分割图（手部尤其重要）以及识别出的衣物项目。
- en: The first generative model (G1) in the Semantic Generation module modifies the
    person’s segmentation map so that it clearly identifies the area on the person’s
    body that should be covered with the target clothes. Having this information received,
    the second generative model (G2) warps the clothing mask so as to correspond to
    the area it should occupy.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 语义生成模块中的第一个生成模型（G1）修改了人的分割图，以清楚标识出应该被目标衣物覆盖的身体区域。获取这些信息后，第二个生成模型（G2）将衣物掩膜进行变形，以符合其应占据的区域。
- en: After that, the warped clothing mask is passed to the Clothes Warping module,
    where the Spatial Transformation Network ([STN](https://arxiv.org/abs/1506.02025))
    warps the clothing image according to the mask. And finally, the warped clothing
    image, the modified segmentation map from Semantic Generation Module, and a person’s
    image are fed into the third generative module (G3), and the final result is produced.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，将变形的衣物掩膜传递给衣物变形模块，其中空间变换网络（[STN](https://arxiv.org/abs/1506.02025)）根据掩膜对衣物图像进行变形。最后，将变形的衣物图像、来自语义生成模块的修改过的分割图和一个人的图像输入到第三个生成模块（G3），并产生最终结果。
- en: '**For testing the capabilities of the selected model, we went through the following
    steps in the order of increasing difficulty:**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**为了测试所选模型的能力，我们按照难度递增的顺序进行了以下步骤：**'
- en: Replication of the [authors’ results](https://arxiv.org/abs/2003.05863v1) on
    the original data and our preprocessing models (Simple).
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在原始数据和我们的预处理模型（简单）的基础上复现了[作者的结果](https://arxiv.org/abs/2003.05863v1)。
- en: Application of custom clothes to default images of a person (Medium).
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将自定义衣物应用于默认人物图像（中等）。
- en: Application of default clothes to custom images of a person (Difficult).
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将默认衣物应用于自定义人物图像（困难）。
- en: Application of custom clothes to custom images of a person (Very difficult).
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将自定义衣物应用于自定义人物图像（非常困难）。
- en: Replication of the Authors’ Results on the Original Data and Our Preprocessing
    Models
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在原始数据和我们的预处理模型上复制作者的结果
- en: The authors of the [original paper](https://arxiv.org/abs/2003.05863v1) did
    not mention the models they used to create person segmentation labels and detect
    the keypoints on a human body. Thus, we picked the models ourselves and ensured
    the quality of the ACGPN model’s outputs were similar to the one reported in the
    paper.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[原始论文](https://arxiv.org/abs/2003.05863v1)的作者没有提到他们用于创建人物分割标签和检测人体上的关键点的模型。因此，我们自行选择了模型，并确保了ACGPN模型输出的质量与论文中报告的相似。'
- en: As a keypoint detector, we chose the **OpenPose** model because it provided
    the appropriate order of keypoints (COCO keypoint dataset) and was used in other
    researches related to the virtual try-on for clothes replacement.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 作为关键点检测器，我们选择了**OpenPose**模型，因为它提供了适当的关键点顺序（COCO关键点数据集），并且在其他与虚拟试衣服更换相关的研究中被使用。
- en: '![](../Images/4c8cce74b90e1f59fd210a896d4af14a.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c8cce74b90e1f59fd210a896d4af14a.png)'
- en: '***Figure 4**: Example of COCO keypoint detections using OpenPose.*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '***图 4**: 使用OpenPose的COCO关键点检测示例。*'
- en: We chose the SCHP model presented in the [Self Correction for Human Parsing](https://arxiv.org/abs/1910.09777)
    paper for the body part segmentation. This model utilizes the common for human
    parsing architecture [CE2P](https://arxiv.org/abs/1809.05996) with some modifications
    of the loss functions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了在[Self Correction for Human Parsing](https://arxiv.org/abs/1910.09777)论文中提出的SCHP模型进行身体部位分割。该模型利用了用于人体解析的常见架构[CE2P](https://arxiv.org/abs/1809.05996)，并对损失函数进行了一些修改。
- en: SCHP segmentation model uses a pre-trained backbone (encoder) to extract features
    from the input image. The recovered features are then used for the contour prediction
    of the person in the edge branch and the person segmentation in the parsing branch.
    The outputs of these two branches alongside feature maps from the encoder were
    fed into the fusion branch to improve the segmentation maps' quality.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: SCHP分割模型使用预训练的骨干（编码器）从输入图像中提取特征。然后使用恢复的特征进行边缘分支中的人体轮廓预测和解析分支中的人体分割。这两个分支的输出连同来自编码器的特征图一起输入到融合分支，以提高分割图的质量。
- en: '![](../Images/3ba651a21dee45db067048896a831b04.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ba651a21dee45db067048896a831b04.png)'
- en: '***Figure 5**: Architecture of the SCHP model (based on CE2P), image credit
    – [Li, et al.](https://arxiv.org/abs/1910.09777)*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '***图 5**: SCHP模型（基于CE2P）的架构，图像来源 – [Li等人](https://arxiv.org/abs/1910.09777)*'
- en: Another new element in the SCHP model is the self-correction feature used to
    iteratively improve the model’s prediction on noisy ground truth labels. These
    labels are commonly used in human parsing tasks since it can be difficult for
    human annotators to produce segmentation labels. During this process, the model,
    firstly trained on inaccurate human annotations, is aggregated with new models
    trained on pseudo-ground truth masks obtained from the previously trained model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: SCHP模型中的另一个新元素是自我纠正特性，用于迭代地改进模型在噪声地面真值标签上的预测。这些标签通常用于人体解析任务，因为人为标注者可能难以生成分割标签。在此过程中，模型首先在不准确的人体标注上进行训练，并与在从先前训练的模型中获得的伪地面真值掩码上训练的新模型进行聚合。
- en: The process is repeated several times until both the model and pseudo-ground
    truth masks reach better accuracy. For the human parsing task, we used the model
    trained on the [Look Into Person (LIP)](http://sysu-hcp.net/lip/) dataset because
    it is the most appropriate for this task.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程会重复几次，直到模型和伪地面真值掩码的准确性达到更好的水平。对于人体解析任务，我们使用了在[Look Into Person (LIP)](http://sysu-hcp.net/lip/)数据集上训练的模型，因为它最适合这个任务。
- en: '![](../Images/7bbd637e56f932ceeef597b5df735c19.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7bbd637e56f932ceeef597b5df735c19.png)'
- en: '***Figure 6**: Examples of human parsing using SCHP model (person - left, segmentation
    - right).*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '***图 6**: 使用SCHP模型的人体解析示例（左侧 - 人体，右侧 - 分割）。*'
- en: Finally, when the keypoint and human parsing models were ready, we used their
    outputs for running the ACGPN model on the same data used by the authors for training.
    In the image below, you can see the results we obtained from the VITON dataset.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当关键点和人体解析模型准备好后，我们使用它们的输出在与作者训练时相同的数据上运行了 ACGPN 模型。在下面的图像中，您可以看到我们从 VITON
    数据集中获得的结果。
- en: The semantic generation module modifies the original segmentation so that it
    reflects the new clothing type. For example, the pullover on the original image
    has long sleeves, whereas the target cloth (T-shirt) has short sleeves. Therefore,
    the segmentation mask should be modified so that the arms are more revealed. This
    transformed segmentation is then used by the Content Fusion module to inpaint
    modified body parts (e.g., draw naked arms), and it is one of the most challenging
    tasks for the system to perform (**Fig. 7**).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 语义生成模块会修改原始分割，以反映新的服装类型。例如，原始图像上的套头衫有长袖，而目标服装（T恤）则有短袖。因此，分割掩码需要修改，以便手臂部分更加显露。这个转化后的分割掩码随后被内容融合模块用于修复修改过的身体部位（例如，绘制裸露的手臂），这是系统执行的**最具挑战性的任务之一**（**图
    7**）。
- en: '![](../Images/60f40ab66588e961626777359429ac2c.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60f40ab66588e961626777359429ac2c.png)'
- en: '***Figure 7**: Inputs and outputs of the ACGPN model.*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '***图 7**：ACGPN 模型的输入和输出。*'
- en: In the image below (**Fig. 8**), you can see the compilation results of successful
    and unsuccessful clothing replacement using the ACGPN model. The most frequent
    errors we encountered were poor inpainting (B1), new clothing overlapping with
    body parts (B2), and edge defects (B3).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图像中（**图 8**），您可以看到使用 ACGPN 模型成功和失败的服装替换结果。我们遇到的最频繁的错误包括修复效果差（B1）、新服装与身体部位重叠（B2）以及边缘缺陷（B3）。
- en: '![](../Images/893869661dd8d833c61984105067b884.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/893869661dd8d833c61984105067b884.png)'
- en: '***Figure 8**: Successful (A1-A3) and unsuccessful (B1-B3) replacement of clothing.
    Artefacts are marked with red rectangles.*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '***图 8**：成功（A1-A3）和失败（B1-B3）的服装替换。伪影用红色矩形标记。*'
- en: Application of Custom Clothes to Default Person Images
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义服装应用于默认人物图像
- en: For this experiment, we picked several clothing items (**Fig. 9**) and applied
    them to images of a person from the VITON dataset. Please note that some images
    are not real clothing photos, but 3D renders or 2D drawings.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实验，我们挑选了几件服装（**图 9**）并将它们应用到 VITON 数据集中人物的图像上。请注意，有些图像不是实际的服装照片，而是 3D 渲染或
    2D 绘图。
- en: '![](../Images/f35954c550e02a8624e596186f1eb490.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f35954c550e02a8624e596186f1eb490.png)'
- en: '***Figure 9**: Clothing images used for virtual try-on (A - photo of an item,
    B, C - 3D renders, D - 2D drawing).*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '***图 9**：用于虚拟试穿的服装图像（A - 项目的照片，B、C - 3D 渲染，D - 2D 绘图）。*'
- en: Moving on to the results of clothing replacement (**Fig. 10**), we can see that
    they may be roughly split into three groups.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 继续查看服装替换的结果（**图 10**），我们可以将其大致分为三组。
- en: '![](../Images/f47a6e251df27ec5d73d4d1963a7a963.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f47a6e251df27ec5d73d4d1963a7a963.png)'
- en: '***Figure 10**: Examples of clothing replacement using custom clothes (Row
    A - successful with minor artifacts, Row B - moderate artifacts, Row C - major
    artifacts).*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '***图 10**：使用自定义服装的替换示例（A 行 - 成功且有少量伪影，B 行 - 中等伪影，C 行 - 大量伪影）。*'
- en: The images in Row A have no defects and look the most natural. This can be attributed
    to the fact that people in the images have a similar upright, facing camera pose.
    As the authors of the [paper](https://arxiv.org/abs/2003.05863v1) explained, such
    a pose makes it easier for the model to define how the new clothing should be
    warped and applied to the person’s image.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: A 行中的图像没有缺陷，看起来最自然。这可以归因于图像中的人物姿势相似，都是直立面向相机。正如[论文](https://arxiv.org/abs/2003.05863v1)的作者所解释的，这种姿势使得模型更容易定义新服装如何被扭曲并应用到人物图像上。
- en: The images in Row B present the more challenging pose to be processed by the
    model. The person’s torso is slightly bent, and arms partially occlude the body
    area where the clothing is supposed to be applied. As shown in **Fig. 8**, a bent
    torso results in the edge defects. Notice that difficult long-sleeve clothing
    (item C from **Fig. 9**) is processed correctly. It is because sleeves should
    go through complicated transformations to be appropriately aligned with the person’s
    arms. It is incredibly complicated if the arms are bent or their silhouette is
    occluded by clothing in the original image.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 行B中的图像展示了模型处理起来更具挑战性的姿势。人物的躯干略微弯曲，手臂部分遮挡了应有服装的身体区域。如**图8**所示，弯曲的躯干会导致边缘缺陷。注意到困难的长袖服装（**图9**中的项目C）被正确处理。这是因为袖子需要经过复杂的变换才能与人物的手臂对齐。如果手臂弯曲或其轮廓被原始图像中的服装遮挡，这会非常复杂。
- en: The images in Row C show examples where the model fails almost completely. It
    is expected behavior since the person in the input images has a hard torso twist
    and arms bent so that they occlude nearly half of the stomach and chest area.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 行C中的图像展示了模型几乎完全失败的例子。这是预期的行为，因为输入图像中的人物躯干扭曲严重，手臂弯曲到遮挡了几乎一半的腹部和胸部区域。
- en: Application of Default Clothes to the Custom Person Images
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 默认服装应用于定制人物图像
- en: Let’s review the experiments of the model application to the unconstrained images
    of people in natural environments. The VITON dataset used for the model training
    has very static lighting conditions and not many variants of camera perspectives
    and poses.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下模型在自然环境中对非约束图像的应用实验。用于模型训练的VITON数据集具有非常静态的光照条件，且相机视角和姿势的变化不多。
- en: When using real images for testing the model, we realized that the difference
    between the training data and unconstrained data significantly diminishes the
    quality of the model’s output. The example of this issue you can see in **Fig.
    11.**
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用真实图像测试模型时，我们意识到训练数据与非约束数据之间的差异显著降低了模型输出的质量。你可以在**图11**中看到这个问题的例子。
- en: '![](../Images/d1d1cba475ffc62fc9df0e4f30e679de.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1d1cba475ffc62fc9df0e4f30e679de.png)'
- en: '**Figure 11**: Clothing replacement - the impact of background dissimilarity
    with the training data. Row A - original background, row B - background replaced
    with a background similar to the one in VITON dataset.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**图11**：服装替换 - 背景与训练数据的不相似性的影响。行A - 原始背景，行B - 用与VITON数据集中的背景相似的背景替换的背景。'
- en: We found images of a person who had a similar pose and camera perspective to
    the training dataset images and saw numerous artifacts present after processing
    (Row A). However, after removing the unusual background texture and filling the
    area with the same background color as in the training dataset, the received output
    quality was improved (although some artifacts were still present).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们找到了一些姿势和相机视角与训练数据集图像相似的人物图像，并且在处理后发现了许多伪影（行A）。然而，在去除不寻常的背景纹理并用与训练数据集相同的背景颜色填充该区域后，输出质量有所提高（尽管仍有一些伪影存在）。
- en: When testing the model using more images, we discovered that the model performed
    semi-decently on the images similar to the ones from the training distribution
    and failed completely where the input was distinct enough. You can see the more
    successful attempts of applying the model and the typical issues we found in **Fig
    12**.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用更多图像测试模型时，我们发现模型在与训练分布相似的图像上表现尚可，而在输入明显不同的图像上则完全失败。你可以在**图12**中看到应用模型的更成功的尝试以及我们发现的典型问题。
- en: '![](../Images/307810ddde2af8e850da9ecc203c0086.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/307810ddde2af8e850da9ecc203c0086.png)'
- en: '***Figure 12**: Outputs of clothing replacement on images with an unconstrained
    environment (Row A -minor artifacts, Row B- moderate artifacts, Row C - major
    artifacts).*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '***图12**：在非约束环境中的图像上的服装替换输出（行A - 小瑕疵，行B - 中等瑕疵，行C - 主要瑕疵）。*'
- en: The images in Row A show the examples of places where the main defects are edge
    defects.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 行A中的图像展示了主要缺陷是边缘缺陷的地方的例子。
- en: The images in Row B show more critical cases of masking errors. For example,
    cloth blurring, holes, and skin/clothing patches in those places where they should
    not be present.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 行B中的图像显示了更严重的遮罩错误情况。例如，布料模糊、孔洞以及出现皮肤/服装补丁的位置不应存在这些问题。
- en: The images in Row C show severe inpainting errors like poorly drawn arms and
    masking errors, like the unmasked part of the body.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 行C中的图像展示了严重的修补错误，比如手臂绘制不佳和遮罩错误，如身体未遮罩的部分。
- en: Application of Custom Clothes to the Custom Person Images
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将定制服装应用于定制人物图像
- en: Here we tested how well the model can handle both custom clothing and custom
    person photos and divided results into three groups again.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们测试了模型在处理定制服装和定制人物照片方面的效果，并将结果再次分为三组。
- en: '![](../Images/502d7f03aefd18a3fad64187f8420a64.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/502d7f03aefd18a3fad64187f8420a64.png)'
- en: '***Figure 13**: Clothing replacement with an unconstrained environment and
    custom clothing images.*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '***图13***：在不受约束的环境和定制服装图像中替换衣物。'
- en: The images in Row A display the best result we could obtain from the model.
    The combination of custom clothes and custom person images proved to be too difficult
    for processing without at least moderate artifacts.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 行A中的图像显示了我们从模型中获得的最佳结果。定制服装和定制人物图像的组合被证明在处理时难度较大，除非有中等程度的伪影。
- en: The images in Row B display results where the artifacts became more abundant.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 行B中的图像显示了伪影变得更加丰富的结果。
- en: The images in Row C display the most severely distorted results due to the transformation
    errors.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 行C中的图像显示了由于变换错误导致的最严重的失真结果。
- en: Future Plans
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 未来计划
- en: ACGPN model has its limitations, such as the training data must contain paired
    images of target clothes and people wearing those specific clothes.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ACGPN模型有其局限性，例如训练数据必须包含目标衣物和穿着这些特定衣物的人物的配对图像。
- en: Considering everything described above, there might be an impression that a
    virtual try on clothes is non-implementable, but it’s not. Being a challengeable
    task now, it is also providing a window of opportunity for AI-based innovations
    in the future. And there are already new approaches designed to solve those issues.
    Another important thing is to take the technology capabilities into account when
    choosing a proper use case scenario.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到上述所有描述，可能会产生虚拟试穿衣物不可实现的印象，但事实并非如此。虽然现在是一个具有挑战性的任务，但它也为未来基于AI的创新提供了机会窗口。而且，已经有新的方法被设计来解决这些问题。另一个重要的事情是，在选择合适的用例场景时，要考虑技术能力。
- en: '**Related:**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Data Science in Fashion](https://www.kdnuggets.com/2018/03/data-science-fashion.html)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[时尚中的数据科学](https://www.kdnuggets.com/2018/03/data-science-fashion.html)'
- en: '[Graduating in GANs: Going From Understanding Generative Adversarial Networks
    to Running Your Own](https://www.kdnuggets.com/2019/04/graduating-gans-understanding-generative-adversarial-networks.html)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在GANs中毕业：从理解生成对抗网络到运行自己的网络](https://www.kdnuggets.com/2019/04/graduating-gans-understanding-generative-adversarial-networks.html)'
- en: '[3D Human Pose Estimation Experiments and Analysis](https://www.kdnuggets.com/2020/08/3d-human-pose-estimation-experiments-analysis.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3D人体姿态估计实验与分析](https://www.kdnuggets.com/2020/08/3d-human-pose-estimation-experiments-analysis.html)'
- en: More On This Topic
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Be Part of The AI Con USA 2024 with a Free Virtual Pass](https://www.kdnuggets.com/2024/05/ai-con-usa-navigate-the-future-of-ai-with-a-free-virtual-pass)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过免费虚拟通行证成为AI Con USA 2024的一部分](https://www.kdnuggets.com/2024/05/ai-con-usa-navigate-the-future-of-ai-with-a-free-virtual-pass)'
- en: '[10 Simple Things to Try Before Neural Networks](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在神经网络之前尝试的10个简单方法](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
- en: '[5 Amazing & Free LLMs Playgrounds You Need to Try in 2023](https://www.kdnuggets.com/5-amazing-free-llms-playgrounds-you-need-to-try-in-2023)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2023年你需要尝试的5个令人惊叹的免费LLMs游乐场](https://www.kdnuggets.com/5-amazing-free-llms-playgrounds-you-need-to-try-in-2023)'
- en: '[The 5 Best Vector Databases You Must Try in 2024](https://www.kdnuggets.com/the-5-best-vector-databases-you-must-try-in-2024)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2024年你必须尝试的5个最佳向量数据库](https://www.kdnuggets.com/the-5-best-vector-databases-you-must-try-in-2024)'
- en: '[Top 5 AI Coding Assistants You Must Try](https://www.kdnuggets.com/top-5-ai-coding-assistants-you-must-try)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你必须尝试的前5款AI编码助手](https://www.kdnuggets.com/top-5-ai-coding-assistants-you-must-try)'
- en: '[7 End-to-End MLOps Platforms You Must Try in 2024](https://www.kdnuggets.com/7-end-to-end-mlops-platforms-you-must-try-in-2024)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2024年你必须尝试的7个平台端到端MLOps平台](https://www.kdnuggets.com/7-end-to-end-mlops-platforms-you-must-try-in-2024)'
