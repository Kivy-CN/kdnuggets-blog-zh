# 数据科学中的基本数学：积分与曲线下的面积

> 原文：[https://www.kdnuggets.com/2020/11/essential-math-data-science-integrals-area-under-curve.html](https://www.kdnuggets.com/2020/11/essential-math-data-science-integrals-area-under-curve.html)

[评论](#comments)[![图片](../Images/45c05efbbd9635efb62adde614427da4.png)](https://www.essentialmathfordatascience.com/)

微积分是数学的一个分支，提供了研究函数变化率的工具，通过两个主要领域：导数和积分。在机器学习和数据科学的背景下，你可能会使用积分来计算曲线下的面积（例如，评估模型性能的ROC曲线，或从密度计算概率）。

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析能力

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的 IT

* * *

在本文中，你将了解积分和曲线下的面积，使用实际数据科学的例子——ROC曲线下的面积，用于比较两个机器学习模型的性能。基于这个例子，你将从数学的角度了解曲线下的面积和积分（来自我的书《数据科学中的基本数学》）。

### 实践项目

假设你希望通过各种化学性质来预测酒的质量。你想对质量进行二元分类（区分非常好的酒和不是很好的酒）。你将开发方法来评估你的模型，考虑到数据不平衡的情况，并使用接收者操作特征（ROC）曲线下的面积。

**数据集**

为了实现这一目标，我们将使用一个展示红酒各种化学性质及其质量评级的数据集。数据集来自这里：[https://archive.ics.uci.edu/ml/datasets/wine+quality](https://archive.ics.uci.edu/ml/datasets/wine+quality)。相关论文是Cortez, Paulo等人的“通过从物理化学性质数据挖掘建模酒品偏好。”《决策支持系统》47.4 (2009): 547-553。

![图示](../Images/622f9846821726cb0886d511fd86d51c.png)

*图1：酒品质量建模的示意图。*

如图1所示，数据集表示了酒的化学分析（特征）和质量评级。这个评级是目标：这是你将尝试估计的内容。

首先，让我们加载数据并查看特征：

[PRE0]

[PRE1]

最后一列`quality`很重要，因为你将其用作分类的目标。质量由3到8的评分描述：

[PRE2]

[PRE3]

由于目标是对*非常好*的红酒进行分类，让我们决定当评分为7或8时酒是非常好的，否则不是非常好。

我们创建数据集，其中`y`是质量（因变量，0表示评分低于7，1表示评分等于或高于7），`X`包含所有其他特征。

[PRE4]

首先，在查看数据之前，你需要将数据分为一部分用于训练你的算法（训练集）和一部分用于测试它们（测试集）。这将允许你评估模型在训练过程中未见过的数据上的表现。

[PRE5]

**预处理**

首先，让我们对数据进行标准化，以帮助算法的收敛。你可以使用Sklearn中的`StandardScaler`类。

注意，你不希望使用测试集的数据来进行标准化。`fit_transform()`方法计算标准化所需的参数并同时应用。然后，你可以将相同的标准化应用于测试集而无需重新拟合。

[PRE6]

**第一个模型**

作为第一个模型，我们在训练集上训练一个逻辑回归模型，并计算测试集上的分类准确率（正确分类的百分比）：

[PRE7]

[PRE8]

准确率大约为0.87，这意味着87%的测试样本被正确分类。你对此结果满意吗？

### 不平衡数据集的指标

**不平衡数据集**

由于我们将数据分为非常好的酒和不是非常好的酒，因此数据集是*不平衡的*：每个目标类别的数据量不同。

让我们检查一下你在负类（不是很好酒）和正类（非常好酒）中有多少观察值：

[PRE9]

[PRE10]

[PRE11]

[PRE12]

这表明大约86.5%的样本对应于类别0，13.5%对应于类别1。

**简单模型**

为了说明准确率和不平衡数据集的问题，我们创建一个模型作为基准，并查看其表现。这将帮助你了解使用其他指标的优势。

一个非常简单的模型使用数据集不平衡的事实，会始终估计观察数量最多的类别。在你的情况下，这样的模型会始终估计所有酒都是坏的，并获得不错的准确率。

我们通过创建低于0.5的随机概率来模拟这个模型（例如，0.15的概率意味着该类别有15%的可能性是正类）。我们需要这些概率来计算准确率和其他指标。

[PRE13]

[PRE14]

假设当概率超过0.5时，类别被估计为正类：

[PRE15]

[PRE16]

变量`y_pred_random`仅包含零。让我们评估这个随机模型的准确率：

[PRE17]

[PRE18]

这表明，即使是随机模型，准确率也并不差：这并不意味着模型是好的。

总结来说，当每个类别的观察数量不同的时候，你不能仅依赖准确率来评估模型的性能。在我们的例子中，模型可能只输出零，你会得到大约86%的准确率。

你需要其他指标来评估具有不平衡数据集的模型性能。

**ROC曲线**

准确率的一个良好替代是接收者操作特征（ROC）曲线。你可以查看Aurélien Géron在《Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems》中的非常好的ROC曲线解释。O'Reilly Media，2019。

主要思想是将模型的估计分为四类：

+   真正的正例（TP）：预测为1，真实类别为1。

+   虚假正例（FP）：预测为1，但真实类别为0。

+   真正的负例（TN）：预测为0，真实类别为0。

+   虚假负例（FN）：预测为0，但真实类别为1。

让我们计算这些值以便用于你的第一个逻辑回归模型。你可以使用Sklearn中的`confusion_matrix`函数。它呈现的表格组织如下：

![图](../Images/3da6c540c32f461e174300a202c16a44.png)

*图2：混淆矩阵的示例。*

[PRE19]

[PRE20]

你可以看到，没有任何正例被随机模型正确分类（TP）。

**决策阈值**

在分类任务中，你想估计数据样本的类别。对于像逻辑回归这样的模型，它输出0到1之间的概率，你需要使用*决策阈值*或仅*阈值*将此分数转换为类别0或1。高于阈值的概率被视为正类。例如，使用默认的决策阈值0.5，当模型输出分数高于0.5时，你认为估计的类别为1。

然而，你可以选择其他阈值，你用来评估模型性能的指标将取决于这个阈值。

使用ROC曲线，你会考虑0到1之间的多个阈值，并计算每个阈值下的真正正例率作为虚假正例率的函数。

你可以使用Sklearn中的`roc_curve`函数来计算虚假正例率（fpr）和真正正例率（tpr）。该函数还输出对应的阈值。让我们尝试用我们的模拟随机模型进行计算，其中输出值仅低于0.5（`y_pred_random_proba`）。

[PRE21]

让我们来看一下输出：

[PRE22]

[PRE23]

[PRE24]

[PRE25]

[PRE26]

[PRE27]

现在你可以从这些值绘制ROC曲线：

[PRE28]

![图](../Images/0551fd39a886a7516aa8de613be795ad.png)

*图3：对应随机模型的ROC曲线。*

图 3 显示了随机模型对应的 ROC 曲线。它给出了每个阈值下假阳性率的函数的真实正例率。

然而，要小心，阈值范围是从 1 到 0。例如，左下角的点对应于阈值为 1：真实正例为 0，假阳性为 0，因为不可能有高于 1 的概率，所以阈值为 1 时，没有观察值可以被分类为正例。在右上角，阈值为 0，因此所有观察值都被分类为正例，这导致 100% 的真实正例，但也有 100% 的假阳性。

对角线附近的 ROC 曲线意味着模型表现不比随机模型更好，这正是这里的情况。一个完美的模型会对应一个 ROC 曲线，其真实正例率在所有假阳性率值下都是 1。

现在让我们来看一下你之前训练的逻辑回归模型对应的 ROC 曲线。你需要从模型中获取概率，可以使用`predict_proba()`代替`predict`来获得：

[PRE29]

[PRE30]

第一列是类别 0 的分数，第二列是类别 1 的分数（因此，每行的总和为 1），所以你只需保留第二列。

[PRE31]

![图](../Images/e3c42300bd0d64dc35a743fc87c27ab1.png)

*图 4：对应于逻辑回归模型的 ROC 曲线。*

你可以在图 4 中看到你的模型实际上比随机模型更好，这是你无法从模型准确性中得知的（它们是等效的：随机模型约为 0.86，你的模型约为 0.87）。

视觉检查是好的，但拥有一个单一的数值指标来比较你的模型也很重要。这通常由 ROC 曲线下的面积提供。你将在接下来的部分中了解曲线下的面积及其计算方法。

### 积分

*积分*是微分的逆操作。取一个函数*f(x)*并计算其导数*f′(x)*，*不定积分*（也称为*原函数*）给你回到*f(x)*（到一个常数，如你很快将看到的）。

你可以使用积分来计算*曲线下的面积*，即由函数限定的形状的面积，如图 5 所示。

![图](../Images/f07a129e494d5752224664b14379ba83.png)

*图 5：曲线下的面积。*

*确定积分*是指在特定区间上的积分。它对应于该区间内曲线下的面积。

### 示例

你将通过这个示例了解函数积分与曲线下的面积之间的关系。为了说明这个过程，你将通过离散化曲线下的面积来近似计算函数*g(x)=2x*的积分。

**示例说明**

再次以移动火车为例。你看到速度作为时间的函数是距离作为时间的函数的导数。这些函数在图 6 中表示。

![图](../Images/3111fddbf419701aa8e167755e97a86c.png)

*图6：左侧面板显示了*f(x)*，它是时间的函数，表示距离，而右侧面板显示了其导数*g(x)*，它是时间的函数，表示速度。*

图6左侧面板中显示的函数定义为*f(x)=x²*。其导数定义为*g(x)=2x*。

在这个例子中，你将学习如何找到*g(x)*曲线下面积的近似值。

**切片函数**

要近似一个形状的面积，你可以使用切片方法：将形状切分成像矩形这样的简单形状的小切片，计算每个切片的面积并求和。

你将做 exactly 以找到*g(x)*曲线下面积的近似值。

![图](../Images/1908cc700b096a7ac86f4da2dd4fb59c.png)

*图7：通过离散化速度对时间的函数曲线下的面积来近似曲线下的面积。*

图7展示了* f′(x) *的曲线下面积被切分为一秒钟的矩形（我们称之为*Δx*）。请注意，我们低估了面积（看一下遗漏的三角形），但我们稍后会修正这个问题。

让我们尝试理解切片的含义。以第一个为例：它的面积定义为2⋅12⋅1。切片的高度是某一秒钟的速度（值为2）。因此，第一个切片中有两个速度单位对应一个时间单位。面积对应于速度与时间的乘积：这就是距离。

例如，如果你以每小时50英里（速度）行驶两个小时（时间），你行驶了*50⋅2=100英里*（距离）。这是因为速度的单位对应于距离与时间之间的比例（例如每小时英里）。你得到：

![图片](../Images/215aa2a2d5a353ae41f3c673251339f1.png)

总结一下，距离与时间函数的导数是速度与时间函数，而速度与时间函数下的曲线下面积（其积分）给出的是一个距离。这就是导数和积分之间的关系。

**实现**

让我们使用切片法来近似* g(x)=2x *的函数的积分。首先，让我们定义函数* g(x)*：

[PRE32]

如图7所示，你将考虑函数是离散的，并采取*Δx=1*的步长。你可以创建一个从零到六的*x*轴，并对这些值应用函数`g_2x()`。你可以使用Numpy方法`arange(start, stop, step)`来创建一个从`start`到`stop`（不包括）的值数组：

[PRE33]

[PRE34]

[PRE35]

[PRE36]

然后，你可以通过迭代并将宽度（*Δx*）与高度（此时的*y*值）相乘来计算切片的面积。正如你所见，这个面积（下方代码中的`delta_x * y[i-1]`）对应于一个距离（移动的火车在第*i*个切片中行驶的距离）。最后，你可以将结果附加到一个数组中（下方代码中的`slice_area_all`）。

请注意，`y`的索引是`i-1`，因为矩形在我们估算的*x*值的左侧。例如，对于*x=0*和*x=1*，面积为零。

[PRE37]

[PRE38]

这些值是切片的面积。

要计算从开始到相应时间点的行程（而不是每个切片的相应时间点），你可以使用Numpy函数`cumsum()`计算`slice_area_all`的累计和：

[PRE39]

[PRE40]

这是*g(x)*在*x*下的曲线下的面积的估计值。我们知道*g(x)*是*f(x)=x²*的导数，所以我们应该通过对*g(x)*的积分得到*f(x)*。

让我们绘制我们的估计值和*f(x)*，我们称之为“真实函数”，以进行比较：

[PRE41]

![图](../Images/7d1175a378bc8901953ffa92884e9a31.png)

*图8：估计函数与原始函数的比较。*

图8中显示的估计结果表明，估计并不差，但还可以改进。这是因为我们错过了图中红色表示的所有这些三角形。

1.  减少误差的一种方法是取较小的*Δx*值，如图9右侧面板所示。

![图](../Images/ac06608555a71e705993b2faafb6a93e.png)

*图9：速度函数的切片中缺失的部分（以红色标出）。随着*Δx*的减小，误差也变小。*

让我们用*Δx=0.1*来估计积分函数：

[PRE42]

![图](../Images/eb7532de963fc9ab64b708791ecd6004.png)

*图10：较小的切片宽度可以更好地估计原始函数。*

如图10所示，我们恢复了（至少，考虑到一个附加常数）我们所积分的原始函数的导数。

**扩展**

在我们之前的例子中，你对函数*2x*进行了积分，这个函数是线性的，但对于任何连续函数原理是相同的（例如，见图11）。

![图](../Images/baae33f15e4740d3b6e55eb126e4e935.png)

*图11：切片方法可以用于许多线性或非线性函数，包括所有连续函数。*

### Riemann和

使用这种切片方法来近似积分被称为*Riemann和*。Riemann和可以通过不同的方式计算，如图12所示。

![图](../Images/479e0604f9d8c94823f9a2b6ebf7a091.png)

*图12：用于积分近似的四种Riemann和。*

如图12所示，左Riemann和使曲线与矩形的左角对齐。右Riemann和使曲线与矩形的右角对齐。中点规则使曲线与矩形的中心对齐。梯形规则则使用梯形代替矩形。曲线穿过梯形的两个顶角。

### 数学定义

在上一节中，你看到了曲线下的面积与积分之间的关系（你从导数中恢复了原始函数）。现在让我们来看一下积分的数学定义。

对函数*f(x)*相对于*x*的积分表示如下：

*∫f(x)dx*

符号*dx*被称为*x*的*微分*，表示*x*的一个无穷小的变化。它是*x*中接近0的差异。积分的主要思想是对宽度无穷小的无限多个切片进行求和。

符号*∫*是积分符号，表示无限多个切片的总和。

每个切片的高度是 *f(x)*。因此， *f(x)* 和 *dx* 的乘积就是每个切片的面积。最后， *∫f(x):dx* 是对无限多个切片的面积求和（切片的宽度趋向于零）。这就是*曲线下的面积*。

你在上一节中看到如何近似函数积分。但是如果你知道一个函数的导数，你可以通过知道它是逆操作来找回积分。例如，如果你知道：

*d(x2)dx=2x*

你可以得出结论，*2x*的积分是*x2*。但是，有一个问题。如果你在我们的函数中添加一个常数，导数是一样的，因为常数的导数为零。例如，

*d(x²+3)dx=2x*

不可能知道常数的值。因此，你需要在表达式中添加一个未知常数，如下：

*∫2xdx=x²+c*

其中 cc 是一个常数。

**定积分**

在*定积分*的情况下，你可以用积分符号下方和上方的数字来表示积分的区间，如下所示：

*∫baf(x)dx*

它对应于函数 *f(x)* 在 *x=a* 和 *x=b* 之间的曲线下的面积，如图13所示。

![图13：$x=a$ 和 $x=b$ 之间的曲线下的面积。](../Images/3449d551c9a611e27e7626847a3037bc.png)*图13：曲线下的面积在 *x=a* 和 *x=b* 之间。*

### ROC曲线下的面积

现在你知道了曲线下的面积如何与积分相关，让我们看看如何计算它，以便数值上比较你的模型。

记住你在图14中展示了ROC曲线：

[PRE43]

![图](../Images/00654d9a1a7a07306947b512ebcec2f4.png)

*图14：随机模型（蓝色）和逻辑回归模型（绿色）的ROC曲线。*

让我们从随机模型开始。你想对每个真实正例率的值乘以*x*轴上宽度的总和，即相应的假正例率与之前的值之间的差异。你可以通过以下方法获得这些差异：

[PRE44]

[PRE45]

随机模型下ROC曲线下的面积为：

[PRE46]

[PRE47]

或者你可以直接使用Sklearn中的`roc_auc_score()`函数，输入真实目标值和概率：

[PRE48]

[PRE49]

ROC曲线下的面积为0.5对应于一个不比随机更好的模型，面积为1则对应于完美预测。

现在，让我们将这个值与模型的ROC曲线下的面积进行比较：

[PRE50]

[PRE51]

这表明你的模型实际上并不差，你对葡萄酒质量的预测并不是随机的。

在机器学习中，你可以用几行代码来训练复杂的算法。然而，正如你在这里看到的，一些数学知识可以帮助你充分利用这些算法并加快工作进程。它会让你在各个方面都更加得心应手，比如理解机器学习库（如 Sklearn）的文档。

**个人简介：[Hadrien Jean](https://hadrienj.github.io/)** 是一位机器学习科学家。他拥有巴黎高等师范学院的认知科学博士学位，研究了听觉感知，并使用行为学和电生理学数据进行研究。他曾在工业界工作，构建了用于语音处理的深度学习管道。在数据科学与环境交汇的领域，他从事利用深度学习分析音频录音进行生物多样性评估的项目。他还定期在 Le Wagon（数据科学训练营）创作内容并授课，并在他的博客中撰写文章（[hadrienj.github.io](http://hadrienj.github.io)）。

[原文](https://hadrienj.github.io/posts/Essential-Math-Integrals/)。已获许可转载。

**相关内容：**

+   [提升你的数据科学技能。学习线性代数。](/2018/05/boost-data-science-skills-learn-linear-algebra.html)

+   [深度学习的预处理：从协方差矩阵到图像白化](/2018/10/preprocessing-deep-learning-covariance-matrix-image-whitening.html)

+   [数据科学中的基础数学：‘为什么’和‘如何’](/2018/09/essential-math-data-science.html)

### 更多相关内容

+   [如何克服对数学的恐惧并学习数据科学中的数学](https://www.kdnuggets.com/2021/03/overcome-fear-learn-math-data-science.html)

+   [数据科学中的基础数学：特征向量及其在 PCA 中的应用](https://www.kdnuggets.com/2022/06/essential-math-data-science-eigenvectors-application-pca.html)

+   [数据科学中的基础数学：奇异值分解的视觉介绍](https://www.kdnuggets.com/2022/06/essential-math-data-science-visual-introduction-singular-value-decomposition.html)

+   [你在数据科学中需要多少数学？](https://www.kdnuggets.com/2020/06/math-data-science.html)

+   [5 门免费的课程，掌握数据科学中的数学](https://www.kdnuggets.com/5-free-courses-to-master-math-for-data-science)

+   [5 门免费的 MIT 课程，学习数据科学中的数学](https://www.kdnuggets.com/5-free-mit-courses-to-learn-math-for-data-science)
