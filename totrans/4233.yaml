- en: How to Train a BERT Model From Scratch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何从零开始训练BERT模型
- en: 原文：[https://www.kdnuggets.com/2021/08/train-bert-model-scratch.html](https://www.kdnuggets.com/2021/08/train-bert-model-scratch.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/08/train-bert-model-scratch.html](https://www.kdnuggets.com/2021/08/train-bert-model-scratch.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [James Briggs](https://youtube.com/c/JamesBriggs), Data Scientist**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由[James Briggs](https://youtube.com/c/JamesBriggs)，数据科学家**'
- en: '![](../Images/1219fc30e3c1359d89549b3b904a982e.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1219fc30e3c1359d89549b3b904a982e.png)'
- en: BERT, but in Italy — image by author
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: BERT，但在意大利——作者提供的图片
- en: Many of my articles have been focused on BERT — the model that came and dominated
    the world of natural language processing (NLP) and marked a new age for language
    models.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我的许多文章集中在BERT——这一模型的出现主导了自然语言处理（NLP）领域，并为语言模型标志着一个新的时代。
- en: 'For those of you that may not have used transformers models (eg what BERT is)
    before, the process looks a little like this:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些之前可能没有使用过变换器模型（例如BERT是什么）的人，过程看起来有点像这样：
- en: '`pip install transformers`'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pip install transformers`'
- en: Initialize a pre-trained transformers model — `from_pretrained`.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化一个预训练的变换器模型——`from_pretrained`。
- en: Test it on some data.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一些数据上进行测试。
- en: '*Maybe* fine-tune the model (train it some more).'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*也许*微调模型（再训练一些）。'
- en: Now, this is a great approach, but if we only ever do this, we lack the understanding
    behind creating our own transformers models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这是一种很好的方法，但如果我们只做这个，我们就缺乏创建自己变换器模型的理解。
- en: 'And, if we cannot create our own transformer models — we must rely on there
    being a pre-trained model that fits our problem, this is not always the case:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，如果我们不能创建自己的变换器模型——我们必须依赖于有一个适合我们问题的预训练模型，但这并不总是可能的：
- en: '![](../Images/62a875f531afa61a4d0e549c6b97b6c4.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62a875f531afa61a4d0e549c6b97b6c4.png)'
- en: A few comments asking about non-English BERT models
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一些关于非英语BERT模型的评论
- en: So in this article, we will explore the steps we must take to build our own
    transformer model — specifically a further developed version of BERT, called RoBERTa.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这篇文章中，我们将探讨构建自己的变换器模型——特别是BERT的进一步发展版本，称为RoBERTa的步骤。
- en: An Overview
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'There are a few steps to the process, so before we dive in let’s first summarize
    what we need to do. In total, there are four key parts:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程有几个步骤，所以在我们深入之前，让我们首先总结一下我们需要做什么。总共有四个关键部分：
- en: Getting the data
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取数据
- en: Building a tokenizer
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个标记器
- en: Creating an input pipeline
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建输入管道
- en: Training the model
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Once we have worked through each of these sections, we will take the tokenizer
    and model we have built — and save them both so that we can then use them in the
    same way we usually would with `from_pretrained`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成了这些部分，我们将使用我们构建的标记器和模型——并保存它们，以便我们可以像平常一样使用`from_pretrained`。
- en: Getting The Data
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取数据
- en: As with any machine learning project, we need data. In terms of data for training
    a transformer model, we really are spoilt for choice — we can use almost any text
    data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 和任何机器学习项目一样，我们需要数据。在训练变换器模型的数据方面，我们真的有很多选择——我们几乎可以使用任何文本数据。
- en: Video walkthrough for downloading OSCAR dataset using HuggingFace’s datasets
    library
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用HuggingFace的datasets库下载OSCAR数据集的视频讲解
- en: And, if there’s one thing that we have plenty of on the internet — it’s unstructured
    text data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，如果有一件事我们在互联网上有很多——那就是非结构化文本数据。
- en: One of the largest datasets in the domain of text scraped from the internet
    is the OSCAR dataset.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 领域中最大的互联网文本抓取数据集之一是OSCAR数据集。
- en: The OSCAR dataset boasts a huge number of different languages — and one of the
    clearest use-cases for training from scratch is so that we can apply BERT to some
    less commonly used languages, such as Telugu or Navajo.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: OSCAR数据集拥有大量不同的语言——从零开始训练的一个最明确的用例是，我们可以将BERT应用于一些不太常用的语言，如泰卢固语或纳瓦荷语。
- en: Unfortunately, the only language I can speak with any degree of competency is
    English — but my girlfriend is Italian, and so she — Laura, will be assessing
    the results of our Italian-speaking BERT model — FiliBERTo.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我唯一可以熟练使用的语言是英语——但我的女朋友是意大利人，她——劳拉，将评估我们意大利语BERT模型——FiliBERTo的结果。
- en: 'So, to download the Italian segment of the OSCAR dataset we will be using HuggingFace’s `datasets` library
    — which we can install with `pip install datasets`. Then we download OSCAR_IT
    with:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要下载OSCAR数据集的意大利语部分，我们将使用HuggingFace的`datasets`库——我们可以通过`pip install datasets`安装它。然后我们用以下命令下载OSCAR_IT：
- en: Let’s take a look at the `dataset` object.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看`dataset`对象。
- en: Great, now let’s store our data in a format that we can use when building our
    tokenizer. We need to create a set of plaintext files containing just the `text` feature
    from our dataset, and we will split each *sample* using a newline `\n`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，现在让我们将数据存储为在构建分词器时可以使用的格式。我们需要创建一组仅包含数据集中的`text`特征的纯文本文件，并且我们将使用换行符`\n`拆分每个*样本*。
- en: 'Over in our `data/text/oscar_it` directory we will find:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`data/text/oscar_it`目录下，我们将找到：
- en: '![A screenshot displaying a Windows explorer window full of .txt files — representing
    the plaintext OSCAR data](../Images/b859ea91862835c1e2c7b7018e5ba4c6.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![一个显示充满.txt文件的Windows资源管理器窗口的截图——代表纯文本OSCAR数据](../Images/b859ea91862835c1e2c7b7018e5ba4c6.png)'
- en: The directory containing our plaintext OSCAR files
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 包含我们纯文本OSCAR文件的目录
- en: Building a Tokenizer
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建分词器
- en: Next up is the tokenizer! When using transformers we typically load a tokenizer,
    alongside its respective transformer model — the tokenizer is a key component
    in the process.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是分词器！使用transformers时，我们通常加载一个分词器，和相应的transformer模型——分词器是过程中的关键组件。
- en: Video walkthrough for building our custom tokenizer
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义分词器构建的视频演示
- en: When building our tokenizer we will feed it all of our OSCAR data, specify our
    vocabulary size (number of tokens in the tokenizer), and any special tokens.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建我们的分词器时，我们将输入所有的OSCAR数据，指定词汇表大小（分词器中的令牌数）和任何特殊令牌。
- en: 'Now, the RoBERTa special tokens look like this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，RoBERTa的特殊令牌看起来像这样：
- en: So, we make sure to include them within the `special_tokens` parameter of our
    tokenizer’s `train` method call.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们确保在分词器的`train`方法调用中的`special_tokens`参数中包含它们。
- en: 'Our tokenizer is now ready, and we can save it file for later use:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分词器现在准备好了，我们可以将其保存以备后用：
- en: 'Now we have two files that define our new *FiliBERTo *tokenizer:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有两个文件定义了我们新的*FiliBERTo*分词器：
- en: '*merges.txt* — performs the initial mapping of text to tokens'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*merges.txt* — 执行文本到令牌的初始映射'
- en: '*vocab.json* — maps the tokens to token IDs'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*vocab.json* — 将令牌映射到令牌ID'
- en: And with those, we can move on to initializing our tokenizer so that we can
    use it as we would use any other `from_pretrained` tokenizer.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们可以继续初始化我们的分词器，以便像使用任何其他`from_pretrained`分词器一样使用它。
- en: Initializing the Tokenizer
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化分词器
- en: 'We first initialize the tokenizer using the two files we built before — using
    a simple `from_pretrained`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用之前构建的两个文件初始化分词器——使用简单的`from_pretrained`：
- en: Now our tokenizer is ready, we can try encoding some text with it. When encoding
    we use the same two methods we would typically use, `encode` and `encode_batch`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的分词器准备好了，我们可以尝试使用它编码一些文本。编码时，我们使用通常使用的两种方法，`encode`和`encode_batch`。
- en: From the encodings object `tokens` we will be extracting the `input_ids` and `attention_mask` tensors
    for use with FiliBERTo.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从编码对象`tokens`中，我们将提取`input_ids`和`attention_mask`张量，以便与FiliBERTo一起使用。
- en: Creating the Input Pipeline
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建输入管道
- en: The input pipeline of our training process is the more complex part of the entire
    process. It consists of us taking our raw OSCAR training data, transforming it,
    and loading it into a `DataLoader` ready for training.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练过程的输入管道是整个过程中的复杂部分。它包括我们处理原始的OSCAR训练数据，将其转换，并加载到`DataLoader`中以备训练。
- en: Video walkthrough of the MLM input pipeline
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: MLM输入管道的视频演示
- en: Preparing the Data
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据
- en: We’ll start with a single sample and work through the preparation logic.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个样本开始，并处理准备逻辑。
- en: First, we need to open our file — the same files that we saved as *.txt* files
    earlier. We split each based on newline characters `\n` as this indicates the
    individual samples.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要打开文件——之前保存为*.txt*文件的相同文件。我们根据换行符`\n`拆分每个文件，因为这标识了单独的样本。
- en: Then we encode our data using the `tokenizer` — making sure to include key parameters
    like `max_length`, `padding`, and `truncation`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用`tokenizer`对数据进行编码——确保包括像`max_length`、`padding`和`truncation`这样的关键参数。
- en: 'And now we can move onto creating our tensors — we will be training our model
    through masked-language modeling (MLM). So, we need three tensors:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以继续创建张量——我们将通过掩码语言建模（MLM）训练我们的模型。因此，我们需要三个张量：
- en: '***input_ids*** — our *token_ids* with ~15% of tokens masked using the mask
    token `<mask>`.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***input_ids*** — 我们的*token_ids*，其中~15%的令牌被掩码令牌`<mask>`遮蔽。'
- en: '***attention_mask*** — a tensor of **1**s and **0**s, marking the position
    of ‘real’ tokens/padding tokens — used in attention calculations.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***attention_mask*** — 一个**1**和**0**的张量，标记‘真实’令牌/填充令牌的位置——用于注意力计算。'
- en: '***labels*** — our *token_ids* with **no** masking.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***labels*** — 我们的*token_ids*，**没有**遮蔽。'
- en: If you’re not familiar with MLM, I’ve explained it [here](https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不熟悉MLM，我已经在[这里](https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c)解释过了。
- en: Our `attention_mask` and `labels` tensors are simply extracted from our `batch`.
    The `input_ids` tensors require more attention however, for this tensor we mask
    ~15% of the tokens — assigning them the token ID `3`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`attention_mask`和`labels`张量直接从我们的`batch`中提取。然而，`input_ids`张量需要更多关注，对于这个张量，我们将约15%的标记进行掩码处理——为它们分配标记ID
    `3`。
- en: In the final output, we can see part of an encoded `input_ids` tensor. The very
    first token ID is `1` — the `[CLS]` token. Dotted around the tensor we have several `3` token
    IDs — these are our newly added `[MASK]` tokens.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在最终输出中，我们可以看到部分编码的`input_ids`张量。第一个标记ID是`1`——`[CLS]`标记。张量中点缀着几个`3`标记ID——这些是我们新添加的`[MASK]`标记。
- en: Building the DataLoader
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建DataLoader
- en: Next, we define our `Dataset` class — which we use to initialize our three encoded
    tensors as PyTorch `torch.utils.data.Dataset` objects.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们的`Dataset`类——我们用它来初始化我们三个编码的张量作为PyTorch `torch.utils.data.Dataset`对象。
- en: Finally, our `dataset` is loaded into a PyTorch `DataLoader` object — which
    we use to load our data into our model during training.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们的`dataset`被加载到一个PyTorch `DataLoader`对象中——我们在训练期间用来将数据加载到模型中。
- en: Training the Model
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: We need two things for training, our `DataLoader` and a model. The `DataLoader` we
    have — but no model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练所需的两个东西是我们的`DataLoader`和一个模型。我们有`DataLoader`——但没有模型。
- en: Initializing the Model
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化模型
- en: For training, we need a raw (not pre-trained) `BERTLMHeadModel`. To create that,
    we first need to create a RoBERTa config object to describe the parameters we’d
    like to initialize FiliBERTo with.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，我们需要一个原始（未预训练的）`BERTLMHeadModel`。为了创建它，我们首先需要创建一个RoBERTa配置对象来描述我们希望用来初始化FiliBERTo的参数。
- en: Then, we import and initialize our RoBERTa model with a language modeling (LM)
    head.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们导入并初始化我们的RoBERTa模型，附带语言建模（LM）头。
- en: Training Preparation
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练准备
- en: Before moving onto our training loop we need to set up a few things. First,
    we set up GPU/CPU usage. Then we activate the training mode of our model — and
    finally, initialize our optimizer.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入训练循环之前，我们需要设置一些东西。首先，我们设置GPU/CPU使用情况。然后，我们激活模型的训练模式——最后，初始化我们的优化器。
- en: Training
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: Finally — training time! We train just as we usually would when training via
    PyTorch.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后——训练时间！我们就像平常训练PyTorch时那样训练。
- en: If we head on over to Tensorboard we’ll find our loss over time — it looks promising.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们前往Tensorboard，我们会发现我们的损失随时间的变化——看起来很有希望。
- en: '![](../Images/03de89b00d40ec89c847eab41c735b6b.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03de89b00d40ec89c847eab41c735b6b.png)'
- en: Loss / time — multiple training sessions have been threaded together in this
    chart
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 损失/时间——图表中显示了多个训练会话的综合情况
- en: The Real Test
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际测试
- en: 'Now it’s time for the real test. We set up an MLM pipeline — and ask Laura
    to assess the results. You can watch the video review at 22:44 here:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候进行真正的测试了。我们设置一个MLM管道——并请Laura评估结果。你可以在这里22:44观看视频评论：
- en: 'We first initialize a `pipeline` object, using the `''fill-mask''` argument.
    Then begin testing our model like so:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先初始化一个`pipeline`对象，使用`'fill-mask'`参数。然后开始这样测试我们的模型：
- en: '*“ciao ****come**** va?” *is the right answer! That’s as advanced as my Italian
    gets — so, let’s hand it over to Laura.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*“ciao ****come**** va?”*是正确的答案！这就是我的意大利语水平——所以，让我们交给Laura吧。'
- en: 'We start with *“buongiorno, come va?”* — or *“good day, how are you?”*:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从*“buongiorno, come va?”*——或*“早安，你好吗？”*开始：
- en: The first answer, “buongiorno, chi va?” means “good day, who is there?” — eg
    nonsensical. But, our second answer is correct!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个答案，“buongiorno, chi va?”意思是“早安，谁在那？”——即毫无意义。但我们的第二个答案是正确的！
- en: 'Next up, a slightly harder phrase, *“ciao, dove ci incontriamo oggi pomeriggio?”* —
    or *“hi, where are we going to meet this afternoon?”*:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是一个稍微复杂一点的短语，*“ciao, dove ci incontriamo oggi pomeriggio?”*——或*“你好，我们今天下午在哪里见面？”*：
- en: 'And we return some more positive results:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了一些更积极的结果：
- en: '[PRE0]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Finally, one more, harder sentence, *“cosa sarebbe successo se avessimo scelto
    un altro giorno?”* — or “what would have happened if we had chosen another day?”:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，再来一个更复杂的句子，*“cosa sarebbe successo se avessimo scelto un altro giorno?”*——或“如果我们选择了另一天，会发生什么？”：
- en: 'We return a few good more good answers here too:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里也返回了几个好的答案：
- en: '[PRE1]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Overall, it looks like our model passed Laura’s tests — and we now have a competent
    Italian language model called FiliBERTo!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来看，我们的模型通过了Laura的测试——现在我们有了一个称为FiliBERTo的意大利语模型！
- en: That’s it for this walkthrough of training a BERT model from scratch!
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是从头开始训练BERT模型的流程！
- en: We’ve covered a lot of ground, from getting and formatting our data — all the
    way through to using language modeling to train our raw BERT model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了大量内容，从获取和格式化数据开始，一直到使用语言建模训练我们的原始 BERT 模型。
- en: I hope you enjoyed this article! If you have any questions, let me know via [Twitter](https://twitter.com/jamescalam) or
    in the comments below. If you’d like more content like this, I post on [YouTube](https://www.youtube.com/c/jamesbriggs) too.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 希望您喜欢这篇文章！如果您有任何问题，请通过 [Twitter](https://twitter.com/jamescalam) 或在下方评论中告诉我。如果您希望获得更多类似的内容，我也会在
    [YouTube](https://www.youtube.com/c/jamesbriggs) 上发布。
- en: Thanks for reading!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: '70% Off! Natural Language Processing: NLP With Transformers in Python'
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 70% 折扣！自然语言处理：Python 中的 NLP 与 Transformers
- en: Transformer models are the de-facto standard in modern NLP. They have proven
    themselves as the most expressive…
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型是现代 NLP 的事实标准。它们已经证明自己是最具表现力的…
- en: '**All images are by the author except where stated otherwise*'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**除非另有说明，所有图片均由作者提供**'
- en: '**Bio: [James Briggs](https://youtube.com/c/JamesBriggs)** is a data scientist
    specializing in natural language processing and working in the finance sector,
    based in London, UK. He is also a freelance mentor, writer, and content creator.
    You can reach the author via email ([jamescalam94@gmail.com](mailto:jamescalam94@gmail.com)).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[詹姆斯·布里格斯](https://youtube.com/c/JamesBriggs)** 是一位数据科学家，专注于自然语言处理，工作于金融行业，总部设在英国伦敦。他还是一名自由职业的导师、作家和内容创作者。您可以通过电子邮件联系作者（[jamescalam94@gmail.com](mailto:jamescalam94@gmail.com)）。'
- en: '[Original](https://towardsdatascience.com/how-to-train-a-bert-model-from-scratch-72cfce554fc6).
    Reposted with permission.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/how-to-train-a-bert-model-from-scratch-72cfce554fc6)。经许可转载。'
- en: '**Related:**'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[How to Apply Transformers to Any Length of Text](/2021/04/apply-transformers-any-length-text.html)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何将 Transformers 应用于任意长度文本](/2021/04/apply-transformers-any-length-text.html)'
- en: '[Understanding BERT with Hugging Face](/2021/07/understanding-bert-hugging-face.html)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Hugging Face 理解 BERT](/2021/07/understanding-bert-hugging-face.html)'
- en: '[Topic Modeling with BERT](/2020/11/topic-modeling-bert.html)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 BERT 进行主题建模](/2020/11/topic-modeling-bert.html)'
- en: '* * *'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的捷径。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的 IT 部门'
- en: '* * *'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题
- en: '[How to Build and Train a Transformer Model from Scratch with…](https://www.kdnuggets.com/how-to-build-and-train-a-transformer-model-from-scratch-with-hugging-face-transformers)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何从头开始使用 Hugging Face Transformers 构建和训练 Transformer 模型](https://www.kdnuggets.com/how-to-build-and-train-a-transformer-model-from-scratch-with-hugging-face-transformers)'
- en: '[A Guide to Train an Image Classification Model Using Tensorflow](https://www.kdnuggets.com/2022/12/guide-train-image-classification-model-tensorflow.html)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Tensorflow 训练图像分类模型指南](https://www.kdnuggets.com/2022/12/guide-train-image-classification-model-tensorflow.html)'
- en: '[Fine-Tuning BERT for Tweets Classification with HuggingFace](https://www.kdnuggets.com/2022/01/finetuning-bert-tweets-classification-ft-hugging-face.html)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 HuggingFace 微调 BERT 进行推文分类](https://www.kdnuggets.com/2022/01/finetuning-bert-tweets-classification-ft-hugging-face.html)'
- en: '[Classifying Long Text Documents Using BERT](https://www.kdnuggets.com/2022/02/classifying-long-text-documents-bert.html)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 BERT 对长文本进行分类](https://www.kdnuggets.com/2022/02/classifying-long-text-documents-bert.html)'
- en: '[How Fast Can BERT Go With Sparsity?](https://www.kdnuggets.com/2022/04/fast-bert-go-sparsity.html)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BERT 在稀疏性条件下能多快？](https://www.kdnuggets.com/2022/04/fast-bert-go-sparsity.html)'
- en: '[8 Innovative BERT Knowledge Distillation Papers That Have Changed…](https://www.kdnuggets.com/2022/09/eight-innovative-bert-knowledge-distillation-papers-changed-nlp-landscape.html)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8 篇创新的 BERT 知识蒸馏论文，它们改变了…](https://www.kdnuggets.com/2022/09/eight-innovative-bert-knowledge-distillation-papers-changed-nlp-landscape.html)'
