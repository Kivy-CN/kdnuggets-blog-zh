- en: 'Multi-Task Learning in Tensorflow: Part 1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2016/07/multi-task-learning-tensorflow-part-1.html](https://www.kdnuggets.com/2016/07/multi-task-learning-tensorflow-part-1.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**By Jonathan Godwin, University College London**.'
  prefs: []
  type: TYPE_NORMAL
- en: A Jupyter notebook accompanies this blog post. Please download [here](https://github.com/jg8610/multi-task-part-1-notebook/tree/master).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Why Multi-Task Learning**'
  prefs: []
  type: TYPE_NORMAL
- en: When you think about the way people learn to do new things, they often use their
    experience and knowledge of the world to speed up the learning process. When I
    learn a new language, especially a related one, I use my knowledge of languages
    I already speak to make shortcuts. The process works the other way too - learning
    a new language can help you understand and speak your own better.
  prefs: []
  type: TYPE_NORMAL
- en: Our brains learn to do multiple different tasks at the same time - we have the
    same brain architecture whether we are translating English to German or English
    to French. If we were to use a Machine Learning algorithm to do both of these
    tasks, we might call that ‘multi-task’ learning.
  prefs: []
  type: TYPE_NORMAL
- en: It’s one of the most interesting and exciting areas of research for Machine
    Learning in coming years, radically reducing the amount of data required to learn
    new concepts. One of the great promises of Deep Learning is that, with the power
    of the models and simple ways to share parameters between tasks, we should be
    able to make significant progress in multi-task learning.
  prefs: []
  type: TYPE_NORMAL
- en: As I started to experiment in this area I came across a bit of a road block
    - while it was easy to understand the architecture changes required to implement
    multi-task learning, it was harder to figure out how to implement it in Tensorflow.
    To do anything but standard nets in Tensorflow requires a good understanding of
    how it works, but most of the stock examples don’t provide helpful guidance. I
    hope the following tutorial explains some key concepts simply, and helps those
    who are struggling.
  prefs: []
  type: TYPE_NORMAL
- en: What We Are Going To Do
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Part 1**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Understand Tensorflow Computation Graphs With An Example**. Doing multi-task
    learning with Tensorflow requires understanding how computation graphs work -
    skip if you already know.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Understand How We Can Use Graphs For Multi-Task Learning**. We’ll go through
    an example of how to adapt a simple graph to do Multi-Task Learning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Part 2**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Build A Graph for POS Tagging and Shallow Parsing**. We’ll fill in a template
    that trains a net for two related linguistic tasks. Don’t worry, you don’t need
    to know what they are!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Train A Net Jointly and Separately**. We’ll actually train a model in two
    different ways. You should be able to do this on your laptop.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understanding Computation Graphs With A Toy Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Computation Graph is the thing that makes Tensorflow (and other similar
    packages) fast. It’s an integral part of machinery of Deep Learning, but can be
    confusing.
  prefs: []
  type: TYPE_NORMAL
- en: There are some neat features of a graph that mean it’s very easy to conduct
    multi-task learning, but first we’ll keep things simple and explain the key concepts.
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition: Computation Graph**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Computation Graph** is a **template** for computation (re: algorithm)
    you are going to run. It **doesn’t perform any calculations**, but it means that
    your computer can conduct backpropagation far more quickly.'
  prefs: []
  type: TYPE_NORMAL
- en: If you ask Tensorflow for a **result of a calculation** it will **only make
    those calculations required** for the job, **not the whole graph**.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Toy Example - Linear Transformation: Setting Up The Graph'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’re going to look at the graph for a simple calculation - a linear transformation
    of our inputs, and taking the square loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Toy Example](../Images/52c75dcdf2f4ae57c1cc42de0b09640c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a few things to emphasis about this graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '**If we were to run this code right now, we would get no output**. Remember
    that a Computation Graph is just a template - it doesn’t do anything. If we want
    an answer, we have to tell Tensorflow to run the computation using a **Session**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**We haven’t explicitly created a graph object**. You might expect that we
    would have to create a graph object somewhere in order for Tensorflow to know
    that we wanted to create a graph. In fact, by using the Tensorflow operations,
    we are telling Tensorflow what parts of our code are in the graph.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tip: Keep Your Graph Separate**. You’ll typically be doing a fair amount
    of data manipulation and computation outside of the graph, which means keeping
    track of what is and isn’t available inside of python a bit confusing. I like
    to put my graph in a separate file, and often in a separate class to keep concerns
    separated, but this isn’t required.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A Toy Example - Linear Transformation: Getting Results'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Computations on your Graph are conducted inside a Tensorflow **Session**. To
    get results from your session you need to provide it with two things: Target Results
    and Inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Target Results or Operations**. You tell Tensorflow what parts of the graph
    you want to return values for, and it will **automatically figure out what calculations
    within need to be run**. You can also call operations, for example, to initialise
    your variables.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Inputs As Required (‘Feed Dict’)**. In most calculations you will provide
    the input data ad-hoc. In this case, you construct the graph with a**placeholder** for
    this data, and feed it in at computation time. Not all calculations or operations
    will require an input - for many, all the information is already contained in
    the graph.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: How To Use Graphs for Multi-Task Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we create a Neural Net that performs multiple tasks we want to have some
    parts of the network that are shared, and other parts of the network that are
    specific to each individual task. When we’re training, we want information from
    each task to be transferred in the shared parts of the network.
  prefs: []
  type: TYPE_NORMAL
- en: So, to start, let’s draw a diagram of a simple two-task network that has a shared
    layer and a specific layer for each individual task. We’re going to feed the outputs
    of this into our loss function with our targets. I’ve labelled where we’re going
    to want to create placeholders in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![Basic shared graph](../Images/8ae5537858dcd61fd6448f72993af879.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: When we are training this network, **we want the parameters of the Task 1 layer
    to not change no matter how wrong we get Task 2**, but **the parameters of the
    shared layer to change with both tasks**. This might seem a little difficult -
    normally you only have one optimiser in a graph, because you only optimise one
    loss function. Thankfully, using the properties of the graph it’s very easy to
    train this sort of model in two ways.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[TensorFlow for Computer Vision - Transfer Learning Made Easy](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PyTorch or TensorFlow? Comparing popular Machine Learning frameworks](https://www.kdnuggets.com/2022/02/packt-pytorch-tensorflow-comparing-popular-machine-learning-frameworks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The "Hello World" of Tensorflow](https://www.kdnuggets.com/2022/05/hello-world-tensorflow.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Guide to Train an Image Classification Model Using Tensorflow](https://www.kdnuggets.com/2022/12/guide-train-image-classification-model-tensorflow.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Free TensorFlow 2.0 Complete Course](https://www.kdnuggets.com/2023/02/free-tensorflow-20-complete-course.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
