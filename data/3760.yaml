- en: 'Deep Learning Research Review: Natural Language Processing'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习研究回顾：自然语言处理
- en: 原文：[https://www.kdnuggets.com/2017/01/deep-learning-review-natural-language-processing.html/2](https://www.kdnuggets.com/2017/01/deep-learning-review-natural-language-processing.html/2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/01/deep-learning-review-natural-language-processing.html/2](https://www.kdnuggets.com/2017/01/deep-learning-review-natural-language-processing.html/2)
- en: Recurrent Neural Networks (RNNs)
  id: totrans-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 循环神经网络（RNNs）
- en: Okay, so now that we have our word vectors, let’s see how they fit into recurrent
    neural networks. RNNs are the go-to for most NLP tasks today. The big advantage
    of the RNN is that it is able to effectively use data from previous time steps.
    This is what a small piece of an RNN looks like.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，现在我们有了词向量，让我们看看它们如何融入到循环神经网络中。RNN是现在大多数自然语言处理任务的首选。RNN的最大优势在于它能够有效地利用来自先前时间步的数据。这是RNN的一个小片段。
- en: '![](../Images/7bff85999c56243549e81f29b186e61b.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7bff85999c56243549e81f29b186e61b.png)'
- en: So, at the bottom we have our word vectors (x[t], x[t-1], x[t+1]). Each of the
    vectors has a hidden state vector at that same time step (h[t], h[t-1], h[t+1]).
    Let’s call this one module.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在底部我们有词向量（x[t], x[t-1], x[t+1]）。每个向量在同一时间步都有一个隐藏状态向量（h[t], h[t-1], h[t+1]）。我们称之为一个模块。
- en: '![](../Images/e15f2e2928b838c1376bdb1ab42de229.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e15f2e2928b838c1376bdb1ab42de229.png)'
- en: The hidden state in each module of the RNN is a *function* of both the word
    vector and the hidden state vector at the previous time step.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: RNN中每个模块的隐藏状态是*一个函数*，它结合了词向量和前一个时间步的隐藏状态向量。
- en: '![](../Images/6d46e022691a5417e0b090aab3daeeec.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d46e022691a5417e0b090aab3daeeec.png)'
- en: If you take a close look at the superscripts, you’ll see that there’s a weight
    matrix W^(hx) which we’re going to multiply with our input, and there’s a recurrent
    weight matrix W^(hh) which is multiplied with the hidden state vector at the *previous*
    time step. Keep in mind that these recurrent weight matrices are the *same* across
    all time steps. **This is the key point of RNNs.** Thinking about this carefully,
    it’s very different from a traditional 2 layer NN for example. In that case, we
    normally have a distinct W matrix for each layer (W1 and W2). Here, the recurrent
    weight matrix is the same through the network.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细看这些上标，你会看到有一个权重矩阵 W^(hx)，我们将用它来与输入相乘，还有一个递归权重矩阵 W^(hh)，它与*前一个*时间步的隐藏状态向量相乘。请记住，这些递归权重矩阵在所有时间步中是*相同的*。**这是RNN的关键点。**
    仔细考虑一下，这与传统的2层神经网络非常不同。在后者中，我们通常为每一层有一个不同的W矩阵（W1和W2）。而在这里，递归权重矩阵在整个网络中是相同的。
- en: To get the output (Yhat) of a particular module, this will be h times W^S, which
    is another weight matrix.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得特定模块的输出（Yhat），这将是h乘以 W^S，这又是一个权重矩阵。
- en: '![](../Images/d7a2aed95a7d28dd490a30831cdf3c68.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7a2aed95a7d28dd490a30831cdf3c68.png)'
- en: Let’s take a step back now and understand what the advantages of an RNN are.
    The most distinct difference from a traditional NN is that an RNN takes in a *sequence*
    of inputs (words in our case). You can contrast this to a typical CNN where you’d
    have just a singular image as input. With an RNN, however, the input can be anywhere
    from a short sentence to a 5 paragraph essay. Additionally, the *order* of inputs
    in this sequence can largely affect how the weight matrices and hidden state vectors
    change during training. The hidden states, after training, will hopefully capture
    the information from the past (the previous time steps).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们退一步，了解RNN的优势。与传统的神经网络不同，RNN接收的是一个*序列*的输入（在我们的例子中是单词）。你可以将其与典型的CNN对比，后者通常只有一张单独的图像作为输入。然而，RNN的输入可以是从短句到5段文章的任意长度。此外，这个序列中输入的*顺序*可以大大影响训练过程中权重矩阵和隐藏状态向量的变化。经过训练，隐藏状态应该能够捕捉到过去的信息（之前的时间步）。
- en: Gated Recurrent Units (GRUs)
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 门控循环单元（GRUs）
- en: Let’s now look at a gated recurrent unit, or GRU. The purpose of this unit is
    to provide a more complex way of computing our hidden state vectors in RNNs. This
    approach will allow us to keep information that capture long distance dependencies.
     Let’s imagine why long term dependencies would be a problem in the traditional
    RNN setup. During backpropagation, the error will flow through the RNN, going
    from the latest time step to the earliest one. If the initial gradient is a small
    number (say < 0.25), then by the 3^(rd) or 4^(th) module, the gradient will have
    practically vanished (chain rule multiplies gradients together) and thus the hidden
    states of the earlier time steps won’t get updated.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下门控循环单元（GRU）。这个单元的目的是提供一种更复杂的方式来计算 RNN 中的隐藏状态向量。这种方法允许我们保留捕捉长距离依赖的信息。让我们想象一下为什么长期依赖在传统的
    RNN 设置中会成为问题。在反向传播过程中，误差会通过 RNN 传播，从最新的时间步到最早的时间步。如果初始梯度是一个小数字（比如 < 0.25），那么在第
    3 或第 4 个模块中，梯度将基本上消失（链式法则将梯度相乘），因此早期时间步的隐藏状态不会更新。
- en: In a traditional RNN, the hidden state vector is computed through this formulation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的 RNN 中，隐藏状态向量是通过这个公式计算的。
- en: '![](../Images/ffa90bec3215efe6639d863d03b64317.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ffa90bec3215efe6639d863d03b64317.png)'
- en: The GRU provides a different way of computing this hidden state vector h(t).
    The computation is broken up into 3 components, an update gate, a reset gate,
    and a new memory container. The two gates are both functions of the input word
    vector and the hidden state at the previous time step.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: GRU 提供了一种不同的方式来计算隐藏状态向量 h(t)。计算被分解为三个组件：更新门、重置门和新记忆容器。这两个门都是输入词向量和前一个时间步的隐藏状态的函数。
- en: '![](../Images/3f332e74d25d460ede9d08a5fda23bb1.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f332e74d25d460ede9d08a5fda23bb1.png)'
- en: The key difference is that different weights are used for each gate. This is
    indicated by the differing superscripts. The update gate uses W^z and U^z while
    the reset gate uses W^r and U^r.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 主要区别在于每个门使用了不同的权重。这通过不同的上标来表示。更新门使用 W^z 和 U^z，而重置门使用 W^r 和 U^r。
- en: Now, the new memory container is computed through the following.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，新记忆容器通过以下公式计算。
- en: '![](../Images/142df130628886519aa24dc197373a75.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/142df130628886519aa24dc197373a75.png)'
- en: (The open dot indicates a [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)))
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: （开放点表示[哈达玛积](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))）
- en: Now, if you take a closer look at the formulation, you’ll see that if the reset
    gate unit is close to 0, then that whole term becomes 0 as well, thus ignoring
    the information in h[t-1] from the previous time steps. In this scenario, the
    unit is only a function of the new word vector x[t].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你仔细查看这个公式，你会发现如果重置门单元接近 0，那么整个项也会变为 0，从而忽略前一个时间步的 h[t-1] 信息。在这种情况下，单元仅是新词向量
    x[t] 的函数。
- en: The final formulation of h(t) is written as
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: h(t) 的最终公式写作：
- en: '![](../Images/06d61e3056f74c6a2aa70f772eb7881c.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06d61e3056f74c6a2aa70f772eb7881c.png)'
- en: 'h[t] is a function of all 3 components: the reset gate, the update gate, and
    the memory container. The best way to understand this is by visualizing what happens
    when z[t] is close to 1 and when it is close to 0\. When z[t] is close to 1, the
    new hidden state vector h[t]is mostly dependent on the previous hidden state and
    we ignore the current memory container because (1-z[t]) goes to 0\. When z[t]is
    close to 0, the new hidden state vector h[t] is mostly dependent on the current
    memory container and we ignore the previous hidden state. An intuitive way of
    looking at these 3 components can be summarized through the following.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: h[t] 是所有三个组件的函数：重置门、更新门和记忆容器。最好的理解方式是通过可视化 z[t] 接近 1 和接近 0 时发生的情况。当 z[t] 接近
    1 时，新的隐藏状态向量 h[t] 主要依赖于前一个隐藏状态，我们忽略当前的记忆容器，因为 (1 - z[t]) 接近 0。当 z[t] 接近 0 时，新的隐藏状态向量
    h[t] 主要依赖于当前的记忆容器，我们忽略前一个隐藏状态。直观地看这三个组件可以通过以下方式总结。
- en: 'Update Gate:'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新门：
- en: If z[t] ~ 1, then h[t] completely ignores the current word vector and just copies
    over the previous hidden state (If this doesn’t make sense, look at the h[t] equation
    and take note of what happens to the 1 - z[t] term when z[t] ~ 1).
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 z[t] ~ 1，则 h[t] 完全忽略当前词向量，只复制前一个隐藏状态（如果这不清楚，查看 h[t] 方程，注意当 z[t] ~ 1 时 1 -
    z[t] 项的变化）。
- en: If z[t] ~ 0, then h[t] completely ignores the hidden state at the previous time
    step and is dependent on the new memory container.
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 z[t] ~ 0，则 h[t] 完全忽略前一个时间步的隐藏状态，依赖于新的记忆容器。
- en: This gate lets the model control how much of the information in the previous
    hidden state should influence the current hidden state.
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个门让模型控制之前隐藏状态中的信息应在多大程度上影响当前隐藏状态。
- en: 'Reset Gate:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重置门：
- en: If r[t] ~ 1, then the memory container keeps the info from the previous hidden
    state.
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 r[t] ~ 1，则内存容器保留来自之前隐藏状态的信息。
- en: If r[t] ~ 0, then the memory container ignores the previous hidden state.
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 r[t] ~ 0，则内存容器会忽略之前的隐藏状态。
- en: This gate lets the model drop information if that info is irrelevant in the
    future.
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个门让模型在信息未来不相关时丢弃该信息。
- en: 'Memory Container: Dependent on the reset gate.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存容器：依赖于重置门。
- en: A common example to illustrate the effectiveness of GRUs is the following. Let’s
    say you have the following passage.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的例子来说明 GRUs 的有效性是以下内容。假设你有以下段落。
- en: '![](../Images/4990d39799f6ecdf4c881e6ddad2211b.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4990d39799f6ecdf4c881e6ddad2211b.png)'
- en: and the associated question “What is the sum of the 2 numbers?”. Since the middle
    sentence has absolutely no impact on the question at hand, the reset and update
    gates will allow the network to “forget” the middle sentence in some sense, and
    learn that only specific information (numbers in this case) should modify the
    hidden state.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 和相关问题“两个数字的和是多少？”相比，由于中间句子对当前问题没有任何影响，重置和更新门将允许网络在某种程度上“忘记”中间句子，并学习只有特定信息（在这种情况下是数字）应该修改隐藏状态。
- en: Long Short Term Memory Units (LSTMs)
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 长短期记忆单元（LSTMs）
- en: If you’re comfortable with GRUs, then LSTMs won’t be too far of a leap forward.
    An LSTM is also made up of a series of gates.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对 GRUs 感到舒适，那么 LSTMs 也不会是太大的跃进。LSTM 也是由一系列门组成的。
- en: '![](../Images/e8890214fc454dfea9501a0e6f873871.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8890214fc454dfea9501a0e6f873871.png)'
- en: Definitely a lot more information to take in. Since this can be thought of as
    an extension to the idea behind a GRU, I won’t go too far into the analysis, but
    for a more in depth walkthrough of each gate and each piece of computation, check
    out Chris Olah’s amazingly well written [blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).
    It is by far, the most popular tutorial on LSTMs, and will definitely help those
    of you looking for more intuition as to why and how these units work so well.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对有很多信息需要吸收。由于这可以看作是 GRU 背后思想的扩展，我不会深入分析，但如果你想了解每个门和每个计算部分的详细情况，请查看 Chris Olah
    精心撰写的 [博客文章](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)。这篇文章迄今为止是关于
    LSTMs 的最受欢迎的教程，它将帮助你更好地理解这些单元为何以及如何工作。
- en: Comparing and Contrasting LSTMs and GRUs
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较和对比 LSTMs 和 GRUs
- en: Let’s start off with the similarities. Both of these units have the special
    function of being able to keep long term dependencies between words in a sequence.
    Long term dependencies refer to situations where two words or phrases may occur
    at very different time steps, but the relationship between them is still critical
    to solving the end goal. LSTMs and GRUs are able to capture these dependencies
    through gates that can ignore or keep certain information in the sequence.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先从相似之处开始。这两种单元都有一个特殊功能，即能够保持序列中词语之间的长期依赖关系。长期依赖关系指的是两词或短语可能在非常不同的时间步骤中出现，但它们之间的关系仍对解决最终目标至关重要。LSTMs
    和 GRUs 能通过门控机制捕捉这些依赖关系，门控机制可以忽略或保留序列中的某些信息。
- en: The difference between the two units lies in the number of gates that they have
    (GRU – 2, LSTM – 3). This affects the number of nonlinearities the input passes
    through and ultimately affects the overall computation. The GRU also doesn’t have
    the same memory cell (c[t]) that the LSTM has.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种单元的区别在于它们拥有的门的数量（GRU – 2，LSTM – 3）。这影响了输入经过的非线性处理的数量，最终影响整体计算。GRU 也没有 LSTM
    所具有的记忆单元（c[t]）。
- en: Before Getting Into the Papers
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在深入阅读论文之前
- en: Just want to make one quick note. There are a couple other deep models that
    are useful in NLP. Recursive neural networks and CNNs for NLP are sometimes used
    in practice, but aren’t as prevalent as RNNs, which really are the backbone behind
    most deep learning NLP systems.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 想快速补充一点。有一些其他深度模型在自然语言处理（NLP）中也很有用。递归神经网络和用于 NLP 的卷积神经网络有时在实际中使用，但不如 RNNs 普及，后者实际上是大多数深度学习
    NLP 系统的核心。
- en: Alright. Now that we have a good understanding of deep learning in relation
    to NLP, let’s look at some papers. Since there are numerous different problem
    areas within NLP (from machine translation to question answering), there are a
    number of papers that we could look into, but here are 3 that I found to be particularly
    insightful. 2016 had some great advancements in NLP, but let’s first start with
    one from 2015.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。既然我们对深度学习与自然语言处理的关系有了充分的理解，让我们来看一些论文。由于自然语言处理领域内存在着众多不同的问题领域（从机器翻译到问答系统），我们可以参考许多论文，但这里有三篇我认为特别有洞察力的。2016
    年在自然语言处理方面有一些重大的进展，但我们首先从2015年的一篇论文开始。
- en: '* * *'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的捷径。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的 IT'
- en: '* * *'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[建立一个稳固的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用管道编写干净的 Python 代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[是什么让 Python 成为初创企业理想的编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
- en: '[N-gram Language Modeling in Natural Language Processing](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[N-gram 语言建模在自然语言处理中的应用](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，并寻找目标来…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学统计学习的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
