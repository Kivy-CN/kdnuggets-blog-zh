- en: Must Read NLP Papers from the Last 12 Months
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/03/must-read-nlp-papers-last-12-months.html](https://www.kdnuggets.com/2023/03/must-read-nlp-papers-last-12-months.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Must Read NLP Papers from the Last 12 Months](../Images/def988fd066629d818503834d972600a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Anil Sharma](https://www.pexels.com/photo/birds-perched-on-tree-branch-10952775/) on [Pexels](https://www.pexels.com/photo/birds-perched-on-tree-branch-10952775/)
  prefs: []
  type: TYPE_NORMAL
- en: Since the **groundbreaking release** of [BERT](https://arxiv.org/abs/1810.04805) in
    October 2018, machine learning has achieved ever greater heights through clever
    optimization and augmented compute. BERT, which stands for Bidirectional Encoder
    Representations from Transformers, introduced a new paradigm in neural network
    architecture. The **transformer** has served as a significant unlock in machine
    learning capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Further advancements in the field of Natural Language Processing (NLP) have
    improved foreign language translation, enhanced no-code applications, increased
    the fluency of chatbots, and very quickly set new standards for an array of state-of-the
    art benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alongside these remarkable accomplishments, the development of large language
    models (LLMs) has not been without controversy. In the 2021 "[Stochastic Parrots](https://dl.acm.org/doi/10.1145/3442188.3445922)"
    paper, a team of researchers including machine learning engineer and ethicist
    Timnit Gebru criticized these models for:'
  prefs: []
  type: TYPE_NORMAL
- en: Levying a damning** environmental cost**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Excluding marginalized voices** through inelegant curation of the training
    data set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plagiarizing** internet content and stealing from human writers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gebru was summarily fired from her position on Google's Ethical Artificial Intelligence
    Team.
  prefs: []
  type: TYPE_NORMAL
- en: In this writeup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We explore four NLP papers published in the past year that represent the latest
    advancements. Understanding these developments will improve your capabilities
    as a Data Scientist and put you at the forefront of this dynamic research space.
  prefs: []
  type: TYPE_NORMAL
- en: 1. [Training Compute Optimal Large Language Models](https://arxiv.org/abs/2203.15556)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper examines the ideal model size and token count for a language model
    using the transformer architecture. It aims to answer the question of what constitutes
    the ideal number of parameters and size of dataset for a model trained under a
    predetermined compute budget.
  prefs: []
  type: TYPE_NORMAL
- en: The researchers found that in prior cases, LLMs seem to have been severely undertrained.
    The authors criticize these teams for overemphasizing the scaling of compute resources
    while underemphasizing the importance of training data volume.
  prefs: []
  type: TYPE_NORMAL
- en: The authors concluded that for compute-optimal training, model size and the
    number of training tokens should be scaled equally. In other words,
  prefs: []
  type: TYPE_NORMAL
- en: for every doubling of model size, the number of training tokens should also
    be doubled.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The research showed that a relatively small model (70B parameters) trained on
    4 times more training data could consistently beat larger models (up to 530B parameters)
    at state-of-the-art benchmark tests such as Multi-task Language Understanding
    ([MMLU](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu)).
  prefs: []
  type: TYPE_NORMAL
- en: The enhanced training data allows the smaller model to utilize significantly
    less compute resources for inference and fine-tuning. This bodes well for downstream
    utilization.
  prefs: []
  type: TYPE_NORMAL
- en: '**TL;DR** — this paper shows that the prior understanding of scaling laws was
    incorrect. In fact, when trained with a properly extensive token count, smaller
    networks can be significantly better than larger ones.'
  prefs: []
  type: TYPE_NORMAL
- en: 2. [Training Language Models to Follow Instructions with Human Feedback](https://arxiv.org/abs/2203.02155)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Enhancing the compute provided to LLMs does not automatically improve their
    ability to interpret user intent. As a troubling consequence of this fact, LLMs
    may provide results that are untruthful or harmful.
  prefs: []
  type: TYPE_NORMAL
- en: This paper highlights a novel method for fine-tuning language models using human
    feedback to better align the output with user intent across a variety of tasks.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The researchers gathered a dataset starting from a collection of OpenAI API
    prompts. They then utilize the data to fine-tune GPT-3 via supervised learning.
    Then, using reinforcement learning based on user input, they generated a new dataset
    ranking model outputs. The researchers then used this data to further fine-tune
    the supervised model, resulting in a model they called InstructGPT.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the original GPT-3, InstructGPT has 100 times fewer parameters,
    and yet it is capable of outperforming GPT-3 in human assessments.
  prefs: []
  type: TYPE_NORMAL
- en: On test data, the InstructGPT model is more likely to respond honestly and less
    likely to create harmful content. Though InstructGPT still occasionally makes
    basic errors, these findings demonstrate that fine-tuning with a human-in-the-loop
    serves as a viable route for matching language models with human intent.
  prefs: []
  type: TYPE_NORMAL
- en: '**TL;DR** — this paper shows that doing reinforcement learning with human feedback
    is an extremely helpful, low-resource way to make existing models more useful.'
  prefs: []
  type: TYPE_NORMAL
- en: 3. [A Generalist Agent](https://www.deepmind.com/publications/a-generalist-agent)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper explores improvements resulting in a model capable of playing Atari,
    captioning pictures, generating text, stacking physical blocks using a robot arm,
    and much more.
  prefs: []
  type: TYPE_NORMAL
- en: The model, Gato, is composed of a single neural network with unchanged weights
    across assorted tasks.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Gato resulted from scaled up behavior cloning, a form of sequence modeling challenge.
    The challenge of encoding many modalities into a single vector space of tokens
    constituted the most significant barrier the researchers faced in their efforts.
    The study makes a number of advancements in tokenization of standard vision and
    language datasets. In addition, the researchers sought novel solutions to the
    typical sequence model problem of determining context window length.
  prefs: []
  type: TYPE_NORMAL
- en: '**TL;DR** — this paper shows that multimodal models can very well and are likely
    the future of the modeling paradigm. In contrast to previous state-of-the-art
    models that were capable of performing only in a narrow area, Gato executes a
    generalist policy capable of a variety tasks and multiple modalities.'
  prefs: []
  type: TYPE_NORMAL
- en: 4. [Large Language Models are Zero Shot Reasoners](https://arxiv.org/abs/2205.11916)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are remarkable few-shot learners using narrow, task-specific examples.
    This research paper demonstrates that LLMs are also competent zero-shot reasoners,
    particularly when prompted with the phrase, "let’s think step by step."
  prefs: []
  type: TYPE_NORMAL
- en: Yes, you read that right.
  prefs: []
  type: TYPE_NORMAL
- en: Instructing an LLM to “think step by step” actually improves results enough
    to justify a paper.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The model created by authors Kojima et al. surpassed existing benchmarks on
    reasoning tasks, such as arithmetic (e.g., MultiArith, GSM8K, AQUA-RAT, SVAMP),
    symbolic reasoning (e.g., Last Letter, Coin Flip), and logical reasoning (e.g.,
    Date Understanding, Tracking Shuffled Objects).
  prefs: []
  type: TYPE_NORMAL
- en: The adaptability of this single prompt, "think step by step," over a wide range
    of reasoning tasks suggests that the zero-shot skills were previously significantly
    underutilized. Remarkably high-level, multi-task capabilities may be retrieved
    simply by employing a linguistic framing of the problem that requests a higher
    cognitive load.
  prefs: []
  type: TYPE_NORMAL
- en: My mind is blown.
  prefs: []
  type: TYPE_NORMAL
- en: '**TL;DR — **this paper shows that the quality of a LLM''s answer is largely
    dependent on the wording of the prompt'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning has advanced significantly in the past four years. Only time
    will tell if this pace of development can be sustained.
  prefs: []
  type: TYPE_NORMAL
- en: These papers discuss the latest enhancements in NLP, revealing considerable
    room for continued improvement in training processes to involve larger datasets
    and human-in-the-loop reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Recent research also explores the creation of multi-modal paradigms and enhanced
    zero-shot reasoning capabilities via simple alterations to the model’s input prompts.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Nicole Janeway Bills](https://www.linkedin.com/in/nicole-janeway-bills/)**
    is the Community Organizer at Data Strategy Professionals. She offers a proven
    track record of training data practitioners to quickly and effectively ace the
    CDMP Exams. In her work as a Data Strategy consultant, Nicole has helped set up
    data collection, data storage, and data analytics functions. She applies best
    practices to solve clients’ most pressing challenges. Furthermore, she has worked
    as a Data Scientist and Project Manager for federal and commercial consulting
    teams. Her business experience includes natural language processing, cloud computing,
    statistical testing, pricing analysis, ETL processes, and web and application
    development.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/towards-data-science/must-read-nlp-papers-f9d38cda0b65).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[KDnuggets News, April 27: A Brief Introduction to Papers With Code;…](https://www.kdnuggets.com/2022/n17.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Machine Learning Papers to Read in 2023](https://www.kdnuggets.com/2023/03/top-machine-learning-papers-read-2023.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Generative Agent Research Papers You Should Read](https://www.kdnuggets.com/generative-agent-research-papers-you-should-read)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Machine Learning Papers to Read in 2024](https://www.kdnuggets.com/5-machine-learning-papers-to-read-in-2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Free Data Science Books You Must Read in 2023](https://www.kdnuggets.com/2023/01/5-free-data-science-books-must-read-2023.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8 Innovative BERT Knowledge Distillation Papers That Have Changed…](https://www.kdnuggets.com/2022/09/eight-innovative-bert-knowledge-distillation-papers-changed-nlp-landscape.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
