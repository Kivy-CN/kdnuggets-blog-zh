- en: 'PySpark SQL Cheat Sheet: Big Data in Python'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark SQL 备忘单：Python 中的大数据
- en: 原文：[https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html](https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html](https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html)
- en: '**By Karlijn Willems, [DataCamp](https://www.datacamp.com/).**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：Karlijn Willems，[DataCamp](https://www.datacamp.com/)。**'
- en: Big Data With Python
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Python 进行大数据分析
- en: 'Big data is everywhere and is traditionally characterized by three V’s: Velocity,
    Variety and Volume. Big data is fast, is varied and has a huge volume. As a data
    scientist, data engineer, data architect, ... or whatever the role is that you’ll
    assume in the data science industry, you’ll definitely get in touch with big data
    sooner or later, as companies now gather an enormous amount of data across the
    board.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据无处不在，传统上由三个 V 特征来描述：速度、种类和体量。大数据是快速的、多样的，且体量庞大。作为数据科学家、数据工程师、数据架构师，...或在数据科学行业中承担的任何角色，你迟早都会接触到大数据，因为公司现在在各个领域收集了大量的数据。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT 部门'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Data doesn’t always mean information, though, and that is where you, data science
    enthusiast, come in. You can make use of Apache Spark, “a fast and general engine
    for large-scale data processing” to start to tackle the challenges that big data
    poses to you and the company you’re working for.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并不总是意味着信息，这就是你，数据科学爱好者，可以发挥作用的地方。你可以利用 Apache Spark，“一个用于大规模数据处理的快速且通用的引擎”来开始应对大数据给你和你所在公司带来的挑战。
- en: 'Nevertheless, doubts may always arise when you’re working with Spark and when
    they do, take a look at DataCamp’s [Apache Spark Tutorial: ML with PySpark tutorial](https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning)
    or download the [cheat sheet](https://www.datacamp.com/community/blog/pyspark-sql-cheat-sheet)
    for free!'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，当你使用 Spark 时，总是可能会出现疑问，这时可以查看 DataCamp 的 [Apache Spark 教程：使用 PySpark 进行机器学习](https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning)
    或免费下载 [备忘单](https://www.datacamp.com/community/blog/pyspark-sql-cheat-sheet)！
- en: In what follows, we’ll dive deeper into the structure and the contents of the
    cheat sheet.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨备忘单的结构和内容。
- en: PySpark Cheat Sheet
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PySpark 备忘单
- en: PySpark is the Spark Python API exposes the Spark programming model to Python.
    Spark SQL, then, is a module of PySpark that allows you to work with structured
    data in the form of DataFrames. This stands in contrast to RDDs, which are typically
    used to work with unstructured data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 是 Spark 的 Python API，将 Spark 编程模型暴露给 Python。Spark SQL 是 PySpark 的一个模块，允许你使用
    DataFrames 形式的结构化数据。这与通常用于处理非结构化数据的 RDDs 相对立。
- en: '![PySpark cheat sheet](../Images/f31b5ce6452a08f76098ca13c56b14cb.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark 备忘单](../Images/f31b5ce6452a08f76098ca13c56b14cb.png)'
- en: '**Tip:** if you want to learn more about the differences between RDDs and DataFrames,
    but also about how Spark DataFrames differ from pandas DataFrames, you should
    definitely check out the [Apache Spark in Python: Beginner''s Guide](https://www.datacamp.com/community/tutorials/apache-spark-python).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示：** 如果你想了解 RDDs 和 DataFrames 之间的区别，还想了解 Spark DataFrames 与 pandas DataFrames
    的不同之处，务必查看 [Python 中的 Apache Spark：初学者指南](https://www.datacamp.com/community/tutorials/apache-spark-python)。'
- en: Initializing SparkSession
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始化 SparkSession
- en: 'If you want to start working with Spark SQL with PySpark, you’ll need to start
    a SparkSession first: you can use this to create DataFrames, register DataFrames
    as tables, execute SQL over the tables and read parquet files. Don’t worry if
    all of this sounds very new to you - You’ll read more about this later on in this
    article!'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想开始使用 PySpark 的 Spark SQL，你需要首先启动一个 SparkSession：你可以用它来创建 DataFrames、将 DataFrames
    注册为表、在表上执行 SQL 查询以及读取 parquet 文件。如果这一切对你来说都很陌生，不用担心 - 你将在本文后续部分了解到更多信息！
- en: You start a SparkSession by first importing it from the sql module that comes
    with the pyspark package. Next, you can initialize a variable spark, for example,
    to not only build the SparkSession, but also give the application a name, set
    the config and then use the getOrCreate() method to either get a SparkSession
    if there is already one running or to create one if that isn’t the case yet! That
    last method will come in extremely handy, especially for future reference, because
    it will prevent you from running multiple SparkSessions at the same time!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你通过首先从 pyspark 包的 sql 模块中导入 SparkSession 来启动 SparkSession。接下来，你可以初始化一个变量 spark，例如，不仅构建
    SparkSession，还可以为应用程序命名、设置配置，然后使用 getOrCreate() 方法来获取当前运行的 SparkSession（如果已经有一个），或者创建一个新的
    SparkSession（如果尚未存在）！最后这个方法会非常有用，特别是在未来的参考中，因为它可以防止同时运行多个 SparkSessions！
- en: Cool, isn’t it?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 很酷，对吧？
- en: '![PySpark cheat sheet](../Images/3c86b85673b094ab3ed9a4afa5142260.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark cheat sheet](../Images/3c86b85673b094ab3ed9a4afa5142260.png)'
- en: Inspecting the Data
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据检查
- en: When you have imported your data, it’s time to inspect the Spark DataFrame by
    using some of the built-in attributes and methods. This way, you’ll get to know
    your data a little bit better before you start manipulating the DataFrames.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当你导入了数据后，使用一些内置的属性和方法来检查 Spark DataFrame。这样，你可以在开始操作 DataFrames 之前更好地了解你的数据。
- en: Now, you’ll probably already know most of the methods and attributes mentioned
    in this section of the cheat sheet  from working with pandas DataFrames or NumPy,
    such as dtypes, head(), describe(), count(),... There are also some methods that
    might be new to you, such as the take() or printSchema() method, or the schema
    attribute.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能已经了解了本节中提到的大部分方法和属性，这些方法和属性来自于使用 pandas DataFrames 或 NumPy 的经验，比如 dtypes、head()、describe()、count()
    等。还有一些方法可能对你来说是新的，比如 take() 或 printSchema() 方法，或者 schema 属性。
- en: '![PySpark cheat sheet](../Images/2b87b5681e4fc864b638d73298521c50.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark cheat sheet](../Images/2b87b5681e4fc864b638d73298521c50.png)'
- en: Nevertheless, you’ll see that the ramp-up is quite modest, as you can leverage
    your previous knowledge on data science packages maximally.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，你会发现 ramp-up 的过程相当温和，因为你可以最大限度地利用之前在数据科学包中的知识。
- en: Duplicate Values
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重复值
- en: As you’re inspecting your data, you might find that there are some duplicate
    values. To remediate this, you can use the dropDuplicates() method, for example,
    to drop duplicate values in your Spark DataFrame.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查数据时，你可能会发现存在一些重复值。为了处理这些问题，你可以使用 dropDuplicates() 方法，例如，用于删除 Spark DataFrame
    中的重复值。
- en: Queries
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查询
- en: You might remember that one of the main reasons for using Spark DataFrames in
    the first place is that you have a more structured way of dealing with your data
    - Querying is one of those examples of handling your data in a more structured
    way, whether you’re using SQL in a relational database, SQL-like language in No-SQL
    databases, the [query()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html?highlight=query#pandas.DataFrame.query)
    method on Pandas DataFrames, and so on.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，使用 Spark DataFrames 的主要原因之一是你可以以更结构化的方式处理数据 - 查询就是处理数据的一种更结构化的方式，无论你是在关系型数据库中使用
    SQL、在 No-SQL 数据库中使用类似 SQL 的语言，还是在 Pandas DataFrames 上使用 [query()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html?highlight=query#pandas.DataFrame.query)
    方法，等等。
- en: '![PySpark cheat sheet](../Images/8a57af6b8937cff2c56c8dfbf857efe5.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark cheat sheet](../Images/8a57af6b8937cff2c56c8dfbf857efe5.png)'
- en: 'Now that we’re talking about Pandas DataFrames, you’ll notice that Spark DataFrames
    follow a similar principle: you’re using methods to get to know your data better.
    In this case, though, you won’t use query(). Instead, you’ll have to make use
    of other methods to get the results you want to get back: in a first instance,
    select() and show() will be your friends whenever you want to retrieve information
    from your DataFrames.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们谈论的是 Pandas DataFrames，你会注意到 Spark DataFrames 遵循类似的原则：你使用方法来更好地了解你的数据。不过，在这种情况下，你不会使用
    query()。相反，你需要利用其他方法来获得你想要的结果：首先，select() 和 show() 将是你获取 DataFrames 信息时的好帮手。
- en: 'Within these methods, you can build up your query. As with standard SQL, you
    specify what columns you exactly want to get back within select(). You can do
    this with a regular string or by specifying the column name with the help of your
    DataFrame itself, as in the following example: df.lastName or df[“firstName”].'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些方法中，你可以构建你的查询。与标准 SQL 一样，你可以在 select() 中指定你到底想要获取哪些列。你可以使用普通字符串或通过 DataFrame
    本身指定列名，如以下示例所示：df.lastName 或 df[“firstName”]。
- en: Note that the latter approach will give you some more freedom in some cases
    where you want to specify what information you exactly want to retrieve with additional
    functions such as isin() or startswith(), for example.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，后一种方法在你想要指定你究竟想要检索哪些信息时，给予了你更多的自由，例如可以使用 isin() 或 startswith() 等附加函数。
- en: 'Besides select(), you can also make use of the functions module in PySpark
    SQL to specify, for example, a when clause in your query, as demonstrated in the
    table below:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 select()，你还可以利用 PySpark SQL 中的函数模块来指定例如查询中的 when 子句，如下表所示：
- en: '![PySpark cheat sheet](../Images/d5234f75f8ad26d04f778d4b0bbf5d52.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark 备忘单](../Images/d5234f75f8ad26d04f778d4b0bbf5d52.png)'
- en: All in all, you can see that there are a lot of possibilities for those who
    want to handle and get to know their data to a larger extent with the help of
    select(), help() and the functions of the functions module.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，你可以看到，通过使用 select()、help() 和函数模块的功能，有很多可能性来处理和深入了解你的数据。
- en: Add, Update & Remove Columns
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加、更新和删除列
- en: You might also want to look into adding, updating or removing some columns from
    your Spark DataFrame. You can easily do this with the withColumn(), withColumnRenamed()
    and drop() methods. You’ll probably know by now that you also have a drop() method
    at your disposal when you’re working with Pandas DataFrames. You can read more
    [here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html?highlight=drop#pandas.DataFrame.drop).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还想查看如何向 Spark DataFrame 中添加、更新或删除一些列。你可以通过 withColumn()、withColumnRenamed()
    和 drop() 方法轻松实现。你现在可能知道在使用 Pandas DataFrames 时，你也可以使用 drop() 方法。你可以在[这里](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html?highlight=drop#pandas.DataFrame.drop)阅读更多信息。
- en: '![PySpark cheat sheet](../Images/751902d5d047e7357ce04767eb6020aa.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark 备忘单](../Images/751902d5d047e7357ce04767eb6020aa.png)'
- en: GroupBy
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GroupBy
- en: Again, just like with Pandas DataFrames, you might want to group by certain
    values and aggregate some values - The example below shows how you use the groupBy()
    method, in combination with count() and show() to retrieve and show your data,
    grouped by age, together with the number of people who have that certain age.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 再次提醒，就像使用 Pandas DataFrames 一样，你可能会想按某些值进行分组并汇总一些值 - 下面的示例展示了如何使用 groupBy()
    方法，结合 count() 和 show() 来检索和显示按年龄分组的数据，以及有多少人拥有那个特定年龄。
- en: '![PySpark cheat sheet](../Images/283cb11c84f4170ae04c5534ce466532.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark 备忘单](../Images/283cb11c84f4170ae04c5534ce466532.png)'
- en: More On This Topic
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多信息
- en: '[PySpark for Data Science](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PySpark 用于数据科学](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
- en: '[Data Validation for PySpark Applications using Pandera](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Pandera 进行 PySpark 应用程序的数据验证](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
- en: '[Data Cleaning with Python Cheat Sheet](https://www.kdnuggets.com/2023/02/data-cleaning-python-cheat-sheet.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 进行数据清理的备忘单](https://www.kdnuggets.com/2023/02/data-cleaning-python-cheat-sheet.html)'
- en: '[Best Python Tools for Building Generative AI Applications Cheat Sheet](https://www.kdnuggets.com/2023/08/best-python-tools-generative-ai-cheat-sheet.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[构建生成 AI 应用程序的最佳 Python 工具备忘单](https://www.kdnuggets.com/2023/08/best-python-tools-generative-ai-cheat-sheet.html)'
- en: '[Python Control Flow Cheat Sheet](https://www.kdnuggets.com/2022/11/python-control-flow-cheatsheet.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 控制流备忘单](https://www.kdnuggets.com/2022/11/python-control-flow-cheatsheet.html)'
- en: '[KDnuggets News, July 5: A Rotten Data Science Project • 10 AI…](https://www.kdnuggets.com/2023/n24.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，7月5日：一个糟糕的数据科学项目 • 10个人工智能…](https://www.kdnuggets.com/2023/n24.html)'
