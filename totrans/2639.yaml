- en: The Ultimate Scikit-Learn Machine Learning Cheatsheet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/01/ultimate-scikit-learn-machine-learning-cheatsheet.html](https://www.kdnuggets.com/2021/01/ultimate-scikit-learn-machine-learning-cheatsheet.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Andre Ye](https://www.linkedin.com/in/andre-ye-501746150/), Cofounder
    at Critiq, Editor & Top Writer at Medium**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c42d1fe3155470995201aebbeebf6af0.png)'
  prefs: []
  type: TYPE_IMG
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '*Source: [Pixabay](https://pixabay.com/illustrations/neurons-brain-cells-brain-structure-1773922/).*'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several areas of data mining and machine learning that will be covered
    in this cheat-sheet:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Predictive Modelling. **Regression and classification algorithms for supervised
    learning (prediction), metrics for evaluating model performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Methods to group data without a label into clusters: K-Means, selecting cluster
    numbers based objective metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dimensionality Reduction. **Methods to reduce the dimensionality of data
    and attributes of those methods: PCA and LDA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature Importance. **Methods to find the most important feature in a dataset:
    permutation importance, SHAP values, Partial Dependence Plots.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Transformation. **Methods to transform the data for greater predictive
    power, for easier analysis, or to uncover hidden relationships and patterns: standardization,
    normalization, box-cox transformations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*All images were created by the author unless explicitly stated otherwise.*'
  prefs: []
  type: TYPE_NORMAL
- en: Predictive Modelling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Train-test-split** is an important part of testing how well a model performs
    by training it on designated training data and testing it on designated testing
    data. This way, the model’s ability to generalize to new data can be measured.
    In *sklearn*, both lists, pandas DataFrames, or NumPy arrays are accepted in *X* and *y* parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/bad0e04447b544e30401407ea0e8d365.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Training a standard supervised learning model** takes the form of an import,
    the creation of an instance, and the fitting of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c0f7ac13ab7b2ac92729b9d595536b72.png)'
  prefs: []
  type: TYPE_IMG
- en: '***sklearn*****classifier models** are listed below, with the branch highlighted
    in blue and the model name in orange.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ddb00a46b37d655a151a482e6d10857.png)'
  prefs: []
  type: TYPE_IMG
- en: '***sklearn*****regressor models** are listed below, with the branch highlighted
    in blue and the model name in orange.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d17b06639eb00f8abe1d27bc25470fdb.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Evaluating model performance** is done with train-test data in this form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38a5da245fb60db3d076a5ceb3b90011.png)'
  prefs: []
  type: TYPE_IMG
- en: '***sklearn*****metrics** for classification and regression are listed below,
    with the most commonly used metric marked in green. Many of the grey metrics are
    more appropriate than the green-marked ones in certain contexts. Each has its
    own advantages and disadvantages, balancing priority comparisons, interpretability,
    and other factors.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b0361373ab3965443949851c7ae368d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before clustering, the data needs to be standardized (information for this can
    be found in the Data Transformation section). Clustering is the process of creating
    clusters based on point distances.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46d0481ce1d2d2cdeb3d5ef04af51320.png)'
  prefs: []
  type: TYPE_IMG
- en: '*[Source](https://i.imgur.com/S65Sk9c.jpg). Image free to share.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training and creating a K-Means clustering model** creates a model that can
    cluster and retrieve information about the clustered data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Accessing the labels** of each of the data points in the data can be done
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the label of each data point can be stored in a column of the data
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Accessing the cluster label of new data** can be done with the following
    command. The *new_data *can be in the form of an array, a list, or a DataFrame.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Accessing the cluster centers of each cluster** is returned in the form of
    a two-dimensional array with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: To **find the optimal number of clusters**, use the silhouette score, which
    is a metric of how well a certain number of clusters fits the data. For each number
    of clusters within a predefined range, a K-Means clustering algorithm is trained,
    and its silhouette score is saved to a list (*scores*). *data *is the *x* that
    the model is trained on.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: After the scores are saved to the list *scores*, they can be graphed out or
    computationally searched for to find the highest one.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality Reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dimensionality reduction is the process of expressing high-dimensional data
    in a reduced number of dimensions such that each one contains the most amount
    of information. Dimensionality reduction may be used for visualization of high-dimensional
    data or to speed up machine learning models by removing low-information or correlated
    features.
  prefs: []
  type: TYPE_NORMAL
- en: '**Principal Component Analysis**, or PCA, is a popular method of reducing the
    dimensionality of data by drawing several orthogonal (perpendicular) vectors in
    the feature space to represent the reduced number of dimensions. The variable *number *represents
    the number of dimensions the reduced data will have. In the case of visualization,
    for example, it would be two dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4c0f0b5a08bcf33466a8b6c6dee6404.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Visual demonstration of how PCA works. [Source](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.09-PCA-rotation.png).*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fitting the PCA Model**: The *.fit_transform* function automatically fits
    the model to the data and transforms it into a reduced number of dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Explained Variance Ratio**: Calling *model.explained_variance_ratio_* will
    yield a list where each item corresponds to that dimension’s “explained variance
    ratio,” which essentially means the percent of the information in the original
    data represented by that dimension. The sum of the explained variance ratios is
    the total percent of information retained in the reduced dimensionality data.'
  prefs: []
  type: TYPE_NORMAL
- en: '**PCA Feature Weights**: In PCA, each newly creates feature is a linear combination
    of the former data’s features. Theselinear weights can be accessed with *model.components_*,
    and are a good indicator for feature importance (a higher linear weight indicates
    more information represented in that feature).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear Discriminant Analysis **(LDA, not to be commonly confused with Latent
    Dirichlet Allocation) is another method of dimensionality reduction. The primary
    difference between LDA and PCA is that LDA is a supervised algorithm, meaning
    it takes into account both *x* and *y*. Principal Component Analysis only considers *x *and
    is hence an unsupervised algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: PCA attempts to maintain the structure (variance) of the data purely based on
    distances between points, whereas LDA prioritizes clean separation of classes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Feature Importance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Feature Importance is the process of finding the most important feature to a
    target. Through PCA, the feature that contains the most information can be found,
    but feature importance concerns a feature’s impact on the target. A change in
    an ‘important’ feature will have a large effect on the *y*-variable, whereas a
    change in an ‘unimportant’ feature will have little to no effect on the *y*-variable.
  prefs: []
  type: TYPE_NORMAL
- en: '**Permutation Importance** is a method to evaluate how important a feature
    is. Several models are trained, each missing one column. The corresponding decrease
    in model accuracy as a result of the lack of data represents how important the
    column is to a model’s predictive power. The *eli5 *library is used for Permutation
    Importance.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/bd84e548102cceccc7a7fbceba6018b5.png)'
  prefs: []
  type: TYPE_IMG
- en: In the data that this Permutation Importance model was trained on, the column *lat *has
    the largest impact on the target variable (in this case, the house price). Permutation
    Importance is the best feature to use when deciding which to remove (correlated
    or redundant features that actually confuse the model, marked by negative permutation
    importance values) in models for best predictive performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**SHAP** is another method of evaluating feature importance, borrowing from
    game theory principles in Blackjack to estimate how much value a player can contribute.
    Unlike permutation importance, **SH**apley **A**ddative Ex**P**lanations use a
    more formulaic and calculation-based method towards evaluating feature importance.
    SHAP requires a tree-based model (Decision Tree, Random Forest) and accommodates
    both regression and classification.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/152c22c8c95f1a6b6c806abaa87bb9bb.png)'
  prefs: []
  type: TYPE_IMG
- en: '**PD(P) Plots**, or partial dependence plots, are a staple in data mining and
    analysis, showing how certain values of one feature influence a change in the
    target variable. Imports required include *pdpbox *for the dependence plots and *matplotlib *to
    display the plots.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**Isolated PDPs**: the following code displays the partial dependence plot,
    where *feat_name *is the feature within *X* that will be isolated and compared
    to the target variable. The second line of code saves the data, whereas the third
    constructs the canvas to display the plot.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/58f7006d67ab60de631e6e2b848049f2.png)'
  prefs: []
  type: TYPE_IMG
- en: The partial dependence plot shows the effect of certain values and changes in
    the number of square feet of living space on the price of a house. Shaded areas
    represent confidence intervals.
  prefs: []
  type: TYPE_NORMAL
- en: '**Contour PDPs**: Partial dependence plots can also take the form of contour
    plots, which compare not one isolated variable but the relationship between two
    isolated variables. The two features that are to be compared are stored in a variable *compared_features*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/bbf4558db9f2dd7c69e554308d0da0fb.png)'
  prefs: []
  type: TYPE_IMG
- en: The relationship between the two features shows the corresponding price when
    only considering these two features. Partial dependence plots are chock-full of
    data analysis and findings, but be conscious of large confidence intervals.
  prefs: []
  type: TYPE_NORMAL
- en: Data Transformation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Standardizing or scaling** is the process of ‘reshaping’ the data such that
    it contains the same information but has a mean of 0 and a variance of 1\. By
    scaling the data, the mathematical nature of algorithms can usually handle data
    better.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The *transformed_data *is standardized and can be used for many distance-based
    algorithms such as Support Vector Machine and K-Nearest Neighbors. The results
    of algorithms that use standardized data need to be ‘de-standardized’ so they
    can be properly interpreted. *.inverse_transform()* can be used to perform the
    opposite of standard transforms.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**Normalizing **data puts it on a 0 to 1 scale, something that, similar to
    standardized data, makes the data mathematically easier to use for the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: While normalizing doesn’t transform the shape of the data as standardizing does,
    it restricts the boundaries of the data. Whether to normalize or standardize data
    depends on the algorithm and the context.
  prefs: []
  type: TYPE_NORMAL
- en: '**Box-cox transformations** involve raising the data to various powers to transform
    it. Box-cox transformations can normalize data, make it more linear, or decrease
    the complexity. These transformations don’t only involve raising the data to powers
    but also fractional powers (square rooting) and logarithms.'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, consider data points situated along the function *g*(*x*). By
    applying the logarithm box-cox transformation, the data can be easily modelled
    with linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6e95740630c13aa7b4eb7586beaf53e.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Created with Desmos.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*sklearn *automatically determines the best series of box-cox transformations
    to apply to the data to make it better resemble a normal distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Because of the nature of box-cox transformation square-rooting, box-cox transformed
    data must be strictly positive (normalizing the data beforehand can take care
    of this). For data with negative data points as well as positive ones, set *method
    = ‘yeo-johnson’* for a similar approach to making the data more closely resemble
    a bell curve.
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/your-ultimate-data-mining-machine-learning-cheat-sheet-9fce3fa16).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[10 Things You Didn’t Know About Scikit-Learn](https://www.kdnuggets.com/2020/09/10-things-know-scikit-learn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Extend Scikit-learn and Bring Sanity to Your Machine Learning Workflow](https://www.kdnuggets.com/2019/10/extend-scikit-learn-bring-sanity-machine-learning-workflow.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning Cheat Sheets](https://www.kdnuggets.com/2018/09/machine-learning-cheat-sheets.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
