# 向前特征选择：Python中的实际示例

> 原文：[https://www.kdnuggets.com/2018/06/step-forward-feature-selection-python.html](https://www.kdnuggets.com/2018/06/step-forward-feature-selection-python.html)

[评论](#comments)

存在许多[特征选择](https://en.wikipedia.org/wiki/Feature_selection)的方法，有些将该过程视为一种艺术，有些则视为科学，实际上，结合领域知识和规范方法往往是最佳选择。

在特征选择的规范方法中，[包裹方法](https://en.wikipedia.org/wiki/Feature_selection#Wrapper_method)是将特征选择过程与构建的模型类型结合在一起的方法，评估特征子集以检测特征之间的模型性能，随后选择表现最佳的子集。换句话说，包裹方法并不是一个在模型构建*之前*独立存在的过程，而是试图与特定的机器学习算法*同步*优化特征选择过程。

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析能力

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你所在的组织进行IT支持

* * *

两种突出的包裹方法是向前特征选择和向后特征选择。

![图片](../Images/17075aa097f90c967aa0ef0730e1a2ba.png)

[图片来源](https://en.wikipedia.org/wiki/Feature_selection)

向前特征选择从评估每个单独的特征开始，选择结果最优的特征，从而形成表现最佳的选定算法模型。什么是“最佳”？这完全取决于定义的评估标准（如AUC、预测准确率、RMSE等）。接下来，评估该选定特征和后续特征的所有可能组合，选择第二个特征，以此类推，直到选择出所需的预定义特征数量。

向后特征选择与之密切相关，正如你可能猜到的那样，它从整个特征集开始，向后工作，移除特征以找到预定义大小的最佳子集。

这些方法都可能计算开销非常大。你有一个大规模、多维的数据集吗？这些方法可能需要过长的时间才会有用，甚至可能完全不可行。不过，若数据集的规模和维度适中，这种方法可能是你最佳的选择。

为了了解它们是如何工作的，我们将特别关注前向特征选择。需要注意的是，正如所讨论的，在开始我们的共生特征选择过程之前，必须先定义一个机器学习算法。

请记住，使用给定算法优化后的特征集在不同算法下的表现可能会有所不同。例如，如果我们使用逻辑回归选择特征，并不能保证这些相同的特征在尝试 K 最近邻或支持向量机时表现最佳。

### 实现特征选择和模型构建

那么，我们如何在 Python 中执行前向特征选择呢？[Sebastian Raschka 的 mlxtend 库](https://github.com/rasbt/mlxtend)包括一个实现（[Sequential Feature Selector](https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/)），因此我们将使用它进行演示。不言而喻，你应该在继续之前安装 mlxtend（请查看 GitHub 仓库）。

![图片](../Images/d13fd31db64ac19a44c703d8cdc720ec.png)

我们将使用随机森林分类器进行特征选择和模型构建（这两者在前向特征选择的情况下密切相关）。

我们需要用于演示的数据，因此让我们使用[葡萄酒质量数据集](https://archive.ics.uci.edu/ml/datasets/wine+quality)。具体来说，我在下面的代码中使用了未经处理的 `winequality-white.csv` 文件作为输入。

我们将任意设定所需的特征数量为 5（数据集中有 12 个特征）。我们能够做的是比较特征选择过程每次迭代的评估得分，因此请记住，如果我们发现更少的特征具有更好的得分，我们可以选择那个表现最佳的子集，用于我们“实时”模型的后续运行。还要记住，将我们所需的特征数量设定得过低可能会导致决定的特征数量和组合不尽如人意（例如，如果在我们的案例中某些 11 特征组合比我们在选择过程中找到的最好的 ≤ 10 特征组合表现更好）。

由于我们更感兴趣的是展示如何实现前向特征选择，而不是对这个特定数据集的实际结果，我们不会过于关注模型的实际表现，但我们还是会比较模型的表现，以展示在有意义的项目中如何进行。

首先，我们将进行导入，加载数据集，并将其拆分为训练集和测试集。

[PRE0]

[PRE1]

接下来，我们将定义一个分类器，以及一个前向特征选择器，然后执行特征选择。mlxtend 中的特征选择器具有一些我们可以定义的参数，所以我们将按如下方式进行：

+   首先，我们将分类器传递给前向特征选择器，以上定义的随机森林分类器

+   接下来，我们定义了我们希望选择的特征子集（k_features=5）。

+   然后我们将浮动设置为False；有关浮动的更多信息，请参阅文档：

> 浮动算法有一个额外的排除或包含步骤，以便在特征被包含（或排除）后移除它们，从而可以采样更多的特征子集组合。

+   我们设置了mlxtend报告所需的详细程度。

+   重要的是，我们将评分设置为准确率；这只是一个可以用于评估基于选择特征构建的模型的指标。

+   mlxtend特征选择器内部使用交叉验证，我们将演示中的期望折数设置为5。

我们选择的数据集并不大，因此以下代码应该不会花费太多时间执行。

[PRE2]

[PRE3]

根据我们的评分指标，我们表现最佳的模型是5个特征的某个子集，得分为0.644（请记住，这里使用的是交叉验证，因此与我们在下面的完整模型中使用训练和测试集报告的结果会有所不同）。那么选择了哪5个特征的子集呢？

[PRE4]

[PRE5]

这些索引的列就是被选择的列。太好了！那么接下来呢？

我们现在可以使用这些特征来构建一个使用训练和测试集的完整模型。如果我们有一个更大的数据集（即更多的实例而不是更多的特征），这将特别有益，因为我们可以在较小的实例子集上使用上述特征选择器，确定我们表现最佳的特征子集，然后将其应用于完整数据集进行分类。

以下代码仅在**选择的特征子集**上构建分类器。

[PRE6]

[PRE7]

不用担心实际准确率；我们关注的是过程，而不是最终结果。

但如果我们*关心*最终结果，并想知道我们的特征选择工作是否值得呢？我们可以将使用选择特征构建的完整模型（如上所述）的结果准确率与使用**所有**特征构建的另一个完整模型的结果准确率进行比较，就像我们下面所做的那样：

[PRE8]

[PRE9]

比较它们，你会发现它们都很差，与我们使用选择特征构建的模型相当（虽然我保证情况并不总是这样！）。只需少量工作，你就可以看到这些选择的特征在不同算法下的表现，以满足对这些特征是否在另一算法中表现同样好的好奇心。

这种特征选择方法可以成为有纪律的机器学习流程中的一个有效部分。请记住，前进（或后退）方法，特别是在处理特别大或高维数据集时，可能会遇到问题。有办法绕过（或**尝试**绕过）这些难点，例如从数据中采样，以找到最有效的特征子集，然后在完整数据集上使用这些特征进行建模。当然，这些也不是唯一的特征选择方法，因此在处理这些较大数据集时，可能需要考虑其他替代方法。

**相关**：

+   [使用 fast.ai 进行快速特征工程与日期](https://www.kdnuggets.com/2018/03/feature-engineering-dates-fastai.html)

+   [用 4 行代码生成文本的 RNN](https://www.kdnuggets.com/2018/06/generating-text-rnn-4-lines-code.html)

+   [特征选择的多目标优化](https://www.kdnuggets.com/2017/12/rapidminer-multi-objective-optimization-feature-selection.html)

### 相关主题

+   [揭示 Midjourney 5.2：AI 图像生成的重大进步](https://www.kdnuggets.com/2023/06/unveiling-midjourney-52-leap-forward.html)

+   [揭示 Meta 的 Llama 2 的力量：生成式 AI 的重大进步？](https://www.kdnuggets.com/2023/07/unveiling-power-metas-llama-2-leap-forward-generative-ai.html)

+   [特征选择：科学与艺术的交汇点](https://www.kdnuggets.com/2021/12/feature-selection-science-meets-art.html)

+   [机器学习中的替代特征选择方法](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)

+   [机器学习模型的高级特征选择技术](https://www.kdnuggets.com/2023/06/advanced-feature-selection-techniques-machine-learning-models.html)

+   [机器学习中的实用特征工程方法](https://www.kdnuggets.com/2023/07/practical-approach-feature-engineering-machine-learning.html)
