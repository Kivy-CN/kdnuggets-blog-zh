- en: 'Reinforcement Learning: Teaching Computers to Make Optimal Decisions'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/07/reinforcement-learning-teaching-computers-make-optimal-decisions.html](https://www.kdnuggets.com/2023/07/reinforcement-learning-teaching-computers-make-optimal-decisions.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What Is Reinforcement Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning is a branch of machine learning that deals with an agent
    learning—through experience—how to interact with a complex environment.
  prefs: []
  type: TYPE_NORMAL
- en: From AI agents that play and surpass human performance in complex board games
    such as chess and Go to autonomous navigation, reinforcement learning has a suite
    of interesting and diverse applications.
  prefs: []
  type: TYPE_NORMAL
- en: Remarkable breakthroughs in the field of reinforcement learning include DeepMind’s
    agent [AlphaGo Zero](https://www.deepmind.com/blog/alphago-zero-starting-from-scratch)
    that can defeat even human champions in the game of Go and [AlphaFold](https://www.deepmind.com/research/highlighted-research/alphafold)
    that can predict complex 3D protein structure.
  prefs: []
  type: TYPE_NORMAL
- en: This guide will introduce you to the reinforcement learning paradigm. We’ll
    take a simple yet motivating real-world example to understand the reinforcement
    learning framework.
  prefs: []
  type: TYPE_NORMAL
- en: The Reinforcement Learning Framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start by defining the components of a reinforcement learning framework.
  prefs: []
  type: TYPE_NORMAL
- en: '![Reinforcement Learning: Teaching Computers to Make Optimal Decisions](../Images/d1344fd46c3456a8c995f0c2d76e076a.png)'
  prefs: []
  type: TYPE_IMG
- en: Reinforcement Learning Framework | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'In a typical reinforcement learning framework:'
  prefs: []
  type: TYPE_NORMAL
- en: There is an **agent** learning to interact with the **environment**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent can measure its **state**, take **actions**, and occasionally gets
    a **reward**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Practical examples of this setting: the agent can play against an opponent
    (say, a game of chess) or try to navigate a complex environment.'
  prefs: []
  type: TYPE_NORMAL
- en: As a super simplified example, consider a mouse in a maze. Here, the agent is
    *not* playing against an opponent but rather trying to figure out a path to the
    exit. If there are more than one paths leading to the exit, we may prefer the
    shortest path out of the maze.
  prefs: []
  type: TYPE_NORMAL
- en: '![Reinforcement Learning: Teaching Computers to Make Optimal Decisions](../Images/343faadea35883d9687ff5a3a9df4e82.png)'
  prefs: []
  type: TYPE_IMG
- en: Mouse in a Maze | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the *mouse* is the *agent* trying to navigate the *environment*
    which is the *maze*. The *action* here is the movement of the mouse within the
    maze. When it successfully navigates the maze to the exit—it gets a *piece of
    cheese* as a *reward*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Reinforcement Learning: Teaching Computers to Make Optimal Decisions](../Images/5ed063fccd92b8c4942e9a76efda7ece.png)'
  prefs: []
  type: TYPE_IMG
- en: Example | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The sequence of actions happens in discrete time steps (say, t = 1, 2, 3,...).
    At any time step **t**, the mouse can only measure its current state in the maze.
    It doesn’t know the whole maze yet.
  prefs: []
  type: TYPE_NORMAL
- en: So the agent (the mouse) measures its state ![Equation](../Images/50d4c306c3da2ab64f5bca9a582b48ce.png)
    in the environment at time step **t**, takes a valid action ![Equation](../Images/8df11873ed817574f0269f21bf1c8037.png)
    and moves to state ![Equation](../Images/1978948a8f905005c08aff7cb743dab0.png).
  prefs: []
  type: TYPE_NORMAL
- en: '![Reinforcement Learning: Teaching Computers to Make Optimal Decisions](../Images/9e636b521640ce1e36e2998cedaa0bab.png)'
  prefs: []
  type: TYPE_IMG
- en: State | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: How Is Reinforcement Learning Different?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Notice how the mouse (the agent) has to figure its way out of the maze through
    **trial and error**. Now if the mouse hits one of the walls of the maze, it has
    to try to find its way back and etch a different route to the exit.
  prefs: []
  type: TYPE_NORMAL
- en: If this were a supervised learning setting, after every move, the agent would
    get to know whether or not that action—was correct—and would lead to a reward.
    Supervised learning is like learning from a teacher.
  prefs: []
  type: TYPE_NORMAL
- en: While a teacher tells you ahead of time, a critic always tells you—after the
    performance is over— how good or bad your performance was. For this reason, reinforcement
    learning is also called learning in the *presence of a critic*.
  prefs: []
  type: TYPE_NORMAL
- en: Terminal State and Episode
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When the mouse has reached the exit, it reaches the **terminal state**. Meaning
    it cannot explore any further.
  prefs: []
  type: TYPE_NORMAL
- en: And the sequence of actions—from the initial state to the terminal state—is
    called an episode. For any learning problem, we need multiple episodes for the
    agent to learn to navigate. Here, for our agent (the mouse) to learn the sequence
    of actions that would lead it to the exit, and subsequently, receive the piece
    of cheese, we’d need many episodes.
  prefs: []
  type: TYPE_NORMAL
- en: Dense and Sparse Rewards
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whenever the agent takes a correct action or a sequence of actions that is correct,
    it gets a **reward**. In this case, the mouse receives a piece of cheese as a
    reward for etching a valid route—through the maze(the environment)—to the exit.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the mouse receives a piece of cheese only at the very end—when
    it reaches the exit.This is an example of a *sparse* and delayed reward.
  prefs: []
  type: TYPE_NORMAL
- en: If the rewards are more frequent, then we will have a *dense* reward system.
  prefs: []
  type: TYPE_NORMAL
- en: Looking back we need to figure out (it’s not trivial) which action or sequence
    of actions caused the agent to get the reward; this is commonly called the [credit
    assignment problem](https://ai.stackexchange.com/questions/12908/what-is-the-credit-assignment-problem).
  prefs: []
  type: TYPE_NORMAL
- en: Policy, Value Function, and the Optimization Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The environment is often not deterministic but probabilistic and so is the policy.
    Given a state ![Equation](../Images/50d4c306c3da2ab64f5bca9a582b48ce.png), the
    agent takes an action and goes to another state ![Equation](../Images/991b31a62368366eaf20405d8514c526.png)
    with a certain probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'The policy helps define a mapping from the set of possible states to the actions.
    It helps answer questions like:'
  prefs: []
  type: TYPE_NORMAL
- en: What actions to take to maximize the expected reward?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Or better yet: Given a state, what is the best possible action that the agent
    can take so as to maximize expected reward?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So you can think of the agent as *enacting a policy* π:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reinforcement Learning: Teaching Computers to Make Optimal Decisions](../Images/c29258389a41a8438d777e35214799e3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another related and helpful concept is the value function. The **value function**
    is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reinforcement Learning: Teaching Computers to Make Optimal Decisions](../Images/0eb789ff1281ec7c694c4f52ee9f8b5b.png)'
  prefs: []
  type: TYPE_IMG
- en: This signifies the value of being in a state given a policy π. The quantity
    denotes the expected reward in the future if the agent starts at state and enacts
    the policy π thereafter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum up: the goal of reinforcement learning is to optimize the policy so
    as to maximize the expected future rewards. Therefore, we can think of it as an
    optimization problem to solve for π.'
  prefs: []
  type: TYPE_NORMAL
- en: Discount Factor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Notice that we have a new quantity ɣ. What does it stand for? ɣ is called the
    **discount factor,** a quantity between 0 and 1\. Meaning future rewards are discounted
    (read: because now is greater than much later).'
  prefs: []
  type: TYPE_NORMAL
- en: Exploration vs. Exploitation Tradeoff
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Going back to the food loop example of mouse in a maze: If the mouse is able
    to figure out a route to exit A with a small piece of cheese, it can keep repeating
    it and collecting the cheese piece. But what if the maze also had another exit
    B with a bigger cheese piece (greater reward)?'
  prefs: []
  type: TYPE_NORMAL
- en: So long as the mouse keeps *exploiting* this current strategy without *exploring*
    new strategies, it is not going to get the much greater reward of a bigger cheese
    piece at exit B.
  prefs: []
  type: TYPE_NORMAL
- en: '![Reinforcement Learning: Teaching Computers to Make Optimal Decisions](../Images/4fd7b3a8aa6aa8366c8d3ed3166d7cf5.png)'
  prefs: []
  type: TYPE_IMG
- en: Exploration vs. Exploitation | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: But the *uncertainty* associated with exploring new strategies and future rewards
    is greater. So how do we exploit and explore? This tradeoff between exploiting
    the current strategy and exploring new ones with potentially better rewards is
    called the **exploration vs exploitation tradeoff**.
  prefs: []
  type: TYPE_NORMAL
- en: One possible approach is the *ε-greedy search*. Given a set of all possible
    actions , the *ε-greedy search* explores one of the possible actions with the
    probability *ε* while exploiting the current strategy with the probability 1 -
    *ε*.
  prefs: []
  type: TYPE_NORMAL
- en: Wrap-up and Next Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s summarize what we’ve covered so far. We learned about the components
    of the reinforcement learning framework:'
  prefs: []
  type: TYPE_NORMAL
- en: The agent interacts with the environment, gets to measure its current state,
    takes actions, and receives rewards as positive reinforcement. The framework is
    probabilistic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then went over value functions and policy, and how the optimization problem
    often boils down to finding the optimal policies that maximize the expected future
    awards.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You’ve now learned just enough to navigate the reinforcement learning landscape.
    Where to go from here? We did not talk about reinforcement learning algorithms
    in this guide, so you can explore some basic algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: If we know everything about the environment (and can model it completely), we
    can use model-based algorithms like [policy iteration and value iteration](http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15381-f01/www/handouts/111301.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, in most cases, we may not be able to model the environment completely.
    In this case, you can look at model-free algorithms such as [Q-learning](https://deeplizard.com/learn/video/qhRNvCVVJaA)
    which optimizes state-action pairs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re looking to further your understanding of reinforcement learning, [David
    Silver’s reinforcement learning lectures](https://www.youtube.com/watch?v=2pWv7GOvuf0)
    on YouTube and [Hugging Face’s Deep Reinforcement Learning Course](https://huggingface.co/learn/deep-rl-course/unit0/introduction)
    are some good resources to look at.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Bala Priya C](https://www.linkedin.com/in/bala-priya/)** is a developer
    and technical writer from India. She likes working at the intersection of math,
    programming, data science, and content creation. Her areas of interest and expertise
    include DevOps, data science, and natural language processing. She enjoys reading,
    writing, coding, and coffee! Currently, she''s working on learning and sharing
    her knowledge with the developer community by authoring tutorials, how-to guides,
    opinion pieces, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[The Optimal Way to Input Missing Data with Pandas fillna()](https://www.kdnuggets.com/2023/02/optimal-way-input-missing-data-pandas-fillna.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Generalized and Scalable Optimal Sparse Decision Trees(GOSDT)](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Driving Better Business Decisions](https://www.kdnuggets.com/2022/04/informs-driving-better-business-decisions.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Explainable AI: 10 Python Libraries for Demystifying Your Model''s Decisions](https://www.kdnuggets.com/2023/01/explainable-ai-10-python-libraries-demystifying-decisions.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hands-on Reinforcement Learning Course Part 3: SARSA](https://www.kdnuggets.com/2022/01/handson-reinforcement-learning-course-part-3-sarsa.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hands-On Reinforcement Learning Course, Part 1](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-course-part-1.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
