["```py\nif local:\n    # Execute pipeline in your local machine.\n    runner_options = {\n        \"runner\": \"DirectRunner\",\n    }\nelse:\n    runner_options = {\n        \"runner\": \"DataflowRunner\",\n        \"temp_location\": \"...\",\n        \"staging_location.\": \"...\",\n        \"max_num_workers\": max_num_workers,\n    }\n\noptions = PipelineOptions(\n      project=project_id,\n      job_name=job_name,\n      region=region,\n      **runner_options\n  )\n\noptions.view_as(SetupOptions).save_main_session = True\noptions.view_as(SetupOptions).setup_file = os.path.join(\n    pathlib.Path(__file__).parent.absolute(), \"..\", \"setup.py\") \n```", "```py\nwith beam.Pipeline(options=options) as p:\n        ... \n```", "```py\np = p | Task(); \n```", "```py\np = p | (\"task_name\" >> Task()); \n```", "```py\n(p\n | \"source\" >> ReadFromText(input_path)\n | \"load\" >> beam.Map(load)\n | \"extract\" >> beam.Map(extract)\n | \"store\" >> beam.ParDo(store, output_path)) \n```", "```py\ndef load(path):\n    \"\"\"\n    Receives an image path and returns a dictionary containing\n    the image path and a resized version of the image as a np.array.\n    \"\"\"\n    buf = GcsIO().open(path, mime_type=\"image/jpeg\")\n    img = Image.open(io.BytesIO(buf.read()))\n    img = img.resize((IMAGE_HEIGHT, IMAGE_WIDTH), Image.ANTIALIAS)\n    return {\"path\": path, \"image\": np.array(img)} \n```", "```py\ndef singleton(cls):\n    instances = {}\n\n    def get_instance(*args, **kwargs):\n        if cls not in instances:\n            instances[cls] = cls(*args, **kwargs)\n        return instances[cls]\n\n    return get_instance\n\n@singleton\nclass Extractor:\n    \"\"\"\n    Extract image embeddings using a pre-trained ResNet50 model.\n    \"\"\"\n    def __init__(self):\n        self.model = ResNet50(\n            input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3),\n            include_top=False,\n            weights=\"imagenet\",\n            pooling=None,\n        )\n\n    def extract(self, image):\n        return self.model.predict(np.expand_dims(image, axis=0))[0]\n\ndef extract(item):\n    \"\"\"\n    Extracts the feature embedding from item[\"image\"].\n    \"\"\"\n    extractor = Extractor()\n    item[\"embedding\"] = extractor.extract(item[\"image\"])\n    # We do not longer need the image, remove it from the dictonary to free memory.\n    del item[\"image\"]\n    return item \n```", "```py\ndef store(item, output_path):\n    \"\"\"\n    Store the image embeddings item[\"embedding\"] in GCS.\n    \"\"\"\n    name = item[\"path\"].split(\"/\")[-1].split(\".\")[0]\n    path = os.path.join(output_path, f\"{name}.npy\")\n\n    fin = io.BytesIO()\n    np.save(fin, item[\"embedding\"])\n\n    fout = beam.io.gcp.gcsio.GcsIO().open(path, mode=\"w\")\n    fin.seek(0)\n    fout.write(fin.read()) \n```", "```py\np = (\n   p\n     | \"source\" >> ReadFromText(input_path)\n     | \"load\" >> beam.Map(load)\n     | \"extract\" >> beam.Map(extract)))\n\n(p | \"store\" >> beam.ParDo(store, output_path))\n(p | \"reduce_dim\" >> beam.ParDo(reduce_dim)) \n```"]