- en: Learn how to use PySpark in under 5 minutes (Installation + Tutorial)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习如何在5分钟内使用PySpark（安装 + 教程）
- en: 原文：[https://www.kdnuggets.com/2019/08/learn-pyspark-installation-tutorial.html](https://www.kdnuggets.com/2019/08/learn-pyspark-installation-tutorial.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/08/learn-pyspark-installation-tutorial.html](https://www.kdnuggets.com/2019/08/learn-pyspark-installation-tutorial.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Georgios Drakos](https://gdcoder.com/author/), Data Scientist at TUI**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由[乔治奥斯·德拉科斯](https://gdcoder.com/author/)，TUI的数据显示科学家**'
- en: I’ve found that is a little difficult to get started with Apache Spark (this
    will focus on PySpark) and install it on local machines for most people. With
    this simple tutorial you’ll get there really fast!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现对于大多数人来说，开始使用Apache Spark（本教程将集中在PySpark上）并在本地计算机上安装它有点困难。通过这个简单的教程，你将能非常快速地完成这项工作！
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速入门网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[**Apache Spark**](http://spark.apache.org/)** is a must for Big data’s lovers **as
    it is a fast, easy-to-use general engine for big data processing with built-in
    modules for streaming, SQL, machine learning and graph processing. This technology
    is an in-demand skill for data engineers, but also data scientists can benefit
    from learning Spark when doing Exploratory Data Analysis (EDA), feature extraction
    and, of course, ML. But please remember that Spark is only truly realized when
    it is run on a cluster with a large number of nodes.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Apache Spark**](http://spark.apache.org/)**是大数据爱好者必备的工具**，它是一个快速、易于使用的通用大数据处理引擎，内置流处理、SQL、机器学习和图形处理模块。这项技术是数据工程师的热门技能，但数据科学家在进行探索性数据分析（EDA）、特征提取以及当然的机器学习时也能从学习Spark中受益。但请记住，Spark只有在大量节点的集群上运行时才能真正发挥作用。'
- en: Table of Contents
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目录
- en: Introduction
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍
- en: Spark definition
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark定义
- en: Spark Application
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark应用程序
- en: Install PySpark on Mac
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Mac上安装PySpark
- en: Open Jupyter Notebook with PySpark
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PySpark打开Jupyter Notebook
- en: Launching a SparkSession
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动SparkSession
- en: Conclussion
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结论
- en: References
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考文献
- en: '![figure-name](../Images/35cf5da02d3e664136e0ff5dee351c8a.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![figure-name](../Images/35cf5da02d3e664136e0ff5dee351c8a.png)'
- en: Introduction
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Apache Spark is one of the hottest and largest open source project in data
    processing framework with rich high-level APIs for the programming languages like
    Scala, Python, Java and R. It realizes the potential of bringing together both
    Big Data and machine learning. This is because:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是数据处理框架中最热门、最大的开源项目之一，拥有丰富的高级API，支持Scala、Python、Java和R等编程语言。它实现了将大数据和机器学习结合在一起的潜力。这是因为：
- en: Spark is fast (up to 100x faster than traditional [Hadoop MapReduce](https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm))
    due to in-memory operation.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于内存操作，Spark的速度很快（比传统的[Hadoop MapReduce](https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm)快达100倍）。
- en: It offers robust, distributed, fault-tolerant data objects (called [RDDs](https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.htm))
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了强大、分布式、容错的数据对象（称为[RDDs](https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.htm)）
- en: It integrates beautifully with the world of machine learning and graph analytics
    through supplementary packages like [MLlib](https://spark.apache.org/mllib/) and [GraphX](https://spark.apache.org/graphx/).
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过像[MLlib](https://spark.apache.org/mllib/)和[GraphX](https://spark.apache.org/graphx/)这样的附加包与机器学习和图形分析的世界完美集成。
- en: Spark is implemented on [Hadoop/HDFS](https://www.bernardmarr.com/default.asp?contentID=1080) and
    written mostly in [Scala](https://www.scala-lang.org/), a functional programming
    language.However, for most beginners, Scala is not a great first language to learn
    when venturing into the world of data science.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, Spark provides a wonderful Python API called [PySpark](https://spark.apache.org/docs/latest/api/python/index.html).
    This allows Python programmers to interface with the Spark framework — letting
    you manipulate data at scale and work with objects over a distributed file system.
    So, Spark is not a new programming language that you have to learn but a framework
    working on top of HDFS.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: This presents new concepts like nodes, lazy evaluation, and the transformation-action
    (or ‘map and reduce’) paradigm of programming.In fact, Spark is versatile enough
    to work with other file systems than Hadoop — like Amazon S3 or Databricks (DBFS).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Internet powerhouses such as Netflix, Yahoo, and eBay have deployed Spark at
    massive scale, collectively processing multiple petabytes of data on clusters
    of over 8,000 nodes.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Spark Definition
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Typically when you think of a computer you think about one machine sitting on
    your desk at home or at work. This machine works perfectly well for applying machine
    learning on small dataset . However, when you have huge dataset(in tera bytes
    or giga bytes), there are some things that your computer is not powerful enough
    to perform. One particularly challenging area is data processing. Single machines
    do not have enough power and resources to perform computations on huge amounts
    of information (or you may have to wait for the computation to finish).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/c36231c95b1b58e1c053a9d632f4efad.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: A cluster, or group of machines, pools the resources of many machines together
    allowing us to use all the cumulative resources as if they were one. Now a group
    of machines alone is not powerful, you need a framework to coordinate work across
    them. **Spark is a tool for just that, managing and coordinating the execution
    of tasks on data across a cluster of computers.**
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Spark Application
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A Spark Application consists of:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Driver
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executors (set of distributed worker processes)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Driver
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Driver runs the main() method of our application having the following duties:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Runs on a node in our cluster, or on a client, and schedules the job execution
    with a cluster manager
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Responds to user’s program or input
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzes, schedules, and distributes work across the executors
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executors
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An executor is a distributed process responsible for the execution of tasks.
    Each Spark Application has its own set of executors, which stay alive for the
    life cycle of a single Spark application.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Executors perform all data processing of a Spark job
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stores results in memory, only persisting to disk when specifically instructed
    by the driver program
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns results to the driver once they have been completed
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each node can have anywhere from 1 executor per node to 1 executor per core
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个节点可以有从每个节点1个执行器到每个核心1个执行器的配置
- en: '** Node is single entity machine or server .'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '** 节点是单一实体的机器或服务器。'
- en: '![figure-name](../Images/ce110256af4c8d0e3e7b93dde3d91146.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![figure-name](../Images/ce110256af4c8d0e3e7b93dde3d91146.png)'
- en: Spark’s Application Workflow
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark的应用程序工作流程
- en: When you submit a job to Spark for processing, there is a lot that goes on behind
    the scenes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当你向Spark提交作业进行处理时，后台会发生很多事情。
- en: Our Standalone Application is kicked off, and initializes its SparkContext.
    Only after having a SparkContext can an app be referred to as a Driver
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的独立应用程序被启动，并初始化其SparkContext。只有在拥有SparkContext之后，应用程序才可以被称为驱动程序
- en: Our Driver program asks the Cluster Manager for resources to launch its executors
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的驱动程序程序向集群管理器请求资源以启动其执行器
- en: The Cluster Manager launches the executors
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集群管理器启动执行器
- en: Our Driver runs our actual Spark code
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的驱动程序运行实际的Spark代码
- en: Executors run tasks and send their results back to the driver
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器运行任务并将结果发送回驱动程序
- en: SparkContext is stopped and all executors are shut down, returning resources
    back to the cluster
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SparkContext被停止，所有执行器被关闭，资源返回到集群
- en: Install Spark on Mac (locally)
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Mac上安装Spark（本地）
- en: '***First Step: Install Brew***'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '***第一步：安装Brew***'
- en: 'You will need to install brew if you have it already skip this step:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经安装了brew，则可以跳过此步骤：
- en: 1\. open terminal on your mac. You can go to spotlight and type terminal to
    find it easily (alternative you can find it on /Applications/Utilities/).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 在Mac上打开终端。你可以去Spotlight搜索终端（或者你可以在/Applications/Utilities/中找到它）。
- en: 2\. Enter the command bellow.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 输入以下命令。
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 3\. Hit Return and the script will run. It will output to your terminal a log
    of what is going to install. Hit Return to continue or any other key to abort.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 按下回车键，脚本将运行。它会在终端输出将要安装的日志。按回车键继续或按其他键中止。
- en: 4\. It might ask for **sudo **privileges. If this happens you will have to type
    your admin password and hit Return again.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 它可能会要求**sudo**权限。如果发生这种情况，你需要输入你的管理员密码并再次按下回车键。
- en: '**Notes:** Command line tools (Apple''s XCode) will be installed after this
    guide.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 命令行工具（Apple的XCode）将在本指南之后安装。'
- en: The installation will look like as the image below.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 安装将如下图所示。
- en: '![figure-name](../Images/0561cfe8388405c80924de741a51e719.png)Installation
    of Homebrew through command line'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![figure-name](../Images/0561cfe8388405c80924de741a51e719.png)通过命令行安装Homebrew'
- en: When the installation finishes successfully it will look  as the image below.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当安装成功完成时，它将显示如下图像。
- en: '![figure-name](../Images/e72a72372711566e135617971f063ce3.png)caption'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![figure-name](../Images/e72a72372711566e135617971f063ce3.png)caption'
- en: By default Homebrew is sending anonymous data and analytics. You can find additional
    information [here](https://docs.brew.sh/Analytics). You can choose to opt-out
    by running the command.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Homebrew会发送匿名数据和分析。你可以在[这里](https://docs.brew.sh/Analytics)找到更多信息。你可以通过运行命令选择退出。
- en: '[PRE1]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***Second Step: Install Anaconda***'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '***第二步：安装Anaconda***'
- en: In the same terminal just simple type: `$ brew cask install anaconda`. Please
    see resources section in case you face any issue in that step.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一终端中简单地输入：`$ brew cask install anaconda`。如果在此步骤中遇到问题，请参见资源部分。
- en: '***Third final Step: Install PySpark***'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '***第三步：安装PySpark***'
- en: 1\. ona terminal type `$ brew install apache-spark`
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 在终端中输入`$ brew install apache-spark`
- en: 2\. if you see this error message, enter `$ brew cask install caskroom/versions/java8`to
    install Java8, you will not see this error if you have it already installed.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 如果你看到此错误消息，输入`$ brew cask install caskroom/versions/java8`来安装Java8，如果你已经安装了它，就不会看到此错误。
- en: '![figure-name](../Images/c41b2ed8a71ba60c5e55ad5b6b5f89fd.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![figure-name](../Images/c41b2ed8a71ba60c5e55ad5b6b5f89fd.png)'
- en: '3\. check if pyspark is properly install by typing on the terminal `$ pyspark`.
    If you see the below it means that it has been installed properly:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 通过在终端输入`$ pyspark`检查pyspark是否正确安装。如果你看到以下内容，说明它已正确安装：
- en: '![figure-name](../Images/48936f60f3da3b12e50bc0a205db863c.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![figure-name](../Images/48936f60f3da3b12e50bc0a205db863c.png)'
- en: Open Jupyter Notebook with PySpark Ready
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用PySpark准备好打开Jupyter Notebook
- en: This section assumes that PySpark has been installed properly and no error appear
    when typing on a terminal `$ pyspark`. At this step, I present the steps you have
    to follow in order create Jupyter Notebooks automatically initialised with SparkContext.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 本节假设PySpark已正确安装，并且在终端输入`$ pyspark`时没有出现错误。在此步骤中，我将介绍创建自动初始化SparkContext的Jupyter
    Notebook的步骤。
- en: In order to create a global profile for your terminal session, you will need
    to create or modify your .bash_profile or .bashrc file. Here, I will use .bash_profile
    as my example
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为你的终端会话创建一个全局配置文件，你需要创建或修改你的 .bash_profile 或 .bashrc 文件。在这里，我将使用 .bash_profile
    作为我的示例
- en: 1\. Check if you have .bash_profile in your system `$ ls -a`, if you don't have
    one, create one using `$ touch ~/.bash_profile`
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 检查你的系统中是否有 .bash_profile ` $ ls -a`，如果没有，使用 `$ touch ~/.bash_profile` 创建一个
- en: 2\. Find Spark path by running `$ brew info apache-spark`
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 通过运行 `$ brew info apache-spark` 查找 Spark 路径
- en: '![figure-name](../Images/0ed7b859ff7ba04f678b5c67416b5240.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![figure-name](../Images/0ed7b859ff7ba04f678b5c67416b5240.png)'
- en: '3\. If you already have a .bash_profile, open it by `$ vim ~/.bash_profile`,
    press `I` in order to insert, and paste the following codes in any location (DO
    NOT delete anything in your file):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 如果你已经有了 .bash_profile，请通过`$ vim ~/.bash_profile`打开它，按`I`以进入插入模式，然后在任何位置粘贴以下代码（**不要删除文件中的任何内容**）：
- en: '[PRE2]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 4\. Press `ESC` to exit insert mode, enter `:wq` to exit VIM. *You could fine
    more VIM commands here.*
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 按 `ESC` 退出插入模式，输入 `:wq` 退出 VIM。*你可以在这里找到更多 VIM 命令。*
- en: 5\. Refresh terminal profile by `$ source ~/.bash_profile`
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 通过 `$ source ~/.bash_profile` 刷新终端配置文件
- en: My favourite way to use PySpark in a Jupyter Notebook is by installing [findSpark](https://github.com/minrk/findspark)package
    which allow me to make a Spark Context available in my code.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 Jupyter Notebook 中使用 PySpark 的最喜欢的方法是通过安装 [findSpark](https://github.com/minrk/findspark)
    包，这样我可以在代码中使用 Spark Context。
- en: '*findSpark package is not specific to Jupyter Notebook, you can use this trick
    in your favorite IDE too.*'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*findSpark 包并不特定于 Jupyter Notebook，你也可以在你喜欢的 IDE 中使用这个技巧。*'
- en: Install findspark by running the following command on a terminal
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在终端中运行以下命令来安装 findspark
- en: '[PRE3]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Launch a regular Jupyter Notebook and run the following command:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 启动一个普通的 Jupyter Notebook 并运行以下命令：
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output should be:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应为：
- en: '![figure-name](../Images/272c85c537c5ad986fc8099a97520f23.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![figure-name](../Images/272c85c537c5ad986fc8099a97520f23.png)'
- en: 'Please note that with Spark 2.2 a lot of people recommend just to simply do `pip
    install pyspark` .I try using `pip` to install `pyspark` but I couldn’t get the `pyspark`cluster
    to get started properly. Reading several answers on Stack Overflow and the [official
    documentation](https://pypi.org/project/pyspark/2.2.0/), I came across this:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用 Spark 2.2 时，很多人推荐简单地运行 `pip install pyspark`。我尝试使用 `pip` 安装 `pyspark`
    但无法让 `pyspark` 集群正常启动。阅读了 Stack Overflow 上的几个答案以及 [官方文档](https://pypi.org/project/pyspark/2.2.0/)，我发现了这一点：
- en: '*The Python packaging for Spark is not intended to replace all of the other
    use cases. This Python packaged version of Spark is suitable for interacting with
    an existing cluster (be it Spark standalone, YARN, or Mesos) - but does not contain
    the tools required to setup your own standalone Spark cluster. You can download
    the full version of Spark from the [Apache Spark downloads page](http://spark.apache.org/downloads.html).*'
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Spark 的 Python 打包版本并不是为了取代所有其他用途。这个 Python 打包的 Spark 版本适合与现有集群（无论是 Spark 独立集群、YARN
    还是 Mesos）进行交互——但不包含设置自己独立 Spark 集群所需的工具。你可以从 [Apache Spark 下载页面](http://spark.apache.org/downloads.html)
    下载 Spark 的完整版本。*'
- en: Therefore, I would suggest to follow the steps that I described above.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我建议按照我上面描述的步骤进行操作。
- en: Launching a SparkSession
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启动一个 SparkSession
- en: 'Well, it’s the main entry point for Spark functionality: it represents the
    connection to a Spark cluster and you can use it to create RDDs and to broadcast
    variables on that cluster. When you’re working with Spark, everything starts and
    ends with this SparkSession. **Note** that SparkSession is a new feature of Spark
    2.0 which minimize the number of concepts to remember or construct. (before Spark
    2.0.0, the three main connection objects were SparkContext, SqlContext and HiveContext).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这是 Spark 功能的主要入口点：它代表了与 Spark 集群的连接，你可以用它来创建 RDDs 和广播变量。当你使用 Spark 时，一切从这个
    SparkSession 开始并以它结束。**注意**：SparkSession 是 Spark 2.0 的新特性，它减少了需要记住或构建的概念数量。（在
    Spark 2.0.0 之前，主要的连接对象是 SparkContext、SqlContext 和 HiveContext）。
- en: In interactive environments, a SparkSession will already be created for you
    in a variable named spark. For consistency, you should use this name when you
    create one in your own application.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在交互环境中，SparkSession 已经为你创建了一个名为 spark 的变量。为了保持一致性，当你在自己的应用程序中创建一个时，应该使用这个名称。
- en: 'You can create a new SparkSession through a Builder pattern which uses a "fluent
    interface" style of coding to build a new object by chaining methods together.
    Spark properties can be passed in, as shown in these examples:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过构建器模式创建一个新的SparkSession，该模式使用“流畅接口”风格的编码，通过链式调用方法来构建一个新的对象。可以传递Spark属性，如下示例所示：
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'At the end of your application, please remember to call `spark.stop()`  in
    order to end the SparkSession. Let''s understand the various settings that we
    define above:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序结束时，请记得调用`spark.stop()`以结束SparkSession。让我们了解一下上面定义的各种设置：
- en: '`**master**`**: **Sets the Spark master URL to connect to, such as “local”
    to run locally, “local[4]” to run locally with 4 cores, or “spark://master:7077”
    to run on a Spark standalone cluster.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**master**`**:** 设置要连接的Spark主节点URL，例如“local”以本地运行，“local[4]”以4个核心本地运行，或“spark://master:7077”以在Spark独立集群上运行。'
- en: '`**config**`**:**Sets a config option by specifying a (key, value) pair.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**config**`**:** 设置配置选项，通过指定（键，值）对来完成。'
- en: '`**appName**`**: **Sets a name for the application, if no name is set, a randomly
    generated name will be used.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**appName**`**:** 设置应用程序的名称，如果未设置名称，将使用随机生成的名称。'
- en: '`**getOrCreate**`**:**Gets an existing [`**SparkSession**`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?#pyspark.sql.SparkSession) or,
    if there is no existing one, creates a new one based on the options set in this
    builder. In case an existing SparkSession is returned, the config options specified
    in this builder affecting the `SQLContext` configuration will applied. As `SparkContext` configuration
    cannot be modified on runtime (you have to stop existing context first) while`SQLContext` configuration
    can be modified on runtime.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**getOrCreate**`**:** 获取一个现有的[`**SparkSession**`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?#pyspark.sql.SparkSession)，如果没有现有的，则根据在此构建器中设置的选项创建一个新的。如果返回现有的SparkSession，将应用此构建器中指定的影响`SQLContext`配置的配置选项。由于`SparkContext`的配置不能在运行时修改（必须首先停止现有上下文），而`SQLContext`配置可以在运行时修改。'
- en: Conclusion
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: Spark has seen immense growth over the past several years. Hundreds of contributors
    working collectively have made Spark an amazing piece of technology powering the
    de facto standard for big data processing and data sciences across all industries.
    But please remember to use it for manipulations of **huge dataset **when facing
    performance issues otherwise it may have opposite effects. For small datasets
    (few gigabytes) it is advisable instead to use [Pandas](https://pandas.pydata.org/).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Spark在过去几年中经历了巨大的增长。数百名贡献者的共同努力使得Spark成为一项出色的技术，成为所有行业大数据处理和数据科学的事实标准。但请记住，在面对性能问题时，**巨大的数据集**应使用它进行操作，否则可能会产生相反的效果。对于小数据集（几GB），建议改用[Pandas](https://pandas.pydata.org/)。
- en: Thanks for reading and I am looking forward to hearing your questions :)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读，期待听到您的问题 :)
- en: '*Stay tuned and Happy Machine Learning.*'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*敬请关注，祝机器学习愉快。*'
- en: References
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[https://medium.com/ayuth/install-anaconda-on-macos-with-homebrew-c94437d63a37](https://medium.com/ayuth/install-anaconda-on-macos-with-homebrew-c94437d63a37)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://medium.com/ayuth/install-anaconda-on-macos-with-homebrew-c94437d63a37](https://medium.com/ayuth/install-anaconda-on-macos-with-homebrew-c94437d63a37)'
- en: '[https://picocoder.io/how-to-install-homebrew-on-mac/](https://picocoder.io/how-to-install-homebrew-on-mac/)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://picocoder.io/how-to-install-homebrew-on-mac/](https://picocoder.io/how-to-install-homebrew-on-mac/)'
- en: '[https://stackoverflow.com/](https://stackoverflow.com/questions/46387574/installing-pyspark-on-macbook)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://stackoverflow.com/](https://stackoverflow.com/questions/46387574/installing-pyspark-on-macbook)'
- en: '[https://www.oreilly.com/library/view/learning-spark/9781449359034/](https://www.oreilly.com/library/view/learning-spark/9781449359034/)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.oreilly.com/library/view/learning-spark/9781449359034/](https://www.oreilly.com/library/view/learning-spark/9781449359034/)'
- en: '**Bio: [Georgios Drakos](https://gdcoder.com/author/)** is a Data Scientist
    with a BSc and MSc in Electrical and Computer Engineering (National Technical
    University of Athens) as well as a MSc from Imperial College London, currently
    working as a Data Scientist for TUI in the travel industry. He specialised in
    Computational Intelligence for Decision Support, Data Engineering, Sophisticated
    Analytics and Technological Innovation Management. Highly interested in Cloud-based
    technologies, Distributed Computing, Big Data and the business applications of
    Data Science & Machine Learning.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[Georgios Drakos](https://gdcoder.com/author/)** 是一名数据科学家，拥有国家技术大学电气与计算机工程的学士和硕士学位，以及帝国理工学院的硕士学位，目前在TUI的旅行行业担任数据科学家。他专注于决策支持的计算智能、数据工程、复杂分析和技术创新管理。对基于云的技术、分布式计算、大数据以及数据科学和机器学习的商业应用非常感兴趣。'
- en: '[Original](https://gdcoder.com/learn-how-to-use-pyspark-in-under-5-minutes/).
    Reposted with permission.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[原始文章](https://gdcoder.com/learn-how-to-use-pyspark-in-under-5-minutes/)。经许可转载。'
- en: '**Related:**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Spark NLP: Getting Started With The World’s Most Widely Used NLP Library In
    The Enterprise](/2019/06/spark-nlp-getting-started-with-worlds-most-widely-used-nlp-library-enterprise.html)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Spark NLP：企业中最广泛使用的NLP库入门](/2019/06/spark-nlp-getting-started-with-worlds-most-widely-used-nlp-library-enterprise.html)'
- en: '[Scalable Python Code with Pandas UDFs: A Data Science Application](/2019/06/scalable-python-code-pandas-udfs.html)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Pandas UDFs的可扩展Python代码：数据科学应用](/2019/06/scalable-python-code-pandas-udfs.html)'
- en: '[Analyzing Tweets with NLP in Minutes with Spark, Optimus and Twint](/2019/05/analyzing-tweets-nlp-spark-optimus-twint.html)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用Spark、Optimus和Twint在几分钟内分析推文](/2019/05/analyzing-tweets-nlp-spark-optimus-twint.html)'
- en: More On This Topic
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[FastAPI Tutorial: Build APIs with Python in Minutes](https://www.kdnuggets.com/fastapi-tutorial-build-apis-with-python-in-minutes)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[FastAPI教程：用Python在几分钟内构建APIs](https://www.kdnuggets.com/fastapi-tutorial-build-apis-with-python-in-minutes)'
- en: '[PySpark for Data Science](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PySpark用于数据科学](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
- en: '[Data Validation for PySpark Applications using Pandera](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Pandera进行PySpark应用的数据验证](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
- en: '[Understanding Bias-Variance Trade-Off in 3 Minutes](https://www.kdnuggets.com/2020/09/understanding-bias-variance-trade-off-3-minutes.html)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3分钟理解偏差-方差权衡](https://www.kdnuggets.com/2020/09/understanding-bias-variance-trade-off-3-minutes.html)'
- en: '[Build a Machine Learning Web App in 5 Minutes](https://www.kdnuggets.com/2022/03/build-machine-learning-web-app-5-minutes.html)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5分钟构建机器学习网页应用](https://www.kdnuggets.com/2022/03/build-machine-learning-web-app-5-minutes.html)'
- en: '[Build a Web Scraper with Python in 5 Minutes](https://www.kdnuggets.com/2022/02/build-web-scraper-python-5-minutes.html)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5分钟用Python构建网页抓取器](https://www.kdnuggets.com/2022/02/build-web-scraper-python-5-minutes.html)'
