- en: Large Language Models Explained in 3 Levels of Difficulty
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型在三个难度级别中的解释
- en: 原文：[https://www.kdnuggets.com/large-language-models-explained-in-3-levels-of-difficulty](https://www.kdnuggets.com/large-language-models-explained-in-3-levels-of-difficulty)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/large-language-models-explained-in-3-levels-of-difficulty](https://www.kdnuggets.com/large-language-models-explained-in-3-levels-of-difficulty)
- en: We live in an era where the machine learning model is at its peak. Compared
    to decades ago, most people would never have heard about ChatGPT or Artificial
    Intelligence. However, those are the topics that people keep talking about. Why?
    Because the values given are so significant compared to the effort.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生活在一个机器学习模型处于巅峰的时代。与几十年前相比，大多数人可能从未听说过ChatGPT或人工智能。然而，这些正是人们不断讨论的话题。为什么？因为它们所带来的价值与投入的努力相比是如此显著。
- en: The breakthrough of AI in recent years could be attributed to many things, but
    one of them is the large language model (LLM). Many text generation AI people
    use are powered by the LLM model; For example, ChatGPT uses their GPT model. As
    LLM is an important topic, we should learn about it.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，人工智能的突破可以归因于许多因素，但其中之一就是大型语言模型（LLM）。许多文本生成AI都依赖于LLM模型；例如，ChatGPT使用的是他们的GPT模型。由于LLM是一个重要的话题，我们应该了解它。
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织IT'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: This article will discuss Large Language Models in 3 difficulty levels, but
    we will only touch on some aspects of LLMs. We would only differ in a way that
    allows every reader to understand what LLM is.  With that in mind, let’s get into
    it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将讨论大型语言模型（LLM）的三个难度级别，但我们只会触及LLM的一些方面。我们将以一种使每位读者都能理解LLM是什么的方式进行讨论。考虑到这一点，我们开始吧。
- en: 'Level 1: LLM Beginner Level'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1级：LLM初学者级别
- en: In the first level, we assume the reader doesn’t know about LLM and may know
    a little about the field of data science/machine learning. So, I would briefly
    introduce AI and Machine Learning before moving to the LLMs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个级别，我们假设读者对LLM不了解，并可能对数据科学/机器学习领域有所了解。因此，我将简要介绍人工智能和机器学习，然后再转到LLM。
- en: '[Artificial Intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence)
    is the science of developing intelligent computer programs. It’s intended for
    the program to perform intelligent tasks that humans could do but does not have
    limitations on human biological needs. [Machine learning](https://en.wikipedia.org/wiki/Machine_learning)
    is a field in artificial intelligence focusing on data generalization studies
    with statistical algorithms. In a way, Machine Learning is trying to achieve Artificial
    Intelligence via data study so that the program can perform intelligence tasks
    without instruction.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[人工智能](https://en.wikipedia.org/wiki/Artificial_intelligence)是开发智能计算机程序的科学。它旨在使程序能够执行人类可以做的智能任务，但不受人类生物需求的限制。[机器学习](https://en.wikipedia.org/wiki/Machine_learning)是人工智能的一个领域，专注于利用统计算法进行数据泛化研究。从某种意义上说，机器学习通过数据研究来实现人工智能，使程序能够在没有指令的情况下执行智能任务。'
- en: Historically, the field that intersects between computer science and linguistics
    is called the Natural [Language Processing](https://en.wikipedia.org/wiki/Natural_language_processing)
    field. The field mainly concerns any activity of machine processing to the human
    text, such as text documents. Previously, this field was only limited to the rule-based
    system but it became more with the introduction of advanced semi-supervised and
    unsupervised algorithms that allow the model to learn without any direction. One
    of the advanced models to do this is the Language Model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，计算机科学和语言学交叉的领域称为自然[语言处理](https://en.wikipedia.org/wiki/Natural_language_processing)领域。该领域主要涉及机器对人类文本（如文本文件）的处理活动。之前，该领域仅限于基于规则的系统，但随着先进的半监督和无监督算法的引入，该领域得到了扩展，使模型能够在没有任何指导的情况下学习。其中一个先进模型是语言模型。
- en: The language [model](https://en.wikipedia.org/wiki/Language_model) is a probabilistic
    NLP model to perform many human tasks such as translation, grammar correction,
    and text generation. The old form of the language model uses purely statistical
    approaches such as the n-gram method, where the assumption is that the probability
    of the next word depends only on the previous word's fixed-size data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 语言[模型](https://en.wikipedia.org/wiki/Language_model)是一个概率性NLP模型，用于执行许多类似翻译、语法纠正和文本生成的人类任务。旧形式的语言模型使用纯统计方法，如n-gram方法，其中假设下一个词的概率仅依赖于前一个词的固定大小数据。
- en: However, the introduction of [Neural Network](https://en.wikipedia.org/wiki/Artificial_neural_network)
    has dethroned the previous approach. An artificial neural network, or NN, is a
    computer program mimicking the human brain's neuron structure. The Neural Network
    approach is good to use because it can handle complex pattern recognition from
    the text data and handle sequential data like text. That’s why the current Language
    Model is usually based on NN.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，[神经网络](https://en.wikipedia.org/wiki/Artificial_neural_network)的引入已经取代了以前的方法。人工神经网络，或称NN，是一种模拟人脑神经结构的计算机程序。神经网络方法之所以好用，是因为它可以处理复杂的模式识别和文本数据的序列数据。这就是为什么当前的语言模型通常基于NN。
- en: '[Large Language Models](https://en.wikipedia.org/wiki/Large_language_model),
    or LLMs, are machine learning models that learn from a huge number of data documents
    to perform general-purpose language generation. They are still a language model,
    but the huge number of parameters learned by the NN makes them considered large.
    In layperson''s terms, the model could perform how humans write by predicting
    the next words from the given input words very well.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[大型语言模型](https://en.wikipedia.org/wiki/Large_language_model)，或LLMs，是通过从大量数据文档中学习来执行通用语言生成的机器学习模型。它们仍然是语言模型，但NN学到的大量参数使它们被认为是大型的。通俗来说，该模型通过从给定的输入词预测下一个词来非常好地模拟人类的写作方式。'
- en: Examples of LLM tasks include language translation, machine chatbot, question
    answering, and many more. From any sequence of data input, the model could identify
    relationships between the words and generate output suitable from the instruction.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: LLM任务的例子包括语言翻译、机器聊天机器人、问答等。从任何数据输入序列中，模型可以识别单词之间的关系，并生成符合指令的输出。
- en: Almost all of the Generative AI products that boast something using text generation
    are powered by the LLMs. Big products like ChatGPT, Google’s Bard, and many more
    are using LLMs as the basis of their product.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有声称使用文本生成的生成式AI产品都由LLMs驱动。像ChatGPT、Google的Bard等大型产品都以LLMs为基础。
- en: 'Level 2: LLM Intermediate Level'
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二级：LLM中级水平
- en: The reader has data science knowledge but needs to learn more about the LLM
    at this level. At the very least, the reader can understand the terms used in
    the data field. At this level, we would dive deeper into the base architecture.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 读者具有数据科学知识，但需要在此级别上进一步了解LLM。至少，读者能够理解数据领域中使用的术语。在这个层面上，我们将更深入地探讨基础架构。
- en: As explained previously, LLM is a Neural Network model trained on massive amounts
    of text data. To understand this concept further, it would be beneficial to understand
    how neural networks and deep learning work.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，LLM是一个在大量文本数据上训练的神经网络模型。要进一步理解这一概念，了解神经网络和深度学习的工作原理将是有益的。
- en: In the previous level, we explained that a neural neuron is a model miming the
    human brain's neural structure. The main element of the Neural Network is the
    neurons, often called nodes. To explain the concept better, see the typical Neural
    Network architecture in the image below.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一层中，我们解释了神经元是模仿人脑神经结构的模型。神经网络的主要元素是神经元，通常称为节点。为了更好地解释这一概念，请参见下面图像中的典型神经网络架构。
- en: '![Large Language Models Explained in 3 Levels of Difficulty](../Images/328cdc6c3c343a7abceb215a3bf4026a.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![大语言模型分三层难度解释](../Images/328cdc6c3c343a7abceb215a3bf4026a.png)'
- en: 'Neural Network Architecture(Image source: [KDnuggets](/2016/10/deep-learning-key-terms-explained.html))'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络架构（图片来源：[KDnuggets](/2016/10/deep-learning-key-terms-explained.html)）
- en: 'As we can see in the image above, the Neural Network consists of three layers:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，神经网络由三层组成：
- en: Input layer where it receives the information and transfers it to the other
    nodes in the next layer.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入层接收信息并将其传输到下一层的其他节点。
- en: Hidden node layers where all the computations take place.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 隐藏节点层是所有计算发生的地方。
- en: Output node layer where the computational outputs are.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出节点层是计算结果所在的地方。
- en: It's called deep learning when we train our Neural Network model with two or
    more hidden layers. It’s called deep because it uses many layers in between. The
    advantage of deep learning models is that they automatically learn and extract
    features from the data that traditional machine learning models are incapable
    of.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们用两个或更多隐藏层训练神经网络模型时，这称为深度学习。之所以叫深度，是因为它使用了许多层。深度学习模型的优势在于它们能够自动学习和提取数据中的特征，而传统的机器学习模型则无法做到这一点。
- en: In the Large Language Model, deep learning is important as the model is built
    upon deep neural network architectures. So, why is it called LLM? It’s because
    billions of layers are trained upon massive amounts of text data. The layers would
    produce model parameters that help the model learn complex patterns in language,
    including grammar, writing style, and many more.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在大语言模型中，深度学习非常重要，因为该模型建立在深度神经网络架构之上。那么，为什么它叫 LLM 呢？因为数十亿层在大量文本数据上进行训练。这些层会产生模型参数，帮助模型学习语言中的复杂模式，包括语法、写作风格等等。
- en: The simplified process of the model training is shown in the image below.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练的简化过程如下面的图像所示。
- en: '![Large Language Models Explained in 3 Levels of Difficulty](../Images/8fe85eacdc181b9a3944ee37e2a1a28f.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![大语言模型分三层难度解释](../Images/8fe85eacdc181b9a3944ee37e2a1a28f.png)'
- en: 'Image by Kumar Chandrakant (Source: [Baeldung.com](https://www.baeldung.com/cs/large-language-models))'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：Kumar Chandrakant（来源：[Baeldung.com](https://www.baeldung.com/cs/large-language-models)）
- en: The process showed that the models could generate relevant text based on the
    likelihood of each word or sentence of the input data. In the LLMs, the advanced
    approach uses [self-supervised learning](https://en.wikipedia.org/wiki/Self-supervised_learning)
    and [semi-supervised learning](https://en.wikipedia.org/wiki/Weak_supervision)
    to achieve the general-purpose capability.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 过程显示模型可以根据输入数据中每个单词或句子的可能性生成相关文本。在 LLM 中，先进的方法使用 [自监督学习](https://en.wikipedia.org/wiki/Self-supervised_learning)
    和 [半监督学习](https://en.wikipedia.org/wiki/Weak_supervision) 来实现通用能力。
- en: Self-supervised learning is a technique where we don’t have labels, and instead,
    the training data provides the training feedback itself. It’s used in the LLM
    training process as the data usually lacks labels. In LLM, one could use the surrounding
    context as a clue to predict the next words. In contrast, Semi-supervised learning
    combines the supervised and unsupervised learning concepts with a small amount
    of labeled data to generate new labels for a large amount of unlabeled data. Semi-supervised
    learning is usually used for LLMs with specific context or domain needs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习是一种技术，其中我们没有标签，而是训练数据提供了训练反馈。它在 LLM 训练过程中被使用，因为数据通常缺乏标签。在 LLM 中，可以利用周围的上下文作为线索来预测下一个单词。相比之下，半监督学习将监督学习和无监督学习概念结合起来，使用少量标记数据为大量未标记数据生成新标签。半监督学习通常用于具有特定上下文或领域需求的
    LLM。
- en: 'Level 3: LLM Advanced Level'
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 3 层：LLM 高级
- en: In the third level, we would discuss the LLM more deeply, especially tackling
    the LLM structure and how it could achieve human-like generation capability.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三层，我们将更深入地讨论 LLM，特别是探讨 LLM 结构以及它如何实现类似人类的生成能力。
- en: We have discussed that LLM is based on the Neural Network model with Deep Learning
    techniques. The LLM has typically been built based on [transformer-based](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    architecture in recent years. The transformer is based on the multi-head attention
    mechanism introduced by [Vaswani *et al.* (2017)](https://arxiv.org/pdf/1706.03762.pdf)
    and has been used in many LLMs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了 LLM 基于深度学习技术的神经网络模型。近年来，LLM 通常基于 [变压器](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    架构构建。变压器基于 [Vaswani *et al.* (2017)](https://arxiv.org/pdf/1706.03762.pdf) 引入的多头注意力机制，并已被许多
    LLM 使用。
- en: Transformers is a model architecture that tries to solve the sequential tasks
    previously encountered in the RNNs and LSTMs. The old way of the Language Model
    was to use RNN and LSTM to process data sequentially, where the model would use
    every word output and loop them back so the model would not forget. However, they
    have problems with long-sequence data once transformers are introduced.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器是一种模型架构，旨在解决 RNN 和 LSTM 之前遇到的序列任务。语言模型的旧方法是使用 RNN 和 LSTM 顺序处理数据，模型会使用每个词输出并将其回环，以免遗忘。然而，一旦引入变压器，这些方法在长序列数据上会出现问题。
- en: Before we go deeper into the Transformers, I want to introduce the concept of
    encoder-decoder that was previously used in RNNs. The encoder-decoder structure
    allows the input and output text to not be of the same length. The example use
    case is a language translation, which often has a different sequence size.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解变压器之前，我想介绍一下以前在 RNN 中使用的编码器-解码器概念。编码器-解码器结构允许输入和输出文本长度不同。一个示例用例是语言翻译，它通常具有不同的序列大小。
- en: The structure can be divided into two. The first part is called Encoder, which
    is a part that receives data sequence and creates a new representation based on
    it. The representation would be used in the second part of the model, which is
    the decoder.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 该结构可以分为两部分。第一部分称为编码器，它接收数据序列并基于它创建新的表示。该表示将在模型的第二部分，即解码器中使用。
- en: '![Large Language Models Explained in 3 Levels of Difficulty](../Images/95917420d2cc5722fe59d5cbeed99b27.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![大型语言模型的三种难度解释](../Images/95917420d2cc5722fe59d5cbeed99b27.png)'
- en: Image by Author
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: The problem with RNN is that the model might need help remembering longer sequences,
    even with the encoder-decoder structure above. This is where the attention mechanism
    could help solve the problem, a layer that could solve long input problems. The
    attention mechanism is introduced in the paper by [Bahdanau *et al*. (2014)](https://arxiv.org/abs/1409.0473)
    to solve the encoder-decoder type RNNs by focusing on an important part of the
    model input while having the output prediction.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的问题在于，即使在上述编码器-解码器结构下，模型可能仍难以记住较长的序列。这时，注意力机制可以帮助解决这个问题，它是一种能够解决长输入问题的层。注意力机制在
    [Bahdanau *et al*. (2014)](https://arxiv.org/abs/1409.0473) 的论文中被引入，用于通过关注模型输入的重要部分来解决编码器-解码器类型的
    RNN 问题，同时进行输出预测。
- en: The transformer's structure is inspired by the encoder-decoder type and built
    with the attention mechanism techniques, so it does not need to process data in
    sequential order. The overall transformers model is structured like the image
    below.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器的结构受编码器-解码器类型的启发，并采用注意力机制技术构建，因此无需按顺序处理数据。整体变压器模型的结构如下面的图像所示。
- en: '![Large Language Models Explained in 3 Levels of Difficulty](../Images/78049f2f056c25c9dabf059929e0109e.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![大型语言模型的三种难度解释](../Images/78049f2f056c25c9dabf059929e0109e.png)'
- en: Transformers Architecture ([Vaswani *et al.* (2017)](https://arxiv.org/pdf/1706.03762.pdf))
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器架构（[Vaswani *et al.* (2017)](https://arxiv.org/pdf/1706.03762.pdf)）
- en: In the structure above, the transformers encode the data vector sequence into
    the word embedding while using the decoding to transform data into the original
    form. The encoding can assign a certain importance to the input with the attention
    mechanism.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述结构中，变压器将数据向量序列编码为词嵌入，同时使用解码将数据转换为原始形式。编码可以利用注意力机制为输入分配一定的重要性。
- en: We have talked a bit about transformers encoding the data vector, but what is
    a data vector? Let’s discuss it. In the machine learning model, we can’t input
    the raw natural language data into the model, so we need to transform them into
    numerical forms. The transformation process is called word embedding, where each
    input word is processed through the word embedding model to get the data vector.
    We can use many initial word embeddings, such as [Word2vec](https://en.wikipedia.org/wiki/Word2vec)
    or [GloVe](https://nlp.stanford.edu/projects/glove/), but many advanced users
    try to refine them using their vocabulary. In a basic form, the word embedding
    process can be shown in the image below.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍微谈了一下变换器如何编码数据向量，但什么是数据向量呢？让我们来讨论一下。在机器学习模型中，我们不能将原始自然语言数据输入模型，因此我们需要将其转化为数字形式。这一转换过程称为词嵌入，其中每个输入单词都通过词嵌入模型处理以获取数据向量。我们可以使用许多初始词嵌入，如
    [Word2vec](https://en.wikipedia.org/wiki/Word2vec) 或 [GloVe](https://nlp.stanford.edu/projects/glove/)，但许多高级用户尝试使用他们自己的词汇来精细调整它们。在基本形式下，词嵌入过程可以在下图中展示。
- en: '![Large Language Models Explained in 3 Levels of Difficulty](../Images/d1502abf72097ce3764fb31f8ea9a66e.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![大型语言模型以3个难度级别解释](../Images/d1502abf72097ce3764fb31f8ea9a66e.png)'
- en: Image by Author
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: The transformers could accept the input and provide more relevant context by
    presenting the words in numerical forms like the data vector above. In the LLMs,
    word embeddings are usually context-dependent, generally refined upon the use
    cases and the intended output.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器可以接受输入，并通过以数字形式呈现单词来提供更相关的上下文，如上面的数据向量。在大语言模型（LLMs）中，词嵌入通常是依赖上下文的，通常会根据使用案例和预期输出进行精细调整。
- en: Conclusion
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We discussed the Large Language Model in three difficulty levels, from beginner
    to advanced. From the general usage of LLM to how it is structured, you can find
    an explanation that explains the concept in more detail.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以三个难度级别讨论了大型语言模型，从初学者到高级用户。从LLM的一般使用到其结构，你可以找到更详细解释这个概念的说明。
- en: '**[](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**[Cornellius Yudha
    Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**** is a data science
    assistant manager and data writer. While working full-time at Allianz Indonesia,
    he loves to share Python and data tips via social media and writing media. Cornellius
    writes on a variety of AI and machine learning topics.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**[](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**[Cornellius Yudha
    Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**** 是一名数据科学助理经理和数据撰稿人。在全职工作于Allianz
    Indonesia的同时，他喜欢通过社交媒体和写作媒体分享Python和数据技巧。Cornellius撰写了各种人工智能和机器学习主题的文章。'
- en: More On This Topic
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[The Difficulty of Estimating the Carbon Footprint of Machine Learning](https://www.kdnuggets.com/2022/07/difficulty-estimating-carbon-footprint-machine-learning.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[估算机器学习碳足迹的难度](https://www.kdnuggets.com/2022/07/difficulty-estimating-carbon-footprint-machine-learning.html)'
- en: '[Top Open Source Large Language Models](https://www.kdnuggets.com/2022/09/john-snow-top-open-source-large-language-models.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[顶级开源大型语言模型](https://www.kdnuggets.com/2022/09/john-snow-top-open-source-large-language-models.html)'
- en: '[Learn About Large Language Models](https://www.kdnuggets.com/2023/03/learn-large-language-models.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[了解大型语言模型](https://www.kdnuggets.com/2023/03/learn-large-language-models.html)'
- en: '[Introducing Healthcare-Specific Large Language Models from John Snow Labs](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍来自John Snow Labs的医疗保健特定大型语言模型](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
- en: '[What are Large Language Models and How Do They Work?](https://www.kdnuggets.com/2023/05/large-language-models-work.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大型语言模型是什么？它们是如何工作的？](https://www.kdnuggets.com/2023/05/large-language-models-work.html)'
- en: '[AI: Large Language & Visual Models](https://www.kdnuggets.com/2023/06/ai-large-language-visual-models.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能：大型语言与视觉模型](https://www.kdnuggets.com/2023/06/ai-large-language-visual-models.html)'
