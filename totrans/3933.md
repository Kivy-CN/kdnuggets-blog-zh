# Cookiecutter 数据科学：如何组织你的数据科学项目

> 原文：[https://www.kdnuggets.com/2018/07/cookiecutter-data-science-organize-data-project.html](https://www.kdnuggets.com/2018/07/cookiecutter-data-science-organize-data-project.html)

![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)

**由 [DrivenData](https://www.drivendata.org/) 提供**

![Image](../Images/cd61bf44f3394ed05156d00b1e97ee5e.png)

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的 IT

* * *

### 为什么使用这个项目结构？

> 我们不是在讨论缩进美学或挑剔的格式标准——归根结底，数据科学代码质量关乎正确性和可重复性。

当我们想到数据分析时，我们常常只想到结果报告、洞察或可视化。虽然这些最终产品通常是主要事件，但很容易专注于让产品*看起来漂亮*，而忽视生成它们的*代码质量*。因为这些最终产品是通过编程生成的，**代码质量仍然很重要**！我们不是在讨论缩进美学或挑剔的格式标准——归根结底，数据科学代码质量关乎正确性和可重复性。

好分析结果往往是非常零散和偶然探索的结果并非秘密。尝试性的实验和快速测试可能不起作用的方法都是获得好结果的一部分，数据探索没有简单的线性进展的灵丹妙药。

也就是说，一旦开始，这个过程不会让你仔细考虑代码或项目布局的结构，所以最好从一个干净、逻辑清晰的结构开始，并在整个过程中坚持使用。我们认为使用这种相对标准化的设置是一个很大的胜利。原因如下：

### 其他人会感谢你

> 没有人在创建新的 Rails 项目之前去琢磨他们想把视图放在哪里；他们只是运行`rails new`来获得一个标准的项目框架，就像其他人一样。

一个明确定义的标准项目结构意味着新手可以在不深入大量文档的情况下开始理解分析。它还意味着他们不必在了解特定内容之前阅读 100% 的代码。

组织良好的代码往往具有自文档化的特性，即组织本身为你的代码提供了上下文，而不需要太多额外的说明。人们会感谢你，因为他们可以：

+   更轻松地与你合作完成这个分析

+   从你的分析中学习关于过程和领域的知识

+   对分析得出的结论感到自信

这方面的一个很好的例子可以在 Django 或 Ruby on Rails 等主要的 web 开发框架中找到。没有人会在创建新的 Rails 项目之前去搞清楚他们想把视图放在哪里；他们只需运行 `rails new` 以获得一个标准的项目骨架，就像其他人一样。因为这个默认的项目结构是 *逻辑性的* 和 *在大多数项目中相对标准的*，所以对于从未见过特定项目的人来说，更容易弄清楚他们会在何处找到各种活动的部分。

另一个很好的例子是针对类Unix系统的 [文件系统层级标准](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard)。`/etc` 目录和 `/tmp` 文件夹都有非常特定的目的，而且每个人（或多或少）都同意遵守这个社会契约。这意味着，无论是 Red Hat 用户还是 Ubuntu 用户，都大致知道在寻找某些类型的文件时应该去哪里，即使在使用对方的系统，或者任何其他符合标准的系统时也是如此！

理想情况下，当同事打开你的数据科学项目时，应该是这样的。

### 你会感谢自己

你是否尝试过重现几个月前甚至几年前做过的分析？你可能已经写了代码，但现在很难弄清楚你应该使用 `make_figures.py.old`、`make_figures_working.py` 还是 `new_make_figures01.py` 来完成任务。以下是我们在带着存在主义恐惧感时学会提出的一些问题：

+   我们是否需要在开始之前将 X 列与数据连接起来，还是这些数据来自其中一个笔记本？

+   想到这一点，我们在运行绘图代码之前需要先运行哪个笔记本：是“处理数据”还是“清理数据”？

+   地理图的 shapefiles 是从哪里下载的？

+   *等等，直到无限。*

这类问题很痛苦，是项目无组织的症状。一个好的项目结构鼓励采用一些实践，使得回到旧的工作变得更容易，比如关注点分离、将分析抽象成一个 [DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph) 和像版本控制这样的工程最佳实践。

### 这里没有任何约束

> “愚蠢的一致性是小心眼的怪物” — 拉尔夫·瓦尔多·爱默生（以及 [PEP 8!](https://www.python.org/dev/peps/pep-0008/#a-foolish-consistency-is-the-hobgoblin-of-little-minds)）

对一些默认的文件夹名称有异议？正在处理一个有点非标准的项目，与当前结构不完全契合？更喜欢使用不同于（少数）默认的包？

**行动起来吧！** 这是一个轻量级结构，旨在成为许多项目的良好 *起点*。或者，正如 PEP 8 所说：

> 在一个项目中，一致性更为重要。在一个模块或函数中的一致性是最重要的。…然而，要知道什么时候可以不一致——有时候，风格指南的建议并不适用。当有疑问时，使用你最好的判断。查看其他示例，决定什么看起来最好。不要犹豫去询问！

### 入门

考虑到这一点，我们为 Python 项目创建了一个数据科学 cookiecutter 模板。你的分析不必使用 Python，但模板确实提供了一些你可能希望移除的 Python 样板代码（例如，在 `src` 文件夹中，以及 `docs` 中的 Sphinx 文档骨架）。

### 需求

+   Python 2.7 或 3.5

+   [cookiecutter Python 包](http://cookiecutter.readthedocs.org/en/latest/installation.html) >= 1.4.0: `pip install cookiecutter`

### 启动新项目

启动新项目就像在命令行运行这个命令一样简单。无需首先创建目录，cookiecutter 会为你完成这一步。

```py
cookiecutter https://github.com/drivendata/cookiecutter-data-science

```

### 示例

### 目录结构

```py
├── LICENSE
├── Makefile           <- Makefile with commands like `make data` or `make train`
├── README.md          <- The top-level README for developers using this project.
├── data
│   ├── external       <- Data from third party sources.
│   ├── interim        <- Intermediate data that has been transformed.
│   ├── processed      <- The final, canonical data sets for modeling.
│   └── raw            <- The original, immutable data dump.
│
├── docs               <- A default Sphinx project; see sphinx-doc.org for details
│
├── models             <- Trained and serialized models, model predictions, or model summaries
│
├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
│                         the creator's initials, and a short `-` delimited description, e.g.
│                         `1.0-jqp-initial-data-exploration`.
│
├── references         <- Data dictionaries, manuals, and all other explanatory materials.
│
├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
│   └── figures        <- Generated graphics and figures to be used in reporting
│
├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
│                         generated with `pip freeze > requirements.txt`
│
├── setup.py           <- Make this project pip installable with `pip install -e`
├── src                <- Source code for use in this project.
│   ├── __init__.py    <- Makes src a Python module
│   │
│   ├── data           <- Scripts to download or generate data
│   │   └── make_dataset.py
│   │
│   ├── features       <- Scripts to turn raw data into features for modeling
│   │   └── build_features.py
│   │
│   ├── models         <- Scripts to train models and then use trained models to make
│   │   │                 predictions
│   │   ├── predict_model.py
│   │   └── train_model.py
│   │
│   └── visualization  <- Scripts to create exploratory and results oriented visualizations
│       └── visualize.py
│
└── tox.ini            <- tox file with settings for running tox; see tox.testrun.org

```

### 意见

项目结构中隐含了一些观点，这些观点源于我们与数据科学项目协作时的经验。有些观点涉及工作流程，有些观点涉及使生活更轻松的工具。以下是本项目所基于的一些信念——如果你有想法，请 [贡献或分享](https://drivendata.github.io/cookiecutter-data-science/#contributing)。

### 数据是不可变的

永远不要编辑你的原始数据，尤其是手动编辑，尤其是在 Excel 中。不要覆盖你的原始数据。不要保存多个版本的原始数据。将数据（及其格式）视为不可变的。你编写的代码应该将原始数据通过管道传输到最终分析中。你不应该每次想要生成一个新图表时都运行所有步骤（见 [分析是一个DAG](https://drivendata.github.io/cookiecutter-data-science/#analysis-is-a-dag)），但任何人都应该能够仅通过 `src` 中的代码和 `data/raw` 中的数据重现最终产品。

此外，如果数据是不可变的，它不需要像代码那样的源代码控制。因此，***默认情况下，数据文件夹包含在 `.gitignore` 文件中。*** 如果你有少量数据且很少更改，你可能希望将数据包括在仓库中。Github 目前会对超过 50MB 的文件发出警告，并拒绝超过 100MB 的文件。其他存储/同步大数据的选项包括 [AWS S3](https://aws.amazon.com/s3/) 配合同步工具（例如，[`s3cmd`](http://s3tools.org/s3cmd)）、 [Git Large File Storage](https://git-lfs.github.com/)、 [Git Annex](https://git-annex.branchable.com/) 和 [dat](http://dat-data.com/)。目前默认情况下，我们要求一个 S3 存储桶，并使用 [AWS CLI](http://docs.aws.amazon.com/cli/latest/reference/s3/index.html) 将`data`文件夹中的数据与服务器同步。

### 笔记本用于探索和沟通

像 [Jupyter notebook](http://jupyter.org/)、 [Beaker notebook](http://beakernotebook.com/)、 [Zeppelin](http://zeppelin-project.org/) 和其他文学编程工具在探索性数据分析中非常有效。然而，这些工具在重现分析方面可能效果较差。当我们在工作中使用笔记本时，我们经常会将`notebooks`文件夹进行细分。例如，`notebooks/exploratory`包含初步探索，而`notebooks/reports`则是更为精致的工作，可以导出为html文件到`reports`目录。

由于笔记本对源代码控制来说是具有挑战性的对象（例如，`json` 的差异通常不易读懂，并且合并几乎不可能），我们建议不要直接与他人合作使用 Jupyter 笔记本。我们建议使用笔记本时采取以下两个步骤：

1.  遵循一种命名约定，以显示所有者和分析完成的顺序。我们使用格式为`<step>-<ghuser>-<description>.ipynb`（例如，`0.3-bull-visualize-distributions.ipynb`）。

1.  重构好的部分。不要在多个笔记本中编写相同任务的代码。如果是数据预处理任务，将其放在 `src/data/make_dataset.py` 的管道中，并从 `data/interim` 加载数据。如果是有用的工具代码，请将其重构到 `src` 中。

现在默认情况下，我们将项目转换为 Python 包（参见 `setup.py` 文件）。你可以导入你的代码，并在笔记本中使用类似于以下的单元格：

```py
# OPTIONAL: Load the "autoreload" extension so that code can change
%load_ext autoreload

# OPTIONAL: always reload modules so that as you change code in src, it gets loaded
%autoreload 2

from src.data import make_dataset

```

### 分析是一个有向无环图（DAG）

在分析中，通常会有长时间运行的步骤，例如预处理数据或训练模型。如果这些步骤已经执行过（并且你已将输出存储在如`data/interim`目录中），你不希望每次都等待重新运行。我们更喜欢[`make`](https://www.gnu.org/software/make/)来管理彼此依赖的步骤，特别是那些长时间运行的步骤。Make是Unix平台上常见的工具（[Windows上也可以使用](https://drivendata.github.io/cookiecutter-data-science/)）。遵循[`make`文档](https://www.gnu.org/software/make/)、[Makefile规范](https://www.gnu.org/prep/standards/html_node/Makefile-Conventions.html#Makefile-Conventions)和[可移植性指南](http://www.gnu.org/savannah-checkouts/gnu/autoconf/manual/autoconf-2.69/html_node/Portable-Make.html#Portable-Make)将有助于确保你的Makefile在不同系统上有效工作。这里有一些[示例](https://blog.kaggle.com/2012/10/15/make-for-data-scientists/)来[入门](https://web.archive.org/web/20150206054212/http://www.bioinformaticszen.com/post/decomplected-workflows-makefiles/)。许多数据人员，包括[Mike Bostock](https://bost.ocks.org/mike/make/)，都使用`make`作为他们的首选工具。

还有其他用Python编写的DAG管理工具，而不是DSL（例如，[Paver](http://paver.github.io/paver/#)、[Luigi](http://luigi.readthedocs.org/en/stable/index.html)、[Airflow](https://pythonhosted.org/airflow/cli.html)、[Snakemake](https://bitbucket.org/snakemake/snakemake/wiki/Home)、[Ruffus](http://www.ruffus.org.uk/)或[Joblib](https://pythonhosted.org/joblib/memory.html)）。如果这些工具更适合你的分析，请随意使用它们。

### 从环境开始构建

重现分析的第一步始终是重现运行分析的计算环境。你需要相同的工具、相同的库和相同的版本，以确保一切协调工作。

一个有效的方法是使用[virtualenv](https://virtualenv.pypa.io/en/latest/)（我们推荐使用[virtualenvwrapper](https://virtualenvwrapper.readthedocs.org/en/latest/)来管理虚拟环境）。通过在代码库中列出所有需求（我们包括了一个`requirements.txt`文件），你可以轻松跟踪重现分析所需的软件包。以下是一个好的工作流程：

1.  创建新项目时运行`mkvirtualenv`

1.  使用`pip install`安装你的分析所需的软件包。

1.  运行`pip freeze > requirements.txt`以固定用于重现分析的确切软件包版本。

1.  如果你发现需要安装另一个软件包，请再次运行`pip freeze > requirements.txt`并将更改提交到版本控制中。

如果你有更复杂的环境重建需求，可以考虑使用虚拟机方法，如 [Docker](https://www.docker.com/) 或 [Vagrant](https://www.vagrantup.com/)。这两个工具都使用基于文本的格式（分别为 Dockerfile 和 Vagrantfile），你可以轻松地将其添加到源控制中，以描述如何创建具有所需要求的虚拟机。

### 保持秘密和配置不在版本控制中。

你 *真的* 不想在 Github 上泄露你的 AWS 秘密密钥或 Postgres 用户名和密码。言尽于此 — 参见 [十二因子应用](http://12factor.net/config) 原则。以下是一种做法：

**将你的秘密和配置变量存储在一个特殊的文件中**

在项目根目录下创建一个 `.env` 文件。由于 `.gitignore`，这个文件应该不会被提交到版本控制库中。以下是一个示例：

```py
# example .env file
DATABASE_URL=postgres://username:password@localhost:5432/dbname
AWS_ACCESS_KEY=myaccesskey
AWS_SECRET_ACCESS_KEY=mysecretkey
OTHER_VARIABLE=something

```

**使用包来自动加载这些变量。**

如果你查看 `src/data/make_dataset.py` 中的示例脚本，它使用一个名为 [python-dotenv](https://github.com/theskumar/python-dotenv) 的包，将该文件中的所有条目作为环境变量加载，以便通过 `os.environ.get` 访问。以下是从 `python-dotenv` 文档中改编的示例代码片段：

```py
# src/data/dotenv_example.py
import os
from dotenv import load_dotenv, find_dotenv

# find .env automagically by walking up directories until it's found
dotenv_path = find_dotenv()

# load up the entries as environment variables
load_dotenv(dotenv_path)

database_url = os.environ.get("DATABASE_URL")
other_variable = os.environ.get("OTHER_VARIABLE")

```

**AWS CLI 配置**

使用 Amazon S3 存储数据时，管理 AWS 访问的一种简单方法是将访问密钥设置为环境变量。然而，管理单台机器上的多个密钥集（例如，当在多个项目中工作时）最好使用 [凭据文件](https://docs.aws.amazon.com/cli/latest/userguide/cli-config-files.html)，通常位于 `~/.aws/credentials`。一个典型的文件可能如下所示：

```py
[default]
aws_access_key_id=myaccesskey
aws_secret_access_key=mysecretkey

[another_project]
aws_access_key_id=myprojectaccesskey
aws_secret_access_key=myprojectsecretkey

```

在初始化项目时，你可以添加配置文件名；如果没有设置适用的环境变量，则将默认使用配置文件中的凭据。

### 修改默认文件夹结构时要谨慎。

为了使这种结构在多种不同类型的项目中广泛适用，我们认为最好的方法是在 *你的* 项目中自由调整文件夹结构，但在 *所有* 项目中对默认结构要保持保守。

我们为提议添加、删除、重命名或移动文件夹的问题创建了一个 `folder-layout` 标签。更一般地，我们也创建了一个 `needs-discussion` 标签，用于那些在实施前需要认真讨论和广泛支持的问题。

### 贡献

Cookiecutter Data Science 项目虽然有其自身的观点，但也不怕犯错。最佳实践会变化，工具会发展，经验也会不断积累。**该项目的目标是使开始、构建和共享分析变得更加简单。** [拉取请求](https://github.com/drivendata/cookiecutter-data-science/pulls) 和 [提交问题](https://github.com/drivendata/cookiecutter-data-science/issues) 是被鼓励的。我们很乐意听到什么对你有用，什么没有用。

如果你使用了 Cookiecutter Data Science 项目，请链接回此页面或 [给我们留言](https://twitter.com/drivendataorg) 并 [让我们知道](mailto:info@drivendata.org)!

### 相关项目和参考链接

R 研究社区更多讨论了项目结构和可重复性。如果你在使用 R 工作，以下项目和博客文章可能会对你有所帮助。

+   [项目模板](http://projecttemplate.net/index.html) - 一个 R 数据分析模板

+   "[设计项目](http://nicercode.github.io/blog/2013-04-05-projects/)" 发表在 Nice R Code 上

+   "[我的研究工作流程](http://www.carlboettiger.info/2012/05/06/research-workflow.html)" 发表在 Carlboettifer.info 上

+   "[计算生物学项目组织速查指南](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000424)" 发表在 PLOS Computational Biology 上

最后，衷心感谢 [Cookiecutter](https://cookiecutter.readthedocs.org/en/latest/) 项目 ([github](https://github.com/audreyr/cookiecutter))，它帮助我们减少了思考和编写模板的时间，更多时间用于完成实际工作。

**简介：[DrivenData](https://www.drivendata.org/)** 是一家以使命驱动的数据科学公司，致力于将数据科学、机器学习和人工智能的强大能力带给解决全球重大挑战的组织。DrivenData Labs（drivendata.co）帮助以使命为导向的组织利用数据更智能地工作，提供更有影响力的服务，并充分发挥机器智能的潜力。DrivenData 还举办在线机器学习竞赛（drivendata.org），全球热情的数据科学家社区在此为社会影响构建算法。

[原版](https://drivendata.github.io/cookiecutter-data-science/)。经许可转载。

**相关：**

+   [与机器学习算法相关的数据结构](/2018/01/data-structures-related-machine-learning-algorithms.html)

+   [Python 正则表达式备忘单](/2018/04/python-regular-expressions-cheat-sheet.html)

+   [Python 中函数式编程简介](/2018/02/introduction-functional-programming-python.html)

### 更多相关话题

+   [成为一名优秀数据科学家所需的 5 项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)

+   [每个初学数据科学者应该掌握的 6 种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)

+   [2021 年最佳 ETL 工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)

+   [停止学习数据科学以寻找目的，并为此找寻目的…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)

+   [学习数据科学统计学的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)

+   [建立一个坚实的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)
