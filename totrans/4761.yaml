- en: Introduction to Deep Learning with Keras
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 的深度学习介绍
- en: 原文：[https://www.kdnuggets.com/2018/10/introduction-deep-learning-keras.html](https://www.kdnuggets.com/2018/10/introduction-deep-learning-keras.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 译文：[https://www.kdnuggets.com/2018/10/introduction-deep-learning-keras.html](https://www.kdnuggets.com/2018/10/introduction-deep-learning-keras.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](/2018/10/introduction-deep-learning-keras.html?page=2#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](/2018/10/introduction-deep-learning-keras.html?page=2#comments)'
- en: '![Image](../Images/c89d61f18b1ba7aa60ff9422437fb38d.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../Images/c89d61f18b1ba7aa60ff9422437fb38d.png)'
- en: In this article, we’ll build a simple neural network using [Keras](http://keras.io/).
    We’ll assume you have prior knowledge of machine learning packages such as [scikit-learn](http://scikit-learn.org/stable/)and
    other scientific packages such as [Pandas](https://pandas.pydata.org/) and [Numpy](http://www.numpy.org/).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将使用 [Keras](http://keras.io/) 构建一个简单的神经网络。我们假设你已经具备如 [scikit-learn](http://scikit-learn.org/stable/)
    和其他科学包如 [Pandas](https://pandas.pydata.org/) 和 [Numpy](http://www.numpy.org/)
    的机器学习知识。
- en: '**Training an Artificial Neural Network**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**训练人工神经网络**'
- en: 'Training an artificial neural network involves the following steps:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 训练人工神经网络包括以下步骤：
- en: Weights are randomly initialized to numbers that are near zero but not zero.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 权重被随机初始化为接近零但不为零的数字。
- en: Feed the observations of your dataset to the input layer.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集的观测值输入到输入层。
- en: 'Forward propagation (from left to right): neurons are activated and the predicted
    values are obtained.'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正向传播（从左到右）：神经元被激活并得到预测值。
- en: Compare predicted results to actual values and measure the error.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较预测结果与实际值，并测量误差。
- en: 'Backward propagation (from right to left): weights are adjusted.'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播（从右到左）：调整权重。
- en: Repeat steps 1–5
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 1–5
- en: One epoch is achieved when the whole training set has gone through the neural
    network.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当整个训练集经过神经网络一次时，称为一个周期。
- en: '**Business Problem**'
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**商业问题**'
- en: Now let’s proceed to solve a real business problem. An insurance company has
    approached you with a dataset of previous claims of their clients. The insurance
    company wants you to develop a model to help them predict which claims look fraudulent.
    By doing so you hope to save the company millions of dollars annually. This is
    a classification problem. These are the columns in our dataset.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始解决一个实际的商业问题。一家保险公司向你提供了一份客户之前索赔的数据集。保险公司希望你开发一个模型，以帮助他们预测哪些索赔看起来像是欺诈行为。这样做，你希望每年为公司节省数百万美元。这是一个分类问题。以下是我们数据集中的列。
- en: '![](../Images/67889150c551e63f6b6a280479b46b8a.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67889150c551e63f6b6a280479b46b8a.png)'
- en: '![](../Images/63f722c7a0d93149d31786aa4c177a2b.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63f722c7a0d93149d31786aa4c177a2b.png)'
- en: '![](../Images/b0148e763fd945baa2c9f0f7cbb1bc81.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0148e763fd945baa2c9f0f7cbb1bc81.png)'
- en: '****Data Preprocessing****'
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '****数据预处理****'
- en: As in many business problems, the data provided will not be processed for us.
    We therefore have to prepare it in a way that our algorithm will accept it. We
    see from the dataset that we have some categorical columns. We need to convert
    these to zeros and ones so that our deep learning model will be able to understand
    them. Another thing to note is that we have to feed our dataset to the model as
    numpy arrays. Below we import the necessary packages and then load in our dataset.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多商业问题一样，提供的数据不会为我们处理。因此，我们必须以算法能够接受的方式来准备数据。我们从数据集中看到有一些类别列。我们需要将这些列转换为零和一，以便我们的深度学习模型能够理解它们。另一个需要注意的是，我们必须将数据集作为
    numpy 数组输入模型。下面我们导入必要的包，然后加载数据集。
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We then to convert the categorical columns to dummy variables.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将类别列转换为虚拟变量。
- en: In this case we use drop_first=True to avoid the dummy variable trap. For example,
    if you have a, b, c, d as categories then you can drop d as a dummy variable.
    This is because if something does not fall into either a, b, or c then it’s definitely
    in d. This is referred to as multicollinearity.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用 drop_first=True 避免虚拟变量陷阱。例如，如果你有 a、b、c、d 作为类别，那么你可以丢弃 d 作为虚拟变量。这是因为如果某些东西既不属于
    a、b，也不属于 c，那么它肯定在 d 中。这被称为多重共线性。
- en: We use [sklearn’s](http://scikit-learn.org/) train_test_split to split the data
    into a training set and a test set.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 [sklearn](http://scikit-learn.org/) 的 train_test_split 将数据分成训练集和测试集。
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next we make sure to drop the column we’re predicting to prevent it from leaking
    into the training set and the test set. We must avoid using the same dataset to
    train and test the model. We set **.values** at the end of the dataset in order
    to get the numpy arrays. This is the way our deep learning model will accept the
    data. This step is important because our machine learning model expects the data
    in form of arrays.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们确保丢弃我们预测的列，以防止其泄漏到训练集和测试集中。我们必须避免使用相同的数据集来训练和测试模型。我们在数据集末尾设置**.values**以获得numpy数组。这是我们的深度学习模型接受数据的方式。这一步很重要，因为我们的机器学习模型期望数据以数组的形式存在。
- en: We then split the data into a training and test set. We use 0.7 of the data
    for training and 0.3 for testing.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将数据拆分为训练集和测试集。我们使用70%的数据用于训练，30%用于测试。
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next we have to scale our dataset using Sklearn’s *StandardScaler. *Due to the
    massive amounts of computations taking place in deep learning, feature scaling
    is compulsory. Feature scaling standardizes the range of our independent variables.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要使用Sklearn的*StandardScaler*对数据集进行缩放。由于深度学习中大量的计算，特征缩放是强制性的。特征缩放标准化了我们自变量的范围。
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '****Building the Artificial Neural Network(ANN)****'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '****构建人工神经网络（ANN）****'
- en: The first thing we need to do is import Keras. By default, Keras will use TensorFlow
    as its backend.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们需要导入Keras。默认情况下，Keras会使用TensorFlow作为其后台。
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next we need to import a few modules from Keras. The Sequential module is required
    to initialize the ANN, and the Dense module is required to build the layers of
    our ANN.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要从Keras中导入一些模块。Sequential模块是初始化ANN所必需的，而Dense模块是构建ANN层所必需的。
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Next we need to initialize our ANN by creating an instance of Sequential. The
    Sequential function initializes a linear stack of layers. This allows us to add
    more layers later using the Dense module.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要通过创建Sequential实例来初始化我们的ANN。Sequential函数初始化了一个线性堆叠的层。这允许我们以后使用Dense模块添加更多层。
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Adding input layer (First Hidden Layer)**'
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**添加输入层（第一个隐藏层）**'
- en: We use the add method to add different layers to our ANN. The first parameter
    is the number of nodes you want to add to this layer. There is no rule of thumb
    as to how many nodes you should add. However a common strategy is to choose the
    number of nodes as the average of nodes in the input layer and the number of nodes
    in the output layer.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用add方法将不同的层添加到我们的ANN中。第一个参数是你想添加到这一层的节点数。没有固定的规则来确定应该添加多少个节点。然而，一种常见的策略是将节点数量选为输入层节点数和输出层节点数的平均值。
- en: Say for example you had five independent variables and one output. Then you
    would take the sum of that and divide by two, which is three. You can also decide
    to experiment with a technique called parameter tuning. The second parameter, *kernel_initializer*,is
    the function that will be used to initialize the weights. In this case, it will
    use a uniform distribution to make sure that the weights are small numbers close
    to zero. The next parameter is the activation function. We use the Rectifier function,
    shortened as relu. We mostly use this function for the hidden layer in ANN. The
    final parameter is input_dim, which is the number of nodes in the input layer.
    It represents the number of independent variables.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你有五个自变量和一个输出。然后你可以将其求和并除以二，即三。你也可以尝试一种叫做参数调优的技术。第二个参数，*kernel_initializer*，是用于初始化权重的函数。在这种情况下，它将使用均匀分布来确保权重是接近零的小数。下一个参数是激活函数。我们使用的是Rectifier函数，简写为relu。我们主要在ANN的隐藏层中使用此函数。最后一个参数是input_dim，它是输入层中的节点数。它代表了自变量的数量。
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '****Adding Second Hidden Layer****'
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '****添加第二个隐藏层****'
- en: Adding the second hidden layer is similar to adding the first hidden layer.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 添加第二个隐藏层类似于添加第一个隐藏层。
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We don’t need to specify the input_dim parameter because we have already specified
    it in the first hidden layer. In the first hidden layer we specified this in order
    to let the layer know how many input nodes to expect. In the second hidden layer
    the ANN already knows how many input nodes to expect so we don’t need to repeat
    ourselves.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要指定input_dim参数，因为我们已经在第一个隐藏层中指定了它。在第一个隐藏层中，我们指定了这个参数以便让该层知道期望多少个输入节点。在第二个隐藏层中，ANN已经知道期望多少个输入节点，因此我们不需要重复。
- en: '****Adding the output layer****'
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '****添加输出层****'
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We change the first parameter because in our output node we expect one node.
    This is because we are only interested in knowing whether a claim was fraudulent
    or not. We change the activation function because we want to get the probabilities
    that a claim is fraudulent. We do this by using the Sigmoid activation function.
    In case you’re dealing with a classification problem that has more than two classes
    (i.e. classifying cats, dogs, and monkeys) we’d need to change two things. We
    ‘d change the first parameter to 3 and change the activation function to softmax.
    Softmax is a sigmoid function applied to an independent variable with more than
    two categories.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更改第一个参数，因为在我们的输出节点中，我们期望只有一个节点。这是因为我们只关心知道一个声明是否是欺诈性的。我们更改激活函数是因为我们想要获得声明是欺诈性的概率。我们通过使用
    Sigmoid 激活函数来实现这一点。如果你处理的是一个有多个类别的分类问题（即对猫、狗和猴子进行分类），我们需要更改两件事。我们将第一个参数更改为 3，并将激活函数更改为
    softmax。Softmax 是一种应用于具有两个以上类别的独立变量的 sigmoid 函数。
- en: '* * *'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT'
- en: '* * *'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 TensorFlow 和 Keras 构建和训练您的第一个神经网络](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
- en: '[Keras 3.0: Everything You Need To Know](https://www.kdnuggets.com/2023/07/keras-30-everything-need-know.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Keras 3.0：你需要知道的一切](https://www.kdnuggets.com/2023/07/keras-30-everything-need-know.html)'
- en: '[Introduction to Deep Learning Libraries: PyTorch and Lightning AI](https://www.kdnuggets.com/introduction-to-deep-learning-libraries-pytorch-and-lightning-ai)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习库介绍：PyTorch 和 Lightning AI](https://www.kdnuggets.com/introduction-to-deep-learning-libraries-pytorch-and-lightning-ai)'
- en: '[A Solid Plan for Learning Data Science, Machine Learning, and Deep Learning](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学、机器学习和深度学习的可靠计划](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
- en: '[KDnuggets News, April 27: A Brief Introduction to Papers With Code;…](https://www.kdnuggets.com/2022/n17.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，4月27日：关于 Papers With Code 的简要介绍；…](https://www.kdnuggets.com/2022/n17.html)'
- en: '[Introduction to Statistical Learning, Python Edition: Free Book](https://www.kdnuggets.com/2023/07/introduction-statistical-learning-python-edition-free-book.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[统计学习简介，Python 版：免费书籍](https://www.kdnuggets.com/2023/07/introduction-statistical-learning-python-edition-free-book.html)'
