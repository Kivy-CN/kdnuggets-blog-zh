["```py\n# Importing necessary library\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport os\nimport nltk.corpus# sample text for performing tokenization\ntext = “In Brazil they drive on the right-hand side of the road. Brazil has a large coastline on the eastern\nside of South America\"# importing word_tokenize from nltk\nfrom nltk.tokenize import word_tokenize# Passing the string text into word tokenize for breaking the sentences\ntoken = word_tokenize(text)\ntoken\n```", "```py\n['In','Brazil','they','drive', 'on','the', 'right-hand', 'side', 'of', 'the', 'road', '.', 'Brazil', 'has', 'a', 'large', 'coastline', 'on', 'the', 'eastern', 'side', 'of', 'South', 'America']\n```", "```py\n# finding the frequency distinct in the tokens\n# Importing FreqDist library from nltk and passing token into FreqDist\nfrom nltk.probability import FreqDist\nfdist = FreqDist(token)\nfdist\n```", "```py\nFreqDist({'the': 3, 'Brazil': 2, 'on': 2, 'side': 2, 'of': 2, 'In': 1, 'they': 1, 'drive': 1, 'right-hand': 1, 'road': 1, ...})\n```", "```py\n# To find the frequency of top 10 words\nfdist1 = fdist.most_common(10)\nfdist1\n```", "```py\n[('the', 3),\n ('Brazil', 2),\n ('on', 2),\n ('side', 2),\n ('of', 2),\n ('In', 1),\n ('they', 1),\n ('drive', 1),\n ('right-hand', 1),\n ('road', 1)]\n```", "```py\n# Importing Porterstemmer from nltk library\n# Checking for the word ‘giving’ \nfrom nltk.stem import PorterStemmer\npst = PorterStemmer()\npst.stem(“waiting”)\n```", "```py\n'wait'\n```", "```py\n# Checking for the list of words\nstm = [\"waited\", \"waiting\", \"waits\"]\nfor word in stm :\n   print(word+ \":\" +pst.stem(word))\n```", "```py\nwaited:wait\nwaiting:wait\nwaits:wait\n```", "```py\n# Importing LancasterStemmer from nltk\nfrom nltk.stem import LancasterStemmer\nlst = LancasterStemmer()\nstm = [“giving”, “given”, “given”, “gave”]\nfor word in stm :\n print(word+ “:” +lst.stem(word))\n```", "```py\ngiving:giv\ngiven:giv\ngiven:giv\ngave:gav\n```", "```py\n# Importing Lemmatizer library from nltk\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer() \n\nprint(“rocks :”, lemmatizer.lemmatize(“rocks”)) \nprint(“corpora :”, lemmatizer.lemmatize(“corpora”))\n```", "```py\nrocks : rock\ncorpora : corpus\n```", "```py\n# importing stopwors from nltk library\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\na = set(stopwords.words(‘english’))text = “Cristiano Ronaldo was born on February 5, 1985, in Funchal, Madeira, Portugal.”\ntext1 = word_tokenize(text.lower())\nprint(text1)stopwords = [x for x in text1 if x not in a]\nprint(stopwords)\n```", "```py\nOutput of text:\n['cristiano', 'ronaldo', 'was', 'born', 'on', 'february', '5', ',', '1985', ',', 'in', 'funchal', ',', 'madeira', ',', 'portugal', '.']Output of stopwords:\n['cristiano', 'ronaldo', 'born', 'february', '5', ',', '1985', ',', 'funchal', ',', 'madeira', ',', 'portugal', '.']\n```", "```py\ntext = “vote to choose a particular man or a group (party) to represent them in parliament”\n#Tokenize the text\ntex = word_tokenize(text)\nfor token in tex:\nprint(nltk.pos_tag([token]))\n```", "```py\n[('vote', 'NN')]\n[('to', 'TO')]\n[('choose', 'NN')]\n[('a', 'DT')]\n[('particular', 'JJ')]\n[('man', 'NN')]\n[('or', 'CC')]\n[('a', 'DT')]\n[('group', 'NN')]\n[('(', '(')]\n[('party', 'NN')]\n[(')', ')')]\n[('to', 'TO')]\n[('represent', 'NN')]\n[('them', 'PRP')]\n[('in', 'IN')]\n[('parliament', 'NN')]\n```", "```py\ntext = “Google’s CEO Sundar Pichai introduced the new Pixel at Minnesota Roi Centre Event”#importing chunk library from nltk\nfrom nltk import ne_chunk# tokenize and POS Tagging before doing chunk\ntoken = word_tokenize(text)\ntags = nltk.pos_tag(token)\nchunk = ne_chunk(tags)\nchunk\n```", "```py\nTree('S', [Tree('GPE', [('Google', 'NNP')]), (\"'s\", 'POS'), Tree('ORGANIZATION', [('CEO', 'NNP'), ('Sundar', 'NNP'), ('Pichai', 'NNP')]), ('introduced', 'VBD'), ('the', 'DT'), ('new', 'JJ'), ('Pixel', 'NNP'), ('at', 'IN'), Tree('ORGANIZATION', [('Minnesota', 'NNP'), ('Roi', 'NNP'), ('Centre', 'NNP')]), ('Event', 'NNP')])\n```", "```py\ntext = “We saw the yellow dog”\ntoken = word_tokenize(text)\ntags = nltk.pos_tag(token)reg = “NP: {<DT>?<JJ>*<NN>}” \na = nltk.RegexpParser(reg)\nresult = a.parse(tags)\nprint(result)\n```", "```py\n(S We/PRP saw/VBD (NP the/DT yellow/JJ dog/NN))\n```"]