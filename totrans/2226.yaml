- en: 'Unveiling Neural Magic: A Dive into Activation Functions'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/unveiling-neural-magic-a-dive-into-activation-functions](https://www.kdnuggets.com/unveiling-neural-magic-a-dive-into-activation-functions)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Unveiling Neural Magic: A Dive into Activation Functions](../Images/23c0c292d492c32ac44d2f06244752bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Why Use Activation Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Learning and Neural Networks consist of interconnected nodes, where data
    is passed sequentially through each hidden layer. However, the composition of
    linear functions is inevitably still a linear function. Activation functions become
    important when we need to learn complex and non-linear patterns within our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two major benefits of using activation functions are:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduces Non-Linearity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear relationships are rare in real-world scenarios. Most real-world scenarios
    are complex and follow a variety of different trends. Learning such patterns is
    impossible with linear algorithms like Linear and Logistic Regression. Activation
    functions add non-linearity to the model, allowing it to learn complex patterns
    and variance in the data. This enables deep learning models to perform complicated
    tasks including the image and language domains.
  prefs: []
  type: TYPE_NORMAL
- en: Allow Deep Neural Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned above, *when we sequentially apply multiple linear functions, the
    output is still a linear combination of the inputs.* Introducing non-linear functions
    between each layer allows them to learn different features of the input data.
    Without activation functions, having a deeply connected neural network architecture
    will be the same as using basic Linear or Logistic Regression algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions allow deep learning architectures to learn complex patterns,
    making them more powerful than simple Machine Learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at some of the most common activation functions used in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Commonly used in binary classification tasks, the Sigmoid function maps real-numbered
    values between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![Unveiling Neural Magic: A Dive into Activation Functions](../Images/80d0e061ecdc2efbcc950004c2c7b5e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The above equation looks as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unveiling Neural Magic: A Dive into Activation Functions](../Images/f27c6ca6f7630bd3a5537f65e246b73e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Hvidberrrg](https://hvidberrrg.github.io/deep_learning/activation_functions/sigmoid_function_and_derivative.html)
  prefs: []
  type: TYPE_NORMAL
- en: '**The Sigmoid function is primarily used in the output layer for binary classification
    tasks where the target label is either 0 or 1.** This naturally makes Sigmoid
    preferable for such tasks, as the output is restricted between this range. For
    highly positive values that approach infinity, the sigmoid function maps them
    close to 1\. On the opposite end, it maps values approaching negative infinity
    to 0\. All real-valued numbers between these are mapped in the range 0 to 1 in
    an S-shaped trend.'
  prefs: []
  type: TYPE_NORMAL
- en: Shortcomings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Saturation Points
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The sigmoid function poses problems for the gradient descent algorithm during
    backpropagation. Except for values close to the center of the S-shaped curve,
    the gradient is extremely close to zero causing problems for training. Close to
    the asymptotes, it can lead to vanishing gradient problems as small gradients
    can significantly slow down convergence.
  prefs: []
  type: TYPE_NORMAL
- en: Not Zero-Centered
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I*t is empirically proven that having a zero-centered non-linear function ensures
    that the mean activation value is close to 0*. Having such normalized values ensures
    faster convergence of gradient descent towards the minima. Although not necessary,
    having zero-centered activation allows faster training. The Sigmoid function is
    centered at 0.5 when the input is 0\. This is one of the drawbacks of using Sigmoid
    in hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: Tanh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The hyperbolic tangent function is an improvement over the Sigmoid function.
    Instead of the [0,1] range, the TanH function maps real-valued numbers between
    -1 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![Unveiling Neural Magic: A Dive into Activation Functions](../Images/0de1d4ed4910a0cb10282f1672de595c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Tanh function looks as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unveiling Neural Magic: A Dive into Activation Functions](../Images/26fa67c0aba4c8db7b99e4515bb868ef.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Wolfram](https://mathworld.wolfram.com/HyperbolicTangent.html)
  prefs: []
  type: TYPE_NORMAL
- en: The TanH function follows the same S-shaped curve as the Sigmoid, but it is
    now zero-centered. This allows faster convergence during training as it improves
    on one of the shortcomings of the Sigmoid function. **This makes it more suitable
    for use in hidden layers in a neural network architecture.**
  prefs: []
  type: TYPE_NORMAL
- en: Shortcomings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Saturation Points
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The TanH function follows the same S-shaped curve as the Sigmoid, but it is
    now zero-centered. This allows faster convergence during training improving upon
    the Sigmoid function. This makes it more suitable for use in hidden layers in
    a neural network architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Computational Expense
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although not a major concern in the modern day, the exponential calculation
    is more expensive than other common alternatives available.
  prefs: []
  type: TYPE_NORMAL
- en: ReLU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most commonly used activation function in practice, Rectified Linear Unit
    Activation (ReLU) is the most simple yet most effective possible non-linear function.
  prefs: []
  type: TYPE_NORMAL
- en: '![Unveiling Neural Magic: A Dive into Activation Functions](../Images/eb93e3d80f0aa40eb31cd9cbf24a7386.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It conserves all non-negative values and clamps all negative values to 0\.
    Visualized, the ReLU functions look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unveiling Neural Magic: A Dive into Activation Functions](../Images/04eed6624e7ff79531aa963d323f5b54.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [**Michiel Straat**](https://michielstraat.com/)
  prefs: []
  type: TYPE_NORMAL
- en: Shortcomings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dying ReLU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The gradient flattens at one end of the graph. All negative values have zero
    gradients, so half of the neurons may have minimal contribution to training.
  prefs: []
  type: TYPE_NORMAL
- en: Unbounded Activation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On the right-hand side of the graph, there is no limit on the possible gradient.
    This can lead to an exploding gradient problem if the gradient values are too
    high. This issue is normally corrected by Gradient Clipping and Weight Initialization
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Not Zero-Centered
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to Sigmoid, the ReLU activation function is also not zero-centered.
    Likewise, this causes problems with convergence and can slow down training.
  prefs: []
  type: TYPE_NORMAL
- en: '**Despite all shortcomings, it is the default choice for all hidden layers
    in neural network architectures and is empirically proven to be highly efficient
    in practice.**'
  prefs: []
  type: TYPE_NORMAL
- en: Key Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know about the three most common activation functions, how do we
    know what is the best possible choice for our scenario?
  prefs: []
  type: TYPE_NORMAL
- en: Although it highly depends on the data distribution and specific problem statement,
    there are still some basic starting points that are widely used in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid is only suitable for output activations of binary problems when target
    labels are either 0 or 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tanh is now majorly replaced by the ReLU and similar functions. However, it
    is still used in hidden layers for RNNs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In all other scenarios, ReLU is the default choice for hidden layers in deep
    learning architectures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[Muhammad Arham](https://www.linkedin.com/in/muhammad-arham-a5b1b1237/)**
    is a Deep Learning Engineer working in Computer Vision and Natural Language Processing.
    He has worked on the deployment and optimizations of several generative AI applications
    that reached the global top charts at Vyro.AI. He is interested in building and
    optimizing machine learning models for intelligent systems and believes in continual
    improvement.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How Activation Functions Work in Deep Learning](https://www.kdnuggets.com/2022/06/activation-functions-work-deep-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Diving into the Pool: Unraveling the Magic of CNN Pooling Layers](https://www.kdnuggets.com/diving-into-the-pool-unraveling-the-magic-of-cnn-pooling-layers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Optimizing Python Code Performance: A Deep Dive into Python Profilers](https://www.kdnuggets.com/2023/02/optimizing-python-code-performance-deep-dive-python-profilers.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Deep Dive into GPT Models: Evolution & Performance Comparison](https://www.kdnuggets.com/2023/05/deep-dive-gpt-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Dive into the Future with Kaggle''s AI Report 2023 – See What''s Hot](https://www.kdnuggets.com/dive-into-the-future-with-kaggle-ai-report-2023-see-what-hot)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unveiling the Potential of CTGAN: Harnessing Generative AI for…](https://www.kdnuggets.com/2023/04/unveiling-potential-ctgan-harnessing-generative-ai-synthetic-data.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
