# 机器学习和数据科学中的决策树指南

> 原文：[https://www.kdnuggets.com/2018/12/guide-decision-trees-machine-learning-data-science.html](https://www.kdnuggets.com/2018/12/guide-decision-trees-machine-learning-data-science.html)

![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)

**由 [George Seif](https://towardsdatascience.com/@george.seif94)，人工智能/机器学习工程师**

决策树是一类非常强大的机器学习模型，能够在许多任务中实现高准确率，同时又具有很高的可解释性。在机器学习模型领域，决策树的特别之处在于其信息表示的清晰性。决策树通过训练获得的“知识”直接被形成一个层次结构。这种结构以一种易于理解的方式展示知识，即使是非专家也能轻松理解。

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速入门网络安全职业

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析能力

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织IT

* * *

![](../Images/247d774fc1b4a93bcba12afc67523f20.png)

### 现实生活中的决策树

你可能以前使用过决策树来做出生活中的决策。举个例子，比如你要决定这个周末做什么活动。你可能会考虑是否想和朋友出去还是独自在家；在这两种情况下，你的决定也取决于天气。如果天气晴朗且你的朋友有空，你可能会想去踢足球。如果下雨，你会去看电影。如果你的朋友根本没有出现，那你就会玩电子游戏，无论天气如何！

![](../Images/4476e73464585bcc9cddaf0ae303d4a1.png)

这是一个清晰的*现实生活中的决策树*的例子。我们构建了一个树来建模一组**顺序的、层次化的决策**，这些决策最终会导致某个最终结果。请注意，我们还选择了比较“高层次”的决策，以保持树的简洁。例如，如果我们为天气设置了*许多*可能的选项，如25度晴天、25度下雨、26度晴天、26度下雨、27度晴天……等等，我们的树会非常庞大！**准确的**温度其实并不是很重要，我们只想知道是否可以在户外活动。

机器学习中的决策树概念是相同的。我们希望构建一个具有一组层次决策的树，这些决策最终给出一个最终结果，即我们的分类或回归预测。决策将被选择，以使树尽可能小，同时追求高分类/回归准确性。

### 机器学习中的决策树

决策树模型是通过两个步骤创建的：归纳和剪枝。归纳是我们实际构建树的地方，即基于我们的数据设置所有层次决策边界。由于训练决策树的性质，它们可能容易出现严重的过拟合。剪枝是从决策树中移除不必要结构的过程，有效地减少复杂性，以应对过拟合，并使其更易于解释。

**归纳**

从高层次来看，决策树归纳经历4个主要步骤来构建树：

1.  从你的训练数据集开始，该数据集应该具有一些特征变量和分类或回归输出。

1.  确定数据集中“最佳特征”来分割数据；稍后会详细说明我们如何定义“最佳特征”

1.  将数据分割成包含最佳特征可能值的子集。这种分裂基本上定义了树上的一个节点，即每个节点是基于我们数据的某个特征的分裂点。

1.  通过使用第3步创建的数据子集递归生成新的树节点。我们不断分裂，直到达到一个点，在这个点上，我们通过某种度量优化了最大准确度，同时最小化了分裂/节点的数量。

第一步很简单，只需获取你的数据集！

对于第二步，选择使用哪个特征以及具体的分裂点通常使用贪心算法来最小化成本函数。如果我们稍微思考一下，构建决策树时的分裂相当于划分特征空间。我们将反复尝试不同的分裂点，然后在最后选择成本最低的那个。当然，我们可以做一些聪明的事情，比如仅在数据集中值的范围内进行分裂。这将避免在测试明显差的分裂点上浪费计算。

对于回归树，我们可以使用一个简单的平方误差作为我们的成本函数：

![](../Images/9e737360b5111722e071106141110538.png)

其中 Y 是我们的实际值，Y-hat 是我们的预测值；我们对数据集中的所有样本求和，以得到总误差。对于分类，我们使用*基尼指数函数*：

![](../Images/cbd57496eddd6d3825f64340081d9c88.png)

其中 pk 是某一预测节点中类别 k 的训练实例比例。一个节点*理想情况下*应该具有零误差值，这意味着每次分裂都能 100% 输出单一类别。这正是我们想要的，因为一旦我们到达那个特定的决策节点，我们就会知道，无论我们在决策边界的一侧还是另一侧，我们的输出到底是什么。

这种在数据集中每次分裂都有一个单一类别的概念称为*信息增益*。查看下面的示例。

![](../Images/a52e214cfcc8a71a438dc70e9007a719.png)

如果我们选择的分裂使每个输出根据输入数据混合了不同的类别，那么我们实际上并没有*获得*任何信息；我们不能更好地知道特定节点即特征在分类数据时是否有影响！另一方面，如果我们的分裂使每个输出具有高比例的每个类别，那么我们*获得*了这样的信息：在特定特征变量上以这种特定方式进行分裂可以给我们一个特定的输出！

当然，我们可以不断地分裂，直到我们的树有成千上万的分支……但这并不是一个好主意！我们的决策树将会非常庞大，速度缓慢，并且过拟合于我们的训练数据集。因此，我们将设置一些预定义的停止标准来停止树的构建。

最常见的停止方法是对分配给每个叶节点的训练示例数量使用最小计数。如果数量小于某个最小值，则该分裂不被接受，节点被视为最终叶节点。如果所有的叶节点都变成最终节点，则训练停止。较小的最小计数会产生更细的分裂，并可能提供更多的信息，但也容易对训练数据进行过拟合。最小计数过大可能会过早停止。因此，最小值通常根据数据集设置，具体取决于每个类别中预计的示例数量。

**剪枝**

由于训练决策树的性质，它们可能容易出现过拟合。为每个节点设置正确的最小实例数量可能具有挑战性。大多数时候，我们可能会选择一个安全的选择，将最小值设得非常小，导致分裂过多，树变得非常大且复杂。关键在于，这些分裂中的许多将最终是冗余的，不会增加我们模型的准确性。

树剪枝是一种利用分裂冗余来移除，即*剪枝*我们树中的不必要分裂的技术。从高层次来看，剪枝将部分树从严格和僵硬的决策边界压缩成更平滑和更具泛化性的边界，从而有效减少树的复杂性。决策树的复杂性定义为树中的分裂数量。

一种简单而有效的剪枝方法是遍历树中的每个节点，评估移除它对成本函数的影响。如果变化不大，则进行剪枝！

### Scikit Learn中的一个示例

Scikit Learn中分类和回归的决策树非常易于使用，提供了内置的类！我们首先加载数据集并初始化分类决策树。然后运行训练只需一行代码！

Scikit Learn还允许我们使用graphviz库可视化我们的树。它提供了一些选项，可以帮助可视化模型学习到的决策节点和分裂，这对于理解其工作原理非常有用！下面我们将根据特征名称对节点进行着色，并显示每个节点的类别和特征信息。

![](../Images/ecaf20a53098ea1fa03240e432bd03e3.png)

在Scikit Learn中，你还可以为决策树模型设置多个参数。以下是一些更有趣的参数，可以尝试以获得更好的结果：

+   **max_depth：** 决定停止分裂节点的树的最大深度。这类似于控制深度神经网络中的最大层数。较低的深度会使模型更快但准确度较低；较高的深度可以提高准确度但风险过拟合，可能会变慢。

+   **min_samples_split：** 分裂节点所需的最小样本数量。我们在上面讨论过决策树的这个方面，设置较高的值有助于减少过拟合。

+   **max_features：** 查找最佳分裂时要考虑的特征数量。较高的值意味着可能得到更好的结果，但训练时间会更长。

+   **min_impurity_split：** 决定树生长的早期停止阈值。如果节点的杂质高于阈值，则会进行分裂。这可以用来权衡抗过拟合（高值，小树）与高准确率（低值，大树）。

+   **预排序：** 是否预排序数据以加快寻找最佳分裂点的速度。如果我们提前对每个特征进行排序，我们的训练算法将更容易找到合适的分裂值。

### 实际应用决策树的技巧

以下是决策树的一些优缺点，可以帮助你决定它是否适合你的问题，并提供一些有效应用它们的技巧：

**优点**

+   **易于理解和解释。** 在每个节点上，我们能够*准确地*看到模型做出的决策。在实践中，我们将能够完全理解我们的准确率和错误来源，模型擅长处理的数据类型，以及特征值如何影响输出。Scikit learn的可视化工具是可视化和理解决策树的绝佳选择。

+   **需要的 数据准备很少。** 许多机器学习模型可能需要大量的数据预处理，如归一化，并可能需要复杂的正则化方案。相比之下，决策树在调整少量参数后，开箱即用效果很好。

+   **使用树进行推理的成本与训练树所用的数据点数量的对数成正比。** 这是一大优势，因为这意味着增加数据不会显著影响我们的推理速度。

**缺点**

+   决策树由于其训练性质，过拟合非常常见。通常建议进行一些类型的降维，例如 [PCA](https://towardsdatascience.com/principal-component-analysis-your-tutorial-and-code-9719d3d3f376)，以便树不必在如此多的特征上学习分裂。

+   由于与过拟合类似的原因，决策树也容易对数据集中占多数的类别产生偏倚。通常，进行一些类型的 [class balancing](https://towardsdatascience.com/handling-imbalanced-datasets-in-deep-learning-f48407a0e758) 如类别权重、采样或专门的损失函数是一个好主意。

### 想要学习？

在[twitter](https://twitter.com/GeorgeSeif94)上关注我，了解最新最棒的人工智能、技术和科学！

**简介：[George Seif](https://towardsdatascience.com/@george.seif94)** 是一位认证的极客和人工智能/机器学习工程师。

[原文](https://towardsdatascience.com/a-guide-to-decision-trees-for-machine-learning-and-data-science-fe2607241956)。经许可转载。

**相关：**

+   [Python中的5种快速简易数据可视化方法（附代码）](/2018/07/5-quick-easy-data-visualizations-python-code.html)

+   [数据科学家需要了解的5种聚类算法](/2018/06/5-clustering-algorithms-data-scientists-need-know.html)

+   [使用Python在数据预处理上获得2–6倍的速度提升](/2018/10/get-speed-up-data-pre-processing-python.html)

### 更多相关内容

+   [停止学习数据科学以寻找目的，并找到目的去…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)

+   [学习数据科学统计学的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)

+   [成功数据科学家的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)

+   [每个数据科学家都应了解的三个R库（即使你使用Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)

+   [分析一个90亿美元的人工智能失败案例](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)

+   [是什么让Python成为初创企业的理想编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)
