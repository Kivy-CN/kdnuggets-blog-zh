- en: 'Ensemble Methods for Machine Learning: AdaBoost'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的集成方法：AdaBoost
- en: 原文：[https://www.kdnuggets.com/2019/09/ensemble-methods-machine-learning-adaboost.html](https://www.kdnuggets.com/2019/09/ensemble-methods-machine-learning-adaboost.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/09/ensemble-methods-machine-learning-adaboost.html](https://www.kdnuggets.com/2019/09/ensemble-methods-machine-learning-adaboost.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[comments](#comments)'
- en: '**By [Valentina Alto](https://www.linkedin.com/in/valentina-alto-6a0590148/),
    MSc in Data Science and Business Analytics**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由[Valentina Alto](https://www.linkedin.com/in/valentina-alto-6a0590148/)、数据科学与商业分析硕士提供**'
- en: '![Figure](../Images/26b9379c1fabe60e9ebe05ecbb777c55.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/26b9379c1fabe60e9ebe05ecbb777c55.png)'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的IT工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In statistics and machine learning, ensemble methods use multiple learning algorithms
    to obtain better predictive performance than could not be obtained from any of
    the constituent learning algorithms alone.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学和机器学习中，集成方法使用多个学习算法来获得比任何单一算法所能获得的更好的预测性能。
- en: The idea of combining multiple algorithms was first developed by computer scientist
    and Professor Michael Kerns, who was wondering whether* “weakly learnability is
    equivalent to strong learnability* “. The goal was turning a weak algorithm, barely
    better than random guessing, into a strong learning algorithm. It turned out that,
    if we ask the weak algorithm to create a whole bunch of classifiers (all weak
    for definition), and then combine them all, what may figure out is a stronger
    classifier.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 组合多种算法的想法最早由计算机科学家兼教授Michael Kerns提出，他在思考*“弱学习是否等同于强学习*”时产生了这个想法。目标是将一个几乎仅比随机猜测好一点的弱算法，转变为一个强学习算法。事实证明，如果我们要求弱算法创建一大堆分类器（这些分类器都很弱），然后将它们组合起来，最终可能会形成一个更强的分类器。
- en: '**AdaBoost**, which stays for ‘Adaptive Boosting’, is a machine learning meta-algorithm
    which can be used in conjunction with many other types of learning algorithms
    to improve performance.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**AdaBoost**，即“自适应提升”，是一种机器学习元算法，可以与许多其他类型的学习算法一起使用，以提高性能。'
- en: In this article, I’m going to provide an idea of the maths behind Adaboost,
    plus I’ll provide an implementation in Python.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将提供AdaBoost背后的数学概念，并提供一个Python实现。
- en: Intuition and Maths behind AdaBoost
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AdaBoost背后的直觉和数学
- en: Imagine we have our sample, divided into a training set and test set, and a
    bunch of classifiers. Each of them is trained on a random subset of the training
    set (note that those subsets can actually overlap-it is not the same as, for example,
    cross-validation). Then, for each observation in each subset, AdaBoost assigns
    a weight, which determines the probability that this observation will appear in
    the training set. Observations with higher weights are more likely to be included
    in the training set. Hence, AdaBoost tends to assign higher weights to those observations
    which have been misclassified, so that they will represent a larger part of the
    next classifiers training set, with the aim that, this time, the next classifier
    trained will perform better on them.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下我们有一个样本，分为训练集和测试集，还有一堆分类器。每个分类器都在训练集的随机子集上进行训练（注意这些子集可以重叠——这与交叉验证不同）。然后，对于每个子集中的每个观察，AdaBoost分配一个权重，这决定了这个观察出现在训练集中的概率。权重较高的观察更可能被包括在训练集中。因此，AdaBoost倾向于给被错误分类的观察分配更高的权重，使其在下一个分类器的训练集中占据更大的比例，目的是让下一个训练出来的分类器在这些观察上表现更好。
- en: 'Considering a simple binary classification problem, whose targets are represented
    by the signs ‘positive’ or ‘negative’ (expressed as 1 and -1), the equation of
    the final classifier looks like that:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个简单的二分类问题，其目标由“正面”或“负面”符号（表示为1和-1）表示，最终分类器的方程如下：
- en: '![](../Images/c03f7c3b8451945e633f3a7f3cf98e2e.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c03f7c3b8451945e633f3a7f3cf98e2e.png)'
- en: Basically, the output of the final classifier for observation x is equal to
    sign of the weighted sum of the outputs h_t(x) of T weak classifiers, with weights
    equal to α_t.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，最终分类器对观测值x的输出等于T个弱分类器输出h_t(x)的加权和的符号，其中权重等于α_t。
- en: 'More specifically, α_t is the weight assigned to the output of classifier t
    (note that this weight is different from that which will be assigned to observations,
    which will be discussed later on). It is computed as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，α_t是分配给分类器t输出的权重（注意，这个权重与分配给观测值的权重不同，后续将讨论）。它的计算方法如下：
- en: '![](../Images/41d5e1430758a1697ef0c4497fdf9154.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41d5e1430758a1697ef0c4497fdf9154.png)'
- en: 'Where ε_t is the error term of classifier t (misclassified observations/total
    observations). Of course, classifiers with low error will be prioritized in the
    sum, hence their weight will be higher. Indeed, if we look at the alpha corresponding
    to different errors:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其中ε_t是分类器t的误差项（分类错误的观测值/总观测值）。当然，低误差的分类器在加权和中会被优先考虑，因此它们的权重会更高。确实，如果我们查看与不同误差对应的alpha：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/43ba9afe4690ce8223532df31f52ab79.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43ba9afe4690ce8223532df31f52ab79.png)'
- en: As you can see, the classifier’s weight grows exponentially as the error approaches
    0, while it grows exponentially negative as the error approaches 1.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，当误差接近0时，分类器的权重呈指数增长，而当误差接近1时，权重则呈指数下降。
- en: 'The value of alpha is then used to compute the other type of weights, those
    that will be attributed to the observations of the subset. For the first classifier,
    the weights are equally initialized, so that each observation has a weight=1/n
    (where n=size of the subset). From the second classifier, each weight is recursively
    computed as:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，alpha的值用于计算另一种类型的权重，即将分配给子集观测值的权重。对于第一个分类器，权重被初始化为相等，因此每个观测值的权重为1/n（其中n为子集的大小）。从第二个分类器开始，每个权重递归计算如下：
- en: '![](../Images/cac8c965366dafdd076e5fa69a729af6.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cac8c965366dafdd076e5fa69a729af6.png)'
- en: Where y_t is the target value (either 1 or -1) and the variable w_t is a vector
    of weights, with one weight for each training example in the training set. ‘i’
    is the training example number. This equation shows you how to update the weight
    for the ith training example.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中y_t是目标值（1或-1），变量w_t是一个权重向量，每个训练示例在训练集中都有一个权重。‘i’是训练示例的编号。这个方程展示了如何更新第i个训练示例的权重。
- en: 'We can look at w_t as a distribution: it is consistent with what we said at
    the beginning, that is, weights represent the probability that training example
    will be selected as part of the training set.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将w_t视为一个分布：这与我们一开始所说的一致，即权重代表了训练示例被选择作为训练集一部分的概率。
- en: To make it a distribution, all of these probabilities should add up to 1\. To
    ensure this, we normalize the weights by dividing each of them by the sum of all
    the weights, Z_t.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其成为一个分布，所有这些概率之和应等于1。为确保这一点，我们通过将每个权重除以所有权重的总和Z_t来规范化权重。
- en: 'Let’s interpret this formula. Since we are dealing with binary classification
    (-1 vs 1), the product y_t*h_t(x_i) will be positive if both the actual and fitted
    values have the same sign (well-classified), negative if they have different signs
    (misclassified). Hence:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解释这个公式。由于我们处理的是二分类问题（-1对1），如果实际值和拟合值具有相同的符号（分类正确），则乘积y_t*h_t(x_i)为正；如果它们具有不同的符号（分类错误），则乘积为负。因此：
- en: if the product is positive and alpha is greater than zero (strong classifier),
    the weight attributed to the ith observation will be small
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果乘积为正且alpha大于零（强分类器），则分配给第i个观测值的权重将较小。
- en: if the product is positive and alpha is less than zero (weak classifier), the
    weight attributed to the ith observation will be high
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果乘积为正且alpha小于零（弱分类器），则分配给第i个观测值的权重将较高。
- en: if the product is negative and alpha is greater than zero (strong classifier),
    the weight attributed to the ith observation will be high
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果乘积为负且alpha大于零（强分类器），则分配给第i个观测值的权重将较高。
- en: if the product is negative and alpha is less than zero (weak classifier), the
    weight attributed to the ith observation will be small
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果产品为负且 alpha 小于零（弱分类器），则分配给第 i 个观测值的权重将很小
- en: 'Note: when I say ‘small’ and ‘high’ I’m mean, if we consider the exponential
    before normalization, respectively less than 1 and greater than 1\. Indeed:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：当我说“少”和“高”时，我是指在归一化之前的指数，分别小于 1 和大于 1。实际上：
- en: '![](../Images/2905ba18f9999d808b571fddea1c10b5.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2905ba18f9999d808b571fddea1c10b5.png)'
- en: Now that we have an idea of how AdaBoost works, let’s see its implementation
    in Python using the well-known Iris Dataset.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 AdaBoost 的工作原理，让我们看看在 Python 中如何使用著名的鸢尾花数据集来实现它。
- en: Implementation with Python
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Python 实现
- en: 'Let’s first import our data:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先导入数据：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The default algorithm in AdaBoost is decision tree, but you can decide to manually
    set a different classifier. Here, I’m going to use the Support Vector Machine
    classifier (you can read more about SVM [here](https://medium.com/swlh/support-vector-machine-from-scratch-ce095a47dc5c)).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 的默认算法是决策树，但你可以选择手动设置不同的分类器。在这里，我将使用支持向量机分类器（你可以在[这里](https://medium.com/swlh/support-vector-machine-from-scratch-ce095a47dc5c)阅读更多关于
    SVM 的内容）。
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, almost 98% of our observations have been correctly classified.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们的几乎 98% 的观测值被正确分类。
- en: Conclusions
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: AdaBoost is a technique easy to implement. It iteratively corrects the mistakes
    of the weak classifier improving accuracy. However, it is not free from *caveats*.
    Indeed, since it looks for a reduction in the training error, it is particularly
    sensitive to outliers, with the risk of producing an overfitted model which won’t
    fit new, unlabeled data well, since it lacks of generalization (you can read more
    about overfitting [here](https://towardsdatascience.com/preventing-overfitting-regularization-5eda7d5753bc)).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 是一种容易实现的技术。它通过迭代修正弱分类器的错误来提高准确性。然而，它并非没有*缺陷*。事实上，由于它寻求减少训练误差，因此它对离群点特别敏感，存在产生过拟合模型的风险，这样的模型对新的、未标记的数据适应性差，因为它缺乏泛化能力（你可以在[这里](https://towardsdatascience.com/preventing-overfitting-regularization-5eda7d5753bc)阅读更多关于过拟合的内容）。
- en: '*Originally published at *[*http://datasciencechalktalk.com*](https://datasciencechalktalk.com/2019/09/07/ensemble-methods-for-machine-learning-adaboost/)* on
    September 7, 2019.*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*最初发布于 *[*http://datasciencechalktalk.com*](https://datasciencechalktalk.com/2019/09/07/ensemble-methods-for-machine-learning-adaboost/)* 于
    2019 年 9 月 7 日。*'
- en: '**Bio: [Valentina Alto](https://www.linkedin.com/in/valentina-alto-6a0590148/)**
    is a Machine Learning and Statistics enthusiast, currently pursuing a MSc in Data
    Science at Bocconi University.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[Valentina Alto](https://www.linkedin.com/in/valentina-alto-6a0590148/)**
    是一位机器学习和统计学爱好者，目前在博科尼大学攻读数据科学硕士学位。'
- en: '[Original](https://medium.com/@valentinaalto/ensemble-methods-for-machine-learning-adaboost-2ff70d5518f3).
    Reposted with permission.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/@valentinaalto/ensemble-methods-for-machine-learning-adaboost-2ff70d5518f3)。经许可转载。'
- en: '**Related:**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Intuitive Ensemble Learning Guide with Gradient Boosting](/2018/07/intuitive-ensemble-learning-guide-gradient-boosting.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[直观的集成学习指南：梯度提升](/2018/07/intuitive-ensemble-learning-guide-gradient-boosting.html)'
- en: '[Ensemble Learning: 5 Main Approaches](/2019/01/ensemble-learning-5-main-approaches.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集成学习：5 种主要方法](/2019/01/ensemble-learning-5-main-approaches.html)'
- en: '[Understanding Gradient Boosting Machines](/2019/02/understanding-gradient-boosting-machines.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解梯度提升机](/2019/02/understanding-gradient-boosting-machines.html)'
- en: More On This Topic
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Implementing Adaboost in Scikit-learn](https://www.kdnuggets.com/2022/10/implementing-adaboost-scikitlearn.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Scikit-learn 中实现 Adaboost](https://www.kdnuggets.com/2022/10/implementing-adaboost-scikitlearn.html)'
- en: '[Ensemble Learning with Examples](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[带实例的集成学习](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
- en: '[Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集成学习技术：Python 中随机森林的详细讲解](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
- en: '[When Would Ensemble Techniques be a Good Choice?](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[何时使用集成技术是一个好选择？](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
- en: '[Alternative Feature Selection Methods in Machine Learning](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中的替代特征选择方法](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
- en: '[Centroid Initialization Methods for k-means Clustering](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[k-means 聚类的质心初始化方法](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)'
