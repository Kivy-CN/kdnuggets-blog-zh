["```py\n!gdown --id 1OlcvGWReJMuyYQuOZm149vHWwPtlboR6 --output train.csv\n!gdown --id 1Oi5cRlTybuIF2Fl5Bfsr-KkqrXrdt77w --output valid.csv\n!gdown --id 1ep9H6-HvhB4utJRLVcLzieWNUSG3P_uF --output test.csv\n```", "```py\ntrain = pd.read_csv(\"train.csv\")\nvalid = pd.read_csv(\"valid.csv\")\ntest = pd.read_csv(\"test.csv\")\n\ntrain = train.append(valid).reset_index(drop=True)\n```", "```py\n!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n!unzip uncased_L-12_H-768_A-12.zip\n```", "```py\ntokenizer = FullTokenizer(\n  vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\")\n)\n```", "```py\ntokenizer.tokenize(\"I can't wait to visit Bulgaria again!\")\n```", "```py\n['i', 'can', \"'\", 't', 'wait', 'to', 'visit', 'bulgaria', 'again', '!']\n```", "```py\ntokens = tokenizer.tokenize(\"I can't wait to visit Bulgaria again!\")\ntokenizer.convert_tokens_to_ids(tokens)\n```", "```py\n[1045, 2064, 1005, 1056, 3524, 2000, 3942, 8063, 2153, 999]\n```", "```py\nclass IntentDetectionData:\n  DATA_COLUMN = \"text\"\n  LABEL_COLUMN = \"intent\"\n\n  def __init__(\n    self,\n    train,\n    test,\n    tokenizer: FullTokenizer,\n    classes,\n    max_seq_len=192\n  ):\n    self.tokenizer = tokenizer\n    self.max_seq_len = 0\n    self.classes = classes\n\n    ((self.train_x, self.train_y), (self.test_x, self.test_y)) =\\\n     map(self._prepare, [train, test])\n\n    print(\"max seq_len\", self.max_seq_len)\n    self.max_seq_len = min(self.max_seq_len, max_seq_len)\n    self.train_x, self.test_x = map(\n      self._pad,\n      [self.train_x, self.test_x]\n    )\n\n  def _prepare(self, df):\n    x, y = [], []\n\n    for _, row in tqdm(df.iterrows()):\n      text, label =\\\n       row[IntentDetectionData.DATA_COLUMN], \\\n       row[IntentDetectionData.LABEL_COLUMN]\n      tokens = self.tokenizer.tokenize(text)\n      tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n      token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n      self.max_seq_len = max(self.max_seq_len, len(token_ids))\n      x.append(token_ids)\n      y.append(self.classes.index(label))\n\n    return np.array(x), np.array(y)\n\n  def _pad(self, ids):\n    x = []\n    for input_ids in ids:\n      input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n      input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n      x.append(np.array(input_ids))\n    return np.array(x)\n```", "```py\ndef create_model(max_seq_len, bert_ckpt_file):\n\n  with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n      bc = StockBertConfig.from_json_string(reader.read())\n      bert_params = map_stock_config_to_params(bc)\n      bert_params.adapter_size = None\n      bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n\n  input_ids = keras.layers.Input(\n    shape=(max_seq_len, ),\n    dtype='int32',\n    name=\"input_ids\"\n  )\n  bert_output = bert(input_ids)\n\n  print(\"bert shape\", bert_output.shape)\n\n  cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n  cls_out = keras.layers.Dropout(0.5)(cls_out)\n  logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n  logits = keras.layers.Dropout(0.5)(logits)\n  logits = keras.layers.Dense(\n    units=len(classes),\n    activation=\"softmax\"\n  )(logits)\n\n  model = keras.Model(inputs=input_ids, outputs=logits)\n  model.build(input_shape=(None, max_seq_len))\n\n  load_stock_weights(bert, bert_ckpt_file)\n\n  return model\n```", "```py\nclasses = train.intent.unique().tolist()\n\ndata = IntentDetectionData(\n  train,\n  test,\n  tokenizer,\n  classes,\n  max_seq_len=128\n)\n```", "```py\nmodel = create_model(data.max_seq_len, bert_ckpt_file)\n```", "```py\nmodel.summar()\n```", "```py\nmodel.compile(\n  optimizer=keras.optimizers.Adam(1e-5),\n  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n  metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n)\n```", "```py\nlog_dir = \"log/intent_detection/\" +\\\n datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\")\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n\nmodel.fit(\n  x=data.train_x,\n  y=data.train_y,\n  validation_split=0.1,\n  batch_size=16,\n  shuffle=True,\n  epochs=5,\n  callbacks=[tensorboard_callback]\n)\n```", "```py\n_, train_acc = model.evaluate(data.train_x, data.train_y)\n_, test_acc = model.evaluate(data.test_x, data.test_y)\n\nprint(\"train acc\", train_acc)\nprint(\"test acc\", test_acc)\n```", "```py\ntrain acc 0.9915119\ntest acc 0.9771429\n```", "```py\nsentences = [\n  \"Play our song now\",\n  \"Rate this book as awful\"\n]\n\npred_tokens = map(tokenizer.tokenize, sentences)\npred_tokens = map(lambda tok: [\"[CLS]\"] + tok + [\"[SEP]\"], pred_tokens)\npred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))\n\npred_token_ids = map(\n  lambda tids: tids +[0]*(data.max_seq_len-len(tids)),\n  pred_token_ids\n)\npred_token_ids = np.array(list(pred_token_ids))\n\npredictions = model.predict(pred_token_ids).argmax(axis=-1)\n\nfor text, label in zip(sentences, predictions):\n  print(\"text:\", text, \"\\nintent:\", classes[label])\n  print()\n```", "```py\ntext: Play our song now\nintent: PlayMusic\n\ntext: Rate this book as awful\nintent: RateBook\n```"]