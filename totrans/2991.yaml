- en: Deep Multi-Task Learning – 3 Lessons Learned
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度多任务学习 – 3个学到的经验
- en: 原文：[https://www.kdnuggets.com/2019/02/deep-multi-task-learning.html](https://www.kdnuggets.com/2019/02/deep-multi-task-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/02/deep-multi-task-learning.html](https://www.kdnuggets.com/2019/02/deep-multi-task-learning.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Zohar Komarovsky](https://www.linkedin.com/in/zohar-komarovsky-7773b9b/),
    Taboola**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Zohar Komarovsky](https://www.linkedin.com/in/zohar-komarovsky-7773b9b/)，Taboola**。'
- en: For the past year, my team and I have been working on a personalized user experience
    in the [Taboola feed](https://blog.taboola.com/taboola-feed/). We used [Multi-Task
    Learning](http://ruder.io/multi-task) (MTL) to predict multiple Key Performance
    Indicators (KPIs) on the same set of input features, and implemented a Deep Learning
    (DL) model in TensorFlow to do so. Back when we started, MTL seemed way more complicated
    to us than it does now, so I wanted to share some of the lessons learned.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的一年里，我和我的团队一直在[Taboola feed](https://blog.taboola.com/taboola-feed/)上致力于个性化用户体验。我们使用了[多任务学习](http://ruder.io/multi-task)（MTL）来预测同一输入特征集上的多个关键绩效指标（KPI），并实现了一个在TensorFlow中进行深度学习（DL）的模型。刚开始时，MTL对我们来说似乎复杂得多，因此我想分享一些学到的经验。
- en: There are already quite a few posts about implementing MTL in a DL model ([1](https://jg8610.github.io/Multi-Task), [2](https://medium.com/@kajalgupta/multi-task-learning-with-deep-neural-networks-7544f8b7b4e3), [3](https://hanxiao.github.io/2017/07/07/Get-10x-Speedup-in-Tensorflow-Multi-Task-Learning-using-Python-Multiprocessing)).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有不少关于在深度学习模型中实现MTL的帖子（[1](https://jg8610.github.io/Multi-Task)， [2](https://medium.com/@kajalgupta/multi-task-learning-with-deep-neural-networks-7544f8b7b4e3)，
    [3](https://hanxiao.github.io/2017/07/07/Get-10x-Speedup-in-Tensorflow-Multi-Task-Learning-using-Python-Multiprocessing)）。
- en: '**Sharing is caring**'
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**分享即关怀**'
- en: We wanted to start with the basic approach of [hard parameter sharing](http://ruder.io/multi-task/index.html#hardparametersharing).
    Hard sharing means we have a shared subnet, followed by task-specific subnets.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想从[硬参数共享](http://ruder.io/multi-task/index.html#hardparametersharing)的基本方法开始。硬共享意味着我们有一个共享的子网络，随后是特定于任务的子网络。
- en: '![](../Images/80214669e0ea6d08cccad7cb7c411c68.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80214669e0ea6d08cccad7cb7c411c68.png)'
- en: An easy way to start playing with such a model in TensorFlow is using [Estimators](https://www.youtube.com/watch?v=4oNdaQk0Qv4) with [multiple
    heads](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/multi_head).
    Since it doesn’t look that different than other NN architectures, you may ask
    yourself what could go wrong?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中开始玩这样的模型的一种简单方法是使用[估算器](https://www.youtube.com/watch?v=4oNdaQk0Qv4)和[多个头](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/multi_head)。由于它看起来与其他神经网络架构没有太大不同，你可能会问什么会出错？
- en: '**Lesson 1 – Combining losses**'
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**第1课 – 组合损失**'
- en: The first challenge we encountered with our MTL model, was defining a single
    loss function for multiple tasks. While a single task has a well defined loss
    function, with multiple tasks come multiple losses.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在MTL模型中遇到的第一个挑战是为多个任务定义一个单一的损失函数。虽然单个任务有一个明确的损失函数，但多个任务则有多个损失。
- en: The first thing we tried was simply to sum the different losses. Soon enough
    we could see that while one task converges to good results, the others look pretty
    bad. When taking a closer look, we could easily see why. The losses’ scales were
    so different, that one task dominated the overall loss, while the rest of the
    tasks didn’t have a chance to affect the learning process of the shared layers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试的第一种方法是简单地将不同的损失进行求和。很快我们发现，虽然一个任务收敛到良好的结果，其他任务却表现得相当差。当我们仔细观察时，我们可以很容易地看到原因。损失的尺度差异如此之大，以至于一个任务主导了整体损失，而其他任务没有机会影响共享层的学习过程。
- en: A quick fix was replacing the sum of losses with a weighted sum, that brought
    all losses to approximately the same scale. However, this solution involves another
    hyperparameter that might need to be tuned every once in a while.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个快速的解决方法是用加权和代替损失之和，这将所有损失调整到大致相同的尺度。然而，这个解决方案涉及到另一个可能需要不时调整的超参数。
- en: Luckily, we found a [great paper](https://arxiv.org/abs/1705.07115) proposing
    to use uncertainty to weigh losses in MTL. The way it is done, is by learning
    another noise parameter that is integrated in the loss function for each task.
    This allows having multiple tasks, possibly regression and classification, and
    bringing all losses to the same scale. Now we could go back to simply summing
    our losses.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们找到了一篇[很棒的论文](https://arxiv.org/abs/1705.07115)，提出了在MTL中使用不确定性来加权损失。它的做法是，通过学习另一个噪声参数，并将其集成到每个任务的损失函数中。这允许处理多个任务，可能是回归和分类，并将所有损失带到相同的尺度。现在我们可以回到简单地求和我们的损失。
- en: Not only did we get better results than with a weighted sum, we could forget
    about the additional weights hyperparameters. [Here](https://github.com/yaringal/multi-task-learning-example/blob/master/multi-task-learning-example.ipynb) is
    a Keras implementation provided by the authors of the paper.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅得到了比加权和更好的结果，而且可以忽略额外的权重超参数。[这里](https://github.com/yaringal/multi-task-learning-example/blob/master/multi-task-learning-example.ipynb)是论文作者提供的Keras实现。
- en: '**Lesson 2 – Tuning learning rates**'
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**第2课 – 调整学习率**'
- en: It’s a common convention that learning rate is one of the most important hyperparameters
    for [tuning neural networks](https://engineering.taboola.com/hitchhikers-guide-hyperparameter-tuning/).
    So we tried tuning, and found a learning rate that looked really good for task
    A, and another one that was really good for task B. Choosing the higher rate caused [dying
    Relu’s](https://www.quora.com/What-is-the-dying-ReLU-problem-in-neural-networks) on
    one of the tasks, while using the lower one brought a slow convergence on the
    other task. Then what could we do? We could tune a separate learning rate for
    each of the “heads” (task-specific subnets), and another rate for the shared subnet.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率是[调整神经网络](https://engineering.taboola.com/hitchhikers-guide-hyperparameter-tuning/)中最重要的超参数之一，这是一个常见的约定。所以我们尝试了调整，并找到了一种对任务A非常有效的学习率，以及一种对任务B非常有效的学习率。选择较高的学习率会导致[死ReLU问题](https://www.quora.com/What-is-the-dying-ReLU-problem-in-neural-networks)出现在一个任务上，而使用较低的学习率则会导致另一个任务收敛缓慢。那么我们能做什么呢？我们可以为每个“头”（任务特定子网）调整一个单独的学习率，并为共享子网调整另一个学习率。
- en: 'Though it may sound complicated, it’s actually pretty simple. Usually when
    training a NN in TensorFlow you use something like:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然听起来可能很复杂，但实际上非常简单。通常在TensorFlow中训练神经网络时，你会使用类似的东西：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*AdamOptimizer* defines how gradients should be applied, and *minimize* computes
    and applies them. We can replace *minimize* with our own implementation that would
    use the appropriate learning rate for each variable in our computational graph
    when applying the gradients:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*AdamOptimizer*定义了如何应用梯度，而*minimize*计算并应用它们。我们可以用自己的实现替代*minimize*，在应用梯度时使用计算图中每个变量的适当学习率：'
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: By the way, this trick can actually also be useful for single-task networks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，这个技巧实际上对单任务网络也有用。
- en: '**Lesson 3 – Using estimates as features**'
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**第3课 – 将估计值用作特征**'
- en: Once we’re past the first phase of creating a NN that predicts multiple tasks,
    we might want to use our estimate for one task as a feature to another. In the
    forward-pass that’s really easy. The estimate is a Tensor, so we can wire it just
    like any other layer’s output. But what happens in backprop?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成了创建一个预测多个任务的神经网络的第一阶段，我们可能想要将一个任务的估计值用作另一个任务的特征。在前向传播中这很简单。估计值是一个张量，因此我们可以像连接其他层的输出一样连接它。但反向传播中会发生什么？
- en: Say the estimate for task A is passed as a feature to task B. We probably wouldn’t
    want to propagate the gradients from task B back to task A, as we already have
    a label for A.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 假设任务A的估计值被作为特征传递给任务B。我们可能不希望将任务B的梯度反向传播到任务A，因为我们已经有了任务A的标签。
- en: Don’t worry, TensorFlow’s API has [tf.stop_gradient](https://www.tensorflow.org/api_docs/python/tf/stop_gradient) just
    for that reason. When computing the gradients, it lets you pass a list of Tensors
    you wish to treat as constants, which is exactly what we need.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 不用担心，TensorFlow的API有[tf.stop_gradient](https://www.tensorflow.org/api_docs/python/tf/stop_gradient)正是为此目的。当计算梯度时，它允许你传递一个张量列表，你希望将其视为常量，这正是我们需要的。
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Again, this is useful in MTL networks, but not only. This technique can be used
    whenever you want to compute a value with TensorFlow, and need to pretend that
    the value was a constant. For example, when training Generative Adversarial Networks
    (GANs), you don’t want to backprop through the generation process of the adversarial
    example.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这在 MTL 网络中很有用，但不仅限于此。这个技术可以用于任何时候你想用 TensorFlow 计算一个值，并且需要假装这个值是一个常量。例如，在训练生成对抗网络
    (GANs) 时，你不希望通过对抗样本的生成过程进行反向传播。
- en: '**So, what’s next?**'
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**那么，接下来是什么？**'
- en: Our models are up and running and Taboola feed is being personalized. However,
    there is still a lot of room for improvement, and lots of interesting architectures
    to explore. In our use case, predicting multiple tasks also means we make a decision
    based on multiple KPIs. That can be a bit more tricky than using a single KPI…
    but that’s already a whole new topic.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型已上线运行，Taboola 的信息流正在个性化。然而，仍然有很多改进空间，还有许多有趣的架构可以探索。在我们的用例中，预测多个任务也意味着我们要基于多个
    KPI 做出决策。这可能比使用单一 KPI 更具挑战性……但这已经是另一个全新的话题。
- en: Thanks for reading, I hope you found this post useful!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读，希望你觉得这篇文章有用！
- en: '**Bio**: [Zohar Komarovsky](https://www.linkedin.com/in/zohar-komarovsky-7773b9b/)
    is an Algorithms Developer at Taboola and works on Machine Learning applications
    for Recommendation Systems.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介**： [Zohar Komarovsky](https://www.linkedin.com/in/zohar-komarovsky-7773b9b/)
    是 Taboola 的算法开发者，专注于推荐系统的机器学习应用。'
- en: '[Original](https://engineering.taboola.com/deep-multi-task-learning-3-lessons-learned).
    Reposted with permission.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://engineering.taboola.com/deep-multi-task-learning-3-lessons-learned)。已获得许可重新发布。'
- en: '**Resources:**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：**'
- en: '[On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning
    education](https://www.kdnuggets.com/education/online.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在线和基于网络的：分析、数据挖掘、数据科学、机器学习教育](https://www.kdnuggets.com/education/online.html)'
- en: '[Software for Analytics, Data Science, Data Mining, and Machine Learning](https://www.kdnuggets.com/software/index.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分析、数据科学、数据挖掘和机器学习的软件](https://www.kdnuggets.com/software/index.html)'
- en: '**Related:**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Trending Deep Learning Github Repositories](https://www.kdnuggets.com/2019/02/trending-top-deep-learning-github-repositories.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[热门深度学习 GitHub 仓库](https://www.kdnuggets.com/2019/02/trending-top-deep-learning-github-repositories.html)'
- en: '[What were the most significant machine learning/AI advances in 2018?](https://www.kdnuggets.com/2019/01/machine-learning-ai-advances-2018.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2018年最重要的机器学习/人工智能进展是什么？](https://www.kdnuggets.com/2019/01/machine-learning-ai-advances-2018.html)'
- en: '[NLP Overview: Modern Deep Learning Techniques Applied to Natural Language
    Processing](https://www.kdnuggets.com/2019/01/nlp-overview-modern-deep-learning-techniques.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NLP 概述：应用于自然语言处理的现代深度学习技术](https://www.kdnuggets.com/2019/01/nlp-overview-modern-deep-learning-techniques.html)'
- en: '* * *'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的捷径。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT'
- en: '* * *'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[What I Learned From Using ChatGPT for Data Science](https://www.kdnuggets.com/what-i-learned-from-using-chatgpt-for-data-science)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我从使用 ChatGPT 进行数据科学中学到了什么](https://www.kdnuggets.com/what-i-learned-from-using-chatgpt-for-data-science)'
- en: '[Ten Key Lessons of Implementing Recommendation Systems in Business](https://www.kdnuggets.com/2022/07/ten-key-lessons-implementing-recommendation-systems-business.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实施推荐系统在商业中的十个关键经验](https://www.kdnuggets.com/2022/07/ten-key-lessons-implementing-recommendation-systems-business.html)'
- en: '[Lessons from a Senior Data Scientist](https://www.kdnuggets.com/2022/09/lessons-senior-data-scientist.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[来自高级数据科学家的经验教训](https://www.kdnuggets.com/2022/09/lessons-senior-data-scientist.html)'
- en: '[KDnuggets News, September 28: Free Algorithms in Python Course •…](https://www.kdnuggets.com/2022/n38.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，9月28日：Python 免费算法课程 •…](https://www.kdnuggets.com/2022/n38.html)'
- en: '[4 Career Lessons That Helped Me Navigate the Difficult Job Market](https://www.kdnuggets.com/2023/05/4-lessons-made-difference-navigating-current-job-market.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4 个帮助我应对困难就业市场的职业教训](https://www.kdnuggets.com/2023/05/4-lessons-made-difference-navigating-current-job-market.html)'
- en: '[A Solid Plan for Learning Data Science, Machine Learning, and Deep Learning](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学、机器学习和深度学习的可靠计划](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
