- en: Deep Learning-based Real-time Video Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的实时视频处理
- en: 原文：[https://www.kdnuggets.com/2021/02/deep-learning-based-real-time-video-processing.html](https://www.kdnuggets.com/2021/02/deep-learning-based-real-time-video-processing.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/02/deep-learning-based-real-time-video-processing.html](https://www.kdnuggets.com/2021/02/deep-learning-based-real-time-video-processing.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Serhii Maksymenko](https://www.linkedin.com/in/sergey-maximenko/), AI/ML
    Solution Architect at [MobiDev](https://mobidev.biz/services/machine-learning-consulting)**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者 [Serhii Maksymenko](https://www.linkedin.com/in/sergey-maximenko/)，AI/ML解决方案架构师，来自
    [MobiDev](https://mobidev.biz/services/machine-learning-consulting)**'
- en: '**What is Serial Video Processing**'
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**什么是串行视频处理**'
- en: Video data is a common asset which is used everyday, whether it is a live stream
    in a personal blog or security camera in a manufacturing building. A machine learning
    appliance is becoming a normal tool for processing video in a variety of tasks.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 视频数据是日常使用的常见资产，无论是个人博客中的直播流，还是制造建筑中的安全摄像头。机器学习应用正成为处理各种视频任务的常用工具。
- en: '* * *'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织IT需求'
- en: '* * *'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The video processing approach in this article is considered within the context
    of a personal data security application which was developed by the MobiDev Team.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中讨论的视频处理方法是基于MobiDev团队开发的个人数据安全应用的背景下的。
- en: In a nutshell, video processing can be seen as a sequence of operations done
    for each frame. Each frame includes processes of decoding, computation and encoding.
    Decoding is a conversion of the video frame from compressed format to the raw
    format. Computation is a certain operation which we need to do with the frame.
    And encoding is conversion of the processed frame back to the compressed state.
    This is a serial process. Although such an approach looks straightforward and
    easy to implement, it is not practical in most cases.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，视频处理可以视为对每一帧进行的一系列操作。每帧包括解码、计算和编码过程。解码是将视频帧从压缩格式转换为原始格式。计算是对帧进行的某种操作。编码是将处理后的帧转换回压缩状态。这是一个串行过程。尽管这种方法看起来简单易行，但在大多数情况下并不实用。
- en: '![Figure](../Images/1a160098d81fe81c45136e4fb28e9f7e.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/1a160098d81fe81c45136e4fb28e9f7e.png)'
- en: Source [https://mobidev.biz/wp-content/uploads/2021/01/serial-video-processing.png](https://mobidev.biz/wp-content/uploads/2021/01/serial-video-processing.png)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 来源 [https://mobidev.biz/wp-content/uploads/2021/01/serial-video-processing.png](https://mobidev.biz/wp-content/uploads/2021/01/serial-video-processing.png)
- en: To make the above mentioned approach a workable one, it is necessary to set
    measurement criteria such as, speed, accuracy and flexibility. However, to get
    all these in one system might be  impossible. Usually there is a trade-off between
    speed and accuracy in such cases. If we choose not to pay attention to speed,
    we’ll have a nice algorithm, but it will be extremely slow, and as a result, useless.
    If we simplify the algorithm too much, then the results may not be as good as
    expected. Consequently, a fast and accurate white elephant is impossible to integrate.
    The solution is supposed to be flexible in terms of input, output and configuration.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使上述方法可行，需要设定如速度、准确性和灵活性等测量标准。然而，在一个系统中兼顾所有这些可能是不可能的。通常，在这种情况下，速度和准确性之间存在权衡。如果我们选择不关注速度，那么我们会有一个很好的算法，但它会极其缓慢，结果也就毫无用处。如果我们过度简化算法，那么结果可能不会如预期那样好。因此，快速且准确的“白象”是不可能集成的。解决方案应在输入、输出和配置方面具有灵活性。
- en: So, how to make the processing faster, while keeping the accuracy at a reasonably
    high level? There are two possible ways to solve the issue. The first one is to
    make the algorithms run in parallel, so they might be possible to keep using slower
    accurate models. The second one is to make a certain effort to accelerate the
    algorithms themselves or their parts with no significant loss of the accuracy.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何在保持较高准确性的同时加快处理速度呢？有两种可能的解决方法。第一种是使算法并行运行，这样可能可以继续使用较慢但准确的模型。第二种是对算法本身或其部分进行加速，同时尽可能减少准确性的损失。
- en: '**Parallel Processing Method**'
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**并行处理方法**'
- en: '**File splitting **'
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**文件拆分**'
- en: The first approach is obvious. The video is split into parts and processed in
    parallel. This is a simple and straightforward way. It can be implemented with
    the help of video time pointers. In this method, splitting is virtual, and is
    not a real sub-file generation. However, in the output, we anyway have multiple
    video files which need to be combined into a single one. This makes the approach
    limited or at least uncomfortable when it comes to the tasks involved, especially
    using neighbouring frames for calculation. In order to deal with such a thing,
    we had to split the video with an overlap, so any information won’t be missed
    at joint places.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方法是显而易见的。视频被拆分成多个部分并进行并行处理。这是一种简单直接的方法。它可以借助视频时间指针来实现。在这种方法中，拆分是虚拟的，并不是实际生成子文件。然而，在输出中，我们依然会有多个视频文件，这些文件需要合并成一个。这使得这种方法在涉及任务时有些受限，尤其是在计算时使用相邻帧。为了处理这种情况，我们不得不在视频拆分时添加重叠，以确保在接缝处不会丢失信息。
- en: '![Figure](../Images/3cb8aa6b74319548afb9ce390a4ccfec.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/3cb8aa6b74319548afb9ce390a4ccfec.png)'
- en: Source [https://mobidev.biz/wp-content/uploads/2021/01/file-splitting-video-processing.png](https://mobidev.biz/wp-content/uploads/2021/01/file-splitting-video-processing.png)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 来源 [https://mobidev.biz/wp-content/uploads/2021/01/file-splitting-video-processing.png](https://mobidev.biz/wp-content/uploads/2021/01/file-splitting-video-processing.png)
- en: Because of this, it is impossible to deal with a stream from a web camera, for
    example. The file splitting approach enables processing of video files only. Besides,
    it may be difficult to pause, resume or even move the processing at a different
    position in timespin with this method. Despite the simplicity of the file splitting,
    it doesn’t meet the flexibility requirements.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，例如处理来自网络摄像头的流是不可能的。文件拆分方法仅能处理视频文件。此外，这种方法可能难以暂停、恢复或甚至在时间轴上移动处理。尽管文件拆分方法简单，但它不符合灵活性的要求。
- en: '**Pipeline Architecture**'
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**管道架构**'
- en: Instead of splitting the video, the pipeline approach is aimed to split and
    parallelize the operations, which are performed during the processing. Basic operations
    for the usual video pipeline are decoding, computation and encoding. At least
    these three can be done in a concurrent way.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于拆分视频，管道方法旨在拆分并行化处理过程中执行的操作。通常视频管道的基本操作包括解码、计算和编码。至少这三项操作可以并发进行。
- en: Why is the pipeline approach more flexible? One of the benefits of the pipeline
    is the ease  with which you can manipulate the components due to requirements.
    Decoding can work by using a video file and encode frames into another file. That’s
    a common case.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么管道方法更灵活？管道方法的一个好处是可以根据需求轻松操作组件。解码可以使用视频文件，并将帧编码到另一个文件中。这是一种常见的情况。
- en: '![Figure](../Images/f339af0b6c6f2c7ce45ae20a996877e6.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/f339af0b6c6f2c7ce45ae20a996877e6.png)'
- en: Source [https://mobidev.biz/wp-content/uploads/2021/01/pipeline-approach-video-processing.png](https://mobidev.biz/wp-content/uploads/2021/01/pipeline-approach-video-processing.png)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 来源 [https://mobidev.biz/wp-content/uploads/2021/01/pipeline-approach-video-processing.png](https://mobidev.biz/wp-content/uploads/2021/01/pipeline-approach-video-processing.png)
- en: Alternatively, input can be RTSP streamed from an IP camera. Output can be through
    a WebRTC connection in the browser or mobile application. There is a unified architecture
    which is based on a video stream for all combinations of input and output formats.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，输入可以通过IP摄像头进行RTSP流传输。输出可以通过浏览器或移动应用程序中的WebRTC连接。所有输入和输出格式的组合都基于视频流的统一架构。
- en: The computation part of the process is not necessarily an atomic operation.
    It can be logically split into sub-operations which can create more pipeline stages.
    It is not always reasonable however, to create too many components because multiprocessing
    latency may fade out the gain in performance.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 过程的计算部分不一定是原子操作。它可以逻辑上拆分成子操作，从而创建更多的管道阶段。然而，创建过多的组件并不总是合理的，因为多线程处理的延迟可能会削弱性能的提升。
- en: Basically, the pipeline looks like exactly what we need in terms of flexibility.
    That’s not only because of  changing the input and output format, but also because
    we can stop and continue the processing at any time.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，管道在灵活性方面正是我们所需要的。这不仅仅是因为可以更改输入和输出格式，还因为我们可以随时暂停和继续处理。
- en: '**Optimizing Pipeline Components**'
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**优化管道组件**'
- en: '**Time Profiling**'
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**时间分析**'
- en: '![Figure](../Images/842677f7cb6785f72acf9036fa1fa1dc.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/842677f7cb6785f72acf9036fa1fa1dc.png)'
- en: Source [https://mobidev.biz/wp-content/uploads/2021/01/time-profiling-video-processing.png](https://mobidev.biz/wp-content/uploads/2021/01/time-profiling-video-processing.png)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 来源 [https://mobidev.biz/wp-content/uploads/2021/01/time-profiling-video-processing.png](https://mobidev.biz/wp-content/uploads/2021/01/time-profiling-video-processing.png)
- en: To understand what needs to be optimized, it is necessary to observe the time
    each stage of the pipeline takes. That way, it is easier to find out where the
    bottleneck is. The slowest pipeline stage defines the output FPS. It may be boring
    to do the measurements, but they can be automated without very much effort. The
    pipeline is using a queue for data exchange. Therefore, there will be a certain
    waiting time in the components. Usually, we need to write down how long each operation
    takes, how long each operation waits for the data from the previous component,
    and how long each operation waits for its result to send to the next component.
    Having this information simplifies performance analysis a lot.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解需要优化的部分，必须观察管道中每个阶段所需的时间。这样可以更容易找出瓶颈所在。最慢的管道阶段决定了输出的 FPS。尽管进行测量可能很乏味，但可以很容易地进行自动化。管道使用队列进行数据交换。因此，组件中会有一定的等待时间。通常，我们需要记录每个操作的耗时，每个操作等待从前一个组件获取数据的时间，以及每个操作等待其结果传递给下一个组件的时间。拥有这些信息可以大大简化性能分析。
- en: '**Decoding and Encoding **'
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**解码与编码**'
- en: 'Decoding and encoding can be done in 2 ways: software and hardware. Hardware
    acceleration is possible due to the decoder and encoder installed in NVIDIA cards
    in addition to CUDA cores. Here is a list of options which can be used for implementing
    the hardware acceleration of decoding and encoding.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 解码和编码可以通过两种方式完成：软件和硬件。硬件加速得益于 NVIDIA 卡中的解码器和编码器以及 CUDA 核心。以下是可以用于实现解码和编码硬件加速的选项列表。
- en: Compile OpenCV with CUDA support
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CUDA 支持编译 OpenCV
- en: Compile FFmpeg with NVDEC/NVENC codecs support
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编译支持 NVDEC/NVENC 编解码器的 FFmpeg
- en: Use GStreamer with NVDEC/NVENC codecs support
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用支持 NVDEC/NVENC 编解码器的 GStreamer
- en: Use NVIDIA Video Processing Framework
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 NVIDIA 视频处理框架
- en: '**Decoding and Encoding**'
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**解码与编码**'
- en: Compiling from the source may be non-trivial, but it should be straightforward
    if you follow the instructions carefully. This can be done in an isolated environment
    such as a docker container, so that it is easy to deploy it on other machines
    or in cloud instances and you do not need to repeat the compiling procedure each
    time.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从源代码编译可能比较复杂，但如果你仔细遵循说明，它应该是直接的。这可以在隔离的环境中进行，例如 Docker 容器中，这样在其他机器或云实例上部署就容易了，你也不需要每次都重复编译过程。
- en: Compiling OpenCV with CUDA can optimize not only decoding, but also other calculations
    in the pipeline if they are performed using OpenCV. However, they will need to
    be written in C++ I suppose, because Python wrapper has no solid support for it.
    But that will be worth it in situations where you’d like to perform both decoding
    and numeric calculations on a GPU without copying from CPU memory.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CUDA 编译 OpenCV 不仅可以优化解码，还可以优化管道中使用 OpenCV 执行的其他计算。然而，我认为它们需要用 C++ 编写，因为 Python
    包装器对此没有良好的支持。但在你希望在 GPU 上同时执行解码和数值计算而不从 CPU 内存中复制数据的情况下，这将是值得的。
- en: Custom installation of FFmpeg and GStreamer allows using a built-in NVIDIA decoder
    and encoder. It is suggested going with FFmpeg first, because that should be easier
    in terms of maintenance. Moreover, many libraries use FFmpeg under the hood, so 
    replacing it will automatically improve these libraries' performance.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义安装FFmpeg和GStreamer可以使用内置的NVIDIA解码器和编码器。建议先使用FFmpeg，因为从维护角度来看，这应该更容易。此外，许多库在底层使用FFmpeg，因此替换它将自动提高这些库的性能。
- en: Python wrapper gives a possibility to decode a frame right into a PyTorch tensor
    on GPU so that we can start inference without an extra copying from CPU to GPU.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Python封装器提供了将帧直接解码为GPU上的PyTorch张量的可能性，从而可以在不额外从CPU到GPU的复制的情况下开始推断。
- en: '**Computation (CPU)**'
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**计算（CPU）**'
- en: Use Mixed Precision technique
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用混合精度技术
- en: TensorRT
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorRT
- en: Resize before inference
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推断前调整大小
- en: Batch inference
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量推断
- en: The customers do not always have a GPU in the list of their hardware. Processing
    using a neural network is rare on a CPU, but it is still possible with certain
    limitations. It’s not appropriate to make large models for this purpose but more
    lightweight models will work. For example, in our work, we were able to get 30
    FPS with an SSD object detection model on the CPU. Latest versions of Tensorflow
    and PyTorch are optimized for running certain operations on multiple cores and
    they can be controlled with the help of parameters. So, the absence of GPU doesn’t
    mean that the mission is impossible. For instance, this can be the case if you
    are dealing with cloud computing with limited resources.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 客户的硬件列表中并不总是有GPU。使用神经网络进行处理在CPU上很少见，但在某些限制下仍然可能。为了这个目的，不适合制作大型模型，但更轻量级的模型会有效。例如，在我们的工作中，我们能够在CPU上使用SSD对象检测模型获得30
    FPS。Tensorflow和PyTorch的最新版本已针对在多个核心上运行某些操作进行了优化，并可以通过参数进行控制。因此，缺少GPU并不意味着任务不可能完成。例如，这可能是在资源有限的云计算环境中遇到的情况。
- en: But of course, a GPU is a must if you want a good model to work at your desired
    speed. We also applied certain techniques in order to speed-up GPU inference even
    more. In those cases, if your GPU supports fp16, it will be simple to apply mixed
    precision, which is part of the latest versions of PyTorch and TensorFlow. It
    enables use of fp16 precision for certain layers and fp32 for the rest, keeping
    the numerical stability of the network and therefore keeping the accuracy. An
    alternative and even more efficient way of model acceleration is TensorRT conversion.
    It is a more challenging method, but it can give 5x faster inference. In addition,
    there are other obvious optimization options such as resizing and batch inference.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 但当然，如果你想让一个好的模型以你期望的速度运行，GPU是必不可少的。我们还应用了一些技术，以进一步加快GPU推断速度。在这些情况下，如果你的GPU支持fp16，应用混合精度将非常简单，它是PyTorch和TensorFlow最新版本的一部分。它允许对某些层使用fp16精度，对其余层使用fp32，从而保持网络的数值稳定性并保持准确性。一个替代且更高效的模型加速方法是TensorRT转换。这是一种更具挑战性的方法，但它可以使推断速度提高5倍。此外，还有其他明显的优化选项，如调整大小和批量推断。
- en: '![Figure](../Images/f294c44f7b39169849067aca049c5bbf.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/f294c44f7b39169849067aca049c5bbf.png)'
- en: Source [https://mobidev.biz/wp-content/uploads/2021/01/flexibility-of-pipeline-approach.png](https://mobidev.biz/wp-content/uploads/2021/01/flexibility-of-pipeline-approach.png)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 来源 [https://mobidev.biz/wp-content/uploads/2021/01/flexibility-of-pipeline-approach.png](https://mobidev.biz/wp-content/uploads/2021/01/flexibility-of-pipeline-approach.png)
- en: '**How the Pipeline Approach Can Be Implemented in the Application**'
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**管道方法如何在应用中实现**'
- en: '![Figure](../Images/2b8692653886d76cc015d8a788e3e699.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/2b8692653886d76cc015d8a788e3e699.png)'
- en: Source [https://mobidev.biz/wp-content/uploads/2021/01/face-blurring-video-processing.png](https://mobidev.biz/wp-content/uploads/2021/01/face-blurring-video-processing.png)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 来源 [https://mobidev.biz/wp-content/uploads/2021/01/face-blurring-video-processing.png](https://mobidev.biz/wp-content/uploads/2021/01/face-blurring-video-processing.png)
- en: The flexibility of the system was essential in this case, because we aimed to
    process not only video files, but also different formats of video live-stream.
    It showed a good FPS in the range of 30-60, depending on the configuration used.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 系统的灵活性在这种情况下至关重要，因为我们旨在处理不仅是视频文件，还包括不同格式的视频直播。它在30-60的范围内显示了良好的FPS，具体取决于所使用的配置。
- en: 'Additionally, we applied some other improvements: interpolation with tracking,
    sharing memory between processes, and adding several workers for the pipeline
    component.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还应用了一些其他改进：带有跟踪的插值、进程之间共享内存，以及为管道组件添加多个工作线程。
- en: '**Interpolation with tracking**'
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**带有跟踪的插值**'
- en: '![Figure](../Images/5083d42b2cf5e5b0ca47f5ac8ca7457a.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/5083d42b2cf5e5b0ca47f5ac8ca7457a.png)'
- en: Source [https://mobidev.biz/wp-content/uploads/2021/01/interpolation-tracking-video-processing.png](https://mobidev.biz/wp-content/uploads/2021/01/interpolation-tracking-video-processing.png)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 来源 [https://mobidev.biz/wp-content/uploads/2021/01/interpolation-tracking-video-processing.png](https://mobidev.biz/wp-content/uploads/2021/01/interpolation-tracking-video-processing.png)
- en: You may be thinking, do we really need to make computation for each single frame?
    No, it is possible to skip frames with a certain interval, assuming that bonding
    boxes will be interpolated with the help of a tracking algorithm. An algorithm
    can be simple. For example, matching by the distance between centroids. That will
    be enough to have a pretty  good interpolation, and therefore much better FPS.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，我们真的需要对每一帧进行计算吗？不，实际上可以跳过某些间隔的帧，假设绑定框将通过跟踪算法进行插值。算法可以很简单。例如，通过质心之间的距离进行匹配。这将足以获得相当好的插值，从而实现更高的
    FPS。
- en: In the tracking stage, this is essential to use more complex tracking algorithms
    or correlation tracker. But they really impact the speed if there are too many
    faces on the video. For example, if it is a crowded street video, then we cannot
    afford the tracking algorithm to be too complex.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在跟踪阶段，使用更复杂的跟踪算法或相关跟踪器是至关重要的。但如果视频中面孔过多，它们确实会影响速度。例如，如果是拥挤的街道视频，我们不能承担过于复杂的跟踪算法。
- en: Since we skip some frames, we’d like to know what the quality of the interpolated
    frames is. Therefore, we calculated the F1 metric and ensured that we don’t have
    too many false positives and false negatives because of interpolation. F1 value
    was around 0,95 for most of the video examples.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们跳过了一些帧，我们希望了解插值帧的质量。因此，我们计算了 F1 指标，并确保插值不会导致过多的假阳性和假阴性。大多数视频示例的 F1 值约为 0.95。
- en: '**Sharing Memory **'
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**共享内存**'
- en: 'The next point is sharing memory between processes. An app can keep sending
    data through the queue but that was really slow. Alternative ways of sharing memory
    between processes in Python are tricky. Here are some of them:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 下一点是进程间的内存共享。一个应用可以不断通过队列发送数据，但这真的很慢。Python 中进程间内存共享的替代方法很棘手。以下是一些方法：
- en: torch.multiprocessing
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: torch.multiprocessing
- en: Linux/dev/shm
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linux/dev/shm
- en: SharedMemory in Python 3.8
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3.8 中的 SharedMemory
- en: 'The PyTorch version of multiprocessing has the ability to pass a tensor handle
    through the queue so that another process can just get a pointer to the existing
    GPU memory. However, we decided to use another approach: system level shared memory.
    This method uses a special folder as a link to a space in RAM memory. There are
    libraries which provide a Python interface for using this memory. It drastically
    improved the speed of interprocess communication. Python 3.8 API for shared memory
    is another possible approach.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 的多进程版本能够通过队列传递张量句柄，以便另一个进程可以直接获取指向现有 GPU 内存的指针。然而，我们决定使用另一种方法：系统级别的共享内存。这种方法使用一个特殊文件夹作为
    RAM 内存空间的链接。有一些库提供了用于使用此内存的 Python 接口。它极大地提高了进程间通信的速度。Python 3.8 的共享内存 API 是另一种可能的方法。
- en: '**Multiple Workers**'
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**多个工作线程**'
- en: Compare latency
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较延迟
- en: Take  into account CUDA streams
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑 CUDA 流
- en: Correct the order of frames
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更正帧的顺序
- en: Finally, we need to add several workers for a pipeline component. That’s something
    good to consider, but it is not always working correctly. We did that for the
    face detection stage. However, the same can be done for any computationally heavy
    operation which doesn’t require an ordered input. The thing is, that it really
    depends on the operations which are done inside. In cases where we had comparatively
    fast face detection, we got lower FPS after adding more detection workers. The
    time which is required for managing one more process can be bigger than the time
    we gained from it. Then, neural networks which are used in the multiple workers,
    will calculate tensors in a serial CUDA stream unless a separate stream is created
    for each network, which may be tricky.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要为管道组件添加几个工作线程。这是一个值得考虑的好方法，但并不总是能正确工作。我们在面部检测阶段做了这件事。然而，同样的方法也可以应用于任何计算密集型操作，只要不需要有序的输入。关键在于，实际效果取决于内部执行的操作。在面部检测相对较快的情况下，添加更多检测工作线程后，FPS
    反而降低了。管理一个额外进程所需的时间可能会大于从中获得的时间。然后，多个工作线程中使用的神经网络将以串行 CUDA 流计算张量，除非为每个网络创建一个单独的流，这可能会很棘手。
- en: '![Figure](../Images/149097ac697aa4502950caf54325d751.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/149097ac697aa4502950caf54325d751.png)'
- en: Source [https://mobidev.biz/wp-content/uploads/2021/01/pipeline-architecture-video-processing.png](https://mobidev.biz/wp-content/uploads/2021/01/pipeline-architecture-video-processing.png)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 来源 [https://mobidev.biz/wp-content/uploads/2021/01/pipeline-architecture-video-processing.png](https://mobidev.biz/wp-content/uploads/2021/01/pipeline-architecture-video-processing.png)
- en: One more technical issue which comes from multiple workers, is that due to their
    concurrent nature, they don’t guarantee the order to be the same as in the input
    sequence. Therefore, it requires additional effort to fix the order in the pipeline
    stages where it is important (for example, encoding). The same problem actually
    happens in the case of skipping frames. Anyway, with a certain configuration,
    multiple workers gave an improvement as well.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个技术问题来自多个工作者，由于其并发性质，它们不能保证顺序与输入序列相同。因此，在重要的管道阶段（例如编码）中需要额外的努力来修正顺序。跳帧的情况实际上也会出现同样的问题。无论如何，经过特定配置，多个工作者也能带来改进。
- en: '**Related:**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[OpenAI Releases Two Transformer Models that Magically Link Language and Computer
    Vision](/2021/01/openai-transformer-models-link-language-computer-vision.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAI 发布两款神奇的 Transformer 模型，链接语言与计算机视觉](/2021/01/openai-transformer-models-link-language-computer-vision.html)'
- en: '[Top Python Libraries for Deep Learning, Natural Language Processing & Computer
    Vision](/2020/11/top-python-libraries-deep-learning-natural-language-processing-computer-vision.html)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习、自然语言处理与计算机视觉的顶级 Python 库](/2020/11/top-python-libraries-deep-learning-natural-language-processing-computer-vision.html)'
- en: '[Top 10 Computer Vision Papers 2020](/2021/01/top-10-computer-vision-papers-2020.html)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2020 年十大计算机视觉论文](/2021/01/top-10-computer-vision-papers-2020.html)'
- en: More On This Topic
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Bounding Box Deep Learning: The Future of Video Annotation](https://www.kdnuggets.com/2022/07/bounding-box-deep-learning-future-video-annotation.html)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[边界框深度学习：视频注释的未来](https://www.kdnuggets.com/2022/07/bounding-box-deep-learning-future-video-annotation.html)'
- en: '[Text-2-Video Generation: Step-by-Step Guide](https://www.kdnuggets.com/2023/08/text2video-generation-stepbystep-guide.html)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本到视频生成：逐步指南](https://www.kdnuggets.com/2023/08/text2video-generation-stepbystep-guide.html)'
- en: '[Natural Language Processing Key Terms, Explained](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理关键术语解析](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
- en: '[Data Representation for Natural Language Processing Tasks](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理任务的数据表示](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
- en: '[Python String Processing Cheatsheet](https://www.kdnuggets.com/2020/01/python-string-processing-primer.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 字符串处理速查表](https://www.kdnuggets.com/2020/01/python-string-processing-primer.html)'
- en: '[Transfer Learning for Image Recognition and Natural Language Processing](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[图像识别与自然语言处理的迁移学习](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
