# 数据科学的基础数学：信息理论

> 原文：[https://www.kdnuggets.com/2021/01/essential-math-data-science-information-theory.html](https://www.kdnuggets.com/2021/01/essential-math-data-science-information-theory.html)

[评论](#comments)[![图片](../Images/e6b504358bea8f63e781d7dbce4b4afc.png)](https://www.essentialmathfordatascience.com/?utm_source=kdnuggets&utm_medium=blog&utm_campaign=kdnuggets_scalars_vectors)

*信息理论*领域研究信号中信息的量化。在机器学习的背景下，这些概念用于表征或比较概率分布。量化信息的能力也被用于决策树算法中，以选择与最大信息增益相关的变量。熵和交叉熵的概念在机器学习中也很重要，因为它们导致了分类任务中广泛使用的损失函数：交叉熵损失或对数损失。

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析能力

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织IT工作

* * *

### 香农信息

**直觉**

理解信息理论的第一步是考虑与随机变量相关的信息量。在信息理论中，这一信息量表示为II，并称为*香农信息*、*信息内容*、*自信息*或*惊讶度*。主要思想是，可能发生的事件传达的信息比不太可能发生的事件少（因此这些事件更*令人惊讶*）。例如，如果来自洛杉矶的朋友告诉你：“今天晴天”，这比她告诉你：“今天下雨”信息量少。因此，将香农信息视为与结果相关的惊讶量可能会有所帮助。在本节中，你也将看到为什么它也是一种信息量，以及为什么可能发生的事件信息量较少。

**信息单位**

量化信息的常见单位是*nat*和*bit*。这些量基于对数函数。*nat*是“信息的自然单位”的缩写，基于自然对数，而bit是“二进制数字”的缩写，基于二进制对数。因此，bit是nat的一个重新标定版本。接下来的部分将主要使用bit和二进制对数进行公式计算，但用自然对数替换只会将单位从bits改为nats。

比特表示可以有两种不同状态（0 或 1）的变量。例如，1 比特用于编码一次掷硬币的结果。如果你掷两次硬币，你至少需要两个比特来编码结果。例如，00 代表 HH，01 代表 HT，10 代表 TH，11 代表 TT。你可以使用其他编码，比如 0 代表 HH，100 代表 HT，101 代表 TH，111 代表 TT。然而，这种编码在平均情况下使用的比特数量更多（假设四种事件的概率分布是均匀的，正如你将看到的）。

让我们举个例子来看一下一个比特描述了什么。Erica 发送给你一个消息，包含三次硬币掷出的结果，用 0 编码“正面”，用 1 编码“反面”。共有 8 种可能的序列，例如 001、101 等。当你收到一个比特的消息时，它将你的不确定性减少了一半。例如，如果第一个比特告诉你第一次掷硬币是“正面”，那么剩下的可能序列是 000、001、010 和 011。这样，可能的序列只有 4 种，而不是 8 种。同样，收到两个比特的消息将把你的不确定性减少到 4；收到三个比特的消息，将把不确定性减少到 8，依此类推。

请注意，我们谈论的是“有用的信息”，但可能消息是冗余的，并且在相同的比特数下传达的信息更少。

**示例**

假设我们想传输一系列八次投掷的结果。你将为每次投掷分配一个比特。因此，你需要八个比特来编码这个序列。这个序列可能是“00110110”，对应于 HHTTHTTH（四个“正面”和四个“反面”）。

然而，假设这个硬币是偏置的：得到“反面”的概率只有 1/8。你可以找到一种更好的方式来编码这个序列。一种选择是编码“反面”结果的索引：这将需要多个比特，但“反面”仅在少数试验中出现。通过这种策略，你可以将更多比特分配给稀有结果。

这个例子说明了更可预测的信息可以被压缩：一个偏置的硬币序列可以用比公平硬币序列更少的信息进行编码。这意味着香农信息依赖于事件的概率。

**数学描述**

香农信息编码了这个想法，并将事件发生的概率转化为相关的信息量。它的特点是，如你所见，可能事件的资讯少于不可能事件的信息，并且不同事件的信息是可加性的（如果事件是独立的）。

从数学角度来看，函数*I(x)* 是事件 *X=x* 的信息，它以结果作为输入，返回信息的数量。它是概率的单调递减函数（即，当概率增加时，函数值从不增加）。香农信息可以描述为：

![方程式](../Images/fd55e7d2ac8f63e589fdb3a404b72031.png)

结果是比特数量的下限，也就是编码一个序列所需的最小比特数，以达到最佳编码。

乘积的对数等于各个元素的和：![方程](../Images/5377a4150756f11be3e984841fce238b.png)。这个性质对编码香农信息的加法性质很有用。两个事件的发生概率是它们各自概率的乘积（因为它们是独立的，正如你在[数据科学的基本数学](https://bit.ly/3njFklS)中看到的）：

![方程](../Images/5ea29c31029510d5ecdebfd8225fb711.png)

这意味着与两个事件发生概率*P(x,y)*相关的信息量等于与*P(x)*相关的信息量加上与*P(y)*相关的信息量。独立事件的信息量是相加的。

让我们绘制这个函数在概率范围从0到1内的曲线，以查看曲线的形状：

[PRE0]

![图](../Images/2ae17b840d7be726e113f81d3473127c.png)

*图1：信息量由概率的负对数给出。*

如图1所示，负对数函数编码了一个非常不可能事件（概率接近0）与大量信息量相关，而一个可能事件（概率接近1）与接近0的信息量相关的观点。

由于你使用了底为二的对数`np.log2()`，信息量*I(x)*以*比特*为单位进行测量。

### 熵

你已经看到香农信息给出了与单个概率相关的信息量。你也可以用*香农熵*，也称为*信息熵*，或者简单地叫做*熵*，来计算离散分布的信息量。

**示例**

比如考虑一个偏向的硬币，其中正面朝上的概率为0.8。

1.  这是你的分布：你有0.8的概率得到‘正面’，以及*1 − 0.8 = 0.2*的概率得到‘反面’。

1.  这些概率分别与香农信息量相关：

![方程](../Images/73a405551777ec222441120448d774c4.png)

和

![方程](../Images/c08c196af95ea74c7b9ce92f2acb466e.png)

1.  正面朝上与大约0.32的信息量相关，反面朝上与2.32的信息量相关。然而，你不会在平均情况下有相同数量的正面和反面，所以你必须用每个概率的概率本身来加权香农信息。例如，如果你想传递一个包含100次试验结果的消息，你将需要大约20倍的反面信息量和80倍的正面信息量。你会得到：

![方程](../Images/cc39e9187d2c55ba6e924101a1997506.png)

和

![方程](../Images/50d392d9bee729268e62964131585acf.png)

1.  这些表达式的总和给你：

![方程](../Images/db046008bb43208d8c491754cd730c6a.png)

描述这一分布的一系列事件所需的平均比特数是0.72比特。

总结来说，你可以把熵看作是与离散分布的概率相关的信息的总结：

1.  你计算了分布中每个概率的香农信息。

1.  你用相应的概率加权香农信息。

1.  你对加权结果进行求和。

**数学公式**

熵是相对于概率分布的信息的期望。请记住，从[数据科学的基础数学](https://bit.ly/3njFklS)中你了解到，期望是你从分布中抽取大量样本时得到的均值：

![方程](../Images/f1c1585d04229c82239e2e4538c0f3bc.png)

随机变量*X*具有*n*个可能的结果，*x[i]*是第*i*个可能的结果，对应的概率为*P(x[i])*。分布的信息的期望值对应于你将获得的信息的平均值。

按照期望和香农信息的公式，随机变量*X*的熵定义为：

![方程](../Images/175b64d27a5282aee38051690df3cb20.png)

熵给你提供了编码随机变量*X*状态所需的平均信息量。

注意，函数*H(X)*的输入是随机变量*X*，而*I(x)*表示事件*X=x*的香农信息。你也可以参考随机变量*X*的熵，它是相对于*P(x)*分布的，记作*H(P)*。

**示意图**

让我们举个例子：如图2底部面板所示，你有一个具有四个可能结果的离散分布，概率分别为0.4、0.4、0.1和0.1。正如你之前看到的，信息是通过对概率进行对数变换得到的（顶部面板）。这是熵公式的最后一部分：*log2P(x)*。

![图](../Images/ecdaafee5cb8e80c770cf60f20c8bdd8.png)

*图2：作为香农信息加权和的熵示意图。*

每个经过变换的概率都按相应的原始概率加权。如果一个结果发生得很频繁，它将对分布的熵产生更大的影响。这意味着低概率（如图2中的0.1）提供了大量的信息（3.32比特），但对最终结果的影响较小。较大概率（如图2中的0.4）则关联的信息较少（如图2所示的1.32比特），但加权更多。

**二进熵函数**

在一个有偏硬币的例子中，你计算了伯努利过程的熵（有关伯努利分布的更多细节请参见[数据科学的基础数学](https://bit.ly/3njFklS)）。在这种特殊情况下，熵被称为*二进熵函数*。

为了描述二进制熵函数，你将计算由各种概率分布（从严重偏向“反面”到严重偏向“正面”）描述的偏置硬币的熵。

让我们首先创建一个函数，计算一个分布的熵，该函数接受一个包含概率的数组作为输入并返回相应的熵：

[PRE1]

你也可以使用`scipy.stats`中的`entropy()`，在这里你可以指定用于计算熵的对数的底数。这里，我使用了以2为底的对数。

以公平硬币为例，其“正面”落地的概率为0.5。分布因此是0.5和1 − 0.5 = 0.5。我们使用刚刚定义的函数来计算相应的熵：

[PRE2]

[PRE3]

该函数计算了`P * np.log2(P)`在你用作输入的数组的每个元素上的总和。如前一节所示，你可以期望偏置硬币的熵更低。让我们绘制不同硬币偏置的熵，从只落“反面”的硬币到只落“正面”的硬币：

[PRE4]

![图示](../Images/aae2e343d1abfe59210c0290a25e6b00.png)

*图3：熵作为“正面”概率的函数。*

图3显示了熵如何随着条件的不确定性增加而增加：也就是说，当“正面”落地的概率等于“反面”落地的概率时。

**差分熵**

连续分布的熵称为*差分熵*。它是离散分布熵的扩展，但不满足相同的要求。问题在于，连续分布中的值的概率趋向于零，而编码这些值需要的位数趋向于无穷大。

定义如下：

![公式](../Images/b083a67b3e20d937ac2bad40e81ff2de.png)

差分熵可能为负值。原因是，如你在[数据科学的基础数学](https://bit.ly/3njFklS)中所见，连续分布不是概率而是概率密度，这意味着它们不满足概率的要求。例如，它们不被限制在1以下。这导致*p(x)*可以取大于1的正值，而*log2p(x)*可以取正值（由于负号，结果变为负值）。

### 交叉熵

熵的概念可以用来比较两个概率分布：这称为两个分布之间的*交叉熵*，它测量了它们之间的差异。

这个想法是计算与分布*Q(x)*的概率相关的信息，但与熵不同的是，你不是根据*Q(x)*加权，而是根据另一个分布*P(x)*加权。注意，你比较的是关于相同随机变量*X*的两个分布。

你还可以将交叉熵视为从*P(x)*中绘制事件时使用*Q(x)*对其进行编码的期望信息量。

这在数学上表示为：

![方程](../Images/6c038f4a404d3dc7c1d1390a0f2c42fd.png)

让我们看看它是如何工作的。

![图](../Images/8d3bb7ddd0e728ab031ab27ec6c29bb8.png)

*图 4：交叉熵作为 *Q(x)* 的香农信息，根据 *P(x)* 的分布加权。*

图 4 显示了两种不同的情况以说明交叉熵。在左侧，你有两个相同的分布 *P(x)* （蓝色）和 *Q(x)* （红色）。它们的交叉熵等于熵，因为 *Q(x)* 的信息是根据 *P(x)* 的分布加权的，而 *P(x)* 与 *Q(x)* 相似。

然而，在右侧面板中，*P(x)* 和 *Q(x)* 是不同的。这导致了较大的交叉熵，因为与大量信息相关的概率权重较小，而与少量信息相关的概率权重较大。

交叉熵不能小于熵。仍在右侧面板中，你可以看到，当概率 *Q(x)* 大于 *P(x)* （因此与较少的信息量相关）时，它会被低权重抵消（导致低权重和低信息）。这些低权重会在分布的其他概率中被较大权重补偿（导致大权重和大量信息）。

还要注意，两个分布 *P(x)* 和 *Q(x)* 必须具有相同的 *支持* （即随机变量可以取的值的集合，并且这些值具有正概率）。

总结来说，当分布相同时，交叉熵最小。如你在 [0.1.4](https://hadrienj.github.io/posts/Essential-Math-information_theory/#sec:ch12_section_kullback_leibler_divergence) 中看到的，这一特性使得交叉熵成为一个有用的度量。还要注意，根据你选择的参考分布，结果是不同的：*H(P,Q) ≠ H(Q,P)*。

**交叉熵作为损失函数**

在机器学习中，交叉熵作为一种损失函数称为 *交叉熵损失* （也称为 *对数损失* 或 *逻辑损失*，因为它在逻辑回归中使用）。

![图](../Images/ec9bac288843cb9f238f77e33842fbf7.png)

*图 5：交叉熵可用于比较真实分布（正确类别的概率为 1，否则为 0）与模型估计的分布。*

假设你想建立一个模型，该模型可以根据音频样本分类三种不同的鸟类。如图 5 所示，音频样本被转换为特征（这里是频谱图），而可能的类别（这三种不同的鸟类）被 *独热编码*，即正确类别编码为 1，否则为 0。此外，机器学习模型会为每个类别输出概率。

要学习如何对鸟类进行分类，模型需要比较估计分布 *Q(x)*（由模型给出）和真实分布 *P(x)*。交叉熵损失被计算为 *P(x)* 和 *Q(x)* 之间的交叉熵。

图 5 显示了你在此示例中考虑的样本对应的真实类别是“欧洲绿啄木鸟”。模型输出一个概率分布，你将计算与此估计相关的交叉熵损失。图 6 显示了两个分布。

![图](../Images/930fad886c9a9c232fbeec17a0e06f0c.png)

*图 6：真实分布 *P(x)* 和估计分布 *Q(x)* 的比较。*

让我们手动计算这两个分布之间的交叉熵：

![方程](../Images/82b182d56293446bad0e1e9ce19444cd.png)

在交叉熵损失中使用的是自然对数而不是以二为底的对数，但原理是相同的。此外，请注意使用 *H(P,Q)* 而不是 *H(Q,P)*，因为参考的是分布 *P*。

由于你进行了独热编码（1 表示真实类别，其他情况为 0），交叉熵就是对真实类别的估计概率的负对数。

**二分类：对数损失**

在机器学习中，交叉熵被广泛用作二分类的损失函数：对数损失。

由于分类是二元的，唯一可能的结果是 *y*（真实标签对应于第一类）和 *1−y*（真实标签对应于第二类）。同样，你有第一类的估计概率 *y^* 和第二类的估计概率 *1−y^*。

从交叉熵的公式来看，*∑x* 在这里对应于两个可能结果的总和（*y* 和 *1−y*）。你有：

![方程](../Images/96f46583f71303d20eed4e2126b5c458.png)

这就是对数损失的公式。

### Kullback-Leibler 散度（KL 散度）

你看到交叉熵是一个依赖于两个分布相似度的值，交叉熵值较小对应于完全相同的分布。你可以利用这一特性来计算两个分布之间的 *散度*：你比较它们的交叉熵与分布完全相同的情况。这种散度称为 *Kullback-Leibler 散度*（或简称 *KL 散度*），或者 *相对熵*。

直观地说，KL 散度是与分布 *Q(x)* 编码相关的补充信息量，相对于真实分布 *P(x)*。它告诉你这两个分布有多么不同。

从数学上讲，两个分布 *P(x)* 和 *Q(x)* 之间的 KL 散度，记作 *DKL(P||Q)*，表示为 *P(x)* 和 *Q(x)* 的交叉熵与 *P(x)* 的熵之间的差值：

![方程](../Images/dfc5ed69b7928089bd2e65221771093b.png)

用交叉熵和熵的表达式替换，你得到：

![公式](../Images/d3c5d4d17b8f3f50b5db64937c389c65.png)

KL 散度总是非负的。由于熵 *H(P)* 与交叉熵 *H(P,P)* 相同，并且最小的交叉熵发生在相同的分布之间 *(H(P,P)H(P,P))*，所以 *H(P,Q)* 必然大于 *H(P)*。此外，当两个分布相同时，KL 散度等于零。

然而，交叉熵不是对称的。将分布 *P(x)* 与分布 *Q(x)* 进行比较可能与将分布 *Q(x)* 与 *P(x)* 进行比较不同——这意味着你不能将 KL 散度视为距离。

**简介: [Hadrien Jean](https://hadrienj.github.io/)** 是一名机器学习科学家。他拥有巴黎高等师范学院的认知科学博士学位，在那里他使用行为学和电生理数据研究听觉感知。他曾在工业界工作，构建了用于语音处理的深度学习管道。在数据科学和环境交汇的领域，他从事使用深度学习对音频录音进行生物多样性评估的项目。他还定期在Le Wagon（数据科学训练营）创建内容和授课，并在他的博客中撰写文章 ([hadrienj.github.io](http://hadrienj.github.io))。

[原文](https://hadrienj.github.io/posts/Essential-Math-information_theory/)。经许可转载。

**相关:**

+   [数据科学中的基本数学: 泊松分布](/2020/12/introduction-poisson-distribution-data-science.html)

+   [数据科学中的基本数学: 概率密度和概率质量函数](/2020/12/essential-math-data-science-probability-density-probability-mass-functions.html)

+   [数据科学中的基本数学: 积分和曲线下的面积](/2020/11/essential-math-data-science-integrals-area-under-curve.html)

### 更多相关话题

+   [成为优秀数据科学家所需的5项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)

+   [每个初学者数据科学家应该掌握的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)

+   [2021年最佳ETL工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)

+   [停止学习数据科学以寻找目标，然后寻找目标来……](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)

+   [学习数据科学统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)

+   [建立一个坚实的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)
