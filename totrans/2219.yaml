- en: 'Diving into the Pool: Unraveling the Magic of CNN Pooling Layers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/diving-into-the-pool-unraveling-the-magic-of-cnn-pooling-layers](https://www.kdnuggets.com/diving-into-the-pool-unraveling-the-magic-of-cnn-pooling-layers)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layers are common in CNN architectures used in all state-of-the-art
    deep learning models. They are prevalent in Computer Vision tasks including Classification,
    Segmentation, Object Detection, Autoencoders and many more; simply used wherever
    we find Convolutional layers.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we'll dig into the math that makes pooling layers work and
    learn when to use different types. We'll also figure out what makes each type
    special and how they are different from one another.
  prefs: []
  type: TYPE_NORMAL
- en: Why use Pooling Layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pooling layers provide various benefits making them a common choice for CNN
    architectures. They play a critical role in managing spatial dimensions and enable
    models to learn different features from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some benefits of using pooling layers in your models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dimensionality Reduction**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All pooling operations select a subsample of values from a complete convolutional
    output grid. This downsamples the outputs resulting in a decrease in parameters
    and computation for subsequent layers, which is a vital benefit of Convolutional
    architectures over fully connected models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Translation Invariance**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling layers make machine learning models invariant to small changes in input
    such as rotations, translations or augmentations. This makes the model suitable
    for basic computer vision tasks allowing it to identify similar image patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let us look at various pooling methods commonly used in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Common Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For ease of comparison let's use a simple 2-dimensional matrix and apply different
    techniques with the same parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layers inherit the same terminology as the Convolutional Layers, and
    the concept of Kernel Size, Stride and Padding is conserved.
  prefs: []
  type: TYPE_NORMAL
- en: So, here we define a 2-D matrix with four rows and four columns. To use Pooling,
    we will use a Kernel size of two and stride two with no padding. Our matrix will
    look as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diving into the Pool: Unraveling the Magic of CNN Pooling Layers](../Images/74067005bf99a0a2f144582c88ad1680.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**It is important to note that pooling is applied on a per-channel basis.**
    So the same pooling operations are repeated for each channel in a feature map.
    The number of channels remains invariant, even though the input feature map is
    downsampled.'
  prefs: []
  type: TYPE_NORMAL
- en: Max Pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We iterate the kernel over the matrix and select the max value from each window.
    In the above example, we use a 2x2 kernel with stride two and iterate over the
    matrix forming four different windows, denoted by different colours.
  prefs: []
  type: TYPE_NORMAL
- en: '**In Max Pooling, we only retain the largest value from each window.** This
    downsamples the matrix, and we obtain a smaller 2x2 grid as our max pooling output.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diving into the Pool: Unraveling the Magic of CNN Pooling Layers](../Images/16229fcc4d9f0ceab1c5fc380c07017c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of Max Pooling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Preserve High Activation Values**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When applied to activation outputs of a convolutional layer, we are effectively
    only capturing the higher activation values. It is useful in tasks where higher
    activations are essential, such as object detection. Effectively we are downsampling
    our matrix, but we can still preserve the critical information in our data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Retain Dominant Features**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum values often signify the important features in our data. When we retain
    such values, we conserve information the model considers important.
  prefs: []
  type: TYPE_NORMAL
- en: '**Resistance to Noise**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we base our decision on a single value in a window, small variations in other
    values can be ignored, making it more robust to noise.
  prefs: []
  type: TYPE_NORMAL
- en: Drawbacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Possible Loss of Information**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basing our decision on the maximal value ignores the other activation values
    in the window. Discarding such information can result in possible loss of valuable
    information, irrecoverable in subsequent layers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Insensitive to Small Shifts**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Max Pooling, small changes in the non-maximal values will be ignored. This
    insensitivity to small changes can be problematic and can bias the results.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sensitive to High Noise**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though small variations in values will be ignored, high noise or error
    in a single activation value can result in the selection of an outlier. This can
    alter the max pooling result significantly, causing degradation of results.
  prefs: []
  type: TYPE_NORMAL
- en: Average Pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In average pooling, we similarly iterate over windows. However, **we consider
    all values in the window, take the mean and then output that as our result.**
  prefs: []
  type: TYPE_NORMAL
- en: '![Diving into the Pool: Unraveling the Magic of CNN Pooling Layers](../Images/b25e1d85a50e5357b1038e0d6d154e88.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of Average Pooling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Preserving Spatial Information**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In theory, we are retaining some information from all values in the window,
    to capture the central tendency of the activation values. In effect, we lose less
    information and can persist more spatial information from the convolutional activation
    values.
  prefs: []
  type: TYPE_NORMAL
- en: '**Robust to Outliers**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Averaging all values makes this method more robust to outliers relative to Max
    Pooling, as a single extreme value can not significantly alter the results of
    the pooling layer.
  prefs: []
  type: TYPE_NORMAL
- en: '**Smoother Transitions**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When taking the mean of values, we obtain less sharp transitions between our
    outputs. This provides a generalized representation of our data, allowing reduced
    contrast between subsequent layers.
  prefs: []
  type: TYPE_NORMAL
- en: Drawbacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Inability to Capture Salient Features**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All values in a window are treated equally when the Average Pooling layer is
    applied. This fails to capture the dominant features from a convolutional layer,
    which can be problematic for some problem domains.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reduced Discrimination Between Features Maps**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When all values are averaged, we can only capture the common features between
    regions. As such, we can lose the distinctions between certain features and patterns
    in an image, which is certainly a problem for tasks such as Object Detection.
  prefs: []
  type: TYPE_NORMAL
- en: Global Average Pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Global Pooling is different from normal pooling layers. It has no concept
    of windows, kernel size or stride.** We consider the complete matrix as a whole
    and consider all values in the grid. In the context of the above example, we take
    the average of all values in the 4x4 matrix and get a singular value as our result.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diving into the Pool: Unraveling the Magic of CNN Pooling Layers](../Images/5729d06229649aa910f506d5b328c6cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: When to Use
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Global Average Pooling allows for straightforward and robust CNN architectures.
    **With the use of Global Pooling, we can implement generalizable models, that
    are applicable to input images of any size.** Global Pooling layers are directly
    used before dense layers.
  prefs: []
  type: TYPE_NORMAL
- en: The convolutional layers downsample each image, depending on kernel iterations
    and strides. However, the same convolutions applied to images of different sizes
    will result in an output of different shapes. All images are downsampled by the
    same ratio, so larger images will have larger output shapes. This can be a problem
    when passing it to Dense layers for classification, as size mismatch can cause
    runtime exceptions.
  prefs: []
  type: TYPE_NORMAL
- en: Without modifications in hyperparameters or model architecture, implementing
    a model applicable to all image shapes can be difficult. This problem is mitigated
    using Global Average Pooling.
  prefs: []
  type: TYPE_NORMAL
- en: When Global Pooling is applied before Dense layers, all input sizes will be
    reduced to a size of 1x1\. So an input of either (5,5) or (50,50) will be downsampled
    to size 1x1\. They can then be flattened and sent to the Dense layers without
    worrying about size mismatches.
  prefs: []
  type: TYPE_NORMAL
- en: Key Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We covered some fundamental pooling methods and the scenarios where each is
    applicable. It is critical to choose the one suitable for our specific tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '**It is essential to clarify that there are no learnable parameters in pooling
    layers.** They are simply sliding windows performing basic mathematical operations.
    Pooling layers are not trainable, yet they supercharge CNN architectures allowing
    faster computation and robustness in learning input features.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Muhammad Arham](https://www.linkedin.com/in/muhammad-arham-a5b1b1237/)**
    is a Deep Learning Engineer working in Computer Vision and Natural Language Processing.
    He has worked on the deployment and optimizations of several generative AI applications
    that reached the global top charts at Vyro.AI. He is interested in building and
    optimizing machine learning models for intelligent systems and believes in continual
    improvement.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Unraveling the Power of Chain-of-Thought Prompting in Large Language Models](https://www.kdnuggets.com/2023/07/power-chain-thought-prompting-large-language-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Semantic Layers are the Missing Piece for AI-Enabled Analytics](https://www.kdnuggets.com/2024/02/cube-semantic-layers-missing-piece-ai-enabled-analytics)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unveiling Neural Magic: A Dive into Activation Functions](https://www.kdnuggets.com/unveiling-neural-magic-a-dive-into-activation-functions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding Python''s Iteration and Membership: A Guide to…](https://www.kdnuggets.com/understanding-pythons-iteration-and-membership-a-guide-to-__contains__-and-__iter__-magic-methods)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introduction to __getitem__: A Magic Method in Python](https://www.kdnuggets.com/2023/03/introduction-getitem-magic-method-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python f-Strings Magic: 5 Game-Changing Tricks Every Coder Needs to Know](https://www.kdnuggets.com/python-fstrings-magic-5-gamechanging-tricks-every-coder-needs-to-know)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
