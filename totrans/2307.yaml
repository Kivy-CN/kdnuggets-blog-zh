- en: 5 Ways to Deal with the Lack of Data in Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/06/5-ways-lack-data-machine-learning.html](https://www.kdnuggets.com/2019/06/5-ways-lack-data-machine-learning.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![5 Ways to Deal with the Lack of Data in Machine Learning](../Images/03ddce27b377cb1a1c82c79162e21088.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Editor
  prefs: []
  type: TYPE_NORMAL
- en: In many projects I carried out, companies, despite having fantastic AI business
    ideas, display a tendency to slowly become frustrated when they realize that they
    do not have enough data… However, solutions do exist! **The purpose of this article
    is to briefly introduce you to some of them (the ones that are proven effective
    in my practice) rather than to list all existing solutions.**
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The problem of data scarcity is very important since data are at the core of
    any AI project. The size of a datasetis often responsible for poor performances
    in ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time, data related issues are the main reason why great AI projects
    cannot be accomplished. In some projects, you come to the conclusion that there
    is no relevant data or the collection process is too difficult and time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised machine learning models are being successfully used to respond to
    a whole range of business challenges. However, these models are data-hungry, and
    their performance relies heavily on the size of training data available. In many
    cases, it is difficult to create training datasets that are large enough.
  prefs: []
  type: TYPE_NORMAL
- en: Another issue I could mention is that project analysts tend to underestimate
    the amount of data necessary to handle common business problems. I remember myself
    struggling to collect big training datasets. It is even more complicated to gather
    data when working for a large company.
  prefs: []
  type: TYPE_NORMAL
- en: '***How much data do I need?***'
  prefs: []
  type: TYPE_NORMAL
- en: Well, you need roughly 10 times as many examples as there are degrees of freedom
    in your model. The more complex the model, the more you are prone to overfitting,
    but that can be avoided by validation. **However, much fewer data can be used
    based on the use case.**
  prefs: []
  type: TYPE_NORMAL
- en: '***Overfitting:**** refers to a model that models the training data too well.
    It happens when a model learns the detail and noise in the training data to the
    extent that it negatively impacts the performance of the model on new data.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is also worth discussing the issue of handling the missing values. Especially
    if the number of missing values in your data is big enough (above 5%).
  prefs: []
  type: TYPE_NORMAL
- en: Once again, dealing with missing values will depend on certain ‘success’ criteria.Moreover,
    these criteria vary for different datasets and even for different applications,
    such as recognition, segmentation, prediction, and classification (given the same
    dataset) even for different applications (recognition, segmentation, prediction,
    classification).
  prefs: []
  type: TYPE_NORMAL
- en: '*It is important to understand that there is no perfect way to deal with missing data.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Different solutions exist, but it depends on the kind of problem — Time-series
    Analysis, ML, Regression, etc.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to predictive techniques, they shall be used only when missing
    values are not observed completely at random, and the variables were chosen to
    impute such missing values have some relationship with it, else it could yield
    imprecise estimates.
  prefs: []
  type: TYPE_NORMAL
- en: In general, different machine learning algorithms can be used to determine the
    missing values. This works by turning missing features to labels themselves and
    now using columns without missing values to predict columns with missing values.
  prefs: []
  type: TYPE_NORMAL
- en: Based on my experience, you will be confronted with a lack of data or missing
    data at some point if you decide to build an AI-powered solution, **but fortunately,
    there are ways to turn that minus into a plus.**
  prefs: []
  type: TYPE_NORMAL
- en: '**Lack of data?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As noted above, it is impossible to precisely estimate the minimum amount of
    data required for an AI project. Obviously, the very nature of your project will
    influence significantly the amount of data you will need. For example, texts,
    images, and videos usually require more data. **However, many other factors should
    be considered in order to make an accurate estimate.**
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of categories to be predicted**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the expected output of your model? Basically, the fewest number or categories
    the better.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Model Performance** If you plan on getting a product in production, you need
    more. **A small dataset might be good enough for a proof of concept, but in production,
    you’ll need way more data.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, small datasets require models that have low complexity (or [high
    bias](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)) to avoid [overfitting](https://en.wikipedia.org/wiki/Overfitting) the
    model to the data.
  prefs: []
  type: TYPE_NORMAL
- en: Non-Technical Solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before exploring technical solutions, let’s analyze what we can do to enhance
    your dataset. It might sound obvious but before getting started with AI, please
    try to obtain as much data as possible by developing your external and internal
    tools with data collection in mind. If you know the tasks that a machine learning
    algorithm is expected to perform, then you can create a data-gathering mechanism
    in advance.
  prefs: []
  type: TYPE_NORMAL
- en: '*Try to establish a real data culture within your organization.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To initiate ML execution, you could rely on open source data. There are a lot
    of data available for ML, and some companies are ready to give it away.
  prefs: []
  type: TYPE_NORMAL
- en: If you need external data for your project, it can be beneficial to form partnerships
    with other organizations in order to get relevant data. Forming partnerships will
    obviously cost you some time, but the proprietary data gained will build a natural
    barrier to any rivals.
  prefs: []
  type: TYPE_NORMAL
- en: Build a useful application, give it away, use the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another approach that I used in my previous project was to give away access
    to a cloud application to customers. The data that makes it into the app can be
    used to build machine learning models. My previous client built an application
    for hospitals and made it free. We gathered a lot of data thanks to it and managed
    to create a unique dataset for our ML solution. It really helps to tell customers
    or investors that you have built your own and unique dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e81a847a79f65283b88220dbf58ca256.png)'
  prefs: []
  type: TYPE_IMG
- en: Small datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Based on my experience, some common approaches that can help with building
    predictive models from small data sets are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/417db596333fed2ef9abd993607d884a.png)'
  prefs: []
  type: TYPE_IMG
- en: In general, the simpler the machine learning algorithm, the better it will learn
    from small data sets. From an ML perspective, **small** data requires models that
    have low complexity (or high bias) to avoid overfitting the model to the data.
    I noticed that the Naive Bayes algorithm is among the simplest classifiers and
    as a result learns remarkably well from relatively small data sets.
  prefs: []
  type: TYPE_NORMAL
- en: '***Naive Bayes methods:**** the set of supervised learning algorithms based
    on applying Bayes’ theorem with the “naive” assumption of conditional independence
    between every pair of features given the value of the class variable.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You can also rely on other linear models and decision trees. Indeed, they can
    also perform relatively well on small data sets. Basically, simple models are
    able to learn from small data sets better than more complicated models (neural
    networks) since they are essentially trying to learn less.
  prefs: []
  type: TYPE_NORMAL
- en: For very **small datasets**, Bayesian methods are generally the best in class,
    although the results can be sensitive **to your choice of prior.** I think that
    the naive Bayes classifier and ridge regression are the best predictive models.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to **small** datasets, you need models that have few parameters
    (low complexity) and/or a strong prior. You can also interpret the “prior” as
    an assumption you can make on how the data behaves.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56e0048653663ded6790e3c968772657.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Many other solutions do exist depending on the exact nature of your business
    issues and the size of your dataset.**'
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '***Definition:**** a framework that leverages existing relevant data or models
    while building a machine learning model.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Transfer learning uses knowledge from a learned task to improve the performance
    on a related task, typically reducing the amount of required training data.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning techniques are useful because they allow models to make predictions
    for a new domain or task (known as the target domain) using knowledge learned
    from another dataset or existing machine learning models (the source domain).
  prefs: []
  type: TYPE_NORMAL
- en: '**Transfer learning techniques should be considered when you do not have enough
    target training data, and the source and target domains have some similarities
    but are not identical.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3571fe02830ac79f4da7148726c51f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Naively aggregating models or different datasets would not always work! If the
    existing datasets are very different from the target data, then the new learner
    can be negatively impacted by existing data or models.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning works well when you have other datasets you can use to infer
    knowledge, but what happens when you have no data at all? This is where data generation
    can play a role. It is used when no data is available or when you need to create
    more data than you could amass even through aggregation.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the small amount of data that does exist is modified to create
    variations on that data to train the model. For example, many images of a car
    can be generated by cropping and downsizing one single image of a car.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the lack of quality labeled data is also one of the largest challenges
    facing data science teams, but by using techniques, such as transfer learning
    and data generation, it is possible to overcome data scarcity.
  prefs: []
  type: TYPE_NORMAL
- en: Another common application of transfer learning is to train models on cross-customer
    datasets to overcome the cold-start problems. I noticed that SaaS companies often
    have to deal with this when onboarding new customers to their ML products. Indeed,
    until the new customer has collected enough data to achieve good model performance
    (which could take several months), it’s hard to provide value.
  prefs: []
  type: TYPE_NORMAL
- en: Data Augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data augmentation means increasing the number of data points. In my latest project,
    we used data augmentation techniques to increase the number of images in our dataset.
    In terms of traditional row/column format data, it means increasing the number
    of rows or objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'We had no choice but to rely on data augmentation for two reasons: time and
    accuracy. Every data collection process is associated with a cost. This cost can
    be in terms of dollars, human effort, computational resources, and, of course,
    time consumed in the process.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ddc94ff72772016c5a35396446926fd9.png)'
  prefs: []
  type: TYPE_IMG
- en: As a consequence, we had to augment existing data to increase the data size
    that we feed to our ML classifiers and to compensate for the cost involved in
    further data collection.
  prefs: []
  type: TYPE_NORMAL
- en: '*There are many ways to augment data.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In our case, you can rotate the original image, change lighting conditions,
    crop it differently, so for one image you can generate different sub-samples. **This
    way, you can reduce overfitting your classifier.**
  prefs: []
  type: TYPE_NORMAL
- en: However, if you are generating artificial data using over-sampling methods,
    such as SMOTE, then there is a fair chance you may introduce overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '***Overfitting:**** An overfitted model is a model with a trend line that reflects
    the errors in the data that it is trained with, instead of accurately predicting
    unseen data.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**This is something you must take into consideration when developing your AI
    solution.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/197d6782b71346bdea402a2e0f249df1.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Synthetic Data**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Synthetic data means fake data that contains the same schema and statistical
    properties as its “real” counterpart. Basically, it looks so real that it’s nearly
    impossible to tell that it’s not.
  prefs: []
  type: TYPE_NORMAL
- en: '**So what’s the point of synthetic data, and why does it matter if we already
    have access to the real thing?**'
  prefs: []
  type: TYPE_NORMAL
- en: I have seen synthetic data applied, especially when we were dealing with private
    data (banking, healthcare, etc.), which makes the use of synthetic data a more
    secure approach to development in certain instances.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data is used mostly when there is not enough real data, or there is
    not enough real data for specific patterns you know about. Its usage is mostly
    the same for training and testing datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic Minority Over-sampling Technique (SMOTE) and Modified-SMOTE are two
    techniques which generate synthetic data. Simply put, SMOTE takes the minority
    class data points and creates new data points that lie between any two nearest
    data points joined by a straight line.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm calculates the distance between two data points in the feature
    space, multiplies the distance by a random number between 0 and 1, and places
    the new data point at this new distance from one of the data points used for distance
    calculation.
  prefs: []
  type: TYPE_NORMAL
- en: In order to generate synthetic data, you have to use a training set to define
    a model, which would require validation, and then by changing the parameters of
    interest, you can generate synthetic data, through simulation. The domain/data
    type is significant since it affects the complexity of the entire process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db543549aa9d731746875d090d0f3264.png)'
  prefs: []
  type: TYPE_IMG
- en: In my opinion, asking yourself if you have enough data will reveal inconsistencies
    that you have probably never spotted before. It will help to highlight issues
    in your business processes that you thought were perfect and make you understand
    why it is the key to creating a successful data strategy within your organization.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Alexandre Gonfalonieri](https://twitter.com/AGonfalonieri)** is a AI consultant
    & writer based in Basel. He writes about Brain-Computer Interfaces, the M2M economy,
    and new AI business models. He was featured in HBR and ABC News.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/predict/dealing-with-the-lack-of-data-in-machine-learning-725f2abd2b92).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How to Deal with Categorical Data for Machine Learning](https://www.kdnuggets.com/2021/05/deal-with-categorical-data-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Black Friday Deal - Master Machine Learning for Less with DataCamp](https://www.kdnuggets.com/2022/11/datacamp-black-friday-deal-master-machine-learning-less-datacamp.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Deal with Missing Data Using Interpolation Techniques in Pandas](https://www.kdnuggets.com/how-to-deal-with-missing-data-using-interpolation-techniques-in-pandas)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Ways Businesses Can Benefit From Machine Learning](https://www.kdnuggets.com/2022/08/6-ways-businesses-benefit-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Ways to Improve Your Machine Learning Models](https://www.kdnuggets.com/7-ways-to-improve-your-machine-learning-models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3 Ways Understanding Bayes Theorem Will Improve Your Data Science](https://www.kdnuggets.com/2022/06/3-ways-understanding-bayes-theorem-improve-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
