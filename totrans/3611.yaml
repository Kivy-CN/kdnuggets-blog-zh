- en: MLOps Best Practices
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**MLOps最佳实践**'
- en: 原文：[https://www.kdnuggets.com/2021/07/mlops-best-practices.html](https://www.kdnuggets.com/2021/07/mlops-best-practices.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/07/mlops-best-practices.html](https://www.kdnuggets.com/2021/07/mlops-best-practices.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Siddharth (Sid) Kashiramka](https://www.linkedin.com/in/siddharthkashiramka/),
    (Sr. Manager, Platform, Capital One),'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Siddharth (Sid) Kashiramka](https://www.linkedin.com/in/siddharthkashiramka/)，（高级经理，平台，Capital
    One），'
- en: '[Anshuman Guha](https://www.linkedin.com/in/anshumanguha/), (Principal Data
    Scientist, Card DS, Capital One),'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[Anshuman Guha](https://www.linkedin.com/in/anshumanguha/)，（首席数据科学家，Card DS，Capital
    One），'
- en: '[DeCarlos Taylor](https://www.linkedin.com/in/decarlos-taylor-igm4844b4b/),
    (Director, Card DS, Capital One)**.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[DeCarlos Taylor](https://www.linkedin.com/in/decarlos-taylor-igm4844b4b/)，（主任，Card
    DS，Capital One）**。'
- en: 'It is now well recognized, across multiple industries, that predictive modeling
    and machine learning may provide tremendous value for organizations that leverage
    these techniques as an integral part of their business model. Many organizations
    spanning both the public and private sectors have adopted a data-driven business
    strategy where insights derived from either comprehensive data analyses or the
    application of highly complex machine learning algorithms are used to influence
    key business or operational decisions. Although there are many organizations leveraging
    machine learning at scale, and the variety of use cases abound at a high level,
    the overall machine learning life cycle has a common structure among all organizations
    irrespective of the specific use case or application. Specifically, for any organization
    leveraging data science at scale, the machine learning life cycle is defined by
    four key components: Model Development, Model Deployment, Model Monitoring, and
    Model Governance (see Figure 1).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在多个行业中，人们普遍认识到预测建模和机器学习可能为那些将这些技术作为其商业模式核心部分的组织提供巨大的价值。许多跨越公共和私营部门的组织已经采用了数据驱动的商业战略，利用从全面的数据分析或高度复杂的机器学习算法中得出的洞察来影响关键的业务或运营决策。虽然有许多组织在大规模应用机器学习，并且各种使用案例层出不穷，但整体机器学习生命周期在所有组织中具有相似的结构，无论具体的使用案例或应用是什么。具体来说，对于任何大规模利用数据科学的组织，机器学习生命周期由四个关键组成部分定义：模型开发、模型部署、模型监控和模型治理（见图1）。
- en: '![](../Images/241b581701323b0a841a67db8eee24f3.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/241b581701323b0a841a67db8eee24f3.png)'
- en: '***Figure 1*** *- Four critical steps of the machine learning life cycle.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '***图1*** *- 机器学习生命周期的四个关键步骤。*'
- en: Most data scientists are well versed in the *model development* part of the
    machine learning life cycle and have a high degree of familiarity with complex
    data queries (e.g., SQL), data wrangling, feature engineering, and algorithm training.
    Further, the performance *model monitoring* component of the lifecycle is somewhat
    germane to a data scientist's function. The performance of models over time in
    response to changing data distributions or for new application domains can be
    monitored using relevant statistical metrics of model performance (e.g., mean
    squared error, precision-recall, etc.) that many data scientists are familiar
    with. In addition, the *model governance* requirements, depending on the industry,
    are generally well defined (although not necessarily well executed!) and are often
    articulated at great length in organizational policy documents and via general
    regulatory mandates issued by a governing body.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据科学家对机器学习生命周期中的*模型开发*部分非常熟悉，并且对复杂的数据查询（例如SQL）、数据处理、特征工程和算法训练有很高的熟练度。此外，生命周期中的*模型监控*部分在某种程度上与数据科学家的职能相关。模型在时间推移中对数据分布变化或新应用领域的性能可以使用相关的统计指标（例如均方误差、精确度-召回率等）来监控，这些都是许多数据科学家熟悉的。此外，根据行业不同，*模型治理*的要求通常定义明确（尽管执行情况未必理想！），并且在组织政策文件和由监管机构发布的一般性法规中常有详细阐述。
- en: 'Although the model development, monitoring, and governance components of the
    machine learning life cycle are complex and fraught with challenges, the ***model
    deployment*** *(or “productionizing”)* component of the process is where many
    organizations seem to have the most trouble. [A recent report](https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/)
    [1] suggests that 87% percent of data science projects never actually reach production,
    and several factors, including:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管机器学习生命周期的模型开发、监控和治理组件复杂且充满挑战，但***模型部署*** *(或称“生产化”)* 组件是许多组织似乎最难应对的部分。[一份最新报告](https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/)
    [1] 表示87%的数据科学项目最终未能进入生产，原因包括：
- en: Lack of necessary deployment expertise in the organization [2].
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组织内缺乏必要的部署专业知识[2]。
- en: The deployment process requires close interplay between multiple stakeholders,
    including data scientists, software engineers, and platform engineers, which may
    be challenging to manage effectively.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署过程需要多个利益相关者之间的紧密合作，包括数据科学家、软件工程师和平台工程师，这可能会带来有效管理的挑战。
- en: There may be a lack of transparency between key stakeholders, and design choices
    adopted by one group may not be fully compatible with baseline requirements or
    practices of other key stakeholders.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键利益相关者之间可能缺乏透明度，一个小组采用的设计选择可能与其他关键利益相关者的基本要求或实践不完全兼容。
- en: Productionizing models at scale may require a resilient, well-architected model
    deployment platform that enables a variety of machine learning applications and
    provides any additional supporting functionality.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大规模生产化模型可能需要一个具有弹性且设计良好的模型部署平台，以支持各种机器学习应用程序并提供任何额外的支持功能。
- en: Deployment platform service level agreements (e.g., maximum model response time)
    may be challenging to adhere to without significant software engineering and code
    optimizations.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署平台服务水平协议（例如，最大模型响应时间）可能很难遵守，除非进行大量的软件工程和代码优化。
- en: This is by no means an exhaustive list of challenges that organizations may
    face when deploying and productionizing their models. Although there is much to
    be said regarding each of the items highlighted above, in this article, we focus
    on the last two things that reference the construction of a well-architected deployment
    platform and techniques to consider when packaging machine learning models. Based
    on our experience with management, maintenance, and enhancement of significant
    model deployment platforms and our extensive experience with packaging and performance
    tuning productionized machine learning models, we share below some high-level
    considerations and best practices that we have identified along our journey. We
    hope that this knowledge share will allow other teams and organizations to reach
    their machine learning destination more rapidly and avoid pitfalls that may ultimately
    lead to failure.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这绝不是组织在部署和生产化模型时可能面临的挑战的详尽清单。虽然对于上述每一项内容都有很多需要讨论的地方，但在本文中，我们专注于最后两项内容，即构建良好架构的部署平台和在打包机器学习模型时需要考虑的技术。基于我们在管理、维护和提升重要模型部署平台的经验，以及在打包和性能调优生产化机器学习模型方面的丰富经验，我们在下文中分享了一些我们在过程中识别出的高级考虑因素和最佳实践。我们希望这些知识分享能够帮助其他团队和组织更快速地实现机器学习目标，避免可能导致失败的陷阱。
- en: Machine Learning Deployment Platform design
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习部署平台设计
- en: 'Building a model deployment platform that serves your organization''s needs
    begins with defining the overall set of requirements that potential end-users
    may need [3]. The conditions may span execution, governance, monitoring, or maintenance
    needs, and a few capabilities that you may need to include as you design a robust
    platform may consist of:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 构建满足组织需求的模型部署平台始于定义潜在最终用户可能需要的总体需求[3]。这些条件可能涵盖执行、治理、监控或维护需求，并且在设计一个稳健的平台时，你可能需要包括的一些功能包括：
- en: '**Real-time/batch processing -** ML platforms can support real-time or batch
    or both depending on the business requirements. While making a decision, it is
    essential to factor in your organization''s data quality needs and system responsiveness/latency.
    Batch-driven processes are more tolerant of data quality (DQ) issues because most
    batch processes have ample time to detect the problems after processing and fixing
    the issues before executing customer-facing decisions. On the contrary, for real-time
    scoring, the decisions are made in a matter of seconds. Data quality checks, data
    pulls from external sources, streaming outputs, etc., may add to the overall model
    response latency requirements.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时/批处理 -** 机器学习平台可以支持实时或批处理，或者两者兼有，具体取决于业务需求。在做出决策时，必须考虑组织的数据质量需求和系统响应/延迟。批处理流程对数据质量问题更具耐受性，因为大多数批处理流程有足够的时间在处理后检测问题并在执行客户决策前修复这些问题。相反，对于实时评分，决策是在几秒钟内做出的。数据质量检查、从外部源提取数据、流数据输出等，可能会增加模型响应延迟的要求。'
- en: '**Model registry/inventory -** While building the platform, lay out the plan
    for the legacy/older versions of the model (ex. do they get decommissioned or
    are there regulatory requirements to have these models for a certain amount of
    time, etc.). Having a consistent and automated path for putting all models to
    production and knowing how often the models are updated will help reduce significant
    technical debt from the legacy models.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型注册/清单 -** 在构建平台时，制定关于遗留/旧版本模型的计划（例如，它们是否会被退役或是否有监管要求这些模型保留一定时间等）。拥有一个一致且自动化的路径将所有模型投入生产，并了解模型更新的频率，将有助于减少遗留模型带来的技术债务。'
- en: '**Model governance and risk management -** In a regulated environment, the
    ability to audit the system is essential. Unfortunately, model governance may
    sometimes get overlooked as one of the critical capabilities. It is, however,
    necessary to build controls for robust tracking and management throughout the
    model''s entire life cycle in the ML system. A few considerations here include
    collecting and persisting detailed model metadata, documenting data lineage, establishing
    controls, and versioning files and models.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型治理和风险管理 -** 在受监管的环境中，系统审计的能力至关重要。不幸的是，模型治理有时可能被忽视作为一个关键能力。然而，构建强大的跟踪和管理控制对于模型在机器学习系统中的整个生命周期是必要的。这里的一些考虑因素包括收集和持久化详细的模型元数据、记录数据血统、建立控制措施，以及对文件和模型进行版本管理。'
- en: '**Concurrent model execution -** Understand the demand/volume of models being
    called and if the model needs to execute concurrently. In most cases, more than
    one model is running in the background. ML platform should ensure it can scale
    to the growing demand while simultaneously keeping execution latency to a minimum.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并发模型执行 -** 了解被调用模型的需求/量以及模型是否需要并发执行。在大多数情况下，多个模型在后台同时运行。机器学习平台应确保能够适应不断增长的需求，同时保持最小的执行延迟。'
- en: '**Model monitoring -** For models to be effective and valuable, they need to
    be monitored throughout their entire life cycle, from the time they are put into
    production until they are retired. Effective ML models create value by reducing
    the risk of putting biased models in production settings but also help with raising
    the data science team productivity by identifying bottlenecks faster.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型监控 -** 为了使模型有效和有价值，它们需要在整个生命周期内进行监控，从投入生产时开始，直到退役。有效的机器学习模型通过减少将偏差模型投入生产环境的风险来创造价值，同时也通过更快地识别瓶颈来提高数据科学团队的生产力。'
- en: 'Enabling all of this functionality is time-consuming (sometimes even multiple
    years of effort) and requires complex designs and infrastructure setup. Your platform
    architecture should help achieve this functionality in your organization. The
    architecture you finalize should be supported by solid engineering best practices.
    For ML systems specifically, the few most relevant best practices are:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 启用所有这些功能是耗时的（有时甚至需要多年的努力）并且需要复杂的设计和基础设施设置。你的平台架构应该有助于实现这些功能。最终确定的架构应受到扎实工程最佳实践的支持。对于机器学习系统，几个最相关的最佳实践是：
- en: Reproducibility - the output of each component must be replicable for any version
    in time
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可重复性 - 每个组件的输出必须在任何版本的时间点上都可以复制。
- en: Scalability - the system should be able to adapt to dynamic changes to volume
    with minimal response time.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展性 - 系统应能够适应动态变化的容量，并保持最小的响应时间。
- en: Automation - eliminate manual steps wherever possible to reduce error chances.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化 - 尽可能消除手动步骤，以减少错误发生的机会。
- en: Deploying Models to the Platform
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将模型部署到平台
- en: 'Another critical design choice for the platform is the tools and techniques
    used for model deployment [4]. A common standard for model deployments is using
    containers that are essentially a fully packaged and portable computing environment
    for a given application. (For more details on containerization, see [link](https://www.citrix.com/solutions/app-delivery-and-security/what-is-containerization.html)
    [5]). From a platform perspective, containerized models offer several benefits
    [6], including:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 平台的另一个关键设计选择是用于模型部署的工具和技术[4]。模型部署的一个常见标准是使用容器，这实际上是针对给定应用程序的完整打包和可移植的计算环境。（有关容器化的更多详细信息，请参见[链接](https://www.citrix.com/solutions/app-delivery-and-security/what-is-containerization.html)
    [5]）。从平台的角度来看，容器化模型提供了几个好处[6]，包括：
- en: Flexibility and portability due to the abstraction of the container from the
    host OS
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于容器与宿主操作系统的抽象，实现了灵活性和可移植性
- en: Better manageability due to the ability to deploy new versions of models without
    interfering with the ones in production
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于能够部署新版本的模型而不会干扰生产中的模型，因此可更好地进行管理
- en: Greater efficiency and faster model deployment
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高的效率和更快的模型部署
- en: Although the use of model containers provides several benefits, several technical
    challenges arise when packaging and deploying containerized models. Ensuring that
    the containerized model returns correct output scores is critical, especially
    in highly regulated industries where model scoring errors may lead to negative
    consequences for the organization or its customer base. Therefore, it is essential
    to have a robust test harness that includes testing the containerized model on
    input data (perhaps simulated) spanning multiple scenarios, including in-range,
    out-of-range, missing, and edge-case values.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用模型容器提供了几个好处，但在打包和部署容器化模型时会出现一些技术挑战。确保容器化模型返回正确的输出分数至关重要，特别是在高度受监管的行业中，模型评分错误可能会对组织或其客户基础产生负面后果。因此，必须拥有一个稳健的测试框架，包括在多个场景下（包括在范围内、范围外、缺失和边界值）的输入数据（也许是模拟数据）上测试容器化模型。
- en: Another critical consideration for containerized model deployment is the overall
    response time ("latency") of the packaged model. High latency models can lead
    to a number of negative consequences, including increased platform costs (mainly
    when a cloud service provider hosts the platform) or lost opportunity if potential
    customers do not complete their applications or transactions due to slow response
    of the platform as high latency models execute in the background. Therefore, containerized
    models' performance tuning to reduce response latency is a crucial part of the
    deployment process.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键考虑因素是容器化模型部署的整体响应时间（“延迟”）。高延迟模型可能会导致许多负面后果，包括平台成本的增加（主要是在云服务提供商托管平台时）或由于平台响应缓慢而导致潜在客户未能完成申请或交易，从而失去机会。因此，容器化模型的性能调优以减少响应延迟是部署过程中的重要部分。
- en: Latency Improvement with cProfiler
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 cProfiler 改善延迟
- en: Depending on the application or baseline thresholds imposed by the organization,
    model containers may be required to return an output within 200-300 milliseconds
    (or less!). In our experience, significant code optimization is often required
    to meet this requirement. Traditionally, the code optimization process involves
    individual timing blocks of code, identifying the rate-limiting functions, and
    then performance tuning those code blocks. A popular open-source tool available
    for this process is [cProfiler](https://docs.python.org/3/library/profile.html)
    [7] that tracks how frequently various methods and procedures are called and monitors
    the overall execution time. As an example, consider the call graph represented
    in Figure 2 below. This call graph suggests that function ‘plustwo’ is called
    10X times and consumes 88%+ of the scoring time. So, any modifications that either
    reduce the number of function calls to this rate-limiting step or optimizations
    within the ‘plustwo’ function that reduce its runtime will lessen the overall
    scoring time for the model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据应用程序或组织施加的基准阈值，模型容器可能需要在200-300毫秒（或更短时间）内返回输出。根据我们的经验，通常需要进行显著的代码优化以满足这一要求。传统上，代码优化过程包括对单独的代码块进行计时，识别限制速率的函数，然后对这些代码块进行性能调整。一个流行的开源工具是[cProfiler](https://docs.python.org/3/library/profile.html)
    [7]，它跟踪各种方法和过程被调用的频率，并监控整体执行时间。举个例子，考虑下面图2中表示的调用图。这个调用图表明函数‘plustwo’被调用了10次，并消耗了88%+的评分时间。因此，任何减少对这个限制步骤的函数调用次数的修改，或优化‘plustwo’函数以减少其运行时间的措施，都将减少模型的整体评分时间。
- en: '![](../Images/8fcc6ef7b9c6e5b0cd34b7a4f861d327.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fcc6ef7b9c6e5b0cd34b7a4f861d327.png)'
- en: '***Figure 2 -*** *Code Performance Analysis with Cprofiler.*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '***图2 -*** *使用Cprofiler的代码性能分析。*'
- en: Load Testing
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负载测试
- en: For many organizations, machine learning models may eventually be deployed to
    customer-facing platforms, e.g., retail websites powered by recommender systems
    or financial services websites where models are used to decide applicants in real-time.
    The number of requests that the platform must process can be highly variable,
    and organizations may experience periods of low activity (few calls to the model
    container) followed by periods of extremely high load where the platform, and
    models, may be required to process a massive volume of requests resulting from
    high customer demand. Therefore, testing of the model container's robustness (and
    accuracy!) under high load is also a key consideration. A valuable tool for this
    type of testing is [Locust](https://locust.io/) [8], an open-source load testing
    tool that is distributed and scalable, has a web-based user interface, and can
    accommodate test scenarios written in Python.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多组织而言，机器学习模型最终可能会部署到面向客户的平台上，例如，由推荐系统支持的零售网站或用于实时决定申请者的金融服务网站。平台必须处理的请求数量可能高度变化，组织可能会经历低活动期（对模型容器的调用较少），随后是极高负载期，在这些期间，平台和模型可能需要处理大量请求，这些请求源于高客户需求。因此，在高负载下测试模型容器的鲁棒性（和准确性！）也是一个关键考虑因素。一个有价值的工具是[Locust](https://locust.io/)
    [8]，这是一个开源负载测试工具，具有分布式和可扩展性，拥有基于Web的用户界面，并可以处理用Python编写的测试场景。
- en: '![](../Images/2cd0a367dbb3eb54be828f3ccafc6856.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2cd0a367dbb3eb54be828f3ccafc6856.png)'
- en: '***Figure 3 -*** *Full latency test with Locust.*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '***图3 -*** *使用Locust的完整延迟测试。*'
- en: A sample output of Locust is presented in Figure 3 that resulted from a test
    scenario where ten requests per second were streamed to a model for a period of
    10 minutes. The output shows the total number of requests and also reports the
    response time for several percentiles. In this example, the 95th and 99th percentile
    have latencies of 170 and 270 milliseconds, respectively, depending on the platform
    thresholds, may or may not be accepted and may indicate that additional performance
    optimizations may be required.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图3展示了Locust的一个样本输出，这个输出来自于一个测试场景，其中每秒向模型发送十个请求，持续了10分钟。输出显示了请求的总数，并报告了几个百分位的响应时间。在这个例子中，95百分位和99百分位的延迟分别为170毫秒和270毫秒，根据平台的阈值，这些延迟可能被接受，也可能表明需要进行额外的性能优化。
- en: Conclusion
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: 'To summarize, below are the key takeaways from this paper:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，以下是本文的主要要点：
- en: Build your ML system/platform taking into consideration the requirements from
    all your stakeholders, including model compliance teams, businesses, and the end
    customer.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在构建你的机器学习系统/平台时，需要考虑所有利益相关者的需求，包括模型合规团队、业务部门和最终客户。
- en: Align on the architecture approach for the ML systems that unlock required capabilities
    in your organization.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于能够解锁组织中所需功能的机器学习系统，确保在架构方法上达成一致。
- en: Select the appropriate tools for model deployment; model containerization is
    the most common method of deploying ML models.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择合适的工具进行模型部署；模型容器化是最常见的机器学习模型部署方法。
- en: Perform thorough model validation tests on the deployment containers and packages.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对部署容器和包进行彻底的模型验证测试。
- en: Independent code base replicating the entire data processing and deployment
    scoring pipeline can serve as a ground truth.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制整个数据处理和部署评分流程的独立代码库可以作为基准真相。
- en: Well-documented quality control measures for upstream and downstream processes,
    contingency plans during production issues, and logging can be helpful.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对上下游过程进行充分记录的质量控制措施、生产问题时的应急计划以及日志记录都可能是有帮助的。
- en: References
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] VB Staff. “Why do 87% of data science projects never make it into production?”
    venturebeat.com [https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/](https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/)
    (accessed July 05, 2021)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] VB Staff. “为何87%的数据科学项目从未进入生产？” venturebeat.com [https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/](https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/)（访问日期：2021年7月5日）'
- en: '[2] Chris. “Machine Learning Is Getting Easier. Software Engineering Is Still
    Hard” towardsdatascience.com [https://towardsdatascience.com/machine-learning-is-getting-easier-software-engineering-is-still-hard-d4e8320bc046](https://towardsdatascience.com/machine-learning-is-getting-easier-software-engineering-is-still-hard-d4e8320bc046)
    (accessed July 05, 2021)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Chris. “机器学习变得越来越简单，软件工程仍然困难” towardsdatascience.com [https://towardsdatascience.com/machine-learning-is-getting-easier-software-engineering-is-still-hard-d4e8320bc046](https://towardsdatascience.com/machine-learning-is-getting-easier-software-engineering-is-still-hard-d4e8320bc046)（访问日期：2021年7月5日）'
- en: '[3] Mckinsey Podcast. “Companies adopting AI across the organization are investing
    as much in people and processes as in technology.” mckinsey.com  https://www.mckinsey.com/business-functions/mckinsey-digital/our-insights/getting-to-scale-with-artificial-intelligence
    (accessed July 05, 2021)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Mckinsey Podcast. “公司在整个组织中采用人工智能的投资与对人员和流程的投资一样多。” mckinsey.com [https://www.mckinsey.com/business-functions/mckinsey-digital/our-insights/getting-to-scale-with-artificial-intelligence](https://www.mckinsey.com/business-functions/mckinsey-digital/our-insights/getting-to-scale-with-artificial-intelligence)（访问日期：2021年7月5日）'
- en: '[4] Assaf Pinhasi. “Deploying Machine Learning models to production — Inference
    service architecture patterns” medium.com'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Assaf Pinhasi. “将机器学习模型部署到生产环境 — 推理服务架构模式” medium.com'
- en: '[https://medium.com/data-for-ai/deploying-machine-learning-models-to-production-inference-service-architecture-patterns-bc8051f70080](https://medium.com/data-for-ai/deploying-machine-learning-models-to-production-inference-service-architecture-patterns-bc8051f70080)
    (accessed July 05, 2021)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://medium.com/data-for-ai/deploying-machine-learning-models-to-production-inference-service-architecture-patterns-bc8051f70080](https://medium.com/data-for-ai/deploying-machine-learning-models-to-production-inference-service-architecture-patterns-bc8051f70080)（访问日期：2021年7月5日）'
- en: '[5] Citrix. “What is containerization and how does it work?” citrix.com'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Citrix. “什么是容器化以及它是如何工作的？” citrix.com'
- en: '[https://www.citrix.com/solutions/application-delivery-controller/what-is-containerization.html](https://www.citrix.com/solutions/application-delivery-controller/what-is-containerization.html)
    (accessed July 05, 2021)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.citrix.com/solutions/application-delivery-controller/what-is-containerization.html](https://www.citrix.com/solutions/application-delivery-controller/what-is-containerization.html)（访问日期：2021年7月5日）'
- en: '[6] Christopher. G.S.“How to Deploy Machine Learning Models” christopherergs.com'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Christopher. G.S.“如何部署机器学习模型” christopherergs.com'
- en: '[https://christophergs.com/machine%20learning/2019/03/17/how-to-deploy-machine-learning-models/](https://christophergs.com/machine%20learning/2019/03/17/how-to-deploy-machine-learning-models/)
    (accessed July 05, 2021)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://christophergs.com/machine%20learning/2019/03/17/how-to-deploy-machine-learning-models/](https://christophergs.com/machine%20learning/2019/03/17/how-to-deploy-machine-learning-models/)（访问日期：2021年7月5日）'
- en: '[7] [https://docs.python.org/3/library/profile.html](https://docs.python.org/3/library/profile.html)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [https://docs.python.org/3/library/profile.html](https://docs.python.org/3/library/profile.html)'
- en: '[8] [https://locust.io](https://locust.io)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [https://locust.io](https://locust.io)'
- en: '**Related:**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[MLOps is an Engineering Discipline: A Beginner’s Overview](https://www.kdnuggets.com/2021/07/mlops-engineering-discipline.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MLOps 是一种工程学科：初学者概述](https://www.kdnuggets.com/2021/07/mlops-engineering-discipline.html)'
- en: '[Continuous Training for Machine Learning – a Framework for a Successful Strategy](https://www.kdnuggets.com/2021/04/continuous-training-machine-learning.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习的持续培训——成功策略框架](https://www.kdnuggets.com/2021/04/continuous-training-machine-learning.html)'
- en: '[Easy MLOps with PyCaret + MLflow](https://www.kdnuggets.com/2021/05/easy-mlops-pycaret-mlflow.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 PyCaret 和 MLflow 轻松实现 MLOps](https://www.kdnuggets.com/2021/05/easy-mlops-pycaret-mlflow.html)'
- en: '* * *'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行 IT 工作'
- en: '* * *'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[MLOps: The Best Practices and How To Apply Them](https://www.kdnuggets.com/2022/04/mlops-best-practices-apply.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MLOps：最佳实践及其应用方法](https://www.kdnuggets.com/2022/04/mlops-best-practices-apply.html)'
- en: '[MLOps Best Practices You Should Know](https://www.kdnuggets.com/2023/04/mlops-best-practices-know.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MLOps 的最佳实践，你应该知道](https://www.kdnuggets.com/2023/04/mlops-best-practices-know.html)'
- en: '[5 Best End-to-End Open Source MLOps Tools](https://www.kdnuggets.com/5-best-end-to-end-open-source-mlops-tools)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5 个最佳端到端开源 MLOps 工具](https://www.kdnuggets.com/5-best-end-to-end-open-source-mlops-tools)'
- en: '[Best Practices for Creating Domain-Specific AI Models](https://www.kdnuggets.com/2022/07/best-practices-creating-domainspecific-ai-models.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[创建领域特定 AI 模型的最佳实践](https://www.kdnuggets.com/2022/07/best-practices-creating-domainspecific-ai-models.html)'
- en: '[Integrating ChatGPT Into Data Science Workflows: Tips and Best Practices](https://www.kdnuggets.com/2023/05/integrating-chatgpt-data-science-workflows-tips-best-practices.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[将 ChatGPT 融入数据科学工作流程：提示与最佳实践](https://www.kdnuggets.com/2023/05/integrating-chatgpt-data-science-workflows-tips-best-practices.html)'
- en: '[5 Python Best Practices for Data Science](https://www.kdnuggets.com/5-python-best-practices-for-data-science)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中 5 个 Python 最佳实践](https://www.kdnuggets.com/5-python-best-practices-for-data-science)'
