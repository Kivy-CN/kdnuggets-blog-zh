- en: PySpark for Data Science
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark 数据科学
- en: 原文：[https://www.kdnuggets.com/2023/02/pyspark-data-science.html](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/02/pyspark-data-science.html](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)
- en: '![PySpark for Data Science](../Images/2ebe106d056da2fc385c3f18101024ef.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark 数据科学](../Images/2ebe106d056da2fc385c3f18101024ef.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: '[PySpark](https://spark.apache.org/docs/latest/api/python/index.html) is an
    Python interference for Apache Spark. It is an open-source library that allows
    you to build Spark applications and analyze the data in a distributed environment
    using a PySpark shell. You can use PySpark for batch processing, running SQL queries,
    Dataframes, real-time analytics, machine learning, and graph processing.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[PySpark](https://spark.apache.org/docs/latest/api/python/index.html) 是 Apache
    Spark 的 Python 接口。它是一个开源库，允许你构建 Spark 应用程序，并在分布式环境中使用 PySpark shell 分析数据。你可以使用
    PySpark 进行批处理、运行 SQL 查询、数据框、实时分析、机器学习和图形处理。'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Advantages of using Spark:**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 Spark 的优势：**'
- en: In-memory caching allows real-time computation and low latency.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内存缓存支持实时计算和低延迟。
- en: 'It can be deployed using multiple ways: Spark’s cluster manager, Mesos, and
    Hadoop via Yarn.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它可以通过多种方式部署：Spark 的集群管理器、Mesos 和通过 Yarn 的 Hadoop。
- en: User-friendly API is available for all popular languages that hide the complexity
    of running distributed systems.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供用户友好的 API 适用于所有流行的语言，隐藏了运行分布式系统的复杂性。
- en: It is 100x faster than Hadoop MapReduce in memory and 10x faster on disk.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它在内存中比 Hadoop MapReduce 快 100 倍，在磁盘上快 10 倍。
- en: In this code-based tutorial, we will learn how to initial spark session, load
    the data, change the schema, run SQL queries, visualize the data, and train the
    machine learning model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个基于代码的教程中，我们将学习如何初始化 Spark 会话、加载数据、更改 Schema、运行 SQL 查询、可视化数据，并训练机器学习模型。
- en: Getting Started
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门指南
- en: If you want to use PySpark on a local machine, you need to install Python, Java,
    Apache Spark, and PySpark. If you want to avoid all of that, you can use [Google
    Colab](https://colab.research.google.com/) or [Kaggle](https://www.kaggle.com/).
    Both platforms come with pre-installed libraries, and you can start coding within
    seconds.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在本地机器上使用 PySpark，需要安装 Python、Java、Apache Spark 和 PySpark。如果想避免这些步骤，你可以使用
    [Google Colab](https://colab.research.google.com/) 或 [Kaggle](https://www.kaggle.com/)。这两个平台都自带预安装的库，你可以在几秒钟内开始编程。
- en: In the Google Colab Notebook, we will start by installing `pyspark` and `py4j`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Colab Notebook 中，我们将首先安装 `pyspark` 和 `py4j`。
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: After that, we will need to provide the session name to initialize the Spark
    session.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们需要提供会话名称以初始化 Spark 会话。
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Importing the Data using PySpark
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PySpark 导入数据
- en: In this tutorial, we will be using [Global Spotify Weekly Chart](https://www.kaggle.com/datasets/kabhishm/global-spotify-weekly-chart)
    from Kaggle. It contains information about the artist and the songs on the Spotify
    global weekly chart.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将使用来自 Kaggle 的 [Global Spotify Weekly Chart](https://www.kaggle.com/datasets/kabhishm/global-spotify-weekly-chart)。它包含有关艺术家和
    Spotify 全球周榜上歌曲的信息。
- en: Just like Pandas, we can load the data from CSV to dataframe using `spark.read.csv`
    function and display Schema using `printSchema()` function.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 Pandas 一样，我们可以使用 `spark.read.csv` 函数从 CSV 加载数据到数据框，并使用 `printSchema()` 函数显示
    Schema。
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Output:**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: As we can observe, PySpark has loaded all of the columns as a string. To perform
    exploratory data analysis, we need to change the Schema.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所观察到的，PySpark 已将所有列加载为字符串。为了进行探索性数据分析，我们需要更改 Schema。
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Updating the Schema
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新 Schema
- en: To change the schema, we need to create a new data schema that we will add to
    `StructType` function. You need to make sure that each column field is getting
    the right data type.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要更改模式，我们需要创建一个新的数据模式，并将其添加到`StructType`函数中。你需要确保每个列字段具有正确的数据类型。
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Then, we will load the CSV files using extra argument `schema`. After that,
    we will print the schema to check if the correct changes were made.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用额外的参数`schema`加载CSV文件。之后，我们将打印模式以检查是否做了正确的更改。
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Output:**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: As we can see, we have different data types for the columns.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，列的数据类型各不相同。
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Exploring the Data
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索数据
- en: You can explore your data as a dataframe by using `toPandas()` function.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`toPandas()`函数将数据作为数据框进行探索。
- en: '**Note:** we have used `limit` to display the first five rows. It is similar
    to SQL commands. This means that we can use PySpark Python API for SQL command
    to run queries.'
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 我们使用了`limit`来显示前五行。这与SQL命令类似。这意味着我们可以使用PySpark Python API执行SQL命令以运行查询。'
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![PySpark for Data Science](../Images/edeb0c2579ecd4916d885888acf5f4e5.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark数据科学](../Images/edeb0c2579ecd4916d885888acf5f4e5.png)'
- en: Just like pandas, we can use `describe()` function to display a summary of data
    distribution.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 就像pandas一样，我们可以使用`describe()`函数来显示数据分布的摘要。
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![PySpark for Data Science](../Images/53998c47fa57c853807d74126e467011.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark数据科学](../Images/53998c47fa57c853807d74126e467011.png)'
- en: The `count()` function used for displaying number of rows. Read [Pandas API
    on Spark](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/index.html)
    to learn about similar APIs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`count()`函数用于显示行数。阅读 [Pandas API on Spark](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/index.html)
    以了解类似的API。'
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Column Manipulation
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 列操作
- en: You can rename your column by using `withColumnRenamed` function. It requires
    an old name and a new name as string.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`withColumnRenamed`函数重命名列。它需要旧名称和新名称作为字符串。
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![PySpark for Data Science](../Images/3d14a8dd2bf66d7283cfb5810f0020e9.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark数据科学](../Images/3d14a8dd2bf66d7283cfb5810f0020e9.png)'
- en: To drop single or multiple columns, you can use `drop()` function.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要删除单个或多个列，你可以使用`drop()`函数。
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![PySpark for Data Science](../Images/6d555820029802ccae6b87b18c37cf10.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark数据科学](../Images/6d555820029802ccae6b87b18c37cf10.png)'
- en: You can use `.na` for dealing with missing valuse. In our case, we are dropping
    all missing values rows.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`.na`来处理缺失值。在我们的例子中，我们删除了所有缺失值的行。
- en: '[PRE12]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Data Analysis
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分析
- en: For data analysis, we will be using PySpark API to translate SQL commands. In
    the first example, we are selecting three columns and display the top 5 rows.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据分析，我们将使用PySpark API来转换SQL命令。在第一个示例中，我们选择了三列，并显示了前5行。
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![PySpark for Data Science](../Images/068bdd7d061c006700e66d9a39e063b1.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark数据科学](../Images/068bdd7d061c006700e66d9a39e063b1.png)'
- en: For more complex queries, we will filter values where ‘Total’ is greater than
    or equal to 600 million to 700 million.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的查询，我们将筛选“Total”大于或等于6亿到7亿的值。
- en: <fo not="" size="+1">**Note:** you can also use `df.Total.between(600000000,
    700000000)` to filter out records.</fo>
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: <fo not="" size="+1">**注意：** 你还可以使用`df.Total.between(600000000, 700000000)`来筛选记录。</fo>
- en: '[PRE14]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![PySpark for Data Science](../Images/7858ab674eb36cec9d98c63941350995.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark数据科学](../Images/7858ab674eb36cec9d98c63941350995.png)'
- en: Write if/else statement to create a categorical column using `when` function.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 编写if/else语句，使用`when`函数创建分类列。
- en: '[PRE15]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![PySpark for Data Science](../Images/ad64ce3b32231f59d3f35b64409056f1.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark数据科学](../Images/ad64ce3b32231f59d3f35b64409056f1.png)'
- en: You can use all of the SQL commands as Python API to run a complete query.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将所有SQL命令作为Python API运行完整的查询。
- en: '[PRE16]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![PySpark for Data Science](../Images/6b9760cfc2a485cf69d86f84929f0699.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark数据科学](../Images/6b9760cfc2a485cf69d86f84929f0699.png)'
- en: Data Visualization
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据可视化
- en: Let’s take above query and try to display it as a bar chart. We are plotting
    “artists v.s average song streams” and we are only displaying the top seven artists.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用上述查询，并尝试将其显示为条形图。我们绘制了“艺术家与平均歌曲播放量”的关系，并且仅显示前七名艺术家。
- en: It is awesome, if you ask me.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你问我，这太棒了。
- en: '[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![PySpark for Data Science](../Images/53998c47fa57c853807d74126e467011.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark数据科学](../Images/53998c47fa57c853807d74126e467011.png)'
- en: Saving the Result
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存结果
- en: After processing the data and running analysis, it is the time for saving the
    results.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理数据和运行分析之后，是时候保存结果了。
- en: You can save the results in all of the popular file types, such as CSV, JSON,
    and Parquet.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将结果保存为所有流行的文件类型，例如CSV、JSON和Parquet。
- en: '[PRE18]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Data Pre-Processing
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: In this section, we are preparing the data for the machine learning model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们正在为机器学习模型准备数据。
- en: '**Categorical Encoding**: converting the categorical columns into integers
    using StringIndexer.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**类别编码**：使用 StringIndexer 将类别列转换为整数。'
- en: '**Assembling Features**: assembling important features into one vector column.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**组装特征**：将重要特征组装成一个向量列。'
- en: '**Scaling**: scaling the data using the StandardScaler scaling function.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**缩放**：使用 StandardScaler 缩放函数对数据进行缩放。'
- en: '[PRE19]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![PySpark for Data Science](../Images/1df15b21db2ceea33176c94650c35653.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark for Data Science](../Images/1df15b21db2ceea33176c94650c35653.png)'
- en: KMeans Clustering
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KMeans 聚类
- en: I have already run the Kmean elbow method to find k. If you want to see all
    of the code sources with the output, you can check out my [notebook](https://colab.research.google.com/drive/1O3XKjoqEk_r08nMyVqjRm2ZalL8le4C5?usp=sharing).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经运行了 Kmean 肘部法来找出 k 值。如果你想查看所有的代码源和输出，可以查看我的 [notebook](https://colab.research.google.com/drive/1O3XKjoqEk_r08nMyVqjRm2ZalL8le4C5?usp=sharing)。
- en: Just like scikit-learn, we will provide a number of clusters and train the Kmeans
    clustering model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 scikit-learn 一样，我们将提供多个聚类，并训练 Kmeans 聚类模型。
- en: '[PRE20]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![PySpark for Data Science](../Images/833dff07fe7525d2deb604b2a1a1419f.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark for Data Science](../Images/833dff07fe7525d2deb604b2a1a1419f.png)'
- en: Visualizing Predictions
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化预测
- en: In this part, we will be using a `matplotlib.pyplot.barplot` to display the
    distribution of 4 clusters.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将使用 `matplotlib.pyplot.barplot` 来显示 4 个聚类的分布。
- en: '[PRE21]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![PySpark for Data Science](../Images/73b5ed95db0eec08514c9c4aa78ee445.png)![PySpark
    for Data Science](../Images/a30a4966f0e14616d98c81c26286af2e.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark for Data Science](../Images/73b5ed95db0eec08514c9c4aa78ee445.png)![PySpark
    for Data Science](../Images/a30a4966f0e14616d98c81c26286af2e.png)'
- en: Wrapping Up
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this tutorial, I have given an overview of what you can do using PySpark
    API. The API allows you to perform SQL-like queries, run pandas functions, and
    training models similar to sci-kit learn. You get the best of all worlds with
    distributed computing.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我概述了你可以通过 PySpark API 做些什么。该 API 允许你执行类似 SQL 的查询、运行 pandas 函数，并训练类似于 scikit-learn
    的模型。你可以在分布式计算中获得所有世界的最佳体验。
- en: It outshines a lot of Python packages when dealing with large datasets (>1GB).
    If you are a programmer and just interested in Python code, check our Google Colab
    [notebook](https://colab.research.google.com/drive/1O3XKjoqEk_r08nMyVqjRm2ZalL8le4C5#scrollTo=bWopwzVg1ih0).
    You just have to download and add the data from [Kaggle](https://www.kaggle.com/datasets/kabhishm/global-spotify-weekly-chart)
    to start working on it.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大型数据集（>1GB）时，它比许多 Python 包表现更为出色。如果你是程序员，并且对 Python 代码感兴趣，请查看我们的 Google Colab
    [notebook](https://colab.research.google.com/drive/1O3XKjoqEk_r08nMyVqjRm2ZalL8le4C5#scrollTo=bWopwzVg1ih0)。你只需下载并添加来自
    [Kaggle](https://www.kaggle.com/datasets/kabhishm/global-spotify-weekly-chart)
    的数据即可开始工作。
- en: Do let me know in the comments, if you want me to keep writing code based-tutorials
    for other Python libraries.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望我继续编写其他 Python 库的基于代码的教程，请在评论中告诉我。
- en: '**[Abid Ali Awan](https://www.polywork.com/kingabzpro)** ([@1abidaliawan](https://twitter.com/1abidaliawan))
    is a certified data scientist professional who loves building machine learning
    models. Currently, he is focusing on content creation and writing technical blogs
    on machine learning and data science technologies. Abid holds a Master''s degree
    in Technology Management and a bachelor''s degree in Telecommunication Engineering.
    His vision is to build an AI product using a graph neural network for students
    struggling with mental illness.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Abid Ali Awan](https://www.polywork.com/kingabzpro)** ([@1abidaliawan](https://twitter.com/1abidaliawan))
    是一名认证的数据科学专家，热衷于构建机器学习模型。目前，他专注于内容创作和撰写有关机器学习和数据科学技术的技术博客。Abid 拥有技术管理硕士学位和电信工程学士学位。他的愿景是使用图神经网络为那些与心理疾病作斗争的学生打造一个
    AI 产品。'
- en: More On This Topic
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题
- en: '[Data Validation for PySpark Applications using Pandera](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Pandera 进行 PySpark 应用的数据验证](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，并寻找目标以…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Data Science Minimum: 10 Essential Skills You Need to Know to Start…](https://www.kdnuggets.com/2020/10/data-science-minimum-10-essential-skills.html)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学最低要求：你需要知道的 10 项基础技能](https://www.kdnuggets.com/2020/10/data-science-minimum-10-essential-skills.html)'
- en: '[KDnuggets™ News 22:n06, Feb 9: Data Science Programming…](https://www.kdnuggets.com/2022/n06.html)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets™ 新闻 22:n06, 2月 9: 数据科学编程…](https://www.kdnuggets.com/2022/n06.html)'
- en: '[Data Science Definition Humor: A Collection of Quirky Quotes…](https://www.kdnuggets.com/2022/02/data-science-definition-humor.html)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学定义幽默：一系列古怪的引用…](https://www.kdnuggets.com/2022/02/data-science-definition-humor.html)'
- en: '[5 Data Science Projects to Learn 5 Critical Data Science Skills](https://www.kdnuggets.com/2022/03/5-data-science-projects-learn-5-critical-data-science-skills.html)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5 个数据科学项目，学习 5 个关键的数据科学技能](https://www.kdnuggets.com/2022/03/5-data-science-projects-learn-5-critical-data-science-skills.html)'
