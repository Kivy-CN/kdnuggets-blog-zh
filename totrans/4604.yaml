- en: A 2019 Guide to Semantic Segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/08/2019-guide-semantic-segmentation.html](https://www.kdnuggets.com/2019/08/2019-guide-semantic-segmentation.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](https://www.kdnuggets.com/2019/08/2019-guide-semantic-segmentation.html?page=2#comments)![Figure](../Images/07f5a30cb23b66cb64bace083c9f4c55.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image Source](https://pixabay.com/photos/traffic-locomotion-roadway-mobility-3612474/)'
  prefs: []
  type: TYPE_NORMAL
- en: Semantic segmentation refers to the process of linking each pixel in an image
    to a class label. These labels could include a person, car, flower, piece of furniture,
    etc., just to mention a few.
  prefs: []
  type: TYPE_NORMAL
- en: We can think of semantic segmentation as [image classification](https://heartbeat.fritz.ai/basics-of-image-classification-with-pytorch-2f8973c51864) at
    a pixel level. For example, in an image that has many cars, segmentation will
    label all the objects as car objects. However, a separate class of models known
    as instance segmentation is able to label the separate instances where an object
    appears in an image. This kind of segmentation can be very useful in applications
    that are used to count the number of objects, such as counting the amount of foot
    traffic in a mall.
  prefs: []
  type: TYPE_NORMAL
- en: Some of its primary applications are in autonomous vehicles, human-computer
    interaction, robotics, and [photo editing/creativity tools](https://heartbeat.fritz.ai/momento-animating-memories-with-immersive-gifs-powered-by-mobile-machine-learning-81ff52806ae3).
    For example, semantic segmentation is very crucial in self-driving cars and robotics
    because it is important for the models to understand the context in the environment
    in which they’re operating.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/0789908d1ccda658ce70c44dd3d29c92.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](http://www.cs.toronto.edu/~tingwuwang/semantic_segmentation.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll now look at a number of research papers on covering state-of-the-art
    approaches to building semantic segmentation models, namely:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Weakly- and Semi-Supervised Learning of a Deep Convolutional Network for Semantic
    Image Segmentation](https://heartbeat.fritz.ai/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc#7efa)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Fully Convolutional Networks for Semantic Segmentation](https://heartbeat.fritz.ai/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc#9141)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://heartbeat.fritz.ai/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc#6ad7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic
    Segmentation](https://heartbeat.fritz.ai/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc#3e4b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Multi-Scale Context Aggregation by Dilated Convolutions](https://heartbeat.fritz.ai/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc#851f)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous
    Convolution, and Fully Connected CRFs](https://heartbeat.fritz.ai/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc#60e4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Rethinking Atrous Convolution for Semantic Image Segmentation](https://heartbeat.fritz.ai/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc#a1f6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation](https://heartbeat.fritz.ai/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc#1fc4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation](https://heartbeat.fritz.ai/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc#f083)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Improving Semantic Segmentation via Video Propagation and Label Relaxation](https://heartbeat.fritz.ai/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc#ec16)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Gated-SCNN: Gated Shape CNNs for Semantic Segmentation](https://heartbeat.fritz.ai/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc#b3f5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Passionate about machine learning? Same! We’re curating each week’s biggest
    stories, best tutorials, and latest research so you don’t have to. [Sign up for
    weekly updates delivered to your inbox](https://www.deeplearningweekly.com/newsletter?utm_campaign=dlweekly-newsletter-timesaver4&utm_source=heartbeat).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Weakly- and Semi-Supervised Learning of a Deep Convolutional Network for Semantic
    Image Segmentation (ICCV, 2015)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This paper proposes a solution to the challenge of dealing with weakly-labeled
    data in deep convolutional neural networks (CNNs), as well as a combination of
    data that’s well-labeled and data that’s not properly labeled.
  prefs: []
  type: TYPE_NORMAL
- en: In the paper, a combination of deep CNNs with a fully-connected conditional [random
    field](https://en.wikipedia.org/wiki/Conditional_random_field) is applied.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation](https://arxiv.org/abs/1502.02734?source=post_page---------------------------)**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Deep convolutional neural networks (DCNNs) trained on a large number of images
    with strong pixel-level annotations have...*'
  prefs: []
  type: TYPE_NORMAL
- en: On the PASCAL VOC segmentation benchmark, this model gives a [mean intersection-over-union
    (IOU)](https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/) score
    above 70%. One of the major challenges faced with this kind of model is that it
    requires images that are annotated at the pixel level during training.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/1020f5e46e86498cb31e25b77c7fcc11.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1502.02734.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main contributions of this paper are:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction of Expectation-Maximization algorithms for bounding box or image-level
    training that can be applied to both weakly-supervised and semi-supervised settings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proves that combining weak and strong annotations improves performance. The
    writers of this paper reach 73.9% IOU performance on PASCAL VOC 2012 after merging
    annotations from the MS-COCO datasets and PASCAL datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proves that their approach achieves higher performance by merging a small number
    of pixel-level annotated images and a large number of bounding-box or image-level
    annotated images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure](../Images/70e1751ad5aa724af01578f46e6e8623.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1502.02734.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Fully Convolutional Networks for Semantic Segmentation (PAMI, 2016)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The model proposed in this paper achieves a performance of 67.2% mean IU on
    PASCAL VOC 2012.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1605.06211?source=post_page---------------------------)**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Convolutional networks are powerful visual models that yield hierarchies of
    features. We show that convolutional...*'
  prefs: []
  type: TYPE_NORMAL
- en: Fully-connected networks take an image of any size and generate an output of
    the corresponding spatial dimensions. In this model, [ILSVRC classifiers](http://image-net.org/challenges/LSVRC/)are
    cast into fully-connected networks and augmented for dense prediction using pixel-wise
    loss and in-network up-sampling. Training for segmentation is then done by fine-tuning.
    Fine-tuning is done by back-propagation on the entire network.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/3ae028eb91479ce7bc8825f79bb2b4a4.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1605.06211.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'U-Net: Convolutional Networks for Biomedical Image Segmentation (MICCAI, 2015)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In biomedical image processing, it’s very crucial to get a class label for every
    cell in the image. The biggest challenge in biomedical tasks is that thousands
    of images for training are not easily accessible.
  prefs: []
  type: TYPE_NORMAL
- en: '[**U-Net: Convolutional Networks for Biomedical Image Segmentation**](https://arxiv.org/abs/1505.04597?source=post_page---------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: '*There is large consent that successful training of deep networks requires
    many thousand annotated training samples. In...*'
  prefs: []
  type: TYPE_NORMAL
- en: This paper builds upon the fully convolutional layer and modifies it to work
    on a few training images and yield more precise segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/31f03b8b104d2569ff4f424247711c4f.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1505.04597)'
  prefs: []
  type: TYPE_NORMAL
- en: Since very little training data is available, this model uses data augmentation
    by applying elastic deformations on the available data. As illustrated in figure
    1 above, the network architecture is made up of a contracting path on the left
    and an expansive path on the right.
  prefs: []
  type: TYPE_NORMAL
- en: The contracting path is made up of two 3x3 convolutions. Each of the convolutions
    is followed by a rectified linear unit and a 2x2 max pooling operation for downsampling.
    Every downsampling stage doubles the number of feature channels. The expansive
    path steps include an upsampling of the feature channels. This is followed by
    2x2 up-convolution that halves the number of feature channels. The final layer
    is a 1x1 convolution that is used to map the component feature vectors to the
    required number of classes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/695df8cbe9fd49e377be06a203db4c6b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1505.04597)'
  prefs: []
  type: TYPE_NORMAL
- en: In this model, training is done using the input images, their segmentation maps,
    and a stochastic gradient descent implementation of Caffe. Data augmentation is
    used to teach the network the required robustness and invariance when very little
    training data is used. This model achieved a [mean intersection-over-union (IOU)](https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/) score
    of 92% on one of the experiments.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/819be25a915d06b500daccd66923bf07.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1505.04597)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic
    Segmentation (2017)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The idea behind DenseNets is that having each layer connected to every layer
    in a feed-forward manner makes the network easier to train and more accurate.
  prefs: []
  type: TYPE_NORMAL
- en: '[**The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic
    Segmentation**](https://arxiv.org/abs/1611.09326?source=post_page---------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: '*State-of-the-art approaches for semantic image segmentation are built on Convolutional
    Neural Networks (CNNs). The...*'
  prefs: []
  type: TYPE_NORMAL
- en: The model’s architecture is built in dense blocks of downsampling and upsampling
    paths. The downsampling path has 2 Transitions Down (TD) while the upsampling
    path has 2 Transitions Up (TU). The circle and arrows represent connectivity patterns
    within the network.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/f81b34a6f31353f1cf4225530d1d1699.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1611.09326.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main contributions of this paper are:'
  prefs: []
  type: TYPE_NORMAL
- en: Extends the DenseNet architecture to fully convolutional networks for use in
    semantic segmentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proposes upsampling paths from dense networks that perform better than other
    upsampling paths.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proves that the network can produce state-of-the-art results on standard benchmarks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This model achieves a global accuracy of 88% on the [CamVid dataset](http://mi.eng.cam.ac.uk/research/projects/VideoRec/).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c59a5ac4ebeee0242d1b5d5324ae9b5.png)'
  prefs: []
  type: TYPE_IMG
- en: '![Figure](../Images/d072fcb801ed0438b4ba74eb68b392fe.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1611.09326.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Misconceptions About Semantic Segmentation Annotation](https://www.kdnuggets.com/2022/01/misconceptions-semantic-segmentation-annotation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Power of a Semantic Layer: A Data Engineer''s Guide](https://www.kdnuggets.com/2023/10/cube-power-of-a-semantic-layer-a-data-engineers-guide)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How Semantic Vector Search Transforms Customer Support Interactions](https://www.kdnuggets.com/how-semantic-vector-search-transforms-customer-support-interactions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Semantic Search with Vector Databases](https://www.kdnuggets.com/semantic-search-with-vector-databases)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Semantic Layer: The Backbone of AI-powered Data Experiences](https://www.kdnuggets.com/2023/10/cube-semantic-layer-backbone-aipowered-data-experiences)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Reasons Why a Universal Semantic Layer is Beneficial to Your Data Stack](https://www.kdnuggets.com/2024/01/cube-6-reasons-why-a-universal-semantic-layer-is-beneficial)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
