- en: 'More Data or Better Algorithms: The Sweet Spot'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/01/more-data-better-algorithms.html](https://www.kdnuggets.com/2017/01/more-data-better-algorithms.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png)[comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By Erik Bernhardsson, CTO (Chief Troll Officer) betterdotcom**'
  prefs: []
  type: TYPE_NORMAL
- en: This blog post [Data sets are the new server rooms](https://medium.com/@josh_nussbaum/data-sets-are-the-new-server-rooms-40fdb5aed6b0)
    makes the point that a bunch of companies raise a ton of money to go get really
    proprietary awesome data as a competitive moat. Because once you have the data,
    you can build a better product, and no one can copy it (at least not very cheaply).
    Ideally you hit a virtuous cycle as well, where usage of your system once it takes
    of gives even more data, which makes the system even better, which attracts more
    users…
  prefs: []
  type: TYPE_NORMAL
- en: The behavior of machine learning models with increasing amounts of data is interesting.
    If you’re building a machine learning based company, first of all you want to
    make sure that *more data gives you better algorithms.*
  prefs: []
  type: TYPE_NORMAL
- en: But that’s a necessary, not sufficient condition. You also need to find a sweet
    spot where
  prefs: []
  type: TYPE_NORMAL
- en: It’s not *too easy*  to collect enough data, because then the value of your
    data is small
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s not *too hard*  to collect enough data, because then you’re going to spend
    way too much money to solve the problem (if ever)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value of the data keeps growing the more data you get
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the recommender system world (where I spent 5 years) it’s not uncommon for
    algorithms to basically converge after say 100M or 1B data points. It depends
    on how many items you have, of course. Some class of models converge before they
    are even useful, in which case obviously there’s no value in more data. Xavier
    Amatriain has an [excellent Quora answer](https://www.quora.com/In-machine-learning-is-more-data-always-better-than-better-algorithms)
    that I urge you to check out if you want to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Anyway let’s simplify the problem. Let’s consider the behavior of some algos:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d804a66faed1c7b1e6e25721424c95f3.png)'
  prefs: []
  type: TYPE_IMG
- en: The blue model represents problems where it’s really easy to get good data pretty
    cheaply. For instance, a cat vs dog classifier is not a useful piece of tech because
    the value of getting that training data is roughly $0\. I would worry about this
    for any company building a general purpose image classifier, for instance. Or
    if you’re building a recommender system with 10k items it might be good enough
    with 10M ratings already. Having 100B ratings isn’t necessarily more valuable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The red model can happen in cases where your data comes from a different distribution
    or your loss function isn’t close to what the product requires. In those cases
    more data is useless at some point. If you’re building a movie recommender system
    by scraping web text it might just converge to a decent but not good enough model.
    (Here’s another hypothesis: maybe collecting *passive* data from driving a car
    isn’t enough to learn how to *actively* drive a car?)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The green model is when your problem may require such a ridiculous amount of
    data that it’s not practical. For instance building a general purpose question
    and answer service that can solve *all the questions in the world* isn’t that
    hard from a ML perspective if you have an infinite amount of data of questions
    and answers. But it’s probably going to be useless with less than terabytes or
    petabytes of input data. If I tried to build a virtual assistant, this would be
    my biggest concern.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some sweet spots where I think you *can* build up a data set, but
    it’s hard. Hard is good because it means once you did it, you have a moat:'
  prefs: []
  type: TYPE_NORMAL
- en: Detect fraud in transaction data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict which loans are going to default
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detect crimes from security footage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hard to remember? Here’s a handy table I made
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/659a06d447ba1a9ffa616d146b4a6455.png)'
  prefs: []
  type: TYPE_IMG
- en: I think the general idea is pretty valid. But is it 100% correct? Probably not.
    Is it oversimplified? Oh yeah, to the extreme.
  prefs: []
  type: TYPE_NORMAL
- en: '[Original post](https://erikbern.com/2016/11/01/are-data-sets-the-new-server-rooms.html).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Erik Bernhardsson (@fulhack)](https://twitter.com/fulhack)**, is CTO
    (Chief Troll Officer) at betterdotcom.'
  prefs: []
  type: TYPE_NORMAL
- en: Ex-Spotify, co-organizing NYC ML meetup, open sourcing sometimes (Luigi, Annoy),
    blogger, dad.
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Data is the New Everything](/2016/03/data-new-every-thing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Random Forests in Python](/2016/12/random-forests-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Most Popular Language For Machine Learning and Data Science Is …](/2017/01/most-popular-language-machine-learning-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Machine Learning’s Sweet Spot: Pure Approaches in NLP and Document Analysis](https://www.kdnuggets.com/2022/05/machine-learning-sweet-spot-pure-approaches-nlp-document-analysis.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Better Leverage Data Science for Business Growth](https://www.kdnuggets.com/2022/08/better-leverage-data-science-business-growth.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[IMPACT: The Data Observability Summit is back November 8th and the…](https://www.kdnuggets.com/2023/10/monte-carlo-impact-the-data-observability-summit-is-back)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Driving Better Business Decisions](https://www.kdnuggets.com/2022/04/informs-driving-better-business-decisions.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Super Bard: The AI That Can Do It All and Better](https://www.kdnuggets.com/2023/05/super-bard-ai-better.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Tips for Writing Better Python Functions](https://www.kdnuggets.com/5-tips-for-writing-better-python-functions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
