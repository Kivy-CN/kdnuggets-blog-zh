- en: A Concise Overview of Standard Model-fitting Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准模型拟合方法的简要概述
- en: 原文：[https://www.kdnuggets.com/2016/05/concise-overview-model-fitting-methods.html](https://www.kdnuggets.com/2016/05/concise-overview-model-fitting-methods.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/05/concise-overview-model-fitting-methods.html](https://www.kdnuggets.com/2016/05/concise-overview-model-fitting-methods.html)
- en: 'In order to explain the differences between alternative approaches to estimating
    the parameters of a model, let''s take a look at a concrete example: Ordinary
    Least Squares (OLS) Linear Regression. The illustration below shall serve as a
    quick reminder to recall the different components of a simple linear regression
    model:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明估计模型参数的不同方法之间的差异，让我们看一个具体的例子：普通最小二乘法（OLS）线性回归。下面的插图将作为快速回顾，以帮助记忆简单线性回归模型的不同组成部分：
- en: '![Simple regresion](../Images/021416dd372001b2ebcefd88bc666f0c.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![简单回归](../Images/021416dd372001b2ebcefd88bc666f0c.png)'
- en: In Ordinary Least Squares (OLS) Linear Regression, our goal is to find the line
    (or hyperplane) that minimizes the vertical offsets. Or, in other words, we define
    the best-fitting line as the line that minimizes the sum of squared errors (SSE)
    or mean squared error (MSE) between our target variable (y) and our predicted
    output over all samples *i* in our dataset of size *n*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在普通最小二乘法（OLS）线性回归中，我们的目标是找到一条线（或超平面），使得垂直偏移最小。换句话说，我们将最佳拟合线定义为使得目标变量（y）和我们对所有样本的预测输出之间的平方误差和（SSE）或均方误差（MSE）最小的线，其中*i*表示数据集中样本的编号，*n*为样本总数。
- en: '![SSE](../Images/fd700aecb123fddccbe1855cb4f39dc2.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![SSE](../Images/fd700aecb123fddccbe1855cb4f39dc2.png)'
- en: 'Now, we can implement a linear regression model for performing ordinary least
    squares regression using one of the following approaches:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用以下方法之一实现一个线性回归模型来执行普通最小二乘法回归：
- en: Solving the model parameters analytically (closed-form equations)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析地求解模型参数（封闭形式的方程）
- en: Using an optimization algorithm (Gradient Descent, Stochastic Gradient Descent,
    Newton's Method, Simplex Method, etc.)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用优化算法（梯度下降法、随机梯度下降法、牛顿法、单纯形法等）
- en: 1) Normal Equations (closed-form solution)
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1) 正规方程（封闭形式解）
- en: 'The closed-form solution may (should) be preferred for "smaller" datasets --
    if computing (a "costly") matrix inverse is not a concern. For very large datasets,
    or datasets where the inverse of XTX may not exist (the matrix is non-invertible
    or singular, e.g., in case of perfect multicollinearity), the GD or SGD approaches
    are to be preferred. The linear function (linear regression model) is defined
    as:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“较小”的数据集，封闭形式的解可能（应该）更受青睐——如果计算（“昂贵的”）矩阵逆不是问题的话。对于非常大的数据集，或者逆矩阵 XTX 可能不存在的数据集（矩阵是不可逆的或奇异的，例如在完美的多重共线性情况下），应优先考虑GD或SGD方法。线性函数（线性回归模型）定义为：
- en: '![Linear model](../Images/18f72e806df0ea656621fbaa5fcd6263.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![线性模型](../Images/18f72e806df0ea656621fbaa5fcd6263.png)'
- en: 'where *y* is the response variable, *x* is an *m*-dimensional sample vector,
    and *w* is the weight vector (vector of coefficients). Note that *w0* represents
    the y-axis intercept of the model and therefore *x0=1*. Using the closed-form
    solution (normal equation), we compute the weights of the model as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*y*是响应变量，*x*是*m*维样本向量，*w*是权重向量（系数向量）。注意，*w0*表示模型的y轴截距，因此*x0=1*。使用封闭形式解（正规方程），我们可以按如下方式计算模型的权重：
- en: '![Closed form](../Images/390bdb586f5ea1904be897034215580a.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![封闭形式](../Images/390bdb586f5ea1904be897034215580a.png)'
- en: 2) Gradient Descent (GD)
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2) 梯度下降法（GD）
- en: Using the Gradient Decent (GD) optimization algorithm, the weights are updated
    incrementally after each epoch (= pass over the training dataset).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度下降法（GD）优化算法，权重在每个周期后（即遍历训练数据集）逐步更新。
- en: 'The cost function *J(⋅)*, the sum of squared errors (SSE), can be written as:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数 *J(⋅)*，即平方误差和（SSE），可以写作：
- en: '![J](../Images/9971ba71f72a930fafe670ba77d3c459.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![J](../Images/9971ba71f72a930fafe670ba77d3c459.png)'
- en: The magnitude and direction of the weight update is computed by taking a step
    in the opposite direction of the cost gradient
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 权重更新的大小和方向是通过在成本梯度的相反方向上迈出一步来计算的
- en: '![dw](../Images/2a8a0c5f09633d23aa090c7a43d83736.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![dw](../Images/2a8a0c5f09633d23aa090c7a43d83736.png)'
- en: 'where *η* is the learning rate. The weights are then updated after each epoch
    via the following update rule:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*η*是学习率。然后在每个周期后，通过以下更新规则更新权重：
- en: '![w_upd](../Images/97ab06d282b695cae3a47b50bd05bbdd.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![w_upd](../Images/97ab06d282b695cae3a47b50bd05bbdd.png)'
- en: 'where Δw is a vector that contains the weight updates of each weight coefficient *w*,
    which are computed as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 Δw 是一个包含每个权重系数 *w* 更新向量的向量，计算方法如下：
- en: '![w_upd explained](../Images/b17273c5b1d049e3c2a73f800eb790f8.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![w_upd 解释](../Images/b17273c5b1d049e3c2a73f800eb790f8.png)'
- en: 'Essentially, we can picture GD optimization as a hiker (the weight coefficient)
    who wants to climb down a mountain (cost function) into a valley (cost minimum),
    and each step is determined by the steepness of the slope (gradient) and the leg
    length of the hiker (learning rate). Considering a cost function with only a single
    weight coefficient, we can illustrate this concept as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，我们可以将梯度下降优化想象成一个徒步旅行者（权重系数），他希望沿着山坡（成本函数）走向山谷（成本最小值），每一步由坡度的陡峭程度（梯度）和徒步者的腿长（学习率）决定。考虑到只有一个权重系数的成本函数，我们可以如下展示这个概念：
- en: '![GD optimization](../Images/9ad85c8821ba0a5eef7027278d573d00.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![GD 优化](../Images/9ad85c8821ba0a5eef7027278d573d00.png)'
- en: '* * *'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT 需求'
- en: '* * *'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关阅读
- en: '[Removing Outliers Using Standard Deviation in Python](https://www.kdnuggets.com/2017/02/removing-outliers-standard-deviation-python.html)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Python 中使用标准差移除异常值](https://www.kdnuggets.com/2017/02/removing-outliers-standard-deviation-python.html)'
- en: '[Developing an Open Standard for Analytics Tracking](https://www.kdnuggets.com/2022/07/developing-open-standard-analytics-tracking.html)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[开发分析追踪的开放标准](https://www.kdnuggets.com/2022/07/developing-open-standard-analytics-tracking.html)'
- en: '[Centroid Initialization Methods for k-means Clustering](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[k-means 聚类的质心初始化方法](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)'
- en: '[Alternative Feature Selection Methods in Machine Learning](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中的替代特征选择方法](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
- en: '[Python String Methods](https://www.kdnuggets.com/2022/12/python-string-methods.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 字符串方法](https://www.kdnuggets.com/2022/12/python-string-methods.html)'
- en: '[11 Python Magic Methods Every Programmer Should Know](https://www.kdnuggets.com/11-python-magic-methods-every-programmer-should-know)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每位程序员都应该知道的 11 个 Python 魔法方法](https://www.kdnuggets.com/11-python-magic-methods-every-programmer-should-know)'
