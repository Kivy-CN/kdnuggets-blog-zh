# 支持向量机（SVM）教程：从示例中学习SVM

> 原文：[https://www.kdnuggets.com/2017/08/support-vector-machines-learning-svms-examples.html/3](https://www.kdnuggets.com/2017/08/support-vector-machines-learning-svms-examples.html/3)

### **核函数**

最后，使SVM运行的秘密。这是我们需要稍微涉及一些数学的地方。

让我们回顾一下迄今为止看到的内容：

1.  对于线性可分的数据，SVM表现得非常好。

1.  对于几乎线性可分的数据，通过使用合适的C值，SVM仍然可以表现得相当好。

1.  对于那些不能线性分离的数据，我们可以将数据投影到一个完美/几乎线性可分的空间，这将问题简化为1或2维，之后我们就可以重新开始。

看起来使SVM普遍适用的一个重要部分是将其投影到更高维度。这就是核函数发挥作用的地方。

首先，稍微偏离一下话题。

支持向量机（SVM）的一个非常惊人的方面是，在它使用的所有数学工具中，确切的投影，甚至维度的数量，都没有出现。你可以用各种数据点（表示为向量）之间的*点积*来表示所有内容。对于*p*维向量*i*和*j*，其中维度的第一个下标标识点，第二个下标表示维度编号：

![](../Images/902cd31e282ee778387dde4bb15d1c37.png)

点积定义为：

![](../Images/bfd27fba90f66326b5f0f9a971760d62.png)

如果我们有*n*个点在数据集中，SVM只需要每对点的点积来找到分类器。仅此而已。当我们想将数据投影到更高维度时也是如此。我们不需要提供SVM确切的投影；我们需要给它投影空间中所有点对的点积。

这很重要，因为这正是核函数的作用。核函数，简称*核函数*，以原始空间中的两个点为输入，直接给出投影空间中的点积。

让我们重新审视之前的投影，看看是否可以提出一个相应的核函数。我们还将跟踪执行投影和然后寻找点积所需的计算数量——以比较使用核函数的效果。

对于一个点*i*：

![](../Images/8e72e7c177f300b021872e4f87444d64.png)

我们对应的投影点是：

![](../Images/d0bfd1ae9bcd0251377040a8edeb24eb.png)

要计算这个投影，我们需要执行以下操作：

+   获取新的第一维度：1次乘法

+   第二维度：1次乘法

+   第三维度：2次乘法

总共1+1+2 = **4次乘法**。

新维度中的点积是：

![](../Images/2b946749d624301f34c7b7cbca94dc21.png)

要计算两个点*i*和*j*的点积，我们需要首先计算它们的投影。因此，需要4+4=8次乘法，然后点积本身需要3次乘法和2次加法。

总的来说是：

+   乘法：8（用于投影）+ 3（在点积中）= 11 次乘法

+   加法：2（在点积中）

这总共是 11 + 2 = **13 次操作**。

我声称这个核函数给了我相同的结果：

![](../Images/9399f8a427d05d64e6a96ea353bf5cb4.png)

我们首先在原始空间中计算向量的点积，然后平方结果。

让我们展开来看我的主张是否确实成立：

![](../Images/85452feb7164d91637298815f179a405.png)

是的。需要多少操作？请看上面的步骤（2）。在二维中计算点积需要2次乘法和1次加法。平方结果是另一个乘法。

所以，总共：

+   乘法：2（用于原始空间的点积）+ 1（用于平方结果）= 3 次乘法

+   加法：1（在原始空间的点积中）

总共 3 + 1 = **4 次操作**。这只是**之前所需操作的31%**。

使用核函数计算所需的点积似乎更快。这在这里可能看起来没什么大不了的：我们看到的是4与13的操作，但对于具有更多维度的输入点以及投影空间有更高维度的大型数据集，计算节省的速度会非常快。这是使用核函数的一个巨大优势。

大多数SVM库已经预装了一些流行的核函数，如*Polynomial, Radial Basis Function (RBF)* 和 *Sigmoid*。当我们不使用投影（如本文的第一个示例）时，我们在原始空间计算点积——这被称为使用*linear kernel*。

这些核函数中的许多给你额外的调节杠杆来进一步优化你的数据。例如，多项式核函数：

![](../Images/1661dfcdaaebf0ea6ee84609f77095fa.png)

允许你选择 *c* 和 *d* （多项式的度数）的值。对于上面的3D投影，我使用了多项式核函数，其中 *c=0* 和 *d=2*。

但我们还没有结束核函数的精彩部分！

还记得我提到过投影到无限维度吗？如果你还没猜到，使其有效的方法是使用正确的核函数。这样，我们实际上无需投影输入数据或担心存储无限维度。

> *核函数计算如果你实际上投影数据时点积会是什么。*

RBF核函数通常用于一个*specific*无限维的投影。我们不会在这里深入探讨其数学原理，但请查看本文末尾的参考资料。

我们怎么能有无限维度，但仍然计算点积？如果你觉得这个问题令人困惑，想一想我们如何计算无限级数的和。这是类似的。点积中有无限项，但恰好存在一个计算它们和的公式。

这回答了我们在前一节中提出的问题。让我们总结一下：

1.  我们通常不会为数据定义特定的投影。相反，我们从可用的核中选择，在某些情况下调整它们，以找到最适合数据的核。

1.  当然，我们可以定义自己的核，或者自己进行投影，但在许多情况下我们不需要这样做。或者我们至少可以先尝试已经可用的选项。

1.  如果我们想要的投影有可用的核，我们更倾向于使用核，因为这样通常更快。

1.  RBF核可以将点投影到无限维空间。

### **SVM库入门**

有相当多的SVM库可以开始练习：

+   [libSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)

+   [SVM-Light](http://svmlight.joachims.org/)

+   [SVMTorch](http://bengio.abracadoudou.com/SVMTorch.html)

许多通用的机器学习库，如[scikit-learn](http://scikit-learn.org/stable/)，也提供SVM模块，这些模块通常是围绕专用SVM库的封装。我的推荐是从经过验证的[*libSVM*](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)开始。

libSVM作为命令行工具提供，但下载包还包括Python、Java和Matlab的封装。只要你有一个libSVM理解的格式的数据文件（下载包中的README解释了这一点，以及其他可用选项），就可以开始使用了。

事实上，如果你需要对不同核、C值等如何影响分隔边界有一个*非常快速*的了解，可以尝试他们的[主页](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)上的“图形界面”。为不同类别标记你的点，选择SVM参数，然后点击运行！

我忍不住，快速标记了一些点：

![](../Images/58389de916f9e685d93975c5f870185d.png)

是的，我并没有让SVM变得容易。

然后我尝试了几个核：

![](../Images/8c6175646063a8d95a54bc9c0737c42e.png)

界面并未显示分隔边界，而是显示SVM学习到的属于特定标签的区域。正如你所见，线性核完全忽略了红色点。它将整个空间视为黄色（-ish绿色）。但RBF核则巧妙地为红色标签划出了一圈！

### **有用资源**

我们主要依赖视觉直觉。虽然这是获得初步理解的好方法，但我强烈建议你深入探究。一个视觉直觉可能不足的例子是理解非线性可分情况下的边际宽度和支持向量。

> *记住这些量是通过优化权衡来决定的*。*除非你查看数学公式，否则有些结果可能会显得反直觉*。

另一个了解数学有帮助的领域是理解核函数。考虑RBF核，我在这篇短文中仅仅介绍了它的一部分。我希望它的“神秘感”——与无限维投影的关系，加上在最后一个数据集（“环”）上的出色结果——能说服你更深入地研究它。

**我推荐的资源：**

1.  [视频讲座：数据学习](https://www.youtube.com/watch?v=MEG35RDD7RA&list=PLCA2C1469EA777F9A) 由亚瑟·阿布-莫斯塔法主讲。第14至第16讲讨论了SVM和核函数。如果你在寻找机器学习的入门，整个系列都非常值得推荐，它在数学和直观之间保持了极好的平衡。

1.  [书籍：统计学习的元素](http://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf) — 特雷弗·哈斯蒂、罗伯特·蒂布希拉尼、杰罗姆·弗里德曼。第4章介绍了SVM的基本思想，而第12章则全面讲解了它。

快乐（机器）学习！

**简历：[阿比谢克·戈斯](https://www.linkedin.com/in/abhishek-ghose-36197624/)** 是24/7公司的高级数据科学家。

[原文](https://blog.statsbot.co/support-vector-machines-tutorial-c1618e635e93)。经许可转载。

**相关：**

+   [支持向量机：简明技术概述](/2016/09/support-vector-machines-concise-technical-overview.html)

+   [什么是支持向量机，我为什么要使用它？](/2017/02/yhat-support-vector-machine.html)

+   [机器学习摘要：支持向量机](/2017/08/machine-learning-abstracts-support-vector-machines.html)

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的IT

* * *

### 相关主题更多

+   [支持向量机：直观方法](https://www.kdnuggets.com/2022/08/support-vector-machines-intuitive-approach.html)

+   [支持向量机的温和介绍](https://www.kdnuggets.com/2023/07/gentle-introduction-support-vector-machines.html)

+   [语义向量搜索如何改变客户支持互动](https://www.kdnuggets.com/how-semantic-vector-search-transforms-customer-support-interactions)

+   [Python向量数据库和向量索引：LLM应用的架构](https://www.kdnuggets.com/2023/08/python-vector-databases-vector-indexes-architecting-llm-apps.html)

+   [选择示例来理解机器学习模型](https://www.kdnuggets.com/2022/11/picking-examples-understand-machine-learning-model.html)

+   [带实例的集成学习](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)
