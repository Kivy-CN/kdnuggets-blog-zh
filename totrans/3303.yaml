- en: 'Data Science for Newbies: An Introductory Tutorial Series for Software Engineers'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学入门：软件工程师的入门教程系列
- en: 原文：[https://www.kdnuggets.com/2017/05/data-science-tutorial-series-software-engineers.html](https://www.kdnuggets.com/2017/05/data-science-tutorial-series-software-engineers.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/05/data-science-tutorial-series-software-engineers.html](https://www.kdnuggets.com/2017/05/data-science-tutorial-series-software-engineers.html)
- en: '**By Harris Brakmić, Software Engineer.**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：Harris Brakmić，软件工程师。**'
- en: '**Editor''s note**: This is an overview of a multi-part tutorial on data science
    for newbies. The author has given the series a different -- tongue-in-cheek --
    title; take it in stride and recognize that the series'' approach and content
    is a fresh look at getting started with various aspects of data science from a
    software engineering perspective.'
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**编辑注**：这是一个多部分教程的概述，旨在为新手介绍数据科学。作者给这个系列起了一个不同的--带有调侃的--标题；接受它，并认识到这个系列的方法和内容是从软件工程的角度出发，对数据科学各个方面的一个全新视角。'
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的快车道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行IT工作'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![PySpark console](../Images/4ae095edf852180b5162a23294b12e52.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark控制台](../Images/4ae095edf852180b5162a23294b12e52.png)'
- en: The PySpark console.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark控制台。
- en: '**[Part 1: Getting Started](http://blog.brakmic.com/data-science-for-losers/)**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第1部分：入门](http://blog.brakmic.com/data-science-for-losers/)**'
- en: To do some serious statistics with Python one should use a proper distribution
    like the one provided by Continuum Analytics. Of course, a manual installation
    of all the needed packages (Pandas, NumPy, Matplotlib etc.) is possible but beware
    the complexities and convoluted package dependencies. In this article we’ll use
    the Anaconda Distribution. The installation under Windows is straightforward but
    avoid the usage of multiple Python installations (for example, Python3 and Python2
    in parallel). It’s best to let Anaconda’s Python binary be your standard Python
    interpreter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要用Python进行一些严肃的统计分析，应该使用像Continuum Analytics提供的适当的发行版。当然，手动安装所有需要的包（如Pandas、NumPy、Matplotlib等）也是可能的，但要注意复杂的和纠缠不清的包依赖关系。在本文中，我们将使用Anaconda
    Distribution。在Windows下的安装是直接的，但避免使用多个Python安装（例如，Python3和Python2并行）。最好让Anaconda的Python二进制文件成为你的标准Python解释器。
- en: '**[Part 2: Analyzing Reddit Comments & Querying Databases](http://blog.brakmic.com/data-science-for-losers-part-2/)**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第2部分：分析Reddit评论与查询数据库](http://blog.brakmic.com/data-science-for-losers-part-2/)**'
- en: Patterns are everywhere but many of them can’t be immediately recognized. This
    is one of the reasons why we’re digging deep holes in our databases, data warehouses,
    and other silos. In this article we’ll use a few more methods from Pandas’ DataFrames
    and generate plots. We’ll also create pivot tables and query an MS SQL database
    via ODBC. SqlAlchemy will be our helper in this case and we’ll see that even Losers
    like us can easily merge and filter SQL tables without touching the SQL syntax.
    No matter the task you always need a powerful tool-set in the first place. Like
    the Anaconda Distribution which we’ll be using here. Our data sources will be
    things like JSON files containing reddit comments or SQL-databases like Northwind.
    Many 90’es kids used Northwind to learn SQL.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 模式无处不在，但许多模式不能立即被识别。这也是我们在数据库、数据仓库和其他信息孤岛中挖掘深坑的原因之一。在本文中，我们将使用Pandas的DataFrames中的一些方法来生成图表。我们还将创建数据透视表并通过ODBC查询MS
    SQL数据库。在这种情况下，SqlAlchemy将成为我们的助手，我们将看到即使像我们这样的“失败者”也能轻松地合并和过滤SQL表，而无需接触SQL语法。不管是什么任务，首先你总是需要一个强大的工具集。比如我们在这里使用的Anaconda
    Distribution。我们的数据源将包括JSON文件（如reddit评论）或SQL数据库（如Northwind）。许多90年代的孩子都使用Northwind来学习SQL。
- en: '![Reddit comment ratings](../Images/cee5b84ed2764fddb38d5668cbeb509b.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![Reddit评论评分](../Images/cee5b84ed2764fddb38d5668cbeb509b.png)'
- en: Highest rated comments for all available sub-reddits.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所有可用子版块的最高评分评论。
- en: '**[Part 2 Addendum: Playing SQL with DataFrames](http://blog.brakmic.com/data-science-for-losers-part-2-addendum/)**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第2部分附录：用DataFrames玩SQL](http://blog.brakmic.com/data-science-for-losers-part-2-addendum/)**'
- en: So, let’s talk about a few features from Pandas I’ve forgot to mention in the
    last two articles.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们谈谈在前两篇文章中我忘记提到的一些Pandas功能。
- en: '**[Part 3: Scala & Apache Spark](http://blog.brakmic.com/data-science-for-losers-part-3-scala-apache-spark/)**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第3部分：Scala与Apache Spark](http://blog.brakmic.com/data-science-for-losers-part-3-scala-apache-spark/)**'
- en: 'By its own definition Spark is a fast, general engine for large-scale data
    processing. Well, someone would say: but we already have Hadoop, so why should
    we use Spark? Such a question I’d answer with a remark that Hadoop is EJB reinvented
    and that we need something more flexible, more general, more expandable and…much
    faster than MapReduce. Spark handles both batch and streaming processing at a
    very fast rate.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 根据其定义，Spark是一个快速的、通用的大规模数据处理引擎。有人会问：但我们已经有Hadoop了，为什么还要使用Spark？我会回答说，Hadoop是EJB的再创造，我们需要的是更加灵活、通用、可扩展的，并且……比MapReduce快得多的东西。Spark能够以非常快的速度处理批量和流处理。
- en: '**[Part 4: Machine Learning](http://blog.brakmic.com/data-science-for-losers-part-4-machine-learning/)**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第4部分：机器学习](http://blog.brakmic.com/data-science-for-losers-part-4-machine-learning/)**'
- en: 'From my non-scientist perspective I’d define ML as a subset of the Artificial
    Intelligence research which develops self-learning (or self-improving?) algorithms
    that try to gain knowledge from data and make predictions based on it. However,
    ML is not only reserved for academia or some “enlightened circles”. We use ML
    every day without being aware of its existence and usefulness. A few examples
    of ML in the wild would be: spam filters, speech-recognition software, automatic
    text-analysis, “intelligent game characters”, or the upcoming self-driving cars.
    All these entities make decisions based on some ML algorithms.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从我非科学家的角度来看，我会将ML定义为人工智能研究的一个子集，它开发出能够从数据中获得知识并基于这些知识做出预测的自学习（或自我改进？）算法。然而，ML不仅仅是学术界或某些“开明圈子”的专属。我们每天都在使用ML，而未意识到它的存在和实用性。一些ML应用的例子包括：垃圾邮件过滤器、语音识别软件、自动文本分析、“智能游戏角色”或即将到来的自动驾驶汽车。所有这些实体都是基于一些ML算法来做出决策的。
- en: '![PySpark](../Images/65b89b9396498da22b3e6822b9a735ae.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark](../Images/65b89b9396498da22b3e6822b9a735ae.png)'
- en: DataFrames in the Spark stack.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Spark堆栈中的DataFrames。
- en: '**[Part 5: Spark DataFrames](http://blog.brakmic.com/data-science-for-losers-part-5-spark-dataframes/)**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第5部分：Spark DataFrames](http://blog.brakmic.com/data-science-for-losers-part-5-spark-dataframes/)**'
- en: Before we start using DataFrames we first have to prepare our environment which
    will run in Jupyter (formerly known as “IPython”). After you’ve downloaded and
    unpacked the Spark Package you’ll find some important Python libraries and scripts
    inside the python/pyspark directory. These files are used, for example, when you
    start the PySpark REPL in the console. As you may know, Spark supports Java, Scala,
    Python and R. Python-based REPL called PySpark offers a nice option to control
    Spark via Python scripts
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始使用DataFrames之前，我们首先需要准备在Jupyter（以前称为“IPython”）中运行的环境。在你下载并解压Spark包后，你会在python/pyspark目录中找到一些重要的Python库和脚本。这些文件在你启动控制台中的PySpark
    REPL时会被使用。正如你可能知道的，Spark支持Java、Scala、Python和R。基于Python的REPL称为PySpark，提供了一个通过Python脚本控制Spark的良好选项。
- en: '**[Part 6: Azure ML](http://blog.brakmic.com/data-science-for-losers-part-6-azure-ml/)**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第6部分：Azure ML](http://blog.brakmic.com/data-science-for-losers-part-6-azure-ml/)**'
- en: In this article we’ll explore Microsoft’s Azure Machine Learning environment
    and how to combine Cloud technologies with Python and Jupyter. As you may know
    I’ve been extensively using them throughout this article series so I have a strong
    opinion on how a Data Science-friendly environment should look like. Of course,
    there’s nothing against other coding environments or languages, for example R,
    so your opinion may greatly differ from mine and this is fine. Also AzureML offers
    a very good R-support! So, feel free to adapt everything from this article to
    your needs. And before we begin, a few words about how I came to the idea to write
    about Azure and Data Science.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨微软的Azure机器学习环境以及如何将云技术与Python和Jupyter结合起来。正如你可能知道的，我在整个文章系列中广泛使用了这些技术，因此我对数据科学友好的环境应该是什么样子有很强的意见。当然，其他编码环境或语言（如R）也没有问题，因此你的意见可能与我的大相径庭，这也是可以的。此外，AzureML提供了非常好的R支持！所以，尽管根据你的需求调整本文中的一切。在开始之前，简要谈谈我为何决定写关于Azure和数据科学的文章。
- en: '**[Part 7: Using Azure ML](http://blog.brakmic.com/data-science-for-losers-part-7-using-azure-ml/)**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**[第七部分：使用Azure ML](http://blog.brakmic.com/data-science-for-losers-part-7-using-azure-ml/)**'
- en: While finishing my Data Science and ML Essentials course, I discovered that
    Azure ML has a built-in support for Jupyter and Python which, of course, made
    it very interesting to me because it makes Azure ML an ideal ground for experimentation.
    They even call one of their working areas “Experiments” so one can expect good
    Python (and R) support and many cool off-the-shelf modules. Being no different
    than other tech-enthusiasts I quickly decided to write an article describing some
    of the key parts of Azure ML.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成我的数据科学和机器学习基础课程时，我发现Azure ML内置了对Jupyter和Python的支持，这让我非常感兴趣，因为这使Azure ML成为实验的理想平台。他们甚至将其中一个工作区域称为“实验”，所以可以期待良好的Python（和R）支持和许多酷炫的现成模块。和其他技术爱好者一样，我很快决定写一篇文章，描述一些Azure
    ML的关键部分。
- en: '**Bio: [Harris Brakmić](http://blog.brakmic.com/)** ([@brakmic](https://twitter.com/brakmic))
    is a Software Developer at Advarics GmbH. He writes WebApps with Ractive, React,
    Backbone & DevExtreme, and Azure-based backends with C#.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[哈里斯·布拉克米奇](http://blog.brakmic.com/)** ([@brakmic](https://twitter.com/brakmic))
    是Advarics GmbH的一个软件开发人员。他使用Ractive、React、Backbone & DevExtreme开发Web应用程序，并使用C#开发基于Azure的后台。'
- en: '**Related:**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Machine Learning: A Complete and Detailed Overview](/2016/10/machine-learning-complete-detailed-overview.html)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习：完整而详细的概述](/2016/10/machine-learning-complete-detailed-overview.html)'
- en: '[An Introduction to the MXNet Python API ](/2017/05/intro-mxnet-python-api.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MXNet Python API 介绍](/2017/05/intro-mxnet-python-api.html)'
- en: '[Getting Into Data Science: What You Need to Know](/2017/05/data-science-need-to-know.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[进入数据科学：你需要知道的](/2017/05/data-science-need-to-know.html)'
- en: More On This Topic
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Introductory Pandas Tutorial](https://www.kdnuggets.com/2022/03/introductory-pandas-tutorial.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[入门Pandas教程](https://www.kdnuggets.com/2022/03/introductory-pandas-tutorial.html)'
- en: '[Reinforcement Learning for Newbies](https://www.kdnuggets.com/2022/05/reinforcement-learning-newbies.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[新手强化学习](https://www.kdnuggets.com/2022/05/reinforcement-learning-newbies.html)'
- en: '[Software Developer vs Software Engineer](https://www.kdnuggets.com/2022/05/software-developer-software-engineer.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[软件开发人员与软件工程师](https://www.kdnuggets.com/2022/05/software-developer-software-engineer.html)'
- en: '[High-Fidelity Synthetic Data for Data Engineers and Data Scientists Alike](https://www.kdnuggets.com/2022/tonic-high-fidelity-synthetic-data-engineers-scientists-alike.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据工程师和数据科学家的高保真合成数据](https://www.kdnuggets.com/2022/tonic-high-fidelity-synthetic-data-engineers-scientists-alike.html)'
- en: '[We Don''t Need Data Scientists, We Need Data Engineers](https://www.kdnuggets.com/2021/02/dont-need-data-scientists-need-data-engineers.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我们不需要数据科学家，我们需要数据工程师](https://www.kdnuggets.com/2021/02/dont-need-data-scientists-need-data-engineers.html)'
- en: '[How Do Data Scientists and Data Engineers Work Together?](https://www.kdnuggets.com/2022/08/data-scientists-data-engineers-work-together.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学家和数据工程师如何协作？](https://www.kdnuggets.com/2022/08/data-scientists-data-engineers-work-together.html)'
