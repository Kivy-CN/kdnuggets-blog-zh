- en: How to Rank 10% in Your First Kaggle Competition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在你的第一次Kaggle比赛中排名前10%
- en: 原文：[https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/4](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/4)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/4](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/4)
- en: Home Depot Search Relevance
  id: totrans-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Home Depot搜索相关性
- en: In this section I will share my solution in [Home Depot Search Relevance Competition](https://www.kaggle.com/c/home-depot-product-search-relevance) and
    what I learned from top teams after the competition.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我将分享我在[Home Depot搜索相关性比赛](https://www.kaggle.com/c/home-depot-product-search-relevance)中的解决方案以及比赛后从顶级团队那里学到的经验。
- en: The task in this competition is to predict how relevant a result is for a search
    term on Home Depot website. The relevance is an average score from three human
    evaluators and ranges between 1 ~ 3\. Therefore it’s a regression task. The datasets
    contains search terms, product titles / descriptions and some attributes like
    brand, size and color. The metric is[RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本次比赛的任务是预测搜索词在Home Depot网站上的结果的相关性。相关性是三位人工评估者的平均分数，范围在1到3之间。因此这是一个回归任务。数据集包含搜索词、产品标题/描述以及一些属性，如品牌、尺寸和颜色。评估指标是[RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation)。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全领域的职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织在IT方面'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: This is much like [Crowdflower Search Results Relevance](https://www.kaggle.com/c/crowdflower-search-relevance).
    The difference is that [Quadratic Weighted Kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa#Weighted_kappa) is
    used in Crowdflower competition and therefore complicated the final cutoff of
    regression scores. Also there were no attributes provided in Crowdflower.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这与[Crowdflower搜索结果相关性](https://www.kaggle.com/c/crowdflower-search-relevance)非常相似。不同之处在于，Crowdflower比赛中使用了[二次加权Kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa#Weighted_kappa)，因此使得回归分数的最终截止点变得复杂。而且Crowdflower中没有提供属性。
- en: '**EDA**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**EDA**'
- en: 'There were several quite good EDAs by the time I joined the competition, especially [this
    one](https://www.kaggle.com/briantc/home-depot-product-search-relevance/homedepot-first-dataexploreation-k).
    I learned that:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当我加入比赛时，有几项相当不错的EDA，特别是[这个](https://www.kaggle.com/briantc/home-depot-product-search-relevance/homedepot-first-dataexploreation-k)。我了解到：
- en: Many search terms / products appeared several times.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多搜索词/产品出现了几次。
- en: Text similarities are great features.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本相似性是很好的特征。
- en: Many products don’t have attributes features. Would this be a problem?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多产品没有属性特征。这会是个问题吗？
- en: Product ID seems to have strong predictive power. However the overlap of product
    ID between the training set and the testing set is not very high. Would this contribute
    to overfitting?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品ID似乎具有强大的预测能力。然而，训练集和测试集之间的产品ID重叠并不是很高。这会导致过拟合吗？
- en: '**Preprocessing**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**预处理**'
- en: 'You can find how I did preprocessing and feature engineering [on GitHub](https://github.com/dnc1994/Kaggle-Playground/blob/master/home-depot/Preprocess.ipynb).
    I’ll only give a brief summary here:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[GitHub](https://github.com/dnc1994/Kaggle-Playground/blob/master/home-depot/Preprocess.ipynb)上查看我如何进行预处理和特征工程。我这里只会做一个简要总结：
- en: Use [typo dictionary](https://www.kaggle.com/steubk/home-depot-product-search-relevance/fixing-typos) posted
    in the forum to correct typos in search terms.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用[拼写错误词典](https://www.kaggle.com/steubk/home-depot-product-search-relevance/fixing-typos)来修正搜索词中的拼写错误。
- en: Count attributes. Find those frequent and easily exploited ones.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算属性。找到那些频繁且容易被利用的属性。
- en: Join the training set with the testing set. This is important because otherwise
    you’ll have to do feature transformation twice.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练集与测试集合并。这一点很重要，因为否则你将不得不进行两次特征转换。
- en: Do **[stemming](https://en.wikipedia.org/wiki/Stemming)** and **[tokenizing](https://en.wikipedia.org/wiki/Text_segmentation#Word_segmentation)** for
    all the text fields. Some **normalization** (with digits and units) and **synonym
    substitutions** are performed manually.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对所有文本字段进行 **[词干提取](https://en.wikipedia.org/wiki/Stemming)** 和 **[分词](https://en.wikipedia.org/wiki/Text_segmentation#Word_segmentation)**。一些
    **规范化**（包括数字和单位）以及 **同义词替换** 是手动执行的。
- en: '**Feature**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征**'
- en: '*Attribute Features'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*属性特征'
- en: Whether the product contains a certain attribute (brand, size, color, weight,
    indoor/outdoor, energy star certified …)
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品是否包含某个属性（品牌、尺寸、颜色、重量、室内/室外、能源星级认证等）
- en: Whether a certain attribute matches with the search term
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否某个属性与搜索词匹配
- en: Meta Features
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元特征
- en: Length of each text field
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个文本字段的长度
- en: Whether the product contains attribute fields
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品是否包含属性字段
- en: Brand (encoded as integers)
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 品牌（编码为整数）
- en: Product ID
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品 ID
- en: Matching
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 匹配
- en: Whether search term appears in product title / description / attributes
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索词是否出现在产品标题/描述/属性中
- en: Count and ratio of search term’s appearance in product title / description /
    attributes
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索词在产品标题/描述/属性中的出现次数和比例
- en: '*Whether the i-th word of search term appears in product title / description
    / attributes'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*搜索词的第 i 个词是否出现在产品标题/描述/属性中'
- en: Text similarities between search term and product title/description/attributes
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索词与产品标题/描述/属性之间的文本相似性
- en: '[BOW](https://en.wikipedia.org/wiki/Bag-of-words_model) [Cosine Similairty](https://en.wikipedia.org/wiki/Cosine_similarity)'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BOW](https://en.wikipedia.org/wiki/Bag-of-words_model) [余弦相似性](https://en.wikipedia.org/wiki/Cosine_similarity)'
- en: '[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) Cosine Similarity'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) 余弦相似性'
- en: '[Jaccard Similarity](https://en.wikipedia.org/wiki/Jaccard_index)'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Jaccard 相似性](https://en.wikipedia.org/wiki/Jaccard_index)'
- en: '*[Edit Distance](https://en.wikipedia.org/wiki/Edit_distance)'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*[编辑距离](https://en.wikipedia.org/wiki/Edit_distance)'
- en: '[Word2Vec](https://en.wikipedia.org/wiki/Word2vec) Distance (I didn’t include
    this because of its poor performance and slow calculation. Yet it seems that I
    was using it wrong.)'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Word2Vec](https://en.wikipedia.org/wiki/Word2vec) 距离（我没有包含这个，因为它的表现差且计算速度慢。但似乎我使用不当。）'
- en: '**[Latent Semantic Indexing](https://en.wikipedia.org/wiki/Latent_semantic_indexing):
    By performing [SVD decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) to
    the matrix obtained from BOW/TF-IDF Vectorization, we get a latent representation
    of different search term / product groups. This enables our model to distinguish
    between groups and assign different weights to features, therefore solving the
    issue of dependent data and products lacking some features (to an extent).**'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[潜在语义索引](https://en.wikipedia.org/wiki/Latent_semantic_indexing)：通过对 BOW/TF-IDF
    向量化获得的矩阵进行[SVD分解](https://en.wikipedia.org/wiki/Singular_value_decomposition)，我们得到不同搜索词/产品组的潜在表示。这使得我们的模型能够区分组别并为特征分配不同的权重，从而在一定程度上解决了相关数据和产品缺少某些特征的问题。**'
- en: Note that features listed above with `*` are the last batch of features I added.
    The problem is that the model trained on data that included these features performed
    worse than the previous ones. At first I thought that the increase in number of
    features would require re-tuning of model parameters. However, after wasting much
    CPU time on grid search, I still could not beat the old model. I think it might
    be the issue of **feature correlation**mentioned above. I actually knew a solution
    that might work, which is to **combine models trained on different version of
    features by stacking**. Unfortunately I didn’t have enough time to try it. **As
    a matter of fact, most of top teams regard the ensemble of models trained with
    different preprocessing and feature engineering pipelines as a key to success**.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，以上带有 `*` 的特征是我最后添加的一批特征。问题是，使用这些特征训练的模型表现比之前的模型更差。起初我认为特征数量的增加需要重新调整模型参数。然而，在浪费了大量
    CPU 时间进行网格搜索后，我仍未能超越旧模型。我认为这可能是上述提到的 **特征相关性** 问题。我实际上知道一个可能有效的解决方案，即 **通过堆叠将训练于不同版本特征的模型结合起来**。不幸的是，我没有足够的时间去尝试。**实际上，大多数顶尖团队认为用不同的预处理和特征工程管道训练的模型集成是成功的关键**。
- en: '**Model**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型**'
- en: At first I was using `RandomForestRegressor` to build my model. Then I tried**Xgboost** and
    it turned out to be more than twice as fast as Sklearn. From that on what I do
    everyday is basically running grid search on my work station while working on
    features on my laptop.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 起初我使用`RandomForestRegressor`来构建我的模型。后来我尝试了**Xgboost**，结果发现它的速度是Sklearn的两倍多。从那时起，我每天做的基本上是在工作站上进行网格搜索，同时在我的笔记本电脑上处理特征。
- en: Dataset in this competition is not trivial to validate. It’s not i.i.d. and
    many records are dependent. Many times I used better features / parameters only
    to end with worse LB scores. As repeatedly stated by many accomplished Kagglers,
    you have to trust your own CV score under such situation. Therefore I decided
    to use 10-fold instead of 5-fold in cross validation and ignore the LB score in
    the following attempts.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本次竞赛的数据集验证起来并不简单。数据不是独立同分布的，许多记录之间是相关的。我多次使用更好的特征/参数却最终得到更差的LB分数。正如许多成功的Kaggler所反复强调的那样，在这种情况下，你必须相信自己的CV分数。因此，我决定在交叉验证中使用10折而不是5折，并在接下来的尝试中忽略LB分数。
- en: '**Ensemble**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**集成模型**'
- en: 'My final model is an ensemble consisting of 4 base models:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我的最终模型是由4个基础模型组成的集成模型：
- en: '`RandomForestRegressor`'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomForestRegressor`'
- en: '`ExtraTreesRegressor`'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ExtraTreesRegressor`'
- en: '`GradientBoostingRegressor`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GradientBoostingRegressor`'
- en: '`XGBRegressor`'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`XGBRegressor`'
- en: The stacker is also a `XGBRegressor`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠器也是一个`XGBRegressor`。
- en: The problem is that all my base models are highly correlated (with a lowest
    correlation of 0.9). I thought of including linear regression, SVM regression
    and `XGBRegressor`with linear booster into the ensemble, but these models had
    RMSE scores that are 0.02 higher (this accounts for a gap of hundreds of places
    on the leaderboard) than the 4 models I finally used. Therefore I decided not
    to use more models although they would have brought much more diversity.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是我的所有基础模型高度相关（最低相关系数为0.9）。我考虑将线性回归、SVM回归和带有线性增强器的`XGBRegressor`纳入集成模型，但这些模型的RMSE分数比我最终使用的4个模型高出0.02（这在排行榜上相当于数百名的差距）。因此，我决定不再使用更多的模型，尽管它们会带来更多的多样性。
- en: The good news is that, despite base models being highly correlated, stacking
    still bumps up my score a lot. **What’s more, my CV score and LB score are in
    complete sync after I started stacking.**
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，尽管基础模型高度相关，堆叠仍然显著提高了我的分数。**更重要的是，我的CV分数和LB分数在我开始堆叠之后完全同步。**
- en: During the last two days of the competition, I did one more thing: **use 20
    or so different random seeds to generate the ensemble and take a weighted average
    of them as the final submission**. This is actually a kind of **bagging**. It
    makes sense in theory because in stacking I used 80% of the data to train base
    models in each iteration, whereas 100% of the data is used to train the stacker.
    Therefore it’s less clean. Making multiple runs with different seeds makes sure
    that **different 80% of the data are used each time**, thus reducing the risk
    of information leak. Yet by doing this I only achieved an increase of `0.0004`,
    which might be just due to randomness.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在竞赛的最后两天，我还做了一件事：**使用20个不同的随机种子生成集成模型，并将它们的加权平均作为最终提交**。这实际上是一种**bagging**。从理论上讲，这是有意义的，因为在堆叠中，我使用了80%的数据来训练基础模型，而100%的数据用于训练堆叠器。因此，这种方法不够干净。使用不同的种子进行多次运行确保了**每次使用不同的80%的数据**，从而降低了信息泄露的风险。然而，这样做我只获得了`0.0004`的提升，这可能只是由于随机性。
- en: After the competition, I found out that my best single model scores `0.46378` on
    the private leaderboard, whereas my best stacking ensemble scores `0.45849`. That
    was the difference between the 174th place and the 98th place. In other words,
    feature engineering and model tuning got me into 10%, whereas stacking got me
    into 5%.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 竞赛结束后，我发现我的最佳单一模型在私人排行榜上的分数是`0.46378`，而我的最佳堆叠集成模型的分数是`0.45849`。这就是174名和98名之间的差距。换句话说，特征工程和模型调优让我进入了前10%，而堆叠让我进入了前5%。
- en: '**Lessons Learned**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**经验教训**'
- en: 'There’s much to learn from the solutions shared by top teams:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从顶级团队分享的解决方案中有很多可以学习的东西：
- en: There’s a pattern in the product title. For example, whether a product is accompanied
    by a certain accessory will be indicated by `With/Without XXX` at the end of the
    title.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品标题中存在一个模式。例如，产品是否附带某种配件将通过标题末尾的`With/Without XXX`来表示。
- en: Use external data. For example use [WordNet](https://wordnet.princeton.edu/) or [Reddit
    Comments Dataset](https://www.kaggle.com/reddit/reddit-comments-may-2015) to train
    synonyms and [hypernyms](https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy).
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用外部数据。例如，使用 [WordNet](https://wordnet.princeton.edu/) 或 [Reddit 评论数据集](https://www.kaggle.com/reddit/reddit-comments-may-2015)
    来训练同义词和 [超类词](https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy)。
- en: Some features based on **letters** instead of **words**. At first I was rather
    confused by this. But it makes perfect sense if you consider it. For example,
    the team that won the 3rd place took the number of letters matched into consideration
    when computing text similarity. They argued that **longer words are more specific
    and thus more likely to be assigned high relevance scores by human**. They also
    used char-by-char sequence comparison (`difflib.SequenceMatcher`) to measure **visual
    similarity**, which they claimed to be important for human.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些特征是基于**字母**而不是**词**。起初我对此感到困惑。但如果你考虑一下，这就很有意义。例如，获得第三名的团队在计算文本相似度时考虑了字母匹配的数量。他们认为**较长的词更具特异性，因此更有可能被人类分配高相关性分数**。他们还使用逐字符序列比较（`difflib.SequenceMatcher`）来衡量**视觉相似度**，他们认为这对人类很重要。
- en: POS-tag words and find **[head](https://en.wikipedia.org/wiki/Head_(linguistics))** in
    phrases and use them when computing various distance metrics.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对词进行 POS 标记，并在短语中找到**[head](https://en.wikipedia.org/wiki/Head_(linguistics))**，在计算各种距离度量时使用它们。
- en: Extract top-ranking trigrams from the TF-IDF of product title / description
    field and compute the ratio of word from search terms that appear in these trigrams.
    Vice versa. This is like computing latent indexes from another point of view.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从产品标题/描述字段的 TF-IDF 中提取排名靠前的三元组，并计算这些三元组中出现的搜索词的比例。反之亦然。这就像从另一个角度计算潜在索引。
- en: Some novel distance metrics like [Word Movers Distance](http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些新颖的距离度量，如 [Word Movers Distance](http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf)。
- en: Apart from SVD, some used [NMF](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization).
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了 SVD，一些人使用了 [NMF](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization)。
- en: Generate **pairwise polynomial interactions** between top-ranking features.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成**成对多项式交互**在排名靠前的特征之间。
- en: '**For CV, construct splits in which product IDs do not overlap between training
    set and testing set, and splits in which IDs do. Then we can use these with corresponding
    ratio to approximate the impact of public/private LB split in our local CV.**'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对于交叉验证，构建训练集和测试集之间产品 ID 不重叠的拆分，以及 ID 重叠的拆分。然后，我们可以使用这些拆分及其对应的比例来近似公共/私人 LB
    拆分在我们本地交叉验证中的影响。**'
- en: Summary
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: '**Takeaways**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**收获**'
- en: It was a good call to **start doing ensembles early in the competition**. As
    it turned out, I was still playing with features during the very last days.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**早早开始做集成学习是一个明智的决定**。事实证明，我在最后几天还在处理特征。'
- en: It’s of high priority that I build a pipeline capable of automatic model training
    and recording best parameters.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优先级很高的是建立一个能够自动训练模型和记录最佳参数的管道。
- en: '**Features matter the most!** I didn’t spend enough time on features in this
    competition.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征最重要！** 在这次比赛中，我没有花足够的时间在特征上。'
- en: If possible, spend some time to manually inspect raw data for patterns.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果可能，花些时间手动检查原始数据以寻找模式。
- en: '**Issues Raised**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**提出的问题**'
- en: Several issues I encountered in this competitions are of high research values.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些竞赛中遇到的一些问题具有很高的研究价值。
- en: How to do reliable CV with dependent data.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何对依赖数据进行可靠的交叉验证。
- en: How to quantify **the trade-off between diversity and accuracy** in ensemble
    learning.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何量化**多样性和准确性之间的权衡**在集成学习中。
- en: How to deal with feature interaction which harms the model’s performance. And**how
    to determine whether new features are effective in such situations**.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何处理对模型性能有害的特征交互。**以及如何确定在这种情况下新特征是否有效**。
- en: '**Beginner Tips**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**新手提示**'
- en: Choose a competition you’re interested in. **It would be better if you’ve already
    have some insights about the problem domain.**
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个你感兴趣的比赛。**如果你对问题领域已有一些见解，那会更好。**
- en: Following my approach or somebody else’s, start exploring, understanding and
    modeling data.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遵循我的方法或他人的方法，开始探索、理解和建模数据。
- en: Learn from forum and scripts. See how others interpret data and construct features.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从论坛和脚本中学习。查看其他人如何解读数据和构建特征。
- en: '**Find winner interviews / blog posts of previous competitions. They’re extremely
    helpful, especially if from competitions that share some similarities with that
    one you’re working on.**'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**寻找之前比赛的获胜者采访或博客文章。这些资料极其有用，特别是那些与您正在进行的比赛有一些相似性的比赛。**'
- en: Start doing ensemble after you have reached a pretty good score (e.g. 10% ~
    20%) or you feel that there isn’t much room for new features (which, sadly, always
    turns out to be false).
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你达到了相当好的分数（例如 10% ~ 20%）或者你觉得新特征空间已经不大（虽然遗憾的是这通常是错误的）之后，开始进行集成学习吧。
- en: If you think you may have a chance to win the prize, try teaming up!
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你认为你可能有机会赢得奖项，尝试组队！
- en: '**Don’t give up until the end of the competition. At least try something new
    every day.**'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**不要在比赛结束前放弃。至少每天尝试一些新事物。**'
- en: Learn from the sharings of top teams after the competition. Reflect on your
    approaches. **If possible, spend some time verifying what you learn.**
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从顶级团队在比赛后的分享中学习。反思你的方法。**如果可能的话，花些时间验证你学到的东西。**
- en: Get some rest!
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 休息一下吧！
- en: Reference
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Beating Kaggle the Easy Way - Dong Ying](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiPxZHewLbMAhVKv5QKHb3PCGwQFggcMAA&url=http%3A%2F%2Fwww.ke.tu-darmstadt.de%2Flehre%2Farbeiten%2Fstudien%2F2015%2FDong_Ying.pdf&usg=AFQjCNE9o2BcEkqdnu_-lQ3EFD3eRAFWiw&sig2=oiU8TCEH57EYF9v9l6Scrw&bvm=bv.121070826,d.dGo)'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[轻松战胜 Kaggle - 董颖](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiPxZHewLbMAhVKv5QKHb3PCGwQFggcMAA&url=http%3A%2F%2Fwww.ke.tu-darmstadt.de%2Flehre%2Farbeiten%2Fstudien%2F2015%2FDong_Ying.pdf&usg=AFQjCNE9o2BcEkqdnu_-lQ3EFD3eRAFWiw&sig2=oiU8TCEH57EYF9v9l6Scrw&bvm=bv.121070826,d.dGo)'
- en: '[Search Results Relevance Winner’s Interview: 1st place, Chenglong Chen](https://github.com/ChenglongChen/Kaggle_CrowdFlower/blob/master/BlogPost/BlogPost.md)'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[搜索结果相关性获胜者采访：第一名，陈成龙](https://github.com/ChenglongChen/Kaggle_CrowdFlower/blob/master/BlogPost/BlogPost.md)'
- en: '[(Chinese) Solution for Prudential Life Insurance Assessment - Nutastray](http://rstudio-pubs-static.s3.amazonaws.com/158725_5d2f977f4004490e9b095c0ef9357c6b.html)'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[(中文) 保诚人寿保险评估解决方案 - Nutastray](http://rstudio-pubs-static.s3.amazonaws.com/158725_5d2f977f4004490e9b095c0ef9357c6b.html)'
- en: '![Linghao Zhang](../Images/47effce75b02d75ca99360d2793970ea.png)**Bio: [Linghao
    Zhang](https://www.linkedin.com/in/linghaozh)** is a senior year Computer Science
    student at Fudan University and Data Mining Engineer at Strikingly. His interests
    include machine learning, data mining, natural language processing, knowledge
    graphs, and big data analytics.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '![张灵昊](../Images/47effce75b02d75ca99360d2793970ea.png)**简介：[张灵昊](https://www.linkedin.com/in/linghaozh)**
    是复旦大学计算机科学专业的高年级学生，同时也是 Strikingly 的数据挖掘工程师。他的兴趣包括机器学习、数据挖掘、自然语言处理、知识图谱和大数据分析。'
- en: '[Original](https://dnc1994.com/2016/05/rank-10-percent-in-first-kaggle-competition-en/).
    Reposted with permission.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://dnc1994.com/2016/05/rank-10-percent-in-first-kaggle-competition-en/)。已获许可转载。'
- en: '**Related:**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Approaching (Almost) Any Machine Learning Problem](/2016/08/approaching-almost-any-machine-learning-problem.html)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[接近（几乎）任何机器学习问题](/2016/08/approaching-almost-any-machine-learning-problem.html)'
- en: '[Automated Data Science & Machine Learning: An Interview with the Auto-sklearn
    Team](/2016/10/interview-auto-sklearn-automated-data-science-machine-learning-team.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动化数据科学与机器学习：Auto-sklearn 团队的访谈](/2016/10/interview-auto-sklearn-automated-data-science-machine-learning-team.html)'
- en: '[Data Science Basics: An Introduction to Ensemble Learners](/2016/11/data-science-basics-intro-ensemble-learners.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学基础：集成学习者简介](/2016/11/data-science-basics-intro-ensemble-learners.html)'
- en: More On This Topic
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[How LinkedIn Uses Machine Learning To Rank Your Feed](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LinkedIn 如何使用机器学习来排序你的动态](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)'
- en: '[How to Get Your First Job in Data Science without Any Work Experience](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在没有任何工作经验的情况下找到数据科学的第一份工作](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)'
- en: '[Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 TensorFlow 和 Keras 构建和训练你的第一个神经网络](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
- en: '[It''s alive! Build your first robots with Python and some cheap,…](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[它活过来了！用 Python 和一些便宜的组件构建你的第一个机器人](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
- en: '[From Zero to Hero: Create Your First ML Model with PyTorch](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从零到英雄：使用 PyTorch 创建你的第一个 ML 模型](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)'
- en: '[Deploying Your First Machine Learning Model](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[部署你的第一个机器学习模型](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)'
