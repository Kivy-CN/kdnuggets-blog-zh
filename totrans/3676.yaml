- en: Fine-Tuning OpenAI Language Models with Noisily Labeled Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/04/finetuning-openai-language-models-noisily-labeled-data.html](https://www.kdnuggets.com/2023/04/finetuning-openai-language-models-noisily-labeled-data.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**By Chris Mauck, Jonas Mueller**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This article demonstrates how data-centric AI tools can improve a fine-tuned
    Large Language Model (LLM; a.k.a. *Foundation* Model). These tools optimize the
    dataset itself rather than altering the model architecture/hyperparameters — running
    the exact same fine-tuning code on the improved dataset boosts test-set performance
    by 37% on a politeness classification task studied here. We achieve similar accuracy
    gains via the same data-centric AI process across 3 state-of-the-art LLM models
    one can fine-tune via the OpenAI API: Davinci, Ada, and Curie. These are variants
    of the base LLM underpinning GPT-3/ChatGPT.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'The above plot shows the test accuracy achieved for 3-class politeness classification
    of text by the same LLM fine-tuning code (fitting Davinci via OpenAI API) run
    on 3 different datasets: (1) the original dataset labeled by human annotators,
    (2) an auto-filtered version of this dataset in which we removed examples automatically
    estimated to be mislabeled via Confident Learning, (3) a cleaned version of the
    original data in which we manually fixed labels of examples estimated to be mislabeled
    (rather than filtering these examples).'
  prefs: []
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Fine-Tuning OpenAI Language Models with Noisily Labeled Data](../Images/c548289b39597699f59845ded1a7d55f.png)'
  prefs: []
  type: TYPE_IMG
- en: Labeled data powers AI/ML in the enterprise, but real-world datasets [have been
    found](https://go.cloudfactory.com/hubfs/02-Contents/3-Reports/Crowd-vs-Managed-Team-Hivemind-Study.pdf)
    to contain between 7-50% annotation errors. Imperfectly-labeled text data hampers
    the training (and evaluation of) ML models across tasks like intent recognition,
    entity recognition, and sequence generation. Although pretrained LLMs are equipped
    with a lot of world knowledge, their performance is adversely affected by noisy
    training data ([as noted by OpenAI](https://arxiv.org/abs/2005.14165)). Here we
    illustrate data-centric techniques to mitigate the effect of label noise without
    changing any code related to model architecture, hyperparameters, or training.
    These data quality improvement techniques should thus remain applicable even for
    future advanced LLMs like GPT-10.
  prefs: []
  type: TYPE_NORMAL
- en: Why Fine-tuning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs acquire powerful generative and discriminative capabilities after being
    pre-trained on most text across the internet. Nonetheless, ensuring the LLM produces
    reliable outputs for a particular business use-case often requires additional
    training on actual data from this domain labeled with the desired outputs. This
    domain-specific training is known as *fine-tuning* the LLM and can be done via
    [APIs offered by OpenAI](https://platform.openai.com/docs/guides/fine-tuning).
    Imperfections in the data annotation process inevitably introduce label errors
    in this domain-specific training data, posing a challenge for proper fine-tuning
    and evaluation of the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Why Data-Centric AI?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are [quotes from OpenAI](https://openai.com/research/dall-e-2-pre-training-mitigations)
    on their strategy for training state-of-the-art AI systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '"*Since training data shapes the capabilities of any learned model, data filtering
    is a powerful tool for limiting undesirable model capabilities.*”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “*We prioritized filtering out all of the bad data over leaving in all of the
    good data. This is because we can always fine-tune our model with more data later
    to teach it new things, but it’s much harder to make the model forget something
    that it has already learned.*”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Clearly dataset quality is a vital consideration. Some organizations like OpenAI
    manually handle issues in their data to produce the very best models, but this
    is tons of work! Data-centric AI is an emerging science of algorithms to detect
    data issues, so you can systematically improve your dataset more easily with automation.
  prefs: []
  type: TYPE_NORMAL
- en: Our LLM in these experiments is the Davinci model from OpenAI, which is their
    most capable GPT-3 model, upon which ChatGPT is based.
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here we consider a 3-class variant of the [Stanford Politeness Dataset](https://convokit.cornell.edu/documentation/wiki_politeness.html),
    which has text phrases labeled as: *impolite*, *neutral*, or *polite*. Annotated
    by human raters, some of these labels are naturally low-quality.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fine-Tuning OpenAI Language Models with Noisily Labeled Data](../Images/446f86a34353c833ed99a82e33629344.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This article walks through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the original data to fine-tune different state-of-the-art LLMs via the
    OpenAI API: Davinci, Ada, and Curie.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish the baseline accuracy of each fine-tuned model on a test set with
    high-quality labels (established via consensus and high-agreement amongst many
    human annotators who rated each test example).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Confident Learning algorithms to automatically identify hundreds of mislabeled
    examples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove the data with automatically-flagged label issues from the dataset, and
    then fine-tune the exact same LLMs on the auto-filtered dataset. **This simple
    step reduces the error in Davinci model predictions by 8%!**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduce a **no-code** solution to efficiently fix the label errors in the
    dataset, and then fine-tune the exact same LLM on the fixed dataset. **This reduces
    the error in Davinci model predictions by 37%!**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar gains are achieved via these same processes for the Ada and Curie models
    — in all cases, nothing was changed about the model nor the fine-tuning code!
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a [notebook](https://colab.research.google.com/github/cmauck10/notebooks/blob/master/LLM_with_noisy_labels.ipynb)
    you can run to reproduce the results demonstrated in this article and understand
    the code to implement each step.
  prefs: []
  type: TYPE_NORMAL
- en: Politeness Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can download the train and test sets here: [train](https://s.cleanlab.ai/stanford-politeness/fine-tuning/train.csv)
    [test](https://s.cleanlab.ai/stanford-politeness/fine-tuning/test.csv)'
  prefs: []
  type: TYPE_NORMAL
- en: Our training dataset has 1916 examples each labeled by a single human annotator,
    and thus some may be unreliable.  The test dataset has 480 examples each labeled
    by five annotators, and we use their consensus label as a high-quality approximation
    of the true politeness (measuring test accuracy against these consensus labels).
    To ensure a fair comparison, this test dataset remains fixed throughout our experiments
    (all label cleaning / dataset modification is only done in the training set).
    We reformat these CSV files into the jsonl file type required by OpenAI’s fine-tuning
    API.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tune and Evaluate LLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here’s how our code looks to fine-tune the Davinci LLM for 3-class classification
    and evaluate its test accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once the job completes, we query a fine_tunes.results endpoint to see the test
    accuracy achieved when fine-tuning this LLM on the original training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Our baseline Davinci LLM achieves a test accuracy of 63% when fine-tuned on
    the raw training data with possibly noisy labels. Even a state-of-the-art LLM
    like the Davinci model produces lackluster results for this classification task,
    is it because the data labels are noisy?
  prefs: []
  type: TYPE_NORMAL
- en: Automatically Find Label Issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Confident Learning](https://arxiv.org/abs/1911.00068) is a recently developed
    suite of algorithms to estimate which data are mislabeled in a classification
    dataset. These algorithms require **out-of-sample** predicted class probabilities
    for all of our training examples and apply a novel form of calibration to determine
    when to trust the model over the given label in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain these predicted probabilities we:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the OpenAI API to compute embeddings from the Davinci model for all of our
    training examples. You can download the embeddings [here](https://cleanlab-public.s3.amazonaws.com/Datasets/stanford-politeness/LLM-blog/train_embed_davinci.npy).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a logistic regression model on the embeddings and labels in the original
    data. We use 10-fold cross-validation which allows us to produce out-of-sample
    predicted class probabilities for every example in the training dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The [cleanlab](https://github.com/cleanlab/cleanlab) package offers an open-source
    Python implementation of Confident Learning. With one line of code, we can run
    Confident Learning using the model predicted probabilities to estimate which examples
    have label issues in our training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a look at a few of the label issues automatically identified in
    our dataset. Here’s one example that is clearly mislabeled:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Phrase: *I''ll take a look at getLogEntries when I have time. Would you mind
    adding me as a committer?*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Label: *impolite*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Labeling errors like this are why we might be seeing poor model results.
  prefs: []
  type: TYPE_NORMAL
- en: '![Fine-Tuning OpenAI Language Models with Noisily Labeled Data](../Images/44471b8c60e405c79bafb621162cff2a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Caption: A few of the top errors that were automatically identified.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** **find_label_issues** **is able to determine which of the given**
    **labels** **are potentially incorrect given only the out-of-sample** **pred_probs****.**'
  prefs: []
  type: TYPE_NORMAL
- en: Filter Label Issues and Fine-tune a more Robust LLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have the indices of potentially mislabeled examples (identified
    via automated techniques), let’s remove these 471 examples from our training dataset.
    Fine-tuning the exact same Davinci LLM on the filtered dataset achieves a test
    accuracy of 66% (on the same test data where our original Davinci LLM achieved
    63% accuracy). We **reduced the error-rate of the model by 8%** using **less**
    but **better quality** training data!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Fixing the Label Errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instead of fixing the auto-detected label issues automatically via filtering,
    the smarter (yet more complex) way to improve our dataset would be to correct
    the label issues by hand. This simultaneously removes a noisy data point and adds
    an accurate one, but making such corrections manually is cumbersome. We did this
    manually using [Cleanlab Studio](https://cleanlab.ai/studio/), an enterprise data
    correction interface.
  prefs: []
  type: TYPE_NORMAL
- en: After replacing the bad labels we spotted with more suitable ones, we fine-tune
    the exact same Davinci LLM on the manually-corrected dataset. The resulting model
    achieves **77%** accuracy (on the same test dataset as before), which is a  **37%
    reduction in error** from our original version of this model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Note: throughout this entire process, we never changed any code related to
    model architecture/hyperparameters, training, or data preprocessing!** All improvement
    strictly comes from increasing the quality of our training data, which leaves
    room for additional optimizations on the modeling side.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating other LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We repeated this same experiment with two other recent LLM models OpenAI offers
    for fine-tuning: Ada and Curie. The resulting improvements look similar to those
    achieved for the Davinci model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fine-Tuning OpenAI Language Models with Noisily Labeled Data](../Images/ee4addf0df247b4768b324a15ee16925.png)'
  prefs: []
  type: TYPE_IMG
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data-centric AI is a powerful paradigm for handling noisy data via AI/automated
    techniques rather than tedious manual effort. There are now tools to help you
    efficiently find and fix data and label issues to improve any ML model (not just
    LLMs) for most types of data (not just text, but also images, audio, tabular data,
    etc). Such tools *utilize* any ML model to diagnose/fix issues in the data and
    then improve the data *for* any other ML model. These tools will remain applicable
    with future advances in ML models like GPT-10, and will only become better at
    identifying issues when used with more accurate models!
  prefs: []
  type: TYPE_NORMAL
- en: Practice data-centric AI to systematically engineer better data via AI/automation.
    This frees you to capitalize on your unique domain knowledge rather than fixing
    general data issues like label errors.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chris Mauck](https://www.linkedin.com/in/chris-mauck/)** is Data Scientist
    at Cleanlab.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[RAG vs Finetuning: Which Is the Best Tool to Boost Your LLM Application?](https://www.kdnuggets.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building AI Products with OpenAI: A Free Course from CoRise](https://www.kdnuggets.com/2023/07/corise-building-ai-products-openai-free-course-corise.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introducing Superalignment by OpenAI](https://www.kdnuggets.com/2023/08/introducing-superalignment-openai.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Best Practices to Use OpenAI GPT Model](https://www.kdnuggets.com/2023/08/best-practices-openai-gpt-model.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenAI API for Beginners: Your Easy-to-Follow Starter Guide](https://www.kdnuggets.com/openai-api-for-beginners-your-easy-to-follow-starter-guide)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Exploring the OpenAI API with Python](https://www.kdnuggets.com/exploring-the-openai-api-with-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
