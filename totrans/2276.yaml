- en: Exploring Unsupervised Learning Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/04/exploring-unsupervised-learning-metrics.html](https://www.kdnuggets.com/2023/04/exploring-unsupervised-learning-metrics.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Exploring Unsupervised Learning Metrics](../Images/2f60ef6cdd25fafa7f251367fc11a8b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [rawpixel](https://www.freepik.com/free-vector/global-communication-background-business-network-vector-design_19585255.htm#query=clustering&position=2&from_view=search&track=sph)
    on [Freepik](https://www.freepik.com/)
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning is a branch of machine learning where the models learn
    patterns from the available data rather than provided with the actual label. We
    let the algorithm come up with the answers.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In unsupervised learning, there are two main techniques; clustering and dimensionality
    reduction. The clustering technique uses an algorithm to learn the pattern to
    segment the data. In contrast, the dimensionality reduction technique tries to
    reduce the number of features by keeping the actual information intact as much
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: An example algorithm for clustering is K-Means, and for dimensionality reduction
    is PCA. These were the most used algorithm for unsupervised learning. However,
    we rarely talk about the metrics to evaluate unsupervised learning.  As useful
    as it is, we still need to evaluate the result to know if the output is precise.
  prefs: []
  type: TYPE_NORMAL
- en: This article will discuss the metrics used to evaluate unsupervised machine
    learning algorithms and will be divided into two sections; Clustering algorithm
    metrics and dimensionality reduction metrics. Let’s get into it.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering Algorithm Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We would not discuss in detail about the clustering algorithm as it’s not the
    main point of this article. Instead, we would focus on examples of the metrics
    used for the evaluation and how to assess the result.
  prefs: []
  type: TYPE_NORMAL
- en: This article will use the [Wine Dataset from Kaggle](https://www.kaggle.com/datasets/harrywang/wine-dataset-for-clustering)
    as our dataset example. Let’s read the data first and use the K-Means algorithm
    to segment the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: I initiate the cluster as 4, which means we segment the data into 4 clusters.
    Is it the right number of clusters? Or is there any more suitable cluster number?
    Commonly, we can use the technique called the **elbow method** to find the appropriate
    cluster. Let me show the code below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![Exploring Unsupervised Learning Metrics](../Images/786421ebff712af35e633c515a215698.png)'
  prefs: []
  type: TYPE_IMG
- en: In the elbow method, we use WCSS or Within-Cluster Sum of Squares to calculate
    the sum of squared distances between data points and the respective cluster centroids
    for various k (clusters). The best k value is expected to be the one with the
    most decrease of WCSS or the elbow in the picture above, which is 2.
  prefs: []
  type: TYPE_NORMAL
- en: However, we can expand the elbow method to use other metrics to find the best
    k. How about the algorithm automatically finding the cluster number without relying
    on the centroid? Yes, we can also evaluate them using similar metrics.
  prefs: []
  type: TYPE_NORMAL
- en: As a note, we can assume a centroid as the data mean for each cluster even though
    we don’t use the K-Means algorithm. So, any algorithm that did not rely on the
    centroid while segmenting the data could still use any metric evaluation that
    relies on the centroid.
  prefs: []
  type: TYPE_NORMAL
- en: Silhouette Coefficient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Silhouette is a technique in clustering to measure the similarity of data within
    the cluster compared to the other cluster. The Silhouette coefficient is a numerical
    representation ranging from -1 to 1\. Value 1 means each cluster completely differed
    from the others, and value  -1 means all the data was assigned to the wrong cluster.
    0 means there are no meaningful clusters from the data.
  prefs: []
  type: TYPE_NORMAL
- en: We could use the following code to calculate the Silhouette coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Silhouette Coefficient: 0.562'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that our segmentation above has a positive Silhouette Coefficient,
    which means there is the degree of separation between the clusters, although some
    overlapping still happens.
  prefs: []
  type: TYPE_NORMAL
- en: Calinski-Harabasz Index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Calinski-Harabasz Index or Variance Ratio Criterion is an index that is
    used to evaluate cluster quality by measuring the ratio of between-cluster dispersion
    to within-cluster dispersion. Basically, we measured the differences between the
    sum squared distance of the data between the cluster and data within the internal
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The higher the Calinski-Harabasz Index score, the better, which means the clusters
    were well separated. However, there are no upper limits for the score means that
    this metric is better for evaluating different k numbers rather than interpreting
    the result as it is.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use the Python code to calculate the Calinski-Harabasz Index score.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Calinski-Harabasz Index: 708.087'
  prefs: []
  type: TYPE_NORMAL
- en: One other consideration for the Calinski-Harabasz Index score is that the score
    is sensitive to the number of clusters. A higher number of clusters could lead
    to a higher score as well. So it’s a good idea to use other metrics alongside
    the Calinski-Harabasz Index to validate the result.
  prefs: []
  type: TYPE_NORMAL
- en: Davies-Bouldin Index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Davies-Bouldin Index is a clustering evaluation metric measured by calculating
    the average similarity between each cluster and its most similar one. The ratio
    of within-cluster distances to between-cluster distances calculates the similarity.
    This means the further apart the clusters and the less dispersed would lead to
    better scores.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast with our previous metrics, the Davies-Bouldin Index aims to have
    a lower score as much as possible. The lower the score was, the more separated
    each cluster was. Let’s use a Python example to calculate the score.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Davies-Bouldin Index: 0.544'
  prefs: []
  type: TYPE_NORMAL
- en: We can’t say that the above score is good or bad because similar to the previous
    metrics, we still need to evaluate the result by using various metrics as support.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality Reduction Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike clustering, dimensionality reduction aims to reduce the number of features
    while preserving the original information as much as possible. Because of that,
    many of the evaluation metrics in dimensionality reduction were all about information
    preservation. Let’s reduce dimensionality with PCA and see how the metric works.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the above example, we fit the PCA to the data, but we haven’t reduced the
    number of the feature yet. Instead, we want to evaluate the dimensionality reduction
    and variance trade-off with the **Cumulative Explained Variance**. It’s the common
    metric for dimensionality reduction to see how information remains with each feature
    reduction.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Exploring Unsupervised Learning Metrics](../Images/781c57c460988a9e7466edfbbaa1deda.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see from the above chart the amount of PC retained compared to the explained
    variance. As a rule of thumb, we often choose around 90-95% retained when we try
    to make dimensionality reduction, so around 14 features are reduced to 8 if we
    follow the chart above.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the other metrics to validate our dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Trustworthiness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Trustworthiness is a measurement of the dimensionality reduction technique quality.
    This metric measured how well the reduced dimension preserved the original data
    nearest neighbor.
  prefs: []
  type: TYPE_NORMAL
- en: Basically, the metric tries to see how well the dimension reduction technique
    preserved the data in maintaining the original data's local structure.
  prefs: []
  type: TYPE_NORMAL
- en: The Trustworthiness metric ranges between 0 to 1, where values closer to 1 are
    means the neighbor that is close to reduced dimension data points are mostly close
    as well in the original dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use the Python code to calculate the Trustworthiness metric.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Trustworthiness: 0.87'
  prefs: []
  type: TYPE_NORMAL
- en: Sammon’s Mapping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sammon’s mapping is a non-linear dimensionality reduction technique to preserve
    the high-dimensionality pairwise distance when being reduced. The objective is
    to use Sammon’s Stress function to calculate the pairwise distance between the
    original data and the reduction space.
  prefs: []
  type: TYPE_NORMAL
- en: The lower Sammon’s stress function score, the better because it indicates better
    pairwise preservation. Let’s try to use the Python code example.
  prefs: []
  type: TYPE_NORMAL
- en: First, we would install an additional package for Sammon’s Mapping.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Then we would use the following code to calculate the Sammon’s stress.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Sammon''s Stress: 1e-05'
  prefs: []
  type: TYPE_NORMAL
- en: The result shown a low Sammon’s Score which means the data preservation was
    there.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unsupervised learning is a machine learning branch that tries to learn the
    pattern from the data. Compared to supervised learning, the output evaluation
    might not discuss much. In this article, we try to learn a few unsupervised learning
    metrics, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Within-Cluster Sum Square
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Silhouette Coefficient
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calinski-Harabasz Index
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Davies-Bouldin Index
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cumulative Explained Variance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Trustworthiness
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sammon’s Mapping
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**[Cornellius Yudha Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**
    is a data science assistant manager and data writer. While working full-time at
    Allianz Indonesia, he loves to share Python and Data tips via social media and
    writing media.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Unsupervised Disentangled Representation Learning in Class…](https://www.kdnuggets.com/2023/01/unsupervised-disentangled-representation-learning-class-imbalanced-dataset-elastic-infogan.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Clustering with scikit-learn: A Tutorial on Unsupervised Learning](https://www.kdnuggets.com/2023/05/clustering-scikitlearn-tutorial-unsupervised-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unveiling Unsupervised Learning](https://www.kdnuggets.com/unveiling-unsupervised-learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hands-On with Unsupervised Learning: K-Means Clustering](https://www.kdnuggets.com/handson-with-unsupervised-learning-kmeans-clustering)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning Evaluation Metrics: Theory and Overview](https://www.kdnuggets.com/machine-learning-evaluation-metrics-theory-and-overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[More Performance Evaluation Metrics for Classification Problems You…](https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
