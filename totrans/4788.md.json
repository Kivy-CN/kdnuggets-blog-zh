["```py\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport base64\nimport string\nimport re\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nstopwords = stopwords.words('english')\n\ndf = pd.read_csv('research_paper.csv')\ndf.head()\n```", "```py\ndf.isnull().sum()\n```", "```py\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df, test_size=0.33, random_state=42)\n\nprint('Research title sample:', train['Title'].iloc[0])\nprint('Conference of this paper:', train['Conference'].iloc[0])\nprint('Training Data Shape:', train.shape)\nprint('Testing Data Shape:', test.shape)\n```", "```py\nfig = plt.figure(figsize=(8,4))\nsns.barplot(x = train['Conference'].unique(), y=train['Conference'].value_counts())\nplt.show()\n```", "```py\nimport spacy\n\nnlp = spacy.load('en_core_web_sm')\npunctuations = string.punctuation\n\ndef cleanup_text(docs, logging=False):\n    texts = []\n    counter = 1\n    for doc in docs:\n        if counter % 1000 == 0 and logging:\n            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n        counter += 1\n        doc = nlp(doc, disable=['parser', 'ner'])\n        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n        tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n        tokens = ' '.join(tokens)\n        texts.append(tokens)\n    return pd.Series(texts)\n\nINFO_text = [text for text in train[train['Conference'] == 'INFOCOM']['Title']]\n\nIS_text = [text for text in train[train['Conference'] == 'ISCAS']['Title']]\n\nINFO_clean = cleanup_text(INFO_text)\nINFO_clean = ' '.join(INFO_clean).split()\n\nIS_clean = cleanup_text(IS_text)\nIS_clean = ' '.join(IS_clean).split()\n\nINFO_counts = Counter(INFO_clean)\nIS_counts = Counter(IS_clean)\n\nINFO_common_words = [word[0] for word in INFO_counts.most_common(20)]\nINFO_common_counts = [word[1] for word in INFO_counts.most_common(20)]\n\nfig = plt.figure(figsize=(18,6))\nsns.barplot(x=INFO_common_words, y=INFO_common_counts)\nplt.title('Most Common Words used in the research papers for conference INFOCOM')\nplt.show()\n\n```", "```py\nIS_common_words = [word[0] for word in IS_counts.most_common(20)]\nIS_common_counts = [word[1] for word in IS_counts.most_common(20)]\n\nfig = plt.figure(figsize=(18,6))\nsns.barplot(x=IS_common_words, y=IS_common_counts)\nplt.title('Most Common Words used in the research papers for conference ISCAS')\nplt.show()\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.base import TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nfrom sklearn.metrics import accuracy_score\nfrom nltk.corpus import stopwords\nimport string\nimport re\nimport spacy\nspacy.load('en')\nfrom spacy.lang.en import English\nparser = English()\n```", "```py\nSTOPLIST = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS))\nSYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\"]\n\nclass CleanTextTransformer(TransformerMixin):\n\n   def transform(self, X, **transform_params):\n        return [cleanText(text) for text in X]\n\n   def fit(self, X, y=None, **fit_params):\n        return self\n\n   def get_params(self, deep=True):\n        return {}\n\ndef cleanText(text):\n    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n    text = text.lower()\n\ndef tokenizeText(sample):\n    tokens = parser(sample)\n    lemmas = []\n    for tok in tokens:\n        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n    tokens = lemmas\n    tokens = [tok for tok in tokens if tok not in STOPLIST]\n    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n    return tokens\n```", "```py\ndef printNMostInformative(vectorizer, clf, N):\n    feature_names = vectorizer.get_feature_names()\n    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n    topClass1 = coefs_with_fns[:N]\n    topClass2 = coefs_with_fns[:-(N + 1):-1]\n    print(\"Class 1 best: \")\n    for feat in topClass1:\n        print(feat)\n    print(\"Class 2 best: \")\n    for feat in topClass2:\n        print(feat)\n\nvectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\nclf = LinearSVC()\n\npipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])\n\n# data\ntrain1 = train['Title'].tolist()\nlabelsTrain1 = train['Conference'].tolist()\n\ntest1 = test['Title'].tolist()\nlabelsTest1 = test['Conference'].tolist()\n\n# train\npipe.fit(train1, labelsTrain1)\n\n# test\npreds = pipe.predict(test1)\nprint(\"accuracy:\", accuracy_score(labelsTest1, preds))\nprint(\"Top 10 features used to predict: \")\n\nprintNMostInformative(vectorizer, clf, 10)\npipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer)])\ntransform = pipe.fit_transform(train1, labelsTrain1)\n\nvocab = vectorizer.get_feature_names()\nfor i in range(len(train1)):\n    s = \"\"\n    indexIntoVocab = transform.indices[transform.indptr[i]:transform.indptr[i+1]]\n    numOccurences = transform.data[transform.indptr[i]:transform.indptr[i+1]]\n    for idx, num in zip(indexIntoVocab, numOccurences):\n        s += str((vocab[idx], num))\n```", "```py\nfrom sklearn import metrics\nprint(metrics.classification_report(labelsTest1, preds, \n    target_names=df['Conference'].unique()))\n\nprecision    recall  f1-score   support\n\n       VLDB       0.75      0.77      0.76       159\n      ISCAS       0.90      0.84      0.87       299\n   SIGGRAPH       0.67      0.66      0.66       106\n    INFOCOM       0.62      0.69      0.65       139\n        WWW       0.62      0.62      0.62       125\n\navg / total       0.75      0.75      0.75       828\n```"]