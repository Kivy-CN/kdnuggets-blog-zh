- en: A Concise Overview of Standard Model-fitting Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2016/05/concise-overview-model-fitting-methods.html/2](https://www.kdnuggets.com/2016/05/concise-overview-model-fitting-methods.html/2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 3) Stochastic Gradient Descent (SGD)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In GD optimization, we compute the cost gradient based on the complete training
    set; hence, we sometimes also call it *batch GD*. In case of very large datasets,
    using GD can be quite costly since we are only taking a single step for one pass
    over the training set -- thus, the larger the training set, the slower our algorithm
    updates the weights and the longer it may take until it converges to the global
    cost minimum (note that the SSE cost function is convex).
  prefs: []
  type: TYPE_NORMAL
- en: 'In Stochastic Gradient Descent (SGD; sometimes also referred to as *iterative* or *on-line* GD),
    we don''t accumulate the weight updates as we''ve seen above for GD:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Iterative GD](../Images/df75da212cfd6178769b879d04e8b876.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead, we update the weights after each training sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Iterative SGD](../Images/6addd7ca1e9e10f4658269976691ca14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the term "stochastic" comes from the fact that the gradient based on
    a single training sample is a "stochastic approximation" of the "true" cost gradient.
    Due to its stochastic nature, the path towards the global cost minimum is not
    "direct" as in GD, but may go "zig-zag" if we are visualizing the cost surface
    in a 2D space. However, it has been shown that SGD almost surely converges to
    the global cost minimum if the cost function is convex (or pseudo-convex)[1].
    Furthermore, there are different tricks to improve the GD-based learning, for
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'An adaptive learning rate η Choosing a decrease constant *d* that shrinks the
    learning rate over time:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/7912a1c822b9d201fb58d0b33f3e8e5d.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Momentum learning by adding a factor of the previous gradient to the weight
    update for faster updates:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/66a8769e0d9c7c2cfea9dbbf8481676f.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '**A note about shuffling**'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several different flavors of SGD, which can be all seen throughout
    the literature. Let''s take a look at the three most common variants:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Shuffle A](../Images/c7e5510be283093535708988c6cc554e.png)'
  prefs: []
  type: TYPE_IMG
- en: '![Shuffle B](../Images/3b510f5554f7ff0ea8e0e76d518ef035.png)'
  prefs: []
  type: TYPE_IMG
- en: '![Shuffle C](../Images/ce7f311ed56b92dba8bf9225e7e5df12.png)'
  prefs: []
  type: TYPE_IMG
- en: In scenario A [3], we shuffle the training set only one time in the beginning;
    whereas in scenario B, we shuffle the training set after each epoch to prevent
    repeating update cycles. In both scenario A and scenario B, each training sample
    is only used once per epoch to update the model weights.
  prefs: []
  type: TYPE_NORMAL
- en: In scenario C, we draw the training samples randomly with replacement from the
    training set [2]. If the number of iterations *t*is equal to the number of training
    samples, we learn the model based on a *bootstrap sample* of the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 4) Mini-Batch Gradient Descent (MB-GD)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mini-Batch Gradient Descent (MB-GD) a compromise between batch GD and SGD. In
    MB-GD, we update the model based on smaller groups of training samples; instead
    of computing the gradient from 1 sample (SGD) or all *n* training samples (GD),
    we compute the gradient from *1 < k < n* training samples (a common mini-batch
    size is *k=50*).
  prefs: []
  type: TYPE_NORMAL
- en: MB-GD converges in fewer iterations than GD because we update the weights more
    frequently; however, MB-GD let's us utilize vectorized operation, which typically
    results in a computational performance gain over SGD.
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Bottou, Léon (1998). "Online Algorithms and Stochastic Approximations".
    Online Learning and Neural Networks. Cambridge University Press. ISBN 978-0-521-65263-6'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Bottou, Léon. "Large-scale machine learning with SGD." Proceedings of COMPSTAT''2010\.
    Physica-Verlag HD, 2010\. 177-186.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Bottou, Léon. "SGD tricks." Neural Networks: Tricks of the Trade. Springer
    Berlin Heidelberg, 2012\. 421-436.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Sebastian Raschka](https://twitter.com/rasbt)** is a ''Data Scientist''
    and Machine Learning enthusiast with a big passion for Python & open source. Author
    of ''[Python Machine Learning](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning)''.
    Michigan State University.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[When Does Deep Learning Work Better Than SVMs or Random Forests?](/2016/04/deep-learning-vs-svm-random-forest.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Development of Classification as a Learning Machine](/2016/04/development-classification-learning-machine.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Why Implement Machine Learning Algorithms From Scratch?](/2016/05/implement-machine-learning-algorithms-scratch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Removing Outliers Using Standard Deviation in Python](https://www.kdnuggets.com/2017/02/removing-outliers-standard-deviation-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Developing an Open Standard for Analytics Tracking](https://www.kdnuggets.com/2022/07/developing-open-standard-analytics-tracking.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Centroid Initialization Methods for k-means Clustering](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Alternative Feature Selection Methods in Machine Learning](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python String Methods](https://www.kdnuggets.com/2022/12/python-string-methods.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11 Python Magic Methods Every Programmer Should Know](https://www.kdnuggets.com/11-python-magic-methods-every-programmer-should-know)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
