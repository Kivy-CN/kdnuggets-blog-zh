- en: Pytorch Cheat Sheet for Beginners and Udacity Deep Learning Nanodegree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/08/pytorch-cheat-sheet-beginners.html](https://www.kdnuggets.com/2019/08/pytorch-cheat-sheet-beginners.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By [Uniqtech](https://theoptips.github.io/uniqtech_1/)**'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Pytorch using a cohesive, top down approach cheatsheet.
    This cheatsheet should be easier to digest than the official documentation and
    should be a transitional tool to get students and beginners to get started reading
    documentations soon. This article is being improved continuously. It is frequently
    updated and will remain under construction until it is significantly improved.
    Your feedback is appreciated hi@uniqtech.co and mistakes, typos will be promptly
    corrected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Big news: we got published on Medium Machine Learning and Data Science homepage.
    Please clap ← and comment to show your support. This cheatsheet below is primarily
    narrative. A PDF JPEG version of a detailed cheatsheet will be released soon,
    posted in this article. Updated June 18, 2019 to make this cheat sheet / tutorial
    more cohesive, we will insert code snippets from a medal winning Kaggle kernel
    to illustrate important Pytorch concepts — [Malaria Detection with Pytorch, an
    image classification, computer vision Kaggle kernel](https://www.kaggle.com/devilsknight/malaria-detection-with-pytorch) [see
    Source 3 below] by author [devilsknight](https://www.kaggle.com/devilsknight)and
    vishnu aka [qwertypsv](https://www.kaggle.com/qwertypsv).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/b6bf45cdb45371fd00627cbdd0ae2c1f.png)'
  prefs: []
  type: TYPE_IMG
- en: pytorch cheatsheet for beginners by uniqtech
  prefs: []
  type: TYPE_NORMAL
- en: Pytorch Defined in Its Own Words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Pytorch is “An open source deep learning platform that provides a seamless
    path from research prototyping to production deployment.”**'
  prefs: []
  type: TYPE_NORMAL
- en: 'According to Facebook Research [Source 1], PyTorch is a Python package that
    provides two high-level features:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tensor computation (like NumPy) with strong GPU acceleration
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Deep neural networks built on a tape-based autograd system
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You can reuse your favorite Python packages such as NumPy, SciPy and Cython
    to extend PyTorch when needed.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Soumith Chintala, Facebook Research Engineer and creator of Pytorch gave some
    interesting facts about Pytorch: autograd used to be written in python, but the
    majority (of code) was changed to C++ (for production readiness). He thinks interesting
    Pytorch 1.0 features are hybrid front end, parsing model for production, using
    Jit compiler to get models production ready for example. Source: Chintala’s interview
    with Udacity learning.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Key Features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Component | Description** [Source 2]'
  prefs: []
  type: TYPE_NORMAL
- en: '[**torch**](https://pytorch.org/docs/stable/torch.html): a Tensor library like
    NumPy, with strong GPU support'
  prefs: []
  type: TYPE_NORMAL
- en: '[**torch.autograd**](https://pytorch.org/docs/stable/autograd.html)** : **a
    tape-based automatic differentiation library that supports all differentiable
    Tensor operations in torch'
  prefs: []
  type: TYPE_NORMAL
- en: '[**torch.jit**](https://pytorch.org/docs/stable/jit.html) : a compilation stack
    (TorchScript) to create serializable and optimizable models from PyTorch code'
  prefs: []
  type: TYPE_NORMAL
- en: '[**torch.nn**](https://pytorch.org/docs/stable/nn.html): a neural networks
    library deeply integrated with autograd designed for maximum flexibility'
  prefs: []
  type: TYPE_NORMAL
- en: '[**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html):
    Python multiprocessing, but with magical memory sharing of torch Tensors across
    processes. Useful for data loading and Hogwild training'
  prefs: []
  type: TYPE_NORMAL
- en: '[**torch.utils**](https://pytorch.org/docs/stable/data.html): DataLoader and
    other utility functions for convenience'
  prefs: []
  type: TYPE_NORMAL
- en: 'Key features: Hybrid Front-End, Distributed Training, Python-First, Tools &
    Libraries.'
  prefs: []
  type: TYPE_NORMAL
- en: These features are elegantly illustrated with side-by-side code example on the [features
    page](https://pytorch.org/features)!
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/da94f943b753a23621a94585147571a6.png)'
  prefs: []
  type: TYPE_IMG
- en: The features page on pytorch documentation shows elegant code sample to illustrated
    each feature. Also note Python 3 short hand for dot product such as “@”
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid Front-End allows switching between eager mode and (computation) graph
    mode. Tensorflow used to be graph mode only, which was considered fast and efficient
    but very hard to modify, prototype and research. This gap is closing since Tensorflow
    now also offers eager mode (no more `session run`.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Distributed Training: supports GPU, CPU and easy switching between the two.
    (Tensorflow supports TPU in addition. Its own Tensor Processing Unit.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python-First: built for python developers. Easily create neural network, run
    deep learning in Pytorch. Tools & Libraries include robust computer vision libraries
    (convolutional neural networks and pretrained models), NLP and more.'
  prefs: []
  type: TYPE_NORMAL
- en: Pytorch also includes great features like **torch.tensor** instantiation and
    computation, model, validation, scoring, Pytorch feature to auto calculate gradient
    using autograd which also does all the backpropagation for you, transfer learning
    ready preloaded models and datasets (read our super short effective article on
    transfer learning), and let’s not forget GPU using CUDA.
  prefs: []
  type: TYPE_NORMAL
- en: When Should You Use Pytorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AWS google GCP GPU supports Pytorch as first class citizen
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Pytorch added production and cloud partner support for 1.0 for AWS, Google Cloud
    Platform, Microsoft Azure. You can now use Pytorch for any deep learning tasks
    including computer vision and NLP, even in production.
  prefs: []
  type: TYPE_NORMAL
- en: Because it is so easy to use and pythonic to Senior Data Scientist [Stefan Otte](https://sotte.github.io/) said
    “if you want to have fun, use pytorch”.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Pytorch is also backed by Facebook AI research so if you want to work for Facebook
    data and ML, you should know Pytorch. If you are great with Python and want to
    be an open source contributor Pytorch is also the way to go.
  prefs: []
  type: TYPE_NORMAL
- en: '**Transfer Learning**'
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning use models to predict the type of the dataset that it wasn’t
    trained on. It can significantly improve training time and accuracy. It can also
    help with the situation where available training data is limited. Pytorch has
    a page dedicated to pre-trained models and its performance across industry standard
    benchmark datasets. Read more in our transfer learning with Pytorch article.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/998ce78c4b9cc073011acd533b33cd36.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[Read about all the available models on Pytorch documentation](https://pytorch.org/docs/stable/torchvision/models.html).
    Note the top-1-error, top-5-error i.e. the performance of the models are also
    available.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Science, Academic Research | Presentation Using Jupyter Notebook**'
  prefs: []
  type: TYPE_NORMAL
- en: Since Python has a huge developer community, it is easier to find Python talent
    among students and researchers to transition into Data Science and academic research,
    even writing production code using Pytorch. Eliminate the need to learn another
    language. Many data analysts and scientists are already familiar with Jupyter
    Notebook, on which Pytorch operates perfectly. Read more in our deploying Pytorch
    model to Amazon Web Service SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pytorch is a Deep Learning Framework**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pytorch is a deep learning framework just like Tensorflow, which means: for
    traditional machine learning models, use another tool for now. Scikit-learn a
    Pythonic deep learning framework with extremely easy-to-use API. The documentation
    is quite good, each page has an example with code snippets at the bottom. Check
    it out. Did you know many Kaggle users including masters still use sklearn train_test_split()
    to split and scaler to pre-process data, sklearn Gradient Boosting Tree or Support
    Vector Machine to benchmark performance, and the top notch high-performance XGBoost
    is notably missing.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tensorflow.js** and **Tensorflow.lite** gives Tensorflow wings in the browser
    and on mobile devices. Apple just announced CreateML for Swift June 2019\. Mobile
    support is not native yet in Pytorch. Do not dispair. Scroll down to read about [ONNX
    an exchange format that is supported by almost of all of the popular frameworks](http://onnx.ai/supported-tools). [Pytorch
    also has a tutorial on moving a model to mobile, though this road is still bit
    detoured compared to Tensorflow](https://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Installation Pytorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/b4233f6a8a1afcd239dbe879eb37cfd2.png)'
  prefs: []
  type: TYPE_IMG
- en: For installation tips use the official Pytorch documentation first. [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/) The
    above screenshot is an example of available installations.
  prefs: []
  type: TYPE_NORMAL
- en: Using Anaconda to install Pytorch is a great start across all systems including
    and Windows. We were able to install Pytorch with Anaconda on a gaming computer
    and start to use its CUDA GPU feature right away. [Read our Anaconda Cheatsheet
    here](https://medium.com/data-science-bootcamp/anaconda-miniconda-cheatsheet-for-data-scientists-2c1be12f56db).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Hello World in Pytorch is as easy as launching a Google Colab (yes, right on
    Google’s turf), and `import torch` , [check out this shared view only notebook](https://colab.research.google.com/drive/1uK7BnOWj_-MXI4a5h4I9SLMrqhPnG9Qu).
    Modern hosted data science notebooks like Kaggle Kernel and Google Colab all come
    with Pytorch pre-intalled.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look Ma: deep learning with no server!'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Prefer Jupyter Notebook based tutorials instead? Getting started using the Udacity
    Intro to Pytorch repo, found at the bottom of this article.
  prefs: []
  type: TYPE_NORMAL
- en: Prefer other installation methods? Binaries, from source, and docker image,
    see Source 2.
  prefs: []
  type: TYPE_NORMAL
- en: '**Code Snippets from Source 3**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`import torch` is needed for core pytorch tasks. You also see torch neural
    nets module `nn`, optimizer `optim` and computer vision module `torchvision`,
    data transformer pipeline `transforms` , `datasets` and existing `models` being
    imported. In this case the Kaggle team also used a `SubsetRandomSampler`, you
    will see in a later snippet how it feeds into the data transformation and loading
    pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: Data Transformation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Code Snippets from Source 3**'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the train, test and validation transformers are similar but different.
    To augment data, training data is randomly rotated, resized and cropped, even
    vertically flipped (in this case a flipped Malaria cell does not negatively affect
    classification results). Because test and validation data should mimic real world
    data, no random noise or flipping is introduced. Just as it is, with center crop.
    Note that the size must match constantly. Deep Learning is a lot of matrix multiplication.
    Dimension size always matters. In image classification tasks, we typically want
    to normalize images according to the pre-trained model or existing dataset we
    will be using.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Why the normalization? And what are those strange numbers? Mean and standard
    deviation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Definitely don’t forget ToTensor to transform all to a pytorch tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Why? Because Pytorch expects it. Read this thread by Chimtala smth creator of
    Pytorch [Source 5].
  prefs: []
  type: TYPE_NORMAL
- en: input image is first loaded to range [0, 1] and then this normalization is applied
    to RGB image as described here .. torch vision — Datasets, Transforms and Models
    specific to Computer Vision
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*All pre-trained models expect input images normalized in the same way, i.e.
    mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected
    to be at least 224.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*The images have to be loaded in to a range of [0, 1] and then normalized using
    mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*An example of such normalization can be found in the imagenet example here
    [Source 4]*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Loading Data Using Train and Test Loaders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Code Snippet from Source 3**'
  prefs: []
  type: TYPE_NORMAL
- en: '`datasets.ImageFolder` and the `torch.utils.data.DataLoader` work together
    to load train, valid and test data separately, based on batch_size, sampling after
    data transformation in the previous section. Each dataset has its own loader.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Pytorch Model in a Nutshell
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using Sequential is one easy way to quickly define a model. A named ordered
    dictionary holds all the layers that are encapsulated in `nn.Sequential`, which
    is then stored in the `model.classifier` variable. This is a quick way to define
    the bare bone of a model but not necessarily the most Pythonic. It helps us illustrate
    a Python model is consisted of fully connected Linear Layers with shape specified
    in (row, col) tuples. ReLU activation layers, Dropout with 20% probability and
    an output Softmax function or LogSoftmax function. Don’t worry about that now.
    All you need to know is that Softmax is usually the last layer of a Deep Learning
    model of **multi-class classification** tasks. The famous ImageNet dataset has
    1000+ classes, so the output of Softmax has at least 1000+ components.
  prefs: []
  type: TYPE_NORMAL
- en: A collection of fully connected layers with ReLU activation in between some
    dropouts and at last, another fully connected linear layer which feeds into a
    Softmax activation is very typical of a vanilla Deep Learning Neural Network.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In Pytorch it is easy to view the structure of your model just use `print(model_name)`.
    More on that later.
  prefs: []
  type: TYPE_NORMAL
- en: '**Code Snippet in Source 3**'
  prefs: []
  type: TYPE_NORMAL
- en: As previously mentioned, in transfer learning in the why pytorch section, we
    can use a pretrained model such as resnet50\. Turning gradient off for all layers
    except the last newly added, fully connected layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the last layer is 2048 by 2 because we are classifying just two classes:
    true or false, malaria or not.'
  prefs: []
  type: TYPE_NORMAL
- en: The model variable returns a massive ResNet model structure with our customized
    last layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the code snippet below, we omitted a lot of details to make the model architecture
    fit on the screen. If you visit the source 3 kernel link, you will see quite a
    few bottlenecks and sequential layers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here we showed that the last customized layer is indeed 2048 by 2\. Note that
    the layer before the average pooling and relu is 2048, hence 2048 by 2.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Pytorch Training Loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The training loop is perhaps the most characteristic of Pytorch as a deep learning
    framework. In Sklearn can make it go away with `fit` and in Tensorflow with `transform`.
    In Pytorch this part is much more involved.
  prefs: []
  type: TYPE_NORMAL
- en: '`model.train()` tells your model that you are training the model. So effectively
    layers like dropout, batchnorm etc. which behave different on the train and test
    procedures know what is going on and hence can behave accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More details: It sets the mode to train (see [source code](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.train)).
    You can call either model.eval() or model.train(mode=False) to tell that you are
    testing. It is somewhat intuitive to expect `train`function to train model but
    it does not do that. It just sets the mode.'
  prefs: []
  type: TYPE_NORMAL
- en: '`model.eval()` will notify all your layers that you are in eval mode, that
    way, batchnorm or dropout layers will work in eval model instead of training mode.'
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.no_grad()` impacts the autograd engine and deactivate it. It will reduce
    memory usage and speed up computations but you won’t be able to backprop (which
    you don’t want in an eval script). — albanD'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code Snippet Training Loop Source 3**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Check CUDA availability, set criterion to CrossEntropyLoss(), and optimizer
    to Stochastic Gradient Descent for example before starting to train. Note that
    the learning rate starts very small at about `lr=0.001`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the training loop typically takes in number of epochs, model architecture
    optimizer and criterion. The loss has to be zero’ed out first at the start of
    every loop! The big loop iterates through the number of epochs. In each epoch,
    we put the model into training model `model.train()`, move model and data to CUDA
    if needed, `optimizer.zero_grad()` zero out the gradient before starting, predict
    output, use criterion to calculate loss, `loss.backward` to calculate the delta
    weight change based on optimizer, `optimizer.step()` to take one backward step
    to update weight. When validating, turns the model to eval model `model.eval()`.
    Save the model if validation loss has decreased, keep track of the lowest validation
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Developer Tools
  prefs: []
  type: TYPE_NORMAL
- en: '[Train on more than one GPU](https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training Loops:'
  prefs: []
  type: TYPE_NORMAL
- en: Train and Forward
  prefs: []
  type: TYPE_NORMAL
- en: '**Free GPU in the Cloud**'
  prefs: []
  type: TYPE_NORMAL
- en: Thanks you new cloud technology like Google Colab. Once you set up the notebook,
    you can continue train and monitor on your mobile devices! Cloud choices including
    Google Colab, Kaggle, AWS. Local choices including your own laptop and your gaming
    computer.
  prefs: []
  type: TYPE_NORMAL
- en: We repurposed an msi NVIDIA GTX 1060 previously for Assassin’s Creed Origin
    :D If you want to know more let us know.
  prefs: []
  type: TYPE_NORMAL
- en: Be able to train on a GPU locally has been a major advantage. We were able to
    iterate through parameter tuning combinations fast without interuption. Google
    Colab has a 12-hour timeout as well as a 12GB quota limit. If you are an advanced
    user, be sure to avoid constantly downloading the dataset, instead store it in
    your Google Drive. After deleting your model in Google Drive, be sure to empty
    trash to actually delete it.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of where the model is trained, if the training loss has gone down
    a lot near zero, but the validation loss does not decrease (there’s no test dataset),
    you may want to watch out your model is overfitting and it may be memorizing training
    data. Halt and tune your parameters. Even if you achieve 99% accuracy your model
    may not generalize, hence it’s a possibility that it cannot be used else where.
    This paragraph of information is especially relevant for Udacity students doing
    scholarship challenges and nanodegrees. Be very suspicious of 99% accuracy, but
    do a brief dance to celebrate first.
  prefs: []
  type: TYPE_NORMAL
- en: Forwarding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pytorch forward pass will actually calculate `y = wx + b` before that we are
    just writing placeholders.
  prefs: []
  type: TYPE_NORMAL
- en: Useful Pytorch Libraries and Modules and Installations.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The Torchvision module is quite powerful. It has image processing and data processing
    codes, Convolutional Neural Nets (CNN), and other pretrained models like ResNet
    and VGG.
  prefs: []
  type: TYPE_NORMAL
- en: Transformation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`transforms.ToTensor()` convert data array to Pytorch tensor. Advantage include
    easy to use in CUDA, GPU training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developers can [build transformation and transformation pipelines](https://pytorch.org/docs/stable/torchvision/transforms.html) in
    Pytorch. Pipeline is a fancy way to say transformations that are chained together,
    and performed sequentially.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pytorch Convolutional Neural Networks (CNN)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section is under construction… check back soon.
  prefs: []
  type: TYPE_NORMAL
- en: Use Convolutional Neural Networks to complete computer vision, image recognition
    tasks. The transformation and data augmentation APIs are very important, especially
    when training data is limited. Alternatively one can also use many of the preloaded
    model architectures — read our transfer learning section.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Maxpooling layer discards detailed spatial information contained in the original
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Inspect Pytorch Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First init the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In Pytorch, use `print(model_name)` to print out the model and architecture
    of the model. You can easily see what the model is all about.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note that each layer is named (numbered and can be queried by this index).We
    used ellipsis to omit model details so that the VGG model can fit on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pro tip: inspecting the model architecture is a must. Transfer learning, in
    a nutshell, is about modifying the last few classifier layers. [Read more in our
    transfer learning article](https://medium.com/data-science-bootcamp/transfer-learning-with-pytorch-code-snippet-load-a-pretrained-model-900374950004).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pro tip: for Tensorflow use `keras model.summary()` to review the entire model
    architecture. It even outputs number of parameters and dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pytorch Versions
  prefs: []
  type: TYPE_NORMAL
- en: For Udacity projects, not all nanodegrees have moved to Pytorch 1.0 It is important
    to use the right version for the right project. You may also need to change the
    Kernel in Jupyter Notebook to use corresponding version of Python. Udacity projects
    have moved onto Python3\. Not all projects in the real world has migrated to Python
    3\. But it’s about time to move on from Python 2.
  prefs: []
  type: TYPE_NORMAL
- en: '**Using CUDA**'
  prefs: []
  type: TYPE_NORMAL
- en: Though we decided to put CUDA in the advanced section, but the reality is CUDA
    is so easy to use. Just use it… today! With Anaconda, Pytorch, and CUDA, we were
    able to turn a gaming computer with an NVDIA graphics card into a home deep learning
    powerhouse. No configuration needed! It just works. The framework just works on
    a windows machine! (It is an msi NVIDIA GTX 1060 previously for Assassin’s Creed
    Origin :D If you want to know more let us know.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: One common error of using CUDA with Pytorch is not moving both the model and
    the data to CUDA. And when needed move both of them back to CPU. Generally your
    model and data should always live in the same space.
  prefs: []
  type: TYPE_NORMAL
- en: '**Deploying Pytorch in Production**: There are two methods of turning existing
    Pytorch models to production ready deployment *trace and scripting*. Tracing does
    not support complex models with control flow in the code. Scripting supports Pytorch
    codes with control flow but supports only a limited number of Python modules.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Choosing the best Softmax result**: in multi-class classification, the activation
    Softmax function is often used. Pytorch has a dedicated function to extract top
    results — the most likely class from Softmax output. `torch.topk(input, k, dim)` returns
    the top probability.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '**Consume Pytorch Models on Other Platforms**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: “Export models in the standard ONNX (Open Neural Network Exchange) format for
    direct access to ONNX-compatible platforms, runtimes, visualizers, and more.”
    — Pytorch 1.0 Documentation
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**More on Pytorch Transfer Learning**'
  prefs: []
  type: TYPE_NORMAL
- en: To use an existing model is equivalent to freeze some of its layers and parameters
    and not train those. Turn off training autograd by setting `require_grad` to False.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '**Hyperparameter Tuning**'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to using the right optimizer and adjusting learning rate according.
    You can use the learning rate scheduler to dynamically adjust your learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[Read more about scheduler here.](https://discuss.pytorch.org/t/how-to-use-torch-optim-lr-scheduler-exponentiallr/12444)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model and Check Point Saving**'
  prefs: []
  type: TYPE_NORMAL
- en: Save and Load Model Checkpoint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Pro tip: Did you know you can save and load models locally and in google drive?
    This way you don’t have to start from scratch every time. For example, if you
    already trained 5 epochs. You can save the weights and train another 5 epochs.
    Now you did 10 epochs total! Very convenient. The free GPU resources time out
    and get erased very often. Remember incremental training is possible.'
  prefs: []
  type: TYPE_NORMAL
- en: You can also save a checkpoint and load it locally. You may see both extension `.pt` and `.pth` .
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '**Code Snippet from Source 3**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note you must save any checkpoint on Google Colab to your Google Drive, else
    your data may be erased every 12 hours or sooner. Though the GPU access is free,
    the storage is temporary on Google Colab.
  prefs: []
  type: TYPE_NORMAL
- en: Making Predictions with Pytorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here’s what we don’t like about Pytorch. Making predictions with Pytorch seems
    to be a bit patched together. Writing your own training loop seems easy-to-customize,
    though it is harder to write than Tensorflow, it makes sense to trade a bit of
    discomfort for customization. But prediction is strange which some functions that
    appear to be hacked together. See these code snippets below to see what we mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code Snippets from Source 3:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`output.data.max`, `np.sum(np.squeeze())` , `cpu().numpy()`, `.unsqueeze(0)`,
    `torch.argmax` …WTF?! It’s super frustrating.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The important take away is: we are working with logits converted to probabilities
    here, and there are tensors that need to be turned into matrices and extra brackets
    removed. We need to know which one is most likely class using `max` or `argmax`.
    We need to move tensors back to CPU so `cpu()` and tensor needs to be turned into
    ndarray for ease of computation so `numpy()`.'
  prefs: []
  type: TYPE_NORMAL
- en: Pytorch is a deep learning framework, and deep learning is frequently about
    matrix math, which is about getting the dimensions right, so squeeze and unsqueeze
    have to be used to make dimensions match.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This part of the cheatsheet will save you a lot of headache and you are welcome!
    `np.squeeze()` removed the extra set of `[]` and an extra dimension out of `[[1,2,3]]`
    of shape `(1, 3)`. Now it is just `(3,)`. A
  prefs: []
  type: TYPE_NORMAL
- en: And `unsqueeze` made `[1]` shape of `(1,)` to `[[1]]` shape `(1,1).`
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Pytorch Data Science Nanodegree Deep Learning Intro to Pytorch notebooks and
    tutorials by Udacity](https://github.com/udacity/DSND_Term1/tree/master/lessons/DeepLearning/new-intro-to-pytorch).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Transfer Learning with Pytorch — Our super short effective article](https://medium.com/data-science-bootcamp/transfer-learning-with-pytorch-code-snippet-load-a-pretrained-model-900374950004)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Source 1: [https://research.fb.com/downloads/pytorch/](https://research.fb.com/downloads/pytorch/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Source 2: Pytorch Github main [https://github.com/pytorch/pytorch#getting-started](https://github.com/pytorch/pytorch#getting-started) also
    makes a great cheat sheet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extremely awesome forum including many active core contributors, supplements
    the documentations [https://discuss.pytorch.org/](https://discuss.pytorch.org/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Source 3: Malaria Detection with Pytorch kaggle Kernel [https://www.kaggle.com/devilsknight/malaria-detection-with-pytorch](https://www.kaggle.com/devilsknight/malaria-detection-with-pytorch)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Source 4: [https://github.com/pytorch/examples/blob/97304e232807082c2e7b54c597615dc0ad8f6173/imagenet/main.py#L197-L198](https://github.com/pytorch/examples/blob/97304e232807082c2e7b54c597615dc0ad8f6173/imagenet/main.py#L197-L198)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Source 5: Pytorch community forum Pytorch image transform normalization [https://discuss.pytorch.org/t/whats-the-range-of-the-input-value-desired-to-use-pretrained-resnet152-and-vgg19/1683](https://discuss.pytorch.org/t/whats-the-range-of-the-input-value-desired-to-use-pretrained-resnet152-and-vgg19/1683)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[Uniqtech](https://theoptips.github.io/uniqtech_1/)**: We write beginner
    friendly cheatsheets like this all the time. Follow our profile and our most popular
    Data Science Bootcamp publication. Check out our one page article on Transfer
    Learning in Pytorch, Pytorch on Amazon SageMaker, and Anaconda Cheatsheet for
    Data Science. We are primarily on Medium, a community we love and find strong
    affinity in. Medium treats its writers well, and has a phenomenal reader community.
    If you would like to find out about our upcoming Data Science Bootcamp course
    releasing Fall 2019, scholarship for high quality articles, or want to write for
    us, contribute feedback please email us [hi@uniqtech.co](mailto:hi@uniqtech.co).
    Thank you Medium community!'
  prefs: []
  type: TYPE_NORMAL
- en: Key author and contributor [Sun](https://medium.com/u/16c164f26caa?source=post_page---------------------------).
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/@uniqtech/pytorch-cheat-sheet-for-beginners-and-udacity-deep-learning-nanodegree-5aadc827de82).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Deep Learning Cheat Sheets](/2018/11/deep-learning-cheat-sheets.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Getting started with NLP using the PyTorch framework](/2019/04/nlp-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3 More Google Colab Environment Management Tips](/2019/01/more-google-colab-environment-management-tips.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Streamlit for Machine Learning Cheat Sheet](https://www.kdnuggets.com/2023/01/streamlit-machine-learning-cheat-sheet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning with ChatGPT Cheat Sheet](https://www.kdnuggets.com/2023/05/machine-learning-chatgpt-cheat-sheet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Scikit-learn for Machine Learning Cheat Sheet](https://www.kdnuggets.com/2022/12/scikit-learn-machine-learning-cheatsheet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The KDnuggets 2023 Cheat Sheet Collection](https://www.kdnuggets.com/the-kdnuggets-2023-cheat-sheet-collection)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Cleaning with Python Cheat Sheet](https://www.kdnuggets.com/2023/02/data-cleaning-python-cheat-sheet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The ChatGPT Cheat Sheet](https://www.kdnuggets.com/2023/01/chatgpt-cheat-sheet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
