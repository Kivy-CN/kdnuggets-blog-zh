- en: How to Build a Scalable Data Architecture with Apache Kafka
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用 Apache Kafka 构建可扩展的数据架构
- en: 原文：[https://www.kdnuggets.com/2023/04/build-scalable-data-architecture-apache-kafka.html](https://www.kdnuggets.com/2023/04/build-scalable-data-architecture-apache-kafka.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/04/build-scalable-data-architecture-apache-kafka.html](https://www.kdnuggets.com/2023/04/build-scalable-data-architecture-apache-kafka.html)
- en: '![How to Build a Scalable Data Architecture with Apache Kafka](../Images/9c5d0e1e11b48a64ac229f965a43009f.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![如何使用 Apache Kafka 构建可扩展的数据架构](../Images/9c5d0e1e11b48a64ac229f965a43009f.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Apache Kafka is a distributed message-passing system that works on a publisher-subscriber
    model. It is developed by Apache Software Foundation and written in Java and Scala.
    Kafka was created to overcome the problem faced by the distribution and scalability
    of traditional message-passing systems. It can handle and store large volumes
    of data with minimal latency and high throughput. Due to these benefits, it can
    be suitable for making real-time data processing applications and streaming services.
    It is currently open-source and used by many organisations like Netflix, Walmart
    and Linkedin.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka 是一个分布式消息传递系统，采用发布-订阅模型。它由 Apache 软件基金会开发，使用 Java 和 Scala 编写。Kafka
    的创建旨在克服传统消息传递系统在分发和扩展性方面面临的问题。它可以以最小的延迟和高吞吐量处理和存储大量数据。由于这些优点，它适用于实时数据处理应用程序和流媒体服务。它目前是开源的，被许多组织如
    Netflix、沃尔玛和 LinkedIn 使用。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: A Message Passing System makes several applications send or receive data from
    each other without worrying about data transmission and sharing. **Point-to-Point**
    and **Publisher-Subscriber** are two widespread message-passing systems. In point-to-point,
    the sender pushes the data into the queue, and the receiver pops from it like
    a standard queue system following FIFO(first in, first out) principle. Also, the
    data gets deleted once it gets read, and only a single receiver is allowed at
    a time. There is no time dependency laid for the receiver to read the message.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 消息传递系统使多个应用程序可以相互发送或接收数据，而无需担心数据传输和共享问题。**点对点**和**发布-订阅**是两种广泛使用的消息传递系统。在点对点系统中，发送方将数据推送到队列中，接收方从队列中弹出数据，就像一个标准的队列系统，遵循
    FIFO（先进先出）原则。此外，数据在被读取后会被删除，并且每次只允许一个接收方。接收方读取消息没有时间依赖。
- en: '![How to Build a Scalable Data Architecture with Apache Kafka](../Images/efa5d4c06f76f04465e52ebcdb70dc19.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![如何使用 Apache Kafka 构建可扩展的数据架构](../Images/efa5d4c06f76f04465e52ebcdb70dc19.png)'
- en: '**Fig.1** Point-to-Point Message System | Image by Author'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1** 点对点消息系统 | 图片由作者提供'
- en: In the Publisher-Subscriber model, the sender is termed a publisher, and the
    receiver is termed a subscriber. In this, multiple senders and receivers can read
    or write data simultaneously. But there is a time dependency in it. The consumer
    has to consume the message before a certain amount of time, as it gets deleted
    after that, even if it didn’t get read. Depending on the user's configuration,
    this time limit can be a day, a week, or a month.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在发布-订阅模型中，发送方称为发布者，接收方称为订阅者。在这种模型下，多个发送方和接收方可以同时读取或写入数据。但是，它存在时间依赖性。消费者必须在一定时间内消费消息，因为超过该时间消息将被删除，即使未被读取。根据用户的配置，这个时间限制可以是一天、一周或一个月。
- en: '![How to Build a Scalable Data Architecture with Apache Kafka](../Images/a47141ce8f4609d6c2550993775a5f18.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![如何使用 Apache Kafka 构建可扩展的数据架构](../Images/a47141ce8f4609d6c2550993775a5f18.png)'
- en: '**Fig.2** Publisher-Subscriber Message System | Image by Author'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2** 发布-订阅消息系统 | 图片由作者提供'
- en: Kafka Architecture
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka 架构
- en: 'Kafka architecture consists of several key components:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 架构由几个关键组件组成：
- en: Topic
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主题
- en: Partition
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分区
- en: Broker
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理
- en: Producer
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生产者
- en: Consumer
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 消费者
- en: Kafka-Cluster
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kafka集群
- en: Zookeeper
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zookeeper
- en: '![How to Build a Scalable Data Architecture with Apache Kafka](../Images/883018809ecf5ef892050f17df50b2d0.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![如何构建一个可扩展的数据架构与Apache Kafka](../Images/883018809ecf5ef892050f17df50b2d0.png)'
- en: '**Fig.3** Kafka Architecture | Image by [ibm-cloud-architecture](https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-overview/)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**图3** Kafka架构 | 图片来源：[ibm-cloud-architecture](https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-overview/)'
- en: Let’s briefly understand each component.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要了解每个组件。
- en: Kafka stores the messages in different **Topics**. A topic is a group that contains
    the messages of a particular category. It is similar to a table in a database.
    A topic can be uniquely identified by its name. We cannot create two topics with
    the same name.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka将消息存储在不同的**主题**中。主题是包含特定类别消息的组。它类似于数据库中的表。一个主题可以通过其名称唯一标识。我们不能创建两个名称相同的主题。
- en: The topics are further classified into **Partitions.** Each record of these
    partitions is associated with a unique identifier termed **Offset**, which denotes
    the position of the record in that partition.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 主题进一步被分类为**分区**。这些分区的每条记录都与一个称为**偏移量**的唯一标识符相关联，这个标识符表示记录在该分区中的位置。
- en: Other than this, there are Producers and Consumers in the system. Producers
    write or publish the data in the topics using the Producing APIs. These producers
    can write either on the topic or partition levels.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，系统中还有生产者和消费者。生产者使用生产API在主题中写入或发布数据。这些生产者可以在主题或分区级别写入数据。
- en: Consumers read or consume the data from the topics using the Consumer APIs.
    They can also read the data either at the topic or partition levels. Consumers
    who perform similar tasks will form a group known as the **Consumer Group**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者使用消费者API从主题中读取或消费数据。他们也可以在主题或分区级别读取数据。执行类似任务的消费者将组成一个称为**消费者组**的群体。
- en: There are other systems like **Broker** and **Zookeeper,** which run in the
    background of Kafka Server. Brokers are the software that maintains and keeps
    the record of published messages. It is also responsible for delivering the right
    message to the right consumer in the correct order using offsets. The set of brokers
    collectively communicating with each other can be called **Kafka clusters**. Brokers
    can be dynamically added or removed from the Kafka cluster without facing any
    downtime in the system. And one of the brokers in the Kafka cluster is termed
    a **Controller**. It manages states and replicas inside the cluster and performs
    administrative tasks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他系统，如**代理**和**Zookeeper**，它们在Kafka服务器的后台运行。代理是维护和记录已发布消息的软件。它还负责使用偏移量将正确的消息传递给正确的消费者，并按照正确的顺序传递。集体相互通信的代理组可以称为**Kafka集群**。代理可以动态添加或从Kafka集群中移除，而不会导致系统停机。Kafka集群中的一个代理被称为**控制器**。它管理集群内的状态和副本，并执行管理任务。
- en: On the other hand, Zookeeper is responsible for maintaining the health status
    of the Kafka cluster and coordinating with each broker of that cluster. It maintains
    the metadata of each cluster in the form of key-value pairs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Zookeeper负责维护Kafka集群的健康状态，并与集群中的每个代理协调。它以键值对的形式维护每个集群的元数据。
- en: This tutorial is mainly focused on the practical implementation of Apache Kafka.
    If you want to read more about its architecture, you can read [this](https://www.upsolver.com/blog/apache-kafka-architecture-what-you-need-to-know#:~:text=Its%20core%20architectural%20concept%20is,maintain%20a%20consistent%20system%20state.)
    article by Upsolver.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程主要集中在Apache Kafka的实际实现上。如果你想了解更多关于其架构的内容，可以阅读[这篇](https://www.upsolver.com/blog/apache-kafka-architecture-what-you-need-to-know#:~:text=Its%20core%20architectural%20concept%20is,maintain%20a%20consistent%20system%20state.)由Upsolver撰写的文章。
- en: 'Taxi Booking App: A Practical Use Case'
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 出租车预订应用程序：一个实际的用例
- en: Consider the use case of a taxi booking service like Uber. This application
    uses Apache Kafka to send and receive messages through various services like Transactions,
    Emails, Analytics, etc.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个像Uber这样的出租车预订服务的用例。这个应用程序使用Apache Kafka通过各种服务（如事务、电子邮件、分析等）发送和接收消息。
- en: '![How to Build a Scalable Data Architecture with Apache Kafka](../Images/664bb35c487cf63d41198f8b0d60b017.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![如何构建一个可扩展的数据架构与Apache Kafka](../Images/664bb35c487cf63d41198f8b0d60b017.png)'
- en: '**Fig.4** Architecture of the Taxi App | Image by Author'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**图4** 出租车应用程序的架构 | 图片来源：作者'
- en: The architecture consists of several services. The `Rides` service receives
    the ride request from the customer and writes the ride details on the Kafka Message
    System.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 架构包括几个服务。`Rides`服务接收来自客户的乘车请求，并将乘车详细信息写入Kafka消息系统。
- en: Then these order details were read by the `Transaction` service, which confirms
    the order and payment status. After confirming that ride, this `Transaction` service
    writes the confirmed ride again in the message system with some additional details.
    And then finally, the confirmed ride details are read by other services like Email
    or Data Analytics to send the confirmation mail to the customer and to perform
    some analysis on it.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这些订单详细信息被`Transaction`服务读取，该服务确认订单和支付状态。确认该乘车后，`Transaction`服务将确认的乘车再次写入消息系统，并附加一些额外的详细信息。最后，确认的乘车详细信息会被其他服务（如电子邮件或数据分析）读取，以向客户发送确认邮件并进行一些分析。
- en: We can execute all these processes in real-time with very high throughput and
    minimum latency. Also, due to the capability of horizontal scaling of Apache Kafka,
    we can scale this application to handle millions of users.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以非常高的吞吐量和最小的延迟实时执行所有这些过程。此外，由于Apache Kafka的水平扩展能力，我们可以扩展此应用程序以处理数百万用户。
- en: Practical Implementation of the above Use Case
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上述用例的实际实现
- en: This section contains a quick tutorial to implement the kafka message system
    in our application. It includes the steps to download kafka, configure it, and
    create producer-consumer functions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含了在我们的应用程序中实现Kafka消息系统的快速教程。它包括下载Kafka、配置它以及创建生产者-消费者函数的步骤。
- en: '**Note:** This tutorial is based on python programming language and uses a
    windows machine.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 本教程基于Python编程语言，并使用Windows机器。'
- en: Apache Kafka Downloading Steps
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Kafka下载步骤
- en: 1.Download the latest version of Apache Kafka from [that](https://kafka.apache.org/downloads)
    link. Kafka is based on JVM languages, so Java 7 or greater version must be installed
    in your system.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从[该链接](https://kafka.apache.org/downloads)下载最新版本的Apache Kafka。Kafka基于JVM语言，因此必须在系统中安装Java
    7或更高版本。
- en: Extract the downloaded zip file from your computer's (C:) drive and rename the
    folder as `/apache-kafka`.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从计算机（C:）驱动器中提取下载的zip文件，并将文件夹重命名为`/apache-kafka`。
- en: The parent directory contain two sub-directories, `/bin` and `/config`, which
    contains the executable and configuration files for the zookeeper and the kafka
    server.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上级目录包含两个子目录，`/bin`和`/config`，其中包含Zookeeper和Kafka服务器的可执行文件和配置文件。
- en: Configuration Steps
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置步骤
- en: First, we need to create log directories for the Kafka and Zookeeper servers.
    These directories will store all the metadata of these clusters and the messages
    of the topics and partitions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要为Kafka和Zookeeper服务器创建日志目录。这些目录将存储这些集群的所有元数据以及主题和分区的消息。
- en: '**Note:** By default, these log directories are created inside the `/tmp` directory,
    a volatile directory that vanishes off all the data inside when the system shuts
    down or restarts. We need to set the permanent path for the log directories to
    resolve this issue. Let’s see how.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 默认情况下，这些日志目录会创建在`/tmp`目录中，这是一个易失性目录，当系统关闭或重新启动时，所有数据都会消失。我们需要为日志目录设置永久路径以解决此问题。让我们看看如何操作。'
- en: Navigate to `apache-kafka` >> `config` and open the `server.properties` file.
    Here you can configure many properties of kafka, like paths for log directories,
    log retention hours, number of partitions, etc.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 导航到`apache-kafka` >> `config`并打开`server.properties`文件。在这里，你可以配置Kafka的许多属性，例如日志目录路径、日志保留时间、分区数量等。
- en: Inside the `server.properties` file, we have to change the path of the log directory's
    file from the temporary `/tmp` directory to a permanent directory. The log directory
    contains the generated or written data in the Kafka Server. To change the path,
    update the `log.dirs` variable from `/tmp/kafka-logs` to `c:/apache-kafka/kafka-logs`.
    This will make your logs stored permanently.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在`server.properties`文件中，我们需要将日志目录文件的路径从临时的`/tmp`目录更改为永久目录。日志目录包含Kafka服务器中生成或写入的数据。要更改路径，请将`log.dirs`变量从`/tmp/kafka-logs`更新为`c:/apache-kafka/kafka-logs`。这将使日志永久存储。
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The Zookeeper server also contains some log files to store the metadata of the
    Kafka servers. To change the path, repeat the above step, i.e open `zookeeper.properties`
    file and replace the path as follows.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Zookeeper服务器还包含一些日志文件，用于存储Kafka服务器的元数据。要更改路径，请重复上述步骤，即打开`zookeeper.properties`文件并按如下方式替换路径。
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This zookeeper server will act as a resource manager for our kafka server.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Zookeeper 服务器将作为我们 Kafka 服务器的资源管理器。
- en: Run the Kafka and Zookeeper Servers
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行 Kafka 和 Zookeeper 服务器
- en: To run the zookeeper server, open a new cmd prompt inside your parent directory
    and run the below command.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行 Zookeeper 服务器，请在父目录下打开一个新的 cmd 提示符，并运行以下命令。
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![How to Build a Scalable Data Architecture with Apache Kafka](../Images/b6dc180652bec959922eb6b09f7361dd.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![如何使用 Apache Kafka 构建可扩展的数据架构](../Images/b6dc180652bec959922eb6b09f7361dd.png)'
- en: Image by Author
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Keep the zookeeper instance running.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 保持 Zookeeper 实例运行。
- en: To run the kafka server, open a separate cmd prompt and execute the below code.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行 Kafka 服务器，请打开一个独立的 cmd 提示符，并执行以下代码。
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Keep the kafka and zookeeper servers running, and in the next section, we will
    create producer and consumer functions which will read and write data to the kafka
    server.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 保持 Kafka 和 Zookeeper 服务器运行，在下一节中，我们将创建生产者和消费者函数，它们将读取和写入 Kafka 服务器中的数据。
- en: Creating Producer & Consumer Functions
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建生产者和消费者函数
- en: For creating the producer and consumer functions, we will take the example of
    our e-commerce app that we discussed earlier. The `Orders` service will function
    as a producer, which writes order details to the kafka server, and the Email and
    Analytics service will function as a consumer, which reads that data from the
    server. The Transaction service will work as a consumer as well as a producer.
    It reads the order details and writes them back again after transaction confirmation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建生产者和消费者函数，我们将以之前讨论的电子商务应用为例。`Orders` 服务将作为生产者，将订单详情写入 Kafka 服务器，而 Email
    和 Analytics 服务将作为消费者，从服务器读取数据。Transaction 服务将既作为消费者又作为生产者工作。它读取订单详情，并在确认交易后将其重新写回。
- en: But first, we need to install the Kafka python library, which contains inbuilt
    functions for Producer and Consumers.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，我们需要安装 Kafka 的 Python 库，它包含用于生产者和消费者的内置函数。
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, create a new directory named `kafka-tutorial`. We will create the python
    files inside that directory containing the required functions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，创建一个名为 `kafka-tutorial` 的新目录。我们将在该目录中创建包含所需函数的 Python 文件。
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Producer Function:**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**生产者函数：**'
- en: Now, create a python file named `rides.py` and paste the following code into
    it.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，创建一个名为 `rides.py` 的 Python 文件，并将以下代码粘贴到其中。
- en: '`rides.py`'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`rides.py`'
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Explanation:**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**解释：**'
- en: Firstly, we have imported all the necessary libraries, including kafka. Then,
    the topic name and a list of various items are defined. Remember that topic is
    a group that contains similar types of messages. In this example, this topic will
    contain all the orders.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入了所有必要的库，包括 kafka。然后，定义了主题名称和各种项目的列表。请记住，主题是一个包含类似类型消息的组。在这个例子中，这个主题将包含所有的订单。
- en: Then, we create an instance of a KafkaProducer function and connect it to the
    kafka server running on the localhost:9092\. If your kafka server is running on
    a different address and port, then you must mention the server’s IP and port number
    there.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个 KafkaProducer 实例，并将其连接到运行在 localhost:9092 的 Kafka 服务器。如果您的 Kafka 服务器运行在其他地址和端口上，则必须在此处指定服务器的
    IP 和端口号。
- en: After that, we will generate some orders in JSON format and write them to the
    kafka server on the defined topic name. Sleep function is used to generate a gap
    between the subsequent orders.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将生成一些 JSON 格式的订单，并将它们写入定义的主题名称的 Kafka 服务器。使用 Sleep 函数在后续订单之间生成间隔。
- en: '**Consumer Functions:**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**消费者函数：**'
- en: '`transaction.py`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`transaction.py`'
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Explanation:**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**解释：**'
- en: The `transaction.py` file is used to confirm the transitions made by the users
    and assign them a driver and estimated pickup time. It reads the ride details
    from the kafka server and writes it again in the kafka server after confirming
    the ride.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`transaction.py` 文件用于确认用户所做的过渡，并分配给他们一个司机和预计的接送时间。它从 Kafka 服务器读取行程详情，并在确认行程后将其重新写入
    Kafka 服务器。'
- en: Now, create two python files named `email.py` and `analytics.py`, which are
    used to send emails to the customer for their ride confirmation and to perform
    some analysis respectively. These files are only created to demonstrate that even
    multiple consumers can read the data from the Kafka server simultaneously.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，创建两个名为 `email.py` 和 `analytics.py` 的 Python 文件，这些文件分别用于向客户发送行程确认邮件和进行一些分析。这些文件的创建只是为了演示即使是多个消费者也可以同时从
    Kafka 服务器读取数据。
- en: '`email.py`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`email.py`'
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`analysis.py`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`analysis.py`'
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, we have done with the application, in the next section, we will run all
    the services simultaneously and check the performance.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经完成了应用程序，在下一部分，我们将同时运行所有服务并检查性能。
- en: Test the Application
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试应用程序
- en: Run each file one by one in four separate command prompts.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在四个不同的命令提示符中逐一运行每个文件。
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![How to Build a Scalable Data Architecture with Apache Kafka](../Images/aa6353d2ed294dbf68800c4b306dd089.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![如何使用Apache Kafka构建可扩展的数据架构](../Images/aa6353d2ed294dbf68800c4b306dd089.png)'
- en: Image by Author
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: You can receive output from all the files simultaneously when the ride details
    are pushed into the server. You can also increase processing speed by removing
    the delay function in the `rides.py` file. The `rides.py` file pushed the data
    into the kafka server, and the other three files simultaneously read that data
    from the kafka server and function accordingly.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当行程详情被推送到服务器时，您可以同时从所有文件中接收输出。您还可以通过删除`rides.py`文件中的延迟函数来提高处理速度。`rides.py`文件将数据推送到kafka服务器，其他三个文件则同时从kafka服务器读取数据并相应地执行功能。
- en: I hope you get a basic understanding of Apache Kafka and how to implement it.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 希望您对Apache Kafka及其实现有一个基本的了解。
- en: Conclusion
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we have learnt about Apache Kafka, its working and its practical
    implementation using a use case of a taxi booking app. Designing a scalable pipeline
    with Kafka requires careful planning and implementation. You can increase the
    number of brokers and partitions to make these applications more scalable. Each
    partition is processed independently so that the load can be distributed among
    them. Also, you can optimise the kafka configuration by setting the size of the
    cache, the size of the buffer or the number of threads.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们了解了Apache Kafka、它的工作原理以及如何使用出租车预订应用的案例来实际应用它。设计一个可扩展的Kafka管道需要精心规划和实施。您可以增加代理和分区的数量，以使这些应用程序更具可扩展性。每个分区是独立处理的，以便将负载分配到各个分区。同时，您可以通过设置缓存大小、缓冲区大小或线程数量来优化kafka配置。
- en: '[GitHub](https://github.com/aryan0141/apache-kafka-tutorial/tree/master) link
    for the complete code used in the article.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[GitHub](https://github.com/aryan0141/apache-kafka-tutorial/tree/master) 链接，获取文章中使用的完整代码。'
- en: Thanks for reading this article. If you have any comments or suggestions, please
    feel free to contact me on [Linkedin](https://www.linkedin.com/in/aryan-garg-1bbb791a3/).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读本文。如果您有任何评论或建议，请随时通过[Linkedin](https://www.linkedin.com/in/aryan-garg-1bbb791a3/)与我联系。
- en: '**[Aryan Garg](https://www.linkedin.com/in/aryan-garg-1bbb791a3/)** is a B.Tech.
    Electrical Engineering student, currently in the final year of his undergrad.
    His interest lies in the field of Web Development and Machine Learning. He have
    pursued this interest and am eager to work more in these directions.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Aryan Garg](https://www.linkedin.com/in/aryan-garg-1bbb791a3/)** 是一名电气工程学士学位学生，目前在本科的最后一年。他的兴趣领域是网页开发和机器学习。他已追求这一兴趣，并渴望在这些方向上进一步工作。'
- en: More On This Topic
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Building a Formula 1 Streaming Data Pipeline With Kafka and Risingwave](https://www.kdnuggets.com/building-a-formula-1-streaming-data-pipeline-with-kafka-and-risingwave)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Kafka和Risingwave构建F1流数据管道](https://www.kdnuggets.com/building-a-formula-1-streaming-data-pipeline-with-kafka-and-risingwave)'
- en: '[Building a Scalable ETL with SQL + Python](https://www.kdnuggets.com/2022/04/building-scalable-etl-sql-python.html)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用SQL + Python构建可扩展的ETL](https://www.kdnuggets.com/2022/04/building-scalable-etl-sql-python.html)'
- en: '[Generalized and Scalable Optimal Sparse Decision Trees(GOSDT)](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[广义和可扩展的最优稀疏决策树（GOSDT）](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
- en: '[KDnuggets™ News 22:n07, Feb 16: How to Learn Math for Machine…](https://www.kdnuggets.com/2022/n07.html)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets™新闻 22:n07, 2月16日：如何为机器学习学习数学…](https://www.kdnuggets.com/2022/n07.html)'
- en: '[Data Mesh & Its Distributed Data Architecture](https://www.kdnuggets.com/2022/02/data-mesh-distributed-data-architecture.html)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据网格及其分布式数据架构](https://www.kdnuggets.com/2022/02/data-mesh-distributed-data-architecture.html)'
- en: '[Data Mesh Architecture: Reimagining Data Management](https://www.kdnuggets.com/2022/05/data-mesh-architecture-reimagining-data-management.html)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据网格架构：重新构想数据管理](https://www.kdnuggets.com/2022/05/data-mesh-architecture-reimagining-data-management.html)'
