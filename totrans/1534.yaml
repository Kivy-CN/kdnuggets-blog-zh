- en: How Optimization Works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/04/how-optimization-works.html](https://www.kdnuggets.com/2019/04/how-optimization-works.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '[Check out the full course content for How Optimization Works](https://end-to-end-machine-learning.teachable.com/p/building-blocks-how-optimization-works/),
    including video, slides, and code.'
  prefs: []
  type: TYPE_NORMAL
- en: Optimization is a fancy word for "finding the best way." We can see how it works
    if we take a closer look at drinking tea.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: There is a best temperature for tea. If your tea is too hot, it will scald your
    tongue and you won't be able to taste anything for three days. If it’s lukewarm,
    it’s entirely unsatisfying. There is a sweet spot in the middle where it is comfortably
    hot, warming you from the inside out all the way down your throat and radiating
    through your belly. This is the ideal temperature for tea.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/0af83ba72f5c26285d49cca5ec57841c.png)'
  prefs: []
  type: TYPE_IMG
- en: This happy medium is what we try to find in optimization. That’s what Goldilocks
    was looking for when she tried Papa Bear's bed and found it too hard, tried Mama
    Bear's bed and found it too soft, then tried Baby Bear's bed and found it to be
    just right.
  prefs: []
  type: TYPE_NORMAL
- en: Finding how to get things just right turns out to be a very common problem.
    Mathematicians and computer scientist love it because it’s very specific and well
    formulated. You know when you’ve got it right, and you can compare your solution
    against others to see who got it right faster.
  prefs: []
  type: TYPE_NORMAL
- en: When a computer scientist tries to find the right temperature for tea, the first
    thing they do is flip the problem upside down. Instead of trying to maximize tea
    drinking enjoyment, they try to minimize suffering while drinking tea. The result
    is the same, and the math works out in the same way. It's not that all computer
    scientists are pessimists, it's just that most optimization problems are naturally
    described in terms of costs - money, time, resources - rather than benefits. In
    math it's convenient to make all your problems look the same before you work out
    a solution, so that you can just solve it the one time.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/6f92a89002e54bf8d8adc545a6514ead.png)'
  prefs: []
  type: TYPE_IMG
- en: In machine learning, this cost is often called an error function, because error
    is the undesirable thing, the suffering, that is being minimized. It can also
    be called a cost function, a loss function, or an energy function. They all mean
    pretty much the same thing.
  prefs: []
  type: TYPE_NORMAL
- en: Exhaustive search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are a handful of ways to go about finding the best temperature for serving
    tea. The most obvious is just to look at the curve and pick the lowest point.
    Unfortunately, we don't actually know what the curve is when we start out. That
    is implicit in the optimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/792e385b51973e6b64777b3e720bf61e.png)'
  prefs: []
  type: TYPE_IMG
- en: But we can make use of our original idea and just measure the curve. We can
    prepare a cup of tea at a given temperature, serve it, and ask our unwitting test
    subject how they enjoyed it. Then we can repeat this process for every temperature
    across the whole range we care about. By the time we're done with this, we do
    know what the whole curve looks like, and then we can just pick temperature for
    which our tea drinker reported the most enjoyment, that is, the least suffering.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/b7444b9c41f4a36d5f81ade31cd3b263.png)'
  prefs: []
  type: TYPE_IMG
- en: This way of finding the best tea temperature is called exhaustive search. It
    is straightforward and effective, but may take a while. If our time is limited,
    it's worth it to check out a few other methods.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you imagine that our tea-suffering curve is actually a physical bowl, then
    we could easily find the bottom by dropping a marble in and letting it roll until
    it stops. This is the intuition behind gradient descent - literally "going downhill".
  prefs: []
  type: TYPE_NORMAL
- en: To use gradient descent we start at an arbitrary temperature. Before beginning,
    we don't know anything about our curve, so we make a random guess. We brew a cup
    of tea at that temperature and see how well our tea drinker likes it.
  prefs: []
  type: TYPE_NORMAL
- en: '![gradient-descent](../Images/bbef97e27c53db36be0fc88b6360293c.png)'
  prefs: []
  type: TYPE_IMG
- en: From there, the next trick is to figure out which direction is downhill and
    which is up. To figure this out, we choose a direction, and choose a new temperature
    a very small distance away. Let's say we choose a temperature to the left. Then
    we brew up another cup of tea at this slightly cooler temperature and see whether
    or not it is better than the first. We discover that it is actually inferior.
    Now we know that "downhill" is to the right - that we need to make our next cup
    warmer to make it better.
  prefs: []
  type: TYPE_NORMAL
- en: '![gradient-descent2](../Images/2dc5e52721d7a4cc169936de222969b9.png)'
  prefs: []
  type: TYPE_IMG
- en: We take a larger step in the direction of warmer tea, brew up a new cup, and
    start the process over again.
  prefs: []
  type: TYPE_NORMAL
- en: '![gradient-descent](../Images/97c46cb23cc11d7686f85726caa3634c.png)'
  prefs: []
  type: TYPE_IMG
- en: We repeat this until we get to the very best temperature for tea.
  prefs: []
  type: TYPE_NORMAL
- en: '![gradient-descent](../Images/8b0f21c06036f3a48a3fe1f9e5568bba.png)'
  prefs: []
  type: TYPE_IMG
- en: The steeper the slope, the larger the step we can take. The shallower the slope,
    the smaller the step.
  prefs: []
  type: TYPE_NORMAL
- en: '![gradient-descent](../Images/532d7f5e4ef792b50d625b690b95760d.png)'
  prefs: []
  type: TYPE_IMG
- en: We will know we are all done when we take a small step away and get the exact
    same level of enjoyment from our tea drinker. This can only happen at the bottom
    of the bowl, where it is flat and there is no downhill.
  prefs: []
  type: TYPE_NORMAL
- en: There are lots of gradient descent methods. Most of them are clever ways to
    measure the slope as efficiently as possible and to get to the bottom of the bowl
    in as few steps as possible - to brew as few cups of tea as we can get away with.
    They use different tricks to avoid completely calculating the slope or to choose
    a step size that is as large as can be gotten away with, but the underlying intuition
    is the same.
  prefs: []
  type: TYPE_NORMAL
- en: Including curvature
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the tricks to find the bottom in the bowl in fewer steps is to use not
    just slope, but also curvature, when deciding how big of a step to take. As the
    marble starts to roll down the side of the bowl, is the slope getting steeper?
  prefs: []
  type: TYPE_NORMAL
- en: '![gradient-descent-curvature](../Images/27363264ec22da7cd127ed7b68c79a3c.png)'
  prefs: []
  type: TYPE_IMG
- en: If so, then the bottom is probably still far away. Take a big step.
  prefs: []
  type: TYPE_NORMAL
- en: '![gradient-descent-curvature](../Images/a8fa8b2d33460100b77b35e0fa5815de.png)'
  prefs: []
  type: TYPE_IMG
- en: Or is the slope getting shallower and starting to bottom out?
  prefs: []
  type: TYPE_NORMAL
- en: '![gradient-descent-curvature](../Images/2ce32476efd49a69d7b4b3365f4f3af1.png)'
  prefs: []
  type: TYPE_IMG
- en: If so, the bottom is probably getting closer. Take smaller steps now.
  prefs: []
  type: TYPE_NORMAL
- en: '![gradient-descent-curvature](../Images/b2c40ca6ff839a9329d3944903fc0fa0.png)'
  prefs: []
  type: TYPE_IMG
- en: Curvature, this slope-of-the-slope or Hessian, to give it its rightful name,
    can be very helpful if you are trying to take as few steps as possible, however
    it can also be much more expensive to compute. This is a trade-off that comes
    up a lot in optimization. We end up choosing between the number of steps we have
    to take and how hard it is to compute where the next step should be.
  prefs: []
  type: TYPE_NORMAL
- en: How gradient descent can break
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Like a lot of math problems, the more assumptions you’re able to make, the better
    the solution you can come up with. Unfortunately, when working with real data,
    those assumptions don’t always apply.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of ways that this drop-a-marble approach can fail. If there
    is more than one valley for a marble to roll into, we might miss the deepest one.
    Each of these little bowls is called a local minimum. We are interested in finding
    the global minimum, the deepest of all the bowls. Imagine that we are testing
    our tea temperatures on a hot day. It may be that once tea becomes cold enough,
    it makes a great iced tea, which is even more popular. We would never find that
    out by gradient descent alone.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/e64b02cacb1d9027c9f860d0dbbd3ff4.png)'
  prefs: []
  type: TYPE_IMG
- en: If the error function is not smooth, there are lots of places a marble could
    get stuck. This could happen if our tea drinkers' enjoyment was heavily impacted
    by passing trains. The periodic occurrence of trains could introduce a wiggle
    into our data.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/e19afd4221e9035b27551b131078720a.png)'
  prefs: []
  type: TYPE_IMG
- en: If the error function you are trying to optimize makes discrete jumps, that
    presents a challenge. Marbles don't roll down stairs well. This could happen if
    our tea drinkers have to rate their enjoyment on a 10-point scale.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/903ddca73d89553328fcb814329978ca.png)'
  prefs: []
  type: TYPE_IMG
- en: If the error function is mostly a plateau, but has a bottom that is narrow and
    deep, then the marble is unlikely to find it. Perhaps our tea drinkers absolutely
    despise all tea that is anything but perfect.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/49d4565f0aab828209cd4291663f7cbe.png)'
  prefs: []
  type: TYPE_IMG
- en: All of these occur in real machine learning optimization problems.
  prefs: []
  type: TYPE_NORMAL
- en: Robust methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we suspect that our tea satisfaction curve has any of these tricky characteristics,
    we can always fall back to exhaustive search. Unfortunately, exhaustive search
    takes an extremely long time for a lot of problems. Luckily for us, there is a
    middle ground. There is a set of methods that is tougher than gradient descent.
    They go by names like genetic algorithms, evolutionary algorithms, and simulated
    annealing. They take longer to compute than gradient descent, and they take more
    steps, but they don't break nearly so easily. Each has its own quirks, but one
    characteristic that most of them share is a randomness to their steps and jumps.
    This helps them discover the deepest valleys of the error function, even when
    they are difficult to find.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization algorithms that rely gradient descent are like Formula One race
    cars. They are extremely fast and efficient, but require a very well behaved track
    (error function) to work well. A poorly-placed speed bump could wreck it. The
    more robust methods are like four-wheel-drive pickup trucks. They don't go nearly
    as fast, but they can handle a lot more variability in the terrain. And exhaustive
    search is like traveling on foot. You can get absolutely anywhere, but it may
    take you *really* long time. They are each invaluable in different situations.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/3398ba44dceb2b906cae0f0f703714b1.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Original](https://brohrer.github.io/how_optimization_works_1.html). Reposted
    with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Optimization 101 for Data Scientists](/2018/08/optimization-101-data-scientists.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Optimization Using R](/2018/05/optimization-using-r.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[An Intuitive Introduction to Gradient Descent](/2018/06/intuitive-introduction-gradient-descent.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How ChatGPT Works: The Model Behind The Bot](https://www.kdnuggets.com/2023/04/chatgpt-works-model-behind-bot.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Burtch Works 2023 Data Science & AI Professionals Salary Report…](https://www.kdnuggets.com/2023/08/burtch-works-2023-data-science-ai-professionals-salary-report.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning Pipeline Optimization with TPOT](https://www.kdnuggets.com/2021/05/machine-learning-pipeline-optimization-tpot.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SQL Query Optimization Techniques](https://www.kdnuggets.com/2023/03/sql-query-optimization-techniques.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Database Optimization: Exploring Indexes in SQL](https://www.kdnuggets.com/2023/07/database-optimization-exploring-indexes-sql.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Gradient Descent: The Mountain Trekker''s Guide to Optimization with…](https://www.kdnuggets.com/gradient-descent-the-mountain-trekker-guide-to-optimization-with-mathematics)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
