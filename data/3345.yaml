- en: Learning to Learn by Gradient Descent by Gradient Descent
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过梯度下降学习学习
- en: 原文：[https://www.kdnuggets.com/2017/02/learning-learn-gradient-descent.html](https://www.kdnuggets.com/2017/02/learning-learn-gradient-descent.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/02/learning-learn-gradient-descent.html](https://www.kdnuggets.com/2017/02/learning-learn-gradient-descent.html)
- en: '[Learning to learn by gradient descent by gradient descent](https://arxiv.org/pdf/1606.04474v2.pdf),
    Andrychowicz et al., *NIPS 2016*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[通过梯度下降学习学习](https://arxiv.org/pdf/1606.04474v2.pdf)，Andrychowicz等，*NIPS 2016*'
- en: One of the things that strikes me when I read these NIPS papers is just how
    short some of them are – between the introduction and the evaluation sections
    you might find only one or two pages! A general form is to start out with a basic
    mathematical model of the problem domain, expressed in terms of functions. Selected
    functions are then *learned*, by reaching into the machine learning toolbox and
    combining existing building blocks in potentially novel ways. When looked at this
    way, we could really call machine learning ‘*function learning*‘.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读这些NIPS论文时，让我印象深刻的一点是有些论文的长度是如此之短——在介绍和评估部分之间你可能只会找到一两页！一种通用形式是从问题领域的基本数学模型开始，用函数表示。然后，选择的函数被*学习*，通过进入机器学习工具箱并以潜在的新方式组合现有的构建块。从这个角度看，我们可以真正称机器学习为‘*函数学习*’。
- en: Thinking in terms of functions like this is a bridge back to the familiar (for
    me at least). We have function composition. For example, given a function *f*
    mapping images to feature representations, and a function *g* acting as a classifier
    mapping image feature representations to objects, we can build a systems that
    classifies objects in images with *g ○ f*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 用这种函数的方式思考对我来说是一种回到熟悉领域的桥梁。我们有函数组合。例如，给定一个将图像映射到特征表示的函数*f*，以及一个作为分类器的函数*g*，将图像特征表示映射到对象，我们可以构建一个用*g
    ○ f*分类图像中的对象的系统。
- en: Each function in the system model could be learned or just implemented directly
    with some algorithm. For example, feature mappings (or encodings) were traditionally
    implemented by hand, but increasingly are learned...
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 系统模型中的每个函数可以是学习得来的，也可以直接用某些算法实现。例如，特征映射（或编码）传统上是手动实现的，但越来越多的是通过学习实现的...
- en: The move from hand-designed features to learned features in machine learning
    has been wildly successful.
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从手动设计的特征到学习得来的特征的转变在机器学习中取得了巨大成功。
- en: Part of the art seems to be to define the overall model in such a way that no
    individual function needs to do too much (avoiding too big a gap between the inputs
    and the target output) so that learning becomes more efficient / tractable, and
    we can take advantage of different techniques for each function as appropriate.
    In the above example, we composed one learned function for creating good representations,
    and another function for identifying objects from those representations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 艺术的一部分似乎在于以一种方式定义整体模型，使得单个函数不需要做太多工作（避免输入和目标输出之间差距过大），从而使学习变得更高效/可处理，并且我们可以根据需要为每个函数采用不同的技术。在上述示例中，我们组合了一个用于创建良好表示的学习函数和另一个用于从这些表示中识别对象的函数。
- en: We can have higher-order functions that combine existing (learned or otherwise)
    functions, and of course that means we can also use *combinators*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以拥有将现有（学习过的或其他）函数结合起来的高阶函数，当然这也意味着我们可以使用*组合子*。
- en: And what do we find when we look at the components of a ‘function learner’ (machine
    learning system)? More functions!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看“函数学习器”（机器学习系统）的组件时，我们发现了什么？更多的函数！
- en: Frequently, tasks in machine learning can be expressed as the problem of optimising
    an objective function *f(θ)* defined over some domain *θ ∈ Θ*.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 机器学习中的任务通常可以表达为优化一个在某些领域*θ ∈ Θ*上定义的目标函数*f(θ)*的问题。
- en: 'The *optimizer* function maps from *f θ* to *argmin[θ ∈ Θ] f θ* . The standard
    approach is to use some form of gradient descent (e.g., SGD – stochastic gradient
    descent). A classic paper in optimisation is ‘No Free Lunch Theorems for Optimization’
    which tells us that no general-purpose optimisation algorithm can dominate all
    others. So to get the best performance, we need to match our optimisation technique
    to the characteristics of the problem at hand:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*优化器*函数从*f θ*映射到*argmin[θ ∈ Θ] f θ*。标准方法是使用某种形式的梯度下降（例如，SGD——随机梯度下降）。优化领域的一篇经典论文是《优化的无免费午餐定理》，它告诉我们没有通用的优化算法可以主导所有其他算法。因此，为了获得最佳性能，我们需要将优化技术与手头问题的特点匹配：'
- en: '... specialisation to a subclass of problems is in fact the *only* way that
    improved performance can be achieved in general.'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '... 针对特定问题子类的专业化实际上是实现普遍性能提升的*唯一*方式。'
- en: Thus there has been a lot of research in defining update rules tailored to different
    classes of problems – within deep learning these include for example *momentum*,
    *Rprop*, *Adagrad*, *RMSprop*, and *ADAM*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在为不同类别的问题定义更新规则方面，已经进行了大量研究——在深度学习中，这些包括例如*momentum*、*Rprop*、*Adagrad*、*RMSprop*和*ADAM*。
- en: But what if instead of hand designing an optimising algorithm (function) we
    *learn* it instead? That way, by training on the class of problems we’re interested
    in solving, we can learn an optimum optimiser for the class!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们不是手动设计一个优化算法（函数），而是*学习*它呢？通过在我们感兴趣的问题类别上进行训练，我们可以为这一类别学习一个最佳优化器！
- en: The goal of this work is to develop a procedure for constructing a learning
    algorithm which performs well on a particular class of optimisation problems.
    Casting algorithm design as a learning problem allows us to specify the class
    of problems we are interested in through example problem instances. This is in
    contrast to the ordinary approach of characterising properties of interesting
    problems analytically and using these analytical insights to design learning algorithms
    by hand.
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这项工作的目标是开发一个构建学习算法的程序，使其在特定类别的优化问题上表现良好。将算法设计视为学习问题使我们能够通过示例问题实例来指定我们感兴趣的问题类别。这与通过分析有趣问题的性质并利用这些分析洞察手动设计学习算法的传统方法形成对比。
- en: If learned representations end up performing better than hand-designed ones,
    can learned optimisers end up performing better than hand-designed ones too? The
    answer turns out to be yes!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果学习的表示最终比手动设计的表示表现更好，那么学习型优化器是否也能比手动设计的优化器表现更好呢？答案确实是肯定的！
- en: Our experiments have confirmed that learned neural optimizers compare favorably
    against state-of-the-art optimization methods used in deep learning.
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们的实验已确认，学习型神经网络优化器与深度学习中使用的最先进优化方法相比表现优越。
- en: '![Figure](../Images/7c69ca09a51e00c66204243f71f2bb53.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7c69ca09a51e00c66204243f71f2bb53.png)'
- en: In fact not only do these learned optimisers perform very well, but they also
    provide an interesting way to transfer learning across problems sets. Traditionally
    *transfer learning* is a hard problem studied in its own right. But in this context,
    because we’re learning how to learn, straightforward *generalization* (the key
    property of ML that lets us learn on a training set and then perform well on previously
    unseen examples) provides for transfer learning!!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这些学习型优化器不仅表现非常好，还提供了一种有趣的跨问题集迁移学习方式。传统上，*迁移学习*是一个作为独立问题研究的困难领域。但在这种情况下，由于我们正在学习如何学习，直接的*泛化*（机器学习的关键特性，使我们能够在训练集上学习，然后在以前未见过的例子上表现良好）提供了迁移学习！
- en: We witnessed a remarkable degree of transfer, with for example the LSTM optimizer
    trained on 12,288 parameter neural art tasks being able to generalize to tasks
    with 49,512 parameters, different styles, and different content images all at
    the same time. We observed similar impressive results when transferring to different
    architectures in the MNIST task.
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们见证了显著的迁移能力，例如，在12,288参数神经艺术任务上训练的LSTM优化器能够在具有49,512参数、不同风格和不同内容图像的任务上进行泛化。我们在将其迁移到MNIST任务的不同架构时也观察到了类似的令人印象深刻的结果。
- en: Learning how to learn
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习如何学习
- en: 'Thinking functionally, here’s my mental model of what’s going on… In the beginning,
    you might have hand-coded a classifier function, *c*, which maps from some *Input*
    to a *Class*:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从功能性思维来看，这是我对发生情况的心理模型……起初，你可能手动编码了一个分类器函数*c*，它将某些*Input*映射到一个*Class*：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'With machine learning, we figured out for certain types of functions it’s better
    to learn an implementation than try and code it by hand. An optimisation function
    *f* takes some *TrainingData* and an existing classifier function, and returns
    an updated classifier function:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过机器学习，我们发现对于某些类型的函数，学习一个实现比手动编码更为有效。一个优化函数*f*接受一些*TrainingData*和一个现有的分类器函数，然后返回一个更新后的分类器函数：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: What we’re doing now is saying, “well, if we can learn a function, why don’t
    we learn *f* itself?”
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的做法是，既然我们可以学习一个函数，为什么不直接学习*f*本身呢？
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let *ϕ* be the (to be learned) update rule for our (optimiser) optimiser. We
    need to evaluate how effective *g* is over a number of iterations, and for this
    reason *g* is modelled using a recurrent neural network (LSTM). The state of this
    network at time *t* is represented by *h[t]*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 设*ϕ*为我们（优化器）的（待学习的）更新规则。我们需要评估在多次迭代中*g*的效果，因此*g*使用递归神经网络（LSTM）进行建模。该网络在时间*t*的状态由*h[t]*表示。
- en: Suppose we are training *g* to optimise an optimisation function *f*. Let *g(ϕ)*
    result in a learned set of parameters for *f θ*, The loss function for training
    *g(ϕ)* uses as *its* expected loss the expected loss of *f* as trained by *g(ϕ)*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在训练*g*以优化一个优化函数*f*。设*g(ϕ)*导致一个学习到的*f θ*参数集，训练*g(ϕ)*的损失函数使用*f*经过*g(ϕ)*训练后的预期损失作为*其*预期损失。
- en: We can minimise the value of *L(ϕ)* using gradient descent on *ϕ*.
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们可以通过对*ϕ*进行梯度下降来最小化*L(ϕ)*的值。
- en: '![Figure](../Images/dc6f1a42cb394731cee484c638eb1562.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/dc6f1a42cb394731cee484c638eb1562.png)'
- en: To scale to tens of thousands of parameters or more, the optimiser network *m*
    operators coordinatewise on the parameters of the objective function, similar
    to update rules like RMSProp and ADAM. The update rule for each coordinate is
    implemented using a 2-layer LSTM network using a forget-gate architecture.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩展到数万或更多的参数，优化器网络*m*在目标函数的参数上逐坐标操作，类似于RMSProp和ADAM等更新规则。每个坐标的更新规则是使用一个具有遗忘门架构的2层LSTM网络实现的。
- en: The network takes as input the optimizee gradient for a single coordinate as
    well as the previous hidden state and outputs the update for the corresponding
    optimise parameter. We refer to this architecture as an LSTM optimiser.
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 网络输入单个坐标的优化梯度以及之前的隐藏状态，并输出对应优化参数的更新。我们称这种架构为LSTM优化器。
- en: '![Figure](../Images/60dbbf12f986b7226c7a994f530dfa64.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/60dbbf12f986b7226c7a994f530dfa64.png)'
- en: Learned learners in action
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行中的学习型学习者
- en: 'We compare our trained optimizers with standard optimisers used in Deep Learning:
    SGD, RMSprop, ADAM, and Nesterov’s accelerated gradient (NAG). For each of these
    optimizers and each problem we tuned the learning rate, and report results with
    the rate that gives the best final error for each problem.'
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们将训练好的优化器与深度学习中使用的标准优化器进行比较：SGD、RMSprop、ADAM以及Nesterov加速梯度（NAG）。对于每个优化器和每个问题，我们调整了学习率，并报告了在每个问题上给出最佳最终误差的学习率结果。
- en: Optimisers were trained for 10-dimensional quadratic functions, for optimising
    a small neural network on MNIST, and on the CIFAR-10 dataset, and on learning
    optimisers for neural art (see e.g. [Texture Networks](https://blog.acolyer.org/2016/09/23/texture-networks-feed-forward-synthesis-of-textures-and-stylized-images/)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器被训练用于10维二次函数、优化MNIST上的小型神经网络、CIFAR-10数据集，以及学习用于神经艺术的优化器（例如，参见[纹理网络](https://blog.acolyer.org/2016/09/23/texture-networks-feed-forward-synthesis-of-textures-and-stylized-images/)）。
- en: 'Here’s a closer look at the performance of the trained LSTM optimiser on the
    Neural Art task vs standard optimisers:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是训练的LSTM优化器在Neural Art任务上的性能与标准优化器的对比：
- en: '![Figure](../Images/113b9f3e96768859a22125e4ac49fdb3.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/113b9f3e96768859a22125e4ac49fdb3.png)'
- en: And because they’re pretty… here are some images styled by the LSTM optimiser!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 而且因为它们非常漂亮……这里有一些由LSTM优化器风格化的图像！
- en: '![Figure](../Images/795d9f37ab45ae6f1c60d68fb066c386.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/795d9f37ab45ae6f1c60d68fb066c386.png)'
- en: A system model and learned components
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 系统模型及学习到的组件
- en: So there you have it. It seems that in the not-too-distant future, the state-of-the-art
    will involve the use of learned optimisers, just as it involves the use of learned
    feature representations today. This appears to be another crossover point where
    machines can design algorithms that outperform those of the best human designers.
    And of course, there’s something especially potent about learning learning algorithms,
    because better learning algorithms accelerate learning…
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所以就是这样。看起来在不久的将来，最先进的技术将涉及使用学习到的优化器，就像今天使用学习到的特征表示一样。这似乎是机器可以设计出比最优秀的人类设计师更优算法的另一个交叉点。当然，学习学习算法有特别强大的潜力，因为更好的学习算法可以加速学习……
- en: 'In this paper, the authors explored how to build a function *g* to optimise
    an function *f*, such that we can write:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，作者探讨了如何构建一个函数*g*来优化函数*f*，使得我们可以写出：
- en: '![f'' = g(d,f)](../Images/011c40a0178f1353ec2e4c14e0f378e1.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![f'' = g(d,f)](../Images/011c40a0178f1353ec2e4c14e0f378e1.png)'
- en: where *d* is some training data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*d*是一些训练数据。
- en: 'When expressed this way, it also begs the obvious question what if I write:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式表达时，显然会引发一个问题，如果我写：
- en: '![g'' = g(d, g)](../Images/885d6bc0d2779eff6ccd89c8a2d9097d.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![g'' = g(d, g)](../Images/885d6bc0d2779eff6ccd89c8a2d9097d.png)'
- en: 'or go one step further using the Y-combinator to find a fixed point:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 或者更进一步使用Y组合子找到一个固定点：
- en: '![g'' = Y g](../Images/7f5509479a0e372c7c4f225a5c3e35b7.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![g'' = Y g](../Images/7f5509479a0e372c7c4f225a5c3e35b7.png)'
- en: Food for thought...
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 发人深省...
- en: '**Bio: [Adrian Colyer](https://twitter.com/adriancolyer)** was CTO of SpringSource,
    then CTO for Apps at VMware and subsequently Pivotal. He is now a Venture Partner
    at Accel Partners in London, working with early stage and startup companies across
    Europe. *If you’re working on an interesting technology-related business he would
    love to hear from you: you can reach him at acolyer at accel dot com*.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介： [Adrian Colyer](https://twitter.com/adriancolyer)** 曾任SpringSource的CTO，随后是VMware和Pivotal的应用CTO。他现在是伦敦Accel
    Partners的风险合伙人，与欧洲的早期阶段和初创公司合作。*如果你正在从事有趣的技术相关业务，他很乐意听到你的消息：你可以通过acolyer at accel
    dot com联系他*。'
- en: '[Original](https://blog.acolyer.org/2017/01/04/learning-to-learn-by-gradient-descent-by-gradient-descent/).
    Reposted with permission.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://blog.acolyer.org/2017/01/04/learning-to-learn-by-gradient-descent-by-gradient-descent/)。转载许可。'
- en: '**Related:**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[A Concise Overview of Standard Model-fitting Methods](/2016/05/concise-overview-model-fitting-methods.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[标准模型拟合方法的简明概述](/2016/05/concise-overview-model-fitting-methods.html)'
- en: '[Deep Learning in Neural Networks: An Overview](/2016/04/deep-learning-neural-networks-overview.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[神经网络中的深度学习：概述](/2016/04/deep-learning-neural-networks-overview.html)'
- en: '[Artificial Intelligence and Life in 2030](/2016/12/artificial-intelligence-life-2030.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能与2030年的生活](/2016/12/artificial-intelligence-life-2030.html)'
- en: '* * *'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT工作'
- en: '* * *'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[5 Concepts You Should Know About Gradient Descent and Cost Function](https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你应该知道的5个梯度下降和成本函数的概念](https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html)'
- en: '[Back To Basics, Part Dos: Gradient Descent](https://www.kdnuggets.com/2023/03/back-basics-part-dos-gradient-descent.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[回归基础，第二部分：梯度下降](https://www.kdnuggets.com/2023/03/back-basics-part-dos-gradient-descent.html)'
- en: '[Gradient Descent: The Mountain Trekker''s Guide to Optimization with…](https://www.kdnuggets.com/gradient-descent-the-mountain-trekker-guide-to-optimization-with-mathematics)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[梯度下降：优化的山地徒步指南](https://www.kdnuggets.com/gradient-descent-the-mountain-trekker-guide-to-optimization-with-mathematics)'
- en: '[Vanishing Gradient Problem: Causes, Consequences, and Solutions](https://www.kdnuggets.com/2022/02/vanishing-gradient-problem.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[梯度消失问题：原因、后果和解决方案](https://www.kdnuggets.com/2022/02/vanishing-gradient-problem.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，并寻找目标来...](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Learn Deep Learning by Building 15 Neural Network Projects in 2022](https://www.kdnuggets.com/2022/01/15-neural-network-projects-build-2022.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过构建15个神经网络项目学习深度学习](https://www.kdnuggets.com/2022/01/15-neural-network-projects-build-2022.html)'
