- en: Neural Networks – an Intuition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/02/neural-networks-intuition.html](https://www.kdnuggets.com/2019/02/neural-networks-intuition.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By [Prateek Karkare](https://www.linkedin.com/in/prateek-karkare-09622a27/),
    Research Associate at Nanyang Technological University** .'
  prefs: []
  type: TYPE_NORMAL
- en: Humans have always been fascinated by nature. The flight of birds led us to
    invent airplanes; shark skin inspired us to make faster swimsuits and numerous
    other machines which draw inspiration from nature. Today we are in the era of
    building intelligent machines, and there is no better inspiration than the brain.
    Humans are particularly blessed by evolution with a brain capable of performing
    the most complex tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Since early 1940 scientists have been trying to build mathematical models and
    algorithms which mimic computations as they are performed inside the brain. Of
    course we are still far away in achieving the kind of feat brain does, but we
    are getting there.
  prefs: []
  type: TYPE_NORMAL
- en: Several simplified learning models have been proposed in the quest of making
    intelligent machines and the most popular among them is the Artificial Neural
    Network or ANN or simply a Neural Network. Neural networks are one of the most
    powerful algorithms used in the field of machine learning and artificial intelligence
    nowadays. As the name suggests it draws inspiration from neurons in our brain
    and the way they are connected. Let us take a quick peek inside our brain.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0247a766b16057ef1b0eb0ff63510eaa.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Source: [http://biomedicalengineering.yolasite.com/neurons.php](http://biomedicalengineering.yolasite.com/neurons.php)**'
  prefs: []
  type: TYPE_NORMAL
- en: Neurons are connected inside our brain as depicted in the picture above. This
    picture shows only 2 neurons connected to each other. In reality, thousands of
    neurons connect to a single neuron’s cell body through dendrites, on an average
    a single neuron connects to 10,000 other neurons. Let us simplify this picture
    to make an artificial neural network model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89d57406c6260452f9f2e839d44d2f79.png)'
  prefs: []
  type: TYPE_IMG
- en: '**A multi-layer neural network**'
  prefs: []
  type: TYPE_NORMAL
- en: For now, assume that the cell body will just hold a number and the connection
    arrow describes the direction of data flow and how **strongly **each neuron is
    connected to other neurons. Don’t worry about what *strongly *means here we will
    see that in a while. We have taken some inspiration from biology about neurons
    and their connectivity. Let’s see how this helps in doing some tasks which our
    brain does.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b94228474fe6e9990397ae18dfd2b88e.png)'
  prefs: []
  type: TYPE_IMG
- en: Perceptron
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Source: [https://www.javascripttuts.com](https://www.javascripttuts.com/)**'
  prefs: []
  type: TYPE_NORMAL
- en: Well, that’s Megatron.
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron is a machine learning algorithm invented by Frank Rosenblatt in 1957\.
    Perceptron is a linear classifier, you can read about what linear classifier is
    and a classification algorithm [**here**](https://medium.com/x8-the-ai-community/practical-aspects-logistic-regression-in-layman-terms-73fbcae58625).
    Let’s look at a very small classification example. Let us try to build a machine
    which identifies whether an object is a cricket ball or not.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1296ee1865e1e53b40b9eec5af027b97.png)'
  prefs: []
  type: TYPE_IMG
- en: Let us arbitrarily choose some properties of this ball.
  prefs: []
  type: TYPE_NORMAL
- en: It is Red, we will call this property **R **of the ball
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is spherical, we will call this property **S **of the ball
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/8eb774bb4d2dbef7a985d092d5e6aac8.png)'
  prefs: []
  type: TYPE_IMG
- en: Depending on the arbitrarily chosen properties, also known as **features, **we
    will classify the object as a cricket ball or not. The above table tells us that
    if a ball is red and spherical, it is a cricket ball, in all other cases, it is
    not. Let’s see how to do this with a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: To build our super tiny *brain *which can identify a cricket ball we will take
    a neuron which just adds up the inputs and outputs the sum —
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4eb9aa4c85f3f466a2c21513a7e3cc9d.png)'
  prefs: []
  type: TYPE_IMG
- en: Breaking down the above figure; *b, R* *and* *S* are input neurons or simply
    the inputs to the network, *w0, w1 and w2* are the **strengths** of connections
    to the middle neuron which sums up the inputs to it.* b *here is a constant which
    is called *bias. *The last step which is the rightmost neuron is just a function
    called **activation function **which outputs 1 if the input to it is positive
    and 0 if the input it negative. Mathematically it looks like —
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08128755263e1243ae4cc440a382dcfa.png)'
  prefs: []
  type: TYPE_IMG
- en: if this SUM > 0, Output = 1 or *Yes* and SUM < 0, Output = 0 or *No*
  prefs: []
  type: TYPE_NORMAL
- en: Let us look at how this helps in classifying our cricket ball. We choose some
    arbitrary numbers for our connection strengths *w *and our constant *b. *How do
    we arrive at those values which is a part of *learning* those weights by *training *the
    neural network is a topic for part-2 of this series. For now let’s just assume
    we got these numbers from somewhere —
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d0a71cf9cd3127fc46453fa01cab4c9a.png)'
  prefs: []
  type: TYPE_IMG
- en: Let us fit these values
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 1: When the object is neither a sphere (S=0) nor red (R=0)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e2ba6d91e20e41d94aaae16120d2ad61.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](../Images/7dea1db2ba4205e17f8921c8b2aaf7aa.png)'
  prefs: []
  type: TYPE_IMG
- en: The SUM<0 which means the output is 0 or *No*. Our perceptron says that this
    is not a cricket ball.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 2: When the object is a sphere (S=1) but not red (R=0)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ceb6acc3227e36bf2fb07bb238c73fe.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](../Images/61e6b2e2cdae448b969a8c6e81cb7e38.png)'
  prefs: []
  type: TYPE_IMG
- en: The SUM<0 which means the output is 0 or No, not a cricket ball
  prefs: []
  type: TYPE_NORMAL
- en: Case:3 When the object is not a sphere (S=0) but red (R=1)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e717f7c2ef77ea3d1a5701c90e2a0a0.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](../Images/f37b7603bc149abbdb293594b66ef49a.png)'
  prefs: []
  type: TYPE_IMG
- en: The SUM<0 which means the output is 0 or No, not a cricket ball
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 4: When the object is a sphere (S=1) and red (R=1)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec712947e9c5cc86f45c86df4d85522f.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](../Images/f8265630f5546c4600369832ee104d26.png)'
  prefs: []
  type: TYPE_IMG
- en: The SUM>0 which means the output is 1\. Et Voila! Our perceptron says it is
    a cricket ball.
  prefs: []
  type: TYPE_NORMAL
- en: This is the most rudimentary idea behind a neural network. We connect lot of
    these perceptrons in a particular manner and what we get is a neural network.
    I’ve oversimplified the idea a little bit but it still captures the essence of
    a perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-layer Perceptron
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We take this idea of a perceptron and stack them together to create *layers *of
    these neurons which is called a **Multi-layer perceptron** (MLP) or a **Neural
    Network**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56f4157954ee99028c3020fb72a65441.png)'
  prefs: []
  type: TYPE_IMG
- en: In our simplified perceptron model we were just using a step function for the
    output. In practice lot of different transformations or **activation functions** are
    used.
  prefs: []
  type: TYPE_NORMAL
- en: We chose our features, red color and spherical shape manually and rather arbitrarily
    but it is not always practical to choose such features for many other complex
    tasks. The MLP addresses this problem to some extent. The inputs to MLP could
    simply be just the pixel values of an image, the hidden layer then combines and
    transforms these pixel values to create features in the hidden layer. Essentially
    a single layer of perceptrons transforms the data and sends it to the following
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/15ef716f5fc6f70da4060dbd37a06b5b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**A 28 pixel x 28 pixel image of a handwritten digit**'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose our MLP is trying to identify a digit by looking at images of handwritten
    digits similar to the one above. With a MLP we can pass this image as their pixel
    values directly without extracting any features out of it to the input layer.
    The hidden layer then *combines and transforms *these pixels to create some features.
    Each neuron in the hidden layer tries to *learn* some features as a part of the
    training process. For instance neuron number 1 in the hidden layer might learn
    to respond to horizontal edges in the image, neuron 2 might respond to a vertical
    edge in the image and so on. Now if the input image has a horizontal edge and
    a vertical edge together, neuron 1 and 2 in the hidden layer would respond and
    the output neuron will now combine these two features and might say it is a 7
    since 7 roughly has a vertical edge and a horizontal edge.
  prefs: []
  type: TYPE_NORMAL
- en: Of course the MLPs don’t always learn features which necessarily make sense
    but it essentially is a transformation of the input data. For example for a MLP
    which takes in the handwritten digit image as input having 16 neurons in hidden
    layer learns the features something like —
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ebc0b7ddd92292edec553e3fee9fa2b6.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Each square represents the learned features by a neuron in the hidden layer**'
  prefs: []
  type: TYPE_NORMAL
- en: As you can clearly see the patterns are more or less random and don’t make sense.
    This inference process and transformations of MLP is best explained in this awesome
    YouTube series by 3Blue1Brown — [**What is a Neural Network**](https://www.youtube.com/watch?v=aircAruvnKk).
    I would highly encourage you to go to this channel and watch the videos to get
    an intuition about neural networks. A nice tool to visualize some of the things
    which a neural network does is [**Tensorflow Playground**](https://playground.tensorflow.org/)and
    if you have some background in Linear Algebra the visualizations in this [blog](http://colah.github.io/) are
    nice.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Networks Today and Tomorrow
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: So what tasks a neural network can perform apart from identifying a cricket
    ball or a handwritten digit? Today neural networks are deployed for a wide range
    of tasks. **Image Recognition, Voice Recognition, Natural Language Processing, **[**Time
    series data**](https://medium.com/x8-the-ai-community/time-series-what-is-all-the-hype-about-e1ffd8957f1a)and
    many many other applications. Image classification is one particular field where
    neural networks have become the de facto algorithm. Neural networks have surpassed
    humans in identifying images accurately. Of course there are lot of sophisticated
    techniques and math to build such high fidelity neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d7aa48fca3223bc1b88a6819843cc05.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Source: xkcd.com**'
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks are everywhere. Companies deploy them to give you recommendations
    about which video you might like to watch on YouTube, identify your voice and
    commands when you speak to Siri or Google Now or Alexa. Neural networks are now
    generating their own paintings and music pieces. Still, there are a lot of challenges.
    Neural networks can be huge with hundreds of layers with hundreds of neurons in
    each layer which makes computing a big challenge with the current hardware technology.
    Training neural networks is another huge challenge. Several GPUs are employed
    together to train these neural networks which take several hours to days.
  prefs: []
  type: TYPE_NORMAL
- en: But they are the present and the future in our quest for building machines with
    intelligence and capabilities as sophisticated as the human brain. We are still
    far away from making an artificial brain which can someday “download” all of your
    consciousness creating a machine copy of yourself. But for now that is just science
    fiction!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cd416cc6dbdd7efc86b66a48d356b54a.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Source: xkcd.com**'
  prefs: []
  type: TYPE_NORMAL
- en: Neural network is a vast subject and the idea behind this article is to elicit
    enough curiosity to give you a small impetus to go and explore the topic. So go
    and click those YouTube and TensorFlow links in the article and have fun learning!.
  prefs: []
  type: TYPE_NORMAL
- en: Look out for the part-2 of this series on How to train your D̶r̶a̶g̶o̶n NN.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio**: [Prateek Karkare](https://www.linkedin.com/in/prateek-karkare-09622a27/) is
    an Electrical engineer in training with a keen interest in Physics, Computer Sciences
    and Biology.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/datadriveninvestor/neural-networks-an-intuition-640821d5bd83).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resources:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning
    education](https://www.kdnuggets.com/education/online.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Software for Analytics, Data Science, Data Mining, and Machine Learning](https://www.kdnuggets.com/software/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[NLP Overview: Modern Deep Learning Techniques Applied to Natural Language
    Processing](https://www.kdnuggets.com/2019/01/nlp-overview-modern-deep-learning-techniques.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Backpropagation Algorithm Demystified](https://www.kdnuggets.com/2019/01/backpropagation-algorithm-demystified.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Supervised Learning: Model Popularity from Past to Present](https://www.kdnuggets.com/2018/12/supervised-learning-model-popularity-from-past-present.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Stable Diffusion: Basic Intuition Behind Generative AI](https://www.kdnuggets.com/2023/06/stable-diffusion-basic-intuition-behind-generative-ai.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Neural Networks Don''t Lead Us Towards AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10 Simple Things to Try Before Neural Networks](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Explainable Forecasting and Nowcasting with State-of-the-art Deep…](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Image Classification with Convolutional Neural Networks (CNNs)](https://www.kdnuggets.com/2022/05/image-classification-convolutional-neural-networks-cnns.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
