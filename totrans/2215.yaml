- en: 'Understanding Classification Metrics: Your Guide to Assessing Model Accuracy'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/understanding-classification-metrics-your-guide-to-assessing-model-accuracy](https://www.kdnuggets.com/understanding-classification-metrics-your-guide-to-assessing-model-accuracy)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Understanding Classification Metrics: Your Guide to Assessing Model Accuracy](../Images/d8c51c838f49efba326a254b57e7ef70.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation metrics are like the measuring tools we use to understand how well
    a machine learning model is doing its job. They help us compare different models
    and figure out which one works best for a particular task. In the world of classification
    problems, there are some commonly used metrics to see how good a model is, and
    it's essential to know which metric is right for our specific problem. When we
    grasp the details of each metric, it becomes easier to decide which one matches
    the needs of our task.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will explore the basic evaluation metrics used in classification
    tasks and examine situations where one metric might be more relevant than others.
  prefs: []
  type: TYPE_NORMAL
- en: Basic Terminology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we dive deep into evaluation metrics, it is critical to understand the
    basic terminology associated with a classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: '**Ground Truth Labels:** These refer to the actual labels corresponding to
    each example in our dataset. These are the basis of all evaluation and predictions
    are compared to these values.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Predicted Labels:** These are the class labels predicted using the machine
    learning model for each example in our dataset. We compare such predictions to
    the ground truth labels using various evaluation metrics to calculate if the model
    could learn the representations in our data.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let us only consider a binary classification problem for an easier understanding.
    With only two different classes in our dataset, comparing ground truth labels
    with predicted labels can result in one of the following four outcomes, as illustrated
    in the diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Classification Metrics: Your Guide to Assessing Model Accuracy](../Images/719be79e30179bd695d06f5b2f699682.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: *Using 1 to denote a positive label and 0 for a negative label,
    the predictions can fall into one of the four categories.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positives:** The model predicts a positive class label when the ground
    truth is also positive. This is the required behaviour as the model can successfully
    predict a positive label.'
  prefs: []
  type: TYPE_NORMAL
- en: '**False Positives:** The model predicts a positive class label when the ground
    truth label is negative. The model falsely identifies a data sample as positive.'
  prefs: []
  type: TYPE_NORMAL
- en: '**False Negatives:** The model predicts a negative class label for a positive
    example. The model falsely identifies a data sample as negative.'
  prefs: []
  type: TYPE_NORMAL
- en: '**True Negatives:** The required behavior as well. The model correctly identifies
    a negative sample, predicting 0 for a data sample having a ground truth label
    of 0.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can build upon these terms to understand how common evaluation metrics
    work.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the most simple yet intuitive way of assessing a model’s performance
    for classification problems. **It measures the proportion of total labels that
    the model correctly predicted. **
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, accuracy can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Classification Metrics: Your Guide to Assessing Model Accuracy](../Images/40587aa0a1b7bb350a1f793b70ad898a.png)'
  prefs: []
  type: TYPE_IMG
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Classification Metrics: Your Guide to Assessing Model Accuracy](../Images/9170ae8f55a5c3e6c98532d10cff1c2e.png)'
  prefs: []
  type: TYPE_IMG
- en: When to Use
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Initial Model Assessment**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given its simplicity, accuracy is a widely used metric. It provides a good starting
    point for verifying if the model can learn well before we use metrics specific
    to our problem domain.
  prefs: []
  type: TYPE_NORMAL
- en: '**Balanced Datasets**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy is only suitable for balanced datasets where all class labels are in
    similar proportions. If that is not the case, and one class label significantly
    outnumbers the others, the model may still achieve high accuracy by always predicting
    the majority class. The accuracy metric equally penalizes the wrong predictions
    for each class, making it unsuitable for imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '***   **When Misclassification costs are equal**'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy is suitable for cases where False Positives or False Negatives are
    equally bad. For example, for a sentiment analysis problem, it is equally bad
    if we classify a negative text as positive or a positive text as negative. For
    such scenarios, accuracy is a good metric.
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Precision focuses on ensuring we get all positive predictions correct. **It
    measures what fraction of the positive predictions were actually positive**.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, it is represented as
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Classification Metrics: Your Guide to Assessing Model Accuracy](../Images/4c4490aeaa7a1666c6fe8ddd63c9a7bd.png)'
  prefs: []
  type: TYPE_IMG
- en: When to Use
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**High Cost of False Positives**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider a scenario where we are training a model to detect cancer. It will
    be more important for us that we do not misclassify a patient who does not have
    cancer i.e. False Positive. We want to be confident when we make a positive prediction
    as wrongly classifying a person as cancer-positive can lead to unnecessary stress
    and expenses. Therefore, we highly value that we predict a positive label only
    when the actual label is positive.
  prefs: []
  type: TYPE_NORMAL
- en: '**Quality over Quantity**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider another scenario where we are building a search engine matching user
    queries to a dataset. In such cases, we value that the search results match closely
    to the user query. We do not want to return any document irrelevant to the user,
    i.e. False Positive. Therefore, we only predict positive for documents that match
    closely to the user query. We value quality over quantity as we prefer a small
    number of closely related results instead of a high number of results that may
    or may not be relevant for the user. For such scenarios, we want high precision.
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall, also known as Sensitivity, measures how well a model can remember the
    positive labels in the dataset. **It measures what fraction of the positive labels
    in our dataset the model predicts as positive.**
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Classification Metrics: Your Guide to Assessing Model Accuracy](../Images/6774d1487ecbaa6f323fe54b59b11268.png)'
  prefs: []
  type: TYPE_IMG
- en: A higher recall means the model is better at remembering what data samples have
    positive labels.
  prefs: []
  type: TYPE_NORMAL
- en: When to Use
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**High Cost of False Negatives**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use Recall when missing a positive label can have severe consequences. Consider
    a scenario where we are using a Machine Learning model to detect credit card fraud.
    In such cases, early detection of issues is essential. We do not want to miss
    a fraudulent transaction as it can increase losses. Hence, we value Recall over
    Precision, where misclassification of a transaction as deceitful may be easy to
    verify and we can afford a few false positives over false negatives.
  prefs: []
  type: TYPE_NORMAL
- en: F1-Score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is the harmonic mean of Precision and Recall. It penalizes models that have
    a significant imbalance between either metric.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Classification Metrics: Your Guide to Assessing Model Accuracy](../Images/4de1a23f859f171be8687854d07e0e45.png)'
  prefs: []
  type: TYPE_IMG
- en: It is widely used in scenarios where both precision and recall are important
    and allows for achieving a balance between both.
  prefs: []
  type: TYPE_NORMAL
- en: When to Use
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Imbalanced Datasets**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike accuracy, the F1-Score is suitable for assessing imbalanced datasets
    as we are evaluating performance based on the model’s ability to recall the minority
    class while maintaining a high precision overall.
  prefs: []
  type: TYPE_NORMAL
- en: '***   **Precision-Recall Trade-off**'
  prefs: []
  type: TYPE_NORMAL
- en: Both metrics are opposite to each other. Empirically, improving one can often
    lead to degradation in the other. F1-Score aids in balancing both metrics and
    is useful in scenarios where both Recall and Precision are equally critical. Taking
    both metrics into account for calculation, the F1-Score is a widely used metric
    for evaluating classification models.
  prefs: []
  type: TYPE_NORMAL
- en: Key Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've learned that different evaluation metrics have specific jobs. Knowing
    these metrics helps us choose the right one for our task. In real life, it's not
    just about having good models; it's about having models that fit our business
    needs perfectly. So, picking the right metric is like choosing the right tool
    to make sure our model does well where it matters most.
  prefs: []
  type: TYPE_NORMAL
- en: Still confused about which metric to use? Starting with accuracy is a good initial
    step. It provides a basic understanding of your model's performance. From there,
    you can tailor your evaluation based on your specific requirements. Alternatively,
    consider the F1-Score, which serves as a versatile metric, striking a balance
    between precision and recall, making it suitable for various scenarios. It can
    be your go-to tool for comprehensive classification evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '**[](https://www.linkedin.com/in/muhammad-arham-a5b1b1237/)**[Muhammad Arham](https://www.linkedin.com/in/muhammad-arham-a5b1b1237/)****
    is a Deep Learning Engineer working in Computer Vision and Natural Language Processing.
    He has worked on the deployment and optimizations of several generative AI applications
    that reached the global top charts at Vyro.AI. He is interested in building and
    optimizing machine learning models for intelligent systems and believes in continual
    improvement.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Classification Metrics Walkthrough: Logistic Regression with…](https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Key Issues Associated with Classification Accuracy](https://www.kdnuggets.com/2023/03/key-issues-associated-classification-accuracy.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[More Performance Evaluation Metrics for Classification Problems You…](https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Sky''s the Limit: Learn how JetBlue uses Monte Carlo and Snowflake…](https://www.kdnuggets.com/2022/12/monte-carlo-jetblue-snowflake-build-trust-improve-model-accuracy.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Beyond Accuracy: Evaluating & Improving a Model with the NLP Test Library](https://www.kdnuggets.com/2023/04/john-snow-beyond-accuracy-nlp-test-library.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Guide to Train an Image Classification Model Using Tensorflow](https://www.kdnuggets.com/2022/12/guide-train-image-classification-model-tensorflow.html)****'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
