- en: Recycling Deep Learning Models with Transfer Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用迁移学习回收深度学习模型
- en: 原文：[https://www.kdnuggets.com/2015/08/recycling-deep-learning-representations-transfer-ml.html](https://www.kdnuggets.com/2015/08/recycling-deep-learning-representations-transfer-ml.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2015/08/recycling-deep-learning-representations-transfer-ml.html](https://www.kdnuggets.com/2015/08/recycling-deep-learning-representations-transfer-ml.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: Deep learning models are indisputably the state of the art for many problems
    in machine perception. Using neural networks with many hidden layers of artificial
    neurons and millions of parameters, deep learning algorithms exploit both hardware
    capabilities and the abundance of gigantic datasets. In comparison, [linear models
    saturate quickly and under-fit.](/2015/03/more-training-data-or-complex-models.html)
    But what if you don't have big data?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型无疑是许多机器感知问题的最先进技术。使用具有多层隐藏层的神经网络和数百万个参数，深度学习算法利用了硬件能力和海量数据集的丰富性。相比之下，[线性模型很快会饱和并且表现不佳。](/2015/03/more-training-data-or-complex-models.html)
    但如果你没有大数据呢？
- en: '[![alex-net](../Images/cea8af8f7a15a3b311bf1c9aced99a13.png)](/wp-content/uploads/alex-net.jpeg)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[![alex-net](../Images/cea8af8f7a15a3b311bf1c9aced99a13.png)](/wp-content/uploads/alex-net.jpeg)'
- en: Say you have a novel research problem, such as identifying cancerous moles given
    photographs. Assume further that you generated a dataset of 10,000 labeled images.
    While this might seem like big data, it's a pittance by comparison to the authoritative
    datasets on which deep learning has its greatest successes. It might seem that
    all is lost. Fortunately there's hope.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个新颖的研究问题，例如根据照片识别癌症痣。进一步假设你生成了一个包含10,000张标记图像的数据集。虽然这看起来像是大数据，但与深度学习取得最大成功的权威数据集相比，这只是微不足道的。似乎一切都已经失去。幸运的是，还有希望。
- en: Consider the famous ImageNet database created by Stanford Professor [Fei-Fei
    Li](http://vision.stanford.edu/feifeili/). The dataset contains millions of images,
    each belonging to 1 of 1000 distinct hierarchically organized object categories.
    As [Dr. Li relates in her TED talk](/2015/03/more-training-data-or-complex-models.html),
    a child sees millions of distinct images while growing up. Intuitively, to train
    a model strong enough to compete with human object recognition capabilities, a
    similarly large training set might be required. Further, the methods which dominate
    at this scale of data might not be those which dominate on the same task given
    smaller datasets. Dr. Li's insight quickly paid off. Alex Krizhevsky, Ilya Sutskever
    and Geoffrey Hinton handily won the 2012 ImageNet competition, establishing convolutional
    neural networks (CNNs) as the state of the art in computer vision.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑斯坦福大学教授[李飞飞](http://vision.stanford.edu/feifeili/)创建的著名ImageNet数据库。该数据集包含数百万张图像，每张图像属于1000个不同的层次化组织的物体类别中的一个。正如[李博士在她的TED演讲中提到的](/2015/03/more-training-data-or-complex-models.html)，一个孩子在成长过程中会看到数百万张不同的图像。从直观上看，为了训练一个足够强大的模型，以便与人类物体识别能力竞争，可能需要一个类似规模的大型训练集。此外，在这种数据规模下占主导地位的方法，可能与在较小数据集上相同任务时的主导方法不同。李博士的洞察力迅速得到了回报。亚历克斯·克里日夫斯基、伊利亚·苏茨克维和杰弗里·辛顿轻松赢得了2012年ImageNet比赛，确立了卷积神经网络（CNN）在计算机视觉中的最先进地位。
- en: '[![imagenet_logo](../Images/cf46dc9cb368c60807a3f825a02b3827.png)](/wp-content/uploads/imagenet_logo.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[![imagenet_logo](../Images/cf46dc9cb368c60807a3f825a02b3827.png)](/wp-content/uploads/imagenet_logo.png)'
- en: To extend Dr. Li's metaphor, although a child sees millions of images throughout
    development, once grown, a human can easily learn to recognize a new class of
    object, without learning to see from scratch. Now consider a CNN trained for object
    recognition on the ImageNet dataset. Given an image from any among the 1000 ImageNet
    categories, the top-most hidden layer of the neural network constitutes a single
    vector representation sufficiently flexible to be useful for classifying images
    into each of the 1000 categories. It seems reasonable to guess that this representation
    would also be useful for an additional as-of-yet unseen object category.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展李博士的隐喻，虽然一个孩子在成长过程中会看到数百万张图像，但一旦长大，人类可以轻松学习识别一个新的物体类别，而无需从头学习如何看图像。现在考虑一个在ImageNet数据集上训练的CNN。给定来自1000个ImageNet类别中的任何一个类别的图像，神经网络的最上层隐藏层构成了一个足够灵活的单一向量表示，可以用于将图像分类到这1000个类别中的每一个。合理的推测是，这种表示也可能对一个尚未见过的额外物体类别有用。
- en: Following this line of thinking, computer vision researchers now commonly use
    pre-trained CNNs to generate representations for novel tasks, where the dataset
    may not be large enough to train an entire CNN from scratch. Another common tactic
    is to take the pre-trained ImageNet network and then to fine-tune the entire network
    to the novel task. This is typically done by training end-to-end with backpropagation
    and stochastic gradient descent. I first learned about this fine-tuning technique
    last year in conversations with [Oscar Beijbom](http://vision.ucsd.edu/~beijbom/website/).
    Beijbom, a vision researcher at UCSD, has successfully used this technique to
    differentiate pictures of various types of coral.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这种思路，计算机视觉研究人员现在通常使用预训练的卷积神经网络（CNN）为新任务生成表示，其中数据集可能不足以从头训练整个CNN。另一种常见的策略是采用预训练的ImageNet网络，然后对整个网络进行微调以适应新任务。这通常通过端到端的反向传播和随机梯度下降来完成。我去年在与[Oscar
    Beijbom](http://vision.ucsd.edu/~beijbom/website/)的对话中首次了解了这种微调技术。Beijbom是UCSD的视觉研究员，他成功地使用这一技术来区分各种类型的珊瑚图像。
- en: '[A terrific paper from last year''s NIPs conference](http://arxiv.org/abs/1411.1792)
    by Jason Yosinski of Cornell, in collaboration with Jeff Clune, Yoshua Bengio
    and Hod Lipson, tackles this issue with a rigorous empirical investigation. The
    authors focus on the ImageNet dataset and the 8-layer AlexNet architecture of
    ImageNet fame. They note that the lowest layers of convolutional neural networks
    have long been known to resemble conventional computer vision features like edge
    detectors and Gabor filters. Further, they note that the topmost hidden layer
    is somewhat specialized to the task it was trained for. Therefore they systematically
    explore the following questions:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[去年NIPs会议上的一篇出色论文](http://arxiv.org/abs/1411.1792)，由康奈尔大学的Jason Yosinski与Jeff
    Clune、Yoshua Bengio和Hod Lipson合作完成，针对这个问题进行了严谨的实证调查。作者重点关注ImageNet数据集以及闻名于ImageNet的8层AlexNet架构。他们指出，卷积神经网络的最底层长期以来已被知道类似于传统的计算机视觉特征，如边缘检测器和Gabor滤波器。此外，他们指出，最上层隐藏层在某种程度上专门化于其训练任务。因此，他们系统地探讨了以下问题：'
- en: Where does the transition from broadly useful to highly specialized features
    take place?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从广泛有用到高度专业化特征的过渡发生在哪里？
- en: What is the relative performance of fine-tuning an entire network vs. freezing
    the transferred layers?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调整个网络与冻结转移层的相对性能如何？
- en: To study these questions, the authors restrict their attention to the ImageNet
    dataset, but consider two 50-50 splits of the data into subsets A and B. In the
    first set of experiments, they split the data by randomly assigning each image
    category to either subset A or subset B. In the second set of experiments, the
    authors consider a split with greater contrast, assigning to one set only natural
    images and to the other only images of man-made objects.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究这些问题，作者将注意力限制在ImageNet数据集上，但考虑了将数据划分为子集A和B的两种50-50分割。在第一组实验中，他们通过随机将每个图像类别分配到子集A或子集B来分割数据。在第二组实验中，作者考虑了更具对比性的分割，将自然图像分配到一个集合，而将人造物体图像分配到另一个集合。
- en: The authors examine the case in which the network is pre-trained on task A and
    then trained further on B, keeping the first k layers and randomly initializing.
    They also consider the same experiment but pre-training on task B, randomly initializing
    the top 8-k layers, and then continuing to train on the same task B.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作者研究了一个案例，其中网络在任务A上进行预训练，然后在任务B上进一步训练，保留前k层并随机初始化。他们还考虑了相同的实验，但在任务B上进行预训练，随机初始化顶部的8-k层，然后继续在任务B上训练。
- en: In all cases, they find that fine-tuning end-to-end on a pre-trained network
    outperforms freezing the transferred k layers completely. Interestingly they also
    find that when pre-trained layers are frozen, the transferability of features
    is nonlinear in the number of pre-trained layers. They hypothesize that this owes
    to complex interactions between the nodes in adjacent layers which are difficult
    to relearn.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有情况下，他们发现对预训练网络进行端到端的微调优于完全冻结转移的k层。有趣的是，他们还发现当预训练层被冻结时，特征的迁移能力在预训练层的数量上是非线性的。他们假设这归因于相邻层之间节点的复杂相互作用，这些作用难以重新学习。
- en: To wax poetic, this might be analogous to performing a hemispherectomy on a
    fully grown human, replacing the right half of the brain with a blank slate child's
    brain. Ignoring the biological absurdity of this example it might seem intuitive
    that a fresh right brain hemisphere would have trouble filling the expected role
    of one to which the left hemisphere was fully adapted.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '-   如果要浪漫地说，这可能类似于对一个完全成长的成年人进行半球切除术，将右半脑替换为一块空白的儿童大脑。忽略这个例子的生物学荒谬性，可能直观地认为，新的右脑半球在填补左半球已经完全适应的预期角色时会遇到困难。'
- en: '[![hemispherectomy](../Images/6756492ffa3a95fef566cc13f22a5e15.png)](/wp-content/uploads/hemispherectomy.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[![半球切除术](../Images/6756492ffa3a95fef566cc13f22a5e15.png)](/wp-content/uploads/hemispherectomy.jpg)'
- en: I defer to [the original paper](http://arxiv.org/abs/1411.1792) for a complete
    description of the experiments. One question which is not addressed in this paper
    but could inspire future research, is how these results hold up when the new task
    has far fewer examples than the original task. In practice, this imbalance in
    the number of labeled examples between the original task and the new one, is often
    what motivates transfer learning. A preliminary approach could be to repeat this
    exact work but with a 999 to 1 split instead of a 500-500 split of the categories.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '-   我参考了[原始论文](http://arxiv.org/abs/1411.1792)以获得实验的完整描述。本文未涉及的一个问题，但可能会激发未来的研究，是当新任务的样本数量远少于原任务时，这些结果是否仍然有效。在实践中，原任务和新任务之间标签样本数量的不平衡，通常是激励迁移学习的原因。一个初步的方法可能是重复这项工作，但将类别的分割比例改为999比1，而不是500比500。'
- en: Generally, data labeling is expensive. For many tasks it may even be prohibitively
    expensive. Thus it's of economic and academic interest to develop a deeper understanding
    of how we could leverage the labels we do have to perform well on tasks not as
    well endowed. Yosinski's work is a great step in this direction.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '-   通常，数据标注是昂贵的。对于许多任务而言，可能甚至是不可承受的。因此，经济和学术界都对如何利用现有标签在任务中表现良好而产生深刻理解充满兴趣。Yosinski
    的工作在这方面迈出了重要的一步。'
- en: '![Zachary Chase Lipton](../Images/240b273c667af1a53a99fd93d1fd39ce.png) **[Zachary
    Chase Lipton](http://zacklipton.com)** is a PhD student in the Computer Science
    Engineering department at the University of California, San Diego. Funded by the
    [Division of Biomedical Informatics](http://healthsciences.ucsd.edu/som/medicine/divisions/dbmi/pages/default.aspx),
    he is interested in both theoretical foundations and applications of machine learning.
    In addition to his work at UCSD, he has interned at Microsoft Research Labs.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![扎卡里·蔡斯·利普顿](../Images/240b273c667af1a53a99fd93d1fd39ce.png) **[扎卡里·蔡斯·利普顿](http://zacklipton.com)**
    是加州大学圣地亚哥分校计算机科学工程系的博士生。在[生物医学信息学部](http://healthsciences.ucsd.edu/som/medicine/divisions/dbmi/pages/default.aspx)资助下，他对机器学习的理论基础和应用都很感兴趣。除了在UCSD的工作，他还曾在微软研究院实习。'
- en: '**Related:**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Deep Learning and the Triumph of Empiricism](/2015/07/deep-learning-triumph-empiricism-over-theoretical-mathematical-guarantees.html)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习与经验主义的胜利](/2015/07/deep-learning-triumph-empiricism-over-theoretical-mathematical-guarantees.html)'
- en: '[Not So Fast: Questioning Deep Learning IQ Results](/2015/06/questioning-deep-learning-iq-results.html)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[别急：质疑深度学习的智商结果](/2015/06/questioning-deep-learning-iq-results.html)'
- en: '[The Myth of Model Interpretability](/2015/04/model-interpretability-neural-networks-deep-learning.html)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[模型可解释性的神话](/2015/04/model-interpretability-neural-networks-deep-learning.html)'
- en: '[(Deep Learning’s Deep Flaws)’s Deep Flaws](/2015/01/deep-learning-flaws-universal-machine-learning.html)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[(深度学习的深层缺陷)](/2015/01/deep-learning-flaws-universal-machine-learning.html)'
- en: '[Data Science’s Most Used, Confused, and Abused Jargon](/2015/02/data-science-confusing-jargon-abused.html)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中最常用、最混淆和滥用的术语](/2015/02/data-science-confusing-jargon-abused.html)'
- en: '[Differential Privacy: How to make Privacy and Data Mining Compatible](/2015/01/differential-privacy-data-mining-compatible.html)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[差分隐私：如何使隐私与数据挖掘兼容](/2015/01/differential-privacy-data-mining-compatible.html)'
- en: '* * *'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的 IT'
- en: '* * *'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解更多相关内容
- en: '[TensorFlow for Computer Vision - Transfer Learning Made Easy](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow 在计算机视觉中的应用 - 迁移学习简化版](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)'
- en: '[What is Transfer Learning?](https://www.kdnuggets.com/2022/01/transfer-learning.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是迁移学习？](https://www.kdnuggets.com/2022/01/transfer-learning.html)'
- en: '[Transfer Learning for Image Recognition and Natural Language Processing](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[图像识别和自然语言处理中的迁移学习](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
- en: '[A Practical Guide to Transfer Learning using PyTorch](https://www.kdnuggets.com/2023/06/practical-guide-transfer-learning-pytorch.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 PyTorch 进行迁移学习的实用指南](https://www.kdnuggets.com/2023/06/practical-guide-transfer-learning-pytorch.html)'
- en: '[Exploring the Potential of Transfer Learning in Small Data Scenarios](https://www.kdnuggets.com/exploring-the-potential-of-transfer-learning-in-small-data-scenarios)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[探索小数据场景中的迁移学习潜力](https://www.kdnuggets.com/exploring-the-potential-of-transfer-learning-in-small-data-scenarios)'
- en: '[Using Transfer Learning to Boost Model Performance](https://www.kdnuggets.com/using-transfer-learning-to-boost-model-performance)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用迁移学习提升模型性能](https://www.kdnuggets.com/using-transfer-learning-to-boost-model-performance)'
