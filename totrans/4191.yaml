- en: Training BPE, WordPiece, and Unigram Tokenizers from Scratch using Hugging Face
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/10/bpe-wordpiece-unigram-tokenizers-using-hugging-face.html](https://www.kdnuggets.com/2021/10/bpe-wordpiece-unigram-tokenizers-using-hugging-face.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Harshit Tyagi](https://www.linkedin.com/in/tyagiharshit/), Data Science
    Instructor | Mentor | YouTuber**'
  prefs: []
  type: TYPE_NORMAL
- en: Continuing the deep dive into the sea of NLP, this post is all about training
    tokenizers from scratch by leveraging **Hugging Face’s tokenizers package.**
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization is often regarded as a subfield of NLP but it has its own [story
    of evolution](https://dswharshit.substack.com/p/the-evolution-of-tokenization-byte) and
    how it has reached its current stage where it is underpinning the state-of-the-art
    NLP models.
  prefs: []
  type: TYPE_NORMAL
- en: Before we get to the fun part of training and comparing the different tokenizers,
    I want to give you a brief summary of the key differences between the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The main difference lies in the **choice of character pairs** to merge and t**he
    merging policy** that each of these algorithms uses to generate the final set
    of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/673d4a24d1ac87e138a8959acc3f40ef.png)'
  prefs: []
  type: TYPE_IMG
- en: BPE - a frequency-based model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Byte Pair Encoding uses the frequency of subword patterns to shortlist them
    for merging.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The drawback of using frequency as the driving factor is that you can end up
    having ambiguous final encodings that might not be useful for the new input text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It still has the scope of improvement in terms of generating unambiguous tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unigram - a probability-based model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Comes in the Unigram model that approaches to solve the merging problem by calculating
    the likelihood of each subword combination rather than picking the most frequent
    pattern.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It calculates the probability of every subword token and then drops it based
    on a loss function that is explained in [this research paper.](https://arxiv.org/pdf/1804.10959.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on a certain threshold of the loss value, you can then trigger the model
    to drop the bottom 20-30% of the subword tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unigram is a completely probabilistic algorithm that chooses both the pairs
    of characters and the final decision to merge(or not) in each iteration based
    on probability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WordPiece
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the release of BERT in 2018, there came a new subword tokenization algorithm
    called WordPiece which can be considered as an intermediary of BPE and Unigram
    algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WordPiece is also a greedy algorithm that leverages likelihood instead of count
    frequency to merge the best pair in each iteration but the choice of characters
    to pair is based on count frequency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, it is similar to BPE in terms of choosing characters to pair and similar
    to Unigram in terms of choosing the best pair to merge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the algorithmic differences covered, I tried to implement each of these
    algorithms(not from scratch) to compare the output generated by each of them.
  prefs: []
  type: TYPE_NORMAL
- en: Training the BPE, Unigram, and WordPiece Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, in order to have an unbiased comparison of outputs, I didn’t want to use
    pretrained algorithms as that would bring size, quality, and the content of the
    dataset into the picture.
  prefs: []
  type: TYPE_NORMAL
- en: One way could be to code these algorithms from scratch using the research papers
    and then test them out which is a good approach in order to truly understand the
    working of each algorithm but you might end up spending weeks doing that.
  prefs: []
  type: TYPE_NORMAL
- en: I instead used the [Hugging Face’s tokenizers](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html) package
    that offers the implementation of all today’s most used tokenizers. It also allows
    me to train these models from scratch on my choice of dataset and then tokenize
    the input string of our own choice.
  prefs: []
  type: TYPE_NORMAL
- en: '**Training datasets**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I chose two different datasets to train these models, one is a free book from
    Gutenberg which serves as a small dataset and the other one is the [wikitext-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) which
    is 516M of text.
  prefs: []
  type: TYPE_NORMAL
- en: In the Colab, you can download the datasets first and unzip them(if required),
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Import the required models and trainers**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Going through the documentation, you’ll find that the main API of the package
    is the class `Tokenizer.`
  prefs: []
  type: TYPE_NORMAL
- en: You can then instantiate any tokenizer with your choice of model(BPE/ Unigram/
    WordPiece).
  prefs: []
  type: TYPE_NORMAL
- en: Here, I imported the main class, all the models I wanted to test, and their
    trainers as I want to train these models from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**3-Step process to automate training and tokenization**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since I need to perform somewhat similar processes for three different models,
    I broke the processes into 3 functions. I’ll only need to call these functions
    for each model and my job here would be done.
  prefs: []
  type: TYPE_NORMAL
- en: So, what do these functions look like?
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1 - Prepare the tokenizer**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Preparing the tokenizer requires us to instantiate the Tokenizer class with
    a model of our choice but since we have four models(added a simple Word-level
    algorithm as well) to test, we’ll write if/else cases to instantiate the tokenizer
    with the right model.
  prefs: []
  type: TYPE_NORMAL
- en: To train the instantiated tokenizer on the small and large datasets, we will
    also need to instantiate a trainer, in our case, these would be **[`BpeTrainer`](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#tokenizers.trainers.BpeTrainer)`,
    WordLevelTrainer, WordPieceTrainer, and UnigramTrainer.`**
  prefs: []
  type: TYPE_NORMAL
- en: The instantiation and training will need us to specify some special tokens.
    These are tokens for unknown words and other special tokens that we’d need to
    use later on to add to our vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: You can also specify other training arguments vocabulary size or minimum frequency
    here.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll also need to add a pre-tokenizer to split our input to words as without
    a pre-tokenizer, we might get tokens that overlap several words: for instance,
    we could get an **`"there is"`** token since those two words often appear next
    to each other.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Using a pre-tokenizer will ensure no token is bigger than a word returned
    by the pre-tokenizer.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This function will return the tokenizer and its trainer object which can be
    used to train the model on a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are using the same pre-tokenizer (`Whitespace`) for all the models.
    You can choose to test it with [others](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.pre_tokenizers).
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2 - Train the tokenizer**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After preparing the tokenizers and trainers, we can start the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a function that will take the file(s) on which we intend to train our
    tokenizer along with the algorithm identifier.
  prefs: []
  type: TYPE_NORMAL
- en: '`‘WLV’` - Word Level Algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`‘WPC’` - WordPiece Algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`‘BPE’` - Byte Pair Encoding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`‘UNI’` - Unigram'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This is the main function that we’ll need to call for training the tokenizer,
    it will first prepare the tokenizer and trainer and then start training the tokenizers
    with the provided files.
  prefs: []
  type: TYPE_NORMAL
- en: After training, it saves the model in a JSON file, loads it from the file, and
    returns the trained tokenizer to start encoding the new input.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3 - Tokenize the input string**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last step is to start encoding the new input strings and compare the tokens
    generated by each algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we’ll be writing a nested for loop to train each model on the smaller
    dataset first followed by training on the larger dataset and tokenizing the input
    string as well.
  prefs: []
  type: TYPE_NORMAL
- en: '**Input string - **“This is a deep learning tokenization tutorial. Tokenization
    is the first step in a deep learning NLP pipeline. We will be comparing the tokens
    generated by each tokenization model. Excited much?!????”'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**##output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/198a394da78449354ccb0821037853a1.png)](https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F43eb1a88-36a1-4343-be1e-ac65843e3837_1306x430.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Analysis of the output:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Looking at the output, you’ll see the difference in how the tokens were generated
    which in turn led to a different number of tokens generated.
  prefs: []
  type: TYPE_NORMAL
- en: A simple **word-level algorithm** created 35 tokens no matter which dataset
    it was trained on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BPE** algorithm created 55 tokens when trained on a smaller dataset and 47
    when trained on a larger dataset. This shows that it was able to merge more pairs
    of characters when trained on a larger dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **Unigram model** created a similar(68 and 67) number of tokens with both
    datasets. But you could see the difference in the generated tokens:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](../Images/19bbf33161f11709f988096b3a59efb5.png)](https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdf3d128-641c-4680-9b43-0e04a505d67c_428x43.png)'
  prefs: []
  type: TYPE_NORMAL
- en: With a larger dataset, merging came closer to generating tokens that are better
    suited to encode real-world English language that we often use.
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/28734781ce5a4cafcbc9aba5f363a43e.png)](https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feb49063b-8896-496e-acec-0dea60d6ea37_260x40.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '**WordPiece** created 52 tokens when trained on a smaller dataset and 48 when
    trained on a larger dataset. The generated tokens have double ## to denote the
    use of the token as a prefix/suffix.[![](../Images/ac28c07cbcdf72cb828877c152386ca3.png)](https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5225119-6158-45e7-83b1-26bf587791f3_391x45.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All three algorithms generated lesser and better subword tokens when trained
    on a larger dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing the Tokens
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To compare the tokens, I stored the output of each algorithm in a dictionary
    and I’ll turn it into a dataframe to view the differences in tokens better.
  prefs: []
  type: TYPE_NORMAL
- en: Since the number of tokens generated by each model is different, I’ve added
    a <PAD> token to make the data rectangular and fit a dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: <PAD> is basically nan in the dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**##output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/5049300edd0a6d1c89ab139f3b07f95c.png)](https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F856ab4bc-7343-4114-9867-27e64a71d21a_306x474.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also look at the difference in tokens using sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/34a662dd103b84cdcf7f2eb321c1a8ea.png)](https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7be68b1a-d979-4688-94e9-b19219f2259d_370x692.png)[![](../Images/76e9e0b32184034f37d1b848d86aa050.png)](https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8942a6e1-d1bc-4a6e-bbec-3473a87ef9ca_370x626.png)'
  prefs: []
  type: TYPE_NORMAL
- en: All the code can be found in this [Colab notebook.](https://colab.research.google.com/drive/10gwzRY55JqzgeEQOX6nwFs6bQ84-mB9f?usp=sharing)
  prefs: []
  type: TYPE_NORMAL
- en: Closing thoughts and next steps!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on the kind of tokens generated, WPC does seem to generate subword tokens
    that are more commonly found in the English language but don’t hold me accountable
    for this observation.
  prefs: []
  type: TYPE_NORMAL
- en: These algorithms are slightly different from each other and do a somewhat similar
    job of developing a decent NLP model. But much of the performance depends on the
    use case of your language model, the vocabulary size, speed, and other factors.
  prefs: []
  type: TYPE_NORMAL
- en: A further advancement to these algorithms is the [SentencePiece algorithm](https://arxiv.org/pdf/1808.06226.pdf) which
    is a wholesome approach to the whole tokenization problem but much of this problem
    is alleviated by HuggingFace and even better, they have all the algorithms implemented
    in a single GitHub repo.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the tokenization algorithms and the next step is to understand
    what are embeddings, how tokenization plays a vital role in creating these embeddings,
    and how they affect a model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: References and Notes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you disagree with my analysis or any of my work in this post, I find highly
    encourage you to check out these resources for a precise understanding of the
    working of each algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Subword regularization: Improving Neural Network Translation Models with Multiple
    Subword Candidates](https://arxiv.org/pdf/1804.10959.pdf) by Taku Kudo'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf) -
    Research paper that discusses different segmentation techniques based BPE compression
    algorithm.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Hugging Face’s tokenizer package.](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you’re looking to start in the field of data science or ML, check out my
    course on [Foundations of Data Science & ML](https://www.wiplane.com/p/foundations-for-data-science-ml).
  prefs: []
  type: TYPE_NORMAL
- en: If you would like to see more of such content and you are not a subscriber,
    consider subscribing to my newsletter using the button below.
  prefs: []
  type: TYPE_NORMAL
- en: Have something to add or suggest, reply to this email, or comment on the post
    on Substack.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Harshit Tyagi](https://www.linkedin.com/in/tyagiharshit/)** is an engineer
    with amalgamated experience in web technologies and data science(aka full-stack
    data science). He has mentored over 1000 AI/Web/Data Science aspirants, and is
    designing data science and ML engineering learning tracks. Previously, Harshit
    developed data processing algorithms with research scientists at Yale, MIT, and
    UCLA.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://dswharshit.substack.com/p/ecde3224-cb0f-425b-97a2-853ea873d279).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Simple Question Answering Web App with HuggingFace Pipelines](/2021/10/simple-question-answering-web-app-hugging-face-pipelines.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Evolution of Tokenization – Byte Pair Encoding in NLP](/2021/10/evolution-tokenization-byte-pair-encoding-nlp.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Text Preprocessing Methods for Deep Learning](/2021/09/text-preprocessing-methods-deep-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How to Use the Hugging Face Tokenizers Library to Preprocess Text Data](https://www.kdnuggets.com/how-to-use-the-hugging-face-tokenizers-library-to-preprocess-text-data)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Build and Train a Transformer Model from Scratch with…](https://www.kdnuggets.com/how-to-build-and-train-a-transformer-model-from-scratch-with-hugging-face-transformers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using Hugging Face Transformers for Emotion Detection in Text](https://www.kdnuggets.com/using-hugging-face-transformers-for-emotion-detection-in-text)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Build AI Chatbot in 5 Minutes with Hugging Face and Gradio](https://www.kdnuggets.com/2023/06/build-ai-chatbot-5-minutes-hugging-face-gradio.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Translate Languages with MarianMT and Hugging Face Transformers](https://www.kdnuggets.com/how-to-translate-languages-with-marianmt-and-hugging-face-transformers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top 10 Machine Learning Demos: Hugging Face Spaces Edition](https://www.kdnuggets.com/2022/05/top-10-machine-learning-demos-hugging-face-spaces-edition.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
