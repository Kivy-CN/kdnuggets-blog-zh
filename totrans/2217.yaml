- en: 'The Quest for Model Confidence: Can You Trust a Black Box?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型置信度的探索：你能相信黑箱吗？
- en: 原文：[https://www.kdnuggets.com/the-quest-for-model-confidence-can-you-trust-a-black-box](https://www.kdnuggets.com/the-quest-for-model-confidence-can-you-trust-a-black-box)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/the-quest-for-model-confidence-can-you-trust-a-black-box](https://www.kdnuggets.com/the-quest-for-model-confidence-can-you-trust-a-black-box)
- en: '![The Quest for Model Confidence: Can You Trust a Black Box?](../Images/894e9ac14c8ab3392f26660526c9f14a.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![模型置信度的探索：你能相信黑箱吗？](../Images/894e9ac14c8ab3392f26660526c9f14a.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'Large Language Models (LLMs) like GPT-4 and LLaMA2 have entered the [data labeling]
    chat. LLMs have come a long way and can now label data and take on tasks historically
    conducted by humans. While obtaining data labels with an LLM is incredibly quick
    and relatively cheap, there’s still one big issue, these models are the ultimate
    black boxes. So the burning question is: how much trust should we put in the labels
    these LLMs generate? In today’s post, we break down this conundrum to establish
    some fundamental guidelines for gauging the confidence we can have in LLM-labeled
    data.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 像GPT-4和LLaMA2这样的**大型语言模型（LLMs）**已经进入了[数据标注](https://example.org)的领域。LLMs已经取得了长足的进步，现在可以进行数据标注并承担历史上由人类完成的任务。尽管使用LLM获取数据标签非常迅速且相对便宜，但仍然存在一个大问题，这些模型是终极的黑箱。因此，燃眉之急是：我们应该对这些LLM生成的标签有多少信任？在今天的文章中，我们将解开这一难题，以建立一些基本准则来评估我们对LLM标注数据的信任度。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织IT需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Background
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: The results presented below are from an experiment conducted by [Toloka](http://toloka.ai)
    using popular models and a dataset in Turkish. This is not a scientific report
    but rather a short overview of possible approaches to the problem and some suggestions
    for  how to determine which method works best for your application.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 下面展示的结果来自于[Toloka](http://toloka.ai)进行的一项实验，该实验使用了流行的模型和土耳其语的数据集。这不是一份科学报告，而是对可能解决该问题的方法的简要概述，以及一些关于如何确定哪种方法最适合您的应用程序的建议。
- en: The Big Question
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重大问题
- en: 'Before we get into the details, here’s the big question: When can we trust
    a label generated by an LLM, and when should we be skeptical? Knowing this can
    help us in automated data labeling and can also be useful in other applied tasks
    like customer support, content generation, and more.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入细节之前，这里有一个重大问题：我们何时可以相信由LLM生成的标签，何时应该保持怀疑？了解这一点可以帮助我们进行自动化数据标注，并且在客户支持、内容生成等其他应用任务中也会很有用。
- en: '**The Current State of Affairs**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**当前的情况**'
- en: So, how are people tackling this issue now? Some directly ask the model to spit
    out a confidence score, some look at the consistency of the model’s answers over
    multiple runs, while others examine the model’s log probabilities. But are any
    of these approaches reliable? Let’s find out.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，人们现在是如何解决这个问题的呢？有些人直接要求模型输出一个置信度分数，有些人查看模型在多次运行中的答案一致性，而另一些人则检查模型的对数概率。但是这些方法中的任何一种是否可靠呢？让我们来找出答案。
- en: The Rule of Thumb
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一般原则
- en: What makes a “good” confidence measure? One simple rule to follow is that there
    should be a positive correlation between the confidence score and the accuracy
    of the label. In other words, a higher confidence score should mean a higher likelihood
    of being correct. You can visualize this relationship using a calibration plot,
    where the X and Y axes represent confidence and accuracy, respectively.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 什么才算一个“好的”置信度测量？一个简单的规则是置信度分数与标签的准确性之间应该存在正相关。换句话说，更高的置信度分数应该意味着更高的正确概率。您可以使用校准图来可视化这种关系，其中X轴和Y轴分别表示置信度和准确性。
- en: Experiments and Their Results
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验及其结果
- en: 'Approach 1: Self-Confidence'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法1：自信度
- en: The self-confidence approach involves asking the model about its confidence
    directly. And guess what? The results weren’t half bad! While the LLMs we tested
    struggled with the non-English dataset, the correlation between self-reported
    confidence and actual accuracy was pretty solid, meaning models are well aware
    of their limitations. We got similar results for GPT-3.5 and GPT-4 here.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![The Quest for Model Confidence: Can You Trust a Black Box?](../Images/5a50406ba0dbb94a4a94a29db976aa15.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: 'Approach 2: Consistency'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Set a high temperature (~0.7–1.0), label the same item several times, and analyze
    the consistency of the answers, for more details, see this [paper](https://arxiv.org/abs/2305.19187).
    We tried this with GPT-3.5 and it was, to put it lightly, a dumpster fire. We
    prompted the model to answer the same question multiple times and the results
    were consistently erratic. This approach is as reliable as asking a Magic 8-Ball
    for life advice and should not be trusted.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![The Quest for Model Confidence: Can You Trust a Black Box?](../Images/0aa9e9852008a4f2896db47a07eb52ab.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: 'Approach 3: Log Probabilities'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Log probabilities offered a pleasant surprise. Davinci-003 returns logprobs
    of the tokens in the completion mode. Examining this output, we got a surprisingly
    decent confidence score that correlated well with accuracy. This method offers
    a promising approach to determining a reliable confidence score.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![The Quest for Model Confidence: Can You Trust a Black Box?](../Images/bf034f50dc67f3b061d140b9d75c8b81.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: The Takeaway
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, what did we learn? Here it is, no sugar-coating:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-Confidence: Useful, but handle with care. Biases are reported widely.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Consistency: Just don’t. Unless you enjoy chaos.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Log Probabilities: A surprisingly good bet for now if the model allows you
    to access them.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The exciting part? Log probabilities appear to be quite robust even without
    fine-tuning the model, despite this [paper](https://arxiv.org/abs/2305.14975)
    reporting this method to be overconfident. There is room for further exploration.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Future Avenues
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A logical next step could be to find a golden formula that combines the best
    parts of each of these three approaches, or explores new ones. So, if you’re up
    for a challenge, this could be your next weekend project!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Up
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alright, ML aficionados and newbies, that’s a wrap. Remember, whether you’re
    working on data labeling or building the next big conversational agent - understanding
    model confidence is key. Don’t take those confidence scores at face value and
    make sure you do your homework!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Hope you found this insightful. Until next time, keep crunching those numbers
    and questioning those models.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '**[Ivan Yamshchikov](https://www.linkedin.com/in/kroniker/?originalSubdomain=de)**
    is a professor of Semantic Data Processing and Cognitive Computing at the Center
    for AI and Robotics, Technical University of Applied Sciences Würzburg-Schweinfurt.
    He also leads the Data Advocates team at Toloka AI. His research interests include
    computational creativity, semantic data processing and generative models.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**[伊万·扬什奇科夫](https://www.linkedin.com/in/kroniker/?originalSubdomain=de)**
    是应用科学大学伍尔茨堡-施韦因富特AI与机器人中心的语义数据处理与认知计算教授。他还领导Toloka AI的数据倡导者团队。他的研究兴趣包括计算创造力、语义数据处理和生成模型。'
- en: More On This Topic
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题更多内容
- en: '[Sky''s the Limit: Learn how JetBlue uses Monte Carlo and Snowflake…](https://www.kdnuggets.com/2022/12/monte-carlo-jetblue-snowflake-build-trust-improve-model-accuracy.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[天空是极限：了解JetBlue如何使用Monte Carlo和Snowflake…](https://www.kdnuggets.com/2022/12/monte-carlo-jetblue-snowflake-build-trust-improve-model-accuracy.html)'
- en: '[Working with Confidence Intervals](https://www.kdnuggets.com/2023/04/working-confidence-intervals.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[处理置信区间](https://www.kdnuggets.com/2023/04/working-confidence-intervals.html)'
- en: '[Trust in AI is Priceless](https://www.kdnuggets.com/2022/08/trust-ai-priceless.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[对AI的信任是无价的](https://www.kdnuggets.com/2022/08/trust-ai-priceless.html)'
- en: '[In Data We Trust: Data Centric AI](https://www.kdnuggets.com/2022/10/data-trust-data-centric-ai.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我们信任数据：数据中心AI](https://www.kdnuggets.com/2022/10/data-trust-data-centric-ai.html)'
- en: '[Bounding Box Deep Learning: The Future of Video Annotation](https://www.kdnuggets.com/2022/07/bounding-box-deep-learning-future-video-annotation.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[边界框深度学习：视频注释的未来](https://www.kdnuggets.com/2022/07/bounding-box-deep-learning-future-video-annotation.html)'
- en: '[Black Friday Deal - Master Machine Learning for Less with DataCamp](https://www.kdnuggets.com/2022/11/datacamp-black-friday-deal-master-machine-learning-less-datacamp.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[黑色星期五优惠 - 用更少的钱在DataCamp上掌握机器学习](https://www.kdnuggets.com/2022/11/datacamp-black-friday-deal-master-machine-learning-less-datacamp.html)'
