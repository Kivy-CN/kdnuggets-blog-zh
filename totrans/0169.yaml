- en: Learn How to Run Alpaca-LoRA on Your Device in Just a Few Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/05/learn-run-alpacalora-device-steps.html](https://www.kdnuggets.com/2023/05/learn-run-alpacalora-device-steps.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Learn How to Run Alpaca-LoRA on Your Device in Just a Few Steps](../Images/b8c4bb8fbbee6508b65e2b6a3ead8c58.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '[ChatGPT](https://openai.com/blog/chatgpt) is an AI language model and gained
    traction in recent months. It has two popular releases, GPT-3.5 and GPT-4\. GPT-4
    is the upgraded version of GPT-3.5 with more accurate answers. But the main problem
    with ChatGPT is that it is not open-source, i.e. it does not allow users to see
    and modify its source code. This leads to many issues like Customization, Privacy
    and AI Democratization.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: There is a need for such AI language chatbots that can work like ChatGPT but
    are free, open-source and less CPU intensive. One such AI model is Aplaca LoRA,
    which we will discuss in the tutorial. By the end of this tutorial, you will have
    a good understanding of it and can run it on your local machine using Python.
    But first, let’s discuss what Alpaca LoRA is.
  prefs: []
  type: TYPE_NORMAL
- en: What is Alpaca LoRA?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Alpaca` is an AI language model developed by a team of researchers from Stanford
    University. It uses `LLaMA`, which is Meta’s large-scale language model. It uses
    OpenAI’s GPT (text-davinci-003) to fine-tune the 7B parameters-sized LLaMA model.
    It is free for academic and research purposes and has low computational requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: The team started with the LLaMA 7B model and pre-trained it with 1 trillion
    tokens. They began with 175 human-written instruction-output pairs and asked ChatGPT’s
    API to generate more pairs using these pairs. They collected 52000 sample conversations,
    which they used to fine-tune their LLaMA model further.
  prefs: []
  type: TYPE_NORMAL
- en: '`LLaMA` models have several versions, i.e. 7B, 13B, 30B and 65B. `Alpaca` can
    be extended to 7B, 13B, 30B and 65B parameter models.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Learn How to Run Alpaca-LoRA on Your Device in Just a Few Steps](../Images/584392b5c100755a7dbd7a7455207f98.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig.1 Aplaca 7B Architecture | Image by [Stanford](https://crfm.stanford.edu/2023/03/13/alpaca.html)
  prefs: []
  type: TYPE_NORMAL
- en: Alpaca-LoRA is a smaller version of [Stanford Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)
    that consumes less power and can able to run on low-end devices like Raspberry
    Pie. Alpaca-LoRA uses [Low-Rank Adaptation(LoRA)](https://arxiv.org/pdf/2106.09685.pdf)
    to accelerate the training of large models while consuming less memory.
  prefs: []
  type: TYPE_NORMAL
- en: Alpaca LoRA Python Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will create a Python environment to run Alpaca-Lora on our local machine.
    You need a GPU to run that model. It cannot run on the CPU (or outputs very slowly).
    If you use the 7B model, at least 12GB of RAM is required or higher if you use
    13B or 30B models.
  prefs: []
  type: TYPE_NORMAL
- en: If you don't have a GPU, you can perform the same steps in the [Google Colab](https://colab.research.google.com/).
    In the end, I will share the Colab link with you.
  prefs: []
  type: TYPE_NORMAL
- en: We will follow [this](https://github.com/tloen/alpaca-lora) GitHub repo of Alpaca-LoRA
    by tloen.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Creating a Virtual Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will install all our libraries in a virtual environment. It is not mandatory
    but recommended. The following commands are for Windows OS. (This step is not
    necessary for Google Colab)
  prefs: []
  type: TYPE_NORMAL
- en: Command to create venv
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Command to activate it
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Command to deactivate it
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Cloning GitHub Repository
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we will clone the repo of Alpaca LoRA.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Installing the libraries
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The python file named `finetune.py` contains the hyperparameters of the LLaMA
    model, like batch size, number of epochs, learning rate (LR), etc., which you
    can play with. Running `finetune.py` is not compulsory. Otherwise, the executor
    file reads the foundation model and weights from `tloen/alpaca-lora-7b`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 4\. Running the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The python file named `generate.py` will read the Hugging Face model and LoRA
    weights from `tloen/alpaca-lora-7b`. It runs a user interface using Gradio, where
    the user can write a question in a textbox and receive the output in a separate
    textbox.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** If you are working in Google Colab, please mark `share=True` in the
    `launch()` function of the `generate.py` file. It will run the interface on a
    public URL. Otherwise, it will run on localhost `[http://0.0.0.0:7860](http://0.0.0.0:7860)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Learn How to Run Alpaca-LoRA on Your Device in Just a Few Steps](../Images/1e0726a6a301a56a44268453e27482b3.png)'
  prefs: []
  type: TYPE_IMG
- en: It has two URLs, one is public, and one is running on the localhost. If you
    use Google Colab, the public link can be accessible.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Dockerize the Application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can [Dockerize](https://docs.docker.com/get-started/overview/) your application
    in a Docker Container if you want it to export somewhere or facing some dependency
    issues. Docker is a tool that creates an immutable image of the application. Then
    this image can be shared and then converted back to the application, which runs
    in a container having all the necessary libraries, tools, codes and runtime. You
    can download Docker for Windows from [here](https://docs.docker.com/desktop/install/windows-install/).
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** You can skip this step if you are using Google Colab.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Build the Container Image:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Run the Container:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: It will run your application on `https://localhost:7860`.
  prefs: []
  type: TYPE_NORMAL
- en: Alpaca-LoRA User Interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, we have our Alpaca-LoRA running. Now we will explore some of its features
    and ask him to write something for us.
  prefs: []
  type: TYPE_NORMAL
- en: '![Learn How to Run Alpaca-LoRA on Your Device in Just a Few Steps](../Images/ade3cd343800038ea531c1f99d39e4f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 2 Aplaca-LoRA User Interface | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: It provides a UI similar to ChatGPT, where we can ask a question, and it answers
    it accordingly. It also takes other parameters like Temperature, Top p, Top k,
    Beams and Max Tokens. Basically, these are generation configurations used at the
    time of evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: There is a checkbox `Stream Output`. If you tick that checkbox, the bot will
    reply one token at a time (i.e. it writes the output line by line, likewise ChatGPT).
    If you don’t tick that option, it will write in a single go.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s ask him some questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Q1: Write a Python code to find the factorial of a number.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Learn How to Run Alpaca-LoRA on Your Device in Just a Few Steps](../Images/e8020577429f8bba9aba273563e7ea9a.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 3 Output-1 | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Q2: Translate "KDnuggets is a leading site on Data Science, Machine Learning,
    AI and Analytics. " into French'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Learn How to Run Alpaca-LoRA on Your Device in Just a Few Steps](../Images/3cdbfaea397d27a0550d4755fa940b9e.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 4 Output-2 | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Unlike ChatGPT, it has some limitations too. It may not provide you with the
    latest information because it is not internet connected. Also, it may spread hate
    and misinformation towards vulnerable sections of society. Despite this, it is
    a great free, open-source tool with lower computation demands. It can be beneficial
    for researchers and academicians for ethical AI and cyber security activities.
  prefs: []
  type: TYPE_NORMAL
- en: Google Colab Link – [Link](https://colab.research.google.com/drive/1t3oXBoRYKzeRUkCBaNlN5u3xFvhJNVVM?usp=sharing)
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GitHub – [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stanford Alpaca – [A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this tutorial, we have discussed the working of Alpaca-LoRA and the commands
    to run it locally or on Google Colab. Alpaca-LoRA is not the only chatbot that
    is open-source. There are many other chatbots that are open-source and free to
    use, like LLaMA, GPT4ALL, Vicuna, etc. If you want a quick synopsis, you can refer
    to [this](/2023/04/8-opensource-alternative-chatgpt-bard.html) article by Abid
    Ali Awan on KDnuggets.
  prefs: []
  type: TYPE_NORMAL
- en: That is all for today. I hope you have enjoyed reading this article. We will
    meet again in some other article. Until then, keep reading and keep learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Aryan Garg](https://www.linkedin.com/in/aryan-garg-1bbb791a3/)** is a B.Tech.
    Electrical Engineering student, currently in the final year of his undergrad.
    His interest lies in the field of Web Development and Machine Learning. He have
    pursued this interest and am eager to work more in these directions.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Breaking the Data Barrier: How Zero-Shot, One-Shot, and Few-Shot…](https://www.kdnuggets.com/2023/08/breaking-data-barrier-zeroshot-oneshot-fewshot-learning-transforming-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[On-device AI with Developer-Ready Software Stacks](https://www.kdnuggets.com/2022/03/qualcomm-ondevice-ai-developer-ready-software-stacks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ensuring Reliable Few-Shot Prompt Selection for LLMs](https://www.kdnuggets.com/2023/07/ensuring-reliable-fewshot-prompt-selection-llms.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Distribute and Run LLMs with llamafile in 5 Simple Steps](https://www.kdnuggets.com/distribute-and-run-llms-with-llamafile-in-5-simple-steps)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Schedule & Run ETLs with Jupysql and GitHub Actions](https://www.kdnuggets.com/2023/05/schedule-run-etls-jupysql-github-actions.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Getting Started with PyTest: Effortlessly Write and Run Tests in Python](https://www.kdnuggets.com/getting-started-with-pytest-effortlessly-write-and-run-tests-in-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
