- en: 'Closing the Gap Between Human Understanding and Machine Learning: Explainable
    AI as a Solution'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缩小人类理解与机器学习之间的差距：可解释人工智能作为解决方案
- en: 原文：[https://www.kdnuggets.com/2023/06/closing-gap-human-understanding-machine-learning-explainable-ai-solution.html](https://www.kdnuggets.com/2023/06/closing-gap-human-understanding-machine-learning-explainable-ai-solution.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/06/closing-gap-human-understanding-machine-learning-explainable-ai-solution.html](https://www.kdnuggets.com/2023/06/closing-gap-human-understanding-machine-learning-explainable-ai-solution.html)
- en: '![Closing the Gap Between Human Understanding and Machine Learning: Explainable
    AI as a Solution](../Images/f61a1b91d9d13f774674910b3eac2305.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![缩小人类理解与机器学习之间的差距：可解释人工智能作为解决方案](../Images/f61a1b91d9d13f774674910b3eac2305.png)'
- en: Image by Bing Image Creator
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 Bing 图像创作者提供
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Have you ever opened your favorite shopping app and the first thing you see
    is a recommendation for a product that you didn’t even know you needed, but you
    end up buying thanks to the timely recommendation? Or have you opened your go-to
    music app and been delighted to see a forgotten gem by your favorite artist recommended
    right on the top as something “you might like”? Knowingly, or unknowingly, all
    of us encounter decisions, actions, or experiences that have been generated by
    Artificial Intelligence (AI) today. While some of these experiences are fairly
    innocuous (spot-on music recommendations, anyone?), some others might sometimes
    cause some unease (“*How did this app know that I have been thinking of doing
    a weight loss program?*”). This unease escalates to worry and distrust when it
    comes to matters of privacy about oneself and one’s loved ones. However, knowing
    *how* or *why* something was recommended to you can help with some of that unease.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾打开你最喜欢的购物应用程序，看到的第一件事是推荐一个你甚至不知道需要的产品，但由于及时的推荐你最终购买了它？或者你是否打开你常用的音乐应用程序，看到一个被遗忘的宝藏由你最喜欢的艺术家推荐在最上面，作为“你可能喜欢”的内容感到高兴？无论是知道还是不知道，我们今天都遇到由人工智能（AI）生成的决策、行动或体验。虽然这些体验中的一些是相当无害的（比如精准的音乐推荐），但有些可能会引起一些不安（“*这个应用怎么知道我一直在考虑进行减肥计划？*”）。当涉及到关于自己和亲人的隐私问题时，这种不安会升级为担忧和不信任。然而，了解*如何*或*为什么*会推荐某些内容，可以帮助缓解一些这种不安。
- en: This is where Explainable AI, or XAI, comes in. As AI-enabled systems become
    more and more ubiquitous, the need to understand how these systems make decisions
    is growing. In this article, we will explore XAI, discuss the challenges in interpretable
    AI models, advancements in making these models more interpretable and provide
    guidelines for companies and individuals to implement XAI in their products to
    foster user trust in AI.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是可解释人工智能（Explainable AI，简称 XAI）的作用所在。随着人工智能系统变得越来越普及，理解这些系统如何做出决策的需求也在增长。本文将探讨
    XAI，讨论可解释 AI 模型中的挑战，介绍使这些模型更具可解释性的进展，并为公司和个人提供在产品中实施 XAI 的指导，以促进用户对人工智能的信任。
- en: What is Explainable AI?
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是可解释人工智能？
- en: 'Explainable AI (XAI) is the ability of AI systems to be able to provide explanations
    for their decisions or actions. XAI bridges the important gap between an AI system
    deciding and the end user understanding *why* that decision was made. Before the
    advent of AI, systems would most often be rule-based (e.g., if a customer buys
    pants, recommend belts. Or if a person switches on their “Smart TV”, keep rotating
    the #1 recommendation between fixed 3 options). These experiences provided a sense
    of predictability. However, as AI became mainstream, connecting the dots backward
    from why something gets shown or why some decision is made by a product isn’t
    straightforward. Explainable AI can help in these instances.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释人工智能（XAI）是指 AI 系统能够提供其决策或行动的解释的能力。XAI 填补了 AI 系统决策与最终用户理解*为什么*做出该决策之间的重要差距。在人工智能出现之前，系统通常是基于规则的（例如，如果客户购买裤子，则推荐皮带；或者如果某人打开“智能电视”，则在固定的三个选项之间循环推荐#1）。这些经历提供了一种可预测的感觉。然而，随着人工智能的普及，倒推为什么会显示某些内容或某些决策是如何由产品做出的并不简单。可解释人工智能可以在这些情况下提供帮助。
- en: Explainable AI (XAI) allows users to understand *why* an AI system decided something
    and *what* factors went into the decision. For example, when you open your music
    app, you might see a widget called “*Because you like Taylor Swift”* followed
    by recommendations that are pop music and similar to Taylor Swift’s songs. Or
    you might open a shopping app and see “*Recommendations based on your recent shopping
    history”* followed by baby product recommendations because you bought some baby
    toys and clothes in the recent few days.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释的 AI（XAI）允许用户理解 AI 系统做出某个决策的*原因*以及*决策所依据的因素*。例如，当你打开音乐应用时，你可能会看到一个名为“*因为你喜欢泰勒·斯威夫特*”的部件，接着是类似泰勒·斯威夫特歌曲的流行音乐推荐。或者你可能打开一个购物应用，看到“*基于你最近的购物历史的推荐*”，接着是婴儿产品推荐，因为你在最近几天购买了一些婴儿玩具和衣物。
- en: XAI is particularly important in areas where high-stakes decisions are made
    by AI. For example, algorithmic trading and other financial recommendations, healthcare,
    autonomous vehicles, and more. Being able to provide an explanation for decisions
    can help users understand the rationale, identify any biases introduced in the
    model’s decision-making because of the data on which it is trained, correct errors
    in the decisions, and help build trust between humans and AI. Additionally, with
    increasing regulatory guidelines and legal requirements that are emerging, the
    importance of XAI is only set to grow.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: XAI 在 AI 做出高风险决策的领域尤为重要。例如，算法交易和其他金融建议、医疗保健、自动驾驶汽车等。能够提供决策解释有助于用户理解决策理由，识别因训练数据引入的模型偏见，纠正决策错误，并帮助建立人类与
    AI 之间的信任。此外，随着日益增加的监管指南和法律要求，XAI 的重要性只会不断增长。
- en: Challenges in XAI
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XAI 的挑战
- en: If XAI provides transparency to users, then why not make all AI models interpretable?
    There are several challenges that prevent this from happening.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 XAI 能够向用户提供透明度，那为什么不使所有 AI 模型都具备可解释性呢？有几个挑战阻碍了这一目标的实现。
- en: Advanced AI models like deep neural networks have multiple hidden layers between
    the inputs and output. Each layer takes in the input from a previous layer, performs
    computation on it, and passes it on as the input to the next layer. The complex
    interactions between layers make it hard to trace the decision-making process
    in order to make it explainable. This is the reason why these models are often
    referred to as black boxes.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 高级 AI 模型如深度神经网络在输入和输出之间有多个隐藏层。每一层接受来自前一层的输入，对其进行计算，然后将结果传递给下一层。层与层之间的复杂交互使得跟踪决策过程以使其可解释变得困难。这就是为什么这些模型通常被称为黑箱的原因。
- en: These models also process high-dimensional data like images, audio, text, and
    more. Being able to interpret the influence of each and every feature in order
    to be able to determine which feature contributed the most to a decision is challenging.
    Simplifying these models to make them more interpretable results in a decrease
    in their performance. For example, simpler and more “understandable” models like
    decision trees might sacrifice predictive performance. As a result, trading off
    performance and accuracy for the sake of predictability is also not acceptable.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型还处理高维数据，如图像、音频、文本等。解释每一个特征的影响以确定哪个特征对决策的贡献最大是具有挑战性的。简化这些模型以提高可解释性会导致性能下降。例如，更简单且“易于理解”的模型如决策树可能会牺牲预测性能。因此，为了可预测性而在性能和准确性之间进行权衡也是不可接受的。
- en: Advancements in XAI
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XAI 的进展
- en: With the growing need for XAI to continue building human trust in AI, there
    have been strides in recent times in this area. For example, there are some models
    like decision trees, or linear models, that make interpretability fairly obvious.
    There are also symbolic or rule-based AI models that focus on the explicit representation
    of information and knowledge. These models often need humans to define rules and
    feed domain information to the models. With the active development happening in
    this field, there are also hybrid models that combine deep learning with interpretability,
    minimizing the sacrifice made on performance.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对 XAI 的需求日益增长以继续建立人类对 AI 的信任，近年来在这一领域取得了一些进展。例如，一些模型如决策树或线性模型使可解释性相当明显。还有一些符号或基于规则的
    AI 模型专注于信息和知识的明确表示。这些模型通常需要人为定义规则并向模型提供领域信息。随着这一领域的积极发展，也出现了结合深度学习与可解释性的混合模型，最大限度地减少了性能上的牺牲。
- en: Guidelines to Implement XAI in Products
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施 XAI 的产品指南
- en: Empowering users to understand more and more why AI models decide what they
    decide can help foster trust and transparency about the models. It can lead to
    improved, and symbiotic, collaboration between humans and machines where the AI
    model helps humans in decision-making with transparency and humans help tune the
    AI model to remove biases, inaccuracies, and errors.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 赋予用户更多理解AI模型决策原因的能力，有助于促进对模型的信任和透明度。这可以导致人机之间改进的、互利的合作，其中AI模型帮助人类做出透明的决策，而人类则帮助调整AI模型以消除偏差、不准确和错误。
- en: 'Below are some ways in which companies and individuals can implement XAI in
    their products:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是公司和个人可以在其产品中实施XAI的一些方式：
- en: '**Select an Interpretable Model where you can** – Where they suffice and serve
    well, interpretable AI models should be selected over those that are not interpretable
    easily. For example, in healthcare, simpler models like decision trees can help
    doctors understand why an AI model recommended a certain diagnosis, which can
    help foster trust between the doctor and the AI model. Feature engineering techniques
    like one-hot coding or feature scaling that improve interpretability should be
    used.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择可解释的模型** – 在可解释的AI模型能满足需求且表现良好的情况下，应优先选择这些模型。例如，在医疗保健领域，像决策树这样的简单模型可以帮助医生理解AI模型推荐某种诊断的原因，这有助于建立医生与AI模型之间的信任。应使用提高可解释性的特征工程技术，如独热编码或特征缩放。'
- en: '**Use Post-hoc Explanations** – Use techniques like feature importance and
    attention mechanisms to generate post-hoc explanations. For example, LIME (Local
    Interpretable Model-agnostic Explanations) is a technique that explains the predictions
    of models. It generates feature importance scores to highlight every feature’s
    contribution to a model’s decision. For example, if you end up “liking” a particular
    playlist recommendation, the LIME method would try to add and remove certain songs
    from the playlist and predict the likelihood of your liking the playlist and conclude
    that the artists whose songs are in the playlist play a big role in your liking
    or disliking the playlist.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用事后解释** – 使用像特征重要性和注意力机制这样的技术生成事后解释。例如，LIME（局部可解释模型无关解释）是一种解释模型预测的技术。它生成特征重要性分数，以突出每个特征对模型决策的贡献。例如，如果你最终“喜欢”某个播放列表推荐，LIME方法会尝试添加和移除播放列表中的某些歌曲，并预测你喜欢该播放列表的可能性，从而得出播放列表中歌曲的艺术家在你喜欢或不喜欢该播放列表中扮演了重要角色。'
- en: '**Communication with Users –** Techniques like LIME or SHAP (SHapley Additive
    exPlanations) can be used to provide a useful explanation about specific local
    decisions or predictions without necessarily having to explain all the complexities
    of the model overall. Visual cues like activation maps or attention maps can also
    be leveraged to highlight what inputs are most relevant to the output generated
    by a model. Recent technologies like Chat GPT can be used to simplify complex
    explanations in simple language that can be understood by users. Finally, giving
    users some control so they can interact with the model can help build trust. For
    example, users could try tweaking inputs in different ways to see how the output
    changes.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**与用户的沟通** – 可以使用像LIME或SHAP（SHapley Additive exPlanations）这样的技术，提供关于具体局部决策或预测的有用解释，而不必解释模型的所有复杂性。还可以利用激活图或注意力图等视觉提示，突出输入与模型生成的输出之间的相关性。像Chat
    GPT这样的新技术可以用来简化复杂的解释，以用户可以理解的简单语言表达。最后，给予用户一定的控制权，以便他们与模型互动，这有助于建立信任。例如，用户可以尝试以不同的方式调整输入，以查看输出的变化。'
- en: '**Continuous Monitoring** – Companies should implement mechanisms to monitor
    the performance of models and automatically detect and alarm when biases or drifts
    are detected. There should be regular updating and fine-tuning of models, as well
    as audits and evaluations to ensure that the models are compliant with regulatory
    laws and meeting ethical standards. Finally, even if sparingly, there should be
    humans in the loop to provide feedback and corrections as needed.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**持续监测** – 公司应实施机制来监控模型的性能，并在检测到偏差或漂移时自动发出警报。应定期更新和微调模型，并进行审计和评估，以确保模型符合监管法律并满足伦理标准。最后，即使很少，也应有人参与提供反馈和必要的修正。'
- en: Conclusion
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In summary, as AI continues to grow, it becomes imperative to build XAI in order
    to maintain user trust in AI. By adopting the guidelines articulated above, companies
    and individuals can build AI that is more transparent, understandable, and simple.
    The more companies adopt XAI, the better the communication between users and AI
    systems will be, and the more users will feel confident about letting AI make
    their lives better
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，随着人工智能的不断发展，构建可解释的人工智能（XAI）变得至关重要，以维持用户对人工智能的信任。通过采用上述指导方针，公司和个人可以构建更透明、易于理解和简单的人工智能。公司越多地采用XAI，用户与人工智能系统之间的沟通就会越好，用户也会越有信心让人工智能改善他们的生活。
- en: '**[Ashlesha Kadam](https://www.linkedin.com/in/ashleshakadam/)** leads a global
    product team at Amazon Music that builds music experiences on Alexa and Amazon
    Music apps (web, iOS, Android) for millions of customers across 45+ countries.
    She is also a passionate advocate for women in tech, serving as co-chair for the
    Human Computer Interaction (HCI) track for Grace Hopper Celebration (biggest tech
    conference for women in tech with 30K+ participants across 115 countries). In
    her free time, Ashlesha loves reading fiction, listening to biz-tech podcasts
    (current favorite - Acquired), hiking in the beautiful Pacific Northwest and spending
    time with her husband, son and 5yo Golden Retriever.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Ashlesha Kadam](https://www.linkedin.com/in/ashleshakadam/)** 领导着亚马逊音乐的全球产品团队，为来自45多个国家的数百万客户在Alexa和亚马逊音乐应用（网页、iOS、Android）上打造音乐体验。她还是女性科技领域的热情倡导者，担任Grace
    Hopper Celebration（全球最大的女性科技会议，参会者超过3万人，来自115个国家）的计算机与人类交互（HCI）分会的联合主席。在空闲时间，Ashlesha喜欢阅读小说、听商业科技播客（当前最爱
    - Acquired），在美丽的太平洋西北地区徒步旅行，以及和丈夫、儿子及5岁的金毛猎犬共度时光。'
- en: '* * *'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三推荐课程
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的IT工作'
- en: '* * *'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[The Gap Between Deep Learning and Human Cognitive Abilities](https://www.kdnuggets.com/2022/10/gap-deep-learning-human-cognitive-abilities.html)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习与人类认知能力之间的差距](https://www.kdnuggets.com/2022/10/gap-deep-learning-human-cognitive-abilities.html)'
- en: '[The AI Education Gap and How to Close It](https://www.kdnuggets.com/2022/11/ai-education-gap-close.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能教育差距及如何弥补](https://www.kdnuggets.com/2022/11/ai-education-gap-close.html)'
- en: '[Is There a Way to Bridge the MLOps Tools Gap?](https://www.kdnuggets.com/2022/08/way-bridge-mlops-tools-gap.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[有没有办法弥补MLOps工具的差距？](https://www.kdnuggets.com/2022/08/way-bridge-mlops-tools-gap.html)'
- en: '[The Range of NLP Applications in the Real World: A Different…](https://www.kdnuggets.com/2022/03/different-solution-problem-range-nlp-applications-real-world.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[现实世界中NLP应用的范围：不同的解决方案](https://www.kdnuggets.com/2022/03/different-solution-problem-range-nlp-applications-real-world.html)'
- en: '[The Difference Between Training and Testing Data in Machine Learning](https://www.kdnuggets.com/2022/08/difference-training-testing-data-machine-learning.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中训练数据与测试数据的区别](https://www.kdnuggets.com/2022/08/difference-training-testing-data-machine-learning.html)'
- en: '[Between Dreams and Reality: Generative Text and Hallucinations](https://www.kdnuggets.com/between-dreams-and-reality-generative-text-and-hallucinations)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[梦境与现实之间：生成文本与幻觉](https://www.kdnuggets.com/between-dreams-and-reality-generative-text-and-hallucinations)'
