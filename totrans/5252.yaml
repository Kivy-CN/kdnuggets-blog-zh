- en: 'PySpark SQL Cheat Sheet: Big Data in Python'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html](https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**By Karlijn Willems, [DataCamp](https://www.datacamp.com/).**'
  prefs: []
  type: TYPE_NORMAL
- en: Big Data With Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Big data is everywhere and is traditionally characterized by three V’s: Velocity,
    Variety and Volume. Big data is fast, is varied and has a huge volume. As a data
    scientist, data engineer, data architect, ... or whatever the role is that you’ll
    assume in the data science industry, you’ll definitely get in touch with big data
    sooner or later, as companies now gather an enormous amount of data across the
    board.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Data doesn’t always mean information, though, and that is where you, data science
    enthusiast, come in. You can make use of Apache Spark, “a fast and general engine
    for large-scale data processing” to start to tackle the challenges that big data
    poses to you and the company you’re working for.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, doubts may always arise when you’re working with Spark and when
    they do, take a look at DataCamp’s [Apache Spark Tutorial: ML with PySpark tutorial](https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning)
    or download the [cheat sheet](https://www.datacamp.com/community/blog/pyspark-sql-cheat-sheet)
    for free!'
  prefs: []
  type: TYPE_NORMAL
- en: In what follows, we’ll dive deeper into the structure and the contents of the
    cheat sheet.
  prefs: []
  type: TYPE_NORMAL
- en: PySpark Cheat Sheet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PySpark is the Spark Python API exposes the Spark programming model to Python.
    Spark SQL, then, is a module of PySpark that allows you to work with structured
    data in the form of DataFrames. This stands in contrast to RDDs, which are typically
    used to work with unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: '![PySpark cheat sheet](../Images/f31b5ce6452a08f76098ca13c56b14cb.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Tip:** if you want to learn more about the differences between RDDs and DataFrames,
    but also about how Spark DataFrames differ from pandas DataFrames, you should
    definitely check out the [Apache Spark in Python: Beginner''s Guide](https://www.datacamp.com/community/tutorials/apache-spark-python).'
  prefs: []
  type: TYPE_NORMAL
- en: Initializing SparkSession
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you want to start working with Spark SQL with PySpark, you’ll need to start
    a SparkSession first: you can use this to create DataFrames, register DataFrames
    as tables, execute SQL over the tables and read parquet files. Don’t worry if
    all of this sounds very new to you - You’ll read more about this later on in this
    article!'
  prefs: []
  type: TYPE_NORMAL
- en: You start a SparkSession by first importing it from the sql module that comes
    with the pyspark package. Next, you can initialize a variable spark, for example,
    to not only build the SparkSession, but also give the application a name, set
    the config and then use the getOrCreate() method to either get a SparkSession
    if there is already one running or to create one if that isn’t the case yet! That
    last method will come in extremely handy, especially for future reference, because
    it will prevent you from running multiple SparkSessions at the same time!
  prefs: []
  type: TYPE_NORMAL
- en: Cool, isn’t it?
  prefs: []
  type: TYPE_NORMAL
- en: '![PySpark cheat sheet](../Images/3c86b85673b094ab3ed9a4afa5142260.png)'
  prefs: []
  type: TYPE_IMG
- en: Inspecting the Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you have imported your data, it’s time to inspect the Spark DataFrame by
    using some of the built-in attributes and methods. This way, you’ll get to know
    your data a little bit better before you start manipulating the DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you’ll probably already know most of the methods and attributes mentioned
    in this section of the cheat sheet  from working with pandas DataFrames or NumPy,
    such as dtypes, head(), describe(), count(),... There are also some methods that
    might be new to you, such as the take() or printSchema() method, or the schema
    attribute.
  prefs: []
  type: TYPE_NORMAL
- en: '![PySpark cheat sheet](../Images/2b87b5681e4fc864b638d73298521c50.png)'
  prefs: []
  type: TYPE_IMG
- en: Nevertheless, you’ll see that the ramp-up is quite modest, as you can leverage
    your previous knowledge on data science packages maximally.
  prefs: []
  type: TYPE_NORMAL
- en: Duplicate Values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you’re inspecting your data, you might find that there are some duplicate
    values. To remediate this, you can use the dropDuplicates() method, for example,
    to drop duplicate values in your Spark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Queries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You might remember that one of the main reasons for using Spark DataFrames in
    the first place is that you have a more structured way of dealing with your data
    - Querying is one of those examples of handling your data in a more structured
    way, whether you’re using SQL in a relational database, SQL-like language in No-SQL
    databases, the [query()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html?highlight=query#pandas.DataFrame.query)
    method on Pandas DataFrames, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![PySpark cheat sheet](../Images/8a57af6b8937cff2c56c8dfbf857efe5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we’re talking about Pandas DataFrames, you’ll notice that Spark DataFrames
    follow a similar principle: you’re using methods to get to know your data better.
    In this case, though, you won’t use query(). Instead, you’ll have to make use
    of other methods to get the results you want to get back: in a first instance,
    select() and show() will be your friends whenever you want to retrieve information
    from your DataFrames.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Within these methods, you can build up your query. As with standard SQL, you
    specify what columns you exactly want to get back within select(). You can do
    this with a regular string or by specifying the column name with the help of your
    DataFrame itself, as in the following example: df.lastName or df[“firstName”].'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the latter approach will give you some more freedom in some cases
    where you want to specify what information you exactly want to retrieve with additional
    functions such as isin() or startswith(), for example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides select(), you can also make use of the functions module in PySpark
    SQL to specify, for example, a when clause in your query, as demonstrated in the
    table below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PySpark cheat sheet](../Images/d5234f75f8ad26d04f778d4b0bbf5d52.png)'
  prefs: []
  type: TYPE_IMG
- en: All in all, you can see that there are a lot of possibilities for those who
    want to handle and get to know their data to a larger extent with the help of
    select(), help() and the functions of the functions module.
  prefs: []
  type: TYPE_NORMAL
- en: Add, Update & Remove Columns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You might also want to look into adding, updating or removing some columns from
    your Spark DataFrame. You can easily do this with the withColumn(), withColumnRenamed()
    and drop() methods. You’ll probably know by now that you also have a drop() method
    at your disposal when you’re working with Pandas DataFrames. You can read more
    [here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html?highlight=drop#pandas.DataFrame.drop).
  prefs: []
  type: TYPE_NORMAL
- en: '![PySpark cheat sheet](../Images/751902d5d047e7357ce04767eb6020aa.png)'
  prefs: []
  type: TYPE_IMG
- en: GroupBy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Again, just like with Pandas DataFrames, you might want to group by certain
    values and aggregate some values - The example below shows how you use the groupBy()
    method, in combination with count() and show() to retrieve and show your data,
    grouped by age, together with the number of people who have that certain age.
  prefs: []
  type: TYPE_NORMAL
- en: '![PySpark cheat sheet](../Images/283cb11c84f4170ae04c5534ce466532.png)'
  prefs: []
  type: TYPE_IMG
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PySpark for Data Science](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Validation for PySpark Applications using Pandera](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Cleaning with Python Cheat Sheet](https://www.kdnuggets.com/2023/02/data-cleaning-python-cheat-sheet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Best Python Tools for Building Generative AI Applications Cheat Sheet](https://www.kdnuggets.com/2023/08/best-python-tools-generative-ai-cheat-sheet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python Control Flow Cheat Sheet](https://www.kdnuggets.com/2022/11/python-control-flow-cheatsheet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, July 5: A Rotten Data Science Project • 10 AI…](https://www.kdnuggets.com/2023/n24.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
