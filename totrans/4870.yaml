- en: 'Understanding Feature Engineering: Deep Learning Methods for Text Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解特征工程：文本数据的深度学习方法
- en: 原文：[https://www.kdnuggets.com/2018/03/understanding-feature-engineering-deep-learning-methods-text-data.html](https://www.kdnuggets.com/2018/03/understanding-feature-engineering-deep-learning-methods-text-data.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/03/understanding-feature-engineering-deep-learning-methods-text-data.html](https://www.kdnuggets.com/2018/03/understanding-feature-engineering-deep-learning-methods-text-data.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**Editor''s note:** This post is only one part of a far more thorough and in-depth
    original, [found here](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa),
    which covers much more than what is included here.'
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**编辑说明：** 这篇文章只是一个更为详尽和深入原文的一部分，[原文链接](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)，涵盖了比这里更多的内容。'
- en: '![Header image](../Images/aefd6a27ee58fbc6ac8f6ff555b80662.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![Header image](../Images/aefd6a27ee58fbc6ac8f6ff555b80662.png)'
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引言
- en: 'Working with unstructured text data is hard especially when you are trying
    to build an intelligent system which interprets and understands free flowing natural
    language just like humans. You need to be able to process and transform noisy,
    unstructured textual data into some structured, vectorized formats which can be
    understood by any machine learning algorithm. Principles from Natural Language
    Processing, Machine Learning or Deep Learning all of which fall under the broad
    umbrella of Artificial Intelligence are effective tools of the trade. Based on
    my previous posts, an important point to remember here is that any machine learning
    algorithm is based on principles of statistics, math and optimization. Hence they
    are not intelligent enough to start processing text in their raw, native form.
    We covered some traditional strategies for extracting meaningful features from
    text data in [***Part-3: Traditional Methods for Text Data***](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41)***. ***I
    encourage you to check out the same for a brief refresher. In this article, we
    will be looking at more advanced [feature engineering](https://www.kdnuggets.com/2018/12/feature-engineering-explained.html)
    strategies which often leverage deep learning models. More specifically we will
    be covering the [**Word2Vec**](https://en.wikipedia.org/wiki/Word2vec), [**GloVe**](https://nlp.stanford.edu/projects/glove/)and [**FastText**](https://research.fb.com/fasttext/)
    models.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 处理非结构化文本数据非常困难，尤其是当你试图构建一个智能系统，该系统像人类一样解释和理解自然语言时。你需要能够将嘈杂的、非结构化的文本数据处理并转换成一些结构化的、向量化的格式，以便任何机器学习算法都能理解。自然语言处理、机器学习或深度学习的原理，都属于人工智能这个大范畴内的有效工具。基于我之前的文章，需要记住的一个重要点是，任何机器学习算法都基于统计学、数学和优化的原理。因此，它们不足以直接处理文本的原始、自然形式。我们在[***第3部分：文本数据的传统方法***](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41)中介绍了一些提取有意义特征的传统策略。***我鼓励你查看该部分以获得简要的回顾。在本文中，我们将探讨更多先进的[特征工程](https://www.kdnuggets.com/2018/12/feature-engineering-explained.html)策略，这些策略通常利用深度学习模型。更具体地说，我们将涵盖[**Word2Vec**](https://en.wikipedia.org/wiki/Word2vec)、[**GloVe**](https://nlp.stanford.edu/projects/glove/)和[**FastText**](https://research.fb.com/fasttext/)模型。
- en: Motivation
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动机
- en: We have discussed time and again including in [***our previous article***](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41) that
    Feature Engineering is the secret sauce to creating superior and better performing
    machine learning models. Always remember that even with the advent of automated
    feature engineering capabilities, you would still need to understand the core
    concepts behind applying the techniques. Otherwise they would just be black box
    models which you wouldn’t know how to tweak and tune for the problem you are trying
    to solve.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经反复讨论，包括在[***我们之前的文章***](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41)中提到，特征工程是创建优质和更高性能的机器学习模型的秘密武器。请始终记住，即使有了自动化特征工程的能力，你仍然需要理解应用这些技术的核心概念。否则，它们将只是黑箱模型，你将不知道如何调整和优化以解决你面临的问题。
- en: '**Shortcomings of traditional models**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**传统模型的不足**'
- en: Traditional (count-based) feature engineering strategies for textual data involve
    models belonging to a family of models popularly known as the Bag of Words model.
    This includes term frequencies, TF-IDF (term frequency-inverse document frequency),
    N-grams and so on. While they are effective methods for extracting features from
    text, due to the inherent nature of the model being just a bag of unstructured
    words, we lose additional information like the semantics, structure, sequence
    and context around nearby words in each text document. This forms as enough motivation
    for us to explore more sophisticated models which can capture this information
    and give us features which are vector representation of words, popularly known
    as embeddings.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的（基于计数的）文本数据特征工程策略包括属于一种被广泛称为词袋模型的模型家族。这包括术语频率、TF-IDF（术语频率-逆文档频率）、N-gram等。虽然它们是从文本中提取特征的有效方法，但由于模型固有的特性是仅仅一个无结构的单词袋，我们丧失了诸如语义、结构、序列和上下文等附加信息。这为我们探索能够捕捉这些信息并提供词向量表示的更复杂模型提供了足够的动机，这些词向量表示被称为嵌入。
- en: '**The need for word embeddings**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**词嵌入的必要性**'
- en: While this does make some sense, why should we be motivated enough to learn
    and build these word embeddings? With regard to speech or image recognition systems,
    all the information is already present in the form of rich dense feature vectors
    embedded in high-dimensional datasets like audio spectrograms and image pixel
    intensities. However when it comes to raw text data, especially count based models
    like Bag of Words, we are dealing with individual words which may have their own
    identifiers and do not capture the semantic relationship amongst words. This leads
    to huge sparse word vectors for textual data and thus if we do not have enough
    data, we may end up getting poor models or even overfitting the data due to the
    curse of dimensionality.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这确实有一些道理，但我们为什么要有足够的动力去学习和构建这些词嵌入呢？对于语音或图像识别系统，所有信息已经以高维数据集中的丰富密集特征向量的形式存在，比如音频谱图和图像像素强度。然而，当涉及到原始文本数据时，特别是像词袋模型这样的基于计数的模型时，我们处理的是单个单词，它们可能有自己的标识符，并不能捕捉单词之间的语义关系。这会导致文本数据的稀疏词向量，因此如果我们没有足够的数据，可能会得到较差的模型，甚至由于维度灾难而导致过拟合数据。
- en: '![](../Images/40ca1b5efc5f8f11dd7c2cef40e687c7.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40ca1b5efc5f8f11dd7c2cef40e687c7.png)'
- en: Comparing feature representations for audio, image and textTo overcome the shortcomings
    of losing out semantics and feature sparsity in bag of words model based features,
    we need to make use of [***Vector Space Models (VSMs)***](https://en.wikipedia.org/wiki/Vector_space_model)in
    such a way that we can embed word vectors in this continuous vector space based
    on semantic and contextual similarity. In fact the [***distributional hypothesis***](https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis) in
    the field of [***distributional semantics***](https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis) tells
    us that words which occur and are used in the same context, are semantically similar
    to one another and have similar meanings. In simple terms, *‘a word is characterized
    by the company it keeps’*. One of the famous papers talking about these semantic
    word vectors and various types in detail is [*‘Don’t count, predict! A systematic
    comparison of context-counting vs. context-predicting semantic vectors’ *by Baroni
    et al](http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf).
    We won’t go into extensive depth but in short, there are two main types of methods
    for contextual word vectors. ***Count-based methods*** like [***Latent Semantic
    Analysis (LSA)***](https://en.wikipedia.org/wiki/Latent_semantic_analysis)which
    can be used to compute some statistical measures of how often words occur with
    their neighboring words in a corpus and then building out dense word vectors for
    each word from these measures. ***Predictive methods*** like [***Neural Network
    based language models***](http://www.scholarpedia.org/article/Neural_net_language_models) try
    to predict words from its neighboring words looking at word sequences in the corpus
    and in the process it learns distributed representations giving us dense word
    embeddings. We will be focusing on these predictive methods in this article.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 比较音频、图像和文本的特征表示。为了克服词袋模型基于特征的语义丧失和特征稀疏的问题，我们需要利用[***向量空间模型 (VSMs)***](https://en.wikipedia.org/wiki/Vector_space_model)，以便根据语义和上下文相似性将词向量嵌入到这个连续向量空间中。实际上，[***分布假设***](https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis)在[***分布语义学***](https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis)领域告诉我们，出现在相同上下文中的词在语义上是相似的，具有相似的意义。简单来说，*“一个词的特征由它的搭配词决定”*。有一篇著名的论文详细讨论了这些语义词向量及其各种类型，[*“不要计数，预测！语境计数与语境预测语义向量的系统比较”*](http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf)由Baroni等人撰写。我们不会深入探讨，但简而言之，针对上下文词向量有两种主要方法。***基于计数的方法***，例如[***潜在语义分析
    (LSA)***](https://en.wikipedia.org/wiki/Latent_semantic_analysis)，可以用来计算词在语料库中与邻近词一起出现的频率，并从这些度量中构建每个词的密集词向量。***预测方法***，如[***基于神经网络的语言模型***](http://www.scholarpedia.org/article/Neural_net_language_models)，试图从邻近词中预测词，观察语料库中的词序列，在这个过程中学习分布式表示，给出密集的词嵌入。我们将在本文中重点关注这些预测方法。
- en: Feature Engineering Strategies
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征工程策略
- en: Let’s look at some of these advanced strategies for handling text data and extracting
    meaningful features from the same, which can be used in downstream machine learning
    systems. Do note that you can access all the code used in this article in [**my
    GitHub repository**](https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/bonus%20content/feature%20engineering%20text%20data) also
    for future reference. We’ll start by loading up some basic dependencies and settings.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些处理文本数据和提取有意义特征的高级策略，这些特征可以用于下游机器学习系统。请注意，你可以在[**我的 GitHub 仓库**](https://github.com/dipanjanS/practical-machine-learning-with-python/tree/master/bonus%20content/feature%20engineering%20text%20data)中访问本文中使用的所有代码，以便将来参考。我们将从加载一些基本依赖项和设置开始。
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will now take a few corpora of documents on which we will perform all our
    analyses. For one of the corpora, we will reuse our corpus from our previous article, [***Part-3:
    Traditional Methods for Text Data***](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41).
    We mention the code as follows for ease of understanding.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将采用一些文档语料库，对其进行所有分析。对于其中一个语料库，我们将重用之前文章中的语料库，[***第3部分：传统文本数据方法***](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41)。我们将为方便理解提到代码。
- en: '![](../Images/44fbb13a0bedc1a2e7f4026c47ee522c.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44fbb13a0bedc1a2e7f4026c47ee522c.png)'
- en: Our sample text corpusOur toy corpus consists of documents belonging to several
    categories. Another corpus we will use in this article is the [***The King James
    Version of the Bible***](https://www.gutenberg.org/files/10/10-h/10-h.htm) available
    freely from [***Project Gutenberg***](https://www.gutenberg.org/) through the `corpus` module
    in `nltk`. We will load this up shortly, in the next section. Before we talk about
    feature engineering, we need to pre-process and normalize this text.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '**Text pre-processing**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: There can be multiple ways of cleaning and pre-processing textual data. The
    most important techniques which are used heavily in Natural Language Processing
    (NLP) pipelines have been highlighted in detail in the ***‘Text pre-processing’*** section
    in [***Part 3 of this series***](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41).
    Since the focus of this article is on feature engineering, just like our previous
    article, we will re-use our simple text pre-processor which focuses on removing
    special characters, extra whitespaces, digits, stopwords and lower casing the
    text corpus.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Once we have our basic pre-processing pipeline ready, let’s first apply the
    same to our toy corpus.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s now load up our other corpus based on [***The King James Version of the
    Bible***](https://www.gutenberg.org/files/10/10-h/10-h.htm)using `nltk` and pre-process
    the text.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: The following output shows the total number of lines in our corpus and how the
    pre-processing works on the textual content.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Bio: [Dipanjan Sarkar](https://www.linkedin.com/in/dipanzan)** is a Data
    Scientist @Intel, an author, a mentor @Springboard, a writer, and a sports and
    sitcom addict.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa).
    Reposted with permission.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[Text Data Preprocessing: A Walkthrough in Python](/2018/03/text-data-preprocessing-walkthrough-python.html)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A General Approach to Preprocessing Text Data](/2017/12/general-approach-preprocessing-text-data.html)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Framework for Approaching Textual Data Science Tasks](/2017/11/framework-approaching-textual-data-tasks.html)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Feature Store Summit 2022: A free conference on Feature Engineering](https://www.kdnuggets.com/2022/10/hopsworks-feature-store-summit-2022-free-conference-feature-engineering.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Alternative Feature Selection Methods in Machine Learning](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding Python''s Iteration and Membership: A Guide to…](https://www.kdnuggets.com/understanding-pythons-iteration-and-membership-a-guide-to-__contains__-and-__iter__-magic-methods)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Practical Approach To Feature Engineering In Machine Learning](https://www.kdnuggets.com/2023/07/practical-approach-feature-engineering-machine-learning.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a Tractable, Feature Engineering Pipeline for Multivariate…](https://www.kdnuggets.com/2022/03/building-tractable-feature-engineering-pipeline-multivariate-time-series.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[构建一个可操作的特征工程管道用于多变量时间序列…](https://www.kdnuggets.com/2022/03/building-tractable-feature-engineering-pipeline-multivariate-time-series.html)'
- en: '[Using RAPIDS cuDF to Leverage GPU in Feature Engineering](https://www.kdnuggets.com/2023/06/rapids-cudf-leverage-gpu-feature-engineering.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 RAPIDS cuDF 利用 GPU 进行特征工程](https://www.kdnuggets.com/2023/06/rapids-cudf-leverage-gpu-feature-engineering.html)'
