- en: 7 Steps to Mastering Large Language Models (LLMs)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 掌握大型语言模型（LLMs）的7个步骤
- en: 原文：[https://www.kdnuggets.com/7-steps-to-mastering-large-language-models-llms](https://www.kdnuggets.com/7-steps-to-mastering-large-language-models-llms)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/7-steps-to-mastering-large-language-models-llms](https://www.kdnuggets.com/7-steps-to-mastering-large-language-models-llms)
- en: '![7 Steps to Mastering Large Language Models (LLMs)](../Images/ffd0f50c79e3ac26439bb6d18f708839.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![掌握大型语言模型（LLMs）的7个步骤](../Images/ffd0f50c79e3ac26439bb6d18f708839.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: GPT-4, Llama, Falcon, and many more—Large Language Models—LLMs—are literally
    the talk of the ~~town~~ year. And if you’re reading this chances are you’ve already
    used one or more of these large language models through a chat interface or an
    API.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4、Llama、Falcon等——大型语言模型（LLMs）——真的是当下的**热门话题**。如果你正在阅读这些内容，你很可能已经通过聊天界面或API使用过一个或多个这些大型语言模型。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: If you’ve ever wondered what LLMs really are, *how* they work, and *what* you
    can build with them, this guide is for you. Whether you’re a data professional
    interested in large language models or someone just curious about them, this is
    a comprehensive guide to navigating the LLM landscape.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经好奇LLMs到底是什么，*它们如何工作*，以及*你可以用它们构建什么*，这份指南将是你所需要的。不论你是对大型语言模型感兴趣的数据专业人士，还是对它们感到好奇的普通人，这都是一份全面的LLM领域指南。
- en: 'From what LLMs are to building and deploying applications with LLMs, we break
    down—into 7 easy steps—learning all about large language models covering:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从LLMs是什么到如何构建和部署应用程序，我们将分解成7个简单的步骤，帮助你全面了解大型语言模型，内容包括：
- en: What you should know
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你应该知道的事项
- en: An overview of the concepts
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概念概览
- en: Learning resources
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习资源
- en: Let’s get started!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: 'Step 1: Understanding LLM Basics'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一步：理解LLM基础知识
- en: 'If you’re new to large language models, it’s helpful to start with a high-level
    overview of LLMs and what makes them so powerful. Start by trying to answer these
    questions:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是大型语言模型的新手，首先了解LLMs的高层次概况以及它们为何如此强大是有帮助的。从尝试回答这些问题开始：
- en: What are LLMs anyways?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs究竟是什么？
- en: Why are they so popular?
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么它们如此受欢迎？
- en: How are LLMs different from other deep learning models?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs与其他深度学习模型有什么不同？
- en: What are the common LLM use cases? (You’d be familiar with this already; still
    a good exercise to list them down)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的LLM使用案例有哪些？（你可能已经熟悉这些；仍然是一个列出它们的好练习）
- en: Were you able to answer them all? Well, let’s do it together!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你能回答所有这些问题吗？好吧，让我们一起做吧！
- en: What are LLMs?
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是LLMs？
- en: Large Language Models—or  LLMs—are a subset of deep learning models **trained
    on massive corpus of text data**. They’re **large**—with tens of billions of parameters—and
    perform extremely well on a wide range of **natural language tasks**.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（Large Language Models，简称LLMs）是深度学习模型的一个子集，**经过大规模文本数据的训练**。它们非常**庞大**——拥有数百亿个参数——并且在各种**自然语言任务**上表现极为出色。
- en: Why Are They Popular?
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它们为什么这么受欢迎？
- en: 'LLMs have the ability to **understand and generate text** that is coherent,
    contextually relevant, and grammatically accurate. Reasons for their popularity
    and wide-spread adoption include:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs具备**理解和生成**连贯、上下文相关且语法准确的文本的能力。它们的受欢迎和广泛应用的原因包括：
- en: Exceptional performance on a wide range of language tasks
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在各种语言任务上表现卓越
- en: Accessibility and availability of pre-trained LLMs, democratizing AI-powered
    natural language understanding and generation
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练LLMs的可访问性和可用性，使AI驱动的自然语言理解和生成得以民主化
- en: So How Are LLMs Different from Other Deep Learning Models?
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 那么LLMs与其他深度学习模型有何不同？
- en: 'LLMs stand out from other deep learning models due to their size and architecture,
    which includes self-attention mechanisms. Key differentiators include:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其规模和架构，包括自注意力机制，LLMs与其他深度学习模型不同。关键差异包括：
- en: The **Transformer architecture**, which revolutionized natural language processing
    and underpins LLMs *(coming up next in our guide)*
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变换器架构**，它彻底改变了自然语言处理并支撑了LLMs *(在我们的指南中即将介绍)*'
- en: The ability to capture **long-range dependencies** in text, enabling **better
    contextual understanding**
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕捉**长距离依赖**的能力，能够**更好地理解上下文**
- en: Ability to handle a wide variety of language tasks, from text generation to
    translation, summarization and question-answering
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够处理各种语言任务，从文本生成到翻译、总结和问答
- en: What Are the Common Use Cases of LLMs?
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMs的常见应用案例有哪些？
- en: 'LLMs have found applications across language tasks, including:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在语言任务中得到了应用，包括：
- en: '**Natural Language Understanding**: LLMs excel at tasks like sentiment analysis,
    named entity recognition, and question answering.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言理解**：LLMs在情感分析、命名实体识别和问答等任务中表现出色。'
- en: '**Text Generation**: They can generate human-like text for chatbots and other
    content generation tasks. *(Shouldn’t be surprising at all if you’ve ever used
    ChatGPT or its alternatives).*'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本生成**：它们可以为聊天机器人和其他内容生成任务生成类人文本。*(如果你曾使用过ChatGPT或其替代品，这完全不会让你感到惊讶)*'
- en: '**Machine Translation**: LLMs have significantly improved machine translation
    quality.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器翻译**：LLMs显著提高了机器翻译的质量。'
- en: '**Content Summarization**: LLMs can generate concise summaries of lengthy documents.
    Ever tried summarizing YouTube video transcripts?'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内容总结**：LLMs可以生成长篇文档的简明总结。你是否尝试过总结YouTube视频的文字记录？'
- en: 'Now that you have a cursory overview of LLMs and their capabilities, here are
    a couple of resources if you’re interested in exploring further:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你对LLMs及其能力有了初步了解，以下是一些资源，如果你有兴趣进一步探索：
- en: '[Introduction to Generative AI](https://www.cloudskillsboost.google/journeys/118/course_templates/536)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[生成AI简介](https://www.cloudskillsboost.google/journeys/118/course_templates/536)'
- en: '[Introduction to Large Language Models](https://www.cloudskillsboost.google/journeys/118/course_templates/539)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大型语言模型简介](https://www.cloudskillsboost.google/journeys/118/course_templates/539)'
- en: 'Step 2: Exploring LLM Architectures'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤2：探索LLM架构
- en: Now that you know what LLMs are, let’s move on to learning the transformer architecture
    that underpins these powerful LLMs. So in this step of your LLM journey, **Transformers
    need all your attention** *(no pun intended)*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经了解了什么是LLMs，让我们继续学习支撑这些强大LLMs的变换器架构。所以，在LLM的学习过程中，这一步中**变换器需要你所有的注意力** *(没有任何双关意思)*
- en: 'The original Transformer architecture, introduced in the paper "[Attention
    Is All You Need](https://arxiv.org/abs/1706.03762)," revolutionized natural language
    processing:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 原始变换器架构，在论文"[Attention Is All You Need](https://arxiv.org/abs/1706.03762)"中介绍，彻底改变了自然语言处理：
- en: '**Key Features**: Self-attention layers, multi-head attention, feed-forward
    neural networks, encoder-decoder architecture.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关键特性**：自注意力层、多头注意力、前馈神经网络、编码器-解码器架构。'
- en: '**Use Cases**: Transformers are the basis for notable LLMs like BERT and GPT.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用案例**：变换器是著名的语言模型（LLMs），如BERT和GPT的基础。'
- en: 'The original Transformer architecture uses an encoder-decoder architecture;
    but encoder-only and decoder-only variants exist. Here’s a comprehensive overview
    of these along with their features, notable LLMs, and use cases:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 原始变换器架构使用编码器-解码器架构；但也存在仅编码器和仅解码器的变体。这里是这些架构的全面概述，包括它们的特性、著名LLMs和应用案例：
- en: '| **Architecture** | **Key Features** | **Notable LLMs** | **Use Cases** |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| **架构** | **关键特性** | **著名LLMs** | **应用案例** |'
- en: '| **Encoder-only** | Captures bidirectional context; suitable for natural language
    understanding |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| **仅编码器** | 捕捉双向上下文；适用于自然语言理解 |'
- en: BERT
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT
- en: Also BERT architecture based RoBERTa, XLNet
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样基于BERT架构的RoBERTa，XLNet
- en: '|'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Text classification
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类
- en: Question answering
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答系统
- en: '|'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Decoder-only** | Unidirectional language model; Autoregressive generation
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| **仅解码器** | 单向语言模型；自回归生成 |'
- en: GPT
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT
- en: PaLM
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PaLM
- en: '|'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Text generation (variety of content creation tasks)
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本生成（各种内容创建任务）
- en: Text completion
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本补全
- en: '|'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Encoder-Decoder** | Input text to target text; any text-to-text task |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| **编码器-解码器** | 输入文本到目标文本；任何文本到文本的任务 |'
- en: T5
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: T5
- en: BART
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BART
- en: '|'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Summarization
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结
- en: Translation
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 翻译
- en: Question answering
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答系统
- en: Document classification
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档分类
- en: '|'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'The following are great resources to learn about transformers:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是学习变换器的绝佳资源：
- en: '[Attention Is All You Need](https://arxiv.org/abs/1706.03762) (must read)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[《注意力机制至关重要》](https://arxiv.org/abs/1706.03762)（必读）'
- en: '[The Illustrated Transformer by Jay Alammar](http://jalammar.github.io/illustrated-transformer/)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[杰伊·阿拉马尔的《插图变换器》](http://jalammar.github.io/illustrated-transformer/)'
- en: '[Module on Modeling from Stanford CS324: Large Language Models](https://stanford-cs324.github.io/winter2022/lectures/modeling/)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[斯坦福CS324：大型语言模型中的建模模块](https://stanford-cs324.github.io/winter2022/lectures/modeling/)'
- en: '[HuggingFace Transformers Course](https://huggingface.co/learn/nlp-course/chapter1/1)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HuggingFace Transformers课程](https://huggingface.co/learn/nlp-course/chapter1/1)'
- en: 'Step 3: Pre-training LLMs'
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3步：预训练LLMs
- en: Now that you’re familiar with the fundamentals of Large Language Models (LLMs)
    and the transformer architecture, you can proceed to learn about pre-training
    LLMs. Pre-training forms the foundation of LLMs by **exposing them to a massive
    corpus of text data, enabling them to understand the aspects and nuances of the
    language**.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经熟悉了大型语言模型（LLMs）和变换器架构的基础知识，你可以继续学习LLMs的预训练。预训练为LLMs奠定基础，通过**让它们接触大量的文本数据，使它们能够理解语言的各个方面和细微差别**。
- en: 'Here’s an overview of concepts you should know:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是你应该了解的概念概览：
- en: '**Objectives of Pre-training LLMs**: Exposing LLMs to massive text corpora
    to learn language patterns, grammar, and context. Learn about the specific pre-training
    tasks, such as *masked language modeling* and *next sentence prediction*.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLMs预训练的目标**：让LLMs接触大量文本语料，以学习语言模式、语法和上下文。了解具体的预训练任务，如*掩蔽语言建模*和*下一句预测*。'
- en: '**Text Corpus for LLM Pre-training**: LLMs are trained on massive and diverse
    text corpora, including web articles, books, and other sources. These are large
    datasets—with billions to trillions of text tokens. Common datasets include C4,
    BookCorpus, Pile, OpenWebText, and more.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLMs预训练的文本语料**：LLMs在大量多样化的文本语料上进行训练，包括网页文章、书籍和其他来源。这些是大规模数据集——包含数十亿到万亿的文本标记。常见的数据集包括C4、BookCorpus、Pile、OpenWebText等。'
- en: '**Training Procedure**: Understand the technical aspects of pre-training, including
    optimization algorithms, batch sizes, and training epochs. Learn about challenges
    such as mitigating biases in data.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练过程**：理解预训练的技术细节，包括优化算法、批量大小和训练轮次。了解挑战，如减轻数据中的偏差。'
- en: 'If you’re interested in learning further, refer to the module on [LLM training](https://stanford-cs324.github.io/winter2022/lectures/training/)
    from CS324: Large Language Models.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '如果你有兴趣进一步学习，请参考CS324: 大型语言模型中的[LLM训练模块](https://stanford-cs324.github.io/winter2022/lectures/training/)。'
- en: Such pre-trained LLMs serve as a starting point for fine-tuning on specific
    tasks. Yes, fine-tuning LLMs is our next step!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预训练的LLMs作为微调特定任务的起点。是的，微调LLMs是我们的下一步！
- en: 'Step 4: Fine-Tuning LLMs'
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4步：微调LLMs
- en: After pre-training LLMs on massive text corpora, the next step is to fine-tune
    them for specific natural language processing tasks. Fine-tuning allows you to
    **adapt pre-trained models to perform specific tasks** like sentiment analysis,
    question answering, or translation with higher accuracy and efficiency.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在对LLMs进行大规模文本语料的预训练后，下一步是对其进行特定自然语言处理任务的微调。微调可以让你**使预训练模型适应执行特定任务**，如情感分析、问答或翻译，以更高的准确性和效率完成任务。
- en: Why Fine-Tune LLMs
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么要微调LLMs
- en: 'Fine-tuning is necessary for several reasons:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是必要的，原因有几个：
- en: Pre-trained LLMs have gained general language understanding but require fine-tuning
    to perform well on specific tasks. And fine-tuning helps the model learn the nuances
    of the target task.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练的LLMs已经获得了通用的语言理解，但需要微调才能在特定任务上表现良好。微调帮助模型学习目标任务的细微差别。
- en: Fine-tuning significantly reduces the amount of data and computation needed
    compared to training a model from scratch. Because it leverages the pre-trained
    model's understanding, the fine-tuning dataset can be much smaller than the pre-training
    dataset.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调相比从头训练模型显著减少了所需的数据和计算量。因为它利用了预训练模型的理解，所以微调数据集可以比预训练数据集小得多。
- en: How to Fine-Tune LLMs
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何微调LLMs
- en: 'Now let''s go over the *how* of fine-tuning LLMs:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入了解微调LLMs的*方法*：
- en: '**Choose the Pre-trained LLM:** Choose the pre-trained LLM that matches your
    task. For example, if you''re working on a question-answering task, select a pre-trained
    model with the architecture that facilitates natural language understanding.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择预训练LLM：** 选择与任务匹配的预训练LLM。例如，如果你在处理问答任务，选择一个有助于自然语言理解的预训练模型。'
- en: '**Data Preparation**: Prepare a dataset for the specific task you want the
    LLM to perform. Ensure it includes labeled examples and is formatted appropriately.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据准备**：为LLM执行的特定任务准备数据集。确保数据集包含标记的示例，并格式正确。'
- en: '**Fine-Tuning**: After you’ve chosen the base LLM and prepared the dataset,
    it’s time to actually fine-tune the model.*But how? *'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调**：在选择了基础LLM并准备好数据集之后，接下来就是真正的微调模型。*但怎么做呢？*'
- en: '*Are there parameter-efficient techniques?* Remember, LLMs have 10s of billions
    of parameters. And the weight matrix is huge!'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*有没有参数高效的技术？* 请记住，LLM有数十亿个参数。权重矩阵非常庞大！'
- en: '*What if you don’t have access to the weights? *'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你无法访问权重怎么办？*'
- en: '![7 Steps to Mastering Large Language Models (LLMs)](../Images/5865046160d7a648c041fdcb15600306.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![掌握大型语言模型（LLMs）的7个步骤](../Images/5865046160d7a648c041fdcb15600306.png)'
- en: Image by Author
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: How do you fine-tune an LLM when you don't have access to the model’s weights
    and accessing the model through an API? Large Language Models are capable of i**n-context
    learning**—without the need for an explicit fine-tuning step. you can leverage
    their ability to learn from analogy by providing input; sample output examples
    of the task.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当你无法访问模型的权重并通过API访问模型时，如何微调LLM？大型语言模型具备**上下文学习**的能力——无需明确的微调步骤。你可以通过提供输入和任务的示例输出来利用它们从类比中学习。
- en: '**Prompt tuning**—modifying the prompts to get more helpful outputs—can be:
    *hard prompt tuning* or *(soft) prompt tuning*.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示调优**——修改提示以获取更有帮助的输出——可以是：*硬提示调优*或*(软)提示调优*。'
- en: Hard prompt tuning involves modifying the input tokens in the prompt directly;
    so it doesn’t update the model's weights.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 硬提示调优涉及直接修改提示中的输入令牌，因此不会更新模型的权重。
- en: Soft prompt tuning concatenates the input embedding with a learnable tensor.
    A related idea is **prefix tuning** where learnable tensors are used with each
    Transformer block as opposed to only the input embeddings.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 软提示调优将输入嵌入与一个可学习的张量拼接在一起。一个相关的想法是**前缀调优**，在每个Transformer块中使用可学习的张量，而不仅仅是输入嵌入。
- en: As mentioned, large language models have tens of billions of parameters. So
    fine-tuning the weights in all the layers is a resource-intensive task. Recently,
    **Parameter-Efficient Fine-Tuning Techniques (PEFT**) like LoRA and QLoRA have
    become popular. With QLoRA you can fine-tune a 4-bit quantized LLM—on a single
    consumer GPU—without any drop in performance.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，大型语言模型具有数十亿个参数。因此，微调所有层中的权重是一项资源密集型任务。最近，**参数高效微调技术（PEFT**）如LoRA和QLoRA变得流行。使用QLoRA，你可以在单个消费级GPU上微调4位量化的LLM，而不会降低性能。
- en: 'These techniques introduce a small set of learnable parameters—**adapters**—are
    tuned instead of the entire weight matrix. Here are useful resources to learn
    more about fine-tuning LLMs:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术引入了一小组可学习的参数——**适配器**——而不是整个权重矩阵。以下是一些有用的资源，以了解更多关于微调LLMs的内容：
- en: '[QLoRA is all you need - Sentdex](https://www.youtube.com/watch?v=J_3hDqSvpmg)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[QLoRA是你所需的一切 - Sentdex](https://www.youtube.com/watch?v=J_3hDqSvpmg)'
- en: '[Making LLMs even more accessible with bitsandbytes, 4-bit quantization, and
    QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过bitsandbytes、4位量化和QLoRA让LLMs更易获取](https://huggingface.co/blog/4bit-transformers-bitsandbytes)'
- en: 'Step 5: Alignment and Post-Training in LLMs'
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5步：LLMs中的对齐和后训练
- en: Large Language models can potentially generate content that may be harmful,
    biased, or misaligned with what users actually want or expect. Alignment refers
    to the **process of aligning an LLM's behavior with human preferences and ethical
    principles**. It aims to mitigate risks associated with model behavior, including
    biases, controversial responses, and harmful content generation.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型可能会生成有害、偏见或与用户实际期望不符的内容。对齐是指**将LLM的行为与人类偏好和伦理原则对齐的过程**。其目的是减少模型行为相关的风险，包括偏见、争议性回应和有害内容生成。
- en: 'You can explore techniques like:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以探索以下技术：
- en: Reinforcement Learning from Human feedback (RLHF)
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从人类反馈中强化学习（RLHF）
- en: Contrastive Post-training
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对比后训练
- en: RLHF uses human preference annotations on LLM outputs and fits a reward model
    on them. Contrastive post-training aims at leveraging contrastive techniques to
    automate the construction of preference pairs.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF使用人类偏好注释来优化LLM输出，并根据这些注释拟合奖励模型。对比后训练旨在利用对比技术自动构建偏好对。
- en: '![7 Steps to Mastering Large Language Models (LLMs)](../Images/81ea1a0f7c02a2a1011c2c87822879ff.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![掌握大语言模型 (LLMs) 的7个步骤](../Images/81ea1a0f7c02a2a1011c2c87822879ff.png)'
- en: Techniques for Alignment in LLMs | [Image Source](https://arxiv.org/abs/2310.02263v1)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: LLM中的对齐技术 | [图片来源](https://arxiv.org/abs/2310.02263v1)
- en: 'To learn more, check out the following resources:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请查看以下资源：
- en: '[Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从人类反馈中阐述强化学习 (RLHF)](https://huggingface.co/blog/rlhf)'
- en: '[Contrastive Post-training Large Language Models on Data Curriculum](https://arxiv.org/abs/2310.02263v1)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[对数据课程的对比后训练大语言模型](https://arxiv.org/abs/2310.02263v1)'
- en: 'Step 6: Evaluation and Continuous Learning in LLMs'
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6步：LLM中的评估与持续学习
- en: Once you've fine-tuned an LLM for a specific task, it's essential to evaluate
    its performance and consider strategies for continuous learning and adaptation.
    This step ensures that your LLM remains effective and up-to-date.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你为特定任务微调了LLM，评估其性能并考虑持续学习和适应的策略至关重要。此步骤确保你的LLM保持有效并与时俱进。
- en: Evaluation of LLMs
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM的评估
- en: 'Evaluate the performance to assess their effectiveness and identify areas for
    improvement. Here are key aspects of LLM evaluation:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 评估性能以衡量其有效性并识别改进领域。以下是LLM评估的关键方面：
- en: '**Task-Specific Metrics**: Choose appropriate metrics for your task. For example,
    in text classification, you may use conventional evaluation metrics like accuracy,
    precision, recall, or F1 score. For language generation tasks, metrics like perplexity
    and BLEU scores are common.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务特定指标**：为你的任务选择合适的指标。例如，在文本分类中，你可以使用传统的评估指标，如准确率、精确率、召回率或F1分数。对于语言生成任务，常用的指标有困惑度和BLEU分数。'
- en: '**Human Evaluation**: Have experts or crowdsourced annotators assess the quality
    of generated content or the model''s responses in real-world scenarios.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人类评估**：让专家或众包标注员在现实场景中评估生成内容或模型的响应质量。'
- en: '**Bias and Fairness**: Evaluate LLMs for biases and fairness concerns, particularly
    when deploying them in real-world applications. Analyze how models perform across
    different demographic groups and address any disparities.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏见与公平性**：评估LLM的偏见和公平性问题，特别是在将其应用于现实世界时。分析模型在不同人口群体中的表现，并解决任何差异。'
- en: '**Robustness and Adversarial Testing**: Test the LLM''s robustness by subjecting
    it to adversarial attacks or challenging inputs. This helps uncover vulnerabilities
    and enhances model security.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**鲁棒性与对抗测试**：通过对LLM进行对抗攻击或挑战性输入来测试其鲁棒性。这有助于发现漏洞并增强模型安全性。'
- en: Continuous Learning and Adaptation
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持续学习与适应
- en: 'To keep LLMs updated with new data and tasks, consider the following strategies:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持LLM与新数据和任务的同步，考虑以下策略：
- en: '**Data Augmentation**: Continuously augment your data store to avoid performance
    degradation due to lack of up-to-date info.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据增强**：持续扩充数据存储，以避免因缺乏最新信息而导致的性能下降。'
- en: '**Retraining**: Periodically retrain the LLM with new data and fine-tune it
    for evolving tasks. Fine-tuning on recent data helps the model stay current.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重新训练**：定期用新数据重新训练LLM，并对其进行针对不断变化任务的微调。对最新数据的微调有助于模型保持最新状态。'
- en: '**Active Learning**: Implement active learning techniques to identify instances
    where the model is uncertain or likely to make errors. Collect annotations for
    these instances to refine the model.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主动学习**：实施主动学习技术以识别模型不确定或可能出错的实例。收集这些实例的注释以完善模型。'
- en: Another common pitfall with LLMs is hallucinations. Be sure to explore techniques
    like **Retrieval augmentation** to mitigate hallucinations.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的另一个常见问题是幻觉。确保探索像**检索增强**这样的技术来减轻幻觉。
- en: 'Here are some helpful resources:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些有用的资源：
- en: '[A Survey on Evaluation of large Language Models](https://arxiv.org/abs/2307.03109)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大语言模型评估综述](https://arxiv.org/abs/2307.03109)'
- en: '[Best Practices for Evaluation of RAG Applications](https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RAG应用评估的最佳实践](https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG)'
- en: 'Step 7: Building and Deploying LLM Apps'
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7步：构建和部署LLM应用
- en: After developing and fine-tuning an LLM for specific tasks, start building and
    deploying applications that leverage the LLM's capabilities. In essence, **use
    LLMs to build useful real-world solutions**.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在为特定任务开发和微调 LLM 后，开始构建和部署利用 LLM 能力的应用程序。实质上，**利用 LLM 构建有用的现实世界解决方案**。
- en: '![7 Steps to Mastering Large Language Models (LLMs)](../Images/a8cd17054947cf516f249c70cfe9901c.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![掌握大型语言模型（LLMs）的 7 个步骤](../Images/a8cd17054947cf516f249c70cfe9901c.png)'
- en: Image by Author
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Building LLM Applications
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建 LLM 应用程序
- en: 'Here are some considerations:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些考虑事项：
- en: '**Task-Specific Application Development**: Develop applications tailored to
    your specific use cases. This may involve creating web-based interfaces, mobile
    apps, chatbots, or integrations into existing software systems.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务特定的应用开发**：开发针对特定用例的应用程序。这可能涉及创建基于网页的界面、移动应用、聊天机器人或集成到现有软件系统中。'
- en: '**User Experience (UX) Design**: Focus on user-centered design to ensure your
    LLM application is intuitive and user-friendly.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户体验（UX）设计**：关注以用户为中心的设计，确保你的 LLM 应用程序直观且易于使用。'
- en: '**API Integration**: If your LLM serves as a language model backend, create
    RESTful APIs or GraphQL endpoints to allow other software components to interact
    with the model seamlessly.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API 集成**：如果你的 LLM 作为语言模型后台，创建 RESTful API 或 GraphQL 端点，以便其他软件组件能够无缝地与模型互动。'
- en: '**Scalability and Performance**: Design applications to handle different levels
    of traffic and demand. Optimize for performance and scalability to ensure smooth
    user experiences.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性和性能**：设计应用程序以处理不同级别的流量和需求。优化性能和可扩展性，以确保流畅的用户体验。'
- en: Deploying LLM Applications
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署 LLM 应用程序
- en: 'You’ve developed your LLM app and are ready to deploy them to production. Here’s
    what you should consider:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经开发了你的 LLM 应用程序，并准备将其部署到生产环境中。以下是你应该考虑的事项：
- en: '**Cloud Deployment**: Consider deploying your LLM applications on cloud platforms
    like AWS, Google Cloud, or Azure for scalability and easy management.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云部署**：考虑将 LLM 应用程序部署到 AWS、Google Cloud 或 Azure 等云平台，以实现可扩展性和便于管理。'
- en: '**Containerization**: Use containerization technologies like Docker and Kubernetes
    to package your applications and ensure consistent deployment across different
    environments.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器化**：使用 Docker 和 Kubernetes 等容器化技术打包应用程序，并确保在不同环境中的一致部署。'
- en: '**Monitoring**: Implement monitoring to track the performance of your deployed
    LLM applications and detect and address issues in real time.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控**：实施监控以跟踪已部署的 LLM 应用程序的性能，并实时检测和解决问题。'
- en: Compliance and Regulations
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合规性和法规
- en: 'Data privacy and ethical considerations are undercurrents:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 数据隐私和伦理考虑是潜在的：
- en: '**Data Privacy**: Ensure compliance with data privacy regulations when handling
    user data and personally identifiable information (PII).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据隐私**：在处理用户数据和个人身份信息（PII）时，确保遵守数据隐私法规。'
- en: '**Ethical Considerations**: Adhere to ethical guidelines when deploying LLM
    applications to mitigate potential biases, misinformation, or harmful content
    generation.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伦理考虑**：在部署 LLM 应用程序时遵循伦理指南，以减轻潜在的偏见、虚假信息或有害内容生成。'
- en: 'You can also use frameworks like [LlamaIndex](/build-your-own-pandasai-with-llamaindex)
    and [LangChain](/2023/04/langchain-101-build-gptpowered-applications.html) to
    help you build end-to-end LLM applications. Some useful resources:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用像 [LlamaIndex](/build-your-own-pandasai-with-llamaindex) 和 [LangChain](/2023/04/langchain-101-build-gptpowered-applications.html)
    这样的框架来帮助你构建端到端的 LLM 应用程序。以下是一些有用的资源：
- en: '[Full Stack LLM Bootcamp](https://fullstackdeeplearning.com/llm-bootcamp/)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[全栈 LLM 训练营](https://fullstackdeeplearning.com/llm-bootcamp/)'
- en: '[Development with Large Language Models - freeCodeCamp](https://www.youtube.com/watch?v=xZDB1naRUlk)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用大型语言模型进行开发 - freeCodeCamp](https://www.youtube.com/watch?v=xZDB1naRUlk)'
- en: Wrapping Up
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started our discussion by defining what large language models are, why they
    are popular, and gradually delved into the technical aspects. We’ve wrapped up
    our discussion with building and deploying LLM applications requiring careful
    planning, user-focused design, robust infrastructure, while prioritizing data
    privacy and ethics.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过定义大型语言模型是什么、为什么它们受到欢迎开始讨论，逐渐深入技术方面。我们总结了构建和部署 LLM 应用程序的讨论，这需要仔细规划、以用户为中心的设计、强大的基础设施，同时优先考虑数据隐私和伦理。
- en: As you might have realized, it’s important to stay updated with the recent advances
    in the field and keep building projects. If you have some experience and natural
    language processing, this guide builds on the foundation. Even if not, no worries.
    We’ve got you covered with our [7 Steps to Mastering Natural Language Processing](/7-steps-to-mastering-natural-language-processing)
    guide. Happy learning!
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经意识到的，跟进该领域的最新进展并不断构建项目是重要的。如果你有一些自然语言处理的经验，本指南将在此基础上进行深入探讨。即使没有经验，也不用担心。我们的[7个步骤掌握自然语言处理](/7-steps-to-mastering-natural-language-processing)指南将帮助你。祝学习愉快！
- en: '**[](https://twitter.com/balawc27)**[Bala Priya C](https://www.kdnuggets.com/wp-content/uploads/bala-priya-author-image-update-230821.jpg)****
    is a developer and technical writer from India. She likes working at the intersection
    of math, programming, data science, and content creation. Her areas of interest
    and expertise include DevOps, data science, and natural language processing. She
    enjoys reading, writing, coding, and coffee! Currently, she''s working on learning
    and sharing her knowledge with the developer community by authoring tutorials,
    how-to guides, opinion pieces, and more. Bala also creates engaging resource overviews
    and coding tutorials.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**[](https://twitter.com/balawc27)**[Bala Priya C](https://www.kdnuggets.com/wp-content/uploads/bala-priya-author-image-update-230821.jpg)****
    是来自印度的开发者和技术作家。她喜欢在数学、编程、数据科学和内容创作的交叉点工作。她的兴趣和专业领域包括DevOps、数据科学和自然语言处理。她喜欢阅读、写作、编码和喝咖啡！目前，她正在通过编写教程、如何做指南、观点文章等，学习并与开发者社区分享她的知识。Bala还制作引人入胜的资源概述和编码教程。'
- en: More On This Topic
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[7 Steps to Mastering Large Language Model Fine-tuning](https://www.kdnuggets.com/7-steps-to-mastering-large-language-model-fine-tuning)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握大型语言模型微调的7个步骤](https://www.kdnuggets.com/7-steps-to-mastering-large-language-model-fine-tuning)'
- en: '[Top Open Source Large Language Models](https://www.kdnuggets.com/2022/09/john-snow-top-open-source-large-language-models.html)'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[顶级开源大型语言模型](https://www.kdnuggets.com/2022/09/john-snow-top-open-source-large-language-models.html)'
- en: '[More Free Courses on Large Language Models](https://www.kdnuggets.com/2023/06/free-courses-large-language-models.html)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[更多免费的大型语言模型课程](https://www.kdnuggets.com/2023/06/free-courses-large-language-models.html)'
- en: '[Learn About Large Language Models](https://www.kdnuggets.com/2023/03/learn-large-language-models.html)'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[了解大型语言模型](https://www.kdnuggets.com/2023/03/learn-large-language-models.html)'
- en: '[Introducing Healthcare-Specific Large Language Models from John Snow Labs](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍来自John Snow Labs的医疗保健专用大型语言模型](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
- en: '[What are Large Language Models and How Do They Work?](https://www.kdnuggets.com/2023/05/large-language-models-work.html)'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大型语言模型是什么以及它们如何工作？](https://www.kdnuggets.com/2023/05/large-language-models-work.html)'
