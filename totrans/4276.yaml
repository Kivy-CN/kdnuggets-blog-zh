- en: Essential Linear Algebra for Data Science and Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/05/essential-linear-algebra-data-science-machine-learning.html](https://www.kdnuggets.com/2021/05/essential-linear-algebra-data-science-machine-learning.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8c614d6b4ed0ff4641960dcd8149bfd.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Image by Benjamin O. Tayo.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear Algebra is a branch of mathematics that is extremely useful in data
    science and machine learning. Linear algebra is the most important math skill in
    machine learning. Most machine learning models can be expressed in matrix form.
    A dataset itself is often represented as a matrix. Linear algebra is used in data
    preprocessing, data transformation, and model evaluation. Here are the topics
    you need to be familiar with:'
  prefs: []
  type: TYPE_NORMAL
- en: Vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transpose of a matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inverse of a matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determinant of a matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trace of a matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dot product
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eigenvalues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eigenvectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this article, we illustrate the application of linear algebra in data science
    and machine learning using the tech stocks dataset, which can be found [here](https://github.com/bot13956/datasets/blob/master/tech-stocks-04-2021.csv).
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Linear Algebra for Data Preprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We begin by illustrating how linear algebra is used in data preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: '**1.1 Import necessary libraries for linear algebra**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**1.2 Read dataset and display features**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3f8596b4af9ce4d5551b74bde933e2a0.png)'
  prefs: []
  type: TYPE_IMG
- en: '* **Table** **1**. Stock prices for selected stock prices for the first 16
    days in April 2021.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The ***data.shape*** function enables us to know the size of our dataset. In
    this case, the dataset has 5 features (date, AAPL, TSLA, GOOGL, and AMZN), and
    each feature has 11 observations. *Date* refers to the trading days in April 2021
    (up to April 16). AAPL, TSLA, GOOGL, and AMZN are the closing stock prices for
    Apple, Tesla, Google, and Amazon, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '**1.3 Data visualization**'
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform data visualization, we would need to define ***column matrices***
    for the features to be visualized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/90e285abeb88fd588a8cf16e1d47e81b.png)'
  prefs: []
  type: TYPE_IMG
- en: '***Figure** **1**. Tesla stock price for first 16 days in April 2021.*'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Covariance Matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The ***covariance matrix*** is one of the most important matrices in data science
    and machine learning. It provides information about co-movement (correlation)
    between features. Suppose we have a features matrix with *4* features and *n *observations
    as shown in **Table 2**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4aa34b2531b17268d1cb8c0401300f7.png)'
  prefs: []
  type: TYPE_IMG
- en: '***Table 2**. Features matrix with 4 variables and n observations.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'To visualize the correlations between the features, we can generate a scatter
    pairplot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/464fc9beacbdf3dbcaba2e5f4d869f6e.png)'
  prefs: []
  type: TYPE_IMG
- en: '***Figure 2**. Scatter pairplot for selected tech stocks.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'To quantify the degree of correlation between features (multicollinearity),
    we can compute the covariance matrix using this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/578f937c46c21a5f644a76465f132eed.png)'
  prefs: []
  type: TYPE_IMG
- en: where  and  are the mean and standard deviation of feature , respectively. This
    equation indicates that when features are standardized, the covariance matrix
    is simply the ***dot product*** between features.
  prefs: []
  type: TYPE_NORMAL
- en: 'In matrix form, the covariance matrix can be expressed as a 4 x 4 real and
    symmetric matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6ceeaef5f815f6bc570010b312b9afd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This matrix can be diagonalized by performing a ***unitary transformation***,
    also referred to as Principal Component Analysis (PCA) transformation, to obtain
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23459b77a76392254d9bc68316d712e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the ***trace of a matrix*** remains invariant under a unitary transformation,
    we observe that the sum of the eigenvalues of the diagonal matrix is equal to
    the total variance contained in features X[1], X[2], X[3], and X[4].
  prefs: []
  type: TYPE_NORMAL
- en: '**2.1 Computing the covariance matrix for tech stocks**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note that this uses the ***transpose*** of the standardized matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '**2.2 Visualization of covariance matrix**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9edc8225e01955301f05cdb7716b6073.png)'
  prefs: []
  type: TYPE_IMG
- en: '***Figure 3**. Covariance matrix plot for selected tech stocks.*'
  prefs: []
  type: TYPE_NORMAL
- en: We observe from Figure 3 that AAPL correlates strongly with GOOGL and AMZN,
    and weakly with TSLA. TSLA correlates generally weakly with AAPL, GOOGL and AMZN,
    while AAPL, GOOGL, and AMZN correlate strongly among each other.
  prefs: []
  type: TYPE_NORMAL
- en: '**2.3 Compute eigenvalues of the covariance matrix**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We observe that the trace of the covariance matrix is equal to the sum of the
    eigenvalues as expected.
  prefs: []
  type: TYPE_NORMAL
- en: '**2.4 Compute the cumulative variance**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the trace of a matrix remains invariant under a unitary transformation,
    we observe that the sum of the eigenvalues of the diagonal matrix is equal to
    the total variance contained in features X[1], X[2], X[3], and X[4]. Hence, we
    can define the following quantities:'
  prefs: []
  type: TYPE_NORMAL
- en: '** ![](../Images/570a08d35b51f049cb7756615db848e7.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that when *p* = 4, the cumulative variance becomes equal to 1 as expected.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We observe from the cumulative variance (***cum_var***) that 85% of the variance
    is contained in the first eigenvalue and 11% in the second. This means when PCA
    is implemented, only the first two principal components could be used, as 97%
    of the total variance is contributed by these 2 components. This can essentially
    reduce the dimensionally of the feature space from 4 to 2 when PCA is implemented.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Linear Regression Matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose we have a dataset that has 4 predictor features and *n* observations,
    as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4c31adfc233ea53bc3f14a4d758bd4c.png)'
  prefs: []
  type: TYPE_IMG
- en: '***Table 3**. Features matrix with 4 variables and n observations. Column 5
    is the target variable (y).*'
  prefs: []
  type: TYPE_NORMAL
- en: We would like to build a multi-regression model for predicting the *y* values
    (column 5). Our model can thus be expressed in the form
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/980b0eef242c79df297da7769f205d77.png)'
  prefs: []
  type: TYPE_IMG
- en: In matrix form, this equation can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/418c2a47995456c4c75ca9f523838474.png)'
  prefs: []
  type: TYPE_IMG
- en: where **X** is the ( n x 4) features matrix, **w** is the (4 x 1) matrix representing
    the regression coefficients to be determined, and **y** is the (n x 1) matrix
    containing the n observations of the target variable y.
  prefs: []
  type: TYPE_NORMAL
- en: Note that **X** is a rectangular matrix, so we can’t solve the equation above
    by taking the inverse of **X**.
  prefs: []
  type: TYPE_NORMAL
- en: To convert **X** into a square matrix, we multiple the left-hand side and right-hand
    side of our equation by the ***transpose***of **X**, that is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/771cf589405282fb9fb9555ce2c32559.png)'
  prefs: []
  type: TYPE_IMG
- en: This equation can also be expressed as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/688e45bebf95be9023c1f7498567013c.png)'
  prefs: []
  type: TYPE_IMG
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7f084abfc4ac0ac2d3bf3d7b5f993eba.png)'
  prefs: []
  type: TYPE_IMG
- en: is the (4×4) regression matrix. Clearly, we observe that **R** is a real and
    symmetric matrix. Note that in linear algebra, the transpose of the product of
    two matrices obeys the following relationship
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/66b2af9229c447994a4bcf012c0d2ea5.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we’ve reduced our regression problem and expressed it in terms of the
    (4×4) real, symmetric, and invertible regression matrix **R**, it is straightforward
    to show that the exact solution of the regression equation is then
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aaf18c0cbb6dd290099f5bb849faecc5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Examples of regression analysis for predicting continuous and discrete variables
    are given in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Linear Regression Basics for Absolute Beginners](https://pub.towardsai.net/linear-regression-basics-for-absolute-beginners-68ed9ff980ae)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Building a Perceptron Classifier Using the Least Squares Method](https://github.com/bot13956/perceptron_classifier)'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Linear Discriminant Analysis Matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another example of a real and symmetric matrix in data science is the Linear
    Discriminant Analysis (LDA) matrix. This matrix can be expressed in the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a8c40240eba3d7f0904f655ced9cd506.png)'
  prefs: []
  type: TYPE_IMG
- en: where **S[W]** is the within-feature scatter matrix, and **S[B ]**is the between-feature
    scatter matrix. Since both matrices **S[W] **and** S[B] **are real and symmetric,
    it follows that **L** is also real and symmetric. The diagonalization of **L** produces
    a feature subspace that optimizes class separability and reduces dimensionality.
    Hence LDA is a supervised algorithm, while PCA is not.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more details about the implementation of LDA, please see the following
    references:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Machine Learning: Dimensionality Reduction via Linear Discriminant Analysis](https://medium.com/towards-artificial-intelligence/machine-learning-dimensionality-reduction-via-linear-discriminant-analysis-cc96b49d2757)'
  prefs: []
  type: TYPE_NORMAL
- en: '[GitHub repository for LDA implementation using Iris dataset](https://github.com/bot13956/linear-discriminant-analysis-iris-dataset)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Python Machine Learning by Sebastian Raschka, 3rd Edition (Chapter 5)](https://github.com/rasbt/python-machine-learning-book-3rd-edition)'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In summary, we’ve discussed several applications of linear algebra in data science
    and machine learning. Using the tech stocks dataset, we illustrated important
    concepts such as the size of a matrix, column matrices, square matrices, covariance
    matrix, transpose of a matrix, eigenvalues, dot products, etc. Linear algebra
    is an essential tool in data science and machine learning. Thus, beginners interested
    in data science must familiarize themselves with essential concepts in linear
    algebra.
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[How To Overcome The Fear of Math and Learn Math For Data Science](https://www.kdnuggets.com/2021/03/overcome-fear-learn-math-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Essential Math for Data Science: Introduction to Matrices and the Matrix Product](https://www.kdnuggets.com/2021/02/essential-math-data-science-matrices-matrix-product.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Matrix Decomposition Decoded](https://www.kdnuggets.com/2020/12/matrix-decomposition-decoded.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top 3 Free Resources to Learn Linear Algebra for Machine Learning](https://www.kdnuggets.com/2022/03/top-3-free-resources-learn-linear-algebra-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
