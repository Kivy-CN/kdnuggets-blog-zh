- en: GPT-4 Details Have Been Leaked!
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-4 细节已泄露！
- en: 原文：[https://www.kdnuggets.com/2023/07/gpt4-details-leaked.html](https://www.kdnuggets.com/2023/07/gpt4-details-leaked.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/07/gpt4-details-leaked.html](https://www.kdnuggets.com/2023/07/gpt4-details-leaked.html)
- en: '![GPT-4 Details Have Been Leaked!](../Images/1067a83c47c992ecaed5b9b38e3cedd7.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![GPT-4 细节已泄露！](../Images/1067a83c47c992ecaed5b9b38e3cedd7.png)'
- en: Image by Editor
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑提供的图片
- en: A lot of people have been wondering what makes GPT-4 so much better than GPT-3\.
    It has taken the world by storm. It is the most anticipated AI model currently,
    and people wanted to know more about it. OpenAI did not release anything regarding
    GPT-4, for example, the size, data, internal structure, or how they trained and
    built it. We’ve all been wondering as to why they have been concealing this information.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 很多人一直在想是什么让 GPT-4 比 GPT-3 更优秀。它在全球范围内引起了轰动。它是目前最受期待的 AI 模型，人们想了解更多。OpenAI 并没有发布任何关于
    GPT-4 的信息，例如其规模、数据、内部结构，或者他们如何训练和构建它。我们都在想他们为什么要隐瞒这些信息。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Well, you’re about to find out because the details on GPT-4 have been [leaked](https://archive.is/2RQ8X#selection-833.1-873.202)!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，你马上就会知道，因为关于 GPT-4 的细节已被[泄露](https://archive.is/2RQ8X#selection-833.1-873.202)！
- en: So what details have we found out about GPT-4? Let’s dive in…
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们发现了关于 GPT-4 的哪些细节呢？让我们深入了解一下……
- en: Model Size
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型规模
- en: Large language models (LLMs) have been growing over the years, and the model
    size reflects this. In 2022, GPT-3 had a model size of 1 trillion, which is a
    [15,000x increase](https://d1.awsstatic.com/events/Summits/reinvent2022/AIM405_Train-and-deploy-large-language-models-on-Amazon-SageMaker.pdf)
    in the past 5 years. It is said that GPT-4 is 10x the size of its predecessor,
    GPT-3\. Stating that it has roughly 1.8 trillion parameters, across 120 layers.
    At 120 layers, GPT-4 is a deep architecture which is able to complex various complex
    tasks - making it one of the most advanced models out there!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）多年来不断增长，模型规模也反映了这一点。在 2022 年，GPT-3 的模型规模为 1 万亿，这在过去 5 年中增长了[15,000
    倍](https://d1.awsstatic.com/events/Summits/reinvent2022/AIM405_Train-and-deploy-large-language-models-on-Amazon-SageMaker.pdf)。据说
    GPT-4 的规模是其前身 GPT-3 的 10 倍。它大约有 1.8 万亿个参数，跨越 120 层。GPT-4 的 120 层深度结构使其能够处理各种复杂任务——使其成为最先进的模型之一！
- en: Mixture of Experts
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 专家混合
- en: OpenAI is using MOE - A mixture of experts. Unlike GPT-3 which is one static
    model, GPT is a mixture of 8 x 220-billion-parameter models. These 8 x 220B models
    were trained on different data and task distributions, utilizing 16 experts within
    their model. Each model is roughly around 111 billion parameters for multi-layer
    perceptrons, with each expert having a specific role for example coding, or formatting.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 使用的是 MOE - 专家混合。与 GPT-3 这个静态模型不同，GPT 是由 8 个 2200 亿参数的模型组成的专家混合。这 8 个
    2200 亿参数的模型是在不同的数据和任务分布上训练的，利用了模型中的 16 个专家。每个模型大约有 1110 亿个参数用于多层感知器，每个专家有特定的角色，例如编码或格式化。
- en: Mixture-of-experts is not a new thing and has been around for a while. For example,
    Google uses a mixture of experts with expert choice routing which means depending
    on what type of question you are asking, it routes you to a different expert that
    answers your questions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 专家混合并不是一个新事物，它已经存在了一段时间。例如，谷歌使用了带有专家选择路由的专家混合，这意味着根据你提出的问题类型，它会将你路由到不同的专家，以回答你的问题。
- en: GPT-4 uses roughly 55b parameters solely for 'attention', for example guiding
    the model to stay on the topic at hand.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 大约使用了 55 亿个参数仅用于“注意力”机制，例如引导模型保持在当前主题上。
- en: Inference
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理
- en: The inference is all about how LLMs make predictions. GPT-4 is doing pretty
    well in comparison to other models. It has been said that each forward-pass inference
    for the generation of 1 token only utilizes roughly 280 billion parameters and
    roughly 560 teraflops (the rate to measure your GPU’s performance).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 推理是关于大语言模型如何进行预测的。与其他模型相比，GPT-4 的表现相当不错。据说每次前向传递推理生成 1 个令牌只使用了大约 2800 亿个参数和大约
    560 万亿次浮点操作（衡量 GPU 性能的速率）。
- en: Datasets
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: You can imagine how many datasets GPT-4 uses based on its performance and being
    a state-of-the-art model. It is stated that GPT-4 is trained on roughly 13 trillion
    tokens, which is roughly 10 trillion words. It uses 2 epochs for text-based data
    and 4 epochs for code-based data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以根据 GPT-4 的性能和作为最先进模型的表现来想象它使用了多少数据集。据称，GPT-4 的训练大约使用了 13 万亿个令牌，约合 10 万亿个词。它对基于文本的数据使用了
    2 个周期，对基于代码的数据使用了 4 个周期。
- en: The actual size of the dataset is unknown, as some of these tokens were re-used,
    so we can roughly estimate that it includes several trillion tokens. Internally,
    there are also millions of rows of instructions which fine-tune data from ScaleAI.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的实际大小未知，因为这些令牌中有一些是重复使用的，因此我们可以粗略估计数据集包含了数万亿个令牌。在内部，还有数百万行的指令，这些指令来自 ScaleAI
    的数据微调。
- en: Context Length
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文长度
- en: For the pre-training phase of GPT-4, it used a context length of 8 thousand
    tokens. After the pre-training, the sequence length was based on fine-tuning the
    8 thousand tokens.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPT-4 的预训练阶段，它使用了 8000 个令牌的上下文长度。预训练后，序列长度是基于 8000 个令牌的微调。
- en: Batch Size
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量大小
- en: The batch size is the number of samples processed before the model is updated.
    The batch size was continuously increasing, with OpenAI using a batch size of
    60 million, which is roughly around 7.5 million tokens per expert. In order to
    find out the real batch size, you will need to divide this number by the sequence
    length.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大小是指在模型更新之前处理的样本数量。批量大小在不断增加，OpenAI 使用了 6000 万的批量大小，相当于每个专家大约 750 万个令牌。为了确定实际的批量大小，你需要将这个数字除以序列长度。
- en: Training Costs
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练成本
- en: This is an area that a lot of you will be interested in - training costs. You
    can imagine how expensive GPT-4 was to build and train.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你们很多人会感兴趣的领域——训练成本。你可以想象 GPT-4 的构建和训练是多么昂贵。
- en: It took OpenAI roughly 2.1e25 FLOPS (floating point operations per second) of
    computing to train on roughly using around 25 though A100 processors in the space
    of 3 months. It is stated that GPT-4 is around 3x more computationally expensive
    to run than GPT-3.5\. It is also said that GPT-4 costs 3x more than GPT-3 in regards
    to prompts.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 大约花费了 2.1e25 次浮点操作（每秒浮点操作次数）来训练，使用了大约 25 个 A100 处理器，历时 3 个月。据说 GPT-4
    的计算开销大约是 GPT-3.5 的 3 倍。还有人说 GPT-4 在提示方面的成本是 GPT-3 的 3 倍。
- en: For example, if OpenAI were training in the cloud was around $1 per A100 hour,
    the training cost for this hour alone would have cost $63 million.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果 OpenAI 在云端训练的费用约为每小时 $1 的 A100 处理器，那么仅这一小时的训练成本将高达 6300 万美元。
- en: Speculative Decoding
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推测解码
- en: It has also been said that OpenAI might be using speculative decoding. The keyword
    ‘might’. This means that they are using smaller and faster models to help decode
    tokens and then feed this into large models as a single batch.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 还有人说 OpenAI 可能使用了推测解码。关键词是‘可能’。这意味着他们使用了较小且更快速的模型来帮助解码令牌，然后将其作为一个批量输入到大型模型中。
- en: This means that if the predictions made from the smaller model were correct,
    the large model will agree with these predictions. However, if the larger model
    rejects the predictions from the smaller model, the rest of the batch is also
    discarded.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着如果较小模型做出的预测是正确的，那么较大模型将会同意这些预测。然而，如果较大模型拒绝了较小模型的预测，那么批量中的其余部分也会被丢弃。
- en: Wrapping it up
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结一下
- en: This leak reflects more of a high-level architecture leak, rather than a model
    leak - which a lot of people were expecting. Although it is not the same, this
    kind of information is still useful to know as we continue to see the growth of
    LLMs and how much it takes to create an AI model such as GPT-4.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这一泄漏更多地反映了高层架构的泄漏，而不是模型的泄漏——这也是许多人所期待的。尽管不同，这种信息仍然很有用，因为我们继续看到大语言模型的增长，以及创建像
    GPT-4 这样的 AI 模型需要多大的成本。
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist,
    Freelance Technical Writer and Community Manager at KDnuggets. She is particularly
    interested in providing Data Science career advice or tutorials and theory based
    knowledge around Data Science. She also wishes to explore the different ways Artificial
    Intelligence is/can benefit the longevity of human life. A keen learner, seeking
    to broaden her tech knowledge and writing skills, whilst helping guide others.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**[尼莎·阿雅](https://www.linkedin.com/in/nisha-arya-ahmed/)** 是一位数据科学家、自由技术写作人以及KDnuggets的社区经理。她特别关注提供数据科学职业建议或教程和理论知识。她还希望探索人工智能如何/可以促进人类寿命的不同方式。作为一名热心的学习者，她寻求拓宽自己的技术知识和写作技能，同时帮助指导他人。'
- en: More On This Topic
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[How to Get Into Data Analytics If You Don''t Have the Right Degree](https://www.kdnuggets.com/2021/12/how-to-get-into-data-analytics.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如果你没有合适的学位，如何进入数据分析领域](https://www.kdnuggets.com/2021/12/how-to-get-into-data-analytics.html)'
- en: '[Unstructured Data: The Must-Have For Analytics In 2022](https://www.kdnuggets.com/2022/01/unstructured-data-analytics-2022.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[非结构化数据：2022年分析的必备条件](https://www.kdnuggets.com/2022/01/unstructured-data-analytics-2022.html)'
- en: '[Top 13 Skills That Every Data Scientist Should Have](https://www.kdnuggets.com/2022/03/top-13-skills-every-data-scientist.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个数据科学家都应具备的13项技能](https://www.kdnuggets.com/2022/03/top-13-skills-every-data-scientist.html)'
- en: '[21 Must-Have Cheat Sheets for Data Science Interviews: Unlocking…](https://www.kdnuggets.com/2022/06/21-cheat-sheets-data-science-interviews.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21份数据科学面试必备备忘单：解锁…](https://www.kdnuggets.com/2022/06/21-cheat-sheets-data-science-interviews.html)'
- en: '[What Does ETL Have to Do with Machine Learning?](https://www.kdnuggets.com/2022/08/etl-machine-learning.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ETL与机器学习有什么关系？](https://www.kdnuggets.com/2022/08/etl-machine-learning.html)'
- en: '[8 Innovative BERT Knowledge Distillation Papers That Have Changed…](https://www.kdnuggets.com/2022/09/eight-innovative-bert-knowledge-distillation-papers-changed-nlp-landscape.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8篇创新的BERT知识蒸馏论文，它们改变了…](https://www.kdnuggets.com/2022/09/eight-innovative-bert-knowledge-distillation-papers-changed-nlp-landscape.html)'
