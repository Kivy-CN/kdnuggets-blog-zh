- en: Tuning Random Forest Hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/08/tuning-random-forest-hyperparameters.html](https://www.kdnuggets.com/2022/08/tuning-random-forest-hyperparameters.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Tuning Random Forest Hyperparameters](../Images/3414b807dafcc166035b47ce74dfcfd6.png)'
  prefs: []
  type: TYPE_IMG
- en: Jungle vector created by [freepik](https://www.freepik.com/vectors/jungle)
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t already know, let’s quickly go over Random Forest.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest is an Ensemble Learning method for classification, regression,
    and other tasks that contain multiple Decision Trees.
  prefs: []
  type: TYPE_NORMAL
- en: '**Ensembling Learning** in the most simplest explanation is stacking together
    a lot of classifiers to improve performance. **Decision Trees** are a non-parametric
    supervised learning method where the end goal is to build a model that predicts
    the value of a target variable by learning rules that have been inferred and created
    based on data features.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**A Random Forest** is made up of many decision trees. A multitude of trees
    builds a forest, I guess that’s why it’s called Random Forest.'
  prefs: []
  type: TYPE_NORMAL
- en: Bagging is the method that creates the ‘forest’ in Random Forests. Its aim is
    to reduce the complexity of models that overfit the training data. Boosting is
    the opposite of Bagging and aims to increase the complexity of models that suffer
    from high bias, resolving underfitting.
  prefs: []
  type: TYPE_NORMAL
- en: The Random Forests outcomes are based on the predictions generated by the Decision
    Trees, which is done by taking the average or mean of the output from the various
    Decision Trees. If there is an increase in the number of trees, the precision
    of the outcome increases - therefore better accuracy, and overfitting is reduced.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of Hyperparameter Tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hyperparameter tuning is important for algorithms. It improves their overall
    performance of a machine learning model and is set before the learning process
    and happens outside of the model. If hyperparameter tuning does not occur, the
    model will produce errors and inaccurate results as the loss function is not minimized.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning is about finding a set of optimal hyperparameter values
    which maximizes the model's performance, minimizes loss and produces better outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters of a Random Forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Below is the list of the most important parameters and below that is a more
    refined section on how to improve prediction power and your model training phase
    easier.
  prefs: []
  type: TYPE_NORMAL
- en: '**max_depth:** The maximum depth of the tree - meaning the longest path between
    the root node and the leaf node.'
  prefs: []
  type: TYPE_NORMAL
- en: '**min_sample_split:** The minimum number of samples required to split an internal
    node:where the default = 2'
  prefs: []
  type: TYPE_NORMAL
- en: '**max_leaf_nodes:** This is the maximum number of leaf nodes a decision tree
    can have..'
  prefs: []
  type: TYPE_NORMAL
- en: '**min_samples_leaf:** This is the minimum number of samples required to be
    at a leaf node where the default = 1'
  prefs: []
  type: TYPE_NORMAL
- en: '**n_estimators:** This is the number of trees in the forest.'
  prefs: []
  type: TYPE_NORMAL
- en: '**max_sample:** This determines the fraction of the original dataset that is
    given to any individual tree.'
  prefs: []
  type: TYPE_NORMAL
- en: '**max_features:** This is the number of features to consider when looking for
    the best split.'
  prefs: []
  type: TYPE_NORMAL
- en: '**bootstrap:** If this is set as False, the whole dataset is used to build
    each tree, but it is set as Default.'
  prefs: []
  type: TYPE_NORMAL
- en: '**criterion:** The function to measure the quality of a split'
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter Tuning to improve predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**n_estimators :** ***int, default=100***'
  prefs: []
  type: TYPE_NORMAL
- en: This is the number of trees in the forest. As mentioned before, with an increase
    in the number of trees, the precision of the outcome increases - therefore better
    accuracy, and overfitting is reduced. However, this will make your model slower
    - therefore choosing an n_estimator value which your processor can handle allows
    your model to be more stable and perform well.
  prefs: []
  type: TYPE_NORMAL
- en: '**max_features*****{“sqrt”, “log2”, None}, int or float, default=”sqrt”}***'
  prefs: []
  type: TYPE_NORMAL
- en: A Random Forest model can only have a maximum number of features in an individual
    tree. Many would assume that if you increase max_features, this will improve the
    overall performance of your model. However, this naturally decreases the diversity
    of individual trees which would also increase the time it took the model to produce
    outputs. Therefore, finding an optimal max_features is important to your model's
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**min_samples_leaf :** ***int or float, default=1***'
  prefs: []
  type: TYPE_NORMAL
- en: This is the minimum number of samples required to be at a leaf node. A leaf
    node is the end node of a decision tree and a smaller min_sample_leaf value will
    make the model more vulnerable to detecting noise. Again, hyperparameter tuning
    is about finding the optimum - therefore trying out different leaf sizes is advised.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter Tuning to improve model training phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**random_state :** ***int, RandomState instance or None, default=None***'
  prefs: []
  type: TYPE_NORMAL
- en: This parameter controls both the randomness of the bootstrapping of the samples
    used when building trees and the sampling of the features to consider when looking
    for the best split at each node.
  prefs: []
  type: TYPE_NORMAL
- en: '**n_jobs :** ***int, default=None***'
  prefs: []
  type: TYPE_NORMAL
- en: This parameter refers to the number of jobs to run in parallel which essentially
    tells the engine how many processors it can use. -1 means that there are no restrictions,
    1 means it can only use 1 processor.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article has given you a breakdown on what Random Forest is, the importance
    of hyperparameter tuning, the most important parameters and how you can improve
    your prediction power as well as your model training phase.
  prefs: []
  type: TYPE_NORMAL
- en: If you would like to know more about these parameters, click on this [link](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).
  prefs: []
  type: TYPE_NORMAL
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and Freelance Technical Writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Random Forest vs Decision Tree: Key Differences](https://www.kdnuggets.com/2022/02/random-forest-decision-tree-key-differences.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Does the Random Forest Algorithm Need Normalization?](https://www.kdnuggets.com/2022/07/random-forest-algorithm-need-normalization.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hyperparameter Tuning Using Grid Search and Random Search in Python](https://www.kdnuggets.com/2022/10/hyperparameter-tuning-grid-search-random-search-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Tuning XGBoost Hyperparameters](https://www.kdnuggets.com/2022/08/tuning-xgboost-hyperparameters.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Tuning Hyperparameters in Neural Networks](https://www.kdnuggets.com/tuning-hyperparameters-in-neural-networks)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Decision Trees vs Random Forests, Explained](https://www.kdnuggets.com/2022/08/decision-trees-random-forests-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
