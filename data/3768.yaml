- en: 'The Human Vector: Incorporate Speaker Embeddings to Make Your Bot More Powerful'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人类向量：融合说话者嵌入，使您的机器人更强大
- en: 原文：[https://www.kdnuggets.com/2016/09/human-vector-incorporate-speaker-embedding-powerful-bot.html](https://www.kdnuggets.com/2016/09/human-vector-incorporate-speaker-embedding-powerful-bot.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/09/human-vector-incorporate-speaker-embedding-powerful-bot.html](https://www.kdnuggets.com/2016/09/human-vector-incorporate-speaker-embedding-powerful-bot.html)
- en: '**By Megan Barnes, Init.ai**.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 Megan Barnes, Init.ai 提供**。'
- en: How do we evaluate AI? You may have heard about [self-driving cars](https://medium.com/self-driving-cars)
    recently; their release seems imminent. Self-driving cars have a clear objective
    for evaluation: *don’t crash*. Beyond avoiding accidents, there isn’t a notion
    of how *well* a car drives.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何评估人工智能？你可能最近听说过[**自动驾驶汽车**](https://medium.com/self-driving-cars)；它们的发布似乎迫在眉睫。自动驾驶汽车有一个明确的评估目标：*不要发生碰撞*。除了避免事故之外，并没有关于汽车驾驶*好坏*的概念。
- en: 'In the engineering challenge of conversational agents, the threshold for what
    we want becomes higher and hazier. There is a *don’t crash* analog in conversational
    AI: functional failures. We can tell when bots don’t really understand us. They
    respond in ways that don’t answer our questions, aren’t relevant to the conversation,
    or just don’t make sense. The researchers behind “[A Persona-Based Neural Conversation
    Model](http://nlp.stanford.edu/pubs/jiwei2016Persona.pdf)” point out a more subtle
    way in which bots can fail: their (lack of) persona.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在对话代理的工程挑战中，我们对期望的标准变得越来越高且模糊。在对话式人工智能中有一个类似于*不要发生碰撞*的概念：功能性失败。我们能够识别出机器人何时真的不了解我们。它们的回答可能不会解答我们的问题，与对话无关，或者根本没有意义。背后研究者在“[A
    Persona-Based Neural Conversation Model](http://nlp.stanford.edu/pubs/jiwei2016Persona.pdf)”中指出了机器人可能失败的一个更微妙的方式：它们的（缺乏）个性。
- en: In human conversations, we rely on assumptions about how other speakers conduct
    themselves. This is known as the [cooperative principle](https://en.wikipedia.org/wiki/Cooperative_principle) in
    the field of [pragmatics](https://en.wikipedia.org/wiki/Pragmatics). This principle
    breaks down into ‘[maxims](https://en.wikipedia.org/wiki/Cooperative_principle#Grice.27s_Maxims)’
    for speech that speakers either follow or flout. In short, we rely on others saying
    truthful statements, providing as much information as possible, being relevant,
    and saying things appropriately. When speakers purposefully flout these maxims,
    it carries [meaning](https://en.wikipedia.org/wiki/Implicature) that we can understand
    (e.g. sarcasm, in which a speaker makes statements that are obviously untrue).
    When deviation from the maxims is unintentional, however, it can derail a conversation.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在人类对话中，我们依赖于对其他说话者行为的假设。这在[**语用学**](https://en.wikipedia.org/wiki/Pragmatics)领域被称为[**合作原则**](https://en.wikipedia.org/wiki/Cooperative_principle)。这一原则分解为‘[**准则**](https://en.wikipedia.org/wiki/Cooperative_principle#Grice.27s_Maxims)’，说话者要么遵循这些准则，要么违背它们。简而言之，我们依赖他人说出真实的陈述，提供尽可能多的信息，保持相关性，并适当地表达。当说话者故意违背这些准则时，它会传达我们可以理解的[**意义**](https://en.wikipedia.org/wiki/Implicature)（例如，讽刺，其中说话者做出明显不真实的陈述）。然而，当偏离准则是无意的时，它可能会使对话偏离轨道。
- en: 'Consider these example exchanges from “A Persona-Based Neural Conversation
    Model”:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下来自“A Persona-Based Neural Conversation Model”的这些示例对话：
- en: '![Conversations](../Images/94dae495e56dfa4cee3d60c042d4aab4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![对话](../Images/94dae495e56dfa4cee3d60c042d4aab4.png)'
- en: '*(Li et al. 1)*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*(Li et al. 1)*'
- en: The problem here is that our knowledge of the world makes this a clear violation
    of the Maxim of Quality (paraphrase: *say things that are true*). One person can’t
    live in two different places or be two different ages at the same time. That means
    that we understand at least some of the responses to be untrue. It’s conceivable
    that a skilled speaker of English could make these exact same statements intentionally
    and make an implicature in the process. In the last exchange above, for example,
    the responder could be making a joke about the amount of reading required for
    a psych major. Whether it’s actually funny is a matter of taste. The difference
    with bots is that we don’t expect humor. It becomes clear to us that inconsistent
    responses are unintentional and that makes communication difficult.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题在于我们对世界的知识使得这明显违反了质量准则（意指：*说实话*）。一个人不能同时生活在两个不同的地方或拥有两个不同的年龄。这意味着我们至少能理解到某些回答是不真实的。可以想象，一个熟练的英语使用者可能会故意做出这些完全相同的陈述，并在过程中产生一种含义。例如，在上面的最后一次交流中，回答者可能在开玩笑关于心理学专业所需的阅读量。是否真的有趣则是一个品味问题。与机器人不同的是，我们不期望看到幽默。我们很清楚不一致的回答是无意的，这使得沟通变得困难。
- en: The specific issue of inconsistent responses is intrinsic to language modeling
    because data-driven systems are geared toward generating responses with the highest
    likelihood, without regard for the source of that response. When searching through
    the output space, an inference is made based on the most likely sequence of words
    to follow another sequence according to the model. In the study referenced above,
    the baseline model is an [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory) [recurrent
    neural network](https://en.wikipedia.org/wiki/Recurrent_neural_network), an architecture
    common in conversational AI. It uses the softmax function to create a probability
    distribution over possible outputs and picks the most likely next word in the
    sequence, no matter who generated it in the training data. Human speakers expect
    consistent *personas*from the bots they speak to and current techniques are ignoring
    that.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 不一致回应的具体问题是语言建模的内在问题，因为数据驱动的系统旨在生成最有可能的回应，而不考虑回应的来源。在输出空间中进行搜索时，模型根据最可能的单词序列来推断另一个序列。在上述研究中，基线模型是一个[LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory)
    [递归神经网络](https://en.wikipedia.org/wiki/Recurrent_neural_network)，这是一种在对话AI中常见的架构。它使用softmax函数在可能的输出上创建概率分布，并选择序列中最可能的下一个单词，无论训练数据中是谁生成的。人类发言者期望与他们交谈的机器人保持一致的*角色*，而当前技术忽视了这一点。
- en: Li et al. describe personas as, “composite[s] of elements of identity (background
    facts or user profile), language behavior, and interaction style”(1). A persona
    is based on a real individual that generated part of the training data, and is
    represented by a vector, the speaker embedding. They randomly initialize speaker
    embeddings and learn them during training.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 李等人将人物角色描述为“由身份元素（背景事实或用户档案）、语言行为和互动风格的组合”（1）。一个人物角色基于生成部分训练数据的真实个体，并由一个向量，即发言者嵌入，表示。他们随机初始化发言者嵌入，并在训练过程中学习这些嵌入。
- en: 'A basic LSTM can be graphically represented like this:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一个基本的LSTM可以用如下图形表示：
- en: '![LSTM](../Images/b44585ca405a53250dbcf2f1ed82428c.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![LSTM](../Images/b44585ca405a53250dbcf2f1ed82428c.png)'
- en: '*(Kevin Gimpel 2016)*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*(Kevin Gimpel 2016)*'
- en: where *x* represents a word embedding in a sequence, *c* represents a hidden
    layer and *h *represents the output of the model, all at time *t. *The colored
    rectangles represent gates, which transform input vectors. The model can also
    be represented by the functions below (in which *e* takes the place of *x*to represent
    a word embedding), where *i, f, o, *and *l* represent the multicolor gates above.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*x*表示序列中的词嵌入，*c*表示隐藏层，*h*表示模型的输出，所有这些在时间*t*。彩色矩形表示门，用于转换输入向量。该模型也可以用下面的函数表示（其中*e*代替*x*表示词嵌入），*i,
    f, o*和*l*代表上面的多色门。
- en: '![Functions](../Images/8c9e44a5c7ed41cbe9deef854076a128.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![函数](../Images/8c9e44a5c7ed41cbe9deef854076a128.png)'
- en: '*(Li et al. 2)*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*(李等人 2)*'
- en: In what Li et al. term the Speaker Model, they inject the model with the speaker
    embedding, *v*, which can be seen in its representation below.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在李等人称之为发言者模型的研究中，他们将模型注入了发言者嵌入向量**v**，如下面的表示所示。
- en: '![Speaker Model](../Images/9ded39e322a6918352d451b1f9d46a5f.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![发言者模型](../Images/9ded39e322a6918352d451b1f9d46a5f.png)'
- en: '*(Li et al. 3)*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*(李等人 3)*'
- en: This adds information about speaker *i* into every time step of the sequence.
    This is equivalent to adding a *v *input node into the hidden layer of the LSTM
    graphical model, marked with a blue gate. Incorporating speaker embeddings into
    the LSTM model improved its performance, decreasing perplexity and increasing
    BLEU score in the majority of the datasets the researchers examined.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这将发言者*i*的信息添加到序列的每一个时间步中。这相当于在LSTM图形模型的隐藏层中添加一个*v*输入节点，标记为蓝色门。将发言者嵌入融入LSTM模型中提高了其性能，降低了困惑度，并在大多数研究人员检查的数据集中提高了BLEU分数。
- en: The researchers also noted that a single persona should be adaptable. A person
    doesn’t address their boss in the same way they address their little brother.
    Because of this, they also decided to try what they termed the Speaker-Addressee
    Model. This model substitutes a speaker pair embedding,*V*, for a speaker embedding,
    of the form below. The speaker pair embedding is meant to model interactions between
    specific individuals.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员还指出，一个单一的角色应该是可适应的。一个人不会用相同的方式称呼他们的老板和他们的小弟弟。因此，他们还决定尝试他们称之为发言者-受话者模型的方案。该模型用发言者对嵌入**V**替代了发言者嵌入，其形式如下。发言者对嵌入旨在建模特定个体之间的互动。
- en: '![Speaker pair embedding](../Images/66669ef94794363104aecd1254650a47.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![说话者对嵌入](../Images/66669ef94794363104aecd1254650a47.png)'
- en: '*(Li et al. 4)*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*(李等，4)*'
- en: 'The Speaker-Addressee model achieved similar success. This is a particularly
    cute result that the Speaker-Addressee model generates when trained on movie conversation
    data (with reference to character relationships in *Friends*and *The Big Bang
    Theory*):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 说话者-听众模型取得了类似的成功。这是一个特别有趣的结果，说明说话者-听众模型在使用电影对话数据进行训练时生成的（参考了《老友记》和《生活大爆炸》中的角色关系）：
- en: '![Training results](../Images/1c90b6e88115928a13bdb98055e7ef42.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![训练结果](../Images/1c90b6e88115928a13bdb98055e7ef42.png)'
- en: '*(Li et al. 8)*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*(李等，8)*'
- en: The important takeaway from Li et al.’s research is that AI is a diverse field
    with a range of different tasks that need nuanced solutions. Neural nets are great,
    but if they are treated like a black box, they can only perform so well on complex
    tasks like conversation. We need to take into account what we actually expect
    out of a bot. Cohesive, adaptive personas and systems tailored to reflect those
    expectations are the key to achieving sophisticated results. After all, we want
    more from bots than just avoiding crash and burn.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 李等研究的重要启示是，人工智能是一个多样化的领域，包含需要细致解决方案的各种任务。神经网络很棒，但如果它们被当作黑箱对待，在复杂任务如对话上只能发挥有限作用。我们需要考虑我们实际期望从机器人中得到什么。连贯的、适应性的个性和量身定制的系统是实现复杂结果的关键。毕竟，我们对机器人的期望不仅仅是避免崩溃。
- en: '**Bio: [Megan Barnes](https://medium.com/@megan.initai)** is a software developer
    working on machine learning infrastructure. If you’re interested in learning more
    about conversational interfaces, follow her on [Medium](https://medium.com/@megan.initai)
    and [Twitter](https://twitter.com/megan_initai).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介: [梅根·巴恩斯](https://medium.com/@megan.initai)** 是一名从事机器学习基础设施工作的软件开发人员。如果你对对话界面感兴趣，可以关注她的[Medium](https://medium.com/@megan.initai)和[Twitter](https://twitter.com/megan_initai)。'
- en: And if you’re looking to create a conversational app for your company, check
    out [**Init.ai**](http://init.ai/) and our blog on [Medium](https://medium.com/init-ai),
    or connect with us on [Twitter](https://twitter.com/initdotai). Check out the
    original research referenced in this article [here](http://nlp.stanford.edu/pubs/jiwei2016Persona.pdf).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打算为你的公司创建一个对话应用程序，可以查看[**Init.ai**](http://init.ai/)和我们在[Medium](https://medium.com/init-ai)上的博客，或在[Twitter](https://twitter.com/initdotai)上与我们联系。查看这篇文章中提到的原始研究[这里](http://nlp.stanford.edu/pubs/jiwei2016Persona.pdf)。
- en: '[Original](https://blog.init.ai/the-human-vector-incorporate-speaker-embeddings-to-make-your-bot-more-powerful-ade6fdfca035).
    Reposted with permission.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://blog.init.ai/the-human-vector-incorporate-speaker-embeddings-to-make-your-bot-more-powerful-ade6fdfca035)。经允许转载。'
- en: '**Related:**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关:**'
- en: '[Deep Learning For Chatbots, Part 2 – Implementing A Retrieval-Based Model
    In TensorFlow](/2016/07/deep-learning-chatbots-part-2.html)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[聊天机器人深度学习，第2部分——在 TensorFlow 中实现检索型模型](/2016/07/deep-learning-chatbots-part-2.html)'
- en: '[A Survey of Available Corpora for Building Data-driven Dialogue Systems](/2016/07/survey-available-corpora-building-data-driven-dialog-systems.html)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[构建数据驱动对话系统的可用语料库调查](/2016/07/survey-available-corpora-building-data-driven-dialog-systems.html)'
- en: '[Artificial Intelligence ‘Chatbots’ – When or if?](/2016/05/ai-chatbots-when-if.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能‘聊天机器人’——何时或是否？](/2016/05/ai-chatbots-when-if.html)'
- en: '* * *'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析水平'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT'
- en: '* * *'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Python Vector Databases and Vector Indexes: Architecting LLM Apps](https://www.kdnuggets.com/2023/08/python-vector-databases-vector-indexes-architecting-llm-apps.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 向量数据库与向量索引：LLM 应用架构](https://www.kdnuggets.com/2023/08/python-vector-databases-vector-indexes-architecting-llm-apps.html)'
- en: '[How ChatGPT Works: The Model Behind The Bot](https://www.kdnuggets.com/2023/04/chatgpt-works-model-behind-bot.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatGPT 的工作原理：机器人背后的模型](https://www.kdnuggets.com/2023/04/chatgpt-works-model-behind-bot.html)'
- en: '[Using Data Science to Make Clean Energy More Equitable](https://www.kdnuggets.com/2022/03/data-science-make-clean-energy-equitable.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用数据科学实现清洁能源的公平性](https://www.kdnuggets.com/2022/03/data-science-make-clean-energy-equitable.html)'
- en: '[Greening AI: 7 Strategies to Make Applications More Sustainable](https://www.kdnuggets.com/greening-ai-7-strategies-to-make-applications-more-sustainable)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[绿色 AI：使应用程序更可持续的 7 种策略](https://www.kdnuggets.com/greening-ai-7-strategies-to-make-applications-more-sustainable)'
- en: '[LLaMA 3: Meta’s Most Powerful Open-Source Model Yet](https://www.kdnuggets.com/llama-3-metas-most-powerful-open-source-model-yet)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LLaMA 3：Meta 最强大的开源模型](https://www.kdnuggets.com/llama-3-metas-most-powerful-open-source-model-yet)'
- en: '[Support Vector Machines: An Intuitive Approach](https://www.kdnuggets.com/2022/08/support-vector-machines-intuitive-approach.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[支持向量机：直观的方法](https://www.kdnuggets.com/2022/08/support-vector-machines-intuitive-approach.html)'
