# 特征工程如何帮助你在 Kaggle 竞赛中表现出色 – 第三部分

> 原文：[https://www.kdnuggets.com/2017/07/feature-engineering-help-kaggle-competition-3.html](https://www.kdnuggets.com/2017/07/feature-engineering-help-kaggle-competition-3.html)

**作者：Gabriel Moreira，CI&T。**

![Header image](../Images/f1e89e36ceb486d673bddb4fa9cd5de5.png)

在[第一部分](https://medium.com/unstructured/how-feature-engineering-can-help-you-do-well-in-a-kaggle-competition-part-i-9cc9a883514d)和[第二部分](https://medium.com/unstructured/how-feature-engineering-can-help-you-do-well-in-a-kaggle-competition-part-ii-3645d92282b8)中，我介绍了[Outbrain 点击预测机器学习竞赛](https://www.kaggle.com/c/outbrain-click-prediction)和我为应对挑战而进行的初步任务。我展示了用于探索性数据分析、特征工程、交叉验证策略和使用基本统计和机器学习建模基线预测器的主要技术。

在系列文章的最后一篇中，我描述了我如何使用更强大的机器学习算法来解决点击预测问题，以及使我在排行榜上升到第19位（前2%）的集成技术。

### Follow-the-Regularized-Leader

一种流行的 CTR 预测方法是带有[Follow-the-Regularized-Leader (FTRL)](https://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf)优化器的逻辑回归，该优化器已在[Google 的生产环境中使用](https://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf)，用于预测每天数十亿的事件，并使用相应的大型特征空间。

这是一种线性模型，具有惰性表示的系数（*权重*），并且与 L1 正则化结合使用时，会导致非常稀疏的系数向量。这种稀疏性特征减少了内存使用，使其能够扩展到具有数十亿维度的特征向量，因为每个实例通常只有几百个非零值。FTRL 通过从磁盘或网络流式传输示例来提供高效的大数据集训练——每个训练示例只需处理一次（在线学习）。

我尝试了两种不同的 FTRL 实现，分别在[Kaggler](https://github.com/jeongyoonlee/Kaggler)和[Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit/wiki)（VW）框架中可用。

对于 CTR 预测，理解特征之间的交互非常重要。例如，来自“耐克”广告商的广告的平均转化率可能会有所不同，这取决于展示广告的发布者（例如 ESPN 与 Vogue）。

我考虑了所有类别特征用于在[Kaggler](https://github.com/jeongyoonlee/Kaggler)上训练的FTRL模型。启用了特征交互选项，即对于所有可能的两特征组合，特征值被乘以并哈希（阅读[第一篇文章](https://medium.com/unstructured/how-feature-engineering-can-help-you-do-well-in-a-kaggle-competition-part-i-9cc9a883514d)了解特征哈希），映射到一个维度为2²⁸的稀疏特征向量的位置。这是我最慢的模型，训练时间超过12小时。但我的LB得分跳升至**0.67659**，在**方法 #6**中取得了较好表现。

我还尝试了在[VW](https://github.com/JohnLangford/vowpal_wabbit/wiki)上使用FTRL，VW是一个在CPU和内存资源使用上非常快速高效的框架。输入数据仍然是类别特征，并增加了一些选定的数值分箱特征（阅读[第一篇文章](https://medium.com/unstructured/how-feature-engineering-can-help-you-do-well-in-a-kaggle-competition-part-i-9cc9a883514d)了解特征分箱）。我的最佳模型训练了两个小时，结果在**方法 #7**中得到了0.67512的LB得分。准确率稍低，但比之前的模型要快得多。

在第二个VW上的FTRL模型中，我使用了VW超参数配置仅对特征（命名空间）的一个子集进行交互（特征配对）。在这种情况下，交互仅配对类别特征和一些选定的数值特征（不进行分箱转换）。这一变化在**方法 #8**中导致了更好的模型得分：0.67697。

这三种FTRL模型都被集成在我的最终提交中，具体细节将在下一部分中描述。

### 领域感知因子分解机

最近的一种因子分解机变体，称为[领域感知因子分解机（FFM）](http://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf)，在2014年的两个CTR预测竞赛中取得了胜利。FFM尝试通过学习每对特征交互的潜在因子来建模特征交互。这种算法作为[LibFFM](https://www.csie.ntu.edu.tw/~cjlin/libffm/)框架发布，并被许多竞争者使用。LibFFM非常高效地利用了大数据集的并行处理和内存。

在**方法 #9**中，我使用了类别特征与成对交互的哈希映射，维度为700,000。我的最佳模型训练时间仅37分钟，结果是我最好的单一模型，LB得分为**0.67932**。

在**方法 #10**中，输入数据除了类别特征外，还包含了一些选定的分箱数值特征。训练数据增加到214分钟，LB得分为**0.67841**。

对于**方法 #11**，我尝试训练一个仅考虑训练集最后30%事件的FFM模型，假设在最后几天进行训练可以改善对测试集接下来两天（50%）的预测。LB得分为**0.6736**，提高了这两天的准确性，但降低了对测试集前期事件的准确性。

在机器学习项目中，一些有前景的方法最终惨败并不罕见。这发生在一些FFM模型中。我尝试将从GBDT模型中获得的学习迁移到带有叶编码的FFM模型，这是一个[Criteo竞赛中的获胜方法](http://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf)。然而，FFM模型的准确性下降了，这可能是因为GBDT模型在这种情况下的准确性不够，添加了更多的噪声而不是信号。

另一个不成功的方法是训练针对测试数据集中事件更多的地理区域的单独FFM模型，例如某些国家（美国 ~ 80%，加拿大 ~ 5%，英国 ~ 5%，澳大利亚 ~ 2%，其他 ~ 8%）和美国各州（加州 ~ 10%，德州 ~ 7%，佛罗里达 ~ 5%，纽约 ~ 5%）。为了生成预测，每个测试集事件都被送到专门针对这些区域的模型中。专门针对美国的模型在预测该区域的点击时表现良好，但对于其他区域的准确性较低。因此，使用全球范围的FFM模型效果更好。

我在最终集成中使用了三个选定的FFM方法，如下所示。

### 集成方法

[集成方法](http://mlwave.com/kaggle-ensembling-guide/)包括将来自不同模型的预测结果结合起来，以提高准确性和泛化能力。模型预测之间的相关性越低，集成的准确性可能越高。集成的主要思想是，单个模型不仅受到信号的影响，还受到随机噪声的影响。通过采取一组多样化的模型并结合它们的预测，可能会取消大量噪声，从而实现更好的模型泛化，如下所示。

![](../Images/2684f72c212750e2759b372eaffb703b.png)

来自竞赛[不要过拟合](https://www.kaggle.com/c/overfitting#description)

一种简单而有效的集成方法是通过[平均](https://en.wikipedia.org/wiki/Average)合并模型预测。对于这次竞赛，我测试了多种加权平均类型，如算术平均、几何平均、调和平均和排序平均等。

我找到的最佳方法是使用预测CTR（概率）的[logit（sigmoidal logistic的反函数）](https://en.wikipedia.org/wiki/Logit)的加权算术平均，如方程1所示。它考虑了三个选定FFM模型（方法 #9、#10 和 #11）以及一个FTRL模型（方法 #6）的预测。这种平均给我的**方法 #12**带来了**0.68418**的LB得分，相较于我最好的单一模型（方法 #10，得分0.67932）有了不错的提升。

![](../Images/74c390999bb9c9f2d3837d6cc21c02a6.png)

方程 1 — 通过模型预测的逻辑回归加权平均进行集成

那时，比赛提交截止日期只有几天了。我在剩下的时间里选择使用 Learning-to-Rank 模型进行集成，以合并最佳模型的预测。

该模型是一个 [GBDT](https://en.wikipedia.org/wiki/Gradient_boosting) 模型，包含 100 棵树，使用 [XGBoost](https://xgboost.readthedocs.io/) 的排名目标。这个集成模型将最佳的 3 个 FFM 和 3 个 FTRL 模型预测以及 15 个选择的工程化数值特征（如用户观看次数、用户偏好相似性和按类别计算的平均 CTR）作为输入特征。

该集成层仅使用验证集数据进行训练（如 [第二篇文章](https://medium.com/unstructured/how-feature-engineering-can-help-you-do-well-in-a-kaggle-competition-part-ii-3645d92282b8) 中所述），在一种名为 [Blending](http://mlwave.com/kaggle-ensembling-guide/) 的设置中。这样做的原因是，如果训练集的预测也被集成模型考虑，那么它会优先考虑更多的过拟合模型，从而降低其对测试集的泛化能力。在最后一天的比赛中，**Approach #13** 给了我最佳的公开 LB 分数 (**0.68688**)。比赛结束后，我的私人 LB 分数被揭示为 (**0.68716**)，使我在最终排行榜中保持第 19 位，如下图所示。

![](../Images/ad9fac701d8bac781d91c18751023241.png)

Kaggle 的 Outbrain 点击预测最终排行榜

我没有跟踪具体的提交日期，但下面的图表展示了我在比赛期间 LB 分数的变化情况。可以看出，最初的提交提供了分数的显著跃升，但进一步提高变得更加困难，这在机器学习项目中是很常见的。

![](../Images/aca13969e6171642ff0693e191e62df1.png)

我在比赛期间的 LB 分数

### 结论

从这次比赛中我学到的一些东西包括：

1.  一个好的交叉验证策略对比赛至关重要。

1.  应该在特征工程上花费大量时间。之后在数据集上添加新特征需要更多的努力和时间。

1.  对于稀疏数据，哈希是必不可少的。事实上，在简便性和效率方面，它比 One-Hot Encoding (OHE) 表现更好。

1.  OHE 类别特征对于决策树集成并不理想。根据竞争对手分享的经验，树形集成使用原始类别值（ids）加上足够的树木可能表现更好，因为这会将特征向量的维度降低到更低的维度，从而增加随机特征集合包含更多预测特征的机会。

1.  测试许多不同的框架对学习非常有帮助，但通常需要大量时间来将数据转换为所需格式、阅读文档和调整超参数。

1.  阅读有关主要技术（FTRL 和 FFM）的科学论文对超参数调整的指导至关重要。

1.  学习论坛帖子、公开的 Kernels（共享代码）以及竞争对手分享的过往解决方案是很好的学习方式，对参赛至关重要。每个人都应该有所回馈！

1.  基于平均值的集成大大提高了准确性，*混合* 与 ML 模型提高了标准，而 *堆叠* 是必不可少的。我没有时间去探索堆叠，但根据其他竞争对手的说法，使用固定折叠的外折预测增加了可用于集成训练的数据量（完整训练集），并提高了最终集成的准确性。

1.  不应该把生成提交文件的工作留到比赛的最后时刻！

Kaggle 就像是前沿机器学习的大学，为那些决定接受挑战并从过程中和同伴中学习的人提供了一个平台。与世界级数据科学家竞争和学习的经历非常有趣。

Google Cloud Platform 是在这次旅程中出色的合作伙伴，提供了克服大数据和分布式计算悬崖所需的所有工具，而我的注意力和努力则集中在登顶。

**个人简介：[Gabriel Moreira](https://about.me/gspmoreira)** 是一位热衷于用数据解决问题的科学家。他是 Instituto Tecnológico de Aeronáutica - ITA 的博士生，研究领域包括推荐系统和深度学习。目前，他是 [CI&T 的首席数据科学家](https://medium.com/unstructured)，带领团队通过在大数据和非结构化数据上使用机器学习来解决客户面临的挑战性问题。

[原文](https://medium.com/unstructured/how-feature-engineering-can-help-you-do-well-in-a-kaggle-competition-part-iii-f67567aaf57c)。经授权转载。

**相关：**

+   [特征工程如何帮助你在 Kaggle 比赛中表现优异 – 第 1 部分](/2017/06/feature-engineering-help-kaggle-competition-1.html)

+   [特征工程如何帮助你在 Kaggle 比赛中表现优异 – 第 2 部分](/2017/06/feature-engineering-help-kaggle-competition-2.html)

+   [独家：与 Jeremy Howard 讨论深度学习、Kaggle、数据科学等话题的采访](/2017/01/exclusive-interview-jeremy-howard-deep-learning-kaggle-data-science.html)

* * *

## 我们的前 3 个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你所在组织的 IT 需求

* * *

### 更多相关内容

+   [Feature Store Summit 2022：关于特征工程的免费会议](https://www.kdnuggets.com/2022/10/hopsworks-feature-store-summit-2022-free-conference-feature-engineering.html)

+   [数据科学项目帮助你解决现实世界问题](https://www.kdnuggets.com/2022/11/data-science-projects-help-solve-real-world-problems.html)

+   [5 种稀有的数据科学技能助你就业](https://www.kdnuggets.com/5-rare-data-science-skills-that-can-help-you-get-employed)

+   [生成式 AI 如何帮助你改善数据可视化图表](https://www.kdnuggets.com/how-generative-ai-can-help-you-improve-your-data-visualization-charts)

+   [免费 Python 资源帮助你成为专家](https://www.kdnuggets.com/free-python-resources-that-can-help-you-become-a-pro)

+   [等级系统如何帮助预测 AI 成本](https://www.kdnuggets.com/2022/03/level-system-help-forecast-ai-costs.html)
