["```py\ncolumn_names = ['id', 'sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\niris_data = pd.read_csv('data/Iris.csv', names= column_names, header=0)\niris_data.head()\n```", "```py\nprint(iris_data.info())\nprint(iris_data.describe())\n```", "```py\n <class>RangeIndex: 150 entries, 0 to 149\nData columns (total 6 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   id            150 non-null    int64  \n 1   sepal_length  150 non-null    float64\n 2   sepal_width   150 non-null    float64\n 3   petal_length  150 non-null    float64\n 4   petal_width   150 non-null    float64\n 5   species       150 non-null    object \ndtypes: float64(4), int64(1), object(1)\nmemory usage: 7.2+ KB\nNone</class>\n```", "```py\nprint(iris_data['species'].value_counts())\n```", "```py\nIris-setosa        50\nIris-versicolor    50\nIris-virginica     50\nName: species, dtype: int64\n```", "```py\niris_data.dropna(inplace=True)\n```", "```py\nduplicate_rows = iris_data.duplicated()\nprint(\"Number of duplicate rows:\", duplicate_rows.sum())\n```", "```py\nNumber of duplicate rows: 0\n```", "```py\niris_data.drop_duplicates(inplace=True)\n```", "```py\nencoded_species = pd.get_dummies(iris_data['species'], prefix='species', drop_first=False).astype('int')\niris_data = pd.concat([iris_data, encoded_species], axis=1)\niris_data.drop(columns=['species'], inplace=True)\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ncols_to_normalize = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\nscaled_data = scaler.fit(iris_data[cols_to_normalize])\niris_data[cols_to_normalize] = scaler.transform(iris_data[cols_to_normalize])\n```", "```py\niris_data.to_csv('cleaned_iris.csv', index=False)\n```"]