- en: 'Introduction to Streaming-LLM: LLMs for Infinite-Length Inputs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/introduction-to-streaming-llm-llms-for-infinite-length-inputs](https://www.kdnuggets.com/introduction-to-streaming-llm-llms-for-infinite-length-inputs)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The large Language Model (LLM) has changed the way people work. With a model
    such as the GPT family that is used widely, everyone has gotten used to these
    models. Leveraging the LLM power, we can quickly get our questions answered, debugging
    code, and others. This makes the model useful in many applications.
  prefs: []
  type: TYPE_NORMAL
- en: One of the LLM challenges is that the model is unsuitable for streaming applications
    because of the model's inability to handle long-conversation chat exceeding the
    predefined training sequence length. Additionally, there is a problem with the
    higher memory consumption.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: That is why these problems above spawn research to solve them. What is this
    research? Let’s get into it.
  prefs: []
  type: TYPE_NORMAL
- en: StreamingLLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: StreamingLLM is a framework established by [Xiao *et al*. (2023)](https://arxiv.org/pdf/2309.17453.pdf)
    research to tackle the streaming application issues. The existing methods are
    challenged because the attention window constrains the LLMs during pre-training.
  prefs: []
  type: TYPE_NORMAL
- en: The attention [window](https://paperswithcode.com/method/sliding-window-attention)
    technique might be efficient but suffers when handling texts longer than its cache
    size. That’s why the researcher tried to use the Key and Value states of several
    initial tokens (attention sink) with the recent tokens. The comparison of StreamingLLM
    and the other techniques can be seen in the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to Streaming-LLM: LLMs for Infinite-Length Inputs](../Images/81538ada1aa8d1aa3f4a6c4af0152c6b.png)'
  prefs: []
  type: TYPE_IMG
- en: StreamingLLM vs Existing Method ([Xiao *et al*. (2023)](https://arxiv.org/pdf/2309.17453.pdf))
  prefs: []
  type: TYPE_NORMAL
- en: We can see how StreamingLLM tackles the challenge using the attention sink method.
    This attention sink (initial tokens) is used for stable attention computation
    and combines it with recent tokens for efficiency and maintains stable performance
    on longer texts.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the existing methods suffer from memory optimization. However,
    LLM  avoids these issues by maintaining a fixed-size window on the Key and Value
    states of the most recent tokens. The author also mentions the benefit of StreamingLLM
    as the sliding window recomputation baseline by up to 22.2× speedup.
  prefs: []
  type: TYPE_NORMAL
- en: Performance-wise, StreamingLLM provides excellent accuracy compared to the existing
    method, as seen in the table below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to Streaming-LLM: LLMs for Infinite-Length Inputs](../Images/ce77ac4fa07427e249d2be0758ba66d5.png)'
  prefs: []
  type: TYPE_IMG
- en: StreamingLLM accuracy ([Xiao *et al*. (2023)](https://arxiv.org/pdf/2309.17453.pdf))
  prefs: []
  type: TYPE_NORMAL
- en: The table above shows that StreamingLLM accuracy can outperform the other methods
    in the benchmark datasets. That’s why StreamingLLM could have potential for many
    streaming applications.
  prefs: []
  type: TYPE_NORMAL
- en: To try out the StreamingLLM, you could visit their [GitHub page](https://github.com/mit-han-lab/streaming-llm).
    Clone the repository on your intended directory and use the following code in
    your CLI to set the environment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Then, you can use the following code to run the Llama chatbot with LLMstreaming.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The overall sample comparison with StreamingLLM can be shown in the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to Streaming-LLM: LLMs for Infinite-Length Inputs](../Images/76e7b64ada489af40bed5d29e4af83dc.png)'
  prefs: []
  type: TYPE_IMG
- en: StreamingLLM showed outstanding performance in more extended conversations ([Streaming-llm](https://github.com/mit-han-lab/streaming-llm))
  prefs: []
  type: TYPE_NORMAL
- en: That’s all for the introduction of StreamingLLM. Overall, I believe StreamingLLM
    can have a place in streaming applications and help change how the application
    works in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having an LLM in streaming applications would help the business in the long
    run; however, there are challenges to implement. Most LLMs can’t exceed the predefined
    training sequence length and have higher memory consumption. [Xiao *et al*. (2023)](https://arxiv.org/pdf/2309.17453.pdf)
    developed a new framework called StreamingLLM to handle these issues. Using the
    StreamingLLM, it is now possible to have working LLM in the streaming application.
  prefs: []
  type: TYPE_NORMAL
- en: '**[](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**[Cornellius Yudha
    Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**** is a data science
    assistant manager and data writer. While working full-time at Allianz Indonesia,
    he loves to share Python and data tips via social media and writing media. Cornellius
    writes on a variety of AI and machine learning topics.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Machine Learning Is Not Like Your Brain Part 5: Biological Neurons…](https://www.kdnuggets.com/2022/07/machine-learning-like-brain-part-5-biological-neurons-cant-summation-inputs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[RedPajama Project: An Open-Source Initiative to Democratizing LLMs](https://www.kdnuggets.com/2023/06/redpajama-project-opensource-initiative-democratizing-llms.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Falcon LLM: The New King of Open-Source LLMs](https://www.kdnuggets.com/2023/06/falcon-llm-new-king-llms.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ensuring Reliable Few-Shot Prompt Selection for LLMs](https://www.kdnuggets.com/2023/07/ensuring-reliable-fewshot-prompt-selection-llms.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How Watermarking Can Help Mitigate The Potential Risks Of LLMs?](https://www.kdnuggets.com/2023/03/watermarking-help-mitigate-potential-risks-llms.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Explore LLMs Easily on Your Laptop with openplayground](https://www.kdnuggets.com/2023/04/explore-llms-easily-laptop-openplayground.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
