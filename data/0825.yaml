- en: Data Validation for PySpark Applications using Pandera
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Pandera 进行 PySpark 应用的数据验证
- en: 原文：[https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)
- en: '![Data Validation for PySpark Applications using Pandera](../Images/dcef72aa649e62b1541405e7fd23c7e0.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Pandera 进行 PySpark 应用的数据验证](../Images/dcef72aa649e62b1541405e7fd23c7e0.png)'
- en: Photo by [Jakub Skafiriak](https://unsplash.com/@jakubskafiriak?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/AljDaiCbCVY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Jakub Skafiriak](https://unsplash.com/@jakubskafiriak?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/photos/AljDaiCbCVY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: If you’re a data practitioner, you’ll appreciate that data validation holds
    utmost importance in ensuring accuracy and consistency. This becomes particularly
    crucial when dealing with large datasets or data originating from diverse sources.
    However, the [Pandera](https://pandera.readthedocs.io/en/stable/) Python library
    can help to streamline and automate the data validation process. Pandera is an
    [open-source library](https://github.com/unionai-oss/pandera) meticulously crafted
    to simplify the tasks of schema and data validation. It builds upon the robustness
    and versatility of pandas and introduces an intuitive and expressive API specifically
    designed for data validation purposes.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是数据从业者，你会认识到数据验证在确保准确性和一致性方面的重要性。这在处理大型数据集或来自不同来源的数据时尤为关键。然而，[Pandera](https://pandera.readthedocs.io/en/stable/)
    Python 库可以帮助简化和自动化数据验证过程。Pandera 是一个 [开源库](https://github.com/unionai-oss/pandera)，精心设计以简化模式和数据验证任务。它在
    pandas 的稳健性和多功能性基础上构建，并引入了一个直观且富有表现力的 API，专门用于数据验证目的。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT 需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: This article briefly introduces the key features of Pandera, before moving on
    to explain how Pandera data validation can be integrated with data processing
    workflows that use native PySpark SQL since the latest release [(Pandera 0.16.0](https://github.com/unionai-oss/pandera/releases/tag/v0.16.0)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文简要介绍了 Pandera 的关键特性，然后深入说明了如何将 Pandera 数据验证集成到使用本地 PySpark SQL 的数据处理工作流中，自最新版本
    [(Pandera 0.16.0](https://github.com/unionai-oss/pandera/releases/tag/v0.16.0))
    起。
- en: Pandera is designed to work with other popular Python libraries such as pandas,
    pyspark.pandas, Dask, etc. This makes it easy to incorporate data validation into
    your existing data processing workflows. Until recently, Pandera lacked native
    support for [PySpark SQL](https://spark.apache.org/docs/latest/api/python/), but
    to bridge this gap, a team at [QuantumBlack, AI by McKinsey](https://www.mckinsey.com/capabilities/quantumblack/how-we-help-clients)
    comprising [Ismail Negm-PARI](https://github.com/mkinegm), [Neeraj Malhotra](https://github.com/NeerajMalhotra-QB),
    [Jaskaran Singh Sidana](https://github.com/jaskaransinghsidana), [Kasper Janehag](https://github.com/kasperjanehag),
    [Oleksandr Lazarchuk](https://github.com/oleksandr-lazarchuk), along with the
    Pandera Founder, [Niels Bantilan](https://github.com/cosmicBboy), developed native
    PySpark SQL support and contributed it to Pandera. The text of this article was
    also prepared by the team, and is written in their words below.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Pandera 设计用于与其他流行的 Python 库如 pandas、pyspark.pandas、Dask 等一起工作。这使得将数据验证融入现有的数据处理工作流程变得容易。直到最近，Pandera
    还缺乏对[PySpark SQL](https://spark.apache.org/docs/latest/api/python/)的原生支持，但为了弥补这一空白，[QuantumBlack,
    AI by McKinsey](https://www.mckinsey.com/capabilities/quantumblack/how-we-help-clients)的团队包括[Ismail
    Negm-PARI](https://github.com/mkinegm)、[Neeraj Malhotra](https://github.com/NeerajMalhotra-QB)、[Jaskaran
    Singh Sidana](https://github.com/jaskaransinghsidana)、[Kasper Janehag](https://github.com/kasperjanehag)、[Oleksandr
    Lazarchuk](https://github.com/oleksandr-lazarchuk)及Pandera 创始人[ Niels Bantilan](https://github.com/cosmicBboy)开发了对
    PySpark SQL 的原生支持，并将其贡献给了 Pandera。本文的文字也是由该团队准备的，以下是他们的描述。
- en: The Key Features of Pandera
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pandera 的关键特性
- en: If you are unfamiliar with using Pandera to validate your data, we recommend
    reviewing [Khuyen Tran’s](https://khuyentran1476.medium.com/) “[Validate Your
    pandas DataFrame with Pandera](https://towardsdatascience.com/validate-your-pandas-dataframe-with-pandera-2995910e564)”
    which describes the basics. In summary here, we briefly explain the key features
    and benefits of a simple and intuitive API, in-built validation functions and
    customisation.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不熟悉使用 Pandera 验证数据，我们建议查看[Khuyen Tran](https://khuyentran1476.medium.com/)的“[使用
    Pandera 验证你的 pandas DataFrame](https://towardsdatascience.com/validate-your-pandas-dataframe-with-pandera-2995910e564)”，其中描述了基础知识。简而言之，我们在这里简要说明了简单直观的
    API、内置验证函数和自定义的关键特性和优势。
- en: Simple and Intuitive API
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单直观的 API
- en: One of the standout features of Pandera is its simple and intuitive API. You
    can define your data schema using a declarative syntax that is easy to read and
    understand. This makes it easy to write data validation code that is both efficient
    and effective.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Pandera 的一个显著特点是其简单直观的 API。你可以使用易于阅读和理解的声明式语法定义数据模式。这使得编写高效且有效的数据验证代码变得简单。
- en: 'Here’s an example of schema definition in Pandera:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Pandera 中模式定义的一个示例：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Inbuilt Validation Functions
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内置验证函数
- en: Pandera provides a set of in-built functions (more commonly called checks) to
    perform data validations. When we invoke `validate()`on a Pandera schema, it will
    perform both schema & data validations. The data validations will invoke `check`
    functions behind the scenes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Pandera 提供了一组内置函数（更常称为检查）来执行数据验证。当我们在 Pandera 模式上调用 `validate()` 时，它将执行模式和数据验证。数据验证将在后台调用
    `check` 函数。
- en: Here’s a simple example of how to run a data `check` on a dataframe object using
    Pandera.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个使用 Pandera 对数据框对象运行数据 `check` 的简单示例。
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As seen above, for `year` field we have defined a check `gt=2000` enforcing
    that all values in this field must be greater than 2000 otherwise there will be
    validation failure raised by Pandera.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，对于 `year` 字段，我们定义了一个检查 `gt=2000`，强制所有此字段的值必须大于 2000，否则 Pandera 将引发验证失败。
- en: 'Here’s a list of all built-in checks available on Pandera by default:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Pandera 默认提供的所有内置检查的列表：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Custom Validation Functions
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义验证函数
- en: In addition to the built-in validation checks, Pandera allows you to define
    your own custom validation functions. This gives you the flexibility to define
    your own validation rules based on use case.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 除了内置的验证检查，Pandera 还允许你定义自定义验证函数。这让你能够根据使用情况定义自己的验证规则。
- en: 'For instance, you can define a lambda function for data validation as shown
    here:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以定义一个用于数据验证的 lambda 函数，如下所示：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Adding Support for PySpark SQL DataFrames to Pandera
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将对 PySpark SQL 数据框的支持添加到 Pandera 中
- en: 'During the process of adding support to PySpark SQL, we adhered to two fundamental
    principles:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加对 PySpark SQL 支持的过程中，我们遵循了两个基本原则：
- en: consistency of interface and user experience
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接口和用户体验的一致性。
- en: performance optimization for PySpark.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 性能优化。
- en: First, let’s delve into the topic of consistency, because it is important that,
    from a user’s perspective, they have a consistent set of APIs and an interface
    irrespective of the chosen framework. As Pandera provides multiple frameworks
    to choose from it was even more critical to have a consistent user experience
    in PySpark SQL APIs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们深入探讨一致性的问题，因为从用户的角度来看，他们需要无论选择什么框架，都能有一套一致的API和接口。由于Pandera提供了多个框架选择，因此在PySpark
    SQL API中拥有一致的用户体验变得尤为重要。
- en: 'With this in mind, we can define the Pandera schema using PySpark SQL as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这一点，我们可以使用PySpark SQL来定义Pandera模式，如下所示：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the above code, `PanderaSchema` defines the schema for incoming pyspark dataframe.
    It has 5 fields with varying `dtypes` and enforcement of data checks on `id` and
    `product_name` fields.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`PanderaSchema`定义了传入的pyspark数据框的模式。它有5个字段，具有不同的`dtypes`，并对`id`和`product_name`字段进行了数据检查的强制执行。
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Next, we crafted a dummy data and enforced native PySpark SQL schema as defined
    in `spark_schema`*.*
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们制作了一个虚拟数据并强制执行` spark_schema`*中定义的本地PySpark SQL模式*。
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This is done to simulate schema and data validation failures.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这是为了模拟模式和数据验证失败。
- en: 'Here’s the contents of `df_fail` dataframe:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`df_fail`数据框的内容：
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next we can invoke Pandera’s validate function to perform schema and data level
    validations as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以调用Pandera的validate函数来执行模式和数据级验证，如下所示：
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We will explore the contents of `df_out` shortly.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将很快探索`df_out`的内容。
- en: Performance Optimization for PySpark
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark性能优化
- en: Our contribution was specifically designed for optimum performance when working
    with PySpark dataframes, which is crucial when working with large datasets in
    order to handle the unique challenges of PySpark’s distributed computing environment.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献专门针对在处理PySpark数据框时的最佳性能进行设计，这对于处理大型数据集以应对PySpark分布式计算环境的独特挑战至关重要。
- en: Pandera uses PySpark’s distributed computing architecture to efficiently process
    large datasets while maintaining data consistency and accuracy. We rewrote Pandera’s
    custom validation functions for PySpark performance to enable faster and more
    efficient validation of large datasets, while reducing the risk of data errors
    and inconsistencies at high volume.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Pandera利用PySpark的分布式计算架构来高效处理大型数据集，同时保持数据的一致性和准确性。我们重写了Pandera的自定义验证函数以提高PySpark性能，使得大型数据集的验证更快、更高效，同时降低了在高负载情况下数据错误和不一致的风险。
- en: Comprehensive Error Reports
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 综合错误报告
- en: We made another addition to Pandera for the capability to generate detailed
    error reports in the form of a Python dictionary object. These reports are accessible
    via the dataframe returned from the validate function. They provide a comprehensive
    summary of all schema and data level validations, as per the user’s configurations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Pandera中新增了生成详细错误报告的功能，这些报告以Python字典对象的形式提供。这些报告可以通过validate函数返回的数据框访问，提供了所有模式和数据级验证的全面摘要，依据用户的配置。
- en: This feature proves to be valuable for developers to swiftly identify and address
    any data-related issues. By using the generated error report, teams can compile
    a comprehensive list of schema and data issues within their application. This
    enables them to prioritize and resolve issues with efficiency and precision.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这一功能对开发人员来说非常有价值，可以快速识别和解决数据相关问题。通过使用生成的错误报告，团队可以编制应用程序中的模式和数据问题的全面列表，从而高效而精准地优先解决问题。
- en: It is important to note that this feature is currently available exclusively
    for PySpark SQL, offering users an enhanced experience when working with error
    reports in Pandera.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，这一功能目前仅对PySpark SQL可用，为用户在处理Pandera中的错误报告时提供了增强的体验。
- en: 'In above code example, remember we had invoked `validate()` on spark dataframe:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码示例中，请记住我们对spark数据框调用了`validate()`：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'It returned a dataframe object. Using accessors we can extract the error report
    out of it as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回了一个数据框对象。使用访问器，我们可以提取其中的错误报告，如下所示：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As seen above, the error report is aggregated on 2 levels in a python dictionary
    object to be easily consumed by downstream applications such as timeseries visualization
    of errors over time using tools like Grafana:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，错误报告以两级汇总的形式存储在一个Python字典对象中，以便下游应用程序轻松使用，比如使用Grafana等工具对错误的时间序列进行可视化：
- en: type of validation = `SCHEMA` or `DATA`
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证类型 = `SCHEMA` 或 `DATA`
- en: category of errors = `DATAFRAME_CHECK` or `WRONG_DATATYPE`, etc.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 错误类别 = `DATAFRAME_CHECK` 或 `WRONG_DATATYPE` 等。
- en: This new format to restructure the error reporting was introduced in 0.16.0
    as part of our contribution.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这种重构错误报告的新格式是在 0.16.0 版本中引入的，作为我们贡献的一部分。
- en: ON/OFF Switch
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开/关开关
- en: For applications that rely on PySpark, having an On/Off switch is an important
    feature that can make a significant difference in terms of flexibility and risk
    management. Specifically, the On/Off switch allows teams to disable data validations
    in production without requiring code changes.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于依赖于 PySpark 的应用程序，开/关开关是一个重要的功能，可以在灵活性和风险管理方面带来显著的差异。具体来说，开/关开关允许团队在生产环境中禁用数据验证，而无需修改代码。
- en: This is especially important for big data pipelines where performance is critical.
    In many cases, data validation can take up a significant amount of processing
    time, which can impact the overall performance of the pipeline. With the On/Off
    switch, teams can quickly and easily disable data validation if necessary, without
    having to go through the time-consuming process of modifying code.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这对大数据管道尤其重要，其中性能至关重要。在许多情况下，数据验证可能会占用大量处理时间，这可能会影响管道的整体性能。使用开/关开关，团队可以在必要时快速轻松地禁用数据验证，而无需经过耗时的代码修改过程。
- en: Our team introduced the On/Off switch to Pandera so users can easily turn off
    data validation in production by simply changing a configuration setting. This
    provides the flexibility needed to prioritize performance, when necessary, without
    sacrificing data quality or accuracy in development.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的团队为 Pandera 引入了开/关开关，使用户可以通过简单地更改配置设置，轻松关闭生产环境中的数据验证。这提供了在必要时优先考虑性能的灵活性，同时不牺牲开发中的数据质量或准确性。
- en: 'To enable validations, set the following in your environment variables:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用验证，请在您的环境变量中设置以下内容：
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This will be picked up by Pandera to disable all validations in the application.
    By default, validation is enabled.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这将被 Pandera 拦截，以禁用应用程序中的所有验证。默认情况下，验证是启用的。
- en: Currently, this feature is only available for PySpark SQL from version 0.16.0
    as it is a new concept introduced by our contribution.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，该功能仅在 0.16.0 版本及更高版本的 PySpark SQL 中可用，因为这是我们贡献的新概念。
- en: Granular Control of Pandera’s Execution
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pandera 执行的细粒度控制
- en: 'In addition to the On/Off switch feature, we also introduced a more granular
    control over the execution of Pandera’s validation flow. This is achieved by introducing
    configurable settings that allow users to control execution at three different
    levels:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 除了开/关开关功能，我们还引入了对 Pandera 验证流程的更细粒度控制。这是通过引入可配置设置实现的，这些设置允许用户在三个不同的层级上控制执行：
- en: '`SCHEMA_ONLY`: This setting performs schema validations only. It checks that
    the data conforms to the schema definition but does not perform any additional
    data-level validations.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`SCHEMA_ONLY`：此设置仅执行模式验证。它检查数据是否符合模式定义，但不执行任何额外的数据级别验证。'
- en: '`DATA_ONLY`: This setting performs data-level validations only. It checks the
    data against the defined constraints and rules but does not validate the schema.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DATA_ONLY`：此设置仅执行数据级别的验证。它检查数据是否符合定义的约束和规则，但不验证模式。'
- en: '`SCHEMA_AND_DATA`: This setting performs both schema and data-level validations.
    It checks the data against both the schema definition and the defined constraints
    and rules.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`SCHEMA_AND_DATA`：此设置执行模式和数据级别的验证。它会检查数据是否符合模式定义以及定义的约束和规则。'
- en: By providing this granular control, users can choose the level of validation
    that best fits their specific use case. For example, if the main concern is to
    ensure that the data conforms to the defined schema, the `SCHEMA_ONLY` setting
    can be used to reduce the overall processing time. Alternatively, if the data
    is known to conform to the schema and the focus is on ensuring data quality, the
    `DATA_ONLY` setting can be used to prioritize data-level validations.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供这种细粒度的控制，用户可以选择最适合其特定用例的验证级别。例如，如果主要关注的是确保数据符合定义的模式，可以使用 `SCHEMA_ONLY` 设置以减少总体处理时间。或者，如果数据已知符合模式，并且关注点是确保数据质量，则可以使用
    `DATA_ONLY` 设置来优先考虑数据级别的验证。
- en: The enhanced control over Pandera’s execution allows users to strike a fine-tuned
    balance between precision and efficiency, enabling a more targeted and optimized
    validation experience.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对 Pandera 执行的增强控制允许用户在精确性和效率之间找到微调平衡，从而实现更有针对性和优化的验证体验。
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: By default, validations are enabled, and depth is set to `SCHEMA_AND_DATA` which
    can be changed to `SCHEMA_ONLY` or `DATA_ONLY` as desired by use case.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，验证是启用的，深度设置为`SCHEMA_AND_DATA`，可以根据用例需要更改为`SCHEMA_ONLY`或`DATA_ONLY`。
- en: Currently, this feature is only available for PySpark SQL from version 0.16.0
    as it is a new concept introduced by our contribution.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，这一功能仅适用于从版本0.16.0开始的PySpark SQL，因为这是我们贡献的新概念。
- en: Metadata at Column and Dataframe levels
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 列和数据框级别的元数据
- en: Our team added a new feature to Pandera that allows users to store additional
    metadata at `Field` and `Schema / Model` levels. This feature is designed to allow
    users to embed contextual information in their schema definitions which can be
    leveraged by other applications.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的团队为Pandera添加了一个新功能，允许用户在`Field`和`Schema / Model`级别存储额外的元数据。此功能旨在让用户在其模式定义中嵌入上下文信息，以供其他应用程序使用。
- en: For example, by storing details about a specific column, such as data type,
    format, or units, developers can ensure that downstream applications are able
    to interpret and use the data correctly. Similarly, by storing information about
    which columns of a schema are needed for a specific use case, developers can optimize
    data processing pipelines, reduce storage costs, and improve query performance.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，通过存储关于特定列的详细信息，如数据类型、格式或单位，开发人员可以确保下游应用程序能够正确解释和使用数据。类似地，通过存储关于某个模式中哪些列对于特定用例是必要的信息，开发人员可以优化数据处理管道，降低存储成本，并提高查询性能。
- en: At the schema level, users can store information to help categorize different
    schema across the entire application. This metadata can include details such as
    the purpose of the schema, the source of the data, or the date range of the data.
    This can be particularly useful for managing complex data processing workflows,
    where multiple schemas are used for different purposes and need to be tracked
    and managed efficiently.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在模式级别，用户可以存储信息以帮助分类整个应用程序中的不同模式。这些元数据可以包括模式的目的、数据来源或数据的日期范围等细节。这对于管理复杂的数据处理工作流尤其有用，其中多个模式用于不同的目的，需要高效地跟踪和管理。
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the above example, we have introduced additional information on the schema
    object itself. This is allowed at 2 levels: field and schema.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，我们引入了有关模式对象本身的附加信息。这允许在两个级别进行：字段和模式。
- en: 'To extract the metadata on schema level (including all fields in it), we provide
    helper functions as:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 要提取模式级别的元数据（包括其中所有字段），我们提供了以下帮助函数：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Currently, this feature is a new concept in 0.16.0 and has been added for PySpark
    SQL and Pandas.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，这一功能在0.16.0中是一个新概念，并已添加到PySpark SQL和Pandas中。
- en: Summary
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We have introduced several new features and concepts, including an On/Off switch
    that allows teams to disable validations in production without code changes, granular
    control over Pandera’s validation flow, and the ability to store additional metadata
    on column and dataframe levels. You can find even more detail in the [updated
    Pandera documentation](https://pandera.readthedocs.io/en/stable/pyspark_sql.html)
    for version 0.16.0.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了几个新功能和概念，包括一个开关，允许团队在生产中禁用验证而无需更改代码，对Pandera验证流程的精细控制，以及在列和数据框级别存储额外元数据的能力。您可以在[更新的Pandera文档](https://pandera.readthedocs.io/en/stable/pyspark_sql.html)中找到更多关于版本0.16.0的详细信息。
- en: As the Pandera Founder, Niels Bantilan, explained in a [recent blog post about
    the release of Pandera 0.16.0:](https://www.union.ai/blog-post/pandera-0-16-going-beyond-pandas-data-validation)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 正如Pandera创始人Niels Bantilan在[关于Pandera 0.16.0发布的近期博客文章中所解释的：](https://www.union.ai/blog-post/pandera-0-16-going-beyond-pandas-data-validation)
- en: '*To prove out the extensibility of Pandera with the new schema specification
    and backend API, we collaborated with the* [*QuantumBlack*](https://www.mckinsey.com/capabilities/quantumblack/how-we-help-clients)
    *team to implement a schema and backend for* [*Pyspark SQL*](https://spark.apache.org/docs/3.3.1/api/python/index.html#:~:text=PySpark%20is%20an%20interface%20for,data%20in%20a%20distributed%20environment.)
    *… and we completed an MVP in a matter of a few months!*'
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*为了证明Pandera在新的模式规范和后端API中的可扩展性，我们与* [*QuantumBlack*](https://www.mckinsey.com/capabilities/quantumblack/how-we-help-clients)
    *团队合作，实现了一个模式和后端，用于* [*Pyspark SQL*](https://spark.apache.org/docs/3.3.1/api/python/index.html#:~:text=PySpark%20is%20an%20interface%20for,data%20in%20a%20distributed%20environment.)
    *…并在几个月内完成了一个MVP！*'
- en: This recent [contribution to Pandera](https://github.com/unionai-oss/pandera/pull/1243)’s
    open-source codebase will benefit teams working with PySpark and other big data
    technologies.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最近对 [Pandera](https://github.com/unionai-oss/pandera/pull/1243) 开源代码库的贡献将惠及使用
    PySpark 和其他大数据技术的团队。
- en: 'The following team members at [QuantumBlack, AI by McKinsey](https://www.mckinsey.com/capabilities/quantumblack/how-we-help-clients)
    are responsible for this recent contribution: [Ismail Negm-PARI](https://github.com/mkinegm),
    [Neeraj Malhotra](https://github.com/NeerajMalhotra-QB), [Jaskaran Singh Sidana](https://github.com/jaskaransinghsidana),
    [Kasper Janehag](https://github.com/kasperjanehag), [Oleksandr Lazarchuk](https://github.com/oleksandr-lazarchuk).
    I’d like to thank Neeraj in particular for his assistance in preparing this article
    for publication.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下在 [QuantumBlack（麦肯锡的人工智能部门）](https://www.mckinsey.com/capabilities/quantumblack/how-we-help-clients)
    的团队成员负责了这次贡献：[Ismail Negm-PARI](https://github.com/mkinegm)、[Neeraj Malhotra](https://github.com/NeerajMalhotra-QB)、[Jaskaran
    Singh Sidana](https://github.com/jaskaransinghsidana)、[Kasper Janehag](https://github.com/kasperjanehag)、[Oleksandr
    Lazarchuk](https://github.com/oleksandr-lazarchuk)。特别感谢 Neeraj 在准备这篇文章出版过程中的协助。
- en: '**[Jo Stichbury](https://www.linkedin.com/in/jostichbury/)** is a technical
    writer at QuantumBlack, AI by McKinsey. Technical content creator writing about
    data science and software. Old-school Symbian C++ developer, now accidental cat
    herder and goose chaser.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Jo Stichbury](https://www.linkedin.com/in/jostichbury/)** 是 QuantumBlack（麦肯锡的人工智能部门）的技术作家。他是一名技术内容创作者，撰写关于数据科学和软件的文章。曾是老派
    Symbian C++ 开发者，现在是偶然的猫群管理者和鹅追逐者。'
- en: More On This Topic
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[PySpark for Data Science](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PySpark 数据科学](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
- en: '[Pydantic Tutorial: Data Validation in Python Made Simple](https://www.kdnuggets.com/pydantic-tutorial-data-validation-in-python-made-simple)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Pydantic 教程：Python 数据验证变得简单](https://www.kdnuggets.com/pydantic-tutorial-data-validation-in-python-made-simple)'
- en: '[MarshMallow: The Sweetest Python Library for Data Serialization and…](https://www.kdnuggets.com/marshmallow-the-sweetest-python-library-for-data-serialization-and-validation)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MarshMallow：最甜美的 Python 数据序列化和验证库](https://www.kdnuggets.com/marshmallow-the-sweetest-python-library-for-data-serialization-and-validation)'
- en: '[Why Use k-fold Cross Validation?](https://www.kdnuggets.com/2022/07/kfold-cross-validation.html)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么使用 k-fold 交叉验证？](https://www.kdnuggets.com/2022/07/kfold-cross-validation.html)'
- en: '[KDnuggets News, May 18: 5 Free Hosting Platform For Machine…](https://www.kdnuggets.com/2022/n20.html)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，5 月 18 日：5 个免费的机器学习平台](https://www.kdnuggets.com/2022/n20.html)'
- en: '[LangChain 101: Build Your Own GPT-Powered Applications](https://www.kdnuggets.com/2023/04/langchain-101-build-gptpowered-applications.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LangChain 101：构建你自己的 GPT 驱动应用](https://www.kdnuggets.com/2023/04/langchain-101-build-gptpowered-applications.html)'
