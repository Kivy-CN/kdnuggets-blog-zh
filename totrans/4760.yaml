- en: Multi-Class Text Classification Model Comparison and Selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2018/11/multi-class-text-classification-model-comparison-selection.html/2](https://www.kdnuggets.com/2018/11/multi-class-text-classification-model-comparison-selection.html/2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](/2018/11/multi-class-text-classification-model-comparison-selection.html?page=2#comments)'
  prefs: []
  type: TYPE_IMG
- en: Word2vec and Logistic Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Word2vec](https://en.wikipedia.org/wiki/Word2vec), like [doc2vec](https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e),
    belongs to the text preprocessing phase. Specifically, to the part that transforms
    a text into a row of numbers. Word2vec is a type of mapping that allows words
    with similar meaning to have similar vector representation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea behind Word2vec is rather simple: we want to use the surrounding words
    to represent the target words with a Neural Network whose hidden layer encodes
    the word representation.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: First we load a word2vec model. It has been pre-trained by Google on a [100
    billion word Google News corpus](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We may want to explore some vocabularies.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/41e409166baa7ad336aa150deeaef613.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9
  prefs: []
  type: TYPE_NORMAL
- en: BOW based approaches that includes averaging, summation, weighted addition.
    The common way is to average the two word vectors. Therefore, we will follow the
    most common way.
  prefs: []
  type: TYPE_NORMAL
- en: We will tokenize the text and apply the tokenization to “post” column, and apply
    word vector averaging to tokenized text.
  prefs: []
  type: TYPE_NORMAL
- en: Its time to see how logistic regression classifiers performs on these word-averaging
    document features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b1151caad1eb1ec7c8150c27b8e07f94.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10
  prefs: []
  type: TYPE_NORMAL
- en: It was disappointing, worst we have seen so far.
  prefs: []
  type: TYPE_NORMAL
- en: '**Doc2vec and Logistic Regression**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The same idea of [word2vec](https://en.wikipedia.org/wiki/Word2vec) can be extended
    to documents where instead of learning feature representations for words, we learn
    it for sentences or documents. To get a general idea of a [word2vec](https://en.wikipedia.org/wiki/Word2vec),
    think of it as a mathematical average of the word vector representations of all
    the words in the document. [Doc2Vec](https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e) extends
    the idea of [word2vec](https://en.wikipedia.org/wiki/Word2vec), however words
    can only capture so much, there are times when we need relationships between documents
    and not just words.
  prefs: []
  type: TYPE_NORMAL
- en: The way to train doc2vec model for our Stack Overflow questions and tags data
    is very similar with when we train [Multi-Class Text Classification with Doc2vec
    and Logistic Regression](https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4).
  prefs: []
  type: TYPE_NORMAL
- en: First, we label the sentences. [Gensim’s Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html) implementation
    requires each document/paragraph to have a label associated with it. and we do
    this by using the `TaggedDocument` method. The format will be “TRAIN_i” or “TEST_i”
    where “i” is a dummy index of the post.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to [Gensim doc2vec tutorial](https://github.com/RaRe-Technologies/gensim/blob/ca0dcaa1eca8b1764f6456adac5719309e0d8e6d/docs/notebooks/doc2vec-IMDB.ipynb),
    its doc2vec class was trained on the entire data, and we will do the same. Let’s
    have a look what the tagged document looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5f82f0a31a878fbe540d4d4bc9f0b132.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11
  prefs: []
  type: TYPE_NORMAL
- en: 'When training the doc2vec, we will vary the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dm=0` , distributed bag of words (DBOW) is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vector_size=300` , 300 vector dimensional feature vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative=5` , specifies how many “noise words” should be drawn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_count=1`, ignores all words with total frequency lower than this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha=0.065` , the initial learning rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We initialize the model and train for 30 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we get vectors from trained doc2vec model.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we get a logistic regression model trained by the doc2vec features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e4f217a1204b49c9cda60ad67ad47cbb.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12
  prefs: []
  type: TYPE_NORMAL
- en: We achieve an accuracy score of 80% which is 1% higher than SVM.
  prefs: []
  type: TYPE_NORMAL
- en: BOW with Keras
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, we are going to do a text classification with [Keras](https://keras.io/) which
    is a Python Deep Learning library.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code were largely taken from a [Google workshop](https://github.com/tensorflow/workshops/blob/master/extras/keras-bag-of-words/keras-bow-model.ipynb).
    The process is like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Separate the data into training and test sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `tokenizer` methods to count the unique words in our vocabulary and assign
    each of those words to indices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calling `fit_on_texts()` automatically creates a word index lookup of our vocabulary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We limit our vocabulary to the top words by passing a `num_words` param to the
    tokenizer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With our tokenizer, we can now use the `texts_to_matrix` method to create the
    training data that we’ll pass our model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We feed a one-hot vector to our model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After we transform our features and labels in a format Keras can read, we are
    ready to build our text classification model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we build our model, all we need to do is tell Keras the shape of our input
    data, output data, and the type of each layer. keras will look after the rest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When training the model, we’ll call the `fit()` method, pass it our training
    data and labels, batch size and epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/7516a2d9bd5e03c5f15735ca30d1adb8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13
  prefs: []
  type: TYPE_NORMAL
- en: 'The accuracy is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/18a33d8393caab65bd5d17840f686145.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14
  prefs: []
  type: TYPE_NORMAL
- en: So, which model is the best for this particular data set? I will leave it to
    you to decide.
  prefs: []
  type: TYPE_NORMAL
- en: '[Jupyter notebook](https://github.com/susanli2016/NLP-with-Python/blob/master/Text%20Classification%20model%20selection.ipynb) can
    be found on [Github](https://github.com/susanli2016/NLP-with-Python/blob/master/Text%20Classification%20model%20selection.ipynb).
    Have a productive day!'
  prefs: []
  type: TYPE_NORMAL
- en: '**References:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/RaRe-Technologies/movie-plots-by-genre/blob/master/ipynb_with_output/Document%20classification%20with%20word%20embeddings%20tutorial%20-%20with%20output.ipynb](https://github.com/RaRe-Technologies/movie-plots-by-genre/blob/master/ipynb_with_output/Document%20classification%20with%20word%20embeddings%20tutorial%20-%20with%20output.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/tensorflow/workshops/blob/master/extras/keras-bag-of-words/keras-bow-model.ipynb](https://github.com/tensorflow/workshops/blob/master/extras/keras-bag-of-words/keras-bow-model.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://datascience.stackexchange.com/questions/20076/word2vec-vs-sentence2vec-vs-doc2vec](https://datascience.stackexchange.com/questions/20076/word2vec-vs-sentence2vec-vs-doc2vec)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bio: [Susan Li](https://www.linkedin.com/in/susanli/)** is changing the world,
    one article at a time. She is a Sr. Data Scientist, located in Toronto, Canada.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Machine Learning for Text Classification Using SpaCy in Python](/2018/09/machine-learning-text-classification-using-spacy-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Multi-Class Text Classification with Scikit-Learn](/2018/08/multi-class-text-classification-scikit-learn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Named Entity Recognition and Classification with Scikit-Learn](/2018/10/named-entity-recognition-classification-scikit-learn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[A Comparison of Machine Learning Algorithms in Python and R](https://www.kdnuggets.com/2023/06/machine-learning-algorithms-python-r.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear Regression Model Selection: Balancing Simplicity and Complexity](https://www.kdnuggets.com/2023/02/linear-regression-model-selection-balancing-simplicity-complexity.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What is Text Classification?](https://www.kdnuggets.com/2022/07/text-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Best Architecture for Your Text Classification Task: Benchmarking…](https://www.kdnuggets.com/2023/04/best-architecture-text-classification-task-benchmarking-options.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ChatGPT vs Google Bard: A Comparison of the Technical Differences](https://www.kdnuggets.com/2023/03/chatgpt-google-bard-comparison-technical-differences.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Deep Dive into GPT Models: Evolution & Performance Comparison](https://www.kdnuggets.com/2023/05/deep-dive-gpt-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
