- en: Essential data science skills that no one talks about
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/11/essential-data-science-skills-no-one-talks-about.html](https://www.kdnuggets.com/2020/11/essential-data-science-skills-no-one-talks-about.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Michael Kolomenkin](https://www.linkedin.com/in/michael-kolomenkin-5284942/),
    AI Researcher**'
  prefs: []
  type: TYPE_NORMAL
- en: Google “the essential skills of a data scientist”. The top results are long
    lists of technical terms, named *hard skills*. Python, algebra, statistics, and
    SQL are some of the most popular ones. Later, there come *soft skills* — communication,
    business acumen, team player, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s pretend that you are a super-human possessing all the above abilities.
    You code from the age of five, you are a Kaggle grandmaster and your conference
    papers are guaranteed to get a best-paper award. And you know what? There is still
    a very high chance that your projects struggle to reach maturity and become full-fledged
    commercial products.
  prefs: []
  type: TYPE_NORMAL
- en: Recent studies estimate that more than 85% of data science projects fail to
    reach production. The studies provide numerous reasons for the failures. And I
    have not seen the so-called *essential skills* mentioned even once as a potential
    reason.
  prefs: []
  type: TYPE_NORMAL
- en: Am I saying that the above skills are not important? Of course, I’m not. Both
    hard and soft skills are vital. The point is that they are necessary, but not
    sufficient. Moreover, they are popular and appear on every google search. So the
    chance is that you already know if you need to improve your math proficiency or
    teamwork.
  prefs: []
  type: TYPE_NORMAL
- en: I want to talk about the skills that complement popular hard and soft skills.
    I call them *engineering* skills. They are especially useful for building real
    products with real customers. Regretfully, engineering skills are seldom taught
    to data scientists. They come with experience. Most junior data scientists lack
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Engineering skills have nothing to do with the area of data engineering. I use
    the term engineering skills to distinguish them from purely scientific or research
    skills. According to the Cambridge dictionary, ***engineering**** is the use of
    scientific principles to design and build machines, structures, and other items*.
    In this paper, engineering is the enabler component that transforms science into
    products. Without proper engineering, the models will keep performing on predefined
    datasets. But they will never get to real customers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take-away:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The important and often neglected skills are:'
  prefs: []
  type: TYPE_NORMAL
- en: Simplicity. Make sure your code and your models are simple, but not simplistic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Robustness. Your assumptions are wrong. Take a breath and continue to code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modularity. Divide and conquer. Dig down to the smallest problem and then find
    an open-source to solve it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fruit picking. Don’t focus only on low-hanging fruits. But make sure you have
    always something to pick.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Simplicity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Figure](../Images/9cbc87b1003e1badd5e4e9876b30dc63.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: shutterstock'
  prefs: []
  type: TYPE_NORMAL
- en: “*Entities should not be multiplied without necessity*“ — William of Ockham.
    “*Simplicity is the ultimate sophistication*” — Leonardo da Vinci. “*Everything
    should be made as simple as possible, but not simpler*” — Albert Einstein. “*That’s
    been one of my mantras — focus and simplicity*” — Steve Jobs.
  prefs: []
  type: TYPE_NORMAL
- en: I could have filled the whole page with citations devoted to simplicity. Researchers,
    designers, engineers, philosophers, and authors praised the simplicity and stated
    that simplicity has a value all of its own. Their reasons changed, but the conclusion
    was the same. You reach perfection not when there is nothing to add, but when
    there is nothing to remove.
  prefs: []
  type: TYPE_NORMAL
- en: Software engineers are absolutely aware of the value of simplicity. There are
    numerous books and articles on how to make software simpler. I remember that KISS
    principle — Keep It Simple, Stupid — was even taught at one of my undergraduate
    courses. Simple software is cheaper to maintain, easier to change, and less prone
    to bugs. There is a wide consensus on it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In data science, the situation is very different. There are a number of articles,
    for example, “[The virtue of simplicity: on ML models in algorithmic trading](https://journals.sagepub.com/doi/full/10.1177/2053951720926558)”
    by [Kristian Bondo Hansen](https://journals.sagepub.com/action/doSearch?target=default&ContribAuthorStored=Hansen%2C+Kristian+Bondo) or
    “[The role of simplicity in data science revolution](https://www.linkedin.com/pulse/role-simplicity-data-science-revolution-alfredo-gemma/)”
    by Alfredo Gemma. But they are an exception and not the rule. The mainstream of
    data scientists does not care at best and prefers complex solutions at worst.'
  prefs: []
  type: TYPE_NORMAL
- en: Before going on to the reasons why data scientists usually don’t care, why they
    should, and what to do with that, let’s see what simplicity means. According to
    the Cambridge dictionary, *it is the quality of being easy to understand or do
    and the quality of being plain, without unnecessary or extra things or decorations*.
  prefs: []
  type: TYPE_NORMAL
- en: I find that the most intuitive way to define simplicity is *via negativa*, as
    the opposite of complexity. According to the same dictionary, complexity is *consisting
    of many interconnecting parts or elements; intricate*. While we can’t always say
    that something is simple, we can usually say that something is complex. And we
    can aim not to be complex and not to create complex solutions.
  prefs: []
  type: TYPE_NORMAL
- en: The reason to seek simplicity in data science is the same reason as in all engineering
    disciplines. Simpler solutions are much, much cheaper. Real-life products are
    not Kaggle competitions. Requirements are constantly modified. A complex solution
    quickly becomes a maintenance nightmare when it needs to be adapted to new conditions.
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to understand why data scientists, especially fresh graduates, prefer
    complex solutions. They have just arrived from the academy. They have finished
    the thesis and probably even published a paper. An academic publication is judged
    by accuracy, mathematical elegance, novelty, methodology, but seldom by practicality
    and simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: A complicated idea that increases the accuracy by 0.5% is a great success for
    any student. The same idea is a failure for a data scientist. Even if its theory
    is sound, it may hide underlying assumptions that will prove as false. In any
    case, incremental improvement is hardly worth the cost of complexity.
  prefs: []
  type: TYPE_NORMAL
- en: So what to do if you, your boss, your colleagues, or your subordinates are fond
    of complex and “optimal” solutions? If it is your boss, you are probably doomed
    and you’d better start looking for a new job. In other cases, keep it simple,
    stupid.
  prefs: []
  type: TYPE_NORMAL
- en: Robustness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Figure](../Images/0721ebf7e415be6307c1feb144d5e19f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: shutterstock'
  prefs: []
  type: TYPE_NORMAL
- en: Russian culture has a concept of [avos](https://en.wiktionary.org/wiki/%D0%B0%D0%B2%D0%BE%D1%81%D1%8C)’.
    Wikipedia describes it as “blind trust in divine providence and counting on pure
    luck”. Avos’ was behind the decision of the truck’s driver to overload the truck.
    And it hides behind any non-robust solution.
  prefs: []
  type: TYPE_NORMAL
- en: What is robustness? Or specifically, what is robustness in data science? The
    definition that is most relevant to our discussion is ”the robustness of an algorithm
    is its sensitivity to discrepancies between the assumed model and reality” from [Mariano
    Scain thesis](https://www.tau.ac.il/~mansour/students/Mariano_Scain_Phd.pdf).
    Incorrect assumptions about reality are the main source of problems for data scientists.
    They are also the source of problems for the truck driver above.
  prefs: []
  type: TYPE_NORMAL
- en: Careful readers may say that robustness is also the ability of an algorithm
    to deal with errors during execution. They would be right. But it is less relevant
    to our discussion. It is a technical topic with well-defined solutions.
  prefs: []
  type: TYPE_NORMAL
- en: The necessity to build robust systems was obvious in the pre-big-data and pre-deep
    world. Feature and algorithm design were manual. Testing was commonly performed
    on hundreds, maybe thousands of examples. Even the smartest algorithm creators
    never assumed that they could think of all possible use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Did the era of big data change the nature of robustness? Why should we care
    if we can design, train, and test our models using millions of data samples representing
    all imaginable scenarios?
  prefs: []
  type: TYPE_NORMAL
- en: It figures out that robustness is still an important and unsolved issue. Each
    year top journals prove it by publishing papers dealing with algorithm robustness,
    for instance, “[Improving the Robustness of Deep Neural Networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zheng_Improving_the_Robustness_CVPR_2016_paper.pdf)”
    and “[Model-Based Robust Deep Learning](https://arxiv.org/abs/2005.10247)”. The
    quantity of data has not been translated into quality. The sheer amount of information
    used for training does not mean we can cover all use cases.
  prefs: []
  type: TYPE_NORMAL
- en: And if people are involved, the reality will always be unexpected and unimaginable.
    Most of us have difficulty telling what we will have for lunch, not to talk about
    tomorrow. Data can hardly help with predicting human behavior.
  prefs: []
  type: TYPE_NORMAL
- en: So what to do in order to make your models more robust? The first option is
    to read the appropriate papers and implement their ideas. This is fine. But the
    papers are not always generalizable. Often, you can’t copy an idea from one area
    to another.
  prefs: []
  type: TYPE_NORMAL
- en: I want to present three general practices. Following the practices does not
    guarantee robust models, but it significantly decreases the chance of fragile
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance safety margin**. Safety margins are the basis of any engineering.
    It is a common practice to take requirements and add 20–30% just to be on the
    safe side. An elevator that can hold 1000kg will easily hold 1300kg. Moreover,
    it is tested to hold 1300kg and not 1000kg. Engineers prepare for unexpected conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: What is the equivalent of a safety margin in data science? I think it is the
    KPI or success criteria. Even if something unexpected happens, you will still
    be above the threshold.
  prefs: []
  type: TYPE_NORMAL
- en: The important consequence of this practice is that you will stop chasing incremental
    improvements. You cannot be robust if your model increases a KPI by 1%. With all
    the statistical significance tests, any small change in the environment will kill
    your effort.
  prefs: []
  type: TYPE_NORMAL
- en: '**Excessive testing**. Forget the single test / train / validation division.
    You have to cross-validate your model over all possible combinations. Do you have
    different users? Divide according to the user ID and do it dozens of times. Does
    your data change over time? Divide according to timestamp and make sure that each
    day appears once in the validation group. “Spam” your data with random values
    or swap values of some features between your data points. And then test on dirty
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: I find it very useful to assume that my models have bugs until proven otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Two interesting sources on data science and ML testing — [Alex Gude’s blog](https://alexgude.com/blog/software-testing-for-data-science/) and
    “[Machine Learning with Python, A Test-Driven Approach](https://www.amazon.com/Thoughtful-Machine-Learning-Python-Test-Driven/dp/1491924136)”.
  prefs: []
  type: TYPE_NORMAL
- en: '**Don’t build castles on the sand.** Decrease the dependence on other untested
    components. And never build your model on top of another high-risk and not validated
    component. Even if the developers of that component swear that nothing can happen.'
  prefs: []
  type: TYPE_NORMAL
- en: Modularity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Figure](../Images/bc40c43aa30cd2dc2dfb04424da06026.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: shutterstock'
  prefs: []
  type: TYPE_NORMAL
- en: Modular design is an underlying principle of all modern science. It is the direct
    consequence of the analytical approach. The analytical approach is a process where
    you break down a big problem into smaller pieces. The analytical approach was
    a cornerstone of the scientific revolution.
  prefs: []
  type: TYPE_NORMAL
- en: The smaller your problem is, the better. And “the better” here is not nice to
    have. It is a must. It will save a lot of time, effort, and money. When a problem
    is small, well defined, and not accompanied by tons of assumptions, the solution
    is accurate and easy to test.
  prefs: []
  type: TYPE_NORMAL
- en: Most data scientists are familiar with modularity in the context of software
    design. But even the best programmers, whose python code is crystal clear, often
    fail to apply the modularity to data science itself.
  prefs: []
  type: TYPE_NORMAL
- en: The failure is easy to justify. Modular design requires a method to combine
    several smaller models into a big one. There exists no such method for machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'But there are practical guidelines that that I find useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transfer learning**. Transfer learning simplifies employing existing solutions.
    You can think of it as dividing your problem into two parts. The first part creates
    a low dimensional feature representation. The second part directly optimizes the
    relevant KPI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open-source**. Use out-of-the-box open-source solutions whenever possible.
    It makes your code modular by definition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Forget being optimal**. It is tempting to build from scratch a system optimized
    for your needs instead of adapting an existing solution. But it is justified only
    when you can prove that your system significantly outperforms the existing one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model ensembles**. Don’t be afraid to take several different approaches and
    throw them into a single pot. This is as most Kaggle competitions are won.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Divide your data**. Don’t try to create “one great model”, while theoretically,
    it may be possible. For example, if you deal with predicting customer behavior,
    don’t build the same model for a completely new customer and someone who has been
    using your service for a year.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check [Compositional Deep Learning ](https://medium.com/@mattia.cd.ferrini/compositional-deep-learning-a40a07351c37)for
    more details about deep learning building blocks. And read [Pruned Neural Networks
    Are Surprisingly Modular](https://arxiv.org/abs/2003.04881) for a scientific proof.
  prefs: []
  type: TYPE_NORMAL
- en: Fruit picking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Figure](../Images/f28d9809656b6c822376e47ccfed2811.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: shutterstock'
  prefs: []
  type: TYPE_NORMAL
- en: There is a constant tension between product managers and data scientists. Product
    managers want data scientists to focus on low hanging fruits. Their logic is clear.
    They say that the business cares only about the number of fruits and about where
    they grow. The more fruits we have, the better we do. They throw in all sorts
    of buzzwords — Pareto, MVP, the best is the enemy of the good, etc.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, data scientists state that the low hanging fruits spoil fast
    and taste badly. In other words, solving the easy problems has a limited impact
    and deals with symptoms and not the cause. Often, it’s an excuse to learn new
    technologies, but often they are right.
  prefs: []
  type: TYPE_NORMAL
- en: Personally, I moved between both viewpoints. After reading [P. Thiel’s Zero-To-One](https://www.amazon.com/Zero-One-Notes-Startups-Future/dp/0804139296) I
    was convinced that the low hanging fruits are a waste of time. After spending
    almost seven years in start-ups, I was sure that creating a low-hanging MVP is
    the right first step.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, I developed my own approach that unifies the two extremes. The typical
    environment of a data scientist is a dynamic and weird world where trees grow
    in all directions. And the trees switch the directions all the time. They can
    grow upside down or sideways.
  prefs: []
  type: TYPE_NORMAL
- en: The best fruits are indeed at the top. But if we spend too much time building
    the ladder, the tree will move. Therefore the best is to aim at the top but to
    constantly monitor where the top is.
  prefs: []
  type: TYPE_NORMAL
- en: Moving from metaphors to practice, there is always a chance that during a long
    development things will change. The original problem will become irrelevant, new
    data sources will appear, the original assumptions will prove false, the KPI will
    be replaced, etc.
  prefs: []
  type: TYPE_NORMAL
- en: It is great to aim at the top, but remember to do it while rolling out a working
    product every few months. The product may not bring the best fruit, but you will
    get a better sense of how the fruits grow.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Michael Kolomenkin](https://www.linkedin.com/in/michael-kolomenkin-5284942/)**
    is a father of three, AI researcher, kayak instructor, adventure seeker, reader
    and writer.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/essential-data-science-skills-that-no-one-talks-about-74bca9ec4892).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[The unspoken difference between junior and senior data scientists](/2020/10/unspoken-difference-junior-senior-data-scientists.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Getting A Data Science Job is Harder Than Ever – How to turn that to your
    advantage](/2020/10/getting-data-science-job-harder.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Advice for Aspiring Data Scientists](/2020/10/advice-aspiring-data-scientists.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
