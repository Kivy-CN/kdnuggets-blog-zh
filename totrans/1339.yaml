- en: AI Papers to Read in 2020
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/09/ai-papers-read-2020.html](https://www.kdnuggets.com/2020/09/ai-papers-read-2020.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Ygor Rebouças Serpa](https://www.linkedin.com/in/ygorreboucas/), developing
    explainable AI tools for the healthcare industry**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/bec677c37d3ca6f467317b670f40b37d.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Alfons Morales](https://unsplash.com/@alfonsmc10?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Artificial Intelligence is one of the most rapidly growing fields in science
    and is one of the most sought skills of the past few years, commonly labeled as
    Data Science. The area has far-reaching applications, being usually divided by
    input type: text, audio, image, video, or graph; or by problem formulation: supervised,
    unsupervised, and reinforcement learning. Keeping up with everything is a massive
    endeavor and usually ends up being a frustrating attempt. In this spirit, I present
    some reading suggestions to keep you updated on the latest and classic breakthroughs
    in AI and Data Science.'
  prefs: []
  type: TYPE_NORMAL
- en: Although most papers I listed deal with image and text, many of their concepts
    are fairly input agnostic and provide insight far beyond vision and language tasks.
    Alongside each suggestion, I listed some of the reasons I believe you should read
    (or re-read) the paper and added some further readings, in case you want to dive
    a bit deeper into a given subject.
  prefs: []
  type: TYPE_NORMAL
- en: Before we begin, I would like to apologize to the Audio and Reinforcement Learning
    communities for not adding these subjects to the list, as I have only limited
    experience with both.
  prefs: []
  type: TYPE_NORMAL
- en: Here we go.
  prefs: []
  type: TYPE_NORMAL
- en: '#1 AlexNet (2012)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. [“Imagenet classification
    with deep convolutional neural networks.”](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networ) *Advances
    in neural information processing systems*. 2012.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In 2012, the authors proposed the use of GPUs to train a large Convolutional
    Neural Network (CNN) for the ImageNet challenge. This was a bold move, as CNNs
    were considered too heavy to be trained on such a large scale problem. To everyone
    surprise, they won first place, with a ~15% Top-5 error rate, against ~26% of
    the second place, which used state-of-the-art image processing techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #1:** While most of us know AlexNet’s historical importance, not everyone
    knows which of the techniques we use today were already present before the boom.
    You might be surprised by how familiar many of the concepts introduced in the
    paper are, such as dropout and ReLU.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #2:** The proposed network had 60 million parameters, complete insanity
    for 2012 standards. Nowadays, we get to see models with over a billion parameters.
    Reading the AlexNet paper gives us a great deal of insight on how things developed
    since then.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Further Reading:** Following the history of ImageNet champions, you can read
    the [ZF Net](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53), [VGG](https://arxiv.org/abs/1409.1556), [Inception-v1, ](https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html)and [ResNet ](http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html)papers.
    This last one achieved super-human performance, solving the challenge. After it,
    other competitions took over the researchers’ attention. Nowadays, ImageNet is
    mainly used for Transfer Learning and to validate low-parameter models, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/0c56de4690eaf23d4c7f29e008cec3e7.png)'
  prefs: []
  type: TYPE_IMG
- en: The original portrayal of the AlexNet structure. The top and bottom halves are
    processed by GPU 1 and 2, respectively. An earlier form of model parallelism.
    Source: [The Alexnet Paper](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networ)
  prefs: []
  type: TYPE_NORMAL
- en: '#2 MobileNet (2017)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Howard, Andrew G., et al. [“Mobilenets: Efficient convolutional neural networks
    for mobile vision applications.”](https://arxiv.org/abs/1704.04861) *arXiv preprint
    arXiv:1704.04861* (2017).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: MobileNet is one of the most famous “low-parameter” networks. Such models are
    ideal for low-resources devices and to speed-up real-time applications, such as
    object recognition on mobile phones. The core idea behind MobileNet and other
    low-parameter models is to decompose expensive operations into a set of smaller
    (and faster) operations. Such compound operations are often orders-of-magnitude
    faster and use substantially fewer parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #1: **Most of us have nowhere near the resources the big tech companies
    have. Understanding the low-parameter networks is crucial to make your own models
    less expensive to train and use. In my experience, using depth-wise convolutions can
    save you hundreds of dollars in cloud inference with almost no loss to accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #2: **Common knowledge is that bigger models are stronger models.
    Papers such as MobileNet show that there is a lot more to it than adding more
    filters. Elegance matters.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Further Reading:** So far, MobileNet [v2 ](https://arxiv.org/abs/1801.04381)and [v3 ](https://arxiv.org/abs/1905.02244)have
    been released, providing new enhancements to accuracy and size. In parallel, other
    authors have devised many techniques to further reduce the model size, such as
    the [SqueezeNet](https://arxiv.org/abs/1602.07360), and to [downsize regular models
    with minimal accuracy loss.](https://arxiv.org/abs/1608.08710) [This paper](https://ieeexplore.ieee.org/abstract/document/8050276) gives
    a comprehensive summary of several models size vs accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Attention is All You Need (2017)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vaswani, Ashish, et al. [“Attention is all you need.”](http://papers.nips.cc/paper/7181-attention-is-all-you-need) *Advances
    in neural information processing systems*. 2017.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The paper that introduced the Transformer Model. Prior to this paper, language
    models relied extensively on Recurrent Neural Networks (RNN) to perform sequence-to-sequence
    tasks. However, RNNs are awfully slow, as they are terrible to parallelize to
    multi-GPUs. In contrast, the Transformer model is based solely on Attention layers,
    which are CNNs that capture the relevance of any sequence element to each other.
    The proposed formulation achieved significantly better state-of-the-art results
    and trains markedly faster than previous RNN models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #1:** Nowadays, most of the novel architectures in the Natural-Language
    Processing (NLP) literature descend from the Transformer. Models such as [GPT-2](https://www.ceid.upatras.gr/webpages/faculty/zaro/teaching/alg-ds/PRESENTATIONS/PAPERS/2019-Radford-et-al_Language-Models-Are-Unsupervised-Multitask-%20Learners.pdf) and [BERT ](https://arxiv.org/abs/1810.04805)are
    at the forefront of innovation. Understanding the Transformer is key to understanding
    most later models in NLP.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #2: **Most transformer models are in the order of billions of parameters.
    While the literature on MobileNets addresses more efficient models, the research
    on NLP addresses more efficient training. In combination, both views provide the
    ultimate set of techniques for efficient training and inference.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #3: **While the transformer model has mostly been restricted to NLP,
    the proposed Attention mechanism has far-reaching applications. Models such as [Self-Attention
    GAN](https://arxiv.org/abs/1805.08318) demonstrate the usefulness of global-level
    reasoning a variety of tasks. New papers on Attention applications pop-up every
    month.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Further Reading: **I highly recommend reading the [BERT ](https://arxiv.org/abs/1810.04805)and [SAGAN ](https://arxiv.org/abs/1805.08318)paper.
    The former is a continuation of the Transformer model, and the latter is an application
    of the Attention mechanism to images in a GAN setup.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Stop Thinking with Your Head / Reformer (~2020)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Merity, Stephen. “[Single Headed Attention RNN: Stop Thinking With Your Head](https://arxiv.org/abs/1911.11423).” *arXiv
    preprint arXiv:1911.11423* (2019).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Kitaev, Nikita, Łukasz Kaiser, and Anselm Levskaya. “[Reformer: The Efficient
    Transformer.](https://arxiv.org/abs/2001.04451)” *arXiv preprint arXiv:2001.04451* (2020).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Transformer / Attention models have attracted a lot of attention. However, these
    tend to be resource-heavy models, not meant for ordinary consumer hardware. Both
    mentioned papers criticize the architecture, providing computationally efficient
    alternatives to the Attention module. As for the MobileNet discussion, elegance
    matters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #1:** “[Stop Thinking With Your Head](https://arxiv.org/abs/1911.11423)”
    is a damn funny paper to read. This counts as a reason on its own.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #2: **Big companies can quickly scale their research to a hundred
    GPUs. We, normal folks, can’t. Scaling the size of models is not the only avenue
    for improvement. I can’t overstate that. Reading about efficiency is the best
    way to ensure you are efficiently using your current resources.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Further Reading:** Since these are late 2019 and 2020, there isn’t much to
    link. Consider reading the [MobileNet paper ](https://arxiv.org/abs/1704.04861)(if
    you haven’t already) for other takes on efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Human Baselines for Pose Estimation (2017)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Xiao, Bin, Haiping Wu, and Yichen Wei. [“Simple baselines for human pose estimation
    and tracking.” ](http://openaccess.thecvf.com/content_iccv_2017/html/Martinez_A_Simple_yet_ICCV_2017_paper.html)*Proceedings
    of the European conference on computer vision (ECCV)*. 2018.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So far, most papers have proposed new techniques to improve the state-of-the-art.
    This paper, on the opposite, argues that a simple model, using current best practices,
    can be surprisingly effective. In sum, they proposed a human pose estimation network
    based solely on a backbone network followed by three de-convolution operations.
    At the time, their approach was the most effective at handling the COCO benchmark,
    despite its simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #1:** Being simple is sometimes the most effective approach. While
    we all want to try the shiny and complicated novel architectures, a baseline model
    might be way faster to code and, yet, achieve similar results. This paper reminds
    us that not all good models need to be complicated.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #2:** Science moves in baby steps. Each new paper pushes the state-of-the-art
    a bit further. Yet, it does not need to be a one-way road. Sometimes it is worthwhile
    to backtrack a bit and take a different turn. “Stop Thinking with Your Head,”
    and “Reformer” are two other good examples of this.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #3:** Proper data augmentation, training schedules, and a good problem
    formulation matter more than most people would acknowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Further Reading: **If interested in the Pose Estimation topic, you might
    consider reading [this comprehensive state-of-the-art review.](https://nanonets.com/blog/human-pose-estimation-2d-guide/)'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Bag of Tricks for Image Classification (2019)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: He, Tong, et al. [“Bag of tricks for image classification with convolutional
    neural networks.”](https://arxiv.org/abs/1812.01187) *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*. 2019.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Many times, what you need is not a fancy new model, just a couple of new tricks.
    In most papers, one or two new tricks are introduced to achieve a one or two percentage
    points improvement. However, these are often forgotten amid the major contributions.
    This paper collects a set of tips used throughout the literature and summarizes
    them for our reading pleasure.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #1: **Most tips are easily applicable'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #2: **High are the odds you are unaware of most approaches. These
    are not the typical “use [ELU](https://www.tensorflow.org/api_docs/python/tf/nn/elu)”
    kind of suggestions.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Further Readings:** Many other tricks exist, some are problem-specific, some
    are not. A topic I believe deserves more attention is class and sample weights.
    Consider reading [this paper on class weights for unbalanced datasets](http://openaccess.thecvf.com/content_CVPR_2019/papers/Cui_Class-Balanced_Loss_Based_on_Effective_Number_of_Samples_CVPR_2019_paper.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 The SELU Activation (2017)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Klambauer, Günter, et al. [“Self-normalizing neural networks.”](https://arxiv.org/abs/1706.02515) *Advances
    in neural information processing systems*. 2017.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Most of us use Batch Normalization layers and the ReLU or ELU activation functions.
    In the SELU paper, the authors propose a unifying approach: an activation that
    self-normalizes its outputs. In practice, this renders batch normalization layers
    obsolete. Therefore, models using SELU activations are simpler and need fewer
    operations.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #1:** In the paper, the authors mostly deal with standard machine
    learning problems (tabular data). Most data scientists deal primarily with images.
    Reading a paper on purely dense networks is a bit of a refreshment.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #2: **If you have to deal with tabular data, this is one of the most
    up-to-date approaches to the topic within the Neural Networks literature.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #3:** The paper is math-heavy and uses a computationally derived proof.
    This, in itself, is a rare but beautiful thing to be seen.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Further Reading:** If you want to dive into the history and usage of the
    most popular activation functions, I wrote a [guide on activation functions](https://towardsdatascience.com/a-comprehensive-guide-on-activation-functions-b45ed37a4fa5) here
    on Medium. Check it out :)'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Bag-of-local-Features (2019)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Brendel, Wieland, and Matthias Bethge. [“Approximating cnns with bag-of-local-features
    models works surprisingly well on imagenet.”](https://arxiv.org/abs/1904.00760) *arXiv
    preprint arXiv:1904.00760* (2019).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you break an image into jigsaw-like pieces, scramble them, and show them
    to a kid, it won’t be able to recognize the original object; a CNN might. In this
    paper, the authors found that classifying all 33x33 patches of an image and then
    averaging their class predictions achieves near state-of-the-art results on ImageNet.
    Moreover, they further explore this idea with VGG and ResNet-50 models, showing
    evidence that CNNs rely extensively on local information, with minimal global
    reasoning
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #1:** While many believe that CNNs “see,” this paper shows evidence
    that they might be way dumber than we would dare to bet our money.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #2: **Only once in a while we get to see a paper with a fresh new
    take on the limitations of CNNs and their interpretability.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Further Reading: **Related in its findings, the adversarial attacks literature
    also shows other striking limitations of CNNs. Consider reading the following
    article (and its reference section):'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Breaking neural networks with adversarial attacks**](https://towardsdatascience.com/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa)'
  prefs: []
  type: TYPE_NORMAL
- en: Are the machine learning models we use intrinsically flawed?
  prefs: []
  type: TYPE_NORMAL
- en: '#9 The Lottery Ticket Hypothesis (2019)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Frankle, Jonathan, and Michael Carbin. [“The lottery ticket hypothesis: Finding
    sparse, trainable neural networks.”](https://arxiv.org/abs/1803.03635) *arXiv
    preprint arXiv:1803.03635* (2018).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Continuing on the theoretical papers, Frankle *et al. *found that if you train
    a big network, prune all low-valued weights, rollback the pruned network, and
    train again, you will get a better performing network. The lottery analogy is
    seeing each weight as a “lottery ticket.” With a billion tickets, winning the
    prize is certain. However, most of the tickets won’t win, only a couple will.
    If you could go back in time and buy only the winning tickets, you would maximize
    your profits. “A billion tickets” is a big initial network. “Training” is running
    the lottery and seeing which weights are high-valued. “Going back in time” is
    rolling-back to the initial untrained network and rerunning the lottery. In the
    end, you will get a better performing network.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #1: **The idea is insanely cool.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #2:** As for the Bag-of-Features paper, this sheds some light on how
    limited our current understanding of CNNs is. After reading this paper, I realized
    how underutilized our millions of parameters are. An open question is how much.
    The authors managed to reduce networks to a tenth of their original sizes, how
    much more might be possible in the future?'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #3: **These ideas also give us more perspective on how inefficient
    behemoth networks are. Consider the Reformer paper, mentioned before. It drastically
    reduced the size of the Transformer by improving the algorithm. How much more
    could be reduced by using the lottery technique?'
  prefs: []
  type: TYPE_NORMAL
- en: '**Further Reading: **Weight initialization is an often overlooked topic. In
    my experience, most people stick to the defaults, [which might not always be the
    best option.](https://towardsdatascience.com/why-default-cnn-are-broken-in-keras-and-how-to-fix-them-ce295e5e5f2) [“All
    You Need is a Good Init”](https://arxiv.org/abs/1511.06422) is a seminal paper
    on the topic. As for the lottery hypothesis, the following is an easy to read
    review:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Breaking down the Lottery Ticket Hypothesis**](https://towardsdatascience.com/breaking-down-the-lottery-ticket-hypothesis-ca1c053b3e58)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Distilling the ideas from MIT CSAIL’s intriguing paper: “The Lottery Ticket
    Hypothesis: Finding Sparse, Trainable...'
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Pix2Pix and CycleGAN (2017)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Isola, Phillip, et al. “[Image-to-image translation with conditional adversarial
    networks.”](https://phillipi.github.io/pix2pix/) *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 2017.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Zhu, Jun-Yan, et al. [“Unpaired image-to-image translation using cycle-consistent
    adversarial networks.”](http://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html) *Proceedings
    of the IEEE international conference on computer vision*. 2017.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This list would not be complete without some GAN papers.
  prefs: []
  type: TYPE_NORMAL
- en: Pix2Pix and CycleGAN are the two seminal works on conditional generative models.
    Both perform the task of converting images from a domain A to a domain B and differ
    by leveraging paired and unpaired datasets. The former perform tasks such as converting
    line drawings to fully rendered images, and the latter excels at replacing entities,
    such as turning horses into zebras or apples into oranges. By being “conditional,”
    these models allow users to have some degree of control over what is being generated
    by tweaking the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #1: **GAN papers are usually focused on the sheer quality of the generated
    results and place no emphasis on artistic control. Conditional models, such as
    these, provide an avenue for GANs to actually become useful in practice. [For
    instance, at being a virtual assistant to artists.](https://www.sbgames.org/sbgames2019/files/papers/ComputacaoFull/197880.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #2:** Adversarial approaches are the best examples of multi-network
    models. While generation might not be your thing, reading about multi-network
    setups might be inspiring for a number of problems.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason #3: **The CycleGAN paper, in particular, demonstrates how an effective
    loss function can work wonders at solving some difficult problems. A similar idea
    is given by the Focal loss paper, which considerably improves object detectors
    by just replacing their traditional losses for a better one.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Further Reading: **While AI is growing fast, GANs are growing faster. I highly
    recommend coding a GAN if you never have. [Here are the official Tensorflow 2
    docs on the matter](https://www.tensorflow.org/tutorials/generative/dcgan). One
    application of GANs that is not so well known (and you should check out) is [semi-supervised
    learning](https://arxiv.org/abs/1905.06484).'
  prefs: []
  type: TYPE_NORMAL
- en: With these twelve papers and their further readings, I believe you already have
    plenty of reading material to look at. This surely isn’t an exhaustive list of
    great papers. However, I tried my best to select the most insightful and seminal
    works I have seen and read. Please let me know if there are any other papers you
    believe should be on this list.
  prefs: []
  type: TYPE_NORMAL
- en: Good reading :)
  prefs: []
  type: TYPE_NORMAL
- en: '**Edit: After writing this list, I compiled a second one with ten more AI papers
    read in 2020 and a third on GANs. If you enjoyed reading this list, you might
    enjoy its continuations:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Ten More AI Papers to Read in 2020**](https://towardsdatascience.com/ten-more-ai-papers-to-read-in-2020-8c6fb4650a9b)'
  prefs: []
  type: TYPE_NORMAL
- en: Additional reading suggestions to keep you up-to-date with the latest and classic
    breakthroughs in AI and Data Science
  prefs: []
  type: TYPE_NORMAL
- en: '[**GAN Papers to Read in 2020**](https://towardsdatascience.com/gan-papers-to-read-in-2020-2c708af5c0a4)'
  prefs: []
  type: TYPE_NORMAL
- en: Reading suggestions on Generative Adversarial Networks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Ygor Rebouças Serpa](https://www.linkedin.com/in/ygorreboucas/)** ([**@ReboucasYgor**](https://twitter.com/ReboucasYgor))
    is a Master in Computer Science, by Universidade de Fortaleza, and is currently
    working on R&D, developing explainable AI tools for the healthcare industry. His
    interests range from music and game development to theoretical computer science
    and AI. See his **[Medium](https://medium.com/@ygorrebouasserpa)** profile for
    more writing.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/ai-papers-to-read-in-2020-ac0e4e91d915).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[13 must-read papers from AI experts](/2020/05/13-must-read-papers-ai-experts.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Must-read NLP and Deep Learning articles for Data Scientists](/2020/08/must-read-nlp-deep-learning-articles.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Essential Papers on Sentiment Analysis](/2020/06/5-essential-papers-sentiment-analysis.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Must Read NLP Papers from the Last 12 Months](https://www.kdnuggets.com/2023/03/must-read-nlp-papers-last-12-months.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, April 27: A Brief Introduction to Papers With Code;…](https://www.kdnuggets.com/2022/n17.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Machine Learning Papers to Read in 2023](https://www.kdnuggets.com/2023/03/top-machine-learning-papers-read-2023.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Generative Agent Research Papers You Should Read](https://www.kdnuggets.com/generative-agent-research-papers-you-should-read)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Machine Learning Papers to Read in 2024](https://www.kdnuggets.com/5-machine-learning-papers-to-read-in-2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Brief Introduction to Papers With Code](https://www.kdnuggets.com/2022/04/brief-introduction-papers-code.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
