["```py\n# local setup\nvirtualenv diff_env –python=python3.8\nsource diff_env/bin/activate\npip install diffusers transformers huggingface-hub\npip install torch --index-url https://download.pytorch.org/whl/cu118\n```", "```py\nimport os\nimport numpy as np\nimport torch\nfrom diffusers import StableDiffusionPipeline, AutoPipelineForImage2Image\nfrom diffusers.pipelines.pipeline_utils import numpy_to_pil\nfrom transformers import CLIPTokenizer, CLIPTextModel\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, \\\n       PNDMScheduler, LMSDiscreteScheduler\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n```", "```py\nprompt = [\" \"]\n\n# image settings\nheight, width = 512, 512\n\n# diffusion settings\nnumber_inference_steps = 64\nguidance_scale = 9.0\nbatch_size = 1\n```", "```py\ndef seed_all(seed):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n\nseed_all(193)\n```", "```py\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", \\\n        subfolder=\"vae\")\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\",\\\n        subfolder=\"unet\")\nscheduler = PNDMScheduler()\nscheduler.set_timesteps(number_inference_steps)\n\nmy_device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nvae = vae.to(my_device)\ntext_encoder = text_encoder.to(my_device)\nunet = unet.to(my_device)\n```", "```py\nprompt = prompt * batch_size\ntokens = tokenizer(prompt, padding=\"max_length\",\\\nmax_length=tokenizer.model_max_length, truncation=True,\\\n        return_tensors=\"pt\")\n\nempty_tokens = tokenizer([\"\"] * batch_size, padding=\"max_length\",\\\nmax_length=tokenizer.model_max_length, truncation=True,\\\n        return_tensors=\"pt\")\nwith torch.no_grad():\n    text_embeddings = text_encoder(tokens.input_ids.to(my_device))[0]\n    max_length = tokens.input_ids.shape[-1]\n    notext_embeddings = text_encoder(empty_tokens.input_ids.to(my_device))[0]\n    text_embeddings = torch.cat([notext_embeddings, text_embeddings])\n```", "```py\nlatents = torch.randn(batch_size, unet.config.in_channels, \\\n        height//8, width//8)\nlatents = (latents * scheduler.init_noise_sigma).to(my_device)\n```", "```py\nimages = []\ndisplay_every = number_inference_steps // 8\n\n# diffusion loop\nfor step_idx, timestep in enumerate(scheduler.timesteps):\n    with torch.no_grad():\n        # concatenate latents, to run null/text prompt in parallel.\n        model_in = torch.cat([latents] * 2)\n        model_in = scheduler.scale_model_input(model_in,\\\n                timestep).to(my_device)\n        predicted_noise = unet(model_in, timestep, \\\n                encoder_hidden_states=text_embeddings).sample\n        # pnu - empty prompt unconditioned noise prediction\n        # pnc - text prompt conditioned noise prediction\n        pnu, pnc = predicted_noise.chunk(2)\n        # weight noise predictions according to guidance scale\n        predicted_noise = pnu + guidance_scale * (pnc - pnu)\n        # update the latents\n        latents = scheduler.step(predicted_noise, \\\n                timestep, latents).prev_sample\n        # Periodically log images and print progress during diffusion\n        if step_idx % display_every == 0\\\n                or step_idx + 1 == len(scheduler.timesteps):\n           image = vae.decode(latents / 0.18215).sample[0]\n           image = ((image / 2.) + 0.5).cpu().permute(1,2,0).numpy()\n           image = np.clip(image, 0, 1.0)\n           images.extend(numpy_to_pil(image))\n           print(f\"step {step_idx}/{number_inference_steps}: {timestep:.4f}\")\n```", "```py\ndef seed_all(seed):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n\ndef grid_show(images, rows=3):\n    number_images = len(images)\n    height, width = images[0].size\n    columns = int(np.ceil(number_images / rows))\n    grid = np.zeros((height*rows,width*columns,3))\n    for ii, image in enumerate(images):\n        grid[ii//columns*height:ii//columns*height+height, \\\n                ii%columns*width:ii%columns*width+width] = image\n        fig, ax = plt.subplots(1,1, figsize=(3*columns, 3*rows))\n        ax.imshow(grid / grid.max())\n    return grid, fig, ax\n\ndef callback_stash_latents(ii, tt, latents):\n    # adapted from fastai/diffusion-nbs/stable_diffusion.ipynb\n    latents = 1.0 / 0.18215 * latents\n    image = pipe.vae.decode(latents).sample[0]\n    image = (image / 2\\. + 0.5).cpu().permute(1,2,0).numpy()\n    image = np.clip(image, 0, 1.0)\n    images.extend(pipe.numpy_to_pil(image))\n\nmy_seed = 193\n```", "```py\nif (1):\n    #Run CompVis/stable-diffusion-v1-4 on GPU\n    pipe_name = \"CompVis/stable-diffusion-v1-4\"\n    my_dtype = torch.float16\n    my_device = torch.device(\"cuda\")\n    my_variant = \"fp16\"\n    pipe = StableDiffusionPipeline.from_pretrained(pipe_name,\\\n    safety_checker=None, variant=my_variant,\\\n        torch_dtype=my_dtype).to(my_device)\nelse:\n    #Run CompVis/stable-diffusion-v1-4 on CPU\n    pipe_name = \"CompVis/stable-diffusion-v1-4\"\n    my_dtype = torch.float32\n    my_device = torch.device(\"cpu\")\n    pipe = StableDiffusionPipeline.from_pretrained(pipe_name, \\\n            torch_dtype=my_dtype).to(my_device)\n```", "```py\nguidance_images = []\nfor guidance in [0.25, 0.5, 1.0, 2.0, 4.0, 6.0, 8.0, 10.0, 20.0]:\n    seed_all(my_seed)\n    my_output = pipe(my_prompt, num_inference_steps=50, \\\n    num_images_per_prompt=1, guidance_scale=guidance)\n    guidance_images.append(my_output.images[0])\n    for ii, img in enumerate(my_output.images):\n        img.save(f\"prompt_{my_seed}_g{int(guidance*2)}_{ii}.jpg\")\n\ntemp = grid_show(guidance_images, rows=3)\nplt.savefig(\"prompt_guidance.jpg\")\nplt.show()\n```", "```py\nmy_prompt = \" \"\nmy_negative_prompt = \" \"\n\noutput_x = pipe(my_prompt, num_inference_steps=50, num_images_per_prompt=9, \\\n        negative_prompt=my_negative_prompt)\n\ntemp = grid_show(output_x)\nplt.show()\n```", "```py\npipe_img2img = AutoPipelineForImage2Image.from_pretrained(\\\n\n        \"runwayml/stable-diffusion-v1-5\", safety_checker=None,\\\n\ntorch_dtype=my_dtype, use_safetensors=True).to(my_device)\n```", "```py\nurl = \\\n\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/38/TRAPPIST-1e_artist_impression_2018.png/600px-TRAPPIST-1e_artist_impression_2018.png\"\nimg_path = url.split(\"/\")[-1]\nif not (os.path.exists(\"600px-TRAPPIST-1e_artist_impression_2018.png\")):\n    os.system(f\"wget \\     '{url}'\")\n    init_image = Image.open(img_path)\n\nseed_all(my_seed)\n\ntrappist_prompt = \"Artist's impression of TRAPPIST-1e\"\\\n                  \"large Earth-like water-world exoplanet with oceans,\"\\\n                  \"NASA, artist concept, realistic, detailed, intricate\"\n\nmy_negative_prompt = \"cartoon, sketch, orbiting moon\"\n\nmy_output_trappist1e = pipe_img2img(prompt=trappist_prompt, num_images_per_prompt=9, \\\n     image=init_image, negative_prompt=my_negative_prompt, guidance_scale=6.0)\n\ngrid_show(my_output_trappist1e.images)\nplt.show()\n```"]