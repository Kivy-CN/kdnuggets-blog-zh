- en: Memory Complexity with Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/12/memory-complexity-transformers.html](https://www.kdnuggets.com/2022/12/memory-complexity-transformers.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The key innovation in Transformers is the introduction of a self-attention mechanism,
    which computes similarity scores for all pairs of positions in an input sequence,
    and can be evaluated in parallel for each token of the input sequence, avoiding
    the sequential dependency of recurrent neural networks, and enabling Transformers
    to vastly outperform previous sequence models like LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of deep explanations elsewhere so here I’d like to share some
    example questions in an **interview setting.**
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: What’s the problem with running a transformer model on a book with 1 million
    tokens? What can be a solution to this problem?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Memory Complexity with Transformers](../Images/80ca231462808dc45630f195ba2547af.png)'
  prefs: []
  type: TYPE_IMG
- en: '![Memory Complexity with Transformers](../Images/2d61db24c250ff1ddf07881a6494581f.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Here are some tips for readers’ reference:**'
  prefs: []
  type: TYPE_NORMAL
- en: Simply put,
  prefs: []
  type: TYPE_NORMAL
- en: If you try to run a large transformer on the long sequence, you just run out
    of memory.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'According to the Google Research Blog (2021):'
  prefs: []
  type: TYPE_NORMAL
- en: A limitation of existing Transformer models and their derivatives is that the
    full [self-attention mechanism](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) has
    computational and memory requirements that are quadratic with the input sequence
    length. With commonly available current hardware and model sizes, this typically
    limits the input sequence to roughly 512 tokens, and prevents Transformers from
    being directly applicable to tasks that require larger context, like [question
    answering](https://huggingface.co/tasks/question-answering), [document summarization](https://arxiv.org/pdf/1804.05685) or [genome
    fragment classification](https://www.biorxiv.org/content/10.1101/353474v3.full).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Check the explanation by***Dr.Younes Bensouda Mourri ***from *[*Deeplearning.ai*](https://www.deeplearning.ai/courses/natural-language-processing-specialization/)*:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Check the explanation!](https://www.coursera.org/lecture/attention-models-in-nlp/transformer-complexity-oGXK3)'
  prefs: []
  type: TYPE_NORMAL
- en: To solve the memory complexity problem of the transformer models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two ‘reforms’ were made to the Transformer to make it more memory and compute
    efficient: the Reversible Layers reduce memory and the Locality Sensitive Hashing(LSH) reduces
    the cost of the Dot Product attention for large input sizes.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Of course, there are other solutions such as [Extended Transformer Construction](https://arxiv.org/abs/2004.08483) (ETC)
    and the like. We will cover more details in a later post!
  prefs: []
  type: TYPE_NORMAL
- en: '*Happy practicing!*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Note:** there are different angles to answer an interview question. The author
    of this newsletter does not try to find a reference that answers a question exhaustively.
    Rather, the author would like to share some quick insights and help the readers
    to think, practice and do further research as necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: Source of images/Good reads: [Paper](https://arxiv.org/pdf/2112.04426.pdf).
    Improving language models by retrieving from trillions of tokens by Deepmind (2022) [Blog](https://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html).
    Constructing Transformers For Longer Sequences with Sparse Attention Methods by
    Google (2021)
  prefs: []
  type: TYPE_NORMAL
- en: Source of video/answers: [Natural Language Processing with Attention Models ](https://www.coursera.org/learn/attention-models-in-nlp)by **Dr.Younes
    Bensouda Mourri ***from *[*Deeplearning.ai*](https://www.deeplearning.ai/courses/natural-language-processing-specialization/)
  prefs: []
  type: TYPE_NORMAL
- en: '**[Angelina Yang](https://www.linkedin.com/in/yangyy/)** is data and machine
    learning senior executive with more than 15 years of experience delivering advanced
    machine learning solutions and capabilities to increase business values in the
    financial service and fintech industry. Expertise includes AI/ML/NLP/DL model
    development and deployment in the areas of customer experience, surveillance,
    conversational AI, risk and compliance, marketing, operations, pricing and data
    services.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/@angelina.yang/memory-complexity-with-transformers-c4e1517670b1).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Linear Regression Model Selection: Balancing Simplicity and Complexity](https://www.kdnuggets.com/2023/02/linear-regression-model-selection-balancing-simplicity-complexity.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introduction to Memory Profiling in Python](https://www.kdnuggets.com/introduction-to-memory-profiling-in-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Perform Memory-Efficient Operations on Large Datasets with Pandas](https://www.kdnuggets.com/how-to-perform-memory-efficient-operations-on-large-datasets-with-pandas)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Simple NLP Pipelines with HuggingFace Transformers](https://www.kdnuggets.com/2023/02/simple-nlp-pipelines-huggingface-transformers.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Comparing Natural Language Processing Techniques: RNNs, Transformers, BERT](https://www.kdnuggets.com/comparing-natural-language-processing-techniques-rnns-transformers-bert)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Fine-Tune BERT for Sentiment Analysis with Hugging Face Transformers](https://www.kdnuggets.com/how-to-fine-tune-bert-sentiment-analysis-hugging-face-transformers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
