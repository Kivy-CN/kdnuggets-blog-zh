- en: Demystifying Bad Science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/01/demystifying-bad-science.html](https://www.kdnuggets.com/2022/01/demystifying-bad-science.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Demystifying Bad Science](../Images/85ecf1cafa11eb770a7a5ab07b19c7be.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [LoganArt on Pixabay](https://pixabay.com/users/loganart-665411/)
  prefs: []
  type: TYPE_NORMAL
- en: Many widely-held scientific theories were later proven wrong, as this brief [**article**](https://blog.chron.com/sciguy/2010/11/the-top-10-most-spectacularly-wrong-widely-held-scientific-theories/) shows.
    How can this happen?
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: First, science is still evolving, and our understanding of many basic phenomena
    remains far from complete. Another reason is that science - at least on our planet
    - is conducted by humans and we humans have many foibles. Biases of various sorts,
    funding conflicts, egos, and sheer incompetence are some of the very human things
    that can undermine any research.
  prefs: []
  type: TYPE_NORMAL
- en: Scientists sometimes get it right but those reporting it get it wrong. Few journalists
    have worked as scientists and most have had no more training in science than the
    majority of their readers or viewers. To be fair, though, many scientists themselves
    have had limited coursework in research methods and statistics, as I point out
    in [****Statistical Mistakes Even Scientists Make****](https://www.linkedin.com/pulse/statistical-mistakes-even-scientists-make-kevin-gray/)**.**
  prefs: []
  type: TYPE_NORMAL
- en: Peer review can sometime resemble chum review and, moreover, some studies make
    the front page without having been peer reviewed at all. Few editors and reviewers
    of scientific publications are statisticians - there aren't that many statisticians
    and they have their day jobs. In "softer"​ fields standards are arguably even
    less rigorous.
  prefs: []
  type: TYPE_NORMAL
- en: Null Hypothesis Significance Testing (NHST) has been heavily criticized by statisticians
    over the years. Many of you will remember this from an introductory statistics
    course. While on the surface it may seem straightforward, NHST is widely misunderstood
    and misused.
  prefs: []
  type: TYPE_NORMAL
- en: '**The American Statistician** has devoted a full open-access [**issue**](https://amstat.tandfonline.com/doi/full/10.1080/00031305.2019.1583913#.X28Ecj_iuUk) to
    this and associated topics. Put simply, one important concern is that findings
    with **p** values greater than .05 are less likely to be accepted for publication,
    or even submitted for publication, than those with statistically significant findings.
    This is known as publication bias or the [**file drawer problem**](https://web.ma.utexas.edu/users/mks/statmistakes/filedrawer.html).'
  prefs: []
  type: TYPE_NORMAL
- en: However, "negative findings" are just as important as statistically significant
    results and many potentially important research results apparently never see the
    light of day. Since accumulation of knowledge is the essence of science, this
    is a serious problem which only recently has been getting the attention many statisticians
    have long felt it warranted. Statistical significance is not the same as decision
    significance, either.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason is that small sample studies are common in many fields. While
    a gigantic sample does not automatically imply that findings can be trusted, estimates
    of [**effect sizes**](https://www.simplypsychology.org/effect-size.html) are much
    more variable from study to study when samples are small. Conversely, trivial
    effect sizes with little clinical or business significance may be statistically
    significant when sample sizes are large and, in some instances, receive extensive
    publicity.
  prefs: []
  type: TYPE_NORMAL
- en: Non-experimental (observational) research is prevalent in many disciplines and
    in the era of big data seems to be experiencing a boom. While randomized experiments
    are not always feasible or ethical, this does not mean that non-experimental research
    is sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'I summarize some of these issues in [****Propensity Scores: What they are and
    what they do****](https://www.linkedin.com/pulse/propensity-scores-what-do-kevin-gray/) and [****Meta-analysis
    and Marketing Research****](https://www.linkedin.com/pulse/meta-analysis-marketing-research-kevin-gray/)**. **Put
    simply, effect size estimates are generally more variable - less reliable - in
    non-experimental research. Back to publication bias again...'
  prefs: []
  type: TYPE_NORMAL
- en: Thousands of studies are conducted each year around the world, which means there
    would be a lot of bad science even if standards were uniformly high. Science is
    hard. Here are a few things to a watch out for.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cross-sectional versus longitudinal** data. Causes should precede their effects
    and, in observational studies, it is usually not possible to ascertain this ordering
    when data pertain to a single slice in time. However, when data are collected
    at more than one point in time, we can often confirm whether or not a hypothetical
    cause did in fact precede its hypothesized effect. Longitudinal data, in general,
    permit a broader range of analyses which can help us better understand how variables
    interrelate.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ecological studies** are often problematic since the unit analysis being
    studied is the group and, therefore inferences cannot be made about individual
    study participants. Researchers often have no data at the individual level regarding
    exposure and disease.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Non-probability sampling.** Inferential statistics assumes probability sampling.
    When the data are from convenience samples or other non-probability samples, it
    is difficult to know which population we can generalize the results too.'
  prefs: []
  type: TYPE_NORMAL
- en: '[**WEIRD**](https://rationalwiki.org/wiki/WEIRD)participants. In some fields,
    such as epidemiology and pharmacology, participants may not even be human. Generalizing
    from rats to humans requires many assumptions, for example.'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Linear no-threshold model (LNT)**](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6376521/)**. **This
    is a highly simplistic type of dose-response model that has been severely criticized.
    In business, linear (straight line) relationships are often assumed between "dose"​
    (e.g. aspects of customer experience) and "response"​ (e.g., overall satisfaction
    with a firm). This is often reasonable, but not always, and may make little sense
    in fields such as toxicology, where it is known that very small doses typically
    have no effect and, beyond a certain level, toxicity no longer increases with
    dose.'
  prefs: []
  type: TYPE_NORMAL
- en: '**An inappropriate statistical model.** This can be hard to detect, but it''s
    fair to say questionable use of statistics is not uncommon in any discipline. An
    appropriate statistical model can also be used in inappropriate ways.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inadequate covariate control.** This is especially prevalent in observational
    research, in which potentially important background variables are not always adjusted
    for. In some studies, continuous variables such as age are grouped into broad
    categories, resulting in a loss of information. Therefore, it may be questionable
    to claim that the variable has been "controlled for."'
  prefs: []
  type: TYPE_NORMAL
- en: '**Omitted variables.** Important variables may not have been available or,
    for whatever reason, might have been left out of the analysis. Many studies are
    criticized on these grounds.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Failure to consider other explanations.** Multiple causes might lead to the
    same effect, and the failure to consider rival explanations undermines a study''s
    credibility. As with omitted variables, this may be accidental or intentional.'
  prefs: []
  type: TYPE_NORMAL
- en: '**No corrections for multiple comparisons.** Statistical tests on the same
    data are not independent of each other. Thus, if pairwise comparisons are made
    among four types of patients at the standard .05 alpha level, six tests will be
    necessary and the overall confidence level for the set of comparisons is about
    75%, not 95%.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use of surrogates as the dependent variable.** Often, it''s not possible
    to measure the outcome directly and researchers must rely on surrogates. An example
    in medical research would be the use of test results to indicate the presence
    of a particular disease. While the use of surrogates is not necessarily a flaw,
    in some studies it may be problematic.'
  prefs: []
  type: TYPE_NORMAL
- en: '**No adjustments for measurement error.** It most research, variables are measured
    with error. In some cases, such as personality assessments or aptitude measurement,
    the error can be substantial. In general, measurement error attenuates correlations,
    thus the relationship between **x** and **y** may be stronger than it appears
    based on correlations or other measures of association. One kind of measurement
    error that plagues surveys is response style, for example, when a respondent tends
    to use the high end of the scale irrespective of what is being rated.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mine until you find** seems to be the motto of some researchers, and this
    is a manifestation of a particularly dangerous form of malpractice known as [**HARKing**](https://en.wikipedia.org/wiki/HARKing).
    [****Stuff Happens****](https://www.linkedin.com/pulse/stuff-happens-kevin-gray/)
    elaborates a bit more on this complex subject.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression to the mean** is a statistical phenomenon that can make natural
    variation in repeated data look like real change. It happens when unusually large
    or small measurements tend to be followed by measurements that are closer to the
    mean. This phenomenon can make it appear that an educational program or therapy,
    for example, was effective when, in actuality, it was not.'
  prefs: []
  type: TYPE_NORMAL
- en: '**"Millions affected."​** Headlines such as this may conceal tiny effect sizes
    that are statistically zero. We need to consider the **base sizes** on which these
    sorts of frightening figures have been calculated.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use of new and untested methodology.** New is not always better and tried-and-true
    methodologies are normally more trustworthy than novel ones that have not yet
    been scrutinized by independent researchers and statisticians.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Conflicts with other research**. A controversial finding may be a paradigm
    buster but may also indicate poor methodology or improper use of statistics.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Funding conflicts** may undermine the credibility of a study, but accusations
    of funding conflicts may themselves have been funded in questionable ways.'
  prefs: []
  type: TYPE_NORMAL
- en: The [**ecological fallacy**](https://www.britannica.com/science/ecological-fallacy), [**Simpson’s
    paradox**](https://www.britannica.com/topic/Simpsons-paradox), and [**Berkson’s
    bias**](https://en.wikipedia.org/wiki/Berkson%27s_paradox) are other things to
    watch out for.
  prefs: []
  type: TYPE_NORMAL
- en: The origin of the popular quote "​**lies, damned lies, and statistics**"​ is
    uncertain, though it has been attributed to Mark Twain, Disraeli, and several
    others. However, regardless of its origin, it did **not **refer to the modern
    field of statistics, which was just beginning to emerge at the time. Most likely,
    it pertained to official figures, which is what statistics originally meant. Here
    are some ways to lie with statistics.
  prefs: []
  type: TYPE_NORMAL
- en: '**Establishing “truth” through repetition** is a very common tactic and one
    Joseph Goebbels, a noted authority on deception, explicitly recommended. Few people
    scrutinize claims, and fewer yet will remember past predictions by the same person
    or organization that turned out to be badly wrong.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Straw man arguments** **and "rebuttals"​** are commonplace, as are **ad hominem**
    attacks, and both are especially useful for those who themselves have something
    to hide.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generalizing from the exception** and making rare events seem typical are
    also popular tactics. Confusing the possible with the plausible and the plausible
    with fact is a variation on this theme.'
  prefs: []
  type: TYPE_NORMAL
- en: There is also **cherry picking** of data, models and previous research. One
    form of cherry picking is selecting only the section of a time series that supports
    one’s case. "Adjusting" data falls short of outright fabrication but is a related
    technique. Clever, if questionable, interpretations of data or statistical models
    are two more weapons of the unethical.
  prefs: []
  type: TYPE_NORMAL
- en: '**Computer simulations** are sometimes misleadingly called experiments and
    simulated data subtlety passed off as empirical data. Cooking up a computer model
    to "prove"​ one''s theory is now easier than ever.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Societies are hierarchical** by nature and humans are inclined to **think
    dichotomously**, thus authorities are often invoked and debates over scientific
    or policy issues frequently take on a good guys versus bad guys flavor. Misrepresenting
    what authorities really believe is also not unheard of.'
  prefs: []
  type: TYPE_NORMAL
- en: We also struggle to migrate between **frequencies**, **percentages**, and **proportions**,
    and this is something we need to be very mindful of. For example, we may read
    that millions of people will be affected if policy makers do this or don’t do
    that. A close reading of the evidence cited, however, may reveal a very weak effect
    size whose confidence or credible interval overlaps with zero. Multiplying a tiny
    faction by hundreds of millions or billions of people will yield a terrifying
    figure. Also, bear in mind that a “50% increase” could mean from .001 to .0015.
  prefs: []
  type: TYPE_NORMAL
- en: I haven’t mentioned **data visualizations**, which can easily deceive us. Many
    people are led to believe that **random** means even when, in fact, evenly-distributed
    figures are very unlikely to be random. There is also what I call **Whack-A-Mole**,
    citing one dubious claim after another in rapid succession without responding
    to criticisms of any of them.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Statistical thinking**](https://www.linkedin.com/pulse/statistical-thinking-nutshell-kevin-gray/),
    critical in science,does not come naturally to humans. No one is born a statistician,
    and educational curricula frequently shortchange statistics.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, rigorous science is challenging and any study can be questioned.
    Deception is part of human nature and scientists are human, as are journalists
    and policymakers. We are too and must be careful not to trust a study just because
    we find it exciting, or because it comforts us or conforms to our beliefs.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Kevin Gray](https://www.linkedin.com/in/cannongray)** is President of [Cannon
    Gray](http://cannongray.com/home), a marketing science and analytics consultancy.
    He has more than 30 years’ experience in marketing research with Nielsen, Kantar,
    McCann and TIAA-CREF.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Data Quality: The Good, The Bad, and The Ugly](https://www.kdnuggets.com/2022/01/data-quality-good-bad-ugly.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Explainable AI: 10 Python Libraries for Demystifying Your Model''s Decisions](https://www.kdnuggets.com/2023/01/explainable-ai-10-python-libraries-demystifying-decisions.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Demystifying Machine Learning](https://www.kdnuggets.com/demystifying-machine-learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Demystifying Decision Trees for the Real World](https://www.kdnuggets.com/demystifying-decision-trees-for-the-real-world)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Science Minimum: 10 Essential Skills You Need to Know to Start…](https://www.kdnuggets.com/2020/10/data-science-minimum-10-essential-skills.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
