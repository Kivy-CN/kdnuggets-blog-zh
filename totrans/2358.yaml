- en: Which Metric Should I Use? Accuracy vs. AUC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/10/metric-accuracy-auc.html](https://www.kdnuggets.com/2022/10/metric-accuracy-auc.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Which Metric Should I Use? Accuracy vs. AUC](../Images/1d94f438ad26a2fc7a44e5b0d05e0d40.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Source](https://img.freepik.com/free-vector/multi-device-targeting-concept-illustration_114360-7305.jpg?w=2000&t=st=1662099419~exp=1662100019~hmac=91e26d7432f6cbd939540caaf21318207563271fa9ffed676e8025ecc06ac0b2)'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy and AUC (Area Under the Curve) are measures to evaluate the goodness
    of model performance. Both are helpful to gauge the model performance depending
    on the type of business problem you are trying to solve. So which one should you
    use and when? Well, the short answer is - It depends!
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we will first describe both the metrics and then learn each of
    the metrics in detail and understand when to use them.
  prefs: []
  type: TYPE_NORMAL
- en: What is Accuracy?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Accuracy is the most popular metric that determines the percentage of correct
    predictions made by the model.
  prefs: []
  type: TYPE_NORMAL
- en: It is computed as a ratio of the number of true predictions with that of the
    total number of samples in the dataset. The resulting quantity is measured in
    percentage terms, for example, if the model correctly predicted 90% of the items
    in your dataset, it has 90% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically it is written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Which Metric Should I Use? Accuracy vs. AUC](../Images/9fd20b2e4e23806fb3021b5a7a8cd4b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The four constituents in the formula are based on the actuals and corresponding
    model predictions defined below:'
  prefs: []
  type: TYPE_NORMAL
- en: True Positive (TP) - number of instances where the model correctly identifies
    a positive class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: True Negative (TN) - number of instances where the model correctly identifies
    a negative class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: False Positive (FP) - number of instances where the model incorrectly identifies
    a negative class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: False Negative (FN) - number of instances where the model incorrectly identifies
    a positive class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These metrics are derived by applying an appropriate cut-off to the predicted
    probability score of the model. A detailed explanation of each of the terms and
    their relationship with the confusion matrix is explained in this [article](/2022/09/visualizing-confusion-matrix-scikitlearn.html).
  prefs: []
  type: TYPE_NORMAL
- en: What is AUC?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AUC stands for “Area Under the Curve” in general, and “Area under the Receiver
    Operating Characteristic Curve” in long form. It captures the area under the ROC
    (Receiver Operating Characteristic) curve and compares the relationship between
    the True Positive Rate (TPR) with that of the False Positive Rate (FPR) across
    different cut-off thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: But before deep-diving into AUC, let's first understand what these new terms
    mean.
  prefs: []
  type: TYPE_NORMAL
- en: TPR or True Positive Rate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is the ratio of correctly predicted positive instances out of all the positive
    samples. For example, if the model is tasked to identify the fraudulent transactions,
    then TPR is defined as the proportion of correctly predicted fraudulent transactions
    among all fraudulent transactions.
  prefs: []
  type: TYPE_NORMAL
- en: FPR or False Positive Rate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is the percentage of incorrectly predicted negative cases. Continuing with
    the fraud detection model, the FPR is defined as the proportion of falsely predicted
    fraudulent alerts out of all the legitimate transactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, TPR and FPR are expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Which Metric Should I Use? Accuracy vs. AUC](../Images/38e23e591627ffd8207a6ac7d4d96122.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we understand TPR and FPR by definition, let's understand how they
    relate to the AUC metric.
  prefs: []
  type: TYPE_NORMAL
- en: '![Which Metric Should I Use? Accuracy vs. AUC](../Images/c8aad0f0fccee9a4ba5795120fe8481b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Editor
  prefs: []
  type: TYPE_NORMAL
- en: As evident from the image above, the top left corner i.e. high TPs and low FPs
    is the desirable state. Thus, the purple color curve makes a perfect classifier
    with an AUC of 1 i.e. square area below this curve is 1.
  prefs: []
  type: TYPE_NORMAL
- en: But, constructing such an ideal classifier is not practically achievable in
    real-world applications. Hence, it is important to understand the lower bound
    of a classifier which is denoted by a red color diagonal line. It is annotated
    as a random classifier and has an AUC of 0.5 i.e. the area of a triangle below
    the red dotted line. It is called a random classifier because its predictions
    are as good as a random flip of a coin.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, a machine learning model’s performance lies between a random classifier
    and the perfect classifier, which indicates that the expected AUC is bounded between
    0.5 (random state) and 1 (perfect state).
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, data scientists aim to maximize AUC i.e. larger area under the
    curve. It signifies the model's goodness in generating correct predictions aka
    striving for the highest TPR while maintaining the lowest possible FPR.
  prefs: []
  type: TYPE_NORMAL
- en: When to Use Accuracy?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Accuracy is used for balanced datasets i.e. when the classes are equally distributed.
  prefs: []
  type: TYPE_NORMAL
- en: A real-life example is fraud detection which must correctly identify and distinguish
    fraudulent transactions (class of interest) from regular transactions. Usually,
    fraudulent transactions are rare i.e. their occurrence in the training data set
    would be less than ~1%.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy in this case would be a biased representation of model performance
    and would declare the model good even if it identifies every transaction as non-fraudulent.
    Such a model would have high accuracy but fails to predict any fraudulent transaction,
    defeating the purpose of building the model.
  prefs: []
  type: TYPE_NORMAL
- en: When to Use AUC?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AUC is well-suited for imbalanced datasets. For example, the fraud detection
    model must correctly identify fraud even if it comes at the cost of flagging some
    (a small number) of the non-fraudulent transactions as fraudulent.
  prefs: []
  type: TYPE_NORMAL
- en: It is highly probable that while focusing on correctly identifying the class
    of interest (fraudulent transactions) i.e. the TPs, the model makes some mistakes
    i.e. FPs (marking non-fraudulent transactions as fraudulent). Thus it’s important
    to look at a measure that compares TPR and FPR. This is where AUC fits in.
  prefs: []
  type: TYPE_NORMAL
- en: Which One Should You Choose and When?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Accuracy and AUC are both used for classification models. However, there are
    a few things to keep in mind when you’re deciding which one to use.
  prefs: []
  type: TYPE_NORMAL
- en: A high accuracy model indicates very few incorrect predictions. However, this
    doesn’t consider the business cost of those incorrect predictions. The use of
    accuracy metrics in such business problems abstracts away the details like TP
    and FP, and gives an inflated sense of confidence in model predictions that is
    detrimental to business objectives.
  prefs: []
  type: TYPE_NORMAL
- en: AUC is the go-to metric in such scenarios as it calibrates the trade-off between
    sensitivity and specificity at the best-chosen threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Further, accuracy measures how well a single model is doing, whereas AUC compares
    two models as well as evaluates the same model’s performance across different
    thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Choosing the appropriate metric for your model is critical to getting the desired
    results. Accuracy and AUC are two popular evaluation metrics to objectively measure
    the model performance. They are both helpful for assessing how well a model is
    doing and comparing one model to another. The post explained why accuracy is a
    sufficient metric for balanced data but AUC is well-suited to measure the model’s
    performance on an imbalanced set.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Vidhi Chugh](https://vidhi-chugh.medium.com/)** is an award-winning AI/ML
    innovation leader and an AI Ethicist. She works at the intersection of data science,
    product, and research to deliver business value and insights. She is an advocate
    for data-centric science and a leading expert in data governance with a vision
    to build trustworthy AI solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Classification Metrics Walkthrough: Logistic Regression with…](https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Sky''s the Limit: Learn how JetBlue uses Monte Carlo and Snowflake…](https://www.kdnuggets.com/2022/12/monte-carlo-jetblue-snowflake-build-trust-improve-model-accuracy.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Key Issues Associated with Classification Accuracy](https://www.kdnuggets.com/2023/03/key-issues-associated-classification-accuracy.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3 Mistakes That Could Be Affecting the Accuracy of Your Data Analytics](https://www.kdnuggets.com/2023/03/3-mistakes-could-affecting-accuracy-data-analytics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Beyond Accuracy: Evaluating & Improving a Model with the NLP Test Library](https://www.kdnuggets.com/2023/04/john-snow-beyond-accuracy-nlp-test-library.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding Classification Metrics: Your Guide to Assessing Model…](https://www.kdnuggets.com/understanding-classification-metrics-your-guide-to-assessing-model-accuracy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
