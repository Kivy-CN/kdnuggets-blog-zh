- en: 'Quantization and LLMs: Condensing Models to Manageable Sizes'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化与LLMs：将模型浓缩到可管理的大小
- en: 原文：[https://www.kdnuggets.com/quantization-and-llms-condensing-models-to-manageable-sizes](https://www.kdnuggets.com/quantization-and-llms-condensing-models-to-manageable-sizes)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/quantization-and-llms-condensing-models-to-manageable-sizes](https://www.kdnuggets.com/quantization-and-llms-condensing-models-to-manageable-sizes)
- en: '![Quantization and LLMs: Condensing Models to Manageable Sizes](../Images/c200004e40b1a5d03763bbf10e7b6bab.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![量化与LLMs：将模型浓缩到可管理的大小](../Images/c200004e40b1a5d03763bbf10e7b6bab.png)'
- en: The Scale and Complexity of LLMs
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM的规模与复杂性
- en: The incredible abilities of LLMs are powered by their vast neural networks which
    are made up of billions of parameters. These parameters are the result of training
    on extensive text corpora and are fine-tuned to make the models as accurate and
    versatile as possible. This level of complexity requires significant computational
    power for processing and storage.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的惊人能力得益于其庞大的神经网络，这些网络由数十亿个参数组成。这些参数是通过在大量文本语料库上进行训练得到的，并经过微调以使模型尽可能准确和多才多艺。这种复杂性水平需要大量的计算能力来处理和存储。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 为你的组织提供IT支持'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![Quantization and LLMs: Condensing Models to Manageable Sizes](../Images/cdcb06c3bedff816f35c168293c2ffc0.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![量化与LLMs：将模型浓缩到可管理的大小](../Images/cdcb06c3bedff816f35c168293c2ffc0.png)'
- en: The accompanying bar graph delineates the number of parameters across different
    scales of language models. As we move from smaller to larger models, we witness
    a significant increase in the number of parameters with 'Small' language models
    at the modest millions of parameters and 'Large' models with tens of billions
    of parameters.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 附图条形图描绘了不同规模语言模型的参数数量。随着模型规模的增大，我们可以看到参数数量显著增加，从具有几百万参数的“小型”语言模型到具有数百亿参数的“大型”模型。
- en: However, it is the GPT-4 LLM model with 175 billion parameters that dwarfs other
    models’ parameter size. Not only is GPT-4 using the most parameters out of the
    graphs, but it also powers the most recognizable generative AI model, ChatGPT.
    This towering presence on the graph is representative of other LLMs of its class,
    displaying the requirements needed to power the future’s AI chatbots, as well
    as the processing power required to support such advanced AI systems.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，具有1750亿个参数的GPT-4 LLM模型在参数规模上超越了其他模型。GPT-4不仅使用了图表中最多的参数，还驱动了最具知名度的生成型AI模型——ChatGPT。图表中的这一庞大存在代表了同类的其他LLMs，展示了驱动未来AI聊天机器人的需求以及支持如此先进AI系统所需的计算能力。
- en: The Cost of Running LLMs and Quantization
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行LLMs的成本与量化
- en: Deploying and operating complex models can get costly due to their need for
    either cloud computing on specialized hardware, such as [high-end GPUs](https://www.exxactcorp.com/category/Consumer-Graphics-Cards),
    AI accelerators, and continuous energy consumption. Reducing the cost by choosing
    an on-premises solution can save a great deal of money and increase flexibility
    in hardware choices and freedom to utilize the system wherever with a trade-off
    in maintenance and employing a skilled professional. High costs can make it challenging
    for small business deployments to train and power an advanced AI. Here is where
    quantization comes in handy.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 部署和操作复杂的模型可能会很昂贵，因为它们需要使用[高端GPU](https://www.exxactcorp.com/category/Consumer-Graphics-Cards)、AI加速器以及持续的能源消耗进行云计算。选择本地解决方案可以通过降低成本来节省大量资金，并提高硬件选择的灵活性和系统使用的自由度，但这需要付出维护成本和雇佣专业人员的代价。高成本使得小型企业在训练和运行先进AI方面面临挑战。这时，量化技术就显得非常重要。
- en: What is Quantization?
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是量化？
- en: Quantization is a technique that reduces the numerical precision of each parameter
    in a model, thereby decreasing its memory footprint. This is akin to compressing
    a high-resolution image to a lower resolution while retaining the essence and
    most important aspects but at a reduced data size. This approach enables the deployment
    of LLMs on with less hardware without substantial performance loss.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一种技术，通过降低模型中每个参数的数值精度，从而减少其内存占用。这类似于将高分辨率图像压缩为较低分辨率，同时保留精髓和最重要的方面，但数据大小有所减少。这种方法使得在硬件条件较差的情况下，也能部署大型语言模型而不会造成显著的性能损失。
- en: ChatGPT was trained and is deployed using thousands of NVIDIA DGX systems, millions
    of dollars of hardware, and tens of thousands more for infrastructure. Quantization
    can enable good proof-of-concept, or even fully fledged deployments with less
    spectacular (but still high performance) hardware.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT的训练和部署使用了数千台NVIDIA DGX系统，数百万美元的硬件以及数万亿美元的基础设施费用。量化可以在性能不逊色（但硬件性能较低）的条件下，实现良好的概念验证甚至完全成熟的部署。
- en: In the sections to follow, we will dissect the concept of quantization, its
    methodologies, and its significance in bridging the gap between the highly resource-intensive
    nature of LLMs and the practicalities of everyday technology use. The transformative
    power of LLMs can become a staple in smaller-scale applications, offering vast
    benefits to a broader audience.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将剖析量化的概念、方法及其在缩小大型语言模型对资源的高度依赖与日常技术使用的实际之间差距方面的意义。大型语言模型的变革力量可以在小规模应用中成为主流，为更广泛的受众提供巨大的好处。
- en: Basics of Quantization
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化基础
- en: Quantizing a large language model refers to the process of reducing the precision
    of numerical values used in the model. In the context of neural networks and deep
    learning models, including large language models, numerical values are typically
    represented as floating-point numbers with high precision (e.g., 32-bit or 16-bit
    floating-point format). Read more about [Floating Point Precision here](https://www.exxactcorp.com/blog/hpc/what-is-fp64-fp32-fp16).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对大型语言模型进行量化是指减少模型中数值值的精度。在神经网络和深度学习模型的上下文中，包括大型语言模型，数值值通常表示为高精度的浮点数（例如32位或16位浮点格式）。有关[浮点精度的更多信息](https://www.exxactcorp.com/blog/hpc/what-is-fp64-fp32-fp16)。
- en: Quantization addresses this by converting these high-precision floating-point
    numbers into lower-precision representations, such as 16- or 8-bit integers to
    make the model more memory-efficient and faster during both training and inference
    by sacrificing precision. As a result, the training and inferencing of the model
    requires less storage, consumes less memory, and can be executed more quickly
    on hardware that supports lower-precision computations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 量化通过将这些高精度浮点数转换为较低精度的表示形式，如16位或8位整数，从而提高模型在训练和推理时的内存效率和速度，尽管牺牲了一些精度。因此，模型的训练和推理需要较少的存储空间，消耗更少的内存，并且可以在支持低精度计算的硬件上更快地执行。
- en: Types of Quantization
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化类型
- en: To add depth and complexity to the topic, it is critical to understand that
    quantization can be applied at various stages in the lifecycle of a model's development
    and deployment. Each method has its distinct advantages and trade-offs and is
    selected based on the specific requirements and constraints of the use case.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加主题的深度和复杂性，了解量化可以在模型开发和部署生命周期的不同阶段应用至关重要。每种方法都有其独特的优点和权衡，根据用例的具体要求和限制进行选择。
- en: 1\. Static Quantization
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1. 静态量化
- en: Static quantization is a technique applied during the training phase of an AI
    model, where the weights and activations are quantized to a lower bit precision
    and applied to all layers. The weights and activations are quantized ahead of
    time and remain fixed throughout. Static quantization is great for known memory
    requirements of the system the model is planning to be deployed to.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 静态量化是一种在AI模型的训练阶段应用的技术，其中权重和激活被量化为较低的位精度，并应用于所有层。权重和激活提前量化并在整个过程中保持固定。静态量化非常适合于已知系统内存需求的模型部署计划。
- en: Pros of Static Quantization
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态量化的优点
- en: Simplifies deployment planning as the quantization parameters are fixed.
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简化部署规划，因为量化参数是固定的。
- en: Reduces model size, making it more suitable for edge devices and real-time applications.
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少模型的大小，使其更适合边缘设备和实时应用。
- en: Cons of Static Quantization
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态量化的缺点
- en: Performance drops are predictable; so certain quantized parts may suffer more
    due to a broad static approach.
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能下降是可预测的；因此某些量化部分可能由于广泛的静态方法而遭受更大的损失。
- en: Limited adaptability for static quantization for varying input patterns and
    less robust update to weights.
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于静态量化适应性有限，适应不同输入模式的能力差，权重更新不够稳健。
- en: 2\. Dynamic Quantization
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2\. 动态量化
- en: Dynamic Quantization involves quantizing weights statically, but activations
    are quantized on the fly during model inference. The weights are quantized ahead
    of time, while the activations are quantized dynamically as data passes through
    the network. This means that quantization of certain parts of the model are executed
    on different precisions as opposed to defaulting to a fixed quantization.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 动态量化涉及静态量化权重，但在模型推理期间激活会动态量化。权重会提前量化，而激活在数据通过网络时动态量化。这意味着模型的某些部分在不同的精度下执行量化，而不是默认固定量化。
- en: Pros of Dynamic Quantization
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态量化的优点
- en: Balances model compression and runtime efficiency without significant drop in
    accuracy.
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平衡了模型压缩和运行效率，准确性没有显著下降。
- en: Useful for models where activation precision is more critical than weight precision.
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于激活精度比权重精度更为关键的模型特别有用。
- en: Cons of Dynamic Quantization
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态量化的缺点
- en: Performance improvements aren’t predictable compared to static methods (but
    this isn’t necessarily a bad thing).
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相较于静态方法，性能提升不可预测（但这不一定是坏事）。
- en: Dynamic calculation means more computational overhead and longer train and inference
    times than the other methods, while still being lighter weight than without quantization
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态计算意味着比其他方法需要更多的计算开销和更长的训练和推理时间，但仍比没有量化时更轻量。
- en: 3\. Post-Training Quantization (PTQ)
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3\. 训练后量化（PTQ）
- en: In this technique, quantization is incorporated into the training process itself.
    It involves analyzing the distribution of weights and activations and then mapping
    these values to a lower bit depth. PTQ is deployed on resource-constrained devices
    like edge devices and mobile phones. PTQ can be either static or dynamic.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种技术中，量化被纳入到训练过程中。它涉及分析权重和激活的分布，然后将这些值映射到更低的位深。PTQ被部署在资源受限的设备上，如边缘设备和手机。PTQ可以是静态的也可以是动态的。
- en: Pros of PTQ
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PTQ的优点
- en: Can be applied directly to a pre-trained model without the need for retraining.
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以直接应用于预训练模型，而无需重新训练。
- en: Reduces the model size and decreases memory requirements.
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少模型大小，降低内存需求。
- en: Improved inference speeds enabling faster computations during and after deployment.
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升了推理速度，实现了部署期间和之后更快的计算。
- en: Cons of PTQ
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PTQ的缺点
- en: Potential loss in model accuracy due to the approximation of weights.
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于权重的近似，可能会导致模型准确性下降。
- en: Requires careful calibration and fine tuning to mitigate quantization errors.
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要仔细的校准和微调以减轻量化误差。
- en: May not be optimal for all types of models, particularly those sensitive to
    weight precision.
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能不适用于所有类型的模型，特别是那些对权重精度敏感的模型。
- en: 4\. Quantization Aware Training (QAT)
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4\. 量化感知训练（QAT）
- en: During training, the model is aware of the quantization operations that will
    be applied during inference and the parameters are adjusted accordingly. This
    allows the model to learn to handle quantization induced errors.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，模型会了解在推理期间将应用的量化操作，并相应调整参数。这使得模型能够学会处理量化引起的误差。
- en: Pros of QAT
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QAT的优点
- en: Tends to preserve model accuracy compared to PTQ since the model training accounts
    for quantization errors during training.
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相比PTQ，QAT倾向于保持模型准确性，因为模型训练时考虑了量化误差。
- en: More robust for models sensitive to precision and is better at inferencing even
    on lower precisions.
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于对精度敏感的模型更为稳健，即使在较低精度下也能更好地进行推理。
- en: Cons of QAT
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QAT的缺点
- en: Requires retraining the model resulting in longer training times.
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要重新训练模型，从而导致训练时间延长。
- en: More computationally intensive since it incorporates quantization error checking.
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于包含量化误差检查，因此计算开销较大。
- en: 5\. Binary Ternary Quantization
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5\. 二进制三进制量化
- en: These methods quantize the weights to either two values (binary) or three values
    (ternary), representing the most extreme form of quantization. Weights are constrained
    to +1, -1 for binary, or +1, 0, -1 for ternary quantization during or after training.
    This would drastically reduce the number of possible quantization weight values
    while still being somewhat dynamic.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法将权重量化为两个值（二值）或三个值（三值），代表了量化的最极端形式。权重在训练过程中或之后被限制为二值量化的 +1 和 -1，或三值量化的 +1、0
    和 -1。这将大幅减少可能的量化权重值数量，同时仍保持一定的动态性。
- en: Pros of Binary Ternary Quantization
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二值三值量化的优点
- en: Maximizes model compression and inferencing speed and has minimal memory requirements.
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大化模型压缩和推理速度，且内存需求最小。
- en: Fast inferencing and quantization calculations enables usefulness on underpowered
    hardware.
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速推理和量化计算使得在性能不足的硬件上仍具备实用性。
- en: Cons of Binary Ternary Quantization
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二值三值量化的缺点
- en: High compression and reduced precision results in a significant drop in accuracy.
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高压缩率和降低精度会显著降低准确性。
- en: Not suitable for all types of tasks or datasets and struggles with complex tasks.
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不适用于所有类型的任务或数据集，并且在处理复杂任务时表现不佳。
- en: The Benefits & Challenges of Quantization
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化的好处与挑战
- en: '![Before and after quantization](../Images/d50020005166b994925cde3b76b3e3db.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![量化前后](../Images/d50020005166b994925cde3b76b3e3db.png)'
- en: The quantization of Large Language Models brings forth multiple operational
    benefits. Primarily, it achieves a significant reduction in the memory requirements
    of these models. Our goal for post-quantization models is for the memory footprint
    to be notably smaller. Higher efficiency permits the deployment of these models
    on platforms with more modest memory capabilities and decreasing the processing
    power needed to run the models once quantized translates directly into heightened
    inference speeds and quicker response times that enhance user experience.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型的量化带来了多种操作上的好处。主要是显著减少了这些模型的内存需求。我们对量化后的模型的目标是内存占用显著减少。更高的效率使得这些模型可以在内存能力较小的平台上部署，并且量化后的模型所需的处理能力减少，直接提高了推理速度和响应时间，从而提升了用户体验。
- en: On the other hand, quantization can also introduce some loss in model accuracy
    since it involves approximating real numbers. The challenge is to quantize the
    model without significantly affecting its performance. This can be done with testing
    the model's precision and time of completion before and after quantization with
    your models to gauge effectiveness, efficiency, and accuracy.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，量化也可能会引入一些模型精度的损失，因为它涉及到对真实数字的近似。挑战在于如何在不显著影响模型性能的情况下进行量化。可以通过在量化前后测试模型的精度和完成时间来评估效果、效率和准确性。
- en: By optimizing the balance between performance and resource consumption, quantization
    not only broadens the accessibility of LLMs but also contributes to more sustainable
    computing practices.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过优化性能和资源消耗之间的平衡，量化不仅扩大了大语言模型的可及性，还促进了更可持续的计算实践。
- en: '[**Original**](https://www.exxactcorp.com/blog/deep-learning/what-is-quantization-and-llms).
    Republished with permission.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[**原文**](https://www.exxactcorp.com/blog/deep-learning/what-is-quantization-and-llms)。经许可转载。'
- en: '**[Kevin Vu](https://blog.exxactcorp.com/)** manages [Exxact Corp blog](https://blog.exxactcorp.com/)
    and works with many of its talented authors who write about different aspects
    of Deep Learning.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Kevin Vu](https://blog.exxactcorp.com/)** 负责管理 [Exxact Corp 博客](https://blog.exxactcorp.com/)，并与许多才华横溢的作者合作，这些作者撰写了有关深度学习不同方面的文章。'
- en: More On This Topic
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[7 Steps to Mastering Large Language Models (LLMs)](https://www.kdnuggets.com/7-steps-to-mastering-large-language-models-llms)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握大型语言模型（LLMs）的 7 个步骤](https://www.kdnuggets.com/7-steps-to-mastering-large-language-models-llms)'
- en: '[Generative AI Playground: LLMs with Camel-5b and Open LLaMA 3B on…](https://www.kdnuggets.com/2024/02/intel-generative-ai-playground-llms-with-camel-5b-and-open-llama-3b)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[生成式 AI 游乐场：Camel-5b 和 Open LLaMA 3B 的 LLMs](https://www.kdnuggets.com/2024/02/intel-generative-ai-playground-llms-with-camel-5b-and-open-llama-3b)'
- en: '[Vector Database for LLMs, Generative AI, and Deep Learning](https://www.kdnuggets.com/vector-database-for-llms-generative-ai-and-deep-learning)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大语言模型、生成式 AI 和深度学习的向量数据库](https://www.kdnuggets.com/vector-database-for-llms-generative-ai-and-deep-learning)'
- en: '[History and Future of LLMs](https://www.kdnuggets.com/history-and-future-of-llms)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LLMs 的历史与未来](https://www.kdnuggets.com/history-and-future-of-llms)'
- en: '[8 Free AI and LLMs Playgrounds](https://www.kdnuggets.com/2023/05/8-free-ai-llms-playgrounds.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8个免费的AI和LLMs试验场](https://www.kdnuggets.com/2023/05/8-free-ai-llms-playgrounds.html)'
- en: '[What are Vector Databases and Why Are They Important for LLMs?](https://www.kdnuggets.com/2023/06/vector-databases-important-llms.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是向量数据库，它们为何对LLMs至关重要？](https://www.kdnuggets.com/2023/06/vector-databases-important-llms.html)'
