- en: Google Open Sources MobileNetV3 with New Ideas to Improve Mobile Computer Vision
    Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/12/google-open-sources-mobilenetv3-improve-mobile-computer-vision-models.html](https://www.kdnuggets.com/2019/12/google-open-sources-mobilenetv3-improve-mobile-computer-vision-models.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)![Figure](../Images/0665646a2b72f0810caf185893592b3e.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Mobile deep learning is becoming one of the most active areas of research in
    the artificial intelligence(AI) space. Designing deep learning models that can
    execute efficiently on mobile runtimes requiring rethinking many of the architecture
    paradigms in neural networks. Mobile deep learning models need to balance the
    accuracy of complex neural network structures with the performance constraints
    of mobile runtimes. Among the areas of mobile deep learning, computer vision remains
    one of the most challenging ones. In 2017, [Google introduced MobileNets](https://ai.googleblog.com/2017/06/mobilenets-open-source-models-for.html),
    a family of computer vision models based on TensorFlow. [The newest architecture
    of architecture of MobileNets was unveiled a few days ago](https://arxiv.org/abs/1905.02244) and
    contains a few interesting ideas to improve mobile computer vision models.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: MobileNetV3 is the third version of the architecture powering the image analysis
    capabilities of many popular mobile applications. The architecture has also been
    incorporated in popular frameworks such as TensorFlow Lite. MobileNets need to
    carefully balance the advancements in computer vision and deep learning in general
    with the limitations of mobile environments. Google has regularly been releasing
    updates to the MobileNets architecture which incorporate some of the most novel
    ideas in the deep learning space.
  prefs: []
  type: TYPE_NORMAL
- en: MobileNetV1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[The first version of MobileNets](https://arxiv.org/abs/1704.04861) was released
    in the spring of 2017\. The core idea was to introduce a series of TensorFlow-based
    computer vision models that maximize accuracy while being mindful of the restricted
    resources for an on-device or embedded application. Conceptually, MobileNetV1
    is trying to achieve two fundamental goals in order to build mobile-first computer
    vision models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Smaller model size**: Fewer number of parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Smaller complexity**: Fewer Multiplications and Additions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following those principles, MobileNetV1 are small, low-latency, low-power models
    parameterized to meet the resource constraints of a variety of use cases. They
    can be built upon for classification, detection, embeddings and segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8fd54267b04b8152578bce676a516a38.png)'
  prefs: []
  type: TYPE_IMG
- en: The core architecture of MobileNetV1 is based on a streamlined architecture
    that uses depth-wise separable convolutions to build light weight deep neural
    networks. In terms of neural network architectures, depth-wise separable convolution
    is a depth-wise convolution followed by a pointwise convolutionas illustrated
    in the following figure. In MobileNetV1, the depth-wise convolution applies a
    single filter to each input channel. The pointwise convolution then applies a
    1×1 convolution to combine the outputs the depth-wise convolution. A standard
    convolution both filters and combines inputs into a new set of outputs in one
    step. The depth-wise separable convolution splits this into two layers, a separate
    layer for filtering and a separate layer for combining.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eab2ecda2c05b8425c766c78bb1a5873.png)'
  prefs: []
  type: TYPE_IMG
- en: The first MobileNetV1 implementation was included as part of the [TensorFlow-Slim
    Image Classification Library](https://github.com/tensorflow/models/blob/master/research/slim/README.md).
    As new mobile applications were built using this new paradigm, new ideas emerged
    to improve the overall architecture.
  prefs: []
  type: TYPE_NORMAL
- en: MobileNetV2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The second version of the MobileNet architecture [was unveiled in early 2018](https://arxiv.org/abs/1801.04381).
    MobileNetV2 built on some of the ideas of its predecessor and incorporated new
    ideas to optimize the architecture for tasks such as classification, object detection
    and semantic segmentation. From the architecture standpoint, MobileNetV2 introduced
    two new features to the architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: linear bottlenecks between the layers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: shortcut connections between the bottlenecks[1](https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html#1).
    The basic structure is shown below.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The core idea behind MobileNetV2 is that the bottlenecks encode the model’s
    intermediate inputs and outputs while the inner layer encapsulates the model’s
    ability to transform from lower-level concepts such as pixels to higher level
    descriptors such as image categories. Finally, as with traditional residual connections,
    shortcuts enable faster training and better accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fb540d2a4d751f9040afe0288cced2ae.png)'
  prefs: []
  type: TYPE_IMG
- en: MobileNetsV3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The latest improvements to the MobileNets architecture were summarized [in a
    research paper published in August this year](https://arxiv.org/abs/1905.02244).
    The main contribution of MobileNetV3 is the use of AutoML to find the best possible
    neural network architecture for a given problem. This contrast with the hand-crafted
    design of previous versions of the architecture. Specifically, MobileNetV3 leverages
    two AutoML techniques: [MnasNet](https://ai.google/research/pubs/pub47217/) and [NetAdapt](https://arxiv.org/pdf/1804.03230).
    MobileNetV3 first searches for a coarse architecture using MnasNet, which uses
    reinforcement learning to select the optimal configuration from a discrete set
    of choices. After that, the model fine-tunes the architecture using NetAdapt,
    a complementary technique that trims under-utilized activation channels in small
    decrements.
  prefs: []
  type: TYPE_NORMAL
- en: Another novel idea of MobileNetV3 is the incorporation of an [squeeze-and-excitation](https://arxiv.org/abs/1709.01507) block
    into the core architecture. The core idea of the squeeze-and-excitation blocks
    is to improve the quality of representations produced by a network by explicitly
    modelling the interdependencies between the channels of its convolutional features.
    To this end, we propose a mechanism that allows the network to perform feature
    recalibration, through which it can learn to use global information to selectively
    emphasize informative features and suppress less useful ones. In the case of MobileNetV3,
    the architecture extends MobileNetV2 incorporates squeeze-and-excitation blocks
    as part of the search space which ended up yielding more robust architectures.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2fa97817013f78283082bc74622c6084.png)'
  prefs: []
  type: TYPE_IMG
- en: An interesting optimization of MobileNetV3 was the redesign of some of the expensive
    layers in the architecture. Some of the layers in MobileNetV2 were foundational
    to the accuracy of the models but also introduced concerning levels of latency.
    By incorporating some basic optimizations, MobileNetV3 was able to remove three
    expensive layers of its predecessor architecture without sacrificing accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ed6adaa7ac831b133ecc63361ecde38.png)'
  prefs: []
  type: TYPE_IMG
- en: 'MobileNetV3 has shown significant improvements over previous architectures.
    For instance, in object detection tasks, MobileNetV3 operated with 25% less latency
    and the same accuracy of previous versions. Similar improvements were seen in
    classification tasks as illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f78c50eca956c18cfbc8df1d9fc489a3.png)'
  prefs: []
  type: TYPE_IMG
- en: MobileNets remain one of the most advanced architecture in mobile computer vision.
    The incorporation of AutoML in MobileNetV3 certainly opens the door to all sorts
    of interesting architectures that we haven’t thought of before. [The latest release
    of MobileNets is available in GitHub](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet) and
    the implementation of MobileNetV3 is included in the [Tensorflow Object Detection
    API](https://github.com/tensorflow/models/tree/master/research/object_detection).
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/google-open-sources-mobilenetv3-with-new-ideas-to-improve-mobile-computer-vision-models-bfba8967a7f1).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Introduction to Image Segmentation with K-Means clustering](/2019/08/introduction-image-segmentation-k-means-clustering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Comparing MobileNet Models in TensorFlow](/2019/03/comparing-mobilenet-models-tensorflow.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Open Source Projects by Google, Uber and Facebook for Data Science and AI](/2019/11/open-source-projects-google-uber-facebook-data-science-ai.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[DINOv2: Self-Supervised Computer Vision Models by Meta AI](https://www.kdnuggets.com/2023/05/dinov2-selfsupervised-computer-vision-models-meta-ai.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TensorFlow for Computer Vision - Transfer Learning Made Easy](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Discover the World of Computer Vision: Introducing MLM''s Latest…](https://www.kdnuggets.com/2024/01/mlm-discover-the-world-of-computer-vision-ebook)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Applications of Computer Vision](https://www.kdnuggets.com/2022/03/5-applications-computer-vision.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Things You Need To Know About Data Management And Why It Matters…](https://www.kdnuggets.com/2022/05/6-things-need-know-data-management-matters-computer-vision.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News March 9, 2022: Build a Machine Learning Web App in 5…](https://www.kdnuggets.com/2022/n10.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
