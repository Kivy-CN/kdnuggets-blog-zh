- en: Four questions to help accurately scope analytics engineering project
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 帮助准确确定分析工程项目范围的四个问题
- en: 原文：[https://www.kdnuggets.com/2019/10/four-questions-scope-analytics-engineering-project.html](https://www.kdnuggets.com/2019/10/four-questions-scope-analytics-engineering-project.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/10/four-questions-scope-analytics-engineering-project.html](https://www.kdnuggets.com/2019/10/four-questions-scope-analytics-engineering-project.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Tristan Handy](https://twitter.com/jthandy), Founder & CEO at Fishtown
    Analytics**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Tristan Handy](https://twitter.com/jthandy)，Fishtown Analytics 创始人兼首席执行官**。'
- en: '![](../Images/c146259e897cff471b4a8235b3db6b2b.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c146259e897cff471b4a8235b3db6b2b.png)'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速通道进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能。'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT。'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'At Fishtown Analytics, we sell and deliver analytics engineering projects in
    two-week sprints, each of which has a fixed price and fixed deliverables. Delivering
    on such a tight timeline with defined outcomes is intense! And we’ve gotten quite
    good at it: we’ve now delivered over 1,000 sprints, with over 95% of them delivered
    on time.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Fishtown Analytics，我们以两周的冲刺周期销售和交付分析工程项目，每个周期都有固定的价格和交付物。在如此紧凑的时间表和明确的结果下交付是非常紧张的！我们在这方面做得相当好：我们已经完成了超过
    1000 个冲刺，其中超过 95% 准时交付。
- en: As a result, we’ve gotten really, really good at scoping analytics projects.
    We’ve found that if we answer four questions up-front, we can significantly reduce
    the chances of uncovering issues before it’s too late. In fact, **ferreting out
    issues early is the single most important factor in delivering analytics work
    on time**. If you spend a week going down one path only to realize it’s a dead
    end, that’s a tremendous amount of wasted time. It’s critical to anticipate the
    cul-de-sac before you find yourself stuck there.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，我们在确定分析项目范围方面变得非常擅长。我们发现，如果我们提前回答四个问题，我们可以显著减少在为时已晚之前发现问题的机会。事实上，**早期发现问题是按时交付分析工作的最重要因素**。如果你花一周时间走上一条道路，结果发现是死胡同，那是一段巨大的浪费时间。关键是要在陷入困境之前预见到死胡同。
- en: 'Our approach to proactive issue identification has become intuitive—it’s something
    that we all *just do* because we’ve all burned our hands on the stove enough times
    to know better: “I should’ve known I needed to check whether those IDs would map. [Shit](https://www.youtube.com/watch?v=Fr0A7TofowE).”'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主动问题识别方法已经变得直观——这是我们大家*自然会做*的事情，因为我们都曾在炉子上烫过手，知道这样做更好：“我本该知道我需要检查那些 ID 是否能匹配。[操](https://www.youtube.com/watch?v=Fr0A7TofowE)。”
- en: 'It’s such an important topic, though, that I wanted to take the time to make
    our intuitive, tacit knowledge more explicit and share it with the larger community.
    Because this skill isn’t just for consultants: anyone who is using [Agile on their
    data team](https://www.locallyoptimistic.com/post/agile-analytics-p1/), or really
    anyone who manages deadlines on data projects at all, should care.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个如此重要的话题，以至于我想花时间使我们的直观、潜在知识更为明确，并与更大的社区分享。因为这项技能不仅仅是为顾问准备的：任何使用 [敏捷方法进行数据团队管理](https://www.locallyoptimistic.com/post/agile-analytics-p1/)
    的人，或者任何管理数据项目截止日期的人，都应该关心。
- en: Why? **Data teams are successful when they build reserves of trust**—that’s
    how you earn the “trusted advisor” status from all of your business stakeholders,
    and how you ultimately impact the trajectory of the business. We earn trust by
    delivering data that is high-quality, tested, and accurate. We also build trust
    when we set clear expectations about how a project will go and then deliver on
    them, time after time after time.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么？ **数据团队成功的关键在于建立信任储备** ——这就是你如何从所有业务利益相关者那里获得“受信任顾问”地位的方式，以及如何最终影响业务的轨迹。我们通过提供高质量、经过测试和准确的数据来赢得信任。我们还通过对项目的过程设定明确期望并一再兑现来建立信任。
- en: Here are the four questions we ask to proactively identify issues and deliver
    analytics engineering projects on time at Fishtown Analytics.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们在 Fishtown Analytics 主动识别问题并按时交付分析工程项目时提出的四个问题。
- en: 1\. Do I have the data from the primary source?
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1\. 我是否从原始来源获得了数据？
- en: 'It’s not unusual to have data that is relevant to your problem but is from
    the wrong system. One common example: your company might have marketing data stored
    in Marketo, but it is ingested into your data warehouse through an existing Salesforce
    integration. Using the Marketo data vis-a-vis Salesforce isn’t *wrong*, but it’s *risky*.
    The two-hop integration will have more opportunities for failure, and the data
    will likely not be as granular or complete as the primary source data. Better
    to get all data directly from its originating source.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据相关但来自错误系统的情况并不罕见。一个常见的例子是：你的公司可能在 Marketo 中存储营销数据，但它通过现有的 Salesforce 集成被引入到数据仓库中。使用
    Marketo 数据而非 Salesforce 数据并不*错误*，但却*有风险*。双重集成将有更多失败的机会，数据也可能不像原始数据那样详细或完整。最好是直接从数据的源头获取所有数据。
- en: 'There are actually tons of examples of this. Should you trust:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这里有很多这样的例子。你应该信任：
- en: '**Shopify’s charges or Stripe’s? **I would choose Stripe’s for financial /
    revenue use cases and Shopify’s for e-commerce use cases.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**是 Shopify 的收费还是 Stripe 的？** 我会选择 Stripe 的用于财务/收入相关的情况，而 Shopify 的用于电子商务相关的情况。'
- en: '**Mixpanel’s events or Segment’s? **I would almost always choose Segment, given
    that most companies push data through Segment to Mixpanel.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mixpanel 的事件还是 Segment 的？** 我几乎总是选择 Segment，因为大多数公司将数据通过 Segment 推送到 Mixpanel。'
- en: '**Hubspot’s attribution or Snowplow’s?** I believe your web analytics tool
    (Snowplow in this case) needs to be the authoritative source of your web event
    data, and thus your attribution.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hubspot 的归因还是 Snowplow 的？** 我相信你的网络分析工具（在这种情况下是 Snowplow）需要成为你网络事件数据的权威来源，从而成为你的归因来源。'
- en: It’s a funny thing to insist on having data directly from the source when you
    start a project because, in some ways, it’s not very pragmatic. Paying for an
    extra integration typically has a clear price tag, and it’s not at all clear what
    the value is at the outset. Why isn’t the data you have “good enough”? The answer
    is that building your analytics on top of the most fundamentally true source of
    data saves a tremendous amount of wasted time, frustration, workarounds, and spaghetti
    code in the future.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始一个项目时，坚持直接从源头获取数据是一件有趣的事，因为在某些方面，这并不非常务实。支付额外的集成费用通常有明确的价格标签，而且一开始并不清楚它的价值。为什么你拥有的数据“不够好”？答案是，基于最基本的真实数据源建立你的分析将节省大量的时间、挫折、变通方法和混乱的代码。
- en: '**If you have true primary source data, you’re virtually omnipotent**: if a
    question can be answered, you can answer it. If you’re in a meeting with a VP
    who says, “can we slice leads by acquisition source?” the answer will either be
    “yes” or “the systems we use don’t give us the data to do that.” If you don’t
    have primary source data, you sometimes end up having to answer “not given our
    current analytics pipeline.” Not exactly trust-inspiring.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你有真正的原始数据，你就几乎无所不能**：如果一个问题可以回答，你就能回答它。如果你在与一位 VP 开会时，他问：“我们能按获取来源来切分潜在客户吗？”答案要么是“可以”，要么是“我们使用的系统没有提供这样的数据。”如果你没有原始数据，你有时只能回答“根据我们目前的分析管道无法做到这一点。”这并不令人信服。'
- en: 'If you don’t have primary source data, do two things:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有原始数据，请做两件事：
- en: See if you can ingest that data straightforwardly. This could be as easy as
    setting up a Stitch or Fivetran integration, or it could require a custom integration.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 试着直接获取这些数据。这可能像设置 Stitch 或 Fivetran 集成一样简单，或者可能需要自定义集成。
- en: If you can’t get primary source data and need to rely on a secondary source,
    alert your stakeholders of the potential drawbacks so that everyone goes into
    the project with the right expectations. Outline the boundaries of the analysis
    you’ll be able to provide, and the amount of re-work required down the road if
    you are able to get at the primary source data. There’s nothing inherently wrong
    with making decisions based on expediency as long as you’ve clearly educated stakeholders
    on the potential downstream impacts.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. Is source data sufficiently granular?
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'According to [Kimball](https://www.kimballgroup.com/2007/07/keep-to-the-grain-in-dimensional-modeling/),
    it’s important to “stick to rich, expressive, atomic-level data that’s closely
    connected to the original source and collection process.” I couldn’t agree more:
    granular data gives you *options*, aggregated data is limiting.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most common examples of this in practice is the difference between
    Google Analytics data exported via the API and Google Analytics 360 data loaded
    automatically into Bigquery. Via the API, GA will only give you aggregated metrics,
    whereas GA 360 will export every single session and every single event into Bigquery.
    The aggregated data is only useful when creating extremely high-level analyses:
    trendlines of page views or unique visitors. With the granular data, you can answer
    any question you want—multi-touch attribution, funnel analysis, A/B testing, etc.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, you want to have access to the absolute most granular data available
    from the source data system. At this early stage, you don’t yet know all of the
    followup questions you’re going to be asked to investigate, and you want to be
    able to follow the trail wherever it leads. Even if your top-level question could
    be answered by an aggregated export, your followup questions almost definitely
    won’t.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to actually learn about the source data systems whose data you’re
    interacting with.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '**If it’s a SaaS product**, read the API docs. What data will the API give
    you? Good APIs give you all of the objects you need; *great*APIs will give you
    change tracking on top of those objects. For example, the Salesforce API will
    give you a history table containing all opportunities and their state changes
    since the beginning of your account. It turns out that the opportunity history
    table is critical for a big chunk of Salesforce pipeline reporting.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**If it’s an internal product database**, read whatever internal documentation
    is available or sit down with an experienced engineer to understand what data
    is stored in the product.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Overall, the closer you can get to events, the better off you’ll be. If you
    have all state changes for a given system, you can always reconstruct the state
    at any given point in time—a very powerful position to be in. For example, the
    only endpoint you actually need from the Stripe API is the events endpoint: from
    that single endpoint, you can derive the outputs from all other endpoints at any
    point in time. You can do a similar thing for email marketing performance using
    Mailchimp’s events table.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, most Stitch and Fivetran integrations pull maximally granular data
    by default—both of these products were built for the modern analytics stack and
    want to give you complete control. It’s important to check for yourself, though.
    A little while ago, I was speccing out a sprint with a client to do sales pipeline
    reporting on top of the Stitch Close.io integration, and it turns out that integration
    (which is community-supported) didn’t include the opportunity history endpoint.
    This dramatically changed the scope of the project as we couldn’t actually produce
    the analysis required without adding that additional endpoint to the integration.
    Better that we discovered that at the outset.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Has this data set been used before?
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If your organization *has not* previously used the data from a given ingestion
    pipeline, you should treat it as guilty until proven innocent. There are a large
    number of ways in which data that you haven’t used before can be wrong, and many
    of them are subtle. There are two main categories:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '**Broken ingestion.**There can be errors in the ingestion that cause data in
    your warehouse to be incorrect. Even when you’re dealing with Stitch and Fivetran
    integrations, *shit happens*. Every time I’ve seen an integration break, it is
    for the strangest, most esoteric problems, each is one-of-a-kind.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unanticipated ingestion behavior.**Sometimes, loading happens in a way that
    you wouldn’t have anticipated. For example, Stitch loads many tables that it calls
    “reporting tables” as an immutable log and then gives specific instructions on
    how to query from these tables. If you don’t read their instructions and simply
    start writing queries, you’ll come up with numbers that are head-scratchingly
    off.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both of these types of errors can be caught by *auditing the data* from new
    data sets before building mission-critical analyses on top of them. What exactly
    does auditing a data set mean? For us, it means producing straightforward metrics
    on top of the table/tables that can be compared to the outputs from another system
    (typically the system that the data was extracted from). For instance, we frequently
    calculate orders and revenue and compare those metrics with the results we get
    natively in Shopify. Both of these metrics are very straightforward to calculate,
    and if the numbers match you immediately have a high degree of confidence that
    the data set is in good shape.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'Problems with your raw data are typically cross-cutting: they’ll affect all
    records in a table or all records in a certain date range. If you audit 2-3 metrics,
    you can generally feel pretty good about the data quality overall.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'If your organization *has* previously used the data from a given ingestion
    pipeline, it’s incumbent that you invest in learning what’s already been built.
    If you’re using dbt:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: lookup the existing modeling work that’s been done using dbt docs
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: read all of the descriptive text for models and columns and make sure you actually
    grok the code
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: find the authors in the git commit history and ask them if there are any gotchas
    you should know about
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Do not just say “screw it” and start your own work from scratch.** The primary
    source of analytics engineering tech debt and errors result from multiple code
    bases attempting to transform and analyze fundamentally the same data. Two code
    paths equate to double the surface area for errors and infinitely more opportunity
    for confusion. It’s not always easy to follow the work that’s been done before
    you, but *it’s critically important that you integrate into an existing code base
    rather than starting from scratch. *I’ve seen teams who are unwilling to write
    code collaboratively in this way, and instead, every analyst builds their own
    silo. **This is a recipe for an incredibly unproductive data team.**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Do the necessary keys exist to join all of the data together?
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'New analysis typically builds on existing concepts by attaching new data to
    data that has already been modeled. It’s typical to start with your application
    database or shopping cart and then spider outwards into other data systems: event
    tracking, customer success, advertising, email, etc. As you bring each one of
    these systems into your pipeline and your data model, you’ll need at least one
    key to join the data.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'It seems like that would be fairly straightforward, but often it just isn’t.
    Here are some examples:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: You’re registering your trial signups as leads in your Salesforce instance,
    but the web form on the marketing site that submits to Salesforce doesn’t have
    the user’s account number at that point in the flow. No key!
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your event tracking only shows 15,000 identified users over the past week, but
    your application database is confident that there have been 20,000 users logging
    in over the same time period. The user identification code for your event tracking
    pipeline is “leaky”—some identify calls are missing! Which ones?
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You want to track customer acquisition cost (CAC) across your entire funnel,
    starting with ad spend. But when you go to integrate your data from your various
    ad sources, you realize that the marketing team hasn’t been adding URL parameters
    to the links they’ve been using! Without these parameters, you have no way of
    joining data from the rest of your warehouse to the advertising cost data.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This stuff happens *constantly*, and knowing where to look for problems before
    you hit them is absolutely critical. If you’re missing a key between two systems,
    it can stop an entire analytics project dead in its tracks, because often a) adding
    the key involves bringing in stakeholders from other parts of the org, and b)
    there is no way to retroactively get the data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure to check your keys before diving in. If they’re missing, go through
    the effort of working with the stakeholders to create them. This “process instrumentation”
    is a critical part of what makes a great analytics engineer: you can’t always
    expect that business processes will naturally spit out the data you need and sometimes
    you’ll need to roll up your sleeves and make sure it gets gathered.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Know the answers before you dive in
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As a consultant, I have to have a strong process: if I mis-scope a sprint,
    it probably means that I’m not going to be getting enough sleep over the coming
    two weeks. Or maybe it means that we lose $$ on the project—the stakes are real.
    That’s why I (and everyone at Fishtown) am so focused on being good at scoping.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: On an internal data team, the stakes are just as high, but the feedback typically
    isn’t as clear or immediate. Your stakeholders will notice if you consistently
    miss deadlines or fail to deliver key results, *they just might not say anything
    to you*. It’s often hard to give direct feedback to a colleague —when’s the last
    time you said something like “Your team’s miss on that deadline led me to miss
    my committed OKR for the quarter, and I’m pissed”?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Even if it isn’t made explicit, consistent failure by a data team to deliver
    predictable outcomes damages trust and thereby damages the team’s ability to make
    an impact on the larger org. Avoid this outcome by identifying issues during the
    scoping phase of a project and by communicating the limitations of your approach
    with stakeholders.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://blog.getdbt.com/4-questions-to-help-you-more-accurately-scope-analytics-engineering-projects/).
    Reposted with permission.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio:** [Tristan Handy](https://twitter.com/jthandy)is currently building
    Fishtown Analytics to help venture-funded companies implement advanced analytics
    through  building tools to facilitate an opinionated analytics workflow.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[The Future of Analytics and Data Science](https://www.kdnuggets.com/2019/09/future-analytics-data-science.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[On Building Effective Data Science Teams](https://www.kdnuggets.com/2019/03/building-effective-data-science-teams.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Beginner’s Guide to Data Engineering  –  Part I](https://www.kdnuggets.com/2018/01/beginners-guide-data-engineering-1.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How to Handle Time Zones and Timestamps Accurately with Pandas](https://www.kdnuggets.com/how-to-handle-time-zones-and-timestamps-accurately-with-pandas)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Master the Power of Data Analytics: The Four Approaches to Analyzing Data](https://www.kdnuggets.com/2023/03/master-power-data-analytics-four-approaches-analyzing-data.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握数据分析的力量：四种数据分析方法](https://www.kdnuggets.com/2023/03/master-power-data-analytics-four-approaches-analyzing-data.html)'
- en: '[Data Analytics: The Four Approaches to Analyzing Data and How To…](https://www.kdnuggets.com/2023/04/data-analytics-four-approaches-analyzing-data-effectively.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据分析：四种数据分析方法及其有效应用](https://www.kdnuggets.com/2023/04/data-analytics-four-approaches-analyzing-data-effectively.html)'
- en: '[Learning Python in Four Weeks: A Roadmap](https://www.kdnuggets.com/2023/02/learning-python-four-weeks-roadmap.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[四周掌握Python：路线图](https://www.kdnuggets.com/2023/02/learning-python-four-weeks-roadmap.html)'
- en: '[Project Ideas to Master Data Engineering](https://www.kdnuggets.com/project-ideas-to-master-data-engineering)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握数据工程的项目创意](https://www.kdnuggets.com/project-ideas-to-master-data-engineering)'
- en: '[7 Data Analytics Interview Questions & Answers](https://www.kdnuggets.com/2022/09/7-data-analytics-interview-questions-answers.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7个数据分析面试问题及答案](https://www.kdnuggets.com/2022/09/7-data-analytics-interview-questions-answers.html)'
