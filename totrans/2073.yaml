- en: 37 Reasons why your Neural Network is not working
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 37 个原因解释你的神经网络为何不起作用
- en: 原文：[https://www.kdnuggets.com/2017/08/37-reasons-neural-network-not-working.html/2](https://www.kdnuggets.com/2017/08/37-reasons-neural-network-not-working.html/2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/08/37-reasons-neural-network-not-working.html/2](https://www.kdnuggets.com/2017/08/37-reasons-neural-network-not-working.html/2)
- en: III. Implementation issues
  id: totrans-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III. 实现问题
- en: '![](../Images/d1d7015310dac70d5bd4d0899f6d5a97.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1d7015310dac70d5bd4d0899f6d5a97.png)'
- en: 'Credit: https://xkcd.com/1838/'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 参考：https://xkcd.com/1838/
- en: 16\. Try solving a simpler version of the problem
  id: totrans-5
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 16\. 尝试解决问题的简化版本
- en: This will help with finding where the issue is. For example, if the target output
    is an object class and coordinates, try limiting the prediction to object class
    only.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这将有助于找到问题所在。例如，如果目标输出是对象类别和坐标，尝试将预测限制为仅对象类别。
- en: 17\. Look for correct loss “at chance”
  id: totrans-7
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 17\. 寻找“随机会”损失的正确性
- en: '* * *'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行 IT 支持'
- en: '* * *'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Again from the excellent [CS231n](http://cs231n.github.io/neural-networks-3/#sanitycheck): *Initialize
    with small parameters, without regularization. For example, if we have 10 classes,
    at chance means we will get the correct class 10% of the time, and the Softmax
    loss is the negative log probability of the correct class so: -ln(0.1) = 2.302.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 再次参考优秀的[CS231n](http://cs231n.github.io/neural-networks-3/#sanitycheck)：*用小参数初始化，不使用正则化。例如，如果我们有
    10 个类别，随机情况下我们将有 10% 的时间得到正确的类别，而 Softmax 损失是正确类别的负对数概率，因此： -ln(0.1) = 2.302。*
- en: After this, try increasing the regularization strength which should increase
    the loss.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，尝试增加正则化强度，这应该会增加损失。
- en: 18\. Check your loss function
  id: totrans-16
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 18\. 检查你的损失函数
- en: If you implemented your own loss function, check it for bugs and add unit tests.
    Often, my loss would be slightly incorrect and hurt the performance of the network
    in a subtle way.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你实现了自己的损失函数，检查它是否有错误并添加单元测试。通常，我的损失函数会略微不准确，从而在微妙的方式上影响网络的性能。
- en: 19\. Verify loss input
  id: totrans-18
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 19\. 验证损失输入
- en: If you are using a loss function provided by your framework, make sure you are
    passing to it what it expects. For example, in PyTorch I would mix up the NLLLoss
    and CrossEntropyLoss as the former requires a softmax input and the latter doesn’t.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用了框架提供的损失函数，确保你传递给它的是它所期望的。例如，在 PyTorch 中，我会混淆 NLLLoss 和 CrossEntropyLoss，因为前者需要
    softmax 输入，而后者则不需要。
- en: 20\. Adjust loss weights
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 20\. 调整损失权重
- en: If your loss is composed of several smaller loss functions, make sure their
    magnitude relative to each is correct. This might involve testing different combinations
    of loss weights.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的损失函数由几个较小的损失函数组成，确保它们相对彼此的大小是正确的。这可能需要测试不同的损失权重组合。
- en: 21\. Monitor other metrics
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 21\. 监控其他指标
- en: Sometimes the loss is not the best predictor of whether your network is training
    properly. If you can, use other metrics like accuracy.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候损失函数并不是网络训练是否正常的最佳预测指标。如果可以，使用其他指标如准确率。
- en: 22\. Test any custom layers
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 22\. 测试任何自定义层
- en: Did you implement any of the layers in the network yourself? Check and double-check
    to make sure they are working as intended.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否自己实现了网络中的任何层？检查并仔细核对，确保它们按预期工作。
- en: 23\. Check for “frozen” layers or variables
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 23\. 检查“冻结”层或变量
- en: Check if you unintentionally disabled gradient updates for some layers/variables
    that should be learnable.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 检查是否无意中禁用了应可学习的某些层/变量的梯度更新。
- en: 24\. Increase network size
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 24\. 增加网络规模
- en: Maybe the expressive power of your network is not enough to capture the target
    function. Try adding more layers or more hidden units in fully connected layers.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你的网络的表达能力不足以捕捉目标函数。尝试增加更多的层或在全连接层中增加更多的隐藏单元。
- en: 25\. Check for hidden dimension errors
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 25\. 检查隐藏维度错误
- en: If your input looks like (k, H, W) = (64, 64, 64) it’s easy to miss errors related
    to wrong dimensions. Use weird numbers for input dimensions (for example, different
    prime numbers for each dimension) and check how they propagate through the network.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的输入看起来像(k, H, W) = (64, 64, 64)，很容易忽略与维度错误相关的错误。使用奇怪的输入维度（例如，每个维度使用不同的质数）并检查它们在网络中的传播。
- en: 26\. Explore Gradient checking
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 26\. 探索梯度检查
- en: If you implemented Gradient Descent by hand, gradient checking makes sure that
    your backpropagation works like it should. More info: [1](http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/) [2](http://cs231n.github.io/neural-networks-3/#gradcheck) [3](https://www.coursera.org/learn/machine-learning/lecture/Y3s6r/gradient-checking).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你手动实现了梯度下降，梯度检查可以确保你的反向传播正常工作。更多信息：[1](http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/)
    [2](http://cs231n.github.io/neural-networks-3/#gradcheck) [3](https://www.coursera.org/learn/machine-learning/lecture/Y3s6r/gradient-checking)。
- en: IV. Training issues
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV. 训练问题
- en: '![](../Images/38da157d4f92ff6ffd306f58f3beba9e.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38da157d4f92ff6ffd306f58f3beba9e.png)'
- en: 'Credit: http://carlvondrick.com/ihog/'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 版权归属：http://carlvondrick.com/ihog/
- en: 27\. Solve for a really small dataset
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 27\. 解决非常小的数据集
- en: '**Overfit a small subset of the data and make sure it works. **For example,
    train with just 1 or 2 examples and see if your network can learn to differentiate
    these. Move on to more samples per class.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**对数据的一个小子集进行过拟合，并确保它有效。**例如，仅用1或2个示例进行训练，看看你的网络是否能学会区分这些示例。然后再增加每类的样本数量。'
- en: 28\. Check weights initialization
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 28\. 检查权重初始化
- en: If unsure, use [Xavier](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) or [He](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf) initialization.
    Also, your initialization might be leading you to a bad local minimum, so try
    a different initialization and see if it helps.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不确定，使用[Xavier](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)或[He](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf)初始化。此外，你的初始化可能导致你陷入不良的局部最小值，因此尝试不同的初始化，看看是否有帮助。
- en: 29\. Change your hyperparameters
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 29\. 更改你的超参数
- en: Maybe you using a particularly bad set of hyperparameters. If feasible, try
    a [grid search](http://scikit-learn.org/stable/modules/grid_search.html).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你使用了一组特别差的超参数。如果可能，尝试进行[网格搜索](http://scikit-learn.org/stable/modules/grid_search.html)。
- en: 30\. Reduce regularization
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 30\. 减少正则化
- en: Too much regularization can cause the network to underfit badly. Reduce regularization
    such as dropout, batch norm, weight/bias L2 regularization, etc. In the excellent
    “[Practical Deep Learning for coders](http://course.fast.ai/)” course, [Jeremy
    Howard](https://twitter.com/jeremyphoward) advises getting rid of underfitting
    first. This means you overfit the training data sufficiently, and only then addressing
    overfitting.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 过多的正则化可能导致网络严重欠拟合。减少正则化，例如dropout、batch norm、权重/偏置L2正则化等。在优秀的“[Practical Deep
    Learning for coders](http://course.fast.ai/)”课程中，[Jeremy Howard](https://twitter.com/jeremyphoward)建议先解决欠拟合。这意味着你要充分过拟合训练数据，然后再处理过拟合。
- en: 31\. Give it time
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 31\. 给它时间
- en: Maybe your network needs more time to train before it starts making meaningful
    predictions. If your loss is steadily decreasing, let it train some more.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你的网络需要更多时间进行训练，才开始做出有意义的预测。如果损失持续下降，可以让它继续训练。
- en: 32\. Switch from Train to Test mode
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 32\. 切换从训练模式到测试模式
- en: Some frameworks have layers like Batch Norm, Dropout, and other layers behave
    differently during training and testing. Switching to the appropriate mode might
    help your network to predict properly.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一些框架中的层，如Batch Norm、Dropout以及其他层在训练和测试期间表现不同。切换到适当的模式可能有助于网络更好地预测。
- en: 33\. Visualize the training
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 33\. 可视化训练
- en: Monitor the activations, weights, and updates of each layer. Make sure their
    magnitudes match. For example, the magnitude of the updates to the parameters
    (weights and biases) [should be 1-e3](https://cs231n.github.io/neural-networks-3/#summary).
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控每层的激活、权重和更新。确保它们的幅度匹配。例如，参数（权重和偏置）更新的幅度[应为 1-e3](https://cs231n.github.io/neural-networks-3/#summary)。
- en: Consider a visualization library like [Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) and [Crayon](https://github.com/torrvision/crayon).
    In a pinch, you can also print weights/biases/activations.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑使用可视化库如[Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard)和[Crayon](https://github.com/torrvision/crayon)。在紧急情况下，你也可以打印权重/偏置/激活值。
- en: Be on the lookout for layer activations with a mean much larger than 0\. Try
    Batch Norm or ELUs.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意层激活值是否远大于0。尝试使用Batch Norm或ELUs。
- en: '[Deeplearning4j](https://deeplearning4j.org/visualization#usingui) points out
    what to expect in histograms of weights and biases:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Deeplearning4j](https://deeplearning4j.org/visualization#usingui) 指出权重和偏差的直方图应注意什么：'
- en: “For weights, these histograms should have an **approximately Gaussian (normal) **distribution,
    after some time. For biases, these histograms will generally start at 0, and will
    usually end up being **approximately Gaussian**(One exception to this is for LSTM).
    Keep an eye out for parameters that are diverging to +/- infinity. Keep an eye
    out for biases that become very large. This can sometimes occur in the output
    layer for classification if the distribution of classes is very imbalanced.”
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “对于权重，这些直方图在一段时间后应具有**大致的高斯（正态）**分布。对于偏差，这些直方图通常从0开始，通常会最终**大致呈高斯分布**（LSTM是一个例外）。注意参数是否发散到正负无穷大。注意偏差是否变得非常大。如果类别分布非常不平衡，这有时会发生在分类的输出层。”
- en: Check layer updates, they should have a Gaussian distribution.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查层更新，它们应具有高斯分布。
- en: 34\. Try a different optimizer
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 34\. 尝试不同的优化器
- en: Your choice of optimizer shouldn’t prevent your network from training unless
    you have selected particularly bad hyperparameters. However, the proper optimizer
    for a task can be helpful in getting the most training in the shortest amount
    of time. The paper which describes the algorithm you are using should specify
    the optimizer. If not, I tend to use Adam or plain SGD with momentum.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 选择的优化器不应该阻止你的网络训练，除非你选择了特别差的超参数。然而，适当的优化器可以帮助你在最短的时间内获得最多的训练。描述你正在使用的算法的论文应该指定优化器。如果没有，我倾向于使用Adam或普通的带动量的SGD。
- en: Check this [excellent post](http://ruder.io/optimizing-gradient-descent/) by
    Sebastian Ruder to learn more about gradient descent optimizers.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这个[精彩的帖子](http://ruder.io/optimizing-gradient-descent/) 来了解更多关于梯度下降优化器的内容。
- en: 35\. Exploding / Vanishing gradients
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 35\. 梯度爆炸/消失
- en: Check layer updates, as very large values can indicate exploding gradients.
    Gradient clipping may help.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查层更新，因为非常大的值可能表示梯度爆炸。梯度裁剪可能会有所帮助。
- en: Check layer activations. From [Deeplearning4j](https://deeplearning4j.org/visualization#usingui) comes
    a great guideline: *“A good standard deviation for the activations is on the order
    of 0.5 to 2.0\. Significantly outside of this range may indicate vanishing or
    exploding activations.”*
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查层激活情况。[Deeplearning4j](https://deeplearning4j.org/visualization#usingui) 提供了一个很好的指导：*“激活的标准偏差应在0.5到2.0之间。显著超出这个范围可能表示激活消失或爆炸。”*
- en: 36\. Increase/Decrease Learning Rate
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 36\. 增加/减少学习率
- en: A low learning rate will cause your model to converge very slowly.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 低学习率会导致模型收敛非常缓慢。
- en: A high learning rate will quickly decrease the loss in the beginning but might
    have a hard time finding a good solution.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 高学习率在开始时会快速减少损失，但可能很难找到一个好的解决方案。
- en: Play around with your current learning rate by multiplying it by 0.1 or 10.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将当前学习率乘以0.1或10来调整它。
- en: 37\. Overcoming NaNs
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 37\. 克服NaNs
- en: 'Getting a NaN (Non-a-Number) is a much bigger issue when training RNNs (from
    what I hear). Some approaches to fix it:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 获取NaN（非数字）在训练RNN时是一个更大的问题（从我听说的）。一些修复方法：
- en: Decrease the learning rate, especially if you are getting NaNs in the first
    100 iterations.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低学习率，特别是当你在前100次迭代中遇到NaNs时。
- en: NaNs can arise from division by zero or natural log of zero or negative number.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NaNs可能源于除以零或自然对数为零或负数。
- en: Russell Stewart has great pointers on [how to deal with NaNs](http://russellsstewart.com/notes/0.html).
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russell Stewart 对于[如何处理NaNs](http://russellsstewart.com/notes/0.html) 提供了很好的建议。
- en: Try evaluating your network layer by layer and see where the NaNs appear.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试逐层评估你的网络，看看NaNs出现的位置。
- en: '**Bio: [Slav Ivanov](https://twitter.com/slavivanov)** is Entrepreneur & ML
    Practitioner in Sofia, Bulgaria. He blogs about Machine Learning at https://blog.slavv.com
    . Previously built http://postplanner.com .'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[Slav Ivanov](https://twitter.com/slavivanov)** 是保加利亚索非亚的企业家和机器学习从业者。他在https://blog.slavv.com
    上写关于机器学习的博客。之前构建了http://postplanner.com。'
- en: '[Original](https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607).
    Reposted with permission.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607)。经许可转载。'
- en: 'Related:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 相关：
- en: '[Deep Learning and Neural Networks Primer: Basic Concepts for Beginners](/2017/08/deep-learning-neural-networks-primer-basic-concepts-beginners.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习和神经网络基础知识：初学者的基本概念](/2017/08/deep-learning-neural-networks-primer-basic-concepts-beginners.html)'
- en: '[Train your Deep Learning model faster and sharper: Snapshot Ensembling — M
    models for the cost of 1](/2017/08/train-deep-learning-faster-snapshot-ensembling.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[更快更精准地训练你的深度学习模型：快照集成——以1的成本实现M个模型](/2017/08/train-deep-learning-faster-snapshot-ensembling.html)'
- en: '[Optimization in Machine Learning: Robust or global minimum?](/2017/06/robust-global-minimum.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中的优化：稳健还是全局最小值？](/2017/06/robust-global-minimum.html)'
- en: More On This Topic
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[3 Reasons Why You Should Use Linear Regression Models Instead of…](https://www.kdnuggets.com/2021/08/3-reasons-linear-regression-instead-neural-networks.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你应该使用线性回归模型而不是……的3个理由](https://www.kdnuggets.com/2021/08/3-reasons-linear-regression-instead-neural-networks.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标并找到目标……](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成为优秀数据科学家所需的5项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个初学者数据科学家应该掌握的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一个90亿美元的AI失败案例，详尽分析](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[建立一个稳固的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
