["```py\n\npip install scikit-optimize\n\n```", "```py\n\nimport skopt\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Integer\nfrom skopt.utils import use_named_args\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom math import sqrt\nimport atexit\nfrom time import time, strftime, localtime\nfrom datetime import timedelta\nfrom sklearn.metrics import mean_squared_error\nfrom skopt.plots import plot_convergence\n\n```", "```py\n\nrandomState = 46\nnp.random.seed(randomState)\ntf.set_random_seed(randomState)\n\n```", "```py\n\ninput_size=1\nfeatures = 2\ncolumn_min_max = [[0, 2000],[0,500000000]]\ncolumns = ['Close', 'Volume']\n\nnum_steps = None\nlstm_size = None\nbatch_size = None\ninit_learning_rate = None\nlearning_rate_decay = None\ninit_epoch = None\nmax_epoch = None\ndropout_rate = None\n```", "```py\n\nlstm_num_steps = Integer(low=2, high=14, name='lstm_num_steps')\nsize = Integer(low=8, high=200, name='size')\nlstm_learning_rate_decay = Real(low=0.7, high=0.99, prior='uniform', name='lstm_learning_rate_decay')\nlstm_max_epoch = Integer(low=20, high=200, name='lstm_max_epoch')\nlstm_init_epoch = Integer(low=2, high=50, name='lstm_init_epoch')\nlstm_batch_size = Integer(low=5, high=100, name='lstm_batch_size')\nlstm_dropout_rate = Real(low=0.1, high=0.9, prior='uniform', name='lstm_dropout_rate')\nlstm_init_learning_rate = Real(low=1e-4, high=1e-1, prior='log-uniform', name='lstm_init_learning_rate')\n\n```", "```py\n\ndimensions = [lstm_num_steps, size, lstm_init_epoch, lstm_max_epoch,\nlstm_learning_rate_decay, lstm_batch_size, lstm_dropout_rate, lstm_init_learning_rate]\n\ndefault_parameters = [2,128,3,30,0.99,64,0.2,0.001]\n\n```", "```py\n\ndef setupRNN(inputs, model_dropout_rate):\n\n  cell = tf.contrib.rnn.LSTMCell(lstm_size, state_is_tuple=True, activation=tf.nn.tanh,use_peepholes=True)\n\n  val1, _ = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n\n  val = tf.transpose(val1, [1, 0, 2])\n\n  last = tf.gather(val, int(val.get_shape()[0]) -1, name=\"last_lstm_output\")\n\n  dropout = tf.layers.dropout(last, rate=model_dropout_rate, training=True,seed=46)\n\n  weight = tf.Variable(tf.truncated_normal([lstm_size, input_size]))\n  bias = tf.Variable(tf.constant(0.1, shape=[input_size]))\n\n  prediction = tf.matmul(dropout, weight) +bias\n\n  return prediction\n\n```", "```py\n\n@use_named_args(dimensions=dimensions)\ndef fitness(lstm_num_steps, size, lstm_init_epoch, lstm_max_epoch,\n            lstm_learning_rate_decay, lstm_batch_size, lstm_dropout_rate, lstm_init_learning_rate):\n\n    global iteration, num_steps, lstm_size, init_epoch, max_epoch, learning_rate_decay, dropout_rate, init_learning_rate, batch_size\n\n    num_steps = np.int32(lstm_num_steps)\n    lstm_size = np.int32(size)\n    batch_size = np.int32(lstm_batch_size)\n    learning_rate_decay = np.float32(lstm_learning_rate_decay)\n    init_epoch = np.int32(lstm_init_epoch)\n    max_epoch = np.int32(lstm_max_epoch)\n    dropout_rate = np.float32(lstm_dropout_rate)\n    init_learning_rate = np.float32(lstm_init_learning_rate)\n\n    tf.reset_default_graph()\n    tf.set_random_seed(randomState)\n    sess = tf.Session()\n\n    train_X, train_y, val_X, val_y, nonescaled_val_y = pre_process()\n\n    inputs = tf.placeholder(tf.float32, [None, num_steps, features], name=\"inputs\")\n    targets = tf.placeholder(tf.float32, [None, input_size], name=\"targets\")\n    model_learning_rate = tf.placeholder(tf.float32, None, name=\"learning_rate\")\n    model_dropout_rate = tf.placeholder_with_default(0.0, shape=())\n    global_step = tf.Variable(0, trainable=False)\n\n    prediction = setupRNN(inputs,model_dropout_rate)\n\n    model_learning_rate = tf.train.exponential_decay(learning_rate=model_learning_rate, global_step=global_step, decay_rate=learning_rate_decay,\n                                                decay_steps=init_epoch, staircase=False)\n\n    with tf.name_scope('loss'):\n        model_loss = tf.losses.mean_squared_error(targets, prediction)\n\n    with tf.name_scope('adam_optimizer'):\n        train_step = tf.train.AdamOptimizer(model_learning_rate).minimize(model_loss,global_step=global_step)\n\n    sess.run(tf.global_variables_initializer())\n\n    for epoch_step in range(max_epoch):\n\n        for batch_X, batch_y in generate_batches(train_X, train_y, batch_size):\n            train_data_feed = {\n                inputs: batch_X,\n                targets: batch_y,\n                model_learning_rate: init_learning_rate,\n                model_dropout_rate: dropout_rate\n            }\n            sess.run(train_step, train_data_feed)\n\n    val_data_feed = {\n        inputs: val_X,\n    }\n    vali_pred = sess.run(prediction, val_data_feed)\n\n    vali_pred_vals = rescle(vali_pred)\n\n    vali_pred_vals = np.array(vali_pred_vals)\n\n    vali_pred_vals = vali_pred_vals.flatten()\n\n    vali_pred_vals = vali_pred_vals.tolist()\n\n    vali_nonescaled_y = nonescaled_val_y.flatten()\n\n    vali_nonescaled_y = vali_nonescaled_y.tolist()\n\n    val_error = sqrt(mean_squared_error(vali_nonescaled_y, vali_pred_vals))\n\n    return val_error\n\n```", "```py\n\nif __name__ == '__main__':\n\n    start = time()\n\n    search_result = gp_minimize(func=fitness,\n                                dimensions=dimensions,\n                                acq_func='EI', # Expected Improvement.\n                                n_calls=11,\n                                x0=default_parameters,\n                                random_state=46)\n\nprint(search_result.x)\nprint(search_result.fun)\nplot = plot_convergence(search_result,yscale=\"log\")\n\natexit.register(endlog)\nlogger(\"Start Program\")\n\n```"]