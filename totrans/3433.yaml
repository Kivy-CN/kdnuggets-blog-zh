- en: 'Contest 2nd Place: Automating Data Science'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2016/08/automating-data-science.html](https://www.kdnuggets.com/2016/08/automating-data-science.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By Ankit Sharma, DataRPM**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Editor''s note**: This blog post was an entrant in the recent KDnuggets Automated
    Data Science and Machine Learning [blog contest](/2016/06/kdnuggets-blog-contest-automated-data-science.html),
    where it tied for second place.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Data scientist is the sexiest job of 21st century. But even Data Scientists
    have to get our hands dirty to get things done. What if some of the manual laborious
    tasks are automated and we bring the cool factor back to the job by only focusing
    on the logic and research.
  prefs: []
  type: TYPE_NORMAL
- en: Any generic data science pipeline will have the following components. To automate
    DS pipeline, we need to automate each individual component separately.
  prefs: []
  type: TYPE_NORMAL
- en: '![Automation pathway](../Images/a06b52be0f0a17540bb8e5678d1d9b9f.png)'
  prefs: []
  type: TYPE_IMG
- en: Lot of research has been undergoing in automating **Data Cleaning & Feature
    Generation & Selection** aspect of DS pipeline because they consume almost 70%
    of the data science project time.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this post we’ll focus on the not much talked about topics: **Algorithm
    Selection** and **Parameter Tuning**. Both of these components are considered
    manual in nature. Data scientists spend hours applying multiple ML algorithms
    and then tuning the parameters. For every new problem they follow the same process
    and over period they develop the intuition of which algorithm will work better
    for which type of dataset and how best they can tune the parameters. If you look
    closely this is in itself a learning problem. Lately lot of research & development
    has been started to automate these components to get best accuracy in least amount
    of time and efforts.'
  prefs: []
  type: TYPE_NORMAL
- en: Automating Algorithm selection can be done using concept called **Meta Learning**
    while to automate Parameter tuning there are different techniques like Grid search,
    bayesian optimization, etc which we’ll be discussing later in the post.
  prefs: []
  type: TYPE_NORMAL
- en: '**Automating Algorithm Selection using Meta Learning**'
  prefs: []
  type: TYPE_NORMAL
- en: There are two essential parts of Meta-learning - Meta Store & Meta-model algorithm.
    Meta store is used to capture metadata of datasets and metadata of algorithm which
    gave best results after hyper-parameter tuning. As soon as dataset is loaded into
    the environment, its metadata is generated and stored in the centralized MetaStore.
    Before running any machine learning algorithm (specially for classification and
    regression model) Meta-model algorithm will analyze the metadata of this dataset
    and run algorithm ranking logic which will predict the machine learning model
    out of 10-20 different models that which model might perform better on this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Meta-model algorithm](../Images/00c72639d3a627a2243f2d50693fdfa4.png)'
  prefs: []
  type: TYPE_IMG
- en: Once the algorithm is selected, next step is to run hyper-parameter tuning algorithms
    to tune this model. Meta model algorithm will essentially use MetaStore and machine
    learning algorithm. Meta-model algorithm will also help in reducing the hyper
    parameter space which boosts  convergence rapidly to provide best result.
  prefs: []
  type: TYPE_NORMAL
- en: 'To set up MetaStore, we need to capture Dataset metadata and Best Algorithm
    run metadata. Lot of features can be stored for both. Some of the useful feature
    list is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset metadata**'
  prefs: []
  type: TYPE_NORMAL
- en: No of observations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ratio of the number of observations by the number of attributes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Number of Categorical features
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Number of Numerical features
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Number of Classes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Class imbalance ratio
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Percentage of missing values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multivariate normality
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Skewness of features
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Percentage of the attributes after Feature Selection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Percentage of outliers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decision Tree model characteristics like nodes per feature, maximum tree depth,
    shape, tree imbalance, etc
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Max fisher’s discriminant ratio
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Maximum (individual) feature efficiency
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collective feature efficiency
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fraction of points on the class boundary
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ratio of average intra/inter class nearest neighbor distance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Leave-one- out error rate of the one-nearest neighbor classifier
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Non-linearity of the one-nearest neighbor classifier
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Average number of points per dimension
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Number of optimal clusters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: WSSE of clusters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Domain of the dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Algorithm metadata**'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset ID
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Algorithm ID
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hyperparameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Runtime
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Meta-model algorithm**'
  prefs: []
  type: TYPE_NORMAL
- en: This is the brain of Meta-learning. This itself is a machine learning algorithm
    which basically learns from the various run of different algorithms on a lot of
    datasets. Algorithm metadata which we derived from best algorithm run on a particular
    dataset and stored it in MetaStore, and dataset metadata are combined together
    to form a new dataset which is fed to machine learning algorithm which ranks section
    of ML algorithms for a particular dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Algorithm meta-learning](../Images/6ad36314803ac0e1dd5a9bf08c411e99.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Metalearning to obtain metaknowledge for algorithm selection, source: Soares,
    C., Giraud-Carrier, C., Brazdil, P., Vilalta, R.: Metalearning: Applications to
    Data Mining. Springer (2009)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hyper parameter tuning**'
  prefs: []
  type: TYPE_NORMAL
- en: Almost all machine learning algorithm have hyper-parameters. SVM has Regularization
    parameter( C ), k-means has Number of clusters(k), Neural network has Number of
    hidden layers, dropout, etc as hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Efficiently finding the best solution of these hyper-parameters is the key
    to any ML solution. There are basically three famous techniques to tune the hyper-parameters
    of any machine learning algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hyper-parameter search techniques](../Images/1e1067df10d4d2318cda740e263e475d.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Manual Search**: Data Scientists try different values and by Luck may end
    up getting the best solution. It is very common among DS community.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Grid Search**: Machine does all the work by evaluating each parameters combinations
    and checking for best result. It works only for less feature; as the feature set
    increases so does the number of combination and thereby computation time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random Search**: Works with large features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There has been some recent advancements in this field where data scientists
    are using **Bayesian approach to optimize hyper-parameters**. This approach works
    well with very large number of features but can be an overkill for less data and
    dataset with less features. The goal is to maximize some true unknown function
    f. Information about this function is gained by making observations, which are
    evaluations of the function at specific hyper-parameter values. These observations
    are used to infer a posterior distribution over the function values representing
    the distribution of possible functions. This approach takes less number of iterations
    to optimize the values of hyperparameters. It may not find the best solution always
    but will give relatively close to best solution in amazingly less number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: There has been lot of buzz in this field lately. There are some [commercial](/software/automated-data-science.html)
    and open source tools available which trying to make lives of Data Scientist easier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open source tools to look for:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Spearmint](https://github.com/JasperSnoek/spearmint/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TPOT](https://github.com/rhiever/tpot)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hyperopt](https://github.com/hyperopt/hyperopt)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Further reading and references:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Metalearning for Data Mining and KDD](http://www.fit.vutbr.cz/study/courses/VPD/public/1213VPD-Striz.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLA Bergstra, James S., et al. "[Algorithms for hyperparameter optimization](http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)."
    Advances in Neural Information Processing Systems. 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TPOT: A Python tool for automating data science](http://www.randalolson.com/2016/05/08/tpot-a-python-tool-for-automating-data-science/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Bayesian Optimization for Hyperparameter Tuning](https://arimo.com/data-science/2016/bayesian-optimization-hyperparameter-tuning/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bio: [Ankit Sharma](https://www.linkedin.com/in/anktsh)** is a Data Scientist
    at DataRPM.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Data Science Automation: Debunking Misconceptions](/2016/08/data-science-automation-debunking-misconceptions.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[And the Winner is… Stepwise Regression](/2016/08/winner-stepwise-regression.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TPOT: A Python Tool for Automating Data Science](/2016/05/tpot-python-automating-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Neural Networks and Deep Learning: A Textbook (2nd Edition)](https://www.kdnuggets.com/2023/07/aggarwal-neural-networks-deep-learning-textbook-2nd-edition.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Announcing a Blog Writing Contest, Winner Gets an NVIDIA GPU!](https://www.kdnuggets.com/2022/11/blog-writing-contest-nvidia-gpu.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Tools for Automating Data Cleaning Processes](https://www.kdnuggets.com/5-tools-for-automating-data-cleaning-processes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Automating the Chain of Thought: How AI Can Prompt Itself to Reason](https://www.kdnuggets.com/2023/07/automating-chain-of-thought-ai-prompt-itself-reason.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Science Minimum: 10 Essential Skills You Need to Know to Start…](https://www.kdnuggets.com/2020/10/data-science-minimum-10-essential-skills.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
