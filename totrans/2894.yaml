- en: OpenStreetMap Data to ML Training Labels for Object Detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenStreetMap 数据到机器学习训练标签用于物体检测
- en: 原文：[https://www.kdnuggets.com/2019/09/openstreetmap-data-ml-training-labels-object-detection.html](https://www.kdnuggets.com/2019/09/openstreetmap-data-ml-training-labels-object-detection.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/09/openstreetmap-data-ml-training-labels-object-detection.html](https://www.kdnuggets.com/2019/09/openstreetmap-data-ml-training-labels-object-detection.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Shay Strong](https://www.linkedin.com/in/shay-strong/), Director of Data
    Science & Machine Learning at EagleView**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Shay Strong](https://www.linkedin.com/in/shay-strong/)，EagleView 数据科学与机器学习总监**'
- en: So I wanted to create a seamless tutorial for taking OpenStreetMap (OSM) vector
    data and converting it for use with machine learning (ML) models. In particular,
    I am really interested in creating a tight, clean pipeline for disaster relief
    applications, where we can use something like crowd sourced building polygons
    from OSM to train a supervised object detector to discover buildings in an unmapped
    location.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我想创建一个无缝的教程，将 OpenStreetMap (OSM) 矢量数据转换为机器学习 (ML) 模型使用的格式。我特别感兴趣于为灾难救援应用创建一个紧密、清晰的流程，我们可以使用类似于
    OSM 的众包建筑多边形来训练一个监督式物体检测器，以发现未标记的位置中的建筑物。
- en: 'The recipe for building a basic deep learning object detector is to have two
    components: (1) training data (raster image + vector label pairs) and (2) model
    framework. The deep learning model itself will be a Single Shot Detector (SSD)
    object detector. We will use OSM polygons as the basis of our label data and Digital
    Globe imagery for the raster data. We won’t go into the details of an SSD here,
    as there are plenty sources available. We will run the object detector in [AWS
    Sagemaker](https://aws.amazon.com/sagemaker/). This current article only focuses
    on the training data generation. I’ll follow up with a separate article on model
    training.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 构建基本深度学习物体检测器的步骤有两个组成部分：（1）训练数据（光栅图像 + 矢量标签对）和（2）模型框架。深度学习模型本身将是一个 Single Shot
    Detector (SSD) 物体检测器。我们将使用 OSM 多边形作为标签数据的基础，使用 Digital Globe 的影像作为光栅数据。这里不会详细讨论
    SSD，因为有很多可用的资源。我们将在 [AWS Sagemaker](https://aws.amazon.com/sagemaker/) 上运行物体检测器。当前文章仅关注训练数据生成。我将跟进一篇关于模型训练的单独文章。
- en: I should note that this tutorial is available in this [Github repo](https://github.com/shaystrong/sagely),
    so you can bypass this article if you want to dive in. Although it is a work in
    progress as part of an upcoming [UW Geohackweek](https://geohackweek.github.io/).
    I anticpate using this tutorial in conjunction with [HOT-OSM](https://www.hotosm.org/) related
    tasks — where there may be crowd-sourced vector data as part of a specific project.
    For the purpose of establishing a demo, we will use a recent HOT OSM task area
    that was impacted by Cyclone Kenneth in 2019, Nzwani, Comores.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该指出，这个教程可以在这个 [Github 仓库](https://github.com/shaystrong/sagely) 中找到，因此如果你想直接开始，可以跳过这篇文章。尽管它仍在进行中，作为即将到来的
    [UW Geohackweek](https://geohackweek.github.io/) 的一部分。我预计将这个教程与 [HOT-OSM](https://www.hotosm.org/)
    相关任务结合使用——那里可能会有作为特定项目一部分的众包矢量数据。为了建立一个演示，我们将使用一个最近的 HOT OSM 任务区域，该区域在 2019 年受到气旋
    Kenneth 的影响，即 Nzwani, Comores。
- en: '![figure-name](../Images/0fff813d4fc63f67a48919ba2489c09e.png)Cyclone Kenneth
    track in late April 2019.![figure-name](../Images/d3896569ea9e025396e63a3818695d41.png)HOT-OSM
    task data from [https://wiki.openstreetmap.org/wiki/2019_Cyclone_Kenneth](https://wiki.openstreetmap.org/wiki/2019_Cyclone_Kenneth).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![figure-name](../Images/0fff813d4fc63f67a48919ba2489c09e.png)2019年4月底的气旋 Kenneth
    路径。![figure-name](../Images/d3896569ea9e025396e63a3818695d41.png)HOT-OSM 任务数据来自
    [https://wiki.openstreetmap.org/wiki/2019_Cyclone_Kenneth](https://wiki.openstreetmap.org/wiki/2019_Cyclone_Kenneth)。'
- en: The OSM vector data is freely available. The imagery we require is often not.
    Welcome to the world of GIS. BUT, [Digital Globe has Open Data](https://www.digitalglobe.com/ecosystem/open-data) imagery
    for many of these natural disaster areas. So we can grab that data for this application
    (a little later on).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: OSM 矢量数据是免费的。我们所需的影像通常不是。欢迎来到 GIS 的世界。但，[Digital Globe 提供开放数据](https://www.digitalglobe.com/ecosystem/open-data)的影像覆盖了许多自然灾害区域。因此，我们可以稍后获取这些数据用于本应用。
- en: I decided to use the [OSMNX](https://automating-gis-processes.github.io/2017/lessons/L7/retrieve-osm-data.html) python
    library for interfacing with OSM (which can be a bit daunting otherwise). Based
    on the HOT-OSM task, Nzwani, Comores was labeled as an ‘Urgent’ location (see
    table above). So I’m going to start there because it seems like a lot of training
    data could be available.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我决定使用[OSMNX](https://automating-gis-processes.github.io/2017/lessons/L7/retrieve-osm-data.html)
    Python 库来与 OSM 接口（否则可能会有些令人生畏）。根据 HOT-OSM 任务，Nzwani, Comores 被标记为“紧急”位置（见上表）。所以我会从那里开始，因为似乎可以获得大量训练数据。
- en: '![figure-name](../Images/4e8c6c678c3ffab9fedbfa3990865d6b.png)(left) The OSM
    ‘DiGraph’ of nodes & edges. (right) The Nzwani landmass with the roads and buildings
    overlaid.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![figure-name](../Images/4e8c6c678c3ffab9fedbfa3990865d6b.png)（左）OSM 的节点和边的‘DiGraph’。（右）Nzwani
    陆地与叠加的道路和建筑物。'
- en: If you inspect the length of the buildings (a geopandas dataframe that is returned),
    you will see a significant amount of features. We will use these as the training
    data for our object detectors.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你检查建筑物的长度（返回的 geopandas 数据框），你会看到大量特征。我们将这些作为我们对象检测器的训练数据。
- en: Let’s grab some DG imagery. To create an object detector, we will mimic the [VOC
    Pascal training data format](http://host.robots.ox.ac.uk/pascal/VOC/), where we
    require pairs of images (jpegs) and vector (xml) labels. The xml files are formated
    in a particular way. You can read about it (on this difficult to navigate) [website](http://host.robots.ox.ac.uk/pascal/VOC/).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们获取一些 DG 影像。为了创建一个对象检测器，我们将模仿[VOC Pascal 训练数据格式](http://host.robots.ox.ac.uk/pascal/VOC/)，我们需要图像（jpegs）和矢量（xml）标签的配对。xml
    文件以特定方式格式化。你可以在这个难以浏览的[网站](http://host.robots.ox.ac.uk/pascal/VOC/)上阅读相关内容。
- en: I would prefer to pull the data directly from the Digital Globe Open Data website,
    but unforuntately at present, there seems to be no imagery that covers this area
    post-event. I really want to keep this location as a point of focus, given the
    amount of buildings we found and the relevance of this task. So, I will just separately
    download the necessary imagery and make a small subset available for this demo.
    You can find a sample GeoTIFF in the [GitHub repo](https://github.com/shaystrong/sagely).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我更愿意直接从 Digital Globe Open Data 网站获取数据，但不幸的是，目前似乎没有涵盖该区域的事件后影像。我真的希望将这个位置作为重点，考虑到我们发现的建筑物数量以及这个任务的相关性。所以，我会单独下载所需的影像，并为这个演示提供一个小子集。你可以在[GitHub
    repo](https://github.com/shaystrong/sagely)中找到一个样本 GeoTIFF。
- en: Let me digress a little about the [DG Open Data website](https://www.digitalglobe.com/ecosystem/open-data).
    This website is rather difficult to search, and even though a huge amount of timely
    and relevant imagery is made available free for areas impacted by natural disasters,
    it is nearly impossible to search efficiently in a geospatial way as an individual
    user. Bottom line, it is a little hard to determine which image may optimally
    contain a significant amount of buildings for the region I am interested in. My
    typical process would be to click through some thumbnails on the website and find
    a region with significant urban-looking growth. There has to be buildings in urban
    areas! I sort of gave up after 20 minutes of looking at patterns in thumbnails
    (well 45 min, as I can get obsessive at pattern matching). Assuming the DG Open
    Data has this image at some point in the future, the subsequent steps will still
    be consistent for any GeoTIFF you download.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我稍微离题谈谈[DG Open Data 网站](https://www.digitalglobe.com/ecosystem/open-data)。这个网站相当难以搜索，尽管提供了大量及时且相关的影像供受自然灾害影响的地区免费使用，但作为个人用户，几乎不可能有效地以地理空间方式进行搜索。底线是，很难确定哪张图像可能最优地包含我感兴趣地区的大量建筑物。我通常的做法是点击网站上的一些缩略图，找到一个显著的城市化增长区域。城市区域里必须有建筑物！我看了20分钟的缩略图模式（实际上是45分钟，因为我对模式匹配可能会变得过于执着）。假设
    DG Open Data 将来会有这张图像，那么随后的步骤对于你下载的任何 GeoTIFF 仍然一致。
- en: If you get imagery directly from the DG Open Data website, they are actually
    in the format of a Cloud Optimized GeoTIFF (COG) which turn out to be super convenient
    to create a virtual raster (vrt) from. The benefit of this is that we can create
    a light-weight file that loads in a local QGIS window without downloading the
    entire tif. We can cut, subset, etc and just wind up with the image we want, and
    not supereflous regions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你直接从 DG 开放数据网站获取图像，它们实际上是 Cloud Optimized GeoTIFF (COG) 格式，这样做创建虚拟栅格（vrt）非常方便。这样做的好处是我们可以创建一个轻量级文件，在本地
    QGIS 窗口中加载，而不需要下载整个 tif。我们可以裁剪、子集化等，只得到我们想要的图像，而不是多余的区域。
- en: Once you have the GeoTIFF, we will use GDAL to translate it to a [MBtile](https://github.com/mapbox/mbtiles-spec) format
    and then unpack it to it x/y/z slippymap (TMS) directory structure. MBtiles are
    super advantageous for us here, since there a 256x256 image chips (png) that lend
    themselves well for deep learning model training formats.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你获得 GeoTIFF，我们将使用 GDAL 将其转换为 [MBtile](https://github.com/mapbox/mbtiles-spec)
    格式，然后解压到 x/y/z slippymap (TMS) 目录结构中。MBtiles 对我们非常有利，因为 256x256 图像块（png）非常适合深度学习模型训练格式。
- en: You may notice that the images generated are in a nested file structure. We
    need to ‘flatten’ them, so that all the images are in a single directory. You
    can’t just copy them to a single directory since the .png filenames are not unique.
    We will also convert them to jpeg from png. We will stick them in a nonsensical
    VOC-like folder to distinguish it from a legit VOC dataset, VOC1900.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到生成的图像以嵌套文件结构存储。我们需要将它们“扁平化”，使所有图像都在一个目录中。你不能直接将它们复制到一个目录，因为 .png 文件名并不唯一。我们还将把它们从
    png 转换为 jpeg。我们将把它们放在一个毫无意义的 VOC 样式文件夹中，以区分于真正的 VOC 数据集 VOC1900。
- en: Yay. Images are done!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了，图像已经完成！
- en: Next, we will take our buildings and buffer them to the nearest rectangle. This
    is what the object detector wants.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对建筑物进行缓冲处理，使其对齐到最近的矩形。这是对象检测器所需要的。
- en: Now that we have the buildings represented as axis-aligned bounding boxes, we
    will want to use my new favorite utility called [Supermercado](https://github.com/mapbox/supermercado).
    We will go to the ‘supermarket’ to identify all the TMS tiles that the building
    boxes overlap. We will map the TMS tile ID to the building box itself and then
    convert the building box, which is in a geospatial lat/long format, to a slippymap
    tile reference frame. Now we will have a building box on a TMS tile grid consistent
    with the imagery we unpacked on the same grid.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将建筑物表示为轴对齐的边界框，我们将使用我新的最爱工具 [Supermercado](https://github.com/mapbox/supermercado)。我们将进入“超市”，识别所有与建筑框重叠的
    TMS 瓦片。我们将 TMS 瓦片 ID 映射到建筑框，然后将建筑框（以地理空间的经纬度格式表示）转换为 slippymap 瓦片参考系。现在，我们将在与我们解压的图像相同的网格上拥有一个建筑框。
- en: Finally, we want to cleanup the xml labels and images to ensure we remove excess
    vector labels. We only want an identical pair of images and annotations.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们要清理 xml 标签和图像，以确保移除多余的矢量标签。我们只想要一对一的图像和注释。
- en: You should now have a clean and ready to go directory of VOC-style images and
    labels ready for training.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在应该有一个干净且准备好的 VOC 样式图像和标签目录，准备进行训练。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next will be the actual training. Stay tuned.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来将是实际的培训，请继续关注。
- en: '**Bio: [Shay Strong](https://www.linkedin.com/in/shay-strong/)** is the Director
    of Data Science & Machine Learning at EagleView and has interests in Geospatial
    Machine Learning & Remote Sensing.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介：[Shay Strong](https://www.linkedin.com/in/shay-strong/)** 是 EagleView
    的数据科学与机器学习总监，对地理空间机器学习与遥感感兴趣。'
- en: Original. Reposted with permission.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 原文。已获授权转载。
- en: '**Related:**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[A 2019 Guide to Object Detection](/2019/08/2019-guide-object-detection.html)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2019 年对象检测指南](/2019/08/2019-guide-object-detection.html)'
- en: '[Analyze a Soccer (Football) Game Using Tensorflow Object Detection and OpenCV](/2018/07/analyze-soccer-game-using-tensorflow-object-detection-opencv.html)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Tensorflow 对象检测和 OpenCV 分析足球比赛](/2018/07/analyze-soccer-game-using-tensorflow-object-detection-opencv.html)'
- en: '[Pedestrian Detection in Aerial Images Using RetinaNet](/2019/03/pedestrian-detection-aerial-images-retinanet.html)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 RetinaNet 进行空中图像行人检测](/2019/03/pedestrian-detection-aerial-images-retinanet.html)'
- en: '* * *'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三名课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的 IT 工作'
- en: '* * *'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Dealing With Noisy Labels in Text Data](https://www.kdnuggets.com/2023/04/dealing-noisy-labels-text-data.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[处理文本数据中的噪声标签](https://www.kdnuggets.com/2023/04/dealing-noisy-labels-text-data.html)'
- en: '[KDnuggets™ News 22:n09, Mar 2: Telling a Great Data Story: A…](https://www.kdnuggets.com/2022/n09.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets™ 新闻 22:n09, 3月2日: 讲述一个精彩的数据故事：A…](https://www.kdnuggets.com/2022/n09.html)'
- en: '[What Is the Difference Between SQL and Object-Relational Mapping (ORM)?](https://www.kdnuggets.com/2022/02/difference-sql-object-relational-mapping-orm.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQL 与面向对象关系映射（ORM）之间的区别是什么？](https://www.kdnuggets.com/2022/02/difference-sql-object-relational-mapping-orm.html)'
- en: '[A Beginner''s Guide to Anomaly Detection Techniques in Data Science](https://www.kdnuggets.com/2023/05/beginner-guide-anomaly-detection-techniques-data-science.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中异常检测技术的初学者指南](https://www.kdnuggets.com/2023/05/beginner-guide-anomaly-detection-techniques-data-science.html)'
- en: '[KDnuggets News, August 17: How to Perform Motion Detection Using…](https://www.kdnuggets.com/2022/n33.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，8月17日: 如何使用…进行运动检测](https://www.kdnuggets.com/2022/n33.html)'
- en: '[Density Kernel Depth for Outlier Detection in Functional Data](https://www.kdnuggets.com/density-kernel-depth-for-outlier-detection-in-functional-data)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[功能数据中的密度核深度异常检测](https://www.kdnuggets.com/density-kernel-depth-for-outlier-detection-in-functional-data)'
