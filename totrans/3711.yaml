- en: Research Papers for NLP Beginners
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/11/research-papers-nlp-beginners.html](https://www.kdnuggets.com/2022/11/research-papers-nlp-beginners.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Research Papers for NLP Beginners](../Images/06c3080b57d4b24c0cd05a468f8c5022.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Sincerely Media](https://unsplash.com/@sincerelymedia) via Unsplash'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re new to the world of data and have a particular interest in NLP (Natural
    Language Processing), you’re probably looking for resources to help grasp a better
    understanding.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: You have probably come across so many different research papers and are sitting
    there confused about which one to choose. Because let’s face it, they’re not short
    and they do consume a lot of brain power. So it would be smart to choose the right
    one that will benefit your path to mastering NLP.
  prefs: []
  type: TYPE_NORMAL
- en: I have done some research and have collected a few NLP research papers that
    have been highly recommended for newbies in the NLP area and overall NLP knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: I will break it up into sections so you can go find exactly what you want.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning and NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**[Text Classification from Labeled and Unlabeled Documents using EM](https://www.ri.cmu.edu/pub_files/pub1/nigam_k_1999_1/nigam_k_1999_1.pdf)
    by Kamal Nigam, 1999**'
  prefs: []
  type: TYPE_NORMAL
- en: This paper is about how you can improve the accuracy of learned text classifiers
    by augmenting a small number of labeled training documents with a large pool of
    unlabeled documents.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://aclanthology.org/2020.acl-main.442.pdf)
    by Marco Tulio Ribeiro et al., 2020**'
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, you will learn more about CheckList, a task-agnostic methodology
    for testing NLP models as unfortunately some of the most used current approaches
    overestimate the performance of NLP models.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**[Natural Language Processing (almost) from Scratch](https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf)
    by Ronan Collobert, 2011**'
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, you will go through the foundations of NLP - as it states in
    the title, it is ALMOST from scratch. Topics include Named Entity Recognition,
    Semantic role labeling, networks, training, and more.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
    by Christopher Olah, 2015**'
  prefs: []
  type: TYPE_NORMAL
- en: Neural Networks are a major part of NLP, therefore having a good understanding
    of it will benefit you in the long run. In this paper, there is a focus on LSTM
    networks which are widely used.
  prefs: []
  type: TYPE_NORMAL
- en: Word/Sentence Representation and Embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**[Distributed Representations of Words and Phrases and their Compositionality](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)
    by Tomas Mikolov, 2013**'
  prefs: []
  type: TYPE_NORMAL
- en: Written by Mikolov, who introduced the Skip-gram model for learning high-quality
    vector representations of words from large amounts of unstructured text data -
    this paper will present several extensions of the original Skip-gram model.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Distributed Representations of Sentences and Documents](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)**
    by Quoc Le and Tomas Mikolov, 2014'
  prefs: []
  type: TYPE_NORMAL
- en: Going into more depth about the two major weaknesses of bag-of-words, the authors
    introduce Paragraph Vector - which is an unsupervised algorithm that learns fixed-length
    feature representations from variable-length pieces of text, such as sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Language Modelling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
    by Alec Radford, 2018**'
  prefs: []
  type: TYPE_NORMAL
- en: Natural language processing tasks are normally approached with supervised learning
    on task-specific datasets. However, Multitask learning is being tested as a promising
    framework for improving general performance in NLP.
  prefs: []
  type: TYPE_NORMAL
- en: '**[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)**
    by Andrej Karpathy, 2015'
  prefs: []
  type: TYPE_NORMAL
- en: This paper goes back to the start of recurrent neural networks and why they
    are so effective and robust with code examples to give you a better understanding
  prefs: []
  type: TYPE_NORMAL
- en: Attention & Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
    by Jacob Devlin et al., 2019**'
  prefs: []
  type: TYPE_NORMAL
- en: As you’re learning about machine learning, you have probably heard about BERT
    - Bidirectional Encoder Representations from Transformers. It is widely used and
    known for being able to pre-train deep bidirectional representations from unlabeled
    text. In this paper, you will further understand and learn how to improve your
    fine-tuning based on BERT.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf) by Ashish
    Vaswani et al., 2017**'
  prefs: []
  type: TYPE_NORMAL
- en: This paper focuses on the Transformer, solely on attention mechanisms which
    differ from models which are typically based on complex recurrent or convolutional
    neural networks. You will learn how Transformer generalizes well to other tasks
    and may be the better option.
  prefs: []
  type: TYPE_NORMAL
- en: '**[HuggingFace''s Transformers: State-of-the-art Natural Language Processing](https://arxiv.org/pdf/1910.03771.pdf)
    by Thomas Wolf et al., 2020**'
  prefs: []
  type: TYPE_NORMAL
- en: Want to learn more about Transformers which has become the dominant architecture
    for natural language processing? In this paper, you will learn more about its
    architecture and how it facilitates the distribution of pre-trained models.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like I said above, I don’t want to overwhelm you with so many different research
    papers - therefore I have kept it at a minimal level.
  prefs: []
  type: TYPE_NORMAL
- en: If you know of any that beginners may benefit from, please drop them in the
    comments so that they can see them. Thank you!
  prefs: []
  type: TYPE_NORMAL
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and Freelance Technical Writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Must Read NLP Papers from the Last 12 Months](https://www.kdnuggets.com/2023/03/must-read-nlp-papers-last-12-months.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8 Innovative BERT Knowledge Distillation Papers That Have Changed…](https://www.kdnuggets.com/2022/09/eight-innovative-bert-knowledge-distillation-papers-changed-nlp-landscape.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Generative Agent Research Papers You Should Read](https://www.kdnuggets.com/generative-agent-research-papers-you-should-read)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top 5 NLP Cheat Sheets for Beginners to Professional](https://www.kdnuggets.com/2022/12/top-5-nlp-cheat-sheets-beginners-professional.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Brief Introduction to Papers With Code](https://www.kdnuggets.com/2022/04/brief-introduction-papers-code.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, April 27: A Brief Introduction to Papers With Code;…](https://www.kdnuggets.com/2022/n17.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
