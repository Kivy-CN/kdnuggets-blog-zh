# 对文本数据的完整探索性数据分析和可视化：结合可视化和NLP生成见解

> 原文：[https://www.kdnuggets.com/2019/05/complete-exploratory-data-analysis-visualization-text-data.html/2](https://www.kdnuggets.com/2019/05/complete-exploratory-data-analysis-visualization-text-data.html/2)

![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](/2019/05/complete-exploratory-data-analysis-visualization-text-data.html?page=2#comments)

### 使用Plotly进行双变量可视化

双变量可视化是一种同时包含两个特征的可视化类型。它描述了两个特征之间的关联或关系。

****情感极性得分的推荐分布****

```py

x1 = df.loc[df['Recommended IND'] == 1, 'polarity']
x0 = df.loc[df['Recommended IND'] == 0, 'polarity']

trace1 = go.Histogram(
    x=x0, name='Not recommended',
    opacity=0.75
)
trace2 = go.Histogram(
    x=x1, name = 'Recommended',
    opacity=0.75
)

data = [trace1, trace2]
layout = go.Layout(barmode='overlay', title='Distribution of Sentiment polarity of reviews based on Recommendation')
fig = go.Figure(data=data, layout=layout)

iplot(fig, filename='overlaid histogram')

```

polarity_recommendation.py[![Plot 129](../Images/1bc9c153aba2aa641649e93b197b3110.png)](https://plot.ly/~susanli2005/129/ "Plot 129")

图22

显然，评价的极性得分越高，更有可能被推荐。

**推荐的评分分布**

```py

x1 = df.loc[df['Recommended IND'] == 1, 'Rating']
x0 = df.loc[df['Recommended IND'] == 0, 'Rating']

trace1 = go.Histogram(
    x=x0, name='Not recommended',
    opacity=0.75
)
trace2 = go.Histogram(
    x=x1, name = 'Recommended',
    opacity=0.75
)

data = [trace1, trace2]
layout = go.Layout(barmode='overlay', title='Distribution of Sentiment polarity of reviews based on Recommendation')
fig = go.Figure(data=data, layout=layout)

iplot(fig, filename='overlaid histogram')

```

rating_recommendation.py[![Plot 132](../Images/f6ed904d8549d438781f93440afe7389.png)](https://plot.ly/~susanli2005/132/ "Plot 132")

图23

推荐的评论比未推荐的评论有更高的评分。

****评论长度的推荐分布****

```py

x1 = df.loc[df['Recommended IND'] == 1, 'review_len']
x0 = df.loc[df['Recommended IND'] == 0, 'review_len']

trace1 = go.Histogram(
    x=x0, name='Not recommended',
    opacity=0.75
)
trace2 = go.Histogram(
    x=x1, name = 'Recommended',
    opacity=0.75
)

data = [trace1, trace2]
layout = go.Layout(barmode = 'group', title='Distribution of Review Lengths Based on Recommendation')
fig = go.Figure(data=data, layout=layout)

iplot(fig, filename='stacked histogram')

```

review_length_recommend.py![figure-name](../Images/9068575cf6e638c67942692947772d61.png)图24

推荐的评论往往比未推荐的评论更长。

****情感极性与评分的2D密度联合图****

```py

trace1 = go.Scatter(
    x=df['polarity'], y=df['Rating'], mode='markers', name='points',
    marker=dict(color='rgb(102,0,0)', size=2, opacity=0.4)
)
trace2 = go.Histogram2dContour(
    x=df['polarity'], y=df['Rating'], name='density', ncontours=20,
    colorscale='Hot', reversescale=True, showscale=False
)
trace3 = go.Histogram(
    x=df['polarity'], name='Sentiment polarity density',
    marker=dict(color='rgb(102,0,0)'),
    yaxis='y2'
)
trace4 = go.Histogram(
    y=df['Rating'], name='Rating density', marker=dict(color='rgb(102,0,0)'),
    xaxis='x2'
)
data = [trace1, trace2, trace3, trace4]

layout = go.Layout(
    showlegend=False,
    autosize=False,
    width=600,
    height=550,
    xaxis=dict(
        domain=[0, 0.85],
        showgrid=False,
        zeroline=False
    ),
    yaxis=dict(
        domain=[0, 0.85],
        showgrid=False,
        zeroline=False
    ),
    margin=dict(
        t=50
    ),
    hovermode='closest',
    bargap=0,
    xaxis2=dict(
        domain=[0.85, 1],
        showgrid=False,
        zeroline=False
    ),
    yaxis2=dict(
        domain=[0.85, 1],
        showgrid=False,
        zeroline=False
    )
)

fig = go.Figure(data=data, layout=layout)
iplot(fig, filename='2dhistogram-2d-density-plot-subplots')

```

sentiment_polarity_rating.py[![Plot 134](../Images/100bb1c6f37173edebf3f828fc5409e0.png)](https://plot.ly/~susanli2005/134/ "Plot 134")

图24

****年龄和情感极性联合图的2D密度****

```py

trace1 = go.Scatter(
    x=df['Age'], y=df['polarity'], mode='markers', name='points',
    marker=dict(color='rgb(102,0,0)', size=2, opacity=0.4)
)
trace2 = go.Histogram2dContour(
    x=df['Age'], y=df['polarity'], name='density', ncontours=20,
    colorscale='Hot', reversescale=True, showscale=False
)
trace3 = go.Histogram(
    x=df['Age'], name='Age density',
    marker=dict(color='rgb(102,0,0)'),
    yaxis='y2'
)
trace4 = go.Histogram(
    y=df['polarity'], name='Sentiment Polarity density', marker=dict(color='rgb(102,0,0)'),
    xaxis='x2'
)
data = [trace1, trace2, trace3, trace4]

layout = go.Layout(
    showlegend=False,
    autosize=False,
    width=600,
    height=550,
    xaxis=dict(
        domain=[0, 0.85],
        showgrid=False,
        zeroline=False
    ),
    yaxis=dict(
        domain=[0, 0.85],
        showgrid=False,
        zeroline=False
    ),
    margin=dict(
        t=50
    ),
    hovermode='closest',
    bargap=0,
    xaxis2=dict(
        domain=[0.85, 1],
        showgrid=False,
        zeroline=False
    ),
    yaxis2=dict(
        domain=[0.85, 1],
        showgrid=False,
        zeroline=False
    )
)

fig = go.Figure(data=data, layout=layout)
iplot(fig, filename='2dhistogram-2d-density-plot-subplots')

```

age_polarity.py[![Plot 136](../Images/3670fe2de21e461d1091db45a30b5ed3.png)](https://plot.ly/~susanli2005/136/ "Plot 136")

图25

很少有人非常积极或非常消极。给予中性到积极评价的人更可能在30多岁。可能这个年龄段的人更活跃。

### 查找特征术语及其关联

有时我们想要分析不同类别使用的词汇，并输出一些显著的术语关联。我们将使用 [scattertext](https://github.com/JasonKessler/scattertext#using-scattertext-as-a-text-analysis-library-finding-characteristic-terms-and-their-associations) 和 [spaCy](https://github.com/explosion/spaCy) 库来完成这些任务。

首先，我们需要将数据框转换为Scattertext语料库。要查找部门名称的差异，将`category_col`参数设置为`'Department Names'`，并使用`Review Text`列中的评论，通过设置`text`列参数进行分析。最后，将spaCy模型传递到`nlp`参数中，并调用`build()`构建语料库。

以下是区分评论文本与一般英语语料库的术语。

```py

corpus = st.CorpusFromPandas(df, category_col='Department Name', text_col='Review Text', nlp=nlp).build()
print(list(corpus.get_scaled_f_scores_vs_background().index[:10]))
```

![figure-name](../Images/28fe2a769350d32ca97a895dfccd8661.png)图 26

以下是与 Tops 部门最相关的评论文本中的术语：

```py

term_freq_df = corpus.get_term_freq_df()
term_freq_df['Tops Score'] = corpus.get_scaled_f_scores('Tops')
pprint(list(term_freq_df.sort_values(by='Tops Score', ascending=False).index[:10]))

```

![figure-name](../Images/27533f8f6456bda4504515f56f09b8ad.png)图 27

以下是与 Dresses 部门最相关的术语：

```py
term_freq_df['Dresses Score'] = corpus.get_scaled_f_scores('Dresses')
pprint(list(term_freq_df.sort_values(by='Dresses Score', ascending=False).index[:10]))
```

![figure-name](../Images/9fc92d9c804114e75397d199304fe16d.png)图 28

### 主题建模评论文本

最后，我们希望探索将主题建模算法应用于这个数据集，以查看它是否能提供任何好处，并与我们正在做的评论文本特征相匹配。

我们将尝试在主题建模中使用潜在语义分析（LSA）技术。

+   从评论文本生成文档-词项矩阵到 [TF-IDF 特征](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) 的矩阵。

+   LSA 模型用 TF-IDF 分数替换了文档-词项矩阵中的原始计数。

+   使用 [truncated SVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) 对文档-词项矩阵进行降维。

+   因为部门数量为 6，我们设置了 `n_topics=6`。

+   取这个主题矩阵中每个评论文本的 `argmax` 将给出数据中每个评论文本的预测主题。然后，我们可以将这些主题按数量排序。

+   为了更好地理解每个主题，我们将找出每个主题中最常见的三个词。

```py

reindexed_data = df['Review Text']
tfidf_vectorizer = TfidfVectorizer(stop_words='english', use_idf=True, smooth_idf=True)
reindexed_data = reindexed_data.values
document_term_matrix = tfidf_vectorizer.fit_transform(reindexed_data)
n_topics = 6
lsa_model = TruncatedSVD(n_components=n_topics)
lsa_topic_matrix = lsa_model.fit_transform(document_term_matrix)

def get_keys(topic_matrix):
    '''
    returns an integer list of predicted topic 
    categories for a given topic matrix
    '''
    keys = topic_matrix.argmax(axis=1).tolist()
    return keys

def keys_to_counts(keys):
    '''
    returns a tuple of topic categories and their 
    accompanying magnitudes for a given list of keys
    '''
    count_pairs = Counter(keys).items()
    categories = [pair[0] for pair in count_pairs]
    counts = [pair[1] for pair in count_pairs]
    return (categories, counts)

lsa_keys = get_keys(lsa_topic_matrix)
lsa_categories, lsa_counts = keys_to_counts(lsa_keys)

def get_top_n_words(n, keys, document_term_matrix, tfidf_vectorizer):
    '''
    returns a list of n_topic strings, where each string contains the n most common 
    words in a predicted category, in order
    '''
    top_word_indices = []
    for topic in range(n_topics):
        temp_vector_sum = 0
        for i in range(len(keys)):
            if keys[i] == topic:
                temp_vector_sum += document_term_matrix[i]
        temp_vector_sum = temp_vector_sum.toarray()
        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)
        top_word_indices.append(top_n_word_indices)   
    top_words = []
    for topic in top_word_indices:
        topic_words = []
        for index in topic:
            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))
            temp_word_vector[:,index] = 1
            the_word = tfidf_vectorizer.inverse_transform(temp_word_vector)[0][0]
            topic_words.append(the_word.encode('ascii').decode('utf-8'))
        top_words.append(" ".join(topic_words))         
    return top_words

    top_n_words_lsa = get_top_n_words(3, lsa_keys, document_term_matrix, tfidf_vectorizer)

for i in range(len(top_n_words_lsa)):
print("Topic {}: ".format(i+1), top_n_words_lsa[i])

```

topic_model_LSA.py![figure-name](../Images/9059fb1e44278c775b451ad01abe95fd.png)图 29

```py

top_3_words = get_top_n_words(3, lsa_keys, document_term_matrix, tfidf_vectorizer)
labels = ['Topic {}: \n'.format(i) + top_3_words[i] for i in lsa_categories]

fig, ax = plt.subplots(figsize=(16,8))
ax.bar(lsa_categories, lsa_counts);
ax.set_xticks(lsa_categories);
ax.set_xticklabels(labels);
ax.set_ylabel('Number of review text');
ax.set_title('LSA topic counts');
plt.show();

```

![figure-name](../Images/28dd8235dc2d19df0413fe550239224c.png)图 30

通过查看每个主题中最常见的词汇，我们可以感受到在主题类别之间可能无法达到任何程度的分离。换句话说，我们无法通过主题建模技术将评论文本按部门分开。

主题建模技术有一些重要的局限性。首先，“主题”这个词有些模糊，现在可能已经清楚，主题模型不会对我们的数据产生高度细化的文本分类。

此外，我们可以观察到绝大多数评论文本被归类为第一个主题（主题 0）。LSA 主题建模的 t-SNE 可视化效果不会很美观。

所有代码可以在 [Jupyter notebook](https://github.com/susanli2016/NLP-with-Python/blob/master/EDA%20and%20visualization%20for%20Text%20Data.ipynb) 找到。代码和交互式可视化可以在 [nbviewer](https://nbviewer.jupyter.org/github/susanli2016/NLP-with-Python/blob/master/EDA%20and%20visualization%20for%20Text%20Data.ipynb) 上查看。

快乐的周一！

**Bio: [Susan Li](https://www.linkedin.com/in/susanli/)** 正在通过一篇文章改变世界。她是一位高级数据科学家，位于加拿大多伦多。

[原文](https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a)。已获许可转载。

**相关：**

+   [ELMo: 上下文语言嵌入](/2019/01/elmo-contextual-language-embedding.html)

+   [你需要了解的 NLP 和机器学习文本预处理知识](/2019/04/text-preprocessing-nlp-machine-learning.html)

+   [使用 SpaCy 在 Python 中进行文本分类的机器学习](/2018/09/machine-learning-text-classification-using-spacy-python.html)

* * *

## 我们的前三名课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持组织的 IT 需求

* * *

### 更多相关内容

+   [通过 Google MusicLM 从文本生成音乐](https://www.kdnuggets.com/2023/06/generate-music-text-google-musiclm.html)

+   [掌握 SQL、Python、数据清理、数据处理和探索性数据分析的指南集合](https://www.kdnuggets.com/collection-of-guides-on-mastering-sql-python-data-cleaning-data-wrangling-and-exploratory-data-analysis)

+   [针对非结构化数据的探索性数据分析技术](https://www.kdnuggets.com/2023/05/exploratory-data-analysis-techniques-unstructured-data.html)

+   [数据科学家的探索性数据分析必备指南](https://www.kdnuggets.com/2023/06/data-scientist-essential-guide-exploratory-data-analysis.html)

+   [掌握探索性数据分析的 7 个步骤](https://www.kdnuggets.com/7-steps-to-mastering-exploratory-data-analysis)

+   [SQL Group By 和 Partition By 场景：何时以及如何结合…](https://www.kdnuggets.com/sql-group-by-and-partition-by-scenarios-when-and-how-to-combine-data-in-data-science)
