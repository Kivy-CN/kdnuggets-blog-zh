["```py\n# * all_xs: All the feature values\n# * all_ys: All the outcome values\n# datapoint_size: Number of points/entries in all_xs/all_ys\n# batch_size: Configure this to:\n#             1: stochastic mode\n#             integer < datapoint_size: mini-batch mode\n#             datapoint_size: batch mode\n# i: Current epoch number\n\nif datapoint_size == batch_size:\n  # Batch mode so select all points starting from index 0\n  batch_start_idx = 0\nelif datapoint_size < batch_size:\n  # Not possible\n  raise ValueError(“datapoint_size: %d, must be greater than         \n                    batch_size: %d” % (datapoint_size, batch_size))\nelse:\n  # stochastic/mini-batch mode: Select datapoints in batches\n  #                             from all possible datapoints\n  batch_start_idx = (i * batch_size) % (datapoint_size — batch_size)\n  batch_end_idx = batch_start_idx + batch_size\n  batch_xs = all_xs[batch_start_idx:batch_end_idx]\n  batch_ys = all_ys[batch_start_idx:batch_end_idx]\n\n# Get batched datapoints into xs, ys, which is fed into\n# 'train_step'\nxs = np.array(batch_xs)\nys = np.array(batch_ys)\n```", "```py\n# Modify [B] to make 'learn_rate' a 'tf.placeholder'\n# and supply it to the 'learning_rate' parameter name of\n# tf.train.GradientDescentOptimizer\nlearn_rate = tf.placeholder(tf.float32, shape=[])\ntrain_step = tf.train.GradientDescentOptimizer(\n    learning_rate=learn_rate).minimize(cost)\n\n# Modify [D] to include feed a 'learn_rate' value,\n# which is the 'initial_learn_rate' divided by\n# 'i' (current epoch number)\n# NOTE: Oversimplified. For example only.\nfeed = { x: xs, y_: ys, learn_rate: initial_learn_rate/i }\nsess.run(train_step, feed_dict=feed)\n```"]