- en: 'Ensemble Learning Techniques: A Walkthrough with Random Forests in Python'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Machine learning models have become an integral component of decision-making
    across multiple industries, yet they often encounter difficulty when dealing with
    noisy or diverse data sets. That is where Ensemble Learning comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: This article will demystify ensemble learning and introduce you to its powerful
    random forest algorithm. No matter if you are a data scientist looking to hone
    your toolkit or a developer looking for practical insights into building robust
    machine learning models, this piece is meant for everyone!
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this article, you will gain a thorough knowledge of Ensemble Learning
    and how Random Forests in Python work. So whether you are an experienced data
    scientist or simply curious to expand your machine-learning abilities, join us
    on this adventure and advance your machine-learning expertise!
  prefs: []
  type: TYPE_NORMAL
- en: 1\. What Is Ensemble Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensemble learning is a machine learning approach in which predictions from multiple
    weak models are combined with each other to get stronger predictions. The concept
    behind ensemble learning is decreasing the bias and errors from single models
    by leveraging the predictive power of each model.
  prefs: []
  type: TYPE_NORMAL
- en: To have a better example let's take a life example imagine that you have seen
    an animal and you do not know what species this animal belongs to. So instead
    of asking one expert, you ask ten experts and you will take the vote of the majority
    of them. This is known as **hard voting**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Hard voting** is when we take into account the class predictions for each
    classifier and then classify an input based on the maximum votes to a particular
    class. On the other hand, **soft voting** is when we take into account the probability
    predictions for each class by each classifier and then classify an input to the
    class with maximum probability based on the average probability (averaged over
    the classifier''s probabilities) for that class.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. When to Use Ensemble Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensemble learning is always used to improve the model performance which includes
    improving the classification accuracy and decreasing the mean absolute error for
    regression models. In addition to this ensemble learners always yield a more stable
    model. Ensemble learners work at their best when the models are not correlated
    then every model can learn something unique and work on improving the overall
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Ensemble Learning Strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although ensemble learning can be applied in many ways, however when it comes
    to applying it to practice there are three strategies that have gained a lot of
    popularity due to their easy implementation and usage. These three strategies
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging**: Bagging which is short for bootstrap aggregation is an ensemble
    learning strategy in which the models are trained using random samples of the
    data set.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Stacking:** Stacking which is short for stacked generalization is an ensemble
    learning strategy in which we train a model to combine multiple models trained
    on our data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Boosting**: Boosting is an ensemble learning technique that focuses on selecting
    the misclassified data to train the models on.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's dive deeper into each of these strategies and see how we can use Python
    to train these models on our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Bagging Ensemble Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bagging takes random samples of data, and uses learning algorithms and the mean
    to find bagging probabilities; also known as bootstrap aggregating; it aggregates
    results from multiple models to get one broad outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach involves:'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the original dataset into multiple subsets with replacement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Develop base models for each of these subsets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Running all models concurrently before running all predictions through to obtain
    final predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[**Scikit-learn**](https://scikit-learn.org/stable/) provides us with the ability
    to implement both a [**BaggingClassifier**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)
    and [**BaggingRegressor**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html).
    A BaggingMetaEstimator identifies random subsets of an original dataset to fit
    each base model, then aggregates individual base model predictions?—?either through
    voting or averaging?—?into a final prediction by aggregating individual base model
    predictions into an aggregate prediction using voting or averaging. This method
    reduces variance by randomizing their construction process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an example in which we use the bagging estimator using scikit learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The bagging classifier takes into consideration several parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**base_estimator**: The base model used in the bagging approach. Here we use
    the decision tree classifier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**n_estimators:** The number of estimators we will use in the bagging approach.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_samples**: The number of samples that will be drawn from the training
    set for each base estimator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_features:** The number of features that will be used to train each base
    estimator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we will fit this classifier on the training set and score it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We can do the same for regression tasks, the difference will be that we will
    be using regression estimators instead.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 5\. Stacking Ensemble Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stacking is a technique for combining multiple estimators in order to minimize
    their biases and produce accurate predictions. Predictions from each estimator
    are then combined and fed into an ultimate prediction meta-model trained through
    cross-validation; stacking can be applied to both classification and regression
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: '![Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](../Images/973ed1cfbb38a533bda3c0f44e520316.png)'
  prefs: []
  type: TYPE_IMG
- en: Stacking ensemble learning
  prefs: []
  type: TYPE_NORMAL
- en: 'Stacking occurs in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the data into a training and validation set
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divide the training set into K folds
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a base model on k-1 folds and make predictions on the k-th fold
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat until you have a prediction for each fold
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the base model on the whole training set
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the model to make predictions on the test set
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 3–6 for other base models
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use predictions from the test set as features of a new model (the meta model)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make final predictions on the test set using the meta-model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this example below, we begin by creating two base classifiers (RandomForestClassifier
    and GradientBoostingClassifier) and one meta-classifier (LogisticRegression) and
    use K-fold cross-validation to use predictions from these classifiers on training
    data (iris dataset) for input features for our meta-classifier (LogisticRegression).
  prefs: []
  type: TYPE_NORMAL
- en: After using K-fold cross-validation to make predictions from the base classifiers
    on test data sets as input features for our meta-classifier, predictions on test
    sets using both sets together and evaluate their accuracy against their stacked
    ensemble counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 6\. Boosting Ensemble Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Boosting is a machine learning ensemble technique that reduces bias and variance
    by turning weak learners into strong learners. These weak learners are applied
    sequentially to the dataset; firstly by creating an initial model and fitting
    it to the training set. Once errors from the first model have been identified,
    another model is designed to correct them.
  prefs: []
  type: TYPE_NORMAL
- en: There are popular algorithms and implementations for boosting ensemble learning
    techniques. Let's explore the most famous ones.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1\. AdaBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AdaBoost is an effective ensemble learning technique, that employs weak learners
    sequentially for training purposes. Each iteration prioritizes incorrect predictions
    while decreasing weight assigned to correctly predicted instances; this strategic
    emphasis on challenging observations compels AdaBoost to become increasingly accurate
    over time, with its ultimate prediction determined by aggregating majority votes
    or weighted sum of its weak learners.
  prefs: []
  type: TYPE_NORMAL
- en: 'AdaBoost is a versatile algorithm suitable for both regression and classification
    tasks, but here we focus on its application to classification problems using Scikit-learn.
    Let’s look at how we can use it for classification tasks in the example below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we used the AdaBoostClassifier from scikit learn and set the
    n_estimators to 100\. The default learn is a decision tree and you can change
    it. In addition to this, the parameters of the decision tree can be tuned.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. EXtreme Gradient Boosting (XGBoost)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: eXtreme Gradient Boosting or is more popularly known as XGBoost, is one of the
    best implementations of boosting ensemble learners due to its parallel computations
    which makes it very optimized to run on a single computer. XGBoost is available
    to use through the xgboost package developed by the machine learning community.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 3\. LightGBM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LightGBM is another gradient-boosting algorithm that is based on tree learning.
    However, it is unlike other tree-based algorithms in that it uses leaf-wise tree
    growth which makes it converge faster.
  prefs: []
  type: TYPE_NORMAL
- en: '![Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](../Images/180766a57c79ba30658b281015966a98.png)'
  prefs: []
  type: TYPE_IMG
- en: Leaf-wise tree growth / Image by [LightGBM](https://lightgbm.readthedocs.io/en/latest/Features.html#other-features)
  prefs: []
  type: TYPE_NORMAL
- en: 'In the example below we will apply LightGBM to a binary classification problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Ensemble learning and random forests are powerful machine learning models that
    are always used by machine learning practitioners and data scientists. In this
    article, we covered the basic intuition behind them, when to use them, and finally,
    we covered the most popular algorithms of them and how to use them in Python.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[A Gentle Introduction to Ensemble Learning Algorithms](https://machinelearningmastery.com/tour-of-ensemble-learning-algorithms/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Comprehensive Guide to Ensemble Learning: What Exactly Do You Need to Know](https://neptune.ai/blog/ensemble-learning-guide)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Comprehensive Guide to Ensemble Learning (with Python codes)](https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[Youssef Rafaat](https://www.linkedin.com/in/youssef-hosni-b2960b135)** is
    a computer vision researcher & data scientist. His research focuses on developing
    real-time computer vision algorithms for healthcare applications. He also worked
    as a data scientist for more than 3 years in the marketing, finance, and healthcare
    domain.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Decision Trees vs Random Forests, Explained](https://www.kdnuggets.com/2022/08/decision-trees-random-forests-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[When Would Ensemble Techniques be a Good Choice?](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Classification Metrics Walkthrough: Logistic Regression with…](https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ensemble Learning with Examples](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hyperparameter Tuning Using Grid Search and Random Search in Python](https://www.kdnuggets.com/2022/10/hyperparameter-tuning-grid-search-random-search-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Random Forest vs Decision Tree: Key Differences](https://www.kdnuggets.com/2022/02/random-forest-decision-tree-key-differences.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
