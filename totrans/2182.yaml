- en: Feature Engineering for Beginners
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/feature-engineering-for-beginners](https://www.kdnuggets.com/feature-engineering-for-beginners)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Feature Engineering for Beginners](../Images/2f15c607ba1b2815fa1fd5510d9140e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by Author
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering is one of the most important aspects of the machine learning
    pipeline. It is the practice of creating and modifying features, or variables,
    for the purposes of improving model performance. Well-designed features can transform
    weak models into strong ones, and it is through feature engineering that models
    can become both more robust and accurate. Feature engineering acts as the bridge
    between the dataset and the model, giving the model everything it needs to effectively
    solve a problem.
  prefs: []
  type: TYPE_NORMAL
- en: This is a guide intended for new data scientists, data engineers, and machine
    learning practitioners. The objective of this article is to communicate fundamental
    feature engineering concepts and provide a toolbox of techniques that can be applied
    to real-world scenarios. My aim is that, by the end of this article, you will
    be armed with enough working knowledge about feature engineering to apply it to
    your own datasets to be fully-equipped to begin creating powerful machine learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Features are measurable characteristics of any phenomenon that we are observing.
    They are the granular elements that make up the data with which models operate
    upon to make predictions. Examples of features can include things like age, income,
    a timestamp, longitude, value, and almost anything else one can think of that
    can be measured or represented in some form.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different feature types, the main ones being:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Numerical Features: Continuous or discrete numeric types (e.g. age, salary)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Categorical Features: Qualitative values representing categories (e.g. gender,
    shoe size type)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text Features: Words or strings of words (e.g. "this" or "that" or "even this")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Time Series Features: Data that is ordered by time (e.g. stock prices)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Features are crucial in machine learning because they directly influence a model's
    ability to make predictions. Well-constructed features improve model performance,
    while bad features make it harder for a model to produce strong predictions. Feature
    selection and feature engineering are preprocessing steps in the machine learning
    process that are used to prepare the data for use by learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'A distinction is made between feature selection and feature engineering, though
    both are crucial in their own right:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature Selection: The culling of important features from the entire set of
    all available features, thus reducing dimensionality and promoting model performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feature Engineering: The creation of new features and subsequent changing of
    existing ones, all in the aid of making a model perform better'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By selecting only the most important features, feature selection helps to only
    leave behind the signal in the data, while feature engineering creates new features
    that help to model the outcome better.
  prefs: []
  type: TYPE_NORMAL
- en: Basic Techniques in Feature Engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While there are a wide range of basic feature engineering techniques at our
    disposal, we will walk through some of the more important and well-used of these.
  prefs: []
  type: TYPE_NORMAL
- en: Handling Missing Values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is common for datasets to contain missing information. This can be detrimental
    to a model''s performance, which is why it is important to implement strategies
    for dealing with missing data. There are a handful of common methods for rectifying
    this issue:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mean/Median Imputation: Filling missing areas in a dataset with the mean or
    median of the column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mode Imputation: Filling missing spots in a dataset with the most common entry
    in the same column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interpolation: Filling in missing data with values of data points around it'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These fill-in methods should be applied based on the nature of the data and
    the potential effect that the method might have on the end model.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with missing information is crucial in keeping the integrity of the
    dataset in tact. Here is an example Python code snippet that demonstrates various
    data filling methods using the `pandas` library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Encoding of Categorical Variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recalling that most machine learning algorithms are best (or only) equipped
    to deal with numeric data, categorical variables must often be mapped to numerical
    values in order for said algorithms to better interpret them. The most common
    encoding schemes are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'One-Hot Encoding: Producing separate columns for each category'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Label Encoding: Assigning an integer to each category'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Target Encoding: Encoding categories by their individual outcome variable averages'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The encoding of categorical data is necessary for planting the seeds of understanding
    in many machine learning models. The right encoding method is something you will
    select based on the specific situation, including both the algorithm at use and
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Below is an example Python script for the encoding of categorical features using
    `pandas` and elements of `scikit-learn`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Scaling and Normalizing Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For good performance of many machine learning methods, scaling and normalization
    needs to be performed on your data. There are several methods for scaling and
    normalizing data, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Standardization: Transforming data so that it has a mean of 0 and a standard
    deviation of 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Min-Max Scaling: Scaling data to a fixed range, such as [0, 1]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Robust Scaling: Scaling high and low values iteratively by the median and interquartile
    range, respectively'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scaling and normalization of data is crucial for ensuring that feature contributions
    are equitable. These methods allow the varying feature values to contribute to
    a model commensurately.
  prefs: []
  type: TYPE_NORMAL
- en: Below is an implementation, using `scikit-learn`, that shows how to complete
    data that has been scaled and normalized.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The basic techniques above along with the corresponding example code provide
    pragmatic solutions for missing data, encoding categorical variables, and scaling
    and normalizing data using powerhouse Python tools `pandas` and `scikit-learn`.
    These techniques can be integrated into your own feature engineering process to
    improve your machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Techniques in Feature Engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now turn our attention to to more advanced featured engineering techniques,
    and include some sample Python code for implementing these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Creation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With feature creation, new features are generated or modified to fashion a
    model with better performance. Some techniques for creating new features include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Polynomial Features: Creation of higher-order features with existing features
    to capture more complex relationships'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interaction Terms: Features generated by combining several features to derive
    interactions between them'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Domain-Specific Feature Generation: Features designed based on the intricacies
    of subjects within the given problem realm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating new features with adapted meaning can greatly help to boost model performance.
    The next script showcases how feature engineering can be used to bring latent
    relationships in data to light.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Dimensionality Reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to simplify models and increase their performance, it can be useful
    to downsize the number of model features. Dimensionality reduction techniques
    that can help achieve this goal include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA (Principal Component Analysis): Transformation of predictors into a new
    feature set comprised of linearly independent model features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 't-SNE (t-Distributed Stochastic Neighbor Embedding): Dimension reduction that
    is used for visualization purposes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LDA (Linear Discriminant Analysis): Finding new combinations of model features
    that are effective for deconstructing different classes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to shrink the size of your dataset and maintain its relevancy, dimensional
    reduction techniques will help. These techniques were devised to tackle the high-dimensional
    issues related to data, such as overfitting and computational demand.
  prefs: []
  type: TYPE_NORMAL
- en: A demonstration of data shrinking implemented with `scikit-learn` is shown next.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Time Series Feature Engineering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With time-based datasets, specific feature engineering techniques must be used,
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lag Features: Former data points are used to derive model predictive features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rolling Statistics: Data statistics are calculated across data windows, such
    as rolling means'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Seasonal Decomposition: Data is partitioned into signal, trend, and random
    noise categories'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Temporal models need varying augmentation compared to direct model fitting.
    These methods follow temporal dependence and patterns to make the predictive model
    sharper.
  prefs: []
  type: TYPE_NORMAL
- en: A demonstration of time series features augmenting applied using `pandas` is
    shown next as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The above examples demonstrate practical applications of advanced feature engineering
    techniques, through usage of `pandas` and `scikit-learn`. By employing these methods
    you can enhance the predictive power of your model.
  prefs: []
  type: TYPE_NORMAL
- en: Practical Tips and Best Practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here are a few simple but important tips to keep in mind while working through
    your feature engineering process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Iteration: Feature engineering is a trial-and-error process, and you will get
    better with it each time you iterate. Test different feature engineering ideas
    to find the best set of features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Domain Knowledge: Utilize expertise from those who know the subject matter
    well when creating features. Sometimes subtle relationships can be captured with
    realm-specific knowledge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Validation and Understanding of Features: By understanding which features are
    most important to your mode, you are equipped to make important decisions. Tools
    for determining feature importance include:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SHAP (SHapley Additive exPlanations): Helping to quantify the contribution
    of each feature in predictions'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LIME (Local Interpretable Model-agnostic Explanations): Showcasing the meaning
    of model predictions in plain language'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An optimal mix of complexity and interpretability is necessary for having both
    good and simple to digest results.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This short guide has addressed fundamental feature engineering concepts, as
    well as basic and advanced techniques, and practical recommendations and best
    practices. What many would consider some of the most important feature engineering
    practices — dealing with missing information, encoding of categorical data, scaling
    data, and creation of new features — were covered.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering is a practice that becomes better with execution, and I
    hope you have been able to take something away with you that may improve your
    data science skills. I encourage you to apply these techniques to your own work
    and to learn from your experiences.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that, while the exact percentage varies depending on who tells it,
    a majority of any machine learning project is spent in the data preparation and
    preprocessing phase. Feature engineering is a part of this lengthy phase, and
    as such should be viewed with the import that it demands. Learning to see feature
    engineering what it is — a helping hand in the modeling process — should make
    it more digestible to newcomers.
  prefs: []
  type: TYPE_NORMAL
- en: Happy engineering!
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.linkedin.com/in/mattmayo13/)****[Matthew Mayo](https://www.kdnuggets.com/wp-content/uploads/./profile-pic.jpg)****
    ([**@mattmayo13**](https://twitter.com/mattmayo13)) holds a master''s degree in
    computer science and a graduate diploma in data mining. As managing editor of
    [KDnuggets](https://www.kdnuggets.com/) & [Statology](https://www.statology.org/),
    and contributing editor at [Machine Learning Mastery](https://machinelearningmastery.com/),
    Matthew aims to make complex data science concepts accessible. His professional
    interests include natural language processing, language models, machine learning
    algorithms, and exploring emerging AI. He is driven by a mission to democratize
    knowledge in the data science community. Matthew has been coding since he was
    6 years old.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Feature Store Summit 2022: A free conference on Feature Engineering](https://www.kdnuggets.com/2022/10/hopsworks-feature-store-summit-2022-free-conference-feature-engineering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a Tractable, Feature Engineering Pipeline for Multivariate…](https://www.kdnuggets.com/2022/03/building-tractable-feature-engineering-pipeline-multivariate-time-series.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using RAPIDS cuDF to Leverage GPU in Feature Engineering](https://www.kdnuggets.com/2023/06/rapids-cudf-leverage-gpu-feature-engineering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Practical Approach To Feature Engineering In Machine Learning](https://www.kdnuggets.com/2023/07/practical-approach-feature-engineering-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Free Data Engineering Course for Beginners](https://www.kdnuggets.com/free-data-engineering-course-for-beginners)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Feature Stores for Real-time AI & Machine Learning](https://www.kdnuggets.com/2022/03/feature-stores-realtime-ai-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
