- en: How Multimodality Makes LLM Alignment More Challenging
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态如何使 LLM 对齐变得更具挑战性
- en: 原文：[https://www.kdnuggets.com/how-multimodality-makes-llm-alignment-more-challenging](https://www.kdnuggets.com/how-multimodality-makes-llm-alignment-more-challenging)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/how-multimodality-makes-llm-alignment-more-challenging](https://www.kdnuggets.com/how-multimodality-makes-llm-alignment-more-challenging)
- en: '![How Multimodality Makes LLM Alignment More Challenging](../Images/e6cbe8ab3a64b72a64c59cf090a65e96.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![多模态如何使 LLM 对齐变得更具挑战性](../Images/e6cbe8ab3a64b72a64c59cf090a65e96.png)'
- en: Image by [Gerd Altmann](https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2814937)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2814937)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Gerd Altmann](https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2814937)
    来自 [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2814937)
- en: About a month ago OpenAI announced that ChatGPT can now see, hear, and speak.
    This means the model can help you with more everyday tasks. For example, you can
    upload a picture of the contents of your fridge and ask for meal ideas to prepare
    with the ingredients you have. Or you can photograph your living room and ask
    ChatGPT for art and decoration tips.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 大约一个月前，OpenAI 宣布 ChatGPT 现在可以看、听和说。这意味着模型可以帮助你处理更多的日常任务。例如，你可以上传冰箱里食材的照片，并请求根据你拥有的材料提出用餐建议。或者你可以拍摄客厅的照片，向
    ChatGPT 请求艺术和装饰建议。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: This is possible because ChatGPT uses multimodal GPT-4 as an underlying model
    that can accept both images and text inputs. However, the new capabilities bring
    new challenges for the model alignment teams that we will discuss in this article.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这是可能的，因为 ChatGPT 使用了多模态的 GPT-4 作为底层模型，它可以接受图像和文本输入。然而，这些新功能为模型对齐团队带来了新的挑战，我们将在本文中讨论。
- en: Alignment in LLMs
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 的对齐
- en: 'The term “*aligning LLMs*” refers to training the model to behave according
    to human expectations. This often means understanding human instructions and producing
    responses that are useful, accurate, safe, and unbiased. To teach the model the
    right behavior, we provide examples using two steps: supervised fine-tuning (SFT)
    and reinforcement learning with human feedback (RLHF).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: “*对齐大型语言模型*”一词指的是训练模型按照人类期望的行为。这通常意味着理解人类指令，并生成有用、准确、安全且无偏见的回答。为了教会模型正确的行为，我们通过两个步骤提供示例：监督微调（SFT）和带有人类反馈的强化学习（RLHF）。
- en: Supervised fine-tuning (SFT) teaches the model to follow specific instructions.
    In the case of ChatGPT, this means providing examples of conversations. The underlying
    base model GPT-4 is not able to do that yet because it was trained to predict
    the next word in a sequence, not to answer chatbot-like questions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 监督微调（SFT）教会模型遵循特定的指令。在 ChatGPT 的情况下，这意味着提供对话示例。基础模型 GPT-4 目前还不能做到这一点，因为它被训练来预测序列中的下一个词，而不是回答类似聊天机器人的问题。
- en: While SFT gives ChatGPT its ‘chatbot’ nature, its answers are still far from
    perfect. Therefore, Reinforcement Learning from Human Feedback (RLHF) is applied
    to improve the truthfulness, harmlessness, and helpfulness of the answers. Essentially,
    the instruction-tuned algorithm is asked to produce several answers which are
    then ranked by humans using the criteria mentioned above. This allows the reward
    algorithm to learn human preferences and is used to retrain the SFT model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 SFT 赋予 ChatGPT “聊天机器人”的特性，但其回答仍远非完美。因此，应用了带有人类反馈的强化学习（RLHF）来提高回答的真实性、无害性和有用性。基本上，指令调整算法被要求生成多个回答，然后由人类根据上述标准进行排名。这使得奖励算法能够学习人类的偏好，并用于重新训练
    SFT 模型。
- en: After this step, a model is aligned with human values, or at least we hope so.
    But why does multimodality make this process a step harder?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal Data and New Challenges
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we talk about the alignment for multimodal LLMs we should focus on images
    and text. It does not cover all the new ChatGPT capabilities to ¨*see, hear, and
    speak*¨ because the latest two use speech-to-text and text-to-speech models and
    are not directly connected to the LLM model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: So this is when things get a bit more complicated. Images and text together
    are harder to interpret in comparison to just textual input. As a result, ChatGPT-4
    hallucinates quite frequently about objects and people it can or can't see in
    the images.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Gary Marcus wrote an excellent [article](https://garymarcus.substack.com/p/hello-multimodal-hallucinations)
    on multimodal hallucinations which exposes different cases. One of the examples
    showcases ChatGPT reading the time incorrectly from an image. It also struggled
    with counting chairs in a picture of a kitchen and was not able to recognize a
    person wearing a watch in a photo.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![How Multimodality Makes LLM Alignment More Challenging](../Images/d7bc069bbd2f12ae82a482f14874603e.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: Image from [https://twitter.com/anh_ng8](https://twitter.com/anh_ng8)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Images as inputs also open a window for adversarial attacks. They can become
    part of prompt injection attacks or used to pass instructions to jailbreak the
    model into producing harmful content.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Simon Willison documented several image injection attacks in this [post](https://simonwillison.net/2023/Oct/14/multi-modal-prompt-injection/).
    One of the basic examples involves uploading an image to ChatGPT that contains
    new instructions you want it to follow. See example below:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![How Multimodality Makes LLM Alignment More Challenging](../Images/732ae41196025f9798323744196d576d.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: Image from [https://twitter.com/mn_google/status/1709639072858436064](https://twitter.com/mn_google/status/1709639072858436064)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, text in the photo could be replaced by instructions for the model
    to produce hate speech or harmful content.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Improving Alignment for Multimodal Data
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So why is multimodal data harder to align? Multimodal models are still in their
    early stages of development in comparison to unimodal language models. OpenAI
    did not reveal details of how multimodality is achieved in GPT-4 but it is clear
    that they have supplied it with a large amount of text-annotated images.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Text-image pairs are harder to source than purely textual data, there are fewer
    curated datasets of this type, and natural examples are harder to find on the
    internet than simple text.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: The quality of image-text pairs presents an additional challenge. An image with
    a one-sentence text tag is not nearly as valuable as an image with a detailed
    description. In order to have the latter we often need [human annotators](https://toloka.ai/global-crowd/)
    who follow a carefully designed set of instructions to provide the text annotations.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: On top of it, training the model to follow the instructions requires a sufficient
    number of real user prompts using both images and text. Organic examples are again
    hard to come by due to the novelty of the approach and training examples often
    need to be created on demand by humans.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，训练模型遵循指令需要足够数量的实际用户提示，包括图像和文本。由于这种方法的新颖性，有机示例很难获得，训练示例通常需要由人工按需创建。
- en: Aligning multimodal models introduces ethical questions that previously did
    not even need to be considered. Should the model be able to comment on people's
    looks, genders, and races, or recognize who they are? Should it attempt to guess
    the photo locations? There are so many more aspects to align compared to text
    data only.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐多模态模型引发了之前甚至无需考虑的伦理问题。模型是否应该能够评论人们的外貌、性别和种族，或者识别他们是谁？它是否应该尝试猜测照片的位置？与仅有文本数据相比，需要对齐的方面更多。
- en: Summary
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Multimodality brings new possibilities for how the model can be used, but it
    also brings new challenges for model developers who need to ensure the harmlessness,
    truthfulness, and usefulness of the answers. With multimodality, an increased
    number of aspects need aligning, and sourcing good training data for SFT and RLHF
    is more challenging. Those wishing to build or fine-tune multimodal models need
    to be prepared for those new challenges with development flows that incorporate
    high-quality human feedback.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态性为模型的使用带来了新的可能性，但也给模型开发人员带来了新的挑战，他们需要确保答案的无害性、真实性和有用性。由于多模态性，需要对更多方面进行对齐，为SFT和RLHF提供优质的训练数据变得更加困难。希望构建或微调多模态模型的人需要为这些新的挑战做好准备，采用包含高质量人类反馈的开发流程。
- en: '**[](https://www.aboutdatablog.com/about)**[Magdalena Konkiewicz](https://www.aboutdatablog.com/about)****
    is a Data Evangelist at Toloka, a global company supporting fast and scalable
    AI development. She holds a Master''s degree in Artificial Intelligence from Edinburgh
    University and has worked as an NLP Engineer, Developer, and Data Scientist for
    businesses in Europe and America. She has also been involved in teaching and mentoring
    Data Scientists and regularly contributes to Data Science and Machine Learning
    publications.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**[](https://www.aboutdatablog.com/about)**[Magdalena Konkiewicz](https://www.aboutdatablog.com/about)
    是Toloka的数据推广专家，这是一家支持快速且可扩展的AI开发的全球公司。她拥有爱丁堡大学的人工智能硕士学位，曾在欧洲和美国的企业担任NLP工程师、开发人员和数据科学家。她还参与了数据科学家的教学和指导，并定期向数据科学和机器学习出版物做出贡献。'
- en: More On This Topic
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[12 Most Challenging Data Science Interview Questions](https://www.kdnuggets.com/2022/07/12-challenging-data-science-interview-questions.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12个最具挑战的数据科学面试问题](https://www.kdnuggets.com/2022/07/12-challenging-data-science-interview-questions.html)'
- en: '[Web LLM: Bring LLM Chatbots to the Browser](https://www.kdnuggets.com/2023/05/webllm-bring-llm-chatbots-browser.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Web LLM：将LLM聊天机器人带到浏览器](https://www.kdnuggets.com/2023/05/webllm-bring-llm-chatbots-browser.html)'
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[是什么让Python成为初创企业的理想编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
- en: '[What makes a visualization good?](https://www.kdnuggets.com/2022/10/sphere-makes-visualization-good.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么使可视化效果良好？](https://www.kdnuggets.com/2022/10/sphere-makes-visualization-good.html)'
- en: '[Interview Kickstart Data Science Interview Course — What Makes It…](https://www.kdnuggets.com/2022/10/interview-kickstart-data-science-interview-course-makes-different.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Interview Kickstart数据科学面试课程——它的独特之处](https://www.kdnuggets.com/2022/10/interview-kickstart-data-science-interview-course-makes-different.html)'
- en: '[7 Ways ChatGPT Makes You Code Better and Faster](https://www.kdnuggets.com/2023/06/7-ways-chatgpt-makes-code-better-faster.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7种ChatGPT让你编程更高效的方式](https://www.kdnuggets.com/2023/06/7-ways-chatgpt-makes-code-better-faster.html)'
