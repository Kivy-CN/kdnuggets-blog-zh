- en: Is Domain Knowledge Important for Machine Learning?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 领域知识对机器学习重要吗？
- en: 原文：[https://www.kdnuggets.com/2022/07/domain-knowledge-important-machine-learning.html](https://www.kdnuggets.com/2022/07/domain-knowledge-important-machine-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/07/domain-knowledge-important-machine-learning.html](https://www.kdnuggets.com/2022/07/domain-knowledge-important-machine-learning.html)
- en: '![Is Domain Knowledge Important for Machine Learning?](../Images/46d05bad71fb684a9344916cf2fde103.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![领域知识对机器学习重要吗？](../Images/46d05bad71fb684a9344916cf2fde103.png)'
- en: Developing machine learning models involves a lot of steps. Whether you’re working
    with labeled or unlabeled data, you might think numbers are just numbers, and
    it doesn’t matter what each of the features of a dataset signifies when it comes
    to spitting out insights with the potential for true impact. It’s true that there
    are tons of great machine learning libraries out there like [scikit-learn](https://scikit-learn.org/stable/)
    which make it straightforward to gather up some data and plop them into a cookie-cutter
    model. Pretty quickly, you might start to think there’s no problem you can’t solve
    with machine learning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 开发机器学习模型涉及很多步骤。无论你是处理标记数据还是未标记数据，你可能会认为数字只是数字，并且在生成具有真正影响力的见解时，数据集中每个特征的意义并不重要。确实，有许多优秀的机器学习库，如[scikit-learn](https://scikit-learn.org/stable/)，使得收集一些数据并将其放入一个模板模型中变得简单快捷。很快，你可能会开始认为没有什么问题是机器学习无法解决的。
- en: Frankly, that’s a beginner’s mindset. You are not yet aware of everything you
    don’t know. Datasets given in machine learning courses or the free ones you find
    online have often already been groomed and are convenient to use when applying
    machine learning models, but once you take your skills and knowledge out of the
    play-pen and into the real world, you’ll face some additional challenges.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 坦率地说，这是一个初学者的思维方式。你还未意识到你不知道的所有东西。机器学习课程中提供的数据集或你在网上找到的免费数据集通常已经被处理过，使用这些数据集应用机器学习模型时很方便，但一旦你将技能和知识从玩具世界带到现实世界，你将面临一些额外的挑战。
- en: Lots of people believe that domain knowledge, or additional knowledge regarding
    the industry or area the data pertains to, is superfluous. And it’s kind of true.
    Do you NEED domain knowledge in the area you’re developing the model? No. You
    can still produce fairly accurate models without it. Theoretically, deep and machine
    learning are black-box approaches. This means you can put labeled data into a
    model without deep knowledge of the area and without even looking at the data
    very closely.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人认为领域知识，或者关于数据所涉及行业或领域的额外知识，是多余的。这确实有一定的道理。你是否需要在你开发模型的领域中具备领域知识？不需要。你仍然可以在没有它的情况下生成相当准确的模型。从理论上讲，深度学习和机器学习是黑箱方法。这意味着你可以将标记数据放入模型中，而无需对领域有深入了解，甚至无需仔细查看数据。
- en: But, if you go down this route though, you’ll have to deal with the consequences.
    This is a very inefficient way to train classifiers, and in order to properly
    function, you’ll require massive amounts of labeled datasets and a lot of computational
    power in order to produce accurate models.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果你走这条路，你将不得不面对后果。这是一种非常低效的训练分类器的方法，并且为了正常运作，你将需要大量标记数据集和大量计算能力来生成准确的模型。
- en: If you incorporate domain knowledge into your architecture and your model, it
    can make it a lot easier to explain the results, both to yourself and to an outside
    viewer. Every bit of domain knowledge can serve as a stepping stone through the
    black box of a machine learning model.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将领域知识融入到你的架构和模型中，这可以使解释结果变得更加容易，无论是对你自己还是对外部观察者。每一份领域知识都可以作为穿越机器学习模型黑箱的垫脚石。
- en: It's very easy to think that domain knowledge isn’t required because for lots
    of visible datasets like [COCO](https://cocodataset.org/#home), the limited domain
    knowledge that is required is part of being a seeing human. Even more complex
    data sets that contain cancer cells are similarly obvious to the human eye, despite
    a lack of expert-level knowledge. You can do basic evaluation of similarity or
    differences between cells without any specific medical knowledge.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易认为领域知识不是必需的，因为对于像[COCO](https://cocodataset.org/#home)这样许多显而易见的数据集，所需的有限领域知识是作为一个“看得见”的人自然具备的。即使是包含癌细胞的更复杂的数据集，对人眼来说也同样明显，尽管缺乏专家级的知识。你可以在没有任何特定医学知识的情况下，对细胞之间的相似性或差异性进行基本评估。
- en: Natural language processing (NLP) and computer vision are prime examples of
    areas where it’s easy to think that domain knowledge is entirely unnecessary,
    but more so because they are such normal tasks for us, we may not even notice
    how we’re applying our domain knowledge.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）和计算机视觉是领域知识看似完全不必要的典型例子，但正因为这些任务对我们来说非常普通，我们可能甚至不会注意到我们是如何应用领域知识的。
- en: If you start working in areas like outlier detection, which isn’t such an everyday
    human task, the importance of domain knowledge quickly becomes apparent.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你开始在异常值检测等不那么常见的领域工作，领域知识的重要性很快会变得显而易见。
- en: Domain Knowledge for Data Pre-Processing
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理中的领域知识
- en: '![Domain Knowledge for Data Pre-Processing](../Images/26a278e22124fc9d45a46252a6230e6e.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![数据预处理中的领域知识](../Images/26a278e22124fc9d45a46252a6230e6e.png)'
- en: Let’s dig into how domain knowledge can be leveraged in the data pre-processing
    step of the machine learning model development cycle.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨领域知识如何在机器学习模型开发周期的数据预处理步骤中发挥作用。
- en: In a dataset, not every data point has the same value. If you collect 100 new
    samples which are identical, they don’t help the model learn any additional information.
    They might actually focus the model in a specific direction which isn’t important.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个数据集中，并非每个数据点的值都是相同的。如果你收集了100个相同的新样本，它们不会帮助模型学习任何额外的信息。它们可能实际上会使模型集中在一个不重要的特定方向上。
- en: If you’re looking at 100 pictures of umbrellas, and you know this model is supposed
    to classify all kinds of accessories, it’s clear that your sample dataset isn’t
    representative of the whole population. Without domain knowledge, it can be very
    challenging to know which data points add value or if they are already represented
    in the data set.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在查看100张伞的图片，并且知道这个模型应该分类所有种类的配件，那么显然你的样本数据集并不代表整个总体。没有领域知识的话，很难知道哪些数据点有价值，或者它们是否已经在数据集中有所代表。
- en: If you’re working in an area that doesn’t so easily lend itself to your existing
    general knowledge, you can build in biases through the training data which can
    hurt the accuracy and robustness of your model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在一个不容易用现有的通用知识解决的领域工作，你可能会通过训练数据引入偏差，这会影响模型的准确性和鲁棒性。
- en: Another way in which domain knowledge can pack a punch in the data pre-processing
    step is in determining feature importance. If you have a good feel for the importance
    of each feature, you can develop better strategies to process the data accordingly.
    It’s really important to understand what the actual features are in order to do
    so. This has a big influence on how you handle the features going forward.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 领域知识在数据预处理步骤中发挥重要作用的另一个方式是确定特征的重要性。如果你对每个特征的重要性有良好的感觉，你可以制定更好的策略来相应地处理数据。理解实际特征是什么非常重要，这对你如何处理特征有很大的影响。
- en: Domain Knowledge for Choosing the Right Model
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择合适模型中的领域知识
- en: '![Domain Knowledge for Choosing the Right Model](../Images/f8f2cafa7ec802677ffacbfdb8b81888.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![选择合适模型中的领域知识](../Images/f8f2cafa7ec802677ffacbfdb8b81888.png)'
- en: There are many different machine learning models, and some may be more fitting
    than others given many factors. Is the data labeled or unlabeled? How much data
    do you have? What kind of data types are the features? Are the data types of the
    features homogeneous? Is your target output a continuous value or a classification?
    Choosing the right model is important, but it’s very rare to be able to apply
    your selected model directly without making adjustments to it. Random forests,
    for example, can handle heterogeneous data types right out of the box.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型有很多种，考虑到许多因素，有些模型可能比其他模型更适合。数据是标记过的还是未标记的？你拥有多少数据？特征的数据类型是什么？特征的数据类型是否同质？你的目标输出是连续值还是分类？选择合适的模型很重要，但能够直接应用你选择的模型而不进行调整的情况非常少见。例如，随机森林可以直接处理异质数据类型。
- en: Selecting the right model requires in-depth machine learning knowledge, but
    there are lots of resources out there to help you make your selection if you’re
    not quite a machine learning expert. I’ve gathered my top three from machine learning
    cheat sheets from [Towards Data Science](https://towardsdatascience.com/5-minutes-cheat-sheet-explaining-all-machine-learning-models-3fea1cf96f05),
    [datacamp](https://www.datacamp.com/cheat-sheet/machine-learning-cheat-sheet),
    and [Microsoft](https://docs.microsoft.com/en-us/azure/machine-learning/algorithm-cheat-sheet).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的模型需要深入的机器学习知识，但如果你不是机器学习专家，还有很多资源可以帮助你做出选择。我从[Towards Data Science](https://towardsdatascience.com/5-minutes-cheat-sheet-explaining-all-machine-learning-models-3fea1cf96f05)、[datacamp](https://www.datacamp.com/cheat-sheet/machine-learning-cheat-sheet)和[Microsoft](https://docs.microsoft.com/en-us/azure/machine-learning/algorithm-cheat-sheet)的机器学习备忘单中收集了我的前三个推荐。
- en: Domain Knowledge for Adjusting Model and Architecture
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整模型和架构的领域知识
- en: '![Domain Knowledge for Adjusting Model and Architecture](../Images/80bea4d39d4c08f84a11607ddad669d9.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![调整模型和架构的领域知识](../Images/80bea4d39d4c08f84a11607ddad669d9.png)'
- en: Domain knowledge allows you to better adjust the model to fit the situation.
    Mathematical optimizations only go so far, and often to get big jumps in improvement,
    it’s crucial to have considerable domain knowledge in the area.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 领域知识使你能够更好地调整模型以适应情况。数学优化只能做到一定程度，通常要实现大的改进跳跃，拥有相当的领域知识至关重要。
- en: A significant way to apply your domain knowledge to drive an improvement in
    the accuracy and robustness of your model is to incorporate the domain knowledge
    into the architecture of the model you are developing.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 将领域知识应用于提高模型的准确性和鲁棒性的一种重要方法是将领域知识融入到你正在开发的模型的架构中。
- en: As I mentioned before, natural language processing is one of the areas of machine
    learning which makes it clear how domain knowledge can be helpful. Let’s talk
    about word embeddings and attention to showcase how speaking a human language
    is a big help, but thinking like a linguist can really elevate the performance
    of a natural language processing model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，自然语言处理是机器学习的一个领域，这清楚地表明领域知识可以带来帮助。让我们谈谈词嵌入和注意力机制，以展示说人类语言是如何大有帮助的，但像语言学家一样思考可以真正提升自然语言处理模型的性能。
- en: Domain Knowledge for Natural Language Processing
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理中的领域知识
- en: Domain knowledge has been applied to all applications of machine learning. Small
    adjustments have been made over the last few decades to better apply machine learning
    models in many areas. Domain knowledge has definitely been applied to the models
    used in natural language processing. Let’s walk through a few examples of how
    these developments came about.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 领域知识已经应用于所有机器学习应用中。在过去几十年里，已经做出了一些小的调整，以更好地在许多领域应用机器学习模型。领域知识肯定已被应用于自然语言处理中的模型。让我们来看看这些发展是如何产生的。
- en: Word Embeddings
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词嵌入
- en: If you think about numbers and words, there’s a pretty big difference in how
    we think about them. If you have the heights of everyone in a group, you can easily
    spit out some stats regarding the median, outliers, etc, all based on height.
    If everyone in the group gave you a word to represent how they are feeling today,
    how would you convert that to any kind of meaningful aggregate?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑数字和文字，我们对它们的思考方式差别很大。如果你有一组人的身高数据，你可以很容易地根据身高输出一些统计数据，例如中位数、离群值等。如果这个小组中的每个人都给你一个词来表示他们今天的感受，你将如何将其转化为任何有意义的汇总？
- en: You should consider what you can do to create a digital representation of a
    word. Should you just use letters? Does that make sense? As a person who speaks
    the language, we immediately have the meaning behind the word. We do not store
    words by their letters. Think about a tree. Did you picture a tree or did your
    mind go to t-r-e-e? Storing the representation of a word as the letters doesn’t
    really bring us any advantage when it comes to understanding meaning or significance.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该考虑如何创建一个单词的数字表示。是否应该仅仅使用字母？这样做是否有意义？作为一个讲这种语言的人，我们立即理解单词背后的意义。我们不会按字母来存储单词。想想树。你是想象了一棵树，还是你的脑海里出现了
    t-r-e-e？以字母形式存储单词的表示方式，在理解意义或重要性时并没有真正带来任何优势。
- en: '[Word embeddings](https://machinelearningmastery.com/what-are-word-embeddings/)
    are “a type of word representation that allows words with similar meaning to have
    a similar [numerical] representation”. The numerical representations are learned
    using [unsupervised learning models](https://www.stratascratch.com/blog/overview-of-machine-learning-algorithms-unsupervised-learning/).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[词嵌入](https://machinelearningmastery.com/what-are-word-embeddings/)是“一种词表示方法，允许具有相似含义的词具有类似的[数值]表示”。这些数值表示是通过[无监督学习模型](https://www.stratascratch.com/blog/overview-of-machine-learning-algorithms-unsupervised-learning/)学习得到的。'
- en: These numerical representations are vectors that represent how a word is used.
    This numerical representation goes so far as to allow you to use the Euclidean
    distance between two-word representations to quantify how similar two words are
    used in the training text.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数值表示是表示一个词如何使用的向量。这种数值表示甚至可以让你使用两个词表示之间的欧氏距离来量化两个词在训练文本中使用的相似程度。
- en: The vectors for “Adidas” and “Nike” would likely be quite similar. Exactly what
    each field of the vector represents is certainly unclear as they’re developed
    using unsupervised learning, but it makes sense that a word that represents a
    similar concept has a similar representation as far as the model understands.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: “Adidas”和“Nike”的向量可能会非常相似。虽然每个向量字段的具体表示不甚清楚，因为它们是通过无监督学习开发的，但一个代表类似概念的词在模型理解中有类似的表示是有意义的。
- en: Check out our post “[Supervised vs Unsupervised Learning](https://www.stratascratch.com/blog/supervised-vs-unsupervised-learning/)”
    if you want to know what supervised and unsupervised learning actually are and
    the algorithms that use these learning approaches.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解什么是监督学习和无监督学习，以及使用这些学习方法的算法，查看我们的帖子“[监督学习与无监督学习](https://www.stratascratch.com/blog/supervised-vs-unsupervised-learning/)”。
- en: Attention
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意力
- en: Attention is a very valuable and useful concept. Attention has made its way
    into natural language processing and image recognition in the world of machine
    learning models for a very good reason.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力是一个非常宝贵和有用的概念。注意力之所以能在自然语言处理和图像识别中得到应用，是有充分理由的。
- en: Natural language processing for translation has been a deep learning model since
    [the early 2000s](https://aylien.com/blog/a-review-of-the-recent-history-of-natural-language-processing).
    Around 2013, [long short-term memory](https://www.analyticsvidhya.com/blog/2021/06/lstm-for-text-classification/)
    (LSTM) debuted in the field and dominated for a few years. An LTSM model reads
    the sentence, creates a hidden representation, and then uses the hidden representation
    to generate the output sentence.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 自[2000年代初](https://aylien.com/blog/a-review-of-the-recent-history-of-natural-language-processing)以来，自然语言处理翻译一直是深度学习模型的一部分。大约在2013年，[长短期记忆](https://www.analyticsvidhya.com/blog/2021/06/lstm-for-text-classification/)（LSTM）在该领域首次出现，并主导了几年。LSTM模型读取句子，创建隐藏表示，然后利用隐藏表示生成输出句子。
- en: As humans, if we translate, we don’t just read the sentence and spit out the
    translation. We tend to look at the whole sentence again and again, or we’ll focus
    on certain parts when we want to revisit the context of the target word.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人类，如果我们进行翻译，我们不仅仅是阅读句子并直接输出翻译。我们倾向于反复查看整个句子，或者在需要重新审视目标词的上下文时专注于某些部分。
- en: For example the word “read”. Is it representing present or past tense? Which
    other parts of the sentence do you focus on to determine that? Do you need information
    from surrounding sentences? “I read books every year” means one thing on its own,
    but if I were to say, “I did many things before I went to university. I read books
    every year. I ate dinner with my parents every day,” the sentence takes on a different
    meaning, and “read” represents an action that took place in the present in the
    first example and in the past for the second.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，词语“read”。它表示的是现在时还是过去时？你需要关注句子的哪些其他部分来确定？是否需要周围句子的上下文信息？“I read books every
    year”单独来看有一个意思，但如果我说，“I did many things before I went to university. I read books
    every year. I ate dinner with my parents every day”，句子的意义就会发生变化，而“read”在第一个例子中表示的是现在时的动作，在第二个例子中则是过去时的动作。
- en: Attention builds these relationships between different words in the sentence
    or sentences. For each word we want to translate, it highlights different words
    in the original sentence according to the importance of these associated words
    with the target word. Someone who isn’t an experienced or professional translator
    might just translate word-for-word, and while the general information would still
    be conveyed, accounting for the importance of associated words when translating
    the target word will produce a much more accurate translation. Attention allows
    us to build that professional translation pattern into the architecture of the
    deep learning model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制在句子或句子之间建立不同词汇之间的关系。对于我们想要翻译的每个词，它会根据这些相关词与目标词的重要性来突出显示原句中的不同词。一个缺乏经验或专业背景的翻译者可能只会逐字翻译，虽然基本信息会传达，但考虑到相关词的重要性会产生更准确的翻译。注意力机制允许我们将这种专业翻译模式融入深度学习模型的架构中。
- en: Why Domain Knowledge Is Crucial for Machine Learning
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么领域知识对机器学习至关重要
- en: Without domain knowledge, you can check all the boxes of producing an acceptable
    model which spits out some numbers. With domain knowledge, you’ll know what data
    is best to use to train and test your model. You will also realize how you can
    tailor the model you use to better represent the data set and the problem you’re
    trying to tackle, and how to make the best use of the insights your model produces.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 没有领域知识，你可以完成一个能够输出一些数字的可接受模型。拥有领域知识，你将知道什么数据最适合用来训练和测试你的模型。你还会意识到如何调整你使用的模型，以更好地代表数据集和你试图解决的问题，以及如何最好地利用模型产生的洞察。
- en: Machine learning is a toolbox. If you pull out an electrical saw, you’ll probably
    manage to cut some wood, but you probably won’t be able to construct a bunch of
    cabinets without the expert knowledge of a carpenter. Domain knowledge will allow
    you to take the impact of your machine learning skills to a much higher level
    of significance.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一个工具箱。如果你拿出一个电锯，你可能能切割一些木头，但没有木匠的专业知识，你可能无法构建一堆橱柜。领域知识将使你的机器学习技能的影响力达到更高的层次。
- en: '**[Nate Rosidi](https://www.stratascratch.com)** is a data scientist and in
    product strategy. He''s also an adjunct professor teaching analytics, and is the
    founder of [StrataScratch](https://www.stratascratch.com/), a platform helping
    data scientists prepare for their interviews with real interview questions from
    top companies. Connect with him on [Twitter: StrataScratch](https://twitter.com/StrataScratch)
    or [LinkedIn](https://www.linkedin.com/in/nathanrosidi/).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**[内特·罗西迪](https://www.stratascratch.com)** 是一位数据科学家和产品策略专家。他同时担任分析课程的兼职教授，并且是
    [StrataScratch](https://www.stratascratch.com/)，一个帮助数据科学家通过顶级公司的真实面试问题为面试做好准备的平台的创始人。你可以在
    [Twitter: StrataScratch](https://twitter.com/StrataScratch) 或 [LinkedIn](https://www.linkedin.com/in/nathanrosidi/)
    上与他联系。'
- en: '* * *'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三名课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行 IT 相关工作'
- en: '* * *'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[KDnuggets News, August 3: 10 Most Used Tableau Functions • Is…](https://www.kdnuggets.com/2022/n31.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，8 月 3 日：最常用的 10 个 Tableau 函数 • 是…](https://www.kdnuggets.com/2022/n31.html)'
- en: '[Best Practices for Creating Domain-Specific AI Models](https://www.kdnuggets.com/2022/07/best-practices-creating-domainspecific-ai-models.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[创建特定领域 AI 模型的最佳实践](https://www.kdnuggets.com/2022/07/best-practices-creating-domainspecific-ai-models.html)'
- en: '[Brewing a Domain-Specific LLM Potion](https://www.kdnuggets.com/2023/08/brewing-domainspecific-llm-potion.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[酿造特定领域 LLM 药水](https://www.kdnuggets.com/2023/08/brewing-domainspecific-llm-potion.html)'
- en: '[8 Innovative BERT Knowledge Distillation Papers That Have Changed…](https://www.kdnuggets.com/2022/09/eight-innovative-bert-knowledge-distillation-papers-changed-nlp-landscape.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8篇改变NLP领域的创新BERT知识蒸馏论文](https://www.kdnuggets.com/2022/09/eight-innovative-bert-knowledge-distillation-papers-changed-nlp-landscape.html)'
- en: '[5 Ways to Expand Your Knowledge in Data Science Beyond Online Courses](https://www.kdnuggets.com/2022/04/5-ways-expand-knowledge-data-science-beyond-online-courses.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5种超越在线课程拓展数据科学知识的方法](https://www.kdnuggets.com/2022/04/5-ways-expand-knowledge-data-science-beyond-online-courses.html)'
- en: '[Unlock the Wealth of Knowledge with ChatPDF](https://www.kdnuggets.com/2023/04/unlock-wealth-knowledge-chatpdf.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过ChatPDF解锁知识的财富](https://www.kdnuggets.com/2023/04/unlock-wealth-knowledge-chatpdf.html)'
