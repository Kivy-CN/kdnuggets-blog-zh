- en: 'Understanding by Implementing: Decision Tree'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/02/understanding-implementing-decision-tree.html](https://www.kdnuggets.com/2023/02/understanding-implementing-decision-tree.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Understanding by Implementing: Decision Tree](../Images/2a3d41a545fa5b47a7f6524b9046821a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Many advanced machine learning models such as random forests or gradient boosting
    algorithms such as XGBoost, CatBoost, or LightGBM (and even [autoencoders](https://medium.com/towards-data-science/building-a-simple-auto-encoder-via-decision-trees-28ba9342a349)!)
    rely on a crucial common ingredient: the **decision tree**!'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Without understanding decision trees, it is impossible to understand any of
    the aforementioned advanced bagging or gradient-boosting algorithms as well, which
    is a disgrace for any data scientist! So, let us demystify the inner workings
    of a decision tree by implementing one in Python.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, you will learn
  prefs: []
  type: TYPE_NORMAL
- en: why and how a decision tree splits data,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the information gain, and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: how to implement decision trees in Python using NumPy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code on [my Github](https://github.com/Garve/Towards-Data-Science---Notebooks/blob/main/TDS%20-%20Decision%20Tree.ipynb).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to make predictions, decision trees rely on **splitting** the dataset
    into smaller parts in a recursive fashion.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding by Implementing: Decision Tree](../Images/9b00e12744e927f329f5e8d19f29d46c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: In the picture above, you can see one example of a split — the original dataset
    gets separated into two parts. In the next step, both of these parts get split
    again, and so on. This continues until some kind of stopping criterion is met,
    for example,
  prefs: []
  type: TYPE_NORMAL
- en: if the split results in a part being empty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if a certain recursion depth was reached
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if (after previous splits) the dataset only consists of only a few elements,
    making further splits unnecessary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we find these splits? And why do we even care? Let’s find out.
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us assume that we want to solve a **binary** **classification problem** that
    we create ourselves now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The two-dimensional data looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding by Implementing: Decision Tree](../Images/554cbca0ca349471b45b9ef532caf57b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that there are two different classes — purple in about 75% and yellow
    in about 25% of the cases. If you feed this data to a decision tree **classifier**,
    this tree has the following thoughts initially:'
  prefs: []
  type: TYPE_NORMAL
- en: “There are two different labels, which is too messy for me. I want to clean
    up this mess by splitting the data into two parts —these parts should be cleaner
    than the complete data before.” — tree that gained consciousness
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And so the tree does.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding by Implementing: Decision Tree](../Images/ece4cb957d4cfa280d757eca55a617d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The tree decides to make a split approximately along the *x*-axis. This has
    the effect that the top part of the data is now perfectly *clean, ***meaning that
    you only find *a single class* (purple in this case) there.**
  prefs: []
  type: TYPE_NORMAL
- en: However, the bottom part is still *messy*, even messier than before in a sense.
    The class ratio used to be around 75:25 in the complete dataset, but in this smaller
    part it is about 50:50, which is as mixed up as it can get
  prefs: []
  type: TYPE_NORMAL
- en: '* ***Note: ****Here, it doesn’t matter that the purple and yellow are nicely
    separated in the picture. Just the ****raw amout of different labels**** in the
    two parts count.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*![Understanding by Implementing: Decision Tree](../Images/cd6a8063e5483dd3283407b8c6e3d717.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Still, this is good enough as a first step for the tree, and so it carries on.
    While it wouldn’t create another split in the top, *clean* part anymore, it can
    create another split in the bottom part to clean it up.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding by Implementing: Decision Tree](../Images/4c38bd5c08fe69bd3e39b5e99888316a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Et voilà, each of the three separate parts is completely clean, as we only find
    a single color (label) per part.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is really easy to make predictions now: If a new data point comes in, you
    just check in which of the **three parts** it lies and give it the corresponding
    color. This works so well now because each part is *clean*. Easy, right?'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding by Implementing: Decision Tree](../Images/3c98457e2b72d4013552e09d1ef5848e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Alright, we were talking about *clean* and *messy* data but so far these words
    only represent some vague idea. In order to implement anything, we have to find
    a way to define *cleanliness*.
  prefs: []
  type: TYPE_NORMAL
- en: Measures for Cleanliness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us assume that we have some labels, for example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Intuitively, *y₁* is the cleanest set of labels, followed by *y₂* and then *y₃*.
    So far so good, but how can we put numbers on this behavior? Maybe the easiest
    thing that comes to mind is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Just count the amount of zeroes and amount of ones. Compute their absolute difference.
    To make it nicer, normalize it by dividing through the length of the arrays.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For example, *y₂* has 8 entries in total — 6 zeroes and 2 ones. Hence, our
    custom-defined **cleanliness score** would be |6 - 2| / 8 = 0.5\. It is easy to
    calculate that cleanliness scores of *y₁* and *y₃* are 1.0 and 0.0 respectively.
    Here, we can see the general formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding by Implementing: Decision Tree](../Images/b7483ae67d73bcffdc9d3bf191b064cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Here, *n₀* and *n₁* are the numbers of zeroes and ones respectively, *n = n₀
    + n₁* is the length of the array and *p₁ = n₁ / n* is the share of the 1 labels.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with this formula is that it is **specifically tailored to the case
    of two classes**, but very often we are interested in multi-class classification.
    One formula that works quite well is the **Gini impurity measure:**
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding by Implementing: Decision Tree](../Images/decde949856b7174d1d4cb86f9ec1860.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'or the general case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding by Implementing: Decision Tree](../Images/3cf6b03fa036e6e4a3f9d432d2384018.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: It works so well that scikit-learn [adopted it as the default measure](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) for
    its `DecisionTreeClassifier` class.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding by Implementing: Decision Tree](../Images/31661e3c05c0e279e73ceb48861b095c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '* ***Note: ****Gini measures ****messiness**** instead of cleanliness. Example:
    if a list only conains a single class (=very clean data!), then all terms in the
    sum are zero, hence the sum is zero. The worst case is if all classes appear the
    exact number of times, in which case the Gini is 1–1/*C* where *C* is the number
    of classes.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Now that we have a measure for cleanliness/messiness, let us see how it can
    be used to find good splits.'
  prefs: []
  type: TYPE_NORMAL
- en: Finding Splits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a lot of splits we choose from, but which is a good one? Let us use
    our initial dataset again, together with the Gini impurity measure.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding by Implementing: Decision Tree](../Images/8ea233e9da061b673a7c6e958b48929c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: We won’t count the points now, but let us assume that 75% are purple and 25%
    are yellow. Using the definition of Gini, the impurity of the complete dataset
    is
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding by Implementing: Decision Tree](../Images/5234e837ea39350ce44f5fe171fbd64f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'If we split the dataset along the *x*-axis, as done before:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding by Implementing: Decision Tree](../Images/f84e74cf628ae0ff15d1ce417d6d1b95.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The **top part has a Gini impurity of 0.0** and the bottom part
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding by Implementing: Decision Tree](../Images/0aa3e0aad38846e2ecbe4149c772c0c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'On average, the two parts have a Gini impurity of (0.0 + 0.5) / 2 = 0.25, which
    is better than the entire dataset’s 0.375 from before. We can also express it
    in terms of the so-called **information gain**:'
  prefs: []
  type: TYPE_NORMAL
- en: The information gain of this split is 0.375 – 0.25 = 0.125.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Easy as that. The higher the information gain (i.e. the lower the Gini impurity),
    the better.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** Another equally good initial split would be along the y-axis.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An important thing to keep in mind is that it is useful to **weigh the Gini
    impurities of the parts by the size of the parts**. For example, let us assume
    that
  prefs: []
  type: TYPE_NORMAL
- en: part 1 consists of 50 datapoints and has a Gini impurity of 0.0 and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: part 2 consists of 450 datapoints and has a Gini impurity of 0.5,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: then the average Gini impurity should not be (0.0 + 0.5) / 2 = 0.25 but rather
    50 / (50 + 450) * 0.0 + 450 / (50 + 450) * 0.5 = **0.45**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, and how do we find the best split? The simple but sobering answer is:'
  prefs: []
  type: TYPE_NORMAL
- en: Just try out all the splits and pick the one with the highest information gain.
    It’s basically a brute-force approach.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To be more precise, standard decision trees use **splits along the coordinate
    axes**, i.e. *xᵢ = c* for some feature* i *and threshold*c.* This means that
  prefs: []
  type: TYPE_NORMAL
- en: one part of the split data consists of all data points *x *with* xᵢ < c*and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the other part of all points* x *with *xᵢ ≥ c*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These simple splitting rules have proven good enough in practice, but you can
    of course also extend this logic to create other splits (i.e. diagonal lines like *xᵢ
    + 2xⱼ = 3*, for example).
  prefs: []
  type: TYPE_NORMAL
- en: Great, these are all the ingredients that we need to get going now!
  prefs: []
  type: TYPE_NORMAL
- en: The Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will implement the decision tree now. Since it consists of nodes, let us
    define a `Node` class first.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A node knows the feature it uses for splitting (`feature`) as well as the splitting
    value (`value`). `value` is also used as a storage for the final prediction of
    the decision tree. Since we will build a binary tree, each node needs to know
    its left and right children, as stored in `left` and `right` .
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s do the actual decision tree implementation. I’m making it scikit-learn
    compatible, hence I use some classes from `sklearn.base` . If you are not familiar
    with that, check out my article about [how to build scikit-learn compatible models](https://towardsdatascience.com/build-your-own-custom-scikit-learn-regression-5d0d718f289).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s implement!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'And that’s it! You can do all of the things that you love about scikit-learn
    now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Since the tree is unregularized, it is overfitting a lot, hence the perfect
    train score. The accuracy would be worse on unseen data. We can also check how
    the tree looks like via
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As a picture, it would be this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding by Implementing: Decision Tree](../Images/e09eb496451c654e69c87a8984dbb0c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we have seen how decision trees work in detail. We started
    out with some vague, yet intuitive ideas and turned them into formulas and algorithms.
    In the end, we were able to implement a decision tree from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'A word of caution though: Our decision tree cannot be regularized yet. Usually,
    we would like to specify parameters like'
  prefs: []
  type: TYPE_NORMAL
- en: max depth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: leaf size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and minimal information gain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: among many others. Luckily, these things are not that difficult to implement,
    which makes this a perfect homework for you. For example, if you specify `leaf_size=10` as
    a parameter, then nodes containing more than 10 samples should not be split anymore.
    Also, this implementation is **not efficient**. Usually, you would not want to
    store parts of the datasets in nodes, but only the indices instead. So your (potentially
    large) dataset is in memory only once.
  prefs: []
  type: TYPE_NORMAL
- en: 'The good thing is that you can go crazy now with this decision tree template.
    You can:'
  prefs: []
  type: TYPE_NORMAL
- en: implement diagonal splits, i.e. *xᵢ + 2xⱼ = 3* instead of just* xᵢ = 3,*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: change the logic that happens inside of the leaves, i.e. you can run a logistic
    regression within each leaf instead of just doing a majority vote, which gives
    you a [linear tree](https://github.com/cerlymarco/linear-tree)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: change the splitting procedure, i.e. instead of doing brute force, try some
    random combinations and pick the best one, which gives you an [extra-tree classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html#sklearn.tree.ExtraTreeClassifier)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[Dr. Robert Kübler](https://www.linkedin.com/in/robert-kuebler/)** is a Senior
    Data Scientist at METRO.digital and Author at Towards Data Science.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/understanding-by-implementing-decision-tree-dd395867af7e).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Simplifying Decision Tree Interpretability with Python & Scikit-learn](https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Decision Tree Algorithm, Explained](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Telling a Great Data Story: A Visualization Decision Tree](https://www.kdnuggets.com/2021/02/telling-great-data-story-visualization-decision-tree.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Random Forest vs Decision Tree: Key Differences](https://www.kdnuggets.com/2022/02/random-forest-decision-tree-key-differences.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets™ News 22:n09, Mar 2: Telling a Great Data Story: A…](https://www.kdnuggets.com/2022/n09.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Complete Guide To Decision Tree Software](https://www.kdnuggets.com/2022/08/complete-guide-decision-tree-software.html)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
