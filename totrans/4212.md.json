["```py\nimport gym\nenv = gym.make(\"CartPole-v0\")\n```", "```py\nenv.action_space\n```", "```py\nenv.reset()\n```", "```py\n# returns an initial observation\nenv.reset()\n\nfor i in range(20):\n\n  # env.action_space.sample() produces either 0 (left) or 1 (right).\n  observation, reward, done, info = env.step(env.action_space.sample())\n\n  print(\"step\", i, observation, reward, done, info)\n\nenv.close()\n```", "```py\n# install dependencies needed for recording videos\n!apt-get install -y xvfb x11-utils\n!pip install pyvirtualdisplay==0.2.*\n```", "```py\nfrom pyvirtualdisplay import Display\ndisplay = Display(visible=False, size=(1400, 900))\n_ = display.start()\n```", "```py\nfrom gym.wrappers.monitoring.video_recorder import VideoRecorder\nbefore_training = \"before_training.mp4\"\n\nvideo = VideoRecorder(env, before_training)\n# returns an initial observation\nenv.reset()\nfor i in range(200):\n  env.render()\n  video.capture_frame()\n  # env.action_space.sample() produces either 0 (left) or 1 (right).\n  observation, reward, done, info = env.step(env.action_space.sample())\n  # Not printing this time\n  #print(\"step\", i, observation, reward, done, info)\n\nvideo.close()\nenv.close()\n```", "```py\nfrom base64 import b64encode\ndef render_mp4(videopath: str) -> str:\n  \"\"\"\n  Gets a string containing a b4-encoded version of the MP4 video\n  at the specified path.\n  \"\"\"\n  mp4 = open(videopath, 'rb').read()\n  base64_encoded_mp4 = b64encode(mp4).decode()\n  return f'<video width=400 controls><source src=\"data:video/mp4;' \\\n         f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'\n```", "```py\nfrom IPython.display import HTML\nhtml = render_mp4(before_training)\nHTML(html)\n```", "```py\n!pip install 'ray[rllib]'==1.6\n```", "```py\nimport ray\nfrom ray.rllib.agents.ppo import PPOTrainer\nconfig = {\n    \"env\": \"CartPole-v0\",\n    # Change the following line to `“framework”: “tf”` to use tensorflow\n    \"framework\": \"torch\",\n    \"model\": {\n      \"fcnet_hiddens\": [32],\n      \"fcnet_activation\": \"linear\",\n    },\n}\n stop = {\"episode_reward_mean\": 195}\n ray.shutdown()\nray.init(\n  num_cpus=3,\n  include_dashboard=False,\n  ignore_reinit_error=True,\n  log_to_driver=False,\n)\n# execute training \nanalysis = ray.tune.run(\n  \"PPO\",\n  config=config,\n  stop=stop,\n  checkpoint_at_end=True,\n)\n```", "```py\n# restore a trainer from the last checkpoint\ntrial = analysis.get_best_logdir(\"episode_reward_mean\", \"max\")\ncheckpoint = analysis.get_best_checkpoint(\n  trial,\n  \"training_iteration\",\n  \"max\",\n)\ntrainer = PPOTrainer(config=config)\ntrainer.restore(checkpoint)\n```", "```py\nafter_training = \"after_training.mp4\"\nafter_video = VideoRecorder(env, after_training)\nobservation = env.reset()\ndone = False\nwhile not done:\n  env.render()\n  after_video.capture_frame()\n  action = trainer.compute_action(observation)\n  observation, reward, done, info = env.step(action)\nafter_video.close()\nenv.close()\n# You should get a video similar to the one below. \nhtml = render_mp4(after_training)\nHTML(html)\n```", "```py\nparameter_search_config = {\n    \"env\": \"CartPole-v0\",\n    \"framework\": \"torch\",\n\n    # Hyperparameter tuning\n    \"model\": {\n      \"fcnet_hiddens\": ray.tune.grid_search([[32], [64]]),\n      \"fcnet_activation\": ray.tune.grid_search([\"linear\", \"relu\"]),\n    },\n    \"lr\": ray.tune.uniform(1e-7, 1e-2)\n}\n\n# To explicitly stop or restart Ray, use the shutdown API.\nray.shutdown()\n\nray.init(\n  num_cpus=12,\n  include_dashboard=False,\n  ignore_reinit_error=True,\n  log_to_driver=False,\n)\n\nparameter_search_analysis = ray.tune.run(\n  \"PPO\",\n  config=parameter_search_config,\n  stop=stop,\n  num_samples=5,\n  metric=\"timesteps_total\",\n  mode=\"min\",\n)\n\nprint(\n  \"Best hyperparameters found:\",\n  parameter_search_analysis.best_config,\n)\n```"]