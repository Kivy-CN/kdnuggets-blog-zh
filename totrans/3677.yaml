- en: 'ChatGLM-6B: A Lightweight, Open-Source ChatGPT Alternative'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/04/chatglm6b-lightweight-opensource-chatgpt-alternative.html](https://www.kdnuggets.com/2023/04/chatglm6b-lightweight-opensource-chatgpt-alternative.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![ChatGLM-6B: A Lightweight, Open-Source ChatGPT Alternative](../Images/9dfdd021af6594d2e14b5f24f29d0ddb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Recently we’ve all been having a super hard time catching up on the latest releases
    in the LLM space. In the last few weeks, several open-source ChatGPT alternatives
    have become popular.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: And in this article we’ll learn about the **ChatGLM** series and **ChatGLM-6B**,
    an open-source and lightweight ChatGPT alternative.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get going!
  prefs: []
  type: TYPE_NORMAL
- en: What is ChatGLM?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Researchers at the Tsinghua University in China have worked on developing the
    ChatGLM series of models that have comparable performance to other models such
    as GPT-3 and BLOOM.
  prefs: []
  type: TYPE_NORMAL
- en: 'ChatGLM is a bilingual large language model trained on both Chinese and English.
    Currently, the following models are available:'
  prefs: []
  type: TYPE_NORMAL
- en: 'ChatGLM-130B: an open-source LLM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ChatGLM-100B: not open-sourced, but available through invite-only access'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ChatGLM-6B: a lightweight open-source alternative'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Though these models may seem similar to the Generative Pretrained Transformer
    (GPT) group of large language models, the **General Language Model (GLM) pretraining
    framework** is what makes them different. We’ll learn more about this in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: How Does ChatGLM Work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, you'd know GLMs as *generalized linear models*, but the
    GLM in ChatGLM stands for *General Language Model*.
  prefs: []
  type: TYPE_NORMAL
- en: GLM Pretraining Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLM pre training has been extensively studied and is still an area of active
    research. Let’s try to understand the key differences between GLM pretraining
    and GPT-style models.
  prefs: []
  type: TYPE_NORMAL
- en: The GPT-3 family of models use decoder-only auto regressive language modeling.
    In GLM, on the other hand, optimization of the objective is formulated as an **auto
    regressive blank infilling problem**.
  prefs: []
  type: TYPE_NORMAL
- en: '![ChatGLM-6B: A Lightweight, Open-Source ChatGPT Alternative](../Images/23df7f00c157d578290e442e5cae1240.png)'
  prefs: []
  type: TYPE_IMG
- en: GLM | [Image Source](https://arxiv.org/abs/2103.10360)
  prefs: []
  type: TYPE_NORMAL
- en: In simple terms, *auto regressive blank infilling* involves blanking out a continuous
    span of text, and then sequentially reconstructing the text this blanking. In
    addition to shorter masks, there is a longer mask that randomly removes long blanks
    of text from the end of sentences. This is done so that the model performs reasonably
    well in natural language understanding as well as generation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Another difference is in the type of attention used. The GPT group of large
    language models use unidirectional attention, whereas the GLM group of LLMs use
    **bidirectional attention**. Using bidirectional attention over unmasked contexts
    can capture dependencies better and can improve performance on natural language
    understanding tasks.
  prefs: []
  type: TYPE_NORMAL
- en: GELU Activation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In GLM, GELU (Gaussian Error Linear Units) activation is used instead of the
    ReLU activation [1].
  prefs: []
  type: TYPE_NORMAL
- en: '![ChatGLM-6B: A Lightweight, Open-Source ChatGPT Alternative](../Images/9f6d2d8afebf440a8d9f0363b8a4db6d.png)'
  prefs: []
  type: TYPE_IMG
- en: GELU, ReLU, and ELU Activations | [Image Source](https://arxiv.org/abs/1606.08415)
  prefs: []
  type: TYPE_NORMAL
- en: 'The GELU activation and has non-zero values for all inputs and has the following
    form [3]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ChatGLM-6B: A Lightweight, Open-Source ChatGPT Alternative](../Images/c42f168b0c87a3a5c1a921182a2a019d.png)'
  prefs: []
  type: TYPE_IMG
- en: The GELU activation is found to improve performance in as compared to ReLU activations,
    though computationally more intensive than ReLU.
  prefs: []
  type: TYPE_NORMAL
- en: In the GLM series of LLMs, ChatGLM-130B which is open-source and performs as
    well as GPT-3’s Da-Vinci model. As mentioned, as of writing this article, there's
    a ChatGLM-100B version, which is restricted to invite-only access.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGLM-6B
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following details about ChatGLM-6B to make it more accessible to end users:'
  prefs: []
  type: TYPE_NORMAL
- en: Has about 6.2 billion parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is pre-trained on 1 trillion tokens—equally from English and Chinese.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subsequently, techniques such as supervised fine-tuning and reinforcement learning
    with human feedback are used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages and Limitations of ChatGLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s wrap up our discussion by going over ChatGLM’s advantages and limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: Advantages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From being a bilingual model to an open-source model that you can run locally,
    ChatGLM-6B has the following advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Most mainstream large language models are trained on large corpora of English
    text, and large language models for other languages are not as common. The ChatGLM
    series of LLMs are bilingual and a great choice for Chinese. The model has good
    performance in both English and Chinese.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ChatGLM-6B is optimized for user devices. End users often have limited computing
    resources on their devices, so it becomes almost impossible to run LLMs locally—without
    access to high-performance GPUs. With [INT4 quantization](https://developer.nvidia.com/blog/int4-for-ai-inference/),
    ChatGLM-6B can run with a modest memory requirement of as low as 6GB.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs well on a variety of tasks including summarization and single and multi-query
    chats.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite the substantially smaller number of parameters as compared to other
    mainstream LLMs, ChatGLM-6B supports context length of up to 2048.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, let’s list a few limitations of ChatGLM-6B:'
  prefs: []
  type: TYPE_NORMAL
- en: Though ChatGLM is a bilingual model, its performance in English is likely suboptimal.
    This can be attributed to the instructions used in training mostly being in Chinese.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because ChatGLM-6B has substantially *fewer parameters* as compared to other
    LLMs such as BLOOM, GPT-3, and ChatGLM-130B, the performance may be worse when
    the context is too long. As a result, ChatGLM-6B may give inaccurate information
    more often than models with a larger number of parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Small language models have *limited memory capacity*. Therefore, in multi-turn
    chats, the performance of the model may degrade slightly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias, misinformation, and toxicity are limitations of all LLMs, and ChatGLM
    is susceptible to these, too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a next step, run ChatGLM-6B locally or try out the demo on HuggingFace spaces.
    If you’d like to delve deeper into the working of LLMs, here's a list of [free
    courses on large language models](/2023/03/top-free-courses-large-language-models.html).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Z Du, Y Qian et al., [GLM: General Language Model Pretraining with Autoregressive
    Blank Infilling](https://arxiv.org/abs/2103.10360), ACL 2022'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] A Zheng, X Liu et al., [GLM-130B - An Open Bilingual Pretrained Model](https://arxiv.org/abs/2210.02414),
    ICML 2023'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] D Hendryks, K Gimpel, [Gaussian Error Linear Units (GELUs)](https://arxiv.org/abs/1606.08415),
    arXiv, 2016'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [ChatGLM-6B: Demo on HuggingFace Spaces](https://huggingface.co/spaces/multimodalart/ChatGLM-6B)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [GitHub Repo](https://github.com/THUDM/ChatGLM-6B/blob/main/README_en.md)'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Bala Priya C](https://www.linkedin.com/in/bala-priya/)** is a technical
    writer who enjoys creating long-form content. Her areas of interest include math,
    programming, and data science. She shares her learning with the developer community
    by authoring tutorials, how-to guides, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[MiniGPT-4: A Lightweight Alternative to GPT-4 for Enhanced…](https://www.kdnuggets.com/2023/04/minigpt4-lightweight-alternative-gpt4-enhanced-visionlanguage-understanding.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenChatKit: Open-Source ChatGPT Alternative](https://www.kdnuggets.com/2023/03/openchatkit-opensource-chatgpt-alternative.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8 Open-Source Alternative to ChatGPT and Bard](https://www.kdnuggets.com/2023/04/8-opensource-alternative-chatgpt-bard.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Dolly 2.0: ChatGPT Open Source Alternative for Commercial Use](https://www.kdnuggets.com/2023/04/dolly-20-chatgpt-open-source-alternative-commercial.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Alternative Feature Selection Methods in Machine Learning](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[HuggingChat Python API: Your No-Cost Alternative](https://www.kdnuggets.com/2023/05/huggingchat-python-api-alternative.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
