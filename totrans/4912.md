# PySpark SQL 备忘单：Python 中的大数据

> 原文：[https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html](https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html)

**作者：Karlijn Willems，来自 [DataCamp](https://www.datacamp.com/)。**

### 使用 Python 处理大数据

大数据无处不在，传统上由三个 V 特征来描述：速度、种类和体量。大数据是快速的、多样的且体量庞大。作为数据科学家、数据工程师、数据架构师……无论你在数据科学行业中扮演什么角色，你都会早晚接触到大数据，因为公司现在在各个领域收集了大量数据。

* * *

## 我们的三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全领域的职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的 IT

* * *

然而，数据并不总是意味着信息，这正是你，数据科学爱好者，能够发挥作用的地方。你可以利用 Apache Spark，一个“快速且通用的大规模数据处理引擎”，来开始应对大数据给你和你所在公司带来的挑战。

尽管如此，在使用 Spark 时疑问总会出现，当你遇到这些问题时，查看一下 DataCamp 的 [Apache Spark 教程：使用 PySpark 进行机器学习](https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning) 或下载 [备忘单](https://www.datacamp.com/community/blog/pyspark-sql-cheat-sheet) 是一个好主意！

接下来，我们将深入探讨备忘单的结构和内容。

### PySpark 备忘单

PySpark 是 Spark 的 Python API，将 Spark 编程模型暴露给 Python。Spark SQL 是 PySpark 的一个模块，允许你以 DataFrames 的形式处理结构化数据。这与通常用于处理非结构化数据的 RDDs 相对立。

![PySpark 备忘单](../Images/f31b5ce6452a08f76098ca13c56b14cb.png)

**提示：** 如果你想了解更多关于 RDD 和 DataFrames 之间的区别，以及 Spark DataFrames 如何与 pandas DataFrames 区别，你一定要查看一下 [Apache Spark in Python: Beginner's Guide](https://www.datacamp.com/community/tutorials/apache-spark-python)。

### 初始化 SparkSession

如果你想开始使用PySpark的Spark SQL，你需要先启动一个SparkSession：你可以用它来创建DataFrames，注册DataFrames为表，执行SQL查询以及读取parquet文件。如果这些听起来对你来说很陌生，不用担心 - 你将在本文后面阅读更多相关内容！

你通过首先从pyspark包附带的sql模块中导入SparkSession来开始一个SparkSession。接下来，你可以初始化一个变量spark，例如，不仅构建SparkSession，还可以为应用程序命名，设置配置，然后使用getOrCreate()方法来获取一个SparkSession（如果已经有一个正在运行），或者创建一个新的SparkSession（如果还没有的话）！这个方法将非常有用，特别是为了将来参考，因为它可以防止你同时运行多个SparkSessions！

很酷，不是吗？

![PySpark备忘单](../Images/3c86b85673b094ab3ed9a4afa5142260.png)

### 检查数据

当你导入数据后，是时候通过使用一些内置的属性和方法来检查Spark DataFrame了。这样，你可以在开始操作DataFrames之前，更好地了解你的数据。

现在，你可能已经了解了备忘单中提到的大多数方法和属性，这些都是你在使用pandas DataFrames或NumPy时熟悉的，比如dtypes、head()、describe()、count()等。还有一些方法可能对你来说是新的，例如take()或printSchema()方法，或者schema属性。

![PySpark备忘单](../Images/2b87b5681e4fc864b638d73298521c50.png)

尽管如此，你会发现上手过程相当平稳，因为你可以最大限度地利用你之前对数据科学包的知识。

### 重复值

当你检查数据时，你可能会发现存在一些重复值。为了解决这个问题，你可以使用dropDuplicates()方法，例如，来删除Spark DataFrame中的重复值。

### 查询

你可能还记得，使用Spark DataFrames的主要原因之一是你可以以更结构化的方式处理数据 - 查询就是这种更结构化的数据处理方式之一，无论你是使用关系数据库中的SQL，还是No-SQL数据库中的类似SQL的语言，亦或是Pandas DataFrames上的[query()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html?highlight=query#pandas.DataFrame.query)方法，等等。

![PySpark备忘单](../Images/8a57af6b8937cff2c56c8dfbf857efe5.png)

现在我们在谈论 Pandas DataFrames 时，你会发现 Spark DataFrames 遵循类似的原则：你使用方法来更好地了解你的数据。然而，在这种情况下，你不会使用 `query()`。相反，你需要利用其他方法来获得你想要的结果：在第一次使用时，`select()` 和 `show()` 将是你检索 DataFrames 信息时的好帮手。

在这些方法中，你可以构建你的查询。与标准 SQL 一样，你可以在 `select()` 中指定你确切想要获取的列。你可以使用普通字符串或通过 DataFrame 本身来指定列名，如以下示例所示：`df.lastName` 或 `df[“firstName”]`。

注意，后一种方法在某些情况下会给你更多的自由，例如当你想通过 `isin()` 或 `startswith()` 等附加函数来指定你确切想要检索的信息时。

除了 `select()` 之外，你还可以利用 PySpark SQL 中的 functions 模块来指定，例如在查询中使用 `when` 子句，如下表所示：

![PySpark 速查表](../Images/d5234f75f8ad26d04f778d4b0bbf5d52.png)

总的来说，你可以看到，有很多方法可以通过 `select()`、`help()` 和 functions 模块的功能来更大程度地处理和了解数据。

### 添加、更新和删除列

你可能还想了解如何添加、更新或删除 Spark DataFrame 中的一些列。你可以使用 `withColumn()`、`withColumnRenamed()` 和 `drop()` 方法轻松完成此操作。你可能现在已经知道，在使用 Pandas DataFrames 时也可以使用 `drop()` 方法。你可以在 [这里](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html?highlight=drop#pandas.DataFrame.drop) 了解更多。

![PySpark 速查表](../Images/751902d5d047e7357ce04767eb6020aa.png)

### 分组

同样，就像在 Pandas DataFrames 中一样，你可能会想按某些值进行分组并聚合一些值 - 下面的示例展示了如何使用 `groupBy()` 方法，并结合 `count()` 和 `show()` 来检索和显示按年龄分组的数据，以及具有特定年龄的人数。

![PySpark 速查表](../Images/283cb11c84f4170ae04c5534ce466532.png)

### 更多相关内容

+   [PySpark 数据科学](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)

+   [使用 Pandera 进行 PySpark 应用程序的数据验证](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)

+   [使用 Python 进行数据清洗速查表](https://www.kdnuggets.com/2023/02/data-cleaning-python-cheat-sheet.html)

+   [用于构建生成 AI 应用程序的最佳 Python 工具速查表](https://www.kdnuggets.com/2023/08/best-python-tools-generative-ai-cheat-sheet.html)

+   [Python 控制流速查表](https://www.kdnuggets.com/2022/11/python-control-flow-cheatsheet.html)

+   [KDnuggets 新闻，7月5日：一个糟糕的数据科学项目 • 10 人工智能…](https://www.kdnuggets.com/2023/n24.html)
