- en: Learn to build an end to end data science project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/11/build-data-science-project.html](https://www.kdnuggets.com/2020/11/build-data-science-project.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Mathang Peddi](https://twitter.com/mathang_peddi), Data Science and Machine
    Learning Enthusiast**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/60d41c87f95db9168c12644c58b73b90.png)'
  prefs: []
  type: TYPE_IMG
- en: '* * *'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_BQ
  - PREF_H2
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '* * *'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*A Data Scientist is the one who is the best programmer among all the statisticians
    and the best statistician among all the programmers.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Every Data Scientist needs an efficient strategy to solve data science problems.
  prefs: []
  type: TYPE_NORMAL
- en: Data Science positions are unique across the country so we can try and predict
    the salary of data science positions based on Job Title, Company, and Geography,
    etc. Here I have built a project where any user can plug in the information, and
    it splits up into a range of salaries, so if anyone is trying to negotiate, then
    this is a pretty cool tool for them to use.
  prefs: []
  type: TYPE_NORMAL
- en: Data Science Workflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/a95db02bd5f88ad6309bf374869606ea.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Business Understanding**'
  prefs: []
  type: TYPE_NORMAL
- en: This stage is significant because it helps clarify the customer’s target. The
    success of any project depends on the quality of the questions asked. If you understand
    the business requirement correctly, then it helps you collect the right data.
    Asking the right questions will help you narrow down the data acquisition part.
  prefs: []
  type: TYPE_NORMAL
- en: '**Analytic Approach**'
  prefs: []
  type: TYPE_NORMAL
- en: This is the stage where, once the business problem has been clearly stated,
    the data scientist can define the analytic approach to solve the problem. This
    step includes explaining the problem in the sense of statistical and machine-learning
    techniques, and it is important as it helps to determine what kind of trends are
    required to solve the issue in the most efficient way possible. If the issue is
    to determine the probabilities of something, then a predictive model might be
    used; if the question is to show relationships, a descriptive approach may be
    required, and if our problem requires counts, then statistical analysis is the
    best way to solve it. For each type of approach, we can use different algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Requirements**'
  prefs: []
  type: TYPE_NORMAL
- en: We find out the necessary data content, formats, and sources for initial data
    collection, and we use this data inside the algorithm of the approach we chose.
  prefs: []
  type: TYPE_NORMAL
- en: '*Data reveals impact, and with data, you can bring more science to your decisions.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Data Collection**'
  prefs: []
  type: TYPE_NORMAL
- en: We identify the available data resources relevant to the problem domain. To
    retrieve the data, we can apply web scraping on a related website, or we can use
    a repository with premade datasets that are ready to use. If you want to collect
    data from any website or repository, use the Pandas library, which is a very useful
    tool to download, convert, and modify datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'So for this purpose, I have tweaked the web scraper to scrape 1000 job postings
    from glassdoor.com. With each job, we get the following: Job title, Salary Estimate,
    Job Description, Rating, Company, Location, Company Headquarters, Company Size,
    Company Founded Date, Type of Ownership, Industry, Sector, Revenue, Competitors.
    So these are the various attributes for determining the salary of a person working
    in the Data Science field.'
  prefs: []
  type: TYPE_NORMAL
- en: To check the Web Scraper Article, click [here](https://towardsdatascience.com/selenium-tutorial-scraping-glassdoor-com-in-10-minutes-3d0915c6d905).
  prefs: []
  type: TYPE_NORMAL
- en: To check the Web Scraper Github code, click [here](https://github.com/arapfaik/scraping-glassdoor-selenium).
  prefs: []
  type: TYPE_NORMAL
- en: '*You can have data without information, but you cannot have information without
    data.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Data Understanding**'
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists try to understand more about the data collected before. We have
    to check the type of each data and have to learn more about the attributes and
    their names.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95848d8879964505bfb3fb2f98a2a688.png)'
  prefs: []
  type: TYPE_IMG
- en: Few salaries contain -1, so those values are not of much importance to us so
    let’s remove them. As the salary estimate column is in a string right now, so
    we need to give -1 in the string format.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/049d6e06238ed69d2e53d37bd8934331.png)'
  prefs: []
  type: TYPE_IMG
- en: So now we can see that the number of rows has come down to 742\. We observe
    that most of our variables are categorical and not numerical. This dataset comprises
    2 numerical and 12 categorical variables. But in reality, our dependent variable,
    Salary Estimate, has to be numerical. So we need to convert that into a numerical
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Preparation**'
  prefs: []
  type: TYPE_NORMAL
- en: Data can be in any format. To analyze it, you need to have data in a certain
    format. Data scientists have to prepare data for modeling, which is one of the
    most crucial steps because the model has to be clean and should not contain any
    errors or null values.
  prefs: []
  type: TYPE_NORMAL
- en: '*In real-world scenarios, data scientists spend 80% of their time cleaning
    the data and only spend 20% of their time giving insights and conclusions.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is a pretty messy process, so that’s something you should be prepared for.
  prefs: []
  type: TYPE_NORMAL
- en: After scraping the data, I needed to clean it up so that it was usable for our
    model. I made a few changes and created new variables.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f41ac081e8f4cb8a80d102be011e5c63.png)'
  prefs: []
  type: TYPE_IMG
- en: When we split on the left parenthesis, what happens is, the left and right sides
    of ‘(‘ of all the rows go into 2 different lists. That’s why we need to include
    [0] to get the salaries. After obtaining the salaries, replace ‘K’,’$’ with an
    empty string. In a few entries, the salary is given as ‘employer provided’ and
    ‘per hour’, so these are inconsistent and should be looked after.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/90a4cd64b49bfcae449f6851023091af.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we shouldn’t have employer provided or per hour in salaries. So we return
    2 lists which contain the minimum and maximum salaries of each entry. This becomes
    our final dependent variable(to predict the average salary of a person)
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploratory Data Analysis**'
  prefs: []
  type: TYPE_NORMAL
- en: EDA plays a very important role at this stage as the summarization of clean
    data helps in identifying the structure, outliers, anomalies, and patterns in
    data. These insights could help us in building the model. However, I am going
    to discuss EDA in detail in a separate article, and you can find it in my medium
    profile.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Building**'
  prefs: []
  type: TYPE_NORMAL
- en: The data scientist has the chance to understand if his work is ready to go or
    if it needs review. Modeling focuses on developing models that are either descriptive
    or predictive. So here, we perform Predictive modeling, which is a process that
    uses data mining and probability to forecast outcomes. For predictive modeling,
    data scientists use a training set that is a set of historical data in which the
    outcomes are already known. This step can be repeated more times until the model
    understands the question and answer to it.
  prefs: []
  type: TYPE_NORMAL
- en: If we have categorical data, then we need to create dummy variables, so that's
    why I transformed the categorical variables into dummy variables. I also split
    the data into train and test sets with a test size of 20%. I tried three different
    models and evaluated them using Mean Absolute Error. I chose MAE because it is
    relatively easy to interpret, and outliers aren’t particularly bad for this type
    of model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72459f03ca6713721ffc2d56463514e1.png)'
  prefs: []
  type: TYPE_IMG
- en: After this conversion, the number of columns in our dataset has increased from
    14 to 178!!
  prefs: []
  type: TYPE_NORMAL
- en: 'I have implemented three different models:'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Linear Regression — Baseline for the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lasso Regression — Because of the sparse data from the many categorical variables,
    I thought a normalized regression like lasso would be effective.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random Forest — Again, with the sparsity associated with the data, I thought
    this would be a good fit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/7c50ef3091e8a0ad62ab0bb81c47a3f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Now when it starts, it’s a little worse. So we try to find the optimal value
    of alpha for which the error is the least.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97c2f6817ec2ae0b23d7e6cd327b47e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Here I have chosen i/10 as well, but the error was still high, so that’s why
    I have reduced the values of alpha.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/395d15057e8d4782d7d2820041246692.png)'
  prefs: []
  type: TYPE_IMG
- en: After plotting the graph and checking the value of alpha, we see that an alpha
    value of 0.13 gives the best error term. Now our error has reduced from 21.09
    to 19.25 (which means 19.25K dollars). We can also improve the model tuning the
    GridSearch.
  prefs: []
  type: TYPE_NORMAL
- en: GridSearch is the process of performing hyperparameter tuning in order to determine
    the optimal values for a given model. GridSearchCV is basically like you put in
    all the parameters which you want, and then it runs all the models and splits
    the ones with the best results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/50f8eede39c41a70fce07519c3ca784e.png)'
  prefs: []
  type: TYPE_IMG
- en: We can even use Support Vector Regression, XGBoost, or any other models.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest Regression is a tree-based decision process, and also, there are
    many 0s, 1s in our dataset, so we expect it to be a better model. So that’s why
    I have preferred Random Forest Regression here.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c39324a94fd3b6dc42f895550ff2772b.png)'
  prefs: []
  type: TYPE_IMG
- en: So here we are getting a smaller value of error than the previous ones, so the
    Random Forest model is better than the previous models. I have combined the Random
    Forest model with the Linear Regression model to make a prediction. So I have
    taken the average of both, which means that I have given 50% weightage to each
    of the models.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time, it’s better to combine different models and then make predictions
    because there are very good chances of increasing our accuracy. These types of
    models are called ensemble models, and they are widely used. The error may or
    may not increase because one model might be overtraining.
  prefs: []
  type: TYPE_NORMAL
- en: The tuned Random Forest model is the best here because it has the least error
    when compared to Lasso and Linear regression. So instead of taking the average
    of both, we can even merge 90% of the random forest model with 10% of any other
    models and test the accuracy/performance. Generally, these types of ensemble models
    are better for classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: The project should not be about trying all the models, but it should be to choose
    the most effective models and should be able to tell a story as to why we have
    chosen those specific ones. Usually, Lasso regression should have more effect
    than linear regression as it has the normalization effect, and we have a sparse
    matrix, but here the Lasso performed worse than the linear regression. Hence it
    depends model to model, and we cannot generalize anything.
  prefs: []
  type: TYPE_NORMAL
- en: Model Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data scientists can evaluate the model in two ways: Hold-Out and Cross-Validation.
    In the Hold-Out method, the dataset is divided into three subsets: a training
    set, a validation set that is a subset that is used to assess the performance
    of the model built in the training phase, and a test set is a subset to test the
    likely future performance of a model. In most of the cases, the training:validation:test
    set ratios will be 3:1:1, which means 60% of the data to the training set, 20%
    of the data to the validation set, and 20% of the data to the test set.'
  prefs: []
  type: TYPE_NORMAL
- en: Model Deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So I have [created a basic webpage](https://glassdoorsalaryprediction-api.herokuapp.com/)
    so that it’s simple to understand. Given the details of the employee and company,
    this model predicts the expected salary for the employee.
  prefs: []
  type: TYPE_NORMAL
- en: I have deployed my Machine Learning model in Heroku using flask. I have trained
    the model using Linear regression (because it’s easy to understand), but you can
    always train your model using any other Machine Learning model, or you can even
    use ensemble models as they provide good accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/604e2b4e6f1b47e4e3e75706c89b897d.png)'
  prefs: []
  type: TYPE_IMG
- en: After deploying the model, I have made an attempt to predict the salary of a
    machine learning engineer where the company’s rating is 4, and the company was
    founded 39 years ago. So, according to my model, the Employee’s Expected salary
    is 117.31K dollars.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I have not discussed everything in detail. But you can always
    refer to my [GitHub Repository](https://github.com/mathangpeddi/Glassdoor-Job-Salaries) for
    the whole project. My conclusion from this article is that you don’t expect a
    perfect model, but expect something you can use in your own company/project today!
  prefs: []
  type: TYPE_NORMAL
- en: A huge shout out to [Ken Jee](https://www.youtube.com/channel/UCiT9RITQ9PW6BhXK0y2jaeg) for
    his amazing contributions and projects on Data Science.
  prefs: []
  type: TYPE_NORMAL
- en: '*“The goal is to turn data into information, and information into insight.”–Carly
    Fiorina*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/dev-genius/learn-to-build-an-end-to-end-data-science-project-c9f79692191).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[A step-by-step guide for creating an authentic data science portfolio project](https://www.kdnuggets.com/2020/10/guide-authentic-data-science-portfolio-project.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8 AI/Machine Learning Projects To Make Your Portfolio Stand Out](https://www.kdnuggets.com/2020/09/8-ml-ai-projects-stand-out.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Build a Data Science Portfolio](https://www.kdnuggets.com/2018/07/build-data-science-portfolio.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Simple to Implement End-to-End Project with HuggingFace](https://www.kdnuggets.com/a-simple-to-implement-end-to-end-project-with-huggingface)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
