- en: Centroid Initialization Methods for k-means Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-means聚类的质心初始化方法
- en: 原文：[https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)
- en: Simple clustering methods such as *k*-means may not be as sexy as contemporary
    neural networks or other recent advanced non-linear classifiers, but they certainly
    have their utility, and knowing how to correctly approach an unsupervised learning
    problem is a great skill to have at your disposal.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的聚类方法，如 *k*-均值，可能没有现代神经网络或其他近期先进的非线性分类器那样引人注目，但它们确实有其用处，掌握如何正确处理无监督学习问题是一项很有价值的技能。
- en: 'This is intended to be the first in a series of articles looking at the different
    aspects of a *k*-means clustering pipeline. In this first article we will discuss
    centroid initialization: what it is, what it accomplishes, and some of the different
    approaches that exist. We will assume familiarity with machine learning, Python
    programming, and the general idea of clustering.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章旨在系列文章中的第一篇，探讨 *k*-均值聚类管道的不同方面。在这第一篇文章中，我们将讨论质心初始化：它是什么，它实现了什么，以及存在的一些不同方法。我们将假设你对机器学习、Python编程以及聚类的一般概念有一定的了解。
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT需求'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*k*-means Clustering'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*k*-均值聚类'
- en: k-means is a simple, yet often effective, approach to clustering. Traditionally,
    *k* data points from a given dataset are randomly chosen as cluster centers, or
    centroids, and all training instances are plotted and added to the closest cluster.
    After all instances have been added to clusters, the centroids, representing the
    mean of the instances of each cluster are re-calculated, with these re-calculated
    centroids becoming the new centers of their respective clusters.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-均值是一种简单而经常有效的聚类方法。传统上，从给定的数据集中随机选择 *k* 个数据点作为聚类中心或质心，所有训练实例都会被绘制并添加到最近的聚类中。在所有实例都添加到聚类后，质心表示每个聚类实例的均值，将被重新计算，这些重新计算的质心成为各自聚类的新中心。'
- en: At this point, all cluster membership is reset, and all instances of the training
    set are re-plotted and re-added to their closest, possibly re-centered, cluster.
    This iterative process continues until there is no change to the centroids or
    their membership, and the clusters are considered settled.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，所有的聚类成员资格都被重置，所有训练集的实例被重新绘制并重新添加到其最近的、可能重新中心化的聚类中。这个迭代过程将继续，直到质心或其成员资格没有变化，聚类被认为已稳定。
- en: 'Convergence is achieved once the re-calculated centroids match the previous
    iteration’s centroids, or are within some preset margin. The measure of distance
    is generally Euclidean in *k*-means, which, given 2 points in the form of (x,
    y), can be represented as:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当重新计算的质心与前一轮迭代的质心匹配，或在某个预设的范围内时，聚合被认为已经实现。在 *k*-均值中，距离的度量通常是欧几里得距离，给定两个点的形式为
    (x, y)，可以表示为：
- en: '![Figure](../Images/6caecd241af5ba356579714b862cb7ae.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/6caecd241af5ba356579714b862cb7ae.png)'
- en: Of technical note, especially in the era of parallel computing, iterative clustering
    in *k*-means is serial in nature; however, the distance calculations within an
    iteration need not be. Therefore, for sets of a significant size, distance calculations
    are a worthy target for parallelization in the *k*-means clustering algorithm.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 技术上需要注意的是，特别是在并行计算时代，*k*-均值中的迭代聚类本质上是串行的；然而，迭代中的距离计算不必如此。因此，对于规模较大的数据集，距离计算是
    *k*-均值聚类算法中值得进行并行化的目标。
- en: Centroid Initialization Methods
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 质心初始化方法
- en: As *k*-means clustering aims to converge on an optimal set of cluster centers
    (centroids) and cluster membership based on distance from these centroids via
    successive iterations, it is intuitive that the more optimal the positioning of
    these initial centroids, the fewer iterations of the *k*-means clustering algorithms
    will be required for convergence. This suggests that some strategic consideration
    to the initialization of these initial centroids could prove useful.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *k*-均值聚类旨在通过连续迭代基于这些中心点的距离来收敛到一个最佳的簇中心（质心）和簇成员，因此直观上来说，初始质心的定位越优化，*k*-均值聚类算法收敛所需的迭代次数就越少。这表明对这些初始质心的初始化进行一些战略性考虑可能会很有用。
- en: 'What methods of centroid initialization exist? While there are a number of
    initialization strategies, let''s focus on the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 质心初始化方法有哪些？虽然存在许多初始化策略，但我们重点关注以下几种：
- en: '**random data points**: In this approach, described in the "traditional" case
    above, *k* random data points are selected from the dataset and used as the initial
    centroids, an approach which is obviously highly volatile and provides for a scenario
    where the selected centroids are not well positioned throughout the entire data
    space.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机数据点**：在这种方法中，如上文“传统”案例所述，从数据集中随机选择 *k* 个数据点作为初始质心，这种方法显然高度不稳定，并提供了一种选定的质心在整个数据空间中没有很好分布的情况。'
- en: '**k-means++**: As spreading out the initial centroids is thought to be a worthy
    goal, *k*-means++ pursues this by assigning the first centroid to the location
    of a randomly selected data point, and then choosing the subsequent centroids
    from the remaining data points based on a probability proportional to the squared
    distance away from a given point''s nearest existing centroid. The effect is an
    attempt to push the centroids as far from one another as possible, covering as
    much of the occupied data space as they can from initialization. *k*-means++ original
    paper, from 2006, can be read [here](http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**k-means++**：由于将初始质心分散开来被认为是一个值得追求的目标，*k*-means++通过将第一个质心分配给随机选择的数据点位置，然后从剩余的数据点中选择后续质心，依据的是与给定点的最近现有质心的平方距离成正比的概率。这种方法试图将质心尽可能远地推开，覆盖尽可能多的占据的数据空间。*k*-means++
    的原始论文（2006年）可以在[这里](http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf)阅读。'
- en: '**naive sharding**: This lesser known (unknown?) centroid initialization method
    was the subject of some of my own graduate research. It is primarily dependent
    on the calculation of a composite summation value reflecting all of the attribute
    values of an instance. Once this composite value is computed, it is used to sort
    the instances of the dataset. The dataset is then horizontally split into *k*
    pieces, or shards. Finally, the original attributes of each shard are independently
    summed, their mean is computed, and the resultant collection of rows of shard
    attribute mean values becomes the set of centroids to be used for initialization.
    The expectation is that, as a deterministic method, it should perform much quicker
    than stochastic methods, and approximate the spread of initial centroids across
    the data space via the composite summation value. You can read more about it [here](/2017/03/naive-sharding-centroid-initialization-method.html)
    if interested.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**朴素分片**：这种较少被人知晓（或许完全未知？）的质心初始化方法曾是我自己研究生研究的课题。它主要依赖于计算反映实例所有属性值的复合总和值。一旦计算出这个复合值，就用它来对数据集中的实例进行排序。然后，将数据集水平分成
    *k* 份或分片。最后，对每个分片的原始属性进行独立求和，计算其均值，并将这些分片属性均值的行集合起来，作为初始化时使用的质心集合。期望是作为一种确定性方法，它应比随机方法执行得更快，并通过复合总和值近似初始质心在数据空间中的分布。如果感兴趣，可以在[这里](/2017/03/naive-sharding-centroid-initialization-method.html)阅读更多内容。'
- en: 'Variants of any of these are possible: you could randomly select from anywhere
    in the data space, as opposed to just the space which contains existing data points;
    you could attempt to find the most centrally located data point first, as opposed
    to a random selection, and proceed with *k*-means++ from there; you could swap
    the post-summation mean operation for an alternate in naive sharding.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 任何这些方法的变体都是可能的：你可以从数据空间的任何位置随机选择，而不仅仅是包含现有数据点的空间；你可以尝试首先找到位置最中心的数据点，而不是随机选择，然后从那里继续使用
    *k*-means++；你也可以在朴素分片中用另一种操作替代求和均值操作。
- en: Also, a form of [hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)
    (often [Ward's method](https://en.wikipedia.org/wiki/Ward%27s_method)) can be
    used as a method to find the initial cluster centers, which can then be passed
    off to *k*-means for the actual data clustering task. This can be effective, but
    since it would mean also discussing hierarchical clustering we will leave this
    until a later article.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一种形式的 [层次聚类](https://en.wikipedia.org/wiki/Hierarchical_clustering)（通常是 [Ward
    方法](https://en.wikipedia.org/wiki/Ward%27s_method)）可以用作找到初始簇中心的方法，然后将其传递给 *k*-means
    执行实际的数据聚类任务。这可能是有效的，但由于这也涉及到层次聚类的讨论，我们会将此内容留待后续文章中。
- en: Centroid Initialization and Scikit-learn
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 质心初始化和 Scikit-learn
- en: 'As we will use Scikit-learn to perform our clustering, let''s have a look at
    its [`KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)
    module, where we can see the following written about available centroid initialization
    methods:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用 Scikit-learn 来执行聚类，让我们查看一下其 [`KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)
    模块，其中可以看到关于可用质心初始化方法的说明：
- en: '`**init{‘k-means++’, ‘random’, ndarray, callable}, default=’k-means++’**`'
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`**init{‘k-means++’, ‘random’, ndarray, callable}, default=’k-means++’**`'
- en: ''
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Method for initialization:'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 初始化方法：
- en: ''
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '''`k-means++`'': selects initial cluster centers for k-mean clustering in a
    smart way to speed up convergence. See section Notes in k_init for more details.'
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '''`k-means++`'': 以一种智能的方式选择 k-means 聚类的初始簇中心，从而加快收敛速度。有关更多细节，请参见 k_init 部分的说明。'
- en: ''
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '''`random`'': choose `n_clusters` observations (rows) at random from data for
    the initial centroids.'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '''`random`'': 从数据中随机选择 `n_clusters` 个观测值（行）作为初始质心。'
- en: ''
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If an ndarray is passed, it should be of shape (n_clusters, n_features) and
    gives the initial centers.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果传递 ndarray，它的形状应该是 (n_clusters, n_features)，并给出初始中心。
- en: ''
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If a callable is passed, it should take arguments X, n_clusters and a random
    state and return an initialization.
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果传递一个可调用对象，它应该接受 X、n_clusters 和随机状态作为参数，并返回一个初始化结果。
- en: With this in mind, and since we want to be able to compare and inspect initialized
    centroids — which we are not able to do with Scikit-learn's implementations —
    we will use our own implementations of the 3 methods discussed above, namely random
    centroid initialization, *k*-means++, and naive sharding. We can then use our
    implementations independent of Scikit-learn to create our centroids, and pass
    them in as an ndarray at the time of clustering. We could also use the callable
    option, as opposed to the ndarray option, and integrate the centroid initialization
    into the Scikit-learn *k*-means execution, but that puts us right back to square
    one with not being able to examine and compare these centroids prior to clustering.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，并且由于我们希望能够比较和检查初始化的质心——这是 Scikit-learn 实现无法做到的——我们将使用上述讨论的三种方法的自定义实现，即随机质心初始化、*k*-means++
    和简单分片。然后，我们可以使用这些实现独立于 Scikit-learn 来创建质心，并在聚类时将其作为 ndarray 传入。我们也可以使用可调用选项，而不是
    ndarray 选项，并将质心初始化集成到 Scikit-learn *k*-means 执行中，但这会让我们回到最初的状态，即在聚类之前无法检查和比较这些质心。
- en: With that said, the `centroid_initialization.py` file contains our centroid
    initialization implementations.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，`centroid_initialization.py` 文件包含了我们的质心初始化实现。
- en: With this file placed in the same directory as where I have started Jupyter
    notebook, I can proceed as follows, starting with imports.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 将此文件放置在与我启动 Jupyter notebook 相同的目录中，我可以按以下步骤进行操作，首先是导入。
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Creating Some Data
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一些数据
- en: Clearly we will need some data. We will create a small synthetic set in order
    to have control over clearly delineating our clusters (see Figure 1).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们需要一些数据。我们将创建一个小的合成数据集，以便能够清晰地划分我们的簇（见图 1）。
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Figure](../Images/a8b9a80d2e8bb4a773aa6e7ab3437aa1.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/a8b9a80d2e8bb4a773aa6e7ab3437aa1.png)'
- en: '**Figure 1: Our synthetic dataset**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1：我们的合成数据集**'
- en: Initializing the Centroids
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化质心
- en: Let's initialize some centroids, using the implementation from above.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们初始化一些质心，使用上述实现。
- en: '**Random Initialization**'
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**随机初始化**'
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**k-means++ Initialization**'
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**k-means++ 初始化**'
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Naive Sharding Initialization**'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**简单分片初始化**'
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you can see, the sets of initialized centroids differ from each other.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，初始化的质心集合彼此不同。
- en: Visualizing Centroid Initialization
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 质心初始化的可视化
- en: Let's see how our centroids compare to each other, and to the data points, visually.
    We will call the plotting function below a number of subsequent times for comparison.
    Figure 2 plots the the 3 sets of centroids created above against the data points.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直观地查看质心彼此之间的比较，以及与数据点的比较。我们将调用下面的绘图函数多次进行比较。图 2 绘制了上述创建的 3 组质心与数据点的对比。
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Figure](../Images/be0f4c3346a8e5ee491db6a2d974e84c.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/be0f4c3346a8e5ee491db6a2d974e84c.png)'
- en: '**Figure 2\. Centroids plotted against data points: random (yellow), *k*-means++
    (red), naive sharding (green)**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2\. 绘制的质心与数据点：随机（黄色），*k*-means++（红色），naive sharding（绿色）**'
- en: Of note, the luck of the draw has placed 3 of the randomly initialized centroids
    in the right-most cluster; the *k*-means++ initialized centroids are located one
    in each of the clusters; and the naive sharding centroids ended up spread across
    the data space in a somewhat arcing fashion upward and to the right across the
    data space, skewed toward where the data is heavily clustered.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，随机初始化的质心中有 3 个恰好位于最右侧的簇中；*k*-means++ 初始化的质心每个簇中都有一个；而 naive sharding
    的质心分布在数据空间中，呈略微弯曲的方式向上和向右偏移，偏向数据高度聚集的区域。
- en: Keep in mind that we created a 2 dimensional data space, so the visualization
    of these centroids is accurate; we aren't stripping away a number of dimensions
    and artificially plotting the data in a stripped-down space which may be problematic.
    This means the comparison of the centroids in this case is as accurate as is possible.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们创建了一个二维数据空间，因此这些质心的可视化是准确的；我们没有去掉一些维度并将数据在简化空间中进行人工绘制，这可能会造成问题。这意味着在这种情况下，质心的比较是尽可能准确的。
- en: You may be asking, what of this initialization is affected by random seeding?
    Both random initialization and and *k*-means++ are stochastic methods, and so
    they can be affected by random seeds. If we pass some different seeds to additional
    runs of the initialization algorithms, the results are visualized in Figure 3.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，这种初始化中哪些因素受随机种子的影响？随机初始化和*k*-means++都是随机方法，因此它们可能会受到随机种子的影响。如果我们对初始化算法进行不同的种子测试，结果如图
    3 所示。
- en: '![Figure](../Images/9e4af4908f968588daddfd8d2af0f776.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/9e4af4908f968588daddfd8d2af0f776.png)'
- en: '**Figure 3\. Experimenting with random seeds for centroid initialization, clockwise
    from top left: 123, 249, 127, 13**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 3\. 实验中随机种子对质心初始化的影响，从左上角开始顺时针：123, 249, 127, 13**'
- en: 'Some observations:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一些观察：
- en: random initialization is (fittingly) all over the place
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机初始化的质心（恰如其分）分布非常广泛
- en: k-means++ is relatively consistent within clusters, with the specific data points
    selected as initial centroids varying slightly
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-means++ 在簇内相对一致，初始质心的具体数据点略有变化
- en: naive sharding is deterministic, and so is not affected by seeding
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: naive sharding 是确定性的，因此不受种子的影响
- en: 'This leads us to some questions, which relate directly to the next steps in
    clustering:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了我们的一些问题，这些问题直接关系到聚类的下一步：
- en: What effect does centroid placement have on the resulting clustering task?
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 质心的位置对最终的聚类任务有什么影响？
- en: What effect does the centroid placement have on the speed of the resulting clustering
    task?
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 质心的位置对最终聚类任务的速度有什么影响？
- en: What effect does the centroid placement have on the accuracy of the resulting
    clustering task?
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 质心的位置对最终聚类任务的准确性有何影响？
- en: When would we use certain of these initialization methods over others?
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们何时会选择某些初始化方法而不是其他方法？
- en: If random seems as though it might be such a poor initialization method from
    an optimal placement standpoint, why is it used?
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果从最优放置的角度来看，随机初始化似乎是一个很差的方法，那么为什么还会使用它呢？
- en: Is there any advantage to a deterministic initialization method, like naive
    sharding?
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定性初始化方法（如 naive sharding）是否有优势？
- en: We will start searching out the answers to these next time.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下次将开始寻找这些问题的答案。
- en: '**[Matthew Mayo](https://www.linkedin.com/in/mattmayo13/)** ([**@mattmayo13**](https://twitter.com/mattmayo13))
    is a Data Scientist and the Editor-in-Chief of KDnuggets, the seminal online Data
    Science and Machine Learning resource. His interests lie in natural language processing,
    algorithm design and optimization, unsupervised learning, neural networks, and
    automated approaches to machine learning. Matthew holds a Master''s degree in
    computer science and a graduate diploma in data mining. He can be reached at editor1
    at kdnuggets[dot]com.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**[马修·梅奥](https://www.linkedin.com/in/mattmayo13/)**（[**@mattmayo13**](https://twitter.com/mattmayo13)）是数据科学家及
    KDnuggets 的主编，这是一个重要的在线数据科学和机器学习资源。他的兴趣包括自然语言处理、算法设计与优化、无监督学习、神经网络以及机器学习的自动化方法。马修拥有计算机科学硕士学位和数据挖掘研究生文凭。他可以通过
    editor1 at kdnuggets[dot]com 联系到。'
- en: More On This Topic
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主题更多内容
- en: '[Clustering Unleashed: Understanding K-Means Clustering](https://www.kdnuggets.com/2023/07/clustering-unleashed-understanding-kmeans-clustering.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[释放聚类的潜力：理解 K-Means 聚类](https://www.kdnuggets.com/2023/07/clustering-unleashed-understanding-kmeans-clustering.html)'
- en: '[What is K-Means Clustering and How Does its Algorithm Work?](https://www.kdnuggets.com/2023/05/kmeans-clustering-algorithm-work.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是 K-Means 聚类以及其算法如何工作？](https://www.kdnuggets.com/2023/05/kmeans-clustering-algorithm-work.html)'
- en: '[Hands-On with Unsupervised Learning: K-Means Clustering](https://www.kdnuggets.com/handson-with-unsupervised-learning-kmeans-clustering)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[无监督学习实操：K-Means 聚类](https://www.kdnuggets.com/handson-with-unsupervised-learning-kmeans-clustering)'
- en: '[Introduction to Clustering in Python with PyCaret](https://www.kdnuggets.com/2021/12/introduction-clustering-python-pycaret.html)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 PyCaret 在 Python 中进行聚类的介绍](https://www.kdnuggets.com/2021/12/introduction-clustering-python-pycaret.html)'
- en: '[Choosing the Right Clustering Algorithm for Your Dataset](https://www.kdnuggets.com/2019/10/right-clustering-algorithm.html)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为您的数据集选择合适的聚类算法](https://www.kdnuggets.com/2019/10/right-clustering-algorithm.html)'
- en: '[DBSCAN Clustering Algorithm in Machine Learning](https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中的 DBSCAN 聚类算法](https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html)'
