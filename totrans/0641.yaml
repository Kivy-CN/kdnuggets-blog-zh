- en: Metrics to Use to Evaluate Deep Learning Object Detectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/08/metrics-evaluate-deep-learning-object-detectors.html](https://www.kdnuggets.com/2020/08/metrics-evaluate-deep-learning-object-detectors.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Venkatesh Wadawadagi](https://www.linkedin.com/in/venkateshwadawadagi/),
    Sahaj Software Solutions**'
  prefs: []
  type: TYPE_NORMAL
- en: Different approaches have been employed to solve the growing need for accurate
    object detection models. More recently, with the popularization of the convolutional
    neural networks (CNN) and GPU-accelerated deep-learning frameworks, object- detection
    algorithms started being developed from a new perspective. CNNs such as R-CNN,
    Fast R-CNN, Faster R-CNN, R-FCN, SSD and Yolo have highly increased the performance
    standards on the field.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Once you have trained your first object detector, the next step is to know its
    performance. Sure enough, you can see the model finds all the objects in the pictures
    you feed it. Great! But how do you quantify that? How should we decide which model
    is better?
  prefs: []
  type: TYPE_NORMAL
- en: Since the classification task only evaluates the probability of the class object
    appearing in the image, it is a straightforward task for a classifier to identify
    correct predictions from incorrect ones. However, the object detection task localizes
    the object further with a bounding box associated with its corresponding confidence
    score to report how certain the bounding box of the object class is detected.
  prefs: []
  type: TYPE_NORMAL
- en: 'A detector outcome is commonly composed of a list of bounding boxes, confidence
    levels and classes, as seen in the following Figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/b09e95eada3f708a1a9c9cc5a9e4fe24.png)'
  prefs: []
  type: TYPE_IMG
- en: Object detection metrics serve as a measure to assess how well the model performs
    on an object detection task. It also enables us to compare multiple detection
    systems objectively or compare them to a benchmark. In most competitions, the
    average precision (AP) and its derivations are the metrics adopted to assess the
    detections and thus rank the teams.
  prefs: []
  type: TYPE_NORMAL
- en: 'Understanding the various metric:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**IoU:**'
  prefs: []
  type: TYPE_NORMAL
- en: Guiding principle in all state-of-the-art metrics is the so-called Intersection-over-Union
    (IoU) overlap measure. It is quite literally defined as the intersection over
    union of the detection bounding box and the ground truth bounding box.
  prefs: []
  type: TYPE_NORMAL
- en: Dividing the area of overlap between predicted bounding box and ground truth
    by the area of their union yields the Intersection over Union.
  prefs: []
  type: TYPE_NORMAL
- en: An Intersection over Union score > 0.5 is normally considered a “good” prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/fa365856db56d3f1bf8a868f88a755ed.png)'
  prefs: []
  type: TYPE_IMG
- en: IoU metric determines how many objects were detected correctly and how many
    false positives were generated (will be discussed below).
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positives [TP]**'
  prefs: []
  type: TYPE_NORMAL
- en: Number of detections with IoU>0.5
  prefs: []
  type: TYPE_NORMAL
- en: '**False Positives [FP]**'
  prefs: []
  type: TYPE_NORMAL
- en: Number of detections with IoU<=0.5 or detected more than once
  prefs: []
  type: TYPE_NORMAL
- en: '**False Negatives [FN]**'
  prefs: []
  type: TYPE_NORMAL
- en: Number of objects that not detected or detected with IoU<=0.5
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision**'
  prefs: []
  type: TYPE_NORMAL
- en: Precision measures how accurate your predictions are. i.e. the percentage of
    your predictions that are correct.
  prefs: []
  type: TYPE_NORMAL
- en: Precision = True positive / (True positive + False positive)
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall**'
  prefs: []
  type: TYPE_NORMAL
- en: Recall measures how good you find all the positives.
  prefs: []
  type: TYPE_NORMAL
- en: Recall = True positive / (True positive + False negative)
  prefs: []
  type: TYPE_NORMAL
- en: '**F1 Score**'
  prefs: []
  type: TYPE_NORMAL
- en: F1 score is HM (Harmonic Mean) of precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/a1168b5a7a008541e56040d5330fcbb6.png)'
  prefs: []
  type: TYPE_IMG
- en: '**AP**'
  prefs: []
  type: TYPE_NORMAL
- en: The general definition for the Average Precision(AP) is finding the area under
    the precision-recall curve.
  prefs: []
  type: TYPE_NORMAL
- en: '**mAP**'
  prefs: []
  type: TYPE_NORMAL
- en: The mAP for object detection is the average of the AP calculated for all the
    classes. mAP@0.5 means that it is the mAP calculated at IOU threshold 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/6ea72c06bc64df3665ef8e6fe8daa6fc.png)'
  prefs: []
  type: TYPE_IMG
- en: '**mAP Vs other metric**'
  prefs: []
  type: TYPE_NORMAL
- en: The mAP is a good measure of the sensitivity of the neural network. So good
    mAP indicates a model that's stable and consistent across different confidence
    thresholds. Precision, Recall and F1 score are computed for given confidence threshold.
  prefs: []
  type: TYPE_NORMAL
- en: If 'model A' has better Precision, Recall and F1 score than 'model B' but say
    mAP of 'model B' is better than that of 'model A',  scenario indicates that either
    'model B' has very bad recall at higher confidence thresholds or very bad precision
    at lower confidence thresholds. So higher Precision, Recall and F1 score of 'model
    A' indicate that at that confidence threshold it is better in terms of all the
    3 metrics compared to that of 'model B'.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/df1e7782c72ce4a2d3f1b2ab0bb7360f.png)'
  prefs: []
  type: TYPE_IMG
- en: Which metric is more important ?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general to analyse better performing models, it's advisable to use both validation
    set (data set that is used to tune hyper-parameters) and test set (data set that
    is used to assess the performance of a fully-trained model).
  prefs: []
  type: TYPE_NORMAL
- en: '**On validation set**'
  prefs: []
  type: TYPE_NORMAL
- en: Use mAP to select the best performing model (model that is more stable and consistent)
    out of all the trained weights across iterations/epochs. Use mAP to understand
    whether the model should be trained/tuned further or not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check class level AP values to ensure the model is stable and good across the
    classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As per use-case/application, if you're completely tolerant to FNs and highly
    intolerant to FPs then to train/tune the model accordingly use Precision.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As per use-case/application, if you're completely tolerant to FPs and highly
    intolerant to FNs then to train/tune the model accordingly use Recall.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**On test set**'
  prefs: []
  type: TYPE_NORMAL
- en: If you're neutral towards FPs and FNs, then use F1 score to evaluate the best
    performing model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If FPs are not acceptable to you (without caring much about FNs) then pick the
    model with higher Precision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If FNs are not acceptable to you (without caring much about FPs) then pick the
    model with higher Recall
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you decide metric you should be using, try out multiple confidence thresholds
    (say for example - 0.25, 0.35 and 0.5) for given model to understand for which
    confidence threshold value the metric you selected works in your favour and also
    to understand acceptable trade off ranges (say you want Precision of at least
    80% and some decent Recall). Once confidence threshold is decided, you use it
    across different models to find out the best performing model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**References**'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://arxiv.org/pdf/1711.00164.pdf](https://arxiv.org/pdf/1711.00164.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.researchgate.net/publication/343194514_A_Survey_on_Performance_Metrics_for_Object-Detection_Algorithms/link/5f1b5a5e45851515ef478268](https://www.researchgate.net/publication/343194514_A_Survey_on_Performance_Metrics_for_Object-Detection_Algorithms/link/5f1b5a5e45851515ef478268/download)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/ultralytics/yolov3/issues/898](https://github.com/ultralytics/yolov3/issues/898)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/](https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bio: [Venkatesh Wadawadagi](https://www.linkedin.com/in/venkateshwadawadagi/)**
    is a solution consultant at [Sahaj Software Solutions](https://sahajsoft.com/).
    He helps businesses solve complex problems using AI-powered solutions. He specialises
    in Deep Learning, Computer Vision, Machine Learning, NLP(Natural Language Processing),
    embedded-AI, business intelligence and data analytics.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Model Evaluation Metrics in Machine Learning](/2020/05/model-evaluation-metrics-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Image Recognition and Object Detection in Retail](/2020/02/image-recognition-object-detection-retail.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[More Performance Evaluation Metrics for Classification Problems You Should
    Know](/2020/04/performance-evaluation-metrics-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[A (Much) Better Approach to Evaluate Your Machine Learning Model](https://www.kdnuggets.com/2022/01/much-better-approach-evaluate-machine-learning-model.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Better Way To Evaluate LLMs](https://www.kdnuggets.com/a-better-way-to-evaluate-llms)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Is the Difference Between SQL and Object-Relational Mapping (ORM)?](https://www.kdnuggets.com/2022/02/difference-sql-object-relational-mapping-orm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets™ News 22:n09, Mar 2: Telling a Great Data Story: A…](https://www.kdnuggets.com/2022/n09.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Exploring Unsupervised Learning Metrics](https://www.kdnuggets.com/2023/04/exploring-unsupervised-learning-metrics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning Evaluation Metrics: Theory and Overview](https://www.kdnuggets.com/machine-learning-evaluation-metrics-theory-and-overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
