["```py\n# x.size = (N, T, S, S, S)\n# scalars.size = (N, s)\nbatch_size = x.shape[0]\nS = x.shape[-1]\nT = x.shape[1]\nx1 = x.permute(0, 2, 3, 4, 1).reshape(batch_size, S, S, S * T)\nx2 = x.permute(0, 4, 2, 3, 1).reshape(batch_size, S, S, S * T)\nx3 = x.permute(0, 3, 4, 2, 1).reshape(batch_size, S, S, S * T)\ninput_list = [x1, x2, x3]\nfor i in range(3):\n    temp = self.linears_1[i](scalars).reshape(batch_size, S, S, 1)\n    input_list[i] = torch.cat([input_list[i], temp], dim=-1)\n    input_list[i] = self.linears_2[i](input_list[i])\nx1, x2, x3 = input_list\n```", "```py\ndef _eval_forward(self, e: torch.Tensor):\n    bs = e.shape[0]\n    future_g = (\n        torch.zeros((bs, self.n_samples, self.n_steps)).long().to(e.device)\n    )\n    ps = torch.ones((bs, self.n_samples)).to(e.device)\n    e = e.unsqueeze(1).repeat(1, self.n_samples, 1, 1)\n\n    future_g = future_g.view(-1, self.n_steps)\n    ps = ps.view(-1)\n    e = e.view(-1, e.shape[-2], e.shape[-1])\n    for i in range(self.n_steps):\n        o_s, z_s = self.core(future_g[:, : i + 1], e)\n        future_g[:, i], p_i = sample_from_logits(o_s[:, i])\n        ps *= p_i\n    future_g = future_g.view(bs, self.n_samples, self.n_steps)\n    ps = ps.view(bs, self.n_samples)\n    return (\n        future_g,\n        ps,\n        z_s[:, 0].view(bs, self.n_samples, *z_s.shape[2:]).mean(1),\n    )\n```", "```py\ndef monte_carlo_tree_search(\n    model: torch.nn.Module,\n    state: torch.Tensor,\n    n_sim: int,\n    t_time: int,\n    n_steps: int,\n    game_tree: Dict,\n    state_dict: Dict,\n):\n\"\"\"Runs the monte carlo tree search algorithm.\n\n    Args:\n        model (torch.nn.Module): The model to use for the simulation.\n        state (torch.Tensor): The initial state.\n        n_sim (int): The number of simulations to run.\n        t_time (int): The current time step.\n        n_steps (int): The maximum number of steps to simulate.\n        game_tree (Dict): The game tree.\n        state_dict (Dict): The dictionary containing the states.\n    \"\"\"\n    state_hash = to_hash(extract_present_state(state))\n    if state_hash in state_dict:\n        with torch.no_grad():\n            N_s_a = state_dict[state_hash][3]\n            n_sim -= int(N_s_a.sum())\n            n_sim = max(n_sim, 0)\n\n    for _ in range(n_sim):\n        simulate_game(model, state, t_time, n_steps, game_tree, state_dict)\n    # return next state\n    possible_states_dict, _, repetitions, N_s_a, q_values, _ = state_dict[\n        state_hash\n    ]\n    possible_states = _recompose_possible_states(possible_states_dict)\n    next_state_idx = select_future_state(\n        possible_states, q_values, N_s_a, repetitions, return_idx=True\n    )\n    next_state = possible_states[next_state_idx]\n    return next_state\n```", "```py\n@torch.no_grad()\ndef simulate_game(\n    model,\n    state: torch.Tensor,\n    t_time: int,\n    max_steps: int,\n    game_tree: Dict,\n    states_dict: Dict,\n    horizon: int = 5,\n):\n\"\"\"Simulates a game from a given state.\n\n  Args:\n      model: The model to use for the simulation.\n      state (torch.Tensor): The initial state.\n      t_time (int): The current time step.\n      max_steps (int): The maximum number of steps to simulate.\n      game_tree (Dict): The game tree.\n      states_dict (Dict): The states dictionary.\n      horizon (int): The horizon to use for the simulation.\n  \"\"\"\n  idx = t_time\n  max_steps = min(max_steps, t_time + horizon)\n  state_hash = to_hash(extract_present_state(state))\n  trajectory = []\n  # selection\n  while state_hash in game_tree:\n      (\n          possible_states_dict,\n          old_idx_to_new_idx,\n          repetition_map,\n          N_s_a,\n          q_values,\n          actions,\n      ) = states_dict[state_hash]\n      possible_states = _recompose_possible_states(possible_states_dict)\n      state_idx = select_future_state(\n          possible_states, q_values, N_s_a, repetition_map, return_idx=True\n      )\n      trajectory.append((state_hash, state_idx))  # state_hash, action_idx\n      future_state = extract_present_state(possible_states[state_idx])\n      state = possible_states[state_idx]\n      state_hash = to_hash(future_state)\n      idx += 1\n\n  # expansion\n  if idx <= max_steps:\n      trajectory.append((state_hash, None))\n      if not game_is_finished(extract_present_state(state)):\n          state = state.to(model.device)\n          scalars = get_scalars(state, idx).to(state.device)\n          actions, probs, q_values = model(state, scalars)\n          (\n              possible_states,\n              cloned_idx_to_idx,\n              repetitions,\n              not_dupl_indexes,\n          ) = extract_children_states_from_actions(\n              state,\n              actions,\n          )\n          not_dupl_actions = actions[:, not_dupl_indexes].to(\"cpu\")\n          not_dupl_q_values = torch.zeros(not_dupl_actions.shape[:-1]).to(\n              \"cpu\"\n          )\n          N_s_a = torch.zeros_like(not_dupl_q_values).to(\"cpu\")\n          present_state = extract_present_state(state)\n          states_dict[to_hash(present_state)] = (\n              _reduce_memory_consumption_before_storing(possible_states),\n              cloned_idx_to_idx,\n              repetitions,\n              N_s_a,\n              not_dupl_q_values,\n              not_dupl_actions,\n          )\n          game_tree[to_hash(present_state)] = [\n              to_hash(extract_present_state(fut_state))\n              for fut_state in possible_states\n          ]\n          leaf_q_value = q_values\n  else:\n      leaf_q_value = -int(torch.linalg.matrix_rank(state).sum())\n  # backup\n  backward_pass(trajectory, states_dict, leaf_q_value=leaf_q_value)\n```", "```py\ndef select_future_state(\n    possible_states: List[torch.Tensor],\n    q_values: torch.Tensor,\n    N_s_a: torch.Tensor,\n    repetitions: Dict[int, list],\n    c_1: float = 1.25,\n    c_2: float = 19652,\n    return_idx: bool = False,\n) -> torch.Tensor:\n\"\"\"Select the future state maximizing the upper confidence bound.\"\"\"\n# q_values (1, K, 1)\n    pi = torch.tensor(\n        [\n            len(repetitions[i])\n            for i in range(len(possible_states))\n            if i in repetitions\n        ]\n    ).to(q_values.device)\n    ucb = q_values.reshape(-1) + pi * torch.sqrt(\n        torch.sum(N_s_a) / (1 + N_s_a)\n    ) * (c_1 + torch.log((torch.sum(N_s_a) + c_2 + 1) / c_2))\n    if return_idx:\n        return ucb.argmax()\n    return possible_states[ucb.argmax()]\n```", "```py\ndef backward_pass(trajectory, states_dict, leaf_q_value: torch.Tensor):\n\"\"\"Backward pass of the montecarlo algorithm\"\"\"\nreward = 0\n    for idx, (state, action_idx) in enumerate(reversed(trajectory)):\n        if action_idx is None:  # leaf node\n            reward += leaf_q_value\n        else:\n            (\n                _,\n                old_idx_to_new_idx,\n                _,\n                N_s_a,\n                q_values,\n                _,\n            ) = states_dict[state]\n            if isinstance(reward, torch.Tensor):\n                reward = reward.to(q_values.device)\n            action_idx = int(action_idx)\n            if action_idx in old_idx_to_new_idx:\n                not_dupl_index = old_idx_to_new_idx[int(action_idx)]\n            else:\n                not_dupl_index = action_idx\n            reward -= 1\n            q_values[:, not_dupl_index] = (\n                N_s_a[:, not_dupl_index] * q_values[:, not_dupl_index] + reward\n            ) / (N_s_a[:, not_dupl_index] + 1)\n            N_s_a[:, not_dupl_index] += 1\n```", "```py\ndef compute_improved_policy(\n    state_dict: Dict,\n    states: List[str],\n    model_n_steps: int,\n    model_n_logits: int,\n    N_bar: int,\n):\n    \"\"\"Compute the improved policy given the state_dict, the list of states.\n    The improved policy is computed as (N_s_a / N_s_a.sum())^(1/tau) where tau\n    is (log(N_s_a.sum()) / log(N_bar)) if N_s_a.sum() > N_bar else 1.\n    \"\"\"\n    policies = torch.zeros(len(states), model_n_steps, model_n_logits)\n    N_bar = torch.tensor(N_bar)\n    for idx, state in enumerate(states):\n        N_s_a = state_dict[state][3]\n        actions = state_dict[state][5]\n        if N_s_a.sum() > N_bar:\n            tau = (torch.log(N_s_a.sum()) / torch.log(N_bar)).item()\n        else:\n            tau = 1\n\t N_s_a = N_s_a ** (1 / tau)\n        improved_policy = N_s_a / N_s_a.sum()\n        for sample_id in range(actions.shape[1]):\n            action_ids = actions[0, sample_id]\n            for step_id, action_id in enumerate(action_ids):\n                policies[idx, step_id, action_id] += improved_policy[\n                    0, sample_id\n                ]\n    return policies\n```"]