- en: An Overview of Topics Extraction in Python with Latent Dirichlet Allocation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/09/overview-topics-extraction-python-latent-dirichlet-allocation.html](https://www.kdnuggets.com/2019/09/overview-topics-extraction-python-latent-dirichlet-allocation.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Félix Revert](https://towardsdatascience.com/@FelixRvrt), DataRobot and
    Velocity**'
  prefs: []
  type: TYPE_NORMAL
- en: A recurring subject in NLP is to understand large corpus of texts through topics
    extraction. Whether you analyze users’ online reviews, products’ descriptions,
    or text entered in search bars, understanding key topics will always come in handy.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/7e62aca1e72dce666a6a4a9b1d01926c.png)Popular picture
    used in literature to explain LDA'
  prefs: []
  type: TYPE_NORMAL
- en: Before going into the LDA method, let me remind you that not reinventing the
    wheel and going for the quick solution is usually the best start. Several providers
    have great API for topic extraction (and it is free up to a certain number of
    calls): [Google](https://cloud.google.com/natural-language/), [Microsoft](https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-entity-linking), [MeaningCloud](https://www.meaningcloud.com/developer/topics-extraction)…
    I tried all of the three and all work very well.
  prefs: []
  type: TYPE_NORMAL
- en: However, if your data is highly specific, and no generic topic can represent
    it, then you will have to go for a more personalized approach. This article focuses
    on one of these approaches: **LDA**.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding LDA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Intuition**'
  prefs: []
  type: TYPE_NORMAL
- en: LDA (*short for Latent Dirichlet Allocation*) is an unsupervised machine-learning
    model that takes documents as input and finds topics as output. The model also
    says in what percentage each document talks about each topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'A topic is represented as a weighted list of words. An example of a topic is
    shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: flower * 0,2 | rose * 0,15 | plant * 0,09 |…
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![figure-name](../Images/88c72a06f8f42663072cabb086f9195a.png)Illustration
    of LDA input/output workflow'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 3 main parameters of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: the number of topics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the number of words per topic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the number of topics per document
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In reality, the last two parameters are not exactly designed like this in the
    algorithm, but I prefer to stick to these simplified versions which are easier
    to understand.
  prefs: []
  type: TYPE_NORMAL
- en: '**Implementation**'
  prefs: []
  type: TYPE_NORMAL
- en: '*[A dedicated Jupyter notebook is shared at the end]*'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, I use a dataset of articles taken from BBC’s website.
  prefs: []
  type: TYPE_NORMAL
- en: To implement the LDA in Python, I use the package *gensim*.
  prefs: []
  type: TYPE_NORMAL
- en: A simple implementation of LDA, where we ask the model to create 20 topics
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameters shown previously are:'
  prefs: []
  type: TYPE_NORMAL
- en: the number of topics is equal to **num_topics**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the *[distribution of the] *number of words per topic is handled by **eta**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the *[distribution of the]* number of topics per document is handled by **alpha**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To print topics found, use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*[the first 3 topics are shown with their first 20 most relevant words]* Topic
    0 seems to be about military and war.'
  prefs: []
  type: TYPE_NORMAL
- en: Topic 1 about health in India, involving women and children.
  prefs: []
  type: TYPE_NORMAL
- en: Topic 2 about Islamists in Northern Mali.
  prefs: []
  type: TYPE_NORMAL
- en: 'To print the % of topics a document is about, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The first document is 99.8% about topic 14.
  prefs: []
  type: TYPE_NORMAL
- en: 'Predicting topics on an unseen document is also doable, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This new document talks 52% about topic 1, and 44% about topic 3\. Note that
    4% could not be labelled as existing topics.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploration**'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a nice way to visualize the LDA model you built using the package *pyLDAvis*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/5e42302952b7900bb2f3408c0cc6047f.png)Output of the
    pyLDAvis'
  prefs: []
  type: TYPE_NORMAL
- en: This visualization allows you to compare topics on two reduced dimensions and
    observe the distribution of words in topics.
  prefs: []
  type: TYPE_NORMAL
- en: Another nice visualization is to show all the documents according to their major
    topic in a diagonal format.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/8b714cb93412b8eefe740c4c79b2b77d.png)Visualization
    of the proportion of topics in the documents *(Documents are rows, topic are columns)*![figure-name](../Images/d0630db2532f240dabfe4840a1da2343.png)Topic
    18 is the most represented topic among documents: 25 documents are mainly about
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: How to successfully implement LDA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LDA is a complex algorithm which is generally perceived as hard to fine-tune
    and interpret. Indeed, getting relevant results with LDA requires a strong knowledge
    of how it works.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data cleaning**'
  prefs: []
  type: TYPE_NORMAL
- en: A common thing you will encounter with LDA is that words appear in multiple
    topics. One way to cope with this is to add these words to your stopwords list.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing is plural and singular forms. I would recommend lemmatizing —
    or stemming if you cannot lemmatize but having stems in your topics is not easily
    understandable.
  prefs: []
  type: TYPE_NORMAL
- en: Removing words with digits in them will also clean the words in your topics.
    Keeping years (2006, 1981) can be relevant if you believe they are meaningful
    in your topics.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering words that appear in at least 3 (or more) documents is a good way
    to remove rare words that will not be relevant in topics.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data preparation**'
  prefs: []
  type: TYPE_NORMAL
- en: Include bi- and tri-grams to grasp more relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another classic preparation step is to use only nouns and verbs using [POS
    tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging) (POS: Part-Of-Speech).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fine-tuning**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Number of topics: try out several numbers of topics to understand which amount
    makes sense. You actually need to *see *the topics to know if your model makes
    sense or not. As for K-Means, LDA converges and the model makes sense at a mathematical
    level, but it does not mean it makes sense at a human level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cleaning your data: adding stop words that are too frequent in your topics
    and re-running your model is a common step. Keeping only nouns and verbs, removing
    templates from texts, testing different cleaning methods iteratively will improve
    your topics. Be prepared to spend some time here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alpha, Eta. If you’re not into technical stuff, forget about these. Otherwise,
    you can tweak alpha and eta to adjust your topics. Start with ‘auto’, and if the
    topics are not relevant, try other values. I recommend using low values of Alpha
    and Eta to have a small number of topics in each document and a small number of
    relevant words in each topic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increase the number of *passes *to have a better model. 3 or 4 is a good number,
    but you can go higher.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Assessing results**'
  prefs: []
  type: TYPE_NORMAL
- en: Are your topics interpretable?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are your topics unique? (two different topics have different words)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are your topics exhaustive? (are all your documents well represented by these
    topics?)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your model follows these 3 criteria, it looks like a good model :)
  prefs: []
  type: TYPE_NORMAL
- en: Main advantages of LDA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**It’s fast**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the *%time* command in Jupyter to verify it. The model is usually fast
    to run. Of course, it depends on your data. Several factors can slow down the
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: Long documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large number of documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large vocabulary size (especially if you use n-grams with a large n)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**It’s intuitive**'
  prefs: []
  type: TYPE_NORMAL
- en: Modelling topics as weighted lists of words is a simple approximation yet a
    very intuitive approach if you need to interpret it. No embedding nor hidden dimensions,
    just bags of words with weights.
  prefs: []
  type: TYPE_NORMAL
- en: '**It can predict topics for new unseen documents**'
  prefs: []
  type: TYPE_NORMAL
- en: Once the model has run, it is ready to allocate topics to any document. Of course,
    if your training dataset is in English and you want to predict the topics of a
    Chinese document it won’t work. But if the new documents have the same structure
    and should have more or less the same topics, it will work.
  prefs: []
  type: TYPE_NORMAL
- en: Main disadvantages of LDA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Lots of fine-tuning**'
  prefs: []
  type: TYPE_NORMAL
- en: If LDA is fast to run, it will give you some trouble to get good results with
    it. That’s why knowing in advance how to fine-tune it will really help you.
  prefs: []
  type: TYPE_NORMAL
- en: '**It needs human interpretation**'
  prefs: []
  type: TYPE_NORMAL
- en: Topics are found by a machine. A human needs to label them in order to present
    the results to non-experts people.
  prefs: []
  type: TYPE_NORMAL
- en: '**You cannot influence topics**'
  prefs: []
  type: TYPE_NORMAL
- en: Knowing that some of your documents talk about a topic you know, and not finding
    it in the topics found by LDA will definitely be frustrating. And there’s no way
    to say to the model that some words should belong together. You have to sit and
    wait for the LDA to give you what you want.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LDA remains one of my favourite model for topics extraction, and I have used
    it many projects. However, it requires some practice to master it. That’s why
    I made this article so that you can jump over the barrier to entry of using LDA
    and use it painlessly.
  prefs: []
  type: TYPE_NORMAL
- en: Code: [https://github.com/FelixChop/MediumArticles/blob/master/LDA-BBC.ipynb](https://github.com/FelixChop/MediumArticles/blob/master/LDA-BBC.ipynb)
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Félix Revert](https://towardsdatascience.com/@FelixRvrt)** is a Customer
    Facing Data Scientist at DataRobot and Velocity'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/the-complete-guide-for-topics-extraction-in-python-a6aaa6cedbbc).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Topic Modeling with LSA, PLSA, LDA & lda2Vec](/2018/08/topic-modeling-lsa-plsa-lda-lda2vec.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unlock and Extract Data from Your PDF Documents](/2019/01/datalogics-unlock-extract-data-pdf-documents.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Domain-Specific Language Processing Mines Value From Unstructured Data](/2019/08/domain-specific-language-processing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Creating a Web Application to Extract Topics from Audio with Python](https://www.kdnuggets.com/2023/01/creating-web-application-extract-topics-audio-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Back to Basics Week 4: Advanced Topics and Deployment](https://www.kdnuggets.com/back-to-basics-week-4-advanced-topics-and-deployment)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Approaches to Text Summarization: An Overview](https://www.kdnuggets.com/2019/01/approaches-text-summarization-overview.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Labeling for Machine Learning: Market Overview, Approaches, and Tools](https://www.kdnuggets.com/2021/12/data-labeling-ml-overview-and-tools.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[An Overview of Logistic Regression](https://www.kdnuggets.com/2022/02/overview-logistic-regression.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[An Overview of Mercury: Creating Data Science Portfolio and…](https://www.kdnuggets.com/2022/05/overview-mercury-creating-data-science-portfolio-notebook-based-webapps.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
