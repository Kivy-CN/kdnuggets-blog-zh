- en: WTF is the Difference Between GBM and XGBoost?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/wtf-is-the-difference-between-gbm-and-xgboost](https://www.kdnuggets.com/wtf-is-the-difference-between-gbm-and-xgboost)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![WTF is the Difference Between GBM and XGBoost?](../Images/ad561fe1a6e273df29c1f7e74542575f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [upklyak](https://www.freepik.com/free-vector/business-startup-project-launch-successful-idea_29222778.htm#query=boost&position=4&from_view=search&track=sph&uuid=b8330f52-58e3-4628-830f-2673f9ab16e6)
    on [Freepik](https://www.freepik.com/)
  prefs: []
  type: TYPE_NORMAL
- en: I am sure everyone knows about the algorithms GBM and XGBoost. They are go-to
    algorithms for many real-world use cases and competition because the metric output
    is often better than the other models.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: For those who don’t know about GBM and XGBoost, GBM (Gradient Boosting Machine)
    and XGBoost (eXtreme Gradient Boosting) are ensemble learning methods. Ensemble
    learning is a machine learning technique where multiple “weak” models (often decision
    trees) are trained and combined for further purposes.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm was based on the ensemble learning boosting technique shown in
    their name. Boosting techniques is a method that tries to combine multiple weak
    learners sequentially, with each one correcting its predecessor.  Each learner
    would learn from their previous mistakes and correct the errors of the previous
    models.
  prefs: []
  type: TYPE_NORMAL
- en: That’s the fundamental similarity between GBM and XGB, but how about the differences?
    We will discuss that in this article, so let’s get into it.
  prefs: []
  type: TYPE_NORMAL
- en: GBM (Gradient Boosting Machine)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned above, GBM is based on boosting, which tries sequentially iterating
    the weak learner to learn from the error and develop a robust model. GBM developed
    a better model for each iteration by minimizing the loss function using gradient
    descent. Gradient descent is a concept to find the minimum function with each
    iteration, such as the loss function. The iteration would keep going until it
    achieves the stopping criterion.
  prefs: []
  type: TYPE_NORMAL
- en: For the GBM concepts, you can see it in the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![WTF is the Difference Between GBM and XGBoost?](../Images/967eac9e5ae21617bb2b8b2302c000f8.png)'
  prefs: []
  type: TYPE_IMG
- en: GBM Model Concept ([Chhetri *et al.* (2022)](https://www.researchgate.net/publication/358924681_A_Combined_System_Metrics_Approach_to_Cloud_Service_Reliability_Using_Artificial_Intelligence))
  prefs: []
  type: TYPE_NORMAL
- en: You can see in the image above that for each iteration, the model tries to minimize
    the loss function and learn from the previous mistake. The final model would be
    the whole weak learner that sums up all the predictions from the model.
  prefs: []
  type: TYPE_NORMAL
- en: XGB (eXtreme Gradient Boosting) and How It Is Different From GBM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: XGBoost or eXtreme Gradient Boosting is a machine-learning algorithm based on
    the gradient boosting algorithm developed by [Tiangqi Chen and Carlos Guestrin
    in 2016](https://arxiv.org/pdf/1603.02754.pdf). At a basic level, the algorithm
    still follows a sequential strategy to improve the next model based on gradient
    descent. However, a few differences of XGBoost push this model as one of the best
    in terms of performance and speed.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regularization is a technique in machine learning to avoid overfitting. It’s
    a collection of methods to constrain the model to become overcomplicated and have
    bad generalization power. It’s become an important technique as many models fit
    the training data too well.
  prefs: []
  type: TYPE_NORMAL
- en: GBM doesn’t implement Regularization in their algorithm, which makes the algorithm
    only focus on achieving minimum loss functions. Compared to the GBM, XGBoost implements
    the regularization methods to penalize the overfitting model.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two kinds of regularization that XGBoost could apply: L1 Regularization
    (Lasso) and L2 Regularization (Ridge). L1 Regularization tries to minimize the
    feature weights or coefficients to zero (effectively becoming a feature selection),
    while L2 Regularization tries to shrink the coefficient evenly (help to deal with
    multicollinearity).  By implementing both regularizations, XGBoost could avoid
    overfitting better than the GBM.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Parallelization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GBM tends to have a slower training time than the XGBoost because the latter
    algorithm implements parallelization during the training process. The boosting
    technique might be sequential, but parallelization could still be done within
    the XGBoost process.
  prefs: []
  type: TYPE_NORMAL
- en: The parallelization aims to speed up the tree-building process, mainly during
    the splitting event. By utilizing all the available processing cores, the XGBoost
    training time can be shortened.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of speeding up the XGBoost process, the developer also preprocessed
    the data into their developed data format, DMatrix, for memory efficiency and
    improved training speed.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Missing Data Handling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our training dataset could contain missing data, which we must explicitly handle
    before passing them into the algorithm. However, XGBoost has its own in-built
    missing data handler, whereas GBM doesn’t.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost implemented their technique to handle missing data, called Sparsity-aware
    Split Finding. For any sparsities data that XGBoost encounters (Missing Data,
    Dense Zero, OHE), the model would learn from these data and find the most optimum
    split. The model would assign where the missing data should be placed during splitting
    and see which direction minimizes the loss.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Tree Pruning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The growth strategy for the GBM is to stop splitting after the algorithm arrives
    at the negative loss in the split. The strategy could lead to suboptimal results
    because it’s only based on local optimization and might neglect the overall picture.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost tries to avoid the GBM strategy and grows the tree until the set parameter
    max depth starts pruning backward. The split with negative loss is pruned, but
    there is a case when the negative loss split was not removed. When the split arrives
    at a negative loss, but the further split is positive, it would still be retained
    if the overall split is positive.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. In-Built Cross-Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cross-validation is a technique to assess our model generalization and robustness
    ability by splitting the data systematically during several iterations. Collectively,
    their result would show if the model is overfitting or not.
  prefs: []
  type: TYPE_NORMAL
- en: Normally, the machine algorithm would require external help to implement the
    Cross-Validation, but XGBoost has an in-built Cross-Validation that could be used
    during the training session. The Cross-Validation would be performed at each boosting
    iteration and ensure the produce tree is robust.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GBM and XGBoost are popular algorithms in many real-world cases and competitions.
    Conceptually, both a boosting algorithms that use weak learners to achieve better
    models. However, they contain few differences in their algorithm implementation.
    XGBoost enhances the algorithm by embedding regularization, performing parallelization,
    better-missing data handling, different tree pruning strategies, and in-built
    cross-validation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '**[](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**[Cornellius Yudha
    Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**** is a data science
    assistant manager and data writer. While working full-time at Allianz Indonesia,
    he loves to share Python and data tips via social media and writing media. Cornellius
    writes on a variety of AI and machine learning topics.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[What Is the Difference Between SQL and Object-Relational Mapping (ORM)?](https://www.kdnuggets.com/2022/02/difference-sql-object-relational-mapping-orm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What’s the Difference Between Data Analysts and Data Scientists?](https://www.kdnuggets.com/2022/03/difference-data-analysts-data-scientists.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Efficiency Spells the Difference Between Biological Neurons and…](https://www.kdnuggets.com/2022/11/efficiency-spells-difference-biological-neurons-artificial-counterparts.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Difference Between L1 and L2 Regularization](https://www.kdnuggets.com/2022/08/difference-l1-l2-regularization.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Difference Between Training and Testing Data in Machine Learning](https://www.kdnuggets.com/2022/08/difference-training-testing-data-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[WTF is Regularization and What is it For?](https://www.kdnuggets.com/wtf-is-regularization-and-what-is-it-for)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
