- en: 'Neural Networks with Numpy for Absolute Beginners: Introduction'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Numpy 进行绝对初学者的神经网络：简介
- en: 原文：[https://www.kdnuggets.com/2019/03/neural-networks-numpy-absolute-beginners-introduction.html](https://www.kdnuggets.com/2019/03/neural-networks-numpy-absolute-beginners-introduction.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/03/neural-networks-numpy-absolute-beginners-introduction.html](https://www.kdnuggets.com/2019/03/neural-networks-numpy-absolute-beginners-introduction.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Suraj Donthi](https://www.surajdonthi.com/), Computer Vision Consultant
    & Course Instructor at DataCamp**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由[Suraj Donthi](https://www.surajdonthi.com/)，计算机视觉顾问及DataCamp课程讲师**'
- en: Artificial Intelligence has become one of the hottest fields in the current
    day and most of us willing to dive into this field start off with Neural Networks!!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能已成为当今最热门的领域之一，我们大多数希望进入这一领域的人都从神经网络开始!!
- en: But on confronting the math intensive concepts of Neural Networks we just end
    up learning a few frameworks like Tensorflow, Pytorch etc., for implementing Deep
    Learning Models.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，当面对神经网络中的数学密集概念时，我们最终只会学习一些框架，如 Tensorflow、Pytorch 等，用于实现深度学习模型。
- en: Moreover, just learning these frameworks and not understanding the underlying
    concepts is like playing with a black box.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，仅仅学习这些框架而不理解其基础概念就像在玩一个黑箱。
- en: Whether you want to work in the industry or academia, you will be working, tweaking
    and playing with the models for which you need to have a clear understanding.
    Both the industry and the academia expect you to have full clarity of these concepts
    including the math.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是想在行业还是学术界工作，你都将涉及到模型的工作、调整和使用，这要求你对这些概念有清晰的理解。行业和学术界都期望你对这些概念，包括数学，有充分的了解。
- en: In this series of tutorials, I’ll make it extremely simple to understand Neural
    Networks by providing step by step explanation. Also, the math you’ll need will
    be the level of high school.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一系列教程中，我将通过逐步讲解让神经网络变得非常简单易懂。此外，你需要的数学知识将是高中水平。
- en: Let us start with the inception of artificial neural networks and gain some
    inspiration as to how it evolved.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从人工神经网络的起源开始，并获得一些关于其演变的灵感。
- en: A little bit into the history of how Neural Networks evolved
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于神经网络如何发展的简要历史
- en: It must be noted that most of the Algorithms for Neural Networks that were developed
    during the period 1950–2000 and now existing, are highly inspired by the working
    of our brain, the neurons, their structure and how they learn and transfer data.
    The most popular works include the Perceptron(1958) and the Neocognitron(1980).
    These papers were extremely instrumental in unwiring the brain code. They try
    to mathematically formulate a model of the neural networks in our brain.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 必须指出的是，大多数在1950–2000年间开发的神经网络算法，及其现存算法，都高度受到我们大脑、神经元的工作方式、其结构以及它们如何学习和传递数据的启发。最著名的作品包括感知机（1958年）和新认知网络（1980年）。这些论文在解开大脑代码方面极为重要。它们试图数学化地构建我们大脑中的神经网络模型。
- en: And everything changed after the God Father of AI Geoffrey Hinton formulated
    the back-propagation algorithm in 1986(That’s right! what you are learning is
    more than 30 years old!).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都在1986年由人工智能教父 Geoffrey Hinton 公式化反向传播算法之后发生了变化（没错，你学习的内容已有30多年历史！）。
- en: A biological Neuron
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个生物神经元
- en: '![figure-name](../Images/e3a2dbe5d4d4c8051fca72a379720e58.png)The figure above
    shows a biological neuron. It has *dendrites* that receive information from neurons.
    The received information is passed on to the *cell body or the nucleus* of the
    neuron. The *nucleus* is where the information is processed and passed on to the
    next layer of neurons through *axons*.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '![figure-name](../Images/e3a2dbe5d4d4c8051fca72a379720e58.png) 上图展示了一个生物神经元。它有*树突*接收来自其他神经元的信息。接收到的信息会传递到神经元的*细胞体或细胞核*。*细胞核*是处理信息的地方，并通过*轴突*将信息传递给下一个神经元层。'
- en: Our brain consists of about 100 billion such neurons which communicate through
    electrochemical signals. Each neuron is connected to 100s and 1000s of other neurons
    which constantly transmit and receive signals.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大脑包含大约1000亿个这样的神经元，它们通过电化学信号进行通信。每个神经元都连接到数百甚至数千个其他神经元，这些神经元不断地传递和接收信号。
- en: But how can our brain process so much information just by sending electrochemical
    signals? How can the neurons understand which signal is important and which isn't?
    How do the neurons know what information to pass forward?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: The electrochemical signals consist of strong and weak signals. The strong signals
    are the ones to dominate which information is important. So only the strong signal
    or a combination of them pass through the nuclues (the CPU of neurons) and are
    transmitted to the next set of neurons through the axons.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: But how are some signals strong and some signals week?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Well, through millions of years of evolution, the neurons have become sensitive
    to certain kinds of signals. When the neuron encounters a specific pattern, they
    get triggered(activated) and as a consequence send strong signals to other neurons
    and hence the information is transmitted.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Most of us also know that different regions of our brain are activated (or receptive)
    for different actions like seeing, hearing, creative thinking and so on. This
    is because the neurons belonging to a specific region in the brain are trained
    to process a certain kind of information better and hence get activated only when
    certain kinds of information is being sent. The figure below gives us a better
    understanding of the different receptive regions of the brain.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![alternate text](../Images/35b55fb1a389baa89cde9023eb63fc32.png)If that is
    so… can the neurons be made sensitive to a different pattern(i.e., if they have
    truly become sensitive based on some patterns)?'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: It has been shown through Neuroplasticity that the different regions of the
    brain can be rewired to perform totally different tasks. Such as the neurons responsible
    for touch sensing can be rewired to become sensitive to smell. Check out this
    great TEDx video below to know more about neuroplasticity.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: TEDx video on Neuroplasticity [[Source]](https://www.youtube.com/watch?v=xzbHtIrb14s)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: But what is the mechanism by which the neurons become sensitive?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, neuroscientists are still trying to figure that out!!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: But fortunately enough, god father of AI Geff has saved the day by inventing
    back propagation which accomplishes the same task for our Artificial Neurons,
    i.e., sensitizing them to certain patterns.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll explore the working of a perceptron and also gain
    a mathematical intuition.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron/Artificial Neuron
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/bbd835297009a0c24b5acc73e39a6d00.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: From the figure, you can observe that the perceptron is a reflection of the
    biological neuron. The inputs combined with the weights(*wᵢ*) are analogous to
    dendrites. These values are summed and passed through an activation function (like
    the thresholding function as shown in fig.). This is analogous to the nucleus.
    Finally, the activated value is transmitted to the next neuron/perceptron which
    is analogous to the axons.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: The latent weights(*wᵢ*) multiplied with each input(*xᵢ*) depicts the significance(strength)
    of the respective input signal. Hence, larger the value of a weight, more important
    is the feature.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: You can infer from this architecture that the weights are what is learned in
    a perceptron so as to arrive at the required result. An additional bias(*b*, here
    *w₀*) is also learned.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, when there are multiple inputs (say *n*), the equation can be generalized
    as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/379910f15ba571c347905c8b1b3342ec.png)![Equation](../Images/68bac74c2701bd3d5270a73e2d19398f.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: Finally, the output of summation (assume as *z*) is fed to the *thresholding
    activation function*, where the function outputs
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: An Example
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![logic-gates](../Images/72bff0e49b84cadc2e36e8077e41cc42.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: Let us consider our perceptron to perform as *logic gates* to gain more intuition.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s choose an *AND Gate*. The Truth Table for the gate is shown below:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: The perceptron for the *AND Gate* can be formed as shown in the figure. It is
    clear that the perceptron has two inputs (here *x₁ = A* and *x₂ = B*).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![threshold-function](../Images/ba979f442dfea6e1392af3988d3a6e18.png)![threshold-function-eq](../Images/5caacf2f8f14fbcac3a7ab4b807dc330.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: We can see that for inputs *x₁, x₂* and *x₀* = 1, setting their weights as
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/87dbdc6f996cdfd46e54f905db58c6a1.png)![Equation](../Images/1cf563cb01c06d75daf731d1c46db107.png)![Equation](../Images/05dee933fdf09eee757a5d693178c757.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: respectively and keeping the *Threshold function* as the activation function
    we can arrive at the *AND Gate*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s get our hands dirty and codify this and test it out!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Similarly for *NOR Gate* the Truth Table is,
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![nor-gate-truth-table](../Images/7a93c890f845f018f767a3c1036c0433.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: 'The perceptron for *NOR Gate* will be as below:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![nor-gate=perceptron](../Images/f00b0279cec3976ab8f92b7359aee1e4.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: You can set the weights as
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/009579f99c16b1c77b7d43fafd76c5ce.png)![Equation](../Images/c14c9c9e6dab4255694305da0f1d7dce.png)![Equation](../Images/7665e03d35078c41cf61daae80a25b50.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: so that you obtain a *NOR Gate.*
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'You can go ahead and implement this in code as:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: What you are actually calculating…
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you analyse what you were trying to do in the above examples, you will realize
    that you were actually trying to adjust the values of the weights to obtain the
    required output.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Lets consider the *NOR Gate* example and break it down to very miniscule steps
    to gain more understanding.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: What you would usually do first is to simply set some values to the weights
    and observe the result, say
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/1664f5c911dd3834b15ae94a0f4adc01.png)![Equation](../Images/fdad0fc3d15c9f516e3eaecd32f765c5.png)![Equation](../Images/e7f0d60adce140c228e6f813e0c73a84.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: 'Then the output will be as shown in below table:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/6c9985311aad0f3841fd23651f371576.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: So, how can you fix the values of weights so that you get the right output?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: By intuition, you can easily observe that *w₀* must be increased and *w₁* and
    *w₀* must be reduced or rather made negative so that you obtain the actual output.
    But if you breakdown this intuition, you will observe that you are actually finding
    the difference between the actual output and the predicted output and finally
    reflecting that on the weights…
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 根据直觉，你可以很容易地观察到，*w₀* 必须增加，而 *w₁* 和 *w₀* 必须减少或变为负值，以获得实际输出。但如果你深入分析这种直觉，你会发现你实际上是在找到实际输出和预测输出之间的差异，并最终将其反映到权重上……
- en: This is a very important concept that you will be digging deeper and will be
    the core to formulate the ideas behind *gradient descent* and also *backward propagation*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常重要的概念，你将深入挖掘，并将成为形成*梯度下降*和*反向传播*背后理念的核心。
- en: What did you learn?
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你学到了什么？
- en: Neurons must be made sensitive to a pattern in order to recognize it.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元必须对模式变得敏感才能识别它。
- en: So, similarly, in our perceptron/artificial neuron, **the weights are what is
    to be learnt**.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，同样地，在我们的感知器/人工神经元中，**需要学习的是权重**。
- en: In the later articles you’ll fully understand how the weights are trained to
    recognize patterns and also the different techniques that exist.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续文章中，你将充分理解权重是如何被训练以识别模式的，以及存在的不同技术。
- en: As you’ll see later, the neural networks are very similar to the structure of
    biological neural networks.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你稍后会看到的，神经网络与生物神经网络的结构非常相似。
- en: While it is true that we learnt only a few small concepts (although very crucial)
    in this first part of the article, they will serve as the strong foundation for
    implementing Neural Networks. Moreover, I’m keeping this article short and sweet
    so that too much is information is not dumped at once and will help absorb more!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在本文的第一部分我们只学习了几个小概念（尽管非常重要），但它们将成为实现神经网络的坚实基础。此外，我保持这篇文章简短而精炼，以免一次性提供过多信息，有助于更好地吸收！
- en: In the next tutorial, you will learn about **Linear Regression** (which can
    otherwise be called a perceptron with linear activation function) in detail and
    also implement them. The **Gradient Descent algorithm which helps learn the weights**
    are described and implemented in detail. Lastly, you’ll be able to **predict the
    outcome of an event** with the help of Linear Regression. So, head on to the next
    article to implement it!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个教程中，你将详细学习 **线性回归**（也可以称为具有线性激活函数的感知器），并进行实现。**帮助学习权重的梯度下降算法**将被详细描述和实现。最后，你将能够**利用线性回归预测事件的结果**。所以，前往下一篇文章进行实现吧！
- en: 'You can checkout the next part of the article here:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里查看文章的下一部分：
- en: '[**Neural Networks with Numpy for Absolute Beginners — Part 2: Linear Regression**](https://medium.com/@surajdonthi95/neural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[**使用 Numpy 的神经网络绝对初学者 — 第 2 部分：线性回归**](https://medium.com/@surajdonthi95/neural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a)'
- en: '**Bio**: [Suraj Donthi](https://www.surajdonthi.com/) is a Computer Vision
    Consultant, Author, Machine Learning and Deep Learning Trainer.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**简历**： [Suraj Donthi](https://www.surajdonthi.com/) 是一位计算机视觉顾问、作者、机器学习和深度学习培训师。'
- en: '[Original](https://towardsdatascience.com/neural-networks-with-numpy-for-absolute-beginners-introduction-c1394639edb2).
    Reposted with permission.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/neural-networks-with-numpy-for-absolute-beginners-introduction-c1394639edb2).
    经许可转载。'
- en: '**Related:**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Neural Networks – an Intuition](/2019/02/neural-networks-intuition.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[神经网络 – 直观理解](/2019/02/neural-networks-intuition.html)'
- en: '[The Backpropagation Algorithm Demystified](/2019/01/backpropagation-algorithm-demystified.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[反向传播算法揭秘](/2019/01/backpropagation-algorithm-demystified.html)'
- en: '[Mastering the Learning Rate to Speed Up Deep Learning](/2018/11/mastering-learning-rate-speed-up-deep-learning.html)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握学习率以加速深度学习](/2018/11/mastering-learning-rate-speed-up-deep-learning.html)'
- en: '* * *'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速入门网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[The Absolute Basics of MLOps](https://www.kdnuggets.com/2022/09/absolute-basics-mlops.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10 Simple Things to Try Before Neural Networks](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Neural Networks Don''t Lead Us Towards AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Explainable Forecasting and Nowcasting with State-of-the-art Deep…](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Image Classification with Convolutional Neural Networks (CNNs)](https://www.kdnuggets.com/2022/05/image-classification-convolutional-neural-networks-cnns.html)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
