- en: Regularization in Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的正则化
- en: 原文：[https://www.kdnuggets.com/2018/01/regularization-machine-learning.html](https://www.kdnuggets.com/2018/01/regularization-machine-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/01/regularization-machine-learning.html](https://www.kdnuggets.com/2018/01/regularization-machine-learning.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Prashant Gupta](https://twitter.com/Prashant_1722)**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Prashant Gupta](https://twitter.com/Prashant_1722) 提供**'
- en: One of the major aspects of training your machine learning model is avoiding
    overfitting. **The model will have a low accuracy if it is overfitting**. This
    happens because your model is trying too hard to capture the noise in your training
    dataset. **By noise we mean the data points that don’t really represent the true
    properties of your data, but random chance**. Learning such data points, makes
    your model more flexible, at the risk of overfitting.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 训练机器学习模型的一个主要方面是避免过拟合。**如果模型过拟合，其准确性会很低**。这发生在你的模型过于努力地捕捉训练数据集中的噪声时。**噪声指的是那些不真正代表数据真实属性的数据点，而是随机的偶然性**。学习这些数据点会使模型更加灵活，但也有过拟合的风险。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The concept of balancing bias and variance, is helpful in understanding the
    phenomenon of overfitting.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡偏差和方差的概念，有助于理解过拟合现象。
- en: One of the ways of avoiding overfitting is using cross validation, that helps
    in estimating the error over test set, and in deciding what parameters work best
    for your model.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 避免过拟合的方法之一是使用交叉验证，这有助于估计测试集上的误差，并决定哪些参数最适合你的模型。
- en: This article will focus on a technique that helps in avoiding overfitting and
    also increasing model interpretability.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将重点介绍一种技术，帮助避免过拟合并提高模型的可解释性。
- en: '**Regularization**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则化**'
- en: This is a form of regression, that constrains/ regularizes or shrinks the coefficient
    estimates towards zero. In other words, **this technique discourages learning
    a more complex or flexible model, so as to avoid the risk of overfitting.**
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种回归形式，约束/正则化或缩小系数估计值到接近零。换句话说，**这种技术旨在避免学习更复杂或灵活的模型，以防止过拟合的风险**。
- en: A simple relation for linear regression looks like this. Here Y represents the
    learned relation and β represents the coefficient estimates for different variables
    or predictors(X).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的简单关系看起来是这样的。这里 Y 代表学到的关系，β 代表不同变量或预测器(X)的系数估计值。
- en: '***Y ≈ β0 + β1X1 + β2X2 + … + βpXp***'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '***Y ≈ β0 + β1X1 + β2X2 + … + βpXp***'
- en: The fitting procedure involves a loss function, known as residual sum of squares
    or RSS. The coefficients are chosen, such that they minimize this loss function.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合过程涉及一个损失函数，称为残差平方和或RSS。系数的选择是为了最小化这个损失函数。
- en: '![](../Images/28c0536edba1951f65d5c1cb9a2e1c16.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28c0536edba1951f65d5c1cb9a2e1c16.png)'
- en: Now, this will adjust the coefficients based on your training data. If there
    is noise in the training data, then the estimated coefficients won’t generalize
    well to the future data. This is where regularization comes in and shrinks or
    regularizes these learned estimates towards zero.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这将根据你的训练数据调整系数。如果训练数据中存在噪声，那么估计的系数对未来数据的泛化能力就会差。这就是正则化发挥作用的地方，它将这些学习到的估计值缩小或正则化到接近零。
- en: '**Ridge Regression**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**岭回归**'
- en: '![](../Images/69c4f06902d08b0a2921390fe8b907c4.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69c4f06902d08b0a2921390fe8b907c4.png)'
- en: Above image shows ridge regression, where the **RSS is modified by adding the
    shrinkage quantity**. Now, the coefficients are estimated by minimizing this function.
    Here, **λ** is the tuning parameter that decides how much we want to penalize
    the flexibility of our model. The increase in flexibility of a model is represented
    by increase in its coefficients, and if we want to minimize the above function,
    then these coefficients need to be small. This is how the Ridge regression technique
    prevents coefficients from rising too high. Also, notice that we shrink the estimated
    association of each variable with the response, except the intercept β0, This
    intercept is a measure of the mean value of the response when xi1 = xi2 = …= xip
    = 0.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了岭回归，其中**RSS通过添加收缩量进行修改**。现在，通过最小化此函数来估计系数。这里，**λ**是调整参数，它决定了我们希望对模型的灵活性施加多大惩罚。模型灵活性的增加由其系数的增加表示，如果我们希望最小化上述函数，则这些系数需要很小。这就是岭回归技术如何防止系数过高的原因。同时，请注意，我们收缩了每个变量与响应的估计关联，除了截距β0。这个截距是当xi1
    = xi2 = …= xip = 0时响应的均值的度量。
- en: When λ = 0, the penalty term has no effect, and the estimates produced by ridge
    regression will be equal to least squares. However, as λ→∞, the impact of the
    shrinkage penalty grows, and the ridge regression coefficient estimates will approach
    zero. As can be seen, selecting a good value of λ is critical. Cross validation
    comes in handy for this purpose. The coefficient estimates produced by this method
    are also known as the L2 norm.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当λ = 0时，惩罚项没有效果，岭回归所产生的估计值将等于最小二乘估计。然而，随着λ→∞，收缩惩罚的影响逐渐增大，岭回归系数估计将趋近于零。如上所示，选择一个合适的λ值是关键。交叉验证对于这个目的非常有用。该方法产生的系数估计也被称为L2范数。
- en: The coefficients that are produced by the standard least squares method are
    scale equivariant, i.e. if we multiply each input by c then the corresponding
    coefficients are scaled by a factor of 1/c. Therefore, regardless of how the predictor
    is scaled, the multiplication of predictor and coefficient(X[j]β[j]) remains the
    same. However, this is not the case with ridge regression, and therefore, we need
    to standardize the predictors or bring the predictors to the same scale before
    performing ridge regression. The formula used to do this is given below.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 标准最小二乘法产生的系数是尺度不变的，即如果我们将每个输入乘以c，则相应的系数将按1/c的因子进行缩放。因此，无论预测变量如何缩放，预测变量与系数的乘积（X[j]β[j]）保持不变。然而，岭回归的情况则不同，因此，在进行岭回归之前，我们需要对预测变量进行标准化或将预测变量调整到相同的尺度。用于此操作的公式如下所示。
- en: '![](../Images/d59604341f9ddc198ff0bb86d9958c0e.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d59604341f9ddc198ff0bb86d9958c0e.png)'
- en: '**Lasso**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**套索回归**'
- en: '![](../Images/1e9dbe3641f030999c8c95a4e8a0d3dd.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e9dbe3641f030999c8c95a4e8a0d3dd.png)'
- en: Lasso is another variation, in which the above function is minimized. Its clear
    that ***this variation differs from ridge regression only in penalizing the high
    coefficients***. It uses |βj| (modulus)instead of squares of β, as its penalty.
    In statistics, this is ***known as the L1 norm***.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 套索回归是另一种变体，其中上述函数被最小化。显然，***这种变体与岭回归的区别在于对高系数的惩罚***。它使用|βj|（绝对值）而不是β的平方作为惩罚。在统计学中，这被***称为L1范数***。
- en: Lets take a look at above methods with a different perspective. *The ridge regression
    can be thought of as solving an equation, where summation of squares of coefficients
    is less than or equal to s*. And *the Lasso can be thought of as an equation where
    summation of modulus of coefficients is less than or equal to s*. Here, s is a
    constant that exists for each value of shrinkage factor λ. ***These equations
    are also referred to as constraint functions.***
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从不同的角度来看待上述方法。*岭回归可以看作是求解一个方程，其中系数的平方和小于或等于s*。而*套索回归可以看作是一个方程，其中系数的绝对值之和小于或等于s*。这里，s是对于每个收缩因子λ值存在的常量。***这些方程也被称为约束函数。***
- en: '***Consider their are 2 parameters in a given problem.*** Then according to
    above formulation, the ***ridge regression is expressed by β1² + β2² ≤ s***. This
    implies that *ridge regression coefficients have the smallest RSS(loss function)
    for all points that lie within the circle given by β1² + β2² ≤ s*.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '***考虑给定问题中有2个参数。*** 然后根据上述公式，***岭回归表示为β1² + β2² ≤ s***。这意味着*岭回归系数在所有位于β1² +
    β2² ≤ s的圆内的点上具有最小的RSS（损失函数）*。'
- en: Similarly, ***for lasso, the equation becomes,|β1|+|β2|≤ s***. This implies
    that *lasso coefficients have the smallest RSS(loss function) for all points that
    lie within the diamond given by |β1|+|β2|≤ s*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，***对于套索回归，方程变为 |β1|+|β2|≤ s***。这意味着*套索回归系数在所有位于由 |β1|+|β2|≤ s 给出的菱形内部的点上具有最小的
    RSS（损失函数）*。
- en: The image below describes these equations.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 下图描述了这些方程。
- en: '![](../Images/e1be3283fd716c9772bdee13dac99120.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1be3283fd716c9772bdee13dac99120.png)'
- en: '***The above image shows the constraint functions(green areas), for lasso (left)
    and ridge regression (right), along with contours for RSS (red ellipse)***. Points
    on the ellipse share the value of RSS. For a very large value of s, the green
    regions will contain the center of the ellipse, making coefficient estimates of
    both regression techniques, equal to the least squares estimates. But, this is
    not the case in the above image. In this case, the lasso and ridge regression
    coefficient estimates are given by the first point at which an ellipse contacts
    the constraint region. ***Since ridge regression has a circular constraint with
    no sharp points, this intersection will not generally occur on an axis, and so
    the ridge regression coefficient estimates will be exclusively non-zero.***'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '***上图展示了套索回归（左）和岭回归（右）的约束函数（绿色区域），以及 RSS 的轮廓（红色椭圆）***。椭圆上的点共享 RSS 的值。对于非常大的
    s 值，绿色区域将包含椭圆的中心，使得两种回归技术的系数估计等于最小二乘估计。然而，上图中的情况并非如此。在这种情况下，套索回归和岭回归的系数估计由椭圆与约束区域接触的第一个点给出。***由于岭回归具有没有尖锐点的圆形约束，因此这种交点通常不会发生在轴上，因此岭回归系数估计将完全非零。***'
- en: '***However, the lasso constraint has corners at each of the axes, and so the
    ellipse will often intersect the constraint region at an axis. When this occurs,
    one of the coefficients will equal zero.*** In higher dimensions(where parameters
    are much more than 2), many of the coefficient estimates may equal zero simultaneously.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '***然而，套索回归的约束在每个轴上都有角，因此椭圆通常会在轴上与约束区域相交。当这种情况发生时，其中一个系数将等于零。*** 在高维空间（参数远多于
    2 的情况）中，许多系数估计可能会同时等于零。'
- en: '**This sheds light on the obvious disadvantage of ridge regression, which is
    model interpretability.** It will shrink the coefficients for least important
    predictors, very close to zero. But it will never make them exactly zero. In other
    words, the final model will include all predictors. However, in the case of the
    lasso, the L1 penalty has the effect of forcing some of the coefficient estimates
    to be exactly equal to zero when the tuning parameter λ is sufficiently large.
    **Therefore, the lasso method also performs variable selection and is said to
    yield sparse models.**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**这揭示了岭回归显而易见的缺点，即模型的可解释性。** 它会将对最不重要预测变量的系数收缩到非常接近零，但永远不会将其完全变为零。换句话说，最终模型将包括所有预测变量。然而，在套索回归的情况下，L1
    惩罚的效果是，当调节参数 λ 足够大时，迫使一些系数估计恰好为零。**因此，套索方法还执行变量选择，并且被认为能够产生稀疏模型。**'
- en: '**What does Regularization achieve?**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则化的目标是什么？**'
- en: A standard least squares model tends to have some variance in it, i.e. this
    model won’t generalize well for a data set different than its training data. ***Regularization,
    significantly reduces the variance of the model, without substantial increase
    in its bias***. So the tuning parameter λ, used in the regularization techniques
    described above, controls the impact on bias and variance. As the value of λ rises,
    it reduces the value of coefficients and thus reducing the variance. ***Till a
    point, this increase in λ is beneficial as it is only reducing the variance(hence
    avoiding overfitting), without loosing any important properties in the data.***
    But after certain value, the model starts loosing important properties, giving
    rise to bias in the model and thus underfitting. Therefore, the value of λ should
    be carefully selected.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 标准最小二乘模型通常具有一定的方差，即该模型对于与其训练数据不同的数据集泛化效果较差。***正则化显著减少了模型的方差，而不会显著增加其偏差***。因此，正则化技术中使用的调节参数
    λ 控制对偏差和方差的影响。随着 λ 值的增加，它会减少系数的值，从而减少方差。***在某一点之前，λ 的增加是有益的，因为它仅减少方差（从而避免过拟合），而不会丢失数据中的任何重要属性。***
    但在某个值之后，模型开始丢失重要属性，导致模型偏差，从而出现欠拟合。因此，λ 的值应仔细选择。
- en: This is all the basic you will need, to get started with Regularization. It
    is a useful technique that can help in improving the accuracy of your regression
    models. A popular library for implementing these algorithms is [**Scikit-Learn**](https://becominghuman.ai/implementing-decision-trees-using-scikit-learn-5057b27221ec).
    It has a wonderful API that can get your model up an running with **just a few
    lines of code in python**.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是你开始学习正则化所需的基础知识。这是一种有用的技术，可以帮助提高回归模型的准确性。一个实现这些算法的流行库是 [**Scikit-Learn**](https://becominghuman.ai/implementing-decision-trees-using-scikit-learn-5057b27221ec)。它拥有一个出色的
    API，可以让你的模型在 **仅用几行 Python 代码** 的情况下运行起来。
- en: If you have any questions, **leave a comment** and I will do my best to answer.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何问题，**请留下评论**，我会尽力回答。
- en: You can also follow me on [**Twitter**](https://twitter.com/Prashant_1722),
    [**email me directly**](mailto:pr.span24@gmail.com) or [**find me on linkedin**](https://www.linkedin.com/in/prashantgupta17/).
    I’d love to hear from you.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过 [**Twitter**](https://twitter.com/Prashant_1722)、[**直接发邮件给我**](mailto:pr.span24@gmail.com)
    或 [**在 LinkedIn 上找我**](https://www.linkedin.com/in/prashantgupta17/)。我很期待你的消息。
- en: That’s all folks, Have a nice day :)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是所有内容了，祝大家有美好的一天 :)
- en: '**Credit**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**致谢**'
- en: Content for this article is inspired and taken from, An Introduction to Statistical
    Learning by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本文内容的灵感来源于《统计学习简介》一书，由 Gareth James、Daniela Witten、Trevor Hastie 和 Robert Tibshirani
    合著。
- en: '**Bio:** [Prashant Gupta](https://towardsdatascience.com/@prashantgupta17)
    is a Machine Learning Engineer, Android Developer, Tech Enthusiast.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：** [Prashant Gupta](https://towardsdatascience.com/@prashantgupta17) 是一名机器学习工程师、安卓开发者、技术爱好者。'
- en: '[Original](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a).
    Reposted with permission.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a)。已获许可转载。'
- en: '**Related**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关**'
- en: '[**Training Sets, Test Sets, and 10-fold Cross-validation**](https://www.kdnuggets.com/2018/01/training-test-sets-cross-validation.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**训练集、测试集和 10 折交叉验证**](https://www.kdnuggets.com/2018/01/training-test-sets-cross-validation.html)'
- en: '[**How To Debug Your Approach To Data Analysis**](https://www.kdnuggets.com/2017/12/debug-data-analysis.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**如何调试你的数据分析方法**](https://www.kdnuggets.com/2017/12/debug-data-analysis.html)'
- en: '[**Top 6 errors novice machine learning engineers make**](https://www.kdnuggets.com/2017/10/top-errors-novice-machine-learning-engineers.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**初学者在机器学习中常犯的 6 个错误**](https://www.kdnuggets.com/2017/10/top-errors-novice-machine-learning-engineers.html)'
- en: More On This Topic
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关内容
- en: '[The Difference Between L1 and L2 Regularization](https://www.kdnuggets.com/2022/08/difference-l1-l2-regularization.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[L1 和 L2 正则化的区别](https://www.kdnuggets.com/2022/08/difference-l1-l2-regularization.html)'
- en: '[WTF is Regularization and What is it For?](https://www.kdnuggets.com/wtf-is-regularization-and-what-is-it-for)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[正则化是什么，它的用途是什么？](https://www.kdnuggets.com/wtf-is-regularization-and-what-is-it-for)'
- en: '[5 Machine Learning Skills Every Machine Learning Engineer Should…](https://www.kdnuggets.com/2023/03/5-machine-learning-skills-every-machine-learning-engineer-know-2023.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个机器学习工程师都应该掌握的 5 种机器学习技能](https://www.kdnuggets.com/2023/03/5-machine-learning-skills-every-machine-learning-engineer-know-2023.html)'
- en: '[KDnuggets News, December 14: 3 Free Machine Learning Courses for…](https://www.kdnuggets.com/2022/n48.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，12 月 14 日：3 个免费的机器学习课程…](https://www.kdnuggets.com/2022/n48.html)'
- en: '[A Solid Plan for Learning Data Science, Machine Learning, and Deep Learning](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学、机器学习和深度学习的完整计划](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
- en: '[AI, Analytics, Machine Learning, Data Science, Deep Learning…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能、分析、机器学习、数据科学、深度学习…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
