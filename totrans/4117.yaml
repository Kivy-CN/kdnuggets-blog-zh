- en: 3 Simple Ways to Speed Up Your Python Code
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加速你的 Python 代码的三种简单方法
- en: 原文：[https://www.kdnuggets.com/2022/10/3-simple-ways-speed-python-code.html](https://www.kdnuggets.com/2022/10/3-simple-ways-speed-python-code.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/10/3-simple-ways-speed-python-code.html](https://www.kdnuggets.com/2022/10/3-simple-ways-speed-python-code.html)
- en: '![3 Simple Ways to Speed Up Your Python Code](../Images/604ed2396cc488ef771a705d9cdfb3f5.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![加速你的 Python 代码的三种简单方法](../Images/604ed2396cc488ef771a705d9cdfb3f5.png)'
- en: '[Source](https://www.freepik.com/free-vector/flat-people-asking-questions_13561931.htm#query=thinking&position=28&from_view=search):
    Image by Freepik'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://www.freepik.com/free-vector/flat-people-asking-questions_13561931.htm#query=thinking&position=28&from_view=search):
    图片由 Freepik 提供'
- en: You just finished a project that used Python. The code is clean and readable,
    but your performance benchmark is not up to the mark. You expected to get a result
    in milliseconds, and instead, you got seconds. What do you do?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚完成了一个使用 Python 的项目。代码干净且可读，但你的性能基准未达标。你原本期望结果在毫秒内出来，却得到了秒级的结果。你该怎么办？
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析水平'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT 需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: If you’re reading this article, you probably already know that Python is an
    interpreted programming language with dynamic semantics and high readability.
    That makes it easy to use and read — but not fast enough for many real-world use
    cases.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在阅读这篇文章，你可能已经知道 Python 是一种具有动态语义和高可读性的解释型编程语言。这使得它易于使用和阅读，但对于许多现实世界的使用场景却不够快速。
- en: So there could be multiple ways to speed up your Python code including but not
    limited to the use of efficient data structures as well as fast and efficient
    algorithms. Some Python libraries also make use of C or C++ underneath to speed
    up computation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可能有多种方法来加速你的 Python 代码，包括但不限于使用高效的数据结构和快速高效的算法。一些 Python 库还在底层使用 C 或 C++
    来加速计算。
- en: 'What if you have exhausted all these options? Here comes parallel processing
    and a step ahead is distributed computing. In this post, you will learn about
    the three popular frameworks in distributed computing in a machine learning context:
    PySpark, Dask, and Ray.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经耗尽了所有这些选项，接下来可以考虑并行处理，进一步则是分布式计算。在这篇文章中，你将了解在机器学习背景下三种流行的分布式计算框架：PySpark、Dask
    和 Ray。
- en: PySpark
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark
- en: 'PySpark as the name suggests is an interface of Apache Spark within Python.
    It allows the user to write Spark programs using Python APIs and provides the
    PySpark shell for interactive data analysis in a distributed environment. It supports
    almost all of Spark’s features such as Streaming, MLlib, Spark SQL, DataFrame,
    and Spark Core as shown below:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark，顾名思义，是 Apache Spark 在 Python 中的接口。它允许用户使用 Python API 编写 Spark 程序，并提供
    PySpark shell 进行分布式环境下的交互式数据分析。它支持几乎所有的 Spark 特性，如 Streaming、MLlib、Spark SQL、DataFrame
    和 Spark Core，如下所示：
- en: '![3 Simple Ways to Speed Up Your Python Code](../Images/d6d5e4cd748fe13a7cd3df8e2e614d9a.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![加速你的 Python 代码的三种简单方法](../Images/d6d5e4cd748fe13a7cd3df8e2e614d9a.png)'
- en: '[Source](https://spark.apache.org/docs/latest/api/python/_images/pyspark-components.png)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://spark.apache.org/docs/latest/api/python/_images/pyspark-components.png)'
- en: 1\. Streaming
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 流处理
- en: The streaming feature in Apache Spark is easy to use and fault-resistant that
    runs on top of Spark. It powers intuitive and analytical systems across streaming
    as well as historical data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 的流处理特性易于使用且具备容错性，运行在 Spark 之上。它为流数据和历史数据提供直观和分析系统。
- en: 2\. MLlib
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. MLlib
- en: MLlib is a scalable machine learning library built on top of the Spark framework.
    It exposes a consistent set of high-level APIs to create and tune scalable machine
    learning pipelines.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib 是建立在 Spark 框架上的可扩展机器学习库。它暴露了一组一致的高层 API，用于创建和调整可扩展的机器学习管道。
- en: 3\. Spark SQL and DataFrame
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. Spark SQL 和 DataFrame
- en: It is a Spark module for tabular data processing. It provides an abstraction
    layer above the tabular data known as DataFrame and can act as a SQL-style query
    engine in a distributed setup.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个用于表格数据处理的 Spark 模块。它提供了一个抽象层，称为 DataFrame，可以在分布式设置中作为 SQL 风格的查询引擎。
- en: 4\. Spark Core
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. Spark Core
- en: Spark Core is the base general execution engine on top of which all other functionality
    is built. It provides a Resilient Distributed Dataset (RDD) and in-memory computation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Core 是一个基础的通用执行引擎，所有其他功能都建立在其上。它提供了一个弹性分布式数据集（RDD）和内存计算。
- en: 5\. Pandas API on Spark
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. Pandas API on Spark
- en: Pandas API is a module that enables scalable processing of pandas' features
    and methods. Its syntax is just like pandas and does not require the user to train
    on a new module. It provides a seamless and integrated codebase for pandas (small/single
    machine datasets) and Spark (large distributed datasets).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas API 是一个模块，使得 pandas 的特性和方法可以进行可扩展处理。它的语法与 pandas 一致，不需要用户培训新的模块。它为 pandas（小型/单机数据集）和
    Spark（大型分布式数据集）提供了无缝集成的代码库。
- en: Dask
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dask
- en: Dask is a versatile open-source library for distributed computing that provides
    a familiar user interface to Pandas, Scikit-learn, and NumPy.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 是一个多功能的开源分布式计算库，它提供了一个与 Pandas、Scikit-learn 和 NumPy 类似的用户界面。
- en: It exposes high-level and low-level APIs enabling users to run custom algorithms
    in parallel on single or distributed machines.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 它提供了高级和低级 API，使用户能够在单机或分布式机器上并行运行自定义算法。
- en: 'It has two components:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 它有两个组件：
- en: Big data collections include High-Level collections such as Dask Array or parallelized
    NumPy arrays, Dask Bag or parallelized lists, Dask DataFrame or parallelized Pandas
    DataFrames, and Parallelized Scikit-learn. They also include Low-Level collections
    such as Delayed and Futures that ease parallel and real-time implementation of
    custom tasks.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据集合包括高级集合，如 Dask Array 或并行化 NumPy 数组、Dask Bag 或并行化列表、Dask DataFrame 或并行化 Pandas
    DataFrames、以及并行化 Scikit-learn。它们还包括低级集合，如 Delayed 和 Futures，这些都简化了自定义任务的并行和实时实现。
- en: Dynamic task scheduling enables the execution of task graphs in parallel scaling
    up to clusters of thousands of nodes.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态任务调度使得任务图可以并行执行，扩展到数千个节点的集群。
- en: Ray
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ray
- en: 'Ray is a single platform framework used in general distributed Python as well
    as AIML-powered applications. It constitutes a core distributed runtime and a
    toolkit of libraries (Ray AI Runtime) for parallelizing AIML computation as shown
    in the diagram below:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 是一个用于通用分布式 Python 以及 AIML 驱动应用程序的单一平台框架。它包含一个核心分布式运行时和一个库工具包（Ray AI Runtime），用于并行化
    AIML 计算，如下图所示：
- en: '![3 Simple Ways to Speed Up Your Python Code](../Images/7ef99e205e7ba4c099ae92184bcb18a2.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![加速你的 Python 代码的 3 种简单方法](../Images/7ef99e205e7ba4c099ae92184bcb18a2.png)'
- en: '[Source](https://docs.ray.io/en/latest/_images/ray-air.svg)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://docs.ray.io/en/latest/_images/ray-air.svg)'
- en: Ray AI Runtime
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ray AI Runtime
- en: Ray AIR or Ray AI Runtime is a one-stop toolkit for distributed ML applications
    that enables easy scaling of individual and end-to-end workflows. It builds on
    Ray’s libraries for a wide range of tasks such as preprocessing, scoring, training,
    tuning, serving, etc.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Ray AIR 或 Ray AI Runtime 是一个一站式工具包，用于分布式 ML 应用，支持轻松扩展个体和端到端工作流。它基于 Ray 的库，处理各种任务，如预处理、评分、训练、调优、服务等。
- en: Ray Core
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ray Core
- en: Ray Core provides core primitives like tasks (Stateless functions executed in
    the cluster), actors (Stateful worker processes created in the cluster), and objects
    (Immutable values accessible across the cluster) to build scalable distributed
    applications.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Core 提供了核心原语，如任务（在集群中执行的无状态函数）、演员（在集群中创建的有状态工作进程）和对象（在集群中可访问的不可变值），用于构建可扩展的分布式应用程序。
- en: Ray scales machine learning workloads with Ray AIR and builds and deploys distributed
    applications with Ray Core and Ray Clusters.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 使用 Ray AIR 扩展机器学习工作负载，并通过 Ray Core 和 Ray Clusters 构建和部署分布式应用程序。
- en: Which One to Choose?
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择哪一个？
- en: Now that you know your options, the natural question is which one to choose.
    The answer depends on a number of factors like the specific business need, the
    core strength of the development team, etc.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了你的选择，接下来的自然问题是选择哪一个。答案取决于多个因素，如具体的业务需求、开发团队的核心实力等。
- en: 'Let’s understand which framework is suitable for the specific requirements
    listed below:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解一下哪种框架适合下面列出的具体要求：
- en: '**Size:** PySpark is the most capable when it comes to dealing with ultra-large
    workloads (TBs and above) while Dask and Ray do fairly well on medium-sized workloads.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规模：** PySpark 在处理超大型工作负载（TB 级及以上）时最为强大，而 Dask 和 Ray 在中等规模的工作负载下表现相当不错。'
- en: '**Generic:** Ray leads the front when it comes to generic solutions, followed
    by PySpark. While Dask is purely aimed at scaling ML pipelines.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用性：** 在通用解决方案方面，Ray 领先，其次是 PySpark。而 Dask 完全旨在扩展 ML 管道。'
- en: '**Speed:** Ray is the best option for NLP or text normalization tasks which
    utilizes GPUs for speeding up computation. Dask on the other hand provides access
    to fast reading of structured files to DataFrame objects but falls behind when
    it comes to joining and merging them. This is where the Spark SQL scores well.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度：** Ray 是处理 NLP 或文本标准化任务的最佳选择，利用 GPU 加速计算。另一方面，Dask 提供了快速读取结构化文件到 DataFrame
    对象的功能，但在连接和合并方面表现较差。这里 Spark SQL 表现优异。'
- en: '**Familiarity:** For teams more inclined toward Pandas'' way of fetching and
    filtering data, Dask seems to be a go-to option whereas PySpark is for those teams
    looking for an SQL-like querying interface.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**熟悉度：** 对于更倾向于 Pandas 数据获取和过滤方式的团队，Dask 似乎是一个首选，而 PySpark 则适合那些寻找类似 SQL 查询接口的团队。'
- en: '**Ease of Use:** All three tools are built over different platforms. While
    PySpark is mostly Java and C++ based, Dask is purely Python which means your ML
    team including data scientists can easily trace back error messages if something
    breaks. Ray on the other hand is C++ on the core but is fairly Pythonic when it
    comes to the AIML module (Ray AIR).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易用性：** 三个工具建立在不同的平台上。虽然 PySpark 主要基于 Java 和 C++，Dask 则完全基于 Python，这意味着你的机器学习团队，包括数据科学家，可以更容易地追踪错误消息。另一方面，Ray
    的核心是 C++，但在 AIML 模块（Ray AIR）中相当 Python 风格。'
- en: '**Installation and Maintenance:** Ray and Dask score equally well when it comes
    to maintenance overhead. Spark infrastructure on the other hand is quite complex
    and difficult to maintain.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安装和维护：** 在维护开销方面，Ray 和 Dask 得分相当。另一方面，Spark 基础设施相当复杂且难以维护。'
- en: '**Popularity and Support:** PySpark being the most mature of all enjoys developer
    community support while Dask comes second. Ray is promising in terms of features
    available in the beta testing phase.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流行度和支持：** PySpark 作为所有选项中最成熟的一个，享有开发者社区的支持，而 Dask 排名第二。Ray 在功能方面表现出色，处于测试阶段。'
- en: '**Compatibility:** While PySpark integrates well with the Apache ecosystem,
    Dask gels with Python and ML libraries quite well.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**兼容性：** 虽然 PySpark 与 Apache 生态系统集成良好，但 Dask 与 Python 和 ML 库的兼容性也很好。'
- en: Summary
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This post discussed how to speed up Python code beyond the usual choice of data
    structures and algorithms. The post focused on three well-known frameworks and
    their components. The post intends to help the reader by weighing the available
    choices on a range of certain attributes and business contexts.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本文讨论了如何在常见的数据结构和算法选择之外加速 Python 代码。文章重点介绍了三个著名的框架及其组件。本文旨在通过权衡不同选项在各种特性和业务背景下的表现，帮助读者做出选择。
- en: '**[Vidhi Chugh](https://vidhi-chugh.medium.com/)** is an award-winning AI/ML
    innovation leader and an AI Ethicist. She works at the intersection of data science,
    product, and research to deliver business value and insights. She is an advocate
    for data-centric science and a leading expert in data governance with a vision
    to build trustworthy AI solutions.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Vidhi Chugh](https://vidhi-chugh.medium.com/)** 是一位获奖的 AI/ML 创新领导者和 AI 伦理学家。她在数据科学、产品和研究的交汇点工作，以提供商业价值和洞察。她是数据中心科学的倡导者，也是数据治理领域的领先专家，致力于构建可信赖的
    AI 解决方案。'
- en: More On This Topic
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多内容
- en: '[How To Speed Up Python Code with Caching](https://www.kdnuggets.com/how-to-speed-up-python-code-with-caching)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何通过缓存加速 Python 代码](https://www.kdnuggets.com/how-to-speed-up-python-code-with-caching)'
- en: '[Personalized AI Made Simple: Your No-Code Guide to Adapting GPTs](https://www.kdnuggets.com/personalized-ai-made-simple-your-no-code-guide-to-adapting-gpts)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[个性化 AI 变得简单：你的无代码 GPTs 适配指南](https://www.kdnuggets.com/personalized-ai-made-simple-your-no-code-guide-to-adapting-gpts)'
- en: '[New Ways of Sharing Code Blocks for Data Scientists](https://www.kdnuggets.com/2022/03/new-ways-sharing-code-blocks.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学家共享代码块的新方式](https://www.kdnuggets.com/2022/03/new-ways-sharing-code-blocks.html)'
- en: '[7 Ways ChatGPT Makes You Code Better and Faster](https://www.kdnuggets.com/2023/06/7-ways-chatgpt-makes-code-better-faster.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatGPT 如何让你的代码更好更快的 7 种方法](https://www.kdnuggets.com/2023/06/7-ways-chatgpt-makes-code-better-faster.html)'
- en: '[5 Ways You Can Use ChatGPT''s Code Interpreter For Data Science](https://www.kdnuggets.com/2023/08/5-ways-chatgpt-code-interpreter-data-science.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你可以利用 ChatGPT 的代码解释器进行数据科学的 5 种方式](https://www.kdnuggets.com/2023/08/5-ways-chatgpt-code-interpreter-data-science.html)'
- en: '[RAPIDS cuDF to Speed up Your Next Data Science Workflow](https://www.kdnuggets.com/2023/04/rapids-cudf-speed-next-data-science-workflow.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RAPIDS cuDF 加速你下一个数据科学工作流程](https://www.kdnuggets.com/2023/04/rapids-cudf-speed-next-data-science-workflow.html)'
