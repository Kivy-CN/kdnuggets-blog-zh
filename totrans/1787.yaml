- en: 17 More Must-Know Data Science Interview Questions and Answers, Part 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/02/17-data-science-interview-questions-answers-part-2.html](https://www.kdnuggets.com/2017/02/17-data-science-interview-questions-answers-part-2.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Editor''s note:** See also part 1 of [**17 More Must-Know Data Science Interview
    Questions and Answers**](/2017/02/17-data-science-interview-questions-answers.html).
    This is part 2. Here is [Part 3](/2017/03/17-data-science-interview-questions-answers-part-3.html).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This post answers questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Q7\. What is overfitting and how to avoid it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q8\. What is the curse of dimensionality?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q9\. How can you determine which features are the most important in your model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q10\. When can parallelism make your algorithms run faster? When could it make
    your algorithms run slower?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q11\. What is the idea behind ensemble learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q12\. In unsupervised learning, if a ground truth about a dataset is unknown,
    how can we determine the most useful number of clusters to be?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Q7\. What is overfitting and how to avoid it?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Gregory Piatetsky**](/author/gregory-piatetsky) **answers:**'
  prefs: []
  type: TYPE_NORMAL
- en: '*(Note: this is a revised version of the answer given in* [*21 Must-Know Data
    Science Interview Questions and Answers, part 2*](/2016/02/21-data-science-interview-questions-answers-part2.html)*)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Overfitting](/tag/overfitting) is when you build a predictive model that fits
    the data "too closely",  so that it captures the random noise in the data rather
    than true patterns.  As a result, the model predictions will be wrong when applied
    to new data.'
  prefs: []
  type: TYPE_NORMAL
- en: We frequently hear about studies that report unusual results (especially if
    you listen to Wait Wait Don't Tell Me) , or see findings like "[an orange used
    car is least likely to be a lemon](/2017/01/siegel-data-science-avoiding-prediction-pitfall.html)",
     or learn that studies overturn previous established findings (eggs are no longer
    bad for you).
  prefs: []
  type: TYPE_NORMAL
- en: Many such studies produce questionable results that cannot be repeated.
  prefs: []
  type: TYPE_NORMAL
- en: This is a big problem, especially in social sciences or medicine, when researchers
     frequently commit the cardinal sin of Data Science - **[Overfitting the data](/tag/overfitting).**
  prefs: []
  type: TYPE_NORMAL
- en: The researchers test too many hypotheses without proper statistical control,
    until they happen to find something interesting. Then they report it.  Not surprisingly,
    next time the effect (which was partly due to chance) will be much smaller or
    absent.
  prefs: []
  type: TYPE_NORMAL
- en: These flaws of research practices were identified and reported by John P. A.
    Ioannidis in his landmark paper [*Why Most Published Research Findings Are False*](http://www.plosmedicine.org/article/info%3Adoi%2F10.1371%2Fjournal.pmed.0020124)
    (PLoS Medicine, 2005). Ioannidis found that very often either the results were
    exaggerated or the findings could not be replicated. In his paper, he presented
    statistical evidence that indeed most claimed research findings are false!
  prefs: []
  type: TYPE_NORMAL
- en: 'Ioannidis noted that in order for a research finding to be reliable, it should
    have:'
  prefs: []
  type: TYPE_NORMAL
- en: Large sample size and with large effects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Greater number of and lesser selection of tested relationship
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Greater flexibility in designs, definitions, outcomes, and analytical modes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimal bias due to financial and other factors (including popularity of that
    scientific field)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately, too often these rules were violated, producing spurious results,
    such as S&P 500 index strongly correlated to [production of butter in Bangladesh](/2016/02/21-data-science-interview-questions-answers-part2.html),
    or US spending on science, space and technology correlated with suicides by hanging,
    strangulation, and suffocation (from http://tylervigen.com/spurious-correlations)
  prefs: []
  type: TYPE_NORMAL
- en: '![Spurious correlations](../Images/6ff3e462b9916e8ad30efb4541b53330.png)'
  prefs: []
  type: TYPE_IMG
- en: '(Source: [Tylervigen.com](http://tylervigen.com/chart-pngs/1.png))'
  prefs: []
  type: TYPE_NORMAL
- en: See more strange and spurious findings at [Spurious correlations](http://www.tylervigen.com/discover)
    by Tyler Vigen or discover them by yourself using tools such as [Google correlate](https://www.google.com/trends/correlate/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Several methods can be used to avoid "overfitting" the data:'
  prefs: []
  type: TYPE_NORMAL
- en: Try to find the simplest possible hypothesis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Regularization](https://en.wikipedia.org/wiki/Regularization_%28mathematics%29)
    (adding a penalty for complexity)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Randomization Testing](/2014/02/3-ways-to-test-accuracy-your-predictive-models.html)
    (randomize the class variable, try your method on this data - if it find the same
    strong results, something is wrong)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nested cross-validation  (do feature selection on one level, then run entire
    method in cross-validation on outer level)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjusting the [False Discovery Rate](https://en.wikipedia.org/wiki/False_discovery_rate)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the [reusable holdout method](/2015/08/feldman-avoid-overfitting-holdout-adaptive-data-analysis.html)
    - a breakthrough approach proposed in 2015
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good data science is on the leading edge of scientific understanding of the
    world, and it is data scientists responsibility to avoid overfitting data and
    educate the public and the media on the dangers of bad data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'See also:'
  prefs: []
  type: TYPE_NORMAL
- en: '[4 Reasons Your Machine Learning Model is Wrong (and How to Fix It)](/2016/12/4-reasons-machine-learning-model-wrong.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[When Good Advice Goes Bad](/2016/03/when-good-advice-goes-bad.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Cardinal Sin of Data Mining and Data Science: Overfitting](/2014/06/cardinal-sin-data-mining-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Big Idea To Avoid Overfitting: Reusable Holdout to Preserve Validity in Adaptive
    Data Analysis](/2015/08/feldman-avoid-overfitting-holdout-adaptive-data-analysis.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Overcoming Overfitting with the reusable holdout: Preserving validity in adaptive
    data analysis](/2015/08/reusable-holdout-preserving-validity-adaptive-data-analysis.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11 Clever Methods of Overfitting and how to avoid them](/2015/01/clever-methods-overfitting-avoid.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Q8\. What is the curse of dimensionality?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[**Prasad Pore**](/author/prasad-pore) **answers:**'
  prefs: []
  type: TYPE_NORMAL
- en: '"As the number of features or dimensions grows, the amount of data we need
    to generalize accurately grows exponentially."'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- [Charles Isbell](http://ccsubs.com/video/yt%3AQZ0DtNFdDko/curse-of-dimensionality-georgia-tech-machine-learning/subtitles?lang=en),
    Professor and Senior Associate Dean, School of Interactive Computing, Georgia
    Tech'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s take an example below. Fig. 1 (a) shows 10 data points in one dimension
    i.e. there is only one feature in the data set. It can be easily represented on
    a line with only 10 values, x=1, 2, 3... 10.
  prefs: []
  type: TYPE_NORMAL
- en: But if we add one more feature, same data will be represented in 2 dimensions
    (Fig.1 (b)) causing increase in dimension space to 10*10 =100\. And again if we
    add 3rd feature, dimension space will increase to 10*10*10 = 1000\. As dimensions
    grows, dimensions space increases exponentially.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![n-dimensional space comparison](../Images/42858fce17f95d0c85abcb15027e0faa.png)'
  prefs: []
  type: TYPE_IMG
- en: This exponential growth in data causes high sparsity in the data set and unnecessarily
    increases storage space and processing time for the particular modelling algorithm.
    Think of image recognition problem of high resolution images 1280 × 720 = 921,600
    pixels i.e. 921600 dimensions. OMG. And that’s why it’s called **[Curse of Dimensionality](/?s=curse+of+dimensionality)**.
    Value added by additional dimension is much smaller compared to overhead it adds
    to the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Bottom line is, the data that can be represented using 10 space units of one
    true dimension, needs 1000 space units after adding 2 more dimensions just because
    we observed these dimensions during the experiment. The true dimension means the
    dimension which accurately generalize the data and observed dimensions means whatever
    other dimensions we consider in dataset which may or may not contribute to accurately
    generalize the data.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Q9\. How can you determine which features are the most important in your model?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[**Thuy Pham**](/author/thuy-pham) **answers:**'
  prefs: []
  type: TYPE_NORMAL
- en: In applied machine learning, success depends significantly on the quality of
    data representation (features).  Highly correlated features can make learning/sorting
    steps in the classification module easy. Conversely if label classes are a very
    complex function of the features, it can be impossible to build a good model [Dom
    2012]. Thus a so-called **[feature engineering](/tags/feature-engineering)**,
    a process of transforming data into features that are most relevant to the problem,
    is often needed.
  prefs: []
  type: TYPE_NORMAL
- en: A [**feature selection**](/tag/feature-selection) scheme often involves techniques
    to automatically select salient features from a large exploratory feature pool.
    Redundant and irrelevant features are well known to cause poor accuracy so discarding
    these features should be the first task. The relevance is often scored using mutual
    information calculation. Furthermore, input features should thus offer a high
    level of discrimination between classes. The separability of features can be measured
    by distance  or variance ratio between classes. One recent work [Pham 2016] proposed
    a systematic voting based feature selection that is a data-driven approach incorporating
    above criteria. This can be used as a common framework for a wide class of problems.
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature selection approach](../Images/52b9b5c9eb46050c3e235ecf3d98c8f6.png)'
  prefs: []
  type: TYPE_IMG
- en: A data-driven feature selection approach incorporating several saliency criteria
    [Pham 2016].
  prefs: []
  type: TYPE_NORMAL
- en: Another approach is penalizing on the features that are not very important (e.g.,
    yield a high error metric) when using regularization  methods like Lasso or Ridge.
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Dom 2012] P. Domingos. A few useful things to know about machine learning.
    *Communications of the ACM*, 55(10):78–87, 2012\. 2.4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pham 2016] T. T. Pham, C. Thamrin, P. D. Robinson, and P. H. W. Leong. Respiratory
    artefact removal in forced oscillation measurements: A machine learning approach.
    *Biomedical Engineering, IEEE Transactions on*, accepted, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Q10\. When can parallelism make your algorithms run faster? When could it make
    your algorithms run slower?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[**Anmol Rajpurohit**](https://twitter.com/hey_anmol) **answers:**'
  prefs: []
  type: TYPE_NORMAL
- en: Parallelism is a good idea when the task can be divided into sub-tasks that
    can be executed independent of each other without communication or shared resources.
    Even then, efficient implementation is key to achieving the benefits of parallelization.
    In real-life, most of the programs have some sections that need to be executed
    in serialized fashion, and the parallelizable sub-tasks need some kind of synchronization
    or data transfer. Thus, it is hard to predict whether parallelization will actually
    make the [**algorithm**](/tag/algorithms) run faster (than the serialized approach).
  prefs: []
  type: TYPE_NORMAL
- en: Parallelism would always have overhead compared to the compute cycles required
    to complete the task sequentially. At the minimum, this overhead will comprise
    of dividing the task into sub-tasks and compiling together the results of sub-tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '**The performance of parallelism against sequential computing is largely determined
    by how the time consumed by this overhead compares to the time saved due to parallelization.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: The overhead associated with parallelism is not just limited to the run-time
    of code, but also includes the extra time required for coding and debugging (parallelism
    versus sequential code).'
  prefs: []
  type: TYPE_NORMAL
- en: 'A widely-known theoretical approach to assessing the benefit of parallelization
    is Amdahl’s law, which gives the following formula to measure the speedup of running
    sub-tasks in parallel (over different processors) versus running them sequentially
    (on a single processor):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ffab2e095120202db6dca9392f1d551.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*S**latency*is the theoretical speedup of the execution of the whole task;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*s* is the speedup of the part of the task that benefits from improved system
    resources;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p *is the proportion of execution time that the part benefiting from improved
    resources originally occupied.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To understand the implication of Amdahl’s Law, look at the following figure
    that illustrates the theoretical speedup against an increasing number of processor
    cores, for tasks with different level of achievable parallelization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Speedup by number of cores](../Images/cc5d074b8a3261a73a6272f99642143c.png)'
  prefs: []
  type: TYPE_IMG
- en: It is important to note that not every program can be effectively parallelized.
    Rather, very few programs will scale with perfect speedups because of the limitations
    due to sequential portions, inter-communication costs, etc. Usually, large data
    sets form a compelling case for parallelization. However, it should not be assumed
    that parallelization would lead to performance benefits. Rather, the performance
    of parallelism and sequential should be compared on a sub-set of the problem,
    before investing effort into parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Q11\. What is the idea behind ensemble learning?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[**Prasad Pore**](/author/prasad-pore) **answers:**'
  prefs: []
  type: TYPE_NORMAL
- en: '"In [statistics](https://en.wikipedia.org/wiki/Statistics) and [machine learning](https://en.wikipedia.org/wiki/Machine_learning), **ensemble
    methods** use multiple learning algorithms to obtain better [predictive performance](https://en.wikipedia.org/wiki/Predictive_inference) than
    could be obtained from any of the constituent learning algorithms alone."'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – [Wikipedia](https://en.wikipedia.org/wiki/Ensemble_learning).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Imagine you are playing the game “Who wants to be millionaire?” and reached
    up to last question of 1 million dollars. You have no clue about the question,
    but you have audience poll and phone a friend life lines. Thank God. At this stage
    you don’t want to take any risk, so what will you do to get sure-shot right answer
    to become millionaire?
  prefs: []
  type: TYPE_NORMAL
- en: You will use both life lines, isn’t it? Let’s say 70% audience is saying right
    answer is D and your friend is also saying the right answer is D with 90% confidence
    because he is an expert in the area of the question. Use of both life lines gives
    you  an average 80% confidence that D is correct and gets you closer to becoming
    a millionaire.
  prefs: []
  type: TYPE_NORMAL
- en: This is the approach of [**ensemble methods**](/tag/ensemble-methods).
  prefs: []
  type: TYPE_NORMAL
- en: The famous [Netflix Prize](https://en.wikipedia.org/wiki/Netflix_Prize) competition
    took almost 3 years before the goal of 10% improvement [was reached](/news/2009/n14/1i.html).
     The winners used gradient boosted decision trees to combine over [500 models](http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/).
  prefs: []
  type: TYPE_NORMAL
- en: In ensemble methods, more diverse the models used, more robust will be the ultimate
    result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Different models used in ensemble improves overall variance from difference
    in population, difference in hypothesis generated, difference in algorithms used
    and difference in parametrization. There are main 3 widely used ensembles techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Boosting](/tag/boosting)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stacking
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So if you have different models built for same data and same response variable,
    you can use one of the above methods to build ensemble model. As every model used
    in the ensemble has its own performance measures, some of the models may perform
    better than ultimate ensemble model and some of them may perform poorer than or
    equal to ensemble model. But overall the ensemble methods will improve overall
    accuracy and stability of the model, although at the expense of model understandability.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more on ensemble methods see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Ensemble Methods: Elegant Techniques to Produce Improved Machine Learning
    Results](/2016/02/ensemble-methods-techniques-produce-improved-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Science Basics: An Introduction to Ensemble Learners](/2016/11/data-science-basics-intro-ensemble-learners.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Q12\. In unsupervised learning, if a ground truth about a dataset is unknown,
    how can we determine the most useful number of clusters to be?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[**Matthew Mayo**](/author/matt-mayo) **answers:**'
  prefs: []
  type: TYPE_NORMAL
- en: With **supervised** learning, the number of classes in a particular set of data
    is known outright, since each data instance in labeled as a member of a particular
    existent class. In the worst case, we can scan the class attribute and count up
    the number of unique entries which exist.
  prefs: []
  type: TYPE_NORMAL
- en: With **[unsupervised](/tag/unsupervised-learning)** learning, the idea of class
    attributes and explicit class membership does not exist; in fact, one of the dominant
    forms of unsupervised learning -- data clustering -- aims to approximate class
    membership by minimizing interclass instance similarity and maximizing intraclass
    similarity. A major drawback with clustering can be the requirement to provide
    the number of classes which exist in the unlabeled dataset at the onset, in some
    form or another. If we are lucky, we may know the data’s **ground truth** -- the
    actual number of classes -- beforehand. However, this is not always the case,
    for numerous reasons, one of which being that there may actually be no defined
    number of classes (and hence, clusters) in the data, with the whole point of the
    unsupervised learning task being to survey the data and attempt to impose some
    meaningful structure of optimal cluster and class numbers upon it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without knowing the ground truth of a dataset, then, how do we know what the
    optimal number of data clusters are? As one may expect, there are actually numerous
    methods to go about answering this question. We will have a look at 2 particular
    popular methods for attempting to answer this question: the elbow method and the
    silhouette method.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Elbow Method**'
  prefs: []
  type: TYPE_NORMAL
- en: The elbow method is often the best place to state, and is especially useful
    due to its ease of explanation and verification via visualization. The elbow method
    is interested in explaining variance as a function of cluster numbers (the *k*
    in *k*-means). By plotting the percentage of variance explained against *k*, the
    first *N* clusters should add significant information, explaining variance; yet,
    some eventual value of *k* will result in a much less significant gain in information,
    and it is at this point that the graph will provide a noticeable angle. This angle
    will be the optimal number of clusters, from the perspective of the elbow method,
  prefs: []
  type: TYPE_NORMAL
- en: It should be self-evident that, in order to plot this variance against varying
    numbers of clusters, varying numbers of clusters must be tested. Successive complete
    iterations of the clustering method must be undertaken, after which the results
    can be plotted and compared.
  prefs: []
  type: TYPE_NORMAL
- en: '![Elbow method](../Images/3edaf0c9413aad166acbf351aa84e46f.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image source](http://elf11.github.io/2015/08/18/Kmeans-analysis.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Silhouette Method**'
  prefs: []
  type: TYPE_NORMAL
- en: The silhouette method measures the similarity of an object to its own cluster
    -- called cohesion -- when compared to other clusters -- called separation. The
    silhouette value is the means for this comparison, which is a value of the range
    [-1, 1]; a value close to 1 indicates a close relationship with objects in its
    own cluster, while a value close to -1 indicates the opposite. A clustered set
    of data in a model producing mostly high silhouette values is likely an acceptable
    and appropriate model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Silhouette method](../Images/6e0efa5ccc574fd54efbf33c7defd138.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image source](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Read more on the silhouette method [here](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html).
    Find the specifics on computing a silhouette value [here](https://en.wikipedia.org/wiki/Silhouette_(clustering)).
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[17 More Must-Know Data Science Interview Questions and Answers](/2017/02/17-data-science-interview-questions-answers.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17 More Must-Know Data Science Interview Questions and Answers, part 3](/2017/03/17-data-science-interview-questions-answers-part3.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21 Must-Know Data Science Interview Questions and Answers](/2016/02/21-data-science-interview-questions-answers.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21 Must-Know Data Science Interview Questions and Answers, part 2](/2016/02/21-data-science-interview-questions-answers-part2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[20 Questions (with Answers) to Detect Fake Data Scientists: ChatGPT…](https://www.kdnuggets.com/2023/01/20-questions-detect-fake-data-scientists-chatgpt-1.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20 Questions (with Answers) to Detect Fake Data Scientists: ChatGPT…](https://www.kdnuggets.com/2023/02/20-questions-detect-fake-data-scientists-chatgpt-2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Data Analytics Interview Questions & Answers](https://www.kdnuggets.com/2022/09/7-data-analytics-interview-questions-answers.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Python Interview Questions & Answers](https://www.kdnuggets.com/2022/09/5-python-interview-questions-answers.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3 More SQL Aggregate Function Interview Questions for Data Science](https://www.kdnuggets.com/2023/01/3-sql-aggregate-function-interview-questions-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Science Interview Guide - Part 2: Interview Resources](https://www.kdnuggets.com/2022/04/data-science-interview-guide-part-2-interview-resources.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
