- en: Using Transfer Learning to Boost Model Performance
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用迁移学习来提升模型性能
- en: 原文：[https://www.kdnuggets.com/using-transfer-learning-to-boost-model-performance](https://www.kdnuggets.com/using-transfer-learning-to-boost-model-performance)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/using-transfer-learning-to-boost-model-performance](https://www.kdnuggets.com/using-transfer-learning-to-boost-model-performance)
- en: '![Using Transfer Learning to Boost Model Performance](../Images/bc3b0d8135629b0cddbd48a07929a790.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![使用迁移学习提升模型性能](../Images/bc3b0d8135629b0cddbd48a07929a790.png)'
- en: Have you thought about how the performance of your ML models can be enhanced
    without developing new models? That is where transfer learning comes into play.
    In this article, we will provide an overview of transfer learning along with its
    benefits and challenges.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否考虑过如何在不开发新模型的情况下提升机器学习模型的性能？这就是迁移学习发挥作用的地方。在本文中，我们将概述迁移学习及其优点和挑战。
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的快车道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行IT工作'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: What is Transfer Learning?
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是迁移学习？
- en: Transfer learning means that a model trained for one task can be used for another
    similar task. You can then use a pre-trained model and make changes in it according
    to the required task. Let’s discuss the stages in transfer learning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习意味着一个为一个任务训练的模型可以用于另一个类似的任务。然后你可以使用一个预训练模型，并根据所需任务对其进行修改。让我们讨论迁移学习的各个阶段。
- en: '![Stages in Transfer Learning](../Images/97b9b89eaa8576db606828d35e818a60.png)Image
    by Author'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![迁移学习的阶段](../Images/97b9b89eaa8576db606828d35e818a60.png)图片来源：作者'
- en: '**Choose a Pre-trained model**: Select a model that has been trained on a large
    dataset for a similar task to the one you want to work on.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择预训练模型**：选择一个在类似任务上用大型数据集训练过的模型。'
- en: '**Modify model architecture**: Adjust the final layers of the pre-trained model
    according to your specific task. Also, add new layers if needed.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**修改模型架构**：根据你的具体任务调整预训练模型的最终层。如果需要，还可以添加新的层。'
- en: '**Re-train the model**: Train the modified model on your new dataset. This
    allows the model to learn the details of your specific task. It also benefits
    from the features it learned during the original training.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**重新训练模型**：在你的新数据集上训练修改后的模型。这使得模型能够学习你特定任务的细节，同时也利用了在原始训练中学到的特征。'
- en: '**Fine-tune the model**: Unfreeze some of the pre-trained layers and continue
    training your model. This allows the model to better adapt to the new task by
    fine-tuning its weights.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**微调模型**：解冻一些预训练层，继续训练你的模型。这使得模型能够通过微调权重更好地适应新任务。'
- en: Benefits of Transfer Learning
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移学习的好处
- en: 'Transfer learning offers several significant advantages:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习提供了几个重要的优势：
- en: '**Saves Time and Resources**: Fine-tuning needs lesser time and computational
    resources as the pre-trained model has been initially trained for a large number
    of iterations for a specific dataset. This process has already captured essential
    features, so it reduces the workload for the new task.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**节省时间和资源**：微调需要的时间和计算资源较少，因为预训练模型已经针对特定数据集进行了大量迭代训练。这个过程已经捕捉了重要特征，因此减少了新任务的工作量。'
- en: '**Improves Performance**: Pre-trained models have learned from extensive datasets,
    so they generalize better. This leads to improved performance on new tasks, even
    when the new dataset is relatively small. The knowledge gained from the initial
    training helps in achieving higher accuracy and better results.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提高性能**：预训练模型从大量数据集中学习，因此具有更好的泛化能力。这使得在新的任务上，即使新数据集相对较小，也能提高性能。初始训练中获得的知识有助于实现更高的准确性和更好的结果。'
- en: '**Needs Less Data**: One of the major benefits of transfer learning is its
    effectiveness with smaller datasets. The pre-trained model has already acquired
    useful pattern and features information. Thus, it can perform fairly even if it
    is given few new data.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**需要更少的数据**：转移学习的一个主要优势是其在较小数据集上的有效性。预训练模型已经获得了有用的模式和特征信息。因此，即使给定的数据很少，它也能表现得相当不错。'
- en: Types of Transfer Learning
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转移学习的类型
- en: 'Transfer learning can be classified into three types:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习可以分为三种类型：
- en: Feature extraction
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征提取
- en: 'Feature extraction means using features learned by a model on new data. For
    instance, in image classification, we can utilize features from a predefined Convolutional
    Neural Network to search for significant features in images. Here’s an example
    using a pre-trained VGG16 model from Keras for image feature extraction:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取意味着使用模型在新数据上学到的特征。例如，在图像分类中，我们可以利用预定义卷积神经网络中的特征来搜索图像中的重要特征。以下是一个使用Keras中的预训练VGG16模型进行图像特征提取的示例：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Fine-tuning
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调
- en: 'Fine-tuning involves tweaking the feature extraction steps and the aspects
    of a new model matching the specific task. This method is most useful with a mid-sized
    data set and where you wish to enhance a particular task-related ability of the
    model. For example, in NLP, a standard BERT model might be adjusted or further
    trained on a small quantity of medical texts to accomplish medical entity recognition
    better. Here’s an example using BERT for sentiment analysis with fine-tuning on
    a custom dataset:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 微调涉及调整特征提取步骤以及新模型的某些方面，以匹配特定任务。这种方法在中等规模的数据集上最为有效，并且当你希望提升模型在特定任务上的能力时非常有用。例如，在自然语言处理（NLP）中，一个标准的BERT模型可能会被调整或在少量医学文本上进一步训练，以更好地完成医学实体识别。以下是一个使用BERT进行情感分析的示例，其中在自定义数据集上进行了微调：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Domain adaptation
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 领域适应
- en: 'Domain adaptation gives an insight on how one can utilize knowledge gained
    from the source domain that the pre-trained model was trained on to the different
    target domain. This is required when the source and target domains differ on the
    features, the data distribution, or even on the language. For instance, in sentiment
    analysis we may apply a sentiment classifier learned from product reviews into
    social media posts because the two uses very different language. Here’s an example
    using sentiment analysis, adapting from product reviews to social media posts:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 领域适应提供了如何利用从预训练模型的源领域获得的知识应用到不同目标领域的见解。当源领域和目标领域在特征、数据分布或甚至语言上有所不同时，这种方法是必需的。例如，在情感分析中，我们可能会将从产品评论中学到的情感分类器应用于社交媒体帖子，因为这两者使用了非常不同的语言。以下是一个将情感分析从产品评论适应到社交媒体帖子的示例：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Pre-trained Models
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练模型
- en: 'Pretrained models are models already trained on large datasets. They capture
    knowledge and patterns from extensive data. These models are used as a starting
    point for other tasks. Let’s discuss some of the common pre-trained models used
    in machine learning: applications.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型是已经在大规模数据集上训练过的模型。它们从广泛的数据中捕获知识和模式。这些模型被用作其他任务的起点。让我们讨论一些常见的在机器学习中使用的预训练模型应用。
- en: VGG (Visual Geometry Group)
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VGG（视觉几何组）
- en: The architecture of VGG include multiple layers of 3×3 convolutional filters
    and pooling layers. It is able to identify detailed features like edges and shapes
    in images. By training on large datasets, VGG learns to recognize different objects
    within images. It can used for object detection and image segmentation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: VGG的架构包括多个3×3的卷积滤波器层和池化层。它能够识别图像中的边缘和形状等详细特征。通过在大规模数据集上训练，VGG学会了识别图像中的不同物体。它可以用于目标检测和图像分割。
- en: '![VGG Architecture](../Images/e3db0a722f10834176f2a7757fcdb9d1.png) VGG-16
    | CNN model ( Source: [GeeksforGeeks](https://www.geeksforgeeks.org/vgg-16-cnn-model/))'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![VGG架构](../Images/e3db0a722f10834176f2a7757fcdb9d1.png) VGG-16 | CNN模型（来源：[GeeksforGeeks](https://www.geeksforgeeks.org/vgg-16-cnn-model/)）'
- en: ResNet (Residual Network)
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ResNet（残差网络）
- en: ResNet uses residual connections to train models. These connections make it
    easier for gradients to flow through the network. This prevents the vanishing
    gradient problem, helping the network train effectively. ResNet can successfully
    train models with hundreds of layers. ResNet is excellent for tasks such as image
    classification and face recognition.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet使用残差连接来训练模型。这些连接使梯度更容易在网络中流动。这可以防止梯度消失问题，帮助网络有效训练。ResNet能够成功训练具有数百层的模型。ResNet在图像分类和人脸识别等任务中表现优异。
- en: '![ResNet Architecture](../Images/4dab0f47c662b0fa5217df2c1c99755a.png) ResNet-50
    Architecture (Source: [Research Paper](https://www.researchgate.net/figure/An-illustration-of-ResNet-50-layers-architecture_fig1_350421671))'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![ResNet架构](../Images/4dab0f47c662b0fa5217df2c1c99755a.png) ResNet-50架构（来源：[研究论文](https://www.researchgate.net/figure/An-illustration-of-ResNet-50-layers-architecture_fig1_350421671)）'
- en: BERT (Bidirectional Encoder Representations from Transformers)
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BERT（双向编码器表示模型）
- en: BERT is used for natural language processing applications. It uses a transformer-based
    model to understand the context of words in a sentence. It learns to guess missing
    words and understand sentence meanings. BERT can be used for sentiment analysis,
    question answering and named entity recognition.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: BERT用于自然语言处理应用。它使用基于变换器的模型来理解句子中词汇的上下文。它学习猜测缺失的词汇并理解句子意义。BERT可用于情感分析、问答和命名实体识别。
- en: '![BERT Architecture](../Images/ffac8f197082ab0d997a44c8c92161d0.png) High-level
    View of the BERT Architecture (Source: [Research Paper](https://www.researchgate.net/figure/Simplified-high-level-view-of-the-BERT-architecture-7-8-The-word-tokens-are_fig2_355938108))'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![BERT架构](../Images/ffac8f197082ab0d997a44c8c92161d0.png) BERT架构的高层次视图（来源：[研究论文](https://www.researchgate.net/figure/Simplified-high-level-view-of-the-BERT-architecture-7-8-The-word-tokens-are_fig2_355938108)）'
- en: Fine-tuning Techniques
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调技术
- en: Layer Freezing
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 层冻结
- en: Layer freezing means choosing certain layers of a pre-trained model and preventing
    them from changing during training with new data. This is done to preserve the
    useful patterns and features the model learned from its original training. Typically,
    we freeze early layers that capture general features like edges in images or basic
    structures in text.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 层冻结意味着选择预训练模型中的某些层，并在用新数据训练时防止这些层发生变化。这是为了保留模型从原始训练中学到的有用模式和特征。通常，我们会冻结捕捉图像中边缘或文本中基本结构的早期层。
- en: Learning Rate Adjustment
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习率调整
- en: Tuning the learning rate is important to balance what the model has learned
    and new data. Usually, fine-tuning involves using a lower learning rate than in
    the initial training with large datasets. This helps the model adapt to new data
    while preserving most of its learned weights.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 调整学习率对于平衡模型已学到的内容和新数据非常重要。通常，微调时使用的学习率低于最初使用大数据集时的学习率。这有助于模型适应新数据，同时保持大部分已学习的权重。
- en: Challenges and Considerations
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 挑战与注意事项
- en: Let’s discuss the challenges of transfer learning and how to address them.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨迁移学习的挑战以及如何解决这些问题。
- en: '**Dataset Size and Domain Shift**: When fine-tuning, there should be abundant
    of data for the task concerned while fine-tuning generalized models. The drawback
    of this approach is that in case the new dataset is either small or significantly
    different from what fits the model at the beginning. To deal with this, one can
    put more data which will be more relevant to what the model already trained on.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据集大小和领域迁移**：在微调时，应当有充足的数据用于相关任务，同时微调通用模型时，这种方法的缺点是新的数据集可能较小或与模型最初适应的数据差异较大。为了解决这个问题，可以添加更多与模型已经训练过的数据更相关的数据。'
- en: '**Hyperparameter Tuning**: Changing hyperparameters is important when working
    with pre trained models. These parameters are dependent on each other and determine
    how good the model is going to be. Techniques such as grid search or automated
    tools to search for the most optimal settings for hyperparameters that would yield
    high performance on validation data.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**超参数调整**：在使用预训练模型时，调整超参数非常重要。这些参数相互依赖，决定了模型的表现如何。可以使用网格搜索等技术或自动化工具来寻找最优的超参数设置，从而在验证数据上获得高性能。'
- en: '**Computational Resources**: Fine-tuning of deep neural networks is computationally
    demanding because such models can have millions of parameters. For training and
    predicting the output, powerful accelerators like GPU or TPU are required. These
    demands are usually addressed by the cloud computing platforms.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算资源**：深度神经网络的微调计算上要求很高，因为这些模型可能拥有数百万个参数。训练和预测输出需要强大的加速器，如GPU或TPU。这些需求通常由云计算平台来解决。'
- en: Wrapping Up
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In conclusion, transfer learning stands as a cornerstone in the quest to enhance
    model performance across diverse applications of artificial intelligence. By leveraging
    pretrained models like VGG, ResNet, BERT, and others, practitioners can efficiently
    harness existing knowledge to tackle complex tasks in image classification, natural
    language processing, healthcare, autonomous systems, and beyond.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，迁移学习在提升人工智能模型性能的各个应用领域中，作为一块基石。通过利用像 VGG、ResNet、BERT 等预训练模型，实践者可以有效地利用现有知识来应对图像分类、自然语言处理、医疗保健、自动驾驶系统等复杂任务。
- en: '**[Jayita Gulati](https://www.linkedin.com/in/jayitagulati1998/)** is a machine
    learning enthusiast and technical writer driven by her passion for building machine
    learning models. She holds a Master''s degree in Computer Science from the University
    of Liverpool.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Jayita Gulati](https://www.linkedin.com/in/jayitagulati1998/)** 是一位对构建机器学习模型充满热情的机器学习爱好者和技术作家。她拥有利物浦大学计算机科学硕士学位。'
- en: More On This Topic
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Boost your machine learning model performance!](https://www.kdnuggets.com/2023/04/manning-boost-machine-learning-model-performance.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提升你的机器学习模型性能！](https://www.kdnuggets.com/2023/04/manning-boost-machine-learning-model-performance.html)'
- en: '[A Practical Guide to Transfer Learning using PyTorch](https://www.kdnuggets.com/2023/06/practical-guide-transfer-learning-pytorch.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 PyTorch 的迁移学习实用指南](https://www.kdnuggets.com/2023/06/practical-guide-transfer-learning-pytorch.html)'
- en: '[Monitor Model Performance in the MLOps Pipeline with Python](https://www.kdnuggets.com/2023/05/monitor-model-performance-mlops-pipeline-python.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 监控 MLOps 流水线中的模型性能](https://www.kdnuggets.com/2023/05/monitor-model-performance-mlops-pipeline-python.html)'
- en: '[Strategies for Optimizing Performance and Costs When Using Large…](https://www.kdnuggets.com/strategies-for-optimizing-performance-and-costs-when-using-large-language-models-in-the-cloud)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用大型语言模型时优化性能和成本的策略](https://www.kdnuggets.com/strategies-for-optimizing-performance-and-costs-when-using-large-language-models-in-the-cloud)'
- en: '[TensorFlow for Computer Vision - Transfer Learning Made Easy](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow 在计算机视觉中的应用 - 迁移学习变得简单](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)'
- en: '[What is Transfer Learning?](https://www.kdnuggets.com/2022/01/transfer-learning.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是迁移学习？](https://www.kdnuggets.com/2022/01/transfer-learning.html)'
