- en: What is Softmax Regression and How is it Related to Logistic Regression?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html](https://www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Softmax Regression (synonyms: Multinomial Logistic, Maximum Entropy Classifier,
    or just Multi-class Logistic Regression) is a generalization of logistic regression
    that we can use for multi-class classification (under the assumption that the
    classes are mutually exclusive). In contrast, we use the (standard) Logistic Regression
    model in binary classification tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let me briefly explain how that works and how softmax regression differs
    from logistic regression. I have a more detailed explanation on logistic regression
    here: [LogisticRegression - mlxtend](http://rasbt.github.io/mlxtend/user_guide/classifier/LogisticRegression/) ,
    but let me re-use one of the figures to make things more clear:'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/f9ab2178b82863a5f535d5fc68c71052.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/logistic_regression_schematic.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'As the name suggests, in softmax regression (SMR), we replace the sigmoid logistic
    function by the so-called *softmax function* φ:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/563c1d77e93e9c16ce2d36ea50e28dc5.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: where we define the net input z as
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/d6d875c14ceed22ddb6fd7edd3b8a778.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (*w* is the weight vector, *x* is the feature vector of 1 training sample, and *w0* is
    the bias unit.)
  prefs: []
  type: TYPE_NORMAL
- en: Now, this softmax function computes the probability that this training sample
    x(i) belongs to class *j* given the weight and net input z(i). So, we compute
    the probability *p(y = j | x(i); wj)* for each class label in *j = 1, ..., k*.
    Note the normalization term in the denominator which causes these class probabilities
    to sum up to one.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the concept of softmax, let us walk through a concrete example.
    Let's assume we have a training set consisting of 4 samples from 3 different classes
    (0, 1, and 2).
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/23f0485b8fb6cbdfdd4bbbee24d2aa8d.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/3.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we want to encode the class labels into a format that we can more easily
    work with; we apply one-hot encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/70781052d5025bf09b5fb487e26961f2.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/4.png)'
  prefs: []
  type: TYPE_NORMAL
- en: A sample that belongs to class 0 (the first row) has a 1 in the first cell,
    a sample that belongs to class 2 has a 1 in the second cell of its row, and so
    forth. Next, let us define the feature matrix of our 4 training samples. Here,
    we assume that our dataset consists of 2 features; thus, we create a 4×(2+1) dimensional
    matrix (+1 one for the bias term).
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/f6c45705f182375ba0180fef08793dfe.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/5.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we created a (2+1)×3 dimensional weight matrix (one row per feature
    and one column for each class).
  prefs: []
  type: TYPE_NORMAL
- en: To compute the net input, we multiply the 4×(2+1) feature matrix X with the
    (2+1)×3 (n_features × n_classes) weight matrixW.
  prefs: []
  type: TYPE_NORMAL
- en: Z = WX
  prefs: []
  type: TYPE_NORMAL
- en: which yields a 4×3 output matrix (n_samples × n_classes).
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/9166d75da035d87587891f4f4d7585d5.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/6.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it''s time to compute the softmax activation that we discussed earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/9ff62d7a0369f7ce71ba5fa06066e8d8.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/7.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/81f5eadda13299d3f2f86cb2bf829b1c.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/8.png)'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the values for each sample (row) nicely sum up to 1 now. E.g.,
    we can say that the first sample
  prefs: []
  type: TYPE_NORMAL
- en: '`[ 0.29450637 0.34216758 0.36332605]` has a 29.45% probability to belong to
    class 0\. Now, in order to turn these probabilities back into class labels, we
    could simply take the argmax-index position of each row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/bc0600f0e12a2a1cafce9e75e6f3b0ef.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/9.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, our predictions are terribly wrong, since the correct class
    labels are `[0, 1, 2, 2]`. Now, in order to train our logistic model (e.g., via
    an optimization algorithm such as gradient descent), we need to define a cost
    function *J* that we want to minimize:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/e3bfeb7a326f23f9a395151da4ab1591.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/10.png)'
  prefs: []
  type: TYPE_NORMAL
- en: which is the average of all cross-entropies over our n training samples. The
    cross-entropy function is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/6944aec9fb70dff09389cbe416878974.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/11.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Here the T stands for "target" (the true class labels) and the O stands for
    output (the computed probability via softmax; *not*the predicted class label).
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/2c2a95ee44b4571b0d77e5820be7bb51.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/12.png)'
  prefs: []
  type: TYPE_NORMAL
- en: In order to learn our softmax model via gradient descent, we need to compute
    the derivative
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/06232355e2240b05d66bd77d89e4aec9.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/13.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'which we then use to update the weights in opposite direction of the gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/256ab5a0ad5a3b30c58c7801f6c7e185.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/14.png) for
    each class j.'
  prefs: []
  type: TYPE_NORMAL
- en: '(Note that w_j is the weight vector for the class *y=j*.) I don''t want to
    walk through more tedious details here, but this cost derivative turns out to
    be simply:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/2c45b481ea49ca43da98e7b2ec62ed1b.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/15.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Using this cost gradient, we iteratively update the weight matrix until we reach
    a specified number of epochs (passes over the training set) or reach the desired
    cost threshold.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Sebastian Raschka](https://twitter.com/rasbt)** is a ''Data Scientist''
    and Machine Learning enthusiast with a big passion for Python & open source. Author
    of ''[Python Machine Learning](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning)''.
    Michigan State University.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression.md).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[When Does Deep Learning Work Better Than SVMs or Random Forests?](/2016/04/deep-learning-vs-svm-random-forest.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Development of Classification as a Learning Machine](/2016/04/development-classification-learning-machine.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Why Implement Machine Learning Algorithms From Scratch?](/2016/05/implement-machine-learning-algorithms-scratch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Data Science Definition Humor: A Collection of Quirky Quotes…](https://www.kdnuggets.com/2022/02/data-science-definition-humor.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Comparing Linear and Logistic Regression](https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Classification Metrics Walkthrough: Logistic Regression with…](https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[An Overview of Logistic Regression](https://www.kdnuggets.com/2022/02/overview-logistic-regression.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear vs Logistic Regression: A Succinct Explanation](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News 22:n12, March 23: Best Data Science Books for…](https://www.kdnuggets.com/2022/n12.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
