- en: Top 10 Statistics Mistakes Made by Data Scientists
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/06/statistics-mistakes-data-scientists.html](https://www.kdnuggets.com/2019/06/statistics-mistakes-data-scientists.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By [Norman Niemer](https://www.linkedin.com/in/normanniemer/), Chief Data
    Scientist**'
  prefs: []
  type: TYPE_NORMAL
- en: A data scientist is a "person who is better at statistics than any software
    engineer and better at software engineering than any statistician". In [Top 10
    Coding Mistakes Made by Data Scientists](/2019/04/top-10-coding-mistakes-data-scientists.html) we
    discussed how statisticians can become better coders. Here we discuss how coders
    can become better statisticians.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Detailed output and code for each of the examples is available in [github](http://tiny.cc/top10-mistakes-stats-code) and
    in an [interactive notebook](http://tiny.cc/top10-mistakes-stats-bind). The code
    uses workflow management library [d6tflow](https://github.com/d6t/d6tflow) and
    data is shared with dataset management library [d6tpipe](https://github.com/d6t/d6tpipe).
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Not fully understanding the objective function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data scientists want to build the "best" model. But beauty is in the eye of
    the beholder. If you don't know what the goal and objective function is and how
    it behaves, it is unlikely you will be able to build the "best" model. And fwiw
    the objective may not even be a mathematical function but perhaps improving a
    business metric.
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution**: most Kaggle winners spend a lot of time understanding the objective
    function and how the data and model relates to the objective function. If you
    are optimizing a business metric, map it to an appropriate mathematical objective
    function.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**: F1 score is typically used to assess classification models. We
    once built a classification model whose success depended on the % of occurrences
    it was right. The F1 score was misleading because it shows the model was correct
    ~60% of the time whereas in fact it was correct only 40% of the time.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Not having a hypothesis on why something should work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Commonly data scientists want to build "models". They heard XGBoost and random
    forests work best so lets use those. They read about deep learning, maybe that
    will improve results further. They throw models at the problem without having
    looked at the data and without having formed a hypothesis which model is most
    likely to best capture the features of the data. It makes explaining your work
    really hard too because you are just randomly throwing models at data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution**: look at the data! Understand its characteristics and form a hypothesis
    which model is likely to best capture those characteristics.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**: Without running any models by just plotting this sample data,
    you can already have a strong view that x1 is linearly related with y and x2 doesn''t
    have much of a relationship with y. [![Example2](../Images/4772ae8f46652bc6cc97546ab86c7b98.png)](https://github.com/d6t/d6t-python/blob/master/blogs/images/top10-stats-example2.png?raw=true)'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Not looking at the data before interpreting results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another problem with not looking at the data is that your results can be heavily
    driven by outliers or other artifacts. This is especially true for models that
    minimize squared sums. Even without outliers, you can have problems with imbalanced
    datasets, clipped or missing values and all sorts of other weird artifacts of
    real-life data that you didn't see in the classroom.
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution**: it''s so important it''s worth repeating: look at the data! Understand
    how the nature of the data is impacting model results.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**: with outliers, x1 slope changed from 0.906 to -0.375! [![Example3](../Images/752bcb7220bcfeb65d1de112c8a96043.png)](https://github.com/d6t/d6t-python/blob/master/blogs/images/top10-stats-example3.png?raw=true)'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Not having a naive baseline model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Modern ML libraries almost make it too easy... Just change a line of code and
    you can run a new model. And another. And another. Error metrics are decreasing,
    tweak parameters - great - error metrics are decreasing further... With all the
    model fanciness, you can forget the dumb way of forecasting data. And without
    that naive benchmark, you don't have a good absolute comparison for how good your
    models are, they may all be bad in absolute terms.
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution**: what''s the dumbest way you can predict a value? Build a "model"
    using the last known value, the (rolling) mean or some constant eg 0\. Compare
    your model performance against a zero-intelligence forecast monkey!'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**: With this time series dataset, model1 must be better than model2
    with MSE of 0.21 and 0.45 respectively. But wait! By just taking the last known
    value, the MSE drops to 0.003!'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 5\. Incorrect out-sample testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the one that could derail your career! The model you built looked great
    in R&D but performs horrible in production. The model you said will do wonders
    is causing really bad business outcomes, potentially costing the company $m+.
    It's so important all the remaining mistakes bar the last one focus on it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution**: Make sure you''ve run your model in realistic outsample conditions
    and understand when it will perform well and when it doesn''t.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**: In-sample the random forest does a lot better than linear regression
    with mse 0.048 vs ols mse 0.183 but out-sample it does a lot worse with mse 0.259
    vs linear regression mse 0.187\. The random forest overtrained and would not perform
    well live in production!'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '6\. Incorrect out-sample testing: applying preprocessing to full dataset'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You probably know that powerful ML models can overtrain. Overtraining means
    it performs well in-sample but badly out-sample. So you need to be aware of having
    training data leak into test data. If you are not careful, any time you do feature
    engineering or cross-validation, train data can creep into test data and inflate
    model performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution**: make sure you have a true test set free of any leakage from training
    set. Especially beware of any time-dependent relationships that could occur in
    production use.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**: This happens a lot. Preprocessing is applied to the full dataset
    BEFORE it is split into train and test, meaning you do not have a true test set.
    Preprocessing needs to be applied separately AFTER data is split into train and
    test sets to make it a true test set. The MSE between the two methods (mixed out-sample
    CV mse 0.187 vs true out-sample CV mse 0.181) in this case is not all that different
    because the distributional properties between train and test are not that different
    but that might not always be the case.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '7\. Incorrect out-sample testing: cross-sectional data & panel data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You were taught cross-validation is all you need. sklearn even provides you
    some nice convenience functions so you think you have checked all the boxes. But
    most cross-validation methods do random sampling so you might end up with training
    data in your test set which inflates performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution**: generate test data such that it accurately reflects data on which
    you would make predictions in live production use. Especially with time series
    and panel data you likely will have to generate custom cross-validation data or
    do roll-forward testing.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**: here you have panel data for two different entities (eg companies)
    which are cross-sectionally highly correlated. If you randomly split data you
    make accurate predictions using data you did not actually have available during
    test, overstating model performance. You think you avoided mistake #5 by using
    cross-validation and found the random forest performs a lot better than linear
    regression in cross-validation. But running a roll-forward out-sample test which
    prevents future data from leaking into test, it performs a lot worse again! (random
    forest MSE goes from 0.047 to 0.211, higher than linear regression!)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 8\. Not considering which data is available at point of decision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you run a model in production, it gets fed with data that is available
    when you run the model. That data might be different than what you assumed to
    be available in training. For example the data might be published with delay so
    by the time you run the model other inputs have changed and you are making predictions
    with wrong data or your true y variable is incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution**: do a rolling out-sample forward test. If I had used this model
    in production, what would my training data look like, ie what data do you have
    to make predictions? That''s the training data you use to make a true out-sample
    production test. Furthermore, think about if you acted on the prediction, what
    result would that generate at the point of decision?'
  prefs: []
  type: TYPE_NORMAL
- en: 9\. Subtle Overtraining
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The more time you spend on a dataset, the more likely you are to overtrain it.
    You keep tinkering with features and optimizing model parameters. You used cross-validation
    so everything must be good.
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution**: After you have finished building the model, try to find another
    "version" of the datasets that can be a surrogate for a true out-sample dataset.
    If you are a manager, deliberately withhold data so that it does not get used
    for training.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**: Applying the models that were trained on dataset 1 to dataset
    2 shows the MSEs more than doubled. Are they still acceptable...? This is a judgement
    call but your results from #4 might help you decide.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 10\. "need more data" fallacy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Counterintuitively, often the best way to get started analyzing data is by working
    on a representative sample of the data. That allows you to familiarize yourself
    with the data and build the data pipeline without waiting for data processing
    and model training. But data scientists seem not to like that - more data is better.
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution**: start working with a small representative sample and see if you
    can get something useful out of it. Give it back to the end user, can they use
    it? Does it solve a real pain point? If not, the problem is likely not because
    you have too little data but with your approach.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Norman Niemer](https://www.linkedin.com/in/normanniemer/)** is the
    Chief Data Scientist at a large asset manager where he delivers data-driven investment
    insights. He holds a MS Financial Engineering from Columbia University and a BS
    in Banking and Finance from Cass Business School (London).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://github.com/d6t/d6t-python/blob/master/blogs/top10-mistakes-statistics.md).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Top 10 Coding Mistakes Made by Data Scientists](/2019/04/top-10-coding-mistakes-data-scientists.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Statistical Mistakes Even Scientists Make](/2017/10/statistical-mistakes-even-scientists-make.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Vital Statistics You Never Learned… Because They’re Never Taught](/2017/08/vital-statistics-never-learned-never-taught.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
