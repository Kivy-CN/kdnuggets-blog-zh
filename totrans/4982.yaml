- en: 7 More Steps to Mastering Machine Learning With Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 掌握 Python 机器学习的 7 个步骤
- en: 原文：[https://www.kdnuggets.com/2017/03/seven-more-steps-machine-learning-python.html/2](https://www.kdnuggets.com/2017/03/seven-more-steps-machine-learning-python.html/2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/03/seven-more-steps-machine-learning-python.html/2](https://www.kdnuggets.com/2017/03/seven-more-steps-machine-learning-python.html/2)
- en: 'Step 4: More Ensemble Methods'
  id: totrans-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 4 步：更多集成方法
- en: 'The first post only dealt with a single ensemble method: Random Forests (RF).
    RF has seen a great deal of success over the years as a top performing classifier,
    but it is certainly not the only ensemble classifier available. We will have a
    look at bagging, boosting, and voting.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 第一篇文章仅涉及单一的集成方法：随机森林（RF）。多年来，RF 作为一种顶级分类器取得了巨大成功，但它肯定不是唯一的集成分类器。我们将查看 bagging、boosting
    和 voting。
- en: '![Boosting](../Images/658f99083c93080b92c1f86c7f94ecf7.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![Boosting](../Images/658f99083c93080b92c1f86c7f94ecf7.png)'
- en: Give me a boost.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 给我一个提升。
- en: '* * *'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的 IT 需求'
- en: '* * *'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'First, read these overviews of ensemble learners, the first being general,
    and the second being as they relate to Scikit-learn:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，阅读这两个集成学习者的概述，第一个是一般性的，第二个是与 Scikit-learn 相关的：
- en: '[An Introduction to Ensemble Learners](/2016/11/data-science-basics-intro-ensemble-learners.html),
    by Matthew Mayo'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集成学习者简介](/2016/11/data-science-basics-intro-ensemble-learners.html)，作者：Matthew
    Mayo'
- en: '[Ensemble methods in Scikit-learn](http://scikit-learn.org/stable/modules/ensemble.html),
    Scikit-learn documentation'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Scikit-learn 中的集成方法](http://scikit-learn.org/stable/modules/ensemble.html)，Scikit-learn
    文档'
- en: 'Then, before moving on to new ensemble methods, get back up to speed on Random
    Forests with a fresh tutorial here:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在转向新的集成方法之前，先通过这里的新教程重新熟悉随机森林：
- en: '[Random Forests in Python](/2016/12/random-forests-python.html), from Yhat'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 中的随机森林](/2016/12/random-forests-python.html)，来自 Yhat'
- en: Bagging, boosting, and voting are all different forms of ensemble classifiers.
    All involve building multiple models; however, what **algorithms** the models
    are built from, the **data** which the models use, and how the results are ultimately
    **combined** differ between schemes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging、boosting 和 voting 都是不同形式的集成分类器。它们都涉及构建多个模型；然而，**算法**、**数据**和最终如何**组合**结果在这些方案中有所不同。
- en: '**Bagging** builds multiple models from the same classification algorithm,
    while using different (independent) samples of data from the training set -- Scikit-learn
    implements BaggingClassifier'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Bagging** 从相同的分类算法构建多个模型，同时使用来自训练集的不同（独立）数据样本 -- Scikit-learn 实现了 BaggingClassifier'
- en: '**Boosting** builds multiple models from the same classification algorithm,
    chaining models one after another in order to boost the learning of each subsequent
    model -- Scikit-learn implements AdaBoost'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Boosting** 从相同的分类算法构建多个模型，将模型一个接一个地串联，以提升每个后续模型的学习效果 -- Scikit-learn 实现了
    AdaBoost'
- en: '**Voting** builds multiple models from different classification algorithms,
    and a criteria is used to determine how the models are best combined -- Scikit-learn
    implements VotingClassifier'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Voting** 从不同的分类算法构建多个模型，并使用标准来确定如何最好地组合这些模型 -- Scikit-learn 实现了 VotingClassifier'
- en: 'So, why combine models? To approach this question from one specific angle,
    here is an overview of the [bias-variance trade-off](/2016/08/bias-variance-tradeoff-overview.html)
    as related specifically to boosting, from the Scikit-learn documentation:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么要组合模型？从一个具体的角度来看，以下是 [偏差-方差权衡](/2016/08/bias-variance-tradeoff-overview.html)
    的概述，特别是与提升（boosting）相关的内容，来自 Scikit-learn 文档：
- en: '[Single estimator versus bagging: bias-variance decomposition](http://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html),
    Scikit-learn documentation'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[单一估计器与自助法：偏差-方差分解](http://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html)，Scikit-learn
    文档'
- en: 'Now that you have read some introductory material on ensemble learners in general,
    and have a basic understanding of a few specific ensemble classifiers, follow
    this introduction to implementing the ensemble classifiers in Python using Scikit-learn,
    from Machine Learning Mastery:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经阅读了一些关于集成学习的一般介绍材料，并对几个具体的集成分类器有了基本的了解，请跟随这个介绍，学习如何使用 Scikit-learn 在 Python
    中实现集成分类器，参考 Machine Learning Mastery 的内容：
- en: '[Ensemble Machine Learning Algorithms in Python with scikit-learn](http://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/),
    by Jason Brownlee'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 中的集成机器学习算法与 scikit-learn](http://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/)，作者：Jason
    Brownlee'
- en: 'Step 5: Gradient Boosting'
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 5 步：梯度提升
- en: Our next step keeps us in the realm of ensemble classifiers, focusing on one
    of the most popular contemporary machine learning algorithms. Gradient boosting
    has made a noticeable impact in machine learning in the recent past, becoming
    (of note) one of the most utilized and successful algorithms in Kaggle competitions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一步仍然在集成分类器的领域，重点关注当代最受欢迎的机器学习算法之一。梯度提升在最近的机器学习中产生了显著影响，成为（值得注意的）Kaggle 竞赛中最常用和成功的算法之一。
- en: '![Gradient boosting](../Images/8a508ad2c2ca40aaed56f1ad7dfa9128.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![梯度提升](../Images/8a508ad2c2ca40aaed56f1ad7dfa9128.png)'
- en: Give me a gradient boost.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 给我一个梯度提升。
- en: 'First, read an overview of what gradient boosting is:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，阅读梯度提升的概述：
- en: '[Gradient boosting on Wikipedia](https://en.wikipedia.org/wiki/Gradient_boosting)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[维基百科上的梯度提升](https://en.wikipedia.org/wiki/Gradient_boosting)'
- en: 'Next, read up on why gradient boosting is the "most winning-est" approach to
    Kaggle competitions:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，了解为什么梯度提升是 Kaggle 竞赛中“最成功”的方法：
- en: '[Why does Gradient boosting work so well for so many Kaggle problems? on Quora](https://www.quora.com/Why-does-Gradient-boosting-work-so-well-for-so-many-Kaggle-problems)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么梯度提升对许多 Kaggle 问题效果这么好？在 Quora 上](https://www.quora.com/Why-does-Gradient-boosting-work-so-well-for-so-many-Kaggle-problems)'
- en: '[A Kaggle Master Explains Gradient Boosting](https://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/),
    by Ben Gorman'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一位 Kaggle 大师解释梯度提升](https://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)，作者：Ben
    Gorman'
- en: While Scikit-learn has its own gradient boosting implementation, we will diverge
    somewhat and use the [XGBoost library](https://github.com/dmlc/xgboost), which
    has [been noted](http://www.jmlr.org/proceedings/papers/v42/chen14.pdf) to be
    a faster implementation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Scikit-learn 自带梯度提升实现，我们将有所不同，使用 [XGBoost 库](https://github.com/dmlc/xgboost)，因为
    [它被指出](http://www.jmlr.org/proceedings/papers/v42/chen14.pdf)是更快速的实现。
- en: 'The following links provide some additional info on the XGBoost library, as
    well as gradient boosting (out of necessity):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接提供了有关 XGBoost 库以及梯度提升的一些额外信息（出于必要）：
- en: '[XGBoost on Wikipedia](https://en.wikipedia.org/wiki/Xgboost)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost 在维基百科上的介绍](https://en.wikipedia.org/wiki/Xgboost)'
- en: '[XGBoost library on Github](https://github.com/dmlc/xgboost)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Github 上的 XGBoost 库](https://github.com/dmlc/xgboost)'
- en: '[XGBoost documentation](https://xgboost.readthedocs.io/en/latest/model.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost 文档](https://xgboost.readthedocs.io/en/latest/model.html)'
- en: 'Now, follow this tutorial which brings it all together:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，跟随这个教程，将所有内容汇总在一起：
- en: '[A Guide to Gradient Boosted Trees with XGBoost in Python](https://jessesw.com/XG-Boost/),
    by Jesse Steinweg-Woods'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 的 XGBoost 梯度提升树指南](https://jessesw.com/XG-Boost/)，作者：Jesse Steinweg-Woods'
- en: 'You can also follow these more concise examples for reinforcement:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以跟随这些更简洁的示例进行巩固：
- en: '[XGBoost Example (Python) on Kaggle](https://www.kaggle.com/cbrogan/titanic/xgboost-example-python)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost 示例（Python）在 Kaggle 上](https://www.kaggle.com/cbrogan/titanic/xgboost-example-python)'
- en: '[Iris Dataset and XGBoost Simple Tutorial](http://ieva.rocks/2016/08/25/iris_dataset_and_xgboost_simple_tutorial/),
    by Ieva Zarina'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[鸢尾数据集和 XGBoost 简单教程](http://ieva.rocks/2016/08/25/iris_dataset_and_xgboost_simple_tutorial/)，作者：Ieva
    Zarina'
- en: 'Step 6: More Dimensionality Reduction'
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 6 步：更多降维
- en: '[Dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction)
    is the act of reducing the variables being used for model building from its initial
    number to a reduced number, by utilizing processes to obtain a set of **principal
    variables**.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[降维](https://en.wikipedia.org/wiki/Dimensionality_reduction)是通过利用过程将用于模型构建的变量从初始数量减少到减少后的数量，以获得一组**主变量**。'
- en: 'There are 2 major forms of dimensionality reduction:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 降维有 2 种主要形式：
- en: '[Feature selection](https://en.wikipedia.org/wiki/Feature_selection) - selecting
    a subset of relevant features'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[特征选择](https://en.wikipedia.org/wiki/Feature_selection) - 选择相关特征的子集'
- en: '[Feature extraction](https://en.wikipedia.org/wiki/Feature_extraction) - building
    an informative and non-redundant set of derived values features'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[特征提取](https://en.wikipedia.org/wiki/Feature_extraction) - 构建一个信息丰富且不冗余的派生特征值集'
- en: The following is a look at a pair of common feature extraction methods.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对一对常见特征提取方法的介绍。
- en: '![LDA Iris dataset](../Images/64b868a81463c323186ccc586b238ac0.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![LDA Iris 数据集](../Images/64b868a81463c323186ccc586b238ac0.png)'
- en: LDA of Iris dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Iris 数据集的 LDA。
- en: '[**Principal component analysis (PCA)**](https://en.wikipedia.org/wiki/Principal_component_analysis)
    is a statistical procedure that uses an orthogonal transformation to convert a
    set of observations of possibly correlated variables into a set of values of linearly
    uncorrelated variables called principal components. The number of principal components
    is less than or equal to the number of original variables. This transformation
    is defined in such a way that the first principal component has the largest possible
    variance (that is, accounts for as much of the variability in the data as possible)[.]'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[**主成分分析（PCA）**](https://en.wikipedia.org/wiki/Principal_component_analysis)
    是一种统计程序，使用正交变换将可能相关变量的观测集转换为称为主成分的线性无关变量集。主成分的数量小于或等于原始变量的数量。该变换被定义为使第一个主成分具有最大的可能方差（即解释数据中尽可能多的变异）[.] '
- en: 'The above definition comes from the [PCA Wikipedia entry](https://en.wikipedia.org/wiki/Principal_component_analysis),
    which you can read further if interested. However, the following overview-slash-tutorial
    is very thorough:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 上述定义来自 [PCA 维基百科条目](https://en.wikipedia.org/wiki/Principal_component_analysis)，如果感兴趣，可以进一步阅读。然而，以下概述式教程非常全面：
- en: '[Principal Component Analysis in 3 Simple Steps](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html),
    by Sebastian Raschka'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3 个简单步骤中的主成分分析](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html)，作者：Sebastian
    Raschka'
- en: '[**Linear discriminant analysis (LDA)**](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)
    is a generalization of Fisher''s linear discriminant, a method used in statistics,
    pattern recognition and machine learning to find a linear combination of features
    that characterizes or separates two or more classes of objects or events. The
    resulting combination may be used as a linear classifier, or, more commonly, for
    dimensionality reduction before later classification.'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[**线性判别分析（LDA）**](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)
    是费舍尔线性判别的推广，这是一种用于统计学、模式识别和机器学习的方法，旨在找到特征的线性组合，以表征或区分两个或多个类别的对象或事件。得到的组合可以用作线性分类器，或者更常见的是，在后续分类之前用于降维。'
- en: ''
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: LDA is closely related to analysis of variance (ANOVA) and regression analysis,
    which also attempt to express one dependent variable as a linear combination of
    other features or measurements. However, ANOVA uses categorical independent variables
    and a continuous dependent variable, whereas discriminant analysis has continuous
    independent variables and a categorical dependent variable (i.e. the class label)
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: LDA 与方差分析（ANOVA）和回归分析密切相关，这些方法也试图将一个因变量表示为其他特征或测量的线性组合。然而，ANOVA 使用分类的自变量和连续的因变量，而判别分析具有连续的自变量和分类的因变量（即类别标签）。
- en: 'The above definition is also from Wikipedia. And again, the following read
    is thorough:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 上述定义也来自维基百科。此外，以下内容的阅读也很全面：
- en: '[Linear Discriminant Analysis – Bit by Bit](http://sebastianraschka.com/Articles/2014_python_lda.html),
    by Sebastian Raschka'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性判别分析 – 一步步详解](http://sebastianraschka.com/Articles/2014_python_lda.html)，作者：Sebastian
    Raschka'
- en: 'Are you at all confused as to the practical difference between PCA and LDA
    for dimensionality reduction? [Sebastian Raschka clarifies](http://sebastianraschka.com/Articles/2014_python_lda.html):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于降维，您是否对 PCA 和 LDA 之间的实际区别感到困惑？[Sebastian Raschka 阐明了](http://sebastianraschka.com/Articles/2014_python_lda.html)：
- en: Both Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA)
    are linear transformation techniques that are commonly used for dimensionality
    reduction. PCA can be described as an “unsupervised” algorithm, since it “ignores”
    class labels and its goal is to find the directions (the so-called principal components)
    that maximize the variance in a dataset. In contrast to PCA, LDA is “supervised”
    and computes the directions (“linear discriminants”) that will represent the axes
    that that maximize the separation between multiple classes.
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For a concise elaboration on this, read the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[What is the difference between LDA and PCA for dimensionality reduction?](https://sebastianraschka.com/faq/docs/lda-vs-pca.html),
    by Sebastian Raschka'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 7: More Deep Learning'
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The original 7 Steps... post provided an entry point to neural networks and
    deep learning. If you have made it this far without much trouble and want to solidify
    your understanding of neural networks, and practice implementing a few common
    neural network models, there is no reason not to continue on.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep learning](../Images/35cca5c97cb4d4033c23fdadcb5820b8.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: The many layers of deep neural networks.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'First, have a look at a few deep learning foundation materials:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[Deep Learning Key Terms, Explained](/2016/10/deep-learning-key-terms-explained.html),
    by Matthew Mayo'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Steps to Understanding Deep Learning](/2016/01/seven-steps-deep-learning.html),
    by Matthew Mayo'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, try a few introductory overview-slash-tutorials on [TensorFlow](https://www.tensorflow.org/),
    Google''s "open-source software library for Machine Intelligence," effectively
    a deep learning framework and nearly *de facto* contemporary go-to neural network
    tool:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[The Gentlest Introduction to Tensorflow – Part 1](/2016/08/gentlest-introduction-tensorflow-part-1.html),
    by Soon Hin Khor'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Gentlest Introduction to Tensorflow – Part 2](/2016/08/gentlest-introduction-tensorflow-part-2.html),
    by Soon Hin Khor'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Gentlest Introduction to Tensorflow – Part 3](/2017/02/gentlest-introduction-tensorflow-part-3.html),
    by Soon Hin Khor'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Gentlest Introduction to Tensorflow – Part 4](/2017/02/gentlest-introduction-tensorflow-part-4.html),
    by Soon Hin Khor'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, try your hand at these tutorials directly from the TensorFlow site,
    which implement a few of the most popular and common neural network models:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[Recurrent Neural Networks](https://www.tensorflow.org/tutorials/recurrent),
    Google TensorFlow tutorial'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Convolutional Neural Networks](https://www.tensorflow.org/tutorials/deep_cnn),
    Google TensorFlow tutorial'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, a 7 Steps... post focusing on deep learning is currently in the works,
    and will focus on the use of a high-level API sitting atop TensorFlow to increase
    the ease and flexibility with which a practitioner can implement models. I will
    also add a link here once complete.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '**Related**:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[5 EBooks to Read Before Getting into A Machine Learning Career](/2016/10/5-free-ebooks-machine-learning-career.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Steps to Understanding Deep Learning](/2016/01/seven-steps-deep-learning.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解深度学习的 7 个步骤](/2016/01/seven-steps-deep-learning.html)'
- en: '[Machine Learning Key Terms, Explained](/2016/05/machine-learning-key-terms-explained.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习关键术语解释](/2016/05/machine-learning-key-terms-explained.html)'
- en: More On This Topic
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用管道编写清晰的 Python 代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[建立一个稳固的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[是什么让 Python 成为初创公司的理想编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个数据科学家都应该知道的三个 R 库（即使你使用 Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，并找到目标……](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学统计的最佳资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
