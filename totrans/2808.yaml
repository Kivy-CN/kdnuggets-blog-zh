- en: A simple and interpretable performance measure for a binary classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/03/interpretable-performance-measure-binary-classifier.html](https://www.kdnuggets.com/2020/03/interpretable-performance-measure-binary-classifier.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Mehmet Suzen](https://msuzen.github.io/), Theoretical Physicist and Research
    Scientist**.'
  prefs: []
  type: TYPE_NORMAL
- en: The core application of machine learning models is a [binary classification
    task](https://en.wikipedia.org/wiki/Binary_classification). This appears in polyhedra
    of areas from medicine for [diagnostic tests](https://en.wikipedia.org/wiki/Medical_test) to [credit
    risk](https://global.oup.com/academic/product/consumer-credit-models-9780199232130?cc=us&lang=en&) decision
    making for consumers. Techniques in building classifiers vary from simple decision
    trees to logistic regression and lately super-cool deep learning models that leverage
    multilayered neural networks. However, they are mathematically different in construction
    and training methodology, when it comes to their performance measure, things get
    tricky. In this post, we propose a simple and interpretable performance measure
    for a binary classifier in practice. Some background in classification is assumed.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Why ROC-AUC is not interpretable?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/28e43caacaab3e2c0e52240c83b9e599.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Varying threshold produces different confusion matrices (Wikipedia).*'
  prefs: []
  type: TYPE_NORMAL
- en: The de-facto standard in reporting classifier performance is to use the [Receiver
    Operating Characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)
    (ROC) - Area Under Curve (AUC) measure. It originates from the 1940s during the
    development of [Radar](https://en.wikipedia.org/wiki/Radar) by the US Navy, in
    measuring the performance of detection.  There are at least 5 different definitions
    of what does ROC-AUC means, and *even if you have a Ph.D. in Machine Learning,
    people have an excessively difficult time explaining what AUC means as a performance
    measure*. As AUC functionality is available in almost all libraries, and it becomes
    almost like a religious ritual to report in Machine Learning papers as a classification
    performance. However, its interpretation is not easy, apart from its absurd comparison
    issues, see [hmeasure](http://www.hmeasure.net/).  AUC measures the area under
    the True Positive Rate (TPR) curve as a function of the False Positive Rate (FPR)
    that are extracted from [confusion matrices](https://en.wikipedia.org/wiki/Confusion_matrix) with
    different thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: '*f(x) = y*'
  prefs: []
  type: TYPE_NORMAL
- en: '*∫ 10 f(x)dx = AUC*'
  prefs: []
  type: TYPE_NORMAL
- en: where *y* is TPR and *x* is FPR. Apart from a multitude of interpretations and
    being easy to have confusion, there is no clear purpose of taking the integral
    over FPR. Obviously, we would like to have perfect classification by having FPR
    zero, but the area is not mathematically clear, which means that what is it as* a
    mathematical object* is not clear.
  prefs: []
  type: TYPE_NORMAL
- en: Probability of correct classification (PCC)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A simple and interpretable performance measure for a binary classifier would
    be great for both highly technical data scientist and non-technical stakeholders. The
    basic tenant in this direction is that the purpose of a classifier technology
    is the ability to differentiate two classes. This boils down to a probability
    value, *Probability of correct classification (PCC).* An obvious choice is the
    so-called balanced accuracy (BA). This is usually recommended for unbalanced problems,
    even by [SAS](https://support.sas.com/resources/papers/proceedings17/0942-2017.pdf);
    though they used multiplication of probabilities. Here we will call BA as PCC
    and use addition instead, due to statistical dependence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*PCC = (TPR + TNR) / 2*'
  prefs: []
  type: TYPE_NORMAL
- en: '*TPR = TP / (ConditionPositive) = TP / (TP + FN)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*TNR = TN / (ConditionNegative) = TN / (TN + FP).*'
  prefs: []
  type: TYPE_NORMAL
- en: PCC tells us how good the classifier in detecting either of the class, and it
    is a probability value, [0,1]. Note that using total accuracy over both positive
    and negative cases is misleading, even if our training data is balanced in production,
    batches we measure the performance may not be balanced, so accuracy alone is not
    a good measure.
  prefs: []
  type: TYPE_NORMAL
- en: Production issues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The immediate question would be how to choose the threshold in generating a
    confusion matrix? One option would be to chose a threshold that maximizes PCC
    for production on the test set. To improve the estimation of PCC, resampling on
    the test set can be performed to get a good uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We try to circumvent in reporting AUCs by introducing PCC, or balanced accuracy
    as a simple and interpretable performance measure for a binary classifier. This
    is easy to explain to a non-technical audience. An improved PCC, that takes into
    account better estimation properties can be introduced, but the main interpretation
    remains the same as the *probability of correct classification. *
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://memosisland.blogspot.com/2020/02/a-simple-and-interpretable-performance.html).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Do You Trust and Understand Your Predictive Models?](https://www.kdnuggets.com/2020/02/h2o-trust-understand-predictive-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Receiver Operating Characteristic Curves Demystified (in Python)](https://www.kdnuggets.com/2018/07/receiver-operating-characteristic-curves-demystified-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Choosing the Right Metric for Evaluating Machine Learning Models — Part 2](https://www.kdnuggets.com/2018/06/right-metric-evaluating-machine-learning-models-2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Introduction to Binary Classification with PyCaret](https://www.kdnuggets.com/2021/12/introduction-binary-classification-pycaret.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Learn how to design, measure and implement trustworthy A/B tests…](https://www.kdnuggets.com/2023/01/sphere-design-measure-implement-trustworthy-ab-tests-ronny-kohavi.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[From Theory to Practice: Building a k-Nearest Neighbors Classifier](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Optimizing Your LLM for Performance and Scalability](https://www.kdnuggets.com/optimizing-your-llm-for-performance-and-scalability)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Strategies for Optimizing Performance and Costs When Using Large…](https://www.kdnuggets.com/strategies-for-optimizing-performance-and-costs-when-using-large-language-models-in-the-cloud)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
