- en: Linear Algebra for Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/08/linear-algebra-natural-language-processing.html](https://www.kdnuggets.com/2021/08/linear-algebra-natural-language-processing.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Taaniya Arora](https://medium.com/@TaaniyaArora), Data Scientist**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0865fd73b4005b8f794326168d6b677f.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Michael Dziedzic](https://unsplash.com/@lazycreekimages?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The field of Natural Language Processing involves building techniques to process
    text in natural language by people like you and me, and extract insights from
    it for performing a variety of tasks from interpreting user queries on search
    engines and returning web pages, to solving customer queries as chatbot assistant.
    The importance of representing every word into a form that captures the meaning
    of the word and the overall context becomes crucial especially when major decisions
    are based upon the insights extracted from text on a large scale — like forecasting
    stock price change with social media.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we’ll begin with the basics of linear algebra to get an intuition
    of sof vectors and their significance for representing specific types of information,
    the different ways of representing text in vector space, and how the concept has
    evolved to the state of the art models we have now.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll step through the following areas -
  prefs: []
  type: TYPE_NORMAL
- en: Unit vectors in our coordinate system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear combination of vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Span in vector coordinate system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collinearity & multicollinearity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear dependence and independence of vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basis vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector Space Model for NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dense Vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit vectors in our coordinate system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '***i***-> Denotes a unit vector (vector of length 1 unit) pointing in the x-direction'
  prefs: []
  type: TYPE_NORMAL
- en: '***j***-> Denotes a unit vector in the y-direction'
  prefs: []
  type: TYPE_NORMAL
- en: Together, they are called the basis of our coordinate vector space.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll come to the term **basis** more in the subsequent parts below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/099b26537ff5a6f1dda889bf00ce567d.png)'
  prefs: []
  type: TYPE_IMG
- en: Standard Unit vectors — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have a vector 3***i***+ 5***j***
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This vector has x,y coordinates : 3 & 5 respectively'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These coordinates are the scalars that flip and scale the unit vectors by 3
    & 5 units in the x & y directions respectively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/ab03155c8441be62dbcb33f9a9f5a7d6.png)'
  prefs: []
  type: TYPE_IMG
- en: A vector in 2D X-Y space — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Linear Combination of 2 vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If ***u*** & ***v*** are two vectors in a 2 dimensional space,then their linear
    combination resulting into a vector ***l*** is represented by -
  prefs: []
  type: TYPE_NORMAL
- en: '***l*** = *x1.* ***u*** + *x2.* ***v***'
  prefs: []
  type: TYPE_NORMAL
- en: The numbers *x1*, *x2* are the components of a vector x
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is essentially a scaling and addition operation by x on the given vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The above expression of linear combination is equivalent to the following linear
    system -
  prefs: []
  type: TYPE_NORMAL
- en: '**B*x*** = ***l***'
  prefs: []
  type: TYPE_NORMAL
- en: Where ***B*** denotes a matrix whose columns are ***u*** and ***v***.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand this by an example below with vectors ***u*** & ***v*** in
    a 2 dimensional space -
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/cb3e7ee2e66b9f581e0804970e01ac37.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear Combination of vectors — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: We can also understand it from this explanation for a similar example with 3
    dimensions given by a Professor in his notes here -
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/209915af4612495ea4b870beb5a0f8e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear Algebra, Chapter 1- Introduction to Vectors, MIT
  prefs: []
  type: TYPE_NORMAL
- en: Taking the 3 vectors from the example in the image and plotting them in 3D space
    (The units of axes are different than the vectors in the plot)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/bf63a365b492826b88939fcb5bc0d4e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Vectors in 3D space — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Span
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A span is a set of all possible combinations of vectors that we can reach with
    a linear combination of a given pair of vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The span of most pairs of 2-D vectors is all vectors in the 2-D space. Except,
    when they line up in the same direction (i.e if they are collinear) , in which
    case, their span is a line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: i.e span( ***a***, ***b***) = **R² **(all vectors in 2D space) , provided they
    are not collinear.
  prefs: []
  type: TYPE_NORMAL
- en: Collinearity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Collinearity is the case when we have p different predictor variables but some
    of them are linear combinations of others, so they don’t add any other information.
    2 collinear vectors / variables will have correlation close to +/- 1 and can be
    detected by their correlation matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '**Multicollinearity** exists when more than 2 vectors are collinear and any
    pair of vectors may not necessarily have high correlation.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear Independence**'
  prefs: []
  type: TYPE_NORMAL
- en: We say that *v1* , *v2*, . . . , *vn* are linearly independent, if none of them
    is
  prefs: []
  type: TYPE_NORMAL
- en: a linear combination of the others. This is equivalent to saying
  prefs: []
  type: TYPE_NORMAL
- en: that *x1.v1* + *x2.v2* + . . . + *xn.vn* = 0 implies *x1 = x2 = . . . = xn =
    0*
  prefs: []
  type: TYPE_NORMAL
- en: Since collinear vectors can be expressed as linear combinations of each other,
    they are **linearly dependent.**
  prefs: []
  type: TYPE_NORMAL
- en: '**Basis**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A basis is that set of linearly independent vectors that span that space.
  prefs: []
  type: TYPE_NORMAL
- en: We call these vectors as basis vectors
  prefs: []
  type: TYPE_NORMAL
- en: Vector space Models in NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **Vector space** is a set V of vectors, where two operations — vector addition
    and scalar multiplication are defined. For E.g. IF two vectors ***u*** & ***v*** are
    in space ***V***, then their sum, ***w = u + v*** will also lie in the vector
    space ***V***.
  prefs: []
  type: TYPE_NORMAL
- en: A 2D vector space is a set of linearly independent vasis vectors with 2 axes.
  prefs: []
  type: TYPE_NORMAL
- en: Each axis represents a dimension in the vector space.
  prefs: []
  type: TYPE_NORMAL
- en: Recalling the previous plot of vector ***a*** = (3,5) = 3 ***i*** + 5 ***j*** again.
    This vector is represented on a 2D space with 2 linearly independent basis vectors
    — X & Y, who also represent the 2 axes as well as the 2 dimenions of the space.
  prefs: []
  type: TYPE_NORMAL
- en: 3 & 5 here are the x,y components of this vector for representation on the X-Y
    2D space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab03155c8441be62dbcb33f9a9f5a7d6.png)'
  prefs: []
  type: TYPE_IMG
- en: Vector in 2D X-Y plane — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Vector space model in NLP**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A vector space model is a representation of text in vector space.
  prefs: []
  type: TYPE_NORMAL
- en: Here, each word in a corpus is a linearly independent basis vector and each
    basis vector represents an axis in the vector space.
  prefs: []
  type: TYPE_NORMAL
- en: This means, each word is orthogonal to other words/axes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a corpus of vocabulary ***|V|***, **R **will contain ***|V|*** axes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combination of terms represent documents as points or vectors in this space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For 3 words, we’ll have a 3D vector model represented like this -
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46eb924f89cb4d6ba86b226ab4c30f0a.png)'
  prefs: []
  type: TYPE_IMG
- en: Vector Space Model for 3 words — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The table above the graph represents the TF-IDF incident matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '***D1*** = (0.91, 0, 0.0011) represents a document vector in the 3 axes — good,
    house, car. Similarly, we have ***D2*** & ***D3*** document vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: How does representation in vector space help us, though?
  prefs: []
  type: TYPE_NORMAL
- en: One of the common application using this representation is information retrieval
    for search engines, question answering systems and much more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By representing the text into vectors, we aim to use vector algebra to extract
    semantics from the text and use it for different applications like searching documents
    containing the similar sementics as those contained in a given search query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For eg. For a search token ‘buy’ , we would want to get all the documents containing
    different forms of this word — buying, bought and even synonyms of the word ‘buy’.
    Such documents can not be captured from other rudimentary methods representing
    documents as Binary [incident matrix](https://en.wikipedia.org/wiki/Incidence_matrix).
  prefs: []
  type: TYPE_NORMAL
- en: This is achieved through distance metrics like cosine similarity between vectors
    of document & query, where the documents closer to the query are ranked the highest.
  prefs: []
  type: TYPE_NORMAL
- en: The number of words / vocabulary size can be as huge as in millions eg. Google
    news corpus is 3 Million, which means as many independent axes/dimensions to represent
    the vectors. Hence, we want to use the operations in vector space to reduce the
    number of dimensions and bring words with similar to each other in the same axis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dense vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The above operations are possible on document vectors which are represented
    by extending the above vector representation of documents to documents represented
    as distributed or dense vectors. These representations capture the semantics of
    the text and also captures the linear combinations of word vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The previous vector space model with each word representing a separate dimension
    results in sparse vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dense vectors captures the context in the vector representation. The dense vector
    of words are such that words appearing in similar contexts will have similar representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These dense vectors are also called as word embeddings or distributed representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: word2vec is one such framework to **learn** dense vectors of words from large
    corpus. It has 2 variants — skip-gram and CBOW(continuous bag of words)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the other frameworks and techniques to obtain dense vectors are Global
    Vectors(GloVe), fastText, ELMo(Embeddings from Language Model) and most recent
    state of the art Bert based approached to obtain contextualized word embeddings
    during inference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article introduced the concept of vector space based on linear algebra
    and highlighted the related concepts as part of their application in Natural Language
    Processing for representing text documents in semantics representation and extraction
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The applications of word embeddings have been extended to more advanced and
    wide applications with much improved performance than earlier.
  prefs: []
  type: TYPE_NORMAL
- en: You can also refer to the source code and run on colab by accessing my [Git
    repository here](https://github.com/Taaniya/linear-algebra-for-ml).
  prefs: []
  type: TYPE_NORMAL
- en: Hope you enjoyed reading it. :)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Linear Independence, Basis & Dimension, MIT](https://www.youtube.com/watch?v=eeMJg4uI7o0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Matrices-Introduction to Vectors, MIT](https://math.mit.edu/~gs/linearalgebra/linearalgebra5_1-3.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Vector space models for NLP, NPTEL](https://www.youtube.com/watch?v=6Nz88LHOIdo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to statistical learning, E-Book by Gareth James, Daniela Witten,
    Trevor Hastie and Robert Tibshirani
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bio: [Taaniya Arora](https://medium.com/@TaaniyaArora)** is a Data Scientist
    and problem solver, especially interested in NLP and reinforcement learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/from-linear-algebra-to-text-representation-for-natural-language-processing-239cd3ccb12f).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[The Best SOTA NLP Course is Free!](/2021/07/best-sota-nlp-course-free.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Essential Linear Algebra for Data Science and Machine Learning](/2021/05/essential-linear-algebra-data-science-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How To Overcome The Fear of Math and Learn Math For Data Science](/2021/03/overcome-fear-learn-math-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[N-gram Language Modeling in Natural Language Processing](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Natural Language Processing Key Terms, Explained](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Representation for Natural Language Processing Tasks](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Transfer Learning for Image Recognition and Natural Language Processing](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Start Using Natural Language Processing With PyTorch](https://www.kdnuggets.com/2022/04/start-natural-language-processing-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Gentle Introduction to Natural Language Processing](https://www.kdnuggets.com/2022/06/gentle-introduction-natural-language-processing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
