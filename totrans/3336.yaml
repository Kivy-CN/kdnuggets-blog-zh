- en: 7 More Steps to Mastering Machine Learning With Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/03/seven-more-steps-machine-learning-python.html](https://www.kdnuggets.com/2017/03/seven-more-steps-machine-learning-python.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So, you have been thinking about picking up machine learning, but given the
    confusing state of the web you don't know where to begin? Or maybe you have finished
    [the first 7 steps](/2015/11/seven-steps-machine-learning-python.html) and are
    looking for some follow-up material, beyond the introductory?
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine learning algorithms](../Images/1c10caeff60f2a083a05b54eaf6eb1fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: This post is the second installment of the [7 Steps to Mastering Machine Learning
    in Python](/2015/11/seven-steps-machine-learning-python.html) series (since there
    are 2 parts, I guess it now qualifies as a series). If you have started with the
    [original post](/2015/11/seven-steps-machine-learning-python.html), you should
    already be satisfactorily up to speed, skill-wise. If not, you may want to review
    that post first, which may take some time, depending on your current level of
    understanding; however, I assure you that doing so will be worth your effort.
  prefs: []
  type: TYPE_NORMAL
- en: After a quick review -- and a few options for a fresh perspective -- this post
    will focus more categorically on several sets of related machine learning tasks.
    Since we can safely skip the foundational modules this time around -- Python basics,
    machine learning basics, etc. -- we will jump right into the various machine learning
    algorithms. We can also categorize our tutorials better along functional lines
    this time.
  prefs: []
  type: TYPE_NORMAL
- en: I will, once again, state that the material contained herein is all freely available
    on the web, and all rights and recognition for the works belong to their original
    authors. If something has not been properly attributed, please feel free to [let
    me know](https://twitter.com/mattmayo13).
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Machine Learning Basics Review & A Fresh Perspective'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Just to review, these are the steps covered in the [original post](/2015/11/seven-steps-machine-learning-python.html):'
  prefs: []
  type: TYPE_NORMAL
- en: Basic Python Skills
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Foundational Machine Learning Skills
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scientific Python Packages Overview
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Getting Started with Machine Learning in Python: Introduction & model evaluation'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Machine Learning Topics with Python: k-means clustering, decision trees, linear
    regression & logistic regression'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Advanced Machine Learning Topics with Python: Support vector machines, random
    forests, dimension reduction with PCA'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deep Learning in Python
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As stated above, if you are looking to start from square one, I would suggest
    going back to the first article and proceeding accordingly. I will also note that
    the appropriate *getting started* material, including any and all installation
    instructions, are including in the previous article.
  prefs: []
  type: TYPE_NORMAL
- en: 'If, however, you are really green, I would start with the following, covering
    the absolute basics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Machine Learning Key Terms, Explained](/2016/05/machine-learning-key-terms-explained.html),
    by Matthew Mayo'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Statistical Classification on Wikipedia](https://en.wikipedia.org/wiki/Statistical_classification)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning: A Complete and Detailed Overview](/2016/10/machine-learning-complete-detailed-overview.html),
    by Alex Castrounis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you are looking for some alternative or complementary approaches to learning
    the basics of machine learning, I have recently been enjoying Shai Ben-David''s
    video lectures and freely available textbook written with Shai Shalev-Shwartz.
    Find them both here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Shai Ben-David''s introductory machine learning video lectures](https://www.youtube.com/watch?v=b5NlRg8SjZg&index=1&list=PLFze15KrfxbH8SE4FgOHpMSY1h5HiRLMm),
    University of Waterloo'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding Machine Learning: From Theory to Algorithms](http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/copy.html),
    by Shai Ben-David & Shai Shalev-Shwartz'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember, the introductory material does not all need to be digested before
    moving forward with the rest of the steps (in either this post or the original).
    Video lectures, texts, and other resources can be consulted when implementing
    models using the reflected machine learning algorithms, or when applicable concepts
    are being used practically in subsequent steps. Use your judgment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: More Classification'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We begin with the new material by first strengthening our classification know-how
    and introducing a few additional algorithms. While part 1 of our post covered
    decision trees, support vector machines, and logistic regression -- as well as
    the ensemble classifier Random Forests -- we will add k-nearest neighbors, the
    Naive Bayes classier, and a multilayer perceptron into the mix.
  prefs: []
  type: TYPE_NORMAL
- en: '![Scikit-learn classifiers](../Images/98e47c639ae115438b94fe52b9ea7cd7.png)'
  prefs: []
  type: TYPE_IMG
- en: Scikit-learn classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: '**k-nearest neighbors (kNN)** is a simple classifier and an example of a lazy
    learner, in which all computation occurs at classification time (as opposed to
    occurring during a training step ahead of time). kNN is [non-parametric](https://en.wikipedia.org/wiki/Nonparametric_statistics),
    and functions by comparing a data instance with the *k* closest instances when
    making decisions about how it should be classified.'
  prefs: []
  type: TYPE_NORMAL
- en: '[K-Nearest Neighbor classification using python](https://ashokharnal.wordpress.com/2015/01/21/k-nearest-neighbor-classification-using-python/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Naive Bayes** is a classifier based on [Bayes'' Theorem](https://en.wikipedia.org/wiki/Bayes''_theorem).
    It assumes that there is independence among features, and that the presence of
    any particular feature in one class is not related to any other feature''s presence
    in the same class.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Document Classification with scikit-learn](http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html),
    by Zac Stewart'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **multilayer perceptron (MLP)** is a simple [feedforward](https://en.wikipedia.org/wiki/Feedforward_neural_network)
    neural network, consisting of multiple layers of nodes, where each layer is fully
    connected with the layer which comes after it. The MLP was introduced in Scikit-learn
    version 0.18.
  prefs: []
  type: TYPE_NORMAL
- en: First read an overview of the MLP classifier from the Scikit-learn documentation,
    and then practice implementation with a tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: '[Neural network models (supervised)](http://scikit-learn.org/stable/modules/neural_networks_supervised.html),
    Scikit-learn documentation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Beginner’s Guide to Neural Networks with Python and SciKit Learn 0.18!](/2016/10/beginners-guide-neural-networks-python-scikit-learn.html),
    by Jose Portilla'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 3: More Clustering'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We now move on to clustering, a form of unsupervised learning. In the first
    post we covered the k-means algorithm; we will introduce DBSCAN and Expectation-maximization
    (EM) herein.
  prefs: []
  type: TYPE_NORMAL
- en: '![Scikit-learn clustering algorithms](../Images/7c9c8013dab7634a2fb3cf9f4a254d5e.png)'
  prefs: []
  type: TYPE_IMG
- en: Scikit-learn clustering algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'First off, read these introductory posts; the first is a quick comparison of
    k-means and EM clustering techniques, a nice segue into new forms of clustering,
    and the second is an overview of clustering techniques available in Scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Comparing Clustering Techniques: A Concise Technical Overview](/2016/09/comparing-clustering-techniques-concise-technical-overview.html),
    by Matthew Mayo'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Comparing different clustering algorithms on toy datasets](http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html),
    Scikit-learn documentation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Expectation-maximization (EM)** is a probabilistic clustering algorithm,
    and, as such, involves determining the probabilities that instances belong to
    particular clusters. EM ”approaches maximum likelihood or maximum a posteriori
    estimates of parameters in statistical models” (Han, Kamber & Pei). The EM process
    begins with a set of parameters, iterating until clustering is maximized, with
    respect to *k* clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: First read a tutorial on the EM algorithm. Next, have a look at the relevant
    Scikit-learn documentation. Finally, follow a tutorial and implement EM clustering
    yourself with Python.
  prefs: []
  type: TYPE_NORMAL
- en: '[A Tutorial on the Expectation Maximization (EM) Algorithm](/2016/08/tutorial-expectation-maximization-algorithm.html),
    by Elena Sharova'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Gaussian mixture models](http://scikit-learn.org/stable/modules/mixture.html),
    Scikit-learn documentation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Quick introduction to gaussian mixture models with Python](http://www.nehalemlabs.net/prototype/blog/2014/04/03/quick-introduction-to-gaussian-mixture-models-with-python/),
    by Tiago Ramalho'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If "[Gaussian mixture models](https://en.wikipedia.org/wiki/Mixture_model)"
    is confusing at first glance, this relevant section from the Scikit-learn documentation
    should alleviate any unnecessary worries:'
  prefs: []
  type: TYPE_NORMAL
- en: The `GaussianMixture` object implements the expectation-maximization (EM) algorithm
    for fitting mixture-of-Gaussian models.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Density-based spatial clustering of applications with noise (DBSCAN)** operates
    by grouping densely-packed data points together, and designating low-density data
    points as outliers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First read and follow an example implementation of DBSCAN from Scikit-learn''s
    documentation, and then follow a concise tutorial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Demo of DBSCAN clustering algorithm](http://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html),
    Scikit-learn documentation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Density-based clustering algorithm (DBSCAN) and Implementation](http://madhukaudantha.blogspot.ca/2015/04/density-based-clustering-algorithm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
