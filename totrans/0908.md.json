["```py\n%pip install pyspark py4j -qq\n```", "```py\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('PySpark_for_DataScience').getOrCreate()\n```", "```py\ndf = spark.read.csv(\n    '/content/spotify_weekly_chart.csv',\n    sep = ',',\n    header = True,\n    )\n\ndf.printSchema()\n```", "```py\nroot\n |-- Pos: string (nullable = true)\n |-- P+: string (nullable = true)\n |-- Artist: string (nullable = true)\n |-- Title: string (nullable = true)\n |-- Wks: string (nullable = true)\n |-- Pk: string (nullable = true)\n |-- (x?): string (nullable = true)\n |-- Streams: string (nullable = true)\n |-- Streams+: string (nullable = true)\n |-- Total: string (nullable = true)\n```", "```py\nfrom pyspark.sql.types import *\n\ndata_schema = [\n               StructField('Pos', IntegerType(), True),\n               StructField('P+', StringType(), True),\n               StructField('Artist', StringType(), True),\n               StructField('Title', StringType(), True),\n               StructField('Wks', IntegerType(), True),\n               StructField('Pk', IntegerType(), True),\n               StructField('(x?)', StringType(), True),\n               StructField('Streams', IntegerType(), True),\n               StructField('Streams+', DoubleType(), True),\n               StructField('Total', IntegerType(), True),\n            ]\n\nfinal_struc = StructType(fields = data_schema)\n```", "```py\ndf = spark.read.csv(\n    '/content/spotify_weekly_chart.csv',\n    sep = ',',\n    header = True,\n    schema = final_struc \n    )\n\ndf.printSchema()\n```", "```py\nroot\n |-- Pos: integer (nullable = true)\n |-- P+: string (nullable = true)\n |-- Artist: string (nullable = true)\n |-- Title: string (nullable = true)\n |-- Wks: integer (nullable = true)\n |-- Pk: integer (nullable = true)\n |-- (x?): string (nullable = true)\n |-- Streams: integer (nullable = true)\n |-- Streams+: double (nullable = true)\n |-- Total: integer (nullable = true)\n```", "```py\ndf.limit(5).toPandas()\n```", "```py\ndf.describe().show()\n```", "```py\ndf.count()\n# 200\n```", "```py\ndf = df.withColumnRenamed('Pos', 'Rank')\n\ndf.show(5)\n```", "```py\ndf = df.drop('P+','Pk','(x?)','Streams+')\n\ndf.show(5)\n```", "```py\ndf = df.na.drop()\n## Or\n#data.na.replace(old_value, new_vallue)\n```", "```py\ndf.select(['Artist', 'Artist', 'Total']).show(5)\n```", "```py\nfrom pyspark.sql.functions import col, lit, when\n\ndf.filter(\n    (col(\"Total\") >= lit(\"600000000\")) & (col(\"Total\") <= lit(\"700000000\"))\n).show(5)\n```", "```py\ndf.select('Artist', 'Title', \n            when(df.Wks >= 35, 1).otherwise(0)\n           ).show(5)\n```", "```py\ndf.select(['Artist','Wks','Total'])\\\n        .groupBy('Artist')\\\n        .mean()\\\n        .orderBy(['avg(Total)'], ascending = [False])\\\n        .show(5)\n```", "```py\nvis_df = (\n    df.select([\"Artist\", \"Wks\", \"Total\"])\n    .groupBy(\"Artist\")\n    .mean()\n    .orderBy([\"avg(Total)\"], ascending=[False])\n    .toPandas()\n)\n\nvis_df.iloc[0:7].plot(\n    kind=\"bar\",\n    x=\"Artist\",\n    y=\"avg(Total)\",\n    figsize=(12, 6),\n    ylabel=\"Average Average Streams\",\n) \n```", "```py\nfinal_data = (\n    df.select([\"Artist\", \"Wks\", \"Total\"])\n    .groupBy(\"Artist\")\n    .mean()\n    .orderBy([\"avg(Total)\"], ascending=[False])\n)\n\n# CSV\nfinal_data.write.csv(\"dataset.csv\")\n\n# JSON\nfinal_data.write.save(\"dataset.json\", format=\"json\")\n\n# Parquet\nfinal_data.write.save(\"dataset.parquet\", format=\"parquet\") \n```", "```py\nfrom pyspark.ml.feature import (\n    VectorAssembler,\n    StringIndexer,\n    OneHotEncoder,\n    StandardScaler,\n)\n\n## Categorical Encoding\nindexer = StringIndexer(inputCol=\"Artist\", outputCol=\"Encode_Artist\").fit(\n    final_data\n)\nencoded_df = indexer.transform(final_data)\n\n## Assembling Features\nassemble = VectorAssembler(\n    inputCols=[\"Encode_Artist\", \"avg(Wks)\", \"avg(Total)\"],\n    outputCol=\"features\",\n)\n\nassembled_data = assemble.transform(encoded_df)\n\n## Standard Scaling\nscale = StandardScaler(inputCol=\"features\", outputCol=\"standardized\")\ndata_scale = scale.fit(assembled_data)\ndata_scale_output = data_scale.transform(assembled_data)\ndata_scale_output.show(5)\n```", "```py\nfrom pyspark.ml.clustering import KMeans\nKMeans_algo=KMeans(featuresCol='standardized', k=4)\nKMeans_fit=KMeans_algo.fit(data_scale_output)\npreds=KMeans_fit.transform(data_scale_output)\n\npreds.show(5)\n```", "```py\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf_viz = preds.select(\n    \"Artist\", \"avg(Wks)\", \"avg(Total)\", \"prediction\"\n).toPandas()\navg_df = df_viz.groupby([\"prediction\"], as_index=False).mean()\n\nlist1 = [\"avg(Wks)\", \"avg(Total)\"]\n\nfor i in list1:\n    sns.barplot(x=\"prediction\", y=str(i), data=avg_df)\n    plt.show() \n```"]