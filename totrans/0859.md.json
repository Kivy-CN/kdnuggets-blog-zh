["```py\n# Load the dataset\nnewsgroups_train = fetch_20newsgroups(subset='train', shuffle=True)\nnewsgroups_test = fetch_20newsgroups(subset='test', shuffle=True)\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\n\n# Preprocess the text data\nvectorizer = TfidfVectorizer(stop_words='english')\nX_train = vectorizer.fit_transform(newsgroups_train.data)\nX_test = vectorizer.transform(newsgroups_test.data)\n\n# Train a Naive Bayes classifier\nclf = MultinomialNB()\nclf.fit(X_train, newsgroups_train.target)\n\n# Evaluate the performance of the classifier\ny_pred = clf.predict(X_test)\nprint(classification_report(newsgroups_test.target, y_pred))\n```", "```py\nOutput >>\nprecision    recall  f1-score   support\n\n           0       0.80      0.69      0.74       319\n           1       0.78      0.72      0.75       389\n           2       0.79      0.72      0.75       394\n           3       0.68      0.81      0.74       392\n           4       0.86      0.81      0.84       385\n           5       0.87      0.78      0.82       395\n           6       0.87      0.80      0.83       390\n           7       0.88      0.91      0.90       396\n           8       0.93      0.96      0.95       398\n           9       0.91      0.92      0.92       397\n          10       0.88      0.98      0.93       399\n          11       0.75      0.96      0.84       396\n          12       0.84      0.65      0.74       393\n          13       0.92      0.79      0.85       396\n          14       0.82      0.94      0.88       394\n          15       0.62      0.96      0.76       398\n          16       0.66      0.95      0.78       364\n          17       0.95      0.94      0.94       376\n          18       0.94      0.52      0.67       310\n          19       0.95      0.24      0.38       251\n\n    accuracy                           0.82      7532\n   macro avg       0.84      0.80      0.80      7532\nweighted avg       0.83      0.82      0.81      7532\n```", "```py\nimport matplotlib.pyplot as plt\n\n# Load the dataset\nnewsgroups_train = fetch_20newsgroups(subset='train', shuffle=True)\n\n# Count the number of samples for each class\nclass_counts = {}\nfor label in newsgroups_train.target:\n    class_name = newsgroups_train.target_names[label]\n    if class_name in class_counts:\n        class_counts[class_name] += 1\n    else:\n        class_counts[class_name] = 1\n\n# Plot the class distribution\nplt.bar(class_counts.keys(), class_counts.values())\nplt.xticks(rotation=90)\nplt.xlabel('Class')\nplt.ylabel('Number of Samples')\nplt.title('Class Distribution')\nplt.show()\n```", "```py\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\n# Define the pipeline with TF-IDF and Multinomial Naive Bayes\npipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(stop_words='english')),\n    ('clf', MultinomialNB())\n])\n\n# Define the hyperparameter grid\nparam_grid = {\n    'tfidf__max_df': [0.5, 0.75, 1.0],\n    'clf__alpha': [0.1, 0.5, 1.0],\n    'clf__fit_prior': [True, False],\n}\n\n# Perform grid search with cross-validation\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5)\ngrid_search.fit(newsgroups_train.data, newsgroups_train.target)\n\n# Print the best hyperparameters and cross-validation score\nprint(\"Best hyperparameters: \", grid_search.best_params_)\nprint(\"Cross-validation score: \", grid_search.best_score_)\n```", "```py\nOutput >>\nBest hyperparameters:  {'clf__alpha': 0.1, 'clf__fit_prior': False, 'tfidf__max_df': 0.5}\nCross-validation score:  0.9088736147919108\n```", "```py\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\nnewsgroups_train = fetch_20newsgroups(subset='train', shuffle=True)\n\n# Convert the text data to a matrix of TF-IDF features\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(newsgroups_train.data)\ny = newsgroups_train.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the SVM classifier\nclf = SVC(kernel='linear')\n\n# Train the SVM classifier\nclf.fit(X_train, y_train)\n\n# Predict the class labels for the test set\ny_pred = clf.predict(X_test)\n\n# Calculate the accuracy of the SVM classifier\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy:', accuracy)\n```", "```py\nOutput >> Accuracy: 0.9019001325673884\n```"]