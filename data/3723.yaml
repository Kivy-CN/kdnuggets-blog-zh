- en: 8 Innovative BERT Knowledge Distillation Papers That Have Changed The Landscape
    of NLP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 种创新的 BERT 知识蒸馏论文改变了 NLP 的格局
- en: 原文：[https://www.kdnuggets.com/2022/09/eight-innovative-bert-knowledge-distillation-papers-changed-nlp-landscape.html](https://www.kdnuggets.com/2022/09/eight-innovative-bert-knowledge-distillation-papers-changed-nlp-landscape.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/09/eight-innovative-bert-knowledge-distillation-papers-changed-nlp-landscape.html](https://www.kdnuggets.com/2022/09/eight-innovative-bert-knowledge-distillation-papers-changed-nlp-landscape.html)
- en: '![8 Innovative BERT Knowledge Distillation Papers That Have Changed The Landscape
    of NLP](../Images/b30dffc14136aec04f62a6de700c7b16.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![8 种创新的 BERT 知识蒸馏论文改变了 NLP 的格局](../Images/b30dffc14136aec04f62a6de700c7b16.png)'
- en: Image by Editor
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑的图像
- en: The article summarizes eight papers carefully selected from many papers related
    to BERT knowledge distillation. NLP model compression and acceleration is an active
    area of research and widely adapted in the industry to deliver low latency features
    and services to end users.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 文章总结了从众多 BERT 知识蒸馏相关论文中精心挑选出的八篇论文。NLP 模型压缩和加速是一个活跃的研究领域，并在行业中广泛应用，以向终端用户提供低延迟的功能和服务。
- en: To put it bluntly, the BERT model is used for converting words into numbers
    and enables you to train machine learning models on text data. Why? Because the
    machine learning models take input as numbers and not words.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 直截了当说，BERT 模型用于将词语转换为数字，并使你能够在文本数据上训练机器学习模型。为什么？因为机器学习模型接受的输入是数字而非词语。
- en: '![8 Innovative BERT Knowledge Distillation Papers That Have Changed The Landscape
    of NLP](../Images/1a1713a423c713923922a2fe5b041bc4.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![8 种创新的 BERT 知识蒸馏论文改变了 NLP 的格局](../Images/1a1713a423c713923922a2fe5b041bc4.png)'
- en: Image from [Devlin et al., 2019](https://arxiv.org/pdf/1810.04805.pdf)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源 [Devlin et al., 2019](https://arxiv.org/pdf/1810.04805.pdf)
- en: Why is BERT so Popular?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么 BERT 如此受欢迎？
- en: First and foremost, BERT is a language model that amplifies the high performance
    of several tasks. BERT (Bidirectional Encoder Representations from Transformers),
    published in 2018, triggered a fuss in the community of machine learning by providing
    a neoteric achieved outcomes in a broad spectrum of NLP tasks, namely language
    understanding, and question-answering.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，BERT 是一种语言模型，它提升了多个任务的高性能。BERT（双向编码器表示从 Transformers）于 2018 年发布，通过在广泛的 NLP
    任务中提供前所未有的结果，引发了机器学习社区的轰动，尤其是在语言理解和问答方面。
- en: 'The main attraction of BERT is employing the bidirectional training of Transformer,
    a prominent attention model for language modeling. But, as for my narration, here
    are a few things that make BERT so much better:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 的主要吸引力在于使用 Transformer 的双向训练，Transformer 是一种突出的语言建模注意力模型。但就我的叙述而言，这里有几个使
    BERT 更加出色的方面：
- en: It is open-source
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是开源的
- en: The best technique in NLP to grasp the context-heavy texts
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP 中掌握语境丰富文本的最佳技术
- en: Bidirectional nature
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双向特性
- en: All of the papers present a particular point of view of findings in the BERT
    utilization.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所有论文都呈现了 BERT 使用中的特定观点。
- en: Paper 1
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 论文 1
- en: '**[DistilBERT, a distilled version of BERT: smaller, faster, cheaper, and lighter](https://arxiv.org/abs/1910.01108)**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**[DistilBERT，BERT 的蒸馏版本：更小、更快、更便宜、更轻量](https://arxiv.org/abs/1910.01108)**'
- en: Authors propose a technique to pre-train a smaller general-purpose language
    representation model, termed DistilBERT, which can then be fine-tuned with good
    performances on a wide range of tasks like its larger counterparts. While most
    prior works investigated the use of distillation for building task-specific models,
    we leverage knowledge distillation during the pre-training phase. We show that
    it is possible to reduce the size of a BERT model by 40% while retaining 97% of
    its language understanding capabilities and being 60% faster. Loss is threefold,
    combining language modeling loss, distillation loss, and cosine-distance loss.
    The data used the same corpus as the original BERT model. Further, **DistilBERT**
    was trained on eight 16GB V100 GPUs for around 90 hours.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们提出了一种技术，旨在预训练一个较小的通用语言表示模型，称为 DistilBERT，该模型可以在多种任务上进行微调，并且表现出色，类似于其较大的同行。虽然大多数先前的工作研究了利用蒸馏来构建特定任务模型，但我们在预训练阶段利用了知识蒸馏。我们展示了可以将
    BERT 模型的大小减少 40%，同时保留 97% 的语言理解能力，并且速度提高 60%。损失函数包含语言建模损失、蒸馏损失和余弦距离损失。使用的数据与原始
    BERT 模型使用的数据相同。此外，**DistilBERT** 在八个 16GB V100 GPU 上训练了大约 90 小时。
- en: Let’s assume
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设
- en: 'For an input x, the teacher outputs:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入 x，教师输出：
- en: '![8 Innovative BERT Knowledge Distillation Papers That Have Changed The Landscape
    of NLP](../Images/0fcb73519e973f009f6351f7947fabad.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![8篇创新性的BERT知识蒸馏论文，改变了NLP领域的格局](../Images/0fcb73519e973f009f6351f7947fabad.png)'
- en: '[Source](https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f)'
- en: 'And the student outputs:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 学生输出：
- en: '![8 Innovative BERT Knowledge Distillation Papers That Have Changed The Landscape
    of NLP](../Images/1b386d50f1943ec897adcd7e92b25d21.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![8篇创新性的BERT知识蒸馏论文，改变了NLP领域的格局](../Images/1b386d50f1943ec897adcd7e92b25d21.png)'
- en: '[Source](https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f)'
- en: 'Consider that softmax and the notations that come with it; we’ll get back to
    it later. Nevertheless, if we want T and S to be close, we can apply a cross-entropy
    loss to S with T as a target. That is what we call teacher-student cross-entropy
    loss:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到softmax及其相关符号，我们稍后会回到这个话题。然而，如果我们希望T和S接近，可以对S应用交叉熵损失，以T作为目标。这就是我们所谓的教师-学生交叉熵损失：
- en: 'Distillation loss: this loss is the same as the typical Knowledge distillation
    loss:'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒸馏损失：此损失与典型的知识蒸馏损失相同：
- en: '![8 Innovative BERT Knowledge Distillation Papers That Have Changed The Landscape
    of NLP](../Images/027fc63e2adf0563b0cfe9ac2751ffa0.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![8篇创新性的BERT知识蒸馏论文，改变了NLP领域的格局](../Images/027fc63e2adf0563b0cfe9ac2751ffa0.png)'
- en: '[Source](https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f)'
- en: Masked language modeling loss(MLM)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒙版语言建模损失（MLM）
- en: Cosine embedding loss(Lcos) was found to be beneficial, which aligned the direction
    of student and teacher hidden state vectors.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现余弦嵌入损失（Lcos）是有益的，它对齐了学生和教师隐藏状态向量的方向。
- en: '![8 Innovative BERT Knowledge Distillation Papers That Have Changed The Landscape
    of NLP](../Images/561015ea973bf811a23fd20e75429969.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![8篇创新性的BERT知识蒸馏论文，改变了NLP领域的格局](../Images/561015ea973bf811a23fd20e75429969.png)'
- en: T(x) is the teacher vector output, and S(x) is the student vector output [Source.](https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: T(x)是教师向量输出，S(x)是学生向量输出 [来源](https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f)
- en: '**Key takeaway:** This is an online distillation technique where the teacher
    and student models are trained.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要点：** 这是一种在线蒸馏技术，其中教师模型和学生模型进行训练。'
- en: Paper 2
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 论文2
- en: '**[Distilling Knowledge Learned in BERT for Text Generation](https://arxiv.org/pdf/1911.03829.pdf)**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**[从BERT中提取知识以进行文本生成](https://arxiv.org/pdf/1911.03829.pdf)**'
- en: This paper presents a generic technique for using pre-trained language models
    to further refine text generation, excluding the specific parameter sharing, feature
    extraction, or augmenting with auxiliary tasks. Their presented Conditional MLM
    mechanism leverages unsupervised language models pre-trained on a large corpus
    followed by readjusting to supervised sequence-to-sequence tasks. The distillation
    approach they offered indirectly impacts the text generation model by delivering
    soft-label distributions only; hence is model-agnostic. Keys points are mentioned
    below.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种通用技术，用于利用预训练语言模型进一步优化文本生成，排除了特定的参数共享、特征提取或使用辅助任务进行增强。他们提出的条件MLM机制利用在大规模语料库上预训练的无监督语言模型，然后调整到监督的序列到序列任务。所提供的蒸馏方法通过仅提供软标签分布间接影响文本生成模型，因此是模型不可知的。关键点如下。
- en: MLM objective that BERT is trained with is not auto-regressive; it’s trained
    in a way that looks at both past and future context.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT训练的MLM目标不是自回归的；它以同时考虑过去和未来上下文的方式进行训练。
- en: A novel C-MLM(conditional Masked language modeling) task requires additional,
    conditional input.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新颖的C-MLM（条件蒙版语言建模）任务需要额外的条件输入。
- en: '![8 Innovative BERT Knowledge Distillation Papers That Have Changed The Landscape
    of NLP](../Images/f5f65039e7f35165e21c0e115b0b9c21.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![8篇创新性的BERT知识蒸馏论文，改变了NLP领域的格局](../Images/f5f65039e7f35165e21c0e115b0b9c21.png)'
- en: Illustration of distilling knowledge from BERT for text generation. [Source](https://arxiv.org/pdf/1911.03829.pdf)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 从BERT中提取知识以进行文本生成的示意图。 [来源](https://arxiv.org/pdf/1911.03829.pdf)
- en: Furthermore, the knowledge distillation technique used here is the same as the
    one in the original Distillation [research paper,](https://arxiv.org/abs/1503.02531)
    where we train the student network on the soft labels generated by the teacher
    network.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这里使用的知识蒸馏技术与原始蒸馏[研究论文](https://arxiv.org/abs/1503.02531)中使用的技术相同，我们在老师网络生成的软标签上训练学生网络。
- en: '**So, what makes this research paper stand out from the rest? Here is the explanation.**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**那么，这篇研究论文与其他论文相比有什么突出的地方呢？以下是解释。**'
- en: The key idea here is to distill the knowledge in BERT into a student model that
    can generate text, while previous works focused only on model compression to do
    the same task as the teacher model. Then, a fine-tuning of the BERT model is done,
    so that fine-tuned model can be used for text generation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键思想是将 BERT 中的知识蒸馏到一个能够生成文本的学生模型，而之前的工作仅关注模型压缩以完成与老师模型相同的任务。然后，对 BERT 模型进行微调，使微调后的模型可以用于文本生成。
- en: Let’s take the use case of language translation, X is the source language sentence,
    and Y is the target language sentence.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以语言翻译为例，X 是源语言句子，Y 是目标语言句子。
- en: '**First Phase:** Fine-tuning of the BERT model'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一阶段：** BERT 模型的微调'
- en: 'Input data: Concatenated X and Y with 15% of the tokens in Y randomly masked'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入数据：将 X 和 Y 连接在一起，Y 中 15% 的词汇被随机屏蔽
- en: 'Labels: masked tokens from Y'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签：来自 Y 的屏蔽词汇
- en: '**Second Phase:** Knowledge distillation of fine-tuned BERT model to Seq2Seq
    model'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二阶段：** 微调 BERT 模型到 Seq2Seq 模型的知识蒸馏'
- en: 'Teacher: fine-tuned BERT model from the first phase'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 老师：第一阶段的微调 BERT 模型
- en: 'Student: Seq2Seq model, for example, attention-based RNN, Transformer, or any
    other sequence-generation models'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学生：Seq2Seq 模型，例如基于注意力的 RNN、Transformer 或任何其他序列生成模型
- en: 'Input data & Label: soft targets from fine-tuned BERT model'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入数据和标签：来自微调 BERT 模型的软目标
- en: Paper 3
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 论文 3
- en: '**[TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/pdf/1909.10351.pdf)**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**[TinyBERT：为自然语言理解蒸馏 BERT](https://arxiv.org/pdf/1909.10351.pdf)**'
- en: The article proposes a novel Transformer distillation technique exclusively
    intended for knowledge distillation (KD) of the Transformer-based models. By leveraging
    this novel KD approach, the heap of knowledge encoded in a large teacher BERT
    can be efficaciously shifted to a small student Tiny-BERT. Then, we introduce
    a new two-stage learning framework for TinyBERT, which performs Transformer distillation
    at both the pretraining and task-specific learning stages. This framework ensures
    that TinyBERT can capture BERT's general domain and task-specific knowledge.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种新颖的 Transformer 蒸馏技术，专门用于 Transformer 基础模型的知识蒸馏 (KD)。通过利用这种新颖的 KD 方法，可以有效地将大型教师
    BERT 中编码的知识转移到小型学生 Tiny-BERT 中。然后，我们引入了一个新的两阶段学习框架，用于 TinyBERT，该框架在预训练和任务特定学习阶段都执行
    Transformer 蒸馏。该框架确保 TinyBERT 能够捕捉 BERT 的通用领域和任务特定知识。
- en: TinyBERT with four layers is empirically effective and achieves more than 96.8%
    of the performance of its teacher BERT-Base on the GLUE benchmark while being
    7.5x smaller and 9.4x faster on inference. TinyBERT with four layers is also significantly
    better than 4-layer state-of-the-art baselines on BERT distillation, with only
    about 28% parameters and about 31% inference time. Moreover, TinyBERT, with six
    layers, performs on par with its teacher BERT-Base.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 具有四层的 TinyBERT 在经验上有效，其性能达到其老师 BERT-Base 在 GLUE 基准测试中的 96.8% 以上，同时体积小 7.5 倍，推理速度快
    9.4 倍。四层的 TinyBERT 也明显优于 4 层最新的 BERT 蒸馏基准，仅使用约 28% 的参数和约 31% 的推理时间。此外，六层的 TinyBERT
    性能与其老师 BERT-Base 相当。
- en: '![8 Innovative BERT Knowledge Distillation Papers That Have Changed The Landscape
    of NLP](../Images/715cb915ee89c540289682993613dbd4.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![8 个创新的 BERT 知识蒸馏论文，改变了 NLP 的格局](../Images/715cb915ee89c540289682993613dbd4.png)'
- en: '[Source](https://arxiv.org/pdf/1909.10351.pdf)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1909.10351.pdf)'
- en: Moreover, this article proposed three main components for distilling transformer
    networks.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，本文提出了三大组件用于蒸馏 Transformer 网络。
- en: '**Transformer-layer distillation:** this includes attention-based distillation
    and hidden states-based distillation:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Transformer 层蒸馏：** 包括基于注意力的蒸馏和基于隐藏状态的蒸馏：'
- en: '![8 Innovative BERT Knowledge Distillation Papers That Have Changed The Landscape
    of NLP](../Images/fa66113c7ebdd15b47a481e784f931ba.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![8 个创新的 BERT 知识蒸馏论文，改变了 NLP 的格局](../Images/fa66113c7ebdd15b47a481e784f931ba.png)'
- en: '[Source](https://arxiv.org/pdf/1909.10351.pdf)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1909.10351.pdf)'
- en: '**Embedding layer distillation:** knowledge distillation is done for the embedding
    layer just like it was done for the hidden states-based distillation'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**嵌入层蒸馏：** 像对隐层状态进行蒸馏一样，对嵌入层进行知识蒸馏。'
- en: '**Prediction layer distillation**: knowledge distillation is done w.r.t the
    predictions obtained from the teacher model, just like in the original work of
    [Hinton](https://arxiv.org/abs/1503.02531). Moreover, the overall loss for the
    TinyBERT model combines the losses of all the three above:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预测层蒸馏：** 就像[Hinton](https://arxiv.org/abs/1503.02531)原作中一样，知识蒸馏是针对从教师模型中获得的预测进行的。此外，TinyBERT模型的整体损失结合了上述三种损失：'
- en: '![8 Innovative BERT Knowledge Distillation Papers That Have Changed The Landscape
    of NLP](../Images/a518f81a71e076428ac3af17fdb4d55d.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![8篇改变NLP领域的创新BERT知识蒸馏论文](../Images/a518f81a71e076428ac3af17fdb4d55d.png)'
- en: '[Source](https://arxiv.org/pdf/1909.10351.pdf)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1909.10351.pdf)'
- en: 'The main steps in TinyBERT training are as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: TinyBERT训练的主要步骤如下：
- en: '**General distillation**: Take original BERT without fine-tuning as a teacher
    and a large-scale text corpus as training data. Now perform the Transformer distillation
    on text from the general domain to get general TinyBERT that can be further fine-tuned
    for downstream tasks. This generic TinyBERT performs worse than BERT because of
    the fewer layers, neurons, etc.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用蒸馏：** 以未微调的原始BERT作为教师，并以大规模文本语料作为训练数据。现在对通用领域的文本进行Transformer蒸馏，得到可以进一步微调以进行下游任务的通用TinyBERT。由于层数、神经元等较少，这种通用TinyBERT的表现比BERT差。'
- en: '**Task-specific distillation**: Fine-tuned BERT is used as the teacher, and
    training data is the task-specific training set.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务特定蒸馏：** 以微调后的BERT作为教师，训练数据为任务特定训练集。'
- en: '**Key takeaway:** This is an offline distillation technique where the teacher
    model BERT is already pre-trained. Then they did two separate distillations: one
    for generic learning and another for task-specific learning. The first step of
    generic distillation involves distillation for all kinds of layers: attention
    layers, embedding layers, and prediction layers.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要点：** 这是一种离线蒸馏技术，其中教师模型BERT已经预训练完成。然后，他们进行了两个独立的蒸馏过程：一个用于通用学习，另一个用于任务特定学习。通用蒸馏的第一步涉及对各种层的蒸馏：注意力层、嵌入层和预测层。'
- en: Paper 4
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 论文4
- en: '**[**FastBERT: a Self-distilling BERT with Adaptive Inference Time**](https://arxiv.org/abs/2004.02178)**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**[**FastBERT: 一种具有自适应推理时间的自蒸馏BERT**](https://arxiv.org/abs/2004.02178)**'
- en: They propose a fresh new speed-tunable FastBERT with adaptive inference time.
    The speed at inference can be flexibly adjusted under varying demands, while redundant
    calculation of samples is avoided. Moreover, this model adopts a unique self-distillation
    mechanism for fine-tuning, further enabling a greater computational efficacy with
    minimal loss in performance. Our model achieves promising results in twelve English
    and Chinese datasets. It can speed up by a wide range from 1 to 12 times than
    BERT if given different speedup thresholds to make a speed-performance tradeoff.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 他们提出了一种全新的可调速度的FastBERT，具有自适应推理时间。推理时的速度可以根据不同需求灵活调整，同时避免了样本的冗余计算。此外，该模型采用了独特的自蒸馏机制进行微调，进一步提高了计算效率，同时性能损失最小。我们的模型在十二个英文和中文数据集上取得了令人满意的结果。如果在不同的加速阈值下进行速度与性能的折中，它的速度可以比BERT提高1到12倍的范围。
- en: 'Comparison with similar work:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 与类似工作的比较：
- en: '**TinyBERT:** performs 2-stage learning using both general-domain and task-specific
    fine-tuning.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**TinyBERT:** 通过使用通用领域和任务特定的微调来进行两阶段学习。'
- en: '**DistilBERT:** introduces triple loss'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**DistilBERT：** 引入了三重损失'
- en: What makes FastBERT better?
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FastBERT的优势是什么？
- en: This work applies **self-distillation**(training phase)and **adaptive mechanism**
    (during inference phase) techniques to NLP language models for efficiency improvements
    for the first time.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 该工作首次将**自蒸馏**（训练阶段）和**自适应机制**（推理阶段）技术应用于NLP语言模型，以提高效率。
- en: '![8 Innovative BERT Knowledge Distillation Papers That Have Changed The Landscape
    of NLP](../Images/4180862354477969f2a5171b875c4e78.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![8篇改变NLP领域的创新BERT知识蒸馏论文](../Images/4180862354477969f2a5171b875c4e78.png)'
- en: '[Source](https://arxiv.org/abs/2004.02178)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/2004.02178)'
- en: '**The Model Architecture**'
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**模型架构**'
- en: 'FastBERT model consists of backbone and branches:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: FastBERT模型由主干和分支组成：
- en: '**Backbone**: It has three parts: embedding layer, encoder containing stacks
    of Transformer blocks, and the teacher classifier. The embedding and encoder layers
    are the same as those of BERT. Finally, we have a teacher classifier that extracts
    task-specific features for downstream tasks and uses a softmax function.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**骨干**：它包含三部分：嵌入层、包含 Transformer 堆栈的编码器以及教师分类器。嵌入层和编码器层与 BERT 的相同。最后，我们有一个教师分类器，用于提取任务特定的特征，以便下游任务使用软最大函数。'
- en: '**Branches**: These contain the student classifiers that'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分支**：这些包含学生分类器'
- en: have the same architecture as the teacher
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 具有与教师相同的架构
- en: are added to the output of each transformer block to enable early outputs
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 被添加到每个变压器块的输出中，以启用早期输出
- en: '**Training Stages**'
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**训练阶段**'
- en: 'It uses separate training steps for backbone and student classifiers. Parameters
    in one module are always frozen while another module is being trained. Three steps:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对骨干和学生分类器使用单独的训练步骤。在一个模块训练时，另一个模块的参数始终被冻结。三步：
- en: '**Backbone pre-training**: Typical pre-training of the BERT model is used.
    No changes here. Highly-quality trained models can be freely loaded in this step.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**骨干预训练**：使用 BERT 模型的典型预训练。这里没有变化。可以在此步骤中自由加载高质量的训练模型。'
- en: '**Backbone fine-tuning**: For each downstream task, task-specific data is used
    to fine-tune both the backbone and teacher classifier. No student classifier is
    enabled at this stage.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**骨干微调**：对于每个下游任务，使用任务特定的数据来微调骨干和教师分类器。在这个阶段，没有启用学生分类器。'
- en: '**Self-distillation of student classifiers**: Now that our teacher model is
    well-trained, we take its output. This soft-label output is high-quality, containing
    both original embedding and generalized knowledge. These soft labels are used
    to train the student classifiers. We are free to use an unlimited amount of unlabeled
    data here. *This work differs from previous work in that this work uses the same
    model for teacher and student models.*'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**学生分类器的自蒸馏**：现在我们的教师模型已经训练好，我们获取其输出。这种软标签输出质量高，包含原始嵌入和概括性知识。这些软标签用于训练学生分类器。我们可以在这里自由使用无限量的未标记数据。*这项工作不同于以往的工作，因为这项工作使用相同的模型作为教师和学生模型。*'
- en: '**Adaptive inference**'
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**自适应推断**'
- en: Let’s talk about inference time. With FastBERT, the inference is performed adaptively,
    i.e., the number of executed encoding layers within the model can be adjusted
    according to input sample complexity.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈推断时间。使用 FastBERT 时，推断是自适应执行的，即模型内执行的编码层数可以根据输入样本的复杂性进行调整。
- en: 'At each transformer layer, the uncertainty of a student classifier’s output
    is computed, and it is determined if the inference can be terminated depending
    upon a threshold. Here is how the adaptive inference mechanism works:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个变压器层中，计算学生分类器输出的不确定性，并根据阈值确定是否可以终止推断。以下是自适应推断机制的工作原理：
- en: At each layer of FastBERT, the corresponding student classifier predicts the
    label of each sample with measured uncertainty.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 FastBERT 的每一层中，相应的学生分类器预测每个样本的标签，并测量不确定性。
- en: Samples with an uncertainty below a certain threshold will be sifted to early
    outputs, while ones with uncertainty above the threshold will move onto the next
    layer.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不确定性低于某个阈值的样本将被筛选到早期输出，而不确定性高于阈值的样本将转移到下一层。
- en: With a higher threshold, fewer samples are sent to higher layers keeping the
    inference speed faster and vice versa.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阈值较高时，较少的样本被发送到更高层，以保持推断速度更快，反之亦然。
- en: Paper 5
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 论文 5
- en: '**[Distilling Task-Specific Knowledge from BERT into Simple Neural Networks](https://arxiv.org/abs/1903.12136)**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**[从 BERT 中提取任务特定知识到简单神经网络](https://arxiv.org/abs/1903.12136)**'
- en: 'In this paper, the authors exhibit that rudimentary, lightweight neural networks
    can even be made competitive apart from architecture modifications, external training
    data, or additional input features. They propose distilling knowledge from BERT
    into a single-layer, bidirectional long short-term memory network (BiLSTM) and
    its siamese equivalent for sentence-pair tasks. Throughout numerous datasets in
    paraphrasing, natural language inference, and sentiment classification, they achieve
    comparable outcomes with ELMo, while using roughly 100 times fewer parameters
    and 15 times less inference time. Further, their approach includes a fine-tuned
    BERT for teacher and BiLSTM student models. The primary motivation of this work
    comprises as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，作者展示了即使没有架构修改、外部训练数据或额外输入特征，基础的轻量级神经网络也可以具有竞争力。他们提出将 BERT 知识蒸馏到单层双向长短期记忆网络（BiLSTM）及其同类模型，用于句子对任务。在大量的重述、自然语言推理和情感分类数据集中，他们在参数量大约少
    100 倍，推理时间减少 15 倍的情况下，取得了与 ELMo 相当的结果。此外，他们的方法包括对教师和 BiLSTM 学生模型进行微调。此工作的主要动机包括：
- en: Can a simple architecture model capture the representation power for text modeling
    at a level of the BERT model?
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 简单的架构模型能否在文本建模中捕捉到与 BERT 模型相当的表示能力？
- en: Study effective approaches to transfer knowledge from BERT to a BiLSTM model.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 研究将知识从 BERT 转移到 BiLSTM 模型的有效方法。
- en: '![8 Innovative BERT Knowledge Distillation Papers That Have Changed The Landscape
    of NLP](../Images/212a49d0abfaf34cf4a553e2c619981c.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![8 篇改变 NLP 领域的创新 BERT 知识蒸馏论文](../Images/212a49d0abfaf34cf4a553e2c619981c.png)'
- en: '[Source](https://arxiv.org/pdf/1903.12136.pdf) | [Reference video by paper
    authors](https://www.youtube.com/watch?v=AKCPPvaz8tU)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1903.12136.pdf) | [论文作者的参考视频](https://www.youtube.com/watch?v=AKCPPvaz8tU)'
- en: Data Augmentation for Distillation
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蒸馏的数据增强
- en: 'A small dataset may not be sufficient for teachers to express their knowledge
    fully, so the training set is augmented using a large unlabeled dataset with pseudo-labels
    generated from the teacher model. In this work, a few heuristics are proposed
    for task-agnostic data augmentation:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 小数据集可能不足以让教师完全表达其知识，因此使用从教师模型生成的伪标签的大型未标记数据集来扩充训练集。在这项工作中，提出了一些用于任务无关的数据增强的启发式方法：
- en: '**Masking**: randomly replace a word in a sentence with a [MASK] token similar
    to BERT training.'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**掩码**：随机将句子中的一个词替换为类似于 BERT 训练的 [MASK] 标记。'
- en: 'POS-guided word replacement: replace a word with another word of the same POS(parts
    of speech) tag, e.g., “What do pigs eat?” is perturbed to “How do pigs eat?'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: POS 引导的词替换：用同一词性标签的另一个词替换句子中的一个词，例如，“猪吃什么？”被扰动为“猪怎么吃？”
- en: '**N-gram sampling**: a more aggressive form of masking where n-gram samples
    are chosen from the input example, where n is randomly selected from {1,2,3,4,5}'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**N-gram 采样**：一种更激进的掩码形式，从输入示例中选择 n-gram 样本，其中 n 从 {1,2,3,4,5} 中随机选择。'
- en: Paper 6
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 论文 6
- en: '**[Patient Knowledge Distillation for BERT Model Compression](https://arxiv.org/abs/1908.09355)**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**[BERT 模型压缩的患者知识蒸馏](https://arxiv.org/abs/1908.09355)**'
- en: 'The authors propose a Patient Knowledge Distillation approach to compress an
    original large model (teacher) into an equally-effective lightweight shallow network
    (student). Their method is quite distinct from previous knowledge distillation
    approaches because the earlier methods only use the output from the last layer
    of the teacher network for distillation; our student model patiently learns from
    multiple intermediate layers of the teacher model for incremental knowledge extraction,
    following two strategies:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提出了一种患者知识蒸馏方法，将原始的大型模型（教师）压缩成一个同样有效的轻量级浅层网络（学生）。他们的方法与之前的知识蒸馏方法有很大不同，因为早期的方法仅使用教师网络最后一层的输出进行蒸馏；而我们的学生模型则耐心地从教师模型的多个中间层中学习，以进行渐进的知识提取，遵循两种策略：
- en: '**PKD-Last**: student model learns from the last *k* layers of the teacher
    (assuming that the last layers contain the max information for the student).'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**PKD-Last**：学生模型从教师的最后 *k* 层中学习（假设最后几层包含了对学生最重要的信息）。'
- en: '**PKD-Skip**: student model learns from every *k* layer of the teacher.'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**PKD-Skip**：学生模型从教师的每个 *k* 层中学习。'
- en: They experimented on several datasets across different NLP tasks demonstrating
    that the proposed PKD approach achieves better performance and generalization
    than standard distillation methods [(Hinton et al., 2015)](https://arxiv.org/abs/1503.02531).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 他们在多个数据集和不同的NLP任务上进行了实验，证明所提出的PKD方法比标准蒸馏方法[(Hinton et al., 2015)](https://arxiv.org/abs/1503.02531)表现更好，泛化能力更强。
- en: '![8 Innovative BERT Knowledge Distillation Papers That Have Changed The Landscape
    of NLP](../Images/f75d5817cf6540620b6cb35634fc5ac3.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![8种创新的BERT知识蒸馏论文，改变了NLP的格局](../Images/f75d5817cf6540620b6cb35634fc5ac3.png)'
- en: '[Source](https://arxiv.org/pdf/1908.09355.pdf)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1908.09355.pdf)'
- en: '**Why not learn from all the hidden states of the teacher model?**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么不从教师模型的所有隐藏状态中学习？**'
- en: The reason is that it can be computationally very expensive and can introduce
    noise into the student model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是这可能会计算上非常昂贵，并且可能会给学生模型引入噪声。
- en: Paper 7
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 论文7
- en: '**[**MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices**](https://arxiv.org/pdf/2004.02984.pdf)**'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**[**MobileBERT：一种紧凑的无任务专用BERT，适用于资源受限设备**](https://arxiv.org/pdf/2004.02984.pdf)**'
- en: They propose MobileBERT for compressing and accelerating the popular BERT model.
    Like the original BERT, MobileBERT is task-agnostic; that is, it can be generically
    applied to various downstream NLP tasks via simple fine-tuning. MobileBERT is
    a thin version of BERTʟᴀʀɢᴇ, while equipped with bottleneck structures and a carefully
    designed balance between self-attentions and feed-forward networks.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 他们提出了MobileBERT，用于压缩和加速流行的BERT模型。像原始BERT一样，MobileBERT是不依赖任务的；即，通过简单的微调，可以通用地应用于各种下游NLP任务。MobileBERT是BERTʟᴀʀɢᴇ的精简版本，同时配备了瓶颈结构，并在自注意力机制和前馈网络之间进行了精心设计的平衡。
- en: Training Steps
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练步骤
- en: '*Step one:* first train a specially designed teacher model, an inverted bottleneck
    incorporated BERTʟᴀʀɢᴇ model.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*第一步：* 首先训练一个特别设计的教师模型，即包含反向瓶颈的BERTʟᴀʀɢᴇ模型。'
- en: '*Step two:* conduct knowledge transfer from this teacher to MobileBERT.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*第二步：* 从这位教师模型向MobileBERT进行知识转移。'
- en: '![8 Innovative BERT Knowledge Distillation Papers That Have Changed The Landscape
    of NLP](../Images/7dab52f268b86516dd1bb59b31f4d9ce.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![8种创新的BERT知识蒸馏论文，改变了NLP的格局](../Images/7dab52f268b86516dd1bb59b31f4d9ce.png)'
- en: Architecture visualization of transformer blocks within (a) BERT, (b) MobileBERT
    teacher, and © MobileBERT student. The green trapezoids marked with “Linear” are
    referred to as bottlenecks. [Source](https://arxiv.org/pdf/2004.02984.pdf)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器块的架构可视化，如(a) BERT，(b) MobileBERT教师，以及(c) MobileBERT学生。标记为“Linear”的绿色梯形图被称为瓶颈。[来源](https://arxiv.org/pdf/2004.02984.pdf)
- en: (a) BERT; (b) Inverted-Bottleneck BERT (IB-BERT); and © MobileBERT. In (b) and
    ©, red lines denote inter-block flows while blue lines intra-block flow. MobileBERT
    is trained by layer-to-layer imitating IB-BERT.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: (a) BERT；(b) 反向瓶颈BERT（IB-BERT）；以及© MobileBERT。在(b)和©中，红色线条表示块间流动，而蓝色线条表示块内流动。MobileBERT通过逐层模仿IB-BERT进行训练。
- en: If you have made it this far, you deserve a high-five. MobileBERT presents *bottlenecks*
    in transformer blocks, which distills the knowledge out of larger teachers into
    smaller students more smoothly. This approach decreases the width instead of the
    depth of the student, which is famous for generating a more proficient model which
    yields true in the given experiments. MobileBERT underlines the conviction that
    it’s achievable to make a student model that can be fine-tuned after the initial
    distillation process.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你读到这里，你值得一个击掌。MobileBERT在变压器块中展示了*瓶颈*，这使得从更大的教师模型中提取知识到更小的学生模型的过程更加平滑。这种方法减少了学生模型的宽度，而不是深度，这在给定的实验中产生了更高效的模型。MobileBERT强调了这样一个信念，即在初始蒸馏过程之后，确实可以使学生模型进行微调。
- en: Moreover, the outcomes also indicate that this holds true in practice, as MobileBERT
    can attain 99.2% of BERT-base’s performance on GLUE with 4x fewer parameters and
    5.5x faster inference on a Pixel 4 phone!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，结果还表明，这在实践中也成立，因为MobileBERT可以在GLUE上达到BERT-base 99.2%的性能，同时参数减少4倍，并且在Pixel
    4手机上的推理速度快5.5倍！
- en: Paper 8
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 论文8
- en: '**[Improving Multi-Task Deep Neural Networks via Knowledge Distillation for
    Natural Language Understanding](https://arxiv.org/pdf/1904.09482.pdf)**'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**[通过知识蒸馏提升多任务深度神经网络以实现自然语言理解](https://arxiv.org/pdf/1904.09482.pdf)**'
- en: 'The key focus of the paper is as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的关键关注点如下：
- en: Train a multi-task neural net model which combines loss across multiple natural
    language understanding tasks.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个多任务神经网络模型，该模型结合了多个自然语言理解任务的损失。
- en: Generate an ensemble of multiple models from the first step, which are essentially
    obtained by training multiple multi-task models from scratch
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从第一步生成多个模型的集成，这些模型本质上是通过从头开始训练多个多任务模型获得的。
- en: The final step is to knowledge distill the ensemble of models from the previous
    step.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终步骤是对前一步的模型集进行知识蒸馏。
- en: '![8 Innovative BERT Knowledge Distillation Papers That Have Changed The Landscape
    of NLP](../Images/d9432bfc820cd0b1fa3185cf01948489.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![8种创新的BERT知识蒸馏论文，改变了自然语言处理的格局](../Images/d9432bfc820cd0b1fa3185cf01948489.png)'
- en: The architecture of the MT-DNN model for representation learning [(Liu et al.,
    2019)](https://arxiv.org/abs/1902.10461). The lower layers are shared across all
    tasks, while the top layers are task-specific. The input X (either a sentence
    or a set of sentences) is first represented as a sequence of embedding vectors,
    one for each word, in l1\. Then the Transformer encoder captures the contextual
    information for each word and generates the shared contextual embedding vectors
    in l2\. Finally, additional task-specific layers generate task-specific representations
    for each task, followed by operations necessary for classification, similarity
    scoring, or relevance ranking. [Source](https://arxiv.org/pdf/1904.09482.pdf)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: MT-DNN模型用于表示学习的架构 [(Liu et al., 2019)](https://arxiv.org/abs/1902.10461)。较低层在所有任务中共享，而顶部层则是任务特定的。输入X（可以是一个句子或一组句子）首先表示为一系列嵌入向量，每个单词一个，在l1层中。然后，Transformer编码器捕捉每个单词的上下文信息，并在l2层中生成共享的上下文嵌入向量。最后，额外的任务特定层生成每个任务的任务特定表示，随后进行分类、相似性评分或相关性排序所需的操作。[来源](https://arxiv.org/pdf/1904.09482.pdf)
- en: '![8 Innovative BERT Knowledge Distillation Papers That Have Changed The Landscape
    of NLP](../Images/bbd4356b24f3d66d264051e9a03dfa6d.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![8种创新的BERT知识蒸馏论文，改变了自然语言处理的格局](../Images/bbd4356b24f3d66d264051e9a03dfa6d.png)'
- en: Process of knowledge distillation for multi-task learning. A set of tasks where
    there is task-specific labeled training data are picked. Then, an ensemble of
    different neural nets (teacher) is trained for each task. The teacher is used
    to generate a set of soft targets for each task-specific training sample. Given
    the soft targets of the training datasets across multiple tasks, a single MT-DNN
    (student) is trained using multi-task learning and backpropagation as described
    in Algorithm 1, except that if task t has a teacher, the task-specific loss in
    Line 3 is the average of two objective functions, one for the correct targets
    and the other for the soft targets assigned by the teacher. [Source](https://arxiv.org/pdf/1904.09482.pdf)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习的知识蒸馏过程。选择一组具有任务特定标签训练数据的任务。然后，为每个任务训练一个不同的神经网络（教师）。教师用于生成每个任务特定训练样本的一组软目标。考虑到多个任务的训练数据集的软目标，使用多任务学习和反向传播来训练一个单一的MT-DNN（学生），如算法1所述，除了如果任务t有教师，则第3行的任务特定损失是两个目标函数的平均值，一个用于正确目标，另一个用于教师分配的软目标。[来源](https://arxiv.org/pdf/1904.09482.pdf)
- en: 'Achievements: On the GLUE datasets, the distilled MT-DNN creates a new state-of-the-art
    result on 7 out of 9 NLU tasks, including the tasks with no teacher, pushing the
    GLUE benchmark (single model) to 83.7%.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 成就：在GLUE数据集上，蒸馏的MT-DNN在9个自然语言理解任务中的7个任务上创造了新的最先进结果，包括没有教师的任务，将GLUE基准（单模型）推高到83.7%。
- en: We show that the distilled MT-DNN retains nearly all of the improvements achieved
    by ensemble models while keeping the model size the same as the vanilla MT-DNN
    model.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了蒸馏的MT-DNN几乎保留了由集成模型所取得的所有改进，同时保持模型大小与原始MT-DNN模型相同。
- en: '**The EndNote**'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**附注**'
- en: Contemporary state-of-the-art NLP models are difficult to be utilized in production.
    Knowledge distillation offers tools for tackling such issues along with several
    others, but it has its quirks.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现代最先进的自然语言处理模型在生产中难以应用。知识蒸馏提供了应对这些问题及其他问题的工具，但它也有其独特之处。
- en: References
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[**Distilling the Knowledge in a Neural Network**'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**在神经网络中蒸馏知识**](https://arxiv.org/abs/1503.02531)'
- en: '*A very simple way to improve the performance of almost any machine learning
    algorithm is to train many different models…*arxiv.org](https://arxiv.org/abs/1503.02531)'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*提高几乎任何机器学习算法性能的一个非常简单的方法是训练多种不同的模型……*arxiv.org](https://arxiv.org/abs/1503.02531)'
- en: '[**BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**BERT：深度双向转换器的预训练用于语言理解**](https://arxiv.org/abs/1503.02531)'
- en: '*We introduce a new language representation model called BERT, which stands
    for Bidirectional Encoder Representations…*arxiv.org](https://arxiv.org/abs/1810.04805)'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*我们介绍了一种新的语言表示模型，称为BERT，代表双向编码器表示…*arxiv.org](https://arxiv.org/abs/1810.04805)'
- en: '[**Distilling Knowledge Learned in BERT for Text Generation**'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**从BERT中提取知识用于文本生成**](https://arxiv.org/abs/1910.01108)'
- en: '*Large-scale pre-trained language model such as BERT has achieved great success
    in language understanding tasks…*arxiv.org](https://arxiv.org/abs/1911.03829)'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*大规模预训练语言模型如BERT在语言理解任务中取得了巨大成功…*arxiv.org](https://arxiv.org/abs/1911.03829)'
- en: '[**DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter**'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**DistilBERT，一个BERT的蒸馏版本：更小、更快、更便宜、更轻量**](https://arxiv.org/abs/1910.01108)'
- en: '*As Transfer Learning from large-scale pre-trained models becomes more prevalent
    in Natural Language Processing (NLP)…*arxiv.org](https://arxiv.org/abs/1910.01108)'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*随着大规模预训练模型在自然语言处理（NLP）中的迁移学习变得越来越普遍…*arxiv.org](https://arxiv.org/abs/1910.01108)'
- en: '[**TinyBERT: Distilling BERT for Natural Language Understanding**'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**TinyBERT：为自然语言理解蒸馏BERT**](https://arxiv.org/abs/1910.01108)'
- en: '*Language model pre-training, such as BERT, has significantly improved the
    performances of many natural language…*arxiv.org](https://arxiv.org/abs/1909.10351)'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*语言模型预训练，如BERT，显著提升了许多自然语言处理任务的性能…*arxiv.org](https://arxiv.org/abs/1909.10351)'
- en: '[**Distilling the Knowledge in a Neural Network**'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**在神经网络中提取知识**](https://arxiv.org/abs/2004.02178)'
- en: '*A very simple way to improve the performance of almost any machine learning
    algorithm is to train many different models…*arxiv.org](https://arxiv.org/abs/1503.02531)'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*提高几乎所有机器学习算法性能的一种非常简单的方法是训练许多不同的模型…*arxiv.org](https://arxiv.org/abs/1503.02531)'
- en: '[**FastBERT: a Self-distilling BERT with Adaptive Inference Time**'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**FastBERT：一种自蒸馏BERT，具有自适应推理时间**](https://arxiv.org/abs/1910.01108)'
- en: '*Pre-trained language models like BERT have proven to be highly performant.
    However, they are often computationally…*arxiv.org](https://arxiv.org/abs/2004.02178)'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*预训练语言模型如BERT已被证明非常高效。然而，它们通常在计算上…*arxiv.org](https://arxiv.org/abs/2004.02178)'
- en: '[**Distilling Task-Specific Knowledge from BERT into Simple Neural Networks**'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**从BERT中提取特定任务知识到简单神经网络**](https://arxiv.org/abs/1910.01108)'
- en: '*In the natural language processing literature, neural networks are becoming
    increasingly deeper and complex. The recent…*arxiv.org](https://arxiv.org/abs/1903.12136)'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*在自然语言处理文献中，神经网络变得越来越深且复杂。最近…*arxiv.org](https://arxiv.org/abs/1903.12136)'
- en: '[**Patient Knowledge Distillation for BERT Model Compression**'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**用于BERT模型压缩的患者知识蒸馏**](https://arxiv.org/abs/1910.01108)'
- en: '*Pre-trained language models such as BERT have proven to be highly effective
    for natural language processing (NLP)…*arxiv.org](https://arxiv.org/abs/1908.09355)'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*预训练语言模型如BERT已被证明在自然语言处理（NLP）中非常有效…*arxiv.org](https://arxiv.org/abs/1908.09355)'
- en: '[**Distilling the Knowledge in a Neural Network**'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**在神经网络中提取知识**](https://arxiv.org/abs/1910.01108)'
- en: '*A very simple way to improve the performance of almost any machine learning
    algorithm is to train many different models…*arxiv.org](https://arxiv.org/abs/1503.02531)'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*提高几乎所有机器学习算法性能的一种非常简单的方法是训练许多不同的模型…*arxiv.org](https://arxiv.org/abs/1503.02531)'
- en: '[**MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices**'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**MobileBERT：一种紧凑型任务无关BERT，适用于资源受限设备**](https://arxiv.org/abs/1910.01108)'
- en: '*Natural Language Processing (NLP) has recently achieved great success by using
    huge pre-trained models with hundreds of…*arxiv.org](https://arxiv.org/abs/2004.02984)'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*自然语言处理（NLP）最近通过使用具有数百亿参数的大型预训练模型取得了巨大成功…*arxiv.org](https://arxiv.org/abs/2004.02984)'
- en: '[**Improving Multi-Task Deep Neural Networks via Knowledge Distillation for
    Natural Language…**'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**通过知识蒸馏改进多任务深度神经网络，以实现自然语言处理…**](https://arxiv.org/abs/1910.01108)'
- en: '*This paper explores the use of knowledge distillation to improve a Multi-Task
    Deep Neural Network (MT-DNN) (Liu et al…*arxiv.org](https://arxiv.org/abs/1904.09482)'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*本文探讨了使用知识蒸馏来改进多任务深度神经网络（MT-DNN）（Liu et al…*arxiv.org](https://arxiv.org/abs/1904.09482)'
- en: '"Opinions expressed here are of Mr. Abhishek, not his employer"'
  id: totrans-169
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '"这里表达的观点是Mr. Abhishek的个人观点，与他的雇主无关"'
- en: '**[Kumar Abhishek](https://www.linkedin.com/in/kumarabhisheknitt/)** is Machine
    Learning Engineer at Expedia, working in the domain of fraud detection and prevention.
    He uses Machine Learning and Natural Language Processing models for risk analysis
    and fraud detection. He has more than a decade of machine learning and software
    engineering experience.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Kumar Abhishek](https://www.linkedin.com/in/kumarabhisheknitt/)** 是Expedia的机器学习工程师，专注于欺诈检测和预防领域。他使用机器学习和自然语言处理模型进行风险分析和欺诈检测。他拥有超过十年的机器学习和软件工程经验。'
- en: '* * *'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT 部门'
- en: '* * *'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Data Science Has Changed, Not Died!](https://www.kdnuggets.com/2023/08/data-science-changed-died.html)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学已经改变，而非消亡！](https://www.kdnuggets.com/2023/08/data-science-changed-died.html)'
- en: '[Overcoming Barriers in Multi-lingual Voice Technology: Top 5…](https://www.kdnuggets.com/2023/08/overcoming-barriers-multilingual-voice-technology-top-5-challenges-innovative-solutions.html)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[克服多语言语音技术的障碍：前五大挑战及创新解决方案](https://www.kdnuggets.com/2023/08/overcoming-barriers-multilingual-voice-technology-top-5-challenges-innovative-solutions.html)'
- en: '[Must Read NLP Papers from the Last 12 Months](https://www.kdnuggets.com/2023/03/must-read-nlp-papers-last-12-months.html)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[过去 12 个月必读的 NLP 论文](https://www.kdnuggets.com/2023/03/must-read-nlp-papers-last-12-months.html)'
- en: '[Research Papers for NLP Beginners](https://www.kdnuggets.com/2022/11/research-papers-nlp-beginners.html)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NLP 初学者的研究论文](https://www.kdnuggets.com/2022/11/research-papers-nlp-beginners.html)'
- en: '[Data Engineering Landscape in the AI-Driven World](https://www.kdnuggets.com/2023/05/data-engineering-landscape-aidriven-world.html)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AI 驱动世界中的数据工程景观](https://www.kdnuggets.com/2023/05/data-engineering-landscape-aidriven-world.html)'
- en: '[Evolution of the Data Landscape](https://www.kdnuggets.com/2023/06/evolution-data-landscape.html)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据景观的演变](https://www.kdnuggets.com/2023/06/evolution-data-landscape.html)'
