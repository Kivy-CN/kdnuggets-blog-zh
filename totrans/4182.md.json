["```py\n# Setup conda and install dependencies needed by PyTorch\nconda install astunparse numpy ninja pyyaml mkl mkl-include setuptools cmake cffi typing_extensions future six requests dataclasses\n\n# Get the PyTorch source code from github\ncd /home/$USER/\n# Repo will be cloned into /home/$USER/pytorch/\ngit clone --recursive https://github.com/pytorch/pytorch\ncd pytorch\n# if you are updating an existing checkout\ngit submodule sync\ngit submodule update --init --recursive --jobs 0\n\n# Build PyTorch\nexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which conda))/../\"}\npython setup.py install\n\n```", "```py\n#include <ATen/ATen.h>\n#include <torch/library.h>\n#include <torch/csrc/jit/mobile/import.h>\n#include \n\nusing namespace std;\n\nint main() {\n  // Load the PyTorch model in lite interpreter format.\n  torch::jit::mobile::Module model = torch::jit::_load_for_mobile(\n    \"AddTensorsModelOptimized.ptl\");\n\n  std::vector inputs;\n\n  // Create 2 float vectors with values for the 2 input tensors.\n  std::vector v1 = {1.0, 5.0, -11.0};\n  std::vector v2 = {10.0, -15.0, 9.0};\n\n  // Create tensors efficiently from the float vector using the\n  // from_blob() API.\n  std::vector dims{static_cast(v1.size())};\n  at::Tensor t1 = at::from_blob(\n    v1.data(), at::IntArrayRef(dims), at::kFloat);\n  at::Tensor t2 = at::from_blob(\n    v2.data(), at::IntArrayRef(dims), at::kFloat);\n\n  // Add tensors to inputs vector.\n  inputs.push_back(t1);\n  inputs.push_back(t2);\n\n  // Run the model and capture results in 'ret'.\n  c10::IValue ret = model.forward(inputs);\n\n  // Print the return value.\n  std::cout << \"Return Value:\\n\" << ret << std::endl;\n\n  // You can also convert the return value into a tensor and\n  // fetch the underlying values using the data_ptr() method.\n  float *data = ret.toTensor().data_ptr();\n  const int numel = ret.toTensor().numel();\n\n  // Print the data buffer.\n  std::cout << \"\\nData Buffer: \";\n  for (int i = 0; i < numel; ++i) {\n    std::cout << data[i];\n    if (i != numel - 1) {\n      std::cout << \", \";\n    }\n  }\n  std::cout << std::endl;\n}\n\n```", "```py\n# This is where your PTDemo.cpp file is\ncd /home/$USER/ptdemo/\n\n# Set LD_LIBRARY_PATH so that the runtime linker can\n# find the .so files\nLD_LIBRARY_PATH=/home/$USER/pytorch/build/lib/\nexport LD_LIBRARY_PATH\n\n# Compile the PTDemo.cpp file. The command below should\n# produce a file named 'a.out'\ng++ PTDemo.cpp \\\n  -I/home/$USER/pytorch/torch/include/ \\\n  -L/home/$USER/pytorch/build/lib/ \\\n  -lc10 -ltorch_cpu -ltorch\n\n```", "```py\n ./a.out\n\n```", "```py\nReturn Value:\n 11\n -5\n  8\n[ CPUFloatType{3} ]\n\nData Buffer: 11, -5, 8\n\n```"]