- en: 20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 2)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2020年你需要了解的20个人工智能、数据科学、机器学习术语（第二部分）
- en: 原文：[https://www.kdnuggets.com/2020/03/ai-data-science-machine-learning-key-terms-part2.html](https://www.kdnuggets.com/2020/03/ai-data-science-machine-learning-key-terms-part2.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/03/ai-data-science-machine-learning-key-terms-part2.html](https://www.kdnuggets.com/2020/03/ai-data-science-machine-learning-key-terms-part2.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: This is the 2nd part of our list of 20 AI, Data Science, Machine Learning terms
    to know for 2020.  Here is [**20 AI, Data Science, Machine Learning Terms You
    Need to Know in 2020 (Part 1)**](https://www.kdnuggets.com/2020/02/ai-data-science-machine-learning-key-terms-2020.html).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们列出的2020年需要了解的20个人工智能、数据科学、机器学习术语的第二部分。这里是[**2020年你需要了解的20个人工智能、数据科学、机器学习术语（第一部分）**](https://www.kdnuggets.com/2020/02/ai-data-science-machine-learning-key-terms-2020.html)。
- en: Those definitions were compiled by KDnuggets Editors [Matthew Dearing](/author/matthew-dearing),
    [Matthew Mayo](/author/matt-mayo), [Asel Mendis](/author/asel-mendis), and [Gregory
    Piatetsky](/author/gregory-piatetsky).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这些定义由KDnuggets编辑[Matthew Dearing](/author/matthew-dearing)、[Matthew Mayo](/author/matt-mayo)、[Asel
    Mendis](/author/asel-mendis)和[Gregory Piatetsky](/author/gregory-piatetsky)编纂。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行IT工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In this installment, we explain
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们解释了
- en: Double Descent
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双重下降
- en: Ethics in AI
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能中的伦理
- en: Explainability (Explainable AI)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释性（可解释的AI）
- en: Full Stack Data Science
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全栈数据科学
- en: Geospatial
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 地理空间
- en: GPT-2
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2
- en: NLG (Natural Language Generation)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLG（自然语言生成）
- en: PyTorch
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch
- en: Reinforcement Learning
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: Transformer Architecture
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer架构
- en: Double Descent
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 双重下降
- en: This is a really interesting concept, which [Pedro Domingos](https://en.wikipedia.org/wiki/Pedro_Domingos),
    a leading AI researcher, called one of the most significant advances in ML theory
    in 2019.  The phenomenon is shown in Figure 1.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常有趣的概念，[Pedro Domingos](https://en.wikipedia.org/wiki/Pedro_Domingos)——一位领先的人工智能研究员，称其为2019年机器学习理论中最重要的进展之一。现象如图1所示。
- en: '![OpenAI Double Descent graph](../Images/74f2dd93c859947a79b736628f930f7f.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![OpenAI Double Descent graph](../Images/74f2dd93c859947a79b736628f930f7f.png)'
- en: '**Fig. 1:  Test/Train Error vs Model Size (Source OpenAI** [**blog**](https://openai.com/blog/deep-double-descent/)**)**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1： 测试/训练误差与模型规模（来源：OpenAI** [**博客**](https://openai.com/blog/deep-double-descent/)**)**'
- en: The error first declines as the model gets larger, then the error increases
    as the model begins to overfit, but then the error declines again with the increasing
    model size, data size, or training time.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型变大，误差首先下降，然后在模型开始过拟合时误差增加，但随后随着模型规模、数据量或训练时间的增加，误差再次下降。
- en: The classical statistical theory says that the bigger model will be worse because
    of overfitting.  However, the modern ML practice shows that a very big Deep Learning
    model is usually better than a smaller one.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 经典统计理论认为，更大的模型因过拟合而表现更差。然而，现代机器学习实践表明，非常大的深度学习模型通常比小模型更好。
- en: OpenAI [blog](https://openai.com/blog/deep-double-descent/) notes that this
    occurs in CNNs, ResNets, and transformers. OpenAI researchers observed that when
    model is not large enough to fit the training set, larger models had higher test
    error. However, after passing this threshold, larger models with more data started
    performing better.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI [博客](https://openai.com/blog/deep-double-descent/) 说明这在CNN、ResNets和transformers中都会发生。OpenAI的研究人员观察到，当模型不够大以适应训练集时，更大的模型测试误差更高。然而，经过这一阈值后，随着数据的增加，更大的模型开始表现更好。
- en: Read the original OpenAI blog and a longer [explanation](https://towardsdatascience.com/deep-double-descent-when-more-data-and-bigger-models-are-a-bad-thing-3a3f108d5538)
    by Rui Aguiar.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读原始的OpenAI博客以及Rui Aguiar提供的更长[解释](https://towardsdatascience.com/deep-double-descent-when-more-data-and-bigger-models-are-a-bad-thing-3a3f108d5538)。
- en: '*Written by [Gregory Piatetsky](/author/gregory-piatetsky)*.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*作者：[Gregory Piatetsky](/author/gregory-piatetsky)*'
- en: '[Ethics](/tag/ethics/) in AI'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[人工智能中的伦理](/tag/ethics/)'
- en: Ethics in AI is concerned with the ethics of practical artificial intelligence
    technology.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能伦理关注于实际人工智能技术的伦理问题。
- en: AI ethics is a very broad field, and encompasses a wide variety of seemingly
    very different aspects of ethical concern. Concerns over the use of AI, and of
    all types of technology, more generally, have existed for as long as these technologies
    were first conceived of. Yet given the recent explosion of AI, machine learning,
    and related technologies, and their increasingly quick adoption and integration
    into society at large, these ethical concerns have risen to the forefront of many
    minds both within and outside of the AI community.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能伦理是一个非常广泛的领域，涵盖了各种看似截然不同的伦理问题。对于人工智能以及更广泛的所有技术的使用的担忧，自这些技术首次构思之时就已存在。然而，鉴于人工智能、机器学习和相关技术的近期爆炸式增长及其日益快速地被社会广泛接受和整合，这些伦理问题已成为人工智能社区内外许多人关注的焦点。
- en: While esoteric and currently abstract ethical concerns such as the potential
    future rights of sentient robots can also be included under the umbrella of AI
    ethics, more pressing contemporary concerns such as AI system transparency, the
    potential biases of these systems, and the representative inclusion of all categories
    of society participants in the engineering of said systems are likely of much
    greater and immediate concern to most people. How are decisions being made in
    AI systems? What assumptions are these systems making about the world and the
    people in it? Are these systems crafted by a single dominant majority class, gender,
    and race of society at large?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像未来有感知的机器人可能拥有的权利这样的深奥且目前尚未具象的伦理问题也可以纳入人工智能伦理的范畴，但像人工智能系统的透明度、这些系统的潜在偏见以及所有社会参与者的代表性纳入等更紧迫的当代问题，显然对大多数人更为重要且直接。人工智能系统中的决策是如何做出的？这些系统对世界和其中的人有什么假设？这些系统是由社会中占主导地位的单一多数阶层、性别和种族群体创建的吗？
- en: 'Rachel Thomas, Director of the USF Center for Applied Data Ethics, [has stated](https://www.fast.ai/2018/09/24/ai-ethics-resources/)
    the following about what constitutes working on AI ethics, which goes beyond concerns
    related directly and solely to the lower-level creation of AI systems, and takes
    into account the proverbial bigger picture:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 美国南方大学应用数据伦理中心主任Rachel Thomas，[已声明](https://www.fast.ai/2018/09/24/ai-ethics-resources/)了关于什么构成从事人工智能伦理工作的观点，这不仅涉及直接和仅仅与人工智能系统的低层次创建相关的问题，还考虑了所谓的更大图景：
- en: '**founding tech companies and building products in ethical ways;'
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**以伦理方式创办科技公司和建设产品；'
- en: advocating and working for more just laws and policies;
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '-   倡导和致力于制定更公正的法律和政策；'
- en: attempting to hold bad actors accountable;
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '-   尝试追究恶意行为者的责任；'
- en: and research, writing, and teaching in the field.**
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '-   以及在该领域进行研究、写作和教学。**'
- en: The dawn of autonomous vehicles has presented additional specific challenges
    related to AI ethics, as have the potential weaponization of AI systems, and a
    growing international AI arms race. Contrary to what some would have us believe,
    these aren't problems predestined for a dystopian future, yet they are problems
    which will require some critical thought, proper preparation, and extensive cooperation.
    Even with what we may believe to be adequate consideration, AI systems might still
    prove to be uniquely and endemically problematic, and the unintended consequences
    of AI systems, another aspect of AI ethics, will need to be considered. *Written
    by [Matthew Mayo](/author/matt-mayo)*
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶汽车的出现带来了与人工智能伦理相关的额外特定挑战，包括人工智能系统的潜在武器化以及日益增长的国际人工智能军备竞赛。与一些人试图让我们相信的相反，这些问题并非注定会成为反乌托邦的未来，但它们确实需要一些关键思考、适当准备和广泛合作。即使我们认为已经充分考虑，人工智能系统仍可能证明是独特且根深蒂固的问题，而人工智能系统的意外后果，作为人工智能伦理的另一个方面，也需要被考虑。*作者：[Matthew
    Mayo](/author/matt-mayo)*
- en: '[Explainability](/tag/explainability) ([Explainable AI](/tag/explainable-ai))'
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[可解释性](/tag/explainability) ([可解释的人工智能](/tag/explainable-ai))'
- en: As AI and Machine Learning become a larger part of our lives, with smartphones,
    medical diagnostics, self-driving cars, intelligent search, automated credit decisions,
    etc. having decisions made by AI, one important aspect this decision making comes
    to the forefront - explainability. Humans can usually explain their knowledge-based
    decisions  (whether such explanations are accurate is a separate question) and
    that contributes to trust by other humans for such decisions.  Can AI and ML algorithms
    explain their decisions?  This is important for
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 AI 和机器学习在我们的生活中扮演越来越重要的角色，例如智能手机、医疗诊断、自动驾驶汽车、智能搜索、自动化信用决策等，AI 做出的决策成为一个重要方面——可解释性。人类通常可以解释他们基于知识的决策（这些解释是否准确是另一个问题），这有助于其他人对这些决策产生信任。AI
    和 ML 算法能否解释它们的决策？这点很重要。
- en: improving understanding and trust in the decision
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改善对决策的理解和信任
- en: deciding accountability or liability in case something goes wrong.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定在出现问题时的问责或责任。
- en: avoiding discrimination and societal bias in decisions
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免在决策中出现歧视和社会偏见
- en: We note that some form of explainability is required by [GDPR](https://www.kdnuggets.com/2018/03/gdpr-machine-learning-illegal.html).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到 [GDPR](https://www.kdnuggets.com/2018/03/gdpr-machine-learning-illegal.html)
    要求某种形式的可解释性。
- en: Explainable AI ([XAI](/tag/xai)) is becoming a major field, with DARPA launching
    [XAI program](https://www.darpa.mil/program/explainable-artificial-intelligence)
    in 2018.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释AI ([XAI](/tag/xai)) 正在成为一个重要领域，DARPA 于 2018 年推出了 [XAI 项目](https://www.darpa.mil/program/explainable-artificial-intelligence)。
- en: '![Explainable AI Venn Diagram](../Images/9e36474a8c22f1ad3a399824608f7a1a.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![可解释AI韦恩图](../Images/9e36474a8c22f1ad3a399824608f7a1a.png)'
- en: '**Fig. 2: Explainable AI Venn Diagram.** ([Source](https://www.kdnuggets.com/2019/01/explainable-ai.html)).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2: 可解释AI韦恩图。** ([来源](https://www.kdnuggets.com/2019/01/explainable-ai.html))。'
- en: Explainability is a multifaceted topic. It encompasses both individual models
    and the larger systems that incorporate them. It refers not only to whether the
    decisions a model outputs are interpretable, but also whether or not the whole
    process and intention surrounding the model can be properly accounted for. The
    goal is to have an efficient trade-off between accuracy and explainability along
    with a great human-computer interface which can help translate the model to understandable
    representation for the end users.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性是一个多方面的话题。它涵盖了个体模型及其所包含的更大系统。它不仅指模型输出的决策是否可以解释，还涉及模型的整个过程和意图是否能够被妥善说明。目标是在准确性和可解释性之间实现有效的权衡，并提供一个出色的人机界面，这可以帮助将模型转化为最终用户易于理解的表示。
- en: Some of the more popular methods for Explainable AI include [LIME](/tag/lime)
    and [SHAP](https://www.kdnuggets.com/2020/01/explaining-black-box-models-ensemble-deep-learning-lime-shap.html).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一些更受欢迎的可解释AI方法包括 [LIME](/tag/lime) 和 [SHAP](https://www.kdnuggets.com/2020/01/explaining-black-box-models-ensemble-deep-learning-lime-shap.html)。
- en: Explainability tools are now offered by Google ([Explainable AI service](https://www.kdnuggets.com/2019/12/googles-new-explainable-ai-service.html)),
    [IBM AIX 360](https://github.com/IBM/AIX360) and other vendors.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Google 提供了 [可解释AI服务](https://www.kdnuggets.com/2019/12/googles-new-explainable-ai-service.html)、[IBM
    AIX 360](https://github.com/IBM/AIX360) 和其他供应商也提供相关工具。
- en: 'See also a  KDnuggets blog on [Explainable AI](https://www.kdnuggets.com/2019/01/explainable-ai.html)
    by Preet Gandhi and [Explainable Artificial Intelligence (XAI): Concepts, Taxonomies,
    Opportunities and Challenges toward Responsible AI](https://arxiv.org/abs/1910.10045)
    (arxiv 1910.10045). *Written by [Gregory Piatetsky](/author/gregory-piatetsky)*.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 另见 Preet Gandhi 关于 [可解释AI](https://www.kdnuggets.com/2019/01/explainable-ai.html)
    的 KDnuggets 博客，以及 [可解释人工智能 (XAI)：概念、分类、机会和面向负责任AI的挑战](https://arxiv.org/abs/1910.10045)（arxiv
    1910.10045）。*由 [Gregory Piatetsky](/author/gregory-piatetsky) 撰写*。
- en: Full Stack Data Science
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 全栈数据科学
- en: The Full Stack Data Scientist is the epitome of the Data Science Unicorn. Someone
    who possesses the skills of a Statistician that can model a real life scenario,
    a Computer Scientist that can manage databases and deploy model to the web, and
    a businessman that translates the insights and the models to actionable insights
    to the end users who are typically senior management that does not care about
    the backend work.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 全栈数据科学家是数据科学独角兽的缩影。他们既具备能够模拟现实场景的统计学家的技能，又具备可以管理数据库和将模型部署到网络的计算机科学家的能力，还具备将洞察和模型转化为可操作的见解的商业头脑，最终用户通常是对后端工作不太关心的高级管理人员。
- en: Below are two great talks that can give you an idea about the different nuances
    of an End-to-End Data Science product.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是两场精彩的讲座，它们可以让你对端到端数据科学产品的不同细节有所了解。
- en: '1\. Going Full Stack with Data Science: Using Technical Readiness, by Emily
    Gorcenski'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 全栈数据科学的实践：利用技术准备，由 Emily Gorcenski 主讲
- en: '2\. [Video: #42 Full Stack Data Science (with Vicki Boykis) - DataCamp](https://www.youtube.com/watch?v=EICvvS6MUt8).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. [视频：#42 全栈数据科学（与 Vicki Boykis） - DataCamp](https://www.youtube.com/watch?v=EICvvS6MUt8)。
- en: Read [#42 Full Stack Data Science (with Vicki Boykis) - Transcript](https://www.r-bloggers.com/full-stack-data-science-transcript/).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读 [#42 全栈数据科学（与 Vicki Boykis） - 报告](https://www.r-bloggers.com/full-stack-data-science-transcript/)。
- en: '*Written by [Asel Mendis](/author/asel-mendis)*.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*作者 [Asel Mendis](/author/asel-mendis)*。'
- en: '[Geospatial](/tag/geospatial)'
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[地理空间](/tag/geospatial)'
- en: Geospatial is a term for any data that has a spatial/location/geographical component
    to it. Geospatial analysis has been gaining in popularity due to the onset of
    technology that tracks user movements and creates geospatial data as a by-product.
    The most famous technologies (Geographic Information Systems – GIS) used for spatial
    analysis are [ArcGIS](https://www.arcgis.com/index.html), [QGIS](https://www.qgis.org/en/site/),
    [CARTO](https://carto.com/) and [MapInfo](https://www.pitneybowes.com/us/location-intelligence/geographic-information-systems/mapinfo-pro.html).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 地理空间是指具有空间/位置/地理成分的任何数据。由于技术的出现，这些技术跟踪用户移动并作为副产品创建地理空间数据，地理空间分析正在获得越来越高的关注。最著名的用于空间分析的技术（地理信息系统
    – GIS）包括 [ArcGIS](https://www.arcgis.com/index.html)、[QGIS](https://www.qgis.org/en/site/)、[CARTO](https://carto.com/)
    和 [MapInfo](https://www.pitneybowes.com/us/location-intelligence/geographic-information-systems/mapinfo-pro.html)。
- en: The current epidemic of Coronavirus is tracked by [ARCGIS dashboard](https://www.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6),
    developed by Johns Hopkins U. Center for Systems Science and Engineering.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的冠状病毒疫情由 [ARCGIS 仪表板](https://www.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6)
    跟踪，该仪表板由约翰斯·霍普金斯大学系统科学与工程中心开发。
- en: '![Coronavirus 2020 Mar 2 Johns Hopkins](../Images/2d9fa602b92c76f86079aed611f721f6.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![2020年3月2日冠状病毒 Johns Hopkins](../Images/2d9fa602b92c76f86079aed611f721f6.png)'
- en: '**Fig. 3: Coronavirus stats as of March 2, 2020, according to Johns Hopkins
    CSSE dashboard.**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 3：根据 Johns Hopkins CSSE 仪表板的 2020年3月2日冠状病毒统计数据。**'
- en: Geospatial data can be used in applications from sales prediction modelling
    to assessing government funding initiative. Because the data is in reference to
    a specific location, there are many insights we can gather. Different countries
    record and measure their spatial data differently and to varying degrees. The
    geographic boundaries of a country are different and must be treated as unique
    to each country.  *Written by [Asel Mendis](/author/asel-mendis)*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 地理空间数据可以用于从销售预测建模到评估政府资助计划的各种应用。由于数据涉及特定位置，我们可以获得许多见解。不同国家对其空间数据的记录和测量方式各不相同，且程度不一。国家的地理边界是不同的，必须被视为每个国家独特的。
    *作者 [Asel Mendis](/author/asel-mendis)*。
- en: '[GPT-2](/tag/gpt-2)'
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[GPT-2](/tag/gpt-2)'
- en: '[GPT-2](https://openai.com/blog/better-language-models/) is a transformer-based
    language model created by [OpenAI](/tag/openai). GPT-2 is a generative language
    model, meaning that it generates text by predicting word by word which word comes
    next in a sequence, based on what the model has previously learned. In practice,
    a user-supplied prompt is presented to the model, following which subsequent words
    are generated.  GPT-2 was trained to predict the next word on a massive amount
    (40 GB) of internet text, and is built solely using transformer decoder blocks
    (contrast this with BERT, which uses encoder blocks). For more information on
    transformers see below.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPT-2](https://openai.com/blog/better-language-models/) 是由 [OpenAI](/tag/openai)
    创建的基于变换器的语言模型。GPT-2 是一种生成语言模型，意味着它通过预测每个词的顺序来生成文本，基于模型之前所学到的内容。实际上，用户提供的提示会呈现给模型，随后生成的词会依次出现。GPT-2
    经过大量（40 GB）互联网文本的训练，用于预测下一个词，且完全使用变换器解码器块（与使用编码器块的 BERT 相对）。有关变换器的更多信息，请参见下文。'
- en: GPT-2 isn't a particularly novel undertaking; what sets it apart from similar
    models, however, is the number of its trainable parameters, and the storage size
    required for these trained parameters. While OpenAI initially released a scaled
    down version of the trained model — out of concerns that there could be malicious
    uses for it — the full model contains 1.5 billion parameters. This 1.5 billion
    trainable parameter model requires 6.5 GB of trained parameter (synonymous to
    "trained model") storage.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 并不是一个特别新颖的项目；然而，使其与类似模型区分开来的，是其可训练参数的数量，以及这些训练参数所需的存储大小。尽管 OpenAI 最初发布了缩小版本的训练模型——出于担心可能会被恶意使用——但完整模型包含
    15 亿个参数。这个 15 亿个可训练参数的模型需要 6.5 GB 的训练参数（等同于“训练模型”）存储。
- en: Upon release, GPT-2 generated a lot of hype and attention in large part due
    to the selected examples which accompanied it, the most famous of which — a news
    report documenting the discovery of English speaking unicorns in the Andes — [can
    be read here](https://pbs.twimg.com/media/DzYpsJOU0AA1PO9.png:large). A unique
    application of the GPT-2 model has surfaced in form of the AI Dungeon, an online
    text adventure game which treats user-supplied text as prompts for input into
    the model, and generated output is used to advance the game and user experience.
    You can [try out AI Dungeon here](https://aidungeon.io/).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 发布时，GPT-2 引起了大量的关注和轰动，这在很大程度上归功于随附的精选示例，其中最著名的一个——记录发现安第斯山脉中会说英语的独角兽的新闻报告——[可以在这里阅读](https://pbs.twimg.com/media/DzYpsJOU0AA1PO9.png:large)。GPT-2
    模型的一个独特应用形式是 AI Dungeon，这是一款在线文本冒险游戏，它将用户提供的文本作为输入模型的提示，而生成的输出用于推动游戏进程和用户体验。你可以[在这里尝试
    AI Dungeon](https://aidungeon.io/)。
- en: While text generation via next word prediction is the bread and butter (and
    pizzazz) of GPT-2 and decoder block transformers more generally, they have shown
    promise in additional related areas, such as language translation, text summarization,
    music generation, and more. For technical details on the GPT-2 model and additional
    information, see Jay Alammar's fantastic [Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2).
    *Written by [Matthew Mayo](/author/matt-mayo)*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管通过下一个词预测生成文本是 GPT-2 和解码器块变压器的基本功能（和吸引力），但它们在其他相关领域，如语言翻译、文本摘要、音乐生成等方面也显示了潜力。有关
    GPT-2 模型的技术细节和更多信息，请参见 Jay Alammar 的精彩[Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2)。*由
    [Matthew Mayo](/author/matt-mayo) 撰写*。
- en: NLG ([Natural Language Generation](/tag/natural-language-generation))
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NLG ([自然语言生成](/tag/natural-language-generation))
- en: Significant progress has been made in natural language understanding – getting
    a computer to interpret human input and provide a meaningful response. Many people
    enjoy this technology every day through personal devices, such as Amazon Alexa
    and Google Home. Not unexpectedly, kids really like asking for jokes.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言理解方面已经取得了显著进展——让计算机解读人类输入并提供有意义的回应。许多人每天都通过个人设备，如 Amazon Alexa 和 Google
    Home 来享受这项技术。出乎意料的是，孩子们非常喜欢问笑话。
- en: The tech here is that the machine learning backend is trained on a wide variety
    of inputs, such as “please tell me a joke,’ to which it can select one from a
    prescribed list of available responses. What if Alexa or Google Home could tell
    an *original* joke, one that was created on the fly based on training from a large
    set of human authored jokes. That’s [natural language *generation*](https://www.kdnuggets.com/2020/01/guide-natural-language-generation.html).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的技术是，机器学习后台在各种输入上进行训练，例如“请讲个笑话”，它可以从预定的响应列表中选择一个。如果 Alexa 或 Google Home 能讲一个*原创*笑话，即基于大量人类编写的笑话进行实时生成的笑话，那就是[自然语言
    *生成*](https://www.kdnuggets.com/2020/01/guide-natural-language-generation.html)。
- en: Original jokes are only the beginning (can a trained [machine learning model
    even be funny](http://joking.abdn.ac.uk/webversion/welcome.php)?), as powerful
    applications of NLG are being developed for analytics that generate human-understandable
    summaries of data sets. The creative side of a computer can also be explored through
    NLG techniques that output [original movie scripts](https://www.youtube.com/watch?v=LY7x2Ihqjmc),
    even ones that star [David Hasselhoff](https://www.youtube.com/watch?v=5qPgG98_CQ8),
    as well as text-based stories, similar to [a tutorial you can follow](https://www.kdnuggets.com/2019/07/training-neural-network-write-like-lovecraft.html)
    that leverages long short-term memory, the recurrent neural network architecture
    with feedback, that is another hot research topic today.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 原创笑话仅仅是开始（经过训练的[机器学习模型甚至能搞笑吗](http://joking.abdn.ac.uk/webversion/welcome.php)？），因为强大的自然语言生成（NLG）应用正在开发中，用于生成可被人类理解的数据集总结。计算机的创造性一面也可以通过NLG技术来探索，这些技术可以输出[原创电影剧本](https://www.youtube.com/watch?v=LY7x2Ihqjmc)，甚至是以[大卫·哈塞尔霍夫](https://www.youtube.com/watch?v=5qPgG98_CQ8)为主演的剧本，以及基于文本的故事，类似于[你可以跟随的教程](https://www.kdnuggets.com/2019/07/training-neural-network-write-like-lovecraft.html)，它利用了长短期记忆（LSTM），一种具有反馈的递归神经网络架构，这也是当前的一个热门研究课题。
- en: While business analysis and entertainment applications of computer-generated
    language might be appealing and culture-altering, ethical concerns are already
    boiling over. NLG's capability to deliver “fake news” that is autonomously generated
    and dispersed is causing distress, even if its intentions were not programmed
    to be evil. For example, OpenAI has been [carefully releasing](https://www.kdnuggets.com/2019/03/openai-gpt-2-model-hype-controversy.html)
    their GPT-2 language model for which studies have shown can generate text output
    that is convincing to humans, difficult to detect as synthetic, and can be fine-tuned
    for misuse. Now, they are using this research on the development of AI that can
    be troublesome for humanity as a way to understand better how to control these
    worrisome biases and potentials for malicious use of text generators.  *Written
    by [Matthew Dearing](/author/matthew-dearing)*.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然计算机生成语言的商业分析和娱乐应用可能很有吸引力并且能改变文化，但伦理问题已经引发了广泛关注。NLG能够生成“假新闻”的能力，尤其是自动生成和传播的新闻，已经引起了担忧，即使其初衷并未被编程为恶意。例如，OpenAI已经[小心地发布](https://www.kdnuggets.com/2019/03/openai-gpt-2-model-hype-controversy.html)了他们的GPT-2语言模型，研究显示其生成的文本对人类具有说服力，难以检测为合成内容，并且可以被调优用于误用。现在，他们正在利用这些研究来开发可能对人类带来麻烦的AI，以更好地理解如何控制这些令人担忧的偏见和文本生成器的恶意使用潜力。
    *由[Matthew Dearing](/author/matthew-dearing)撰写*。
- en: '[PyTorch](/tag/pytorch)'
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[PyTorch](/tag/pytorch)'
- en: First released in 2002 and implemented in C, the [Torch package](https://github.com/torch/torch7)
    is a tensor library developed with a range of algorithms to support deep learning.
    [Facebook's AI Research](https://research.fb.com/) lab took a liking to Torch
    and open sourced the library in early 2015 that also incorporated many of its
    machine learning tools. The following year, they released a Python implementation
    of the framework, called PyTorch, optimized for GPU-acceleration.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[Torch包](https://github.com/torch/torch7)于2002年首次发布，并用C语言实现，是一个包含多种算法以支持深度学习的张量库。[Facebook的AI研究](https://research.fb.com/)实验室对Torch产生了兴趣，并在2015年初将其开源，同时也纳入了许多机器学习工具。次年，他们发布了一个名为PyTorch的Python实现，经过优化以支持GPU加速。'
- en: With the powerful Torch tools now accessible to Python developers, many major
    players integrated PyTorch into their development stack. Today, this once Facebook-internal
    machine learning framework is now one of the [most used deep learning libraries](https://www.kdnuggets.com/2020/01/openai-pytorch-adoption.html)
    with OpenAI being the latest to join a growing slate of corporations and researchers
    leveraging PyTorch. The competing package released by Google in 2017, TensorFlow,
    has dominated the deep learning community since its inception and is now clearly
    trending toward being outpaced by PyTorch later in 2020.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 随着强大的Torch工具现在可以被Python开发者使用，许多主要玩家将PyTorch集成到他们的开发堆栈中。今天，这个曾经只在Facebook内部使用的机器学习框架，现在已成为[最常用的深度学习库之一](https://www.kdnuggets.com/2020/01/openai-pytorch-adoption.html)，OpenAI成为了最新一个加入的公司和研究者。谷歌于2017年发布的竞争包TensorFlow，自诞生以来一直主导深度学习社区，并且现在明显呈现出在2020年后被PyTorch超越的趋势。
- en: If you are looking for your first machine learning package to study or are a
    seasoned TensorFlow user, you can [get started](https://www.kdnuggets.com/2019/09/gentle-introduction-pytorch-12.html)
    with PyTorch to find out for yourself which is the best framework for your development
    needs. *Written by [Matthew Dearing](/author/matthew-dearing)*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在寻找第一个机器学习包来学习，或者你是经验丰富的 TensorFlow 用户，你可以[开始了解](https://www.kdnuggets.com/2019/09/gentle-introduction-pytorch-12.html)
    PyTorch，亲自发现哪个框架最适合你的开发需求。*由 [Matthew Dearing](/author/matthew-dearing) 撰写*。
- en: '[Reinforcement Learning](/tag/reinforcement-learning)'
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[强化学习](/tag/reinforcement-learning)'
- en: Along with supervised and unsupervised learning, [reinforcement learning](https://www.kdnuggets.com/2017/12/interview-rich-sutton-reinforcement-learning.html)
    (RL) is a fundamental approach in machine learning. The essential idea is a training
    algorithm that provides a reward feedback to a trial-and-error decision-making
    “agent” that attempts to perform some computational task. In other words, if you
    toss a stick across the yard for Rover to fetch, and your new puppy decides to
    return it to you for a treat, then it will repeat the same decision faster and
    more efficiently next time. The exciting feature of this approach is that labeled
    data is not necessary – the model can explore known and unknown data with guidance
    toward an optimal solution through an encoded reward.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 除了监督学习和无监督学习，[强化学习](https://www.kdnuggets.com/2017/12/interview-rich-sutton-reinforcement-learning.html)
    (RL) 是机器学习中的基本方法。其基本思想是一个训练算法向试错决策“代理”提供奖励反馈，该代理尝试执行某些计算任务。换句话说，如果你把一根棍子扔到院子里让
    Rover 捡回来，而你的小狗决定把它带回给你以获得奖励，那么它下次会更快、更有效地做出相同的决定。这种方法的令人兴奋的特点是无需标记数据——模型可以在指导下通过编码奖励探索已知和未知的数据，寻找最佳解决方案。
- en: RL is the foundation of the incredible, record-breaking, and human-defeating
    competitions in chess, video games, and [AlphaGo’s crushing blow](https://www.kdnuggets.com/2017/10/alphago-zero-biggest-ai-advance.html)
    that learned the game of Go without any instructions hardcoded into its algorithms.
    However, while these developments in AI’s superhuman capabilities are significant,
    they perform within well-defined computer representations, such as games with
    unchanging rules. RL is [not directly generalizable to the messiness of the real
    world](https://www.kdnuggets.com/2020/01/modern-ai-from-neurips-2019.html), as
    seen with [OpenAI’s Rubik’s Cube model](https://openai.com/blog/solving-rubiks-cube/)
    that could solve the puzzle in simulation, but took years to see much-less-than-perfect
    results when translated through robotic arms.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是棋类、视频游戏中令人惊叹、创纪录和战胜人类的比赛的基础，以及[AlphaGo的重创一击](https://www.kdnuggets.com/2017/10/alphago-zero-biggest-ai-advance.html)，它在没有任何硬编码指令的情况下学会了围棋。然而，虽然这些
    AI 超人能力的发展是显著的，但它们仅在定义明确的计算机表示中表现良好，例如具有不变规则的游戏。强化学习[无法直接推广到现实世界的复杂性](https://www.kdnuggets.com/2020/01/modern-ai-from-neurips-2019.html)，正如[OpenAI的魔方模型](https://openai.com/blog/solving-rubiks-cube/)所示，该模型在模拟中能够解决难题，但在通过机器人手臂转化时需要多年才取得相对较差的结果。
- en: So, a great deal is yet to be developed and improved in the area of reinforcement
    learning, and 2019 witnessed that a [potential renaissance is underway](https://www.kdnuggets.com/2019/12/review-what-happened-ai.html).
    Expanding RL to real-world applications will be a hot topic in 2020, with [important
    implementations](https://www.kdnuggets.com/2020/01/created-lazy-ai.html) already
    underway. *Written by [Matthew Dearing](https://www.kdnuggets.com/author/matthew-dearing)*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，强化学习领域还有大量需要开发和改进的内容，2019年见证了[潜在的复兴正在进行中](https://www.kdnuggets.com/2019/12/review-what-happened-ai.html)。将强化学习扩展到现实世界应用将是2020年的热门话题，[重要的实施](https://www.kdnuggets.com/2020/01/created-lazy-ai.html)已经在进行中。*由
    [Matthew Dearing](https://www.kdnuggets.com/author/matthew-dearing) 撰写*。
- en: '[Transformer](/tag/transformer)'
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[变换器](/tag/transformer)'
- en: The Transformer is a novel neural network architecture based on self-attention
    mechanism that is especially well-suited to NLP and Natural Language Understanding.
    It was proposed in [Attention Is All You Need](https://goo.gl/dwSBxB), 2017 paper
    by Google AI researchers. The Transformer is an architecture for "transforming"
    one sequence to another with the help of Encoder and Decoder, but it does not
    use recurrent networks or LSTM. Instead it uses the attention mechanism which
    allows it to look at other positions in the input sequence to help improve encoding.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer是一种新颖的神经网络架构，基于自注意力机制，特别适合自然语言处理和自然语言理解。它在2017年由谷歌AI研究人员提出的[Attention
    Is All You Need](https://goo.gl/dwSBxB)论文中提出。Transformer是一种通过编码器和解码器将一个序列“转换”成另一个序列的架构，但它不使用递归网络或LSTM。它使用注意力机制，使其能够查看输入序列中的其他位置，从而改善编码。
- en: Here is an example, well-explained by Jay Alammar. Suppose we want to translate
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子，由Jay Alammar详细解释。假设我们想翻译
- en: '"The animal didn''t cross the street because it was too tired"'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: “动物没有过马路，因为它太累了”
- en: What does "it" refer to?  Humans understand that "it" refers to the animal,
    not the street, but this question is hard for computers.  When encoding the word
    "it", the self-attention mechanism focuses on "The Animal" and associates these
    words with "it".
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: “it”指的是什么？人类知道“it”指的是动物，而不是街道，但这个问题对计算机来说很难。当编码单词“it”时，自注意力机制集中在“The Animal”上，并将这些词与“it”关联起来。
- en: '![Transformer Self-Attention Mechanism](../Images/90f5c8c65f8f3dc6a706383cc8c4d0ab.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![Transformer 自注意力机制](../Images/90f5c8c65f8f3dc6a706383cc8c4d0ab.png)'
- en: '**Fig. 4: As transformer is encoding the word "it", part of the attention mechanism
    was focusing on "The Animal", and connected its representation into the encoding
    of "it".** ([Source](https://jalammar.github.io/illustrated-transformer/).)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**图4：当Transformer正在编码单词“it”时，注意力机制的一部分集中在“The Animal”上，并将其表示与“it”的编码连接起来。**
    ([来源](https://jalammar.github.io/illustrated-transformer/).)'
- en: Google reports that Transformer has significantly outperformed other approaches
    on translation tasks. Transformer architecture was used in many NLP frameworks,
    such as BERT (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers)
    and its descendants.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌报告称，Transformer在翻译任务上明显优于其他方法。Transformer架构被用于许多自然语言处理框架中，例如BERT（**B**idirectional
    **E**ncoder **R**epresentations from **T**ransformers）及其后续版本。
- en: For a great visual illustration, see [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/),
    by Jay Alammar. *Written by [Gregory Piatetsky](/author/gregory-piatetsky)*.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 想要更好的视觉展示，请参阅[图解 Transformer](https://jalammar.github.io/illustrated-transformer/)，作者是Jay
    Alammar。*由[Gregory Piatetsky](/author/gregory-piatetsky)撰写*。
- en: '**Related:**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part
    1)](https://www.kdnuggets.com/2020/02/ai-data-science-machine-learning-key-terms-2020.html)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2020年你需要了解的20个AI、数据科学和机器学习术语（第1部分）](https://www.kdnuggets.com/2020/02/ai-data-science-machine-learning-key-terms-2020.html)'
- en: '[277 Data Science Key Terms, Explained](https://www.kdnuggets.com/2017/09/data-science-key-terms-explained.html)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[277个数据科学关键术语解释](https://www.kdnuggets.com/2017/09/data-science-key-terms-explained.html)'
- en: '[What is Data Science?](https://www.kdnuggets.com/2019/11/what-is-data-science.html)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是数据科学？](https://www.kdnuggets.com/2019/11/what-is-data-science.html)'
- en: More On This Topic
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[建立一个稳固的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用管道编写清晰的Python代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个数据科学家都应了解的三个R库（即使你使用Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，并寻找目标去…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功数据科学家的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
