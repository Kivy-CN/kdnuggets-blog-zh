- en: Building Your First ETL Pipeline with Bash
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/building-your-first-etl-pipeline-with-bash](https://www.kdnuggets.com/building-your-first-etl-pipeline-with-bash)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Building Your First ETL Pipeline with Bash](../Images/25385ed4de8b6638a1ca5f30f762b1f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author | Midjourney & Canva
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: ETL, or Extract, Transform, Load, is a necessary data engineering process, which
    involves extracting data from various sources, converting it into a workable form,
    and moving it to some destination, such as a database. ETL pipelines automate
    this process, making sure that data is processed in a consistent and efficient
    manner, which provides a framework for tasks like data analysis, reporting, and
    machine learning, and ensures data is clean, reliable, and ready to use.
  prefs: []
  type: TYPE_NORMAL
- en: Bash, short for short for Bourne-Again Shell — aka the Unix shell — is a powerful
    tool for building ETL pipelines, due to its simplicity, flexibility, and extremely
    wide applicability, and thus it is an excellent option for novices and seasoned
    pros alike. Bash scripts can do things like automate tasks, move files around,
    and talk to other tools on the command line, meaning that it is a good choice
    for ETL work. Moreover, bash is ubiquitous on Unix-like systems (Linux, BSD, macOS,
    etc.), so it is ready to use on most such systems with no extra work on your part.
  prefs: []
  type: TYPE_NORMAL
- en: This article is intended for beginner and practitioner data scientists and data
    engineers who are looking to build their first ETL pipeline. It assumes a basic
    understanding of the command line and aims to provide a practical guide to creating
    an ETL pipeline using bash. The goal of the article is to guide readers through
    the process of building a basic ETL pipeline using bash. By the end of the article,
    readers will have a working understanding of implementing an ETL pipeline that
    extracts data from a source, transforms it, and loads it into a destination database.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up Your Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we begin, ensure you have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A Unix-based system (Linux or macOS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bash shell (usually pre-installed on Unix systems)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic understanding of command-line operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For our ETL pipeline, we will need these specific command line tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '`curl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jq`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`awk`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sed`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sqlite3`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can install them using your system''s package manager. On a Debian-based
    system, you can use `apt-get`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'On macOS, you can use `brew`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s set up a dedicated directory for our ETL project. Open your terminal
    and run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This creates a new directory called `etl_project` and navigates into it.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data can come from various sources such as APIs, CSV files, or databases. For
    this tutorial, we'll demonstrate extracting data from a public API and a CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: Let's use `curl` to fetch data from a public API. For example, we'll extract
    data from a mock API that provides sample data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This command will download the data and save it as `data.json`.
  prefs: []
  type: TYPE_NORMAL
- en: We can also use `curl` to download a CSV file from a remote server.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This will save the CSV file as `data.csv` in our working directory.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data transformation is necessary to convert raw data into a format suitable
    for analysis or storage. This may involve parsing JSON, filtering CSV files, or
    cleaning text data.
  prefs: []
  type: TYPE_NORMAL
- en: '`jq` is a powerful tool for working with JSON data. Let''s use it to extract
    specific fields from our JSON file.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This command extracts the `id`, `name`, and `value` fields from each entry in
    the JSON data and saves the result in `transformed_data.json`.
  prefs: []
  type: TYPE_NORMAL
- en: '`awk` is a versatile tool for processing CSV files. We''ll use it to extract
    specific columns from our CSV file.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This command extracts the first and third columns from `data.csv` and saves
    them in `transformed_data.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: '`sed` is a stream editor for filtering and transforming text. We can use it
    to perform text replacements and clean up our data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This command replaces occurrences of `old_text` with `new_text` in `transformed_data.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: Loading Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Common destinations for loading data include databases and files. For this tutorial,
    we'll use SQLite, a commonly used lightweight database.
  prefs: []
  type: TYPE_NORMAL
- en: First, let's create a new SQLite database and a table to hold our data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This command creates a database file named `etl_database.db` and a table named
    `data` with three columns.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll insert our transformed data into the SQLite database.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This block of commands sets the mode to CSV and imports `transformed_data.csv`
    into the `data` table.
  prefs: []
  type: TYPE_NORMAL
- en: We can verify that the data has been inserted correctly by querying the database.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This command retrieves all rows from the `data` table and displays them.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have covered the following steps while building our ETL pipeline with bash,
    including:'
  prefs: []
  type: TYPE_NORMAL
- en: Environment setup and tool installation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data extraction from a public API and CSV file with `curl`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data transformation using `jq`, `awk`, and `sed`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data loading in an SQLite database with `sqlite3`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bash is a good choice for ETL due to its simplicity, flexibility, automation
    capabilities, and interoperability with other CLI tools.
  prefs: []
  type: TYPE_NORMAL
- en: For further investigation, think about incorporating error handling, scheduling
    the pipeline via cron, or learning more advanced bash concepts. You may also wish
    to investigate alternative transformation apps and methods to increase your pipeline
    skillset.
  prefs: []
  type: TYPE_NORMAL
- en: Try out your own ETL projects, putting what you have learned to the test, in
    more elaborate scenarios. With some luck, the basic concepts here will be a good
    jumping-off point to more complex data engineering tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.linkedin.com/in/mattmayo13/)****[Matthew Mayo](https://www.kdnuggets.com/wp-content/uploads/./profile-pic.jpg)****
    ([**@mattmayo13**](https://twitter.com/mattmayo13)) holds a master''s degree in
    computer science and a graduate diploma in data mining. As managing editor of
    [KDnuggets](https://www.kdnuggets.com/) & [Statology](https://www.statology.org/),
    and contributing editor at [Machine Learning Mastery](https://machinelearningmastery.com/),
    Matthew aims to make complex data science concepts accessible. His professional
    interests include natural language processing, language models, machine learning
    algorithms, and exploring emerging AI. He is driven by a mission to democratize
    knowledge in the data science community. Matthew has been coding since he was
    6 years old.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[ETL vs ELT: Which One is Right for Your Data Pipeline?](https://www.kdnuggets.com/2023/03/etl-elt-one-right-data-pipeline.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a Scalable ETL with SQL + Python](https://www.kdnuggets.com/2022/04/building-scalable-etl-sql-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a Tractable, Feature Engineering Pipeline for Multivariate…](https://www.kdnuggets.com/2022/03/building-tractable-feature-engineering-pipeline-multivariate-time-series.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a Formula 1 Streaming Data Pipeline With Kafka and Risingwave](https://www.kdnuggets.com/building-a-formula-1-streaming-data-pipeline-with-kafka-and-risingwave)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Step-by-Step Tutorial to Building Your First Machine Learning Model](https://www.kdnuggets.com/step-by-step-tutorial-to-building-your-first-machine-learning-model)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
