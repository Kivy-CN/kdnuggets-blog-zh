- en: 'A Top Machine Learning Algorithm Explained: Support Vector Machines (SVM)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/03/machine-learning-algorithm-svm-explained.html](https://www.kdnuggets.com/2020/03/machine-learning-algorithm-svm-explained.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91db1982754222467ae9d4c78622811c.png)'
  prefs: []
  type: TYPE_IMG
- en: One of the most prevailing and exciting supervised learning models with associated
    learning algorithms that analyse data and recognise patterns is Support Vector
    Machines (SVMs). It is used for solving both regression and classification problems.
    However, it is mostly used in solving classification problems. SVMs were first
    introduced by B.E. Boser et al. in 1992 and has become popular due to success
    in handwritten digit recognition in 1994\. Before the emergence of Boosting Algorithms,
    for example, XGBoost and AdaBoost, SVMs had been commonly used.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to have a consolidated foundation of Machine Learning algorithms,
    you should definitely have it in your arsenal. The algorithm of SVMs is powerful,
    but the concepts behind are not as complicated as you think.
  prefs: []
  type: TYPE_NORMAL
- en: Problem with Logistic Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In my previous article, I have explained clearly what Logistic Regression is
    ([link](https://towardsdatascience.com/from-linear-to-logistic-regression-explained-step-by-step-11d7f0a9c29)).
    It helps solve classification problems separating the instances into two classes.
    However, there is an infinite number of decision boundaries, and Logistic Regression
    only picks an arbitrary one.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/39279a875d0df753a4fba76f717f44dc.png)'
  prefs: []
  type: TYPE_IMG
- en: For point C, since it’s far away from the decision boundary, we are quite certain
    to classify it as 1 (green).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For point A, even though we classify it as 1 for now, since it is pretty close
    to the decision boundary, if the boundary moves a little to the right, we would
    mark point A as “0” instead. Hence, we’re much more confident about our prediction
    at C than at A
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic Regression doesn’t care whether the instances are close to the decision
    boundary. Therefore, the decision boundary it picks may not be optimal. As we
    can see from the above graph, if a point is far from the decision boundary, we
    may be more confident in our predictions. Therefore, the optimal decision boundary
    should be able to maximize the distance between the decision boundary and all
    instances. i.e., maximize the margins. That’s why the SVM algorithm is important!
  prefs: []
  type: TYPE_NORMAL
- en: What is Support Vector Machines (SVMs)?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a set of training examples, each marked as belonging to one or the other
    of two categories, an SVM training algorithm builds a model that assignsnew examples
    to one category or the other, making it a non-probabilistic binary linear classifier.
  prefs: []
  type: TYPE_NORMAL
- en: The objective of applying SVMs is to find the best line in two dimensions or
    the best hyperplane in more than two dimensions in order to help us separate our
    space into classes. The hyperplane (line) is found through the **maximum margin,** i.e.,
    the maximum distance between data points of both classes.
  prefs: []
  type: TYPE_NORMAL
- en: '*Don’t you think the definition and idea of SVM look a bit abstract? No worries,
    let me explain in details.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Support Vector Machines**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine the labelled training set are two classes of data points (two dimensions):
    Alice and Cinderella. To separate the two classes, there are so many possible
    options of hyperplanes that separate correctly. As shown in the graph below, we
    can achieve exactly the same result using different hyperplanes (L1, L2, L3).
    However, if we add new data points, the consequence of using various hyperplanes
    will be very different in terms of classifying new data point into the right group
    of class.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/406bd07c65610e1a5eaa38508158192e.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Different hyperplanes (L1, L2, L3).*'
  prefs: []
  type: TYPE_NORMAL
- en: How can we decide a separating line for the classes? Which hyperplane shall
    we use?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bfc565312d220ed098edf7569016af83.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Support Vector, Hyperplane, and Margin**'
  prefs: []
  type: TYPE_NORMAL
- en: The vector points closest to the hyperplane are known as the **support vector
    points **because only these two points are contributing to the result of the algorithm,
    and other points are not. If a data point is not a support vector, removing it
    has no effect on the model. On the other hand, deleting the support vectors will
    then change the position of the hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: The dimension of the hyperplane depends upon the number of features. If the
    number of input features is 2, then the hyperplane is just a line. If the number
    of input features is 3, then the hyperplane becomes a two-dimensional plane. It
    becomes difficult to imagine when the number of features exceeds 3.
  prefs: []
  type: TYPE_NORMAL
- en: The distance of the vectors from the hyperplane is called the ***margin,***
    which is a separation of a line to the closest class points. We would like to
    choose a hyperplane that maximises the margin between classes. The graph below
    shows what good margin and bad margin are.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c1a8111c00614ef6c8ba2c307c9137f.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](../Images/ed84e1285cc734e9851df80c831d7e3d.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Hard Margin**'
  prefs: []
  type: TYPE_NORMAL
- en: If the training data is linearly separable, we can select two parallel hyperplanes
    that separate the two classes of data, so that the distance between them is as
    large as possible.
  prefs: []
  type: TYPE_NORMAL
- en: '**Soft Margin**'
  prefs: []
  type: TYPE_NORMAL
- en: As most of the real-world data are not fully linearly separable, we will allow
    some margin violation to occur, which is called soft margin classification. It
    is better to have a large margin, even though some constraints are violated. Margin
    violation means choosing a hyperplane, which can allow some data points to stay
    in either the incorrect side of the hyperplane and between the margin and the
    correct side of the hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: In order to find the **maximal margin**, we need to maximize the margin between
    the data points and the hyperplane. In the following session, I will share the
    mathematical concepts behind this algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df9fc3ca480269b642c12eeefb862ee7.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear Algebra Revisited
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we move on, let’s review some concepts in Linear Algebra.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9f16893706b282b46ff91456d49f453.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](../Images/831ada4b7d56ffed1f9254873c51efdc.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Maximising the Margin**'
  prefs: []
  type: TYPE_NORMAL
- en: 'You probably learned that an equation of a line is** y=ax+b**. However, you
    will often find that the equation of a hyperplane is defined by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c36f0aebf4bc6549f652d47469e6fdc7.png)'
  prefs: []
  type: TYPE_IMG
- en: '*The two equations are just two different ways of expressing the same thing.*'
  prefs: []
  type: TYPE_NORMAL
- en: For** Support Vector Classifier** (SVC), we use ????T????+???? where ???? is
    the weight vector, and ???? is the bias.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a892e732e8b60e8dcb203f8e597164e3.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that the name of the variables in the hyperplane equation are **w **and **x,**
    whichmeans they are vectors! A vector has magnitude (size) and direction, which
    works perfectly well in 3 or more dimensions. Therefore, the application of “**vector”** is
    used in the SVMs algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de8274f5698d8253d78493d27ea4c41d.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](../Images/896a1dbe6fdba5bf8d21f3282dc4eb8b.png)'
  prefs: []
  type: TYPE_IMG
- en: '*The equation of calculating the Margin.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cost Function and Gradient Updates**'
  prefs: []
  type: TYPE_NORMAL
- en: Maximizing-Margin is equivalent to Minimizing Loss.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the SVM algorithm, we are looking to maximize the **margin** between the
    data points and the hyperplane. The loss function that helps maximize the margin
    is **hinge loss**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ef1c7a5848fbf5414ade5e0fda3d16f9.png)'
  prefs: []
  type: TYPE_IMG
- en: '*λ=1/C (C is always used for regularization coefficient).*'
  prefs: []
  type: TYPE_NORMAL
- en: The function of the first term, **hinge loss,** is topenalize misclassifications.
    It measures the error due to misclassification (or data points being closer to
    the classification boundary than the margin). The second term is the regularization
    term, which is a technique to avoid overfitting by penalizing large coefficients
    in the solution vector. The λ(lambda) is the regularization coefficient, and its
    major role is to determine the trade-off between increasing the margin size and
    ensuring that the xi lies on the correct side of the margin.
  prefs: []
  type: TYPE_NORMAL
- en: “Hinge” describes the fact that the error is 0 if the data point is classified
    correctly (and is not too close to the decision boundary).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56f845d28a04c8b2f2ec8b93a4180f56.png)'
  prefs: []
  type: TYPE_IMG
- en: When the true class is -1 (as in your example), the hinge loss looks like this
    in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: We need to minimise the above loss function to find the max-margin classifier.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We can derive the formula for the margin from the **hinge-loss**. If a data
    point is on the margin of the classifier, the hinge-loss is exactly zero. Hence,
    on the margin, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/500b5c42ac6cd4ac554ae76a1d06a6f2.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Note that ????i is either +1 or -1.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/acd2e6cec0a25204058e9f0492113d35.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our objective function is then:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/522973f17b5a9080a222c383f77193bb.png)'
  prefs: []
  type: TYPE_IMG
- en: '*To minimize such an objection function, we should then use Lagrange Multiplier.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3b8afc784ae1860a54a4e562712078d8.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Classifying non-linear data**'
  prefs: []
  type: TYPE_NORMAL
- en: What about data points are not linearly separable?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0a7afb4b1e1e40981433a7c2cc60f601.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Non-linear separate.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'SVM has a technique called the [**kernel**](https://en.wikipedia.org/wiki/Kernel_method)** trick**.
    These are functions that take low dimensional input space and transform it into
    a higher-dimensional space, i.e., it converts not separable problem to separable
    problem. It is mostly useful in non-linear separation problems. This is shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mapping to a Higher Dimension**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d80782f04cdb36746d79e83200cba24.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](../Images/2f4be06f70d62e727f8003a9053e7e7b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Some Frequently Used Kernels**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df3ce05a5e3fd991a0ebb47c100c0da2.png)'
  prefs: []
  type: TYPE_IMG
- en: Why SVMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Solve the data points are not linearly separable
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Effective in a higher dimension.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Suitable for small data set: effective when the number of features is more
    than training examples.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Overfitting problem: The hyperplane is affected by only the support vectors,
    so SVMs are not robust to the outliner.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Summary: Now you should know'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Problem with Logistic Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Support Vector, Hyperplane, and Margin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to find the maximised margin using hinge-loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to deal with non-linear separable data using different kernels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/one-of-the-top-machine-learning-algorithms-for-supervised-learning-support-vector-machines-svms-fc45ac0667f4).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[A Friendly Introduction to Support Vector Machines](https://www.kdnuggets.com/2019/09/friendly-introduction-support-vector-machines.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Support Vector Machine (SVM) Tutorial: Learning SVMs From Examples](https://www.kdnuggets.com/2017/08/support-vector-machines-learning-svms-examples.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What is a Support Vector Machine, and Why Would I Use it?](https://www.kdnuggets.com/2017/02/yhat-support-vector-machine.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Support Vector Machines: An Intuitive Approach](https://www.kdnuggets.com/2022/08/support-vector-machines-intuitive-approach.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Gentle Introduction to Support Vector Machines](https://www.kdnuggets.com/2023/07/gentle-introduction-support-vector-machines.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
