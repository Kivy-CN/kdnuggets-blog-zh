- en: 'Graph of Thoughts: A New Paradigm for Elaborate Problem-Solving in Large Language
    Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 思维图谱：大型语言模型中复杂问题解决的新范式
- en: 原文：[https://www.kdnuggets.com/graph-of-thoughts-a-new-paradigm-for-elaborate-problem-solving-in-large-language-models](https://www.kdnuggets.com/graph-of-thoughts-a-new-paradigm-for-elaborate-problem-solving-in-large-language-models)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/graph-of-thoughts-a-new-paradigm-for-elaborate-problem-solving-in-large-language-models](https://www.kdnuggets.com/graph-of-thoughts-a-new-paradigm-for-elaborate-problem-solving-in-large-language-models)
- en: '![Graph of Thoughts: A New Paradigm for Elaborate Problem-Solving in Large
    Language Models](../Images/64427c89afa9e69d0bb7407300ba5d50.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![思维图谱：大型语言模型中复杂问题解决的新范式](../Images/64427c89afa9e69d0bb7407300ba5d50.png)'
- en: '# Key Takeaways'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '# 关键要点'
- en: Graph of Thoughts (GoT) is a novel framework designed to enhance the prompting
    capabilities of Large Language Models (LLMs) for complex problem-solving tasks.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 思维图谱（GoT）是一个新颖的框架，旨在增强大型语言模型（LLMs）在复杂问题解决任务中的提示能力。
- en: GoT surpasses existing paradigms like Chain-of-Thought (CoT) and Tree of Thoughts
    (ToT) by representing the information generated by an LLM as a graph, allowing
    for more flexible and efficient reasoning.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GoT通过将LLM生成的信息表示为图，超越了现有的范式，如思维链（CoT）和思维树（ToT），从而实现了更灵活和高效的推理。
- en: The framework has shown significant improvements in task performance, including
    a 62% increase in sorting quality and a cost reduction of over 31% compared to
    Tree of Thoughts.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该框架在任务性能上显示了显著改进，包括排序质量提高了62%，与思维树相比，成本降低了31%以上。
- en: '* * *'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织IT'
- en: '* * *'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: This work brings the LLM reasoning closer to human thinking or brain mechanisms
    such as recurrence, both of which form complex networks.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这项工作使LLM推理更接近人类思维或脑机制，如递归，这两者都形成了复杂的网络。
- en: Introduction
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The burgeoning landscape of artificial intelligence has given rise to increasingly
    sophisticated Large Language Models (LLMs) capable of a wide range of tasks. Yet,
    one of the ongoing challenges is improving these models' ability to solve elaborate
    problems efficiently. Enter [Graph of Thoughts (GoT)](https://arxiv.org/abs/2308.09687),
    a framework hoping to take a giant leap in this direction. GoT advances the prompting
    capabilities of LLMs by structuring the information they generate into a graph,
    thereby enabling a more intricate and flexible form of reasoning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能快速发展的背景下，出现了越来越复杂的大型语言模型（LLMs），能够处理各种任务。然而，持续的挑战之一是提高这些模型高效解决复杂问题的能力。进入[思维图谱（GoT）](https://arxiv.org/abs/2308.09687)，这一框架希望在这方面取得巨大进展。GoT通过将生成的信息结构化为图，从而提高了LLMs的提示能力，实现更复杂和灵活的推理。
- en: While existing paradigms like Chain-of-Thought (CoT) and Tree of Thoughts (ToT)
    have contributed to the structured output and hierarchical reasoning in LLMs,
    they often operate within a linear or tree-like constraint. This limitation can
    sometimes hinder the model's ability to handle complex problem-solving tasks that
    require multi-dimensional reasoning and the ability to combine disparate pieces
    of information. Graph of Thoughts addresses this gap by introducing a graph-based
    structure for managing "LLM thoughts." This allows for an unprecedented level
    of flexibility in how information is stored, accessed, and manipulated within
    the model. With GoT, developers and researchers can fine-tune the prompting strategy
    to navigate this graph effectively, enabling LLMs to solve intricate problems
    in a more human-like manner.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像链式思维（CoT）和思维树（ToT）等现有范式已对 LLM 的结构化输出和层次推理做出了贡献，但它们往往在线性或树状结构的限制下运行。这种限制有时会阻碍模型处理需要多维推理和整合不同信息片段的复杂问题。思维图通过引入图形结构来管理“LLM
    思考”，解决了这一问题。这为信息在模型内的存储、访问和操作提供了前所未有的灵活性。通过 GoT，开发者和研究人员可以细化提示策略，以有效地导航这一图形，从而使
    LLM 能够以更类似人类的方式解决复杂问题。
- en: Understanding Graph of Thoughts
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解思维图
- en: 'Graph of Thoughts operates on a simple yet powerful concept: it models the
    information produced by an LLM as a graph where each vertex represents a unit
    of information, often referred to as "LLM thoughts." The edges between these vertices
    signify the dependencies or relationships between different units of thought.
    This graph-based approach allows for:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 思维图基于一个简单却强大的概念：它将 LLM 生成的信息建模为图形，其中每个顶点代表一个信息单元，通常称为“LLM 思考”。这些顶点之间的边表示不同思维单元之间的依赖关系或联系。这种基于图形的方法允许：
- en: Combining arbitrary LLM thoughts into harmonious outcomes
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将任意的 LLM 思考组合成和谐的结果
- en: Refining the essence of complex networks of thoughts
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提炼复杂思维网络的本质
- en: Strengthening thoughts with the use of feedback loops
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用反馈循环强化思维
- en: In comparison to existing paradigms like CoT and ToT, GoT offers a more flexible
    and efficient way to manage and manipulate the information generated by LLMs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 与现有的如 CoT 和 ToT 等范式相比，GoT 提供了一种更灵活高效的方式来管理和操作 LLM 生成的信息。
- en: '![Graph of Thoughts process compared](../Images/ac1f50e79f940d0de84691bc31a6464e.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![GoT 过程比较图](../Images/ac1f50e79f940d0de84691bc31a6464e.png)'
- en: '**Figure 1**: Comparison of Graph of Thoughts (GoT) to other prompting strategies
    (Image from paper)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1**：思维图（GoT）与其他提示策略的比较（图片来自论文）'
- en: Implementing Graph of Thoughts
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施思维图
- en: To implement GoT, developers need to represent the problem-solving process as
    a graph, where each node or vertex represents a thought or a piece of information.
    Then, the relationships or dependencies between these thoughts are mapped as edges
    in the graph. This mapping allows for various operations like merging nodes to
    create more complex thoughts, or applying transformations to enhance the existing
    thoughts.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现 GoT，开发者需要将问题解决过程表示为图形，其中每个节点或顶点代表一个思考或一条信息。然后，这些思考之间的关系或依赖被映射为图中的边。这种映射允许进行各种操作，例如合并节点以创建更复杂的思考，或应用转换以增强现有的思考。
- en: One of the standout features of GoT is its extensibility, allowing it to adapt
    to a variety of tasks and domains. Unlike more rigid structures, the graph-based
    representation in GoT can be dynamically altered during the problem-solving process.
    This means that as an LLM generates new thoughts or gains additional insights,
    these can be seamlessly incorporated into the existing graph without requiring
    a complete overhaul.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: GoT 的一个显著特点是其可扩展性，使其能够适应各种任务和领域。与更僵化的结构不同，GoT 中的图形表示在问题解决过程中可以动态调整。这意味着当 LLM
    生成新的思考或获得额外的见解时，这些可以无缝地融入现有图形中，而无需完全重做。
- en: Moreover, GoT enables the implementation of feedback loops, where the model
    can revisit and refine its earlier thoughts based on newly acquired information.
    This dynamic and iterative process serves to significantly enhance the quality
    of the model's output, making it a particularly powerful tool for complex tasks
    that require ongoing refinement and adaptation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，GoT 实现了反馈循环的机制，在这种机制下，模型可以根据新获得的信息重新审视和完善其早期的思考。这种动态的迭代过程显著提升了模型输出的质量，使其成为一个特别强大的工具，适用于需要持续改进和调整的复杂任务。
- en: Conclusion
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The introduction of GoT may mark a significant advancement in the field of LLMs
    and their application in complex problem-solving tasks. By adopting a graph-based
    approach to represent and manipulate the information generated by LLMs, GoT offers
    a more flexible and efficient form of reasoning. Its success in improving task
    performance and reducing computational costs makes it a promising framework for
    future research and applications. Developers and researchers should explore this
    new paradigm in order to attempt to unlock the full problem-solving potential
    of their LLMs and improve their prompting.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: GoT的引入可能标志着LLMs领域及其在复杂问题解决任务中的应用的重要进展。通过采用基于图的方法来表示和操控LLMs生成的信息，GoT提供了一种更灵活和高效的推理形式。它在提高任务表现和降低计算成本方面的成功，使其成为未来研究和应用的有前途的框架。开发人员和研究人员应探索这一新范式，以尝试解锁LLMs的全部问题解决潜力，并改进其提示。
- en: '[](https://www.linkedin.com/in/mattmayo13/)****[Matthew Mayo](https://www.kdnuggets.com/wp-content/uploads/./profile-pic.jpg)****
    ([**@mattmayo13**](https://twitter.com/mattmayo13)) holds a master''s degree in
    computer science and a graduate diploma in data mining. As managing editor of
    [KDnuggets](https://www.kdnuggets.com/) & [Statology](https://www.statology.org/),
    and contributing editor at [Machine Learning Mastery](https://machinelearningmastery.com/),
    Matthew aims to make complex data science concepts accessible. His professional
    interests include natural language processing, language models, machine learning
    algorithms, and exploring emerging AI. He is driven by a mission to democratize
    knowledge in the data science community. Matthew has been coding since he was
    6 years old.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.linkedin.com/in/mattmayo13/)****[马修·梅约](https://www.kdnuggets.com/wp-content/uploads/./profile-pic.jpg)****
    ([**@mattmayo13**](https://twitter.com/mattmayo13)) 拥有计算机科学硕士学位和数据挖掘研究生文凭。作为 [KDnuggets](https://www.kdnuggets.com/)
    和 [Statology](https://www.statology.org/) 的主编，以及 [Machine Learning Mastery](https://machinelearningmastery.com/)
    的特约编辑，马修致力于使复杂的数据科学概念变得易于理解。他的专业兴趣包括自然语言处理、语言模型、机器学习算法和探索新兴的人工智能。他的使命是使数据科学社区的知识民主化。马修从6岁开始编程。'
- en: More On This Topic
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Top Open Source Large Language Models](https://www.kdnuggets.com/2022/09/john-snow-top-open-source-large-language-models.html)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[顶级开源大型语言模型](https://www.kdnuggets.com/2022/09/john-snow-top-open-source-large-language-models.html)'
- en: '[More Free Courses on Large Language Models](https://www.kdnuggets.com/2023/06/free-courses-large-language-models.html)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[更多关于大型语言模型的免费课程](https://www.kdnuggets.com/2023/06/free-courses-large-language-models.html)'
- en: '[Learn About Large Language Models](https://www.kdnuggets.com/2023/03/learn-large-language-models.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[了解大型语言模型](https://www.kdnuggets.com/2023/03/learn-large-language-models.html)'
- en: '[Introducing Healthcare-Specific Large Language Models from John Snow Labs](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍来自John Snow Labs的医疗专用大型语言模型](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
- en: '[What are Large Language Models and How Do They Work?](https://www.kdnuggets.com/2023/05/large-language-models-work.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大型语言模型是什么，如何运作？](https://www.kdnuggets.com/2023/05/large-language-models-work.html)'
- en: '[AI: Large Language & Visual Models](https://www.kdnuggets.com/2023/06/ai-large-language-visual-models.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能：大型语言模型与视觉模型](https://www.kdnuggets.com/2023/06/ai-large-language-visual-models.html)'
