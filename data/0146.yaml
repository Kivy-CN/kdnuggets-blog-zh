- en: 'Orca LLM: Simulating the Reasoning Processes of ChatGPT'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Orca LLM: 模拟ChatGPT的推理过程'
- en: 原文：[https://www.kdnuggets.com/2023/06/orca-llm-reasoning-processes-chatgpt.html](https://www.kdnuggets.com/2023/06/orca-llm-reasoning-processes-chatgpt.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/06/orca-llm-reasoning-processes-chatgpt.html](https://www.kdnuggets.com/2023/06/orca-llm-reasoning-processes-chatgpt.html)
- en: '![Orca LLM: Simulating the Reasoning Processes of ChatGPT](../Images/480d8edf9cc4de3cce60f27feb4daa41.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Orca LLM: 模拟ChatGPT的推理过程](../Images/480d8edf9cc4de3cce60f27feb4daa41.png)'
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的捷径。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行IT管理'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In the realm of large language models (LLMs), there has been a constant pursuit
    to enhance the capabilities of smaller models without compromising their efficiency.
    The traditional approach has been to use imitation learning, where smaller models
    learn from the outputs generated by large foundation models (LFMs). However, this
    approach has been marred by several challenges, including limited imitation signals
    from shallow LFM outputs, small-scale homogeneous training data, and a lack of
    rigorous evaluation. This often leads to smaller models imitating the style but
    not the reasoning process of LFMs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型语言模型（LLM）领域，始终在追求在不影响效率的情况下提升小型模型的能力。传统的方法是使用模仿学习，小型模型从大型基础模型（LFM）生成的输出中学习。然而，这种方法面临几个挑战，包括来自浅层LFM输出的有限模仿信号、小规模的同质训练数据以及缺乏严格评估。这常常导致小型模型模仿风格但无法模仿LFM的推理过程。
- en: 'The paper **[Orca: Progressive Learning from Complex Explanation Traces of
    GPT-4](https://arxiv.org/abs/2306.02707)** introduces Orca, a 13-billion parameter
    model designed to imitate the reasoning process of large foundation models (LFMs)
    such as GPT-4\. Unlike traditional large language models (LLMs), Orca employs
    a unique training approach that combines progressive learning and teacher assistance
    to overcome the capacity gap between smaller student models and their larger counterparts.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '论文**[Orca: 从GPT-4的复杂解释踪迹中渐进学习](https://arxiv.org/abs/2306.02707)**介绍了Orca，一个设计用来模仿大型基础模型（LFM）如GPT-4的推理过程的13亿参数模型。与传统的大型语言模型（LLM）不同，Orca采用了一种独特的训练方法，将渐进学习和教师辅助相结合，以克服小型学生模型与其大型对应模型之间的能力差距。'
- en: Training Methodology
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练方法
- en: Orca's training process consists of two stages.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Orca的训练过程包括两个阶段。
- en: In the first stage, Orca is trained on FLAN-5M, which includes ChatGPT augmentations.
    This intermediate teacher assistant helps bridge the capacity gap between Orca
    and GPT-4, which has a significantly larger parameter size. By leveraging ChatGPT's
    capabilities, Orca benefits from improved imitation learning performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一阶段，Orca在FLAN-5M上进行训练，该训练包括ChatGPT的增强技术。这一中级教师助手有助于弥合Orca与GPT-4之间的能力差距，后者具有显著更大的参数规模。通过利用ChatGPT的能力，Orca受益于改进的模仿学习表现。
- en: In the second stage, Orca undergoes training on FLAN-1M, which incorporates
    GPT-4 augmentations. This progressive learning approach follows a curriculum learning
    paradigm, where the student model learns from easier examples before tackling
    more challenging ones. By gradually exposing Orca to increasingly complex reasoning
    and step-by-step explanations, the model enhances its reasoning abilities and
    mimicking skills.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二阶段，Orca接受FLAN-1M的训练，该训练融合了GPT-4的增强技术。这种渐进的学习方法遵循课程学习范式，其中学生模型从简单的例子学习，然后再处理更具挑战性的例子。通过逐步将Orca暴露于越来越复杂的推理和逐步解释中，该模型提升了其推理能力和模仿技能。
- en: Advantages and Contributions
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优势和贡献
- en: Orca's training methodology offers several advantages over traditional LLMs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Orca的训练方法相比传统LLM具有多个优势。
- en: Firstly, it addresses the capacity gap issue by utilizing an intermediate teacher
    model, allowing Orca to learn from a more capable source. This approach has been
    shown to improve imitation learning performance for smaller student models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它通过利用中间教师模型解决了能力差距问题，使Orca能够从更有能力的来源学习。这种方法已被证明能够提高较小学生模型的模仿学习性能。
- en: Secondly, the progressive learning aspect of Orca's training enables the model
    to build upon its knowledge incrementally. By starting with simpler examples and
    gradually introducing more complex ones, Orca develops a stronger foundation for
    reasoning and explanation generation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，Orca训练的渐进学习方面使模型能够逐步积累知识。通过从简单的示例开始，逐渐引入更复杂的示例，Orca为推理和生成解释打下了更坚实的基础。
- en: Furthermore, Orca's ability to imitate the reasoning process of LFMs like GPT-4
    opens up possibilities for enhanced performance in various tasks. By tapping into
    the rich signals provided by GPT-4's explanation traces and step-by-step thought
    processes, Orca gains valuable insights and improves its own capabilities.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Orca模仿像GPT-4这样的LFMs的推理过程的能力，为各种任务的增强性能打开了可能性。通过利用GPT-4的解释痕迹和逐步思维过程提供的丰富信号，Orca获得了宝贵的见解，并提升了自身能力。
- en: Performance Benchmarks
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能基准
- en: Orca has shown remarkable performance in complex zero-shot reasoning benchmarks.
    It outperforms traditional state-of-the-art instruction-tuned models like Vicuna-13B
    by over 100% on benchmarks like Big-Bench Hard (BBH) and over 42% on AGIEval.
    Additionally, Orca achieves the same scores as ChatGPT on the BBH benchmarks and
    shows competitive performance on professional and academic exams such as the SAT,
    LSAT, GRE, and GMAT. This is particularly impressive considering that these are
    zero-shot settings without chain-of-thought, and Orca still performs competitively
    while trailing behind GPT-4.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Orca在复杂的零-shot推理基准测试中表现出色。在Big-Bench Hard (BBH)等基准测试中，比传统的最先进的指令调整模型Vicuna-13B高出100%以上，在AGIEval中高出42%以上。此外，Orca在BBH基准测试中取得了与ChatGPT相同的分数，并在SAT、LSAT、GRE和GMAT等专业和学术考试中表现具有竞争力。考虑到这些都是零-shot设置且没有链式思维，Orca仍表现出竞争力，但与GPT-4相比略逊一筹，这一点尤其令人印象深刻。
- en: Implications and Future Directions
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 含义与未来方向
- en: The development of Orca represents a significant advancement in the field of
    LLMs. By learning from rich signals and imitating the reasoning process of LFMs,
    Orca is able to perform complex reasoning tasks with a high degree of accuracy.
    This has wide-ranging implications, especially in areas where complex reasoning
    and problem-solving are required.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Orca的发展代表了LLMs领域的重大进步。通过学习丰富的信号并模仿LFMs的推理过程，Orca能够以高度准确性执行复杂的推理任务。这具有广泛的影响，尤其是在需要复杂推理和问题解决的领域。
- en: Moreover, this research indicates that learning from step-by-step AI model explanations
    is a promising direction for improving model capabilities. This opens up new avenues
    for research and development in the field of LLMs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这项研究表明，从逐步AI模型解释中学习是一种有前景的方向，有助于提升模型能力。这为LLMs领域的研究和开发开辟了新的途径。
- en: Conclusion
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Orca presents a novel approach to training large language models, combining
    progressive learning and teacher assistance to enhance imitation learning. By
    leveraging intermediate teacher models and gradually exposing the student model
    to more complex examples, Orca overcomes the capacity gap and improves its reasoning
    and explanation generation abilities. The paper's findings contribute to the advancement
    of imitation learning techniques and have implications for the development of
    future language models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Orca提出了一种新的大规模语言模型训练方法，将渐进学习和教师辅助结合在一起，以增强模仿学习。通过利用中间教师模型，并逐渐将学生模型暴露于更复杂的示例中，Orca克服了能力差距，提升了推理和生成解释的能力。该论文的发现有助于模仿学习技术的进步，并对未来语言模型的发展产生了影响。
- en: For more details on Orca and its research, refer to the [introductory article
    from Microsoft](https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/)
    and the [accompanying research paper](https://arxiv.org/pdf/2306.02707.pdf).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Orca及其研究的更多细节，请参阅[微软的介绍文章](https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/)和[相关研究论文](https://arxiv.org/pdf/2306.02707.pdf)。
- en: More On This Topic
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Enhancing LLM Reasoning: Unveiling Chain of Code Prompting](https://www.kdnuggets.com/enhancing-llm-reasoning-unveiling-chain-of-code-prompting)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[增强 LLM 推理：揭示链式代码提示](https://www.kdnuggets.com/enhancing-llm-reasoning-unveiling-chain-of-code-prompting)'
- en: '[Web LLM: Bring LLM Chatbots to the Browser](https://www.kdnuggets.com/2023/05/webllm-bring-llm-chatbots-browser.html)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Web LLM：将 LLM 聊天机器人带到浏览器](https://www.kdnuggets.com/2023/05/webllm-bring-llm-chatbots-browser.html)'
- en: '[ReAct, Reasoning and Acting augments LLMs with Tools!](https://www.kdnuggets.com/react-reasoning-and-acting-augments-llms-with-tools)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ReAct，推理与行动：用工具增强 LLM！](https://www.kdnuggets.com/react-reasoning-and-acting-augments-llms-with-tools)'
- en: '[Thought Propagation: An Analogical Approach to Complex Reasoning…](https://www.kdnuggets.com/thought-propagation-an-analogical-approach-to-complex-reasoning-with-large-language-models)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[思维传播：复杂推理的类比方法……](https://www.kdnuggets.com/thought-propagation-an-analogical-approach-to-complex-reasoning-with-large-language-models)'
- en: '[Visual ChatGPT: Microsoft Combine ChatGPT and VFMs](https://www.kdnuggets.com/2023/03/visual-chatgpt-microsoft-combine-chatgpt-vfms.html)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Visual ChatGPT：微软结合 ChatGPT 和 VFMs](https://www.kdnuggets.com/2023/03/visual-chatgpt-microsoft-combine-chatgpt-vfms.html)'
- en: '[ChatGPT CLI: Transform Your Command-Line Interface Into ChatGPT](https://www.kdnuggets.com/2023/07/chatgpt-cli-transform-commandline-interface-chatgpt.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatGPT CLI：将你的命令行界面转变为 ChatGPT](https://www.kdnuggets.com/2023/07/chatgpt-cli-transform-commandline-interface-chatgpt.html)'
