- en: A 2019 Guide to Semantic Segmentation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2019 年语义分割指南
- en: 原文：[https://www.kdnuggets.com/2019/08/2019-guide-semantic-segmentation.html/2](https://www.kdnuggets.com/2019/08/2019-guide-semantic-segmentation.html/2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/08/2019-guide-semantic-segmentation.html/2](https://www.kdnuggets.com/2019/08/2019-guide-semantic-segmentation.html/2)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](https://www.kdnuggets.com/2019/08/2019-guide-semantic-segmentation.html?page=2#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](https://www.kdnuggets.com/2019/08/2019-guide-semantic-segmentation.html?page=2#comments)'
- en: Multi-Scale Context Aggregation by Dilated Convolutions (ICLR, 2016)
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多尺度上下文聚合通过空洞卷积（ICLR, 2016）
- en: In this paper, a convolution network module that blends multi-scale context
    information without loss of resolution is developed. This module can then be plugged
    into existing architectures at any resolution. The module is based on [dilated
    convolutions](https://www.kdd.org/kdd2018/accepted-papers/view/smoothed-dilated-convolutions-for-improved-dense-prediction).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本文开发了一种卷积网络模块，该模块结合了多尺度上下文信息而不会丢失分辨率。该模块可以插入到现有的任何分辨率架构中。该模块基于 [空洞卷积](https://www.kdd.org/kdd2018/accepted-papers/view/smoothed-dilated-convolutions-for-improved-dense-prediction)。
- en: '[**Multi-Scale Context Aggregation by Dilated Convolutions**](https://arxiv.org/abs/1511.07122?source=post_page---------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[**多尺度上下文聚合通过空洞卷积**](https://arxiv.org/abs/1511.07122?source=post_page---------------------------)'
- en: '*State-of-the-art models for semantic segmentation are based on adaptations
    of convolutional networks that had...*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*最先进的语义分割模型基于对卷积网络的适配，这些网络曾...*'
- en: The module was tested on the Pascal VOC 2012 dataset. It proves that adding
    a context module to existing semantic segmentation architectures improves their
    accuracy.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该模块在 Pascal VOC 2012 数据集上进行了测试。结果证明，向现有的语义分割架构中添加上下文模块可以提高它们的准确性。
- en: '![Figure](../Images/7b31b59519ee3e707e8990ce830359c1.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/7b31b59519ee3e707e8990ce830359c1.png)'
- en: '[source](https://arxiv.org/abs/1511.07122)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1511.07122)'
- en: The front-end module trained in experimentation achieves 69.8% mean IoU on the
    VOC-2012 validation set and 71.3% mean IoU on the test set. The prediction accuracy
    of this model on different objects is shown below
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中训练的前端模块在 VOC-2012 验证集上达到了 69.8% 的平均 IoU，在测试集上达到了 71.3% 的平均 IoU。该模型在不同物体上的预测准确性如下所示
- en: '![Figure](../Images/69426a7b1e59ea33190b47e6bbf40b28.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/69426a7b1e59ea33190b47e6bbf40b28.png)'
- en: '[source](https://arxiv.org/abs/1511.07122)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1511.07122)'
- en: 'DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,
    and Fully Connected CRFs (TPAMI, 2017)'
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DeepLab：利用深度卷积网络、空洞卷积和全连接 CRFs 的语义图像分割（TPAMI, 2017）
- en: 'In this paper the authors make the following contributions to the task of semantic
    segmentation with deep learning:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，作者对深度学习中的语义分割任务做出了以下贡献：
- en: Convolutions with upsampled filters for dense prediction tasks
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于密集预测任务的上采样卷积
- en: Atrous spatial pyramid pooling (ASPP) for segmenting objects at multiple scales
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用空洞空间金字塔池化（ASPP）来分割多尺度的物体
- en: Improving localization of object boundaries by using DCNNs.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用深度卷积神经网络（DCNNs）来改善物体边界的定位。
- en: '[**DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous
    Convolution, and Fully...**](https://arxiv.org/abs/1606.00915?source=post_page---------------------------)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[**DeepLab：利用深度卷积网络、空洞卷积和全连接 CRFs 的语义图像分割**](https://arxiv.org/abs/1606.00915?source=post_page---------------------------)'
- en: '*In this work we address the task of semantic image segmentation with Deep
    Learning and make three main contributions...*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*在这项工作中，我们处理了使用深度学习的语义图像分割任务，并做出了三个主要贡献...*'
- en: The paper’s proposed DeepLab system achieves a 79.7% mIOU on the PASCAL VOC-2012
    semantic image segmentation task.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 论文提出的 DeepLab 系统在 PASCAL VOC-2012 语义图像分割任务中达到了 79.7% 的 mIOU。
- en: '![Figure](../Images/01f79fab4b9e140c47232232a501d9ac.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/01f79fab4b9e140c47232232a501d9ac.png)'
- en: '[source](https://arxiv.org/abs/1606.00915)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1606.00915)'
- en: 'The paper tackles the main challenges of using deep CNNs in semantic segmentation,
    which include:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 论文解决了在语义分割中使用深度卷积神经网络的主要挑战，包括：
- en: Reduced feature resolution caused by a repeated combination of max-pooling and
    downsampling.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于最大池化和下采样的重复组合，导致特征分辨率降低。
- en: Existence of objects at multiple scales.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体在多个尺度下的存在。
- en: Reduced localization accuracy caused by DCNN’s invariance since an object-centric
    classifier requires invariance to spatial transformations.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 DCNN 的不变性导致的定位准确度降低，因为以物体为中心的分类器需要对空间变换具有不变性。
- en: '![Figure](../Images/0b565a8a78333d41537f308caabde165.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/0b565a8a78333d41537f308caabde165.png)'
- en: '[source](https://arxiv.org/abs/1606.00915)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1606.00915)'
- en: Atrous convolution is applied by either upsampling the filters by inserting
    zeros or sparsely sampling the input feature maps. The second method entails subsampling
    the input feature maps by a factor equal to the atrous convolution rate r, and
    deinterlacing it to produce r^2 reduced resolution maps, one for each of the r×r
    possible shifts. After this, a standard convolution is applied to the immediate
    feature maps, interlacing them with the image’s original resolution.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Atrous 卷积的应用包括通过插入零进行滤波器的上采样，或稀疏地采样输入特征图。第二种方法涉及将输入特征图按 atrous 卷积率 r 进行子采样，并对其进行解交错以生成
    r^2 分辨率降低的图像，每个图像对应 r×r 的可能偏移。之后，对立即特征图应用标准卷积，将其与图像的原始分辨率进行交错。
- en: '![Figure](../Images/378c73a5645fb156805f9e89ebeb46e4.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/378c73a5645fb156805f9e89ebeb46e4.png)'
- en: '[source](https://arxiv.org/abs/1606.00915)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1606.00915)'
- en: Rethinking Atrous Convolution for Semantic Image Segmentation (2017)
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重新思考 Atrous 卷积在语义图像分割中的应用 (2017)
- en: This paper addresses two challenges (mentioned previously) in using DCNNs for
    semantic segmentation; reduced feature resolution that occurs when consecutive
    pooling operations are applied and the existence of objects at multiple scales.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本文解决了使用 DCNN 进行语义分割时的两个挑战（如前所述）；当应用连续池化操作时特征分辨率降低，以及存在多尺度的对象。
- en: '**[Rethinking Atrous Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1706.05587?source=post_page---------------------------)**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**[重新思考 Atrous 卷积在语义图像分割中的应用](https://arxiv.org/abs/1706.05587?source=post_page---------------------------)**'
- en: '*In this work, we revisit atrous convolution, a powerful tool to explicitly
    adjust filter''s field-of-view as well as...*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*在这项工作中，我们重新审视了 atrous 卷积，这是一种强大的工具，可以显式调整滤波器的视野范围以及...*'
- en: In order to address the first problem, the paper suggests the use of atrous
    convolution, also known as dilated convolution. It proposes solving the second
    problem using atrous convolution to enlarge the field of view and hence include
    multi-scale context.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决第一个问题，论文建议使用 atrous 卷积，也称为扩张卷积。它提出使用 atrous 卷积来解决第二个问题，以扩大视野范围，从而包含多尺度上下文。
- en: '![Figure](../Images/3c33eb86cdc88eede6b9bd18d33b2e50.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/3c33eb86cdc88eede6b9bd18d33b2e50.png)'
- en: '[source](https://arxiv.org/pdf/1706.05587.pdf)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1706.05587.pdf)'
- en: The paper’s ‘DeepLabv3’ achieves a performance of 85.7% on the PASCAL VOC 2012
    test set without DenseCRF post-processing.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中的“DeepLabv3”在 PASCAL VOC 2012 测试集上取得了 85.7% 的表现，无需 DenseCRF 后处理。
- en: '![Figure](../Images/e2214bef39b3eb6d6164ee06edb8551d.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/e2214bef39b3eb6d6164ee06edb8551d.png)'
- en: '[source](https://arxiv.org/pdf/1706.05587.pdf)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1706.05587.pdf)'
- en: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation
    (ECCV, 2018)
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Atrous Separable Convolution 用于语义图像分割 (ECCV, 2018)
- en: This paper’s approach ‘DeepLabv3+’ achieves the test set performance of 89.0%
    and 82.1% without any post-processing on PASCAL VOC 2012 and Cityscapes datasets.
    This model is an extension of DeepLabv3 by adding a simple decoder module to refine
    the segmentation results.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的方法“DeepLabv3+”在 PASCAL VOC 2012 和 Cityscapes 数据集上取得了 89.0% 和 82.1% 的测试集表现，而无需对结果进行后处理。该模型是
    DeepLabv3 的扩展，通过添加一个简单的解码器模块来细化分割结果。
- en: '[**Papers With Code : Encoder-Decoder with Atrous Separable Convolution for
    Semantic Image...**](https://paperswithcode.com/paper/encoder-decoder-with-atrous-separable?source=post_page---------------------------)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Papers With Code : Encoder-Decoder with Atrous Separable Convolution for
    Semantic Image...**](https://paperswithcode.com/paper/encoder-decoder-with-atrous-separable?source=post_page---------------------------)'
- en: '*???? SOTA for Semantic Segmentation on PASCAL VOC 2012(Mean IoU metric)*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*???? 在 PASCAL VOC 2012 上的语义分割 SOTA (平均 IoU 评估指标)*'
- en: '![Figure](../Images/bb5dce103215c636b6e086a85413794f.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/bb5dce103215c636b6e086a85413794f.png)'
- en: '[source](https://arxiv.org/pdf/1802.02611v3.pdf)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1802.02611v3.pdf)'
- en: The paper implements two types of neural networks that use a spatial pyramid
    pooling module for semantic segmentation. One captures contextual information
    by pooling features at different resolutions, while the other obtain sharp object
    boundaries.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 论文实现了两种使用空间金字塔池化模块进行语义分割的神经网络。一种通过在不同分辨率下池化特征来捕捉上下文信息，而另一种则获得清晰的对象边界。
- en: '![Figure](../Images/2d03bc0ce0b093742ea7c906dc1fc8f7.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/2d03bc0ce0b093742ea7c906dc1fc8f7.png)'
- en: '[source](https://arxiv.org/pdf/1802.02611v3.pdf)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1802.02611v3.pdf)'
- en: 'FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation
    (2019)'
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'FastFCN: 重新思考语义分割骨干中的膨胀卷积（2019）'
- en: This paper proposes a joint upsampling module named Joint Pyramid Upsampling
    (JPU) to replace the dilated convolutions that consume a lot of time and memory.
    It works by formulating the function of extracting high-resolution maps as a joint
    upsampling problem.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种名为联合金字塔上采样（JPU）的联合上采样模块，以替代耗时和占用大量内存的膨胀卷积。它通过将提取高分辨率图的功能表述为一个联合上采样问题来工作。
- en: '[**Papers With Code : FastFCN: Rethinking Dilated Convolution in the Backbone
    for Semantic...**](https://paperswithcode.com/paper/fastfcn-rethinking-dilated-convolution-in-the?source=post_page---------------------------)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Papers With Code : FastFCN: 重新思考语义分割骨干中的膨胀卷积**](https://paperswithcode.com/paper/fastfcn-rethinking-dilated-convolution-in-the?source=post_page---------------------------)'
- en: '*???? SOTA for Semantic Segmentation on PASCAL Context(mIoU metric)*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*???? PASCAL Context上的语义分割SOTA（mIoU指标）*'
- en: This method achieves a performance of mIoU of 53.13% on the Pascal Context dataset
    and runs 3 times faster.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法在Pascal Context数据集上实现了53.13%的mIoU性能，并且运行速度快了3倍。
- en: '![Figure](../Images/e0703ffa8618c145755c0fc9dab0e42d.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/e0703ffa8618c145755c0fc9dab0e42d.png)'
- en: '[source](https://arxiv.org/pdf/1903.11816v1.pdf)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[源文档](https://arxiv.org/pdf/1903.11816v1.pdf)'
- en: The method implements a fully-connected network(FCN) as the backbone while applying
    JPU to upsample the low-resolution final feature maps, resulting in high-resolution
    feature maps. Replacing the dilated convolutions with the JPU does not result
    in any loss of performance.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法将全连接网络（FCN）作为骨干，同时应用JPU对低分辨率的最终特征图进行上采样，从而生成高分辨率的特征图。用JPU替代膨胀卷积不会导致性能损失。
- en: '![Figure](../Images/410cb503d4162b164e84717edb16b04a.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/410cb503d4162b164e84717edb16b04a.png)'
- en: '[source](https://arxiv.org/pdf/1903.11816v1.pdf)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[源文档](https://arxiv.org/pdf/1903.11816v1.pdf)'
- en: Joint sampling uses low-resolution target images and high-resolution guidance
    images. It then generates high-resolution target images by transferring the structure
    and details of the guidance image.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 联合采样使用低分辨率的目标图像和高分辨率的引导图像。然后，通过转移引导图像的结构和细节来生成高分辨率的目标图像。
- en: Improving Semantic Segmentation via Video Propagation and Label Relaxation (CVPR,
    2019)
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过视频传播和标签松弛改进语义分割（CVPR, 2019）
- en: This paper proposes a video-based method to scale the training set by synthesizing
    new training samples. This is aimed at improving the accuracy of semantic segmentation
    networks. It explores the ability of video prediction models to predict future
    frames in order to predict future labels.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种基于视频的方法，通过合成新的训练样本来扩展训练集，旨在提高语义分割网络的准确性。它探索了视频预测模型预测未来帧的能力，以便预测未来标签。
- en: '[**Papers With Code : Improving Semantic Segmentation via Video Propagation
    and Label Relaxation**](https://paperswithcode.com/paper/improving-semantic-segmentation-via-video?source=post_page---------------------------)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Papers With Code : 通过视频传播和标签松弛改进语义分割**](https://paperswithcode.com/paper/improving-semantic-segmentation-via-video?source=post_page---------------------------)'
- en: '*???? SOTA for Semantic Segmentation on Cityscapes(Mean IoU metric)*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*???? Cityscapes上的语义分割SOTA（平均IoU指标）*'
- en: '![Figure](../Images/4deeb32092f38d2a5da84d20ff655615.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/4deeb32092f38d2a5da84d20ff655615.png)'
- en: '[source](https://arxiv.org/pdf/1812.01593v3.pdf)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[源文档](https://arxiv.org/pdf/1812.01593v3.pdf)'
- en: The paper demonstrates that training segmentation networks on datasets from
    synthesized data lead to improved prediction accuracy. The methods proposed in
    this paper achieve mIoUs of 83.5% on [Cityscapes](https://www.cityscapes-dataset.com/) and
    82.9% on [CamVid](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 论文表明，在合成数据集上训练分割网络可以提高预测准确性。本文提出的方法在 [Cityscapes](https://www.cityscapes-dataset.com/)
    上实现了83.5%的mIoU，在 [CamVid](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/)
    上实现了82.9%的mIoU。
- en: '![Figure](../Images/7cf5ebc3ee5d35ffef467de891ea805c.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/7cf5ebc3ee5d35ffef467de891ea805c.png)'
- en: '[source](https://arxiv.org/pdf/1812.01593v3.pdf)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[源文档](https://arxiv.org/pdf/1812.01593v3.pdf)'
- en: 'The paper proposes two ways of predicting future labels:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 论文提出了两种预测未来标签的方法：
- en: '**Label Propagation (LP) **creating new training samples by pairing a propagated
    label with the original future frame'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签传播 (LP)** 通过将传播的标签与原始未来帧配对来创建新的训练样本'
- en: '**Joint image-label Propagation (JP) **creating new training samples by pairing
    a propagated label with the corresponding propagated image'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**联合图像-标签传播 (JP)** 通过将传播的标签与相应的传播图像配对来创建新的训练样本'
- en: The paper has three main propositions; utilizing video prediction models to
    propagate labels to immediate neighbor frames, introducing joint image-label propagation
    to deal with the misalignment problem, and relaxing one-hot label training by
    maximizing the likelihood of the union of class probabilities along the boundary.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 论文有三个主要建议：利用视频预测模型将标签传播到相邻帧，引入联合图像-标签传播来解决对齐问题，以及通过最大化边界上类别概率的联合可能性来放宽单热标签训练。
- en: '![Figure](../Images/d6168cf5a46fda0f9565c98783d0acd6.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/d6168cf5a46fda0f9565c98783d0acd6.png)'
- en: '[source](https://arxiv.org/pdf/1812.01593v3.pdf)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1812.01593v3.pdf)'
- en: 'Gated-SCNN: Gated Shape CNNs for Semantic Segmentation (2019)'
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Gated-SCNN: 用于语义分割的门控形状CNN（2019）'
- en: This paper is the newest kid on the semantic segmentation block. The authors
    proposes a two-stream CNN architecture. In this architecture, shape information
    is processed as a separate branch. This shape stream processes only boundary related
    information. This is enforced by the model’s Gated Convolution Layer (GCL) and
    local supervision.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文是语义分割领域的新星。作者提出了一种双流CNN架构。在这个架构中，形状信息作为一个单独的分支处理。这个形状流只处理与边界相关的信息。这由模型的门控卷积层（GCL）和局部监督来强制执行。
- en: '[**Gated-SCNN: Gated Shape CNNs for Semantic Segmentation**](https://arxiv.org/abs/1907.05740?source=post_page---------------------------)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Gated-SCNN: 用于语义分割的门控形状CNN**](https://arxiv.org/abs/1907.05740?source=post_page---------------------------)'
- en: '*Current state-of-the-art methods for image segmentation form a dense image
    representation where the color, shape and...*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*当前最先进的图像分割方法形成了一个密集的图像表示，其中包括颜色、形状等...*'
- en: '![Figure](../Images/e0c0fa9198af1bb5c32cd5ac5e5f2c1f.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/e0c0fa9198af1bb5c32cd5ac5e5f2c1f.png)'
- en: '[source](https://arxiv.org/abs/1907.05740)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1907.05740)'
- en: This model outperforms the DeepLab-v3+ by 1.5 % on mIoU and 4% in F-boundary
    score. The model has been evaluated using the Cityscapes benchmark. On smaller
    and thinner objects, the model achieves an improvement of 7% on IoU.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在mIoU上比DeepLab-v3+提高了1.5%，在F-boundary分数上提高了4%。该模型已使用Cityscapes基准进行评估。在较小和较薄的物体上，该模型在IoU上提高了7%。
- en: The table below shows the performance of the Gated-SCNN in comparison to other
    models.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了Gated-SCNN与其他模型的性能比较。
- en: '![Figure](../Images/bab352e5f44b2d7a50f1d9f0aac5364f.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/bab352e5f44b2d7a50f1d9f0aac5364f.png)'
- en: '[source](https://arxiv.org/abs/1907.05740)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1907.05740)'
- en: Conclusion
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: We should now be up to speed on some of the most common — and a couple of very
    recent — techniques for performing semantic segmentation in a variety of contexts.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在应该了解一些最常见的——以及几个非常近期的——语义分割技术在各种环境中的应用。
- en: The papers/abstracts mentioned and linked to above also contain links to their
    code implementations. We’d be happy to see the results you obtain after testing
    them.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 上述论文/摘要也包含了其代码实现的链接。我们很乐意看到你在测试后获得的结果。
- en: '**Bio: [Derrick Mwiti](https://derrickmwiti.com/)** is a data analyst, a writer,
    and a mentor. He is driven by delivering great results in every task, and is a
    mentor at Lapid Leaders Africa.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介： [德里克·穆伊提](https://derrickmwiti.com/)** 是数据分析师、作家和导师。他致力于在每项任务中取得出色成果，并且是Lapid
    Leaders Africa的导师。'
- en: '[Original](https://heartbeat.fritz.ai/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc).
    Reposted with permission.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://heartbeat.fritz.ai/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc)。已获得许可转载。'
- en: '**Related:**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[A 2019 Guide to Object Detection](/2019/08/2019-guide-object-detection.html)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2019年目标检测指南](/2019/08/2019-guide-object-detection.html)'
- en: '[Object Detection with Luminoth](/2019/03/object-detection-luminoth.html)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Luminoth进行目标检测](/2019/03/object-detection-luminoth.html)'
- en: '[Automated Machine Learning in Python](/2019/01/automated-machine-learning-python.html)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python中的自动化机器学习](/2019/01/automated-machine-learning-python.html)'
- en: '* * *'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行 IT 事务'
- en: '* * *'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Misconceptions About Semantic Segmentation Annotation](https://www.kdnuggets.com/2022/01/misconceptions-semantic-segmentation-annotation.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于语义分割注释的误解](https://www.kdnuggets.com/2022/01/misconceptions-semantic-segmentation-annotation.html)'
- en: '[The Power of a Semantic Layer: A Data Engineer''s Guide](https://www.kdnuggets.com/2023/10/cube-power-of-a-semantic-layer-a-data-engineers-guide)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[语义层的力量：数据工程师指南](https://www.kdnuggets.com/2023/10/cube-power-of-a-semantic-layer-a-data-engineers-guide)'
- en: '[How Semantic Vector Search Transforms Customer Support Interactions](https://www.kdnuggets.com/how-semantic-vector-search-transforms-customer-support-interactions)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[语义向量搜索如何改变客户支持互动](https://www.kdnuggets.com/how-semantic-vector-search-transforms-customer-support-interactions)'
- en: '[Semantic Search with Vector Databases](https://www.kdnuggets.com/semantic-search-with-vector-databases)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用向量数据库进行语义搜索](https://www.kdnuggets.com/semantic-search-with-vector-databases)'
- en: '[Semantic Layer: The Backbone of AI-powered Data Experiences](https://www.kdnuggets.com/2023/10/cube-semantic-layer-backbone-aipowered-data-experiences)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[语义层：AI 驱动的数据体验的支柱](https://www.kdnuggets.com/2023/10/cube-semantic-layer-backbone-aipowered-data-experiences)'
- en: '[6 Reasons Why a Universal Semantic Layer is Beneficial to Your Data Stack](https://www.kdnuggets.com/2024/01/cube-6-reasons-why-a-universal-semantic-layer-is-beneficial)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为何通用语义层对你的数据架构有益的 6 个理由](https://www.kdnuggets.com/2024/01/cube-6-reasons-why-a-universal-semantic-layer-is-beneficial)'
