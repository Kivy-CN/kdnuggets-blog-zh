- en: What is the difference between Bagging and Boosting?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bagging和Boosting有什么区别？
- en: 原文：[https://www.kdnuggets.com/2017/11/difference-bagging-boosting.html](https://www.kdnuggets.com/2017/11/difference-bagging-boosting.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/11/difference-bagging-boosting.html](https://www.kdnuggets.com/2017/11/difference-bagging-boosting.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png)[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png)[评论](#comments)'
- en: '**By xristica, [Quantdare](https://quantdare.com/)**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由xristica，[Quantdare](https://quantdare.com/)提供**。'
- en: '![](../Images/6b1eafba9a76281b988e0532c98b4d3e.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b1eafba9a76281b988e0532c98b4d3e.png)'
- en: 'Bagging and Boosting are similar in that they are both **ensemble techniques**,
    where a set of weak learners are combined to create a strong learner that obtains
    better performance than a single one. So, let’s start from the beginning:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging和Boosting的相似之处在于它们都是**集成技术**，在这些技术中，一组弱学习器被组合以创建一个强学习器，从而获得比单一学习器更好的性能。所以，让我们从头开始：
- en: What is an ensemble method?
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是集成方法？
- en: Ensemble is a Machine Learning concept in which the idea is to train **multiple
    models** using the same learning algorithm. The ensembles take part in a bigger
    group of methods, called **multiclassifiers,** where a set of hundreds or thousands
    of learners with a common objective are fused together to solve the problem.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 集成是一个机器学习概念，其思想是使用相同的学习算法训练**多个模型**。集成方法属于更大的方法群体，称为**多分类器**，在这些方法中，数百或数千个具有共同目标的学习器被融合在一起以解决问题。
- en: The second group of multiclassifiers contain **the hybrid methods**. They use
    a set of learners too, but they can be trained using different learning techniques.
    Stacking is the most well-known. If you want to learn more about Stacking, you
    can read my previous post, “[Dream team combining classifiers](http://quantdare.com/dream-team-combining-classifiers-2/)“.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 第二组多分类器包含**混合方法**。它们也使用一组学习器，但可以使用不同的学习技术进行训练。Stacking是最著名的。如果你想了解更多关于Stacking的信息，你可以阅读我之前的文章，“[梦之队组合分类器](http://quantdare.com/dream-team-combining-classifiers-2/)”。
- en: The main causes of error in learning are due to **noise, bias and variance**.
    Ensemble helps to minimize these factors. These methods are designed to improve
    the stability and the accuracy of Machine Learning algorithms. Combinations of
    multiple classifiers decrease variance, especially in the case of unstable classifiers,
    and may produce a more reliable classification than a single classifier.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 学习中的主要误差原因是**噪声、偏差和方差**。集成有助于最小化这些因素。这些方法旨在提高机器学习算法的稳定性和准确性。多个分类器的组合减少了方差，尤其是在不稳定分类器的情况下，并可能产生比单一分类器更可靠的分类。
- en: To use Bagging or Boosting you must select a base learner algorithm. For example,
    if we choose a classification tree, Bagging and Boosting would consist of a pool
    of trees as big as we want.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Bagging或Boosting，你必须选择一个基础学习算法。例如，如果我们选择一个分类树，Bagging和Boosting将由一个数量随意的树池组成。
- en: '![](../Images/921aafecdda40adb53dd2bdba7c5c20f.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/921aafecdda40adb53dd2bdba7c5c20f.png)'
- en: How do Bagging and Boosting get N learners?
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Bagging和Boosting如何获得N个学习器？
- en: Bagging and Boosting get N learners by generating additional data in the training
    stage. N new training data sets are produced by **random sampling with replacement** from
    the original set. By sampling with replacement some observations may be repeated
    in each new training data set.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging和Boosting通过在训练阶段生成额外的数据来获得N个学习器。通过**随机采样带替换**从原始集合中生成N个新的训练数据集。通过带替换的采样，一些观察值可能在每个新训练数据集中重复出现。
- en: 'In the case of Bagging, any element has the same probability to appear in a
    new data set. However, for Boosting the observations are weighted and therefore
    some of them will take part in the new sets more often:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Bagging，任何元素在新数据集中出现的概率是相同的。然而，对于Boosting，观察值是加权的，因此其中一些会更频繁地参与新集合：
- en: '![](../Images/c04ea35b2f2c64b7ac560c54c6b7abbe.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c04ea35b2f2c64b7ac560c54c6b7abbe.png)'
- en: These multiple sets are used to train the same learner algorithm and therefore
    different classifiers are produced.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些多个集合用于训练相同的学习算法，因此产生了不同的分类器。
- en: Why are the data elements weighted?
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么数据元素会被加权？
- en: 'At this point, we begin to deal with the main difference between the two methods.
    While the training stage is parallel for Bagging (i.e., each model is built independently),
    Boosting builds the new learner in a sequential way:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们开始处理这两种方法之间的主要区别。虽然Bagging的训练阶段是并行的（即每个模型独立构建），但Boosting以顺序方式构建新的学习器：
- en: '![](../Images/d54f29b1e2365a4d2704687ec12e5e62.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d54f29b1e2365a4d2704687ec12e5e62.png)'
- en: In Boosting algorithms each classifier is trained on data, taking into account
    the previous classifiers’ success. After each training step, the weights are redistributed. M**isclassified
    data increases its weights** to emphasise the most difficult cases. In this way,
    subsequent learners will focus on them during their training.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Boosting 算法中，每个分类器都是在数据上训练的，并考虑了之前分类器的成功。每次训练步骤后，权重会重新分配。**被错误分类的数据增加其权重**，以强调最困难的案例。这样，后续学习器在训练过程中会集中关注这些案例。
- en: How does the classification stage work?
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类阶段是如何工作的？
- en: To predict the class of new data we only need to **apply the N learners to the
    new observations.** In Bagging the result is obtained by averaging the responses
    of the N learners (or majority vote). However, Boosting assigns a second set of
    weights, this time for the N classifiers, in order to take a **weighted average** of
    their estimates.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要预测新数据的类别，我们只需**将 N 个学习器应用于新观察数据**。在 Bagging 中，结果是通过对 N 个学习器的响应进行平均（或多数投票）获得的。然而，Boosting
    为 N 个分类器分配第二组权重，以便对它们的估计进行**加权平均**。
- en: '![](../Images/e374bcb35da6c40895bf7459957d9e9a.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e374bcb35da6c40895bf7459957d9e9a.png)'
- en: 'In the Boosting training stage, the algorithm allocates weights to each resulting
    model. A learner with good a classification result on the training data will be
    assigned a higher weight than a poor one. So when evaluating a new learner, Boosting
    needs to keep track of learners’ errors, too. Let’s see the differences in the
    procedures:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Boosting 训练阶段，算法会为每个结果模型分配权重。对训练数据分类效果好的学习器会被分配比效果差的学习器更高的权重。因此，在评估新的学习器时，Boosting
    还需要跟踪学习器的错误。我们来看看过程中的差异：
- en: '![](../Images/9013ae5145570a4c536dd687e1058b44.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9013ae5145570a4c536dd687e1058b44.png)'
- en: Some of the Boosting techniques include an extra-condition to keep or discard
    a single learner. For example, in AdaBoost, the most renowned, an error less than
    50% is required to maintain the model; otherwise, the iteration is repeated until
    achieving a learner better than a random guess.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一些 Boosting 技术包括额外的条件来决定是否保留或丢弃单个学习器。例如，在最著名的 AdaBoost 中，需要误差小于 50% 才能维持模型；否则，迭代将重复进行，直到达到比随机猜测更好的学习器。
- en: The previous image shows the general process of a Boosting method, but several
    alternatives exist with different ways to determine the weights to use in the
    next training step and in the classification stage. Click here if you like to
    go into detail: [AdaBoost](https://en.wikipedia.org/wiki/AdaBoost), [LPBoost](https://en.wikipedia.org/wiki/LPBoost), [XGBoost](http://arxiv.org/pdf/1603.02754v1.pdf), [GradientBoost](https://en.wikipedia.org/wiki/Gradient_boosting), [BrownBoost](https://en.wikipedia.org/wiki/BrownBoost).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的图像展示了 Boosting 方法的一般过程，但存在几种替代方法，它们在确定下一训练步骤和分类阶段使用的权重方面有所不同。如果你想详细了解，请点击这里：[AdaBoost](https://en.wikipedia.org/wiki/AdaBoost)、[LPBoost](https://en.wikipedia.org/wiki/LPBoost)、[XGBoost](http://arxiv.org/pdf/1603.02754v1.pdf)、[GradientBoost](https://en.wikipedia.org/wiki/Gradient_boosting)、[BrownBoost](https://en.wikipedia.org/wiki/BrownBoost)。
- en: Which is the best, Bagging or Boosting?
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Bagging 和 Boosting 哪个更好？
- en: There’s not an outright winner; it depends on the data, the simulation and the
    circumstances.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 没有绝对的赢家；这取决于数据、模拟和具体情况。
- en: Bagging and Boosting decrease the variance of your single estimate as they combine
    several estimates from different models. So the result may be a model with **higher
    stability**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging 和 Boosting 通过结合来自不同模型的多个估计来降低单个估计的方差。因此，结果可能是一个**更高稳定性**的模型。
- en: If the problem is that the single model gets a very low performance, Bagging
    will rarely get a **better bias**. However, Boosting could generate a combined
    model with lower errors as it optimises the advantages and reduces pitfalls of
    the single model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果问题在于单个模型性能非常低，Bagging 很少能获得**更好的偏差**。然而，Boosting 可以生成一个综合模型，具有较低的错误，因为它优化了单个模型的优点并减少了陷阱。
- en: By contrast, if the difficulty of the single model is **over-fitting**, then
    Bagging is the best option. Boosting for its part doesn’t help to avoid over-fitting;
    in fact, this technique is faced with this problem itself. For this reason, Bagging
    is effective more often than Boosting.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果单个模型的困难是**过拟合**，那么 Bagging 是最佳选择。Boosting 本身并不能避免过拟合；实际上，这一技术本身面临这个问题。因此，Bagging
    比 Boosting 更常有效。
- en: 'To sum up:'
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结：
- en: '| **Similarities** |  | **Differences** |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| **相似之处** |  | **差异** |'
- en: '| Both are ensemble methods to get N learners from 1 learner… |  | … but, while
    they are built independently for Bagging, Boosting tries to add new models that
    do well where previous models fail. |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 两者都是集成方法，将N个学习者从1个学习者中获取… |  | … 但Bagging是独立构建的，而Boosting尝试添加在之前模型失败的地方表现良好的新模型。
    |'
- en: '| Both generate several training data sets by random sampling… |  | … but only
    Boosting determines weights for the data to tip the scales in favor of the most
    difficult cases. |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 两者都通过随机抽样生成多个训练数据集… |  | … 但只有Boosting为数据确定权重，以便将偏差倾斜到最困难的案例。 |'
- en: '| Both make the final decision by averaging  the N learners (or taking the
    majority of them)… |  | … but it is an equally weighted average for Bagging and
    a weighted average for Boosting, more weight to those with better performance
    on training data. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 两者都通过对N个学习者的结果取平均（或取多数）来做出最终决定… |  | … 但Bagging采用的是等权重平均，而Boosting采用加权平均，更多权重分配给在训练数据中表现更好的学习者。
    |'
- en: '| Both are good at reducing variance and provide higher stability… |  | … but
    only Boosting tries to reduce bias. On the other hand, Bagging may solve the over-fitting
    problem, while Boosting can increase it. |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 两者都擅长减少方差并提供更高的稳定性… |  | … 但只有Boosting尝试减少偏差。另一方面，Bagging可能解决过拟合问题，而Boosting可能增加过拟合。
    |'
- en: '[Original](https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/).
    Reposted with permission.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/)。经许可转载。'
- en: '**Related:**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Ensemble Learning to Improve Machine Learning Results](/2017/09/ensemble-learning-improve-machine-learning-results.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集成学习以提升机器学习结果](/2017/09/ensemble-learning-improve-machine-learning-results.html)'
- en: '[Data Science Primer: Basic Concepts for Beginners](/2017/08/data-science-primer-basic-concepts-for-beginners.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学入门：初学者的基本概念](/2017/08/data-science-primer-basic-concepts-for-beginners.html)'
- en: '[Must-Know: What is the idea behind ensemble learning?](/2017/05/must-know-ensemble-learning.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[必须了解：集成学习的基本思想是什么？](/2017/05/must-know-ensemble-learning.html)'
- en: '* * *'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行IT管理'
- en: '* * *'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[What Is the Difference Between SQL and Object-Relational Mapping (ORM)?](https://www.kdnuggets.com/2022/02/difference-sql-object-relational-mapping-orm.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQL与对象关系映射（ORM）的区别是什么？](https://www.kdnuggets.com/2022/02/difference-sql-object-relational-mapping-orm.html)'
- en: '[What’s the Difference Between Data Analysts and Data Scientists?](https://www.kdnuggets.com/2022/03/difference-data-analysts-data-scientists.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据分析师和数据科学家的区别是什么？](https://www.kdnuggets.com/2022/03/difference-data-analysts-data-scientists.html)'
- en: '[Efficiency Spells the Difference Between Biological Neurons and…](https://www.kdnuggets.com/2022/11/efficiency-spells-difference-biological-neurons-artificial-counterparts.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[效率决定生物神经元与…](https://www.kdnuggets.com/2022/11/efficiency-spells-difference-biological-neurons-artificial-counterparts.html)'
- en: '[The Difference Between L1 and L2 Regularization](https://www.kdnuggets.com/2022/08/difference-l1-l2-regularization.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[L1与L2正则化的区别](https://www.kdnuggets.com/2022/08/difference-l1-l2-regularization.html)'
- en: '[The Difference Between Training and Testing Data in Machine Learning](https://www.kdnuggets.com/2022/08/difference-training-testing-data-machine-learning.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中训练数据和测试数据的区别](https://www.kdnuggets.com/2022/08/difference-training-testing-data-machine-learning.html)'
- en: '[WTF is the Difference Between GBM and XGBoost?](https://www.kdnuggets.com/wtf-is-the-difference-between-gbm-and-xgboost)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GBM与XGBoost的区别是什么？](https://www.kdnuggets.com/wtf-is-the-difference-between-gbm-and-xgboost)'
