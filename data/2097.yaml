- en: 'Introducing Falcon2: Next-Gen Language Model by TII'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Falcon2：TII 的下一代语言模型
- en: 原文：[https://www.kdnuggets.com/introducing-falcon2-next-gen-language-model-by-tii](https://www.kdnuggets.com/introducing-falcon2-next-gen-language-model-by-tii)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/introducing-falcon2-next-gen-language-model-by-tii](https://www.kdnuggets.com/introducing-falcon2-next-gen-language-model-by-tii)
- en: '![Falcon2](../Images/d2e9caefaf85567b2d00eb13c5cadc25.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Falcon2](../Images/d2e9caefaf85567b2d00eb13c5cadc25.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于作者
- en: 'The Technology Innovation Institute (TII) in Abu Dhabi released its next series
    of Falcon language models on May 14\. The new models match the TII mission as
    technology enablers and are available as open-source models on HuggingFace. They
    released two variants of the Falcon 2 models: **Falcon-2-11B** and **Falcon-2-11B-VLM**.
    The new VLM model promises exceptional multi-model compatibilities that perform
    on par with other open-source and closed-source models.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 阿布扎比的技术创新研究所（TII）于 5 月 14 日发布了其下一系列 Falcon 语言模型。新模型符合 TII 作为技术推动者的使命，并作为开源模型在
    HuggingFace 上提供。他们发布了两种 Falcon 2 模型变体：**Falcon-2-11B** 和 **Falcon-2-11B-VLM**。新的
    VLM 模型承诺具有卓越的多模型兼容性，性能与其他开源和闭源模型相当。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Model Features and Performance
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型特性与性能
- en: 'The recent Falcon-2 language model has 11 billion parameters and is trained
    on 5.5 trillion tokens from the [falcon-refinedweb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)
    dataset. The newer, more efficient models compete well against the Meta’s recent
    Llama3 model with 8 billion parameters. The results are summarized in the below
    table shared by TII:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的 Falcon-2 语言模型拥有 110 亿个参数，并在来自 [falcon-refinedweb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)
    数据集的 5.5 万亿个标记上进行了训练。更新的、更高效的模型与 Meta 最近的 Llama3 模型（拥有 80 亿个参数）竞争良好。结果汇总在下面由 TII
    分享的表格中：
- en: '![Falcon 2 Results](../Images/3be394b7d26dd694b77312d3b8ddb8a3.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![Falcon 2 结果](../Images/3be394b7d26dd694b77312d3b8ddb8a3.png)'
- en: Image by [TII](https://falconllm.tii.ae/falcon-2.html)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [TII](https://falconllm.tii.ae/falcon-2.html)
- en: In addition, the Falcon-2 model fares well against Google’s Gemma with 7 billion
    parameters. Gemma-7B outperforms the Falcon-2 average performance by only 0.01\.
    In addition, the model is multi-lingual, trained on commonly used languages inclduing
    English, French, Spanish and German amongst others.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Falcon-2 模型在与谷歌的 Gemma（拥有 70 亿个参数）的比较中表现良好。Gemma-7B 仅比 Falcon-2 平均性能高出 0.01。该模型还是多语言的，训练了包括英语、法语、西班牙语和德语等常用语言。
- en: However, the groundbreaking achievement is the release of Falcon-2-11B Vision
    Language Model that adds image understanding and multi-modularity to the same
    language model. The image-to-text conversation capability with comparable capabilities
    with recent models like Llama3 and Gemma is a significant advancement.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，突破性的成就是 Falcon-2-11B 视觉语言模型的发布，该模型在同一语言模型中增加了图像理解和多模态功能。与 Llama3 和 Gemma
    等近期模型具有相当能力的图像到文本对话能力是一项重要进展。
- en: How to Use the Models for Inference
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用模型进行推断
- en: Let’s get to the coding part so we can run the model on our local system and
    generate responses. First, like any other project, let us set up a fresh environment
    to avoid dependency conflicts. Given the model is released recently, we will the
    need the latest versions of all libraries to avoid missing support and pipelines.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入编码部分，这样我们就可以在本地系统上运行模型并生成响应。首先，像处理其他项目一样，让我们设置一个新的环境以避免依赖冲突。由于模型刚刚发布，我们将需要所有库的最新版本，以避免缺少支持和管道。
- en: 'Create a new Python virtual environment and activate it using the below commands:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的 Python 虚拟环境，并使用以下命令激活它：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now we have a clean environment, we can install our required libraries and dependencies
    using Python package manager. For this project, we will use images available on
    the internet and load them in Python. The requests and Pillow library are suitable
    for this purpose. Moreover, for loading the model, we will you use the transformers
    library that has internal support for HuggingFace model loading and inference.
    We will use bitsandbytes, PyTorch and accelerate as a model loading utility and
    quantization.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个干净的环境，我们可以使用 Python 包管理器安装所需的库和依赖项。对于这个项目，我们将使用互联网上可用的图像并在 Python 中加载它们。`requests`
    和 `Pillow` 库适用于此目的。此外，为了加载模型，我们将使用 `transformers` 库，该库内置支持 HuggingFace 模型加载和推理。我们将使用
    `bitsandbytes`、`PyTorch` 和 `accelerate` 作为模型加载实用工具和量化工具。
- en: 'To ease up the set up process, we can create a simple requirements text file
    as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化设置过程，我们可以创建一个简单的需求文本文件，如下所示：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can now install all the dependencies in a single line using:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用以下命令在一行中安装所有依赖项：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can now start working on our code to use the model for inference. Let’s start
    by loading the model in our local system. The model is available on [HuggingFace](https://huggingface.co/tiiuae/falcon-11B-vlm)
    and the total size exceeds 20GB of memory. We can not load the model in consumer
    grade GPUs which usually have around 8-16GB RAM. Hence, we will need to quantize
    the model i.e. we will load the model in 4-bit floating point numbers instead
    of the usual 32-bit precision to decrease the memory requirements.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始编写代码以使用模型进行推理。让我们从在本地系统中加载模型开始。模型可以在 [HuggingFace](https://huggingface.co/tiiuae/falcon-11B-vlm)
    找到，总大小超过 20GB。我们无法在通常具有 8-16GB 内存的消费级 GPU 上加载模型。因此，我们需要对模型进行量化，即将模型加载为 4 位浮点数，而不是通常的
    32 位精度，以减少内存需求。
- en: 'The bitsandbytes library provides an easy interface for quantization of Large
    Language Models in HuggingFace. We can initalize a quantization configuration
    that can be passed to the model. HuggingFace internally handles all required operations
    and sets the correct precision and adjustments for us. The config can be set as
    follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`bitsandbytes` 库提供了一个简单的接口，用于量化 HuggingFace 中的大型语言模型。我们可以初始化一个量化配置并将其传递给模型。HuggingFace
    内部处理所有必要的操作，并为我们设置正确的精度和调整。配置可以设置如下：'
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This allows the model to fit in under 16GB GPU RAM, making it easier to load
    the model without offloading and distribution. We can now load the Falcon-2B-VLM.
    Being a multi-modal model, we will be handling images alongside textual prompts.
    The LLava model and pipelines are designed for this purpose as they allow CLIP-based
    image embeddings to be projected to language model inputs. The transformers library
    has built-in Llava model processors and pipelines. We can then load the model
    as below:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得模型可以适应 16GB GPU 内存，从而更容易加载模型而无需卸载和分发。我们现在可以加载 Falcon-2B-VLM。作为一个多模态模型，我们将同时处理图像和文本提示。`LLava`
    模型和管道专为此目的而设计，它们允许基于 CLIP 的图像嵌入投影到语言模型输入中。`transformers` 库具有内置的 Llava 模型处理器和管道。我们可以按如下方式加载模型：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We pass the model url from the HuggingFace model card to the processor and generator.
    We also pass the bitsandbytes quantization config to the generative model, so
    it will be automatically loaded in 4-bit precision.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 HuggingFace 模型卡上的模型 URL 传递给处理器和生成器。我们还将 `bitsandbytes` 量化配置传递给生成模型，这样它将自动以
    4 位精度加载。
- en: 'We can now start using the model to generate responses! To explore the multi-modal
    nature of Falcon-11B, we will need to load an image in Python. For a test sample,
    let us load this standard image available [here](http://images.cocodataset.org/val2017/000000039769.jpg).
    To load an image from a web URL, we can use the Pillow and requests library as
    below:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始使用模型生成响应！为了探索 Falcon-11B 的多模态特性，我们需要在 Python 中加载一张图像。作为测试样本，我们加载这个标准图像
    [这里](http://images.cocodataset.org/val2017/000000039769.jpg)。要从网络 URL 加载图像，我们可以使用
    `Pillow` 和 `requests` 库，如下所示：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The requests library downloads the image from the URL, and the Pillow library
    can read the image from bytes to a standard image format. Now that can have our
    test image, we can now generate a sample response from our model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`requests` 库从 URL 下载图像，`Pillow` 库可以将字节读取为标准图像格式。现在我们有了测试图像，我们可以从模型生成示例响应。'
- en: Let’s set up a sample prompt template that the model is sensitive to.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置一个示例提示模板，模型对其很敏感。
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The prompt template itself is self-explanatory and we need to follow it for
    best responses from the VLM. We pass the prompt and the image to the Llava image
    processor. It internally uses CLIP to create a combined embedding of the image
    and the prompt.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 提示模板本身是自解释的，我们需要按照它来获得VLM的最佳响应。我们将提示和图像传递给Llava图像处理器。它内部使用CLIP创建图像和提示的组合嵌入。
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The returned tensor embedding acts as an input for the generative model. We
    pass the embeddings and the transformer-based Falcon-11B model generates a textual
    response based on the image and instruction provided originally.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的张量嵌入作为生成模型的输入。我们传递这些嵌入，基于原始提供的图像和指令，使用基于变换器的Falcon-11B模型生成文本响应。
- en: 'We can generate the response using the below code:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码生成响应：
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: There we have it! The generated_captions variable is a string that contains
    the generated response from the model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！`generated_captions`变量是一个字符串，包含了模型生成的响应。
- en: Results
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: We tested various images using the above code and the responses for some of
    them are summarized in this image below. We see that the Falcon-2 model has a
    strong understanding of the image and generates legible answers to show its comprehension
    of the scenarios in the images. It can read text and also highlights the global
    information as a whole. To summarize, the model has excellent capabilities for
    visual tasks, and can be used for image-based conversations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用上述代码测试了各种图像，对其中一些图像的响应进行了总结，见下图。我们看到，Falcon-2模型对图像有很强的理解能力，并生成了清晰的答案，显示出其对图像场景的理解。它可以读取文本，还突出显示了整体信息。总之，模型在视觉任务上表现出色，可用于基于图像的对话。
- en: '![Falcon 2 Inference Results](../Images/69694f523f6c6f8effcbbe361ca41ddd.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![Falcon 2推理结果](../Images/69694f523f6c6f8effcbbe361ca41ddd.png)'
- en: 'Image by Author| Inference images from the Internet. Sources: [Cats Image](http://images.cocodataset.org/val2017/000000039769.jpg),
    [Card Image](https://usa.visa.com/dam/VCOM/global/common-assets/cards/visa-prepaid-card-800x450.png),
    [Football Image](https://static.theprint.in/wp-content/uploads/2020/07/football.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者 | 图片来自互联网。来源：[猫咪图片](http://images.cocodataset.org/val2017/000000039769.jpg)、[卡片图片](https://usa.visa.com/dam/VCOM/global/common-assets/cards/visa-prepaid-card-800x450.png)、[足球图片](https://static.theprint.in/wp-content/uploads/2020/07/football.jpg)
- en: License and Compliance
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 许可与合规
- en: In addition to being open-source, the models are released with the Apache2.0
    License making them available for Open Access. This allows the modification and
    distribution of the model for personal and commercial uses. This means that you
    can now use Falcon-2 models to supercharge your LLM-based applications and open-source
    models to provide multi-modal capabilities for your users.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 除了开源外，这些模型还以Apache2.0许可证发布，使其可供公开访问。这允许对模型进行修改和分发，用于个人和商业用途。这意味着你现在可以使用Falcon-2模型来提升基于LLM的应用程序，并利用开源模型为用户提供多模态能力。
- en: Wrapping Up
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Overall, the new Falcon-2 models show promising results. But that is not all!
    TII is already working on the next iteration to further push performance. They
    look to integrate the Mixture-of-Experts (MoE) and other machine learning capabilities
    into their models to improve accuracy and intelligence. If Falcon-2 seems like
    an improvement, be ready for their next announcement.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，新版Falcon-2模型显示了令人鼓舞的结果。但这还不是全部！TII已经在着手下一次迭代，以进一步提升性能。他们计划将专家混合（MoE）和其他机器学习能力整合到模型中，以提高准确性和智能性。如果Falcon-2看起来像是一个改进，请准备好迎接他们的下一个公告。
- en: '**[](https://www.linkedin.com/in/kanwal-mehreen1/)**[Kanwal Mehreen](https://www.linkedin.com/in/kanwal-mehreen1/)****
    Kanwal is a machine learning engineer and a technical writer with a profound passion
    for data science and the intersection of AI with medicine. She co-authored the
    ebook "Maximizing Productivity with ChatGPT". As a Google Generation Scholar 2022
    for APAC, she champions diversity and academic excellence. She''s also recognized
    as a Teradata Diversity in Tech Scholar, Mitacs Globalink Research Scholar, and
    Harvard WeCode Scholar. Kanwal is an ardent advocate for change, having founded
    FEMCodes to empower women in STEM fields.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**[](https://www.linkedin.com/in/kanwal-mehreen1/)**[Kanwal Mehreen](https://www.linkedin.com/in/kanwal-mehreen1/)**
    Kanwal 是一位机器学习工程师和技术作家，对数据科学及AI与医学的交集充满深厚的热情。她共同撰写了电子书《使用ChatGPT最大化生产力》。作为2022年亚太地区的Google
    Generation学者，她倡导多样性和学术卓越。她还被认定为Teradata多样性技术学者、Mitacs Globalink研究学者和哈佛WeCode学者。Kanwal
    是变革的坚定倡导者，她创立了FEMCodes，旨在赋能女性在STEM领域中的发展。'
- en: More On This Topic
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Introducing the Testing Library for Natural Language Processing](https://www.kdnuggets.com/2023/04/introducing-testing-library-natural-language-processing.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍自然语言处理测试库](https://www.kdnuggets.com/2023/04/introducing-testing-library-natural-language-processing.html)'
- en: '[Introducing Healthcare-Specific Large Language Models from John Snow Labs](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍 John Snow Labs 的医疗专用大型语言模型](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
- en: '[Introducing TPU v4: Googles Cutting Edge Supercomputer for Large…](https://www.kdnuggets.com/2023/04/introducing-tpu-v4-googles-cutting-edge-supercomputer-large-language-models.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍 TPU v4：谷歌前沿超算用于大型语言模型](https://www.kdnuggets.com/2023/04/introducing-tpu-v4-googles-cutting-edge-supercomputer-large-language-models.html)'
- en: '[Introducing Objectiv: Open-source product analytics infrastructure](https://www.kdnuggets.com/2022/06/objectiv-introducing-objectiv-opensource-product-analytics-infrastructure.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍 Objectiv：开源产品分析基础设施](https://www.kdnuggets.com/2022/06/objectiv-introducing-objectiv-opensource-product-analytics-infrastructure.html)'
- en: '[Introducing OpenChat: The Free & Simple Platform for Building…](https://www.kdnuggets.com/2023/06/introducing-openchat-free-simple-platform-building-custom-chatbots-minutes.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍 OpenChat：构建自定义聊天机器人的免费简易平台](https://www.kdnuggets.com/2023/06/introducing-openchat-free-simple-platform-building-custom-chatbots-minutes.html)'
- en: '[Introducing OpenLLM: Open Source Library for LLMs](https://www.kdnuggets.com/2023/07/introducing-openllm-open-source-library-llms.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍 OpenLLM：开源大型语言模型库](https://www.kdnuggets.com/2023/07/introducing-openllm-open-source-library-llms.html)'
