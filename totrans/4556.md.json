["```py\nimport pandas as pd\nimport numpy as npfrom sklearn.datasets import load_wine\nfrom sklearn.datasets import load_bostonfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessingfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrtfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn import linear_model\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.svm import SVRfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom yellowbrick.cluster import SilhouetteVisualizer\n```", "```py\nwine = load_wine()\nwine_df = pd.DataFrame(wine.data, columns=wine.feature_names)\nwine_df['TARGET'] = pd.Series(wine.target)\n```", "```py\nX_w = wine_df.drop(['TARGET'], axis=1)\ny_w = wine_df['TARGET']\nX_train_w, X_test_w, y_train_w, y_test_w = train_test_split(X_w, y_w, test_size=0.2)\n```", "```py\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    NuSVC(probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier()\n    ]\nfor classifier in classifiers:\n    model = classifier\n    model.fit(X_train_w, y_train_w)  \n    y_pred_w = model.predict(X_test_w)\n    print(classifier)\n    print(\"model score: %.3f\" % f1_score(y_test_w, y_pred_w, average='weighted'))\n```", "```py\nboston = load_boston()\nboston_df = pd.DataFrame(boston.data, columns=boston.feature_names)\nboston_df['TARGET'] = pd.Series(boston.target)X_b = boston_df.drop(['TARGET'], axis=1)\ny_b = boston_df['TARGET']\nX_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_b, y_b, test_size=0.2)\n```", "```py\nregressors = [\n    linear_model.Lasso(alpha=0.1),\n    linear_model.LinearRegression(),\n    ElasticNetCV(alphas=None, copy_X=True, cv=5, eps=0.001, fit_intercept=True,\n       l1_ratio=0.5, max_iter=1000, n_alphas=100, n_jobs=None,\n       normalize=False, positive=False, precompute='auto', random_state=0,\n       selection='cyclic', tol=0.0001, verbose=0),\n    SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n    gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n    tol=0.001, verbose=False),\n    linear_model.Ridge(alpha=.5)                \n    ]for regressor in regressors:\n    model = regressor\n    model.fit(X_train_b, y_train_b)  \n    y_pred_b = model.predict(X_test_b)\n    print(regressor)\n    print(\"mean squared error: %.3f\" % sqrt(mean_squared_error(y_test_b, y_pred_b)))\n```", "```py\nnp.random.seed(0)\nmsk = np.random.rand(len(X_w)) < 0.8\ntrain_w = X_w[msk]\ntest_w = X_w[~msk]\n```", "```py\nx = train_w.values\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\nX_scaled = pd.DataFrame(x_scaled,columns=train_w.columns)\n```", "```py\nmodel = KMeans()\nvisualizer = KElbowVisualizer(model, k=(1,8))\nvisualizer.fit(X_scaled)       \nvisualizer.show()\n```", "```py\nmodel = KMeans(3, random_state=42)\nvisualizer = SilhouetteVisualizer(model, colors='yellowbrick')visualizer.fit(X_scaled)      \nvisualizer.show()\n```"]