["```py\n# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.tokenize import RegexpTokenizer\nimport re\nimport string\nimport random\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Reading the file\ndf = pd.read_csv(\"goodread.csv\")\n\n#Reading the first five records\ndf.head()\n\n#Checking the shape of the file\ndf.shape()\n```", "```py\n# Genre distribution\ndf['genre'].value_counts().plot(x = 'genre', y ='count', kind = 'bar', figsize = (10,5)  )\n```", "```py\n# Printing the book title and description randomly\ndf['title'] [2464]\ndf['Desc'][2464]\n```", "```py\n# Printing the book title and description randomly\ndf['title'] [367]\ndf['Desc'][367]\n```", "```py\n# Calculating the word count for book description\ndf['word_count'] = df2['Desc'].apply(lambda x: len(str(x).split()))# Plotting the word count\ndf['word_count'].plot(\n    kind='hist',\n    bins = 50,\n    figsize = (12,8),title='Word Count Distribution for book descriptions')\n```", "```py\nfrom textblob import TextBlob\nblob = TextBlob(str(df['Desc']))\npos_df = pd.DataFrame(blob.tags, columns = ['word' , 'pos'])\npos_df = pos_df.pos.value_counts()[:20]\npos_df.plot(kind = 'bar', figsize=(10, 8), title = \"Top 20 Part-of-speech tagging for comments\")\n```", "```py\n#Converting text descriptions into vectors using TF-IDF using Bigram\ntf = TfidfVectorizer(ngram_range=(2, 2), stop_words='english', lowercase = False)\ntfidf_matrix = tf.fit_transform(df['Desc'])\ntotal_words = tfidf_matrix.sum(axis=0) \n#Finding the word frequency\nfreq = [(word, total_words[0, idx]) for word, idx in tf.vocabulary_.items()]\nfreq =sorted(freq, key = lambda x: x[1], reverse=True)\n#converting into dataframe \nbigram = pd.DataFrame(freq)\nbigram.rename(columns = {0:'bigram', 1: 'count'}, inplace = True) \n#Taking first 20 records\nbigram = bigram.head(20)\n\n#Plotting the bigram distribution\nbigram.plot(x ='bigram', y='count', kind = 'bar', title = \"Bigram disribution for the top 20 words in the book description\", figsize = (15,7), )\n```", "```py\n#Converting text descriptions into vectors using TF-IDF using Trigram\ntf = TfidfVectorizer(ngram_range=(3, 3), stop_words='english', lowercase = False)\ntfidf_matrix = tf.fit_transform(df['Desc'])\ntotal_words = tfidf_matrix.sum(axis=0) \n#Finding the word frequency\nfreq = [(word, total_words[0, idx]) for word, idx in tf.vocabulary_.items()]\nfreq =sorted(freq, key = lambda x: x[1], reverse=True)#converting into dataframe \ntrigram = pd.DataFrame(freq)\ntrigram.rename(columns = {0:'trigram', 1: 'count'}, inplace = True) \n#Taking first 20 records\ntrigram = trigram.head(20)\n\n#Plotting the trigramn distribution\ntrigram.plot(x ='trigram', y='count', kind = 'bar', title = \"Bigram disribution for the top 20 words in the book description\", figsize = (15,7), )\n```", "```py\n# Function for removing NonAscii characters\ndef _removeNonAscii(s):\n    return \"\".join(i for i in s if  ord(i)<128)\n\n# Function for converting into lower case\ndef make_lower_case(text):\n    return text.lower()\n\n# Function for removing stop words\ndef remove_stop_words(text):\n    text = text.split()\n    stops = set(stopwords.words(\"english\"))\n    text = [w for w in text if not w in stops]\n    text = \" \".join(text)\n    return text\n\n# Function for removing punctuation\ndef remove_punctuation(text):\n    tokenizer = RegexpTokenizer(r'\\w+')\n    text = tokenizer.tokenize(text)\n    text = \" \".join(text)\n    return text\n\n# Function for removing the html tags\ndef remove_html(text):\n    html_pattern = re.compile('<.*?>')\n    return html_pattern.sub(r'', text)\n\n# Applying all the functions in description and storing as a cleaned_desc\ndf['cleaned_desc'] = df['Desc'].apply(_removeNonAscii)\ndf['cleaned_desc'] = df.cleaned_desc.apply(func = make_lower_case)\ndf['cleaned_desc'] = df.cleaned_desc.apply(func = remove_stop_words)\ndf['cleaned_desc'] = df.cleaned_desc.apply(func=remove_punctuation)\ndf['cleaned_desc'] = df.cleaned_desc.apply(func=remove_html)\n```", "```py\n# Function for recommending books based on Book title. It takes book title and genre as an input.def recommend(title, genre):\n\n    # Matching the genre with the dataset and reset the index\n    data = df2.loc[df2['genre'] == genre]  \n    data.reset_index(level = 0, inplace = True) \n\n    # Convert the index into series\n    indices = pd.Series(data.index, index = data['title'])\n\n   ** #Converting the book title into vectors and used bigram**\n    tf = TfidfVectorizer(analyzer='word', ngram_range=(2, 2), min_df = 1, stop_words='english')\n    tfidf_matrix = tf.fit_transform(data['title'])\n\n    # Calculating the similarity measures based on Cosine Similarity\n    sg = cosine_similarity(tfidf_matrix, tfidf_matrix)\n\n    # Get the index corresponding to original_title\n\n    idx = indices[title]# Get the pairwsie similarity scores \n    sig = list(enumerate(sg[idx]))# Sort the books\n    sig = sorted(sig, key=lambda x: x[1], reverse=True)# Scores of the 5 most similar books \n    sig = sig[1:6]# Book indicies\n    movie_indices = [i[0] for i in sig]\n\n    # Top 5 book recommendation\n    rec = data[['title', 'url']].iloc[movie_indices]\n\n    # It reads the top 5 recommended book urls and print the images\n\n    for i in rec['url']:\n        response = requests.get(i)\n        img = Image.open(BytesIO(response.content))\n        plt.figure()\n        print(plt.imshow(img))\n```", "```py\nrecommend(\"Steve Jobs\", \"Business\")\n```", "```py\n# Function for recommending books based on Book title. It takes book title and genre as an input.def recommend(title, genre):\n\n    global rec\n    # Matching the genre with the dataset and reset the index\n    data = df2.loc[df2['genre'] == genre]  \n    data.reset_index(level = 0, inplace = True) \n\n    # Convert the index into series\n    indices = pd.Series(data.index, index = data['title'])\n\n    **#Converting the book description into vectors and used bigram**\n    tf = TfidfVectorizer(analyzer='word', ngram_range=(2, 2), min_df = 1, stop_words='english')\n    tfidf_matrix = tf.fit_transform(data['cleaned_desc'])\n\n    # Calculating the similarity measures based on Cosine Similarity\n    sg = cosine_similarity(tfidf_matrix, tfidf_matrix)\n\n    # Get the index corresponding to original_title\n\n    idx = indices[title]# Get the pairwsie similarity scores \n    sig = list(enumerate(sg[idx]))# Sort the books\n    sig = sorted(sig, key=lambda x: x[1], reverse=True)# Scores of the 5 most similar books \n    sig = sig[1:6]# Book indicies\n    movie_indices = [i[0] for i in sig]\n\n    # Top 5 book recommendation\n    rec = data[['title', 'url']].iloc[movie_indices]\n\n    # It reads the top 5 recommend book url and print the images\n\n    for i in rec['url']:\n        response = requests.get(i)\n        img = Image.open(BytesIO(response.content))\n        plt.figure()\n        print(plt.imshow(img))\n```", "```py\nrecommend(\"Harry Potter and the Prisoner of Azkaban\", \"Non-Fiction\")\n```", "```py\nrecommend(\"Norwegian Wood\", \"Non-Fiction\")\n```"]