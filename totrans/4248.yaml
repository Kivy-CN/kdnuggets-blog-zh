- en: 'From Scratch: Permutation Feature Importance for ML Interpretability'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始：用于机器学习解释性的置换特征重要性
- en: 原文：[https://www.kdnuggets.com/2021/06/from-scratch-permutation-feature-importance-ml-interpretability.html](https://www.kdnuggets.com/2021/06/from-scratch-permutation-feature-importance-ml-interpretability.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/06/from-scratch-permutation-feature-importance-ml-interpretability.html](https://www.kdnuggets.com/2021/06/from-scratch-permutation-feature-importance-ml-interpretability.html)
- en: '**By [Seth Billiau](https://www.linkedin.com/in/seth-billiau-b4b062136/), Data
    Scientist and Statistician**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：[Seth Billiau](https://www.linkedin.com/in/seth-billiau-b4b062136/)，数据科学家和统计学家**'
- en: '![](../Images/b22185d12db44de4fd434cebf30c3851.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b22185d12db44de4fd434cebf30c3851.png)'
- en: Photo by [Arno Senoner](https://unsplash.com/@arnosenoner) on [Unsplash](https://unsplash.com/photos/ZxA9vV0phGI)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Arno Senoner](https://unsplash.com/@arnosenoner)提供，来源于[Unsplash](https://unsplash.com/photos/ZxA9vV0phGI)
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍
- en: Advanced topics in machine learning are dominated by black box models. As the
    name suggests, black box models are complex models where it’s extremely hard to
    understand how model inputs are combined to make predictions. Deep learning models
    like [artificial neural networks](https://link.springer.com/article/10.1007/s12518-021-00360-9) and
    ensemble models like [random forests](https://towardsdatascience.com/understanding-random-forest-58381e0602d2),
    gradient boosting learners, and [model stacking](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/) are
    examples of black box models that yield remarkably accurate predictions in a variety
    of domains from [urban planning](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6567884/) to [computer
    vision](https://medium.com/swlh/computer-vision-with-convolutional-neural-networks-22f06360cac9).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 高级机器学习主题通常被黑箱模型主导。顾名思义，黑箱模型是复杂模型，很难理解模型输入如何组合以进行预测。深度学习模型如[人工神经网络](https://link.springer.com/article/10.1007/s12518-021-00360-9)和集成模型如[随机森林](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)、梯度提升学习者和[模型堆叠](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/)是黑箱模型的例子，它们在从[城市规划](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6567884/)到[计算机视觉](https://medium.com/swlh/computer-vision-with-convolutional-neural-networks-22f06360cac9)等多个领域提供了极其准确的预测。
- en: '![](../Images/8975079b777cc8ef90ea8076241d93cc.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8975079b777cc8ef90ea8076241d93cc.png)'
- en: Diagram of a Black Box Model
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 黑箱模型的示意图
- en: However, one drawback to using these black box models is that it’s often difficult
    to interpret how predictors influence the predictions — especially with conventional
    statistical methods. This article will explain an alternative way to interpret
    black box models called permutation feature importance. Permutation feature importance
    is a powerful tool that allows us to detect which features in our dataset have
    predictive power regardless of what model we’re using.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用这些黑箱模型的一个缺点是，很难解释预测因子如何影响预测结果——尤其是使用传统统计方法时。本文将解释一种解释黑箱模型的替代方法，称为置换特征重要性。置换特征重要性是一个强大的工具，允许我们检测数据集中哪些特征具有预测能力，无论我们使用什么模型。
- en: We will begin by discussing the differences between traditional statistical
    inference and feature importance to motivate the need for permutation feature
    importance. Then, we’ll explain permutation feature importance and implement it
    from scratch to discover which predictors are important for predicting house prices
    in Blotchville. We’ll conclude by discussing some drawbacks to this approach and
    introducing some packages that can help us with permutation feature importance
    in the future.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先讨论传统统计推断与特征重要性之间的差异，以激发对置换特征重要性的需求。然后，我们将解释置换特征重要性并从头开始实现它，以发现哪些预测因子对预测Blotchville的房价很重要。最后，我们将讨论这种方法的一些缺点，并介绍一些未来可以帮助我们进行置换特征重要性的包。
- en: Statistical Inference vs. Feature Importance
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 统计推断与特征重要性
- en: When using traditional, parametric statistical models, we can rely on statistical
    inference to make precise statements about how our inputs relate to our outputs.
    When we use linear regression, for example, we know that a one-unit change in
    our predictor corresponds to a *linear* change in our output. The magnitude of
    that change is estimated during model fitting and we can provide uncertainty measures
    for these estimates using probability theory.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用传统的参数统计模型时，我们可以依靠统计推断来精确说明我们的输入与输出之间的关系。例如，当我们使用线性回归时，我们知道预测因子的单位变化对应于输出的*线性*变化。这个变化的幅度在模型拟合过程中被估计出来，我们可以使用概率理论为这些估计提供不确定性度量。
- en: '![](../Images/9eb6adf2cc2ec36a05fd68bede54b580.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9eb6adf2cc2ec36a05fd68bede54b580.png)'
- en: Photo by [Javier Allegue Barros](https://unsplash.com/@soymeraki) on [Upsplash](https://unsplash.com/photos/0nOP5iHVaZ8)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [Javier Allegue Barros](https://unsplash.com/@soymeraki) 在 [Upsplash](https://unsplash.com/photos/0nOP5iHVaZ8)
    上拍摄的照片
- en: Unfortunately, it’s often impossible for us to make these kinds of statements
    when using a black box model. A deep neural network likely has hundreds, thousands,
    or even [millions](https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33) of
    trainable weights that connect the input predictors to the output predictions
    (ResNet-50 has over 23 million trainable parameters) along with several non-linear
    activation functions. When dealing with a model this complex, it becomes extremely
    challenging to map out the relationship between predictor and prediction analytically.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，当使用黑箱模型时，我们通常无法做出这些声明。深度神经网络可能具有数百、数千甚至[百万](https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33)个可训练权重，这些权重将输入预测器连接到输出预测（ResNet-50
    具有超过 2300 万个可训练参数），以及多个非线性激活函数。处理如此复杂的模型时，极其困难的是从分析角度绘制出预测器与预测之间的关系。
- en: Feature importance techniques were developed to help assuage this interpretability
    crisis. Feature importance techniques assign a score to each predictor based on
    its ability to improve predictions. This allows us to rank the predictors in our
    model based on their relative predictive power.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性技术的发展旨在帮助缓解解释性危机。特征重要性技术根据每个预测器改善预测的能力为其分配分数。这使我们能够根据预测器的相对预测能力对模型中的预测器进行排序。
- en: One method for generating these feature importance scores is by leveraging the
    power of random permutations. The next section explains how to perform permutation
    feature importance using python.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 生成这些特征重要性分数的一种方法是利用随机排列的强大功能。下一部分将解释如何使用 Python 执行排列特征重要性。
- en: Permutation Feature Importance
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 排列特征重要性
- en: The idea behind feature importance is simple. Inputs that are useful for prediction
    contain valuable information. If you destroy that information by randomly shuffling
    the feature values, the quality of your predictions should decrease. If the decrease
    in quality is small, then the information in the original predictor wasn’t very
    impactful in determining your predictions — your model is still pretty good without
    it. Furthermore, if the decrease is large, then the information in the original
    predictor had a large impact on your predictions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性的理念很简单。对预测有用的输入包含有价值的信息。如果你通过随机打乱特征值来破坏这些信息，那么预测质量应该会下降。如果质量下降很小，那么原始预测器中的信息在确定预测时并没有很大影响——你的模型即使没有这些信息也仍然很好。此外，如果质量下降很大，那么原始预测器中的信息对预测的影响很大。
- en: 'This idea is implemented in three simple steps. Say that you’ve trained an
    ML model and recorded some measure of quality for the predictions (ex. MSE, log-loss,
    etc). For each predictor in the dataset:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这个理念可以通过三个简单的步骤来实现。假设你已经训练了一个 ML 模型，并记录了预测的某些质量度量（例如 MSE、对数损失等）。对于数据集中的每个预测器：
- en: Randomly shuffle the data in the predictor while keeping the values of other
    predictors constant
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在保持其他预测器值不变的情况下，随机打乱预测器中的数据。
- en: Generate new predictions based on the shuffled values and evaluate the quality
    of your new predictions
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于打乱的值生成新的预测，并评估新预测的质量。
- en: Compute the feature importance score by calculating the decrease in the quality
    of your new predictions relative to your original predictions
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过计算新预测相对于原始预测质量的下降来计算特征重要性分数。
- en: Once you’ve computed feature importance scores for all of your features, you
    can rank them in terms of predictive usefulness. To help explain permutation feature
    importance more concretely, consider the following synthetic case study.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你计算了所有特征的特征重要性分数，你可以根据预测的有用性对它们进行排名。为了更具体地解释排列特征重要性，请考虑以下合成案例研究。
- en: 'Case Study: Predicting House Prices'
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 案例研究：预测房价
- en: '*Note: Code is included when most instructive. Follow along with the full code
    for this guide *[*here*](https://github.com/sethbilliau/FeatureImportance)*.*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：代码包括在内时最具指导性。请参考本指南的完整代码*[*这里*](https://github.com/sethbilliau/FeatureImportance)*。*'
- en: 'Suppose that the prices of 10,000 houses in [Blotchville](http://www.hcs.harvard.edu/cs50-probability/hw0705.php) are
    determined by four factors: house color, neighborhood density score, neighborhood
    crime rate score, and the neighborhood education score. Houses in Blotchville
    are either red or blue, so color is encoded as a binary indicator. The three quantitative
    scores are standardized and approximately normally distributed. The price of house *i
    c*an be determined from these factors according to the following data-generating
    equation:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在[Blotchville](http://www.hcs.harvard.edu/cs50-probability/hw0705.php)的10,000栋房子的价格由四个因素决定：房屋颜色、社区密度分数、社区犯罪率分数和社区教育分数。Blotchville的房屋要么是红色，要么是蓝色，因此颜色被编码为二进制指示符。三个定量分数已标准化并近似呈正态分布。房屋价格 *i
    c*可以根据以下数据生成方程从这些因素中确定：
- en: '![](../Images/2b8a74c54d0e62acac1872934d2494dc.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b8a74c54d0e62acac1872934d2494dc.png)'
- en: Data Generating Equation
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 数据生成方程
- en: The dataset also contains five other predictors that are uncorrelated with the
    price of houses and have no predictive power. Here’s a snapshot of the first five
    rows of the dataset, `df`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中还包含五个与房价无关且没有预测能力的预测变量。下面是数据集前五行的快照，`df`。
- en: '![](../Images/d80fb79133017e0625f51e15d8f20f41.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d80fb79133017e0625f51e15d8f20f41.png)'
- en: Snapshot of the Dataset
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集快照
- en: Say that we want to train a model to predict price from the other nine predictors.
    We could use any black box model, but for the sake of this example, let’s train
    a random forest regressor. To do this, we split our data into a train and test
    dataset. Then, we use sklearn to fit a simple random forest model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想训练一个模型，根据其他九个预测变量来预测房价。我们可以使用任何黑箱模型，但为了这个例子，我们训练一个随机森林回归模型。为此，我们将数据分为训练集和测试集。然后，我们使用sklearn拟合一个简单的随机森林模型。
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: At this point, feel free to take some time to tune the hyperparameters of your
    random forest regressor. But, since this isn’t a guide on [hyperparameter tuning](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74),
    I am going to continue with this naive random forest model — it’ll be fine for
    illustrating the usefulness of permutation feature importance.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您可以随意花些时间调整随机森林回归模型的超参数。但由于这不是关于[超参数调整](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)的指南，我将继续使用这个简单的随机森林模型——它对于说明置换特征重要性的用途足够了。
- en: One commonly-used metric to assess the quality of regression predictions is [root
    mean squared error (RMSE)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) evaluated
    onthe test set. Let’s calculate the RMSE of our model predictions and store it
    as `rmse_full_mod`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常用的度量回归预测质量的指标是 [均方根误差 (RMSE)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)，它是在测试集上评估的。让我们计算模型预测的RMSE并将其存储为 `rmse_full_mod`。
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, we can implement permutation feature importance by shuffling each predictor
    and recording the increase in RMSE. This will allow us to assess which predictors
    are useful for making predictions. Here’s the code to do this from scratch. See
    if you can match up the comments of this code to our algorithm from earlier.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过打乱每个预测变量并记录RMSE的增加来实现置换特征重要性。这将帮助我们评估哪些预测变量对于做出预测是有用的。下面是从头开始实现这一点的代码。看看你是否能将这段代码的注释与我们之前的算法对上。
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The resulting dataframe contains permutation feature importance scores. Large
    scores correspond to large increases in RMSE — evidence of worse model performance
    when a predictor was shuffled. Upon inspection of the table, we see that the four
    data-generating predictors (education, color, density, and crime) have relatively
    large values, meaning that they have predictive power in our model. On the other
    hand, the five dummy predictors have relatively small values, meaning that they
    are not as useful for making predictions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 结果数据框包含置换特征重要性分数。较大的分数对应于RMSE的显著增加——这是预测变量被打乱时模型性能变差的证据。检查表格时，我们看到四个数据生成的预测变量（教育、颜色、密度和犯罪）具有相对较大的值，这意味着它们在我们的模型中具有预测能力。另一方面，五个虚拟预测变量的值相对较小，这意味着它们在预测中不那么有用。
- en: '![](../Images/ee79363356fd5e28bdbdfb7a808183f2.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee79363356fd5e28bdbdfb7a808183f2.png)'
- en: Results of Permutation Dataframe
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 置换数据框结果
- en: We can graph our permutation feature importance scores as well for easier comparison
    using matplotlib.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用matplotlib绘制置换特征重要性分数图，以便于比较。
- en: '![](../Images/8ef8103386aea8ad2e72be9f404ee06c.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ef8103386aea8ad2e72be9f404ee06c.png)'
- en: Permutation Feature Importance Plot
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 排列特征重要性图
- en: From this analysis, we gain valuable insights into how our model makes predictions.
    We see that education score is the predictor that offers the most valuable information
    when predicting house price in our model. House color, density score, and crime
    score also appear to be important predictors. Finally, it appears that the five
    dummy predictors do not have very much predictive power. In fact, since dropping
    dummy predictor 3 actually led to a decrease in RMSE, we might consider performing
    feature selection and removing these unimportant predictors in future analysis.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从这项分析中，我们获得了关于模型如何进行预测的宝贵见解。我们看到教育得分是预测房价时提供最有价值信息的预测变量。房屋颜色、密度得分和犯罪得分似乎也是重要的预测变量。最后，似乎这五个虚拟预测变量的预测能力并不强。事实上，由于删除虚拟预测变量3实际上导致了RMSE的下降，我们可能考虑在未来的分析中进行特征选择，移除这些不重要的预测变量。
- en: Drawbacks to Permutation Feature Importance
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 排列特征重要性的缺点
- en: '![](../Images/176fcc49ddc9f174be73b548a949f6a7.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/176fcc49ddc9f174be73b548a949f6a7.png)'
- en: Photo by [Martin Esteve](https://unsplash.com/@tme18) on [Upsplash](https://unsplash.com/photos/4y8A6Ve-3GE)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[马丁·埃斯特维](https://unsplash.com/@tme18)拍摄，发布在[Upsplash](https://unsplash.com/photos/4y8A6Ve-3GE)
- en: 'While we’ve seen the many benefits of permutation feature importance, it’s
    equally important to acknowledge its drawbacks (no pun intended). Here are a few
    disadvantages of using permutation feature importance:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经看到排列特征重要性的诸多好处，但同样重要的是承认其缺点（不是双关）。以下是使用排列特征重要性的一些缺点：
- en: '**Computational Time: **This process can be computationally expensive since
    it requires you to iterate through each of your predictors and make predictions.
    If it’s not cheap to make predictions or if you have many, many predictors, this
    could be costly.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算时间：** 这个过程可能计算开销较大，因为它需要你对每个预测变量进行迭代并进行预测。如果预测成本不低或预测变量非常多，这可能会非常昂贵。'
- en: '**Poor Performance in the Presence of Multicollinearity: **Permutation feature
    importance can perform poorly if your dataset has correlated features. If the
    information in one predictor is also stored in a correlated predictor, then the
    model may still perform well when one of those predictors is shuffled.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在多重共线性存在时表现不佳：** 如果数据集中有相关特征，排列特征重要性可能表现不佳。如果一个预测变量的信息也存在于一个相关的预测变量中，那么当这些预测变量之一被打乱时，模型仍然可能表现良好。'
- en: '**Scores are relative, not absolute: **Permutation importance scores show the *relative* predictive
    power of features in a model. However, these scores don’t really have any meaningful
    value out of context — any score could be really good or really bad depending
    on the other scores.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**得分是相对的，而非绝对的：** 排列重要性得分显示了模型中特征的*相对*预测能力。然而，这些得分在没有上下文的情况下实际上没有任何有意义的价值——任何得分都可能因其他得分的不同而显得好或坏。'
- en: '**Feature importance still isn’t statistical inference:** Feature importance
    techniques can only tell you how useful a predictor is — they don’t give any insight
    into the nature of the relationship (ex. linear, quadratic, etc.) or the magnitude
    of the predictor’s effect. Permutation feature importance is not a replacement
    for statistical inference, but rather an alternative solution for when it’s impossible
    to perform traditional inference.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征重要性仍不是统计推断：** 特征重要性技术只能告诉你一个预测变量有多有用——它们不能提供关于关系的性质（例如线性、二次等）或预测变量效应的大小的任何见解。排列特征重要性不是统计推断的替代方案，而是当无法进行传统推断时的替代解决方案。'
- en: Conclusion
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: Permutation feature importance is a valuable tool to have in your toolbox for
    analyzing black box models and providing ML interpretability. With these tools,
    we can better understand the relationships between our predictors and our predictions
    and even perform more principled feature selection.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 排列特征重要性是分析黑箱模型和提供机器学习可解释性的有价值工具。通过这些工具，我们可以更好地理解预测变量与预测结果之间的关系，甚至可以进行更有原则的特征选择。
- en: Though we implemented permutation feature importance from scratch, there are
    several packages that offer sophisticated implementations of permutation feature
    importance along with other model-agnostic methods. Python users should look into
    the `eli5`, `alibi`, `scikit-learn`, `LIME`, and `rfpimp` packages while R users
    turn to `iml`, `DALEX`, and `vip`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们是从零开始实现了排列特征重要性，但也有多个包提供了复杂的排列特征重要性实现及其他与模型无关的方法。Python 用户应查看 `eli5`、`alibi`、`scikit-learn`、`LIME`
    和 `rfpimp` 包，而 R 用户则应转向 `iml`、`DALEX` 和 `vip`。
- en: Happy permuting! If you have any questions, feel free to leave a comment, and
    I’ll do my best to provide an answer.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 祝你在排列中愉快！如果你有任何问题，随时留言，我会尽力提供答案。
- en: '*Acknowledgments: A big thank you to the wonderful Claire Hoffman for proofreading
    and editing this article and putting up with my neglect of the Oxford comma. I’m
    also grateful to Leo Saenger for reading the article and providing his suggestions.*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*鸣谢：非常感谢出色的 Claire Hoffman 校对和编辑这篇文章，并忍受我对牛津逗号的忽视。我也感谢 Leo Saenger 阅读这篇文章并提供建议。*'
- en: '**Bio: [Seth Billiau](https://www.linkedin.com/in/seth-billiau-b4b062136/)**
    is a Data Scientist and Statistician, A.B. in Statistics at Harvard University,
    Former Vice President of the Harvard College Open Data Project. Email: sethbilliau
    [at] college.harvard.edu'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介： [Seth Billiau](https://www.linkedin.com/in/seth-billiau-b4b062136/)**
    是数据科学家和统计学家，哈佛大学统计学 A.B. 学位，前哈佛大学开放数据项目副总裁。电子邮件：sethbilliau [at] college.harvard.edu'
- en: '[Original](https://towardsdatascience.com/from-scratch-permutation-feature-importance-for-ml-interpretability-b60f7d5d1fe9).
    Reposted with permission.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/from-scratch-permutation-feature-importance-for-ml-interpretability-b60f7d5d1fe9)。转载已获许可。'
- en: '**Related:**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[This Data Visualization is the First Step for Effective Feature Selection](/2021/06/data-visualization-feature-selection.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[这种数据可视化是有效特征选择的第一步](/2021/06/data-visualization-feature-selection.html)'
- en: '[Feature Selection – All You Ever Wanted To Know](/2021/06/feature-selection-overview.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[特征选择 – 你想知道的一切](/2021/06/feature-selection-overview.html)'
- en: '[Dashboards for Interpreting & Comparing Machine Learning Models](/2021/06/dashboards-interpreting-comparing-machine-learning-models.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用于解释和比较机器学习模型的仪表盘](/2021/06/dashboards-interpreting-comparing-machine-learning-models.html)'
- en: '* * *'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织在 IT 领域'
- en: '* * *'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[The Importance of Permutation in Neural Network Predictions](https://www.kdnuggets.com/2022/12/importance-permutation-neural-network-predictions.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[排列在神经网络预测中的重要性](https://www.kdnuggets.com/2022/12/importance-permutation-neural-network-predictions.html)'
- en: '[Simplifying Decision Tree Interpretability with Python & Scikit-learn](https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 和 Scikit-learn 简化决策树可解释性](https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html)'
- en: '[Using SHAP Values for Model Interpretability in Machine Learning](https://www.kdnuggets.com/2023/08/shap-values-model-interpretability-machine-learning.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 SHAP 值进行机器学习模型可解释性分析](https://www.kdnuggets.com/2023/08/shap-values-model-interpretability-machine-learning.html)'
- en: '[Feature Store Summit 2022: A free conference on Feature Engineering](https://www.kdnuggets.com/2022/10/hopsworks-feature-store-summit-2022-free-conference-feature-engineering.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[特征存储峰会 2022：关于特征工程的免费会议](https://www.kdnuggets.com/2022/10/hopsworks-feature-store-summit-2022-free-conference-feature-engineering.html)'
- en: '[Celebrating Awareness of the Importance of Data Privacy](https://www.kdnuggets.com/2022/01/celebrating-awareness-importance-data-privacy.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[庆祝对数据隐私重要性的认知](https://www.kdnuggets.com/2022/01/celebrating-awareness-importance-data-privacy.html)'
- en: '[The Importance of Experiment Design in Data Science](https://www.kdnuggets.com/2022/08/importance-experiment-design-data-science.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中实验设计的重要性](https://www.kdnuggets.com/2022/08/importance-experiment-design-data-science.html)'
