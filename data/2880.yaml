- en: Applying Data Science to Cybersecurity Network Attacks & Events
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据科学应用于网络安全网络攻击与事件
- en: 原文：[https://www.kdnuggets.com/2019/09/applying-data-science-cybersecurity-network-attacks-events.html](https://www.kdnuggets.com/2019/09/applying-data-science-cybersecurity-network-attacks-events.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/09/applying-data-science-cybersecurity-network-attacks-events.html](https://www.kdnuggets.com/2019/09/applying-data-science-cybersecurity-network-attacks-events.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Aakash Sharma](https://www.linkedin.com/in/aakashsharma21/), Data Scientist**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Aakash Sharma](https://www.linkedin.com/in/aakashsharma21/)，数据科学家**'
- en: The cyber world is such a vast concept to comprehend. At the time, I decided
    I wanted to get into cybersecurity during my undergrad in college. What intrigued
    me was understanding the concepts of malware, network security, penetration testing
    & the encryption aspect that really plays a role in what cybersecurity really
    is.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 网络世界是一个如此庞大的概念。在大学本科期间，我决定进入网络安全领域。让我感兴趣的是理解恶意软件、网络安全、渗透测试和加密方面的概念，这些概念在网络安全中扮演了重要角色。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析水平'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的IT工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Being able to protect the infrastructure is important but interesting nonetheless.
    Sure there is coding but I never really learned how we can implement code into
    cybersecurity principles. This is what I really want to know next that could stretch
    my knowledge in information technology & computer science. I learned more about
    coding especially in Python. I dabbled a little in Scala & I already had a good
    foundation of Sequel & Java applications during my undergrad that learning it
    during my boot camp allowed me to feel a little more comfortable with it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 能够保护基础设施固然重要，但仍然颇具趣味。虽然涉及到编程，但我从未真正了解如何将代码应用于网络安全原则。这正是我接下来想要深入了解的内容，它可以扩展我在信息技术与计算机科学方面的知识。我学到了更多关于编程的知识，特别是Python。我稍微接触过Scala，并且在本科期间对Sequel和Java应用有了很好的基础，这使得在训练营期间学习这些内容时，我感到更加舒适。
- en: The data science immersive program taught me how to gather data through Sequel,
    JSON, HTML or web-scrapping applications where I went into cleaning the data &
    then applying Python related code for statistical analysis. I then as able to
    model the data to either find trends, make predictions, or provide suggestions/recommendations.
    I wanted to apply this to my background in Cisco NetaCad, cybersecurity principles
    & software development
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学沉浸式项目教会了我如何通过Sequel、JSON、HTML或网页抓取应用程序收集数据，然后进行数据清理，并应用与Python相关的代码进行统计分析。接着，我能够对数据建模，以发现趋势、做出预测或提供建议/推荐。我希望将这些应用到我在Cisco
    NetaCad、网络安全原则及软件开发方面的背景中。
- en: '![Figure](../Images/b1cbeb14e9d610cf85aa10111f51dd87.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/b1cbeb14e9d610cf85aa10111f51dd87.png)'
- en: FCC Logo
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: FCC 标志
- en: I then wanted to relate this to my cybersecurity background. I decided to gather
    data from Data.org regarding a ton of cyber attacks regarding the Federal Communications
    Commission (FCC). In this blog post, I decided to write about how I was able to
    connect my data science knowledge to my cybersecurity background in real world
    industries. I will provide some background of the project, a code along & some
    insight into what the FCC could do to better understand the data as I did. This
    could be useful for future situations or other government related cybersecurity
    projects.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我想将其与我的网络安全背景联系起来。我决定从Data.org收集有关联邦通信委员会（FCC）大量网络攻击的数据。在这篇博客文章中，我决定写下我如何将数据科学知识与网络安全背景在现实世界行业中相结合。我将提供项目背景、代码示例及一些对FCC如何更好地理解数据的见解。这可能对未来的情况或其他政府相关的网络安全项目有用。
- en: So the FCC, to the best of my knowledge, follows the CSRIC Best Practices Search
    Tool which allows you to search CSRIC’s collection of Best Practices using a variety
    of criteria including Network Type, Industry Role, Keywords, Priority Levels &
    BP Number.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 据我了解，FCC 遵循 CSRIC 最佳实践搜索工具，该工具允许你使用各种标准（包括网络类型、行业角色、关键词、优先级水平和最佳实践编号）搜索 CSRIC
    的最佳实践集合。
- en: The Communications Security, Reliability & Interoperability Council’s (CSRIC)
    mission is to provide recommendations to the FCC to ensure, among other things,
    optimal security & reliability of communications systems, including telecommunications,
    media & public safety.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通信安全、可靠性和互操作性委员会（CSRIC）的使命是向 FCC 提供建议，以确保包括电信、媒体和公共安全在内的通信系统的最佳安全性和可靠性等。
- en: 'CSRIC’s members focus on a range of public safety & homeland security-related
    communications matters, including: (1) the reliability & security of communications
    systems & infrastructure, particularly mobile systems; (2) 911, Enhanced 911 (E911),
    & Next Generation 911 (NG911); & (3) emergency alerting.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: CSRIC 的成员专注于一系列公共安全和国土安全相关的通信事务，包括：（1）通信系统和基础设施的可靠性和安全性，特别是移动系统；（2）911、增强型 911
    (E911) 和下一代 911 (NG911)；（3）紧急警报。
- en: The CSRIC’s recommendations will address the prevention & remediation of detrimental
    cyber events, the development of best practices to improve overall communications
    reliability, the availability & performance of communications services & emergency
    alerting during natural disasters, terrorist attacks, cyber security attacks or
    other events that result in exceptional strain on the communications infrastructure,
    the rapid restoration of communications services in the event of widespread or
    major disruptions & the steps communications providers can take to help secure
    end-users & servers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: CSRIC 的建议将涉及防止和修复有害的网络事件，制定最佳实践以提高整体通信可靠性，提升通信服务的可用性和性能，确保在自然灾害、恐怖袭击、网络安全攻击或其他对通信基础设施造成异常压力的事件中进行紧急警报，在广泛或重大中断事件中快速恢复通信服务，以及通信服务提供商可以采取的措施来帮助保护终端用户和服务器。
- en: The first step for me in tackling this project was understanding what I needed
    to accomplish & what kind of direction this project had me taking. I had remembered
    that I needed to provide recommendations to the FCC to ensure optimal security
    & reliability of communication systems in telecommunications, media & public safety.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，处理这个项目的第一步是理解我需要完成的任务和这个项目的方向。我记得我需要向 FCC 提供建议，以确保电信、媒体和公共安全系统的最佳安全性和可靠性。
- en: There are different approaches I had taken into account when attempting this
    project. The 1st is diving straight in in order to better understand the data
    itself where I focused on just the priority of the event & applying a ton of machine
    learning classification models to it. The 2nd approach was being able to apply
    natural language processing techniques on the description of the event & see how
    that correlates to the priority of the event.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试这个项目时，我考虑了不同的方法。第一种方法是直接深入以更好地理解数据本身，我专注于事件的优先级，并对其应用了大量的机器学习分类模型。第二种方法是能够对事件描述应用自然语言处理技术，并查看它与事件优先级的相关性。
- en: With this we can make predictions & following that, recommendations to better
    prevent, understand or control the event. My idea is if we can focus on the more
    critical events by fixing the less complex ones, we can save enough assets to
    further improve the systems to combat the more complex ones.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们可以做出预测，并在此基础上提出建议，以更好地预防、理解或控制事件。我的想法是，如果我们能通过解决较少复杂的事件来集中精力处理更关键的事件，我们可以节省足够的资源来进一步改善系统，以应对更复杂的事件。
- en: 'What is needed:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 需要的东西：
- en: A Python IDE
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 Python IDE
- en: Machine Learning & Statistical Packages
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习和统计包
- en: Strong knowledge in Data Science Concepts
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扎实的数据科学概念知识
- en: Some knowledge in cybersecurity & network related concepts
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对网络安全和网络相关概念有一定的了解
- en: Let’s begin! Let’s first start by importing any packages we intend on using,
    I usually copy & paste a list of useful modules that has helped me with previous
    data science projects.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！首先导入我们打算使用的任何包，我通常会复制和粘贴一个对我之前的数据科学项目有帮助的有用模块列表。
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We then need to load in the data & view it by:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要加载数据并通过以下方式查看它：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we have our data we can explore, clean & understand the data. Below,
    I have provided a function for basic data exploratory analysis. We want to do
    this to fully understand the data we’re dealing with & what the overall goal needs
    to be met.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以探索、清理和理解我们的数据。下面，我提供了一个用于基本数据探索分析的函数。我们这样做是为了充分理解我们处理的数据以及需要达成的整体目标。
- en: The following function will allow us to view any null values, replace any blank
    spaces with an underscore, reformat the data frame index, see the data types for
    each column, display any duplicated data, describes the statistical analysis of
    the data & checks the shape.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数将允许我们查看任何空值，用下划线替换任何空白，重新格式化数据框索引，查看每列的数据类型，显示任何重复的数据，描述数据的统计分析，并检查数据的形状。
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Based off of the data we say that this was a unbalanced classification problem!
    What we need to do next is eliminate any “NaN” or null values & somewhat balance
    our class. Based off of our exploratory data analysis, we can see that most of
    the data has “NaN” values in the object type columns. We can fix this with the
    following function below!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据，我们可以说这是一个不平衡的分类问题！接下来我们需要做的是消除任何“NaN”或空值，并在一定程度上平衡我们的类别。根据我们的探索性数据分析，我们可以看到大多数数据在对象类型的列中有“NaN”值。我们可以通过下面的函数来解决这个问题！
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Figure](../Images/00875b80a3ceb8555178033818446813.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/00875b80a3ceb8555178033818446813.png)'
- en: Checking the amount of values in the Priorities column
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 检查优先级列中的值数量
- en: Looking at the priority column(s), we have a object related column that ranks
    the severity of the event to important, highly important & critical. Another column
    that corresponds to that priority column ranks them 1–3, with 1 being important,
    2 as highly important & 3 being critical. Based off of the variety of priorities,
    we see that the data is unbalanced. We fix this by renaming our column for better
    understand & then balancing it where we focus on the highly important & critical
    events.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 查看优先级列，我们有一个与事件严重性相关的对象列，将事件分类为重要、高度重要和关键。另一个对应的列将这些优先级分为1-3，其中1为重要，2为高度重要，3为关键。根据优先级的多样性，我们发现数据是不平衡的。我们通过重命名列以更好地理解，然后平衡数据，重点关注高度重要和关键事件来解决这个问题。
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Figure](../Images/bbc7ce6635439f561897a56d1e140338.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/bbc7ce6635439f561897a56d1e140338.png)'
- en: Result of the above code after we’ve balanced the class
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们平衡了类别后，上述代码的结果
- en: '**First Approach (Understanding the Data)**'
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**第一种方法（理解数据）**'
- en: In this next section I will discuss my initial approach in understanding the
    priorities column. I learned fast that this approach was not the best but was
    very informative in where I should look elsewhere when looking at the priorities
    of the attack in making recommendations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我将讨论我在理解优先级列时的初步方法。我很快发现这种方法不是最佳的，但在查看攻击优先级并提出建议时，它提供了关于我应该去哪里寻找的信息。
- en: My next step was understanding which columns correlated best for patterns &
    trends with my priorities column. It seemed that all columns that worked well
    were binary! The description column were text related & machines don’t like handling
    text objects. With the below code, we can see which columns are the most positively
    correlated & most negatively correlated with our predictor column.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我的下一步是理解哪些列与我的优先级列的模式和趋势相关性最好。看起来所有有效的列都是二元的！描述列与文本相关，而机器不喜欢处理文本对象。通过下面的`代码`，我们可以查看哪些列与我们的预测变量列相关性最强，以及哪些列相关性最弱。
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Figure](../Images/63555a72fc10d4e0f147599a5698eda0.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/63555a72fc10d4e0f147599a5698eda0.png)'
- en: Most Negatively Correlated![Figure](../Images/87dc709d7e3fba09a31ae8e2e6793512.png)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性最弱的列![图示](../Images/87dc709d7e3fba09a31ae8e2e6793512.png)
- en: Most Positively Correlated
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性最强的列
- en: Now we can start making our models to see what we’ve done will give us accurate
    predictions or enough information for recommendations. Let’s first start with
    a train-test split with these correlated columns serving as our features & our
    priorities column serving as our predictor variable.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始建立模型，看看我们做的工作是否能提供准确的预测或足够的建议信息。首先，我们从训练-测试拆分开始，使用这些相关列作为特征，将优先级列作为预测变量。
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that we’ve train-test split our features, let’s apply grid search to find
    the best parameters or features for full accuracy on a **Naive Bayes Classifier,
    Random Forest Classifier, Adaboost/Gradient Boost Classifier** & a **Keras Neural
    Network**! But what do these classifier models even mean?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对特征进行了训练-测试分割，让我们应用网格搜索来找到 **朴素贝叶斯分类器、随机森林分类器、AdaBoost/梯度增强分类器** 和 **Keras
    神经网络** 的最佳参数或特征，以实现完整的准确性！但这些分类器模型到底是什么意思呢？
- en: In simple terms, a **Naive Bayes** classifier assumes that the presence of a
    particular feature in a class is unrelated to the presence of any other feature.
    Let’s see it on our data!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，**朴素贝叶斯** 分类器假设类中某一特征的存在与任何其他特征的存在无关。让我们在我们的数据上看看！
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Random Forest Classifier** creates a set of decision trees from a randomly
    selected subset of the training set, which then aggregates the votes from different
    decision trees to decide the final class of the test object. Each individual tree
    in the random forest spits out a class prediction and the class with the most
    votes becomes our model’s prediction. Let’s model it!'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林分类器** 从训练集的随机选择子集中创建一组决策树，然后聚合不同决策树的投票以决定测试对象的最终类别。随机森林中的每棵树都会给出一个类别预测，获得最多投票的类别成为我们模型的预测。让我们来建模吧！'
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Figure](../Images/091f19d0b69e88e11a54841c6c23f8b7.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/091f19d0b69e88e11a54841c6c23f8b7.png)'
- en: Graph of our Adaboost Model, it’s overfit!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 AdaBoost 模型图，它已经过拟合了！
- en: '**AdaBoost** is short for Adaptive Boosting. It is basically a machine learning
    algorithm that is used as a classifier. Whenever you have a large amount of data
    and you want divide it into different categories, we need a good classification
    algorithm to do it. Hence the word ‘boosting’, as in it boosts other algorithms!'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**AdaBoost** 是自适应增强的简称。它基本上是一种用于分类器的机器学习算法。每当你有大量数据并且希望将其划分为不同的类别时，我们需要一个好的分类算法来完成这个任务。因此，‘增强’这个词的意义在于它能提升其他算法的性能！'
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Neural networks** are a set of algorithms, modeled loosely after the human
    brain, that are designed to recognize patterns. They interpret sensory data through
    a kind of machine perception, labeling or clustering raw input. We can even apply
    regularization to combat the **over fitting** issue!'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经网络** 是一组算法，松散地模仿人脑，旨在识别模式。它们通过一种机器感知方式来解读感官数据，对原始输入进行标记或聚类。我们甚至可以应用正则化来应对
    **过拟合** 问题！'
- en: '![Figure](../Images/1d09ab2060c853b6e6407693f94189ed.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/1d09ab2060c853b6e6407693f94189ed.png)'
- en: Our regularized Neural Network, it’s overfit!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的正则化神经网络，已经过拟合了！
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![Figure](../Images/113883da063f2e405f4c0c7d3b679bb6.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/113883da063f2e405f4c0c7d3b679bb6.png)'
- en: The Accuracy of Our Models
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的准确性
- en: What is this telling us?! Based off of the training score & test score we can
    see that our model is **over fit** & is not great at making predictions or analyzing
    trends. What does it mean that our model is **over fit**? We have **high variance** & **low
    bias**! **High variance** can cause an algorithm to model the random noise in
    the training data, rather than the intended outputs. We still can’t make recommendations
    off of this since it really doesn’t give us much information!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们什么？！根据训练分数和测试分数，我们可以看到我们的模型 **过拟合** 了，并且在做出预测或分析趋势方面表现不佳。模型 **过拟合** 意味着什么？我们有
    **高方差** 和 **低偏差**！**高方差** 可能导致算法对训练数据中的随机噪声建模，而不是预期的输出。我们仍然不能基于此做出推荐，因为它确实没有提供太多信息！
- en: Second Approach (Natural Language Processing) — The Best Route
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第二种方法（自然语言处理）——最佳路径
- en: My 2nd approach was focusing on the description column. After my 1st approach,
    I wanted to see how the priority of the attack correlated with what was given
    in the description. The description column gave us a short explanation of what
    happened & a suggested FCC compliant resolution in what they might do in stopping
    that similar event.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我的第二种方法是集中于描述列。在第一种方法之后，我想查看攻击的优先级与描述中给出的内容之间的相关性。描述列给出了发生了什么的简短解释，并建议了一种符合
    FCC 标准的解决方案，用于阻止类似事件的发生。
- en: In order to better understand the description column, I needed to apply natural
    language processing (NLP) since computers & statistical models dislike handling
    text & words. But we can get around this! My approach is similar when cleaning
    the data & balancing the priorities column, however I have applied some NLP concepts
    to better understand the description, analyze it, make recommendations & even
    predict what the next event would be based off of the occurrence of words specific
    to the events.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解描述列，我需要应用自然语言处理（NLP），因为计算机和统计模型不擅长处理文本和单词。但我们可以解决这个问题！我的方法类似于清洗数据和调整优先级列，但我应用了一些NLP概念来更好地理解描述、分析它、提出建议，并基于特定于事件的单词出现情况预测下一个事件。
- en: 'Some of the concepts include:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一些概念包括：
- en: '**Pre-processing **is the technique of converting raw data into a clean data
    set.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理** 是将原始数据转换为干净数据集的技术。'
- en: '**Regex, **regular expressionis a string of text that allows you to create
    patterns that help match, locate & manage text. Another way to clean the text.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则表达式** 是一串文本，允许你创建模式来帮助匹配、定位和管理文本。另一种清理文本的方法。'
- en: '**Lemmatizing** is the process of grouping together the inflected forms of
    a word so they can be analyzed as a single term.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词形还原** 是将一个词的词形变化形式归为一类，以便作为一个单一术语进行分析的过程。'
- en: '**Stemming** is the process of reducing inflected words to their stem, base
    or root form.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词干提取** 是将词形变化的单词还原为其词干、基础形式或根形式的过程。'
- en: '**Countvectorize **counts the word frequencies.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词频向量化** 计算单词的频率。'
- en: '**TFIDFVectorizer** is the value of a word increases proportionally to count,
    but is offset by the frequency of the word in the corpus.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TFIDFVectorizer** 是一种随着单词计数增加而增加其值，但会受到语料库中该单词频率影响的机制。'
- en: Let’s start off by applying regex concepts to our already cleaned data. We also
    want to strip or clean out common words that are useful but are sporadic in every
    single description.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从将正则表达式概念应用于已经清洗过的数据开始。我们还希望剔除或清理掉那些有用但在每个描述中都很零散的常见词汇。
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now that we’ve cleaned our data, we should apply some pre-processing techniques
    to better understand the words given to use in every description of the event.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经清洗了数据，我们应该应用一些预处理技术，以更好地理解每个事件描述中给出的词汇。
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We then want to apply countvectorize on our stemmed, lemmatized & tokenized
    description words in order to control common stop words in the English language,
    we can do this with the following code. We then can see what the most common words
    are in the entire data set.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们希望对词干提取、词形还原和标记化后的描述词应用词频向量化，以控制英语语言中的常见停用词，我们可以使用以下代码来实现。然后我们可以查看整个数据集中最常见的词汇。
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![Figure](../Images/f5c1a6526d8e53db4a11ca3ad0dc150a.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/f5c1a6526d8e53db4a11ca3ad0dc150a.png)'
- en: Top Word Count in the Entire Data Set
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 整个数据集中的词频统计
- en: We can see that most of the words that popped up are network or security related.
    We can use this information to better understand the scope of what these events
    are! Are the network attacks? Are they network warehouse related? Etc.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，大多数出现的单词与网络或安全相关。我们可以利用这些信息更好地理解这些事件的范围！这些是网络攻击吗？还是与网络仓库相关？等等。
- en: But what if we wanted more information? We can group the descriptions based
    on the urgency or severity of the importance of the event. Maybe it’s nothing
    serious so it’s ranked a 0 (Not Important) or really bad ranked as 1 (Really Important).
    We can do this with the below code based on our pre-processed columns. We can
    then visualize what the most common really important words are.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们想要更多的信息呢？我们可以根据事件的重要性或紧急程度对描述进行分组。也许它并不严重，所以它被标记为0（不重要），或者真的很糟糕，被标记为1（非常重要）。我们可以使用下面的代码来完成这项工作，基于我们预处理过的列。然后我们可以可视化最常见的非常重要的词汇。
- en: '[PRE14]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![Figure](../Images/a5b0249a84ab149056d3d6a93b280bc3.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/a5b0249a84ab149056d3d6a93b280bc3.png)'
- en: Top Word Count of the Really IMPORTANT words
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 重要词汇的词频统计
- en: Finally, we can start modeling regression & classification metrics on the tokenized
    data. Let’s start by applying a logistic regression model through a pipeline where
    can apply a grid search tool in order to tune our BEST features or BEST parameters.
    Let’s establish our X variable, our features! We will use the words or features
    within the tokenized column within the processed data frame we created above.
    The processed data frame is an entirely NEW data frame that contains our tokenized,
    stemmed & lemmatized columns.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以开始在标记化数据上建模回归和分类指标。让我们通过一个管道应用逻辑回归模型，在那里我们可以应用网格搜索工具来调整我们的**最佳特征**或**最佳参数**。让我们确定我们的X变量，即特征！我们将使用标记化列中的单词或特征，这些列在我们上面创建的处理数据框中。处理数据框是一个完全新的数据框，包含我们的标记化、词干化和词形还原列。
- en: Here I decided to focus on the tokenized columns because this specific column
    functioned the best for parameter tuning & accuracy. To cut on time length for
    this blog post I decided to focus on the tokenized, what works best, as well!
    Let’s train-test split it as well.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我决定关注标记化的列，因为这个特定的列在参数调整和准确性上效果最好。为了节省这篇博客文章的时间长度，我决定也专注于标记化的效果最好的部分！让我们也进行训练-测试分割。
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now let’s created our **pipeline** that uses grid search concepts for the best
    hyper parameters. Once the grid search has fit (this can take awhile!) we can
    pull out a variety of information and useful objects from the grid search object.
    Often, we’ll want to apply several transformers to a data set & then finally build
    a model. If you do all of these steps independently, your code when predicting
    on test data can be messy. It’ll also be prone to errors. Luckily, we’ll have **pipelines**!
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个使用网格搜索概念来寻找最佳超参数的**管道**。一旦网格搜索完成（这可能需要一段时间！），我们可以从网格搜索对象中提取各种信息和有用对象。通常，我们会希望对数据集应用多个转换器，然后最终构建一个模型。如果你独立执行所有这些步骤，那么在测试数据上进行预测时，代码可能会很混乱，也容易出错。幸运的是，我们有**管道**！
- en: Here we will be applying a logistic model which can take into consideration
    LASSO & Ridge penalties.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将应用一个逻辑模型，它可以考虑LASSO和Ridge惩罚。
- en: '**You should:**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**你应该：**'
- en: Fit and validate the accuracy of a default logistic regression on the data.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据上的默认逻辑回归进行拟合和验证其准确性。
- en: Perform a gridsearch over different regularization strengths, Lasso & Ridge
    penalties.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在不同的正则化强度、Lasso和Ridge惩罚上执行网格搜索。
- en: Compare the accuracy on the test set of your optimized logistic regression to
    the baseline accuracy & the default model.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将优化后的逻辑回归模型在测试集上的准确性与基线准确性及默认模型进行比较。
- en: Look at the best parameters found. What was chosen? What does this suggest about
    our data?
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看找到的最佳参数。选择了什么？这对我们的数据有什么暗示？
- en: Look at the (non-zero, if Lasso was selected as best) coefficients & associated
    predictors for your optimized model. What appears to be the most important predictors?
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看优化模型的（非零，如果Lasso被选为最佳的）系数和相关预测变量。哪些预测变量似乎是最重要的？
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now we can apply the pipeline on our logistic regression model within a grid
    search object. Notice the countvectorize model being instantiated. We did this
    because we want see how that factors into our accuracy & importance of the words
    we associated with our network attacks.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在逻辑回归模型中应用管道，通过网格搜索对象。注意到countvectorize模型正在实例化。我们这样做是因为我们希望查看它对我们准确性和与网络攻击相关的单词的重要性的影响。
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Figure](../Images/decbbb88ade18117d56b64ad0fc88c23.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/decbbb88ade18117d56b64ad0fc88c23.png)'
- en: Our Improved Accuracy of our Logistic Model
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们逻辑模型的改进准确性
- en: So what can infer from this? Well it looks like we’ve seen a massive increase
    in model accuracy for a training data as well as 10% increase in accuracy for
    our testing data. However, the model is still **over fit**! But still great job!
    What were our best parameters? We could use this information if we want to tune
    future logistic models! The below code will show us that!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们能从中推断出什么呢？看起来我们在训练数据上观察到了模型准确性的显著提升，并且在测试数据上的准确性提高了10%。然而，模型仍然**过拟合**！但依然做得很棒！我们的最佳参数是什么？如果我们想调整未来的逻辑模型，可以利用这些信息！下面的代码将展示这一点！
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Figure](../Images/d40ca1a01a557a1969435a75fbb8c5a8.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/d40ca1a01a557a1969435a75fbb8c5a8.png)'
- en: Our BEST Hyper Parameters from our Logistic Gridsearch within the Pipeline
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在管道内逻辑网格搜索中的**最佳超参数**
- en: A **regression** model that uses **L1** regularization technique is called Lasso **Regression** and
    model which uses **L2** is called Ridge **Regression**. From our best hyper parameters,
    our models favors a Ridge Regression technique. Now we want to make predictions
    off of our logistic regression & be able to make suggestions. How do we go upon
    doing that? Let’s look at the coefficients associated with our features that will
    predict the outcomes of the best y variable.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一个使用**L1**正则化技术的**回归**模型称为Lasso**回归**，使用**L2**正则化技术的模型称为Ridge**回归**。根据我们最佳的超参数，我们的模型更倾向于使用Ridge回归技术。现在我们想要基于逻辑回归进行预测并提出建议。我们如何去做呢？让我们来看看与我们的特征相关的系数，这些特征将预测最佳y变量的结果。
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Figure](../Images/dc670ed8a1c26c818a2d5dc00edfceec.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/dc670ed8a1c26c818a2d5dc00edfceec.png)'
- en: Predictions for Suggestions Based off Occurrence & Importance of Words
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 基于词汇出现频率和重要性的建议预测
- en: Recommendations & Conclusions
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推荐与结论
- en: Now that I’ve finished modeling & analyzing the data, I can now make suggestions
    to the FCC as well as any other data scientist or data analyst that plans on doing
    a project similar to mine.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我已经完成了建模和数据分析，我可以向FCC以及任何计划进行类似项目的数据科学家或数据分析师提出建议。
- en: For future data scientists doing a similar project. Get more data, a better
    sample of data, increase/decrease complexity of model & **regularize**. This can
    help combat the issue of **over fitting** your data as I experienced through this
    project. Understand unbalanced classification problems. This can lead to your
    main direction in solving the problem.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对于未来进行类似项目的数据科学家，获取更多数据，使用更好的数据样本，增加/减少模型复杂度并**正则化**。这可以帮助应对**过拟合**数据的问题，正如我在这个项目中所经历的。了解不平衡分类问题，这可以成为解决问题的主要方向。
- en: For the FCC’s CSRIC’s best practices, my best recommendation would be to fix
    the simple issues first so they don’t happen to often & soak up your resources.
    This can allow them to focus on the more important & complex events or attacks.
    Based off what I could predict & analyzing the given data.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于FCC的CSRIC最佳实践，我的最佳建议是首先解决简单问题，以免它们频繁发生并耗尽你的资源。这可以让他们专注于更重要和复杂的事件或攻击。基于我所能预测和分析的数据。
- en: '![](../Images/01f7dec0f4c03bc987d72b6290a162a2.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01f7dec0f4c03bc987d72b6290a162a2.png)'
- en: 'Simple problems:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 简单问题：
- en: Different Cables
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的电缆
- en: Color Code Cables
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 颜色编码电缆
- en: Better ventilation of warehouse
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仓库的更好通风
- en: Increase power capacity
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加电力容量
- en: Better hardware
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的硬件
- en: Spacing of antennas
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天线间距
- en: '![](../Images/b66fa8ec368b2c891e0a298da5eb282c.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b66fa8ec368b2c891e0a298da5eb282c.png)'
- en: 'Moderate problems:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 中等问题：
- en: Utilize Network Surveillance
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用网络监控
- en: Provide secure electrical software where feasible
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供安全的电气软件（如有可能）
- en: Find thresholds for new hardware & software
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找新硬件和软件的阈值
- en: Virus protection
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 病毒保护
- en: '![](../Images/ce63df369ea4719967572ba1ca60e2d1.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce63df369ea4719967572ba1ca60e2d1.png)'
- en: 'Complex:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂：
- en: Minimize single points of failure & software faults
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化单点故障和软件缺陷
- en: Device Management Architecture
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设备管理架构
- en: Secure networks/encrypted systems
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全网络/加密系统
- en: '**Bio: [Aakash Sharma](https://www.linkedin.com/in/aakashsharma21/)** is a
    Data Scientist, Data Analyst, Cybersecurity Network Engineer, Full Stack Software
    Developer, and Machine Learning & A.I. Enthusiast.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介：[Aakash Sharma](https://www.linkedin.com/in/aakashsharma21/)** 是一名数据科学家、数据分析师、网络安全工程师、全栈软件开发人员以及机器学习与人工智能爱好者。'
- en: '[Original](https://towardsdatascience.com/reddit-post-classification-b70258d6affe).
    Reposted with permission.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/reddit-post-classification-b70258d6affe)。经许可转载。'
- en: '**Related:**'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Machine Learning Security](/2019/01/machine-learning-security.html)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习安全](/2019/01/machine-learning-security.html)'
- en: '[PySyft and the Emergence of Private Deep Learning](/2019/06/pysyft-emergence-deep-learning.html)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PySyft与私人深度学习的出现](/2019/06/pysyft-emergence-deep-learning.html)'
- en: '[Top 5 domains Big Data analytics helps to transform](/2018/11/top-5-domains-big-data-analytics.html)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大数据分析帮助转型的前 5 个领域](/2018/11/top-5-domains-big-data-analytics.html)'
- en: More On This Topic
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[GPT-4 is Vulnerable to Prompt Injection Attacks on Causing Misinformation](https://www.kdnuggets.com/2023/05/gpt4-vulnerable-prompt-injection-attacks-causing-misinformation.html)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT-4容易受到提示注入攻击，导致虚假信息]（https://www.kdnuggets.com/2023/05/gpt4-vulnerable-prompt-injection-attacks-causing-misinformation.html）'
- en: '[Be prepared to manage the threat with an MS in Cybersecurity from…](https://www.kdnuggets.com/2022/07/baypath-prepared-manage-threat-ms-cybersecurity.html)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[准备好通过获得网络安全硕士学位来应对威胁…](https://www.kdnuggets.com/2022/07/baypath-prepared-manage-threat-ms-cybersecurity.html)'
- en: '[Be prepared to manage the threat with an MS in Cybersecurity from…](https://www.kdnuggets.com/2022/12/baypath-prepared-manage-threat-ms-cybersecurity.html)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[准备好通过获得网络安全硕士学位来应对威胁…](https://www.kdnuggets.com/2022/12/baypath-prepared-manage-threat-ms-cybersecurity.html)'
- en: '[Applying Descriptive and Inferential Statistics in Python](https://www.kdnuggets.com/applying-descriptive-and-inferential-statistics-in-python)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Python 中应用描述性和推断性统计](https://www.kdnuggets.com/applying-descriptive-and-inferential-statistics-in-python)'
- en: '[AI-Automated Cybersecurity: What to Automate?](https://www.kdnuggets.com/ai-automated-cybersecurity-what-to-automate)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AI 自动化网络安全：该自动化什么？](https://www.kdnuggets.com/ai-automated-cybersecurity-what-to-automate)'
- en: '[Learn Deep Learning by Building 15 Neural Network Projects in 2022](https://www.kdnuggets.com/2022/01/15-neural-network-projects-build-2022.html)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过构建 15 个神经网络项目来学习深度学习（2022）](https://www.kdnuggets.com/2022/01/15-neural-network-projects-build-2022.html)'
