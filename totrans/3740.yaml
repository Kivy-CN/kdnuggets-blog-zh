- en: A Comprehensive Guide to Natural Language Generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《自然语言生成综合指南》
- en: 原文：[https://www.kdnuggets.com/2020/01/guide-natural-language-generation.html](https://www.kdnuggets.com/2020/01/guide-natural-language-generation.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/01/guide-natural-language-generation.html](https://www.kdnuggets.com/2020/01/guide-natural-language-generation.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Sciforce](https://sciforce.solutions).**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Sciforce](https://sciforce.solutions) 提供。**'
- en: As long as Artificial Intelligence helps us to get more out of natural language,
    we see more tasks and fields mushrooming at the intersection of AI and linguistics.
    In one of our previous articles, we discussed the difference between [Natural
    Language Processing and Natural Language Understanding](https://medium.com/sciforce/nlp-vs-nlu-from-understanding-a-language-to-its-processing-1bf1f62453c1).
    Both fields, however, have natural languages as input. At the same time, the urge
    to establish two-way communication with computers has lead to the emergence of
    a separate subcategory of tasks dealing with producing (quasi)-natural speech.
    This subcategory, called Natural Language Generation will be the focus of this
    blog post.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 只要人工智能帮助我们从自然语言中获得更多信息，我们就会看到更多在AI和语言学交集处出现的任务和领域。在我们之前的一篇文章中，我们讨论了[Natural
    Language Processing和Natural Language Understanding的区别](https://medium.com/sciforce/nlp-vs-nlu-from-understanding-a-language-to-its-processing-1bf1f62453c1)。然而，这两个领域都有自然语言作为输入。同时，与计算机建立双向通信的需求导致了处理生成（准）自然语言的任务的一个独立子类别的出现。这个子类别被称为自然语言生成，将成为本博客文章的重点。
- en: What is NLG?
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是NLG？
- en: 'Natural Language Generation,as defined by[ *Artificial Intelligence: Natural
    Language Processing Fundamentals*](http://www.techprenuer.com/entrepreneur/artificial-intelligence-natural-language-processing-fundamentals/),
    is the “process of producing meaningful phrases and sentences in the form of natural
    language.” In its essence, it automatically generates narratives that describe,
    summarize or explain input structured data in a human-like manner at the speed
    of thousands of pages per second.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言生成，如[《人工智能：自然语言处理基础》](http://www.techprenuer.com/entrepreneur/artificial-intelligence-natural-language-processing-fundamentals/)所定义，是“以自然语言形式生成有意义的短语和句子的过程。”本质上，它以每秒数千页的速度自动生成描述、总结或解释输入结构化数据的叙述。
- en: However, while NLG software can write, it can’t read. The part of NLP that reads
    human language and turns its unstructured data into structured data understandable
    to computers is called Natural Language Understanding.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，虽然NLG软件可以写作，但它不能阅读。阅读自然语言并将其非结构化数据转化为计算机可理解的结构化数据的NLP部分被称为自然语言理解。
- en: 'In general terms, NLG (Natural Language Generation) and NLU (Natural Language
    Understanding) are subsections of a more general NLP domain that encompasses all
    software which interprets or produces human language, in either spoken or written
    form:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，NLG（自然语言生成）和NLU（自然语言理解）是更广泛的NLP领域的子部分，该领域包括所有解释或生成自然语言的软件，无论是口头还是书面形式：
- en: NLU takes up the understanding of the data based on grammar, the context in
    which it was said, and decide on intent and entities.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLU负责基于语法、上下文以及意图和实体的理解数据。
- en: NLP converts a text into structured data.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP将文本转换为结构化数据。
- en: NLG generates text based on structured data.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLG基于结构化数据生成文本。
- en: '![](../Images/cae08313cb8a064678399bac726404a0.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cae08313cb8a064678399bac726404a0.png)'
- en: Major applications of NLG
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NLG的主要应用
- en: 'NLG makes data universally understandable, making the writing of data-driven
    financial reports, product descriptions, meeting memos, and more much easier and
    faster. Ideally, it can take the burden of summarizing the data from analysts
    to automatically write reports that would be tailored to the audience. The main
    practical present-day applications of NLG are, therefore, connected with writing
    analysis or communicating necessary information to customers:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: NLG使数据具有普遍可理解性，使得编写数据驱动的财务报告、产品描述、会议备忘录等变得更加容易和快速。理想情况下，它可以将总结数据的负担从分析师那里转移到自动编写针对受众的报告上。因此，NLG当前的主要实际应用与写作分析或向客户传达必要信息相关：
- en: '![](../Images/291b353fb1c7648ca621b3b2799fa33e.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/291b353fb1c7648ca621b3b2799fa33e.png)'
- en: '*Practical Applications of NLG.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*NLG的实际应用。*'
- en: 'At the same time, NLG has more theoretical applications that make it a valuable
    tool not only in Computer Science and Engineering but also in Cognitive Science
    and Psycholinguistics. These include:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，NLG 还有更多的理论应用，使其不仅在计算机科学与工程领域，而且在认知科学和心理语言学中成为有价值的工具。这些包括：
- en: '![](../Images/ced5165a8f06bc74ef0b726507f83c2a.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ced5165a8f06bc74ef0b726507f83c2a.png)'
- en: '*NLG Applications in Theoretical Research.*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*NLG 在理论研究中的应用。*'
- en: Evolution of NLG Design and Architecture
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NLG 设计与架构的演变
- en: 'In the attempts to mimic human speech, NLG systems used different methods and
    tricks to adapt their writing style, tone, and structure according to the audience,
    the context and purpose of the narrative. In 2000, [Reiter and Dale](https://pdfs.semanticscholar.org/728e/18fbf00f5a80e9a070db4f4416d66c7b28f4.pdf) pipelined
    NLG architecture distinguishing three stages in the NLG process:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在模仿人类语言的尝试中，NLG 系统使用了不同的方法和技巧来根据观众、背景和叙述目的调整其写作风格、语气和结构。在 2000 年，[Reiter 和 Dale](https://pdfs.semanticscholar.org/728e/18fbf00f5a80e9a070db4f4416d66c7b28f4.pdf)
    设计了一个 NLG 架构，将 NLG 过程划分为三个阶段：
- en: '**Document planning**: deciding what is to be said and creating an abstract
    document that outlines the structure of the information to be presented.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**文档规划**：决定要表达的内容，并创建一个概述信息结构的抽象文档。'
- en: '**Microplanning**: generation of referring expressions, word choice, and aggregation
    to flesh out the document specifications.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**微观规划**：生成引用表达、词汇选择和聚合，以充实文档规格。'
- en: '**Realisation**: converting the abstract document specifications to a real
    text, using domain knowledge about syntax, morphology, etc.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**实现**：将抽象文档规格转换为真实文本，使用有关语法、形态学等的领域知识。'
- en: '![](../Images/ab758a9d93b12a85a0ec360ca6a68637.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab758a9d93b12a85a0ec360ca6a68637.png)'
- en: '*Three Stages of the NLG Process.*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*NLG 过程的三个阶段。*'
- en: This pipeline shows the milestones of natural language generation. However,
    specific steps and approaches, as well as the models used, can vary significantly
    with technology development.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个流程展示了自然语言生成的里程碑。然而，具体步骤和方法，以及所使用的模型，随着技术的发展可能会有显著变化。
- en: 'There are two major approaches to language generation: using templates and
    dynamic creation of documents. While only the latter is considered to be “real”
    NLG, there was a long and multistage way from basic, straightforward templates
    to the state-of-the-art and each new approach expanded functionality and added
    linguistic capacities:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 语言生成有两种主要方法：使用模板和动态创建文档。虽然只有后者被认为是“真实”的 NLG，但从基础的简单模板到最先进技术的过程中经历了漫长而多阶段的演变，每一种新方法都扩展了功能并增加了语言能力：
- en: Simple Gap-Filling Approach
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的填空方法
- en: 'One of the oldest approaches is a simple fill-in-the-gap template system. In
    texts that have a predefined structure and need just a small amount of data to
    be filled in, this approach can automatically fill in such gaps with data retrieved
    from a spreadsheet row, database table entry, etc. In principle, you can vary
    certain aspects of the text: for example, you can decide whether to spell numbers
    or leave them as is, this approach is quite limited in its use and is not considered
    to be “real” NLG.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最古老的方法之一是简单的填空模板系统。在结构预定义且只需少量数据填充的文本中，这种方法可以自动从电子表格行、数据库表条目等中填充这些空白。原则上，你可以变更文本的某些方面：例如，你可以决定是拼写数字还是保留原样，这种方法的使用相当有限，不被认为是“真实”的
    NLG。
- en: '**Scripts or Rules-Producing Text**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**脚本或规则生成文本**'
- en: Basic gap-filling systems were expanded with general-purpose programming constructs
    via a scripting language or by using business rules. The scripting approach, such
    as [using web templating languages](https://en.wikipedia.org/wiki/Web_template_system),
    embeds a template inside a general-purpose scripting language, so it allows for
    complex conditionals, loops, access to code libraries, etc. Business rule approaches,
    which are adopted by most [document composition](https://en.wikipedia.org/wiki/Document_composition) tools,
    work similarly, but focus on writing business rules rather than scripts. Though
    more powerful than straightforward gap filling, such systems still lack linguistic
    capabilities and cannot reliably generate complex, high-quality texts.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 基础的填空系统通过脚本语言或使用业务规则扩展了通用编程构造。脚本方法，如[使用网页模板语言](https://en.wikipedia.org/wiki/Web_template_system)，将模板嵌入通用脚本语言中，因此它允许复杂的条件、循环、代码库访问等。业务规则方法，由大多数[文档编排](https://en.wikipedia.org/wiki/Document_composition)工具采用，工作原理类似，但侧重于编写业务规则而非脚本。尽管比简单的填空更强大，这些系统仍然缺乏语言能力，无法可靠地生成复杂、高质量的文本。
- en: '**Word-Level Grammatical Functions**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**词级语法功能**'
- en: A logical development of template-based systems was adding word-level grammatical
    functions to deal with morphology, morphophonology, and orthography as well as
    to handle possible exceptions. These functions made it easier to generate grammatically
    correct texts and to write complex template systems.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 模板系统的逻辑发展是添加词级语法功能，以处理形态学、形态音系学和正字法以及处理可能的例外。这些功能使生成语法正确的文本和编写复杂的模板系统变得更加容易。
- en: '**Dynamic Sentence Generation**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态句子生成**'
- en: Finally taking a step from template-based approaches to dynamic NLG, this approach
    dynamically creates sentences from representations of the meaning to be conveyed
    by the sentence or its desired linguistic structure. Dynamic creation means that
    the system can do sensible things in unusual cases, without needing the developer
    to explicitly write code for every boundary case. It also allows the system to
    linguistically “optimise” sentences in a number of ways, including reference,
    aggregation, ordering, and connectives.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，从基于模板的方法转向动态自然语言生成（NLG），这种方法从句子的意义表示或期望的语言结构动态创建句子。动态创建意味着系统能够在不需要开发人员为每个边界情况显式编写代码的情况下，在不寻常的情况下做出合理的处理。它还允许系统在参考、聚合、排序和连接词等方面以多种方式进行语言上的“优化”。
- en: '**Dynamic Document Creation**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态文档创建**'
- en: While dynamic sentence generation works at a certain “micro-level”, the “macro-writing”
    task produces a document which is relevant and useful to its readers, and also
    well-structured as a narrative. How it is done depends on the goal of the text.
    For example, a piece of persuasive writing may be based on models of argumentation
    and behavior change to mimic human rhetoric; and a text that summarizes data for
    business intelligence may be based on an analysis of key factors that influence
    the decision.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然动态句子生成在某种“微观层面”上工作，但“宏观写作”任务生成与读者相关且有用的文档，并且在叙事上结构良好。如何完成这项任务取决于文本的目标。例如，一篇说服性写作可能基于论证和行为变化的模型，以模仿人类修辞；而一篇总结数据以供商业智能使用的文本可能基于对影响决策的关键因素的分析。
- en: NLG Models
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NLG模型
- en: 'Even after NLG shifted from templates to the dynamic generation of sentences,
    it took the technology years of experimenting to achieve satisfactory results.
    As a part of NLP and, more generally, AI, natural language generation relies on
    a number of algorithms that address certain problems of creating human-like texts:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在NLG从模板转向动态句子生成后，这项技术仍然经历了多年的实验才取得令人满意的结果。作为自然语言处理（NLP）的一部分，更多地说作为人工智能（AI）的一部分，自然语言生成依赖于一系列算法，这些算法解决了生成类人文本的某些问题：
- en: '**Markov chain**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**马尔可夫链**'
- en: The Markov chain was one of the first algorithms used for language generation.
    This model predicts the next word in the sentence by using the current word and
    considering the relationship between each unique word to calculate the probability
    of the next word. In fact, you have seen them a lot in earlier versions of the
    smartphone keyboard, where they were used to generate suggestions for the next
    word in the sentence.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫链是用于语言生成的第一个算法之一。该模型通过使用当前词并考虑每个独特词之间的关系来预测句子中的下一个词，从而计算下一个词的概率。事实上，你在早期版本的智能手机键盘上见过它们，当时它们用于生成下一个词的建议。
- en: '**Recurrent neural network (RNN)**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**递归神经网络（RNN）**'
- en: Neural networks are models that try to mimic the operation of the human brain.
    RNNs pass each item of the sequence through a feedforward network and use the
    output of the model as input to the next item in the sequence, allowing the information
    in the previous step to be stored. In each iteration, the model stores the previous
    words encountered in its memory and calculates the probability of the next word.
    For each word in the dictionary, the model assigns a probability based on the
    previous word, selects the word with the highest probability, and stores it in
    memory. RNN’s “memory” makes this model ideal for language generation because
    it can remember the background of the conversation at any time. However, as the
    length of the sequence increases, RNNs cannot store words that were encountered
    remotely in the sentence and makes predictions based on only the most recent word.
    Due to this limitation, RNNs are unable to produce coherent long sentences.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是尝试模拟人脑运作的模型。RNN 将序列中的每个项通过前馈网络，并使用模型的输出作为下一个序列项的输入，从而允许将前一步的信息存储起来。在每次迭代中，模型将之前遇到的单词存储在其记忆中，并计算下一个单词的概率。对于字典中的每个单词，模型根据前一个单词分配一个概率，选择概率最高的单词并将其存储在记忆中。RNN
    的“记忆”使得该模型非常适合语言生成，因为它可以随时记住对话的背景。然而，随着序列长度的增加，RNN 无法存储句子中远处遇到的单词，并且仅根据最近的单词进行预测。由于这一限制，RNN
    无法生成连贯的长句子。
- en: '**LSTM**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**LSTM**'
- en: 'To address the problem of long-range dependencies, a variant of RNN called
    Long short-term memory(LSTM) was introduced. Though similar to RNN, LSTM models
    include a four-layer neural network. The LSTM consists of four parts: the unit,
    the input door, the output door, and the forgotten door. These allow the RNN to
    remember or forget words at any time interval by adjusting the information flow
    of the unit. When a period is encountered, the Forgotten Gate recognizes that
    the context of the sentence may change and can ignore the current unit state information.
    This allows the network to selectively track only relevant information while also
    minimizing the disappearing gradient problem, which allows the model to remember
    information over a longer period.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决长程依赖问题，提出了一种名为长短期记忆（LSTM）的 RNN 变体。虽然与 RNN 相似，但 LSTM 模型包括一个四层神经网络。LSTM 包含四个部分：单元、输入门、输出门和遗忘门。这些允许
    RNN 通过调整单元的信息流，在任何时间间隔记住或忘记单词。当遇到一个时期时，遗忘门识别到句子的上下文可能发生变化，可以忽略当前单元状态信息。这使得网络能够有选择地跟踪相关信息，同时最小化梯度消失问题，从而使模型能够在更长的时间内记住信息。
- en: Still, the capacity of the LSTM memory is limited to a few hundred words due
    to their inherently complex sequential paths from the previous unit to the current
    unit. The same complexity results in high computational requirements that make
    LSTM difficult to train or parallelize.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，由于 LSTM 的记忆容量受限于前一单元到当前单元的复杂序列路径，因此其容量仅限于几百个单词。这种复杂性也导致了高计算需求，使得 LSTM 难以训练或并行化。
- en: '**Transformer**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer**'
- en: A relatively new model was first introduced in the 2017 Google paper ["Attention
    is all you need,"](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)
    which proposed a new method called “self-attention mechanism.” The Transformer
    consists of a stack of encoders for processing inputs of any length and another
    set of decoders to output the generated sentences. In contrast to LSTM, the Transformer
    performs only a small, constant number of steps, while applying a self-attention
    mechanism that directly simulates the relationship between all words in a sentence.
    Unlike previous models, the Transformer uses the representation of all words in
    context without having to compress all the information into a single fixed-length
    representation that allows the system to handle longer sentences without the skyrocketing
    of computational requirements.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相对较新的模型在 2017 年的 Google 论文中首次提出 [“Attention is all you need”](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)，提出了一种称为“自注意力机制”的新方法。Transformer
    由一个处理任意长度输入的编码器堆栈和另一组输出生成句子的解码器组成。与 LSTM 相比，Transformer 仅执行少量、恒定的步骤，同时应用直接模拟句子中所有单词之间关系的自注意力机制。与之前的模型不同，Transformer
    使用上下文中所有单词的表示，而不必将所有信息压缩到单个固定长度的表示中，从而使系统能够处理更长的句子，而不会导致计算需求急剧增加。
- en: One of the most famous examples of the Transformer for language generation is
    OpenAI, their GPT-2 language model. The model learns to predict the next word
    in a sentence by focusing on words that were previously seen in the model and
    related to predicting the next word. A more recent upgrade by Google, the Transformers
    two-way encoder representation (BERT), provides the most advanced results for
    various NLP tasks.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 语言生成的一个著名例子是 OpenAI 的 GPT-2 语言模型。该模型通过关注模型中之前见过的与预测下一个词相关的词来学习预测句子中的下一个词。谷歌的最新升级版
    Transformers 双向编码器表示（BERT）为各种 NLP 任务提供了最先进的结果。
- en: '![](../Images/16274db06070527c65142aea9394a04d.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16274db06070527c65142aea9394a04d.png)'
- en: NLG Tools
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NLG 工具
- en: You can see that natural language generation is a complicated task that needs
    to take into account multiple aspects of language, including its structure, grammar,
    word usage, and perception. Luckily, you probably won’t build the whole NLG system
    from scratch as the market offers multiple ready-to-use tools, both commercial
    and open-source.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，自然语言生成是一个复杂的任务，需要考虑语言的多个方面，包括结构、语法、词汇使用和感知。幸运的是，你可能不会从头开始构建整个 NLG 系统，因为市场上提供了多种现成的工具，包括商业和开源的。
- en: '**Commercial NLG Tools**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**商业 NLG 工具**'
- en: '[Arria NLG PLC](https://www.arria.com/) is believed to be one of the global
    leaders in NLG technologies and tools and can boast the most advanced NLG engine
    and reports generated by NLG narratives. The company has patented NLG technologies
    available for use via the Arria NLG platform.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[Arria NLG PLC](https://www.arria.com/) 被认为是全球 NLG 技术和工具的领导者之一，拥有最先进的 NLG 引擎和由
    NLG 叙述生成的报告。该公司拥有可通过 Arria NLG 平台使用的专利 NLG 技术。'
- en: '[AX Semantics:](https://cloud.ax-semantics.com/) offers eCommerce, journalistic,
    and data reporting (e.g., BI or financial reporting) NLG services for over 100
    languages. It is a developer-friendly product that uses AI and machine learning
    to train the platform’s NLP engine.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[AX Semantics:](https://cloud.ax-semantics.com/) 为超过 100 种语言提供电子商务、新闻和数据报告（例如
    BI 或财务报告）的 NLG 服务。它是一个开发者友好的产品，利用 AI 和机器学习来训练平台的 NLP 引擎。'
- en: '[Yseop](https://yseop.com/) is known for its smart customer experience across
    platforms like mobile, online, or face-to-face. From the NLG perspective, it offers
    Compose that can be consumed on-premises, in the cloud or as a service, and offers
    Savvy, a plug-in for Excel and other analytics platforms.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[Yseop](https://yseop.com/) 以其在移动、在线或面对面等平台上的智能客户体验而闻名。从 NLG 角度来看，它提供了可以在本地、云端或作为服务消费的
    Compose，并且还提供了用于 Excel 和其他分析平台的插件 Savvy。'
- en: '[Quill](https://narrativescience.com/Platform) by Narrative Science is an NLG
    platform powered by advanced NLG. Quill converts data to human-intelligent narratives
    by developing a story, analysing it and extracting the required amount of data
    from it.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[Quill](https://narrativescience.com/Platform) 由 Narrative Science 提供，是一个由先进
    NLG 技术驱动的 NLG 平台。Quill 通过开发故事、分析并提取所需数据，将数据转换为具有智能的人类叙述。'
- en: '[Wordsmith](https://automatedinsights.com/wordsmith) by Automated Insights
    is an NLG engine that works chiefly in the sphere of advanced template-based approaches.
    It allows users to convert data into text in any format or scale. Wordsmith also
    provides a plethora of language options for data conversion.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wordsmith](https://automatedinsights.com/wordsmith) 由 Automated Insights 提供，是一个主要在高级模板化方法领域工作的
    NLG 引擎。它允许用户将数据转换为任何格式或规模的文本。Wordsmith 还提供了丰富的语言选项用于数据转换。'
- en: '**Open-Source NLG Tools**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**开源 NLG 工具**'
- en: '[Simplenlg](https://github.com/simplenlg/simplenlg) is probably the most widely
    used open-source realiser, especially by system-builders. It is an open-source
    Java API for NLG written by the founder of Arria. It has the least functionality
    but also is the easiest to use and best documented.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[Simplenlg](https://github.com/simplenlg/simplenlg) 可能是最广泛使用的开源实现工具，特别是被系统构建者使用。它是由
    Arria 创始人编写的开源 Java API。它功能最少，但使用最简单且文档最完善。'
- en: '[NaturalOWL](https://sourceforge.net/projects/naturalowl/) is an open-source
    toolkit that can be used to generate descriptions of OWL classes and individuals
    to configure an NLG framework to specific needs, without doing much programming.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[NaturalOWL](https://sourceforge.net/projects/naturalowl/) 是一个开源工具包，用于生成 OWL
    类和个体的描述，以将 NLG 框架配置到特定需求，而无需进行大量编程。'
- en: Conclusion
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: NLG capabilities have become the de-facto option as analytical platforms try
    to democratize data analytics and help anyone understand their data*. *Close to
    human narratives automatically explain insights that otherwise could be lost in
    tables, charts, and graphs via natural language and act as a companion throughout
    the data discovery process. Besides, NLG coupled with NLP are the core of chatbots
    and other automated chats and assistants that provide us with everyday support.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: NLG 功能已成为分析平台试图实现数据分析民主化的事实选择，并帮助任何人理解他们的数据*。* 接近人类叙述的自然语言自动解释了那些可能在表格、图表和图形中丢失的见解，并在数据发现过程中充当助手。此外，NLG
    与 NLP 结合是聊天机器人和其他自动化聊天及助手的核心，提供日常支持。
- en: As NLG continues to evolve, it will become more diversified and will provide
    effective communication between us and computers in a natural fashion that many
    SciFi writers dreamed of in their books.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 随着自然语言生成（NLG）的不断发展，它将变得更加多样化，并在我们与计算机之间提供自然的有效沟通，这是许多科幻作家在他们的书中梦想的。
- en: '[Original](https://medium.com/sciforce/a-comprehensive-guide-to-natural-language-generation-dd63a4b6e548).
    Reposted with permission.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/sciforce/a-comprehensive-guide-to-natural-language-generation-dd63a4b6e548)。经许可转载。'
- en: '**Bio:** [SciForce](https://sciforce.solutions/) is a Ukraine-based IT company
    specialized in development of software solutions based on science-driven information
    technologies. We have wide-ranging expertise in many key AI technologies, including
    Data Mining, Digital Signal Processing, Natural Language Processing, Machine Learning,
    Image Processing and Computer Vision.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：** [SciForce](https://sciforce.solutions/) 是一家总部位于乌克兰的 IT 公司，专注于基于科学驱动的信息技术的软件解决方案开发。我们在许多关键的
    AI 技术领域拥有广泛的专业知识，包括数据挖掘、数字信号处理、自然语言处理、机器学习、图像处理和计算机视觉。'
- en: '**Related:**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Training a Neural Network to Write Like Lovecraft](https://www.kdnuggets.com/2019/07/training-neural-network-write-like-lovecraft.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[训练神经网络模仿洛夫克拉夫特的写作风格](https://www.kdnuggets.com/2019/07/training-neural-network-write-like-lovecraft.html)'
- en: '[Natural Language Generation overview – is NLG is worth a thousand pictures?](https://www.kdnuggets.com/2017/05/nlg-natural-language-generation-overview.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言生成概述——NLG是否值千言万语？](https://www.kdnuggets.com/2017/05/nlg-natural-language-generation-overview.html)'
- en: '[Your Guide to Natural Language Processing (NLP)](https://www.kdnuggets.com/2019/05/guide-natural-language-processing-nlp.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理（NLP）指南](https://www.kdnuggets.com/2019/05/guide-natural-language-processing-nlp.html)'
- en: More On This Topic
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Retrieval Augmented Generation: Where Information Retrieval Meets…](https://www.kdnuggets.com/retrieval-augmented-generation-where-information-retrieval-meets-text-generation)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[检索增强生成：信息检索与…的结合](https://www.kdnuggets.com/retrieval-augmented-generation-where-information-retrieval-meets-text-generation)'
- en: '[N-gram Language Modeling in Natural Language Processing](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[N-gram 语言建模在自然语言处理中的应用](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
- en: '[Exploring the Zephyr 7B: A Comprehensive Guide to the Latest Large…](https://www.kdnuggets.com/exploring-the-zephyr-7b-a-comprehensive-guide-to-the-latest-large-language-model)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[探索 Zephyr 7B：最新大型语言模型的综合指南](https://www.kdnuggets.com/exploring-the-zephyr-7b-a-comprehensive-guide-to-the-latest-large-language-model)'
- en: '[A Guide to Top Natural Language Processing Libraries](https://www.kdnuggets.com/2023/04/guide-top-natural-language-processing-libraries.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[顶级自然语言处理库指南](https://www.kdnuggets.com/2023/04/guide-top-natural-language-processing-libraries.html)'
- en: '[Text-2-Video Generation: Step-by-Step Guide](https://www.kdnuggets.com/2023/08/text2video-generation-stepbystep-guide.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本到视频生成：一步一步指南](https://www.kdnuggets.com/2023/08/text2video-generation-stepbystep-guide.html)'
- en: '[A Comprehensive List of Resources to Master Large Language Models](https://www.kdnuggets.com/a-comprehensive-list-of-resources-to-master-large-language-models)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握大型语言模型的全面资源列表](https://www.kdnuggets.com/a-comprehensive-list-of-resources-to-master-large-language-models)'
