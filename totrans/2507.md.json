["```py\nvirtualenv ptl_env --python=python3\nsource ptl_env/bin/activate\npip install pytorch-lightning\npip install torchvision\npip install scikit-learn\n```", "```py\nconda create -n ptl_env\nconda activate ptl_env\nconda install -n ptl_env pytorch-lighnting -c conda-forge\nconda install -n ptl_env torchvision \nconda install -n ptl_env scikit-learn\n```", "```py\nconda create -n ptl_env\nconda activate ptl_env\nconda install -n ptl_env pip\npip install pytorch-lightning\npip install torchvision\npip install scikit-learn\n```", "```py\nimport os\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import MNIST\n\n# for rapid prototyping with a small dataset\nimport sklearn\nimport sklearn.metrics\nimport sklearn.datasets\n# for building intuition with a few tens of thousands of samples\nfrom torchvision.datasets import MNIST\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.metrics import functional as FM\n```", "```py\nclass MyClassifier(pl.LightningModule):\n\n    def __init__(self, dim=28, activation=nn.ReLU()):\n\n        super(MyClassifier, self).__init__()\n\n        self.image_dim = dim\n        self.hid_dim = 128\n        self.num_classes = 10\n        self.act = activation\n\n        self.feature_extractor = nn.Sequential(\\\n                nn.Conv2d(1, 4, 3, padding=1), \\\n                self.act, \\\n                nn.Conv2d(4, 4, 3, padding=1), \\\n                self.act, \\\n                nn.Conv2d(4, 1, 3, padding=1), \\\n                self.act, \\\n                nn.Flatten())\n\n        self.head = nn.Sequential(\\\n                nn.Linear(self.image_dim**2, self.hid_dim), \\\n                self.act, \\\n                nn.Linear(self.hid_dim, self.hid_dim), \\\n                self.act, \\\n                nn.Linear(self.hid_dim, self.num_classes))\n\n    def forward(self, x):\n\n        x = self.feature_extractor(x)\n        output = self.head(x)\n\n        return output\n\n    def training_step(self, batch, batch_index):\n\n        x, y = batch\n\n        output = self.forward(x)\n\n        loss = F.nll_loss(F.log_softmax(output, dim = -1), y)\n\n        y_pred = output.argmax(-1).cpu().numpy()\n        y_tgt = y.cpu().numpy()\n        accuracy = sklearn.metrics.accuracy_score(y_tgt, y_pred)\n        self.log(\"train loss\", loss)\n        self.log(\"train accuracy\", accuracy)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n\n        x, y = batch\n\n        output = self.forward(x)\n\n        loss = F.cross_entropy(output, y)\n\n        pred = output.argmax(-1)\n\n        return output, pred, y\n\n    def validation_epoch_end(self, validation_step_outputs):\n\n        losses = 0\n        outputs = None\n        preds = None\n        tgts = None\n        for output, pred, tgt in validation_step_outputs:\n        preds = torch.cat([preds, pred]) if preds is not None else pred\n        outputs = torch.cat([outputs, output], dim = 0) \\\n        if outputs is not None else output\n        tgts = torch.cat([tgts, tgt]) if tgts is not None else tgt\n\n        loss = F.nll_loss(F.log_softmax(outputs, dim = -1), tgts)\n\n        y_preds = preds.cpu().numpy()\n        y_tgts = tgts.cpu().numpy()\n\n        fm_accuracy = FM.accuracy(outputs, tgts)\n\n        # pytorch lightning prints a deprecation warning for FM.accuracy,\n        # so we'll include sklearn.metrics.accuracy_score as an alternative\n        accuracy = sklearn.metrics.accuracy_score(y_tgts, y_preds)\n\n        self.log(\"val_accuracy\", accuracy)\n        self.log(\"val_loss\", loss)\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=3e-4)\n```", "```py\nclass SKDigitsDataset(torch.utils.data.Dataset):\n\n    def __init__(self, mode=\"train\"):\n        super(SKDigitsDataset, self).__init__()\n        x, y = sklearn.datasets.load_digits(return_X_y = True)\n\n        num_samples = int(x.shape[0] * 0.8)\n        np.random.seed(42)\n        np.random.shuffle(x)\n        np.random.seed(42)\n        np.random.shuffle(y)\n\n        if mode == \"train\":\n        self.x = x[:num_samples]\n        self.y = y[:num_samples]\n        elif mode == \"val\":\n        self.x = x[num_samples:]\n        self.y = y[num_samples:]\n        else:\n        self.x = x\n        self.y = y\n\n        self.transform = lambda my_dict: \\\n        (torch.tensor(my_dict[\"x\"]).float(), \\\n        torch.tensor(my_dict[\"y\"]).long())\n\n    def __len__(self):\n        return self.x.shape[0]\n\n    def __getitem__(self, index):\n\n        got_x = self.x[index].reshape(-1, 8, 8)\n        got_y = self.y[index]\n\n        sample = {\"x\": got_x, \"y\": got_y}\n\n        sample = self.transform(sample)\n\n        return sample\n```", "```py\ndataset = SKDigitsDataset()\ndataloader = DataLoader(dataset)\nmodel = MyClassifier(dim=8)\ntrainer = pl.Trainer()\ntrainer.fit(model, dataloader)\n```", "```py\nif __name__ == \"__main__\":\n    # if using digits from sklearn\n\n    train_dataset = SKDigitsDataset(mode = \"train\")\n    val_dataset = SKDigitsDataset(mode = \"val\")\n\n    dim = 8\n    validation_interval = 1.0\n\n    train_dataloader = DataLoader(train_dataset)\n    val_dataloader = DataLoader(val_dataset)\n\n    model = MyClassifier(dim=dim, activation=nn.ReLU())\n    trainer = pl.Trainer(max_epochs = 100, \\\n    val_check_interval = validation_interval)\n\n    trainer.fit(model, train_dataloader, val_dataloader)\n\n    print(\"Training finished, all ok\")\n```", "```py\ntensorboard --logdir=lightning_logs\n```", "```py\nif __name__ == \"__main__\":\n\n    if(1):\n        # if using digits from sklearn\n\n        train_dataset = SKDigitsDataset(mode = \"train\")\n        val_dataset = SKDigitsDataset(mode = \"val\")\n\n        dim = 8\n        validation_interval = 1.0\n\n    else:\n        # if using MNIST\n        train_dataset = MNIST(os.getcwd(), download=True, \\\n                train=True, transform=transforms.ToTensor())\n        val_dataset = MNIST(os.getcwd(), download=True, \\\n                train=False, transform=transforms.ToTensor())\n\n        dim = 28\n        validation_interval = 0.1\n\n    train_dataloader = DataLoader(train_dataset)\n    val_dataloader = DataLoader(val_dataset)\nclass Swish(nn.Module):\n\n    def __init__(self):\n        super(Swish, self).__init__()\n\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\nfor replicate in range(3):\n    for activation in [Swish(), nn.ReLU()]:\n\n        model = MyClassifier(dim=dim, activation=activation)\n\n        trainer = pl.Trainer(max_epochs = 100, \\\n        val_check_interval = validation_interval)\n        trainer.fit(model, train_dataloader, val_dataloader)\n\n        print(f\" round {replicate} finished.\")\n```", "```py\ntrainer = pl.Trainer(max_epochs = 100, \\\n                val_check_interval = validation_interval, \\ \n                gpus=[0])\n```"]