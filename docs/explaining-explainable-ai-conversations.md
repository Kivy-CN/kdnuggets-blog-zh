# 解释可解释的对话 AI

> 原文：[`www.kdnuggets.com/2022/10/explaining-explainable-ai-conversations.html`](https://www.kdnuggets.com/2022/10/explaining-explainable-ai-conversations.html)

![解释可解释的对话 AI](img/0f947088eb9a54bb2ba82223c96b1b2d.png)

在短短的二三十年内，人工智能（AI）已从科幻小说的页面走向现代社会的基石技术之一。机器学习（ML）的成功催生了几乎无法计数的新 AI 应用，从自主机器和生物特征识别到预测分析和聊天机器人。

近年来，人工智能的一个新兴应用是对话智能（CI）。虽然自动聊天机器人和虚拟助手关注的是人机互动，但 CI 旨在更详细地探索人际互动。从人类对话中监测和提取数据，包括语气、[情感](https://symbl.ai/blog/sentiment-analysis/)和背景，其潜力似乎是无限的。

例如，呼叫中心互动的数据可以生成并记录，从说话者比例和客户满意度到通话摘要和行动点，全部自动归档。这将大幅减少呼叫中心处理中的繁文缛节，为客服代表提供更多与客户交谈的时间。而且，生成的数据甚至可以用于制定员工培训计划，并识别和奖励出色的工作表现。

但仍然缺少一些东西——信任。以这种方式部署人工智能极其有用，但目前仍然需要企业的信任。

# 我们信任人工智能吗？

作为企业，也作为整个社会，我们在人工智能系统上投入了大量信任。社交媒体公司如 Twitter 现在使用[基于 AI 的算法](https://venturebeat.com/ai/twitter-cto-on-machine-learning-challenges-im-not-proud-that-we-miss-a-lot-of-misinformation/)来打击仇恨言论，保障用户在线安全。全球的医疗服务提供者越来越多地利用 AI，从能够进行患者分类的聊天机器人到可以帮助病理学家进行更准确诊断的算法。[英国政府](https://www.publictechnology.net/articles/features/ai-week-automation-makes-work-less-taxing-hmrc)最近采用了一种名为“Connect”的 AI 工具来帮助解析税务记录和检测欺诈行为。甚至还有利用 AI 改善执法效果的例子，如使用面部识别、群体监控和步态分析等工具来识别嫌疑犯。

我们为了一个更高效、连接和无缝的世界而做出这个信任的飞跃。这个世界建立在“大数据”之上，我们需要 AI 来帮助我们管理数据流并将其用到实处。在宏观层面和个体企业中，这都是如此。但尽管我们对 AI 技术的依赖越来越大，我们对其内部工作却知之甚少。随着数据量的增加，以及 AI 做出判断的路径变得更加复杂，我们作为人类已经失去了理解和追溯这些路径的能力。我们所剩下的是一个几乎无法解释的“黑箱”。

这就引出了一个问题：如果我们无法理解*这些*决策是如何做出的，我们怎么能信任基于 AI 的决策？这对于希望确保系统正常运行、符合正确的监管标准或最大化效率的企业来说，越来越成为一种挫折。考虑一下亚马逊的招聘团队，他们在意识到自己的秘密 AI 招聘工具存在性别偏见后不得不[放弃了该工具](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)。他们以为自己找到了招聘的“圣杯”——一个能够扫描数百份简历并挑选出前几名进行审核的工具，节省了大量工作时间。通过重复和强化，AI 竟然说服自己男性候选人比女性候选人更可取。如果团队盲目信任 AI——虽然时间非常短暂——公司的后果将会是毁灭性的。

在商业挫折和对过度信任 AI 的担忧方面，CI 的新兴领域是一个理想的例证。

# 对话智能如何获得信任？

人际互动的世界多年来一直是 AI 创新的热土。使用自然语言处理（NLP）创建聊天机器人或将语音转录为文本是一回事，但从对话中得出意义和理解则完全是另一回事。这就是对话智能（CI）所做的。它超越了确定性的“A 到 B”结果，旨在分析对话中不那么明确的方面，如语调、情感和意义。

如果在呼叫中心使用 CI，例如，它可能用于确定接听员的效果、客户的情绪状态，或提供带有行动要点的自动通话摘要。这些是复杂且主观的互动，不一定有对错之分。如果呼叫中心打算利用 CI 来简化互动、培训代理并更新客户记录，它需要对基础 AI 的有效性有信心。这就是可解释 AI 或“XAI”发挥作用的地方。

每个企业都是不同的，对于系统应学习和预测的内容有不同的定义。解决方案必须提供相对于使用系统的人类行为者的预测的完整视图，以便他们可以持续批准或拒绝系统所做的预测。与其采用一个黑箱式的深度学习系统来执行任务，不如采用一个模块化的系统，其中对系统预测的每个方面都有完全的透明度和控制。例如，可以使用确定性可编程系统来跟踪通话情绪、寻找话题、生成摘要、检测特定方面（如支持通话中的问题类型或客户反馈通话中的请求）等，而不是一个单一的深度学习系统来完成所有这些任务。通过创建这样的模块化架构，整体对话智能解决方案将建立为可追溯和确定的。

# 揭开面纱

当人工智能处理过程简单且确定时，对这些过程的信任从未成为问题。现在，这些过程变得更加复杂和不透明，如上述 CI 示例所示，信任已经成为希望投资人工智能的企业的关键。在他仍然相关的十年前的[论文](https://www.dhi.ac.uk/san/waysofbeing/data/data-crone-taddeo-2011.pdf)中，Mariarosaria Taddeo 提到这被称为“电子信任”——人类如何信任计算机化的过程，以及我们在多大程度上允许人工智能代理参与这种关系。

可解释人工智能（XAI）是机器学习中的一个新兴领域，旨在使这些人工智能代理完全透明且更易于解释。美国的[国防高级研究计划局](https://www.darpa.mil/program/explainable-artificial-intelligence)（DARPA）是追求 XAI 解决方案的领先组织之一。DARPA 认为，人工智能系统的潜力被其无法向人类用户解释其行为的能力严重阻碍。换句话说，组织对人工智能缺乏信任阻碍了他们探索人工智能和机器学习所能提供的全部可能性。

目标是创建一套机器学习技术，能够生成可解释的模型，使人类用户能够理解和管理下一代人工智能解决方案。这些机器学习系统将能够解释其推理，识别自身的优势和不足，并传达它们如何从输入的数据中“学习”。对于 DARPA 来说，这是一种推动其所称的第三代人工智能系统的努力，届时机器将能够理解其操作的上下文和环境。

为了充分实现人工智能的潜力，我们需要从“0”和“1”中迈出一步，引入更多主观分析。技术已经存在，我们只需要更多的信任理由。

**[Surbhi Rathore](https://www.linkedin.com/in/surbhi-rathore/)** 是 Symbl.ai 的首席执行官兼联合创始人。Symbl 正在实现她对一个可编程平台的愿景，该平台使开发者和企业能够在其产品和工作流程中大规模地监控、操作和遵守语音、视频对话，而无需建立内部的数据科学专业知识。

### 更多相关话题

+   [扩散与去噪：解释文本到图像的生成式 AI](https://www.kdnuggets.com/diffusion-and-denoising-explaining-text-to-image-generative-ai)

+   [最先进的深度学习技术下的可解释预测和即时预测](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)

+   [可解释的 AI：揭秘模型决策的 10 个 Python 库](https://www.kdnuggets.com/2023/01/explainable-ai-10-python-libraries-demystifying-decisions.html)

+   [弥合人类理解与机器学习之间的差距：…](https://www.kdnuggets.com/2023/06/closing-gap-human-understanding-machine-learning-explainable-ai-solution.html)
