- en: 'HuggingGPT: The Secret Weapon to Solve Complex AI Tasks'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/05/hugginggpt-secret-weapon-solve-complex-ai-tasks.html](https://www.kdnuggets.com/2023/05/hugginggpt-secret-weapon-solve-complex-ai-tasks.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![HuggingGPT: The Secret Weapon to Solve Complex AI Tasks](../Images/c3a67fbc0536fa8043d01f54b81d98c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you heard of the term Artificial General Intelligence (AGI)? If not, let
    me clarify. AGI can be thought of as an AI system that can understand, process,
    and respond the intellectual tasks just like humans do. It's a challenging task
    that requires an in-depth understanding of how the human brain works so we can
    replicate it. However, the advent of ChatGPT has drawn immense interest from the
    research community to develop such systems. Microsoft has released one such key
    AI-powered system called HuggingGPT (Microsoft Jarvis). It is one of the most
    mind-blowing things that I have come across.
  prefs: []
  type: TYPE_NORMAL
- en: Before I dive into the details of what is new in HuggingGPT and how it works,
    let us first understand the issue with ChatGPT and why it struggles to solve complex
    AI tasks. Large Language models like ChatGPT excel at interpreting textual data
    and handling general tasks. However,  they often struggle with specific tasks
    and may generate absurd responses. You might have encountered bogus replies from
    ChatGPT while solving complex mathematical problems. On the other side, we have
    expert AI models like Stable Diffusion, and DALL-E that have a deeper understanding
    of their subject area but struggle with the broader tasks. We cannot fully harness
    the potential of LLMs to solve challenging AI tasks unless we develop a connection
    between them and the Specialized AI models. This is what HuggingGPT did. It combined
    the strengths of both to create more efficient, accurate, and versatile AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: What is HuggingGPT?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: According to a [recent paper](https://arxiv.org/abs/2303.17580) published by
    Microsoft, HuggingGPT leverages the power of LLMs by using it as a controller
    to connect them to various AI models in Machine Learning communities (HuggingFace).
    Rather than training the ChatGPT for various tasks, we enable it to use external
    tools for greater efficiency. [HuggingFace](https://huggingface.co/) is a website
    that provides numerous tools and resources for developers and researchers. It
    also has a wide variety of specialized and high-accuracy models. HuggingGPT uses
    these models for sophisticated AI tasks in different domains and modalities thereby
    achieving impressive results. It has similar multimodal capabilities to OPenAI
    GPT-4 when it comes to text and images. But, it also connected you to the Internet
    and you can provide an external web link to ask questions about it.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you want the model to generate an audio reading of the text written
    on an image. HuggingGPT will perform this task serially using the best-suited
    models. Firstly, it will generate the image from text and use its result for audio
    generation. You can check the response details in the image below. Simply Amazing!
  prefs: []
  type: TYPE_NORMAL
- en: '![HuggingGPT: The Secret Weapon to Solve Complex AI Tasks](../Images/6dd819fb645e69a61220b3f7328ccd66.png)'
  prefs: []
  type: TYPE_IMG
- en: Qualitative analysis of multi-model cooperation on video and audio modalities
    ([Source](https://arxiv.org/abs/2303.17580))
  prefs: []
  type: TYPE_NORMAL
- en: How Does HuggingGPT Work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![HuggingGPT: The Secret Weapon to Solve Complex AI Tasks](../Images/4158bcf3dc73ee7c60466ea16263076c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'HuggingGPT is a collaborative system that uses LLMs as an interface to send
    user requests to expert models. The complete process starting from the user prompt
    to the model till receiving the response can be broken down into the following
    discrete steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Task Planning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this stage, HuggingGPT makes use of ChatGPT to understand the user prompt
    and then breaks down the query into small actionable tasks. It also determines
    the dependencies of these tasks and defines their execution sequence. HuggingGPT
    has four slots for task parsing i.e. task type, task ID, task dependencies, and
    task arguments. Chat logs between the HuggingGPT and the user are recorded and
    displayed on the screen that shows the history of the resources.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Model Selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on the user context and the available models, HuggingGPT uses an in-context
    task-model assignment mechanism to select the most appropriate model for a particular
    task. According to this mechanism, the selection of a model is considered a single-choice
    problem and it initially filters out the model based on the type of the task.
    After that, the models are ranked based on the number of downloads as it is considered
    a reliable measure that reflects the quality of the model. “Top-K” models are
    selected based on this ranking. Here K is just a constant that reflects the number
    of models, for example, if it is set to 3 then it will select 3 models with the
    highest number of downloads.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Task Execution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here the task is assigned to a specific model, it performs the inference on
    it and returns the result. To enhance the efficiency of this process, HuggingGPT
    can run different models at the same time as long as they don’t need the same
    resources. For example, if I give a prompt to generate pictures of cats and dogs
    then separate models can run in parallel to execute this task. However, sometimes
    models may need the same resources which is why HuggingGPT maintains an **<resource>**
    attribute to keep the track of the resources. It ensures that the resources are
    being used effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Response Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final step involves generating the response to the user. Firstly, it integrates
    all the information from the previous stages and the inference results. The information
    is presented in a structured format. For example, if the prompt was to detect
    the number of lions in an image, it will draw the appropriate bounding boxes with
    detection probabilities. The LLM (ChatGPT) then uses this format and presents
    it in human-friendly language.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up HuggingGPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'HuggingGPT is built on top of Hugging Face''s state-of-the-art GPT-3.5 architecture,
    which is a deep neural network model that can generate natural language text.
    Here is how you can set it up on your local computer:'
  prefs: []
  type: TYPE_NORMAL
- en: System Requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The default configuration requires Ubuntu 16.04 LTS, VRAM of at least 24GB,
    RAM of at least 12GB (minimal), 16GB (standard), or 80GB (full), and disk space
    of at least 284 GB. Additionally, you'll need 42GB of space for damo-vilab/text-to-video-ms-1.7b,
    126GB for ControlNet, 66GB for stable-diffusion-v1-5, and 50GB for other resources.
    For the "lite" configuration, you'll only need Ubuntu 16.04 LTS.
  prefs: []
  type: TYPE_NORMAL
- en: Steps to Get Started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, replace the OpenAI Key and the Hugging Face Token in the server/configs/config.default.yaml
    file with your keys. Alternatively, you can put them in the environment variables
    **OPENAI_API_KEY** and **HUGGINGFACE_ACCESS_TOKEN**, respectively
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '**For Server:**'
  prefs: []
  type: TYPE_NORMAL
- en: Set up the Python environment and install the required dependencies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Download the required models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Run the server
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can access Jarvis'' services by sending HTTP requests to the Web API
    endpoints. Send a request to :'
  prefs: []
  type: TYPE_NORMAL
- en: '**/hugginggpt** endpoint using the POST method to access the full service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**/tasks** endpoint using the POST method to access intermediate results for
    Stage #1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**/results** endpoint using the POST method to access intermediate results
    for Stages #1-3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The requests should be in JSON format and should include a list of messages
    that represent the user's inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**For Web:**'
  prefs: []
  type: TYPE_NORMAL
- en: Install node js and npm on your machine after starting your application awesome_chat.py
    in server mode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the web directory and install the following dependencies
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Set http://{LAN_IP_of_the_server}:{port}/ to HUGGINGGPT_BASE_URL of web/src/config/index.ts, 
    in case you are running the web client on another machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you want to use the video generation feature, compile ffmpeg manually with
    H.264.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Double-click on the setting icon to switch back to ChatGPT.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**For CLI:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting up Jarvis using CLI is quite simple. Just run the command mentioned
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**For Gradio:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Gradio demo](https://huggingface.co/spaces/microsoft/HuggingGPT) is also being
    hosted on Hugging Face Space. You can experiment with it after entering the OPENAI_API_KEY
    and HUGGINGFACE_ACCESS_TOKEN.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To run it locally:'
  prefs: []
  type: TYPE_NORMAL
- en: Install the required dependencies, clone the project repository from the Hugging
    Face Space, and navigate to the project directory
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Start the model server followed by the Gradio demo using:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Access the demo in your browser at [http://localhost:7860](http://localhost:7860)
    and test by entering various inputs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Optionally, you can also run the demo as a Docker image by running the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Note: In case of any issue please refer to the [official Github Repo](https://github.com/microsoft/JARVIS).'
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HuggingGPT also has certain limitations that I want to highlight here. For instance,
    the efficiency of the system is a major bottleneck and during all the stages mentioned
    earlier, HuggingGPT requires multiple interactions with LLMs. These interactions
    can lead to degraded user experience and increased latency. Similarly, the maximum
    context length is also limited by the number of allowed tokens. Another problem
    is the System's reliability, as the LLMs may misinterpret the prompt and generate
    a wrong sequence of tasks which in turn affects the whole process. Nonetheless,
    it has significant potential to solve complex AI tasks and is an excellent advancement
    toward AGI. Let's see in which direction this research leads us too. That’s a
    wrap, feel free to express your views in the comment section below.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Kanwal Mehreen](https://www.linkedin.com/in/kanwal-mehreen1)** is an aspiring
    software developer with a keen interest in data science and applications of AI
    in medicine. Kanwal was selected as the Google Generation Scholar 2022 for the
    APAC region. Kanwal loves to share technical knowledge by writing articles on
    trending topics, and is passionate about improving the representation of women
    in tech industry.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[GPT-4: 8 Models in One; The Secret is Out](https://www.kdnuggets.com/2023/08/gpt4-8-models-one-secret.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Getting Started with LLMOps: The Secret Sauce Behind Seamless Interactions](https://www.kdnuggets.com/getting-started-with-llmops-the-secret-sauce-behind-seamless-interactions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Want to Use Your Data Skills to Solve Global Problems? Here’s What…](https://www.kdnuggets.com/2022/04/jhu-want-data-skills-solve-global-problems.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Science Projects That Can Help You Solve Real World Problems](https://www.kdnuggets.com/2022/11/data-science-projects-help-solve-real-world-problems.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Use NumPy to Solve Systems of Nonlinear Equations](https://www.kdnuggets.com/how-to-use-numpy-to-solve-systems-of-nonlinear-equations)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Solving 5 Complex SQL Problems: Tricky Queries Explained](https://www.kdnuggets.com/2022/07/5-hardest-things-sql.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
