- en: 'arXiv Paper Spotlight: Why Does Deep and Cheap Learning Work So Well?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: arXiv 论文聚焦：为何深度学习和廉价学习效果如此显著？
- en: 原文：[https://www.kdnuggets.com/2016/12/arxiv-spotlight-deep-cheap-learning.html](https://www.kdnuggets.com/2016/12/arxiv-spotlight-deep-cheap-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/12/arxiv-spotlight-deep-cheap-learning.html](https://www.kdnuggets.com/2016/12/arxiv-spotlight-deep-cheap-learning.html)
- en: Why does deep learning work so well? And... cheap learning?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习为何如此有效？而... 廉价学习呢？
- en: '![arXiv](../Images/7d2d792a27b6d8c8f5ce5b64714828b1.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![arXiv](../Images/7d2d792a27b6d8c8f5ce5b64714828b1.png)'
- en: A [recent paper](https://arxiv.org/abs/1608.08225) by Henry W. Lin (Harvard)
    and Max Tegmark (MIT), titled "Why does deep and cheap learning work so well?"
    looks to examine from a different perspective what it is about deep learning that
    makes it work so well. It also introduces (at least, to me) the term "cheap learning."
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[Henry W. Lin（哈佛大学）和 Max Tegmark（麻省理工学院）](https://arxiv.org/abs/1608.08225)
    的一篇名为“深度学习和廉价学习为何如此有效？”的最新论文从不同的角度探讨了深度学习为何如此有效。它还介绍了（至少对我来说）“廉价学习”这一术语。'
- en: 'First off, to be clear, "cheap learning" does not refer to using a low end
    GPU; instead, the following explains its relationship to parameter reduction:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为了明确，“廉价学习”并不是指使用低端 GPU；接下来解释它与参数减少的关系：
- en: '[A]lthough well-known mathematical theorems guarantee that neural networks
    can approximate arbitrary functions well, the class of functions of practical
    interest can be approximated through "cheap learning" with exponentially fewer
    parameters than generic ones, because they have simplifying properties tracing
    back to the laws of physics.'
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 尽管著名的数学定理保证了神经网络可以很好地逼近任意函数，但实际兴趣的函数类可以通过“廉价学习”用比通用函数少得多的参数来逼近，因为它们具有追溯到物理定律的简化特性。
- en: This central idea of this paper is that neural network success owes as much
    to physics as it does to mathematics (perhaps more), and that simplistic physics
    functions owing to concepts such as symmetry, locality, compositionality, and
    polynomial log-probability can be viewed similarly to deep learning's relationship
    with the reality which it seeks to model. You [may have heard something about
    this](https://www.technologyreview.com/s/602344/the-extraordinary-link-between-deep-neural-networks-and-the-nature-of-the-universe/)
    in September; this is the paper on which said news was based.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的核心思想是，神经网络的成功不仅归功于数学，也同样归功于物理（可能更多），并且由于对称性、局部性、组合性和多项式对数概率等概念的简化物理函数，可以与深度学习与其试图建模的现实之间的关系类似地看待。你[可能已经听说过这个](https://www.technologyreview.com/s/602344/the-extraordinary-link-between-deep-neural-networks-and-the-nature-of-the-universe/)在九月份的新闻；这是该新闻的基础论文。
- en: '![Pyramid](../Images/e9f37985b69e1cf4bb6f72da199a5fa1.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![金字塔](../Images/e9f37985b69e1cf4bb6f72da199a5fa1.png)'
- en: 'More from the abstract:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 更多来自摘要：
- en: 'We further argue that when the statistical process generating the data is of
    a certain hierarchical form prevalent in physics and machine-learning, a deep
    neural network can be more efficient than a shallow one. We formalize these claims
    using information theory and discuss the relation to renormalization group procedures.
    We prove various "no-flattening theorems" showing when such efficient deep networks
    cannot be accurately approximated by shallow ones without efficiency loss: flattening
    even linear functions can be costly, and flattening polynomials is exponentially
    expensive; we use group theoretic techniques to show that n variables cannot be
    multiplied using fewer than 2^n neurons in a single hidden layer.'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们进一步论证了，当生成数据的统计过程具有物理学和机器学习中普遍存在的某种层次结构形式时，深度神经网络比浅层网络更高效。我们通过信息理论来形式化这些主张，并讨论与重整化群程序的关系。我们证明了各种“无平坦定理”，表明在没有效率损失的情况下，这些高效深度网络无法被浅层网络准确逼近：平坦化即使是线性函数也可能代价高昂，而平坦化多项式的代价则是指数级的；我们使用群论技术表明，在一个隐藏层中，n
    个变量的乘积不能使用少于 2^n 个神经元。
- en: '![Information loss](../Images/f979328362b981e94aaa5b04e410a42d.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![信息丢失](../Images/f979328362b981e94aaa5b04e410a42d.png)'
- en: 'With a bit of maths and a few theorems, the paper proceeds as follows: presents
    "shallow" neural network results, with a handful of layers; demonstrates how increasing
    network depth provides polynomial or exponential efficiency gains without adding
    expressivity; summarizes conclusions and discusses a technical point about renormalization
    and deep learning.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一些数学和几个定理，论文如下：展示了“浅层”神经网络的结果，具有少量层；展示了增加网络深度如何在不增加表现力的情况下提供多项式或指数级的效率提升；总结了结论并讨论了关于重整化和深度学习的技术问题。
- en: 'An interesting excerpt from the paper''s conclusion:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 论文结论中的一个有趣摘录：
- en: Whereas previous universality theorems guarantee that there exists a neural
    network that approximates any smooth function to within an error ε, they cannot
    guarantee that the size of the neural network does not grow to infinity with shrinking
    ε or that the activation function σ does not become pathological. We show constructively
    that given a multivariate polynomial and any generic non-linearity, a neural network
    with a fixed size and a generic smooth activation function can indeed approximate
    the polynomial highly efficiently.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 虽然以前的普遍性定理保证存在一个神经网络，它可以在误差 ε 范围内逼近任何光滑函数，但它们不能保证神经网络的规模不会随着 ε 的减小而增长到无穷大，或者激活函数
    σ 不会变得病态。我们通过构造性证明，给定一个多变量多项式和任何通用非线性，具有固定规模和通用光滑激活函数的神经网络确实可以高效地逼近该多项式。
- en: The abstract can be found [here](https://arxiv.org/abs/1608.08225), while this
    is a [direct link](https://arxiv.org/pdf/1608.08225v2.pdf) to the paper.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要可以在 [这里](https://arxiv.org/abs/1608.08225) 找到，而这是一份 [直接链接](https://arxiv.org/pdf/1608.08225v2.pdf)
    到论文的链接。
- en: '**Related**:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关**：'
- en: '[arXiv Paper Spotlight: Stealing Machine Learning Models via Prediction APIs](/2016/11/arxiv-spotlight-stealing-machine-learning-models-prediction-apis.html)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[arXiv 论文聚焦：通过预测 API 偷窃机器学习模型](/2016/11/arxiv-spotlight-stealing-machine-learning-models-prediction-apis.html)'
- en: '[arXiv Paper Spotlight: Automated Inference on Criminality Using Face Images](/2016/12/arxiv-spotlight-automated-inference-criminality-face-images.html)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[arXiv 论文聚焦：利用面部图像进行自动化犯罪推断](/2016/12/arxiv-spotlight-automated-inference-criminality-face-images.html)'
- en: '[5 More arXiv Deep Learning Papers, Explained](/2016/01/more-arxiv-deep-learning-papers-explained.html)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[再看5篇 arXiv 深度学习论文的解释](/2016/01/more-arxiv-deep-learning-papers-explained.html)'
- en: '* * *'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的快车道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT'
- en: '* * *'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[What Is ChatGPT Doing and Why Does It Work?](https://www.kdnuggets.com/2023/04/chatgpt-work.html)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatGPT 是做什么的，它为何有效？](https://www.kdnuggets.com/2023/04/chatgpt-work.html)'
- en: '[It''s alive! Build your first robots with Python and some cheap,…](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[它活了！用 Python 和一些便宜的基础组件构建你的第一个机器人](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
- en: '[5 Cheap Books to Master Machine Learning](https://www.kdnuggets.com/5-cheap-books-to-master-machine-learning)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5本便宜的机器学习大师书籍](https://www.kdnuggets.com/5-cheap-books-to-master-machine-learning)'
- en: '[5 Cheap Books to Master Data Science](https://www.kdnuggets.com/5-cheap-books-to-master-data-science)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5本便宜的数据科学大师书籍](https://www.kdnuggets.com/5-cheap-books-to-master-data-science)'
- en: '[What is K-Means Clustering and How Does its Algorithm Work?](https://www.kdnuggets.com/2023/05/kmeans-clustering-algorithm-work.html)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是 K-Means 聚类，它的算法是如何工作的？](https://www.kdnuggets.com/2023/05/kmeans-clustering-algorithm-work.html)'
- en: '[How Does Logistic Regression Work?](https://www.kdnuggets.com/2022/07/logistic-regression-work.html)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[逻辑回归是如何工作的？](https://www.kdnuggets.com/2022/07/logistic-regression-work.html)'
