- en: Large Language Models Explained in 3 Levels of Difficulty
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/large-language-models-explained-in-3-levels-of-difficulty](https://www.kdnuggets.com/large-language-models-explained-in-3-levels-of-difficulty)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We live in an era where the machine learning model is at its peak. Compared
    to decades ago, most people would never have heard about ChatGPT or Artificial
    Intelligence. However, those are the topics that people keep talking about. Why?
    Because the values given are so significant compared to the effort.
  prefs: []
  type: TYPE_NORMAL
- en: The breakthrough of AI in recent years could be attributed to many things, but
    one of them is the large language model (LLM). Many text generation AI people
    use are powered by the LLM model; For example, ChatGPT uses their GPT model. As
    LLM is an important topic, we should learn about it.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: This article will discuss Large Language Models in 3 difficulty levels, but
    we will only touch on some aspects of LLMs. We would only differ in a way that
    allows every reader to understand what LLM is.  With that in mind, let’s get into
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Level 1: LLM Beginner Level'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first level, we assume the reader doesn’t know about LLM and may know
    a little about the field of data science/machine learning. So, I would briefly
    introduce AI and Machine Learning before moving to the LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '[Artificial Intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence)
    is the science of developing intelligent computer programs. It’s intended for
    the program to perform intelligent tasks that humans could do but does not have
    limitations on human biological needs. [Machine learning](https://en.wikipedia.org/wiki/Machine_learning)
    is a field in artificial intelligence focusing on data generalization studies
    with statistical algorithms. In a way, Machine Learning is trying to achieve Artificial
    Intelligence via data study so that the program can perform intelligence tasks
    without instruction.'
  prefs: []
  type: TYPE_NORMAL
- en: Historically, the field that intersects between computer science and linguistics
    is called the Natural [Language Processing](https://en.wikipedia.org/wiki/Natural_language_processing)
    field. The field mainly concerns any activity of machine processing to the human
    text, such as text documents. Previously, this field was only limited to the rule-based
    system but it became more with the introduction of advanced semi-supervised and
    unsupervised algorithms that allow the model to learn without any direction. One
    of the advanced models to do this is the Language Model.
  prefs: []
  type: TYPE_NORMAL
- en: The language [model](https://en.wikipedia.org/wiki/Language_model) is a probabilistic
    NLP model to perform many human tasks such as translation, grammar correction,
    and text generation. The old form of the language model uses purely statistical
    approaches such as the n-gram method, where the assumption is that the probability
    of the next word depends only on the previous word's fixed-size data.
  prefs: []
  type: TYPE_NORMAL
- en: However, the introduction of [Neural Network](https://en.wikipedia.org/wiki/Artificial_neural_network)
    has dethroned the previous approach. An artificial neural network, or NN, is a
    computer program mimicking the human brain's neuron structure. The Neural Network
    approach is good to use because it can handle complex pattern recognition from
    the text data and handle sequential data like text. That’s why the current Language
    Model is usually based on NN.
  prefs: []
  type: TYPE_NORMAL
- en: '[Large Language Models](https://en.wikipedia.org/wiki/Large_language_model),
    or LLMs, are machine learning models that learn from a huge number of data documents
    to perform general-purpose language generation. They are still a language model,
    but the huge number of parameters learned by the NN makes them considered large.
    In layperson''s terms, the model could perform how humans write by predicting
    the next words from the given input words very well.'
  prefs: []
  type: TYPE_NORMAL
- en: Examples of LLM tasks include language translation, machine chatbot, question
    answering, and many more. From any sequence of data input, the model could identify
    relationships between the words and generate output suitable from the instruction.
  prefs: []
  type: TYPE_NORMAL
- en: Almost all of the Generative AI products that boast something using text generation
    are powered by the LLMs. Big products like ChatGPT, Google’s Bard, and many more
    are using LLMs as the basis of their product.
  prefs: []
  type: TYPE_NORMAL
- en: 'Level 2: LLM Intermediate Level'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The reader has data science knowledge but needs to learn more about the LLM
    at this level. At the very least, the reader can understand the terms used in
    the data field. At this level, we would dive deeper into the base architecture.
  prefs: []
  type: TYPE_NORMAL
- en: As explained previously, LLM is a Neural Network model trained on massive amounts
    of text data. To understand this concept further, it would be beneficial to understand
    how neural networks and deep learning work.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous level, we explained that a neural neuron is a model miming the
    human brain's neural structure. The main element of the Neural Network is the
    neurons, often called nodes. To explain the concept better, see the typical Neural
    Network architecture in the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Large Language Models Explained in 3 Levels of Difficulty](../Images/328cdc6c3c343a7abceb215a3bf4026a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Neural Network Architecture(Image source: [KDnuggets](/2016/10/deep-learning-key-terms-explained.html))'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in the image above, the Neural Network consists of three layers:'
  prefs: []
  type: TYPE_NORMAL
- en: Input layer where it receives the information and transfers it to the other
    nodes in the next layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hidden node layers where all the computations take place.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Output node layer where the computational outputs are.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It's called deep learning when we train our Neural Network model with two or
    more hidden layers. It’s called deep because it uses many layers in between. The
    advantage of deep learning models is that they automatically learn and extract
    features from the data that traditional machine learning models are incapable
    of.
  prefs: []
  type: TYPE_NORMAL
- en: In the Large Language Model, deep learning is important as the model is built
    upon deep neural network architectures. So, why is it called LLM? It’s because
    billions of layers are trained upon massive amounts of text data. The layers would
    produce model parameters that help the model learn complex patterns in language,
    including grammar, writing style, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: The simplified process of the model training is shown in the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Large Language Models Explained in 3 Levels of Difficulty](../Images/8fe85eacdc181b9a3944ee37e2a1a28f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Kumar Chandrakant (Source: [Baeldung.com](https://www.baeldung.com/cs/large-language-models))'
  prefs: []
  type: TYPE_NORMAL
- en: The process showed that the models could generate relevant text based on the
    likelihood of each word or sentence of the input data. In the LLMs, the advanced
    approach uses [self-supervised learning](https://en.wikipedia.org/wiki/Self-supervised_learning)
    and [semi-supervised learning](https://en.wikipedia.org/wiki/Weak_supervision)
    to achieve the general-purpose capability.
  prefs: []
  type: TYPE_NORMAL
- en: Self-supervised learning is a technique where we don’t have labels, and instead,
    the training data provides the training feedback itself. It’s used in the LLM
    training process as the data usually lacks labels. In LLM, one could use the surrounding
    context as a clue to predict the next words. In contrast, Semi-supervised learning
    combines the supervised and unsupervised learning concepts with a small amount
    of labeled data to generate new labels for a large amount of unlabeled data. Semi-supervised
    learning is usually used for LLMs with specific context or domain needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Level 3: LLM Advanced Level'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the third level, we would discuss the LLM more deeply, especially tackling
    the LLM structure and how it could achieve human-like generation capability.
  prefs: []
  type: TYPE_NORMAL
- en: We have discussed that LLM is based on the Neural Network model with Deep Learning
    techniques. The LLM has typically been built based on [transformer-based](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    architecture in recent years. The transformer is based on the multi-head attention
    mechanism introduced by [Vaswani *et al.* (2017)](https://arxiv.org/pdf/1706.03762.pdf)
    and has been used in many LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers is a model architecture that tries to solve the sequential tasks
    previously encountered in the RNNs and LSTMs. The old way of the Language Model
    was to use RNN and LSTM to process data sequentially, where the model would use
    every word output and loop them back so the model would not forget. However, they
    have problems with long-sequence data once transformers are introduced.
  prefs: []
  type: TYPE_NORMAL
- en: Before we go deeper into the Transformers, I want to introduce the concept of
    encoder-decoder that was previously used in RNNs. The encoder-decoder structure
    allows the input and output text to not be of the same length. The example use
    case is a language translation, which often has a different sequence size.
  prefs: []
  type: TYPE_NORMAL
- en: The structure can be divided into two. The first part is called Encoder, which
    is a part that receives data sequence and creates a new representation based on
    it. The representation would be used in the second part of the model, which is
    the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: '![Large Language Models Explained in 3 Levels of Difficulty](../Images/95917420d2cc5722fe59d5cbeed99b27.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The problem with RNN is that the model might need help remembering longer sequences,
    even with the encoder-decoder structure above. This is where the attention mechanism
    could help solve the problem, a layer that could solve long input problems. The
    attention mechanism is introduced in the paper by [Bahdanau *et al*. (2014)](https://arxiv.org/abs/1409.0473)
    to solve the encoder-decoder type RNNs by focusing on an important part of the
    model input while having the output prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The transformer's structure is inspired by the encoder-decoder type and built
    with the attention mechanism techniques, so it does not need to process data in
    sequential order. The overall transformers model is structured like the image
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Large Language Models Explained in 3 Levels of Difficulty](../Images/78049f2f056c25c9dabf059929e0109e.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformers Architecture ([Vaswani *et al.* (2017)](https://arxiv.org/pdf/1706.03762.pdf))
  prefs: []
  type: TYPE_NORMAL
- en: In the structure above, the transformers encode the data vector sequence into
    the word embedding while using the decoding to transform data into the original
    form. The encoding can assign a certain importance to the input with the attention
    mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: We have talked a bit about transformers encoding the data vector, but what is
    a data vector? Let’s discuss it. In the machine learning model, we can’t input
    the raw natural language data into the model, so we need to transform them into
    numerical forms. The transformation process is called word embedding, where each
    input word is processed through the word embedding model to get the data vector.
    We can use many initial word embeddings, such as [Word2vec](https://en.wikipedia.org/wiki/Word2vec)
    or [GloVe](https://nlp.stanford.edu/projects/glove/), but many advanced users
    try to refine them using their vocabulary. In a basic form, the word embedding
    process can be shown in the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Large Language Models Explained in 3 Levels of Difficulty](../Images/d1502abf72097ce3764fb31f8ea9a66e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The transformers could accept the input and provide more relevant context by
    presenting the words in numerical forms like the data vector above. In the LLMs,
    word embeddings are usually context-dependent, generally refined upon the use
    cases and the intended output.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We discussed the Large Language Model in three difficulty levels, from beginner
    to advanced. From the general usage of LLM to how it is structured, you can find
    an explanation that explains the concept in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: '**[](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**[Cornellius Yudha
    Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**** is a data science
    assistant manager and data writer. While working full-time at Allianz Indonesia,
    he loves to share Python and data tips via social media and writing media. Cornellius
    writes on a variety of AI and machine learning topics.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[The Difficulty of Estimating the Carbon Footprint of Machine Learning](https://www.kdnuggets.com/2022/07/difficulty-estimating-carbon-footprint-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Open Source Large Language Models](https://www.kdnuggets.com/2022/09/john-snow-top-open-source-large-language-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Learn About Large Language Models](https://www.kdnuggets.com/2023/03/learn-large-language-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introducing Healthcare-Specific Large Language Models from John Snow Labs](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are Large Language Models and How Do They Work?](https://www.kdnuggets.com/2023/05/large-language-models-work.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AI: Large Language & Visual Models](https://www.kdnuggets.com/2023/06/ai-large-language-visual-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
