["```py\nprice = page_content.find(id='listings_prices')\n# check if the element with such id exists or not\nif price is None:\n    # NOTIFY! LOG IT, COUNT IT\nelse:\n    # do something\n\n```", "```py\n# library to generate user agent\nfrom user_agent import generate_user_agent\n# generate a user agent\nheaders = {'User-Agent': generate_user_agent(device_type=\"desktop\", os=('mac', 'linux'))}\n#headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux i686 on x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.63 Safari/537.36'}\npage_response = requests.get(page_link, timeout=5, headers=headers)\n\n```", "```py\n# timeout is set to 5 seconds\npage_response = requests.get(page_link, timeout=5, headers=headers)\n\n```", "```py\ntry:\n    page_response = requests.get(page_link, timeout=5)\n    if page_response.status_code == 200:\n        # extract\n    else:\n        print(page_response.status_code)\n        # notify, try again\nexcept requests.Timeout as e:\n    print(\"It is time to timeout\")\n    print(str(e))\nexcept # other exception\n\n```", "```py\nproxies = {'http' : 'http://10.10.0.0:0000',  \n          'https': 'http://120.10.0.0:0000'}\npage_response = requests.get(page_link, proxies=proxies, timeout=5)  \n\n```", "```py\nimport numpy as np\nimport multiprocessing as multi\n\ndef chunks(n, page_list):\n    \"\"\"Splits the list into n chunks\"\"\"\n    return np.array_split(page_list,n)\n\ncpus = multi.cpu_count()\nworkers = []\npage_list = ['www.website.com/page1.html', 'www.website.com/page2.html'\n             'www.website.com/page3.html', 'www.website.com/page4.html']\n\npage_bins = chunks(cpus, page_list)\n\nfor cpu in range(cpus):\n    sys.stdout.write(\"CPU \" + str(cpu) + \"\\n\")\n    # Process that will send corresponding list of pages \n    # to the function perform_extraction\n    worker = multi.Process(name=str(cpu), \n                           target=perform_extraction, \n                           args=(page_bins[cpu],))\n    worker.start()\n    workers.append(worker)\n\nfor worker in workers:\n    worker.join()\n\ndef perform_extraction(page_ranges):\n    \"\"\"Extracts data, does preprocessing, writes the data\"\"\"\n    # do requests and BeautifulSoup\n    # preprocess the data\n    file_name = multi.current_process().name+'.txt'\n    # write into current process file\n\n```"]