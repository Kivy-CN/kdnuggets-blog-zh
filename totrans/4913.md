# PySpark SQL 备忘单：使用 Python 处理大数据

> 原文：[https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html](https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html)

**作者：Karlijn Willems，[DataCamp](https://www.datacamp.com/)。**

### 使用 Python 处理大数据

大数据无处不在，传统上由三大 V 特征定义：速度（Velocity）、种类（Variety）和体积（Volume）。大数据是快速的、多样的，并且体积巨大。作为数据科学家、数据工程师、数据架构师，……或你在数据科学行业中担任的其他角色，你迟早会接触到大数据，因为公司现在收集了大量的数据。

* * *

## 我们的前三名课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的 IT

* * *

数据并不总是意味着信息，这正是数据科学爱好者的你需要介入的地方。你可以利用 Apache Spark，“一个快速且通用的大规模数据处理引擎”，开始解决大数据给你和你所在公司带来的挑战。

然而，当你在使用 Spark 时，总会出现一些疑问。遇到这些问题时，可以查看 DataCamp 的 [Apache Spark 教程：ML with PySpark 教程](https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning)，或者免费下载 [备忘单](https://www.datacamp.com/community/blog/pyspark-sql-cheat-sheet)！

接下来，我们将深入探讨备忘单的结构和内容。

### PySpark 备忘单

PySpark 是 Spark 的 Python API，它将 Spark 编程模型暴露给 Python。Spark SQL 则是 PySpark 的一个模块，允许你以 DataFrames 的形式处理结构化数据。这与 RDDs 相对，RDDs 通常用于处理非结构化数据。

![PySpark 备忘单](../Images/f31b5ce6452a08f76098ca13c56b14cb.png)

**提示：** 如果你想了解 RDDs 和 DataFrames 之间的区别，以及 Spark DataFrames 如何与 pandas DataFrames 不同，你应该查看 [Apache Spark in Python: 初学者指南](https://www.datacamp.com/community/tutorials/apache-spark-python)。

### 初始化 SparkSession

如果你想开始使用 PySpark 的 Spark SQL，你需要首先启动一个 SparkSession：你可以使用它来创建 DataFrames，将 DataFrames 注册为表，执行 SQL 查询这些表和读取 parquet 文件。如果这一切对你来说都很陌生，不用担心——你将在本文后面详细阅读这些内容！

你通过首先从 pyspark 包的 sql 模块中导入 SparkSession 来开始一个 SparkSession。接下来，你可以初始化一个变量 spark，例如，不仅构建 SparkSession，还为应用程序命名，设置配置，然后使用 getOrCreate() 方法来获取现有的 SparkSession（如果已经存在），或者在没有的情况下创建一个！这个方法将非常有用，特别是作为未来的参考，因为它可以防止你同时运行多个 SparkSessions！

很酷，不是吗？

![PySpark cheat sheet](../Images/3c86b85673b094ab3ed9a4afa5142260.png)

### 数据检查

当你导入数据后，是时候通过使用一些内置的属性和方法来检查 Spark DataFrame。这样，你可以在开始操作 DataFrames 之前，更好地了解你的数据。

现在，你可能已经熟悉了此部分 cheat sheet 中提到的大部分方法和属性，比如 dtypes、head()、describe()、count()，等等。还有一些可能对你来说是新的方法，如 take() 或 printSchema() 方法，或 schema 属性。

![PySpark cheat sheet](../Images/2b87b5681e4fc864b638d73298521c50.png)

尽管如此，你会发现入门过程相当简单，因为你可以最大限度地利用你在数据科学包方面的知识。

### 重复值

在检查数据时，你可能会发现有一些重复值。为了解决这个问题，你可以使用 dropDuplicates() 方法，例如，来删除 Spark DataFrame 中的重复值。

### 查询

你可能还记得，使用 Spark DataFrames 的主要原因之一是你可以以更结构化的方式处理数据——查询就是这种以更结构化的方式处理数据的例子之一，无论你是在关系数据库中使用 SQL，还是在 No-SQL 数据库中使用类似 SQL 的语言，亦或是在 Pandas DataFrames 中使用 [query()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html?highlight=query#pandas.DataFrame.query) 方法，等等。

![PySpark cheat sheet](../Images/8a57af6b8937cff2c56c8dfbf857efe5.png)

现在我们谈论 Pandas DataFrames，你会发现 Spark DataFrames 遵循类似的原则：你使用方法来更好地了解数据。然而，在这种情况下，你不会使用 query()。相反，你需要使用其他方法来获得你想要的结果：一开始，select() 和 show() 将是你获取 DataFrames 信息时的好伙伴。

在这些方法中，你可以构建你的查询。与标准 SQL 一样，你可以在 select() 中指定你想要获取的列。你可以使用常规字符串或借助 DataFrame 自身指定列名，如以下示例：df.lastName 或 df[“firstName”]。

请注意，后者的方法在某些情况下会给你更多的自由度，例如你想要使用额外的函数如 isin() 或 startswith() 来精确指定要检索的信息。

除了 select()，你还可以利用 PySpark SQL 中的 functions 模块，例如在查询中指定 when 子句，如下表所示：

![PySpark 备忘单](../Images/d5234f75f8ad26d04f778d4b0bbf5d52.png)

总的来说，你可以看到，对于那些希望在 select()、help() 和 functions 模块的功能帮助下更大程度上处理和了解数据的人，有很多可能性。

### 添加、更新和删除列

你可能还希望查看如何向 Spark DataFrame 添加、更新或删除一些列。你可以使用 withColumn()、withColumnRenamed() 和 drop() 方法轻松做到这一点。你现在可能也知道在处理 Pandas DataFrames 时，你也有一个 drop() 方法可以使用。你可以在 [这里](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html?highlight=drop#pandas.DataFrame.drop) 阅读更多信息。

![PySpark 备忘单](../Images/751902d5d047e7357ce04767eb6020aa.png)

### GroupBy

再次，就像 Pandas DataFrames 一样，你可能会希望按某些值分组并聚合一些值 - 下面的示例展示了如何使用 groupBy() 方法，结合 count() 和 show() 来检索和展示数据，按年龄分组，以及拥有该特定年龄的人数。

![PySpark 备忘单](../Images/283cb11c84f4170ae04c5534ce466532.png)

### 更多相关主题

+   [PySpark 数据科学](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)

+   [使用 Pandera 进行 PySpark 应用程序的数据验证](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)

+   [使用 Python 数据清洗备忘单](https://www.kdnuggets.com/2023/02/data-cleaning-python-cheat-sheet.html)

+   [构建生成式 AI 应用程序的最佳 Python 工具备忘单](https://www.kdnuggets.com/2023/08/best-python-tools-generative-ai-cheat-sheet.html)

+   [Python 控制流备忘单](https://www.kdnuggets.com/2022/11/python-control-flow-cheatsheet.html)

+   [KDnuggets 新闻，7月5日：一个糟糕的数据科学项目 • 10个人工智能…](https://www.kdnuggets.com/2023/n24.html)
