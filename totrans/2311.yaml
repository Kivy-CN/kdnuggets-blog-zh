- en: Idiot’s Guide to Precision, Recall, and Confusion Matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/01/guide-precision-recall-confusion-matrix.html](https://www.kdnuggets.com/2020/01/guide-precision-recall-confusion-matrix.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/a95264811733914d21e66c25e993e334.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluating ML models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Regression models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RMSE is a good measure to evaluate how a [machine learning](https://hackernoon.com/tagged/machine-learning) model
    is performing.
  prefs: []
  type: TYPE_NORMAL
- en: If RMSE is significantly higher in test set than training-set — There is a good
    chance model is overfitting. (Make sure train and test set are from same/similar
    distribution)
  prefs: []
  type: TYPE_NORMAL
- en: What about Classification models?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Guess what, evaluating a Classification model is not that simple
  prefs: []
  type: TYPE_NORMAL
- en: '**But why?**'
  prefs: []
  type: TYPE_NORMAL
- en: You must be wondering *‘Can’t we just use **accuracy** of the model as the holy
    grail metric?’*
  prefs: []
  type: TYPE_NORMAL
- en: 'Accuracy is very important, but it might not be the best metric all the time.
    Let’s look at why with an example -:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Let’s say we are building a model which predicts if a bank loan will default
    or not**'
  prefs: []
  type: TYPE_NORMAL
- en: (The S&P/Experian Consumer Credit Default Composite Index reported a default
    rate of 0.91%)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a dummy model that always predicts that a loan will not default.
    Guess what would be the accuracy of this model?
  prefs: []
  type: TYPE_NORMAL
- en: '**===> 99.10%**'
  prefs: []
  type: TYPE_NORMAL
- en: Impressive, right? Well, the probability of a bank buying this model is absolute
    zero. ????
  prefs: []
  type: TYPE_NORMAL
- en: While our model has a stunning accuracy, this is an apt example where accuracy
    is definitely not the right metric.
  prefs: []
  type: TYPE_NORMAL
- en: '**If not accuracy, what else?**'
  prefs: []
  type: TYPE_NORMAL
- en: Along with accuracy, there are a bunch of other methods to evaluate the performance
    of a classification model
  prefs: []
  type: TYPE_NORMAL
- en: Confusion [matrix](https://hackernoon.com/tagged/matrix),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision, Recall
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ROC and AUC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before moving forward, we will look into some terms which will be constantly
    repeated and might make the whole thing an incomprehensible maze if not understood
    clearly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Easy right?**'
  prefs: []
  type: TYPE_NORMAL
- en: Well, not the same feeling after I saw all these ????
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3276acf85b710c28314fffb54871e9ad.png)'
  prefs: []
  type: TYPE_IMG
- en: But then as they say — Every cloud has a silver lining
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand it one by one, starting with the fundamental terms.
  prefs: []
  type: TYPE_NORMAL
- en: The Positives and Negatives — TP, TN, FP, FN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I use this hack to remember the meaning of each of these correctly.
  prefs: []
  type: TYPE_NORMAL
- en: (Binary classification problem. Ex — Predicting if a bank loan will default)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b31ea3d15baa750ff57a927af3dfe5ed.png)'
  prefs: []
  type: TYPE_IMG
- en: So what is the meaning of a **True Negative**?
  prefs: []
  type: TYPE_NORMAL
- en: '*True Negative*: We were right when we predicted that a loan would not default.'
  prefs: []
  type: TYPE_NORMAL
- en: '*False Positive*: We falsely predicted that a loan would default.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Let''s reinforce what we learned**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c84748fb19c7968163d993535a49af55.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Another picture which stamps it in my mind.*'
  prefs: []
  type: TYPE_NORMAL
- en: Confusion Matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As now we are familiar with TP, TN, FP, FN — It will be very easy to understand
    what confusion matrix is.
  prefs: []
  type: TYPE_NORMAL
- en: It is a summary table showing how good our model is at predicting examples of
    various classes. Axes here are predicted-lables vs actual-labels.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cba748e7b6fb6e7777777b687ce98b54.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Confusion matrix for a classification model predicting if a loan will default
    or not.*'
  prefs: []
  type: TYPE_NORMAL
- en: Precision and Recall
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Precision — **Also called Positive predictive value'
  prefs: []
  type: TYPE_NORMAL
- en: The ratio of correct positive predictions to the total predicted positives.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8fcfd2cde808ff5daa0d62a073cecae5.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Recall — **Also called Sensitivity, Probability of Detection, True Positive
    Rate'
  prefs: []
  type: TYPE_NORMAL
- en: The ratio of correct positive predictions to the total positives examples.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f39a6fc6cf97cb01b4f181c1fcd031c.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Understanding**'
  prefs: []
  type: TYPE_NORMAL
- en: To understand *Precision* and *Recall*, let’s take an example of Search. Think
    about the search box on the Amazon home page.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c44811b616f7c00a17df8d8c8ee4716.png)'
  prefs: []
  type: TYPE_IMG
- en: '*The **precision** is the proportion of relevant results in the list of all
    returned search results. The **recall** is the ratio of the relevant results returned
    by the search engine to the total number of the relevant results that could have
    been returned.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In our case of predicting if a loan would default — It would be better to have
    a high Recall as the banks don’t want to lose money and would be a good idea to
    alarm the bank even if there is a slight doubt about defaulter.
  prefs: []
  type: TYPE_NORMAL
- en: Low precision, in this case, might be okay.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note*: Mostly, we have to pick one over other. It’s almost impossible to have
    both high Precision and Recall.'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Talking about accuracy, our favourite metric!
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy is defined as the ratio of correctly predicted examples by the total
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3aea35e572ca3d665399ba2bbdd4a93.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In terms of confusion matrix it is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f628e665090e9476c0069424a991e196.png)'
  prefs: []
  type: TYPE_IMG
- en: Remember, accuracy is a very useful metric when all the classes are equally
    important. But this might not be the case if we are predicting if a patient has
    cancer. In this example, we can probably tolerate FPs but not FNs.
  prefs: []
  type: TYPE_NORMAL
- en: ROC curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A ROC curve (receiver operating characteristic curve) graph shows the performance
    of a classification model at all classification thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: '(Using thresholds: Say, if you want to compute TPR and FPR for the threshold
    equal to 0.7, you apply the model to each example, get the score, and, if the
    score if higher than or equal to 0.7, you predict the positive class; otherwise,
    you predict the negative class)'
  prefs: []
  type: TYPE_NORMAL
- en: 'It plots 2 parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True positive rate **(Recall)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/16bfd0be67c4586839a9df445fdf0da2.png)'
  prefs: []
  type: TYPE_IMG
- en: '**False Positive rate**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/37d6dcee31b3a382d55b42cc732721a9.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Tells what % of people who were not defaulter were identified as defaulter.*'
  prefs: []
  type: TYPE_NORMAL
- en: predictions to the total predicted positives.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/44d0b599e62e393949b412b3e439b055.png)'
  prefs: []
  type: TYPE_IMG
- en: '*A typical ROC curve.*'
  prefs: []
  type: TYPE_NORMAL
- en: Lowering the classification threshold classifies more items as positive, thus
    increasing both False Positives and True Positives.
  prefs: []
  type: TYPE_NORMAL
- en: AUC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**AUC** stands for *Area under the ROC Curve. *It provides an aggregate measure
    of performance across all possible classification thresholds.'
  prefs: []
  type: TYPE_NORMAL
- en: The higher the **area under the ROC curve **(AUC), the better the classifier.
    A perfect classifier would have an AUC of 1\. Usually, if your model behaves well,
    you obtain a good classifier by selecting the value of the threshold that gives
    TPR close to 1 while keeping FPR near 0.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, we saw how a classification model could be effectively evaluated,
    especially in situations where looking at standalone accuracy is not enough. We
    understood concepts like TP, TN, FP, FN, Precision, Recall, Confusion matrix,
    ROC and AUC. I hope it made things clearer!
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/swlh/idiots-guide-to-precision-recall-and-confusion-matrix-b32d36463556).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Vipul Jain](https://www.linkedin.com/in/jnvipul/)** is a data scientist
    with a focus on machine learning with experience building end-to-end data products
    from ideation to production.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Confusion Matrix, Precision, and Recall Explained](https://www.kdnuggets.com/2022/11/confusion-matrix-precision-recall-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, November 16: How LinkedIn Uses Machine Learning •…](https://www.kdnuggets.com/2022/n45.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Classification Metrics Walkthrough: Logistic Regression with…](https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Visualizing Your Confusion Matrix in Scikit-learn](https://www.kdnuggets.com/2022/09/visualizing-confusion-matrix-scikitlearn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Vector and Matrix Norms with NumPy Linalg Norm](https://www.kdnuggets.com/2023/05/vector-matrix-norms-numpy-linalg-norm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Sparse Matrix Representation in Python](https://www.kdnuggets.com/2020/05/sparse-matrix-representation-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
