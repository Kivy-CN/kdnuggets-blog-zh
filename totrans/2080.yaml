- en: How to Build and Train a Transformer Model from Scratch with Hugging Face Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/how-to-build-and-train-a-transformer-model-from-scratch-with-hugging-face-transformers](https://www.kdnuggets.com/how-to-build-and-train-a-transformer-model-from-scratch-with-hugging-face-transformers)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Hugging Face Transformers library](../Images/7c2590f4db8e141e3008c7f98ba82fe5.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Image by Editor | Midjourney
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face Transformers library provides tools for easily loading and using
    pre-trained Language Models (LMs) based on the transformer architecture. But,
    did you know this library also allows you to implement and train your transformer
    model from scratch? This tutorial illustrates how through a step-by-step sentiment
    classification example.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '**Important note:** Training a transformer model from scratch is computationally
    expensive, with a training loop typically requiring hours to say the least. To
    run the code in this tutorial, it is highly recommended to have access to high-performance
    computing resources, be it on-premises or via a cloud provider.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Step-by-Step Process
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Initial Setup and Dataset Loading
  id: totrans-13
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Depending on the type of Python development environment you are working on,
    you may need to install Hugging Face's **transformers** and **datasets** libraries,
    as well as the **accelerate** library to train your transformer model in a distributed
    computing setting.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once the necessary libraries are installed, let''s load the [emotions dataset](https://huggingface.co/datasets/jeffnyman/emotions)
    for sentiment classification of Twitter messages from Hugging Face hub:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using the data for training a transformer-based LM requires tokenizing the text.
    The following code initializes a BERT tokenizer (BERT is a family of transformer
    models suitable for text classification tasks), defines a function to tokenize
    text data with padding and truncation, and applies it to the dataset in batches.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Before moving on to initialize the transformer model, let's verify the unique
    labels in the dataset. Having a verified set of existing class labels helps prevent
    GPU-related errors during training by verifying label consistency and correctness.
    We will use this label set later on.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, we create and define a model configuration, and then instantiate the transformer
    model with this configuration. This is where we specify hyperparameters about
    the transformer architecture like embedding size, number of attention heads, and
    the previously calculated set of unique labels, key in building the final output
    layer for sentiment classification.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建并定义一个模型配置，然后用这个配置实例化 transformer 模型。在这里，我们指定关于 transformer 架构的超参数，比如嵌入大小、注意力头的数量，以及先前计算出的唯一标签集合，这些都是构建情感分类最终输出层的关键。
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We are almost ready to train our transformer model. It just remains to instantiate
    two necessary instances: **TrainingArguments**, with specifications about the
    training loop such as the number of epochs, and **Trainer**, which glues together
    the model instance, the training arguments, and the data utilized for training
    and validation.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎准备好训练我们的 transformer 模型了。只剩下实例化两个必要的实例：**TrainingArguments**，它包含关于训练循环的规范，如
    epoch 数量，以及 **Trainer**，它将模型实例、训练参数和用于训练和验证的数据粘合在一起。
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Time to train the model, sit back, and relax. Remember this instruction will
    take a significant amount of time to complete:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型的时间到了，坐下来放松一下吧。记住，这个过程将需要相当长的时间来完成：
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once trained, your transformer model should be ready for passing in input examples
    for sentiment prediction.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，你的 transformer 模型应该已经准备好接收输入示例进行情感预测。
- en: Troubleshooting
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 故障排除
- en: 'If problems appear or persist when executing the training loop or during its
    setup, you may need to inspect the configuration of the GPU/CPU resources being
    used. For instance, if using a CUDA GPU, adding these instructions at the beginning
    of your code can help prevent errors in the training loop:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在执行训练循环或设置过程中出现或持续存在问题，你可能需要检查所使用的 GPU/CPU 资源的配置。例如，如果使用 CUDA GPU，可以在代码开始处添加这些指令，以帮助防止训练循环中的错误：
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: These lines disable the GPU and make CUDA operations synchronous, providing
    more immediate and accurate error messages for debugging.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这些行禁用 GPU 并使 CUDA 操作同步，从而提供更及时和准确的错误消息以供调试。
- en: 'On the other hand, if you are trying this code in a Google Colab instance,
    chances are this error message shows up during execution, even if you have previously
    installed the accelerate library:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你在 Google Colab 实例中尝试这段代码，执行期间可能会出现这个错误信息，即使你之前已经安装了 accelerate 库：
- en: '[PRE8]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To address this issue, try restarting your session in the ''Runtime'' menu:
    the accelerate library typically requires resetting the run environment after
    being installed.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，尝试在 'Runtime' 菜单中重启你的会话：accelerate 库通常需要在安装后重置运行环境。
- en: Summary and Wrap-Up
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结与结束
- en: 'This tutorial showcased the key steps to build your transformer-based LM from
    scratch using Hugging Face libraries. The main steps and elements involved can
    be summarized as:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程展示了从零开始使用 Hugging Face 库构建基于 transformer 的语言模型的关键步骤。主要步骤和涉及的元素可以总结如下：
- en: Loading the dataset and tokenizing the text data.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载数据集并标记文本数据。
- en: Initializing your model by using a model configuration instance for the type
    of model (language task) it is intended for, e.g. **BertConfig**.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型配置实例初始化你的模型，以适应特定的模型类型（语言任务），例如 **BertConfig**。
- en: Setting up a **Trainer** and **TrainingArguments** instances and running the
    training loop.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置 **Trainer** 和 **TrainingArguments** 实例并运行训练循环。
- en: As a next learning step, we encourage you to explore how to make predictions
    and inferences with your newly trained model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 作为下一个学习步骤，我们鼓励你探索如何使用你新训练的模型进行预测和推断。
- en: '[](https://www.linkedin.com/in/ivanpc/)****[Iván Palomares Carrascosa](https://www.linkedin.com/in/ivanpc/)****
    is a leader, writer, speaker, and adviser in AI, machine learning, deep learning
    & LLMs. He trains and guides others in harnessing AI in the real world.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.linkedin.com/in/ivanpc/)****[Iván Palomares Carrascosa](https://www.linkedin.com/in/ivanpc/)****
    是人工智能、机器学习、深度学习和大型语言模型领域的领导者、作者、演讲者和顾问。他培训和指导他人如何在现实世界中利用人工智能。'
- en: More On This Topic
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关信息
- en: '[How to Fine-Tune BERT for Sentiment Analysis with Hugging Face Transformers](https://www.kdnuggets.com/how-to-fine-tune-bert-sentiment-analysis-hugging-face-transformers)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 Hugging Face Transformers 对 BERT 进行情感分析微调](https://www.kdnuggets.com/how-to-fine-tune-bert-sentiment-analysis-hugging-face-transformers)'
- en: '[How to Use GPT for Generating Creative Content with Hugging Face…](https://www.kdnuggets.com/how-to-use-gpt-for-generating-creative-content-with-hugging-face-transformers)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 GPT 生成创意内容与 Hugging Face…](https://www.kdnuggets.com/how-to-use-gpt-for-generating-creative-content-with-hugging-face-transformers)'
- en: '[Building a Recommendation System with Hugging Face Transformers](https://www.kdnuggets.com/building-a-recommendation-system-with-hugging-face-transformers)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Hugging Face Transformers 构建推荐系统](https://www.kdnuggets.com/building-a-recommendation-system-with-hugging-face-transformers)'
- en: '[Using Hugging Face Transformers for Emotion Detection in Text](https://www.kdnuggets.com/using-hugging-face-transformers-for-emotion-detection-in-text)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Hugging Face Transformers 进行文本情感检测](https://www.kdnuggets.com/using-hugging-face-transformers-for-emotion-detection-in-text)'
- en: '[Build AI Chatbot in 5 Minutes with Hugging Face and Gradio](https://www.kdnuggets.com/2023/06/build-ai-chatbot-5-minutes-hugging-face-gradio.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Hugging Face 和 Gradio 在 5 分钟内构建 AI 聊天机器人](https://www.kdnuggets.com/2023/06/build-ai-chatbot-5-minutes-hugging-face-gradio.html)'
- en: '[A Guide to Train an Image Classification Model Using Tensorflow](https://www.kdnuggets.com/2022/12/guide-train-image-classification-model-tensorflow.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Tensorflow 训练图像分类模型指南](https://www.kdnuggets.com/2022/12/guide-train-image-classification-model-tensorflow.html)'
