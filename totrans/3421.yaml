- en: Introduction to Local Interpretable Model-Agnostic Explanations (LIME)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2016/08/introduction-local-interpretable-model-agnostic-explanations-lime.html](https://www.kdnuggets.com/2016/08/introduction-local-interpretable-model-agnostic-explanations-lime.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**By Marco Tulio Ribeiro*, Sameer Singh^, and Carlos Guestrin*.**'
  prefs: []
  type: TYPE_NORMAL
- en: '* University of Washington'
  prefs: []
  type: TYPE_NORMAL
- en: ^ University of Califronia, Irvine
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning is at the core of many recent advances in science and technology.
    With computers [beating professionals in games like Go](https://deepmind.com/alpha-go.html),
    many people have started asking if machines would also make for better [drivers](https://www.google.com/selfdrivingcar/)or
    even better doctors.
  prefs: []
  type: TYPE_NORMAL
- en: In many applications of machine learning, users are asked to trust a model to
    help them make decisions. A doctor will certainly not operate on a patient simply
    because “the model said so.” Even in lower-stakes situations, such as when choosing
    a movie to watch from Netflix, a certain measure of trust is required before we
    surrender hours of our time based on a model. Despite the fact that many machine
    learning models are black boxes, understanding the rationale behind the model's
    predictions would certainly help users decide when to trust or not to trust their
    predictions. An example is shown in Figure 1, in which a model predicts that a
    certain patient has the flu. The prediction is then explained by an "explainer"
    that highlights the symptoms that are most important to the model. With this information
    about the rationale behind the model, the doctor is now empowered to trust the
    model—or not.
  prefs: []
  type: TYPE_NORMAL
- en: '[![Explaining individual predictions to a human decision-maker](../Images/7dfadf0c6bcefc6b7f93ac09df8bee1e.png)](https://d3ansictanv2wj.cloudfront.net/figure1-a9533a3fb9bb9ace6ee96b4cdc9b6bcb.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 1. Explaining individual predictions to a human decision-maker. Source:
    Marco Tulio Ribeiro.*In a sense, every time an engineer uploads a machine learning
    model to production, the engineer is implicitly trusting that the model will make
    sensible predictions. Such assessment is usually done by looking at held-out accuracy
    or some other aggregate measure. However, as anyone who has ever used machine
    learning in a real application can attest, such metrics can be very misleading.
    Sometimes data that shouldn''t be available accidentally leaks into the training
    and into the held-out data (e.g., looking into the future). Sometimes the model
    makes mistakes that are too embarrassing to be acceptable. These and many other
    tricky problems indicate that understanding the model''s predictions can be an
    additional useful tool when deciding if a model is trustworthy or not, because
    humans often have good intuition and business intelligence that is hard to capture
    in evaluation metrics. Assuming a “pick step” in which certain representative
    predictions are selected to be explained to the human would make the process similar
    to the one illustrated in Figure 2.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![Explaining a model to a human decision-maker](../Images/ca1507e7b92934a5cf58507d7727675a.png)](https://d3ansictanv2wj.cloudfront.net/figure2-802e0856e423b6bf8862843102243a8b.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2. Explaining a model to a human decision-maker. Source: Marco Tulio
    Ribeiro.*In ["Why Should I Trust You?" Explaining the Predictions of Any Classifier](http://arxiv.org/pdf/1602.04938.pdf),
    a joint work by [Marco Tulio Ribeiro](http://homes.cs.washington.edu/~marcotcr/), [Sameer
    Singh](http://sameersingh.org/), and [Carlos Guestrin](https://homes.cs.washington.edu/~guestrin/) (to
    appear in ACM''s [Conference on Knowledge Discovery and Data Mining](http://www.kdd.org/kdd2016/) -[-](http://www.kdd.org/kdd2016/)KDD2016),
    we explore precisely the question of trust and explanations. We propose Local
    Interpretable Model-Agnostic Explanations (LIME), a technique to explain the predictions
    of *any* machine learning classifier, and evaluate its usefulness in various tasks
    related to trust.'
  prefs: []
  type: TYPE_NORMAL
- en: Intuition behind LIME
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because we want to be model-agnostic, what we can do to learn the behavior of
    the underlying model is to perturb the input and see how the predictions change.
    This turns out to be a benefit in terms of interpretability, because we can perturb
    the input by changing components that make sense to humans (e.g., words or parts
    of an image), even if the model is using much more complicated components as features
    (e.g., word embeddings).
  prefs: []
  type: TYPE_NORMAL
- en: We generate an explanation by approximating the underlying model by an interpretable
    one (such as a linear model with only a few non-zero coefficients), learned on
    perturbations of the original instance (e.g., removing words or hiding parts of
    the image). The key intuition behind LIME is that it is much easier to approximate
    a black-box model by a simple model *locally* (in the neighborhood of the prediction
    we want to explain), as opposed to trying to approximate a model globally. This
    is done by weighting the perturbed images by their similarity to the instance
    we want to explain. Going back to our example of a flu prediction, the three highlighted
    symptoms may be a faithful approximation of the black-box model for patients who
    look like the one being inspected, but they probably do not represent how the
    model behaves for all patients.
  prefs: []
  type: TYPE_NORMAL
- en: See Figure 3 for an example of how LIME works for image classification. Imagine
    we want to explain a classifier that predicts how likely it is for the image to
    contain a tree frog. We take the image on the left and divide it into interpretable
    components (contiguous superpixels).
  prefs: []
  type: TYPE_NORMAL
- en: '[![Transforming an image into interpretable components](../Images/36d4700080d6db03a84ea0fee05bd526.png)](https://d3ansictanv2wj.cloudfront.net/figure3-2cea505fe733a4713eeff3b90f696507.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3. Transforming an image into interpretable components. Sources: Marco
    Tulio Ribeiro, [Pixabay](https://pixabay.com/en/love-valentine-s-day-pose-heart-903178/).*As
    illustrated in Figure 4, we then generate a data set of perturbed instances by
    turning some of the interpretable components “off” (in this case, making them
    gray). For each perturbed instance, we get the probability that a tree frog is
    in the image according to the model. We then learn a simple (linear) model on
    this data set, which is locally weighted—that is, we care more about making mistakes
    in perturbed instances that are more similar to the original image. In the end,
    we present the superpixels with highest positive weights as an explanation, graying
    out everything else.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![Explaining a prediction with LIME](../Images/e59c638c0e76de599a57f43ef6a885d4.png)](https://d3ansictanv2wj.cloudfront.net/figure4-99d9ea184dd35876e0dbae81f6fce038.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4. Explaining a prediction with LIME. Sources: Marco Tulio Ribeiro, [Pixabay](https://pixabay.com/en/love-valentine-s-day-pose-heart-903178/).*'
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We used LIME to explain a myriad of classifiers (such as [random forests](https://en.wikipedia.org/wiki/Random_forest),[support
    vector machines (SVM)](https://en.wikipedia.org/wiki/Support_vector_machine),
    and [neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network))
    in the text and image domains. Here are a few examples of the generated explanations.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, an example from text classification. The famous [20 newsgroups data
    set](http://qwone.com/~jason/20Newsgroups/) is a benchmark in the field, and has
    been used to compare different models in several papers. We take two classes that
    are hard to distinguish because they share many words: Christianity and atheism.
    Training a random forest with 500 trees, we get a test set accuracy of 92.4%,
    which is surprisingly high. If accuracy was our only measure of trust, we would
    definitely trust this classifier. However, let’s look at an explanation in Figure
    5 for an arbitrary instance in the test set (a one liner in Python with[our open
    source package](https://github.com/marcotcr/lime)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![Explanation for a prediction in the 20 newsgroups data set](../Images/1b8744c7b6325991e013951aaa09340b.png)](https://d3ansictanv2wj.cloudfront.net/figure5-cd7d3c5128549df1e957bf8f9f93bb2b.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5. Explanation for a prediction in the 20 newsgroups data set. Source:
    Marco Tulio Ribeiro.*This is a case in which the classifier predicts the instance
    correctly, but for the wrong reasons. Additional exploration shows us that the
    word "posting" (part of the email header) appears in 21.6% of the examples in
    the training set but only two times in the class “Christianity.” This is also
    the case in the test set, where the word appears in almost 20% of the examples
    but only twice in “Christianity.” This kind of artifact in the data set makes
    the problem much easier than it is in the real world, where we wouldn''t expect
    such patterns to occur. These insights become easy once you understand what the
    models are actually doing, which in turn leads to models that generalize much
    better.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a second example, we explain [Google''s Inception neural network](https://github.com/google/inception) on
    arbitrary images. In this case, illustrated in Figure 6, the classifier predicts
    “tree frog” as the most likely class, followed by "pool table" and "balloon" with
    lower probabilities. The explanation reveals that the classifier primarily focuses
    on the frog''s face as an explanation for the predicted class. It also sheds light
    on why "pool table" has non-zero probability: the frog''s hands and eyes bear
    a resemblance to billiard balls, especially on a green background. Similarly,
    the heart bears a resemblance to a red balloon.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![Explanation for a prediction from Inception](../Images/1d97052096ab8d53d15988f05e4a327e.png)](https://d3ansictanv2wj.cloudfront.net/Figure-6-c8db425eefec7cff5a3cf035a40d8841.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6. Explanation for a prediction from Inception. The top three predicted
    classes are "tree frog," "pool table," and "balloon." Sources: Marco Tulio Ribeiro,
    Pixabay ([frog](https://pixabay.com/en/love-valentine-s-day-pose-heart-903178/), [billiards](https://pixabay.com/en/billiards-dutch-colors-pool-cue-1263076/), [hot
    air balloon](https://pixabay.com/en/hot-air-balloon-balloon-1378998/)).*In the
    experiments in [our research paper](http://arxiv.org/abs/1602.04938), we demonstrate
    that both machine learning experts and lay users greatly benefit from explanations
    similar to Figures 5 and 6 and are able to choose which models generalize better,
    improve models by changing them, and get crucial insights into the models'' behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Trust is crucial for effective human interaction with machine learning systems,
    and we think explaining individual predictions is an effective way of assessing
    trust. LIME is an efficient tool to facilitate such trust for machine learning
    practitioners and a good choice to add to their tool belts (did we mention we
    have [an open source project](https://github.com/marcotcr/lime)?), but there is
    still plenty of work to be done to better explain machine learning models. We're
    excited to see where this research direction will lead us. The video below provides
    an overview of LIME, with more details available in [our paper](http://arxiv.org/abs/1602.04938).
  prefs: []
  type: TYPE_NORMAL
- en: '**[Marco Tulio Ribeiro](https://homes.cs.washington.edu/~marcotcr/)** is a
    PhD student at the University of Washington working under Carlos Guestrin. His
    research focus is making it easier for humans to understand and interact with
    machine learning models.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Carlos Guestrin](https://twitter.com/guestrin)** is the CEO of Turi, Inc.,
    and the Amazon Professor of Machine Learning in Computer Science & Engineering
    at the University of Washington. A world-recognized leader in the field of Machine
    Learning, Carlos was named one of the 2008 “Brilliant 10" by Popular Science Magazine,
    received the 2009 IJCAI Computers and Thought Award for his contributions to Artificial
    Intelligence, and a Presidential Early Career Award for Scientists and Engineers
    (PECASE).'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Dr. Sameer Singh](http://sameersingh.org/)** is an Assistant Professor of
    Computer Science at University of California, Irvine, conducting research on large-scale,
    interactive machine learning and natural language processing.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Contest Winner: Winning the AutoML Challenge with Auto-sklearn](/2016/08/winning-automl-challenge-auto-sklearn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Approaching (Almost) Any Machine Learning Problem](/2016/08/approaching-almost-any-machine-learning-problem.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TPOT: A Python Tool for Automating Data Science](/2016/05/tpot-python-automating-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Explain NLP Models with LIME](https://www.kdnuggets.com/2022/01/explain-nlp-models-lime.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GPT4All is the Local ChatGPT for your Documents and it is Free!](https://www.kdnuggets.com/2023/06/gpt4all-local-chatgpt-documents-free.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LangChain + Streamlit + Llama: Bringing Conversational AI to Your…](https://www.kdnuggets.com/2023/08/langchain-streamlit-llama-bringing-conversational-ai-local-machine.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Octoparse 8.5: Empowering Local Scraping and More](https://www.kdnuggets.com/2022/02/octoparse-85-empowering-local-scraping.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Steps to Running a Small Language Model on a Local CPU](https://www.kdnuggets.com/7-steps-to-running-a-small-language-model-on-a-local-cpu)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
