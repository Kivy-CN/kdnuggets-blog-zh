- en: Exploring Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索递归神经网络
- en: 原文：[https://www.kdnuggets.com/2017/12/exploring-recurrent-neural-networks.html](https://www.kdnuggets.com/2017/12/exploring-recurrent-neural-networks.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/12/exploring-recurrent-neural-networks.html](https://www.kdnuggets.com/2017/12/exploring-recurrent-neural-networks.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By Packtpub.**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 Packtpub 提供。**'
- en: '**In this tutorial, taken from** [**Hands-on Deep Learning with Theano**](https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow?utm_source=kdnuggets&utm_medium=referral&utm_campaign=outreach)
    **by Dan Van Boxel, we’ll be exploring recurrent neural networks. We’ll start
    off by looking at the basics, before looking at RNNs through a motivating weather
    modeling problem. We’ll also implement and train an RNN in TensorFlow.**'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**在本教程中，选自** [**《Theano 深度学习实战》**](https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow?utm_source=kdnuggets&utm_medium=referral&utm_campaign=outreach)
    **由 Dan Van Boxel 编写，我们将探索递归神经网络。我们将从基础知识开始，然后通过一个激励性的天气建模问题来研究 RNNs。我们还将在 TensorFlow
    中实现并训练一个 RNN。**'
- en: '![](../Images/84d853461087d54cfc8652330bb59d7b.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84d853461087d54cfc8652330bb59d7b.png)'
- en: In a typical model, you have some X input features and some Y output you want
    to predict. We usually consider our different training samples as independent
    observations. So, the features from data point one shouldn't impact the prediction
    for data point two. But what if our data points are correlated? The most common
    example is that each data point, **Xt**, represents features collected at time
    **t**. It's natural to suppose that the features at time t and time t+1 will both
    be important to the prediction at time t+1\. In other words, history matters.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个典型的模型中，你有一些 X 输入特征和一些 Y 输出你想要预测。我们通常认为不同的训练样本是独立的观测值。因此，数据点一的特征不应该影响数据点二的预测。但如果我们的数据点是相关的呢？最常见的例子是每个数据点，**Xt**，表示在时间**t**收集的特征。自然地，我们可以假设时间
    t 和 t+1 的特征对时间 t+1 的预测都很重要。换句话说，历史是重要的。
- en: Now, when modeling, you could just include twice as many input features, adding
    the previous time step to the current ones, and computing twice as many input
    weights. But, if you're going through all the effort of building a neural network
    to compute transform features, it would be nice if you could use the intermediate
    features from the previous time step, in the current time step network.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在建模时，你可以简单地包含两倍的输入特征，将前一个时间步骤的特征添加到当前特征中，并计算两倍的输入权重。但是，如果你要花费所有精力来构建一个神经网络来计算变换特征，那么最好在当前时间步骤网络中使用来自前一个时间步骤的中间特征。
- en: RNNs do exactly this. Consider your input, **Xt** as usual, but add in some
    state, **St-1** that comes from the previous time step, as additional features.
    Now you can compute weights as usual to predict **Yt**, and you produce a new
    internal state, **St**, to be used in the next time step. For the first time step,
    it's typical to use a default or zero initial state. Classic RNNs are literally
    this simple, but there are more advanced structures common in literature today,
    such as gated recurrent units and long short-term memory circuits. These are beyond
    the scope of this tutorial, but work on the same principles and generally apply
    to the same types of problems.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs 正是如此。考虑你的输入，**Xt**，如往常一样，但添加一些状态，**St-1**，来自前一个时间步骤，作为额外特征。现在你可以像往常一样计算权重来预测**Yt**，并生成一个新的内部状态，**St**，以便在下一个时间步骤中使用。对于第一个时间步骤，通常使用默认或零初始状态。经典的
    RNNs 就是这么简单，但今天文献中有更先进的结构，如门控递归单元和长短期记忆电路。这些超出了本教程的范围，但遵循相同的原则，并通常适用于相同类型的问题。
- en: Modeling the weights
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权重建模
- en: 'You might be wondering how we''ll compute weights with all these dependents
    on the previous time step. Computing the gradients does involve recursing back
    through the time computation, but fear not, TensorFlow handles the tedious stuff
    and let''s us do the modeling:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，我们如何计算所有这些对前一时间步骤的依赖权重。计算梯度确实涉及回溯时间计算，但不用担心，TensorFlow 处理繁琐的部分，让我们可以进行建模：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To use RNNs, we need a data modeling problem with a time component.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 RNNs，我们需要一个具有时间组件的数据建模问题。
- en: The font classification problem isn't really appropriate here. So, let's take
    a look at some weather data. The **weather.npz** file is a collection of weather
    station data from a city in the United States over several decades. The **daily**
    array contains measurements from every day of the year. There are six columns
    to the data, starting with the date. Next, is the precipitation, measuring any
    rainfall in inches that day. After this, come two columns for snow—the first is
    measured snow currently on the ground, while the latter is snowfall on that day,
    again, in inches. Finally, we have some temperature information, the daily high
    and the daily low in degrees Fahrenheit.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 字体分类问题在这里并不适用。所以，让我们来看一些天气数据。**weather.npz** 文件是来自美国某城市几十年的天气站数据的集合。**daily**
    数组包含了每一天的测量数据。数据有六列，从日期开始。接下来是降水量，测量当天的降雨量（单位为英寸）。之后是两列关于雪的数据——第一列是地面上当前的积雪量，第二列是当天的降雪量，单位仍然是英寸。最后，我们还有一些温度信息，包括每日的最高温度和最低温度，单位为华氏度。
- en: The **weekly** array, which we'll use, is a weekly summary of the daily information.
    We'll use the middle date to indicate the week, then, we'll sum up all rainfall
    for the week. For snow, however, we'll average the snow on the ground, since it
    doesn't make sense to add snow from one cold day to the same snow sitting on the
    ground the next day. Snowfall though, we'll total for the week, just like rain.
    Finally, we'll average the high and low temperatures for the week respectively.
    Now that you've got a handle on the dataset, what shall we do with it? One interesting
    time-based modeling problem would be trying to predict the season of a particular
    week using it's weather information and the history of previous weeks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的 **weekly** 数组是每日信息的每周总结。我们会使用中间的日期来表示一周，然后，我们会汇总该周的所有降雨量。然而，对于雪，我们将对地上的雪进行平均，因为将某一天的雪加到第二天仍然在地上的雪上是没有意义的。不过，降雪量我们会按周总和计算，就像雨一样。最后，我们将分别计算一周的最高温度和最低温度的平均值。现在你已经掌握了数据集，我们该怎么做呢？一个有趣的基于时间的建模问题是尝试预测某一周的季节，利用其天气信息和前几周的历史。
- en: In the Northern Hemisphere, in the United States, it's warmer during the months
    of June through August and colder during December through February, with transitions
    in between. Spring months tend to be rainy, and winter often includes snow. While
    one week can be highly variable, a history of weeks should provide some predictive
    power.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在北半球，在美国，6 月至 8 月的月份较暖，而 12 月至 2 月的月份则较冷，中间有过渡期。春季通常较为多雨，冬季则常常包括降雪。虽然一周的天气可能变化多端，但一段时间的天气历史应能提供一些预测能力。
- en: Understanding RNNs
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解 RNN
- en: 'First, let''s read in the data from a compressed NumPy array. The `weather.npz`
    file happens to include the daily data as well, if you wish to explore your own
    model; `np.load` reads both arrays into a dictionary and will set weekly to be
    our data of interest; `num_weeks` is naturally how many data points we have, here,
    several decades worth of information:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从压缩的 NumPy 数组中读取数据。`weather.npz` 文件恰好也包括每日数据，如果你想探索你自己的模型的话；`np.load`
    会将两个数组读入一个字典，并将 weekly 设置为我们关注的数据；`num_weeks` 自然是我们拥有的数据点数量，在这里，有几几十年的信息：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To format the weeks, we use a Python `datetime.datetime` object reading the
    storage string in year month day format:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了格式化这些周，我们使用一个 Python `datetime.datetime` 对象来读取以年、月、日格式存储的字符串：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can use the date of each week to assign its season. For this model, because
    we''re looking at weather data, we use the meteorological season rather than the
    common astronomical season. Thankfully, this is easy to implement with the Python
    function. Grab the month from the `datetime` object and we can directly compute
    this season. Spring, season zero, is March through May, summer is June through
    August, autumn is September through November, and finally, winter is December
    through February. The following is the simple function that just evaluates the
    month and implements that:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用每周的日期来分配其季节。对于这个模型，因为我们关注的是天气数据，我们使用气象季节而非常见的天文季节。幸运的是，这可以通过 Python 函数轻松实现。提取
    `datetime` 对象中的月份，我们可以直接计算出这个季节。春季，季节零，是 3 月至 5 月，夏季是 6 月至 8 月，秋季是 9 月至 11 月，最后冬季是
    12 月至 2 月。以下是一个简单的函数，它只是评估月份并实现这一点：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s note that we have four seasons and five input variables and, say, 11
    values in our history state:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意我们有四个季节和五个输入变量，并且，假设我们有 11 个历史状态值：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now you''re ready to compute the labels:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你准备计算标签了：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We do this directly in one-hot format, by making an all-zeroes array and putting
    a one in the position of the assign season.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们直接以独热格式进行此操作，通过创建一个全零数组，并在分配季节的位置放置一个 1。
- en: Cool! You just summarized decades of time with a few commands.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 很酷！你刚刚用几个命令总结了几十年的时间。
- en: 'As these input features measure very different things, namely rainfall, snow,
    and temperature, on very different scales, we should take care to put them all
    on the same scale. In the following code, we grab the input features, skipping
    the date column of course, and subtract the average to center all features at
    zero:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些输入特征测量的是非常不同的事物，即降雨、雪和温度，并且量纲也不同，我们应当注意将它们统一到相同的尺度。在下面的代码中，我们获取输入特征，当然跳过日期列，并减去平均值以将所有特征中心化到零：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Then, we scale each feature by dividing by its standard deviation. This accounts
    for temperatures ranging roughly 0 to 100, while rainfall only changes between
    about 0 and 10\. Nice work on the data prep! It isn't always fun, but it's a key
    part of machine learning and TensorFlow.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过除以标准差来缩放每个特征。这考虑到温度范围大约从 0 到 100，而降雨量仅在 0 到 10 之间变化。数据准备做得很好！虽然这不是总是有趣，但它是机器学习和
    TensorFlow 的关键部分。
- en: 'Let''s now jump into the TensorFlow model:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们跳到 TensorFlow 模型中：
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We input our data as normal with a placeholder variable, but then you see this
    strange reshaping of the entire data set into one big tensor. Don''t worry, this
    is because we technically have one long, unbroken sequence of observations. The
    `y_` variable is just our output:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像往常一样使用占位符变量输入数据，但你会看到整个数据集被重新塑造成一个大张量。这是因为我们技术上拥有一个长而连续的观察序列。`y_` 变量只是我们的输出：
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We'll be computing a probability for every week for each season.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为每个季节的每周计算一个概率。
- en: 'The `cell` variable is the key to the recurrent neural network:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`cell` 变量是递归神经网络的关键：'
- en: '[PRE9]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This tells TensorFlow how the current time step depends on the previous. In
    this case, we'll use a basic RNN cell. So, we're only looking back one week at
    a time. Suppose that it has state size or 11 values. Feel free to experiment with
    more exotic cells and different state sizes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉 TensorFlow 当前时间步如何依赖于之前的时间步。在这种情况下，我们将使用一个基本的 RNN 单元。因此，我们每次只回顾一周。假设它有 11
    个状态值。你可以随意尝试更多的异国情调的单元和不同的状态大小。
- en: 'To put that cell to use, we''ll use `tf.nn.dynamic_rnn`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用这个单元，我们将使用 `tf.nn.dynamic_rnn`：
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This intelligently handles the recursion rather than simply unrolling all the
    time steps into a giant computational graph. As we have thousands of observations
    in one sequence, this is critical to attain reasonable speed. After the cell,
    we specify our input `x_`, then `dtype` to use 32 bits to store decimal numbers
    in a float, and then the empty `initial_state`. We use the outputs from this to
    build a simple model. From this point on, the model is almost exactly as you would
    expect from any neural network:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这智能地处理了递归，而不是简单地将所有时间步展开为一个巨大的计算图。由于我们在一个序列中有数千个观察值，这对于获得合理的速度至关重要。单元之后，我们指定输入
    `x_`，然后设置 `dtype` 使用 32 位浮点数存储小数，然后是空的 `initial_state`。我们使用这些输出构建一个简单的模型。从这一点开始，模型几乎和你从任何神经网络中预期的一样：
- en: 'We''ll multiply the output of the RNN cells, some weights, and add a bias to
    get a score for each class for that week:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 RNN 单元的输出、一些权重相乘，并加上一个偏置，以获得每周每个类别的得分：
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Our categorical cross_entropy loss function and train optimizer should be very
    familiar to you:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分类交叉熵损失函数和训练优化器对你来说应该非常熟悉：
- en: '[PRE12]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Great work setting up the TensorFlow model! To train this, we''ll use a familiar
    loop:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒的 TensorFlow 模型设置！要训练这个模型，我们将使用一个熟悉的循环：
- en: '[PRE13]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Since this is a fictitious problem, we''ll not worry too much about how accurate
    the model really is. The goal here is just to see how an RNN works. You can see
    that it runs just like any TensorFlow model:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个虚拟问题，我们不必过于担心模型的准确性。这里的目标只是查看 RNN 的工作原理。你可以看到它的运行方式就像任何 TensorFlow 模型一样：
- en: '![](../Images/e065c883fa632039fd0f6cae87a3439f.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e065c883fa632039fd0f6cae87a3439f.png)'
- en: If you do look at the accuracy, you can see that it's doing pretty well; much
    better than the 25 percent random guessing, but still has a lot to learn.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看准确率，你会发现它做得很好；远远超过 25% 的随机猜测，但仍然有很多需要学习的地方。
- en: '**We hope you enjoyed this extract from** [**Hands On Deep Learning with TensorFlow**](https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow?utm_source=kdnuggets&utm_medium=referral&utm_campaign=outreach)**.
    If you’d like to learn more visit packtpub.com.**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们希望您喜欢这段摘录自** [**动手深度学习与 TensorFlow**](https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow?utm_source=kdnuggets&utm_medium=referral&utm_campaign=outreach)**。如果您想了解更多，请访问
    packtpub.com。**'
- en: '**Related:**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[**A Guide For Time Series Prediction Using Recurrent Neural Networks (LSTMs)**](https://www.kdnuggets.com/2017/10/guide-time-series-prediction-recurrent-neural-networks-lstms.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**使用递归神经网络（LSTM）进行时间序列预测的指南**](https://www.kdnuggets.com/2017/10/guide-time-series-prediction-recurrent-neural-networks-lstms.html)'
- en: '[**Going deeper with recurrent networks: Sequence to Bag of Words Model**](https://www.kdnuggets.com/2017/08/deeper-recurrent-networks-sequence-bag-words-model.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**深入了解递归网络：序列到词袋模型**](https://www.kdnuggets.com/2017/08/deeper-recurrent-networks-sequence-bag-words-model.html)'
- en: '[**7 Steps to Mastering Deep Learning with Keras**](https://www.kdnuggets.com/2017/10/seven-steps-deep-learning-keras.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**掌握 Keras 深度学习的 7 个步骤**](https://www.kdnuggets.com/2017/10/seven-steps-deep-learning-keras.html)'
- en: '* * *'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的 IT'
- en: '* * *'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Exploring Neural Networks](https://www.kdnuggets.com/exploring-neural-networks)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[探索神经网络](https://www.kdnuggets.com/exploring-neural-networks)'
- en: '[10 Simple Things to Try Before Neural Networks](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在使用神经网络之前尝试的 10 件简单事](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 PyTorch 可解释的神经网络](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
- en: '[Deep Neural Networks Don''t Lead Us Towards AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度神经网络并未引领我们走向 AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
- en: '[Explainable Forecasting and Nowcasting with State-of-the-art Deep…](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[最先进的深度学习可解释预测和现在预测](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
- en: '[Image Classification with Convolutional Neural Networks (CNNs)](https://www.kdnuggets.com/2022/05/image-classification-convolutional-neural-networks-cnns.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[卷积神经网络（CNNs）的图像分类](https://www.kdnuggets.com/2022/05/image-classification-convolutional-neural-networks-cnns.html)'
