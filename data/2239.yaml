- en: 'Text-2-Video Generation: Step-by-Step Guide'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Text-2-Video 生成：逐步指南
- en: 原文：[https://www.kdnuggets.com/2023/08/text2video-generation-stepbystep-guide.html](https://www.kdnuggets.com/2023/08/text2video-generation-stepbystep-guide.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/08/text2video-generation-stepbystep-guide.html](https://www.kdnuggets.com/2023/08/text2video-generation-stepbystep-guide.html)
- en: '![Text-2-Video Generation: Step-by-Step Guide](../Images/d49e4f6113c87792b1bbf9caa97904ab.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Text-2-Video 生成：逐步指南](../Images/d49e4f6113c87792b1bbf9caa97904ab.png)'
- en: Gif by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的 GIF
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前 3 个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 加速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT 方面的工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Diffusion-based image generation models represent a revolutionary breakthrough
    in the field of Computer Vision. Pioneered by models including Imagen, DallE,
    and MidJourney, these advancements demonstrate remarkable capabilities in text-conditioned
    image generation. For an introduction to the inner workings of these models, you
    can read this [article](/2023/06/stable-diffusion-basic-intuition-behind-generative-ai.html).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 基于扩散的图像生成模型在计算机视觉领域代表了一项革命性的突破。由包括 Imagen、DallE 和 MidJourney 在内的模型开创，这些进展展示了文本条件图像生成的卓越能力。要了解这些模型的内部工作原理，你可以阅读这篇[文章](/2023/06/stable-diffusion-basic-intuition-behind-generative-ai.html)。
- en: However, the development of Text-2-Video models poses a more formidable challenge.
    The goal is to achieve coherence and consistency across each generated frame and
    maintain generation context from the video's inception to its conclusion.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Text-2-Video 模型的开发带来了更大的挑战。目标是确保生成的每一帧之间的一致性和连贯性，并在视频的开头到结尾之间保持生成的上下文。
- en: Yet, recent advancements in Diffusion-based models offer promising prospects
    for Text-2-Video tasks as well. Most Text-2-Video models now employ fine-tuning
    techniques on pre-trained Text-2-Image models, integrating dynamic image motion
    modules, and leveraging diverse Text-2-Video datasets like WebVid or HowTo100M.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最近在基于扩散的模型中的进展为 Text-2-Video 任务提供了有前景的前景。目前，大多数 Text-2-Video 模型都采用了对预训练的
    Text-2-Image 模型进行微调的技术，集成了动态图像运动模块，并利用了 WebVid 或 HowTo100M 等各种 Text-2-Video 数据集。
- en: In this article, our approach involves utilizing a fine-tuned model provided
    by HuggingFace, which proves instrumental in generating the videos.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们的方法涉及使用 HuggingFace 提供的微调模型，这对于生成视频非常重要。
- en: Implementation
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: Pre-requisites
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 先决条件
- en: We use the Diffusers library provided by HuggingFace, and a utility library
    called Accelerate, that allows PyTorch code to run in parallel threads. This speeds
    up our generation process.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 HuggingFace 提供的 Diffusers 库，以及一个名为 Accelerate 的实用库，它允许 PyTorch 代码在并行线程中运行。这加速了我们的生成过程。
- en: First, we must install our dependencies and import relevant modules for our
    code.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须安装所需的依赖项并导入相关模块以便我们的代码正常运行。
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then, import the relevant modules from each library.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，从每个库中导入相关模块。
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Creating Pipelines
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建管道
- en: We load the Text-2-Video model provided by [ModelScope](https://modelscope.cn/)
    on [HuggingFace](https://huggingface.co/damo-vilab/text-to-video-ms-1.7b), in
    the Diffusion Pipeline. The model has 1.7 billion parameters and is based on UNet3D
    architecture that generates a video from pure noise through an iterative de-noising
    process. It works in a 3-part process. The model firsts perform text-feature extraction
    from the simple English prompt. The text features are then encoded to the video
    latent space and de-noised. Lastly, the video latent space is decoded back to
    the visual space and a short video is generated.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [HuggingFace](https://huggingface.co/damo-vilab/text-to-video-ms-1.7b) 上加载了由
    [ModelScope](https://modelscope.cn/) 提供的 Text-2-Video 模型，在扩散管道中进行处理。该模型具有17亿参数，基于UNet3D架构，通过迭代去噪过程从纯噪声中生成视频。其工作过程分为三部分。模型首先从简单的英语提示中提取文本特征。然后将文本特征编码到视频潜在空间并去噪。最后，将视频潜在空间解码回视觉空间，并生成一个短视频。
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Moreover, we use 16-bit floating-point precision to reduce GPU utilization.
    In addition, CPU offloading is enabled that removes unnecessary parts from GPU
    during runtime.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们使用16位浮点精度来减少GPU的利用率。此外，还启用了CPU卸载功能，这可以在运行时从GPU中移除不必要的部分。
- en: Generating Video
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成视频
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We then pass a prompt to the Video Generation pipeline that provides a sequence
    of generated frames. We use 25 inference steps so that the model will perform
    25 de-noising iterations. A higher number of inference steps can improve video
    quality but requires higher computational resources and time.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将一个提示传递给视频生成管道，该管道提供生成的帧序列。我们使用25次推理步骤，以便模型执行25次去噪迭代。较高的推理步骤数可以提高视频质量，但需要更高的计算资源和时间。
- en: The separate image frames are then combined using a diffuser's utility function,
    and a video is saved on the disk.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 单独的图像帧随后通过扩散器的工具函数进行合成，视频被保存在磁盘上。
- en: We then pass a prompt to the Video Generation pipeline that provides a sequence
    of generated frames. The separate image frames are then combined using a diffuser's
    utility function, and a video is saved on the disk.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将一个提示传递给视频生成管道，该管道提供生成的帧序列。单独的图像帧随后通过扩散器的工具函数进行合成，视频被保存在磁盘上。
- en: '[FinalVideo](https://vimeo.com/846481462) from [Muhammad Arham](https://vimeo.com/user182110512)
    on [Vimeo](https://vimeo.com).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[Muhammad Arham 的最终视频](https://vimeo.com/846481462) 来自 [Muhammad Arham](https://vimeo.com/user182110512)
    在 [Vimeo](https://vimeo.com) 上。'
- en: Conclusion
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Simple enough! We get a video of Spiderman surfing. Although it is a short not-so-high-quality
    video, it still symbolizes the promising prospect of this process, which can attain
    similar results as Image-2-Text models soon. Nonetheless, testing your creativity
    and playing with the model is still good enough. You can use this [Colab Notebook](https://colab.research.google.com/drive/1IYe2MQZX86n3o22PR7HmSFgw54h31poZ?usp=sharing)
    to try it out.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 非常简单！我们得到了一个关于蜘蛛侠冲浪的视频。虽然这是一个短的、质量不是很高的视频，但它仍然象征着这一过程的良好前景，该过程很快就能达到与图像-文本模型类似的结果。不过，测试你的创造力并与模型互动仍然非常有趣。你可以使用这个
    [Colab Notebook](https://colab.research.google.com/drive/1IYe2MQZX86n3o22PR7HmSFgw54h31poZ?usp=sharing)
    试一试。
- en: '**[Muhammad Arham](https://www.linkedin.com/in/muhammad-arham-a5b1b1237/)**
    is a Deep Learning Engineer working in Computer Vision and Natural Language Processing.
    He has worked on the deployment and optimizations of several generative AI applications
    that reached the global top charts at Vyro.AI. He is interested in building and
    optimizing machine learning models for intelligent systems and believes in continual
    improvement.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**[穆罕默德·阿赫曼](https://www.linkedin.com/in/muhammad-arham-a5b1b1237/)** 是一名深度学习工程师，专注于计算机视觉和自然语言处理。他曾在Vyro.AI工作，负责多个生成式AI应用的部署和优化，这些应用达到了全球排行榜的顶端。他对构建和优化智能系统中的机器学习模型感兴趣，并相信持续改进。'
- en: More On This Topic
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关话题
- en: '[Retrieval Augmented Generation: Where Information Retrieval Meets…](https://www.kdnuggets.com/retrieval-augmented-generation-where-information-retrieval-meets-text-generation)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[检索增强生成：信息检索与…的结合](https://www.kdnuggets.com/retrieval-augmented-generation-where-information-retrieval-meets-text-generation)'
- en: '[A Guide On How To Become A Data Scientist (Step By Step Approach)](https://www.kdnuggets.com/2021/05/guide-become-data-scientist.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何成为数据科学家的指南（逐步方法）](https://www.kdnuggets.com/2021/05/guide-become-data-scientist.html)'
- en: '[How To Structure a Data Science Project: A Step-by-Step Guide](https://www.kdnuggets.com/2022/05/structure-data-science-project-stepbystep-guide.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何结构化数据科学项目：逐步指南](https://www.kdnuggets.com/2022/05/structure-data-science-project-stepbystep-guide.html)'
- en: '[A Step-by-Step Guide to Web Scraping with Python and Beautiful Soup](https://www.kdnuggets.com/2023/04/stepbystep-guide-web-scraping-python-beautiful-soup.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 和 Beautiful Soup 进行网页抓取的逐步指南](https://www.kdnuggets.com/2023/04/stepbystep-guide-web-scraping-python-beautiful-soup.html)'
- en: '[A Step by Step Guide to Reading and Understanding SQL Queries](https://www.kdnuggets.com/a-step-by-step-guide-to-reading-and-understanding-sql-queries)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[逐步阅读和理解 SQL 查询的指南](https://www.kdnuggets.com/a-step-by-step-guide-to-reading-and-understanding-sql-queries)'
- en: '[Breaking Down DENSE_RANK(): A Step-by-Step Guide for SQL Enthusiasts](https://www.kdnuggets.com/breaking-down-denserank-a-step-by-step-guide-for-sql-enthusiasts)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[解析 DENSE_RANK()：SQL 爱好者的逐步指南](https://www.kdnuggets.com/breaking-down-denserank-a-step-by-step-guide-for-sql-enthusiasts)'
