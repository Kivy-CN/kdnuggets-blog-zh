# 数据科学中的降维技术

> 原文：[https://www.kdnuggets.com/2022/09/dimensionality-reduction-techniques-data-science.html](https://www.kdnuggets.com/2022/09/dimensionality-reduction-techniques-data-science.html)

![数据科学中的降维技术](../Images/800a46a763162a8519b021b1f506541a.png)

图片来源：编辑

# 介绍

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全领域的职业道路。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持您的组织 IT

* * *

在机器学习中，分析包含多个变量的数据需要大量的资源和计算，更不用说所需的人工劳动了。这正是**降维技术**发挥作用的地方。降维技术是一种将高维数据集转化为低维数据集的过程，同时不丢失原始数据的有价值属性。这些**降维技术**基本上是数据预处理步骤的一部分，在训练模型之前进行。

# 什么是数据科学中的降维？

想象一下，你正在训练一个模型，该模型可以基于当日的各种气候条件预测第二天的天气。当前的气候条件可能包括阳光、湿度、寒冷、温度等数百万个环境特征，这些特征过于复杂而难以分析。因此，我们可以通过观察哪些特征彼此强相关并将它们合并成一个来减少特征数量。

![数据科学中的降维技术](../Images/8b21115b2119e4c5418d96147df87680.png)

图片来源：作者

在这里，我们可以将湿度和降雨量合并为一个依赖特征，因为我们知道它们之间有很强的相关性。就这样！这就是降维技术用于将复杂数据压缩成更简单形式而不丧失数据本质的方法。此外，数据科学和 AI 专家现在还使用**数据科学解决方案**来提高业务 ROI。数据可视化、数据挖掘、预测分析以及 Datatobiz 提供的其他**数据分析服务**正在改变商业游戏规则。

## 为什么降维是必要的？

机器学习和深度学习技术通过输入大量数据来学习波动、趋势和模式。不幸的是，这种巨大的数据包含许多特征，通常会导致“维度诅咒”。

此外，稀疏性在大型数据集中很常见。稀疏性指的是具有微不足道或无值的特征，如果将其输入训练模型，模型在测试时表现不佳。此外，这些冗余特征会导致数据相似特征的聚类问题。

因此，为了应对维度诅咒，降维技术发挥了作用。为什么降维有用的问题的答案是：

+   模型表现得更准确，因为冗余数据将被移除，这将减少假设的空间。

+   更少的计算资源使用，将节省时间和财务预算

+   一些机器学习/深度学习技术在高维数据上无法工作，而这个问题会在降维后得到解决。

+   干净且非稀疏的数据会产生更具统计显著性的结果，因为这种数据的聚类更容易且更准确。

现在让我们通过示例来了解**用于数据降维的算法**。

# 什么是降维技术

降维技术大致分为两类，即：

1.  线性方法

1.  非线性方法

## 1\. 线性方法

### PCA

主成分分析（PCA）是数据科学中使用的降维技术之一。考虑一组互相关联的'*p*' 变量。该技术将这组'*p*' 变量减少为较小数量的不相关变量，通常用'*k*' 表示，其中 (*k<p*)。这些'*k*' 变量称为主成分，它们的变化类似于原始数据集。

PCA 用于找出特征之间的相关性，它将这些相关性结合在一起。因此，结果数据集具有较少的彼此线性相关的特征。这样，模型在同时计算原始数据集的最大方差时，会减少相关特征。找到这些方差的方向后，它将这些方向引导到一个较小的维度空间中，产生称为主成分的新成分。

这些成分在表示原始特征方面非常充分。因此，它在寻找最佳成分的同时减少了重构误差。这样，数据得到了减少，使得机器学习算法表现得更好、更快。PrepAI 是一个完美的 AI 示例，它在后台利用 PCA 技术从给定的原始文本中智能生成问题。

### 因素分析

该技术是主成分分析（PCA）的扩展。此技术的主要关注点不仅仅是减少数据集。它更多地关注发现潜在变量，这些变量是数据集中其他变量的结果。它们不是在单一变量中直接测量的。

潜在变量也称为因子。因此，建立一个测量这些潜在变量的模型的过程称为因子分析。它不仅有助于减少变量，还帮助区分响应簇。例如，你需要建立一个预测客户满意度的模型。你将准备一个问卷，其中包含类似的问题，

“你对我们的产品满意吗？”

“你愿意与熟人分享你的经验吗？”

如果你想创建一个变量来评估客户满意度，你可以选择对反馈进行平均或创建一个因子依赖的变量。这可以通过PCA完成，并将第一个因子作为主成分。

### 线性判别分析

这是一种主要用于监督分类问题的降维技术。逻辑回归在多分类问题中效果不佳。因此，LDA应运而生，以弥补这一不足。它有效地区分了各个类别的训练变量。此外，它与PCA不同，因为它通过计算输入特征之间的线性组合来优化区分不同类别的过程。

这里有一个例子帮助你理解LDA：

考虑一组属于两类的球：红球和蓝球。假设它们随机地绘制在二维平面上，以至于不能用直线将它们分成两个不同的类别。在这种情况下，使用LDA可以将二维图转换为一维图，从而最大限度地区分球的类别。球被投影到一个新的轴上，这个轴以最佳方式将它们分隔到各自的类别中。新的轴是通过两个步骤形成的：

+   通过最大化两个类别均值之间的距离

+   通过最小化每个类别内部的变异

### SVD

考虑一个具有'*m*'列的数据。截断奇异值分解方法（TSVD）是一种投影方法，其中这些'*m*'列（特征）被投影到一个具有'*m*'列或更少列的子空间中，而不会丢失数据的特征。

一个TSVD可以使用的例子是包含电子商务产品评论的数据集。评论列通常是空白的，这会导致数据中出现空值，而TSVD可以有效处理这种情况。这个方法可以通过TruncatedSVD()函数轻松实现。

PCA使用的是密集数据，而SVD使用的是稀疏数据。此外，PCA因式分解使用协方差矩阵，而TSVD是在数据矩阵上进行的。

## 2\. 非线性方法

### 核PCA

PCA对于线性可分的数据集非常有效。然而，如果我们将它应用于非线性数据集，数据集的降维可能不准确。因此，这时核PCA变得高效。

数据集经过一个核函数并暂时投影到更高维的特征空间。在这里，类别被转换，可以用直线分开并加以区分。然后，应用一般的 PCA，将数据投影回降维空间。在该空间中进行线性降维方法将与在实际空间中进行非线性降维一样有效。

核主成分分析（Kernel PCA）操作有 3 个重要的超参数：我们希望保留的组件数量、我们要使用的核类型和核系数。有不同类型的核，即“线性”（'linear'）、“多项式”（'poly'）、“径向基函数”（'rbf'）、“ sigmoid”和“余弦”（'cosine'）。其中，径向基函数核（RBF）被广泛使用。

### T-分布随机邻域嵌入（T-Distributed Stochastic Neighbor Embedding）

这是一种非线性降维方法，主要用于数据可视化、图像处理和自然语言处理（NLP）。T-SNE 具有一个灵活的参数，即“困惑度”（'perplexity'）。它展示了如何在数据集的全局和局部方面之间保持平衡。它提供了每个数据点的近邻数量的估计。此外，它将不同数据点之间的相似性转换为联合概率，并在低维嵌入和高维数据集的联合概率之间最小化 Kullback-Leibler 散度。此外，T-SNE 还提供了一个非凸的成本函数，使用不同的初始化方法可能会得到不同的结果。

T-SNE 仅保留最小的成对距离或局部相似性，而 PCA 保留最大对距离以最大化方差。此外，推荐使用 PCA 或 TSVD 将数据集中特征的维度减少到超过 50，因为在这种情况下 T-SNE 会失效。

### 多维缩放（Multi-Dimensional Scaling）

缩放指的是通过将数据减少到较低的维度来简化数据。这是一种非线性降维技术，通过可视化的方式展示特征集之间的距离或差异。特征之间距离较短被认为是相似的，而距离较大的则被认为是不相似的。

MDS 减少数据维度并解释数据中的差异。此外，数据在缩小后不会失去本质；两个数据点的距离总是相同的，无论它们的维度如何。此技术仅适用于具有关系数据的矩阵，例如相关性、距离等。让我们通过一个示例来理解这一点。

假设你需要制作一张地图，其中提供了城市位置的列表。地图还应该展示两个城市之间的距离。唯一可能的方法是通过卷尺测量城市之间的距离。但如果你只得到城市之间的距离和它们的相似性，而不是城市的位置呢？你仍然可以通过逻辑假设和广泛的几何知识来绘制地图。

在这里，你基本上是在应用MDS来创建一个地图。MDS观察数据集中的差异，并创建一个地图，以计算原始距离并告诉你它们的位置。

### 等距映射（Isomap）

这是一种非线性降维技术，基本上是MDS或Kernel PCA的扩展。它通过连接每个特征的曲线或测地距离来减少维度。

Isomap通过建立邻域网络来启动。然后，它使用图距离来估计每对点之间的测地距离。最后，通过分解测地矩阵的特征值，将数据集嵌入到较低维度中。可以使用*Isomap()*类的*n_neighbours*超参数指定每个数据点要考虑的邻居数量。该类实现了Isomap算法。

# 数据挖掘中为何需要降维？

数据挖掘是观察大规模数据集中的隐藏模式、关系和异常的过程，以估计结果。大规模数据集包含许多以指数速率增长的变量。因此，在数据挖掘过程中找到并分析这些模式需要大量的资源和计算时间。因此，可以在数据挖掘时应用降维技术，通过将数据特征合并来限制这些特征，同时仍然足够代表原始数据集。

# 降维的优缺点

## 优势

+   存储空间和处理时间减少

+   被解释变量的多重共线性已被消除

+   减少模型过拟合的机会

+   数据可视化变得更容易

## 劣势

+   会丢失一定量的数据。

+   PCA无法应用于那些不能通过均值和协方差定义的数据。

+   不是每个变量都需要线性相关，而PCA倾向于发现这种相关性。

+   LDA需要标记数据才能运行，这在某些情况下不可用。

# 结论

每秒生成大量数据。因此，利用资源进行准确分析同样重要。降维技术帮助以精确高效的方式进行数据预处理—这也是为什么它被认为是数据科学家的福音。

**[Kavika Roy](https://www.datatobiz.com/)** 是 [DataToBiz](https://www.datatobiz.com/) 的信息管理部门负责人。她负责技术监督的识别、获取、分配和组织。她对细节的高度关注使她能够向正确的受众提供有关功能方面的精确信息。

### 更多相关话题

+   [KDnuggets 新闻，8月31日：完整的数据科学学习路线图…](https://www.kdnuggets.com/2022/n35.html)

+   [2022年及以后顶级人工智能和数据科学工具与技术](https://www.kdnuggets.com/2022/03/nvidia-0317-top-ai-data-science-tools-techniques-2022-beyond.html)

+   [数据科学中的异常检测技术初学者指南](https://www.kdnuggets.com/2023/05/beginner-guide-anomaly-detection-techniques-data-science.html)

+   [数据科学中重采样技术的作用](https://www.kdnuggets.com/2023/02/role-resampling-techniques-data-science.html)

+   [用于非结构化数据的探索性数据分析技术](https://www.kdnuggets.com/2023/05/exploratory-data-analysis-techniques-unstructured-data.html)

+   [处理不平衡数据的7种技术](https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html)
