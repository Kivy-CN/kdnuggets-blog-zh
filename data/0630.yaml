- en: Adversarial Examples in Deep Learning – A Primer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的对抗样本——入门
- en: 原文：[https://www.kdnuggets.com/2020/11/adversarial-examples-deep-learning-primer.html](https://www.kdnuggets.com/2020/11/adversarial-examples-deep-learning-primer.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/11/adversarial-examples-deep-learning-primer.html](https://www.kdnuggets.com/2020/11/adversarial-examples-deep-learning-primer.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: Introducing adversarial examples in vision deep learning models
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍视觉深度学习模型中的对抗样本
- en: '![](../Images/ebb427604a83a0db8ff8f1fedaf4fe54.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ebb427604a83a0db8ff8f1fedaf4fe54.png)'
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三名课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织进行 IT 工作'
- en: '* * *'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: We have seen the advent of state-of-the-art (SOTA) deep learning models for
    computer vision ever since we started getting bigger and better compute (GPUs
    and TPUs), more data (ImageNet etc.) and easy to use open-source software and
    tools (TensorFlow and PyTorch). Every year (and now every few months!) we see
    the next SOTA deep learning model dethrone the previous model in terms of Top-k
    accuracy for benchmark datasets. The following figure depicts some of the latest
    SOTA deep learning vision models (and doesn't depict some like Google's BigTransfer!).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 自从我们开始拥有更强大更先进的计算能力（如 GPU 和 TPU）、更多的数据（如 ImageNet 等）以及易于使用的开源软件和工具（如 TensorFlow
    和 PyTorch）以来，我们已经见证了最先进（SOTA）计算机视觉深度学习模型的出现。每年（现在甚至每几个月！），我们都会看到下一个最先进的深度学习模型在基准数据集的
    Top-k 准确性方面超越之前的模型。下图展示了一些最新的最先进深度学习视觉模型（但没有展示像 Google 的 BigTransfer 这样的模型！）。
- en: '![Figure](../Images/0fca4b3e363b7c823577a5764247957d.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/0fca4b3e363b7c823577a5764247957d.png)'
- en: 'SOTA Deep Learning Vision Models (Source: https://arxiv.org/abs/1905.11946)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '最先进的深度学习视觉模型（来源: https://arxiv.org/abs/1905.11946）'
- en: However most of these SOTA deep learning models are brought down to their knees
    when it tries to make predictions on a specific class of images, called as adversarial
    images. The whole idea of an adversarial example can be a natural example or a
    synthetic example. We will look at a few examples in this article to get familiar
    with different adversarial examples and attacks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数这些最先进的深度学习模型在对特定类别的图像（称为对抗图像）进行预测时会受到严重影响。对抗样本的整体概念可以是自然样本或合成样本。我们将在本文中查看一些例子，以熟悉不同的对抗样本和攻击。
- en: Adversarial Examples
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对抗样本
- en: A natural adversarial example is a natural, organic image which is tough for
    the model to comprehend. A synthetic adversarial example is where an attacker
    (a malicious user) purposely injects some noise into an image which visually remains
    very similar to the original image but the model ends up making a vastly different
    (and wrong) prediction. Let's look at a few of these in more detail!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 自然对抗样本是指自然生成的、对于模型难以理解的图像。合成对抗样本则是攻击者（恶意用户）故意向图像中注入一些噪声，使其在视觉上与原始图像非常相似，但模型最终做出截然不同（且错误）的预测。让我们更详细地查看一些这些例子！
- en: Natural Adversarial Examples
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自然对抗样本
- en: These examples, as defined in the paper *['**Natural Adversarial Examples, **Hendrycks
    et al.'](https://arxiv.org/abs/1907.07174)*are *real-world, unmodified, and naturally
    occurring examples that cause classifier accuracy to significantly degrade*.  They
    have introduced two new datasets of natural adversarial examples. The first dataset
    contains 7,500 natural adversarial examples for ImageNet classifiers and serves
    as a hard ImageNet classifier test set, called IMAGENET-A. The following figure
    shows some of these adversarial examples from the ImageNet-A dataset.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例，如论文*['**自然对抗示例，**Hendrycks等'](https://arxiv.org/abs/1907.07174)*中定义的，是*现实世界中的、未经修改的、自然发生的示例，这些示例导致分类器的准确性显著下降*。他们引入了两个新的自然对抗示例数据集。第一个数据集包含7500个用于ImageNet分类器的自然对抗示例，并作为ImageNet分类器的困难测试集，称为IMAGENET-A。下图展示了ImageNet-A数据集中的一些对抗示例。
- en: '![Figure](../Images/d167a41fbc511bac464f0c8f5dce96a5.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/d167a41fbc511bac464f0c8f5dce96a5.png)'
- en: 'ResNet-50 fails horribly on examples from ImageNet-A (Source: https://arxiv.org/abs/1907.07174)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 'ResNet-50在ImageNet-A上的示例表现极差（来源: https://arxiv.org/abs/1907.07174）'
- en: You can clearly see how wrong (and silly!) are the predictions of a state-of-the-art
    (SOTA) ResNet-50 model on the above examples. In-fact the DenseNet-121 pre-trained
    model obtains an accuracy of only 2% on ImageNet-A!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以清楚地看到最先进的（SOTA）ResNet-50模型在上述示例中的预测有多么错误（和可笑！）。事实上，DenseNet-121预训练模型在ImageNet-A上的准确率仅为2%！
- en: '![Image](../Images/245adbeec92ee068f7a7a345b6b22a8f.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/245adbeec92ee068f7a7a345b6b22a8f.png)'
- en: The authors have also curated an adversarial out-of-distribution detection dataset
    called IMAGENET-O, which they claim is the first out-of-distribution detection
    dataset created for ImageNet models. The following figure shows some interesting
    examples of ResNet-50 inference on images from the ImageNet-O dataset.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们还策划了一个名为IMAGENET-O的对抗性分布外检测数据集，他们声称这是第一个为ImageNet模型创建的分布外检测数据集。下图展示了ResNet-50对ImageNet-O数据集中图像进行推断的一些有趣示例。
- en: '![Figure](../Images/e98f7763d547bff66c3fe1e8122e6674.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/e98f7763d547bff66c3fe1e8122e6674.png)'
- en: 'ResNet-50 fails horribly on examples from ImageNet-O (Source: https://arxiv.org/abs/1907.07174)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 'ResNet-50在ImageNet-O上的示例表现极差（来源: https://arxiv.org/abs/1907.07174）'
- en: The examples are indeed interesting and showcase the limitations of SOTA pre-trained
    vision models on some of these images which are more complex for these models
    to interpret. Some of the reasons of failure can be attributed to what deep learning
    models are trying to focus on when making predictions for a specific image. Let's
    look at some more examples to try and understand this.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例确实很有趣，并展示了SOTA预训练视觉模型在一些这些图像上的局限性，这些图像对于这些模型来说更为复杂。一些失败的原因可以归因于深度学习模型在对特定图像进行预测时试图关注的内容。让我们看一些更多的示例来尝试理解这一点。
- en: '![Figure](../Images/afe0e0bd7d51032c2c476c4f57e34f2e.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/afe0e0bd7d51032c2c476c4f57e34f2e.png)'
- en: 'Natural adversarial examples in ImageNet-A (Source: https://arxiv.org/abs/1907.07174)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 'ImageNet-A中的自然对抗示例（来源: https://arxiv.org/abs/1907.07174）'
- en: 'Based on the examples showcased in the figure above, it is pretty clear that
    there are some specific patterns w.r.t mis-interpretations made by deep learning
    vision models. For instance:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述图中展示的示例，很明显深度学习视觉模型在误解方面存在一些特定的模式。例如：
- en: Candles are predicted as a jack-o’-lanterns, despite the absence of a pumpkin
    due to model focusing more on aspects like the flame and its illumination
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管模型更多关注于火焰及其照明，但蜡烛仍被预测为南瓜灯，因为缺少南瓜。
- en: Dragonfly is predicted as a skunk or a banana due to the model focusing more
    on color and texture
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蜻蜓被预测为臭鼬或香蕉，因为模型更多地关注于颜色和纹理。
- en: Mushroom being classified as a nail because the models learn to associate certain
    elements together e.g. wood - nails
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蘑菇被分类为钉子，因为模型学会将某些元素联系在一起，例如木材-钉子。
- en: Models also end up suffering from overgeneralization problems e.g. shadows to
    sundials
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型也会遭受过度泛化问题，例如阴影到日晷。
- en: The overall performance of SOTA deep learning vision models are pretty poor
    on these examples as depicted in the following figure.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: SOTA深度学习视觉模型在这些示例中的整体表现非常差，如下图所示。
- en: '![Figure](../Images/ff669216f2adbbe8b99863679db0e866.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/ff669216f2adbbe8b99863679db0e866.png)'
- en: 'SOTA deep learning vision models performance on ImageNet-A (Source: https://arxiv.org/abs/1907.07174)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 'SOTA深度学习视觉模型在ImageNet-A上的表现（来源: https://arxiv.org/abs/1907.07174）'
- en: The sad part is that robust adversarial training methods hardly help in tacking
    problems associated with mis-interpreting natural adversarial examples as mentioned
    in the same paper by Hendrycks et al. Some of these methods include training against
    specific synthetic attacks like Projected Gradient Descent (PGD) and Fast Gradient
    Sign Method (FGSM) which we will look at in more detail in subsequent articles.
    Luckily these methods work well for handling malicious synthetic attacks which
    are usually a larger concern.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 悲哀的是，鲁棒的对抗训练方法几乎无法解决Hendrycks等人论文中提到的将自然对抗样本误解释为问题的困难。一些这些方法包括针对特定合成攻击进行训练，如投影梯度下降（PGD）和快速梯度符号方法（FGSM），我们将在后续文章中详细讨论。幸运的是，这些方法在处理恶意合成攻击时表现良好，这通常是更大的问题。
- en: Synthetic Adversarial Examples
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 合成对抗样本
- en: These examples basically involve artificially inducing some noise in an input
    image such that visually it still remains very similar to the original image,
    but the infused noise ends up degrading classifier accuracy. While there are a
    wide variety of synthetic adversarial attacks, all of them operate on a few core
    set of principles as depicted in the following figure.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子基本上涉及在输入图像中人为地引入一些噪声，使其在视觉上仍与原始图像非常相似，但注入的噪声最终会降低分类器的准确性。尽管合成对抗攻击种类繁多，但它们都基于以下图形中所示的一些核心原则。
- en: '![Figure](../Images/511f0d71af10388e2199ee49f2178b2a.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/511f0d71af10388e2199ee49f2178b2a.png)'
- en: 'Key Steps for Adversarial Attacks (Source: https://github.com/dipanjanS/adversarial-learning-robustness)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击的关键步骤（来源：https://github.com/dipanjanS/adversarial-learning-robustness）
- en: The focus is always to figure out a way to perfect the noise \ perturbation
    tensor (matrix of values) which can be super-imposed on top of the original image
    such that these perturbations are invisible to the human eye but ends up making
    the deep learning model fail in making correct predictions.  The example depicted
    above showcases a Fast Gradient Sign Method (FGSM) attack where we add in a small
    multiplier to the sign of the gradients of the input image and super-impose with
    the image of a panda making the model fail in its prediction thinking that the
    image is that of a gibbon. The following graphic showcases some of the more popular
    types of adversarial attacks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 重点始终是找出一种完美的噪声\扰动张量（数值矩阵）的方法，这些张量可以叠加在原始图像上，使得这些扰动对人眼不可见，但最终使深度学习模型在做出正确预测时失败。上面展示的例子展示了一种快速梯度符号方法（FGSM）攻击，其中我们向输入图像的梯度符号添加了一个小的乘数，并与一张熊猫的图像叠加，使得模型在预测时失败，以为图像是长臂猿。以下图形展示了一些更常见的对抗攻击类型。
- en: '![Figure](../Images/431ae8a31b8b51aae38721c6937e5040.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/431ae8a31b8b51aae38721c6937e5040.png)'
- en: 'Popular Adversarial Attacks (Source: https://github.com/dipanjanS/adversarial-learning-robustness)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 常见对抗攻击（来源：https://github.com/dipanjanS/adversarial-learning-robustness）
- en: What's Next?
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 接下来是什么？
- en: In the next couple of articles we will discuss about each of the above mentioned
    adversarial attack methodologies and showcase with hands-on code examples how
    you can fool the latest and best SOTA vision models. Stay tuned!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几篇文章中，我们将讨论上述每种对抗攻击方法，并展示如何通过实际代码示例欺骗最新最好的SOTA视觉模型。敬请关注！
- en: The content in this article has been taken from some of the recent work on adversarial
    learning done by [myself](https://www.linkedin.com/in/dipanzan/) and [Sayak](https://in.linkedin.com/in/sayak-paul)
    and you can find detailed examples in [this GitHub Repository](https://github.com/dipanjanS/adversarial-learning-robustness).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本文内容来自我和[Sayak](https://in.linkedin.com/in/sayak-paul)在对抗学习方面的一些近期工作，详细示例可以在[这个GitHub仓库](https://github.com/dipanjanS/adversarial-learning-robustness)中找到。
- en: '[Original](https://blog.djsarkar.ai/adversarial-learning-attacks-1/). Reposted
    with permission.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://blog.djsarkar.ai/adversarial-learning-attacks-1/)。经许可转载。'
- en: '**Related:**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Adversarial Validation Overview](/2020/02/adversarial-validation-overview.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[对抗验证概述](/2020/02/adversarial-validation-overview.html)'
- en: '[Are Computer Vision Models Vulnerable to Weight Poisoning Attacks?](/2020/08/computer-vision-models-vulnerable-weight-poisoning-attacks.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[计算机视觉模型是否易受权重中毒攻击？](/2020/08/computer-vision-models-vulnerable-weight-poisoning-attacks.html)'
- en: '[Roadmap to Computer Vision](/2020/10/roadmap-computer-vision.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[计算机视觉路线图](/2020/10/roadmap-computer-vision.html)'
- en: More On This Topic
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Ensemble Learning with Examples](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集成学习与示例](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
- en: '[Picking Examples to Understand Machine Learning Model](https://www.kdnuggets.com/2022/11/picking-examples-understand-machine-learning-model.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[挑选示例以理解机器学习模型](https://www.kdnuggets.com/2022/11/picking-examples-understand-machine-learning-model.html)'
- en: '[SQL LIKE Operator Examples](https://www.kdnuggets.com/2022/09/sql-like-operator-examples.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQL LIKE 操作符示例](https://www.kdnuggets.com/2022/09/sql-like-operator-examples.html)'
- en: '[What is Adversarial Machine Learning?](https://www.kdnuggets.com/2022/03/adversarial-machine-learning.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是对抗性机器学习？](https://www.kdnuggets.com/2022/03/adversarial-machine-learning.html)'
- en: '[Describing Data: A Statology Primer](https://www.kdnuggets.com/describing-data-statology-primer)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[描述数据：Statology 初学者指南](https://www.kdnuggets.com/describing-data-statology-primer)'
- en: '[Introduction to Statistics: A Statology Primer](https://www.kdnuggets.com/introduction-to-statistics-statology-primer)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[统计学入门：Statology 初学者指南](https://www.kdnuggets.com/introduction-to-statistics-statology-primer)'
