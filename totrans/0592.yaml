- en: 'Segment Anything Model: Foundation Model for Image Segmentation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/07/segment-anything-model-foundation-model-image-segmentation.html](https://www.kdnuggets.com/2023/07/segment-anything-model-foundation-model-image-segmentation.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Segmentation, the process of identifying image pixels that belong to objects,
    is at the core of computer vision. This process is used in applications from scientific
    imaging to photo editing, and technical experts must possess both highly skilled
    abilities and access to AI infrastructure with large quantities of annotated data
    for accurate modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Meta AI recently unveiled its [Segment Anything project](https://github.com/facebookresearch/segment-anything)?
    which is ?an image segmentation dataset and model with the Segment Anything Model
    (SAM) and [the SA-1B mask dataset](https://ai.facebook.com/datasets/segment-anything/)?—?the
    largest ever segmentation dataset support further research in foundation models
    for computer vision. They made SA-1B available for research use while the SAM
    is licensed under Apache 2.0 open license for anyone to try SAM with your images
    using this [**demo**](https://segment-anything.com/demo)!
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![Segment Anything Model: Foundation Model for Image Segmentation](../Images/8ff2e4f2d8b63cf085c76b165d8ad402.png)'
  prefs: []
  type: TYPE_IMG
- en: Segment Anything Model / Image by [**Meta AI**](https://segment-anything.com/)
  prefs: []
  type: TYPE_NORMAL
- en: Toward Generalizing the Segmentation Task
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before, segmentation problems were approached using two classes of approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Interactive segmentation in which the users guide the segmentation task by iteratively
    refining a mask.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic segmentation allowed selective object categories like cats or chairs
    to be segmented automatically but it required large numbers of annotated objects
    for training (i.e. thousands or even tens of thousands of examples of segmented
    cats) along with computing resources and technical expertise to train a segmentation
    model neither approach provided a general, fully automatic solution to segmentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SAM** uses both interactive and automatic segmentation in one model. The
    proposed interface enables flexible usage, making a wide range of segmentation
    tasks possible by engineering the appropriate prompt (such as clicks, boxes, or
    text).'
  prefs: []
  type: TYPE_NORMAL
- en: SAM was developed using an expansive, high-quality dataset containing more than
    one billion masks collected as part of this project, giving it the capability
    of generalizing to new types of objects and images beyond those observed during
    training. As a result, practitioners no longer need to collect their segmentation
    data and tailor a model specifically to their use case.
  prefs: []
  type: TYPE_NORMAL
- en: These capabilities enable SAM to generalize both across tasks and domains something
    no other image segmentation software has done before.
  prefs: []
  type: TYPE_NORMAL
- en: SAM Capabilities & Use Cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SAM comes with powerful capabilities that make the segmentation task more effective:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Variety of input prompts**: Prompts that direct segmentation allow users
    to easily perform different segmentation tasks without additional training requirements.
    You can apply segmentation using interactive points and boxes, automatically segment
    everything in an image, and generate multiple valid masks for ambiguous prompts.
    In the figure below we can see the segmentation is done for certain objects using
    an input text prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Segment Anything Model: Foundation Model for Image Segmentation](../Images/7f04ca955056286284698c7ed2ae9cb9.png)'
  prefs: []
  type: TYPE_IMG
- en: Bounding box using text prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '**Integration with other systems**: SAM can accept input prompts from other
    systems, such as in the future taking the user''s gaze from an AR/VR headset and
    selecting objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extensible outputs**: The output masks can serve as inputs to other AI systems.
    For instance, object masks can be tracked in videos, enabled imaging editing applications,
    lifted into 3D space, or even used creatively such as collating'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Zero-shot generalization**: SAM has developed an understanding of objects
    which allows him to quickly adapt to unfamiliar ones without additional training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiple mask generation**: SAM can produce multiple valid masks when faced
    with uncertainty regarding an object being segmented, providing crucial assistance
    when solving segmentation in real-world settings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time mask generation**: SAM can generate a segmentation mask for any
    prompt in real time after precomputing the image embedding, enabling real-time
    interaction with the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Understanding SAM: How Does It Work?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![Segment Anything Model: Foundation Model for Image Segmentation](../Images/a46efe7d097d397cc92e12351492b055.png)'
  prefs: []
  type: TYPE_IMG
- en: Overview of SAM model / Image by [Segment Anything](https://arxiv.org/abs/2304.02643)
  prefs: []
  type: TYPE_NORMAL
- en: One of the recent advances in natural language processing and computer vision
    has been foundation models that enable zero-shot and few-shot learning for new
    datasets and tasks through “prompting”. Meta AI researchers trained SAM to return
    a valid segmentation mask for any prompt, such as foreground/background points,
    rough boxes/masks or masks, freeform text, or any information indicating the target
    object within an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'A valid mask simply means that even when the prompt could refer to multiple
    objects (for instance: one point on a shirt may represent both itself or someone
    wearing it), its output should provide a reasonable mask for one object only?—?thus
    pre-training the model and solving general downstream segmentation tasks via prompting.'
  prefs: []
  type: TYPE_NORMAL
- en: The researchers observed that pretraining tasks and interactive data collection
    imposed specific constraints on model design. Most significantly, real-time simulation
    must run efficiently on a CPU in a web browser to allow annotators to use SAM
    interactively in real-time for efficient annotation. Although runtime constraints
    resulted in tradeoffs between quality and runtime constraints, simple designs
    produced satisfactory results in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Underneath SAM’s hood, an image encoder generates a one-time embedding for images
    while a lightweight encoder converts any prompt into an embedding vector in real-time.
    These information sources are then combined by a lightweight decoder that predicts
    segmentation masks based on image embeddings computed with SAM, so SAM can produce
    segments in just 50 milliseconds for any given prompt in a web browser.
  prefs: []
  type: TYPE_NORMAL
- en: 'Building SA-1B: Segmenting 1 Billion Masks'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building and training the model requires access to an enormous and diverse pool
    of data that did not exist at the start of training. Today’s segmentation dataset
    release is by far the largest to date. Annotators used SAM interactively annotate
    images before updating SAM with this new data?—?repeating this cycle many times
    to continuously refine both the model and dataset.
  prefs: []
  type: TYPE_NORMAL
- en: SAM makes collecting segmentation masks faster than ever, taking only 14 seconds
    per mask annotated interactively; that process is only two times slower than annotating
    bounding boxes which take only 7 seconds using fast annotation interfaces. Comparable
    large-scale segmentation data collection efforts include COCO fully manual polygon-based
    mask annotation which takes about 10 hours; SAM model-assisted annotation efforts
    were even faster; its annotation time per mask annotated was 6.5x faster versus
    2x slower in terms of data annotation time than previous model assisted large
    scale data annotations efforts!
  prefs: []
  type: TYPE_NORMAL
- en: Interactively annotating masks is insufficient to generate the SA-1B dataset;
    thus a data engine was developed. This data engine contains three “gears”, starting
    with assisted annotators before moving onto fully automated annotation combined
    with assisted annotation to increase the diversity of collected masks and finally
    fully automatic mask creation for the dataset to scale.
  prefs: []
  type: TYPE_NORMAL
- en: SA-1B’s final dataset features more than 1.1 billion segmentation masks collected
    on over 11 million licensed and privacy-preserving images, making up 4 times as
    many masks than any existing segmentation dataset, according to human evaluation
    studies. As verified by these human assessments, these masks exhibit high quality
    and diversity compared with previous manually annotated datasets with much smaller
    sample sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Images for SA-1B were obtained via an image provider from multiple countries
    that represented different geographic regions and income levels. While certain
    geographic regions remain underrepresented, SA-1B provides greater representation
    due to its larger number of images and overall better coverage across all regions.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers conducted tests aimed at uncovering any biases in the model across
    gender presentation, skin tone perception, the age range of people as well as
    the perceived age of persons presented, finding that the SAM model performed similarly
    across various groups. They hope this will make the resulting work more equitable
    when applied in real-world use cases.
  prefs: []
  type: TYPE_NORMAL
- en: While SA-1B enabled the research output, it can also enable other researchers
    to train foundation models for image segmentation. Furthermore, this data may
    become the foundation for new datasets with additional annotations.
  prefs: []
  type: TYPE_NORMAL
- en: Future Work & Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Meta AI researchers hope that by sharing their research and dataset, they can
    accelerate the research in image segmentation and image and video understanding.
    Since this segmentation model can perform this function as part of larger systems.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we covered what is SAM and its capability and use cases. After
    that, we went through how it works, and how it was trained so as to give an overview
    of the model. Finally, we conclude the article with the future vision and work.
    If you would like to know more about SAM make sure to read the [paper](https://arxiv.org/abs/2304.02643)
    and try the [demo](https://segment-anything.com/demo).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Introducing Segment Anything: Working toward the first foundation model for
    image segmentation](https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SA-1B Dataset](https://ai.facebook.com/datasets/segment-anything/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Segment Anything](https://arxiv.org/abs/2304.02643?fbclid=IwAR1Hl8xCPsDfrqqKSVfRwBzSOkKiBDXO2i0HhCvwHE6uvAq_9YtiOfvYsfs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[Youssef Rafaat](https://www.linkedin.com/in/youssef-hosni-b2960b135)** is
    a computer vision researcher & data scientist. His research focuses on developing
    real-time computer vision algorithms for healthcare applications. He also worked
    as a data scientist for more than 3 years in the marketing, finance, and healthcare
    domain.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Learning How to Use ChatGPT to Learn Python (or anything else)](https://www.kdnuggets.com/2023/02/learn-python-chatgpt.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Are Foundation Models and How Do They Work?](https://www.kdnuggets.com/2023/05/foundation-models-work.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using Cluster Analysis to Segment Your Data](https://www.kdnuggets.com/using-cluster-analysis-to-segment-your-data)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Guide to Train an Image Classification Model Using Tensorflow](https://www.kdnuggets.com/2022/12/guide-train-image-classification-model-tensorflow.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Misconceptions About Semantic Segmentation Annotation](https://www.kdnuggets.com/2022/01/misconceptions-semantic-segmentation-annotation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Customer Segmentation in Python: A Practical Approach](https://www.kdnuggets.com/customer-segmentation-in-python-a-practical-approach)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
