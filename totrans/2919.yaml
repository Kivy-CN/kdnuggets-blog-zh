- en: Classifying Heart Disease Using K-Nearest Neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/07/classifying-heart-disease-using-k-nearest-neighbors.html](https://www.kdnuggets.com/2019/07/classifying-heart-disease-using-k-nearest-neighbors.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](/2019/07/classifying-heart-disease-using-k-nearest-neighbors.html?page=2#comments)'
  prefs: []
  type: TYPE_IMG
- en: Classification of objects is an important area of research and application in
    a variety of fields. In the presence of full knowledge of the underlying probabilities,
    Bayes decision theory gives optimal error rates. In those cases where this information
    is not present, many algorithms make use of distance or similarity among samples
    as a means of classification.
  prefs: []
  type: TYPE_NORMAL
- en: The article has been divided into 2 parts. In the first part, we’ll talk all
    about the K-NN machine learning algorithm and in the second part, we will implement
    K-NN in real life and classify Heart disease patients.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table of content**'
  prefs: []
  type: TYPE_NORMAL
- en: What is a K-NN algorithm?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the K-NN algorithm work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When to choose K-NN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to choose the optimal value of K?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Curse of dimensionality?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Building K-NN classifier using python sci-kit learn.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to improve the performance of your classifier?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a K-NN Algorithm?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/fe225aa90f1f3ec39506d32db01afc17.png)'
  prefs: []
  type: TYPE_IMG
- en: '[K-NN Algorithm representation](https://acadgild.com/blog/k-nearest-neighbor-algorithm)'
  prefs: []
  type: TYPE_NORMAL
- en: K-NN or K-Nearest Neighbors is one of the most famous classification algorithms
    as of now in the industry simply because of its simplicity and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: K-NN is a simple algorithm that stores all available cases and classifies new
    cases based on a similarity measure (e.g., distance functions). KNN has been used
    in statistical estimation and pattern recognition already at the beginning of
    the 1970s as a non-parametric technique.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm assumes that similar things exist in close proximity. In other
    words, entities which are similar exist together.
  prefs: []
  type: TYPE_NORMAL
- en: How the K-NN algorithm works?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In K-NN, K is the number of nearest neighbors. The number of neighbors is the
    core deciding factor. **K is generally an odd number if the number of classes
    is 2**. When K=1, then the algorithm is known as the nearest neighbor algorithm.
    This is the simplest case.
  prefs: []
  type: TYPE_NORMAL
- en: In the below figure, suppose yellow colored “**?**” let's say P is the point,
    for which label needs to predict. First, you find the one closest point to P and
    then the label of the nearest point assigned to P.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/861a2f7b786e09051bbd0d37b8db1857.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Second, you find the k closest point to P and then classify points by majority
    vote of its K neighbors. Each object votes for their class and the class with
    the most votes is taken as the prediction. For finding closest similar points,
    we find the distance between points using distance measures such as Euclidean
    distance, Hamming distance, Manhattan distance, and Minkowski distance. The algorithm
    has the following basic steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate distance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find closest neighbors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vote for labels
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/be2fc1f9343745ff2ec1e5a9747b2f7a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Three most commonly used distance measures used to calculate the distance between
    point P and its nearest neighbors are represented as :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/82d48a4486aae9019ed734c5115b1878.png)'
  prefs: []
  type: TYPE_IMG
- en: In this article we will go ahead with Euclidean distance, so let's understand
    it first.
  prefs: []
  type: TYPE_NORMAL
- en: '**Euclidean distance:** It is the most commonly used distance measure also
    called simply distance. The usage of a Euclidean distance measure is highly recommended
    when the data is dense or continuous. Euclidean distance is the best proximity
    measure. The Euclidean distance between two points is the length of the path connecting
    them. The Pythagorean theorem gives this distance between two points.'
  prefs: []
  type: TYPE_NORMAL
- en: Below figure shows how to calculate Euclidean distance between two points in
    a 2-dimensional plane.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/181a263e28b55514150e7ae38d495336.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Euclidean distance between two points in 2-D](https://mccormickml.com/2013/08/15/the-gaussian-kernel/)'
  prefs: []
  type: TYPE_NORMAL
- en: When to use K-NN algorithm?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'KNN can be used for both classification and regression predictive problems.
    However, it is more widely used in classification problems in the industry. To
    evaluate any technique we generally look at 3 important aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: Ease to interpret the output
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculation time of the algorithm
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Predictive Power
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let us compare KNN with different models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/d52f21040e5ef16e8e24e34609aa2528.png)'
  prefs: []
  type: TYPE_IMG
- en: 'source: Analytics Vidhya'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see K-NN surpasses Logistic Regression, CART and Random Forest in
    terms of the aspects which we are considering.
  prefs: []
  type: TYPE_NORMAL
- en: How to choose the optimal value of K?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The number of neighbors(K) in K-NN is a hyperparameter that you need to choose
    at the time of building your model. You can think of K as a controlling variable
    for the prediction model.
  prefs: []
  type: TYPE_NORMAL
- en: Now, choosing the optimal value for K is best done by first inspecting the data.
    In general, a large K value is more precise as it reduces the overall noise but
    there is no guarantee. Cross-validation is another way to retrospectively determine
    a good K value by using an independent dataset to validate the K value. Historically,
    the optimal K for most datasets has been between 3–10\. That produces much better
    results than 1NN(when K=1).
  prefs: []
  type: TYPE_NORMAL
- en: Generally, an odd number is chosen if the number of classes is even. You can
    also check by generating the model on different values of K and check their performance.
  prefs: []
  type: TYPE_NORMAL
- en: Curse of Dimensionality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: K-NN performs better with a lower number of features than a large number of
    features. You can say that when the number of features increases than it requires
    more data. Increase in dimension also leads to the problem of overfitting. To
    avoid overfitting, the needed data will need to grow exponentially as you increase
    the number of dimensions. This problem of higher dimension is known as the Curse
    of Dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/40e1bba71a6b963a2370165dc5a23d79.png)'
  prefs: []
  type: TYPE_IMG
- en: From the above graphical representation, it is clearly visible that the performance
    of your model decreases with an increase in the number of features(dimensions).
  prefs: []
  type: TYPE_NORMAL
- en: To deal with the problem of the curse of dimensionality, you need to perform
    principal component analysis(PCA) before applying any machine learning algorithm,
    or you can also use feature selection approach. Research has shown that in large
    dimension Euclidean distance is not useful anymore. Therefore, you can prefer
    other measures such as cosine similarity, which get decidedly less affected by
    high dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '**The KNN algorithm can compete with the most accurate models because it makes
    highly accurate predictions. Therefore, you can use the KNN algorithm for applications
    that require high accuracy but that do not require a human-readable model. — **source:[IBM](https://www.ibm.com/support/knowledgecenter/en/SS6NHC/com.ibm.swg.im.dashdb.analytics.doc/doc/r_knn_usage.html)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Steps to compute K-NN algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Determine parameter K = number of nearest neighbors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the distance between the query-instance and all the training samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort the distance and determine nearest neighbors based on the K-th minimum
    distance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gather the category of the nearest neighbors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a simple majority of the category of nearest neighbors as the prediction
    value of the query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next section, we are going to solve a real world scenario using K-NN
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[From Theory to Practice: Building a k-Nearest Neighbors Classifier](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Nearest Neighbors for Classification](https://www.kdnuggets.com/2022/04/nearest-neighbors-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[K-nearest Neighbors in Scikit-learn](https://www.kdnuggets.com/2022/07/knearest-neighbors-scikitlearn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Classifying Long Text Documents Using BERT](https://www.kdnuggets.com/2022/02/classifying-long-text-documents-bert.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Automate Microsoft Excel and Word Using Python](https://www.kdnuggets.com/2021/08/automate-microsoft-excel-word-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Determine the Best Fitting Data Distribution Using Python](https://www.kdnuggets.com/2021/09/determine-best-fitting-data-distribution-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
