# 从零开始的 Python 机器学习工作流 第二部分：k-means 聚类

> 原文：[https://www.kdnuggets.com/2017/06/machine-learning-workflows-python-scratch-part-2.html](https://www.kdnuggets.com/2017/06/machine-learning-workflows-python-scratch-part-2.html)

在这一系列的第一部分中，我们开始得相当缓慢但有条不紊。[上一篇文章](/2017/05/machine-learning-workflows-python-scratch-part-1.html)阐述了我们的目标，并开始了机器学习工作流和管道的一些基本构建块。如果你还没有阅读该系列的第一部分，建议你先阅读。

这次我们将加快进度，通过实现 k-means 聚类算法来进行。我们将在编码过程中讨论 k-means 的具体方面，但如果你对算法的概述以及它如何与其他聚类方法相关感兴趣，可以[查看这个](https://www.kdnuggets.com/2017/06/machine-learning-workflows-python-scratch-part-2.html)。

![ML workflows header](../Images/decb4017351d3ef6e708866e8c04fed4.png)

从零开始的 Python 中的 k-means 聚类算法。

继续进行的唯一真正先决条件是我们在第一篇文章中创建的 [dataset.py](https://gist.github.com/mmmayo13/9859a457760db10ec4842be3aa1a2334) 模块，以及原始的 [iris.csv](https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv) 文件，所以确保你手头有这两个文件。

### k-means 聚类算法

k-means 是一种简单但通常有效的聚类方法。k 个点被随机选择作为聚类中心或质心，所有训练实例被绘制并分配到最近的聚类中。在所有实例被分配到聚类后，代表每个聚类实例均值的质心被重新计算，这些重新计算的质心成为各自聚类的新中心。

在此时，所有的聚类成员资格被重置，所有训练集的实例被重新绘制并分配到其最近的、可能重新定位的聚类中。这个迭代过程会持续进行，直到质心或其成员资格没有变化，聚类被视为稳定。

就聚类方法而言，k-means 几乎是最简单的——它的概念简单优雅，几乎可以说是诗意的。它也是一个经过验证的工作马，具有持久性，常常产生有用的结果。

简而言之，我们要做的是编码一个简单的 k-means 实现，它将相似的东西分组在一起，至少在理论上将不同的东西分开。很简单。请记住，“相似”在这里被简化为“在欧几里得空间中相对紧密地共处”，或者类似非常非哲学的东西。

我们在这里需要一些函数。考虑算法中涉及的步骤可以帮助我们确定这些函数是什么。

+   设置初始质心

+   测量数据实例与质心之间的距离

+   将数据实例添加为最近质心的成员

+   重新计算质心

+   如有必要，重新测量、重新聚类、重新计算

这就是算法的要点。但在我们到达那里之前，暂时后退一步。

### 数据准备……再一次

在写这篇文章时，我终于意识到我们数据准备工作流中缺少了一个重要部分。在我们将 pandas DataFrame 转换为 numpy ndarray（矩阵）之前，我们需要确保我们的数值*实际上*是数值，而不是伪装成数值的字符串。由于上次我们从 CSV 文件读取数据，即使是我们的数值也被存储为字符串（可以从上一篇文章底部看出，数字被单引号包围——例如 '5.7'）。

因此，处理此问题的最佳方法是创建另一个函数，并将其添加到我们的 dataset.py 文件中，该函数将字符串转换为其数值表示（我们已经有一个将类别名称转换为数值的函数，并跟踪这些变化）。在我调整函数的过程中，它经历了 3 次具体的迭代，分别是：1) 一个数据集和一个属性名称，其对应列的所有值应从字符串转换为浮点数；2) 一个数据集和一个属性名称列表……；以及 3) 一个数据集和单个属性作为字符串，或属性列表作为 **喘息** 列表。

最终迭代（第三个，更灵活的选项）如下所示。让我们将其添加到上次的 dataset.py 模块中。

好了，处理完这些问题后，我们就可以加载数据集，清理数据，并创建一个（完全数字化的）矩阵表示，然后将其输入到我们的 k-means 聚类算法中，一旦我们有了它。说到这儿……

### 初始化质心

我们的算法需要做的第一件事是创建一组 k 个初始质心。有多种方法可以实现这一点，但我们将从最基本的开始：随机初始化。我们需要一个接受数据集和整数 k 的函数，并返回该数量的随机生成的质心的 ndarray。

你可能已经想象过随机初始化可能出现的问题。举个简单的例子，考虑在二维空间中存在 5 个不同且紧密聚集的类别，以及一组初始化不佳的质心，如下所示。

![初始化不佳](../Images/a358cc3edb5802c92183f5a3e01fc170.png)

显然是非最优的质心初始化。

即使没有数学支持直觉，也很明显这些质心并没有被最优地放置。然而，k-means 的一个强大特性是它能够从这种初始化中恢复，随着聚类的多轮迭代，通过最小化簇实例成员和簇质心之间的平均距离，逐步趋向于最优放置。

尽管这可能发生得非常快，但对于大量高维数据的糟糕初始化可能会导致更多的聚类迭代。随机放置的初始数据空间调查也可能变得冗长。因此，有一些替代的质心初始化方法，我们可能在未来会考虑。

关于测试我们代码的一个快速说明：在我们进行的过程中用高度构造的场景进行测试似乎不值得，因此我们会等到最后看看整体效果。

### 测量距离

手头有数据集和一组初始化的质心后，我们最终需要进行大量的测量计算。实际上，对于每次聚类迭代，我们必须测量每个数据点到每个质心的距离，以确定实例属于哪个簇（也许是暂时的）。所以让我们为欧几里得空间编写一个测量函数。我们将在这里使用[Scipy](https://www.scipy.org/)进行繁重的计算；虽然编写距离测量并不困难，[Scipy 包含一个函数](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.euclidean.html)专门优化用于向量计算，这正是我们将要做的。

让我们把这个功能封装到我们自己的函数中，以便以后我们想要更改或实验距离计算方式时使用。

通过初始化质心并进行测量，我们现在可以编写一个函数来控制聚类算法的逻辑，并执行一些额外的必要步骤。

### 聚类实例

在查看代码之前，这里是我们算法将遵循的过程的简单概述，考虑到上面和下面的一些函数。

![k-means 图示](../Images/5d017022938992c978fc2e1c85279b66.png)

我们的 k-means 聚类算法过程。

下面的代码有详细的注释，但让我们逐一了解几个要点。

首先，我们的函数接受一个 numpy ndarray 数据集以及我们希望在聚类过程中使用的聚类数量。它会返回几个东西：

+   聚类完成后的结果质心

+   聚类后的簇分配

+   聚类算法所需的迭代次数

+   原始质心

创建一个 ndarray 来存储实例簇分配及其误差，然后初始化质心，并保留一个副本以供后续返回。

然后 while 循环继续，直到聚类分配没有变化，这意味着已达到收敛——注意，在此时并没有额外的机制来限制迭代次数。计算实例到质心的距离，并跟踪最小距离，这用于确定聚类分配。然后记录最近的质心以及两个实体之间的实际距离（误差），并检查特定实例的聚类分配是否发生了变化。

在对每个实例执行此操作后，通过使用成员实例的均值作为质心坐标，来更新质心位置。迭代次数也会被记录。当检测到已达到收敛时，控制将跳出 while 循环，并返回上述项目。

我想指出的是，一些代码以及额外的灵感来自于 Peter Harrington 的书《[机器学习实战](https://www.amazon.com/Machine-Learning-Action-Peter-Harrington/dp/1617290181/)》（MLIA）。我在这本书首次发布时就购买了它，并且它已经证明对我来说是非常宝贵的，原因与这本书经常受到的批评有关，大多数批评集中在理论不足和/或代码问题上。然而，我认为这两者实际上都是资产。如果你像我一样，已经具备理论理解，并且愿意并且能够处理可能需要调整的代码，或者可以提供自己的修改，那么 MLIA 对任何想要首次尝试编写机器学习算法的人来说，都是一个有用的资源。

### 测试我们的 k-means 聚类算法

有几个典型任务我们将跳过，稍后再回顾，但我想在这里指出这些任务。

首先，在进行聚类时，特别是当属性具有不同的尺度时，通常最好至少考虑对数据进行缩放或归一化，以确保某个尺度远大于其他尺度的单一属性（或属性集合）不会占据不应有的权重。如果我们有 3 个属性，其中前两个在 [0, 1] 范围内，第三个在 [0, 100] 范围内，那么很容易看出第三个变量将主导测量结果和随后的聚类成员分配。

其次，在聚类（如同大多数机器学习任务）时，我们可以将数据集分成训练集和测试集，允许我们在一个子集上训练模型，然后在另一个（独立的）数据集上测试模型。虽然这并不总是给定聚类任务的目标，因为我们可能只是想构建一个聚类模型而不关心将其用于分类后续实例，但这往往是有用的。我们将在下面的测试代码中继续进行，而此时不考虑这两者，但可能会在后续的帖子中重新回顾。

在继续之前，请确保你已将上面概述的数据集相关函数添加到现有的 [dataset.py](https://gist.github.com/mmmayo13/935684dd226ef05f7d291e8cf5ed873a) 模块中，并已创建一个 [kmeans.py](https://gist.github.com/mmmayo13/956937ec1fc695163b8e052b55c09208) 模块来容纳所有相关函数。

让我们尝试一下我们的代码：

```py
Number of iterations: 5

Final centroids:
[[ 6.62244898  2.98367347  5.57346939  2.03265306  2\.        ]
 [ 5.006       3.418       1.464       0.244       0\.        ]
 [ 5.91568627  2.76470588  4.26470588  1.33333333  1.01960784]]

Cluster membership and error of first 10 instances:
[[ 1\.        0.021592]
 [ 1\.        0.191992]
 [ 1\.        0.169992]
 [ 1\.        0.269192]
 [ 1\.        0.039192]
 [ 1\.        0.467592]
 [ 1\.        0.172392]
 [ 1\.        0.003592]
 [ 1\.        0.641592]
 [ 1\.        0.134392]]

Original centroids:
[[ 4.38924071  3.94546253  5.49200482  0.40216215  1.95277771]
 [ 5.43873792  3.58653594  2.73064731  0.79820023  0.97661014]
 [ 4.62570586  2.46497863  3.14311939  2.4121321   0.43495676]]
```

看起来不错！这次我们代码的测试结果如上所示，但你会发现随后的迭代会返回不同的结果——至少是不同的迭代次数和初始质心集合。

### 展望未来

尽管我们还没有评估我们的聚类结果，但我们现在暂时停在这里……不过，说到这一点，我敢打赌你可以猜到下一篇文章会涉及什么。下次我们将重点关注更多与聚类相关的活动。我们有一个可以用来构建模型的算法，但还需要一些机制来评估和可视化它们的结果。这将是我们接下来要做的事情。

进一步思考，我计划将注意力转向使用k最近邻算法进行分类，以及一些与分类相关的任务。再次希望你觉得这部分内容足够有用，可以查看下一期内容。

**相关**：

+   [从头开始的Python机器学习工作流第1部分：数据准备](/2017/05/machine-learning-workflows-python-scratch-part-1.html)

+   [利用朴素分片质心初始化方法提升k-means聚类效率](/2017/03/naive-sharding-centroid-initialization-method.html)

+   [K-Means与其他聚类算法：使用Python的快速入门](/2017/03/k-means-clustering-algorithms-intro-python.html)

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速入门网络安全职业生涯

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织进行IT管理

* * *

### 更多相关内容

+   [聚类揭秘：理解 K-Means 聚类](https://www.kdnuggets.com/2023/07/clustering-unleashed-understanding-kmeans-clustering.html)

+   [无监督学习实操：K-Means 聚类](https://www.kdnuggets.com/handson-with-unsupervised-learning-kmeans-clustering)

+   [K-Means 聚类的质心初始化方法](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)

+   [什么是 K-Means 聚类及其算法如何工作？](https://www.kdnuggets.com/2023/05/kmeans-clustering-algorithm-work.html)

+   [机器学习中的 DBSCAN 聚类算法](https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html)

+   [使用 PyCaret 在 Python 中进行聚类的简介](https://www.kdnuggets.com/2021/12/introduction-clustering-python-pycaret.html)
