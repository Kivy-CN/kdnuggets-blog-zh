# 《集成学习综合指南 – 你需要了解的全部信息》

> 原文：[https://www.kdnuggets.com/2021/05/comprehensive-guide-ensemble-learning.html](https://www.kdnuggets.com/2021/05/comprehensive-guide-ensemble-learning.html)

[评论](#comments)

[集成学习技术](https://www.toptal.com/machine-learning/ensemble-methods-machine-learning)已经被证明在机器学习问题上表现更佳。我们可以将这些技术用于回归以及分类问题。

这些集成技术的最终预测是通过结合多个基础模型的结果来获得的。平均法、投票法和堆叠法是将结果结合以获得最终预测的一些方法。

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业轨道

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 在 IT 领域支持你的组织

* * *

在这篇文章中，我们将探讨如何使用集成学习来制定最佳的机器学习模型。

### 什么是集成学习？

集成学习是将多个机器学习模型结合在一个问题上的方法。这些模型被称为弱学习者。直观上，当你将多个弱学习者结合起来时，它们可以成为强学习者。

每个弱学习者都在训练集上进行训练，并提供获得的预测。最终预测结果是通过结合所有弱学习者的结果来计算的。

### 基本集成学习技术

让我们花一点时间看看简单的集成学习技术。

**最大投票法**

在分类中，每个模型的预测都是一次投票。在最大投票中，最终预测来自于获得最多投票的预测。

让我们以三个分类器的预测结果为例：

+   分类器 1 – 类别 A

+   分类器 2 – 类别 B

+   分类器 3 – 类别 B

这里的最终预测将是类别 B，因为它获得了最多的投票。

**平均法**

在平均法中，最终输出是所有预测的平均值。这适用于回归问题。例如，在 [随机森林回归](https://neptune.ai/blog/random-forest-regression-when-does-it-fail-and-why) 中，最终结果是来自各个决策树预测的平均值。

让我们以三个回归模型预测商品价格的例子来说明：

+   回归模型 1 – 200

+   回归模型 2 – 300

+   回归模型 3 – 400

最终预测将是 200、300 和 400 的平均值。

**加权平均**

在加权平均中，具有更高预测能力的基础模型更为重要。在价格预测示例中，每个回归器都会被分配一个权重。

权重的总和应等于一。假设回归器的权重分别为0.35、0.45和0.2。最终模型预测可以按如下方式计算：

0.35 * 200 + 0.45*300 + 0.2*400 = 285

### 高级集成学习技术

上述是简单技术，现在让我们看看高级集成学习技术。

### 堆叠

堆叠是将各种估计器组合在一起以减少它们的偏差的过程。每个估计器的预测结果被堆叠在一起，并作为最终估计器（通常称为*元模型*）的输入，最终估计器计算最终预测。最终估计器的训练通过交叉验证完成。

堆叠可以用于回归和分类问题。

![集成学习技术](../Images/171b9e69dd3a956d32a20582ebe4f151.png)

[*来源*](https://towardsdatascience.com/stacking-classifiers-for-higher-predictive-performance-566f963e4840)

堆叠可以被认为发生在以下步骤中：

1.  将数据拆分为训练集和验证集，

1.  将训练集划分为K折，例如10折，

1.  在9个折叠上训练基础模型（例如SVM），并对第10个折叠进行预测，

1.  重复直到每个折叠都有预测结果，

1.  在整个训练集上拟合基础模型，

1.  使用模型对测试集进行预测，

1.  对其他基础模型（例如决策树）重复步骤3-6，

1.  使用测试集的预测结果作为新模型的特征——*元模型*，

1.  使用元模型对测试集进行最终预测。

在回归问题中，传递给元模型的值是数值型的。在分类问题中，它们是概率或类别标签。

### 混合

混合类似于堆叠，但使用从训练集中保留的集合进行预测。因此，预测仅在保留集上进行。预测和保留集用于构建一个最终模型，该模型在测试集上进行预测。

你可以将混合看作是堆叠的一种类型，其中*元模型*在基础模型对保留验证集做出的预测上进行训练。

你可以将*混合*过程视为：

+   将数据拆分为测试集和验证集，

+   在验证集上拟合基础模型，

+   对验证集和测试集进行预测，

+   使用验证集及其预测结果来构建最终模型，

+   使用此模型进行最终预测。

混合的概念[由 Netflix 奖竞赛](https://netflixprize.com/assets/ProgressPrize2008_BellKor.pdf)流行开来。获胜团队使用了混合解决方案，在 Netflix 的电影推荐算法上实现了10倍的性能提升。

根据[Kaggle 集成指南](https://mlwave.com/kaggle-ensembling-guide/)：

> “混合是Netflix获奖者提出的一个词。它与堆叠泛化非常相似，但稍微简单一些且信息泄漏风险较低。一些研究人员将“堆叠集成”和“混合”互换使用。
> 
> 在混合中，而不是为训练集创建折叠外预测，你需要创建一个比如说占训练集10%的小保留集。然后，堆叠模型仅在这个保留集上进行训练。”

### 混合与堆叠

混合比堆叠更简单，并且防止了模型中的信息泄漏。一般化模型和堆叠模型使用不同的数据集。然而，混合使用的数据较少，可能导致过拟合。

交叉验证在堆叠方法中比混合方法更为可靠。它是通过更多的折叠来计算的，而不是在混合中使用一个小的保留数据集。

### Bagging

Bagging会随机抽取数据样本，构建学习算法，并使用均值来找出bagging概率。它也被称为*自助聚合*。Bagging通过汇总多个模型的结果来获得一个通用的结果。

该方法包括：

+   从原始数据集中创建多个子集并进行替换，

+   为每个子集构建基础模型，

+   并行运行所有模型，

+   将所有模型的预测结果结合起来以获得最终预测。

### 提升

提升是一种机器学习集成技术，通过将弱学习者转换为强学习者来减少偏差和方差。弱学习者以顺序方式应用于数据集。第一步是构建初始模型并将其拟合到训练集上。

第二个模型尝试修正第一个模型产生的错误。整个过程如下：

+   从原始数据中创建一个子集，

+   使用这些数据构建初始模型，

+   在整个数据集上运行预测，

+   使用预测结果和实际值来计算错误，

+   对错误的预测分配更多的权重，

+   创建另一个模型，尝试修正上一个模型的错误，

+   使用新模型在整个数据集上进行预测，

+   创建多个模型，每个模型旨在修正前一个模型产生的错误，

+   通过对所有模型的均值加权来获得最终模型。

### 集成学习库

在介绍完这些之后，我们来谈谈你可以用于集成的方法。广义上讲，有两类：

+   Bagging算法，

+   提升算法。

### Bagging算法

Bagging算法基于上述描述的bagging技术。我们来看看其中的一些。

**Bagging元估计器**

Scikit-learn 让我们可以实现一个 `[BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)` 和一个 `[BaggingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html)`。bagging 元估算器在原始数据集的随机子集上拟合每个基础模型。然后通过聚合个体基础模型的预测来计算最终预测。聚合是通过投票或平均完成的。该方法通过在构建过程中引入随机化来减少估算器的方差。

bagging 有几种不同的形式：

+   从数据中绘制随机子集作为样本的随机子集被称为 *pasting*。

+   当样本是带替换地抽取时，算法被称为 *bagging*。

+   如果随机数据子集被视为特征的随机子集，则算法被称为 *Random Subspaces*。

+   当你从样本和特征的子集创建基础估算器时，这被称为 *Random Patches*。

让我们看看如何使用 Scikit-learn 创建一个 bagging 估算器。

这需要几个步骤：

+   导入 `BaggingClassifier`。

+   导入一个基础估算器——决策树分类器，

+   创建一个 `BaggingClassifier` 实例。

[PRE0]

bagging 分类器接受几个参数：

+   基础估算器——这里是决策树分类器，

+   你想要在集成中包含的估算器数量，

+   `max_samples` 用来定义从训练集中抽取的样本数量，用于每个基础估算器，

+   `max_features` 用来指定将用于训练每个基础估算器的特征数量。

接下来，你可以在训练集上拟合这个分类器并对其进行评分。

[PRE1]

这个过程对于回归问题是相同的，唯一的区别是你将使用回归估算器。

[PRE2]

**随机树森林**

一个 [随机森林](https://neptune.ai/blog/random-forest-regression-when-does-it-fail-and-why)® 是一个随机决策树的集成。每棵决策树是从数据集的不同样本中创建的。这些样本是带替换地抽取的。每棵树都会产生自己的预测。

在回归中，这些结果会被平均以获得最终结果。

在分类中，最终结果可以作为票数最多的类别来获得。

平均和投票通过防止过拟合来提高模型的准确性。

在 Scikit-learn 中，随机森林可以通过 `RandomForestClassifier` 和 `ExtraTreesClassifier` 实现。类似的估算器也适用于回归问题。

[PRE3]

### 提升算法

这些算法基于之前描述的提升框架。我们来看看其中几个。

**AdaBoost**

AdaBoost通过拟合一系列弱学习器来工作。它在后续迭代中给予错误预测更多的权重，而对正确预测给予较少的权重。这迫使算法关注更难预测的观察结果。最终预测来自于加权多数投票或求和。

AdaBoost可以用于回归和分类问题。让我们花一点时间来看看如何使用Scikit-learn将该算法应用于分类问题。

我们使用`AdaBoostClassifier`。`n_estimators`决定了集成中弱学习器的数量。每个弱学习器对最终组合的贡献由`learning_rate`控制。

默认情况下，使用决策树作为基础估计器。为了获得更好的结果，可以调整决策树的参数。你还可以调整基础估计器的数量。

[PRE4]

**梯度树提升**

梯度树提升还将一组弱学习器组合成一个强学习器。关于梯度提升树，有三个主要事项需要注意：

+   需要使用差异损失函数，

+   决策树作为弱学习器，

+   这是一个加法模型，所以树一个接一个地添加。梯度下降法用于在添加后续树时最小化损失。

你可以使用Scikit-learn来构建基于梯度树提升的模型。

[PRE5]

**极端梯度提升**

极端梯度提升，通常被称为 [XGoost](https://neptune.ai/blog/how-to-organize-your-xgboost-machine-learning-ml-model-development-process)，是一个顶级的梯度提升框架。它基于一组弱决策树，可以在单台计算机上进行并行计算。

该算法使用回归树作为基础学习器。它还内置了交叉验证。开发人员喜欢它的准确性、效率和可行性。

[PRE6]

**LightGBM**

[LightGBM](https://lightgbm.readthedocs.io/en/latest/) 是一个基于树学习的梯度提升算法。与使用深度增长的其他树基算法不同，LightGBM使用叶子-wise树增长。叶子-wise增长算法通常比基于深度的算法收敛更快。

![层级-wise树增长](../Images/4d9ff80253d95bfb7b410103ab1659c3.png)[*来源*](https://lightgbm.readthedocs.io/en/latest/Features.html?highlight=dart#other-features)

![叶子-wise树增长](../Images/a8823777b46d64bee45eb7773159b426.png)[*来源*](https://lightgbm.readthedocs.io/en/latest/Features.html?highlight=dart#other-features)

![叶子-wise树增长](../Images/f8c9ea60502b0df73fed238f77564c07.png)[*来源*](https://lightgbm.readthedocs.io/en/latest/Features.html?highlight=dart#other-features)

LightGBM可以通过设置适当的目标用于回归和分类问题。

以下是如何将LightGBM应用于二分类问题的方法。

[PRE7]

**CatBoost**

[CatBoost](https://github.com/catboost) 是由 [Yandex](https://yandex.com/company/) 开发的深度梯度提升库。它使用遗忘决策树来生长平衡树。正如下面的图片所示，每一层的左侧和右侧分裂时使用的是相同的特征。

![Gradient boosting catboost](../Images/4b59e5ba164ef50d380c1c489426a112.png)*[Source](https://heartbeat.fritz.ai/fast-gradient-boosting-with-catboost-38779b0d5d9a)*

研究人员需要 Catboost 的原因如下：

+   处理分类特征的原生能力，

+   模型可以在多个 GPU 上进行训练，

+   它通过提供默认参数的出色结果来减少参数调整时间，

+   模型可以导出到 Core ML 以进行设备上的推理（iOS），

+   它内部处理缺失值，

+   它可用于回归和分类问题。

这是将 CatBoost 应用于分类问题的方法。

[PRE8]

### 帮助你在基础模型上进行堆叠的库

在堆叠时，个体模型的输出被堆叠，最终使用一个估计器来计算最终预测。估计器在整个训练集上进行拟合。最终估计器在基础估计器的交叉验证预测上进行训练。

Scikit-learn 可用于堆叠估计器。让我们看看如何为分类问题进行堆叠估计器。

首先，你需要设置你想要使用的基础估计器。

[PRE9]

接下来，实例化堆叠分类器。它的参数包括：

+   上述定义的估计器，

+   你想要使用的最终估计器。默认使用逻辑回归估计器，

+   `cv` 交叉验证生成器。默认使用 5 倍交叉验证，

+   `stack_method` 用于指定应用于每个估计器的方法。如果是 `auto`，它将按顺序尝试 `predict_proba`、`decision_function` 或 `predict`。

[PRE10]

然后，你可以将数据拟合到训练集，并在测试集上评分。

[PRE11]

Scikit-learn 还允许你实现投票估计器。它使用基础估计器的多数投票或概率平均值来进行最终预测。

这可以通过 `VotingClassifier` 实现分类问题，通过 `VotingRegressor` 实现回归问题。就像堆叠一样，你首先需要定义一组基础估计器。

让我们看看如何在分类问题中实现它。`VotingClassifier` 让你选择投票类型：

+   `soft` 意味着将使用概率的平均值来计算最终结果，

+   `hard` 通知分类器使用预测类别进行多数投票。

[PRE12]

投票回归器使用多个估计器，并将最终结果作为预测值的平均值返回。

### 使用 Mlxtend 进行堆叠

你也可以使用 [Mlxtend](http://rasbt.github.io/mlxtend/) 的`[StackingCVClassifier](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/)`进行堆叠。第一步是定义一组基础估计器，然后将估计器传递给分类器。

你还需要定义最终用于聚合预测的模型。在这种情况下，它是逻辑回归模型。

[PRE13]

### 何时使用集成学习

当你想提升机器学习模型的性能时，可以使用集成学习技术。例如，可以提高分类模型的准确性或减少回归模型的平均绝对误差。集成方法还会使模型更加稳定。

当你的模型在训练集上过拟合时，你也可以使用集成学习方法来创建一个更复杂的模型。集成中的模型将通过结合预测结果来提高数据集上的性能。

### 集成学习效果最佳的时机

集成学习在基础模型不相关时效果最佳。例如，你可以在不同的数据集或特征上训练不同的模型，如线性模型、决策树和神经网络。基础模型相关性越低，效果越好。

使用非相关模型的理念在于每个模型可能在解决其他模型的弱点。它们也有不同的优势，结合起来将形成一个表现良好的估计器。例如，仅创建树模型的集成可能不如将树型算法与其他类型算法结合起来有效。

### 最终想法

在本文中，我们探讨了如何使用集成学习来提升机器学习模型的性能。我们还介绍了各种可以用于集成的工具和技术。希望你的机器学习知识库有所增长。

祝你集成愉快！

### 资源

+   [Kaggle 集成学习指南](https://mlwave.com/kaggle-ensembling-guide/)

+   [Scikit-learn 集成学习指南](https://scikit-learn.org/stable/modules/ensemble.html)

+   [文章中使用的笔记本](https://colab.research.google.com/drive/1MEcl4W1Mr9_rRJEPcY2IHWppJq08bgc2?usp=sharing)

**个人简介: [德里克·穆伊提](https://www.linkedin.com/in/mwitiderrick/)** 是一位对分享知识充满热情的数据科学家。他是数据科学社区的积极贡献者，通过博客如 Heartbeat、Towards Data Science、Datacamp、Neptune AI 和 KDnuggets 等进行分享。他的内容在互联网上的浏览量超过一百万次。德里克还是一名作者和在线讲师。他还与各种机构合作，实施数据科学解决方案并提升员工技能。你可能想了解他的 [Python 数据科学与机器学习完整训练营课程](https://www.udemy.com/course/data-science-bootcamp-in-python/?referralCode=9F6DFBC3F92C44E8C7F4)。

[原文](https://neptune.ai/blog/ensemble-learning-guide)。转载许可。

**相关内容：**

+   [XGBoost：它是什么，以及何时使用它](/2020/12/xgboost-what-when.html)

+   [梯度提升决策树 – 概念性解释](/2021/04/gradient-boosted-trees-conceptual-explanation.html)

+   [Scikit-learn 的最佳机器学习框架及扩展](/2021/03/best-machine-learning-frameworks-extensions-scikit-learn.html)

### 更多相关内容

+   [KDnuggets 新闻，4月13日：数据科学家应该了解的 Python 库…](https://www.kdnuggets.com/2022/n15.html)

+   [带有示例的集成学习](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)

+   [集成学习技术：使用 Python 中的随机森林进行演练](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)

+   [什么时候集成技术是一个好的选择？](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)

+   [机器学习中的统计学：成为认证专家所需了解的内容](https://www.kdnuggets.com/2024/03/sas-statistics-machine-learning-need-know-become-certified-expert)

+   [想利用你的数据技能解决全球问题？这里有一些建议…](https://www.kdnuggets.com/2022/04/jhu-want-data-skills-solve-global-problems.html)
