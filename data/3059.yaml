- en: Introduction to Active Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活跃学习简介
- en: 原文：[https://www.kdnuggets.com/2018/10/introduction-active-learning.html](https://www.kdnuggets.com/2018/10/introduction-active-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/10/introduction-active-learning.html](https://www.kdnuggets.com/2018/10/introduction-active-learning.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Jennifer Prendki](https://www.linkedin.com/in/jennifer-prendki), VP of
    Machine Learning, Figure Eight**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Jennifer Prendki](https://www.linkedin.com/in/jennifer-prendki), Figure
    Eight的机器学习副总裁**'
- en: 'As data gets cheaper and cheaper to collect and store, data scientists are
    left with more data to deal with that they will ever be capable of analyzing.
    And this trend doesn’t show signs of slowing down: the explosion of IoT devices
    paired with the appearance of new memory-greedy data formats are leaving data
    professionals fending for themselves in an ocean of raw data.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据收集和存储变得越来越便宜，数据科学家面临的数据量超过了他们能够分析的能力。而且这一趋势没有减缓的迹象：物联网设备的爆炸性增长以及新的内存密集型数据格式的出现，使得数据专业人员在原始数据的海洋中挣扎。
- en: 'Given that the most exciting advances in machine learning require large volumes
    of data, this is an exciting time. But it also raises a brand new challenge for
    the ML community: unless the data is labelled, it remains essentially useless
    for all ML applications relying on a supervised learning approach.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于机器学习中最激动人心的进展需要大量数据，这确实是一个令人兴奋的时代。但这也给机器学习社区带来了全新的挑战：除非数据被标注，否则它对所有依赖监督学习方法的机器学习应用基本上是无用的。
- en: 'Data labeling: the new bottleneck in Machine Learning'
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据标注：机器学习中的新瓶颈
- en: Some of the most promising advances in AI over the last decade have come from
    the usage of deep learning models. While neural networks were discovered decades
    ago, their practical use was enabled only recently thanks to the increase in both
    the volume of data available and in compute power that older hardware simply could
    not provide. And because most deep learning algorithms use a supervised approach
    while being data-greedy, the new bottleneck in machine learning nowadays is not
    about the collection of data anymore, but about the speed and accuracy of the
    labeling process. After all, most of us are familiar with the axiom, “garbage
    in, garbage out.” The performance of a model will ultimately suffer unless you
    have high quality data to train that model on.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 过去十年中，人工智能最有前景的进展之一来自于深度学习模型的使用。虽然神经网络几十年前就被发现了，但它们的实际使用直到最近才得以实现，这要归功于数据量和计算能力的增加，而旧硬件根本无法提供这些。由于大多数深度学习算法使用监督学习方法且对数据需求量大，因此现在机器学习中的新瓶颈不再是数据的收集，而是标注过程的速度和准确性。毕竟，我们大多数人都熟悉“垃圾进，垃圾出”的格言。除非你拥有高质量的数据来训练模型，否则模型的性能最终会受到影响。
- en: Since obtaining an unlabeled instance is essentially free nowadays, winning
    the AI race now comes down to having the fastest, most scalable, flexible and
    reliable way to obtain quality labels for your dataset. Labeling high volumes
    of data has become a critical issue ever since the volume of data to label has
    become so large that even all human labor on the planet would not be sufficient
    to meet the need. For example, even at a rate of a few seconds per bounding box,
    annotating pedestrians on a 10-second video could take several hours for a human
    alone, which is why more and more companies are now focusing on building machine
    learning algorithms to automate the annotation process. A human-only approach
    simply isn’t scalable.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由于获取未标记实例现在基本上是免费的，赢得人工智能竞赛现在取决于拥有最快、最具扩展性、灵活和可靠的方式来获取数据集的质量标签。标注大量数据已经成为一个关键问题，因为待标注的数据量变得如此庞大，甚至全球所有的人工劳动都无法满足需求。例如，即使每个边界框标注几秒钟，单靠一个人对一个10秒钟的视频进行行人标注也可能需要几个小时，这就是为什么越来越多的公司现在专注于构建机器学习算法以自动化标注过程。单纯的人力方法根本无法扩展。
- en: But the sheer *volume* of data is not the only problem. Getting labels is also
    often time-consuming (for example, when annotators are asked about the topic of
    a longer document), often error-prone (and sometimes subjective), expensive (for
    example, “annotating” a patient record as a positive case of cancer might require
    a MRI scan or some other costly medical tests, or validating the presence of oil
    might require drilling), or even plain dangerous (for example, in the case of
    landmine field detection).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但数据的*数量*并不是唯一的问题。获取标签也往往耗时（例如，当注释员被要求对较长文档的主题进行标注时），经常容易出错（有时也具有主观性），费用昂贵（例如，“标注”患者记录为癌症阳性可能需要MRI扫描或其他昂贵的医学检查，或验证石油存在可能需要钻探），甚至可能很危险（例如，在地雷探测的情况下）。
- en: Labeling faster vs. labeling smarter
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更快标注与更聪明标注
- en: To address the exploding need in quality annotations, a Human-in-the-Loop AI
    approach where a human annotator validates the output of a machine learning algorithm
    seems like a promising approach. Not only does it enable a faster process, it
    also helps with quality, since the human intervention helps make up for the inaccuracy
    of the algorithm.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为应对对高质量注释的急剧需求，一种人机协作的AI方法，其中人类注释员验证机器学习算法的输出，看起来是一种有前途的方法。它不仅可以加快处理速度，还能提高质量，因为人类干预有助于弥补算法的不准确性。
- en: 'But while this brute-force approach is sometimes the best way forward, another
    one is slowly gaining more traction among the ML community: the concept of labeling
    only the data instances that matter the most. As data scientists, we have been
    trained to think that more data equates a high model accuracy, but while this
    is absolutely true, it is also important to acknowledge that not all data is created
    equal because not all examples carry the same quantity of information.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种蛮力方法有时是最佳途径，但另一个方法在ML社区中正在慢慢获得更多关注：即仅标注最重要的数据实例。作为数据科学家，我们习惯于认为更多的数据等于更高的模型准确率，但虽然这绝对正确，但也要承认并非所有数据都同等重要，因为并非所有例子都包含相同数量的信息。
- en: '![Active Learning Figure 1](../Images/f91da62539b031f5c44429c602abb0fb.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![主动学习图 1](../Images/f91da62539b031f5c44429c602abb0fb.png)'
- en: '**Figure 1: The two ways to address the dawning Big Labeling crisis**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1：应对新兴的大标签危机的两种方法**'
- en: 'Identifying the best instances for a model to learn from can happen at two
    different times: either before the model is even being built, or while the model
    is trained. The former is often referred to as prioritization, while the latter
    is called Active Learning.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 确定模型学习的最佳实例可以发生在两个不同的时间：要么在模型构建之前，要么在模型训练期间。前者通常称为优先级排序，而后者称为主动学习。
- en: What is Active Learning?
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是主动学习？
- en: In spite of its low coverage in popular ML blogs, Active Learning is actually
    surprisingly well-motivated in many modern ML problems, in particular when labels
    are difficult, time-consuming, or expensive to collect.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在流行的ML博客中覆盖率较低，但主动学习实际上在许多现代ML问题中得到了令人惊讶的充分激励，特别是当标签难以获得、耗时或昂贵时。
- en: In Active Learning, the learning algorithm is allowed to proactively select
    the subset of available examples to be labeled next from a pool of yet unlabeled
    instances. The fundamental belief behind the concept is that a Machine Learning
    algorithm could potentially achieve a better accuracy while using fewer training
    labels if it were allowed to choose the data it wants to learn from. Such an algorithm
    is referred to as an **active learner**. Active learners are allowed to dynamically
    pose queries during the training process, usually in the form of unlabeled data
    instances to be labeled by what is called an **oracle**, usually a human annotator.
    As such, active learning is one of the most powerful examples of the success of
    the Human-in-the-Loop paradigm.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在主动学习中，学习算法被允许从尚未标注的实例池中主动选择下一步要标注的样本。该概念背后的基本信念是，如果机器学习算法能够选择它想学习的数据，它可能在使用较少的训练标签的情况下实现更好的准确性。这种算法被称为**主动学习者**。主动学习者可以在训练过程中动态提出查询，通常以未标注的数据实例的形式，由被称为**oracle**的人类注释员进行标注。因此，主动学习是人机协作范式成功的最有力示例之一。
- en: '![Active Learning Figure 2](../Images/24677b9367112d57bd66fda411cfdeb2.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![主动学习图 2](../Images/24677b9367112d57bd66fda411cfdeb2.png)'
- en: '**Figure 2: As expected, the more data available to train a model, the higher
    the accuracy regardless of whether or not an Active Learning approach is used.
    However, with Active Learning, a given accuracy can be reached with significantly
    less data; in this example, a 80% accuracy can be achieved with only 45% of the
    total volume of available data as opposed to 70% in the case of regular supervised
    learning.**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2：正如预期的那样，训练模型所需的数据越多，准确度越高，无论是否使用主动学习方法。然而，使用主动学习时，可以用显著更少的数据达到特定的准确度；在这个例子中，仅使用45%的可用数据总量就可以实现80%的准确度，而在常规监督学习的情况下则需要70%。**'
- en: How does Active Learning work?
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主动学习是如何工作的？
- en: 'Active Learning can be implemented through different **scenarios**. In essence,
    deciding whether or not to query a specific label comes down to deciding whether
    the gain from obtaining the label offsets the cost of collecting that information.
    In practice, making that decision can take several forms depending on whether
    the scientist has a limited budget or simply tries to minimize his/her labeling
    bill. Overall, three different categories of active learning can be listed:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习可以通过不同的**场景**来实现。本质上，决定是否查询特定标签归结为判断获取标签的收益是否抵消了收集该信息的成本。实际上，这个决策可以根据科学家是否有有限的预算或只是试图最小化标记费用的不同而采取几种形式。总体而言，主动学习可以分为三种不同的类别：
- en: -           The **stream-based selective sampling scenario**, which consists
    in determining if it would be sufficiently beneficial to enquire for the label
    of a specific unlabeled entry in the dataset. As the model is being trained and
    is presented with a data instance, it immediately decides if it wants to see the
    label. The disadvantage of that approach naturally comes from the absence of guarantee
    that the data scientist will stay within his/her budget.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于流的选择性采样场景**涉及确定询问数据集中某个特定未标记条目的标签是否足够有益。随着模型的训练和数据实例的呈现，它会立即决定是否需要查看标签。这种方法的缺点自然在于没有保证数据科学家会保持在其预算范围内。'
- en: -           The **pool-based sampling scenario** is also the best-known one.
    It attempts to evaluate the entire dataset before selecting the best query, or
    a set of best queries. The active learner is usually initially trained on a fully
    labeled fraction of the data which generates a first version of the model which
    is subsequently used to identify which instances would be the most beneficial
    to inject in the training set for the next iteration (or active learning loop). 
    One of its biggest downsides comes from its memory-greediness.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于池的采样场景**也是最为人知的一个。它尝试在选择最佳查询或一组最佳查询之前评估整个数据集。主动学习者通常最初在一个完全标记的数据子集上进行训练，从而生成模型的第一个版本，该模型随后用于识别哪些实例对下一次迭代（或主动学习循环）注入训练集最有益。其最大的缺点之一在于其内存需求非常高。'
- en: -           The **membership query synthesis scenario** might not be applicable
    to all cases as it implies the generation of synthetic data. In this scenario,
    the learner is allowed to construct its own examples for labeling. This approach
    is promising to solve cold-start problems (like in search) when generating a data
    instance is easy to do.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**成员查询合成场景**可能并不适用于所有情况，因为它涉及生成合成数据。在这种场景下，学习者可以构造自己的例子进行标记。这种方法在生成数据实例较为容易时，对于解决冷启动问题（如搜索引擎中的问题）是非常有前途的。'
- en: How is active learning different than reinforcement learning?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习与强化学习有何不同？
- en: Although both reinforcement and active learning can reduce the number of required
    labels for your models, they are very different concepts.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管强化学习和主动学习都可以减少模型所需的标签数量，但它们是非常不同的概念。
- en: 'On the one hand, we have reinforcement learning. Reinforcement learning is
    a goal-oriented learning approach inspired by behavioral psychology that allows
    you to take inputs from the environment. As such, reinforcement learning implies
    that the agent will get better as it is in use: it learns while in usage. When
    we humans learn from our mistakes, we are actually functioning through a reinforcement
    learning approach. There is no actual training phase; instead the agent learns
    through trial-and-error using a predetermined reward function that sends back
    the input about how optimal a specific action it took turned out to be. Technically,
    reinforcement learning does not need to be fed with data, but instead generates
    its own as it goes.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，我们有强化学习。强化学习是一种目标导向的学习方法，受到行为心理学的启发，它允许你从环境中获得输入。因此，强化学习意味着代理在使用过程中会变得更好：它在使用中学习。当我们人类从错误中学习时，我们实际上是在进行一种强化学习方法。没有实际的训练阶段；而是代理通过试错过程使用预定的奖励函数来学习，该函数反馈关于特定动作的效果如何。技术上讲，强化学习不需要提供数据，而是随着进程生成自己的数据。
- en: Active learning, on the other end, is closer to traditional supervised learning.
    It is, in fact, a type of semi-supervised learning (where both labeled and unlabeled
    data is used). The idea behind the concept is that not all data is created equal,
    and that labeling just a small sample of data might get to the same accuracy (if
    not hire), the only challenge being to identify what that sample is. Active learning
    is about incrementally and dynamically labeling data during the training phase
    in order to allow the algorithm to identify what label would be the most informational
    for it to learn faster.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习，另一方面，更接近于传统的监督学习。实际上，它是一种半监督学习（同时使用标注数据和未标注数据）。这一概念的核心思想是，并非所有数据的价值相同，标注一个小样本的数据可能会达到相同的准确性（如果不是更高），唯一的挑战是确定这个样本是什么。主动学习是指在训练阶段逐步动态地标注数据，以便让算法识别哪个标签对其学习更有信息量，从而更快地学习。
- en: How to decide which row to label?
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何决定标注哪一行？
- en: 'The approach used to determine which data instance to label next is referred
    to as a **querying strategy**. Below, we are listing those that are the most commonly
    used and studied ones:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 用于确定下一个需要标注的数据实例的方法称为**查询策略**。下面列出的是那些最常用和研究的策略：
- en: Uncertainty sampling
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不确定性采样
- en: For those querying strategies, a model is first trained on a fairly small sample
    of labeled data; this model is then applied on the (unlabeled) remainder of the
    dataset. The algorithm chooses which instances to label over the next active learning
    loop based on the information it gained through this inference step.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些查询策略，首先在一个相对较小的标注数据样本上训练一个模型；然后将这个模型应用于数据集的（未标注的）其余部分。该算法基于通过推理步骤获得的信息选择下一个主动学习循环中需要标注的实例。
- en: 'The most popular among those strategies is the **least confidence** strategy,
    where the model selects those instances with the lowest confidence level to be
    labeled next. Another common strategy is called **margin sampling**: in this case,
    the algorithm selects the instances where the margin between the two most likely
    labels is narrow, meaning that the classifier is struggling to differentiate between
    those two most likely classes. The two strategies aim at helping the model discriminate
    among specific classes and overall do a great job reducing specific classification
    error. However, if the objective function consists of reducing log-loss, an **entropy-based**
    where the learner simply queries the unlabeled instance for which the model has
    the highest output variance in its prediction, is usually more appropriate.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些策略中，最受欢迎的是**最小置信度**策略，其中模型选择那些置信度最低的实例进行标注。另一种常见策略叫做**边际采样**：在这种情况下，算法选择那些两个最可能的标签之间的边际较小的实例，这意味着分类器在区分这两个最可能的类别时遇到了困难。这两种策略旨在帮助模型在特定类别之间进行区分，并且在减少特定分类错误方面表现出色。然而，如果目标函数是减少对数损失，则通常**基于熵**的方法更为合适，在这种方法中，学习者简单地查询模型在预测中具有最高输出方差的未标注实例。
- en: Query-by-Committee (aka, QBC)
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 查询委员会（即，QBC）
- en: With those strategies, a committee of different models is trained on the same
    labeled training set and used for inference on the rest of the (yet unlabeled)
    data. The instances for which the largest disagreement is observed are deemed
    to be the most informative ones. The core idea behind this framework is minimizing
    the version space, which is the set of hypotheses that are consistent with the
    current labeled training data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些策略，对相同标记训练集的不同模型进行训练，并用于推断其余（尚未标记）数据。对于观察到最大分歧的实例，被认为是最具信息性的。这一框架的核心思想是最小化版本空间，即与当前标记训练数据一致的假设集合。
- en: Expected Impact
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 预期影响
- en: Those querying strategies examine the impact of the addition of new labels on
    the overall performance of the model. The **expected model change** strategy aims
    at identifying the instances that would lead to the biggest change to the current
    model should the labels for those instances be known. The **expected error reduction**
    strategy measures not how much the model is likely to change, but how much its
    generalization error is likely to be reduced. One challenge with that strategy
    is that it is also the most computationally expensive query framework. And because
    minimizing an expected loss function usually doesn’t have a close form solution,
    a **variance reduction** approach is used as a proxy.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这些查询策略考察新标签的增加对模型整体性能的影响。**期望模型变化**策略旨在识别那些如果已知标签将对当前模型产生最大变化的实例。**期望误差减少**策略衡量的不是模型可能发生的变化量，而是模型的泛化误差可能减少的量。这种策略面临的挑战之一是，它也是最计算上昂贵的查询框架。由于最小化期望损失函数通常没有封闭形式的解，因此使用**方差减少**方法作为代理。
- en: Overall, these methodologies look at the input space as a whole rather than
    focusing on individual instances as uncertainty sampling or QBC would, which gives
    them the singular advantage to be less prone to selecting outliers.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这些方法学将输入空间整体考虑，而不是像不确定性采样或QBC那样关注个别实例，这使得它们在选择离群点时具有明显的优势。
- en: Density-Weighted Methods
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 密度加权方法
- en: 'The previous frameworks describe strategies that identify the most informative
    instances as being those with a highest level of uncertainty. However, another
    way to look at the problem is to consider that the most valuable instances in
    a dataset are not necessarily those for which the uncertainty is maximal, but
    rather those which are the most representative of the underlying distribution
    of the data: this is what the **information density** framework is all about.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的框架描述了那些识别最具信息性的实例的策略，这些实例被认为是不确定性最高的。然而，另一种看待问题的方法是，数据集中最有价值的实例不一定是那些不确定性最大的，而是那些最能代表数据底层分布的实例：这就是**信息密度**框架的核心思想。
- en: Performance & Limitations
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能与限制
- en: More than 9 researchers out of 10 who have attempted some work involving Active
    Learning claim that their expectations were met either fully or partially. That’s
    very encouraging, but one can’t help but wonder what happened in the other cases.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 超过90%的进行过主动学习工作的研究者声称他们的期望得到或完全或部分满足。这非常令人鼓舞，但人们不禁想知道其他情况下发生了什么。
- en: The truth here is that Active Learning is still not well understood. There is
    some [promising work on deep active learning for NER](https://arxiv.org/pdf/1707.05928.pdf)
    for example, but [many of the large questions remain](http://burrsettles.com/pub/settles.activelearning.pdf).
    There is very little work, for example, on predicting ahead of time if a specific
    task or dataset is particularly prone to benefit from an active learning approach.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现实情况是，主动学习仍未被很好地理解。例如，有一些关于NER的[深度主动学习的有前景的工作](https://arxiv.org/pdf/1707.05928.pdf)，但[许多重大问题仍然存在](http://burrsettles.com/pub/settles.activelearning.pdf)。例如，几乎没有研究预测特定任务或数据集是否特别容易从主动学习方法中受益。
- en: Ultimately, Active Learning is a particular case of semi-supervised learning,
    a category of algorithms which has been shown to be highly vulnerable to biases,
    in particular because they are at risk of self-indulging in initial beliefs derived
    from the patterns identified by a model trained on a fairly small dataset. With
    the efficient labeling becoming an ever-more critical component of Machine Learning,
    it is to be expected that a lot more research on the topic will be published in
    the coming years.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，主动学习是一种特定的半监督学习情况，这类算法已被证明对偏见非常敏感，特别是因为它们容易自我满足于从基于相对较小数据集训练的模型中识别出的模式中得到的初始信念。随着高效标注成为机器学习中越来越关键的组成部分，可以预期未来几年将会有更多关于这个主题的研究成果发表。
- en: '**Bio**: [Jennifer Prendki](https://www.linkedin.com/in/jennifer-prendki) is
    a Data Science Leader and a Data Strategist, who takes great pleasure in envisioning
    the future of Data Science and evangelizing it. Jennifer is particularly skilled
    in growing and managing early-stage data teams, and is highly comfortable managing
    hybrid teams, including product managers, data scientists, data analysts and engineers
    across the board (Front-End, Backend, Systems, etc.)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介**: [詹妮弗·普伦基](https://www.linkedin.com/in/jennifer-prendki)是一位数据科学领袖和数据策略师，她非常享受设想数据科学的未来并推广它。詹妮弗特别擅长培养和管理早期阶段的数据团队，并且非常擅长管理混合团队，包括产品经理、数据科学家、数据分析师和各类工程师（前端、后端、系统等）。'
- en: '**Related:**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关:**'
- en: '[How to Put Active Learning to Work for Your Enterprise](https://www.kdnuggets.com/2018/09/figure-eight-active-learning-work-your-enterprise.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何将主动学习应用于您的企业](https://www.kdnuggets.com/2018/09/figure-eight-active-learning-work-your-enterprise.html)'
- en: '[The Essential Guide to Training Data for Machine Learning](https://www.kdnuggets.com/2018/08/figure-eight-essential-guide-training-data-machine-learning.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习训练数据的基本指南](https://www.kdnuggets.com/2018/08/figure-eight-essential-guide-training-data-machine-learning.html)'
- en: '[The 2018 Data Scientist Report is Here](https://www.kdnuggets.com/2018/08/figure-eight-2018-data-scientist-report.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2018年数据科学家报告已发布](https://www.kdnuggets.com/2018/08/figure-eight-2018-data-scientist-report.html)'
- en: '* * *'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三推荐课程
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析水平'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织在IT领域'
- en: '* * *'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题
- en: '[KDnuggets News, April 27: A Brief Introduction to Papers With Code;…](https://www.kdnuggets.com/2022/n17.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets新闻，4月27日：关于带代码的论文简要介绍；…](https://www.kdnuggets.com/2022/n17.html)'
- en: '[Introduction to Statistical Learning, Python Edition: Free Book](https://www.kdnuggets.com/2023/07/introduction-statistical-learning-python-edition-free-book.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[统计学习导论，Python版：免费书籍](https://www.kdnuggets.com/2023/07/introduction-statistical-learning-python-edition-free-book.html)'
- en: '[Introduction to Deep Learning Libraries: PyTorch and Lightning AI](https://www.kdnuggets.com/introduction-to-deep-learning-libraries-pytorch-and-lightning-ai)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习库介绍：PyTorch和Lightning AI](https://www.kdnuggets.com/introduction-to-deep-learning-libraries-pytorch-and-lightning-ai)'
- en: '[Back to Basics Week 3: Introduction to Machine Learning](https://www.kdnuggets.com/back-to-basics-week-3-introduction-to-machine-learning)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[回到基础第3周：机器学习导论](https://www.kdnuggets.com/back-to-basics-week-3-introduction-to-machine-learning)'
- en: '[Introduction to Binary Classification with PyCaret](https://www.kdnuggets.com/2021/12/introduction-binary-classification-pycaret.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用PyCaret进行二分类入门](https://www.kdnuggets.com/2021/12/introduction-binary-classification-pycaret.html)'
- en: '[Introduction to Clustering in Python with PyCaret](https://www.kdnuggets.com/2021/12/introduction-clustering-python-pycaret.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用PyCaret在Python中进行聚类入门](https://www.kdnuggets.com/2021/12/introduction-clustering-python-pycaret.html)'
