- en: How to Create a Custom Tokenizer for Non-English Languages with Hugging Face
    Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用 Hugging Face Transformers 为非英文语言创建自定义分词器
- en: 原文：[https://www.kdnuggets.com/how-to-create-a-custom-tokenizer-for-non-english-languages-with-hugging-face-transformers](https://www.kdnuggets.com/how-to-create-a-custom-tokenizer-for-non-english-languages-with-hugging-face-transformers)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/how-to-create-a-custom-tokenizer-for-non-english-languages-with-hugging-face-transformers](https://www.kdnuggets.com/how-to-create-a-custom-tokenizer-for-non-english-languages-with-hugging-face-transformers)
- en: '![Custom Tokenizer for Non-English Languages](../Images/cc11ae130d36388a8ade84799bcca8fe.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![非英文语言的自定义分词器](../Images/cc11ae130d36388a8ade84799bcca8fe.png)'
- en: Image by Author | Canva
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作者插图 | Canva
- en: Whenever you would think of doing some Natural Language task, the first step
    you would most likely need to take is the tokenization of the text you are using.
    This is very important and the performance of downstream tasks greatly depends
    on it. We can easily find tokenizers online that can tokenize English language
    datasets. But what if you want to tokenize a non-English language data. What if
    you do not find any tokenizer for the specific type of data you are using? How
    would you tokenize it? Well, this article explains exactly that.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 每当你考虑进行自然语言处理任务时，你最可能需要采取的第一步是对你使用的文本进行分词。这非常重要，因为下游任务的性能在很大程度上依赖于此。我们可以很容易地在网上找到可以分词英文数据集的分词器。但是，如果你想对非英文语言数据进行分词呢？如果你找不到适合你所使用的特定类型数据的分词器，你将如何进行分词？好吧，本文正是解释了这些问题。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: We will tell you everything you need to know to train your own custom tokenizer.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将告诉你所有你需要知道的关于训练自定义分词器的知识。
- en: What is Tokenization and Why is it Important?
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是分词，为什么它很重要？
- en: The tokenization process takes a body of text and breaks it into pieces, or
    tokens, which can then be transformed into a format the model can work with. In
    this case, the model can only process numbers, so the tokens are converted into
    numbers. This process is critical because it changes raw content into a structured
    format that models can understand and analyze. Without tokenization, we would
    have no way to pass text to machine learning models as these models can only understand
    numbers and not strings of texts directly.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 分词过程将一段文本拆分成片段或词汇，这些片段随后可以转换成模型可以处理的格式。在这种情况下，模型只能处理数字，因此这些词汇被转换成数字。这个过程至关重要，因为它将原始内容转换为模型可以理解和分析的结构化格式。如果没有分词，我们将无法将文本传递给机器学习模型，因为这些模型只能理解数字，而不能直接理解文本字符串。
- en: Why Need a Custom Tokenizer?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么需要自定义分词器？
- en: Training tokenizers from scratch is particularly important when you are working
    with non-English languages or specific domains. Standard pretrained tokenizers
    may not effectively handle the unique characteristics, vocabulary, and syntax
    of different languages or specialized characters. A new tokenizer can be trained
    to manage specific language features and terminologies. This leads to better tokenization
    accuracy and improved model performance. Not only that, custom tokenizers allows
    us to include domain-specific tokens and the handling of rare or out-of-vocabulary
    words. This improves the effectiveness of NLP models in applications involving
    many languages.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始训练分词器在处理非英文语言或特定领域时尤其重要。标准的预训练分词器可能无法有效处理不同语言或专业字符的独特特征、词汇和语法。可以训练新的分词器以管理特定语言的特征和术语。这可以提高分词的准确性和模型的性能。不仅如此，自定义分词器还允许我们包含特定领域的词汇，并处理稀有或词汇表外的词汇。这提高了
    NLP 模型在涉及多语言的应用中的效果。
- en: Now you must be thinking that you know tokenization is important and that you
    may need to train your own tokenizer. But how would you do that? Now that you
    know the need and importance of training your own tokenizer, we will explain step
    by step how to do this.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你一定在想，你知道分词化很重要，并且可能需要训练你自己的分词器。那么你会怎么做呢？既然你已经了解了训练自己分词器的需求和重要性，我们将一步一步地解释如何做到这一点。
- en: Step-by-Step Process to Train a Custom Tokenizer
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义分词器的逐步训练过程
- en: 'Step 1: Install the Required Libraries'
  id: totrans-18
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第一步：安装所需的库
- en: 'First, ensure you have the necessary libraries installed. You can install the
    Hugging Face transformers and datasets libraries using pip:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，确保你已安装所需的库。你可以使用 pip 安装 Hugging Face 的 transformers 和 datasets 库：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Step 2: Prepare Your Dataset'
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第二步：准备你的数据集
- en: To train a tokenizer, you'll need a text dataset in the target language. Hugging
    Face's datasets library offers a convenient way to load and process datasets.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个分词器，你需要一个目标语言的文本数据集。Hugging Face 的 datasets 库提供了一种方便的方式来加载和处理数据集。
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Output:**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '![output](../Images/e7cc24805f76c9489210cc7c3484ba78.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![output](../Images/e7cc24805f76c9489210cc7c3484ba78.png)'
- en: 'Note that you can load dataset of any language in this step. One example of
    our Urdu dataset looks like this:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你可以在此步骤中加载任何语言的数据集。我们的乌尔都语数据集的一个示例如下所示：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Step 3: Initialize a Tokenizer'
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第三步：初始化分词器
- en: Hugging Face provides several tokenizer models. I will use the BPE tokenizer
    from the hugging face library. This is the exact same algorithm which GPT-3 and
    GPT-4 tokenizers use. But you can still choose a different tokenizer model based
    on your needs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 提供了几种分词器模型。我将使用来自 Hugging Face 库的 BPE 分词器。这是 GPT-3 和 GPT-4 分词器使用的完全相同的算法。但你仍然可以根据需要选择不同的分词器模型。
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Step 4: Train the Tokenizer'
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第四步：训练分词器
- en: Train the tokenizer on your dataset. Ensure the dataset is preprocessed and
    formatted correctly. You can specify various parameters such as vocab_size, min_frequency,
    and special_tokens.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的数据集上训练分词器。确保数据集经过预处理并正确格式化。你可以指定各种参数，如 vocab_size、min_frequency 和 special_tokens。
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Step 5: Save the Tokenizer'
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第五步：保存分词器
- en: After training, save the tokenizer to disk. This allows you to load and use
    it later.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，将分词器保存到磁盘。这允许你以后加载并使用它。
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Replace `path_to_save_tokenizer` with the desired path to save the tokenizer
    files. Running this code would save two files, namely:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `path_to_save_tokenizer` 替换为保存分词器文件的所需路径。运行此代码将保存两个文件，即：
- en: '![](../Images/308b6424472f4314a60e0cf5e71da9b2.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/308b6424472f4314a60e0cf5e71da9b2.png)'
- en: 'Step 6: Load and Use the Tokenizer'
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第六步：加载和使用分词器
- en: You can load the tokenizer for future use and tokenize texts in your target
    language.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以加载分词器以备将来使用，并在目标语言中对文本进行分词。
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output of this cell would be:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此单元格的输出将是：
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Wrapping Up
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: To train your own tokenizer for non-English languages with Hugging Face Transformers,
    you would have to prepare a dataset, initialize a tokenizer, train it and finally
    save it for future use. You can create tokenizers tailored to specific languages
    or domains by following these steps. This will improve the performance of any
    of the specific NLP task that you might be working on. You are now able to easily
    train tokenizer on any language. Let us know what you created.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Hugging Face Transformers 为非英语语言训练你自己的分词器，你需要准备一个数据集，初始化一个分词器，训练它，并最终保存以备将来使用。通过以下步骤，你可以创建针对特定语言或领域的分词器。这将提高你所处理的任何特定
    NLP 任务的性能。你现在可以轻松地在任何语言上训练分词器。告诉我们你创建了什么。
- en: '**[](https://www.linkedin.com/in/kanwal-mehreen1/)**[Kanwal Mehreen](https://www.linkedin.com/in/kanwal-mehreen1/)****
    Kanwal is a machine learning engineer and a technical writer with a profound passion
    for data science and the intersection of AI with medicine. She co-authored the
    ebook "Maximizing Productivity with ChatGPT". As a Google Generation Scholar 2022
    for APAC, she champions diversity and academic excellence. She''s also recognized
    as a Teradata Diversity in Tech Scholar, Mitacs Globalink Research Scholar, and
    Harvard WeCode Scholar. Kanwal is an ardent advocate for change, having founded
    FEMCodes to empower women in STEM fields.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**[](https://www.linkedin.com/in/kanwal-mehreen1/)**[Kanwal Mehreen](https://www.linkedin.com/in/kanwal-mehreen1/)****
    Kanwal 是一名机器学习工程师和技术作家，对数据科学及 AI 与医学的交集充满了深厚的热情。她共同撰写了电子书《利用 ChatGPT 最大化生产力》。作为
    2022 年 APAC 的 Google Generation Scholar，她倡导多样性和学术卓越。她还被认可为 Teradata 多样性技术学者、Mitacs
    Globalink 研究学者和哈佛 WeCode 学者。Kanwal 是变革的热情倡导者，她创立了 FEMCodes 以赋能 STEM 领域的女性。'
- en: More On This Topic
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关话题
- en: '[How to Translate Languages with MarianMT and Hugging Face Transformers](https://www.kdnuggets.com/how-to-translate-languages-with-marianmt-and-hugging-face-transformers)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用MarianMT和Hugging Face Transformers进行语言翻译](https://www.kdnuggets.com/how-to-translate-languages-with-marianmt-and-hugging-face-transformers)'
- en: '[How to Fine-Tune BERT for Sentiment Analysis with Hugging Face Transformers](https://www.kdnuggets.com/how-to-fine-tune-bert-sentiment-analysis-hugging-face-transformers)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用Hugging Face Transformers微调BERT进行情感分析](https://www.kdnuggets.com/how-to-fine-tune-bert-sentiment-analysis-hugging-face-transformers)'
- en: '[How to Use GPT for Generating Creative Content with Hugging Face…](https://www.kdnuggets.com/how-to-use-gpt-for-generating-creative-content-with-hugging-face-transformers)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何利用GPT生成创意内容，使用Hugging Face…](https://www.kdnuggets.com/how-to-use-gpt-for-generating-creative-content-with-hugging-face-transformers)'
- en: '[Building a Recommendation System with Hugging Face Transformers](https://www.kdnuggets.com/building-a-recommendation-system-with-hugging-face-transformers)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Hugging Face Transformers构建推荐系统](https://www.kdnuggets.com/building-a-recommendation-system-with-hugging-face-transformers)'
- en: '[Using Hugging Face Transformers for Emotion Detection in Text](https://www.kdnuggets.com/using-hugging-face-transformers-for-emotion-detection-in-text)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Hugging Face Transformers进行文本情感检测](https://www.kdnuggets.com/using-hugging-face-transformers-for-emotion-detection-in-text)'
- en: '[How to Build and Train a Transformer Model from Scratch with…](https://www.kdnuggets.com/how-to-build-and-train-a-transformer-model-from-scratch-with-hugging-face-transformers)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何从零开始构建和训练Transformer模型…](https://www.kdnuggets.com/how-to-build-and-train-a-transformer-model-from-scratch-with-hugging-face-transformers)'
