- en: An Excellent Resource To Learn The Foundations Of Everything Underneath ChatGPT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个学习 ChatGPT 背后所有基础的优秀资源
- en: 原文：[https://www.kdnuggets.com/023/08/excellent-resource-learn-foundations-everything-underneath-chatgpt.html](https://www.kdnuggets.com/023/08/excellent-resource-learn-foundations-everything-underneath-chatgpt.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/023/08/excellent-resource-learn-foundations-everything-underneath-chatgpt.html](https://www.kdnuggets.com/023/08/excellent-resource-learn-foundations-everything-underneath-chatgpt.html)
- en: '![An Excellent Resource To Learn The Foundations Of Everything Underneath ChatGPT](../Images/2e8906182a21478030c961f264a7ac68.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![一个学习 ChatGPT 背后所有基础的优秀资源](../Images/2e8906182a21478030c961f264a7ac68.png)'
- en: Image by [Freepik](https://www.freepik.com/free-photo/opened-ai-chat-laptop_38259334.htm#query=chatgpt&position=0&from_view=search&track=sph)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Freepik](https://www.freepik.com/free-photo/opened-ai-chat-laptop_38259334.htm#query=chatgpt&position=0&from_view=search&track=sph)
- en: OpenAI, ChatGPT, the GPT-series, and Large Language Models (LLMs) in general
    – if you are remotely associated with the AI profession or a technologist, chances
    are high that you’d hear these words in almost all your business conversations
    these days.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI、ChatGPT、GPT 系列以及大型语言模型（LLMs） – 如果你与 AI 行业或技术行业有任何联系，那么你很可能会在几乎所有的商务对话中听到这些词汇。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速通道进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT 需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: And the hype is real. We can not call it a bubble anymore. After all, this time,
    the hype is living up to its promises.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 而且这种炒作是真的。我们不能再称其为泡沫。毕竟，这次，炒作确实兑现了它的承诺。
- en: Who would have thought that machines could understand and revert in human-like
    intelligence and do almost all those tasks previously considered human forte,
    including creative applications of music, writing poetry, and even programming
    applications?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 谁会想到机器能够理解并以类人智能作出回应，并执行几乎所有以前被认为是人类特长的任务，包括音乐创作、诗歌写作，甚至编程应用？
- en: The ubiquitous proliferation of LLMs in our lives has made us all curious about
    what lies underneath this powerful technology.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型在我们生活中的普及让我们都对这一强大技术背后的内容充满好奇。
- en: So, if you are holding yourself back because of the gory-looking details of
    algorithms and the complexities of the AI domain, I highly recommend this resource
    to learn all about “[What Is ChatGPT Doing … and Why Does It Work?](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)”
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你因为算法的复杂细节和 AI 领域的复杂性而感到退缩，我强烈推荐这个资源来了解“[ChatGPT 在做什么……以及它为何有效？](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)”
- en: '![An Excellent Resource To Learn The Foundations Of Everything Underneath ChatGPT](../Images/5fbf35ca39bc7777659de8d7c2fcd566.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![一个学习 ChatGPT 背后所有基础的优秀资源](../Images/5fbf35ca39bc7777659de8d7c2fcd566.png)'
- en: Image from [Stephen Wolfram Writings](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Stephen Wolfram Writings](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)
- en: Yes, that's the title of the article by Wolfram.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这是 Wolfram 文章的标题。
- en: Why am I recommending this? Because it is crucial to understand the absolute
    essentials of machine learning and how deep neural networks are related to human
    brains before learning about Transformers, LLMs, or even Generative AI.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么推荐这个？因为在学习 Transformers、LLMs 或者生成型 AI 之前，理解机器学习的基本要素以及深度神经网络如何与人脑相关是至关重要的。
- en: It looks like a mini-book which is literature on its own, but take your time
    with the length of this resource.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来像一本迷你书，独立成章，但请花时间阅读这篇资源。
- en: In this article, I will share how to start reading it to make the concepts easier
    to grasp.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将分享如何开始阅读这些内容，以便更容易掌握概念。
- en: Understanding the ‘Model’ Is Crucial
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解‘模型’至关重要
- en: Its key highlight is the focus on the ‘model’ part of “Large Language Models”,
    illustrated by an example of the time it takes the ball to reach the ground from
    each floor.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其主要亮点是关注“大型语言模型”中的‘模型’部分，通过一个例子说明了球从每一层到达地面的时间。
- en: '![An Excellent Resource To Learn The Foundations Of Everything Underneath ChatGPT](../Images/6ead3cc06ef1bdae2f274d2aefdfba10.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![一个很好的资源，帮助你了解 ChatGPT 背后的基础知识](../Images/6ead3cc06ef1bdae2f274d2aefdfba10.png)'
- en: Image from [Stephen Wolfram Writings](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[斯蒂芬·沃尔弗拉姆的写作](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)
- en: There are two ways to achieve this – repeating this exercise from each floor
    or building a model that could compute it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以实现这一点 – 从每一层重复这个过程，或建立一个可以计算它的模型。
- en: In this example, there exists an underlying mathematical formulation that makes
    it easier to calculate, but how would one estimate such a phenomenon using a ‘model’?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，存在一个基本的数学公式，使得计算变得更容易，但如何使用“模型”来估计这种现象呢？
- en: The best bet would be to fit a straight line for estimating the variable of
    interest, in this case, time.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳做法是拟合一条直线来估计感兴趣的变量，在这个例子中就是时间。
- en: A more profound read into this section would explain that there is never a “model-less
    model”, which seamlessly takes you to the varied deep learning concepts.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对这一部分更深入的阅读会解释没有“无模型的模型”，这将自然引导你了解各种深度学习概念。
- en: The Core Of Deep Learning
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的核心
- en: You will learn that a model is a complex function that takes in certain variables
    as input and results in an output, say a number in digit recognition tasks.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你将了解到，模型是一个复杂的函数，它接受某些变量作为输入，并生成输出，比如在数字识别任务中的一个数字。
- en: The article goes from digit recognition to a typical cat vs. dog classifier
    to lucidly explain what features are picked by each layer, starting with the outline
    of the cat. Notably, the first few layers of a neural network pick out certain
    aspects of images, like the edges of objects.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 文章从数字识别到典型的猫与狗分类器，清楚地解释了每一层所提取的特征，从猫的轮廓开始。值得注意的是，神经网络的前几层会提取图像的某些方面，如物体的边缘。
- en: '![An Excellent Resource To Learn The Foundations Of Everything Underneath ChatGPT](../Images/44a51cb82820df7bde09c52a76f64876.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![一个很好的资源，帮助你了解 ChatGPT 背后的基础知识](../Images/44a51cb82820df7bde09c52a76f64876.png)'
- en: Image by [Freepik](https://www.freepik.com/free-vector/gradient-brain-background_44416640.htm#query=deep%20learning&position=6&from_view=search&track=ais)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Freepik](https://www.freepik.com/free-vector/gradient-brain-background_44416640.htm#query=deep%20learning&position=6&from_view=search&track=ais)
- en: Key Terminologies
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键术语
- en: 'In addition to explaining the role of multiple layers, multiple facets of deep
    learning algorithms are also explained, such as:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了说明多个层的作用外，还解释了深度学习算法的多个方面，例如：
- en: Architecture Of Neural Networks
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络的架构
- en: It is a mix of art and science, says the post – “But mostly things have been
    discovered by trial and error, adding ideas and tricks that have progressively
    built significant lore about how to work with neural nets”.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 文章称这是一种艺术与科学的结合 – “但大多数情况是通过反复试验发现的，添加了想法和技巧，逐步积累了关于如何处理神经网络的丰富经验”。
- en: Epochs
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 轮次
- en: Epochs are an effective way to remind the model of a particular example to get
    it to “remember that example”
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 轮次是一种有效的方法，可以提醒模型特定的示例，让它“记住这个示例”
- en: Since repeating the same example multiple times isn’t enough, it is important
    to show different variations of the examples to the neural net.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于重复相同的例子多次并不足够，因此向神经网络展示不同的示例变体非常重要。
- en: Weights (Parameters)
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权重（参数）
- en: You must have heard that one of the LLMs has whopping 175B parameters. Well,
    that shows how the structure of the model varies based on how the knobs are adjusted.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你一定听说过某个大型语言模型拥有惊人的1750亿个参数。嗯，这说明了模型的结构如何根据旋钮的调整而变化。
- en: Essentially, parameters are the “knobs you can turn” to fit the data. The post
    highlights that the actual learning process of neural networks is all about finding
    the right weights – *“In the end, it’s all about determining what weights will
    best capture the training examples that have been given”*
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，参数是“你可以调节的旋钮”，用于拟合数据。文章强调，神经网络的实际学习过程就是寻找正确的权重 – *“最终，这一切都是关于确定哪些权重能够最好地捕捉给定的训练示例”*
- en: Generalization
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 泛化
- en: The neural networks learn to “interpolate between the shown examples in a reasonable
    way”.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络学会了以“合理的方式在展示的示例之间进行插值”。
- en: This generalization helps to predict unseen records by learning from multiple
    input-output examples.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这种泛化有助于通过学习多个输入-输出示例来预测未见过的记录。
- en: Loss Function
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 损失函数
- en: But how do we know what is reasonable? It is defined by how far the output values
    are from the expected values, which are encapsulated in the loss function.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何知道什么是合理的呢？它由输出值与期望值之间的距离定义，这些期望值被封装在损失函数中。
- en: It gives us a “distance between the values we’ve got and the true values”. To
    reduce this distance, the weights are iteratively adjusted, but there must be
    a way to systemically reduce the weights in a direction that takes the shortest
    path.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 它为我们提供了“我们得到的值与真实值之间的距离”。为了减少这种距离，权重会被反复调整，但必须有一种系统化的方法来将权重调整到最短路径的方向。
- en: Gradient Descent
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度下降
- en: Finding the steepest path to descent on a weight landscape is called gradient
    descent.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在权重景观中找到最陡下降路径称为梯度下降。
- en: It is all about finding the correct weights that best represent the ground truth
    by navigating the weight landscape.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都在于通过在权重景观中导航来找到最佳的权重，以最准确地表示真实情况。
- en: Backpropagation
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播
- en: Continue reading through the concept of backpropagation, which takes the loss
    function and works backward to progressively find weights to minimize the associated
    loss.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 继续阅读关于反向传播的概念，它利用损失函数向后工作，逐步找到最小化相关损失的权重。
- en: Hyperparameters
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超参数
- en: In addition to weights (aka the parameters), there are hyperparameters that
    include different choices of the loss function, loss minimization, or even choosing
    how big a “batch” of examples should be.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 除了权重（即参数），还有超参数，包括不同的损失函数选择、损失最小化，甚至选择“批次”大小。
- en: Neural Networks For Complex Problems
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 复杂问题中的神经网络
- en: The use of neural networks for complex problems is widely discussed. Still,
    the logic underneath such an assumption was unclear until this post which explains
    how multiple weight variables in a high-dimensional space enable various directions
    that can lead to the minimum.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在复杂问题中的使用被广泛讨论。然而，直到这篇文章解释了高维空间中的多个权重变量如何提供通向最小值的各种方向，这种假设的逻辑才变得清晰。
- en: Now, compare this with fewer variables, which implies the possibility of getting
    stuck in a local minimum with no direction to get out.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将此与较少的变量进行比较，这意味着可能会陷入局部最小值而没有方向离开。
- en: Conclusion
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: ''
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With this read, we have covered a lot of ground, from understanding the model
    and how human brains work to taking it to neural nets, their design, and associated
    terminologies.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这次阅读，我们涵盖了许多内容，从理解模型及人脑如何工作到将其应用于神经网络、它们的设计及相关术语。
- en: Stay tuned for a follow-up post on how to build upon this knowledge to understand
    how chatgpt works.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 请关注后续文章，了解如何在此基础上深入理解chatgpt的工作原理。
- en: '**[Vidhi Chugh](https://vidhi-chugh.medium.com/)** is an AI strategist and
    a digital transformation leader working at the intersection of product, sciences,
    and engineering to build scalable machine learning systems. She is an award-winning
    innovation leader, an author, and an international speaker. She is on a mission
    to democratize machine learning and break the jargon for everyone to be a part
    of this transformation.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Vidhi Chugh](https://vidhi-chugh.medium.com/)** 是一位AI策略专家和数字化转型领导者，专注于产品、科学和工程的交汇点，以构建可扩展的机器学习系统。她是一位获奖的创新领导者、作者和国际演讲者。她的使命是使机器学习民主化，并打破术语，使每个人都能参与这场转型。'
- en: More On This Topic
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Can ChatGPT Be Trusted as an Educational Resource?](https://www.kdnuggets.com/2023/05/chatgpt-trusted-educational-resource.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatGPT能否作为教育资源被信任？](https://www.kdnuggets.com/2023/05/chatgpt-trusted-educational-resource.html)'
- en: '[Back to Basics Week 1: Python Programming & Data Science Foundations](https://www.kdnuggets.com/back-to-basics-week-1-python-programming-data-science-foundations)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基础回顾第1周：Python编程与数据科学基础](https://www.kdnuggets.com/back-to-basics-week-1-python-programming-data-science-foundations)'
- en: '[ChatGPT Plugins: Everything You Need To Know](https://www.kdnuggets.com/2023/06/chatgpt-plugins-everything-need-know.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatGPT插件：你需要了解的一切](https://www.kdnuggets.com/2023/06/chatgpt-plugins-everything-need-know.html)'
- en: '[ChatGPT: Everything You Need to Know](https://www.kdnuggets.com/2023/01/chatgpt-everything-need-know.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatGPT：你需要了解的一切](https://www.kdnuggets.com/2023/01/chatgpt-everything-need-know.html)'
- en: '[Visual ChatGPT: Microsoft Combine ChatGPT and VFMs](https://www.kdnuggets.com/2023/03/visual-chatgpt-microsoft-combine-chatgpt-vfms.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Visual ChatGPT: 微软将 ChatGPT 和 VFM 结合](https://www.kdnuggets.com/2023/03/visual-chatgpt-microsoft-combine-chatgpt-vfms.html)'
- en: '[ChatGPT CLI: Transform Your Command-Line Interface Into ChatGPT](https://www.kdnuggets.com/2023/07/chatgpt-cli-transform-commandline-interface-chatgpt.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatGPT CLI: 将你的命令行界面转变为 ChatGPT](https://www.kdnuggets.com/2023/07/chatgpt-cli-transform-commandline-interface-chatgpt.html)'
