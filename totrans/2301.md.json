["```py\nimport numpy as np\nnp.random.seed(0)\n\nX = np.random.randn(100, 2) # features\ny = ((X[:, 0] > 0) * (X[:, 1] < 0)) # labels (0 and 1)\n```", "```py\ny_1 = [0, 0, 0, 0, 0, 0, 0, 0]\n\ny_2 = [1, 0, 0, 0, 0, 0, 1, 0]\ny_3 = [1, 0, 1, 1, 0, 0, 1, 0]\n```", "```py\nfrom dataclasses import dataclass\n\n@dataclass\nclass Node:\n    feature: int = None # feature for the split\n    value: float = None # split threshold OR final prediction\n    left: np.array = None # store one part of the data\n    right: np.array = None # store the other part of the data\n```", "```py\nimport numpy as np\nfrom sklearn.base import BaseEstimator, ClassifierMixin\n\nclass DecisionTreeClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(self):\n        self.root = Node()\n\n    @staticmethod\n    def _gini(y):\n        \"\"\"Gini impurity.\"\"\"\n        counts = np.bincount(y)\n        p = counts / counts.sum()\n\n        return (p * (1 - p)).sum()\n\n    def _split(self, X, y):\n        \"\"\"Bruteforce search over all features and splitting points.\"\"\"\n        best_information_gain = float(\"-inf\")\n        best_feature = None\n        best_split = None\n\n        for feature in range(X.shape[1]):\n            split_candidates = np.unique(X[:, feature])\n            for split in split_candidates:\n                left_mask = X[:, feature] < split\n                X_left, y_left = X[left_mask], y[left_mask]\n                X_right, y_right = X[~left_mask], y[~left_mask]\n\n                information_gain = self._gini(y) - (\n                    len(X_left) / len(X) * self._gini(y_left)\n                    + len(X_right) / len(X) * self._gini(y_right)\n                )\n\n                if information_gain > best_information_gain:\n                    best_information_gain = information_gain\n                    best_feature = feature\n                    best_split = split\n\n        return best_feature, best_split\n\n    def _build_tree(self, X, y):\n        \"\"\"The heavy lifting.\"\"\"\n        feature, split = self._split(X, y)\n\n        left_mask = X[:, feature] < split\n\n        X_left, y_left = X[left_mask], y[left_mask]\n        X_right, y_right = X[~left_mask], y[~left_mask]\n\n        if len(X_left) == 0 or len(X_right) == 0:\n            return Node(value=np.argmax(np.bincount(y)))\n        else:\n            return Node(\n                feature,\n                split,\n                self._build_tree(X_left, y_left),\n                self._build_tree(X_right, y_right),\n            )\n\n    def _find_path(self, x, node):\n        \"\"\"Given a data point x, walk from the root to the corresponding leaf node. Output its value.\"\"\"\n        if node.feature == None:\n            return node.value\n        else:\n            if x[node.feature] < node.value:\n                return self._find_path(x, node.left)\n            else:\n                return self._find_path(x, node.right)\n\n    def fit(self, X, y):\n        self.root = self._build_tree(X, y)\n        return self\n\n    def predict(self, X):\n        return np.array([self._find_path(x, self.root) for x in X])\n```", "```py\ndt = DecisionTreeClassifier().fit(X, y)\nprint(dt.score(X, y)) # accuracy\n\n# Output\n# 1.0\n```", "```py\nprint(dt.root)\n\n# Output (prettified manually):\n# Node(\n#   feature=1,\n#   value=-0.14963454032767076,\n#   left=Node(\n#          feature=0,\n#          value=0.04575851730144607,\n#          left=Node(\n#                 feature=None,\n#                 value=0,\n#                 left=None,\n#                 right=None\n#          ),\n#          right=Node(\n#                  feature=None,\n#                  value=1,\n#                  left=None,\n#                  right=None\n#          )\n#        ),\n#   right=Node(\n#           feature=None,\n#           value=0,\n#           left=None,\n#           right=None\n#   )\n# )\n```"]