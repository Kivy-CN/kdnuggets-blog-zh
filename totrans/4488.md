# 使用 Apache Spark 和 PySpark 的好处与示例

> 原文：[https://www.kdnuggets.com/2020/04/benefits-apache-spark-pyspark.html](https://www.kdnuggets.com/2020/04/benefits-apache-spark-pyspark.html)

[评论](#comments)

### **什么是 Apache Spark?**

[Apache Spark](https://spark.apache.org/)是技术领域中最炙手可热的趋势之一。它可能是实现大数据与机器学习结合成果的潜力最大的框架。

它运行速度很快（比传统的[Hadoop MapReduce](https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm)快最多100倍，因其内存操作），提供强大、分布式、容错的数据对象（称为[RDD](https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.htm)），并通过像[Mlib](https://spark.apache.org/mllib/)和[GraphX](https://spark.apache.org/graphx/)等附加包与机器学习和图分析领域完美集成。

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升您的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持您组织的 IT

* * *

![使用 Apache Spark 和 PySpark 在 Python 中的好处与示例 | Apache Spark](../Images/30cb3911f9df0322fd922f31ff23852a.png)

Spark 是在[Hadoop/HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html)上实现的，主要用[Scala](https://www.scala-lang.org/)编写，Scala 是一种类似于 Java 的函数式编程语言。事实上，Scala 需要系统上最新的 Java 安装，并运行在 JVM 上。然而，对于大多数初学者来说，Scala 不是他们学习数据科学领域的第一语言。幸运的是，Spark 提供了一个极好的 Python 集成，称为**PySpark**，它允许 Python 程序员与 Spark 框架进行接口，并学习如何在大规模下处理数据以及在分布式文件系统上处理对象和算法。

在这篇文章中，我们将学习 PySpark 的基础知识。虽然有很多概念（不断发展和引入），因此，我们仅关注基础知识，并通过几个简单的示例进行讲解。鼓励读者在此基础上进一步探索。

### **Apache Spark 的简短历史**

Apache Spark 起初是 UC Berkeley AMPLab 在 2009 年的一个研究项目，并在 2010 年初开源。它是 UC Berkeley 的一个课程项目。其理念是构建一个集群管理框架，能够支持不同类型的集群计算系统。多年来，系统背后的许多理念在各种研究论文中得到了展示。发布后，Spark 发展成为一个庞大的开发者社区，并在 2013 年迁移至 Apache 软件基金会。如今，该项目由来自数百个组织的数百名开发者共同开发。

### **Spark 不是编程语言**

需要记住的一点是，Spark 不是像 Python 或 Java 这样的编程语言。它是一个通用的分布式数据处理引擎，适用于各种情况。它特别适合大数据处理，无论是大规模还是高速度的处理。

应用程序开发者和数据科学家通常将 Spark 集成到他们的应用程序中，以快速查询、分析和转换大规模数据。与 Spark 最常关联的一些任务包括：– 对大数据集（通常是几个 TB 大小）进行 ETL 和 SQL 批处理作业，– 处理来自 IoT 设备和节点的流数据、各种传感器的数据、各类金融和事务系统的数据，以及 – 用于电子商务或 IT 应用的机器学习任务。

从本质上讲，Spark 构建在 Hadoop/HDFS 框架之上，用于处理分布式文件。它主要用 Scala 实现，Scala 是 Java 的一种函数式语言变体。Spark 拥有一个核心的数据处理引擎，但在其之上，还有许多用于 SQL 类型查询分析、分布式机器学习、大规模图计算和流数据处理的库。Spark 支持多种编程语言，通过易于使用的接口库提供：Java、Python、Scala 和 R。

### **Spark 使用 MapReduce 范式进行分布式处理**

分布式处理的基本思想是将数据块划分为小的可管理部分（包括一些过滤和排序），将计算靠近数据，即使用大型集群中的小节点进行特定作业，然后再将它们重新组合。划分部分称为“Map”操作，重新组合部分称为“Reduce”操作。它们共同形成了著名的“MapReduce”范式，该范式由 Google 于 2004 年左右引入（请参见[原始论文](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)）。

例如，如果一个文件有 100 条记录需要处理，100 个映射器可以一起运行来处理每条记录。或者 50 个映射器可以一起运行来处理每两个记录。所有映射器完成处理后，框架会在将结果传递给归约器之前对其进行洗牌和排序。一个归约器在映射器仍在进行时无法启动。所有具有相同键的映射输出值都会分配给一个归约器，该归约器随后会聚合该键的值。

### **如何设置 PySpark**

如果你已经熟悉 Python 及 Pandas 和 Numpy 等库，那么 PySpark 是一个很好的扩展/框架，可以通过利用后台 Spark 的强大功能来创建更具可扩展性的数据密集型分析和管道。

安装和设置 PySpark 环境（在独立机器上）的具体过程相当复杂，可能会因系统和环境的不同而有所变化。目标是使你常规的 Jupyter 数据科学环境在后台使用 PySpark 包来运行 Spark。

[**这篇文章**](https://medium.com/free-code-camp/how-to-set-up-pyspark-for-your-jupyter-notebook-7399dd3cb389) 提供了有关逐步设置过程的更多细节。

![](../Images/881536150793aa5d361af9092960c036.png)

或者，你可以使用 Databricks 设置来练习 Spark。这家公司由 Spark 的原始创建者创建，拥有一个优秀的即开即用环境来进行分布式分析。

但思想总是相同的。你是将大型数据集分成许多固定的小块在多个节点上进行分发（和复制）。然后将计算引擎靠近这些节点，从而使整个操作并行化、容错并且可扩展。

通过使用 PySpark 和 Jupyter notebook，你可以学习所有这些概念而无需在 AWS 或 Databricks 平台上花费任何费用。你还可以轻松地与 SparkSQL 和 MLlib 进行接口以进行数据库操作和机器学习。如果你事先掌握了这些概念，那么开始使用现实世界的大型集群将会容易得多！

### **弹性分布式数据集 (RDD) 和 SparkContext**

许多 Spark 程序围绕弹性分布式数据集（RDD）的概念展开，RDD 是一个容错的元素集合，可以并行操作。SparkContext 存在于 Driver 程序中，并通过集群管理器管理分布式数据。使用 PySpark 的好处是数据分区和任务管理的复杂性在后台自动处理，程序员可以专注于具体的分析或机器学习任务。

![使用 Apache Spark 与 PySpark 在 Python 中的好处与示例 | Rdd-1](../Images/5bb0f6532941bb2fa5a40915e804a5df.png)

*rdd-1*

创建 RDD 有两种方式——在你的驱动程序中并行化现有集合，或引用外部存储系统中的数据集，如共享文件系统、HDFS、HBase 或任何提供 Hadoop InputFormat 的数据源。

为了用基于 Python 的方法进行说明，我们将在这里给出第一种类型的示例。我们可以使用 Numpy random.randint() 创建一个包含 20 个随机整数（介于 0 和 10 之间）的简单 Python 数组，然后按照以下方式创建 RDD 对象，

```py
from pyspark import SparkContext
import numpy as np
sc=SparkContext(master="local[4]")
lst=np.random.randint(0,10,20)
A=sc.parallelize(lst)

```

***注意参数中的‘4’*****。它表示为这个 ****SparkContext**** 对象使用4个计算核心（在你的本地机器上）。如果我们检查RDD对象的类型，我们会得到以下结果，

```py
type(A)
>> pyspark.rdd.RDD
```

与并行化相对的是收集（使用 collect()），它将所有分布式元素带回头节点。

```py
A.collect()
>> [4, 8, 2, 2, 4, 7, 0, 3, 3, 9, 2, 6, 0, 0, 1, 7, 5, 1, 9, 7]
```

但是 A 不再是简单的 Numpy 数组。我们可以使用 glom() 方法来检查分区是如何创建的。

```py
A.glom().collect()
>> [[4, 8, 2, 2, 4], [7, 0, 3, 3, 9], [2, 6, 0, 0, 1], [7, 5, 1, 9, 7]]
```

现在停止 SC，并用 2 个核心重新初始化，看看当你重复这个过程时会发生什么。

```py
sc.stop()
sc=SparkContext(master="local[2]")
A = sc.parallelize(lst)
A.glom().collect()
>> [[4, 8, 2, 2, 4, 7, 0, 3, 3, 9], [2, 6, 0, 0, 1, 7, 5, 1, 9, 7]]
```

***RDD 现在分布在两个块上，而不是四个块！***

**你已经了解了分布式数据分析的第一步，即控制数据如何被划分为更小的块以便进一步处理**

### **RDD 和 PySpark 的基本操作示例**

**计算元素个数**

```py
>> 20
```

**第一个元素（****第一个****）和前几个元素（****取****）**

```py
A.first()
>> 4
A.take(3)
>> [4, 8, 2]
```

**使用 ****distinct** 进行去重

*注意*: 这个操作需要一个**shuffle**以便检测分区间的重复。因此，它是一个较慢的操作。不要过度使用它。

```py
A_distinct=A.distinct()
A_distinct.collect()
>> [4, 8, 0, 9, 1, 5, 2, 6, 7, 3]
```

**要对所有元素求和，使用 ****reduce**** 方法**

注意其中使用了 lambda 函数，

```py
A.reduce(lambda x,y:x+y)
>> 80
```

**或者直接使用 ****sum()**** 方法**

```py
A.sum()
>> 80
```

**通过 ****reduce** 查找最大元素

```py
A.reduce(lambda x,y: x if x > y else y)
>> 9
```

**在一段文本中查找最长单词**

```py
words = 'These are some of the best Macintosh computers ever'.split(' ')
wordRDD = sc.parallelize(words)
wordRDD.reduce(lambda w,v: w if len(w)>len(v) else v)
>> 'computers'
```

**使用 ****filter**** 进行基于逻辑的过滤**

```py
# Return RDD with elements (greater than zero) divisible by 3
A.filter(lambda x:x%3==0 and x!=0).collect()
>> [3, 3, 9, 6, 9]
```

**编写常规 Python 函数以配合 ****reduce()**

```py
def largerThan(x,y):
  """
  Returns the last word among the longest words in a list
  """
  if len(x)> len(y):
    return x
  elif len(y) > len(x):
    return y
  else:
    if x < y: return x
    else: return y

wordRDD.reduce(largerThan)
>> 'Macintosh'
```

注意这里 x < y 进行的是词典序比较，并确定 Macintosh 大于 computers！

**使用 PySpark 的 lambda 函数的映射操作**

```py
B=A.map(lambda x:x*x)
B.collect()
>> [16, 64, 4, 4, 16, 49, 0, 9, 9, 81, 4, 36, 0, 0, 1, 49, 25, 1, 81, 49]
```

**在 PySpark 中使用常规 Python 函数进行映射**

```py
def square_if_odd(x):
  """
  Squares if odd, otherwise keeps the argument unchanged
  """
  if x%2==1:
    return x*x
  else:
    return x

A.map(square_if_odd).collect()
>> [4, 8, 2, 2, 4, 49, 0, 9, 9, 81, 2, 6, 0, 0, 1, 49, 25, 1, 81, 49]
```

**groupby**** 返回一个根据给定分组操作的分组元素（可迭代）RDD**

在下面的示例中，我们使用列表推导式结合 groupby 创建一个包含两个元素的列表，每个元素都有一个标题（lambda 函数的结果，这里是简单的模 2），以及生成该结果的元素的排序列表。你可以很容易地想象这种分离在需要根据特定操作进行分箱/处理的数据处理时特别有用。

```py
result=A.groupBy(lambda x:x%2).collect()
sorted([(x, sorted(y)) for (x, y) in result])
>> [(0, [0, 0, 0, 2, 2, 2, 4, 4, 6, 8]), (1, [1, 1, 3, 3, 5, 7, 7, 7, 9, 9])]
```

**使用 ****histogram**

histogram() 方法接受一个桶/箱的列表，并返回一个包含直方图（分箱）结果的元组，

```py
B.histogram([x for x in range(0,100,10)])
>> ([0, 10, 20, 30, 40, 50, 60, 70, 80, 90], [10, 2, 1, 1, 3, 0, 1, 0, 2])
```

**集合操作**

你还可以在 RDD 上进行常规集合操作，如 union()、 intersection()、 subtract() 或 cartesian()。

查看[**这个Jupyter笔记本**](https://github.com/tirthajyoti/Spark-with-Python/blob/master/SparkContext%20and%20RDD%20Basics.ipynb)以获取更多示例。

**PySpark中的惰性评估（以及**Caching**）**

惰性评估是一种评估/计算策略，它为计算任务准备了详细的逐步内部执行流程图，但会将最终执行推迟到绝对需要时。这个策略是Spark加速许多并行大数据操作的核心。

让我们用两个CPU核心来做这个例子，

```py
sc = SparkContext(master="local[2]")
```

**创建一个包含100万元素的RDD**

```py
%%time
rdd1 = sc.parallelize(range(1000000))
>> CPU times: user 316 µs, sys: 5.13 ms, total: 5.45 ms, Wall time: 24.6 ms
```

**某个计算函数 – **taketime**

```py
from math import cos
def taketime(x):
[cos(j) for j in range(100)]
return cos(x)
```

**检查taketime函数所需的时间**

```py
%%time
taketime(2)
>> CPU times: user 21 µs, sys: 7 µs, total: 28 µs, Wall time: 31.5 µs
>> -0.4161468365471424
```

记住这个结果，taketime()函数的实际时间为31.5微秒。当然，具体的数字会依赖于你所使用的机器。

**现在对函数进行映射操作**

```py
%%time
interim = rdd1.map(lambda x: taketime(x))
>> CPU times: user 23 µs, sys: 8 µs, total: 31 µs, Wall time: 34.8 µs
```

*为什么每个**taketime**函数都需要45.8微秒，但一个包含100万元素的RDD的映射操作也花费了类似的时间？*

**由于惰性评估，即在上一步中没有进行任何计算，仅仅制定了执行计划**。变量interim并不指向数据结构，而是指向一个执行计划，表示为依赖图。依赖图定义了RDD如何相互计算。

**通过**reduce**方法的实际执行**

```py
%%time
print('output =',interim.reduce(lambda x,y:x+y))
>> output = -0.28870546796843666
>> CPU times: user 11.6 ms, sys: 5.56 ms, total: 17.2 ms, Wall time: 15.6 s
```

所以，这里的实际时间为15.6秒。记住，taketime()函数的时间是31.5微秒？因此，我们预计1百万数组的总时间约为31秒。由于在两个核心上并行操作，它花费了约15秒。

现在，我们没有在interim中保存（物化）任何中间结果，所以另一个简单操作（例如计数元素> 0）将花费几乎相同的时间。

```py
%%time
print(interim.filter(lambda x:x>0).count())
>> 500000
>> CPU times: user 10.6 ms, sys: 8.55 ms, total: 19.2 ms, Wall time: 12.1 s
```

### **缓存以减少类似操作的计算时间（消耗内存）**

记住我们在上一步中构建的依赖图吗？我们可以使用缓存方法进行相同的计算，以便告诉依赖图规划缓存。

```py
%%time
interim = rdd1.map(lambda x: taketime(x)).cache()
```

第一次计算不会有所改进，但它缓存了中间结果，

```py
%%time
print('output =',interim.reduce(lambda x,y:x+y))
>> output = -0.28870546796843666
>> CPU times: user 16.4 ms, sys: 2.24 ms, total: 18.7 ms, Wall time: 15.3 s
```

现在使用缓存结果运行相同的过滤方法，

```py
%%time
print(interim.filter(lambda x:x>0).count())
>> 500000
>> CPU times: user 14.2 ms, sys: 3.27 ms, total: 17.4 ms, Wall time: 811 ms
```

哇！计算时间从之前的12秒降到了不到1秒！通过这种方式，使用缓存和惰性执行的并行化是使用Spark编程的核心特性。

### **DataFrame和SparkSQL**

除了RDD，Spark框架中的第二个关键数据结构是*DataFrame*。如果你有使用Python Pandas或R DataFrame的经验，这个概念可能很熟悉。

DataFrame是按命名列组织的分布式行集合。它在概念上等同于关系数据库中的表、带有列头的Excel表格或R/Python中的数据框，但在底层具有更丰富的优化。DataFrame可以从各种来源构建，如：结构化数据文件、Hive中的表、外部数据库或现有的RDD。它还与RDD共享一些共同的特性：

+   本质上是不可变的：我们可以创建DataFrame / RDD，但不能更改它。我们可以在应用转换后对DataFrame / RDD进行转换。

+   惰性求值：这意味着在执行操作之前不会执行任务。分布式：RDD和DataFrame本质上都是分布式的。

![使用Apache Spark与PySpark在Python中的好处和示例 | PySpark中的DataFrame](../Images/197e1ce09b894ea1628c7da1c8d3c87d.png)

### **DataFrame的优点**

+   DataFrame设计用于处理大量结构化或半结构化数据。

+   Spark DataFrame中的观察结果按命名列组织，这帮助Apache Spark理解DataFrame的模式。这有助于Spark优化这些查询的执行计划。

+   Apache Spark中的DataFrame能够处理PB级别的数据。

+   DataFrame支持多种数据格式和数据源。

+   它支持Python、R、Scala、Java等不同语言的API。

### **DataFrame基础示例**

有关DataFrame的基础知识和典型使用示例，请参见以下Jupyter Notebooks，

[**Spark DataFrame基础**](https://github.com/tirthajyoti/Spark-with-Python/blob/master/Dataframe_basics.ipynb)

[**Spark DataFrame操作**](https://github.com/tirthajyoti/Spark-with-Python/blob/master/DataFrame_operations_basics.ipynb)

### **SparkSQL帮助弥合PySpark的差距**

关系型数据存储易于构建和查询。用户和开发者通常更喜欢用易于理解的声明性查询语言，如SQL。然而，随着数据量和数据种类的增加，关系型方法在构建大数据应用和分析系统时的扩展性不足。

在Hadoop和MapReduce范式下，我们在大数据分析领域取得了成功。这很强大，但往往很慢，并且提供了一个低级的**过程编程接口**，要求用户为即使是非常简单的数据转换编写大量代码。然而，一旦Spark发布，它确实彻底革新了大数据分析的方式，专注于内存计算、容错、高级抽象和易用性。

Spark SQL本质上试图弥合我们之前提到的两种模型——关系模型和过程模型之间的差距。Spark SQL通过DataFrame API来处理对外部数据源和Spark的内置分布式集合进行关系操作——在大规模下！

![使用 Apache Spark 和 PySpark 在 Python 中的好处与示例 | Spark SQL](../Images/fbbca3e99c3728cb51c0284181460269.png)

为什么 Spark SQL 会如此快速且优化？原因在于一种新的可扩展优化器**Catalyst**，它基于 Scala 中的函数式编程结构。Catalyst 支持规则基础优化和成本基础优化。虽然过去曾提出过可扩展的优化器，但它们通常需要复杂的领域特定语言来指定规则。这通常导致有显著的学习曲线和维护负担。相比之下，Catalyst 使用 Scala 编程语言的标准特性，如模式匹配，让开发者可以使用完整的编程语言，同时仍然使规则的指定变得简单。

你可以参考以下 Jupyter 笔记本，了解 SparkSQL 数据库操作的介绍：

[**SparkSQL 数据库操作基础**](https://github.com/tirthajyoti/Spark-with-Python/blob/master/Dataframe_SQL_query.ipynb)

### **你打算如何在项目中使用 PySpark？**

我们涵盖了 Apache Spark 生态系统的基本概念及其工作原理，并提供了一些 PySpark 核心数据结构 RDD 的基本用法示例。此外，还讨论了 DataFrame 和 SparkSQL，并提供了示例代码笔记本的参考链接。

使用 Python 的 Apache Spark 有更多的学习和实验空间。可以参考 [PySpark 网站](https://spark.apache.org/docs/latest/api/python/index.html)，它是一个很好的参考资源，并且他们会定期更新和增强功能——请保持关注。

如果你对使用 Apache Spark 进行大规模分布式机器学习感兴趣，可以查看 [PySpark 生态系统的 MLLib 部分](https://spark.apache.org/mllib/)。

[原文](https://blog.exxactcorp.com/the-benefits-examples-of-using-apache-spark-with-pyspark-using-python/)。经许可转载。

**相关内容：**

+   [在 5 分钟内学习如何使用 PySpark（安装 + 教程](/2019/08/learn-pyspark-installation-tutorial.html)

+   [时间序列分析：使用 KNIME 和 Spark 的简单示例](/2019/10/time-series-analysis-simple-example-knime-spark.html)

+   [Spark NLP 101: LightPipeline](/2019/11/spark-nlp-101-lightpipeline.html)

### 更多相关主题

+   [使用 Pandera 进行 PySpark 应用程序的数据验证](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)

+   [成为数据优先企业的好处](https://www.kdnuggets.com/2022/07/benefits-becoming-datafirst-enterprise.html)

+   [A/B 测试的 3 大好处 (+ 如何入门)](https://www.kdnuggets.com/2022/08/sphere-3-benefits-ab-testing-get-started.html)

+   [自然语言 AI 对内容创作者的好处](https://www.kdnuggets.com/2022/08/benefits-natural-language-ai-content-creators.html)

+   [PySpark 在数据科学中的应用](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)

+   [挑选示例以理解机器学习模型](https://www.kdnuggets.com/2022/11/picking-examples-understand-machine-learning-model.html)
