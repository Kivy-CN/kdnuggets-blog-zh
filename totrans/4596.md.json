["```py\nimport numpy as np  # import numpy library\nfrom util.paramInitializer import initialize_parameters  # import function to initialize weights and biases\n\nclass LinearLayer:\n    \"\"\"\n        This Class implements all functions to be executed by a linear layer\n        in a computational graph\n        Args:\n            input_shape: input shape of Data/Activations\n            n_out: number of neurons in layer\n            ini_type: initialization type for weight parameters, default is \"plain\"\n                      Opitons are: plain, xavier and he\n        Methods:\n            forward(A_prev)\n            backward(upstream_grad)\n            update_params(learning_rate)\n    \"\"\"\n\n    def __init__(self, input_shape, n_out, ini_type=\"plain\"):\n        \"\"\"\n        The constructor of the LinearLayer takes the following parameters\n        Args:\n            input_shape: input shape of Data/Activations\n            n_out: number of neurons in layer\n            ini_type: initialization type for weight parameters, default is \"plain\"\n        \"\"\"\n\n        self.m = input_shape[1]  # number of examples in training data\n        # `params` store weights and bias in a python dictionary\n        self.params = initialize_parameters(input_shape[0], n_out, ini_type)  # initialize weights and bias\n        self.Z = np.zeros((self.params['W'].shape[0], input_shape[1]))  # create space for resultant Z output\n\n    def forward(self, A_prev):\n        \"\"\"\n        This function performs the forwards propagation using activations from previous layer\n        Args:\n            A_prev:  Activations/Input Data coming into the layer from previous layer\n        \"\"\"\n\n        self.A_prev = A_prev  # store the Activations/Training Data coming in\n        self.Z = np.dot(self.params['W'], self.A_prev) + self.params['b']  # compute the linear function\n\n    def backward(self, upstream_grad):\n        \"\"\"\n        This function performs the back propagation using upstream gradients\n        Args:\n            upstream_grad: gradient coming in from the upper layer to couple with local gradient\n        \"\"\"\n\n        # derivative of Cost w.r.t W\n        self.dW = np.dot(upstream_grad, self.A_prev.T)\n\n        # derivative of Cost w.r.t b, sum across rows\n        self.db = np.sum(upstream_grad, axis=1, keepdims=True)\n\n        # derivative of Cost w.r.t A_prev\n        self.dA_prev = np.dot(self.params['W'].T, upstream_grad)\n\n    def update_params(self, learning_rate=0.1):\n        \"\"\"\n        This function performs the gradient descent update\n        Args:\n            learning_rate: learning rate hyper-param for gradient descent, default 0.1\n        \"\"\"\n\n        self.params['W'] = self.params['W'] - learning_rate * self.dW  # update weights\n        self.params['b'] = self.params['b'] - learning_rate * self.db  # update bias(es)\n\n```", "```py\nimport numpy as np  # import numpy library\n\nclass SigmoidLayer:\n    \"\"\"\n    This file implements activation layers\n    inline with a computational graph model\n    Args:\n        shape: shape of input to the layer\n    Methods:\n        forward(Z)\n        backward(upstream_grad)\n    \"\"\"\n\n    def __init__(self, shape):\n        \"\"\"\n        The consturctor of the sigmoid/logistic activation layer takes in the following arguments\n        Args:\n            shape: shape of input to the layer\n        \"\"\"\n        self.A = np.zeros(shape)  # create space for the resultant activations\n\n    def forward(self, Z):\n        \"\"\"\n        This function performs the forwards propagation step through the activation function\n        Args:\n            Z: input from previous (linear) layer\n        \"\"\"\n        self.A = 1 / (1 + np.exp(-Z))  # compute activations\n\n    def backward(self, upstream_grad):\n        \"\"\"\n        This function performs the  back propagation step through the activation function\n        Local gradient => derivative of sigmoid => A*(1-A)\n        Args:\n            upstream_grad: gradient coming into this layer from the layer above\n        \"\"\"\n        # couple upstream gradient with local gradient, the result will be sent back to the Linear layer\n        self.dZ = upstream_grad * self.A*(1-self.A)\n\n```", "```py\ndef compute_cost(Y, Y_hat):\n    \"\"\"\n    This function computes and returns the Cost and its derivative.\n    The is function uses the Squared Error Cost function -> (1/2m)*sum(Y - Y_hat)^.2\n    Args:\n        Y: labels of data\n        Y_hat: Predictions(activations) from a last layer, the output layer\n    Returns:\n        cost: The Squared Error Cost result\n        dY_hat: gradient of Cost w.r.t the Y_hat\n    \"\"\"\n    m = Y.shape[1]\n\n    cost = (1 / (2 * m)) * np.sum(np.square(Y - Y_hat))\n    cost = np.squeeze(cost)  # remove extraneous dimensions to give just a scalar\n\n    dY_hat = -1 / m * (Y - Y_hat)  # derivative of the squared error cost function\n\n    return cost, dY_hat\n\n```", "```py\ndef compute_cost(Y, Y_hat):\n    \"\"\"\n    This function computes and returns the Cost and its derivative.\n    The is function uses the Squared Error Cost function -> (1/2m)*sum(Y - Y_hat)^.2\n    Args:\n        Y: labels of data\n        Y_hat: Predictions(activations) from a last layer, the output layer\n    Returns:\n        cost: The Squared Error Cost result\n        dY_hat: gradient of Cost w.r.t the Y_hat\n    \"\"\"\n    m = Y.shape[1]\n\n    cost = (1 / (2 * m)) * np.sum(np.square(Y - Y_hat))\n    cost = np.squeeze(cost)  # remove extraneous dimensions to give just a scalar\n\n    dY_hat = -1 / m * (Y - Y_hat)  # derivative of the squared error cost function\n\n    return cost, dY_hat\n\n```", "```py\n# define training constants\nlearning_rate = 1\nnumber_of_epochs = 5000\n\nnp.random.seed(48) # set seed value so that the results are reproduceable\n                  # (weights will now be initailzaed to the same pseudo-random numbers, each time)\n\n# Our network architecture has the shape: \n#                   (input)--> [Linear->Sigmoid] -> [Linear->Sigmoid] -->(output)  \n\n#------ LAYER-1 ----- define hidden layer that takes in training data \nZ1 = LinearLayer(input_shape=X_train.shape, n_out=3, ini_type='xavier')\nA1 = SigmoidLayer(Z1.Z.shape)\n\n#------ LAYER-2 ----- define output layer that take is values from hidden layer\nZ2= LinearLayer(input_shape=A1.A.shape, n_out=1, ini_type='xavier')\nA2= SigmoidLayer(Z2.Z.shape)\n\n```", "```py\ncosts = [] # initially empty list, this will store all the costs after a certian number of epochs\n\n# Start training\nfor epoch in range(number_of_epochs):\n\n    # ------------------------- forward-prop -------------------------\n    Z1.forward(X_train)\n    A1.forward(Z1.Z)\n\n    Z2.forward(A1.A)\n    A2.forward(Z2.Z)\n\n    # ---------------------- Compute Cost ----------------------------\n    cost, dA2 = compute_cost(Y=Y_train, Y_hat=A2.A)\n\n    # print and store Costs every 100 iterations.\n    if (epoch % 100) == 0:\n        #print(\"Cost at epoch#\" + str(epoch) + \": \" + str(cost))\n        print(\"Cost at epoch#{}: {}\".format(epoch, cost))\n        costs.append(cost)\n\n    # ------------------------- back-prop ----------------------------\n    A2.backward(dA2)\n    Z2.backward(A2.dZ)\n\n    A1.backward(Z2.dA_prev)\n    Z1.backward(A1.dZ)\n\n    # ----------------------- Update weights and bias ----------------\n    Z2.update_params(learning_rate=learning_rate)\n    Z1.update_params(learning_rate=learning_rate)\n\n```", "```py\n...\nCost at epoch#4600: 0.001018305488651183\nCost at epoch#4700: 0.000983783942124411\nCost at epoch#4800: 0.0009514180100050973\nCost at epoch#4900: 0.0009210166430616655\n```", "```py\nThe predicted outputs:\n [[ 0\\.  1\\.  1\\.  0.]]\nThe accuracy of the model is: 100.0%\n```"]