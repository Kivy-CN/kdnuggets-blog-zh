- en: Matrix Decomposition Decoded
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵分解解读
- en: 原文：[https://www.kdnuggets.com/2020/12/matrix-decomposition-decoded.html](https://www.kdnuggets.com/2020/12/matrix-decomposition-decoded.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/12/matrix-decomposition-decoded.html](https://www.kdnuggets.com/2020/12/matrix-decomposition-decoded.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Tanveer Sayyed](https://www.linkedin.com/in/tanveer-sayyed-739105185/),
    Data Science Enthusiast**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Tanveer Sayyed](https://www.linkedin.com/in/tanveer-sayyed-739105185/),
    数据科学爱好者**'
- en: To understand matrix decomposition we’ll have to first understand eigenvalues(referred
    to as lambdas here on) and eigenvectors. And even before comprehending the intuition
    behind lambdas and eigenvectors we’ll first need to uncover the purpose behind
    using matrix and vectors in linear algebra.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解矩阵分解，我们首先需要理解特征值（从现在开始称为 lambda）和特征向量。在理解 lambda 和特征向量的直觉之前，我们首先需要揭示在线性代数中使用矩阵和向量的目的。
- en: So what is the purpose of a matrix and a vector in linear algebra?
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 那么矩阵和向量在线性代数中的目的是什么呢？
- en: 'In machine learning we are usually concerned with matrices which have features
    as their columns. We have thus innately made matrices sacrosanct in our minds.
    Now this might come as a surprise because —* the* *purpose of a matrix is to “act”
    on (input) vectors to give (output) vectors****!*** Recall the equation of linear
    regression using linear algebra:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们通常关注的矩阵的列是特征。因此，我们在心中天然地将矩阵视为神圣的。现在这可能会让你感到惊讶，因为—*矩阵的* *目的是“作用于”（输入）向量，以给出（输出）向量****!***
    记住线性回归的线性代数方程：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The correct way to read this is — we *‘apply’* the matrix to a vector*(y)* to
    get the output vector*(coefficients)*. Or in other words — the matrix *‘transforms/distorts’* the
    input vector to become the output vector. In goes a vector, gets transformed/distorted
    by a matrix, and out comes another vector. This is just like a function in calculus:
    in goes a number x(say 3) and outcomes a number *f*(x)[say 27, if *f*(x) = x³].'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的阅读方式是——我们*“应用”*矩阵到一个向量*(y)*上，以得到输出向量*(系数)*。换句话说——矩阵*“转换/扭曲”*输入向量以成为输出向量。一个向量输入，通过矩阵变换/扭曲，输出另一个向量。这就像是微积分中的函数：一个数字
    x（比如 3）输入，得到一个数字 *f*(x)（例如 27，如果 *f*(x) = x³）。
- en: The intuition behind lambdas and eigenvectors
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于 lambda 和特征向量的直觉
- en: 'Since the purpose of matrices are now clear, defining eigenvectors would be
    easy. If we find that, the input vector which goes in and the output vector which
    comes out, are ***still*** parallel to each other, only then that vector is called
    eigenvector. Which means they have sustained the distortion or are not affected
    by it. “Parallel” refers to either one of the two:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 既然矩阵的目的已经明确，那么定义特征向量就会变得容易。如果我们发现输入向量和输出向量*仍然*彼此平行，那么这个向量就是特征向量。这意味着它们保持了变形或未受到影响。“平行”指的是以下两种情况之一：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'At school we were taught that **Force **is a vector. It has a *direction* as
    well as a *magnitude*. The same way think of eigenvectors as the *direction* of
    the distortion and lambdas as the *magnitude* of distortion in ‘**that**’ direction.
    Thus a matrix is decomposed/factorized(just like 6= 3*2) into a vector and its
    magnitude which can be written as a product of both:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在学校里，我们被教导**力**是一个向量。它具有*方向*以及*magnitude*。同样，特征向量可以被看作是*变形的方向*，而 lambda 则是*在该方向上的变形幅度*。因此，矩阵被分解/因式分解（就像
    6= 3*2 一样）为一个向量及其幅度，可以表示为两者的乘积：
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This is called eigen-decomposition. But how do we find eigenvectors? *From
    the λs!* Here is how:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这称为特征分解。但是我们如何找到特征向量呢？*从 λs 中找！* 这里是方法：
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'What was important was knowing and understanding these steps so we’ll *not* do
    this manually because a function already exists in *numpy* library. (And we’ll
    be using [***this code***](https://gist.github.com/Vernal-Inertia/90b81a3bf6c5abb96c495c905749d072) to
    display and re-scale the eigenvectors). An example:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是了解和理解这些步骤，因此我们将*不会*手动执行这些操作，因为在*numpy*库中已经存在一个函数。（我们将使用[***这段代码***](https://gist.github.com/Vernal-Inertia/90b81a3bf6c5abb96c495c905749d072)来展示和重新缩放特征向量）。例如：
- en: '![Figure](../Images/3014ed7c401fe5275d1ac0ec09796845.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/3014ed7c401fe5275d1ac0ec09796845.png)'
- en: Eigenvectors e1 = [-1, 0.5] and e2 = [-1, -1] are in *PINK* while the output
    vector is in *GREEN*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量 e1 = [-1, 0.5] 和 e2 = [-1, -1] 是*粉色*的，而输出向量是*绿色*的。
- en: Note the effect of lambdas [-1, 5]. The lambdas are responsible for the scaling(change
    in size) of the eigenvectors. λ1 is *negative*, hence we observe negative transformation
    — output vector moves **opposite** to e1 and gets scaled 1 times length of e1\.
    While λ2 is *positive* and hence we observe positive transformation — output vector
    moves in the **same** direction as e2 and gets scaled 5 times length of e2.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 λ 的效果 [-1, 5]。λ 负责特征向量的缩放（大小的变化）。λ1 是 *负的*，因此我们观察到负向变换——输出向量相对于 e1 移动 **相反**，并且缩放为
    e1 长度的 1 倍。而 λ2 是 *正的*，因此我们观察到正向变换——输出向量与 e2 在 **相同** 方向上移动，并且缩放为 e2 长度的 5 倍。
- en: 'Now before we move to applications of the eigenvectors and lambdas lets first
    get their properties into place:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在我们转向特征向量和 λ 的应用之前，让我们先了解它们的属性：
- en: 1\. We can only find eigenvectors and lambdas of a **square** matrix.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 我们只能找到 **方形** 矩阵的特征向量和 λ。
- en: 2\. If A has the shape(n,n) then there would be at-most n no. of **independent** lambdas
    and their corresponding eigenvectors.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 如果 A 的形状是（n,n），则最多会有 n 个 **独立的** λ 及其对应的特征向量。
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 3\. The **sum** of lambdas = trace(sum of diagonals) of the matrix.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 3. λ 的 **和** = 矩阵的迹（对角线之和）。
- en: 4\. The **product** of lambdas = determinant of the matrix-A.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 4. λ 的 **积** = 矩阵 A 的行列式。
- en: 5\. For a **triangular** matrix the lambdas are the diagonal values.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 对于 **三角形** 矩阵，λ 是对角线值。
- en: 6. **Repeated** lambdas are a source of trouble. For every one lambda repeated
    we will have one less independent eigenvector.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 6. **重复的** λ 是问题的根源。每重复一个 λ，我们将减少一个独立的特征向量。
- en: 7\. The more **symmetric** the matrix the better. Symmetric means *A = transpose(A)*.
    Symmetric matrices produce “*real*” lambdas. As we start moving away from symmetric
    to asymmetric matrices the lambdas start becoming complex numbers(*a +****i****b*)
    and hence eigenvectors start mapping to the imaginary space instead of the real
    space. This happens despite every element of matrix-A being a real number as we
    see in the adjacent code.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 7. 矩阵的 **对称性** 越好越好。对称矩阵意味着 *A = transpose(A)*。对称矩阵产生“*实数*” λ。随着我们从对称矩阵转向非对称矩阵，λ
    开始变成复数（*a + i b*），因此特征向量开始映射到虚拟空间而不是实空间。尽管矩阵 A 的每个元素都是实数，但如代码所示，这种情况仍然发生。
- en: Applications of eigenvectors and lambdas
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征向量和 λ 的应用
- en: 'Firstly, let us be clear that eigenvalues(lambdas) are not linear. Which means
    if x, α and y, β are eigenvalues and lambdas of matrices A, B respectively then:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要明确特征值（λ）不是线性的。这意味着如果 x, α 和 y, β 分别是矩阵 A, B 的特征值和 λ，则：
- en: A . B **≠** αβ (x . y), and, A + B **≠** αx + βy.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: A . B **≠** αβ (x . y)，而且，A + B **≠** αx + βy。
- en: 'But they do come handy when it comes to calculating powers of matrices i.e.
    finding A²⁰ or B⁵⁰⁰. If A, B are small matrices it might be possible to calculate
    the result on our machines. But imagine a huge matrix with lots of features as
    columns, would it be feasible then? Would we punch-in B 500 times or run a loop
    500 times? No, because matrix multiplication are computationally exhaustive. So
    let’s understand how “the Eigens” come to the rescue by discussing the second
    way of factorization/decomposition of the same matrix. And that is derived from
    the following equation:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 但是当涉及到计算矩阵的幂时，即寻找 A²⁰ 或 B⁵⁰⁰ 时，它们确实很有用。如果 A, B 是小矩阵，可能可以在我们的机器上计算结果。但是想象一下一个巨大的矩阵，列数众多，那时是否可行？我们是否会重复输入
    B 500 次或循环 500 次？不，因为矩阵乘法是计算密集型的。所以让我们通过讨论同一矩阵的第二种因式分解/分解方式来理解“特征量”如何提供帮助。这是从以下方程推导出的：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now see how convenient it is to calculate large powers via factorization.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看看通过因式分解计算大幂是多么方便。
- en: 'Now let’s dive a little more deeper into why exactly does this happen. Take
    another example:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更深入地探讨一下为什么会这样。再举一个例子：
- en: Observe the eigenvectors of F**¹** and F**⁵**. They remain undistorted and keep
    pointing in the same direction. Thus powers of a matrix has absolutely no effect
    on eigenvectors! But observe both the lambdas when degree is changed from 1 to
    5; λ1 is increasing(1.618 to 11.09) much more rapidly than λ2 is decreasing(**-**0.618
    to** -**0.09). Thus the effect of λ2 almost negligible. This shows that overall
    the matrix is an increasing one, which can be validated from its values in F**⁵**[5,3,3,2].
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 观察 F**¹** 和 F**⁵** 的特征向量。它们保持不变并始终指向相同的方向。因此，矩阵的幂对特征向量完全没有影响！但是观察 λ，当指数从 1 改变为
    5 时；λ1 增加（1.618 到 11.09）的速度远远快于 λ2 减少（**-**0.618 到 **-**0.09）。因此 λ2 的影响几乎可以忽略不计。这表明矩阵总体上是递增的，这可以从
    F**⁵**[5,3,3,2] 的值中验证。
- en: Since we know that matrix multiplications are computationally exhaustive then
    for any square matrix, if it’s each |λ|is < 0 then we know that the matrix is
    a *stable* *matrix* and if each |λ|is >0 then the matrix is blowing up and is
    an *unstable* *matrix* so its better not to move ahead.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道矩阵乘法在计算上是耗时的，因此对于任何方阵，如果它的每个 |λ| < 0，则我们知道矩阵是一个*稳定*的*矩阵*；如果每个 |λ| > 0，则矩阵在膨胀，是一个*不稳定*的*矩阵*，所以最好不要继续前进。
- en: 'The above example is actually the Fibonacci matrix where the matrix F increases/gets
    scaled approx** 1.6180399 **times each time it multiplies itself! Let’s confirm
    this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的例子实际上是斐波那契矩阵，其中矩阵 F 每次自乘时会增加/扩展约**1.6180399**倍！我们来验证一下：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*And that number is the value of lambda itself … isn’t that amazing!!! *'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*而这个数字就是 λ 的值……这难道不是令人惊讶吗！！！*'
- en: The beauty of lambdas**(eigenvalues)** is that despite being *very few* in number
    they can open-up hidden secrets about the properties of that matrix/the transforming *f*unction.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 特征值**(eigenvalues)**的美妙之处在于，尽管它们数量*非常少*，但它们可以揭示矩阵/变换*f*函数的属性的隐藏秘密。
- en: Another application of “the Eigens” is of-course, principle component analysis
    — PCA !
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: “特征值”的另一个应用当然是主成分分析——PCA！
- en: Why PCA?
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么选择 PCA？
- en: 'PCA is used for *feature extraction*/*dimensionality reduction*, which refers
    to the method of reducing the known variables of the data, by *projection*, into
    lesser number of variables holding the *“almost”* the same amount of information.
    There are two *equivalent* ways to interpret PCA:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 用于*特征提取*/*维度减少*，这指的是通过*投影*将数据的已知变量减少到较少的变量中，同时保留*“几乎”*相同数量的信息。有两种*等价的*方式来解释
    PCA：
- en: (i) minimize the projection error,
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: (i) 最小化投影误差，
- en: (ii) maximize the variance of the projection.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: (ii) 最大化投影的方差。
- en: Here it is extremely important to note that both the above statements are actually
    two sides of the same coin, so minimizing one is *equivalent* to maximizing the
    second.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，极其重要的一点是，上述两个陈述实际上是同一个事物的两个方面，因此最小化一个就*等同于*最大化另一个。
- en: '![Figure](../Images/40368ba4eb95d19b7b10363cc9c2c11d.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/40368ba4eb95d19b7b10363cc9c2c11d.png)'
- en: '[Credits](http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/)'
- en: What is the need for projection?
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 投影的必要性是什么？
- en: 'We need it because when there are say 1000 features, then, we have 1000 unknown
    variables. It turns out we need 1000 simultaneous equations to solve for all the
    variables! But *“what if”* no solutions exists to the problem at hand?...! For
    example just take a case of 2-variables:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要它，因为当有 1000 个特征时，我们有 1000 个未知变量。结果是，我们需要 1000 个同时方程来解所有变量！但是*“如果”*没有解决方案呢？例如，考虑一下
    2 个变量的情况：
- en: a + b = 2
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: a + b = 2
- en: a + 2b = 2 …. ?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: a + 2b = 2 ….？
- en: What PCA does is, it finds the closest approximation to the problem at hand
    such that certain properties of the initial data are preserved(in other words
    noise is reduced). Projection thus helps separate noise from the data(code illustration
    below). This is done by minimizing the least square projection error, which gives
    the best possible solution.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 的作用是，它找到最接近当前问题的近似，以便保留初始数据的某些属性（换句话说，噪声被减少）。因此，投影有助于将噪声与数据分离（代码示例见下文）。这是通过最小化最小二乘投影误差来完成的，这给出了最佳可能的解决方案。
- en: Is minimizing projection error same as minimizing least square error in linear
    regression?
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最小化投影误差是否与线性回归中的最小二乘误差最小化相同？
- en: 'Nope. Here is how and why:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 不。以下是原因：
- en: '![Figure](../Images/992839f308e22326683ab9a69b32e57c.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/992839f308e22326683ab9a69b32e57c.png)'
- en: 'Credits: Andrew Ng'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 贡献者：Andrew Ng
- en: This diagram also gives a hint as to why PCA can also be used for unsupervised
    learning!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图示也暗示了为什么 PCA 也可以用于无监督学习！
- en: But where do “the Eigens” pitch in?
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 那么“特征值”在哪些方面有所贡献？
- en: 'For that we’ll have to make ourselves aware of the steps of PCA:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们需要了解 PCA 的步骤：
- en: Since PCA is sensitive to scaling, lets normalize/standardize out matrix-A first.
    M = mean(A) and then, C = A − M.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于 PCA 对缩放非常敏感，因此我们首先需要对矩阵 A 进行标准化/归一化。M = mean(A)，然后 C = A − M。
- en: 'The next step is to *remember* that for best results i.e. to make use of “the
    Eigens” we need a square matrix and which is symmetrical(property-7 above). So
    which matrix satisfies both the conditions? Ummm… Ahaan! — the *covariance matrix*!
    So we do: V = cov(C)'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是*记住*，为了获得最佳结果，即利用“特征值”，我们需要一个方阵，并且它是对称的（属性-7）。那么，哪个矩阵满足这两个条件呢？嗯……啊哈！——*协方差矩阵*！所以我们得到：V
    = cov(C)
- en: As we now have what we want, lets just quickly decompose it.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 既然我们已经得到我们想要的东西，就快速分解一下。
- en: lambdas, eigenvectors = np.linalg.eig(V )
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特征值, 特征向量 = np.linalg.eig(V)
- en: The lambdas are then sorted in descending order. Their corresponding eigenvectors
    now represent the components of the reduced subspace. In the reduced subspace
    these components (eigenvectors) have now become our new axes, and we know that
    axes are always **orthogonal **to one another. (This can only happen when each
    component in PCA is an independent eigenvector). The combinations of these axes
    produce the projected data.(Click [***this ***](https://www.joyofdata.de/public/pca-3d/)link
    to get a better understanding. Make each component point directly at you. You’ll
    see that the first component has captured the highest variance followed by second
    and then the third)
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将特征值按降序排列。它们对应的特征向量现在代表了降维子空间的组件。在降维子空间中，这些组件（特征向量）现在已经成为我们的新坐标轴，我们知道坐标轴总是**正交**的。（这只有在PCA中的每个组件都是独立的特征向量时才会发生）。这些坐标轴的组合产生了投影数据。（点击[***这里***](https://www.joyofdata.de/public/pca-3d/)链接以获得更好的理解。让每个组件直接指向你。你会看到第一个组件捕获了最高的方差，其次是第二个，然后是第三个）
- en: Select k lambdas to retain maximum explained variance. The amount of variance
    captured by k number of lambdas is called *explained variance*.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择k个特征值以保留最大解释方差。k个特征值捕获的方差量称为*解释方差*。
- en: 'What was important was knowing and understanding these steps so we’ll *not* do
    this manually because a function already exists in the *sklearn* library. For
    simplicity we will reduce a 2-dimenstional data to 1-dimensional data while observing
    the effect of noise during the process. Here is the code:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是了解这些步骤，所以我们将*不会*手动执行这些操作，因为*sklearn*库中已经存在一个函数。为了简单起见，我们将把二维数据降到一维数据，同时观察噪声在过程中产生的效果。以下是代码：
- en: '![Figure](../Images/6238965ce4f89bf609d352458ff04222.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/6238965ce4f89bf609d352458ff04222.png)'
- en: '**Note the projections are exactly parallel to the principle component. The
    near uniform closeness of the red and green dots (as compared to black and green
    dots) depicts that noise has been reduced to a good extent.**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意投影与主成分完全平行。红点和绿点的接近度（相对于黑点和绿点）表明噪声已经在很大程度上减少。**'
- en: (If you identify anything wrong/incorrect, please do respond. Criticism is welcomed).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: （如果你发现任何错误/不正确的地方，请回复。欢迎批评。）
- en: 'References:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[https://www.youtube.com/playlist?list=PLE7DDD91010BC51F8(Prof.](https://www.youtube.com/playlist?list=PLE7DDD91010BC51F8(Prof.) Gilbert
    Strang, Stanford University. Most of my content comes from here.)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/playlist?list=PLE7DDD91010BC51F8(Prof.](https://www.youtube.com/playlist?list=PLE7DDD91010BC51F8(Prof.)
    吉尔伯特·斯特朗，斯坦福大学。我的大部分内容都来自这里。)'
- en: '[A Gentle Introduction to Singular-Value Decomposition (SVD) for Machine Learning](https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[对机器学习的奇异值分解（SVD）的温和介绍](https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/)'
- en: '[https://www.youtube.com/watch?v=vs2sRvSzA3o](https://www.youtube.com/watch?v=vs2sRvSzA3o) (probably
    the best visual representation of eigenvalues and eigenvectors)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=vs2sRvSzA3o](https://www.youtube.com/watch?v=vs2sRvSzA3o)（可能是对特征值和特征向量的最佳可视化表示）'
- en: '[https://hadrienj.github.io/posts/Deep-Learning-Book-Series-Introduction/](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-Introduction/) (the
    code to visualize vectors)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://hadrienj.github.io/posts/Deep-Learning-Book-Series-Introduction/](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-Introduction/)（可视化向量的代码）'
- en: '[https://en.wikipedia.org/wiki/Singular_value_decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Singular_value_decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)'
- en: '[Everything you did and didn''t know about PCA · Its Neuronal](http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[你对PCA了解的与不了解的一切 · Its Neuronal](http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/)'
- en: '[Illustration of principal component analysis (PCA)](https://www.joyofdata.de/blog/illustration-of-principal-component-analysis-pca/)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[主成分分析（PCA）的示意图](https://www.joyofdata.de/blog/illustration-of-principal-component-analysis-pca/)'
- en: '[https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues) (superb
    thread)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)（精彩讨论）'
- en: '[Practical Guide to Principal Component Analysis (PCA) in R & Python](https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[R 和 Python 中的主成分分析（PCA）实用指南](https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/)'
- en: '[In Depth: Principal Component Analysis](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[深入解析：主成分分析](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html)'
- en: '[https://medium.com/@zhang_yang/python-code-examples-of-pca-v-s-svd-4e9861db0a71](https://medium.com/@zhang_yang/python-code-examples-of-pca-v-s-svd-4e9861db0a71)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://medium.com/@zhang_yang/python-code-examples-of-pca-v-s-svd-4e9861db0a71](https://medium.com/@zhang_yang/python-code-examples-of-pca-v-s-svd-4e9861db0a71)'
- en: '**Bio: [Tanveer Sayyed](https://www.linkedin.com/in/tanveer-sayyed-739105185/)**
    is a Data science enthusiast. Ardent reader. Art lover. Dissent lover... rest
    of the time swinging on the rings of Saturn.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介：[Tanveer Sayyed](https://www.linkedin.com/in/tanveer-sayyed-739105185/)**
    是数据科学爱好者。热情的读者。艺术爱好者。异议爱好者……其余时间则在土星的环上摇摆。'
- en: '[Original](https://medium.com/@tanveer2407/b06ba3470ca2). Reposted with permission.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/@tanveer2407/b06ba3470ca2)。经许可转载。'
- en: '**Related:**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Mathematics for Machine Learning: The Free eBook](/2020/04/mathematics-machine-learning-book.html)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习数学：免费电子书](/2020/04/mathematics-machine-learning-book.html)'
- en: '[Essential Math for Data Science: Integrals And Area Under The Curve](/2020/11/essential-math-data-science-integrals-area-under-curve.html)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学的基础数学：积分与曲线下的面积](/2020/11/essential-math-data-science-integrals-area-under-curve.html)'
- en: '[Free Mathematics Courses for Data Science & Machine Learning](/2020/02/free-mathematics-courses-data-science-machine-learning.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学和机器学习的免费数学课程](/2020/02/free-mathematics-courses-data-science-machine-learning.html)'
- en: '* * *'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT 部门'
- en: '* * *'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Essential Math for Data Science: Visual Introduction to Singular…](https://www.kdnuggets.com/2022/06/essential-math-data-science-visual-introduction-singular-value-decomposition.html)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学的基础数学：奇异值分解的视觉介绍](https://www.kdnuggets.com/2022/06/essential-math-data-science-visual-introduction-singular-value-decomposition.html)'
- en: '[Sparse Matrix Representation in Python](https://www.kdnuggets.com/2020/05/sparse-matrix-representation-python.html)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 中的稀疏矩阵表示](https://www.kdnuggets.com/2020/05/sparse-matrix-representation-python.html)'
- en: '[Idiot''s Guide to Precision, Recall, and Confusion Matrix](https://www.kdnuggets.com/2020/01/guide-precision-recall-confusion-matrix.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[傻瓜指南：精确度、召回率和混淆矩阵](https://www.kdnuggets.com/2020/01/guide-precision-recall-confusion-matrix.html)'
- en: '[Visualizing Your Confusion Matrix in Scikit-learn](https://www.kdnuggets.com/2022/09/visualizing-confusion-matrix-scikitlearn.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Scikit-learn 中可视化你的混淆矩阵](https://www.kdnuggets.com/2022/09/visualizing-confusion-matrix-scikitlearn.html)'
- en: '[Convert Text Documents to a TF-IDF Matrix with tfidfvectorizer](https://www.kdnuggets.com/2022/09/convert-text-documents-tfidf-matrix-tfidfvectorizer.html)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 tfidfvectorizer 将文本文档转换为 TF-IDF 矩阵](https://www.kdnuggets.com/2022/09/convert-text-documents-tfidf-matrix-tfidfvectorizer.html)'
- en: '[Confusion Matrix, Precision, and Recall Explained](https://www.kdnuggets.com/2022/11/confusion-matrix-precision-recall-explained.html)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[混淆矩阵、精确度和召回率解释](https://www.kdnuggets.com/2022/11/confusion-matrix-precision-recall-explained.html)'
