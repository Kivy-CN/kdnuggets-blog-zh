- en: How to Create a Vocabulary for NLP Tasks in Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在 Python 中为 NLP 任务创建词汇表
- en: 原文：[https://www.kdnuggets.com/2019/11/create-vocabulary-nlp-tasks-python.html](https://www.kdnuggets.com/2019/11/create-vocabulary-nlp-tasks-python.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/11/create-vocabulary-nlp-tasks-python.html](https://www.kdnuggets.com/2019/11/create-vocabulary-nlp-tasks-python.html)
- en: '[comments](#comments)![Figure](../Images/38cd18ae0fa365efd5e4fa3ce7d05c1c.png)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)![图示](../Images/38cd18ae0fa365efd5e4fa3ce7d05c1c.png)'
- en: 'When performing a [natural language processing task](/2017/11/framework-approaching-textual-data-tasks.html),
    our text data transformation proceeds more or less in this manner:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行一个 [自然语言处理任务](/2017/11/framework-approaching-textual-data-tasks.html) 时，我们的文本数据转换大致按照以下方式进行：
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 加速你的网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT 工作'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**raw text corpus → processed text → tokenized text → corpus vocabulary → text
    representation**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**原始文本语料库 → 处理后的文本 → 分词文本 → 语料库词汇表 → 文本表示**'
- en: Keep in mind that this all happens prior to the actual NLP task even beginning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这一切都发生在实际的 NLP 任务开始之前。
- en: The corpus vocabulary is a holding area for processed text before it is transformed
    into some [representation](/2018/11/data-representation-natural-language-processing.html)
    for the [impending task](/2018/10/main-approaches-natural-language-processing-tasks.html),
    be it classification, or language modeling, or something else.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库词汇表是处理文本的储存区域，在其被转化为某种 [表示形式](/2018/11/data-representation-natural-language-processing.html)
    用于 [即将到来的任务](/2018/10/main-approaches-natural-language-processing-tasks.html)
    之前，无论是分类，语言建模，还是其他任务。
- en: 'The vocabulary serves a few primary purposes:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表有几个主要目的：
- en: help in the preprocessing of the corpus text
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帮助预处理语料库文本
- en: serve as storage location in memory for processed text corpus
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为处理文本语料库在内存中的存储位置
- en: collect and store metadata about the corpus
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集并存储语料库的元数据
- en: allow for pre-task munging, exploration, and experimentation
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许进行任务前的数据清理、探索和实验
- en: The vocabulary serves a few related purposes and can be thought of in a few
    different ways, but the main takeaway is that, once a corpus has made its way
    to the vocabulary, the text has been processed and any relevant metadata should
    be collected and stored.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表有几个相关的目的，可以从几个不同的角度来理解，但主要的要点是，一旦语料库进入词汇表，文本就已经被处理，所有相关的元数据应当被收集和存储。
- en: This post will take a step by step look at a Python implementation of a useful
    vocabulary class, showing what is happening in the code, why we are doing what
    we are doing, and some sample usage. We will start with some code from [this PyTorch
    tutorial](https://pytorch.org/tutorials/beginner/chatbot_tutorial.html), and will
    make a few modifications as we go. Though this won't be terribly programming heavy,
    if you are wholly unfamiliar with Python object oriented programming, [I recommend
    you first look here](https://realpython.com/python3-object-oriented-programming/).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将逐步介绍一个有用的词汇表类的 Python 实现，展示代码中发生的事情，我们为什么这样做，以及一些示例用法。我们将从 [这个 PyTorch 教程](https://pytorch.org/tutorials/beginner/chatbot_tutorial.html)
    开始，并在过程中进行一些修改。虽然这不会太依赖编程，但如果你对 Python 面向对象编程完全陌生， [我建议你先查看这里](https://realpython.com/python3-object-oriented-programming/)。
- en: The first thing to do is to create values for our start of sentence, end of
    sentence, and sentence padding special tokens. When we tokenize text (split text
    into its atomic constituent pieces), we need special tokens to delineate both
    the beginning and end of a sentence, as well as to pad sentence (or some other
    text chunk) storage structures when sentences are shorter then the maximum allowable
    space. More on this later.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的是为句子开始、句子结束和句子填充的特殊标记创建值。当我们对文本进行分词（将文本拆分成其原子组成部分）时，我们需要特殊标记来划分句子的开始和结束，并在句子短于最大允许空间时填充句子（或其他文本块）的存储结构。稍后会详细介绍。
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: What the above states is that our stat of sentence token (literally 'SOS', below)
    will take index spot '1' in our token lookup table once we make it. Likewise,
    end of sentence ('EOS') will take index spot '2', while the sentence padding token
    ('PAD') will take index spot '0'.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 上述内容表示，我们的句子开始标记（字面上是‘SOS’）在我们创建标记查找表时将占据索引位置‘1’。同样，句子结束标记（‘EOS’）将占据索引位置‘2’，而句子填充标记（‘PAD’）将占据索引位置‘0’。
- en: 'The next thing we will do is create a constructor for our Vocabulary class:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将为我们的词汇表类创建一个构造函数：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first line is our `__init__()` declaration, which requires 'self' as its
    first parameter (again, [see this link](https://realpython.com/python3-object-oriented-programming/)),
    and takes a Vocabulary 'name' as its second.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行是我们的`__init__()`声明，它要求`self`作为第一个参数（如[查看此链接](https://realpython.com/python3-object-oriented-programming/)），并将一个词汇表的'name'作为第二个参数。
- en: Line by line, here's what the object variable initializations are doing
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 逐行来看，这些对象变量初始化的作用
- en: '`self.name = name` → this is instantiated to the name passed to the constructor,
    as something by which to refer to our Vocabulary object'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.name = name` → 这将被实例化为传递给构造函数的名称，用于引用我们的词汇表对象'
- en: '`self.word2index = {}` → a dictionary to hold word token to corresponding word
    index values, eventually in the form of `''the'': 7`, for example'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.word2index = {}` → 一个字典，用于保存词标记到相应词索引值，最终形式类似于`''the'': 7`'
- en: '`self.word2count = {}` → a dictionary to hold individual word counts (tokens,
    actually) in the corpus'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.word2count = {}` → 一个字典，用于保存语料库中每个词的计数（实际上是标记）'
- en: '`self.index2word = {PAD_token: "PAD", SOS_token: "SOS", EOS_token: "EOS"}`
    → a dictionary holding the reverse of `word2index` (word index keys to word token
    values); special tokens added right away'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.index2word = {PAD_token: "PAD", SOS_token: "SOS", EOS_token: "EOS"}`
    → 一个字典，保存了`word2index`的反向映射（词索引键到词标记值）；特殊标记立即添加'
- en: '`self.num_words = 3` → this will be a count of the number of words (tokens,
    actually) in the corpus'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.num_words = 3` → 这将是语料库中词语的数量（实际上是标记）'
- en: '`self.num_sentences = 0` → this will be a count of the number of sentences
    (text chunks of any indiscriminate length, actually) in the corpus'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.num_sentences = 0` → 这将是语料库中句子的数量（实际上是任何长度的文本块）'
- en: '`self.longest_sentence = 0` → this will be the length of the longest corpus
    sentence by number of tokens'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.longest_sentence = 0` → 这将是最长语料库句子的长度，以标记数量为单位'
- en: From the above, you should be able to see what metadata about our corpus we
    are concerned with at this point. Try and think of some additional corpus-related
    data you might want to keep track of, which we are not.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述内容，你应该能够看到我们目前关注的语料库元数据。尝试考虑一些你可能想要跟踪的额外语料库相关数据，这些数据我们尚未涉及。
- en: Since we have defined that metadata which we are interested in collecting and
    storing, we can move on to performing the work to do so. A basic unit of work
    we will need to do to fill up our vocabulary is to add words to it.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经定义了我们感兴趣的元数据，我们可以继续进行相关工作。填充词汇表的基本工作单元是向其中添加词语。
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, there are 2 scenarios we can encounter when trying to add a
    word token to our vocabulary; either it does not already exists in the vocabulary
    (`if word not in self.word2index:`) or it does (`else:`). If the word does not
    exist in our vocabulary, we want to add it to our `word2index` dict, instantiate
    our count of that word to 1, add the index of the word (the next available number
    in the counter) to the `index2word` dict, and increment our overall word count
    by 1\. On the other hand, if the word already exists in the vocabulary, simply
    increment the counter for that word by 1.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在尝试将一个单词标记添加到词汇表时，我们可能会遇到2种情况：要么该单词在词汇表中不存在（`if word not in self.word2index:`），要么存在（`else:`）。如果单词在词汇表中不存在，我们要将其添加到`word2index`字典中，将该单词的计数初始化为1，将单词的索引（计数器中的下一个可用数字）添加到`index2word`字典中，并将总单词计数增加1。另一方面，如果单词已经存在于词汇表中，则只需将该单词的计数器增加1。
- en: How are we going to add words to the vocabulary? We will do so by feeding sentences
    in and tokenizing them as well go, processing the resulting tokens one by one.
    Note, again, that these need not be sentences, and naming these 2 functions `add_token`
    and `add_chunk` may be more appropriate than `add_word` and `add_sentence`, respectively.
    We will leave the renaming for another day.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将如何将单词添加到词汇表中？我们将通过输入句子并进行分词处理来完成这项工作，逐一处理生成的标记。请再次注意，这些不一定是句子，命名这两个函数为`add_token`和`add_chunk`可能比`add_word`和`add_sentence`更为合适。我们将把重命名的工作留到另一天。
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This function takes a chunk of text, a single string, and splits it on whitespace
    for tokenization purposes. This is not robust tokenization, and is not good practice,
    but will suffice for our purposes at the moment. We will revisit this in a follow-up
    post and build a better approach to tokenization into our vocabulary class. In
    the meantime, you can read more on text data preprocessing [here](/2017/12/general-approach-preprocessing-text-data.html)
    and [here](/2018/03/text-data-preprocessing-walkthrough-python.html).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数接收一段文本，一个单一的字符串，并通过空白字符进行分割，以实现分词。这不是一种稳健的分词方法，也不是最佳实践，但目前足以满足我们的需求。我们将在后续文章中重新审视这个问题，并在我们的词汇类中构建更好的分词方法。在此期间，你可以在[这里](/2017/12/general-approach-preprocessing-text-data.html)和[这里](/2018/03/text-data-preprocessing-walkthrough-python.html)了解更多关于文本数据预处理的内容。
- en: After splitting our sentence on whitespace, we then increment our sentence length
    counter by one for each word we pass to the `add_word` function for processing
    and addition to our vocabulary (see above). We then check to see if this sentence
    is longer than other sentences we have processed; if it is, we make note. We also
    increment our count of corpus sentences we have added to the vocabulary thus far.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过空白字符分割句子后，我们将句子长度计数器为每个传递给`add_word`函数处理并添加到词汇表中的单词增加1（见上文）。然后我们检查该句子是否比我们处理过的其他句子更长；如果是，我们做个记录。我们还增加了到目前为止我们已经添加到词汇表中的语料库句子的计数。
- en: 'We will then add a pair of helper functions to help us more easily access 2
    of our most important lookup tables:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将添加一对辅助函数，以便更容易访问我们两个最重要的查找表：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The first of these functions performs the index to word lookup in the appropriate
    dictionary for a given index; the other performs the reverse lookup for a given
    word. This is essential functionality, as once we get our processed text into
    the vocabulary object, we will want to get it back out at some point, as well
    as perform lookups and reference metadata. These 2 functions will be handy for
    much of this.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个函数中的第一个在适当的字典中根据给定的索引执行词到索引的查找；另一个函数则执行反向查找。这是基本功能，因为一旦我们将处理过的文本放入词汇表对象中，我们将希望在某个时候将其取出，并执行查找和引用元数据。这两个函数在这方面非常有用。
- en: Putting this all together, we get the following.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 把这些内容综合在一起，我们得到以下结果。
- en: 'Let''s see how this works. First, let''s create an empty vocabulary object:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看效果如何。首先，让我们创建一个空的词汇表对象：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: <__main__.Vocabulary object at 0x7f80a071c470>
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: <__main__.Vocabulary object at 0x7f80a071c470>
- en: 'Then we create a simple corpus:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建一个简单的语料库：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s loop through the sentences in our corpus and add the words in each to
    our vocabulary. Remember that `add_sentence` makes calls to `add_word`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们遍历语料库中的句子，并将每个句子中的单词添加到我们的词汇表中。请记住，`add_sentence`会调用`add_word`：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now let''s test what we''ve done:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们测试一下我们所做的工作：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This is the output, which seems to work well.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果，似乎效果不错。
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Since our corpus is so small, let's print out the entire vocabulary of tokens.
    Note that since we have not yet implemented any sort of useful tokenization beyond
    splitting on white space, we have some tokens with capitlized first letters, and
    others with trailing punctuation. Again, we will deal with this more appropriately
    in a follow-up.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的语料库非常小，让我们打印出整个词汇表。注意，由于我们尚未实施任何有用的分词，只是基于空格分割，我们有一些标记以大写字母开头，还有一些带有尾随标点符号。我们将在后续中更恰当地处理这些问题。
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Let's create and print out lists of corresponding tokens and indexes of a particular
    sentence. Note this time that we have not yet trimmed the vocabulary, nor have
    we added padding or used the SOS or EOS tokens. We add this to the list of items
    to take care of next time.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建并打印出特定句子的对应标记和索引。注意这次我们尚未修剪词汇表，也没有添加填充或使用 SOS 或 EOS 标记。我们将这些添加到下次需要处理的项目清单中。
- en: '[PRE13]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: And there you go. It seems that, even with our numerous noted shortcomings,
    we have a vocabulary that might end up being useful, given that it exhibits much
    of the core necessary functionality which would make it eventually useful.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。即便我们有诸多显著的不足，但我们似乎仍拥有一套可能最终会有用的词汇表，因为它展示了许多核心的必要功能，这些功能将使其最终变得有用。
- en: 'A review of the items we must take care of next time include:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须在下次处理的项目包括：
- en: perform normalization of our text data (force all to lowercase, deal with punctuation,
    etc.)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对我们的文本数据进行规范化（强制全部小写，处理标点符号等）
- en: properly tokenize chunks of text
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确地对文本块进行分词
- en: make use of SOS, EOS, and PAD tokens
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SOS、EOS 和 PAD 标记
- en: trim our vocabulary (minimum number of token occurrences before stored permanently
    in our vocabulary)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修剪我们的词汇表（在词汇表中永久存储之前的最小标记出现次数）
- en: Next time we will implement this functionality, and test our Python vocabulary
    implementation on a more robust corpus. We will then move data from our vocabulary
    object into a useful data representation for NLP tasks. Finally, we will get to
    performing an NLP task on the data we have gone to the trouble of so aptly preparing.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 下次我们将实现此功能，并在更强大的语料库上测试我们的 Python 词汇表实现。然后，我们将把数据从词汇对象中转移到适用于 NLP 任务的有用数据表示中。最后，我们将对我们精心准备的数据执行
    NLP 任务。
- en: '**Related**:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关**：'
- en: '[Data Representation for Natural Language Processing Tasks](/2018/11/data-representation-natural-language-processing.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理任务的数据表示](/2018/11/data-representation-natural-language-processing.html)'
- en: '[The Main Approaches to Natural Language Processing Tasks](/2018/10/main-approaches-natural-language-processing-tasks.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理任务的主要方法](/2018/10/main-approaches-natural-language-processing-tasks.html)'
- en: '[A Framework for Approaching Textual Data Science Tasks](/2017/11/framework-approaching-textual-data-tasks.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[处理文本数据科学任务的框架](/2017/11/framework-approaching-textual-data-tasks.html)'
- en: More On This Topic
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Cleaning and Preprocessing Text Data in Pandas for NLP Tasks](https://www.kdnuggets.com/cleaning-and-preprocessing-text-data-in-pandas-for-nlp-tasks)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Pandas 中清理和预处理 NLP 任务的文本数据](https://www.kdnuggets.com/cleaning-and-preprocessing-text-data-in-pandas-for-nlp-tasks)'
- en: '[5 Tasks To Automate With Python](https://www.kdnuggets.com/2021/06/5-tasks-automate-python.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 自动化的 5 个任务](https://www.kdnuggets.com/2021/06/5-tasks-automate-python.html)'
- en: '[Data Representation for Natural Language Processing Tasks](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理任务的数据表示](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
- en: '[HuggingGPT: The Secret Weapon to Solve Complex AI Tasks](https://www.kdnuggets.com/2023/05/hugginggpt-secret-weapon-solve-complex-ai-tasks.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HuggingGPT：解决复杂 AI 任务的秘密武器](https://www.kdnuggets.com/2023/05/hugginggpt-secret-weapon-solve-complex-ai-tasks.html)'
- en: '[5 Coding Tasks ChatGPT Can''t Do](https://www.kdnuggets.com/5-coding-tasks-chatgpt-cant-do)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatGPT 无法完成的 5 个编码任务](https://www.kdnuggets.com/5-coding-tasks-chatgpt-cant-do)'
- en: '[Create a Dashboard Using Python and Dash](https://www.kdnuggets.com/2023/08/create-dashboard-python-dash.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 和 Dash 创建仪表盘](https://www.kdnuggets.com/2023/08/create-dashboard-python-dash.html)'
