# Docker 上的 Apache Spark 集群

> 原文：[https://www.kdnuggets.com/2020/07/apache-spark-cluster-docker.html](https://www.kdnuggets.com/2020/07/apache-spark-cluster-docker.html)

[评论](#comments)

**作者 [André Perez](https://www.linkedin.com/in/andremarcosperez/)，Experian 数据工程师**

![图示](../Images/6504ccaf02651d78847df87f699dffe1.png)

Sparks by [Jez Timms](https://unsplash.com/@jeztimms) on [Unsplash](https://unsplash.com/photos/r4lM2v9M84Q)

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织 IT

* * *

[Apache Spark](https://spark.apache.org/) 可以说是最受欢迎的大数据处理引擎。它在 [GitHub](https://github.com/apache/spark) 上拥有超过 25k 的星标，是学习使用 Python、Scala 和 R 进行分布式系统并行计算的绝佳起点。

要开始，你可以通过使用许多优秀的 Docker 发行版之一在你的机器上运行 Apache Spark。 [Jupyter](https://github.com/jupyter/docker-stacks) 提供了一个很棒的 *dockerized* Apache Spark 和 JupyterLab 界面，但由于在单个容器上运行，它缺少框架的分布式核心。一些 GitHub [项目](https://github.com/big-data-europe/docker-spark) 提供了分布式集群体验，但缺少 JupyterLab 界面，从而削弱了 IDE 提供的可用性。

我相信，一个全面的环境用于学习和实践 Apache Spark 代码必须保持其分布式特性，同时提供出色的用户体验。

本文正是关于这一信念的。

在接下来的章节中，我将向你展示如何构建自己的集群。到最后，你将拥有一个完全功能的 Apache Spark 集群，它是通过 Docker 构建的，包含一个 Spark 主节点、两个 Spark 工作节点和一个 JupyterLab 界面。它还包括 Apache Spark Python API（PySpark）和一个模拟的 Hadoop 分布式文件系统（HDFS）。

**TL;DR**

本文展示了如何使用 Docker 作为基础设施层在 [独立模式](http://spark.apache.org/docs/latest/spark-standalone.html) 下构建一个 Apache Spark 集群。它包括以下内容：

+   Python 3.7 配合 PySpark 3.0.0 和 Java 8；

+   Apache Spark 3.0.0 配备一个主节点和两个工作节点；

+   JupyterLab IDE 2.1.5；

+   模拟的 HDFS 2.7。

为了搭建集群，我们需要创建、构建和组合 JupyterLab 和 Spark 节点的 Docker 镜像。你可以通过使用我在 [GitHub](https://github.com/andre-marcos-perez/spark-cluster-on-docker) 上托管的**开箱即用的发行版**来跳过教程。

**要求**

+   **Docker** 1.13.0+；* **Docker Compose** 3.0+。

    **目录**

    1.  集群概述；

    1.  创建镜像；

    1.  构建镜像；

    1.  组合集群；

    1.  创建一个 PySpark 应用。

    ### 1\. 集群概述

    集群由四个主要组件组成：JupyterLab IDE、Spark 主节点和两个 Spark 工作节点。用户连接到主节点，并通过 Jupyter notebooks 提供的优雅 GUI 提交 Spark 命令。主节点处理输入并将计算工作负载分配给工作节点，将结果发送回 IDE。组件通过本地主机网络连接，并通过模拟 HDFS 的共享挂载卷相互共享数据。

    ![Figure](../Images/37dc76fd9b4537e013b05fa07e2602ab.png)

    Apache Spark 集群概述

    如前所述，我们需要创建、构建和组合 JupyterLab 和 Spark 节点的 Docker 镜像来搭建集群。我们将使用以下 Docker 镜像层级：

    ![Figure](../Images/fb19f5b9139a812de78f5788550224b4.png)

    Docker 镜像层级

    集群基础镜像将下载并安装常用软件工具（Java、Python 等），并为 HDFS 创建共享目录。在 Spark 基础镜像上，Apache Spark 应用将被下载并为主节点和工作节点配置。Spark 主镜像将配置框架以运行主节点。类似地，Spark 工作节点将配置 Apache Spark 应用以运行工作节点。最后，JupyterLab 镜像将使用集群基础镜像来安装和配置 IDE 和 PySpark，Apache Spark 的 Python API。

    ### 2\. 创建镜像

    ### 2.1\. 集群基础镜像

    对于基础镜像，我们将使用 Linux 发行版来安装 Java 8（或 11），[Apache Spark 唯一要求](https://spark.apache.org/docs/latest/#downloading)。我们还需要安装 Python 3 以支持 PySpark，并创建共享卷以模拟 HDFS。

    集群基础镜像的 Dockerfile

    首先，选择 Linux 操作系统。Apache Spark 官方 GitHub 仓库有一个用于 Kubernetes 部署的 [Dockerfile](https://github.com/apache/spark/blob/master/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile)，它使用了一个内置 Java 8 运行时环境（JRE）的 Debian 小镜像。通过选择 [相同的基础镜像](https://hub.docker.com/_/openjdk?tab=tags&page=1&name=8-jre-slim)，我们解决了操作系统选择和 Java 安装的问题。然后，我们从 Debian 官方包仓库获取 [最新的 Python 版本](https://packages.debian.org/stable/python/python3)（目前为 3.7），并创建共享卷。

    ### 2.2\. Spark 基础镜像

    对于 Spark 基础镜像，我们将获取并设置 Apache Spark 以 [standalone mode](http://spark.apache.org/docs/latest/spark-standalone.html) 运行，这是一种最简单的部署配置。在这种模式下，我们将使用其资源管理器设置容器，以主节点或工作节点的身份运行。相比之下，资源管理器如 [Apache YARN](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html) 会根据用户工作负载动态分配容器作为主节点或工作节点。此外，我们将获取一个支持 Apache Hadoop 的 Apache Spark 版本，以便集群能够使用基础集群镜像中创建的共享卷来模拟 HDFS。

    Spark 基础镜像的 Dockerfile

    首先，我们从官方 [Apache repository](https://archive.apache.org/dist/spark/) 下载最新版本的 Apache Spark（目前是 3.0.0）并支持 Apache Hadoop。然后，我们对下载的包进行一些操作（解压、移动等），准备好进入设置阶段。最后，我们配置四个 Spark 变量，这些变量对主节点和工作节点都是通用的：

    1.  **SPARK_HOME** 是框架用于设置任务的已安装 Apache Spark 位置；

    1.  **SPARK_MASTER_HOST** 是工作节点用于连接的主节点 **主机名**；

    1.  **SPARK_MASTER_PORT** 是工作节点用于连接的主节点 **端口**；

    1.  **PYSPARK_PYTHON** 是 Apache Spark 用于支持其 Python API 的已安装 Python 位置。

    ### 2.3\. Spark 主节点镜像

    对于 Spark 主节点镜像，我们将设置 Apache Spark 应用程序以主节点身份运行。我们将配置网络端口以允许与工作节点的网络连接，并暴露主节点的 Web UI，即一个用于监控主节点活动的网页。最后，我们将设置容器启动命令以启动主节点实例。

    Spark 主节点镜像的 Dockerfile

    我们首先暴露配置在 **SPARK_MASTER_PORT** 环境变量中的端口，以允许工作节点连接到主节点。接着，我们暴露 **SPARK_MASTER_WEBUI_PORT** 端口，以便我们访问主节点的 Web UI 页面。最后，我们设置容器启动命令，以 [master class](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/master/Master.scala) 作为参数运行 Spark 内置的部署脚本。

    ### 2.4\. Spark 工作节点镜像

    对于 Spark 工作节点镜像，我们将设置 Apache Spark 应用程序以工作节点身份运行。与主节点类似，我们将配置网络端口以暴露工作节点的 Web UI，即一个用于监控工作节点活动的网页，并设置容器启动命令以启动工作节点实例。

    Spark 工作节点镜像的 Dockerfile

    首先，我们暴露 **SPARK_WORKER_WEBUI_PORT** 端口，以便访问工作节点的 Web UI 页面，就像我们对主节点所做的那样。然后，我们设置容器启动命令，以运行 Spark 内置的部署脚本，使用 [worker class](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala) 和主节点网络地址作为其参数。这将使工作节点在启动过程中连接到主节点。

    ### 2.5\. JupyterLab 镜像

    对于 JupyterLab 镜像，我们稍微回到基础镜像，从集群基础镜像开始。我们将安装和配置 IDE，并安装与 Spark 节点上不同的 Apache Spark 发行版。

    JupyterLab 镜像的 Dockerfile

    我们首先安装 pip，Python 的包管理器，以及 Python 开发工具，以便在镜像构建和容器运行时安装 Python 包。接着，从 Python 包索引（PyPI）中获取 [JupyterLab](https://pypi.org/project/jupyterlab/) 和 [PySpark](https://pypi.org/project/pyspark/)。最后，我们暴露默认端口以便访问 JupyterLab 网络界面，并将容器启动命令设置为运行 IDE 应用程序。

    ### 3\. 构建镜像

    Docker 镜像已准备好，让我们构建它们。请注意，由于我们在 Dockerfile 中使用了 Docker *arg* 关键字来指定软件版本，因此我们可以轻松更改集群的默认 Apache Spark 和 JupyterLab 版本。

    构建集群镜像

    ### 4\. 组建集群

    Docker compose 文件包含我们集群的配方。在这里，我们将创建 JupyterLab 和 Spark 节点容器，暴露其端口以供本地网络使用，并将它们连接到模拟 HDFS。

    集群的 Docker compose 文件

    我们首先创建用于模拟 HDFS 的 Docker 卷。接下来，为每个集群组件创建一个容器。*jupyterlab* 容器暴露 IDE 端口，并将其共享工作目录绑定到 HDFS 卷。同样，*spark-master* 容器暴露其 Web UI 端口和 *master-worker* 连接端口，并将其绑定到 HDFS 卷。

    我们通过创建两个名为 *spark-worker-1* 和 *spark-worker-2* 的 Spark 工作容器来完成。每个容器暴露其 Web UI 端口（分别映射为 8081 和 8082）并绑定到 HDFS 卷。这些容器具有一个环境步骤，指定其硬件分配：

    +   **SPARK_WORKER_CORE** 是核心数；

    +   **SPARK_WORKER_MEMORY** 是 RAM 的数量。

    默认情况下，我们为每个容器选择一个核心和 512 MB 的 RAM。可以自由调整硬件分配，但请确保尊重您的机器限制，以避免内存问题。此外，提供足够的资源以供 Docker 应用程序处理所选值。

    要组建集群，运行 Docker compose 文件：

    组建集群

    完成后，检查组件的 Web UI：

    +   **JupyterLab** 在 [localhost:8888](http://localhost:8888/)；

    +   **Spark master** 在 [localhost:8080](http://localhost:8080/)；

    +   **Spark worker I** 在 [localhost:8081](http://localhost:8081/)；

    +   **Spark worker II** 在 [localhost:8082](http://localhost:8082/)；

    ### 5. 创建 PySpark 应用程序

    集群启动并运行后，让我们创建第一个 PySpark 应用程序。

    创建 PySpark 应用程序

    打开 JupyterLab IDE 并创建一个 Python Jupyter notebook。通过使用以下参数的 Spark session 对象连接到 Spark master 节点，创建一个 PySpark 应用程序：

    +   **appName** 是我们应用程序的名称；

    +   **master** 是 Spark master 的连接 URL，Spark 工作节点用来连接到 Spark master 节点的 URL；

    +   **config** 是一个通用的 [Spark 独立模式配置](https://spark.apache.org/docs/latest/configuration.html)。在这里，我们将执行器内存（即 Spark 工作节点 JVM 进程）与配置的工作节点内存进行匹配。

    运行单元格后，你将能够在 Spark master 的网页 UI 下的“运行中的应用程序”中看到应用程序。最后，我们从 PyPI 安装 Python [wget](https://pypi.org/project/wget/) 包，并将鸢尾花数据集从 [UCI 代码库](https://archive.ics.uci.edu/ml/datasets/iris) 下载到模拟 HDFS 中。然后我们使用 PySpark 读取并打印数据。

    就这些了。我希望我能帮助你多了解一些 Apache Spark 的内部工作原理以及分布式应用程序的工作方式。祝学习愉快！

    **简介: [André Perez](https://www.linkedin.com/in/andremarcosperez/)** ([**@dekoperez**](https://twitter.com/dekoperez)) 是 Experian 的数据工程师，同时也是圣保罗大学的硕士数据科学学生。

    [原文](https://towardsdatascience.com/apache-spark-cluster-on-docker-ft-a-juyterlab-interface-418383c95445)。经授权转载。

    **相关：**

    +   [使用 Apache Spark 和 PySpark 的好处及示例](/2020/04/benefits-apache-spark-pyspark.html)

    +   [五个有趣的数据工程项目](/2020/03/data-engineering-projects.html)

    +   [数据工程需要掌握的技能](/2020/06/skills-build-data-engineering.html)

    ### 相关话题

    +   [使用聚类分析对数据进行分段](https://www.kdnuggets.com/using-cluster-analysis-to-segment-your-data)

    +   [Apache Druid 的演变](https://www.kdnuggets.com/2022/07/evolution-apache-druid.html)

    +   [如何使用 Apache Kafka 构建可扩展的数据架构](https://www.kdnuggets.com/2023/04/build-scalable-data-architecture-apache-kafka.html)

    +   [通过 Apache Gobblin 扩展数据管理](https://www.kdnuggets.com/2023/01/scaling-data-management-apache-gobblin.html)

    +   [Kubernetes 中高可用性 SQL Server Docker 容器](https://www.kdnuggets.com/2022/04/high-availability-sql-server-docker-containers-kubernetes.html)

    +   [每个数据科学家都应该知道的 12 个 Docker 命令](https://www.kdnuggets.com/2023/01/12-docker-commands-every-data-scientist-know.html)
