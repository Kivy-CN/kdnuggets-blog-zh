- en: Running Mixtral 8x7b On Google Colab For Free
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/running-mixtral-8x7b-on-google-colab-for-free](https://www.kdnuggets.com/running-mixtral-8x7b-on-google-colab-for-free)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Running Mixtral 8x7b On Google Colab For Free](../Images/6b600750560720253d21d9da4c0e64ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we will explore the new state-of-the-art open-source model called
    Mixtral 8x7b. We will also learn how to access it using the LLaMA C++ library
    and how to run large language models on reduced computing and memory.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: What is Mixtral 8x7b?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Mixtral 8x7b](https://mistral.ai/news/mixtral-of-experts/) is a high-quality
    sparse mixture of experts (SMoE) model with open weights, created by Mistral AI.
    It is licensed under Apache 2.0 and outperforms Llama 2 70B on most benchmarks
    while having 6x faster inference. Mixtral matches or beats GPT3.5 on most standard
    benchmarks and is the best open-weight model regarding cost/performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Running Mixtral 8x7b On Google Colab For Free](../Images/a3cdcbacf215cf2d37c88bf6a9e58dad.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Mixtral of experts](https://mistral.ai/news/mixtral-of-experts/)
  prefs: []
  type: TYPE_NORMAL
- en: Mixtral 8x7B uses a decoder-only sparse mixture-of-experts network. This involves
    a feedforward block selecting from 8 groups of parameters, with a router network
    choosing two of these groups for each token, combining their outputs additively.
    This method enhances the model's parameter count while managing cost and latency,
    making it as efficient as a 12.9B model, despite having 46.7B total parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Mixtral 8x7B model excels in handling a wide context of 32k tokens and supports
    multiple languages, including English, French, Italian, German, and Spanish. It
    demonstrates strong performance in code generation and can be fine-tuned into
    an instruction-following model, achieving high scores on benchmarks like MT-Bench.
  prefs: []
  type: TYPE_NORMAL
- en: Running Mixtral 8x7b using LLaMA C++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[LLaMA.cpp](https://github.com/ggerganov/llama.cpp) is a C/C++ library that
    provides a high-performance interface for large language models (LLMs) based on
    Facebook''s LLM architecture. It is a lightweight and efficient library that can
    be used for a variety of tasks, including text generation, translation, and question
    answering. LLaMA.cpp supports a wide range of LLMs, including LLaMA, LLaMA 2,
    Falcon, Alpaca, Mistral 7B, Mixtral 8x7B, and GPT4ALL. It is compatible with all
    operating systems and can function on both CPUs and GPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will be running the llama.cpp web application on Colab.
    By writing a few lines of code, you will be able to experience the new state-of-the-art
    model performance on your PC or on Google Colab.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we will download the llama.cpp GitHub repository using the command line
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: After that, we will change directory into the repository and install the llama.cpp
    using the `make` command. We are installing the llama.cpp for the NVidia GPU with
    CUDA installed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Download the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can download the model from the Hugging Face Hub by selecting the appropriate
    version of the `.gguf` model file. More information on various versions can be
    found in [TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF](https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF#provided-files).
  prefs: []
  type: TYPE_NORMAL
- en: '![Running Mixtral 8x7b On Google Colab For Free](../Images/f0245bfd4ff28d35bbad7830b74de525.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF](https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/tree/main)
  prefs: []
  type: TYPE_NORMAL
- en: You can use the command `wget` to download the model in the current directory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: External Address for LLaMA Server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we run the LLaMA server it will give us a localhost IP which is useless
    for us on Colab. We need the connection to the localhost proxy by using the Colab
    kernel proxy port.
  prefs: []
  type: TYPE_NORMAL
- en: After running the code below, you will get the global hyperlink. We will use
    this link to access our webapp later.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Running the Server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To run the LLaMA C++ server, you need to provide the server command with the
    location of the model file and the correct port number. It's important to make
    sure that the port number matches the one we initiated in the previous step for
    the proxy port.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Running Mixtral 8x7b On Google Colab For Free](../Images/c08977e32df06174dee0deab33f7fec8.png)'
  prefs: []
  type: TYPE_IMG
- en: The chat webapp can be accessed by clicking on the proxy port hyperlink in the
    previous step since the server is not running locally.
  prefs: []
  type: TYPE_NORMAL
- en: LLaMA C++ Webapp
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we begin using the chatbot, we need to customize it. Replace "LLaMA"
    with your model name in the prompt section. Additionally, modify the user name
    and bot name to distinguish between the generated responses.
  prefs: []
  type: TYPE_NORMAL
- en: '![Running Mixtral 8x7b On Google Colab For Free](../Images/041c82e7530fabeadb9e877db16fc582.png)'
  prefs: []
  type: TYPE_IMG
- en: Start chatting by scrolling down and typing in the chat section. Feel free to
    ask technical questions that other open source models have failed to answer properly.
  prefs: []
  type: TYPE_NORMAL
- en: '![Running Mixtral 8x7b On Google Colab For Free](../Images/12c1e90834baaa365a694671d711c2b7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you encounter issues with the app, you can try running it on your own using
    my Google Colab: https://colab.research.google.com/drive/1gQ1lpSH-BhbKN-DdBmq5r8-8Rw8q1p9r?usp=sharing'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This tutorial provides a comprehensive guide on how to run the advanced open-source
    model, Mixtral 8x7b, on Google Colab using the LLaMA C++ library. Compared to
    other models, Mixtral 8x7b delivers superior performance and efficiency, making
    it an excellent solution for those who want to experiment with large language
    models but do not have extensive computational resources. You can easily run it
    on your laptop or on a free cloud compute. It is user-friendly, and you can even
    deploy your chat app for others to use and experiment with.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you found this simple solution to running the large model helpful. I
    am always looking for simple and better options. If you have an even better solution,
    please let me know, and I will cover it next time.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.polywork.com/kingabzpro)****[Abid Ali Awan](https://www.polywork.com/kingabzpro)****
    ([@1abidaliawan](https://www.linkedin.com/in/1abidaliawan)) is a certified data
    scientist professional who loves building machine learning models. Currently,
    he is focusing on content creation and writing technical blogs on machine learning
    and data science technologies. Abid holds a Master''s degree in technology management
    and a bachelor''s degree in telecommunication engineering. His vision is to build
    an AI product using a graph neural network for students struggling with mental
    illness.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Running Redis on Google Colab](https://www.kdnuggets.com/2022/01/running-redis-google-colab.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Fine Tuning LLAMAv2 with QLora on Google Colab for Free](https://www.kdnuggets.com/fine-tuning-llamav2-with-qlora-on-google-colab-for-free)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[From Google Colab to a Ploomber Pipeline: ML at Scale with GPUs](https://www.kdnuggets.com/2022/03/google-colab-ploomber-pipeline-ml-scale-gpus.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[RAPIDS cuDF for Accelerated Data Science on Google Colab](https://www.kdnuggets.com/2023/01/rapids-cudf-accelerated-data-science-google-colab.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Get Up and Running with SQL - A List of Free Learning Resources](https://www.kdnuggets.com/2022/10/get-running-sql-list-free-learning-resources.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Steps to Running a Small Language Model on a Local CPU](https://www.kdnuggets.com/7-steps-to-running-a-small-language-model-on-a-local-cpu)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
