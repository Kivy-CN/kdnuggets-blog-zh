- en: 'Thought Propagation: An Analogical Approach to Complex Reasoning with Large
    Language Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/thought-propagation-an-analogical-approach-to-complex-reasoning-with-large-language-models](https://www.kdnuggets.com/thought-propagation-an-analogical-approach-to-complex-reasoning-with-large-language-models)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Thought Propagation: An Analogical Approach to Complex Reasoning with Large
    Language Models](../Images/0312168e6257c21dcf33fe00142a7d83.png)'
  prefs: []
  type: TYPE_IMG
- en: '## Key Takeaways'
  prefs: []
  type: TYPE_NORMAL
- en: Thought Propagation (TP) is a novel method that enhances the complex reasoning
    abilities of Large Language Models (LLMs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TP leverages analogous problems and their solutions to improve reasoning, rather
    than making LLMs reason from scratch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiments across various tasks show TP substantially outperforms baseline
    methods, with improvements ranging from 12% to 15%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: TP first prompts LLMs to propose and solve a set of analogous problems that
    are related to the input one. Then, TP reuses the results of analogous problems
    to directly yield a new solution or derive a knowledge-intensive plan for execution
    to amend the initial solution obtained from scratch.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The versatility and computational power of Large Language Models (LLMs) are
    undeniable, yet they are not without limit. One of the most significant and consistent
    challenges to LLMs is their general approach to problem-solving, consisting of
    reasoning from first principles for every new task encountered. This is problematic,
    as it allows for a high degree of adaptability, but also increases the likelihood
    of errors, particularly in tasks that require multi-step reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge of "reasoning from scratch" is especially pronounced in complex
    tasks that demand multiple steps of logic and inference. For example, if an LLM
    is asked to find the shortest path in a network of interconnected points, it typically
    would not leverage prior knowledge or analogous problems to find a solution. Instead,
    it would attempt to solve the problem in isolation, which can lead to suboptimal
    results or even outright errors. Enter [Thought Propagation](https://arxiv.org/abs/2310.03965v2)
    (TP), a method designed to augment the reasoning capabilities of LLMs. TP aims
    to overcome the inherent limitations of LLMs by allowing them to draw from a reservoir
    of analogous problems and their corresponding solutions. This innovative approach
    not only improves the accuracy of LLM-generated solutions but also significantly
    enhances their ability to tackle multi-step, complex reasoning tasks. By leveraging
    the power of analogy, TP provides a framework that amplifies the innate reasoning
    capabilities of LLMs, bringing us one step closer to the realization of truly
    intelligent artificial systems.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Thought Propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Thought Propagation involves two main steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the LLM is prompted to propose and solve a set of analogous problems
    related to the input problem
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the solutions to these analogous problems are used to either directly
    yield a new solution or to amend the initial solution
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The process of identifying analogous problems allows the LLM to reuse problem-solving
    strategies and solutions, thereby improving its reasoning abilities. TP is compatible
    with existing prompting methods, providing a generalizable solution that can be
    incorporated into various tasks without significant task-specific engineering.
  prefs: []
  type: TYPE_NORMAL
- en: '![Thought Propagation process](../Images/fbbafecf6b71c1e84f2fc31deef5317b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 1**: The Thought Propagation process (Image from paper)'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the adaptability of TP should not be underestimated. Its compatibility
    with existing prompting methods makes it a highly versatile tool. This means that
    TP is not limited to any specific kind of problem-solving domain. This opens up
    exciting avenues for task-specific fine-tuning and optimization, thereby elevating
    the utility and efficacy of LLMs in a broad spectrum of applications.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Thought Propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The implementation of Thought Propagation can be integrated into the workflow
    of existing LLMs. For example, in a Shortest-path Reasoning task, TP could first
    solve a set of simpler, analogous problems to understand various possible paths.
    It would then use these insights to solve the complex problem, thereby increasing
    the likelihood of finding the optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 1**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Task**: Shortest-path Reasoning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Analogous Problems**: Shortest path between point A and B, Shortest path
    between point B and C'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Final Solution**: Optimal path from point A to C considering the solutions
    of analogous problems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example 2**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Task**: Creative Writing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Analogous Problems**: Write a short story about friendship, Write a short
    story about trust'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Final Solution**: Write a complex short story that integrates themes of friendship
    and trust'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process involves solving these analogous problems first, and then using
    the insights gained to tackle the complex task at hand. This method has demonstrated
    its effectiveness across multiple tasks, showcasing substantial improvements in
    performance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Thought Propagation's implications go beyond merely improving existing metrics.
    This prompting technique has the potential to alter how we understand and deploy
    LLMs. The methodology underscores a shift from isolated, atomic problem-solving
    towards a more holistic, interconnected approach. It prompts us to consider how
    LLMs can learn not just from data, but from the process of problem-solving itself.
    By continuously updating their understanding through the solutions to analogous
    problems, LLMs equipped with TP are better prepared to tackle unforeseen challenges,
    rendering them more resilient and adaptable in rapidly evolving environments.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thought Propagation is a promising addition to the toolbox of prompting methods
    aimed at enhancing the capabilities of LLMs. By allowing LLMs to leverage analogous
    problems and their solutions, TP provides a more nuanced and effective reasoning
    method. Experiments confirm its efficacy, making it a candidate strategy for improving
    the performance of LLMs across a variety of tasks. TP may ultimately represent
    a significant step forward in the search for more capable AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.linkedin.com/in/mattmayo13/)****[Matthew Mayo](https://www.kdnuggets.com/wp-content/uploads/./profile-pic.jpg)****
    ([**@mattmayo13**](https://twitter.com/mattmayo13)) holds a master''s degree in
    computer science and a graduate diploma in data mining. As managing editor of
    [KDnuggets](https://www.kdnuggets.com/) & [Statology](https://www.statology.org/),
    and contributing editor at [Machine Learning Mastery](https://machinelearningmastery.com/),
    Matthew aims to make complex data science concepts accessible. His professional
    interests include natural language processing, language models, machine learning
    algorithms, and exploring emerging AI. He is driven by a mission to democratize
    knowledge in the data science community. Matthew has been coding since he was
    6 years old.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Unraveling the Power of Chain-of-Thought Prompting in Large Language Models](https://www.kdnuggets.com/2023/07/power-chain-thought-prompting-large-language-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Open Source Large Language Models](https://www.kdnuggets.com/2022/09/john-snow-top-open-source-large-language-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[More Free Courses on Large Language Models](https://www.kdnuggets.com/2023/06/free-courses-large-language-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Learn About Large Language Models](https://www.kdnuggets.com/2023/03/learn-large-language-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introducing Healthcare-Specific Large Language Models from John Snow Labs](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are Large Language Models and How Do They Work?](https://www.kdnuggets.com/2023/05/large-language-models-work.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
