# 机器学习和数据科学中的决策树指南

> 原文：[https://www.kdnuggets.com/2018/12/guide-decision-trees-machine-learning-data-science.html](https://www.kdnuggets.com/2018/12/guide-decision-trees-machine-learning-data-science.html)

![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)

**作者：[乔治·塞夫](https://towardsdatascience.com/@george.seif94)，人工智能/机器学习工程师**

决策树是一类非常强大的机器学习模型，能够在许多任务中实现高准确率，同时具有很高的可解释性。决策树在机器学习模型领域的特别之处在于其信息表示的清晰性。通过训练，决策树“学到的知识”直接被形成一个层次结构。这一结构以一种易于理解的方式保存和展示知识，即使是非专家也能轻松理解。

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你所在的组织的 IT

* * *

![](../Images/247d774fc1b4a93bcba12afc67523f20.png)

### 现实生活中的决策树

你可能之前在生活中使用过决策树来做决策。例如，考虑一下你这个周末应该做什么活动。这可能取决于你是否想和朋友外出还是一个人度过周末；在这两种情况下，你的决定还取决于天气。如果晴天而且你的朋友有空，你可能会想踢足球。如果结果是下雨，你会去看电影。如果你的朋友根本没有出现，那你不管天气如何，都喜欢玩视频游戏！

![](../Images/4476e73464585bcc9cddaf0ae303d4a1.png)

这是一个*真实生活中的决策树*的清晰示例。我们建立了一个树来模拟一组**顺序的、层次化的决策**，最终得出某个结果。请注意，我们还选择了相当“高层次”的决策，以保持树的简洁。例如，如果我们为天气设置了*许多*可能的选项，如 25 度晴天、25 度下雨、26 度晴天、26 度下雨、27 度晴天……等等，我们的树将会非常庞大！**确切的**温度其实并不太重要，我们只想知道是否可以在外面活动。

机器学习中的决策树概念是相同的。我们想要构建一棵具有一系列层次决策的树，最终给出一个最终结果，即我们的分类或回归预测。决策将被选择，以使树尽可能小，同时追求高分类/回归准确率。

### 机器学习中的决策树

决策树模型通过两个步骤创建：生成和剪枝。生成是我们实际构建树的过程，即基于我们的数据设置所有的层次决策边界。由于训练决策树的特性，它们可能会严重过拟合。剪枝是去除决策树中不必要结构的过程，有效地减少复杂性以应对过拟合，同时还有使其更易于解释的额外好处。

**生成**

从高层次来看，决策树生成过程经历四个主要步骤来构建树：

1.  从你的训练数据集开始，它应该具有一些特征变量和分类或回归输出。

1.  确定数据集中用于分割数据的“最佳特征”；关于如何定义“最佳特征”我们稍后会详细讨论。

1.  将数据分割成包含该最佳特征的所有可能值的子集。这种分割基本上定义了树上的一个节点，即每个节点是基于我们数据中某个特征的分割点。

1.  通过使用步骤3中创建的数据子集递归生成新的树节点。我们不断分割，直到达到一个点，即在某种度量下优化了最大准确性，同时最小化分割/节点的数量。

第一步很简单，只需获取你的数据集！

对于步骤2，选择使用哪个特征以及具体的分割通常使用贪心算法来最小化成本函数。如果我们稍加思考，构建决策树时的分割等同于划分特征空间。我们将迭代尝试不同的分割点，然后在最后选择具有最低成本的点。当然，我们可以做一些聪明的事情，比如仅在数据集中的值范围内进行分割。这将避免我们浪费计算资源在测试那些明显不佳的分割点上。

对于回归树，我们可以使用简单的平方误差作为成本函数：

![](../Images/9e737360b5111722e071106141110538.png)

其中Y是我们的真实值，Y-hat是我们的预测值；我们对数据集中的所有样本求和以获得总误差。对于分类，我们使用*基尼指数函数：*

![](../Images/cbd57496eddd6d3825f64340081d9c88.png)

其中pk是某个特定预测节点中类别k的训练实例的比例。一个节点应该*理想上*具有零误差值，这意味着每个分裂在100%的时间内输出一个单一类别。这正是我们想要的，因为这样一来，一旦我们到达那个特定的决策节点，我们就知道，无论我们处于决策边界的哪一侧，我们的输出将是什么。

在我们的数据集中，每个分裂都有一个单一类别的概念被称为*信息增益*。查看下面的示例。

![](../Images/a52e214cfcc8a71a438dc70e9007a719.png)

如果我们选择的分裂使得每个输出在不同的输入数据下都有不同的类别混合，那么我们实际上并没有*获得*任何信息；我们不知道某个特定节点，即特征是否在分类我们的数据中有任何影响！另一方面，如果我们的分裂在每个输出中具有高比例的每个类别，那么我们就*获得*了信息，即在特定特征变量上以这种方式分裂能给我们一个特定的输出！

当然，我们可以继续不断分裂，直到树拥有成千上万的分支……但这并不是一个好主意！我们的决策树会变得庞大、缓慢，并且对我们的训练数据集过拟合。因此，我们将设定一些预定义的停止标准来停止树的构建。

最常见的停止方法是对分配给每个叶节点的训练样本数量设置一个最小值。如果计数小于某个最小值，则该分裂不被接受，节点被视为最终叶节点。如果所有叶节点都变成最终节点，则训练停止。较小的最小计数会产生更精细的分裂并潜在地提供更多信息，但也容易导致对训练数据的过拟合。最小计数过大，则可能过早停止。因此，最小值通常根据数据集设置，具体取决于每个类别中预期有多少样本。

**剪枝**

由于训练决策树的性质，它们可能容易出现严重的过拟合。为每个节点设置正确的最小实例数量可能是具有挑战性的。大多数时候，我们可能只是选择一个安全的最小值，这样会导致许多分裂和一个非常大、复杂的树。关键是这些分裂中的许多最终会变得冗余且不必要，从而不会提高我们模型的准确性。

树剪枝是一种利用这种分裂冗余来删除即*剪枝*树中不必要的分裂的技术。从高层次来看，剪枝将树的一部分从严格且僵化的决策边界压缩为更加平滑并具有更好泛化性的决策边界，从而有效地减少树的复杂性。决策树的复杂性定义为树中的分裂数量。

一种简单而高效的剪枝方法是遍历树中的每个节点，并评估移除该节点对成本函数的影响。如果变化不大，就剪枝吧！

### Scikit Learn中的示例

使用Scikit Learn中的内置类，分类和回归的决策树非常容易使用！我们首先加载数据集并初始化分类决策树。然后，训练只需要一行代码！

Scikit Learn还允许我们使用graphviz库可视化我们的树。它提供了一些选项，帮助我们可视化模型学到的决策节点和分裂，这对于理解整个过程非常有用！下面我们将根据特征名称为节点上色，并显示每个节点的类别和特征信息。

![](../Images/ecaf20a53098ea1fa03240e432bd03e3.png)

在Scikit Learn中，你还可以为决策树模型设置几个参数。以下是一些有趣的参数，可以用来尝试获得更好的结果：

+   **max_depth:** 树的最大深度，即我们将停止分裂节点的位置。这类似于控制深度神经网络中的最大层数。较低的深度会使模型更快，但可能不那么准确；较高的深度可以提高准确率，但有过拟合的风险，并且可能比较慢。

+   **min_samples_split: ** 分裂节点所需的最小样本数。我们在上面讨论了决策树的这一方面，设置为较高的值可以帮助缓解过拟合。

+   **max_features: ** 当寻找最佳分裂时要考虑的特征数量。数量越高，可能结果越好，但训练时间会更长。

+   **min_impurity_split: ** 树生长中的提前停止阈值。如果节点的杂质高于该阈值，则该节点会进行分裂。这可以用来权衡防止过拟合（高值，小树）与高准确率（低值，大树）之间的关系。

+   **presort:** 是否预排序数据以加速找到最佳分裂的过程。如果我们提前对每个特征进行排序，我们的训练算法将更容易找到好的分裂值。

### 实际应用决策树的提示

下面是决策树的一些优缺点，可以帮助你决定它是否适合你的问题，并提供一些有效应用的提示：

**优点**

+   **易于理解和解释。** 在每个节点上，我们可以*准确*地看到我们的模型在做什么决策。在实践中，我们将能够完全理解准确率和错误来自哪里，模型适合什么类型的数据，以及特征值如何影响输出。Scikit Learn的可视化工具是可视化和理解决策树的绝佳选择。

+   **需要非常少的数据准备。** 许多机器学习模型可能需要大量的数据预处理，如归一化，并可能需要复杂的正则化方案。而决策树在调整几个参数后，开箱即用表现得很好。

+   **使用树进行推断的成本与训练树所用的数据点数量的对数成正比。** 这是一个巨大的优势，因为这意味着更多的数据不一定会对我们的推断速度造成巨大影响。

**缺点**

+   由于训练的性质，决策树的过拟合相当常见。通常建议执行一些类型的降维，如[PCA](https://towardsdatascience.com/principal-component-analysis-your-tutorial-and-code-9719d3d3f376)，以避免树学习过多特征上的分裂。

+   与过拟合的情况类似，决策树也容易对数据集中占多数的类别产生偏见。进行一些类平衡（例如类权重、采样或专门的损失函数）总是一个好主意。

### 想学习？

在[twitter](https://twitter.com/GeorgeSeif94)上关注我，我会发布关于最新和最伟大的AI、技术和科学的内容！

**简介: [George Seif](https://towardsdatascience.com/@george.seif94)** 是一名认证极客和AI/机器学习工程师。

[原文](https://towardsdatascience.com/a-guide-to-decision-trees-for-machine-learning-and-data-science-fe2607241956)。经允许转载。

**相关：**

+   [5个快速简单的Python数据可视化示例及代码](/2018/07/5-quick-easy-data-visualizations-python-code.html)

+   [数据科学家需要知道的5种聚类算法](/2018/06/5-clustering-algorithms-data-scientists-need-know.html)

+   [使用Python提升数据预处理速度2到6倍](/2018/10/get-speed-up-data-pre-processing-python.html)

### 相关阅读

+   [停止学习数据科学，找到目标，再找目标来……](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)

+   [数据科学学习统计学的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)

+   [成功数据科学家的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)

+   [每个数据科学家都应该知道的三个R库（即使你使用Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)

+   [一个90亿美元的AI失败案例，分析](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)

+   [是什么让Python成为初创公司的理想编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)
