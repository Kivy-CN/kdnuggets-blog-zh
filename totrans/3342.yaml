- en: 'Apache Arrow and Apache Parquet: Why We Needed Different Projects for Columnar
    Data, On Disk and In-Memory'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Arrow 和 Apache Parquet：为何我们需要不同的列式数据项目，分别用于磁盘和内存
- en: 原文：[https://www.kdnuggets.com/2017/02/apache-arrow-parquet-columnar-data.html](https://www.kdnuggets.com/2017/02/apache-arrow-parquet-columnar-data.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/02/apache-arrow-parquet-columnar-data.html](https://www.kdnuggets.com/2017/02/apache-arrow-parquet-columnar-data.html)
- en: '**By Julien LeDem, architect, [Dremio](http://www.dremio.com/).**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**由架构师Julien LeDem撰写，来自 [Dremio](http://www.dremio.com/)。**'
- en: Columnar data structures provide a number of performance advantages over traditional
    row-oriented data structures for analytics. These include benefits for data on
    disk – fewer disk seeks, more effective compression, faster scan rates – as well
    as more efficient use of CPU for data in memory.  Today columnar data is very
    common and is implemented by most analytical databases, including Teradata, Vertica,
    Oracle, and others.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 列式数据结构在分析方面提供了许多性能优势。包括磁盘上数据的好处——减少磁盘寻址次数，更有效的压缩，更快的扫描速度——以及内存中数据的CPU使用效率。今天，列式数据非常普遍，并且被大多数分析数据库实现，包括Teradata、Vertica、Oracle等。
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析水平'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In 2012 and 2013 several of us at Twitter and Cloudera created Apache Parquet
    (originally called Red Elm, an anagram for Google’s Dremel) to bring these ideas
    to the Hadoop ecosystem. Four years later, Parquet is the standard for columnar
    data on disk, and a new project called Apache Arrow has emerged to become the
    standard way of representing columnar data in memory. In this article we’ll take
    a closer look at why we need two projects, one for storing data on disk and one
    for processing data in memory, and how they work together.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在2012年和2013年，我们Twitter和Cloudera的一些人创建了Apache Parquet（最初叫做Red Elm，是Google的Dremel的一个字母重排），旨在将这些理念引入Hadoop生态系统。四年后，Parquet成为了磁盘上列式数据的标准，而一个名为Apache
    Arrow的新项目也出现了，成为了内存中表示列式数据的标准。在这篇文章中，我们将深入探讨为什么我们需要两个项目，一个用于磁盘上的数据存储，另一个用于内存中的数据处理，以及它们如何协同工作。
- en: What’s wrong with rows, and what’s right with columns?
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 行有什么问题，列又有什么优点？
- en: Back in 2007 Vertica, one of the early commercial columnar databases, had a
    clever marketing slogan “The Tables Have Turned.”  This isn’t a bad way of visualizing
    how columnar databases work – turn a table 90 degrees and now what used to be
    reading a row is reading a single column. This approach has a lot of advantages
    when it comes to analytical workloads.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 回到2007年，Vertica作为早期的商业列式数据库之一，有一个巧妙的营销口号“桌子已经翻转。”这并不是一个不好的方式来形象化列式数据库的工作方式——将桌子旋转90度，现在曾经读取的行变成了读取单一列。这种方法在处理分析性负载时有很多优势。
- en: '![](../Images/ed551a87e16d9473437bf40dfeb79e19.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed551a87e16d9473437bf40dfeb79e19.png)'
- en: Most systems are engineered to minimize the number of disk seeks and the amount
    of data scanned, as these operations can add tremendous latency. In transactional
    workloads, as data is written to a table in a row-oriented database, the columns
    for a given row are written out to disk contiguously, which is very efficient
    for writes. Analytical workloads differ in that most queries read a small subset
    of the columns for large numbers of rows at a time. In a traditional row-oriented
    database, the system might perform a seek for each row, and most of the columns
    would be read from disk into memory unnecessarily.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数系统都被设计为最小化磁盘寻址次数和扫描的数据量，因为这些操作可能会带来巨大的延迟。在事务性负载中，当数据被写入到行导向数据库中的表时，给定行的列会连续地写入磁盘，这对于写操作非常高效。分析性负载则不同，因为大多数查询一次读取大量行中的一小部分列。在传统的行导向数据库中，系统可能会对每一行执行一次寻址，而大多数列则会被不必要地从磁盘读入内存。
- en: A columnar database organizes the values for a given column contiguously on
    disk. This has the advantage of significantly reducing the number of seeks for
    multi-row reads. Furthermore, compression algorithms tend to be much more effective
    on a single data type rather than the mix of types present in a typical row. The
    tradeoff is that writes are slower, but this is a good optimization for analytics
    where reads typically far outnumber writes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 列式数据库将给定列的值连续地组织在磁盘上。这具有显著减少多行读取时寻址次数的优势。此外，压缩算法在单一数据类型上往往比在典型行中的混合类型上更有效。权衡之处在于写入速度较慢，但对于分析来说，这是一种良好的优化，因为读取通常远多于写入。
- en: Columnar, meet Hadoop
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 列式，遇见 Hadoop
- en: At Twitter we were big users of Hadoop, which is very scalable, good at storing
    a wide range of data, and good at parallelizing workloads across hundreds or thousands
    of servers. In general Hadoop is fairly high latency, and so we were also big
    users of Vertica, which gave us great performance but for only a small subset
    of our data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Twitter，我们是 Hadoop 的大量用户，Hadoop 非常可扩展，能够存储广泛的数据，并能在数百或数千台服务器之间并行化工作负载。一般来说，Hadoop
    的延迟较高，因此我们也大量使用 Vertica，它为我们提供了很好的性能，但仅适用于数据的一个小子集。
- en: We felt that by doing a better job of organizing the data in a columnar model
    in HDFS we could significantly improve the performance of Hadoop for analytical
    jobs, primarily for Hive queries, but for other projects as well. At the time
    we had around 400M users and were generating over 100TB of compressed data per
    day, so there was a lot of interest in this project.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为通过在 HDFS 中以列式模型更好地组织数据，可以显著提高 Hadoop 在分析作业中的性能，主要是 Hive 查询，但也包括其他项目。当时我们有大约
    4 亿用户，每天生成超过 100TB 的压缩数据，因此对这个项目有很大的兴趣。
- en: 'We saw a lot of benefits in our early tests: we reduced storage overhead by
    28%, and we saw a 90% reduction in time to read a single column. We kept at it,
    adding column-specific compression options, dictionary compression, bit packing
    and run length encoding, ultimately reducing storage another 52% and read times
    another 48%.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在早期测试中看到了很多好处：我们减少了 28% 的存储开销，单列读取时间减少了 90%。我们继续优化，增加了列特定的压缩选项、字典压缩、位打包和游程编码，最终将存储减少了
    52%，读取时间减少了 48%。
- en: '![Columnar data](../Images/db84f998e32079445312e5e853a9089d.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![列式数据](../Images/db84f998e32079445312e5e853a9089d.png)'
- en: Parquet was also designed to handle richly structured data like JSON. It was
    very beneficial to us at Twitter and many other early adopters, and today most
    Hadoop users store their data in Parquet. There is pervasive support for Parquet
    across the Hadoop ecosystem, including Spark, Presto, Hive, Impala, Drill, Kite,
    and others. Even outside of Hadoop it has been adopted in some scientific communities,
    such as CERN’s ROOT project.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 也被设计用来处理像 JSON 这样丰富结构的数据。这对我们在 Twitter 和许多其他早期采用者非常有利，如今大多数 Hadoop 用户将他们的数据存储在
    Parquet 中。Hadoop 生态系统中广泛支持 Parquet，包括 Spark、Presto、Hive、Impala、Drill、Kite 等。即使在
    Hadoop 之外，它也被一些科学社区采用，如 CERN 的 ROOT 项目。
- en: Building on the ideas of Parquet for in-memory processing
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于 Parquet 的内存处理思想
- en: Hardware has changed a lot in the 15 years since original Google papers that
    inspired Hadoop. The most important change is the dramatic decline in RAM prices.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 自原始 Google 论文启发 Hadoop 以来，硬件在过去 15 年里发生了很大变化。最重要的变化是 RAM 价格的大幅下降。
- en: '![](../Images/ad2bc32d89913243206f9bbc9fe58f27.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad2bc32d89913243206f9bbc9fe58f27.png)'
- en: Servers today have a lot more RAM than they did when I was at Twitter, and because
    reading data from memory is thousands of times faster than reading data from disk,
    there is enormous interest in the data world in how to make optimal use of RAM
    for analytics.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的服务器拥有比我在 Twitter 时期更多的 RAM，由于从内存中读取数据的速度比从磁盘中读取数据快上千倍，因此在数据领域中，如何为分析任务优化使用
    RAM 引起了巨大的兴趣。
- en: The trade-offs being for columnar data are different for in-memory. For data
    on disk, usually IO dominates latency, which can be addressed with aggressive
    compression, at the cost of CPU. In memory, access is much faster and we want
    to optimize for CPU throughput by paying attention to cache locality, pipelining,
    and SIMD instructions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 列式数据的权衡与内存中的数据不同。对于磁盘上的数据，通常 IO 主导延迟，这可以通过积极的压缩来解决，但会以 CPU 为代价。在内存中，访问速度要快得多，我们希望通过关注缓存局部性、流水线和
    SIMD 指令来优化 CPU 吞吐量。
- en: Streamlining the interface between systems
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简化系统间的接口
- en: One of the funny things about computer science is that while there is a common
    set of resources – RAM, CPU, storage, network – each language has an entirely
    different way of interacting with those resources. When different programs need
    to interact – within and across languages – there are inefficiencies in the handoffs
    that can dominate the overall cost. This is a little bit like traveling in Europe
    before the Euro where you needed a different currency for each country, and by
    the end of the trip you could be sure you had lost a lot of money with all the
    exchanges!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学中一个有趣的现象是，尽管有一套通用资源——RAM、CPU、存储、网络——但每种语言与这些资源的交互方式完全不同。当不同程序需要交互时——无论是在语言内部还是跨语言——这些数据传递中的低效性可能会占据整体成本的一部分。这有点像在欧元出现之前在欧洲旅行，你需要每个国家不同的货币，到旅程结束时，你可以肯定你在所有的货币兑换中损失了很多钱！
- en: '![](../Images/16e8a5fea28863305881f3286da70634.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16e8a5fea28863305881f3286da70634.png)'
- en: '![](../Images/016ddeda494124b5d26a1629156392b8.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/016ddeda494124b5d26a1629156392b8.png)'
- en: We viewed these handoffs as the next obvious bottleneck for in-memory processing,
    and set out to work across a wide range of projects to develop a common set of
    interfaces that would remove unnecessary serialization and deserialization when
    marshalling data. Apache Arrow standardizes an efficient in-memory columnar representation
    that is the same as the wire representation. Today it includes first class bindings
    in over 13 projects, including Spark, Hadoop, R, Python/Pandas, and my company,
    Dremio.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些数据传递视为内存处理中的下一个明显瓶颈，并开始在广泛的项目中工作，以开发一套通用接口，消除在数据传输过程中不必要的序列化和反序列化。Apache
    Arrow 标准化了一种高效的内存列式表示，这种表示与网络传输表示相同。如今，它在超过13个项目中包括了一级绑定，包括 Spark、Hadoop、R、Python/Pandas
    以及我的公司 Dremio。
- en: '![](../Images/c80acb3b2f2ee35321befe15be4dccee.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c80acb3b2f2ee35321befe15be4dccee.png)'
- en: Python in particular has very strong support in the Pandas library, and supports
    working directly with Arrow record batches and persisting them to Parquet.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Python 尤其在 Pandas 库中得到了非常强的支持，并且支持直接处理 Arrow 记录批次并将其持久化为 Parquet。
- en: These are still early days for Apache Arrow, but the results are very promising.
    It is not uncommon for users to see 10x-100x improvements in performance across
    a range of workloads.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Apache Arrow 来说，这些仍然是早期阶段，但结果非常有希望。用户在各种工作负载中看到性能提高 10 倍到 100 倍并不少见。
- en: Parquet and Arrow, working together
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Parquet 和 Arrow 的协同工作
- en: Interoperability between Parquet and Arrow has been a goal since day 1\. While
    each project can be used independently, both provide APIs to read and write between
    the formats. ![](../Images/3fe6329df67b98a388942afb32a69f2d.png)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 和 Arrow 之间的互操作性从一开始就是一个目标。虽然每个项目可以独立使用，但两者都提供了在格式之间读取和写入的 API。 ![](../Images/3fe6329df67b98a388942afb32a69f2d.png)
- en: Since both are columnar we can implement efficient vectorized converters from
    one to the other and read from Parquet to Arrow much faster than in a row-oriented
    representation. We are still working on ways to make this integration even more
    seamless, including a vectorized Java reader, and full type equivalence.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于两者都是列式的，我们可以实现高效的矢量化转换器，将数据从一个格式转换到另一个格式，并且从 Parquet 读取到 Arrow 的速度比行式表示要快得多。我们仍在寻找使这种集成更加无缝的方法，包括一个矢量化的
    Java 阅读器和完全的类型等价。
- en: Pandas is a good example of using both projects. Users can save a Pandas data
    frame to Parquet and read a Parquet file to in-memory Arrow. Pandas can directly
    work on top of Arrow columns, paving the way for a faster Spark integration.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 是一个很好的例子，展示了如何使用这两个项目。用户可以将 Pandas 数据框保存为 Parquet，并将 Parquet 文件读取到内存中的
    Arrow。Pandas 可以直接在 Arrow 列上进行操作，为更快的 Spark 集成铺平道路。
- en: At my current company, Dremio, we are hard at work on a new project that makes
    extensive use of Apache Arrow and Apache Parquet. You can learn more at [www.dremio.com](http://www.dremio.com).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在我目前的公司 Dremio，我们正在全力推进一个新项目，该项目广泛使用 Apache Arrow 和 Apache Parquet。你可以在 [www.dremio.com](http://www.dremio.com)
    了解更多信息。
- en: '**Bio:** Julien LeDem, architect, [Dremio](http://www.dremio.com) is the co-author
    of Apache Parquet and the PMC Chair of the project. He is also a committer and
    PMC Member on Apache Pig. Julien is an architect at Dremio, and was previously
    the Tech Lead for Twitter’s Data Processing tools, where he also obtained a two-character
    Twitter handle ([@J_](https://twitter.com/j_)). Prior to Twitter, Julien was a
    Principal engineer and tech lead working on Content Platforms at Yahoo! where
    he received his Hadoop initiation.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：** Julien LeDem，架构师，[Dremio](http://www.dremio.com) 的联合创始人，Apache Parquet
    的共同作者及项目 PMC 主席。他还是 Apache Pig 的 committer 和 PMC 成员。Julien 是 Dremio 的架构师，之前曾担任
    Twitter 数据处理工具的技术负责人，并获得了一个两个字符的 Twitter 账号（[@J_](https://twitter.com/j_)）。在 Twitter
    之前，Julien 是 Yahoo! 内容平台的首席工程师和技术负责人，并在那里获得了 Hadoop 方面的经验。'
- en: '**Related:**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Spark with Tungsten Burns Brighter](/2016/05/spark-tungsten-burns-brighter.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[闪耀的 Tungsten：Spark 的光芒更亮](/2016/05/spark-tungsten-burns-brighter.html)'
- en: '[Spark for Scale: Machine Learning for Big Data](/2016/09/spark-scale-machine-learning-big-data.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[规模化的 Spark：大数据的机器学习](/2016/09/spark-scale-machine-learning-big-data.html)'
- en: '[Big RAM is eating big data – Size of datasets used for analytics](/2015/11/big-ram-big-data-size-datasets.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大内存正在吞噬大数据——用于分析的数据集大小](/2015/11/big-ram-big-data-size-datasets.html)'
- en: More On This Topic
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成为一名优秀数据科学家需要的 5 项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
- en: '[Are Data Scientists Still Needed in the Age of Generative AI?](https://www.kdnuggets.com/2023/06/data-scientists-still-needed-age-generative-ai.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[生成式 AI 时代数据科学家还需要吗？](https://www.kdnuggets.com/2023/06/data-scientists-still-needed-age-generative-ai.html)'
- en: '[Low Code: Are Developers Still Needed?](https://www.kdnuggets.com/2022/04/low-code-developers-still-needed.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[低代码：开发者还需要吗？](https://www.kdnuggets.com/2022/04/low-code-developers-still-needed.html)'
- en: '[5 Different Ways to Load Data in Python](https://www.kdnuggets.com/2020/08/5-different-ways-load-data-python.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 中加载数据的 5 种不同方式](https://www.kdnuggets.com/2020/08/5-different-ways-load-data-python.html)'
- en: '[How is Data Mining Different from Machine Learning?](https://www.kdnuggets.com/2022/06/data-mining-different-machine-learning.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据挖掘与机器学习有何不同？](https://www.kdnuggets.com/2022/06/data-mining-different-machine-learning.html)'
- en: '[How to Transition into Data Science from a Different Background?](https://www.kdnuggets.com/2023/05/transition-data-science-different-background.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何从不同背景过渡到数据科学？](https://www.kdnuggets.com/2023/05/transition-data-science-different-background.html)'
