- en: 3 Simple Ways to Speed Up Your Python Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/10/3-simple-ways-speed-python-code.html](https://www.kdnuggets.com/2022/10/3-simple-ways-speed-python-code.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![3 Simple Ways to Speed Up Your Python Code](../Images/604ed2396cc488ef771a705d9cdfb3f5.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Source](https://www.freepik.com/free-vector/flat-people-asking-questions_13561931.htm#query=thinking&position=28&from_view=search):
    Image by Freepik'
  prefs: []
  type: TYPE_NORMAL
- en: You just finished a project that used Python. The code is clean and readable,
    but your performance benchmark is not up to the mark. You expected to get a result
    in milliseconds, and instead, you got seconds. What do you do?
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re reading this article, you probably already know that Python is an
    interpreted programming language with dynamic semantics and high readability.
    That makes it easy to use and read — but not fast enough for many real-world use
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: So there could be multiple ways to speed up your Python code including but not
    limited to the use of efficient data structures as well as fast and efficient
    algorithms. Some Python libraries also make use of C or C++ underneath to speed
    up computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if you have exhausted all these options? Here comes parallel processing
    and a step ahead is distributed computing. In this post, you will learn about
    the three popular frameworks in distributed computing in a machine learning context:
    PySpark, Dask, and Ray.'
  prefs: []
  type: TYPE_NORMAL
- en: PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PySpark as the name suggests is an interface of Apache Spark within Python.
    It allows the user to write Spark programs using Python APIs and provides the
    PySpark shell for interactive data analysis in a distributed environment. It supports
    almost all of Spark’s features such as Streaming, MLlib, Spark SQL, DataFrame,
    and Spark Core as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![3 Simple Ways to Speed Up Your Python Code](../Images/d6d5e4cd748fe13a7cd3df8e2e614d9a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Source](https://spark.apache.org/docs/latest/api/python/_images/pyspark-components.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Streaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The streaming feature in Apache Spark is easy to use and fault-resistant that
    runs on top of Spark. It powers intuitive and analytical systems across streaming
    as well as historical data.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. MLlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MLlib is a scalable machine learning library built on top of the Spark framework.
    It exposes a consistent set of high-level APIs to create and tune scalable machine
    learning pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Spark SQL and DataFrame
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is a Spark module for tabular data processing. It provides an abstraction
    layer above the tabular data known as DataFrame and can act as a SQL-style query
    engine in a distributed setup.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Spark Core
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark Core is the base general execution engine on top of which all other functionality
    is built. It provides a Resilient Distributed Dataset (RDD) and in-memory computation.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Pandas API on Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pandas API is a module that enables scalable processing of pandas' features
    and methods. Its syntax is just like pandas and does not require the user to train
    on a new module. It provides a seamless and integrated codebase for pandas (small/single
    machine datasets) and Spark (large distributed datasets).
  prefs: []
  type: TYPE_NORMAL
- en: Dask
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dask is a versatile open-source library for distributed computing that provides
    a familiar user interface to Pandas, Scikit-learn, and NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: It exposes high-level and low-level APIs enabling users to run custom algorithms
    in parallel on single or distributed machines.
  prefs: []
  type: TYPE_NORMAL
- en: 'It has two components:'
  prefs: []
  type: TYPE_NORMAL
- en: Big data collections include High-Level collections such as Dask Array or parallelized
    NumPy arrays, Dask Bag or parallelized lists, Dask DataFrame or parallelized Pandas
    DataFrames, and Parallelized Scikit-learn. They also include Low-Level collections
    such as Delayed and Futures that ease parallel and real-time implementation of
    custom tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic task scheduling enables the execution of task graphs in parallel scaling
    up to clusters of thousands of nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ray
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ray is a single platform framework used in general distributed Python as well
    as AIML-powered applications. It constitutes a core distributed runtime and a
    toolkit of libraries (Ray AI Runtime) for parallelizing AIML computation as shown
    in the diagram below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![3 Simple Ways to Speed Up Your Python Code](../Images/7ef99e205e7ba4c099ae92184bcb18a2.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Source](https://docs.ray.io/en/latest/_images/ray-air.svg)'
  prefs: []
  type: TYPE_NORMAL
- en: Ray AI Runtime
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ray AIR or Ray AI Runtime is a one-stop toolkit for distributed ML applications
    that enables easy scaling of individual and end-to-end workflows. It builds on
    Ray’s libraries for a wide range of tasks such as preprocessing, scoring, training,
    tuning, serving, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Ray Core
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ray Core provides core primitives like tasks (Stateless functions executed in
    the cluster), actors (Stateful worker processes created in the cluster), and objects
    (Immutable values accessible across the cluster) to build scalable distributed
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Ray scales machine learning workloads with Ray AIR and builds and deploys distributed
    applications with Ray Core and Ray Clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Which One to Choose?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you know your options, the natural question is which one to choose.
    The answer depends on a number of factors like the specific business need, the
    core strength of the development team, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand which framework is suitable for the specific requirements
    listed below:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Size:** PySpark is the most capable when it comes to dealing with ultra-large
    workloads (TBs and above) while Dask and Ray do fairly well on medium-sized workloads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generic:** Ray leads the front when it comes to generic solutions, followed
    by PySpark. While Dask is purely aimed at scaling ML pipelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speed:** Ray is the best option for NLP or text normalization tasks which
    utilizes GPUs for speeding up computation. Dask on the other hand provides access
    to fast reading of structured files to DataFrame objects but falls behind when
    it comes to joining and merging them. This is where the Spark SQL scores well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Familiarity:** For teams more inclined toward Pandas'' way of fetching and
    filtering data, Dask seems to be a go-to option whereas PySpark is for those teams
    looking for an SQL-like querying interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ease of Use:** All three tools are built over different platforms. While
    PySpark is mostly Java and C++ based, Dask is purely Python which means your ML
    team including data scientists can easily trace back error messages if something
    breaks. Ray on the other hand is C++ on the core but is fairly Pythonic when it
    comes to the AIML module (Ray AIR).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Installation and Maintenance:** Ray and Dask score equally well when it comes
    to maintenance overhead. Spark infrastructure on the other hand is quite complex
    and difficult to maintain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Popularity and Support:** PySpark being the most mature of all enjoys developer
    community support while Dask comes second. Ray is promising in terms of features
    available in the beta testing phase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compatibility:** While PySpark integrates well with the Apache ecosystem,
    Dask gels with Python and ML libraries quite well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This post discussed how to speed up Python code beyond the usual choice of data
    structures and algorithms. The post focused on three well-known frameworks and
    their components. The post intends to help the reader by weighing the available
    choices on a range of certain attributes and business contexts.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Vidhi Chugh](https://vidhi-chugh.medium.com/)** is an award-winning AI/ML
    innovation leader and an AI Ethicist. She works at the intersection of data science,
    product, and research to deliver business value and insights. She is an advocate
    for data-centric science and a leading expert in data governance with a vision
    to build trustworthy AI solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How To Speed Up Python Code with Caching](https://www.kdnuggets.com/how-to-speed-up-python-code-with-caching)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Personalized AI Made Simple: Your No-Code Guide to Adapting GPTs](https://www.kdnuggets.com/personalized-ai-made-simple-your-no-code-guide-to-adapting-gpts)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[New Ways of Sharing Code Blocks for Data Scientists](https://www.kdnuggets.com/2022/03/new-ways-sharing-code-blocks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Ways ChatGPT Makes You Code Better and Faster](https://www.kdnuggets.com/2023/06/7-ways-chatgpt-makes-code-better-faster.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Ways You Can Use ChatGPT''s Code Interpreter For Data Science](https://www.kdnuggets.com/2023/08/5-ways-chatgpt-code-interpreter-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[RAPIDS cuDF to Speed up Your Next Data Science Workflow](https://www.kdnuggets.com/2023/04/rapids-cudf-speed-next-data-science-workflow.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
