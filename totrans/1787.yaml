- en: 17 More Must-Know Data Science Interview Questions and Answers, Part 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/02/17-data-science-interview-questions-answers-part-2.html](https://www.kdnuggets.com/2017/02/17-data-science-interview-questions-answers-part-2.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Editor''s note:** See also part 1 of [**17 More Must-Know Data Science Interview
    Questions and Answers**](/2017/02/17-data-science-interview-questions-answers.html).
    This is part 2. Here is [Part 3](/2017/03/17-data-science-interview-questions-answers-part-3.html).'
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This post answers questions:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Q7\. What is overfitting and how to avoid it?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q8\. What is the curse of dimensionality?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q9\. How can you determine which features are the most important in your model?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q10\. When can parallelism make your algorithms run faster? When could it make
    your algorithms run slower?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q11\. What is the idea behind ensemble learning?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q12\. In unsupervised learning, if a ground truth about a dataset is unknown,
    how can we determine the most useful number of clusters to be?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Q7\. What is overfitting and how to avoid it?
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[**Gregory Piatetsky**](/author/gregory-piatetsky) **answers:**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '*(Note: this is a revised version of the answer given in* [*21 Must-Know Data
    Science Interview Questions and Answers, part 2*](/2016/02/21-data-science-interview-questions-answers-part2.html)*)*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[Overfitting](/tag/overfitting) is when you build a predictive model that fits
    the data "too closely",  so that it captures the random noise in the data rather
    than true patterns.  As a result, the model predictions will be wrong when applied
    to new data.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: We frequently hear about studies that report unusual results (especially if
    you listen to Wait Wait Don't Tell Me) , or see findings like "[an orange used
    car is least likely to be a lemon](/2017/01/siegel-data-science-avoiding-prediction-pitfall.html)",
     or learn that studies overturn previous established findings (eggs are no longer
    bad for you).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Many such studies produce questionable results that cannot be repeated.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: This is a big problem, especially in social sciences or medicine, when researchers
     frequently commit the cardinal sin of Data Science - **[Overfitting the data](/tag/overfitting).**
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: The researchers test too many hypotheses without proper statistical control,
    until they happen to find something interesting. Then they report it.  Not surprisingly,
    next time the effect (which was partly due to chance) will be much smaller or
    absent.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员在没有适当统计控制的情况下测试太多的假设，直到他们偶然发现一些有趣的东西，然后就报告出来。不出所料，下次这种效果（部分由于偶然性）会大大减小或消失。
- en: These flaws of research practices were identified and reported by John P. A.
    Ioannidis in his landmark paper [*Why Most Published Research Findings Are False*](http://www.plosmedicine.org/article/info%3Adoi%2F10.1371%2Fjournal.pmed.0020124)
    (PLoS Medicine, 2005). Ioannidis found that very often either the results were
    exaggerated or the findings could not be replicated. In his paper, he presented
    statistical evidence that indeed most claimed research findings are false!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些研究实践中的缺陷由John P. A. Ioannidis在其开创性论文[*为什么大多数已发表的研究结果是错误的*](http://www.plosmedicine.org/article/info%3Adoi%2F10.1371%2Fjournal.pmed.0020124)（PLoS
    Medicine，2005年）中进行了识别和报告。Ioannidis发现，结果往往被夸大或无法复制。他在论文中提供了统计证据，表明大多数声称的研究结果确实是错误的！
- en: 'Ioannidis noted that in order for a research finding to be reliable, it should
    have:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Ioannidis指出，为了使研究结果可靠，应该具备以下条件：
- en: Large sample size and with large effects
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大样本量和显著效果
- en: Greater number of and lesser selection of tested relationship
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试关系的数量更多且选择更少
- en: Greater flexibility in designs, definitions, outcomes, and analytical modes
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计、定义、结果和分析模式的更大灵活性
- en: Minimal bias due to financial and other factors (including popularity of that
    scientific field)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化由于财务和其他因素（包括该科学领域的受欢迎程度）造成的偏差
- en: Unfortunately, too often these rules were violated, producing spurious results,
    such as S&P 500 index strongly correlated to [production of butter in Bangladesh](/2016/02/21-data-science-interview-questions-answers-part2.html),
    or US spending on science, space and technology correlated with suicides by hanging,
    strangulation, and suffocation (from http://tylervigen.com/spurious-correlations)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这些规则往往被违反，产生虚假结果，例如S&P 500指数与[孟加拉国黄油生产](https://2016/02/21-data-science-interview-questions-answers-part2.html)的强相关，或美国在科学、空间和技术上的支出与悬挂、窒息和勒死的自杀率相关（来自http://tylervigen.com/spurious-correlations）
- en: '![Spurious correlations](../Images/6ff3e462b9916e8ad30efb4541b53330.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![虚假相关](../Images/6ff3e462b9916e8ad30efb4541b53330.png)'
- en: '(Source: [Tylervigen.com](http://tylervigen.com/chart-pngs/1.png))'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: (来源：[Tylervigen.com](http://tylervigen.com/chart-pngs/1.png))
- en: See more strange and spurious findings at [Spurious correlations](http://www.tylervigen.com/discover)
    by Tyler Vigen or discover them by yourself using tools such as [Google correlate](https://www.google.com/trends/correlate/).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在[虚假相关](http://www.tylervigen.com/discover)中查看更多奇怪和虚假的发现，或使用[Google correlate](https://www.google.com/trends/correlate/)等工具自行发现。
- en: 'Several methods can be used to avoid "overfitting" the data:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用几种方法来避免“过拟合”数据：
- en: Try to find the simplest possible hypothesis
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试寻找最简单的假设
- en: '[Regularization](https://en.wikipedia.org/wiki/Regularization_%28mathematics%29)
    (adding a penalty for complexity)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[正则化](https://en.wikipedia.org/wiki/Regularization_%28mathematics%29)（对复杂性进行惩罚）'
- en: '[Randomization Testing](/2014/02/3-ways-to-test-accuracy-your-predictive-models.html)
    (randomize the class variable, try your method on this data - if it find the same
    strong results, something is wrong)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[随机化测试](/2014/02/3-ways-to-test-accuracy-your-predictive-models.html)（随机化类别变量，尝试在此数据上应用你的方法——如果找到相同的强结果，说明存在问题）'
- en: Nested cross-validation  (do feature selection on one level, then run entire
    method in cross-validation on outer level)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌套交叉验证（在一个层级上进行特征选择，然后在外层级上进行整个方法的交叉验证）
- en: Adjusting the [False Discovery Rate](https://en.wikipedia.org/wiki/False_discovery_rate)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整[假发现率](https://en.wikipedia.org/wiki/False_discovery_rate)
- en: Using the [reusable holdout method](/2015/08/feldman-avoid-overfitting-holdout-adaptive-data-analysis.html)
    - a breakthrough approach proposed in 2015
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用[可重用的保留方法](/2015/08/feldman-avoid-overfitting-holdout-adaptive-data-analysis.html)——2015年提出的突破性方法
- en: Good data science is on the leading edge of scientific understanding of the
    world, and it is data scientists responsibility to avoid overfitting data and
    educate the public and the media on the dangers of bad data analysis.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 优秀的数据科学处于对世界科学理解的前沿，数据科学家有责任避免数据过拟合，并向公众和媒体普及糟糕数据分析的危险。
- en: 'See also:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 另见：
- en: '[4 Reasons Your Machine Learning Model is Wrong (and How to Fix It)](/2016/12/4-reasons-machine-learning-model-wrong.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你的机器学习模型为何错误的4个原因（以及如何修正）](/2016/12/4-reasons-machine-learning-model-wrong.html)'
- en: '[When Good Advice Goes Bad](/2016/03/when-good-advice-goes-bad.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[当好的建议变坏](/2016/03/when-good-advice-goes-bad.html)'
- en: '[The Cardinal Sin of Data Mining and Data Science: Overfitting](/2014/06/cardinal-sin-data-mining-data-science.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据挖掘和数据科学的根本罪过：过拟合](/2014/06/cardinal-sin-data-mining-data-science.html)'
- en: '[Big Idea To Avoid Overfitting: Reusable Holdout to Preserve Validity in Adaptive
    Data Analysis](/2015/08/feldman-avoid-overfitting-holdout-adaptive-data-analysis.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[避免过拟合的大主意：可重用的保留集以保持适应性数据分析中的有效性](/2015/08/feldman-avoid-overfitting-holdout-adaptive-data-analysis.html)'
- en: '[Overcoming Overfitting with the reusable holdout: Preserving validity in adaptive
    data analysis](/2015/08/reusable-holdout-preserving-validity-adaptive-data-analysis.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过可重用保留集克服过拟合：保持适应性数据分析中的有效性](/2015/08/reusable-holdout-preserving-validity-adaptive-data-analysis.html)'
- en: '[11 Clever Methods of Overfitting and how to avoid them](/2015/01/clever-methods-overfitting-avoid.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11种聪明的过拟合方法及其避免方法](/2015/01/clever-methods-overfitting-avoid.html)'
- en: '* * *'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Q8\. What is the curse of dimensionality?
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q8\. 什么是维度灾难？
- en: '[**Prasad Pore**](/author/prasad-pore) **answers:**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[**普拉萨德·波雷**](/author/prasad-pore) **回答：**'
- en: '"As the number of features or dimensions grows, the amount of data we need
    to generalize accurately grows exponentially."'
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '"随着特征或维度数量的增加，我们需要准确泛化的数据量呈指数增长。"'
- en: ''
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- [Charles Isbell](http://ccsubs.com/video/yt%3AQZ0DtNFdDko/curse-of-dimensionality-georgia-tech-machine-learning/subtitles?lang=en),
    Professor and Senior Associate Dean, School of Interactive Computing, Georgia
    Tech'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- [查尔斯·伊斯贝尔](http://ccsubs.com/video/yt%3AQZ0DtNFdDko/curse-of-dimensionality-georgia-tech-machine-learning/subtitles?lang=en)，乔治亚理工学院互动计算学院教授及高级副院长'
- en: Let’s take an example below. Fig. 1 (a) shows 10 data points in one dimension
    i.e. there is only one feature in the data set. It can be easily represented on
    a line with only 10 values, x=1, 2, 3... 10.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个例子。图1（a）显示了一个维度中的10个数据点，即数据集中只有一个特征。它可以用只有10个值的线轻松表示，x=1, 2, 3... 10。
- en: But if we add one more feature, same data will be represented in 2 dimensions
    (Fig.1 (b)) causing increase in dimension space to 10*10 =100\. And again if we
    add 3rd feature, dimension space will increase to 10*10*10 = 1000\. As dimensions
    grows, dimensions space increases exponentially.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们再添加一个特征，相同的数据将在2维中表示（图1（b）），导致维度空间增加到10*10 =100。再添加第3个特征，维度空间将增加到10*10*10
    = 1000。随着维度的增加，维度空间呈指数增长。
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![n-dimensional space comparison](../Images/42858fce17f95d0c85abcb15027e0faa.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![n维空间比较](../Images/42858fce17f95d0c85abcb15027e0faa.png)'
- en: This exponential growth in data causes high sparsity in the data set and unnecessarily
    increases storage space and processing time for the particular modelling algorithm.
    Think of image recognition problem of high resolution images 1280 × 720 = 921,600
    pixels i.e. 921600 dimensions. OMG. And that’s why it’s called **[Curse of Dimensionality](/?s=curse+of+dimensionality)**.
    Value added by additional dimension is much smaller compared to overhead it adds
    to the algorithm.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的指数增长导致数据集的高度稀疏，并不必要地增加了特定建模算法的存储空间和处理时间。想象一下高分辨率图像的图像识别问题，1280 × 720 = 921,600
    像素，即921600维。天哪。这就是为什么它被称为**[维度灾难](/?s=curse+of+dimensionality)**。额外维度带来的价值比其对算法增加的开销要小得多。
- en: Bottom line is, the data that can be represented using 10 space units of one
    true dimension, needs 1000 space units after adding 2 more dimensions just because
    we observed these dimensions during the experiment. The true dimension means the
    dimension which accurately generalize the data and observed dimensions means whatever
    other dimensions we consider in dataset which may or may not contribute to accurately
    generalize the data.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 总而言之，原本可以用10个空间单位表示的数据，在添加了2个维度后，需要1000个空间单位，仅仅因为我们在实验中观察到了这些维度。真正的维度是指准确泛化数据的维度，观察到的维度是指我们在数据集中考虑的其他维度，这些维度可能对准确泛化数据有或没有贡献。
- en: '* * *'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Q9\. How can you determine which features are the most important in your model?
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q9\. 如何确定模型中哪些特征最为重要？
- en: '[**Thuy Pham**](/author/thuy-pham) **answers:**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[**翠玉·范**](/author/thuy-pham) **回答：**'
- en: In applied machine learning, success depends significantly on the quality of
    data representation (features).  Highly correlated features can make learning/sorting
    steps in the classification module easy. Conversely if label classes are a very
    complex function of the features, it can be impossible to build a good model [Dom
    2012]. Thus a so-called **[feature engineering](/tags/feature-engineering)**,
    a process of transforming data into features that are most relevant to the problem,
    is often needed.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用机器学习中，成功在很大程度上依赖于数据表示（特征）的质量。高度相关的特征可以使分类模块中的学习/排序步骤变得容易。相反，如果标签类别是特征的非常复杂的函数，可能很难建立一个良好的模型
    [Dom 2012]。因此，通常需要所谓的**[特征工程](/tags/feature-engineering)**，即将数据转化为与问题最相关的特征的过程。
- en: A [**feature selection**](/tag/feature-selection) scheme often involves techniques
    to automatically select salient features from a large exploratory feature pool.
    Redundant and irrelevant features are well known to cause poor accuracy so discarding
    these features should be the first task. The relevance is often scored using mutual
    information calculation. Furthermore, input features should thus offer a high
    level of discrimination between classes. The separability of features can be measured
    by distance  or variance ratio between classes. One recent work [Pham 2016] proposed
    a systematic voting based feature selection that is a data-driven approach incorporating
    above criteria. This can be used as a common framework for a wide class of problems.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[**特征选择**](/tag/feature-selection)方案通常涉及自动从大规模探索性特征池中选择显著特征的技术。冗余和无关的特征已知会导致准确性较差，因此丢弃这些特征应当是首要任务。相关性通常通过互信息计算进行评分。此外，输入特征应提供较高的类别区分度。特征的可分离性可以通过类别之间的距离或方差比来测量。最近的工作
    [Pham 2016] 提出了基于系统投票的特征选择方法，这是一种结合上述标准的数据驱动方法。这可以作为广泛类别问题的通用框架。
- en: '![Feature selection approach](../Images/52b9b5c9eb46050c3e235ecf3d98c8f6.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![特征选择方法](../Images/52b9b5c9eb46050c3e235ecf3d98c8f6.png)'
- en: A data-driven feature selection approach incorporating several saliency criteria
    [Pham 2016].
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一种数据驱动的特征选择方法，结合了几种显著性标准 [Pham 2016]。
- en: Another approach is penalizing on the features that are not very important (e.g.,
    yield a high error metric) when using regularization  methods like Lasso or Ridge.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是对那些不太重要的特征（例如，导致高错误度量的特征）进行惩罚，这些方法包括 Lasso 或 Ridge 等正则化方法。
- en: 'References:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[Dom 2012] P. Domingos. A few useful things to know about machine learning.
    *Communications of the ACM*, 55(10):78–87, 2012\. 2.4'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Dom 2012] P. Domingos. 关于机器学习的一些有用知识。*ACM 通讯*，55(10):78–87, 2012。 2.4'
- en: '[Pham 2016] T. T. Pham, C. Thamrin, P. D. Robinson, and P. H. W. Leong. Respiratory
    artefact removal in forced oscillation measurements: A machine learning approach.
    *Biomedical Engineering, IEEE Transactions on*, accepted, 2016.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Pham 2016] T. T. Pham, C. Thamrin, P. D. Robinson, 和 P. H. W. Leong. 强迫振荡测量中的呼吸伪影去除：一种机器学习方法。*生物医学工程，IEEE
    汇刊*，已接受，2016。'
- en: '* * *'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Q10\. When can parallelism make your algorithms run faster? When could it make
    your algorithms run slower?
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q10\. 何时并行化能使你的算法运行得更快？何时可能使你的算法运行得更慢？
- en: '[**Anmol Rajpurohit**](https://twitter.com/hey_anmol) **answers:**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Anmol Rajpurohit**](https://twitter.com/hey_anmol) **回答：**'
- en: Parallelism is a good idea when the task can be divided into sub-tasks that
    can be executed independent of each other without communication or shared resources.
    Even then, efficient implementation is key to achieving the benefits of parallelization.
    In real-life, most of the programs have some sections that need to be executed
    in serialized fashion, and the parallelizable sub-tasks need some kind of synchronization
    or data transfer. Thus, it is hard to predict whether parallelization will actually
    make the [**algorithm**](/tag/algorithms) run faster (than the serialized approach).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务可以分解为可以独立执行而无需通信或共享资源的子任务时，并行计算是一个好主意。即便如此，效率的实现是实现并行化好处的关键。在实际应用中，大多数程序有些部分需要以串行方式执行，并且并行可执行的子任务需要某种同步或数据传输。因此，很难预测并行化是否真的能使[**算法**](/tag/algorithms)比串行方法运行得更快。
- en: Parallelism would always have overhead compared to the compute cycles required
    to complete the task sequentially. At the minimum, this overhead will comprise
    of dividing the task into sub-tasks and compiling together the results of sub-tasks.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于顺序完成任务所需的计算周期，并行计算总是会有开销。至少，这些开销包括将任务分解为子任务和汇总子任务结果。
- en: '**The performance of parallelism against sequential computing is largely determined
    by how the time consumed by this overhead compares to the time saved due to parallelization.**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**并行计算相对于顺序计算的性能主要取决于这些开销所消耗的时间与并行化带来的时间节省的比较。**'
- en: 'Note: The overhead associated with parallelism is not just limited to the run-time
    of code, but also includes the extra time required for coding and debugging (parallelism
    versus sequential code).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：并行化带来的开销不仅限于代码的运行时间，还包括编码和调试所需的额外时间（并行化与顺序代码）。
- en: 'A widely-known theoretical approach to assessing the benefit of parallelization
    is Amdahl’s law, which gives the following formula to measure the speedup of running
    sub-tasks in parallel (over different processors) versus running them sequentially
    (on a single processor):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 评估并行化收益的一个广为人知的理论方法是阿姆达尔定律，它提供了以下公式来测量并行运行子任务（在不同处理器上）与顺序运行（在单一处理器上）相比的加速：
- en: '![](../Images/9ffab2e095120202db6dca9392f1d551.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ffab2e095120202db6dca9392f1d551.png)'
- en: 'where:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*S**latency*is the theoretical speedup of the execution of the whole task;'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*S**延迟*是整个任务执行的理论加速；'
- en: '*s* is the speedup of the part of the task that benefits from improved system
    resources;'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*s* 是受益于改进系统资源的任务部分的加速；'
- en: '*p *is the proportion of execution time that the part benefiting from improved
    resources originally occupied.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p* 是受益于改进资源的部分在原始执行时间中所占的比例。'
- en: 'To understand the implication of Amdahl’s Law, look at the following figure
    that illustrates the theoretical speedup against an increasing number of processor
    cores, for tasks with different level of achievable parallelization:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解阿姆达尔定律的含义，请查看以下图示，该图展示了针对不同水平的可并行化任务，理论上随着处理器核心数量的增加而加速的情况：
- en: '![Speedup by number of cores](../Images/cc5d074b8a3261a73a6272f99642143c.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![核心数量带来的加速](../Images/cc5d074b8a3261a73a6272f99642143c.png)'
- en: It is important to note that not every program can be effectively parallelized.
    Rather, very few programs will scale with perfect speedups because of the limitations
    due to sequential portions, inter-communication costs, etc. Usually, large data
    sets form a compelling case for parallelization. However, it should not be assumed
    that parallelization would lead to performance benefits. Rather, the performance
    of parallelism and sequential should be compared on a sub-set of the problem,
    before investing effort into parallelization.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，并不是每个程序都可以有效地并行化。实际上，由于顺序部分、通讯成本等限制，很少有程序能实现完美的加速。通常，大数据集构成了并行化的一个有力理由。然而，不应假设并行化会带来性能上的好处。应该在投入并行化努力之前，将并行化和顺序执行的性能在问题的子集上进行比较。
- en: '* * *'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Q11\. What is the idea behind ensemble learning?
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q11. 集成学习背后的思想是什么？
- en: '[**Prasad Pore**](/author/prasad-pore) **answers:**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Prasad Pore**](/author/prasad-pore) **回答：**'
- en: '"In [statistics](https://en.wikipedia.org/wiki/Statistics) and [machine learning](https://en.wikipedia.org/wiki/Machine_learning), **ensemble
    methods** use multiple learning algorithms to obtain better [predictive performance](https://en.wikipedia.org/wiki/Predictive_inference) than
    could be obtained from any of the constituent learning algorithms alone."'
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “在[统计学](https://en.wikipedia.org/wiki/Statistics)和[机器学习](https://en.wikipedia.org/wiki/Machine_learning)中，**集成方法**使用多种学习算法来获得比任何单一学习算法更好的[预测性能](https://en.wikipedia.org/wiki/Predictive_inference)。”
- en: ''
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – [Wikipedia](https://en.wikipedia.org/wiki/Ensemble_learning).
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: – [维基百科](https://en.wikipedia.org/wiki/Ensemble_learning)。
- en: Imagine you are playing the game “Who wants to be millionaire?” and reached
    up to last question of 1 million dollars. You have no clue about the question,
    but you have audience poll and phone a friend life lines. Thank God. At this stage
    you don’t want to take any risk, so what will you do to get sure-shot right answer
    to become millionaire?
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你在玩“谁想成为百万富翁？”并且达到了最后一个百万美元的问题。你对这个问题毫无头绪，但你还有观众投票和电话朋友的救生圈。谢天谢地。在这个阶段，你不想冒任何风险，那么你会怎么做以确保得到正确答案，成为百万富翁呢？
- en: You will use both life lines, isn’t it? Let’s say 70% audience is saying right
    answer is D and your friend is also saying the right answer is D with 90% confidence
    because he is an expert in the area of the question. Use of both life lines gives
    you  an average 80% confidence that D is correct and gets you closer to becoming
    a millionaire.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你会使用两个生命线，对吗？假设70%的观众说正确答案是D，而你的朋友也以90%的信心说正确答案是D，因为他是该问题领域的专家。使用两个生命线可以给你80%的平均信心，认为D是正确的，并使你更接近成为百万富翁。
- en: This is the approach of [**ensemble methods**](/tag/ensemble-methods).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 [**集成方法**](/tag/ensemble-methods) 的方法。
- en: The famous [Netflix Prize](https://en.wikipedia.org/wiki/Netflix_Prize) competition
    took almost 3 years before the goal of 10% improvement [was reached](/news/2009/n14/1i.html).
     The winners used gradient boosted decision trees to combine over [500 models](http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 著名的 [Netflix 奖](https://en.wikipedia.org/wiki/Netflix_Prize) 竞赛花费了近3年时间才实现10%的改进
    [目标](/news/2009/n14/1i.html)。获胜者使用了梯度提升决策树来结合超过 [500 个模型](http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/)。
- en: In ensemble methods, more diverse the models used, more robust will be the ultimate
    result.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在集成方法中，使用的模型越多样，最终结果的稳健性就会越高。
- en: 'Different models used in ensemble improves overall variance from difference
    in population, difference in hypothesis generated, difference in algorithms used
    and difference in parametrization. There are main 3 widely used ensembles techniques:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 集成中使用的不同模型通过人口差异、生成的假设差异、使用的算法差异以及参数化差异来改善整体方差。主要有3种广泛使用的集成技术：
- en: Bagging
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 装袋
- en: '[Boosting](/tag/boosting)'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[提升](/tag/boosting)'
- en: Stacking
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 堆叠
- en: So if you have different models built for same data and same response variable,
    you can use one of the above methods to build ensemble model. As every model used
    in the ensemble has its own performance measures, some of the models may perform
    better than ultimate ensemble model and some of them may perform poorer than or
    equal to ensemble model. But overall the ensemble methods will improve overall
    accuracy and stability of the model, although at the expense of model understandability.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你为相同的数据和相同的响应变量构建了不同的模型，你可以使用上述方法之一来构建集成模型。由于集成中使用的每个模型都有其自身的性能指标，因此一些模型的表现可能会优于最终的集成模型，而一些模型的表现可能会差于或与集成模型相当。但总体而言，集成方法将提高模型的整体准确性和稳定性，尽管这会以模型可理解性为代价。
- en: 'For more on ensemble methods see:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于集成方法的信息请参见：
- en: '[Ensemble Methods: Elegant Techniques to Produce Improved Machine Learning
    Results](/2016/02/ensemble-methods-techniques-produce-improved-machine-learning.html)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集成方法：优雅的技巧以提高机器学习结果](/2016/02/ensemble-methods-techniques-produce-improved-machine-learning.html)'
- en: '[Data Science Basics: An Introduction to Ensemble Learners](/2016/11/data-science-basics-intro-ensemble-learners.html)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学基础：集成学习者简介](/2016/11/data-science-basics-intro-ensemble-learners.html)'
- en: '* * *'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Q12\. In unsupervised learning, if a ground truth about a dataset is unknown,
    how can we determine the most useful number of clusters to be?
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q12. 在无监督学习中，如果数据集的真实情况未知，我们如何确定最有用的聚类数量？
- en: '[**Matthew Mayo**](/author/matt-mayo) **answers:**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[**马修·梅奥**](/author/matt-mayo) **回答：**'
- en: With **supervised** learning, the number of classes in a particular set of data
    is known outright, since each data instance in labeled as a member of a particular
    existent class. In the worst case, we can scan the class attribute and count up
    the number of unique entries which exist.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **有监督** 学习中，数据集中类的数量是明确知道的，因为每个数据实例都标记为某个现存类别的成员。在最坏的情况下，我们可以扫描类别属性并计算存在的唯一条目数量。
- en: With **[unsupervised](/tag/unsupervised-learning)** learning, the idea of class
    attributes and explicit class membership does not exist; in fact, one of the dominant
    forms of unsupervised learning -- data clustering -- aims to approximate class
    membership by minimizing interclass instance similarity and maximizing intraclass
    similarity. A major drawback with clustering can be the requirement to provide
    the number of classes which exist in the unlabeled dataset at the onset, in some
    form or another. If we are lucky, we may know the data’s **ground truth** -- the
    actual number of classes -- beforehand. However, this is not always the case,
    for numerous reasons, one of which being that there may actually be no defined
    number of classes (and hence, clusters) in the data, with the whole point of the
    unsupervised learning task being to survey the data and attempt to impose some
    meaningful structure of optimal cluster and class numbers upon it.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在**[无监督学习](/tag/unsupervised-learning)**中，类属性和明确的类成员关系并不存在；事实上，无监督学习的一种主要形式——数据聚类——旨在通过最小化类间实例相似性和最大化类内相似性来近似类成员关系。聚类的一个主要缺点是需要在开始时以某种形式提供未标记数据集中存在的类数。如果幸运的话，我们可能会提前知道数据的**真实情况**——实际的类数。然而，这并不总是如此，原因有很多，其中之一是数据中可能根本没有定义的类数（从而没有定义的簇），无监督学习任务的整个重点就是调查数据并尝试强加某些有意义的最优簇和类数结构。
- en: 'Without knowing the ground truth of a dataset, then, how do we know what the
    optimal number of data clusters are? As one may expect, there are actually numerous
    methods to go about answering this question. We will have a look at 2 particular
    popular methods for attempting to answer this question: the elbow method and the
    silhouette method.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在不知道数据集的真实情况的情况下，我们如何知道数据簇的最优数量？正如预期的那样，实际上有许多方法可以回答这个问题。我们将查看两种特别流行的方法来尝试回答这个问题：肘部法则和轮廓法。
- en: '**The Elbow Method**'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**肘部法则**'
- en: The elbow method is often the best place to state, and is especially useful
    due to its ease of explanation and verification via visualization. The elbow method
    is interested in explaining variance as a function of cluster numbers (the *k*
    in *k*-means). By plotting the percentage of variance explained against *k*, the
    first *N* clusters should add significant information, explaining variance; yet,
    some eventual value of *k* will result in a much less significant gain in information,
    and it is at this point that the graph will provide a noticeable angle. This angle
    will be the optimal number of clusters, from the perspective of the elbow method,
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 肘部法则通常是一个最佳的起点，尤其因为它易于解释和通过可视化进行验证。肘部法则旨在解释方差作为簇数（*k* 在 *k*-均值中的那个 *k*）的函数。通过绘制解释的方差百分比与
    *k* 的关系，前 *N* 个簇应该提供显著的信息，解释方差；然而，某个最终的 *k* 值将导致信息增益显著减少，此时图表将呈现出明显的角度。这个角度将是肘部法则视角下的最优簇数。
- en: It should be self-evident that, in order to plot this variance against varying
    numbers of clusters, varying numbers of clusters must be tested. Successive complete
    iterations of the clustering method must be undertaken, after which the results
    can be plotted and compared.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，为了将方差绘制与不同簇数的关系，必须测试不同数量的簇。必须进行连续的完整聚类方法迭代，然后可以绘制和比较结果。
- en: '![Elbow method](../Images/3edaf0c9413aad166acbf351aa84e46f.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![肘部法则](../Images/3edaf0c9413aad166acbf351aa84e46f.png)'
- en: '[Image source](http://elf11.github.io/2015/08/18/Kmeans-analysis.html).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[图片来源](http://elf11.github.io/2015/08/18/Kmeans-analysis.html)。'
- en: '**The Silhouette Method**'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**轮廓法**'
- en: The silhouette method measures the similarity of an object to its own cluster
    -- called cohesion -- when compared to other clusters -- called separation. The
    silhouette value is the means for this comparison, which is a value of the range
    [-1, 1]; a value close to 1 indicates a close relationship with objects in its
    own cluster, while a value close to -1 indicates the opposite. A clustered set
    of data in a model producing mostly high silhouette values is likely an acceptable
    and appropriate model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓法衡量对象与其自身簇的相似性——称为凝聚度——与其他簇的相似性——称为分离度。轮廓值是这种比较的手段，其范围为[-1, 1]；值接近1表示与自身簇中的对象关系紧密，而值接近-1则表示相反。在模型中产生大多数高轮廓值的聚类数据集可能是一个可接受且合适的模型。
- en: '![Silhouette method](../Images/6e0efa5ccc574fd54efbf33c7defd138.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![轮廓法](../Images/6e0efa5ccc574fd54efbf33c7defd138.png)'
- en: '[Image source](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[图片来源](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)。'
- en: Read more on the silhouette method [here](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html).
    Find the specifics on computing a silhouette value [here](https://en.wikipedia.org/wiki/Silhouette_(clustering)).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 了解轮廓法的更多信息请[点击这里](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)。关于计算轮廓值的具体内容请见[这里](https://en.wikipedia.org/wiki/Silhouette_(clustering))。
- en: '**Related:**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[17 More Must-Know Data Science Interview Questions and Answers](/2017/02/17-data-science-interview-questions-answers.html)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17个更多必须知道的数据科学面试问题及答案](/2017/02/17-data-science-interview-questions-answers.html)'
- en: '[17 More Must-Know Data Science Interview Questions and Answers, part 3](/2017/03/17-data-science-interview-questions-answers-part3.html)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17个更多必须知道的数据科学面试问题及答案，第3部分](/2017/03/17-data-science-interview-questions-answers-part3.html)'
- en: '[21 Must-Know Data Science Interview Questions and Answers](/2016/02/21-data-science-interview-questions-answers.html)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21个必须知道的数据科学面试问题及答案](/2016/02/21-data-science-interview-questions-answers.html)'
- en: '[21 Must-Know Data Science Interview Questions and Answers, part 2](/2016/02/21-data-science-interview-questions-answers-part2.html)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21个必须知道的数据科学面试问题及答案，第2部分](/2016/02/21-data-science-interview-questions-answers-part2.html)'
- en: More On This Topic
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[20 Questions (with Answers) to Detect Fake Data Scientists: ChatGPT…](https://www.kdnuggets.com/2023/01/20-questions-detect-fake-data-scientists-chatgpt-1.html)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20个问题（带答案）检测虚假数据科学家：ChatGPT…](https://www.kdnuggets.com/2023/01/20-questions-detect-fake-data-scientists-chatgpt-1.html)'
- en: '[20 Questions (with Answers) to Detect Fake Data Scientists: ChatGPT…](https://www.kdnuggets.com/2023/02/20-questions-detect-fake-data-scientists-chatgpt-2.html)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20个问题（带答案）检测虚假数据科学家：ChatGPT…](https://www.kdnuggets.com/2023/02/20-questions-detect-fake-data-scientists-chatgpt-2.html)'
- en: '[7 Data Analytics Interview Questions & Answers](https://www.kdnuggets.com/2022/09/7-data-analytics-interview-questions-answers.html)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7个数据分析面试问题与答案](https://www.kdnuggets.com/2022/09/7-data-analytics-interview-questions-answers.html)'
- en: '[5 Python Interview Questions & Answers](https://www.kdnuggets.com/2022/09/5-python-interview-questions-answers.html)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5个Python面试问题与答案](https://www.kdnuggets.com/2022/09/5-python-interview-questions-answers.html)'
- en: '[3 More SQL Aggregate Function Interview Questions for Data Science](https://www.kdnuggets.com/2023/01/3-sql-aggregate-function-interview-questions-data-science.html)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3个更多SQL聚合函数数据科学面试问题](https://www.kdnuggets.com/2023/01/3-sql-aggregate-function-interview-questions-data-science.html)'
- en: '[Data Science Interview Guide - Part 2: Interview Resources](https://www.kdnuggets.com/2022/04/data-science-interview-guide-part-2-interview-resources.html)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学面试指南 - 第2部分：面试资源](https://www.kdnuggets.com/2022/04/data-science-interview-guide-part-2-interview-resources.html)'
