["```py\npip install scikit-learn\n```", "```py\nfrom sklearn.datasets import load_wine \n X, y = load_wine(return_X_y=True)\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42\n)\n```", "```py\nmodel_lr = LogisticRegression()\nmodel_lr.fit(X_train, y_train)\ny_pred_lr = model_lr.predict(X_test)\n\nprint(\"Accuracy Score: \", accuracy_score(y_pred_lr, y_test))\nprint(classification_report(y_pred_lr, y_test))\n```", "```py\nAccuracy Score:  0.9830508474576272\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.95      0.98        21\n           1       0.96      1.00      0.98        23\n           2       1.00      1.00      1.00        15\n\n    accuracy                           0.98        59\n   macro avg       0.99      0.98      0.98        59\nweighted avg       0.98      0.98      0.98        59\n```", "```py\nmodel_knn = KNeighborsClassifier(n_neighbors=1)\nmodel_knn.fit(X_train, y_train)\ny_pred_knn = model_knn.predict(X_test)\n\nprint(\"Accuracy Score:\", accuracy_score(y_pred_knn, y_test))\nprint(classification_report(y_pred_knn, y_test))\n```", "```py\nAccuracy Score: 0.7796610169491526\n\n              precision    recall  f1-score   support\n\n           0       0.90      0.78      0.84        23\n           1       0.75      0.82      0.78        22\n           2       0.67      0.71      0.69        14\n\n    accuracy                           0.78        59\n   macro avg       0.77      0.77      0.77        59\nweighted avg       0.79      0.78      0.78        59\n```", "```py\nAccuracy Score: 0.6949152542372882\n\n              precision    recall  f1-score   support\n\n           0       0.90      0.72      0.80        25\n           1       0.75      0.69      0.72        26\n           2       0.33      0.62      0.43         8\n\n    accuracy                           0.69        59\n   macro avg       0.66      0.68      0.65        59\nweighted avg       0.76      0.69      0.72        59\n```", "```py\nfrom sklearn.naive_bayes import GaussianNB\n\nmodel_nb = GaussianNB()\nmodel_nb.fit(X_train, y_train)\ny_pred_nb = model_nb.predict(X_test)\n\nprint(\"Accuracy Score:\", accuracy_score(y_pred_nb, y_test))\nprint(classification_report(y_pred_nb, y_test))\n```", "```py\nAccuracy Score: 1.0\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        20\n           1       1.00      1.00      1.00        24\n           2       1.00      1.00      1.00        15\n\n    accuracy                           1.00        59\n   macro avg       1.00      1.00      1.00        59\nweighted avg       1.00      1.00      1.00        59\n```", "```py\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel_dtclassifier = DecisionTreeClassifier()\nmodel_dtclassifier.fit(X_train, y_train)\ny_pred_dtclassifier = model_dtclassifier.predict(X_test)\n\nprint(\"Accuracy Score:\", accuracy_score(y_pred_dtclassifier, y_test))\nprint(classification_report(y_pred_dtclassifier, y_test))\n```", "```py\nAccuracy Score: 0.9661016949152542\n\n              precision    recall  f1-score   support\n\n           0       0.95      0.95      0.95        20\n           1       1.00      0.96      0.98        25\n           2       0.93      1.00      0.97        14\n\n    accuracy                           0.97        59\n   macro avg       0.96      0.97      0.97        59\nweighted avg       0.97      0.97      0.97        59 \n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\ndef get_best_parameters():\n\n    params = {\n        \"n_estimators\": [10, 50, 100],\n        \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n        \"max_depth\": [5, 10, 20, 50],\n        \"min_samples_split\": [2, 4, 6],\n        \"min_samples_leaf\": [2, 4, 6],\n        \"bootstrap\": [True, False],\n    }\n\n    model_rfclassifier = RandomForestClassifier(random_state=42)\n\n    rf_randomsearch = RandomizedSearchCV(\n        estimator=model_rfclassifier,\n        param_distributions=params,\n        n_iter=5,\n        cv=3,\n        verbose=2,\n        random_state=42,\n    )\n\n    rf_randomsearch.fit(X_train, y_train)\n\n    best_parameters = rf_randomsearch.best_params_\n\n    print(\"Best Parameters:\", best_parameters)\n\n    return best_parameters\n\nparameters_rfclassifier = get_best_parameters()\n\nmodel_rfclassifier = RandomForestClassifier(\n    **parameters_rfclassifier, random_state=42\n)\n\nmodel_rfclassifier.fit(X_train, y_train)\n\ny_pred_rfclassifier = model_rfclassifier.predict(X_test)\n\nprint(\"Accuracy Score:\", accuracy_score(y_pred_rfclassifier, y_test))\nprint(classification_report(y_pred_rfclassifier, y_test))\n```", "```py\n Best Parameters: {'n_estimators': 100, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 5, 'bootstrap': True}\nAccuracy Score: 0.9830508474576272\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.95      0.98        21\n           1       0.96      1.00      0.98        23\n           2       1.00      1.00      1.00        15\n\n    accuracy                           0.98        59\n   macro avg       0.99      0.98      0.98        59\nweighted avg       0.98      0.98      0.98        59\n```"]