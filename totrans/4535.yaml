- en: Fighting Overfitting in Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的过拟合问题
- en: 原文：[https://www.kdnuggets.com/2019/12/fighting-overfitting-deep-learning.html](https://www.kdnuggets.com/2019/12/fighting-overfitting-deep-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/12/fighting-overfitting-deep-learning.html](https://www.kdnuggets.com/2019/12/fighting-overfitting-deep-learning.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [ActiveWizards](https://activewizards.com/)**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [ActiveWizards](https://activewizards.com/) 提供**'
- en: '![Figure](../Images/ed0cde5a50a91862ae85dce52e2095cb.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/ed0cde5a50a91862ae85dce52e2095cb.png)'
- en: Problem
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题
- en: While training the model, we want to get the best possible result according
    to the chosen metric. And at the same time we want to keep a similar result on
    the new data. The cruel truth is that we can’t get 100% accuracy. And even if
    we did, the result is still not without errors. There are simply too few test
    situations to find them. You may ask, what is the matter?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，我们希望根据选择的指标获得最佳结果。同时，我们还希望在新数据上保持类似的结果。残酷的现实是，我们无法获得100%的准确性。即使我们做到了，结果仍然难免有错误。测试情况太少，难以发现它们。你可能会问，这有什么关系？
- en: There are 2 types of errors: reducible and irreducible. Irreducible errors arise
    due to a lack of data. For example, not only the genre, duration, actors but also
    the mood of the person and the atmosphere while watching the film affects the
    rating. But we can't predict the mood of the person in the future. The other reason
    is the quality of the data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 错误有两种类型：可减少的和不可减少的。不可减少的错误是由于数据不足产生的。例如，除了类型、时长、演员之外，人们的心情和观看电影时的氛围也会影响评分。但我们无法预测未来人的心情。另一个原因是数据质量。
- en: Our goal is to reduce the reducible errors, which in turn are divided into Bias and Variance.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是减少可减少的错误，这些错误又分为偏差和方差。
- en: Bias
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 偏差
- en: Bias arises when we try to describe a complex process with a simple model, and
    as a result an error occurs. For example, it is impossible to describe a nonlinear
    interaction by a linear function.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差出现在我们试图用简单模型描述复杂过程时，结果产生了错误。例如，用线性函数描述非线性互动是不可行的。
- en: Variance
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 方差
- en: Variance describes the robust of the forecast or how much the forecast will
    change when the data changes. Ideally, with small changes in the data, the forecast
    also changes slightly.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 方差描述了预测的稳健性，即当数据变化时预测会变化多少。理想情况下，数据有小的变化时，预测也会有轻微的变化。
- en: '![Figure](../Images/78746f1bfe3c49494312fdd94c8060d0.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/78746f1bfe3c49494312fdd94c8060d0.png)'
- en: The effect of Bias and Variance on the prediction
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差和方差对预测的影响
- en: These errors are interrelated so that a decrease in one leads to an increase
    in the other. This issue is known as the Bias-Variance tradeoff.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些错误是相互关联的，导致一个减少时另一个增加。这个问题被称为偏差-方差权衡。
- en: The more complex (flexible) is the model (better explains the variance of the
    target variable), the less Bias is. But, nevertheless, the more complex is the
    model, the better it adapts to the training data, increasing Variance. At some
    point, the model will begin to find random patterns that are not repeated on new
    data, thereby reducing the ability of the model to generalize and increasing the
    error on the test data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 模型越复杂（灵活），对目标变量的方差解释得越好，偏差就越小。但是，尽管如此，模型越复杂，对训练数据的适应性越强，方差也会增加。在某个时点，模型将开始寻找在新数据上不会重复的随机模式，从而降低模型的泛化能力，并增加测试数据上的错误。
- en: There is an image, describing Bias-Variance tradeoff. The red line is loss function
    (e.g. MSE - mean square error). The blue line is Bias and orange is Variance.
    As you can see, the best solution will be placed somewhere in the lines cross.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有一张图描述了偏差-方差权衡。红线是损失函数（例如 MSE - 均方误差）。蓝线是偏差，橙线是方差。正如你所见，最佳解决方案会出现在这些线交点的某处。
- en: '![Figure](../Images/8ad6f1fff405cfe68a39b0b9affbc63b.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/8ad6f1fff405cfe68a39b0b9affbc63b.png)'
- en: Decomposition of the error function
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 错误函数的分解
- en: You also can recognize that the moment when loss begins to grow is the moment
    when the model is overfitted.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以认识到，当损失开始增加的时刻就是模型过拟合的时刻。
- en: 'As a result: we need to control error, to prevent not only overfitting but
    also underfitting.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是：我们需要控制错误，以防止不仅仅是过拟合，还包括欠拟合。
- en: How can we fix this?
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们如何解决这个问题？
- en: Regularization is a technique that discourages learning a more complex or flexible
    model, so as to avoid the risk of overfitting.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是一种技术，用于避免学习更复杂或灵活的模型，以降低过拟合的风险。
- en: 'Note:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注：
- en: The problem is that model with the best accuracy in train data does not provide
    any guarantees, that the same model will be the best in the test data. As a result,
    we can choose hyperparameters with cross-validation.
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问题是，在训练数据中表现最佳的模型不能保证在测试数据中也是最佳的。因此，我们可以通过交叉验证选择超参数。
- en: Regularization
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正则化
- en: '![Figure](../Images/ef95db2d8a50a5a51c548cc02b25523c.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/ef95db2d8a50a5a51c548cc02b25523c.png)'
- en: 'Or, in  other words:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，换句话说：
- en: '*Cost function = Loss + Regularization term*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*成本函数 = 损失 + 正则化项*'
- en: Where Loss function is usually a function defined on a data point, prediction,
    and label, and measures the penalty. The Cost function is usually more general.
    It might be a sum of loss functions over your training set plus some model complexity
    penalty (regularization term)****.****
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数通常是一个在数据点、预测和标签上定义的函数，并衡量惩罚。成本函数通常更为一般。它可能是训练集上的损失函数的总和加上一些模型复杂性惩罚（正则化项）****。
- en: Often an overfitted model has large weights with different signs. So that in
    total they level each other getting the result. The way we can improve the score
    is to reduce weight.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，过拟合的模型具有不同符号的大权重。这样，它们在总和上互相抵消，得到结果。改善评分的方法是减少权重。
- en: Each neuron can be represented as
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元可以表示为
- en: '![Figure](../Images/f215f4ce311ca65ee2e1082564e19b21.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/f215f4ce311ca65ee2e1082564e19b21.png)'
- en: where f is an activation function, *w* is weight and X is data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 f 是激活函数，*w* 是权重，X 是数据。
- en: The “length” of the coefficient vector can be described by such a concept as
    the norm of the vector. And to control how much we will reduce the length of the
    coefficients (penalty the model for complexity and thereby prevent it from overfitting
    or adjusting too much to the data), we will weigh the norm by multiplying it by λ.
    There are no analytical methods on how to choose λ, so you can choose it by grid-search.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 系数向量的“长度”可以通过向量的范数来描述。为了控制我们将减少系数的长度（惩罚模型的复杂性，从而防止其过拟合或过度调整数据），我们将通过将其乘以 λ 来加权范数。选择
    λ 没有分析方法，因此可以通过网格搜索来选择。
- en: '****L2 norm (L2 regularization, Ridge)****'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '****L2 范数（L2 正则化，岭回归）****'
- en: '![Figure](../Images/b70e3f5187a63ae32403856a7e04deed.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/b70e3f5187a63ae32403856a7e04deed.png)'
- en: If the loss is MSE, then cost function with L2 norm can be solved analytically
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果损失是 MSE，那么具有 L2 范数的成本函数可以通过解析方法求解。
- en: '![Figure](../Images/8ce8dbe63c2dc8a7af0dd7f41e0957ef.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8ce8dbe63c2dc8a7af0dd7f41e0957ef.png)'
- en: There you can see that we just add an eye matrix (ridge) multiplied by λ in
    order to obtain a non-singular matrix and increase the convergence of the problem.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，我们只是添加了一个乘以 λ 的单位矩阵（岭），以获得一个非奇异矩阵，并增加问题的收敛性。
- en: '![Figure](../Images/efacff284bf0087323f5a99baa0c62c2.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/efacff284bf0087323f5a99baa0c62c2.png)'
- en: Turning multicollinear XTX matrix to nonsingular one by adding an eye-matrix.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加单位矩阵，将多重共线性的 XTX 矩阵转换为非奇异矩阵。
- en: In other words, we need to minimize the cost function, so we minimize losses
    and the vector rate (in this case, this is the sum of the squared weights). So,
    the weights will be closer to zero.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们需要最小化成本函数，因此我们最小化损失和向量的速率（在这种情况下，这是权重平方的总和）。因此，权重会更接近零。
- en: 'Let’s calculate:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算：
- en: The w^T is a vector-row of weights.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: w^T 是权重的向量行。
- en: 'Before regularization:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化之前：
- en: '![Figure](../Images/c84979bf97f86f93cd5d251224df7aa9.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/c84979bf97f86f93cd5d251224df7aa9.png)'
- en: Little change in data
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的微小变化
- en: '![Figure](../Images/2fbbd8f9fe43ab51fcc9e8a90c7f91c0.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/2fbbd8f9fe43ab51fcc9e8a90c7f91c0.png)'
- en: After regularization, we will have the same result on test data,
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化后，我们将在测试数据上得到相同的结果，
- en: '![Figure](../Images/6ac186edf099c845a012f3abf9282217.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/6ac186edf099c845a012f3abf9282217.png)'
- en: But, little change in data will cause little change in result
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 但，数据的微小变化会导致结果的微小变化。
- en: '![Figure](../Images/92881580f84db5d3409f297884f479b7.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/92881580f84db5d3409f297884f479b7.png)'
- en: '****Example****'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '****示例****'
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '****L1 norm (L1 regularization, Lasso)****'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '****L1 范数（L1 正则化，Lasso）****'
- en: L1 norm means that we use absolute values of weights but not squared. There
    is no analytical approach to solve this.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: L1 范数意味着我们使用权重的绝对值而不是平方值。没有解析方法来解决这个问题。
- en: '![Figure](../Images/950fc3ae7822523377aaded861a145f0.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/950fc3ae7822523377aaded861a145f0.png)'
- en: This type of regression equates some weights to zero. It is very useful when
    we are trying to compress our model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这种回归将一些权重置为零。在我们尝试压缩模型时，它非常有用。
- en: '****Example****'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '****示例****'
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s take a simple loss function with 2 arguments (B1 and B2) and draw a 3d
    plot.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一个有两个参数（B1 和 B2）的简单损失函数，并绘制一个三维图。
- en: '![Figure](../Images/343a9fd77c2c8aea5d0077dd1c9cb24d.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/343a9fd77c2c8aea5d0077dd1c9cb24d.png)'
- en: Gradient descent in 3-dim with contour representation | [Credits](https://towardsdatascience.com/regularization-in-machine-learning-connecting-the-dots-c6e030bfaddd)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在三维空间中的梯度下降与等高线表示 | [来源](https://towardsdatascience.com/regularization-in-machine-learning-connecting-the-dots-c6e030bfaddd)
- en: As you can see, loss function can have the same values with different arguments.
    Let’s project this on arguments surface, where each point on the same curve will
    have the same function value. This line called the level line.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，损失函数可能在不同参数下具有相同的值。我们将其投影到参数表面上，其中同一曲线上的每一点具有相同的函数值。这条线称为等高线。
- en: Let’s take a Lambda = 1\. L2 regularization for 2 arguments function is B12 +
    B22 that is a circle on the plot. L2 regularization is |B1|+ |B2|, which is diamond
    on the plot.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 设 Lambda = 1。L2 正则化对于两个参数的函数是 B12 + B22，这在图上形成一个圆。L1 正则化是 |B1| + |B2|，这在图上形成一个菱形。
- en: '![Figure](../Images/66b2320cd4352efa1a1178346deb522b.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/66b2320cd4352efa1a1178346deb522b.png)'
- en: L1 and L2 norms with different cost functions | [Credits](https://towardsdatascience.com/regularization-in-machine-learning-connecting-the-dots-c6e030bfaddd)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: L1 和 L2 范数与不同的成本函数 | [来源](https://towardsdatascience.com/regularization-in-machine-learning-connecting-the-dots-c6e030bfaddd)
- en: Now, let’s draw different loss functions and a blue diamond (L1) and black circle
    (L2) regularization terms (where Lambda = 1). The truth is that the cost function
    will be minimum in the interception point of the red circle and the black regularization
    curve for L2 and in the interception of blue diamond with the level curve for
    L1.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制不同的损失函数以及蓝色菱形（L1）和黑色圆圈（L2）正则化项（Lambda = 1）。实际情况是，成本函数在红色圆圈与黑色正则化曲线（L2）的交点处达到最小值，而在蓝色菱形与等高线交点处达到最小值（L1）。
- en: Dropouts
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**Dropout**（随机失活）'
- en: Imagine a simple network. For example, the development team at the hackathon.
    There are more experienced or smarter developers who pull the whole development
    on themselves, while others only help a little. If all this continues, experienced
    developers will become more experienced, and the rest will hardly be trained.
    So it is with neurons.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个简单的网络。例如，黑客马拉松中的开发团队。团队中有经验丰富或更聪明的开发者，他们承担了整个开发的主要工作，而其他人只做一些小帮助。如果这种情况持续下去，经验丰富的开发者会变得更有经验，而其他人则很难得到训练。神经元也是如此。
- en: '![Figure](../Images/e9dc1415db6283be40232cec7c534fbb.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/e9dc1415db6283be40232cec7c534fbb.png)'
- en: Fully Connected Network
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**全连接网络**'
- en: But imagine that we randomly disconnect some part of the developers at each
    iteration of product development. Then the rest have to turn on more and everyone
    learns much better.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 但想象一下，在每次产品开发迭代中，我们随机断开一些开发者的连接。那么其余的开发者不得不更加投入，大家学习得更好。
- en: '![Figure](../Images/3dfdcfb6d929ddbb227c06b64bce4db6.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/3dfdcfb6d929ddbb227c06b64bce4db6.png)'
- en: Network with randomly dropped nodes
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 随机丢失节点的网络
- en: What could go wrong? If you turn off too many neurons, then remaining neurons
    simply will not be able to cope with their work and the result will only get worse.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会发生什么问题？如果你关闭了太多的神经元，那么剩下的神经元可能无法应对它们的工作，结果只会变得更糟。
- en: It can also be thought of as an ensemble technique in machine learning. Remember,
    that the ensemble of strong-learners performs better than a single model as they
    capture more randomness and less prone to overfitting. But ensemble of weak-learners
    more prone to retraining than the original model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以被看作是机器学习中的一种集成技术。记住，强学习者的集成比单一模型表现更好，因为它们能捕捉更多的随机性，且不容易过拟合。但是，弱学习者的集成比原始模型更容易过拟合。
- en: As a result, dropout takes place only with huge neural networks.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，**dropout** 只在大型神经网络中使用。
- en: '****Example ****'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '****示例****'
- en: '[PRE2]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Data augmentation
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据增强
- en: The simplest way to reduce overfitting is to increase the size of the training
    data. In machine learning, it is hard to increase the data amount because of the
    high cost.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 减少过拟合的最简单方法是增加训练数据的规模。在机器学习中，由于成本高昂，增加数据量是困难的。
- en: But, what about image processing? In this case, there are a few ways of increasing
    the size of the training data – rotating the image, flipping, scaling, shifting,
    etc. We also could add images without the target class to learn the network how
    to differentiate target from noise.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，图像处理呢？在这种情况下，有几种方法可以增加训练数据的规模——旋转图像、翻转、缩放、平移等。我们还可以添加没有目标类的图像，以教会网络如何区分目标与噪声。
- en: '****Example****'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '****示例****'
- en: '[PRE3]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '****Original image:****'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/9c61094028e19f432db482e6933ff0a5.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: '****Augmented images:****'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/8249bc4d5b185a0dda76243edf1b8859.png)![Figure](../Images/01dee9199a92378367caad7d25114b0a.png)![Figure](../Images/01dee9199a92378367caad7d25114b0a.png)![Figure](../Images/a3420c9ad50b4452112de6744f1d936e.png)![Figure](../Images/10a73410076a8d297c4f20f864d92a40.png)![Figure](../Images/2202c647c9cf80fbd279a67fadafc005.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: Early stopping
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Early stopping is a kind of cross-validation strategy where we keep one part
    of the training set as the validation set. When we see that the performance on
    the validation set is getting worse, we immediately stop the training on the model.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '****Example****'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: That is all we need for the simplest form of early stopping. Training will stop
    when the chosen performance measure stops improving. To discover the training
    epoch on which training was stopped, the “verbose” argument can be set to 1.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Often, the first sign of no further improvement may not be the best time to
    stop training. This is because the model may coast into a plateau of no improvement
    or even get slightly worse before getting much better.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: We can account for this by adding a delay to the trigger in terms of the number
    of epochs on which we would like to see no improvement. This can be done by setting
    the “patience” argument.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The exact amount of patience will vary between models and problems.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: By default, any change in the performance measure, no matter how fractional,
    will be considered an improvement. You may want to consider an improvement that
    is a specific increment, such as 1 unit for mean squared error or 1% for accuracy.
    This can be specified via the “min_delta” argument.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: EarlyStopping callback can be applied with other callbacks, such as tensorboard.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Learn more: [here](https://keras.io/callbacks/)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to attach Tensorboard and monitor changes manually.  Every
    5 minutes model will be saved into logs dir, so you can always check out to a
    better version.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Neural network architecture
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is well-known that large enough networks of depth 2 can already approximate
    any continuous target function on [0,1]d to arbitrary accuracy (Cybenko,1989;
    Hornik, 1991). On the other hand, it has long been evident that deeper networks
    tend to perform better than shallow ones.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: The [recent analysis](http://proceedings.mlr.press/v49/eldan16.pdf) showed that
    “depth – even if increased by 1 – can be exponentially more valuable than width
    for standard feedforward neural networks”.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: You can think that each new layer extracts a new feature, so that increases
    a non-linearity.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Remember that, increasing the depth means your model is more complex and the
    optimization function may not be able to find the optimal set of weights.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'Image, that you have a neural network with 2 hidden layers of 5 neurons on
    each layer. Height = 2, width = 5\. Let’s add one neuron per layer and calc a
    number of connections: (5+1)*(5+1) = 36 connections. Now, let’s add a new layer
    to the original network and calc connections: 5*5*5 = 125 connections. So, each
    layer will significantly increase the number of connections and execution time.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你有一个具有2层隐藏层的神经网络，每层5个神经元。高度 = 2，宽度 = 5。让我们在每层添加一个神经元并计算连接数：（5+1）*（5+1）=
    36个连接。现在，让我们在原始网络中添加一层并计算连接数：5*5*5 = 125个连接。因此，每层将显著增加连接数和执行时间。
- en: But, at the same time, this comes with the cost of increasing the chance of
    overfitting.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 但与此同时，这也会增加过拟合的机会。
- en: Very wide neural networks are good at memorizing data, so you shouldn't build
    very wide networks too. Try to build as small network, as possible, to solve your
    problem. Remember that the larger and complex the network is, the higher is a
    chance of overfitting.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 非常宽的神经网络擅长记忆数据，因此你不应该构建非常宽的网络。尽量构建尽可能小的网络来解决问题。记住，网络越大越复杂，过拟合的机会就越高。
- en: You can find more [here](https://pdfs.semanticscholar.org/69b4/68459daa2b2e5c096c0cf6da8735dba26a4a.pdf).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[这里](https://pdfs.semanticscholar.org/69b4/68459daa2b2e5c096c0cf6da8735dba26a4a.pdf)找到更多信息。
- en: Transfer learning
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 迁移学习
- en: Why should we always start from scratch? Let’s take pre-trained models' weights
    and just optimize them for our task. The problem is that we haven’t a model pre-trained
    on our data. But we can use a network, pre-trained on a really huge amount of
    data. Then, we use our relatively small data to fine-tune the pre-trained model.
    The common practice is to freeze all layers except the last few from training.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们总是从头开始？我们可以利用预训练模型的权重并仅对其进行优化。问题是我们没有一个在我们数据上预训练的模型。但我们可以使用在大量数据上预训练的网络。然后，我们用相对较小的数据来微调预训练的模型。常见做法是冻结除最后几层以外的所有层。
- en: The main advantage of transfer learning is that it mitigates the problem of
    insufficient training data. As you can remember, this is one of the reasons for
    overfitting.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习的主要优点是它缓解了训练数据不足的问题。正如你所记得的，这是导致过拟合的原因之一。
- en: Transfer learning only works in deep learning if the model features learned
    from the first task are general. It’s very popular to use a pre-trained model
    for image processing ([here](https://github.com/BVLC/caffe/wiki/Model-Zoo)) and
    text processing, e.g. [google word2vec](https://code.google.com/archive/p/word2vec/)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习只有在深度学习中如果模型在第一个任务中学到的特征是通用的才有效。使用预训练模型进行图像处理（[这里](https://github.com/BVLC/caffe/wiki/Model-Zoo)）和文本处理非常流行，例如[google
    word2vec](https://code.google.com/archive/p/word2vec/)。
- en: 'Another benefit is that transfer learning increases productivity and reduce
    training time:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个好处是迁移学习可以提高生产力并减少训练时间：
- en: '![Figure](../Images/86a67d6d2409c24bb3985ea500815371.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/86a67d6d2409c24bb3985ea500815371.png)'
- en: Metrics function
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量函数
- en: Batch normalization
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量归一化
- en: '[Batch normalization ](https://arxiv.org/abs/1502.03167?context=cs)allows us
    to not only work as a regularizer but also reduce training time by increasing
    a learning rate. The problem is that during a training process the distribution
    on each layer is changed. So we need to reduce the learning rate that slows our
    gradient descent optimization. But, if we will apply a normalization for each
    training mini-batch, then we can increase the learning rate and find a minimum
    faster.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[批量归一化](https://arxiv.org/abs/1502.03167?context=cs)不仅可以作为正则化器，还可以通过增加学习率来减少训练时间。问题是，在训练过程中，每一层的分布都会发生变化。因此，我们需要减少学习率，这会减慢梯度下降优化的速度。但是，如果我们对每个训练小批量应用归一化，我们可以增加学习率并更快地找到最小值。'
- en: '****Example****'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '****示例****'
- en: '[PRE8]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Other
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他
- en: 'There are some other less popular methods of fighting the overfitting in deep
    neural networks. It is not necessary that they will work. But if you have tried
    all other approaches and want to experiment with something else, you can read
    more about them here: small [batch size](https://arxiv.org/pdf/1705.08741.pdf), [noise
    in weights](https://www.semanticscholar.org/paper/On-Weight-Noise-Injection-Training-Ho-Leung/1023043d2a5a76a07388c3e17c1284018937dbfc).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他不太流行的深度神经网络防止过拟合的方法。它们并不一定有效。如果你已经尝试过所有其他方法并且想尝试其他东西，你可以在这里阅读更多内容：小[批量大小](https://arxiv.org/pdf/1705.08741.pdf)，[权重噪声](https://www.semanticscholar.org/paper/On-Weight-Noise-Injection-Training-Ho-Leung/1023043d2a5a76a07388c3e17c1284018937dbfc)。
- en: Conclusion
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: Overfitting appears when we have a too complicated model. Our model begins to
    recognize noisy or random relations, that will never appear again in the new data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合出现在我们拥有过于复杂的模型时。我们的模型开始识别噪声或随机关系，这些关系在新数据中不会再出现。
- en: One of the characteristics of this condition is large weights of different signs
    in neurons. There is a direct solution to this issue known as L1 and L2 regularization
    that can be applied to each layer separately.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况的一个特点是神经元中存在不同符号的大权重。一个直接解决方案是 L1 和 L2 正则化，可以分别应用于每一层。
- en: The other way is to apply dropouts to the large neural network or to increase
    a data amount for example by data augmentation. You can also configure an early
    stopping callback, that will detect a moment when the model becomes overfitted.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是对大型神经网络应用 dropout，或者通过数据增强等方法增加数据量。你也可以配置早停回调，以检测模型何时开始过拟合。
- en: Also, try to build such a small neural network, as possible. Choose depth and
    width carefully.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尽量构建尽可能小的神经网络。谨慎选择深度和宽度。
- en: Don't forget that you can always use a pre-trained model and increase model
    productivity. At least, you can apply batch normalization to increase the learning
    rate and decrease overfitting at the same time.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记，你总是可以使用预训练模型来提高模型生产力。至少，你可以应用批量归一化来提高学习速率并同时减少过拟合。
- en: Different combinations of these methods will give you a result and allow to
    solve your task.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法的不同组合将给你一个结果，并帮助你解决任务。
- en: '**[ActiveWizards](https://activewizards.com/)** is a team of data scientists
    and engineers, focused exclusively on data projects (big data, data science, machine
    learning, data visualizations). Areas of core expertise include data science (research,
    machine learning algorithms, visualizations and engineering), data visualizations
    ( d3.js, Tableau and other), big data engineering (Hadoop, Spark, Kafka, Cassandra,
    HBase, MongoDB and other), and data intensive web applications development (RESTful
    APIs, Flask, Django, Meteor).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**[ActiveWizards](https://activewizards.com/)** 是一个专注于数据项目（大数据、数据科学、机器学习、数据可视化）的数据科学家和工程师团队。核心专长领域包括数据科学（研究、机器学习算法、可视化和工程）、数据可视化（d3.js、Tableau
    等）、大数据工程（Hadoop、Spark、Kafka、Cassandra、HBase、MongoDB 等）和数据密集型 Web 应用开发（RESTful
    API、Flask、Django、Meteor）。'
- en: '[Original](https://activewizards.com/blog/fighting-overfitting-in-deep-learning/).
    Reposted with permission.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://activewizards.com/blog/fighting-overfitting-in-deep-learning/)。经授权转载。'
- en: '**Related:**'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Enabling the Deep Learning Revolution](/2019/12/enabling-deep-learning-revolution.html)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[开启深度学习革命](/2019/12/enabling-deep-learning-revolution.html)'
- en: '[Deep Learning for Image Classification with Less Data](/2019/11/deep-learning-image-classification-less-data.html)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用更少数据进行图像分类的深度学习](/2019/11/deep-learning-image-classification-less-data.html)'
- en: '[Popular Deep Learning Courses of 2019](/2019/12/deep-learning-courses.html)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2019 年热门深度学习课程](/2019/12/deep-learning-courses.html)'
- en: '* * *'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前 3 个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的 IT'
- en: '* * *'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Fighting AI with AI Fraud Monitoring for Deepfake Applications](https://www.kdnuggets.com/2023/05/fighting-ai-ai-fraud-monitoring-deepfake-applications.html)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 AI 对抗 AI：深伪应用的欺诈监测](https://www.kdnuggets.com/2023/05/fighting-ai-ai-fraud-monitoring-deepfake-applications.html)'
- en: '[How to Avoid Overfitting](https://www.kdnuggets.com/2022/08/avoid-overfitting.html)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何避免过拟合](https://www.kdnuggets.com/2022/08/avoid-overfitting.html)'
- en: '[KDnuggets News, August 24: Implementing DBSCAN in Python • How to…](https://www.kdnuggets.com/2022/n34.html)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，8 月 24 日：在 Python 中实现 DBSCAN • 如何…](https://www.kdnuggets.com/2022/n34.html)'
- en: '[A Solid Plan for Learning Data Science, Machine Learning, and Deep Learning](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学、机器学习和深度学习的可靠计划](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
- en: '[AI, Analytics, Machine Learning, Data Science, Deep Learning…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能、分析、机器学习、数据科学、深度学习……](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
- en: '[15 Free Machine Learning and Deep Learning Books](https://www.kdnuggets.com/2022/10/15-free-machine-learning-deep-learning-books.html)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15本免费机器学习和深度学习书籍](https://www.kdnuggets.com/2022/10/15-free-machine-learning-deep-learning-books.html)'
