# 8 种创新的 BERT 知识蒸馏论文改变了 NLP 的格局

> 原文：[https://www.kdnuggets.com/2022/09/eight-innovative-bert-knowledge-distillation-papers-changed-nlp-landscape.html](https://www.kdnuggets.com/2022/09/eight-innovative-bert-knowledge-distillation-papers-changed-nlp-landscape.html)

![8 种创新的 BERT 知识蒸馏论文改变了 NLP 的格局](../Images/b30dffc14136aec04f62a6de700c7b16.png)

编辑的图像

文章总结了从众多 BERT 知识蒸馏相关论文中精心挑选出的八篇论文。NLP 模型压缩和加速是一个活跃的研究领域，并在行业中广泛应用，以向终端用户提供低延迟的功能和服务。

直截了当说，BERT 模型用于将词语转换为数字，并使你能够在文本数据上训练机器学习模型。为什么？因为机器学习模型接受的输入是数字而非词语。

![8 种创新的 BERT 知识蒸馏论文改变了 NLP 的格局](../Images/1a1713a423c713923922a2fe5b041bc4.png)

图片来源 [Devlin et al., 2019](https://arxiv.org/pdf/1810.04805.pdf)

# 为什么 BERT 如此受欢迎？

首先，BERT 是一种语言模型，它提升了多个任务的高性能。BERT（双向编码器表示从 Transformers）于 2018 年发布，通过在广泛的 NLP 任务中提供前所未有的结果，引发了机器学习社区的轰动，尤其是在语言理解和问答方面。

BERT 的主要吸引力在于使用 Transformer 的双向训练，Transformer 是一种突出的语言建模注意力模型。但就我的叙述而言，这里有几个使 BERT 更加出色的方面：

+   它是开源的

+   NLP 中掌握语境丰富文本的最佳技术

+   双向特性

所有论文都呈现了 BERT 使用中的特定观点。

# 论文 1

**[DistilBERT，BERT 的蒸馏版本：更小、更快、更便宜、更轻量](https://arxiv.org/abs/1910.01108)**

作者们提出了一种技术，旨在预训练一个较小的通用语言表示模型，称为 DistilBERT，该模型可以在多种任务上进行微调，并且表现出色，类似于其较大的同行。虽然大多数先前的工作研究了利用蒸馏来构建特定任务模型，但我们在预训练阶段利用了知识蒸馏。我们展示了可以将 BERT 模型的大小减少 40%，同时保留 97% 的语言理解能力，并且速度提高 60%。损失函数包含语言建模损失、蒸馏损失和余弦距离损失。使用的数据与原始 BERT 模型使用的数据相同。此外，**DistilBERT** 在八个 16GB V100 GPU 上训练了大约 90 小时。

让我们假设

对于输入 x，教师输出：

![8篇创新性的BERT知识蒸馏论文，改变了NLP领域的格局](../Images/0fcb73519e973f009f6351f7947fabad.png)

[来源](https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f)

学生输出：

![8篇创新性的BERT知识蒸馏论文，改变了NLP领域的格局](../Images/1b386d50f1943ec897adcd7e92b25d21.png)

[来源](https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f)

考虑到softmax及其相关符号，我们稍后会回到这个话题。然而，如果我们希望T和S接近，可以对S应用交叉熵损失，以T作为目标。这就是我们所谓的教师-学生交叉熵损失：

+   蒸馏损失：此损失与典型的知识蒸馏损失相同：

![8篇创新性的BERT知识蒸馏论文，改变了NLP领域的格局](../Images/027fc63e2adf0563b0cfe9ac2751ffa0.png)

[来源](https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f)

+   蒙版语言建模损失（MLM）

+   发现余弦嵌入损失（Lcos）是有益的，它对齐了学生和教师隐藏状态向量的方向。

![8篇创新性的BERT知识蒸馏论文，改变了NLP领域的格局](../Images/561015ea973bf811a23fd20e75429969.png)

T(x)是教师向量输出，S(x)是学生向量输出 [来源](https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f)

**关键要点：** 这是一种在线蒸馏技术，其中教师模型和学生模型进行训练。

# 论文2

**[从BERT中提取知识以进行文本生成](https://arxiv.org/pdf/1911.03829.pdf)**

本文提出了一种通用技术，用于利用预训练语言模型进一步优化文本生成，排除了特定的参数共享、特征提取或使用辅助任务进行增强。他们提出的条件MLM机制利用在大规模语料库上预训练的无监督语言模型，然后调整到监督的序列到序列任务。所提供的蒸馏方法通过仅提供软标签分布间接影响文本生成模型，因此是模型不可知的。关键点如下。

+   BERT训练的MLM目标不是自回归的；它以同时考虑过去和未来上下文的方式进行训练。

+   新颖的C-MLM（条件蒙版语言建模）任务需要额外的条件输入。

![8篇创新性的BERT知识蒸馏论文，改变了NLP领域的格局](../Images/f5f65039e7f35165e21c0e115b0b9c21.png)

从BERT中提取知识以进行文本生成的示意图。 [来源](https://arxiv.org/pdf/1911.03829.pdf)

此外，这里使用的知识蒸馏技术与原始蒸馏[研究论文](https://arxiv.org/abs/1503.02531)中使用的技术相同，我们在老师网络生成的软标签上训练学生网络。

**那么，这篇研究论文与其他论文相比有什么突出的地方呢？以下是解释。**

这里的关键思想是将 BERT 中的知识蒸馏到一个能够生成文本的学生模型，而之前的工作仅关注模型压缩以完成与老师模型相同的任务。然后，对 BERT 模型进行微调，使微调后的模型可以用于文本生成。

我们以语言翻译为例，X 是源语言句子，Y 是目标语言句子。

**第一阶段：** BERT 模型的微调

+   输入数据：将 X 和 Y 连接在一起，Y 中 15% 的词汇被随机屏蔽

+   标签：来自 Y 的屏蔽词汇

**第二阶段：** 微调 BERT 模型到 Seq2Seq 模型的知识蒸馏

+   老师：第一阶段的微调 BERT 模型

+   学生：Seq2Seq 模型，例如基于注意力的 RNN、Transformer 或任何其他序列生成模型

+   输入数据和标签：来自微调 BERT 模型的软目标

# 论文 3

**[TinyBERT：为自然语言理解蒸馏 BERT](https://arxiv.org/pdf/1909.10351.pdf)**

本文提出了一种新颖的 Transformer 蒸馏技术，专门用于 Transformer 基础模型的知识蒸馏 (KD)。通过利用这种新颖的 KD 方法，可以有效地将大型教师 BERT 中编码的知识转移到小型学生 Tiny-BERT 中。然后，我们引入了一个新的两阶段学习框架，用于 TinyBERT，该框架在预训练和任务特定学习阶段都执行 Transformer 蒸馏。该框架确保 TinyBERT 能够捕捉 BERT 的通用领域和任务特定知识。

具有四层的 TinyBERT 在经验上有效，其性能达到其老师 BERT-Base 在 GLUE 基准测试中的 96.8% 以上，同时体积小 7.5 倍，推理速度快 9.4 倍。四层的 TinyBERT 也明显优于 4 层最新的 BERT 蒸馏基准，仅使用约 28% 的参数和约 31% 的推理时间。此外，六层的 TinyBERT 性能与其老师 BERT-Base 相当。

![8 个创新的 BERT 知识蒸馏论文，改变了 NLP 的格局](../Images/715cb915ee89c540289682993613dbd4.png)

[来源](https://arxiv.org/pdf/1909.10351.pdf)

此外，本文提出了三大组件用于蒸馏 Transformer 网络。

1.  **Transformer 层蒸馏：** 包括基于注意力的蒸馏和基于隐藏状态的蒸馏：

![8 个创新的 BERT 知识蒸馏论文，改变了 NLP 的格局](../Images/fa66113c7ebdd15b47a481e784f931ba.png)

[来源](https://arxiv.org/pdf/1909.10351.pdf)

1.  **嵌入层蒸馏：** 像对隐层状态进行蒸馏一样，对嵌入层进行知识蒸馏。

1.  **预测层蒸馏：** 就像[Hinton](https://arxiv.org/abs/1503.02531)原作中一样，知识蒸馏是针对从教师模型中获得的预测进行的。此外，TinyBERT模型的整体损失结合了上述三种损失：

![8篇改变NLP领域的创新BERT知识蒸馏论文](../Images/a518f81a71e076428ac3af17fdb4d55d.png)

[来源](https://arxiv.org/pdf/1909.10351.pdf)

TinyBERT训练的主要步骤如下：

+   **通用蒸馏：** 以未微调的原始BERT作为教师，并以大规模文本语料作为训练数据。现在对通用领域的文本进行Transformer蒸馏，得到可以进一步微调以进行下游任务的通用TinyBERT。由于层数、神经元等较少，这种通用TinyBERT的表现比BERT差。

+   **任务特定蒸馏：** 以微调后的BERT作为教师，训练数据为任务特定训练集。

**关键要点：** 这是一种离线蒸馏技术，其中教师模型BERT已经预训练完成。然后，他们进行了两个独立的蒸馏过程：一个用于通用学习，另一个用于任务特定学习。通用蒸馏的第一步涉及对各种层的蒸馏：注意力层、嵌入层和预测层。

# 论文4

**[**FastBERT: 一种具有自适应推理时间的自蒸馏BERT**](https://arxiv.org/abs/2004.02178)**

他们提出了一种全新的可调速度的FastBERT，具有自适应推理时间。推理时的速度可以根据不同需求灵活调整，同时避免了样本的冗余计算。此外，该模型采用了独特的自蒸馏机制进行微调，进一步提高了计算效率，同时性能损失最小。我们的模型在十二个英文和中文数据集上取得了令人满意的结果。如果在不同的加速阈值下进行速度与性能的折中，它的速度可以比BERT提高1到12倍的范围。

与类似工作的比较：

1.  **TinyBERT:** 通过使用通用领域和任务特定的微调来进行两阶段学习。

1.  **DistilBERT：** 引入了三重损失

## FastBERT的优势是什么？

该工作首次将**自蒸馏**（训练阶段）和**自适应机制**（推理阶段）技术应用于NLP语言模型，以提高效率。

![8篇改变NLP领域的创新BERT知识蒸馏论文](../Images/4180862354477969f2a5171b875c4e78.png)

[来源](https://arxiv.org/abs/2004.02178)

### **模型架构**

FastBERT模型由主干和分支组成：

1.  **骨干**：它包含三部分：嵌入层、包含 Transformer 堆栈的编码器以及教师分类器。嵌入层和编码器层与 BERT 的相同。最后，我们有一个教师分类器，用于提取任务特定的特征，以便下游任务使用软最大函数。

1.  **分支**：这些包含学生分类器

1.  具有与教师相同的架构

1.  被添加到每个变压器块的输出中，以启用早期输出

### **训练阶段**

对骨干和学生分类器使用单独的训练步骤。在一个模块训练时，另一个模块的参数始终被冻结。三步：

1.  **骨干预训练**：使用 BERT 模型的典型预训练。这里没有变化。可以在此步骤中自由加载高质量的训练模型。

1.  **骨干微调**：对于每个下游任务，使用任务特定的数据来微调骨干和教师分类器。在这个阶段，没有启用学生分类器。

1.  **学生分类器的自蒸馏**：现在我们的教师模型已经训练好，我们获取其输出。这种软标签输出质量高，包含原始嵌入和概括性知识。这些软标签用于训练学生分类器。我们可以在这里自由使用无限量的未标记数据。*这项工作不同于以往的工作，因为这项工作使用相同的模型作为教师和学生模型。*

### **自适应推断**

让我们谈谈推断时间。使用 FastBERT 时，推断是自适应执行的，即模型内执行的编码层数可以根据输入样本的复杂性进行调整。

在每个变压器层中，计算学生分类器输出的不确定性，并根据阈值确定是否可以终止推断。以下是自适应推断机制的工作原理：

1.  在 FastBERT 的每一层中，相应的学生分类器预测每个样本的标签，并测量不确定性。

1.  不确定性低于某个阈值的样本将被筛选到早期输出，而不确定性高于阈值的样本将转移到下一层。

1.  阈值较高时，较少的样本被发送到更高层，以保持推断速度更快，反之亦然。

# 论文 5

**[从 BERT 中提取任务特定知识到简单神经网络](https://arxiv.org/abs/1903.12136)**

在本文中，作者展示了即使没有架构修改、外部训练数据或额外输入特征，基础的轻量级神经网络也可以具有竞争力。他们提出将 BERT 知识蒸馏到单层双向长短期记忆网络（BiLSTM）及其同类模型，用于句子对任务。在大量的重述、自然语言推理和情感分类数据集中，他们在参数量大约少 100 倍，推理时间减少 15 倍的情况下，取得了与 ELMo 相当的结果。此外，他们的方法包括对教师和 BiLSTM 学生模型进行微调。此工作的主要动机包括：

1.  简单的架构模型能否在文本建模中捕捉到与 BERT 模型相当的表示能力？

1.  研究将知识从 BERT 转移到 BiLSTM 模型的有效方法。

![8 篇改变 NLP 领域的创新 BERT 知识蒸馏论文](../Images/212a49d0abfaf34cf4a553e2c619981c.png)

[来源](https://arxiv.org/pdf/1903.12136.pdf) | [论文作者的参考视频](https://www.youtube.com/watch?v=AKCPPvaz8tU)

## 蒸馏的数据增强

小数据集可能不足以让教师完全表达其知识，因此使用从教师模型生成的伪标签的大型未标记数据集来扩充训练集。在这项工作中，提出了一些用于任务无关的数据增强的启发式方法：

1.  **掩码**：随机将句子中的一个词替换为类似于 BERT 训练的 [MASK] 标记。

1.  POS 引导的词替换：用同一词性标签的另一个词替换句子中的一个词，例如，“猪吃什么？”被扰动为“猪怎么吃？”

1.  **N-gram 采样**：一种更激进的掩码形式，从输入示例中选择 n-gram 样本，其中 n 从 {1,2,3,4,5} 中随机选择。

# 论文 6

**[BERT 模型压缩的患者知识蒸馏](https://arxiv.org/abs/1908.09355)**

作者提出了一种患者知识蒸馏方法，将原始的大型模型（教师）压缩成一个同样有效的轻量级浅层网络（学生）。他们的方法与之前的知识蒸馏方法有很大不同，因为早期的方法仅使用教师网络最后一层的输出进行蒸馏；而我们的学生模型则耐心地从教师模型的多个中间层中学习，以进行渐进的知识提取，遵循两种策略：

1.  **PKD-Last**：学生模型从教师的最后 *k* 层中学习（假设最后几层包含了对学生最重要的信息）。

1.  **PKD-Skip**：学生模型从教师的每个 *k* 层中学习。

他们在多个数据集和不同的NLP任务上进行了实验，证明所提出的PKD方法比标准蒸馏方法[(Hinton et al., 2015)](https://arxiv.org/abs/1503.02531)表现更好，泛化能力更强。

![8种创新的BERT知识蒸馏论文，改变了NLP的格局](../Images/f75d5817cf6540620b6cb35634fc5ac3.png)

[来源](https://arxiv.org/pdf/1908.09355.pdf)

**为什么不从教师模型的所有隐藏状态中学习？**

原因是这可能会计算上非常昂贵，并且可能会给学生模型引入噪声。

# 论文7

**[**MobileBERT：一种紧凑的无任务专用BERT，适用于资源受限设备**](https://arxiv.org/pdf/2004.02984.pdf)**

他们提出了MobileBERT，用于压缩和加速流行的BERT模型。像原始BERT一样，MobileBERT是不依赖任务的；即，通过简单的微调，可以通用地应用于各种下游NLP任务。MobileBERT是BERTʟᴀʀɢᴇ的精简版本，同时配备了瓶颈结构，并在自注意力机制和前馈网络之间进行了精心设计的平衡。

## 训练步骤

*第一步：* 首先训练一个特别设计的教师模型，即包含反向瓶颈的BERTʟᴀʀɢᴇ模型。

*第二步：* 从这位教师模型向MobileBERT进行知识转移。

![8种创新的BERT知识蒸馏论文，改变了NLP的格局](../Images/7dab52f268b86516dd1bb59b31f4d9ce.png)

变压器块的架构可视化，如(a) BERT，(b) MobileBERT教师，以及(c) MobileBERT学生。标记为“Linear”的绿色梯形图被称为瓶颈。[来源](https://arxiv.org/pdf/2004.02984.pdf)

(a) BERT；(b) 反向瓶颈BERT（IB-BERT）；以及© MobileBERT。在(b)和©中，红色线条表示块间流动，而蓝色线条表示块内流动。MobileBERT通过逐层模仿IB-BERT进行训练。

如果你读到这里，你值得一个击掌。MobileBERT在变压器块中展示了*瓶颈*，这使得从更大的教师模型中提取知识到更小的学生模型的过程更加平滑。这种方法减少了学生模型的宽度，而不是深度，这在给定的实验中产生了更高效的模型。MobileBERT强调了这样一个信念，即在初始蒸馏过程之后，确实可以使学生模型进行微调。

此外，结果还表明，这在实践中也成立，因为MobileBERT可以在GLUE上达到BERT-base 99.2%的性能，同时参数减少4倍，并且在Pixel 4手机上的推理速度快5.5倍！

# 论文8

**[通过知识蒸馏提升多任务深度神经网络以实现自然语言理解](https://arxiv.org/pdf/1904.09482.pdf)**

论文的关键关注点如下：

1.  训练一个多任务神经网络模型，该模型结合了多个自然语言理解任务的损失。

1.  从第一步生成多个模型的集成，这些模型本质上是通过从头开始训练多个多任务模型获得的。

1.  最终步骤是对前一步的模型集进行知识蒸馏。

![8种创新的BERT知识蒸馏论文，改变了自然语言处理的格局](../Images/d9432bfc820cd0b1fa3185cf01948489.png)

MT-DNN模型用于表示学习的架构 [(Liu et al., 2019)](https://arxiv.org/abs/1902.10461)。较低层在所有任务中共享，而顶部层则是任务特定的。输入X（可以是一个句子或一组句子）首先表示为一系列嵌入向量，每个单词一个，在l1层中。然后，Transformer编码器捕捉每个单词的上下文信息，并在l2层中生成共享的上下文嵌入向量。最后，额外的任务特定层生成每个任务的任务特定表示，随后进行分类、相似性评分或相关性排序所需的操作。[来源](https://arxiv.org/pdf/1904.09482.pdf)

![8种创新的BERT知识蒸馏论文，改变了自然语言处理的格局](../Images/bbd4356b24f3d66d264051e9a03dfa6d.png)

多任务学习的知识蒸馏过程。选择一组具有任务特定标签训练数据的任务。然后，为每个任务训练一个不同的神经网络（教师）。教师用于生成每个任务特定训练样本的一组软目标。考虑到多个任务的训练数据集的软目标，使用多任务学习和反向传播来训练一个单一的MT-DNN（学生），如算法1所述，除了如果任务t有教师，则第3行的任务特定损失是两个目标函数的平均值，一个用于正确目标，另一个用于教师分配的软目标。[来源](https://arxiv.org/pdf/1904.09482.pdf)

成就：在GLUE数据集上，蒸馏的MT-DNN在9个自然语言理解任务中的7个任务上创造了新的最先进结果，包括没有教师的任务，将GLUE基准（单模型）推高到83.7%。

我们展示了蒸馏的MT-DNN几乎保留了由集成模型所取得的所有改进，同时保持模型大小与原始MT-DNN模型相同。

**附注**

现代最先进的自然语言处理模型在生产中难以应用。知识蒸馏提供了应对这些问题及其他问题的工具，但它也有其独特之处。

## 参考文献

1.  [**在神经网络中蒸馏知识**](https://arxiv.org/abs/1503.02531)

    *提高几乎任何机器学习算法性能的一个非常简单的方法是训练多种不同的模型……*arxiv.org](https://arxiv.org/abs/1503.02531)

1.  [**BERT：深度双向转换器的预训练用于语言理解**](https://arxiv.org/abs/1503.02531)

    *我们介绍了一种新的语言表示模型，称为BERT，代表双向编码器表示…*arxiv.org](https://arxiv.org/abs/1810.04805)

1.  [**从BERT中提取知识用于文本生成**](https://arxiv.org/abs/1910.01108)

    *大规模预训练语言模型如BERT在语言理解任务中取得了巨大成功…*arxiv.org](https://arxiv.org/abs/1911.03829)

1.  [**DistilBERT，一个BERT的蒸馏版本：更小、更快、更便宜、更轻量**](https://arxiv.org/abs/1910.01108)

    *随着大规模预训练模型在自然语言处理（NLP）中的迁移学习变得越来越普遍…*arxiv.org](https://arxiv.org/abs/1910.01108)

1.  [**TinyBERT：为自然语言理解蒸馏BERT**](https://arxiv.org/abs/1910.01108)

    *语言模型预训练，如BERT，显著提升了许多自然语言处理任务的性能…*arxiv.org](https://arxiv.org/abs/1909.10351)

1.  [**在神经网络中提取知识**](https://arxiv.org/abs/2004.02178)

    *提高几乎所有机器学习算法性能的一种非常简单的方法是训练许多不同的模型…*arxiv.org](https://arxiv.org/abs/1503.02531)

1.  [**FastBERT：一种自蒸馏BERT，具有自适应推理时间**](https://arxiv.org/abs/1910.01108)

    *预训练语言模型如BERT已被证明非常高效。然而，它们通常在计算上…*arxiv.org](https://arxiv.org/abs/2004.02178)

1.  [**从BERT中提取特定任务知识到简单神经网络**](https://arxiv.org/abs/1910.01108)

    *在自然语言处理文献中，神经网络变得越来越深且复杂。最近…*arxiv.org](https://arxiv.org/abs/1903.12136)

1.  [**用于BERT模型压缩的患者知识蒸馏**](https://arxiv.org/abs/1910.01108)

    *预训练语言模型如BERT已被证明在自然语言处理（NLP）中非常有效…*arxiv.org](https://arxiv.org/abs/1908.09355)

1.  [**在神经网络中提取知识**](https://arxiv.org/abs/1910.01108)

    *提高几乎所有机器学习算法性能的一种非常简单的方法是训练许多不同的模型…*arxiv.org](https://arxiv.org/abs/1503.02531)

1.  [**MobileBERT：一种紧凑型任务无关BERT，适用于资源受限设备**](https://arxiv.org/abs/1910.01108)

    *自然语言处理（NLP）最近通过使用具有数百亿参数的大型预训练模型取得了巨大成功…*arxiv.org](https://arxiv.org/abs/2004.02984)

1.  [**通过知识蒸馏改进多任务深度神经网络，以实现自然语言处理…**](https://arxiv.org/abs/1910.01108)

    *本文探讨了使用知识蒸馏来改进多任务深度神经网络（MT-DNN）（Liu et al…*arxiv.org](https://arxiv.org/abs/1904.09482)

> "这里表达的观点是Mr. Abhishek的个人观点，与他的雇主无关"

**[Kumar Abhishek](https://www.linkedin.com/in/kumarabhisheknitt/)** 是Expedia的机器学习工程师，专注于欺诈检测和预防领域。他使用机器学习和自然语言处理模型进行风险分析和欺诈检测。他拥有超过十年的机器学习和软件工程经验。

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业道路。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析能力

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的 IT 部门

* * *

### 更多相关话题

+   [数据科学已经改变，而非消亡！](https://www.kdnuggets.com/2023/08/data-science-changed-died.html)

+   [克服多语言语音技术的障碍：前五大挑战及创新解决方案](https://www.kdnuggets.com/2023/08/overcoming-barriers-multilingual-voice-technology-top-5-challenges-innovative-solutions.html)

+   [过去 12 个月必读的 NLP 论文](https://www.kdnuggets.com/2023/03/must-read-nlp-papers-last-12-months.html)

+   [NLP 初学者的研究论文](https://www.kdnuggets.com/2022/11/research-papers-nlp-beginners.html)

+   [AI 驱动世界中的数据工程景观](https://www.kdnuggets.com/2023/05/data-engineering-landscape-aidriven-world.html)

+   [数据景观的演变](https://www.kdnuggets.com/2023/06/evolution-data-landscape.html)
