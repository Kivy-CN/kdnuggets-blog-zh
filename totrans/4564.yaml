- en: 'Build an Artificial Neural Network From Scratch: Part 1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始构建人工神经网络：第一部分
- en: 原文：[https://www.kdnuggets.com/2019/11/build-artificial-neural-network-scratch-part-1.html](https://www.kdnuggets.com/2019/11/build-artificial-neural-network-scratch-part-1.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/11/build-artificial-neural-network-scratch-part-1.html](https://www.kdnuggets.com/2019/11/build-artificial-neural-network-scratch-part-1.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: In my previous article [**Introduction to Artificial Neural Networks(ANN)**](https://towardsdatascience.com/introduction-to-artificial-neural-networks-ann-1aea15775ef9)**, **we
    learned about various concepts related to ANN so I would recommend going through
    it before moving forward because here I’ll be focusing on the implementation part
    only. In this article series, we are going to build ANN from scratch using only
    the [numpy](https://www.guru99.com/numpy-tutorial.html) Python library.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前的文章[**人工神经网络（ANN）简介**](https://towardsdatascience.com/introduction-to-artificial-neural-networks-ann-1aea15775ef9)**中，**我们学习了与ANN相关的各种概念，因此我建议在继续之前先阅读它，因为在这里我将只关注实现部分。在这个系列文章中，我们将仅使用[numpy](https://www.guru99.com/numpy-tutorial.html)
    Python库从头开始构建ANN。
- en: In this part-1, we will build a fairly easy ANN with just having 1 input layer
    and 1 output layer and no hidden layer.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分中，我们将构建一个相当简单的ANN，只包含1个输入层和1个输出层，没有隐藏层。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行IT管理'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In part-2, we will build ANN with 1 input layer, 1 hidden layer, and 1 output
    layer.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分中，我们将构建一个具有1个输入层、1个隐藏层和1个输出层的ANN。
- en: '**Why from scratch?**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么从头开始？**'
- en: Well, there are many deep learning libraries([Keras](https://keras.io/), [TensorFlow](https://www.tensorflow.org/), [PyTorch](https://pytorch.org/) etc)
    that can be used to create a neural network in a few lines of code. However, if
    you really want to understand the in-depth working of a neural network, I suggest
    you learn how to code it from scratch using Python or any other programming language.
    So let's get started.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，市面上有许多深度学习库（[Keras](https://keras.io/)、[TensorFlow](https://www.tensorflow.org/)、[PyTorch](https://pytorch.org/)等），可以用几行代码创建一个神经网络。然而，如果你真的想深入理解神经网络的工作原理，我建议你学习如何使用Python或其他编程语言从头开始编写代码。让我们开始吧。
- en: 'Let''s create some random dataset:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来创建一些随机数据集：
- en: '![Figure](../Images/c81bdcaab407a183e947b7ab63b0c89f.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/c81bdcaab407a183e947b7ab63b0c89f.png)'
- en: Random dataset with binary values for simplicity
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，使用二进制值的随机数据集
- en: In the above table, we have five columns: `Person, X1, X2, X3, and Y`. Here
    1 refers to true and 0 refers to false. Our task is to create an artificial neural
    network that is capable of predicting the value of `Y` based on values of `X1,
    X2 and X3`.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的表格中，我们有五列：`Person, X1, X2, X3和Y`。其中1表示真，0表示假。我们的任务是创建一个能够根据`X1, X2和X3`的值预测`Y`值的人工神经网络。
- en: 'We will create an artificial neural network with one input layer and one output
    layer with no hidden layer. Before we start coding, let’s first let’s see how
    our neural network will execute in theory:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个只有一个输入层和一个输出层，没有隐藏层的人工神经网络。在开始编码之前，让我们首先看看我们的神经网络在理论上是如何运行的：
- en: '**Theory of ANN**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络理论**'
- en: An artificial neural network is a supervised learning algorithm which means
    that we provide it the input data containing the independent variables and the
    output data that contains the dependent variable. For instance, in our example
    our independent variables are `X1, X2 and X3`. The dependent variable is `Y`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络是一种监督学习算法，这意味着我们提供给它包含自变量的输入数据和包含因变量的输出数据。例如，在我们的例子中，自变量是`X1, X2和X3`。因变量是`Y`。
- en: In the beginning, the ANN makes some random predictions, these predictions are
    compared with the correct output and the error(the difference between the predicted
    values and the actual values) is calculated. The function that finds the difference
    between the actual value and the propagated values is called the cost function.
    The cost here refers to the error. Our objective is to minimize the cost function.
    Training a neural network basically refers to minimizing the cost function. We
    will see how we can perform this task.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，人工神经网络（ANN）会做一些随机预测，这些预测与正确输出进行比较，计算误差（预测值与实际值之间的差异）。找到实际值与传播值之间差异的函数称为成本函数。这里的成本指的是误差。我们的目标是最小化成本函数。训练神经网络基本上就是指最小化成本函数。我们将看到如何执行这一任务。
- en: A neural network executes in two phases: **Feed Forward** phase and **Back Propagation **phase.
    Let us discuss both these steps in detail.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络分为两个阶段执行：**前向传播**阶段和**反向传播**阶段。让我们详细讨论这两个步骤。
- en: '**Feed Forward**'
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**前向传播**'
- en: '![Figure](../Images/fb1e559a1856525bc66cecb2914cf480.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/fb1e559a1856525bc66cecb2914cf480.png)'
- en: Source: [Single-layer neural network also called Perceptron](https://towardsdatascience.com/single-neuron-training-3fc7f84d67d)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[单层神经网络，也称为感知器](https://towardsdatascience.com/single-neuron-training-3fc7f84d67d)
- en: 'In the feed-forward phase of ANN, predictions are made based on the values
    in the input nodes and the weights. If you look at the neural network in the above
    figure, you will see that we have three features in the dataset: X1, X2, and X3,
    therefore we have three nodes in the first layer, also known as the input layer.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ANN 的前向传播阶段，基于输入节点的值和权重进行预测。如果你查看上图中的神经网络，你会看到数据集中有三个特征：X1、X2 和 X3，因此在第一层，也称为输入层，有三个节点。
- en: The weights of a neural network are basically the strings that we have to adjust
    in order to be able to correctly predict our output. For now, just remember that
    for each input feature, we have one weight.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的权重基本上是我们必须调整的参数，以便能够正确预测输出。目前只需记住，对于每个输入特征，我们有一个权重。
- en: 'The following are the steps that execute during the feedforward phase of ANN:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 ANN 前向传播阶段执行的步骤：
- en: '**Step 1: Calculate the dot product between inputs and weights**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1：计算输入与权重之间的点积**'
- en: The nodes in the input layer are connected with the output layer via three weight
    parameters. In the output layer, the values in the input nodes are multiplied
    with their corresponding weights and are added together. Finally, the bias term `b` is
    added to the sum.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层的节点通过三个权重参数与输出层相连。在输出层，输入节点中的值与其对应的权重相乘后相加。最后，将偏置项`b`添加到总和中。
- en: '***Why do we even need a bias term?***'
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***我们为什么需要偏置项？***'
- en: Suppose if we have a person who has input values (0,0,0), the sum of the products
    of input nodes and weights will be zero. In that case, the output will always
    be zero no matter how much we train the algorithms. Therefore, in order to be
    able to make predictions, even if we do not have any non-zero information about
    the person, we need a bias term. The bias term is necessary to make a robust neural
    network.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个输入值为 (0,0,0) 的人，输入节点和权重的乘积之和将为零。在这种情况下，无论我们如何训练算法，输出将始终为零。因此，为了能够进行预测，即使我们对该人的信息没有任何非零值，我们也需要一个偏置项。偏置项对于构建一个稳健的神经网络是必要的。
- en: 'Mathematically, the summation of dot product:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，点积的总和：
- en: '`X.W=x1.w1 + x2.w2 + x3.w3 + b`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`X.W=x1.w1 + x2.w2 + x3.w3 + b`'
- en: '**Step 2: Pass the summation of dot products (X.W) through an activation function**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2：将点积的总和 (X.W) 通过激活函数**'
- en: The dot product XW can produce any set of values. However, in our output, we
    have the values in the form of 1 and 0\. We want our output to be in the same
    format. To do so we need an A[ctivation Function](https://en.wikipedia.org/wiki/Activation_function),
    which restricts the input values between 0 and 1\. So of course we’ll go ahead
    with Sigmoid activation function.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 点积 XW 可以产生任何一组值。然而，在我们的输出中，我们有 1 和 0 的形式。我们希望输出也以相同格式呈现。为此，我们需要一个[A[ctivation
    Function](https://en.wikipedia.org/wiki/Activation_function)]，它将输入值限制在 0 和 1 之间。所以我们当然会选择
    Sigmoid 激活函数。
- en: '![Figure](../Images/494bbc0bc64ef03adaad348fecdd42c7.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/494bbc0bc64ef03adaad348fecdd42c7.png)'
- en: '[Sigmoid activation function](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sigmoid 激活函数](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)'
- en: The sigmoid function returns 0.5 when the input is 0\. It returns a value close
    to 1 if the input is a large positive number. In the case of negative input, the
    sigmoid function outputs a value close to zero.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入为 0 时，sigmoid 函数返回 0.5。如果输入是一个大的正数，它返回接近 1 的值。在负输入的情况下，sigmoid 函数输出接近零的值。
- en: Therefore, it is especially used for models where we have to **predict the probability** as
    an output. Since probability of anything exists only between the range of **0
    and 1,** sigmoid is the right choice for our problem.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它特别用于需要**预测概率**作为输出的模型。由于概率值仅在**0 和 1**之间，sigmoid 是解决我们问题的正确选择。
- en: In the above figure `z` is the summation of dot product X.W
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，`z` 是点积 X.W 的总和。
- en: 'Mathematically, the sigmoid activation function is:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，sigmoid 激活函数是：
- en: '![Figure](../Images/8251273f7914ec76939f7616fc54756e.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/8251273f7914ec76939f7616fc54756e.png)'
- en: '[Sigmoid activation function](https://analyticsindiamag.com/most-common-activation-functions-in-neural-networks-and-rationale-behind-it/)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sigmoid 激活函数](https://analyticsindiamag.com/most-common-activation-functions-in-neural-networks-and-rationale-behind-it/)'
- en: Let us summarize what we have done so far. First, we have to find the dot product
    of the input features(matrix of independent variables) with the weights. Next,
    pass the summation of dot products through an activation function. **The result
    of the activation function is basically the predicted output for the input features.**
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下到目前为止的工作。首先，我们需要计算输入特征（自变量矩阵）与权重的点积。接下来，将点积的总和通过激活函数。**激活函数的结果基本上是输入特征的预测输出。**
- en: '**Back Propagation**'
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**反向传播**'
- en: In the beginning, before you do any training, the neural network makes random
    predictions which are of course incorrect.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始时，在进行任何训练之前，神经网络会做出随机的预测，这些预测显然是错误的。
- en: We start by letting the network make random output predictions. We then compare
    the predicted output of the neural network with the actual output. Next, we update
    the weights and the bias in such a manner that our predicted output comes closer
    to the actual output. In this phase, we train our algorithm. Let’s take a look
    at the steps involved in the backpropagation phase.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始时让网络进行随机输出预测。然后，我们将神经网络的预测输出与实际输出进行比较。接下来，我们以使预测输出更接近实际输出的方式更新权重和偏置。在这个阶段，我们训练我们的算法。让我们看看反向传播阶段涉及的步骤。
- en: '**Step 1: Calculate the cost**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 1 步：计算成本**'
- en: The first step in this phase is to find the cost of the predictions. The cost
    of the prediction can be calculated by finding the difference between the predicted
    output values and the actual output values. If the difference is large then cost
    will also be large.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此阶段的第一步是计算预测的成本。预测成本可以通过计算预测输出值和实际输出值之间的差异来得到。如果差异很大，则成本也会很高。
- en: We will use the [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error) or
    MSE cost function. A cost function is a function that finds the cost of the given
    output predictions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用[均方误差](https://en.wikipedia.org/wiki/Mean_squared_error)或 MSE 成本函数。成本函数是一个计算给定输出预测成本的函数。
- en: '![Figure](../Images/923b7bdfd93b37400fee96b508ddc1c0.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/923b7bdfd93b37400fee96b508ddc1c0.png)'
- en: '[mean squared error](https://stackoverflow.com/questions/44038581/mse-cost-function-for-training-neural-network)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[均方误差](https://stackoverflow.com/questions/44038581/mse-cost-function-for-training-neural-network)'
- en: '*Here, *`*Yi is the actual output value*`* and *`*Ŷi is the predicted output
    value *`*and *`*n is the number of Observations.*`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*这里，*`*Yi 是实际输出值*`* 和 *`*Ŷi 是预测输出值*`* 以及 *`*n 是观察数量。*`'
- en: '**Step 2: Minimize the cost**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 2 步：最小化成本**'
- en: Our ultimate goal is to fine-tune the weights of our neural network in such
    a way that the cost is minimized the minimum. If you notice carefully, you’ll
    get to know that we can only control the weights and the bias. Everything else
    is beyond our control. We cannot control the inputs, we cannot control the dot
    products, and we cannot manipulate the sigmoid function.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的终极目标是调整神经网络的权重，以使成本达到最低。如果你仔细观察，你会发现我们只能控制权重和偏置。其他一切都超出了我们的控制范围。我们不能控制输入，不能控制点积，也不能操控
    sigmoid 函数。
- en: In order to minimize the cost, we need to find the weight and bias values for
    which the cost function returns the smallest value possible. The smaller the cost,
    the more correct our predictions are.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化成本，我们需要找到使成本函数返回最小值的权重和偏置值。成本越小，我们的预测就越准确。
- en: 'To find the minima of a function, we can use the [gradient descent](https://en.wikipedia.org/wiki/Optimization_problem) algorithm.
    The gradient descent can be mathematically represented as:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到一个函数的极小值，我们可以使用 [梯度下降](https://en.wikipedia.org/wiki/Optimization_problem)
    算法。梯度下降可以用数学公式表示为：
- en: '![Figure](../Images/5a71058e6f68584257f32317adaaf57f.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/5a71058e6f68584257f32317adaaf57f.png)'
- en: '[Weight update using gradient descent](http://hmkcode.com/ai/backpropagation-step-by-step/)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[使用梯度下降更新权重](http://hmkcode.com/ai/backpropagation-step-by-step/)'
- en: '**????Error** is the cost function. The above equation tells us to find the
    partial derivative of the cost function with respect to each weight and bias and
    subtract the result from the existing weights to get new weights.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**????错误** 是成本函数。上述方程告诉我们要计算成本函数相对于每个权重和偏置的偏导数，并将结果从现有权重中减去，以获得新的权重。'
- en: The derivative of a function gives us its slope at any given point. To find
    if the cost increases or decreases, given the weight value, we can find the derivative
    of the function at that particular weight value. If the cost increases with the
    increase in weight, the derivative will return a positive value which will then
    be subtracted from the existing value.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的导数给我们在任何给定点的斜率。为了找到成本是否增加或减少，给定权重值，我们可以在该特定权重值处找到函数的导数。如果成本随着权重的增加而增加，导数将返回一个正值，然后从现有值中减去。
- en: On the other hand, if the cost is decreasing with an increase in weight, a negative
    value will be returned, which will be added to the existing weight value since
    negative into negative is positive.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果成本随着权重的增加而减少，则会返回一个负值，该负值会被添加到现有的权重值中，因为负数加负数是正数。
- en: In the above equation `**a**` is called the learning rate, which is multiplied
    by the derivative. The learning rate decides how fast our algorithm learns.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中 `**a**` 被称为学习率，它与导数相乘。学习率决定了我们的算法学习的速度。
- en: We need to repeat the execution of gradient descent for all the weights and
    biases until the cost is minimized and for which the cost function returns a value
    close to zero.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要对所有的权重和偏置重复执行梯度下降，直到成本最小化，并且成本函数返回的值接近零。
- en: Now is the time to implement what we have studied so far. We will create a simple
    neural network with one input and one output layer in Python.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候实现我们到目前为止学习的内容了。我们将在 Python 中创建一个简单的神经网络，包含一个输入层和一个输出层。
- en: Artificial Neural Network Implementation using numpy
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 numpy 实现人工神经网络
- en: '![Figure](../Images/19407a669cab463068bc45cc68f1e3fc.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/19407a669cab463068bc45cc68f1e3fc.png)'
- en: Pic Credit: [hackernoon.com](https://hackernoon.com/building-a-neural-network-only-using-numpy-7ba75da60ec0)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[hackernoon.com](https://hackernoon.com/building-a-neural-network-only-using-numpy-7ba75da60ec0)
- en: '**Steps to follow:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**要遵循的步骤：'
- en: 1\. Define independent variables and dependent variable
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 定义自变量和因变量
- en: 2\. Define Hyperparameters
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 定义超参数
- en: 3\. Define Activation Function and its derivative
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 定义激活函数及其导数
- en: 4\. Train the model
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 训练模型
- en: 5\. Make predictions**
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 进行预测**
- en: '**Step-1: Let’s first create our independent variables or input feature set
    and the corresponding dependent variable or labels.**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤-1：首先创建我们的自变量或输入特征集以及对应的因变量或标签。**'
- en: '[PRE0]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Our input set contains seven records. Similarly, we also created a `labels` set
    that contains corresponding labels for each record in the input set. The labels
    are the values that we want our ANN to predict.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入集包含七条记录。类似地，我们还创建了一个 `labels` 集，其中包含输入集中每条记录的对应标签。这些标签是我们希望 ANN 预测的值。
- en: '**Step-2: Define Hyperparameters**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤-2：定义超参数**'
- en: we will use the `random.seed` function of numpy so that we can get the same
    random values whenever we execute the below code.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `random.seed` 函数，以便每次执行下面的代码时都能获得相同的随机值。
- en: Next, we initialize our weights with normally distributed random numbers. Since
    we have three features in the input, we have a vector of three weights. We then
    initialize the bias value with another random number. Finally, we set the learning
    rate to 0.05.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们用正态分布的随机数初始化权重。由于输入中有三个特征，我们有一个包含三个权重的向量。然后，我们用另一个随机数初始化偏置值。最后，我们将学习率设置为
    0.05。
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Step-3: Define Activation Function and it’s derivative: Our activation function
    is the sigmoid function.**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤-3：定义激活函数及其导数：我们的激活函数是 sigmoid 函数。**'
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now define a function that calculates the derivative of the sigmoid function.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在定义一个计算 sigmoid 函数导数的函数。
- en: '[PRE3]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Step-4: Its time to train our ANN model**'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤-4：是时候训练我们的 ANN 模型了**'
- en: We’ll start off by defining the number of epochs. An epoch is the number of
    times we want to train the algorithm on our dataset. We will train the algorithm
    on our data 25,000 times so our epoch will be 25000\. You can try a different
    number to further decrease the cost.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从定义 epoch 数量开始。一个 epoch 是我们希望在数据集上训练算法的次数。我们将对数据训练 25,000 次，因此我们的 epoch 将是
    25000。你可以尝试不同的数量以进一步降低成本。
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let's understand each step one by one and then we will go to our last step of
    making predictions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地理解每一个步骤，然后我们将进入最后一步：做预测。
- en: we store the values from the input `input_set` to the `inputs` variable so that
    the value of `input_set` remain as it is in each iteration and whatever changes
    are done that must be done to `inputs` variable.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从输入 `input_set` 中存储的值保存在 `inputs` 变量中，以确保 `input_set` 的值在每次迭代中保持不变，所有的修改都必须应用到
    `inputs` 变量中。
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Next, we find the dot product of the input and the weight and add bias to it.
    (**Step-1 of Feedforward phase)**
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们找到输入和权重的点积并加上偏置。 (**前向传播阶段的步骤-1)**
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Next, We pass the dot product through the sigmoid activation function. (**Step-2
    of Feedforward phase)**
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将点积通过 sigmoid 激活函数。 (**前向传播阶段的步骤-2)**
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This completes the feedforward part of our algorithm and now is the time to
    start backpropagation.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们算法的前向传播部分，现在是开始反向传播的时候了。
- en: The variable `z` contains the predicted outputs. The first step of the backpropagation
    is to find the error.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 变量`z`包含了预测输出。反向传播的第一步是找到误差。
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We know that our cost function is:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道我们的成本函数是：
- en: '![](../Images/923b7bdfd93b37400fee96b508ddc1c0.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/923b7bdfd93b37400fee96b508ddc1c0.png)'
- en: We need to differentiate this function with respect to each weight and this
    can be done easily using [chain rule of differentiation](https://en.wikipedia.org/wiki/Chain_rule).
    I’ll skip the derivation part but if anyone is interested let me know I’ll post
    it in the comment section.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要对这个函数进行关于每个权重的导数，这可以通过使用 [链式法则](https://en.wikipedia.org/wiki/Chain_rule)
    来轻松完成。我会跳过推导部分，但如果有人感兴趣，请告诉我，我会在评论区发布。
- en: 'So our final derivative of the cost function with respect to any weight is:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们对任何权重的成本函数的最终导数是：
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now the slope can be simplified as:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，斜率可以简化为：
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: we have the `z_del` variable, which contains the product of `dcost` and `dpred`.
    Instead of looping through each record and multiplying the input with the corresponding `z_del`,
    we take the transpose of the input feature matrix and multiply it with the `z_del`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有 `z_del` 变量，它包含 `dcost` 和 `dpred` 的乘积。我们不是对每条记录进行循环，将输入与对应的 `z_del` 相乘，而是取输入特征矩阵的转置，并将其与
    `z_del` 相乘。
- en: Finally, we multiply the learning rate variable `lr` with the derivative to
    increase the speed of learning.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将学习率变量 `lr` 与导数相乘，以提高学习速度。
- en: Along with updating the weights we also have to update the bias term.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 除了更新权重，我们还需要更新偏置项。
- en: '[PRE11]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Once the loop starts, you will see that the total error starts decreasing and
    by the end of the training the error is left with a very small value.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦循环开始，你会看到总误差开始减少，并且在训练结束时，误差会剩下一个非常小的值。
- en: '[PRE12]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Step-5: Make predictions**'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤-5：做预测**'
- en: Its time to make some predictions. Let's try with [1,0,0]
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是做一些预测的时候了。让我们试试 [1,0,0]
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`Output : [0.01031463]`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出 : [0.01031463]`'
- en: As you can see the output is closer to 0 than 1 so it is classified as 0.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，输出值更接近 0 而不是 1，因此它被分类为 0。
- en: Let's try again with [0,1,0]
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再试一次 [0,1,0]
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`Output : [0.99440207]`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出 : [0.99440207]`'
- en: As you can see the output is closer to 1 than 0 so it is classified as 1.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，输出值更接近 1 而不是 0，因此它被分类为 1。
- en: Conclusion
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we learned how to create a very simple artificial neural network
    with one input layer and one output layer from scratch using numpy python library.
    This ANN is able to classify linearly separable data.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们学习了如何使用 numpy python 库从零开始创建一个非常简单的人工神经网络，它包含一个输入层和一个输出层。这个人工神经网络能够对线性可分的数据进行分类。
- en: What if we have non-linearly separated data, our ANN will not be able to classify
    that type of data. This is what we are going to build in part-2 of this article
    series.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有非线性分隔的数据，我们的人工神经网络将无法对这种数据进行分类。这是我们将在本系列文章的第二部分中构建的内容。
- en: Well, that's all. I hope you guys have enjoyed reading this article. Please
    share your thoughts/doubts in the comment section.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，就这些。我希望你们喜欢阅读这篇文章。请在评论区分享你的想法或疑问。
- en: You can reach me out over [LinkedIn](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/?source=post_page---------------------------) for
    any query.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/73b355546c187367af36f2a1ff722ef5.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: '[Pic credit](https://gifer.com/en/7DFc)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!!!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Nagesh Singh Chauhan](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)**
    is a Big data developer at CirrusLabs. He has over 4 years of working experience
    in various sectors like Telecom, Analytics, Sales, Data Science having specialisation
    in various Big data components.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/build-an-artificial-neural-network-ann-from-scratch-part-1-a21988497962).
    Reposted with permission.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[Introduction to Artificial Neural Networks](/2019/10/introduction-artificial-neural-networks.html)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Friendly Introduction to Support Vector Machines](/2019/09/friendly-introduction-support-vector-machines.html)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Classifying Heart Disease Using K-Nearest Neighbors](/2019/07/classifying-heart-disease-using-k-nearest-neighbors.html)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How to Build and Train a Transformer Model from Scratch with…](https://www.kdnuggets.com/how-to-build-and-train-a-transformer-model-from-scratch-with-hugging-face-transformers)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Learn Deep Learning by Building 15 Neural Network Projects in 2022](https://www.kdnuggets.com/2022/01/15-neural-network-projects-build-2022.html)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Neural Network Optimization with AIMET](https://www.kdnuggets.com/2022/04/qualcomm-neural-network-optimization-aimet.html)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a Convolutional Neural Network with PyTorch](https://www.kdnuggets.com/building-a-convolutional-neural-network-with-pytorch)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Importance of Permutation in Neural Network Predictions](https://www.kdnuggets.com/2022/12/importance-permutation-neural-network-predictions.html)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
