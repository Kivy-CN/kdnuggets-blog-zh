- en: 'LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/08/langchain-streamlit-llama-bringing-conversational-ai-local-machine.html](https://www.kdnuggets.com/2023/08/langchain-streamlit-llama-bringing-conversational-ai-local-machine.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine](../Images/27ddeaf200b7c811937a171c487ffd39.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Author
  prefs: []
  type: TYPE_NORMAL
- en: In the past few months, ***Large Language Models (LLMs)*** have gained significant
    attention, capturing the interest of developers across the planet. These models
    have created exciting prospects, especially for developers working on chatbots,
    personal assistants, and content creation. The possibilities that LLMs bring to
    the table have sparked a wave of enthusiasm in the Developer | AI | NLP community.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: What are LLMs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) refer to machine learning models capable of producing
    text that closely resembles human language and comprehending prompts in a natural
    manner. These models undergo training using extensive datasets comprising books,
    articles, websites, and other sources. By analyzing statistical patterns within
    the data, LLMs predict the most probable words or phrases that should follow a
    given input.
  prefs: []
  type: TYPE_NORMAL
- en: '![LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine](../Images/e991e62406a4b0d7a08a09757b8f64d8.png)'
  prefs: []
  type: TYPE_IMG
- en: '**A timeline of LLMs in recent years: **[**A Survey of Large Language Models**](https://arxiv.org/abs/2303.18223)'
  prefs: []
  type: TYPE_NORMAL
- en: By utilizing Large Language Models (LLMs), we can incorporate domain-specific
    data to address inquiries effectively. This becomes especially advantageous when
    dealing with information that was not accessible to the model during its initial
    training, such as a company’s internal documentation or knowledge repository.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture employed for this purpose is known as ***Retrieval Augmentation
    Generation*** or, less commonly, ***Generative Question Answering.***
  prefs: []
  type: TYPE_NORMAL
- en: What is LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LangChain is an impressive and freely available framework meticulously crafted
    to empower developers in creating applications fueled by the might of language
    models, particularly large language models (LLMs).
  prefs: []
  type: TYPE_NORMAL
- en: LangChain revolutionizes the development process of a wide range of applications,
    including chatbots, Generative Question-Answering (GQA), and summarization. By
    seamlessly **chaining** together components sourced from multiple modules, LangChain
    enables the creation of exceptional applications tailored around the power of
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Read More: **[**Official Documentation**](https://python.langchain.com/docs/get_started/introduction.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Motivation?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine](../Images/76904d8d6b7759d86c1d9c30d6e1e002.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Image By Author**'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will demonstrate the process of creating your own Document
    Assistant from the ground up, utilizing LLaMA 7b and Langchain, an open-source
    library specifically developed for seamless integration with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an overview of the blog’s structure, outlining the specific sections
    that will provide a detailed breakdown of the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**Setting up the virtual environment and creating file structure**`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`**Getting LLM on your local machine**`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`**Integrating LLM with LangChain and customizing PromptTemplate**`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`**Document Retrieval and Answer Generation**`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`**Building application using Streamlit**`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Section 1: Setting Up the Virtual Environment and Creating File Structure'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Setting up a virtual environment provides a controlled and isolated environment
    for running the application, ensuring that its dependencies are separate from
    other system-wide packages. This approach simplifies the management of dependencies
    and helps maintain consistency across different environments.
  prefs: []
  type: TYPE_NORMAL
- en: To set up the virtual environment for this application, I will provide the pip
    file in my GitHub repository. First, let’s create the necessary file structure
    as depicted in the figure. Alternatively, you can simply clone the repository
    to obtain the required files.
  prefs: []
  type: TYPE_NORMAL
- en: '![LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine](../Images/a4c8b4dc938d177ac3e69737c4730181.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Image By Author: File Structure**'
  prefs: []
  type: TYPE_NORMAL
- en: Inside the models' folder, we will store the LLMs that we will download, while
    the pip file will be located in the root directory.
  prefs: []
  type: TYPE_NORMAL
- en: To create the virtual environment and install all the dependencies within it,
    we can use the `**pipenv install**`command from the same directory or simply run
    `**setup_env.bat**` batch file, It will install all the dependencies from the `**pipfile**`.
    This will ensure that all the necessary packages and libraries are installed in
    the virtual environment. Once the dependencies are successfully installed, we
    can proceed to the next step, which involves downloading the desired models. Here
    is the [repo](https://github.com/afaqueumer/DocQA).
  prefs: []
  type: TYPE_NORMAL
- en: 'Section 2: Getting LLaMA on your local machine'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is LLaMA?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLaMA is a new large language model designed by Meta AI, which is Facebook’s
    parent company. With a diverse collection of models ranging from 7 billion to
    65 billion parameters, LLaMA stands out as one of the most comprehensive language
    models available. On February 24th, 2023, Meta released the LLaMA model to the
    public, demonstrating their dedication to open science.
  prefs: []
  type: TYPE_NORMAL
- en: '![LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine](../Images/821f71d5744b7f82f05e5a4e844c6461.png)'
  prefs: []
  type: TYPE_IMG
- en: Image Source: [LLaMA](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)
  prefs: []
  type: TYPE_NORMAL
- en: Considering the remarkable capabilities of LLaMA, we have chosen to utilize
    this powerful language model for our purposes. Specifically, we will be employing
    the smallest version of LLaMA, known as LLaMA 7B. Even at this reduced size, LLaMA
    7B offers significant language processing capabilities, allowing us to achieve
    our desired outcomes efficiently and effectively.
  prefs: []
  type: TYPE_NORMAL
- en: '***Official Research Paper* : **`[**LLaMA: Open and Efficient Foundation Language
    Models**](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To execute the LLM on a local CPU, we need a local model in GGML format. Several
    methods can achieve this, but the simplest approach is to download the bin file
    directly from the [**Hugging Face Models repository**](https://huggingface.co/models).
    In our case, we will download the Llama 7B model. These models are open-source
    and freely available for download.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re looking to save time and effort, don’t worry — I’ve got you covered.
    Here’s the direct link for you to download the models [?](https://huggingface.co/TheBloke/LLaMa-7B-GGML).
    Simply download any version of it and then move the file into the models directory
    within our root directory. This way, you’ll have the model conveniently accessible
    for your usage.
  prefs: []
  type: TYPE_NORMAL
- en: What is GGML? Why GGML? How GGML? LLaMA CPP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GGML is a Tensor library for machine learning, it is just a C++ library that
    allows you to run LLMs on just the CPU or CPU + GPU. It defines a binary format
    for distributing large language models (LLMs). GGML makes use of a technique called ***quantization*** that
    allows for large language models to run on consumer hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Now what is Quantization?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLM weights are floating point (decimal) numbers. Just like it requires more
    space to represent a large integer (e.g. 1000) compared to a small integer (e.g.
    1), it requires more space to represent a high-precision floating point number
    (e.g. 0.0001) compared to a low-precision floating number (e.g. 0.1). The process
    of ***quantizing*** a large language model involves reducing the precision with
    which weights are represented in order to reduce the resources required to use
    the model. GGML supports a number of different quantization strategies (e.g. 4-bit,
    5-bit, and 8-bit quantization), each of which offers different trade-offs between
    efficiency and performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine](../Images/1c2bbc0d51b28e1de91c6ef15b2ec866.png)'
  prefs: []
  type: TYPE_IMG
- en: Quantized Size of Llama
  prefs: []
  type: TYPE_NORMAL
- en: To effectively use the models, it is essential to consider the memory and disk
    requirements. Since the models are currently loaded entirely into memory, you
    will need sufficient disk space to store them and enough RAM to load them during
    execution. When it comes to the 65B model, even after quantization, it is recommended
    to have at least 40 gigabytes of RAM available. It’s worth noting that the memory
    and disk requirements are currently equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization plays a crucial role in managing these resource demands. Unless
    you have access to exceptional computational resources
  prefs: []
  type: TYPE_NORMAL
- en: By reducing the precision of the model’s parameters and optimizing memory usage,
    quantization enables the models to be utilized on more modest hardware configurations.
    This ensures that running the models remains feasible and efficient for a wider
    range of setups.
  prefs: []
  type: TYPE_NORMAL
- en: How do we use it in Python if it's a C++ library?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: That's where Python bindings come into play. Binding refers to the process of
    creating a bridge or interface between two languages for us python and C++. We
    will use `**llama-cpp-python**`which is a Python binding for `**llama.cpp**` which
    acts as an Inference of the LLaMA model in pure C/C++. The main goal of `**llama.cpp**` is
    to run the LLaMA model using 4-bit integer quantization. This integration allows
    us to effectively utilize the LLaMA model, leveraging the advantages of C/C++
    implementation and the benefits of 4-bit integer quantization
  prefs: []
  type: TYPE_NORMAL
- en: '![LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine](../Images/36db440161ec441ebaedf10dddfeb864.png)'
  prefs: []
  type: TYPE_IMG
- en: Supported Models by llama.cpp : [Source](https://github.com/ggerganov/llama.cpp)
  prefs: []
  type: TYPE_NORMAL
- en: With the GGML model prepared and all our dependencies in place (thanks to the
    pipfile), it’s time to embark on our journey with LangChain. But before diving
    into the exciting world of LangChain, let’s kick things off with the customary **“Hello
    World”** ritual — a tradition we follow whenever exploring a new language or framework,
    after all, LLM is also a language model.
  prefs: []
  type: TYPE_NORMAL
- en: '![LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine](../Images/7cb74721780631998d742ea12e5c5e43.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image By Author: Interaction with LLM on CPU'
  prefs: []
  type: TYPE_NORMAL
- en: '*Voilà !!! *We have successfully executed our first LLM on the CPU, completely
    offline and in a fully randomized fashion(you can play with the hyper param **temperature**).'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this exciting milestone accomplished, we are now ready to embark on our
    primary objective: question answering of custom text using the LangChain framework.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Section 3: Getting Started with LLM — LangChain Integration'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last section, we initialized LLM using llama cpp. Now, let’s leverage
    the LangChain framework to develop applications using LLMs. The primary interface
    through which you can interact with them is through text. As an oversimplification,
    a lot of models are **text in, text out**. Therefore, a lot of the interfaces
    in LangChain are centered around the text.
  prefs: []
  type: TYPE_NORMAL
- en: The Rise of Prompt Engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the ever-evolving field of programming a fascinating paradigm has emerged: **Prompting**.
    Prompting involves providing specific input to a language model to elicit a desired
    response. This innovative approach allows us to shape the output of the model
    based on the input we provide.
  prefs: []
  type: TYPE_NORMAL
- en: It’s remarkable how the nuances in the way we phrase a prompt can significantly
    impact the nature and substance of the model’s response. The outcome may vary
    fundamentally based on the wording, highlighting the importance of careful consideration
    when formulating prompts.
  prefs: []
  type: TYPE_NORMAL
- en: For providing seamless interaction with LLMs, LangChain provides several classes
    and functions to make constructing and working with prompts easy using a prompt
    template. It is a reproducible way to generate a prompt. It contains a text string **the
    template**, that can take in a set of parameters from the end user and generates
    a prompt. Let’s take a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: '![LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine](../Images/5a8edcd9f81501a0e08b0070b2bf93fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image By Author: Prompt with no Input Variables'
  prefs: []
  type: TYPE_NORMAL
- en: '![LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine](../Images/8be0c291e1eeb6878ebd2a97aff72294.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image By Author: Prompt with one Input Variables'
  prefs: []
  type: TYPE_NORMAL
- en: '![LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine](../Images/a2765b9f4326d4810b39972bcfa3f5ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image By Author: Prompt with multiple Input Variables'
  prefs: []
  type: TYPE_NORMAL
- en: I hope that the previous explanation has provided a clearer grasp of the concept
    of prompting. Now, let’s proceed to prompt the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: '![LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine](../Images/27412fb10867abc0e375460b956ed473.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image By Author: Prompting through Langchain LLM'
  prefs: []
  type: TYPE_NORMAL
- en: This worked perfectly fine but this ain’t the optimum utilisation of LangChain.
    So far we have used individual components. We took the prompt template formatted
    it, then took the llm, and then passed those params inside llm to generate the
    answer. Using an LLM in isolation is fine for simple applications, but more complex
    applications require chaining LLMs — either with each other or with other components.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain provides the Chain interface for such **chained**applications. We
    define a Chain very generically as a sequence of calls to components, which can
    include other chains. Chains allow us to combine multiple components together
    to create a single, coherent application. For example, we can create a chain that
    takes user input, formats it with a Prompt Template, and then passes the formatted
    response to an LLM. We can build more complex chains by combining multiple chains
    together, or by combining chains with other components.
  prefs: []
  type: TYPE_NORMAL
- en: To understand one let’s create a very simple **chain** that will take user input,
    format the prompt with it, and then send it to the LLM using the above individual
    components that we’ve already created.
  prefs: []
  type: TYPE_NORMAL
- en: '![LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine](../Images/d0aef0755947767e82235ed12042833a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image By Author: Chaining in LangChain'
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with multiple variables, you have the option to input them collectively
    by utilizing a dictionary. That concludes this section. Now, let’s dive into the
    main part where we’ll incorporate external text as a retriever for question-answering
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Section 4: Generating Embeddings and Vectorstore for Question Answering'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In numerous LLM applications, there is a need for user-specific data that isn’t
    included in the model’s training set. LangChain provides you with the essential
    components to load, transform, store, and query your data.
  prefs: []
  type: TYPE_NORMAL
- en: '![LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine](../Images/f43a713bee756508b0a6b149f7f224a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Data Connection in LangChain: Source'
  prefs: []
  type: TYPE_NORMAL
- en: 'The five stages are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document Loader:** It is used for loading data as documents.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Document Transformer:** It split the document into smaller chunks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Embeddings:** It transforms the chunks into vector representations a.k.a
    embedding.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Vector Stores:** It is used to store the above chunk vectors in a vector
    database.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Retrievers:** It is used for retrieving a set/s of vector/s that are most
    similar to a query in a form of a vector that is embedded in the same Latent space.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine](../Images/6a911e0e7b279f2a0f0b85b8d52ad652.png)'
  prefs: []
  type: TYPE_IMG
- en: Document Retrieval / Question-Answering Cycle
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will walk through each of the five steps to perform a retrieval of chunks
    of documents that are most similar to the query. Following that, we can generate
    an answer based on the retrieved vector chunk, as illustrated in the provided
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, before proceeding further, we will need to prepare a text for executing
    the aforementioned tasks. For the purpose of this fictitious test, I have copied
    a text from Wikipedia regarding some popular DC Superheroes. Here is the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '![LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine](../Images/46d8e1d9012cf4743545a981ede46510.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image By Author: Raw Text for Testing'
  prefs: []
  type: TYPE_NORMAL
- en: Loading & Transforming Documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To begin, let’s create a document object. In this example, we’ll utilize the
    text loader. However, Lang chain offers support for multiple documents, so depending
    on your specific document, you can employ different loaders. Next, we’ll employ
    the `**load**` method to retrieve data and load it as documents from a preconfigured
    source.
  prefs: []
  type: TYPE_NORMAL
- en: Once the document is loaded, we can proceed with the transformation process
    by breaking it into smaller chunks. To achieve this, we’ll utilize the TextSplitter.
    By default, the splitter separates the document at the ‘\n\n’ separator. However,
    if you set the separator to null and define a specific chunk size, each chunk
    will be of that specified length. Consequently, the resulting list length will
    be equal to the length of the document divided by the chunk size. In summary,
    it will resemble something like this: `**list length = length of doc / chunk size**`.
    Let’s walk the talk.
  prefs: []
  type: TYPE_NORMAL
- en: '![LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine](../Images/c370b5096a0c441ef7de81ca1cf84cca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image By Author: Loading and Transforming Doc'
  prefs: []
  type: TYPE_NORMAL
- en: Part of the journey is the Embeddings !!!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the most important step. Embeddings generate a vectorized portrayal
    of textual content. This has practical significance since it allows us to conceptualize
    text within a vector space.
  prefs: []
  type: TYPE_NORMAL
- en: Word embedding is simply a vector representation of a word, with the vector
    containing real numbers. Since languages typically contain at least tens of thousands
    of words, simple binary word vectors can become impractical due to a high number
    of dimensions. Word embeddings solve this problem by providing dense representations
    of words in a low-dimensional vector space.
  prefs: []
  type: TYPE_NORMAL
- en: When we talk about retrieval, we refer to retrieving a set of vectors that are
    most similar to a query in a form of a vector that is embedded in the same Latent
    space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The base Embeddings class in LangChain exposes two methods: one for embedding
    documents and one for embedding a query. The former takes as input multiple texts,
    while the latter takes a single text.'
  prefs: []
  type: TYPE_NORMAL
- en: '![LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine](../Images/e907d5209ef2487d1333684af40c818a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image By Author: Embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: For a comprehensive understanding of embeddings, I highly recommend delving
    into the fundamentals as they form the core of how neural networks handle textual
    data. I have extensively covered this topic in one of my blogs utilizing TensorFlow.
    Here is the link.
  prefs: []
  type: TYPE_NORMAL
- en: Word Embeddings — Text Representation for Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating Vector Store & Retrieving Docs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A vector store efficiently manages the storage of embedded data and facilitates
    vector search operations on your behalf. Embedding and storing the resulting embedding
    vectors is a prevalent method for storing and searching unstructured data. During
    query time, the unstructured query is also embedded, and the embedding vectors
    that exhibit the highest similarity to the embedded query are retrieved. This
    approach enables effective retrieval of relevant information from the vector store.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will utilize Chroma, an embedding database and vector store specifically
    crafted to simplify the development of AI applications incorporating embeddings.
    It offers a comprehensive suite of built-in tools and functionalities to facilitate
    your initial setup, all of which can be conveniently installed on your local machine
    by executing a simple `**pip install chromadb**` command.
  prefs: []
  type: TYPE_NORMAL
- en: '![LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine](../Images/edd0230ac4a9b9ce09f964137857ea7d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image By Author: Creating Vector Store'
  prefs: []
  type: TYPE_NORMAL
- en: Up until now, we’ve witnessed the remarkable capability of embeddings and vector
    stores in retrieving relevant chunks from extensive document collections. Now,
    the moment has come to present this retrieved chunk as a context alongside our
    query, to the LLM. With a flick of its magical wand, we shall beseech the LLM
    to generate an answer based on the information that we provided to it. The important
    part is the prompt structure.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is crucial to emphasize the significance of a well-structured prompt.
    By formulating a well-crafted prompt, we can mitigate the potential for the LLM
    to engage in **hallucination **— wherein it might invent facts when faced with
    uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: Without prolonging the wait any further, let us now proceed to the final phase
    and discover if our LLM is capable of generating a compelling answer. The time
    has come to witness the culmination of our efforts and unveil the outcome. Here
    we Goooooo ?
  prefs: []
  type: TYPE_NORMAL
- en: '![LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine](../Images/944a48f62adf73996e3ea293794b3a81.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image By Author: Q/A with the Doc'
  prefs: []
  type: TYPE_NORMAL
- en: This is the moment we’ve been waiting for! We’ve accomplished it! We have just
    built our very own question-answering bot utilizing the LLM running locally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Section 5: Chain All using Streamlit'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section is entirely optional since it doesn’t serve as a comprehensive
    guide to Streamlit. I won’t delve deep into this part; instead, I’ll present a
    basic application that allows users to upload any text document. They will then
    have the option to ask questions through text input. Behind the scenes, the functionality
    will remain consistent with what we covered in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is a caveat when it comes to file uploads in Streamlit. To prevent
    potential out-of-memory errors, particularly considering the memory-intensive
    nature of LLMs, I’ll simply read the document and write it to the temporary folder
    within our file structure, naming it `**raw.txt.**` This way, regardless of the
    document’s original name, Textloader will seamlessly process it in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, the app is designed for text files, but you can adapt it for PDFs,
    CSVs, or other formats. The underlying concept remains the same since LLMs are
    primarily designed for text input and output. Additionally, you can experiment
    with different LLMs supported by the Llama C++ bindings.
  prefs: []
  type: TYPE_NORMAL
- en: Without delving further into intricate details, I present the code for the app.
    Feel free to customize it to suit your specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s what the streamlit app will look like.
  prefs: []
  type: TYPE_NORMAL
- en: '![LangChain + Streamlit + Llama: Bringing Conversational AI to Your Local Machine](../Images/53f1d5576ae768ec3ddedd9e447eb1d3.png)'
  prefs: []
  type: TYPE_IMG
- en: This time I fed the plot of **The Dark Knight** copied from Wiki and just asked **Whose
    face is severely burnt? **and the LLM replied — **Harvey Dent**.
  prefs: []
  type: TYPE_NORMAL
- en: All right, all right, all right! With that, we come to the end of this blog.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you enjoyed this article! and found it informative and engaging. You
    can **follow me **[Afaque Umer](https://medium.com/u/430bc504f9d9) for **more** such
    articles.
  prefs: []
  type: TYPE_NORMAL
- en: I will try to bring up more **M****achine learning/Data science concepts **and
    will try to break down fancy-sounding terms and concepts into simpler ones.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Afaque Umer](https://www.linkedin.com/in/afaque-umer/)** is a passionate
    Machine Learning Engineer. He love tackling new challenges using latest tech to
    find efficient solutions. Let''s push the boundaries of AI together!'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://ai.plainenglish.io/%EF%B8%8F-langchain-streamlit-llama-bringing-conversational-ai-to-your-local-machine-a1736252b172).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Llama, Llama, Llama: 3 Simple Steps to Local RAG with Your Content](https://www.kdnuggets.com/3-simple-steps-to-local-rag-with-your-content)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3 Crucial Challenges in Conversational AI Development and How to Avoid Them](https://www.kdnuggets.com/3-crucial-challenges-in-conversational-ai-development-and-how-to-avoid-them)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[DIY Automated Machine Learning with Streamlit](https://www.kdnuggets.com/2021/11/diy-automated-machine-learning-app.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Streamlit for Machine Learning Cheat Sheet](https://www.kdnuggets.com/2023/01/streamlit-machine-learning-cheat-sheet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Answering Questions with HuggingFace Pipelines and Streamlit](https://www.kdnuggets.com/2021/10/simple-question-answering-web-app-hugging-face-pipelines.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deploying a Streamlit WebApp to Heroku using DAGsHub](https://www.kdnuggets.com/2022/02/deploying-streamlit-webapp-heroku-dagshub.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
