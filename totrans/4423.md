# 在 Azure Databricks 上使用 Spark、Python 或 SQL

> 原文：[https://www.kdnuggets.com/2020/08/spark-python-sql-azure-databricks.html](https://www.kdnuggets.com/2020/08/spark-python-sql-azure-databricks.html)

[评论](#comments)

**由 [Ajay Ohri](http://linkedin.com/in/ajayohri)，数据科学经理**

**Azure Databricks** 是微软提供的基于 Apache Spark 的大数据分析服务，旨在数据科学和数据工程领域。它支持协作工作以及 Python、Spark、R 和 SQL 等多种语言的使用。使用 Databricks 的好处包括云计算的优势 - 可扩展、成本较低、按需数据处理和数据存储。

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速开启网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升您的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持您组织的 IT

* * *

在这里，我们将探讨一些在 Python、PySpark 和 SQL 之间互换使用的方法。我们学习如何从 CSV 文件中导入数据，先上传文件，然后选择在笔记本中创建它。我们学习如何将 SQL 表转换为 Spark Dataframe，并将 Spark Dataframe 转换为 Python Pandas Dataframe。我们还学习如何将 Spark Dataframe 转换为永久或临时的 SQL 表。

为什么我们需要学习如何在 SQL、Spark 和 Python Panda Dataframe 之间互换代码？SQL 在数据处理方面书写简单且易读，Spark 在大数据和机器学习方面速度极快，而 Python Pandas 可以用于数据处理、机器学习以及 seaborn 或 matplotlib 库中的绘图等各种任务。

![Image](../Images/10e3236941f0b760eb80a0b99e6a2abe.png)

我们选择 SQL 笔记本以方便操作，然后选择适当的集群，包括适当的 RAM、核心、Spark 版本等。即使这是一个 SQL 笔记本，我们也可以通过在该单元格的代码前输入 %python 来编写 Python 代码。

![Image](../Images/f90682a67b15f98366a13a8280832397.png)

现在让我们开始数据输入、数据检查和数据互换的基础知识

**第 1 步** 读取上传的数据

```py
%python

# Reading in Uploaded Data
# File location and type
file_location = "/FileStore/tables/inputdata.csv"
file_type = "csv"

# CSV options
infer_schema = "false"
first_row_is_header = "true"
delimiter = ","

# The applied options are for CSV files. For other file types, these will be ignored.
df = spark.read.format(file_type) \
  .option("inferSchema", infer_schema) \
  .option("header", first_row_is_header) \
  .option("sep", delimiter) \
  .load(file_location)

display(df)
```

**第 2 步** 从 SPARK Dataframe 创建临时视图或表

```py
%python
#Create a temporary view or table from SPARK Dataframe
temp_table_name = "temp_table"
df.createOrReplaceTempView(temp_table_name)
```

**第 3 步** 从 SPARK Dataframe 创建永久 SQL 表

```py
--Creating Permanent SQL Table from SPARK Dataframe
permanent_table_name = "cdp.perm_table"

df.write.format("parquet").saveAsTable(permanent_table_name)
```

**第 4 步** 检查 SQL 表

```py
--Inspecting SQL Table
select * from cdp.perm_table
```

**第 5 步** 将 SQL 表转换为 SPARK Dataframe

```py
%python
#Converting SQL Table to SPARK Dataframe
sparkdf = spark.sql("select *  from cdp.perm_table")
```

**第 6 步** 检查 SPARK Dataframe

```py
%python
#Inspecting Spark Dataframe 
sparkdf.take(10)
```

**第 7 步** 将 Spark Dataframe 转换为 Python Pandas Dataframe

```py
%python
#Converting Spark Dataframe to Python Pandas Dataframe
%python
pandasdf=sparkdf.toPandas()
```

**第 8 步** 检查 Python Dataframe

```py
%python
#Inspecting Python Dataframe 
pandasdf.head()
```

**参考资料**

1.  Azure Databricks 介绍 - [https://www.slideshare.net/jamserra/introduction-to-azure-databricks-83448539](https://www.slideshare.net/jamserra/introduction-to-azure-databricks-83448539)

1.  Dataframes 和 Datasets - [https://docs.databricks.com/spark/latest/dataframes-datasets/index.html](https://docs.databricks.com/spark/latest/dataframes-datasets/index.html)

1.  优化 PySpark 与 pandas DataFrames 之间的转换 - [https://docs.databricks.com/spark/latest/spark-sql/spark-pandas.html](https://docs.databricks.com/spark/latest/spark-sql/spark-pandas.html)

1.  pyspark 包 - [https://spark.apache.org/docs/latest/api/python/pyspark.html](https://spark.apache.org/docs/latest/api/python/pyspark.html)

**简介： [Ajay Ohri](http://linkedin.com/in/ajayohri)** 是数据科学经理（Publicis Sapient）以及《R for Cloud Computing》和《Python for R Users》等4本数据科学书籍的作者。

**相关内容**：

+   [Apache Spark 在 Dataproc 与 Google BigQuery 之间的比较](/2020/07/apache-spark-dataproc-vs-google-bigquery.html)

+   [使用 Kubernetes 容器化 PySpark](/2020/08/containerization-pyspark-kubernetes.html)

+   [5 个 Apache Spark 数据科学最佳实践](/2020/08/5-spark-best-practices-data-science.html)

### 更多相关主题

+   [优化数据分析：在 Databricks 中集成 GitHub Copilot](https://www.kdnuggets.com/optimizing-data-analytics-integrating-github-copilot-in-databricks)

+   [在 Python 中处理 SQLite 数据库的指南](https://www.kdnuggets.com/a-guide-to-working-with-sqlite-databases-in-python)

+   [在机器学习模型中处理稀疏特征](https://www.kdnuggets.com/2021/01/sparse-features-machine-learning-models.html)

+   [处理大数据：工具和技术](https://www.kdnuggets.com/working-with-big-data-tools-and-techniques)

+   [让深度学习在实际应用中发挥作用：数据驱动课程](https://www.kdnuggets.com/2022/04/corise-deep-learning-wild-data-centric-course.html)

+   [6 种远程工作的数据科学家的软技能](https://www.kdnuggets.com/2022/05/6-soft-skills-data-scientists-working-remotely.html)
