- en: A Comprehensive Guide to Ensemble Learning – Exactly What You Need to Know
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/05/comprehensive-guide-ensemble-learning.html](https://www.kdnuggets.com/2021/05/comprehensive-guide-ensemble-learning.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Ensemble learning techniques](https://www.toptal.com/machine-learning/ensemble-methods-machine-learning) have
    been proven to yield better performance on machine learning problems. We can use
    these techniques for regression as well as classification problems.'
  prefs: []
  type: TYPE_NORMAL
- en: The final prediction from these ensembling techniques is obtained by combining
    results from several base models. Averaging, voting and stacking are some of the
    ways the results are combined to obtain a final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will explore how ensemble learning can be used to come up
    with optimal machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: What is ensemble learning?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ensemble learning is a combination of several machine learning models in one
    problem. These models are known as weak learners. The intuition is that when you
    combine several weak learners, they can become strong learners.
  prefs: []
  type: TYPE_NORMAL
- en: Each weak learner is fitted on the training set and provides predictions obtained.
    The final prediction result is computed by combining the results from all the
    weak learners.
  prefs: []
  type: TYPE_NORMAL
- en: Basic ensemble learning techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s take a moment and look at simple ensemble learning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '**Max voting**'
  prefs: []
  type: TYPE_NORMAL
- en: In classification, the prediction from each model is a vote. In max voting,
    the final prediction comes from the prediction with the most votes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take an example where you have three classifiers with the following predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: classifier 1 – class A
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: classifier 2 – class B
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: classifier 3 – class B
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final prediction here would be class B since it has the most votes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Averaging**'
  prefs: []
  type: TYPE_NORMAL
- en: In averaging, the final output is an average of all predictions. This goes for
    regression problems. For example, in [random forest regression](https://neptune.ai/blog/random-forest-regression-when-does-it-fail-and-why),
    the final result is the average of the predictions from individual decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take an example of three regression models that predict the price of
    a commodity as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: regressor 1 – 200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: regressor 2 – 300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: regressor 3 – 400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final prediction would be the average of 200, 300, and 400.
  prefs: []
  type: TYPE_NORMAL
- en: '**Weighted average**'
  prefs: []
  type: TYPE_NORMAL
- en: In weighted averaging, the base model with higher predictive power is more important.
    In the price prediction example, each of the regressors would be assigned a weight.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sum of the weights would equal one. Let’s say that the regressors are given
    weights of 0.35, 0.45, and 0.2 respectively. The final model prediction can be
    computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 0.35 * 200 + 0.45*300 + 0.2*400 = 285
  prefs: []
  type: TYPE_NORMAL
- en: Advanced ensemble learning techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Above are simple techniques, now let’s take a look at advanced techniques for
    ensemble learning.
  prefs: []
  type: TYPE_NORMAL
- en: Stacking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Stacking is the process of combining various estimators in order to reduce their
    biases. Predictions from each estimator are stacked together and used as input
    to a final estimator (usually called a *meta-model*) that computes the final prediction.
    Training of the final estimator happens via cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: Stacking can be done for both regression and classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: '![Ensemble learning techniques](../Images/171b9e69dd3a956d32a20582ebe4f151.png)'
  prefs: []
  type: TYPE_IMG
- en: '[*Source*](https://towardsdatascience.com/stacking-classifiers-for-higher-predictive-performance-566f963e4840)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stacking can be considered to happen in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the data into a training and validation set,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divide the training set into K folds, for example 10,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a base model (say SVM) on 9 folds and make predictions on the 10th fold,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat until you have a prediction for each fold,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the base model on the whole training set,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the model to make predictions on the test set,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step 3 – 6 for other base models (for example decision trees),
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use predictions from the test set as features to a new model – *the meta-model,*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make final predictions on the test set using the meta model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With regression problems, the values passed to the meta-model are numeric. With
    classification problems, they’re probabilities or class labels.
  prefs: []
  type: TYPE_NORMAL
- en: Blending
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Blending is similar to stacking, but uses a holdout set from the training set
    to make predictions. So, predictions are done on the holdout set only. The predictions
    and holdout set are used to build a final model that makes predictions on the
    test set.
  prefs: []
  type: TYPE_NORMAL
- en: You can think of blending as a type of stacking, where the meta-model is trained
    on predictions made by the base model on the hold-out validation set.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can consider the *blending* process to be:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the data into a test and validation set,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fit base models on the validation set,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make predictions on the validation and test set,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the validation set and its predictions to build a final model,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make final predictions using this model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concept of blending [was made popular](https://netflixprize.com/assets/ProgressPrize2008_BellKor.pdf) by
    the [Netflix Prize competition](https://en.wikipedia.org/wiki/Netflix_Prize).
    The winning team used a blended solution to achieve a 10-fold performance improvement
    on Netflix’s movie recommendation algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to this [Kaggle ensembling guide](https://mlwave.com/kaggle-ensembling-guide/):'
  prefs: []
  type: TYPE_NORMAL
- en: “Blending is a word introduced by the Netflix winners. It’s very close to stacked
    generalization, but a bit simpler and less risk of an information leak. Some researchers
    use “stacked ensembling” and “blending” interchangeably.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: With blending, instead of creating out-of-fold predictions for the train set,
    you create a small holdout set of say 10% of the train set. The stacker model
    then trains on this holdout set only.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Blending vs stacking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Blending is simpler than stacking and prevents leakage of information in the
    model. The generalizers and the stackers use different datasets.  However, blending
    uses less data and may lead to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation is more solid on stacking than blending. It’s calculated over
    more folds, compared to using a small hold-out dataset in blending.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bagging takes random samples of data, builds learning algorithms, and uses the
    mean to find bagging probabilities. It’s also called *bootstrap aggregating*.
    Bagging aggregates the results from several models in order to obtain a generalized
    result.
  prefs: []
  type: TYPE_NORMAL
- en: 'The method involves:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating multiple subsets from the original dataset with replacement,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a base model for each of the subsets,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running all the models in parallel,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining predictions from all models to obtain final predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Boosting is a machine learning ensemble technique that reduces bias and variance
    by converting weak learners into strong learners. The weak learners are applied
    to the dataset in a sequential manner. The first step is building an initial model
    and fitting it into the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'A second model that tries to fix the errors generated by the first model is
    then fitted. Here’s what the entire process looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a subset from the original data,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build an initial model with this data,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run predictions on the whole data set,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the error using the predictions and the actual values,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign more weight to the incorrect predictions,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create another model that attempts to fix errors from the last model,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run predictions on the entire dataset with the new model,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create several models with each model aiming at correcting the errors generated
    by the previous one,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obtain the final model by weighting the mean of all the models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Libraries for ensemble learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With that introduction out of the way, let’s talk about libraries that you
    can use for ensembling. Broadly speaking, there are two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Bagging algorithms,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bagging algorithms are based on the bagging technique described above. Let’s
    take a look at a couple of them.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging meta-estimator**'
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn lets us implement a `[BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)`
    and a `[BaggingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html)`.
    The bagging meta-estimator fits each base model on random subsets of the original
    dataset. It then computes the final prediction by aggregating individual base
    model predictions. Aggregation is done by voting or averaging. The method reduces
    the variance of estimators by introducing randomization in their construction
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several flavors of bagging:'
  prefs: []
  type: TYPE_NORMAL
- en: Drawing random subsets of the data as random subsets of the samples is referred
    to as *pasting*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm is referred to as *bagging* when the samples are drawn with replacement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If random data subsets are taken as random subsets of the feature, the algorithm
    is referred to as *Random Subspaces*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you create base estimators from subsets of both samples and features, it’s *Random
    Patches*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s take a look at how you can create a bagging estimator using Scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'This takes a few steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the `BaggingClassifier`,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Import a base estimator – a decision tree classifier,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create an instance of the `BaggingClassifier`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The bagging classifier takes several arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: The base estimator – here, a decision tree classifier,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of estimators you want in the ensemble,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_samples` to define the number of samples that will be drawn from the training
    set for each base estimator ,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_features` to dictate the number of features that will be used to train
    each base estimator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, you can fit this classifier on the training set and score it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The process will be the same for regression problems, the only difference being
    that you will work with regression estimators.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Forests of randomized trees**'
  prefs: []
  type: TYPE_NORMAL
- en: A [Random Forest](https://neptune.ai/blog/random-forest-regression-when-does-it-fail-and-why)® is
    an ensemble of random decision trees. Each decision tree is created from a different
    sample of the dataset. The samples are drawn with replacement. Every tree produces
    its own prediction.
  prefs: []
  type: TYPE_NORMAL
- en: In regression, these results are averaged to obtain the final result.
  prefs: []
  type: TYPE_NORMAL
- en: In classification, the final result can be obtained as the class with the most
    votes.
  prefs: []
  type: TYPE_NORMAL
- en: The averaging and voting improves the accuracy of the model by preventing overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: In Scikit-learn a forest of randomized trees can be implemented via `RandomForestClassifier`
    and the `ExtraTreesClassifier`. Similar estimators are available for regression
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Boosting algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These algorithms are based on the boosting framework described earlier. Let’s
    look at a few of them.
  prefs: []
  type: TYPE_NORMAL
- en: '**AdaBoost**'
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost works by fitting a sequence of weak learners. It gives incorrect predictions
    more weight in subsequent iterations, and less weight to correct predictions.
    This forces the algorithm to focus on observations that are harder to predict.
    The final prediction comes from weighing the majority vote or sum.
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost can be used for both regression and classification problems. Let’s
    take a moment and look at how you can apply the algorithm to a classification
    problem using Scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: We use the `AdaBoostClassifier`. `n_estimators` dictates the number of weak
    learners in the ensemble. The contribution of each weak learner to the final combination
    is controlled by the `learning_rate`.
  prefs: []
  type: TYPE_NORMAL
- en: By default, decision trees are used as base estimators. In order to obtain better
    results, the parameters of the decision tree can be tuned. You can also tune the
    number of base estimators.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Gradient tree boosting**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradient tree boosting also combines a set of weak learners to form a strong
    learner. There are three main items to note, as far as gradient boosting trees
    are concerned:'
  prefs: []
  type: TYPE_NORMAL
- en: a differential loss function has to be used,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: decision trees are used as weak learners,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it’s an additive model, so trees are added one after the other. Gradient descent
    is used to minimize the loss when adding subsequent trees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use Scikit-learn to build a model based on gradient tree boosting.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**eXtreme Gradient Boosting**'
  prefs: []
  type: TYPE_NORMAL
- en: eXtreme Gradient Boosting, popularly known as [XGoost](https://neptune.ai/blog/how-to-organize-your-xgboost-machine-learning-ml-model-development-process),
    is a top gradient boosting framework. It’s based on an ensemble of weak decision
    trees. It can do parallel computations on a single computer.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm uses regression trees for the base learner. It also has cross-validation
    built-in. Developers love it for its accuracy, efficiency, and feasibility.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**LightGBM**'
  prefs: []
  type: TYPE_NORMAL
- en: '[LightGBM](https://lightgbm.readthedocs.io/en/latest/) is a gradient boosting
    algorithm based on tree learning. Unlike other tree-based algorithms that use
    depth-wise growth, LightGBM uses leaf-wise tree growth. Leaf-wise growth algorithms
    tend to converge faster than dep-wise-based algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Level-wise tree growth](../Images/4d9ff80253d95bfb7b410103ab1659c3.png)[*Source*](https://lightgbm.readthedocs.io/en/latest/Features.html?highlight=dart#other-features)'
  prefs: []
  type: TYPE_IMG
- en: '![Leaf-wise tree growth](../Images/a8823777b46d64bee45eb7773159b426.png)[*Source*](https://lightgbm.readthedocs.io/en/latest/Features.html?highlight=dart#other-features)'
  prefs: []
  type: TYPE_IMG
- en: '![Leaf-wise tree growth](../Images/f8c9ea60502b0df73fed238f77564c07.png)[*Source*](https://lightgbm.readthedocs.io/en/latest/Features.html?highlight=dart#other-features)'
  prefs: []
  type: TYPE_IMG
- en: LightGBM can be used for both regression and classification problems by setting
    the appropriate objective.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s how you can apply LightGBM to a binary classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**CatBoost**'
  prefs: []
  type: TYPE_NORMAL
- en: '[CatBoost](https://github.com/catboost) is a depth-wise gradient boosting library
    developed by [Yandex](https://yandex.com/company/). It grows a balanced tree using
    oblivion decision trees. As you can see in the image below, the same features
    are used when making left and right splits at each level.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gradient boosting catboost](../Images/4b59e5ba164ef50d380c1c489426a112.png)*[Source](https://heartbeat.fritz.ai/fast-gradient-boosting-with-catboost-38779b0d5d9a)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Researchers need Catboost for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to handle categorical features natively,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Models can be trained on several GPUs,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It reduces parameter tuning time by providing great results with default parameters,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Models can be exported to Core ML for on-device inference (iOS),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It handles missing values internally,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be used for both regression and classification problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here’s how you can apply CatBoost to a classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Libraries that help you do stacking on base models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When stacking, the output of individual models is stacked and a final estimator
    used to compute the final prediction. The estimators are fitted on the whole training
    set. The final estimator is trained on the cross-validated predictions of the
    base estimators.
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn can be used to stack estimators. Let’s take a look at how you can
    stack estimators for a classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: First, you need to set up the base estimator that you want to use.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, instantiate the stacking classifier. Its parameters include:'
  prefs: []
  type: TYPE_NORMAL
- en: The estimators defined above,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final estimator that you’d like to use. The logistic regression estimator
    is used by default,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cv` the cross-validation generator. Uses 5 k-fold cross-validation by default,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stack_method` to dictate the method to be applied to each estimator. If `auto`,
    it will try `predict_proba`, `decision_function` or `predict`’ in that order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: After that, you can fit the data to the training set and score it on the test
    set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Scikit-learn also lets you implement a voting estimator. It uses the majority
    vote or the average of the probabilities from the base estimators to make the
    final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: This can be implemented using the  `VotingClassifier` for classification problems
    and the `VotingRegressor` for regression problems. Just like stacking, you will
    first have to define a set of base estimators.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at how you can implement it for classification problems. The `VotingClassifier`
    lets you select the voting type:'
  prefs: []
  type: TYPE_NORMAL
- en: '`soft` means that the average of the probabilities will be used to compute
    the final result,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hard` notifies the classifier to use the predicted classes for majority voting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The voting regressor uses several estimators and returns the final result as
    the average of predicted values.
  prefs: []
  type: TYPE_NORMAL
- en: Stacking with Mlxtend
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can also perform stacking using [Mlxtend’s](http://rasbt.github.io/mlxtend/) `[StackingCVClassifier](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/)`.
    The first step is to define a list of base estimators, and then pass the estimators
    to the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: You also have to define the final model that will be used to aggregate the predictions.
    In this case, it’s the logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: When to use ensemble learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can employ ensemble learning techniques when you want to improve the performance
    of machine learning models. For example to increase the accuracy of classification
    models or to reduce the mean absolute error for regression models. Ensembling
    also results in a more stable model.
  prefs: []
  type: TYPE_NORMAL
- en: When your model is overfitting on the training set, you can also employ ensembling
    learning methods to create a more complex model. The models in the ensemble would
    then improve performance on the dataset by combining their predictions.
  prefs: []
  type: TYPE_NORMAL
- en: When ensemble learning works best
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ensemble learning works best when the base models are not correlated. For instance,
    you can train different models such as linear models, decision trees, and neural
    nets on different datasets or features. The less correlated the base models, the
    better.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind using uncorrelated models is that each may be solving a weakness
    of the other. They also have different strengths which, when combined, will result
    in a well-performing estimator. For example, creating an ensemble of just tree-based
    models may not be as effective as combining tree-type algorithms with other types
    of algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Final thoughts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this article, we explored how to use ensemble learning to improve the performance
    of machine learning models. We’ve also gone through various tools and techniques
    that you can use for ensembling. Your machine learning repertoire has hopefully
    grown.
  prefs: []
  type: TYPE_NORMAL
- en: Happy ensembling!
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Kaggle ensembling guide](https://mlwave.com/kaggle-ensembling-guide/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Scikit-learn ensembling guide ](https://scikit-learn.org/stable/modules/ensemble.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Notebook used in the article](https://colab.research.google.com/drive/1MEcl4W1Mr9_rRJEPcY2IHWppJq08bgc2?usp=sharing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bio: [Derrick Mwiti](https://www.linkedin.com/in/mwitiderrick/)** is a data
    scientist who has a great passion for sharing knowledge. He is an avid contributor
    to the data science community via blogs such as Heartbeat, Towards Data Science,
    Datacamp, Neptune AI, KDnuggets just to mention a few. His content has been viewed
    over a million times on the internet. Derrick is also an author and online instructor.
    He also trains and works with various institutions to implement data science solutions
    as well as to upskill their staff. You might want to check his [Complete Data
    Science & Machine Learning Bootcamp in Python course](https://www.udemy.com/course/data-science-bootcamp-in-python/?referralCode=9F6DFBC3F92C44E8C7F4).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://neptune.ai/blog/ensemble-learning-guide). Reposted with
    permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[XGBoost: What it is, and when to use it](/2020/12/xgboost-what-when.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Gradient Boosted Decision Trees – A Conceptual Explanation](/2021/04/gradient-boosted-trees-conceptual-explanation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best Machine Learning Frameworks & Extensions for Scikit-learn](/2021/03/best-machine-learning-frameworks-extensions-scikit-learn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[KDnuggets News, April 13: Python Libraries Data Scientists Should…](https://www.kdnuggets.com/2022/n15.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ensemble Learning with Examples](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[When Would Ensemble Techniques be a Good Choice?](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Statistics for Machine Learning: What you need to know to become a…](https://www.kdnuggets.com/2024/03/sas-statistics-machine-learning-need-know-become-certified-expert)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Want to Use Your Data Skills to Solve Global Problems? Here’s What…](https://www.kdnuggets.com/2022/04/jhu-want-data-skills-solve-global-problems.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
