# 处理（几乎）任何机器学习问题

> 原文：[https://www.kdnuggets.com/2016/08/approaching-almost-any-machine-learning-problem.html/2](https://www.kdnuggets.com/2016/08/approaching-almost-any-machine-learning-problem.html/2)

接下来，我们来到了堆叠模块。堆叠模块不是模型堆叠器，而是特征堆叠器。经过上述处理步骤后的不同特征可以使用堆叠模块进行组合。

![](../Images/0d2fc441e33e2479dc1398c5c92db1e8.png)

你可以在进一步处理之前，使用 numpy hstack 或 sparse hstack 将所有特征水平堆叠，这取决于你有密集特征还是稀疏特征。

![](../Images/534f1f2ac0d4c9ea55dda8088c038ad7.png)

如果还有其他处理步骤，如 pca 或特征选择，也可以通过 FeatureUnion 模块实现（稍后我们会讨论降解和特征选择）。

![](../Images/226c8c202aa480e3cafff0426108d0ab.png)

一旦我们将特征堆叠在一起，我们可以开始应用机器学习模型。在这个阶段，你应该选择的模型是集成树模型。这些模型包括：

+   RandomForestClassifier

+   RandomForestRegressor

+   ExtraTreesClassifier

+   ExtraTreesRegressor

+   XGBClassifier

+   XGBRegressor

由于上述特征未经过归一化，我们不能应用线性模型。要使用线性模型，可以使用 scikit-learn 中的 Normalizer 或 StandardScaler。

这些归一化方法仅适用于密集特征，如果应用于稀疏特征，效果不是很好。是的，可以在稀疏矩阵上应用 StandardScaler，而不使用均值（参数：with_mean=False）。

如果上述步骤生成了一个“好”的模型，我们可以进行超参数优化，如果没有，我们可以采取以下步骤改进模型。

接下来的步骤包括降解方法：

![](../Images/2ea88d8b389703c648a4ba1fbb629e14.png)

为了简单起见，我们将忽略 LDA 和 QDA 转换。对于高维数据，通常使用 PCA 来降解数据。对于图像，从 10-15 个组件开始，如果结果质量显著提高，则增加这个数字。对于其他类型的数据，我们初步选择 50-60 个组件（只要可以处理数字数据，我们倾向于避免 PCA）。

![](../Images/932bf19fff5e1fdbc2d3d4278c273f24.png)

可以在这里找到：

对于文本数据，在将文本转换为稀疏矩阵后，使用奇异值分解（SVD）。在 scikit-learn 中可以找到一个 SVD 的变体，称为 TruncatedSVD。

![](../Images/4283cfbfce157dff2410f4c7bf25e78b.png)

通常适用于 TF-IDF 或计数的 SVD 组件数量在 120-200 之间。高于这个数量可能会提高性能，但不会显著增加，并且会增加计算成本。

在进一步评估模型的表现后，我们转向数据集的扩展，以便也能评估线性模型。归一化或缩放的特征可以发送到机器学习模型或特征选择模块。

![](../Images/1f724aaae7b4f9ab788d90bf2c4aea82.png)

特征选择可以通过多种方式实现。最常见的一种是贪婪特征选择（前向或后向）。在贪婪特征选择中，我们选择一个特征，训练一个模型并在固定的评估指标上评估模型的表现。我们逐一添加和删除特征，并记录每一步的模型表现。然后选择具有最佳评估分数的特征。一个以AUC作为评估指标的贪婪特征选择实现可以在这里找到：[https://github.com/abhishekkrthakur/greedyFeatureSelection](https://github.com/abhishekkrthakur/greedyFeatureSelection)。需要注意的是，这个实现并不完美，需要根据要求进行更改/修改。

其他更快的特征选择方法包括从模型中选择最佳特征。我们可以查看logit模型的系数，或者训练一个随机森林来选择最佳特征，然后将其与其他机器学习模型一起使用。

![](../Images/f4689040f98c60eb3104949992f9d01d.png)

记住保持较少的估计器数量和最小化超参数优化，以免过拟合。

特征选择也可以使用梯度提升机来实现。如果我们使用xgboost而不是scikit-learn中的GBM实现，这将更好，因为xgboost更快且更具可扩展性。

![](../Images/fb2c548d2699931784540723a0f77760.png)

我们还可以使用RandomForestClassifier / RandomForestRegressor和xgboost进行稀疏数据集的特征选择。

从正稀疏数据集中选择特征的另一种流行方法是基于chi-2的特征选择，我们在scikit-learn中也实现了这种方法。

![](../Images/be57230fe837bc13a2b4b0ca2914c157.png)

在这里，我们使用chi2结合SelectKBest从数据中选择20个特征。这也成为我们要优化的超参数，以改进机器学习模型的结果。

不要忘记在所有步骤中保存你使用的任何类型的变换器。你将需要它们来评估验证集上的表现。

下一个（或中间）主要步骤是模型选择 + 超参数优化。

![](../Images/02a6679552c13c5f074c5f62ed401777.png)

我们通常在选择机器学习模型的过程中使用以下算法：

+   **分类**：

    +   随机森林

    +   GBM

    +   逻辑回归

    +   朴素贝叶斯

    +   支持向量机

    +   k-最近邻

+   **回归**

    +   随机森林

    +   GBM

    +   线性回归

    +   岭回归

    +   Lasso

    +   SVR

我应该优化哪些参数？我如何选择最接近最佳的参数？这些是人们最常提出的问题。没有在大量数据集上使用不同模型 + 参数的经验，无法回答这些问题。同时，有经验的人也不愿意分享他们的秘密。幸运的是，我也有相当多的经验，并且愿意分享一些内容。

让我们深入探讨一下超参数，按模型分类：

[![](../Images/999cce9d8f4839d8aa947045af1c85d4.png)](https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAilAAAAJDhiNGE5YTVmLTJhNTAtNGNkZS1hNDAwLTY5YTJiMTE1ZmI3Zg.png)

RS* = 无法确定适当值，请在这些超参数中使用随机搜索。

在我看来，严格来说，我认为上述模型将优于其他任何模型，我们不需要评估其他模型。

再次提醒，记得保存变换器：

[![](../Images/e03f6db43132da2f4d6b3b4ab2aa98cd.png)](https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAhzAAAAJGNhOTU4MGRjLWE3YzAtNDFmMi1hMjg0LWUyOGIwMDBlNzUxYw.png)

然后分别应用于验证集：

[![](../Images/da94fbdea0582fd1492738a7335435b4.png)](https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAllAAAAJDgxZjdjNjcwLTQ0ZDktNDU0Mi04YzQzLThjMGM1NWY5ZWFkNQ.png)

以上规则和框架在我处理的大多数数据集上表现非常好。当然，对于非常复杂的任务，它们也会失败。没有什么是完美的，我们不断改进所学。就像机器学习一样。

如有疑问，请与我联系：abhishek4 [at] gmail [dot] com

**简介： [Abhishek Thakur](https://www.linkedin.com/in/abhisvnit)** 拥有波恩大学计算机科学硕士学位，目前是 Searchmetrics Inc. 数据科学团队的高级数据科学家，从事一些最有趣的数据驱动研究、应用机器学习算法以及从海量数据中提取洞见。他还是 Kaggle 竞赛大师。

[原文](https://www.linkedin.com/pulse/approaching-almost-any-machine-learning-problem-abhishek-thakur)。经许可重新发布。

**相关：**

+   [比赛冠军：使用 Auto-sklearn 赢得 AutoML 挑战](/2016/08/winning-automl-challenge-auto-sklearn.html)

+   [比赛二等奖：自动化数据科学](/2016/08/automating-data-science.html)

+   [TPOT：用于自动化数据科学的 Python 工具](/2016/05/tpot-python-automating-data-science.html)

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织进行IT管理

* * *

### 相关主题

+   [NExT-GPT简介：任意对任意的多模态大型语言模型](https://www.kdnuggets.com/introduction-to-nextgpt-anytoany-multimodal-large-language-model)

+   [处理机器学习过程的框架](https://www.kdnuggets.com/2018/05/general-approaches-machine-learning-process.html)

+   [SHAP：在Python中解释任何机器学习模型](https://www.kdnuggets.com/2022/11/shap-explain-machine-learning-model-python.html)

+   [如何在没有任何工作经验的情况下获得你的第一份数据科学工作](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)

+   [在参加任何免费的数据科学课程之前阅读此内容](https://www.kdnuggets.com/read-this-before-you-take-any-free-data-science-course)

+   [现实世界中NLP应用的范围：一种不同的解决方案](https://www.kdnuggets.com/2022/03/different-solution-problem-range-nlp-applications-real-world.html)
