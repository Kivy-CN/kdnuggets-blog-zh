["```py\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n```", "```py\nfrom torchvision import datasets\n\ntrain = datasets.MNIST(\n    root=\"image_data\",\n    train=True,\n    download=True\n)\n\ntest = datasets.MNIST(\n    root=\"image_data\",\n    train=False,\n    download=True,\n)\n```", "```py\nimport matplotlib.pyplot as plt\n\nfor i, (img, label) in enumerate(list(train)[:10]):\n    plt.subplot(2, 5, i+1)\n    plt.imshow(img, cmap=\"gray\")\n    plt.title(f'Label: {label}')\n    plt.axis('off')\n\nplt.show()\n```", "```py\nfrom torchvision.transforms import ToTensor\ntrain = datasets.MNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\ntest = datasets.MNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)\n```", "```py\nfrom torch.utils.data import DataLoader\nsize = 64\n\ntrain_dl = DataLoader(train, batch_size=size)\ntest_dl = DataLoader(test, batch_size=size)\n\nfor X, y in test_dl:\n    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\n    break\n```", "```py\nShape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\nShape of y: torch.Size([64]) torch.int64 \n```", "```py\nfrom torch import nn\n\n#Change to 'cuda' if you have access to GPU\ndevice = 'cpu'\n\nclass NNModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.lr_stack = nn.Sequential(\n            nn.Linear(28*28, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.lr_stack(x)\n        return logits\n\nmodel = NNModel().to(device)\nprint(model)\n```", "```py\nNNModel(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (lr_stack): Sequential(\n    (0): Linear(in_features=784, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=128, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=128, out_features=10, bias=True)\n  )\n) \n```", "```py\nnn.Linear(28*28, 128),\nnn.ReLU(),\nnn.Linear(128, 128),\nnn.ReLU(),\nnn.Linear(128, 10)\n```", "```py\nfrom torch.optim import SGD\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = SGD(model.parameters(), lr=1e-3)\n```", "```py\nimport torch\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), (batch + 1) * len(X)\n            print(f\"loss: {loss:>2f}  [{current:>5d}/{size:>5d}]\")\n\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>2f} \\n\")\n```", "```py\nepoch = 5\nfor i in range(epoch):\n    print(f\"Epoch {i+1}\\n-------------------------------\")\n    train(train_dl, model, loss_fn, optimizer)\n    test(test_dl, model, loss_fn)\nprint(\"Done!\")\n```", "```py\npip install lightning\n```", "```py\npip install torchmetrics\n```", "```py\nimport torch\nimport torchmetrics\nimport pytorch_lightning as pl\nfrom torch import nn\nfrom torch.optim import SGD\n\n# Change to 'cuda' if you have access to GPU\ndevice = 'cpu'\n\nclass NNModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.lr_stack = nn.Sequential(\n            nn.Linear(28 * 28, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10)\n        )\n        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n        self.valid_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.lr_stack(x)\n        return logits\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x, y = x.to(device), y.to(device)\n        pred = self(x)\n        loss = nn.CrossEntropyLoss()(pred, y)\n        self.log('train_loss', loss)\n\n        # Compute training accuracy\n        acc = self.train_acc(pred.softmax(dim=-1), y)\n        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n        return loss\n\n    def configure_optimizers(self):\n        return SGD(self.parameters(), lr=1e-3)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        x, y = x.to(device), y.to(device)\n        pred = self(x)\n        loss = nn.CrossEntropyLoss()(pred, y)\n        self.log('test_loss', loss)\n\n        # Compute test accuracy\n        acc = self.valid_acc(pred.softmax(dim=-1), y)\n        self.log('test_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n        return loss\n```", "```py\n# Create a PyTorch Lightning trainer\ntrainer = pl.Trainer(max_epochs=5)\n\n# Create the model\nmodel = NNModel()\n\n# Fit the model\ntrainer.fit(model, train_dl)\n\n# Test the model\ntrainer.test(model, test_dl)\n\nprint(\"Training Finish\")\n```"]