# 如何避免在 TensorFlow 图中编程错误

> 原文：[https://www.kdnuggets.com/2017/05/how-not-program-tensorflow-graph.html](https://www.kdnuggets.com/2017/05/how-not-program-tensorflow-graph.html)

**由 Aaron Schumacher, Deep Learning Analytics 提供。**

从 Python 使用 TensorFlow 就像是用 Python 编程 [另一台计算机](http://planspace.org/20170328-tensorflow_as_a_distributed_virtual_machine/)。一些 Python 语句构建你的 TensorFlow 程序，一些 Python 语句执行该程序，当然，一些 Python 语句根本不涉及 TensorFlow。对你构建的图进行深思熟虑可以帮助你避免混淆和性能陷阱。以下是一些注意事项。![](../Images/e09c1745abd025c932b954fd964eedfb.png)

### 避免拥有许多相同的操作

TensorFlow 中许多方法创建了计算图中的操作，但并不会执行它们。你可能希望多次执行，但这并不意味着你应该创建许多相同操作的副本。

* * *

## 我们的三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析能力

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织 IT

* * *

一个清晰的例子是 `tf.global_variables_initializer()`。

```py
 >>> import tensorflow as tf
>>> session = tf.Session()
# Create some variables...
>>> initializer = tf.global_variables_initializer()
# Variables are not yet initialized.
>>> session.run(initializer)
# Now variables are initialized.
# Do some more work...
>>> session.run(initializer)
# Now variables are re-initialized.
```

如果 `tf.global_variables_initializer()` 的调用被重复执行，例如直接作为 `session.run()` 的参数，会有一些负面影响。

```py
 >>> session.run(tf.global_variables_initializer())
>>> session.run(tf.global_variables_initializer())
```

每次在 `session.run()` 中评估参数时，都会创建一个新的初始化器操作。这在图中创建了多个初始化器操作。对于交互式会话中的小操作，拥有多个副本并不是大问题，如果你创建了更多需要初始化的变量，可能甚至希望这样做。但要考虑是否真的需要许多重复的操作。

在 `session.run()` 中创建操作时，你也没有一个 Python 变量引用这些操作，因此无法轻松重用它们。

在 Python 中，如果你创建了一个没有任何引用的对象，它可以被垃圾回收。被遗弃的对象将被删除，使用的内存将被释放。在 TensorFlow 图中并不会发生这种情况；你放入图中的一切都会留在那里。

很明显，`tf.global_variables_initializer()` 返回一个操作。但操作也可以通过不那么明显的方式创建。

让我们来对比一下 NumPy 的工作方式。

```py
 >>> import numpy as np
>>> x = np.array(1.0)
>>> y = x + 1.0
```

此时内存中有两个数组，`x` 和 `y`。`y` 的值为 2.0，但没有记录 *它是如何* 得到这个值的。加法没有留下任何记录。

TensorFlow 是不同的。

```py
 >>> x = tf.Variable(1.0)
>>> y = x + 1.0
```

现在只有 `x` 是 TensorFlow 变量；`y` 是一个 `add` 操作，如果我们运行它，它可以返回那个加法的结果。

再做一个比较。

```py
 >>> x = np.array(1.0)
>>> y = x + 1.0
>>> y = x + 1.0
```

在这里，`y` 被赋值为指向一个结果数组 `x + 1.0`，然后重新赋值为指向另一个数组。第一个数组将被垃圾回收并消失。

```py
 >>> x = tf.Variable(1.0)
>>> y = x + 1.0
>>> y = x + 1.0
```

在这种情况下，`y` 指代 TensorFlow 图中的一个 `add` 操作，然后 `y` 被重新赋值为指向图中的另一个 `add` 操作。由于 `y` 现在只指向第二个 `add`，我们没有方便的方法来处理第一个操作。但这两个 `add` 操作仍然存在于图中，并将保持在那里。

（顺便提一下，Python 定义类特定加法的机制 [以及其他方法](http://www.python-course.eu/python3_magic_methods.php)，即 `+` 如何用于创建 TensorFlow 操作，实际上很巧妙。）

尤其是当你仅使用默认图并在常规 REPL 或笔记本中进行交互时，你的图中可能会有很多废弃的操作。每次你重新运行定义任何图操作的笔记本单元时，你不仅是在重新定义操作——你还在创建新的操作。

在实验时，拥有一些额外的操作是可以的。但事情可能会失控。

```py
 for _ in range(1e6):
    x = x + 1
```

如果 `x` 是一个 NumPy 数组，或只是一个普通的 Python 数字，这将以恒定的内存运行，并用一个值完成 x。

但如果 `x` 是一个 TensorFlow 变量，你的 TensorFlow 图中将会有超过一百万个操作，仅仅是定义一个计算，甚至没有 *执行* 它。

TensorFlow 的一个直接解决方法是使用 `tf.assign` 操作，它的行为更符合你的期望。

```py
est
increment_x = tf.assign(x, x + 1)
for _ in range(1e6):
    session.run(increment_x)
```

这个修订版本在循环中没有创建任何操作，这通常是个好建议。TensorFlow 确实有 [控制流构造](https://www.tensorflow.org/api_guides/python/control_flow_ops)，包括 [while 循环](https://www.tensorflow.org/api_docs/python/tf/while_loop)。但仅在真正需要时使用这些构造。

注意你何时创建操作，并仅创建你需要的操作。尽量将操作创建与操作执行分开。在进行交互式实验后，最终将状态转移到一个脚本中，只创建你需要的操作。

### 避免在图中使用常量

一个特别不幸的操作是无意中将常量操作添加到图中，尤其是大型常量。

```py
 >>> many_ones = np.ones((1000, 1000))
```

NumPy 数组 `many_ones` 中有一百万个一。我们可以将它们加起来。

```py
 >>> many_ones.sum()
## 1000000.0
```

如果我们使用 TensorFlow 来加总它们会怎样？

```py
 >>> session.run(tf.reduce_sum(many_ones))
## 1000000.0
```

结果是相同的，但机制却大相径庭。这不仅将一些操作添加到图中——它将整个百万元素数组的副本作为常量放入图中。

这种模式的变体可能会意外地将整个数据集加载到图中作为常量。程序可能仍然运行，适用于小数据集。否则，你的系统可能会失败。

避免在图中存储数据的一种简单方法是使用 `feed_dict` 机制。

```py
 >>> many_things = tf.placeholder(tf.float64)
>>> adder = tf.reduce_sum(many_things)
>>> session.run(adder, feed_dict={many_things: many_ones})
## 1000000.0
```

和以前一样，要明确你在何时以及如何向图中添加内容。具体数据通常只在评估时进入图中。

### TensorFlow 作为函数式编程

TensorFlow 操作类似于函数。当一个操作有一个或多个占位符输入时，这一点尤为明显；在会话中评估操作就像用这些参数调用一个函数。因此，返回 TensorFlow 操作的 Python 函数就像 [高阶函数](https://en.wikipedia.org/wiki/Higher-order_function)。

你可能会决定将常量放入图中。例如，如果你需要将许多张量调整为 28×28，你可能会创建一个执行此操作的操作。

```py
 >>> input_tensor = tf.placeholder(tf.float32)
>>> reshape_to_28 = tf.reshape(input_tensor, shape=[28, 28])
```

这类似于 [柯里化](https://en.wikipedia.org/wiki/Currying)，因为 `shape` 参数现在已经设置好了。`[28, 28]` 已经成为图中的常量并指定了该参数。现在要评估 `reshape_to_28`，我们只需提供 `input_tensor`。

有人建议 [神经网络、类型和函数式编程](http://colah.github.io/posts/2015-09-NN-Types-FP/) 之间存在更广泛的联系。将 TensorFlow 视为支持这种构建方式的系统很有趣。

* * *

我正在进行 [构建 TensorFlow 系统的组件](http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823)，这是 [OSCON 2017](https://conferences.oreilly.com/oscon/oscon-tx) 上的一个研讨会。

* * *

在 [Twitter](https://twitter.com/planarrowspace) | [LinkedIn](https://www.linkedin.com/in/ajschumacher) | [Google+](https://plus.google.com/112658546306232777448/) | [GitHub](https://github.com/ajschumacher) | [email](mailto:ajschumacher@gmail.com) 找到 [Aaron](http://planspace.org/aaron/)

[原文](http://planspace.org/20170404-how_not_to_program_the_tensorflow_graph/)。已获许可转载。

**简历：[Aaron Schumacher](https://www.linkedin.com/in/ajschumacher/)** 是 Deep Learning Analytics 的高级数据科学家和软件工程师。他在 [@planarrowspace](https://twitter.com/planarrowspace) 上发推文。

**相关：**

+   [如何在 TensorFlow 中构建递归神经网络](/2017/04/build-recurrent-neural-network-tensorflow.html)

+   [Tensorflow 的最温和介绍 – 第 1 部分](/2016/08/gentlest-introduction-tensorflow-part-1.html)

+   [Python 深度学习框架概述](/2017/02/python-deep-learning-frameworks-overview.html)

### 更多相关话题

+   [AWS AI & ML 奖学金项目概述](https://www.kdnuggets.com/2022/09/aws-ai-ml-scholarship-program-overview.html)

+   [免费报名参加 4 年制计算机科学学位课程](https://www.kdnuggets.com/enroll-in-a-4-year-computer-science-degree-program-for-free)

+   [加入 UC 的商业硕士课程信息会话](https://www.kdnuggets.com/2022/10/ucincinnati-join-ucs-information-session-masters-business-analytics-program.html)

+   [最大化您的价值，通过第三名最佳在线数据硕士课程](https://www.kdnuggets.com/2023/05/bay-path-maximize-value-online-masters-data-science.html)

+   [通过第3名的在线数据科学硕士项目来提升你的职业生涯…](https://www.kdnuggets.com/2023/07/bay-path-advance-career-3rd-best-online-masters-data-science-program.html)

+   [通过第3名的在线数据科学硕士项目来追求硕士学位](https://www.kdnuggets.com/2023/09/bay-path-pursue-masters-data-science-3rd-best-online-program)
