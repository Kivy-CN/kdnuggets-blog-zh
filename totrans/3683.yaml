- en: 'How ChatGPT Works: The Model Behind The Bot'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何 ChatGPT 工作：机器人背后的模型
- en: 原文：[https://www.kdnuggets.com/2023/04/chatgpt-works-model-behind-bot.html](https://www.kdnuggets.com/2023/04/chatgpt-works-model-behind-bot.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/04/chatgpt-works-model-behind-bot.html](https://www.kdnuggets.com/2023/04/chatgpt-works-model-behind-bot.html)
- en: '![How ChatGPT Works: The Model Behind The Bot](../Images/7730ae959e57c4829d752c0f1ac5995f.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![如何 ChatGPT 工作：机器人背后的模型](../Images/7730ae959e57c4829d752c0f1ac5995f.png)'
- en: Photo by [Matheus Bertelli](https://www.pexels.com/photo/man-people-woman-laptop-16094042/)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Matheus Bertelli](https://www.pexels.com/photo/man-people-woman-laptop-16094042/)
    提供
- en: This gentle introduction to the machine learning models that power ChatGPT,
    will start at the introduction of Large Language Models, dive into the revolutionary
    self-attention mechanism that enabled GPT-3 to be trained, and then burrow into
    Reinforcement Learning From Human Feedback, the novel technique that made ChatGPT
    exceptional.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇对支撑 ChatGPT 的机器学习模型的温和介绍将从大型语言模型的介绍开始，深入探讨使 GPT-3 能够进行训练的革命性自注意力机制，然后深入 Reinforcement
    Learning From Human Feedback，这一创新技术使 ChatGPT 异常出色。
- en: Large Language Models
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型
- en: ChatGPT is an extrapolation of a class of machine learning Natural Language
    Processing models known as Large Language Model (LLMs). LLMs digest huge quantities
    of text data and infer relationships between words within the text. These models
    have grown over the last few years as we’ve seen advancements in computational
    power. LLMs increase their capability as the size of their input datasets and
    parameter space increase.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 是一种大型语言模型（LLM）的外推。LLM 消化大量的文本数据，并推断文本中的单词之间的关系。随着计算能力的进步，近年来这些模型不断增长。LLM
    的能力随着输入数据集和参数空间的增大而提升。
- en: The most basic training of language models involves predicting a word in a sequence
    of words. Most commonly, this is observed as either next-token-prediction and
    masked-language-modeling.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的最基本训练涉及预测单词序列中的一个单词。最常见的是下一个标记预测和掩码语言建模。
- en: '![How ChatGPT Works: The Model Behind The Bot](../Images/c1743d07d25017656ab2f6719ae42fc8.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![如何 ChatGPT 工作：机器人背后的模型](../Images/c1743d07d25017656ab2f6719ae42fc8.png)'
- en: Arbitrary example of next-token-prediction and masked-language-modeling generated
    by the author.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者生成的下一个标记预测和掩码语言建模的任意示例。
- en: In this basic sequencing technique, often deployed through a Long-Short-Term-Memory
    (LSTM) model, the model is filling in the blank with the most statistically probable
    word given the surrounding context. There are two major limitations with this
    sequential modeling structure.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种基本的序列技术中，通常通过长短期记忆（LSTM）模型来实现，模型根据周围的上下文填充最具统计概率的单词。这种序列建模结构有两个主要的局限性。
- en: The model is unable to value some of the surrounding words more than others.
    In the above example, while ‘reading’ may most often associate with ‘hates’, in
    the database ‘Jacob’ may be such an avid reader that the model should give more
    weight to ‘Jacob’ than to ‘reading’ and choose ‘love’ instead of ‘hates’.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型无法对某些周围的单词赋予比其他单词更多的权重。在上述示例中，尽管‘reading’通常与‘hates’最为相关，但在数据库中‘Jacob’可能是如此热衷于阅读，以至于模型应该给予‘Jacob’比‘reading’更多的权重，从而选择‘love’而不是‘hates’。
- en: The input data is processed individually and sequentially rather than as a whole
    corpus. This means that when an LSTM is trained, the window of context is fixed,
    extending only beyond an individual input for several steps in the sequence. This
    limits the complexity of the relationships between words and the meanings that
    can be derived.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入数据是单独处理并按序列处理的，而不是作为一个整体语料库。这意味着，当 LSTM 进行训练时，上下文窗口是固定的，仅扩展到序列中的若干步骤之外。这限制了单词之间关系的复杂性以及可以推导出的意义。
- en: In response to this issue, in 2017 a team at Google Brain introduced transformers.
    Unlike LSTMs, transformers can process all input data simultaneously. Using a
    self-attention mechanism, the model can give varying weight to different parts
    of the input data in relation to any position of the language sequence. This feature
    enabled massive improvements in infusing meaning into LLMs and enables processing
    of significantly larger datasets.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这一问题，2017 年 Google Brain 团队引入了变换器。与 LSTM 不同，变换器可以同时处理所有输入数据。通过自注意力机制，模型可以根据语言序列的任何位置对输入数据的不同部分赋予不同的权重。这一特性大大提高了
    LLM 的意义注入，并使得处理更大规模的数据集成为可能。
- en: GPT and Self-Attention
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT 和自注意力
- en: Generative Pre-training Transformer (GPT) models were first launched in 2018
    by openAI as GPT-1\. The models continued to evolve over 2019 with GPT-2, 2020
    with GPT-3, and most recently in 2022 with InstructGPT and ChatGPT. Prior to integrating
    human feedback into the system, the greatest advancement in the GPT model evolution
    was driven by achievements in computational efficiency, which enabled GPT-3 to
    be trained on significantly more data than GPT-2, giving it a more diverse knowledge
    base and the capability to perform a wider range of tasks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 生成预训练变压器（GPT）模型首次于 2018 年由 openAI 推出，名为 GPT-1。模型在 2019 年通过 GPT-2、2020 年通过 GPT-3
    继续发展，并且在 2022 年通过 InstructGPT 和 ChatGPT 取得了最新进展。在将人类反馈整合到系统之前，GPT 模型进化的最大进步是计算效率的提升，这使得
    GPT-3 能够在比 GPT-2 更多的数据上进行训练，拥有更广泛的知识基础和执行更广泛任务的能力。
- en: '![How ChatGPT Works: The Model Behind The Bot](../Images/65ed6096b51d104fbb3f50920add94da.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![ChatGPT 工作原理：模型背后的机器人](../Images/65ed6096b51d104fbb3f50920add94da.png)'
- en: Comparison of GPT-2 (left) and GPT-3 (right). Generated by the author.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2（左）和 GPT-3（右）的对比。由作者生成。
- en: All GPT models leverage the transformer architecture, which means they have
    an encoder to process the input sequence and a decoder to generate the output
    sequence. Both the encoder and decoder have a multi-head self-attention mechanism
    that allows the model to differentially weight parts of the sequence to infer
    meaning and context. In addition, the encoder leverages masked-language-modeling
    to understand the relationship between words and produce more comprehensible responses.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 GPT 模型都利用了变压器架构，这意味着它们都有一个编码器来处理输入序列和一个解码器来生成输出序列。编码器和解码器都有一个多头自注意力机制，允许模型对序列的部分进行差异化加权，以推断含义和上下文。此外，编码器利用掩蔽语言模型来理解单词之间的关系，并生成更易理解的响应。
- en: The self-attention mechanism that drives GPT works by converting tokens (pieces
    of text, which can be a word, sentence, or other grouping of text) into vectors
    that represent the importance of the token in the input sequence. To do this,
    the model,
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动 GPT 的自注意力机制通过将标记（文本片段，可以是单词、句子或其他文本分组）转换为表示标记在输入序列中重要性的向量来工作。为此，模型，
- en: Creates a query, key, and value vector for each token in the input sequence.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为输入序列中的每个标记创建查询、键和值向量。
- en: Calculates the similarity between the query vector from step one and the key
    vector of every other token by taking the dot product of the two vectors.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过计算步骤一中的查询向量与其他每个标记的键向量的点积，计算它们之间的相似性。
- en: Generates normalized weights by feeding the output of step 2 into a [softmax
    function](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer).
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将步骤 2 的输出输入到 [softmax 函数](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer)中，生成归一化权重。
- en: Generates a final vector, representing the importance of the token within the
    sequence by multiplying the weights generated in step 3 by the value vectors of
    each token.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成最终向量，通过将步骤 3 中生成的权重与每个标记的值向量相乘，表示标记在序列中的重要性。
- en: The ‘multi-head’ attention mechanism that GPT uses is an evolution of self-attention.
    Rather than performing steps 1–4 once, in parallel the model iterates this mechanism
    several times, each time generating a new linear projection of the query, key,
    and value vectors. By expanding self-attention in this way, the model is capable
    of grasping sub-meanings and more complex relationships within the input data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 使用的“多头”注意力机制是自注意力的演变。模型不是一次性执行步骤 1-4，而是并行迭代多次，每次生成查询、键和值向量的新线性投影。通过这种方式扩展自注意力，模型能够掌握输入数据中的子含义和更复杂的关系。
- en: '![How ChatGPT Works: The Model Behind The Bot](../Images/21c8fab1e4493df62fc661834f3c6699.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![ChatGPT 工作原理：模型背后的机器人](../Images/21c8fab1e4493df62fc661834f3c6699.png)'
- en: Screenshot from ChatGPT generated by the author.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 作者生成的 ChatGPT 截图。
- en: Although GPT-3 introduced remarkable advancements in natural language processing,
    it is limited in its ability to align with user intentions. For example, GPT-3
    may produce outputs that
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 GPT-3 在自然语言处理方面取得了显著进展，但在与用户意图对齐方面仍然有限。例如，GPT-3 可能生成
- en: '**Lack of helpfulness **meaning they donot follow the user’s explicit instructions.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏帮助**意味着它们没有遵循用户的明确指示。'
- en: '**Contain hallucinations **that reflect non-existing or incorrect facts.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**包含幻觉**，即反映不存在或不正确的事实。'
- en: '**Lack interpretability** making it difficult for humans to understand how
    the model arrived at a particular decision or prediction.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏可解释性** 使得人们难以理解模型如何得出特定的决策或预测。'
- en: '**Include toxic or biased content** that is harmful or offensive and spreads
    misinformation.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**包含有毒或偏见内容** 的提示，这些内容有害或令人反感，并传播虚假信息。'
- en: Innovative training methodologies were introduced in ChatGPT to counteract some
    of these inherent issues of standard LLMs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在ChatGPT中引入了创新的训练方法，以解决标准LLM的一些固有问题。
- en: ChatGPT
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ChatGPT
- en: ChatGPT is a spinoff of InstructGPT, which introduced a novel approach to incorporating
    human feedback into the training process to better align the model outputs with
    user intent. Reinforcement Learning from Human Feedback (RLHF) is described in
    depth in [openAI’s 2022](https://arxiv.org/pdf/2203.02155.pdf) paper **Training
    language models to follow instructions with human feedback **and is simplified
    below.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT是InstructGPT的衍生产品，它引入了一种新颖的方法，将人类反馈融入训练过程中，以更好地使模型输出符合用户意图。人类反馈的强化学习（RLHF）在[OpenAI
    2022](https://arxiv.org/pdf/2203.02155.pdf)的论文《用人类反馈训练语言模型以遵循指令》中进行了深入描述，下面进行了简化。
- en: 'Step 1: Supervised Fine Tuning (SFT) Model'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '步骤1: 监督微调（SFT）模型'
- en: The first development involved fine-tuning the GPT-3 model by hiring 40 contractors
    to create a supervised training dataset, in which the input has a known output
    for the model to learn from. Inputs, or prompts, were collected from actual user
    entries into the Open API. The labelers then wrote an appropriate response to
    the prompt thus creating a known output for each input. The GPT-3 model was then
    fine-tuned using this new, supervised dataset, to create GPT-3.5, also called
    the SFT model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步开发涉及通过雇佣40名承包商来创建一个监督训练数据集，对GPT-3模型进行微调，在该数据集中，输入有已知的输出供模型学习。输入或提示是从实际用户输入的Open
    API中收集的。标注者随后为每个提示编写了适当的回应，从而为每个输入创建了已知的输出。然后使用这个新的监督数据集对GPT-3模型进行微调，创建了GPT-3.5，也称为SFT模型。
- en: In order to maximize diversity in the prompts dataset, only 200 prompts could
    come from any given user ID and any prompts that shared long common prefixes were
    removed. Finally, all prompts containing personally identifiable information (PII)
    were removed.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化提示数据集的多样性，每个用户ID只能有200个提示，并且去除了共享长公共前缀的提示。最后，所有包含个人身份信息（PII）的提示都被移除。
- en: After aggregating prompts from OpenAI API, labelers were also asked to create
    sample prompts to fill-out categories in which there was only minimal real sample
    data. The categories of interest included
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在汇总OpenAI API的提示后，标注者还被要求创建样本提示，以填补只有最少真实样本数据的类别。感兴趣的类别包括
- en: '**Plain prompts:** any arbitrary ask.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**普通提示:** 任何任意的要求。'
- en: '**Few-shot prompts: **instructions that contain multiple query/response pairs.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**少量示例提示:** 包含多个查询/回应对的指令。'
- en: '**User-based prompts: **correspond to a specific use-case that was requested
    for the OpenAI API.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于用户的提示:** 对应于为OpenAI API请求的特定使用案例。'
- en: When generating responses, labelers were asked to do their best to infer what
    the instruction from the user was. The paper describes the main three ways that
    prompts request information.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成回复时，标注者被要求尽力推测用户的指令是什么。本文描述了提示请求信息的三种主要方式。
- en: '**Direct:** “Tell me about…”'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**直接:** “告诉我关于……”'
- en: '**Few-shot:** Given these two examples of a story, write another story about
    the same topic.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**少量示例:** 给定这两个故事示例，写另一个关于相同主题的故事。'
- en: '**Continuation: **Given the start of a story, finish it.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**续写:** 给定故事的开头，完成它。'
- en: The compilation of prompts from the OpenAI API and hand-written by labelers
    resulted in 13,000 input / output samples to leverage for the supervised model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从OpenAI API和标注者手工编写的提示中编制出的样本达到了13,000个输入/输出样本，用于监督模型。
- en: '![How ChatGPT Works: The Model Behind The Bot](../Images/b5d8c40da94f348fb00cdf51640d82b0.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![ChatGPT的工作原理：模型背后的机器人](../Images/b5d8c40da94f348fb00cdf51640d82b0.png)'
- en: Image (left) inserted from** Training language models to follow instructions
    with human feedback ***OpenAI et al., 2022 *[https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf).
    Additional context added in red (right) by the author.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图像（左）插入自**《用人类反馈训练语言模型以遵循指令》** *OpenAI等，2022* [https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)。作者在右侧以红色添加了额外的背景信息。
- en: 'Step 2: Reward Model'
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '步骤2: 奖励模型'
- en: After the SFT model is trained in step 1, the model generates better aligned
    responses to user prompts. The next refinement comes in the form of training a
    reward model in which a model input is a series of prompts and responses, and
    the output is a scaler value, called a reward. The reward model is required in
    order to leverage Reinforcement Learning in which a model learns to produce outputs
    to maximize its reward (see step 3).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤1中训练了SFT模型之后，模型能够生成更符合用户提示的响应。下一步的改进形式是训练奖励模型，其中模型输入是一系列提示和响应，而输出是一个标量值，称为奖励。奖励模型是为了利用强化学习，在这种学习中，模型学习生成输出以最大化其奖励（见步骤3）。
- en: To train the reward model, labelers are presented with 4 to 9 SFT model outputs
    for a single input prompt. They are asked to rank these outputs from best to worst,
    creating combinations of output ranking as follows.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练奖励模型，标注员会看到4到9个SFT模型输出，针对单一输入提示。他们需要对这些输出进行从最好到最差的排名，生成以下输出排名组合。
- en: '![How ChatGPT Works: The Model Behind The Bot](../Images/7549965d3e66d9211c90a69dffe31139.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![ChatGPT工作原理：模型背后的秘密](../Images/7549965d3e66d9211c90a69dffe31139.png)'
- en: Example of response ranking combinations. Generated by the author.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 响应排名组合的示例。由作者生成。
- en: Including each combination in the model as a separate datapoint led to overfitting
    (failure to extrapolate beyond seen data). To solve, the model was built leveraging
    each group of rankings as a single batch datapoint.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型中的每个组合作为一个单独的数据点会导致过拟合（未能推断超出见过的数据）。为了解决这个问题，模型被构建为将每组排名作为一个单独的批量数据点。
- en: '![How ChatGPT Works: The Model Behind The Bot](../Images/166558b26a91ebce9ed1a778bb50a2d5.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![ChatGPT工作原理：模型背后的秘密](../Images/166558b26a91ebce9ed1a778bb50a2d5.png)'
- en: Image (left) inserted from** Training language models to follow instructions
    with human feedback ***OpenAI et al., 2022 *[https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf).
    Additional context added in red (right) by the author.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图片（左）来源于** 训练语言模型以遵循人类反馈的指令 ***OpenAI 等人，2022 *[https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)。作者在右侧添加了红色的附加上下文。
- en: 'Step 3: Reinforcement Learning Model'
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步：强化学习模型
- en: In the final stage, the model is presented with a random prompt and returns
    a response. The response is generated using the ‘policy’ that the model has learned
    in step 2\. The policy represents a strategy that the machine has learned to use
    to achieve its goal; in this case, maximizing its reward. Based on the reward
    model developed in step 2, a scaler reward value is then determined for the prompt
    and response pair. The reward then feeds back into the model to evolve the policy.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后阶段，模型接收到一个随机提示并返回响应。该响应使用模型在步骤2中学到的“策略”生成。策略代表了机器为实现其目标而学到的策略；在这种情况下，就是最大化其奖励。基于在步骤2中开发的奖励模型，为提示和响应对确定一个标量奖励值。然后，奖励反馈到模型中以演变策略。
- en: In 2017, Schulman *et al. *introduced [Proximal Policy Optimization (PPO)](https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b),
    the methodology that is used in updating the model’s policy as each response is
    generated. PPO incorporates a per-token Kullback–Leibler (KL) penalty from the
    SFT model. The KL divergence measures the similarity of two distribution functions
    and penalizes extreme distances. In this case, using a KL penalty reduces the
    distance that the responses can be from the SFT model outputs trained in step
    1 to avoid over-optimizing the reward model and deviating too drastically from
    the human intention dataset.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年，Schulman *等人* 介绍了[近端策略优化（PPO）](https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b)，这一方法用于在每次生成响应时更新模型的策略。PPO结合了来自SFT模型的每个令牌Kullback–Leibler（KL）惩罚。KL散度衡量两个分布函数的相似性，并惩罚极端距离。在这种情况下，使用KL惩罚可以减少响应与步骤1中SFT模型输出之间的距离，以避免过度优化奖励模型并过于偏离人类意图数据集。
- en: '![How ChatGPT Works: The Model Behind The Bot](../Images/12ed0915e61e1b70b0fe389a2e59db86.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![ChatGPT工作原理：模型背后的秘密](../Images/12ed0915e61e1b70b0fe389a2e59db86.png)'
- en: Image (left) inserted from** Training language models to follow instructions
    with human feedback ***OpenAI et al., 2022 *[https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf).
    Additional context added in red (right) by the author.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图片（左）来源于** 训练语言模型以遵循人类反馈的指令 ***OpenAI 等人，2022 *[https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)。作者在右侧添加了红色的附加上下文。
- en: Steps 2 and 3 of the process can be iterated through repeatedly though in practice
    this has not been done extensively.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 过程的第2步和第3步可以重复迭代，尽管在实践中这并未广泛进行。
- en: '![How ChatGPT Works: The Model Behind The Bot](../Images/33d187f66497693d2bfb8c34a85c95f0.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![ChatGPT 的工作原理：模型背后的秘密](../Images/33d187f66497693d2bfb8c34a85c95f0.png)'
- en: Screenshot from ChatGPT generated by the author.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 截图由作者生成的ChatGPT。
- en: Evaluation of the Model
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估
- en: Evaluation of the model is performed by setting aside a test set during training
    that the model has not seen. On the test set, a series of evaluations are conducted
    to determine if the model is better aligned than its predecessor, GPT-3.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的评估是通过在训练过程中留出未见过的测试集来进行的。在测试集上，进行一系列评估以确定模型是否比其前身GPT-3更对齐。
- en: '**Helpfulness:** the model’s ability to infer and follow user instructions.
    Labelers preferred outputs from InstructGPT over GPT-3 85 ± 3% of the time.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**有用性：** 模型推断和遵循用户指令的能力。标注者85 ± 3%的时间偏好InstructGPT的输出而非GPT-3。'
- en: '**Truthfulness**: the model’s tendency for hallucinations. The PPO model produced
    outputs that showed minor increases in truthfulness and informativeness when assessed
    using the [TruthfulQA](https://arxiv.org/abs/2109.07958) dataset.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**真实性：** 模型的幻觉倾向。PPO模型在使用[TruthfulQA](https://arxiv.org/abs/2109.07958)数据集评估时，表现出真实性和信息性的小幅增加。'
- en: '**Harmlessness**: the model’s ability to avoid inappropriate, derogatory, and
    denigrating content. Harmlessness was tested using the RealToxicityPrompts dataset.
    The test was performed under three conditions.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**无害性：** 模型避免不当、贬低和侮辱内容的能力。使用RealToxicityPrompts数据集测试了无害性。该测试在三种条件下进行。'
- en: 'Instructed to provide respectful responses: resulted in a significant decrease
    in toxic responses.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指示提供尊重的回应：导致有害回应显著减少。
- en: 'Instructed to provide responses, without any setting for respectfulness: no
    significant change in toxicity.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指示提供回应，而不设置尊重性：有害性没有显著变化。
- en: 'Instructed to provide toxic response: responses were in fact significantly
    more toxic than the GPT-3 model.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指示提供有害回应：回应实际上比GPT-3模型更有害。
- en: For more information on the methodologies used in creating ChatGPT and InstructGPT,
    read the original paper published by OpenAI **Training language models to follow
    instructions with human feedback**, *2022 *[https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解有关创建ChatGPT和InstructGPT的方法更多信息，请阅读OpenAI发布的原始论文**《训练语言模型以遵循带有人类反馈的指令》**，*2022年*
    [https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)。
- en: '![How ChatGPT Works: The Model Behind The Bot](../Images/a40fd301b5beb50c84b39d01b82bc662.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![ChatGPT 的工作原理：模型背后的秘密](../Images/a40fd301b5beb50c84b39d01b82bc662.png)'
- en: Screenshot from ChatGPT generated by the author.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 截图由作者生成的ChatGPT。
- en: Happy learning!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 祝学习愉快！
- en: Sources
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 来源
- en: '[https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/)'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/)'
- en: '[https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)'
- en: '[https://medium.com/r/?url=https%3A%2F%2Fdeepai.org%2Fmachine-learning-glossary-and-terms%2Fsoftmax-layer](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer)'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://medium.com/r/?url=https%3A%2F%2Fdeepai.org%2Fmachine-learning-glossary-and-terms%2Fsoftmax-layer](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer)'
- en: '[https://www.assemblyai.com/blog/how-chatgpt-actually-works/](https://www.assemblyai.com/blog/how-chatgpt-actually-works/)'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.assemblyai.com/blog/how-chatgpt-actually-works/](https://www.assemblyai.com/blog/how-chatgpt-actually-works/)'
- en: '[https://medium.com/r/?url=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b](https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b)'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://medium.com/r/?url=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b](https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b)'
- en: '**[Molly Ruby](https://www.linkedin.com/in/mollyliebeskind/)** is a Data Scientist
    at Mars and a content writer.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**[莫莉·鲁比](https://www.linkedin.com/in/mollyliebeskind/)** 是Mars的一名数据科学家和内容作者。'
- en: '[Original](https://towardsdatascience.com/how-chatgpt-works-the-models-behind-the-bot-1ce5fca96286).
    Reposted with permission.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/how-chatgpt-works-the-models-behind-the-bot-1ce5fca96286)。经许可转载。'
- en: '* * *'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT'
- en: '* * *'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[The Burtch Works 2023 Data Science & AI Professionals Salary Report…](https://www.kdnuggets.com/2023/08/burtch-works-2023-data-science-ai-professionals-salary-report.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Burtch Works 2023 数据科学与 AI 专业人员薪资报告…](https://www.kdnuggets.com/2023/08/burtch-works-2023-data-science-ai-professionals-salary-report.html)'
- en: '[Stable Diffusion: Basic Intuition Behind Generative AI](https://www.kdnuggets.com/2023/06/stable-diffusion-basic-intuition-behind-generative-ai.html)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[稳定扩散：生成式 AI 的基本直觉](https://www.kdnuggets.com/2023/06/stable-diffusion-basic-intuition-behind-generative-ai.html)'
- en: '[Getting Started with LLMOps: The Secret Sauce Behind Seamless Interactions](https://www.kdnuggets.com/getting-started-with-llmops-the-secret-sauce-behind-seamless-interactions)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[入门 LLMOps：无缝互动背后的秘密武器](https://www.kdnuggets.com/getting-started-with-llmops-the-secret-sauce-behind-seamless-interactions)'
- en: '[The Growth Behind LLM-based Autonomous Agents](https://www.kdnuggets.com/the-growth-behind-llmbased-autonomous-agents)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基于 LLM 的自主智能体背后的成长](https://www.kdnuggets.com/the-growth-behind-llmbased-autonomous-agents)'
- en: '[Visual ChatGPT: Microsoft Combine ChatGPT and VFMs](https://www.kdnuggets.com/2023/03/visual-chatgpt-microsoft-combine-chatgpt-vfms.html)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[视觉 ChatGPT：微软结合 ChatGPT 和 VFMs](https://www.kdnuggets.com/2023/03/visual-chatgpt-microsoft-combine-chatgpt-vfms.html)'
- en: '[ChatGPT CLI: Transform Your Command-Line Interface Into ChatGPT](https://www.kdnuggets.com/2023/07/chatgpt-cli-transform-commandline-interface-chatgpt.html)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatGPT CLI：将你的命令行界面转变为 ChatGPT](https://www.kdnuggets.com/2023/07/chatgpt-cli-transform-commandline-interface-chatgpt.html)'
