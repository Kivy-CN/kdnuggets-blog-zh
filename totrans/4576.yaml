- en: 'Natural Language in Python using spaCy: An Introduction'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/09/natural-language-python-using-spacy-introduction.html](https://www.kdnuggets.com/2019/09/natural-language-python-using-spacy-introduction.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Paco Nathan](https://twitter.com/pacoid)**'
  prefs: []
  type: TYPE_NORMAL
- en: '*This article provides a brief introduction to natural language using spaCy
    and related libraries in Python. The [complementary Domino project is also available](https://try.dominodatalab.com/u/domino-johnjoo/spacy-dev/overview).*'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This article and paired [Domino project](https://try.dominodatalab.com/u/domino-johnjoo/spacy-dev/overview) provide
    a brief introduction to working with *natural language* (sometimes called “text
    analytics”) in Python using [*spaCy* ](https://spacy.io/)and related libraries.
    Data science teams in industry must work with lots of text, one of the top four
    categories of data used in machine learning. Usually it’s human-generated text,
    but not always.
  prefs: []
  type: TYPE_NORMAL
- en: 'Think about it: how does the “operating system” for business work? Typically,
    there are contracts (sales contracts, work agreements, partnerships), there are
    invoices, there are insurance policies, there are regulations and other laws,
    and so on. All of those are represented as text.'
  prefs: []
  type: TYPE_NORMAL
- en: You may run across a few acronyms: *natural language processing* (NLP), n*atural
    language understanding* (NLU), *natural language generation* (NLG)—which are roughly
    speaking “read text”, “understand meaning”, “write text” respectively. Increasingly
    these tasks overlap and it becomes difficult to categorize any given feature.
  prefs: []
  type: TYPE_NORMAL
- en: The *spaCy* framework—along with a wide and growing range of plug-ins and other
    integrations—provides features for a wide range of natural language tasks. It’s
    become one of the most widely used natural language libraries in Python for industry
    use cases, and has quite a large community—and with that, much support for commercialization
    of research advances as this area continues to evolve rapidly.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have configured the default Compute Environment in Domino to include all
    of the packages, libraries, models, and data you’ll need for this tutorial.  [Check
    out the Domino project to run the code.](https://try.dominodatalab.com/u/domino-johnjoo/spacy-dev/overview)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/12edb7abbae9b1e4aad03968716e8847.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](../Images/6a52a1f569a31b20cf65e6d774fbf268.png)'
  prefs: []
  type: TYPE_IMG
- en: If you’re interested in how Domino’s Compute Environments work, check out the [Support
    Page](https://support.dominodatalab.com/hc/en-us/articles/115000392643-Environment-management).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s load *spaCy* and run some code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'That `nlp` variable is now your gateway to all things *spaCy* and loaded with
    the `en_core_web_sm` small model for English. Next, let’s run a small “document”
    through the natural language parser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: First we created a [doc](https://spacy.io/api/doc) from the text, which is a
    container for a document and all of its annotations. Then we iterated through
    the document to see what *spaCy *had parsed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Good, but it’s a lot of info and a bit difficult to read. Let’s reformat the *spaCy* parse
    of that sentence as a [pandas](https://pandas.pydata.org/) dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6efac198b9c30e6826f149f2fef68f55.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Much more readable! In this simple case, the entire document is merely one
    short sentence. For each word in that sentence *spaCy* has created a token, and
    we accessed fields in each token to show:'
  prefs: []
  type: TYPE_NORMAL
- en: raw text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[lemma](https://en.wikipedia.org/wiki/Lemma_(morphology) – a root form of the
    word'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[part of speech](https://en.wikipedia.org/wiki/Part_of_speech)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a flag for whether the word is a *stopword*—i.e., a common word that may be
    filtered out
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next let’s use the [displaCy](https://ines.io/blog/developing-displacy) library
    to visualize the parse tree for that sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/11f4063ce82c16944df254f90c40ecbd.png)'
  prefs: []
  type: TYPE_IMG
- en: Does that bring back memories of grade school? Frankly, for those of us coming
    from more of a computational linguistics background, that diagram sparks joy.
  prefs: []
  type: TYPE_NORMAL
- en: But let’s backup for a moment. How do you handle multiple sentences?
  prefs: []
  type: TYPE_NORMAL
- en: 'There are features for *sentence boundary detection* (SBD)—also known as *sentence
    segmentation*—based on the builtin/default [sentencizer](https://spacy.io/api/sentencizer/):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When *spaCy* creates a document, it uses a principle of *non-destructive tokenization,* meaning
    that the tokens, sentences, etc., are simply indexes into a long array. In other
    words, they don’t carve the text stream into little pieces. So each sentence is
    a [span](https://spacy.io/api/span) with a *start* and an *end* index into the
    document array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can index into the document array to pull out the tokens for one sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Or simply index into a specific token, such as the verb `went` in the last
    sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: At this point we can parse a document, segment that document into sentences,
    then look at annotations about the tokens in each sentence. That’s a good start.
  prefs: []
  type: TYPE_NORMAL
- en: Acquiring Text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we can parse texts, where do we get texts? One quick source is to leverage
    the interwebs. Of course when we download web pages we’ll get HTML, and then need
    to extract text from them. [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is
    a popular package for that.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, a little housekeeping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following function `get_text()` we’ll parse the HTML to find all of
    the `<p/>`tags, then extract the text for those:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s grab some text from online sources. We can compare open source licenses
    hosted on the [Open Source Initiative](https://opensource.org/licenses/) site:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'One common use case for natural language work is to compare texts. For example,
    with those open source licenses we can download their text, parse, then compare [similarity](https://spacy.io/api/doc#similarity) metrics
    among them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This is interesting since the [BSD](https://opensource.org/licenses/BSD-3-Clause) and [MIT](https://opensource.org/licenses/MIT) licenses
    appear to be the most similar documents. In fact they are closely related.
  prefs: []
  type: TYPE_NORMAL
- en: Admittedly, there was some extra text included in each document due to the OSI
    disclaimer in the footer—but this provides a reasonable approximation for comparing
    the licenses.
  prefs: []
  type: TYPE_NORMAL
- en: Natural Language Understanding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now let’s dive into some of the* spaCy* features for NLU. Given that we have
    a parse of a document, from a purely grammatical standpoint we can pull the [noun
    chunks](https://spacy.io/usage/linguistic-features#noun-chunks), i.e., each of
    the noun phrases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Not bad. The noun phrases in a sentence generally provide more information content—as
    a simple filter used to reduce a long document into a more “distilled” representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can take this approach further and identify [named entities](https://spacy.io/usage/linguistic-features#named-entities) within
    the text, i.e., the proper nouns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The *displaCy* library provides an excellent way to visualize named entities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d89ad761f54624be253828504cdedc5e.png)'
  prefs: []
  type: TYPE_IMG
- en: If you’re working with [knowledge graph](https://www.akbc.ws/2019/) applications
    and other [linked data](http://linkeddata.org/), your challenge is to construct
    links between the named entities in a document and other related information for
    the entities, which is called[ entity linking](http://nlpprogress.com/english/entity_linking.html).
    Identifying the named entities in a document is the first step in this particular
    kind of AI work. For example, given the text above, one might link the `Steve
    Wozniak` named entity to a [lookup in DBpedia](http://dbpedia.org/page/Steve_Wozniak).
  prefs: []
  type: TYPE_NORMAL
- en: In more general terms, one can also link *lemmas* to resources that describe
    their meanings. For example, in an early section we parsed the sentence `The gorillas
    just went wild` and were able to show that the lemma for the word `went` is the
    verb `go`. At this point we can use a venerable project called [WordNet](https://wordnet.princeton.edu/) which
    provides a lexical database for English—in other words, it’s a computable thesaurus.
  prefs: []
  type: TYPE_NORMAL
- en: There’s a *spaCy* integration for WordNet called [spacy-wordnet](https://github.com/recognai/spacy-wordnet) by [Daniel
    Vila Suero](https://twitter.com/dvilasuero), an expert in natural language and
    knowledge graph work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we’ll load the WordNet data via NLTK (these things happen):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that *spaCy* runs as a “pipeline” and allows means for customizing parts
    of the pipeline in use. That’s excellent for supporting really interesting workflow
    integrations in data science work. Here we’ll add the *WordnetAnnotator* from
    the *spacy-wordnet* project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Within the English language, some words are infamous for having many possible
    meanings. For example, click through the results online in a [WordNet](http://wordnetweb.princeton.edu/perl/webwn?s=star&sub=Search+WordNet&o2&o0=1&o8=1&o1=1&o7&o5&o9&o6&o3&o4&h) search
    to find the meanings related to the word `withdraw`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s use *spaCy* to perform that lookup automatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Again, if you are working with knowledge graphs, those “word sense” links from *WordNet* could
    be used along with graph algorithms to help identify the meanings for a particular
    word. This can also be used to develop summaries for larger sections of text through
    a technique called *summarization*. It’s beyond the scope of this tutorial, but
    an interesting application currently for natural language in industry.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going in the other direction, if you know *a priori* that a document was about
    a particular domain or set of topics, then you can constrain the meanings returned
    from *WordNet*. In the following example, we want to consider NLU results that
    are within Finance and Banking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'That example may look simple but, if you play with the `domains` list, you’ll
    find that the results have a kind of combinatorial explosion when run without
    reasonable constraints. Imagine having a knowledge graph with millions of elements:
    you’d want to constrain searches where possible to avoid having every query take
    days/weeks/months/years to compute.'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes the problems encountered when trying to understand a text—or better
    yet when trying to understand a *corpus* (a dataset with many related texts)—become
    so complex that you need to visualize it first. Here’s an interactive visualization
    for understanding texts: [scattertext](https://spacy.io/universe/project/scattertext),
    a product of the genius of [Jason Kessler](https://twitter.com/jasonkessler).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s analyze text data from the party conventions during the 2012 US Presidential
    elections. Note: this cell may take a few minutes to run but the results from
    all that number crunching is worth the wait.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have the `corpus` ready, generate an interactive visualization in
    HTML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’ll render the HTML—give it a minute or two to load, it’s worth the wait:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/82c17273d5bee33940a24dde852dbb2a.png)'
  prefs: []
  type: TYPE_IMG
- en: Imagine if you had text from the past three years of customer support for a
    particular product in your organization. Suppose your team needed to understand
    how customers have been talking about the product? This *scattertext* library
    might come in quite handy! You could cluster (k=2) on *NPS scores* (a customer
    evaluation metric) then replace the Democrat/Republican dimension with the top
    two components from the clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Five years ago, if you’d asked about open source in Python for natural language,
    a default answer from many people working in data science would’ve been [NLTK](https://www.nltk.org/).
    That project includes just about everything but the kitchen sink and has components
    which are relatively academic. Another popular natural language project is [CoreNLP](https://stanfordnlp.github.io/CoreNLP/) from
    Stanford. Also quite academic, albeit powerful, though CoreNLP can be challenging
    to integrate with other software for production use.
  prefs: []
  type: TYPE_NORMAL
- en: Then a few years ago everything in this natural language corner of the world
    began to change. The two principal authors for *spaCy*,[ Matthew Honnibal](https://twitter.com/honnibal) and [Ines
    Montani,](https://twitter.com/_inesmontani) launched the project in 2015 and industry
    adoption was rapid. They focused on an *opinionated* approach (do what’s needed,
    do it well, no more, no less) which provided simple, rapid integration into data
    science workflows in Python, as well as faster execution and better accuracy than
    the alternatives. Based on these priorities, *spaCy* became sort of the opposite
    of *NLTK.* Since 2015, *spaCy* has consistently focused on being an open source
    project (i.e., depending on its community for directions, integrations, etc.)
    and being commercial-grade software (not academic research). That said, *spaCy* has
    been quick to incorporate the SOTA advances in machine learning, effectively becoming
    a conduit for moving research into industry.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that machine learning for natural language got a big
    boost during the mid-2000’s as Google began to win international language translation
    competitions. Another big change occurred during 2017-2018 when, following the
    many successes of *deep learning,* those approaches began to out-perform previous
    machine learning models. For example, see the [ELMo](https://arxiv.org/abs/1802.05365) work
    on language embedding by Allen AI, followed by [BERT from Google](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html),
    and more recently [ERNIE by Baidu](https://medium.com/syncedreview/baidus-ernie-tops-google-s-bert-in-chinese-nlp-tasks-d6a42b49223d)—in
    other words, the search engine giants of the world have gifted the rest of us
    with a Sesame Street repertoire of open source embedded language models based
    on deep learning, which is now state of the art (SOTA). Speaking of which, to
    keep track of SOTA for natural language keep an eye on [NLP-Progress](http://nlpprogress.com/) and [Papers
    with Code](https://paperswithcode.com/sota).
  prefs: []
  type: TYPE_NORMAL
- en: The use cases for natural language have shifted dramatically over the past two
    years, after deep learning techniques arose to the fore. Circa 2014, a natural
    language tutorial in Python might have shown *word count* or *keyword search* or *sentiment
    detection* and the target use cases were relatively underwhelming. Circa 2019,
    we’re talking about analyzing thousands of documents for vendor contracts in an
    industrial *supply chain optimization*…or hundreds of millions of documents for
    policyholders of an insurance company or gazillions of documents regarding financial
    disclosures. More contemporary natural language work tends to be in NLU, often
    to support construction of *knowledge graphs*, and increasingly in NLG where large
    numbers of similar documents can be summarized at human scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'The [spaCy Universe](https://spacy.io/universe) is a great place to check for
    deep-dives into particular use cases and to see how this field is evolving. Some
    selections from this “universe” include:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Blackstone](https://spacy.io/universe/project/blackstone) – parsing unstructured
    legal texts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kindred](https://spacy.io/universe/project/kindred) – extracting entities
    from biomedical texts (e.g., Pharma)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[mordecai](https://spacy.io/universe/project/mordecai) – parsing geographic
    information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Prodigy](https://spacy.io/universe/project/prodigy) – human-in-the-loop annotation
    for labelling datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[spacy-raspberry](https://spacy.io/universe/project/spacy-raspberry) – Raspberry
    PI image for running spaCy and deep learning on edge devices'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Rasa NLU](https://spacy.io/universe/project/rasa) – Rasa integration for chat
    apps'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also, a couple super new items to mention:'
  prefs: []
  type: TYPE_NORMAL
- en: '[spacy-pytorch-transformers](https://explosion.ai/blog/spacy-pytorch-transformers) to
    fine tune (i.e., use transfer learning with) the Sesame Street characters and
    friends: BERT, GPT-2, XLNet, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[spaCy IRL 2019](https://irl.spacy.io/2019/) conference – check out videos
    from the talks!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s so much more we can be done with *spaCy*— hopefully this tutorial provides
    an introduction. We wish you all the best in your natural language work.
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://blog.dominodatalab.com/natural-language-in-python-using-spacy/).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[The State of Transfer Learning in NLP](/2019/09/state-transfer-learning-nlp.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Reddit Post Classification](/2019/09/reddit-post-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2018’s Top 7 Python Libraries for Data Science and AI](/2019/01/vazquez-2018-top-7-python-libraries.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Natural Language Processing with spaCy](https://www.kdnuggets.com/2023/01/natural-language-processing-spacy.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Getting Started with spaCy for NLP](https://www.kdnuggets.com/2022/11/getting-started-spacy-nlp.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[N-gram Language Modeling in Natural Language Processing](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Gentle Introduction to Natural Language Processing](https://www.kdnuggets.com/2022/06/gentle-introduction-natural-language-processing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introduction to Natural Language Processing](https://www.kdnuggets.com/introduction-to-natural-language-processing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Start Using Natural Language Processing With PyTorch](https://www.kdnuggets.com/2022/04/start-natural-language-processing-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
