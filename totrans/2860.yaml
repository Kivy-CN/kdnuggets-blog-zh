- en: Beginners Guide to the Three Types of Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习三种类型的初学者指南
- en: 原文：[https://www.kdnuggets.com/2019/11/beginners-guide-three-types-machine-learning.html](https://www.kdnuggets.com/2019/11/beginners-guide-three-types-machine-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/11/beginners-guide-three-types-machine-learning.html](https://www.kdnuggets.com/2019/11/beginners-guide-three-types-machine-learning.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Rebecca Vickery](https://www.linkedin.com/in/rebecca-vickery-20b94133/),
    Data Scientist**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：[Rebecca Vickery](https://www.linkedin.com/in/rebecca-vickery-20b94133/)，数据科学家**'
- en: '![Figure](../Images/97361e3877e513bd6978ee0085f57086.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/97361e3877e513bd6978ee0085f57086.png)'
- en: Visualising KMeans performance with [Yellowbrick](https://www.scikit-yb.org/en/latest/)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[Yellowbrick](https://www.scikit-yb.org/en/latest/)可视化KMeans性能
- en: Machine learning problems can generally be divided into three types. Classification
    and regression, which are known as supervised learning, and unsupervised learning
    which in the context of machine learning applications often refers to clustering.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习问题通常可以分为三种类型。分类和回归被称为监督学习，而无监督学习在机器学习应用的背景下通常指的是聚类。
- en: In the following article, I am going to give a brief introduction to each of
    these three problems and will include a walkthrough in the popular python library [scikit-learn](https://scikit-learn.org/stable/index.html).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的文章中，我将简要介绍这三种问题中的每一种，并包含在流行的Python库[scikit-learn](https://scikit-learn.org/stable/index.html)中的演示。
- en: Before I start I’ll give a brief explanation for the meaning behind the terms
    supervised and unsupervised learning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我将简要解释监督学习和无监督学习这两个术语的含义。
- en: '**Supervised Learning:** *In supervised learning, you have a known set of inputs
    (features) and a known set of outputs (labels). Traditionally these are known
    as X and y. The goal of the algorithm is to learn the mapping function that maps
    the input to the output. So that when given new examples of X the machine can
    correctly predict the corresponding y labels.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督学习：** *在监督学习中，你有一组已知的输入（特征）和一组已知的输出（标签）。这些通常被称为X和y。算法的目标是学习将输入映射到输出的映射函数。这样，当给出新的X示例时，机器可以正确预测相应的y标签。*'
- en: '**Unsupervised Learning:*** In unsupervised learning, you only have a set of
    inputs (X) and no corresponding labels (y). The goal of the algorithm is to find
    previously unknown patterns in the data. Quite often these algorithms are used
    to find meaningful clusters of similar samples of X so in effect finding the categories
    intrinsic to the data.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**无监督学习：** *在无监督学习中，你只有一组输入（X）而没有对应的标签（y）。算法的目标是发现数据中以前未知的模式。这些算法经常用于寻找类似样本X的有意义的聚类，从而在实际中找到数据的内在类别。*'
- en: Classification
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类
- en: In classification, the outputs (y) are categories. These can be binary, for
    example, if we were classifying spam email vs not spam email. They can also be
    multiple categories such as classifying species of [flowers](https://archive.ics.uci.edu/ml/datasets/iris),
    this is known as multiclass classification.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类中，输出（y）是类别。这些可以是二分类，例如，如果我们在分类垃圾邮件与非垃圾邮件。也可以是多类别，例如分类[花卉](https://archive.ics.uci.edu/ml/datasets/iris)，这被称为多类分类。
- en: Let’s walk through a simple example of classification using scikit-learn. If
    you don’t already have this installed it can be installed either via pip or conda
    as outlined [here](https://scikit-learn.org/stable/install.html).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个使用scikit-learn的简单分类示例来演示。如果你尚未安装，可以通过pip或conda按照[这里](https://scikit-learn.org/stable/install.html)的说明进行安装。
- en: Scikit-learn has a number of datasets that can be directly accessed via the
    library. For ease in this article, I will be using these example datasets throughout.
    To illustrate classification I will use the wine dataset which is a multiclass
    classification problem. In the dataset, the inputs (X) consist of 13 features
    relating to various properties of each wine type. The known outputs (y) are wine
    types which in the dataset have been given a number 0, 1 or 2.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn有许多可以通过库直接访问的数据集。为了方便起见，本文将使用这些示例数据集。为了说明分类，我将使用酒数据集，这是一个多类分类问题。在数据集中，输入（X）由与每种葡萄酒类型相关的13个特征组成。已知的输出（y）是葡萄酒类型，在数据集中被标记为0、1或2。
- en: The imports I am using for all the code in this article are shown below.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中所有代码所使用的导入如下所示。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the below code I am downloading the data and converting to a pandas data
    frame.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我正在下载数据并将其转换为pandas数据框。
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The next stage in a supervised learning problem is to split the data into test
    and train sets. The train set can be used by the algorithm to learn the mapping
    between inputs and outputs, and then the reserved test set can be used to evaluate
    how well the model has learned this mapping. In the below code I am using the
    scikit-learn model_selection function `train_test_split` to do this.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习问题的下一阶段是将数据拆分为测试集和训练集。训练集可以被算法用来学习输入和输出之间的映射，然后保留的测试集可以用来评估模型学习这种映射的效果。在下面的代码中，我使用了scikit-learn的model_selection函数`train_test_split`来实现这一点。
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the next step, we need to choose the algorithm that will be best suited to
    learn the mapping in your chosen dataset. In scikit-learn there are many different
    algorithms to choose from, all of which use different functions and methods to
    learn the mapping, you can view the full list [here](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们需要选择最适合学习你选择的数据集中的映射的算法。在scikit-learn中有许多不同的算法可供选择，这些算法使用不同的函数和方法来学习映射，你可以在[这里](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)查看完整列表。
- en: To determine the best model I am running the following code. I am training the
    model using a selection of algorithms and obtaining the F1-score for each one.
    The F1 score is a good indicator of the overall accuracy of a classifier. I have
    written a detailed description of the various metrics that can be used to evaluate
    a classifier [here](https://towardsdatascience.com/understanding-the-confusion-matrix-and-its-business-applications-c4e8aaf37f42).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定最佳模型，我正在运行以下代码。我使用一系列算法训练模型，并获取每种算法的F1得分。F1得分是分类器整体准确性的良好指标。我在[这里](https://towardsdatascience.com/understanding-the-confusion-matrix-and-its-business-applications-c4e8aaf37f42)写了详细的描述，介绍了可以用于评估分类器的各种指标。
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/b228fa9a0ee7af8de72a061aabe63c88.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b228fa9a0ee7af8de72a061aabe63c88.png)'
- en: A perfect F1 score would be 1.0, therefore, the closer the number is to 1.0
    the better the model performance. The results above suggest that the Random Forest
    Classifier is the best model for this dataset.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 完美的F1得分是1.0，因此，得分越接近1.0，模型性能越好。上述结果表明，随机森林分类器是此数据集的最佳模型。
- en: Regression
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回归
- en: In regression, the outputs (y) are continuous values rather than categories.
    An example of regression would be predicting how many sales a store may make next
    month, or what the future price of your house might be.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归中，输出（y）是连续值，而不是类别。回归的一个例子是预测商店下个月可能的销售额，或预测你房子的未来价格。
- en: Again to illustrate regression I will use a dataset from scikit-learn known
    as the boston housing dataset. This consists of 13 features (X) which are various
    properties of a house such as the number of rooms, the age and crime rate for
    the location. The output (y) is the price of the house.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步说明回归，我将使用一个来自scikit-learn的数据集，称为波士顿住房数据集。该数据集包含13个特征（X），这些特征是房子的各种属性，如房间数量、年龄和犯罪率。输出（y）是房价。
- en: I am loading the data using the code below and splitting it into test and train
    sets using the same method I used for the wine dataset.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在使用下面的代码加载数据，并使用与处理葡萄酒数据集相同的方法将数据拆分为测试集和训练集。
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can use this [cheat sheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) to
    see the available algorithms suited to regression problems in scikit-learn. We
    will use similar code to the classification problem to loop through a selection
    and print out the scores for each.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个[备忘单](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)来查看scikit-learn中适用于回归问题的可用算法。我们将使用类似于分类问题的代码，循环遍历选择并打印每个算法的分数。
- en: There are a number of different metrics used to evaluate regression models.
    These are all essentially error metrics and measure the difference between the
    actual and predicted values achieved by the model. I have used the root mean squared
    error (RMSE). For this metric, the closer to zero the value is the better the
    performance of the model. This [article](https://www.dataquest.io/blog/understanding-regression-error-metrics/) gives
    a really good explanation of error metrics for regression problems.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 评估回归模型使用了许多不同的指标。这些指标本质上都是误差指标，衡量模型实际值和预测值之间的差异。我使用了均方根误差（RMSE）。对于这个指标，值越接近零，模型性能越好。这个[文章](https://www.dataquest.io/blog/understanding-regression-error-metrics/)对回归问题的误差指标进行了非常好的解释。
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/112c20479514357f35a9a727672f387f.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/112c20479514357f35a9a727672f387f.png)'
- en: The RMSE score suggests that either the linear regression and ridge regression
    algorithms perform best for this dataset.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE 分数表明线性回归和岭回归算法在这个数据集上表现最佳。
- en: Unsupervised learning
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无监督学习
- en: There are a number of different types of unsupervised learning but for simplicity
    here I am going to focus on the [clustering methods](https://en.wikipedia.org/wiki/Cluster_analysis).
    There are many different algorithms for clustering all of which use slightly different
    techniques to find clusters of inputs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种不同类型的无监督学习，但为了简便，我将专注于[聚类方法](https://en.wikipedia.org/wiki/Cluster_analysis)。聚类有许多不同的算法，每种算法都使用稍微不同的技术来寻找输入的簇。
- en: Probably one of the most widely used methods is Kmeans. This algorithm performs
    an iterative process whereby a specified number of randomly generated means are
    initiated. A distance metric, [Euclidean](https://en.wikipedia.org/wiki/Euclidean_distance) distance
    is calculated for each data point from the centroids, thus creating clusters of
    similar values. The centroid of each cluster then becomes the new mean and this
    process is repeated until the optimum result has been achieved.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最广泛使用的方法之一是 Kmeans。这个算法执行一个迭代过程，其中会启动指定数量的随机生成的均值。计算每个数据点到质心的距离度量，[欧几里得](https://en.wikipedia.org/wiki/Euclidean_distance)距离，从而创建相似值的簇。每个簇的质心随后成为新的均值，这个过程会重复直到达到最佳结果。
- en: Let’s use the wine dataset we used in the classification task, with the y labels
    removed, and see how well the k-means algorithm can identify the wine types from
    the inputs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用在分类任务中使用的葡萄酒数据集（去掉 y 标签），看看 k-means 算法能从输入中识别出酒的类型的效果如何。
- en: As we are only using the inputs for this model I am splitting the data into
    test and train using a slightly different method.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只使用输入来构建这个模型，我将数据分为测试集和训练集，使用稍微不同的方法。
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As Kmeans is reliant on the distance metric to determine the clusters it is
    usually necessary to perform feature scaling (ensuring that all features have
    the same scale) before training the model. In the below code I am using the MinMaxScaler
    to scale the features so that all values fall between 0 and 1.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Kmeans 依赖于距离度量来确定簇，因此在训练模型之前通常需要进行特征缩放（确保所有特征具有相同的尺度）。在下面的代码中，我使用了 MinMaxScaler
    来缩放特征，使所有值都落在 0 和 1 之间。
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With K-means you have to specify the number of clusters the algorithm should
    use. So one of the first steps is to identify the optimum number of clusters.
    This is achieved by iterating through a number of values of k and plotting the
    results on a chart. This is known as the Elbow method as it typically produces
    a plot with a curve that looks a little like the curve of your elbow. The yellowbrick [library](https://www.scikit-yb.org/en/latest/quickstart.html) (which
    is a great library for visualising scikit-learn models and can be pip installed)
    has a really nice plot for this. The code below produces this visualisation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 K-means 时，你必须指定算法应使用的簇数。因此，首先需要确定最佳的簇数。这是通过迭代多个 k 值并将结果绘制在图表上来实现的。这被称为肘部法，因为它通常会产生一个看起来有点像你肘部曲线的图。yellowbrick
    [库](https://www.scikit-yb.org/en/latest/quickstart.html)（这是一个很棒的可视化 scikit-learn
    模型的库，可以通过 pip 安装）有一个非常好的图。下面的代码生成了这个可视化。
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/a4ff56635ba4125cf527a067392ceba7.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4ff56635ba4125cf527a067392ceba7.png)'
- en: Ordinarily, we wouldn’t already know how many categories we have in a dataset
    where we are using a clustering technique. However, in this case, we know that
    there are three wine types in the data — the curve has correctly selected three
    as the optimum number of clusters to use in the model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们不会知道数据集中有多少个类别来使用聚类技术。然而，在这种情况下，我们知道数据中有三种酒——曲线已经正确选择了三个作为模型中使用的最佳簇数。
- en: The next step is to initialise the K-means algorithm and fit the model to the
    training data and evaluate how effectively the algorithm has clustered the data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是初始化 K-means 算法，并将模型拟合到训练数据上，评估算法对数据的聚类效果。
- en: One method used for this is known as the [silhouette score](https://en.wikipedia.org/wiki/Silhouette_(clustering)).
    This measures the consistency of values within the clusters. Or in other words
    how similar to each other the values in each cluster are, and how much separation
    there is between the clusters. The silhouette score is calculated for each value
    and will range from -1 to +1\. These values are then plotted to form a silhouette
    plot. Again yellowbrick provides a simple way to construct this type of plot.
    The code below creates this visualisation for the wine dataset.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 用于此目的的一种方法被称为 [轮廓系数](https://en.wikipedia.org/wiki/Silhouette_(clustering))。它衡量簇内值的一致性。换句话说，它衡量每个簇内值的相似程度以及簇之间的分离度。轮廓系数针对每个值进行计算，范围从-1到+1。这些值随后被绘制成轮廓图。再次，yellowbrick
    提供了一种简单的方法来构建这种图形。下面的代码为酒类数据集创建了这种可视化效果。
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/97361e3877e513bd6978ee0085f57086.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97361e3877e513bd6978ee0085f57086.png)'
- en: 'A silhouette plot can be interpreted in the following way:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓图可以按如下方式解读：
- en: The closer the mean score (which is the red dotted line in the above) is to
    +1 the better matched the data points are within the cluster.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均得分（即上图中的红色虚线）越接近 +1，簇内数据点匹配度越好。
- en: Data points with a score of 0 are very close to the decision boundary for another
    cluster (so the separation is low).
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 得分为0的数据点非常接近另一个簇的决策边界（因此分离度低）。
- en: Negative values indicate that the data points may have been assigned to the
    wrong cluster.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负值表示数据点可能被分配到了错误的簇。
- en: The width of each cluster should be reasonably uniform if they aren’t then the
    incorrect value of k may have been used.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果每个簇的宽度不够均匀，则可能使用了不正确的k值。
- en: The plot for the wine data set above shows that cluster 0 may not be as consistent
    as the others due to most data points being below the average score and a few
    data points having a score below 0.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的酒类数据集图表显示，簇0可能没有其他簇一致，因为大多数数据点低于平均分数，并且一些数据点的得分低于0。
- en: Silhouette scores can be particularly useful in comparing one algorithm against
    another or different values of k.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓系数在比较不同算法或不同k值时特别有用。
- en: In this post, I wanted to give a brief introduction to each of the three types
    of machine learning. There are many other steps involved in all of these processes
    including feature engineering, data processing and hyperparameter optimisation
    to determine both the best data preprocessing techniques and the best models to
    use.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我想简要介绍一下三种机器学习的类型。这些过程涉及许多其他步骤，包括特征工程、数据处理和超参数优化，以确定最佳的数据预处理技术和模型。
- en: Thanks for reading!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: '**Bio: [Rebecca Vickery](https://www.linkedin.com/in/rebecca-vickery-20b94133/)**
    is learning data science through self study. Data Scientist @ Holiday Extras.
    Co-Founder of alGo.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介：[Rebecca Vickery](https://www.linkedin.com/in/rebecca-vickery-20b94133/)**
    通过自学学习数据科学。Holiday Extras 数据科学家。alGo 联合创始人。'
- en: '[Original](https://towardsdatascience.com/beginners-guide-to-the-three-types-of-machine-learning-3141730ef45d).
    Reposted with permission.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/beginners-guide-to-the-three-types-of-machine-learning-3141730ef45d)。已获许可转载。'
- en: '**Related:**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Machine Learning Classification: A Dataset-based Pictorial](/2018/11/machine-learning-classification-dataset-based-pictorial.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习分类：基于数据集的图解](/2018/11/machine-learning-classification-dataset-based-pictorial.html)'
- en: '[Python Libraries for Interpretable Machine Learning](/2019/09/python-libraries-interpretable-machine-learning.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[可解释机器学习的 Python 库](/2019/09/python-libraries-interpretable-machine-learning.html)'
- en: '[Five Command Line Tools for Data Science](/2019/07/five-command-line-tools-data-science.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学的五个命令行工具](/2019/07/five-command-line-tools-data-science.html)'
- en: '* * *'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的 IT 工作'
- en: '* * *'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个数据科学家都应该知道的三大 R 语言库（即使你使用 Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
- en: '[Python Basics: Syntax, Data Types, and Control Structures](https://www.kdnuggets.com/python-basics-syntax-data-types-and-control-structures)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 基础：语法、数据类型和控制结构](https://www.kdnuggets.com/python-basics-syntax-data-types-and-control-structures)'
- en: '[Optimizing Data Storage: Exploring Data Types and Normalization in SQL](https://www.kdnuggets.com/optimizing-data-storage-exploring-data-types-and-normalization-in-sql)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化数据存储：探索 SQL 中的数据类型和规范化](https://www.kdnuggets.com/optimizing-data-storage-exploring-data-types-and-normalization-in-sql)'
- en: '[Types of Visualization Frameworks](https://www.kdnuggets.com/types-of-visualization-frameworks)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[可视化框架的类型](https://www.kdnuggets.com/types-of-visualization-frameworks)'
- en: '[KDnuggets News, December 14: 3 Free Machine Learning Courses for…](https://www.kdnuggets.com/2022/n48.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，12月14日：3 门免费的机器学习课程…](https://www.kdnuggets.com/2022/n48.html)'
- en: '[OpenAI API for Beginners: Your Easy-to-Follow Starter Guide](https://www.kdnuggets.com/openai-api-for-beginners-your-easy-to-follow-starter-guide)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAI API 初学者指南：易于跟随的入门指南](https://www.kdnuggets.com/openai-api-for-beginners-your-easy-to-follow-starter-guide)'
