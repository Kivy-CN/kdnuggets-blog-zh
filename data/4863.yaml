- en: Robust Word2Vec Models with Gensim & Applying Word2Vec Features for Machine
    Learning Tasks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Gensim 的稳健 Word2Vec 模型与应用 Word2Vec 特征于机器学习任务
- en: 原文：[https://www.kdnuggets.com/2018/04/robust-word2vec-models-gensim.html](https://www.kdnuggets.com/2018/04/robust-word2vec-models-gensim.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/04/robust-word2vec-models-gensim.html](https://www.kdnuggets.com/2018/04/robust-word2vec-models-gensim.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**Editor''s note:** This post is only one part of a far more thorough and in-depth
    original, [found here](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa),
    which covers much more than what is included here.'
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**编辑注：** 本文只是一个更全面、更深入的原文的一部分，[原文链接](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)包含了更多内容。'
- en: Robust Word2Vec Models with Gensim
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Gensim 构建稳健的 Word2Vec 模型
- en: While our implementations are decent enough, they are not optimized enough to
    work well on large corpora. The `[**gensim**](https://radimrehurek.com/gensim/)` framework,
    created by Radim Řehůřek consists of a robust, efficient and scalable implementation
    of the Word2Vec model. We will leverage the same on our Bible corpus. In our workflow,
    we will tokenize our normalized corpus and then focus on the following four parameters
    in the Word2Vec model to build it.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的实现已经相当不错，但在大语料库上并没有得到优化。`[**gensim**](https://radimrehurek.com/gensim/)`
    框架由 Radim Řehůřek 创建，提供了一个健壮、高效且可扩展的 Word2Vec 模型实现。我们将在我们的圣经语料库上利用它。在我们的工作流程中，我们将对规范化的语料库进行分词，然后关注
    Word2Vec 模型中的以下四个参数来构建它。
- en: '`**size**`**:** The word embedding dimensionality'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**size**`**:** 词嵌入的维度'
- en: '`**window**`**:** The context window size'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**window**`**:** 上下文窗口大小'
- en: '`**min_count**`**:** The minimum word count'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**min_count**`**:** 最小词频'
- en: '`**sample**`**: **The downsample setting for frequent words'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**sample**`**:** 高频词的下采样设置'
- en: After building our model, we will use our words of interest to see the top similar
    words for each of them.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型后，我们将使用感兴趣的词查看每个词的最相似词。
- en: '![](../Images/ca2b2f3fb686c91a092f9e806ac00ddb.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca2b2f3fb686c91a092f9e806ac00ddb.png)'
- en: The similar words here definitely are more related to our words of interest
    and this is expected given that we ran this model for more number of iterations
    which must have yield better and more contextual embeddings. Do you notice any
    interesting associations?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的相似词确实与我们感兴趣的词更相关，这符合预期，因为我们运行了更多次数的模型，这应该带来了更好、更具上下文的嵌入。你注意到任何有趣的关联吗？
- en: '![](../Images/775e2a392863dacc34e2c4181fd5dee0.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/775e2a392863dacc34e2c4181fd5dee0.png)'
- en: Noah’s sons come up as the most contextually similar entities from our model!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 诺亚的儿子被我们的模型识别为最具上下文相似性的实体！
- en: Let’s also visualize the words of interest and their similar words using their
    embedding vectors after reducing their dimensions to a 2-D space with t-SNE.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在使用 t-SNE 将维度减少到二维空间后，使用词嵌入向量可视化感兴趣的词及其相似词。
- en: '![](../Images/72f48cdd6154c442a2a26cae94e64f8b.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72f48cdd6154c442a2a26cae94e64f8b.png)'
- en: Visualizing our word2vec word embeddings using t-SNE
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 t-SNE 可视化我们的 word2vec 词嵌入
- en: The red circles have been drawn by me to point out some interesting associations
    which I found out. We can clearly see based on what I depicted earlier that ***noah ***and
    his sons are quite close to each other based on the word embeddings from our model!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 红色圆圈是我画的，用于指出我发现的一些有趣的关联。根据我之前描述的内容，我们可以清楚地看到，基于我们模型的词嵌入，***noah*** 和他的儿子们彼此之间非常接近！
- en: Applying Word2Vec features for Machine Learning Tasks
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用 Word2Vec 特征于机器学习任务
- en: 'If you remember reading the previous article [***Part-3: Traditional Methods
    for Text Data***](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41)you
    might have seen me using features for some actual machine learning tasks like
    clustering. Let’s leverage our other top corpus and try to achieve the same. To
    start with, we will build a simple Word2Vec model on the corpus and visualize
    the embeddings.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '如果你记得阅读过之前的文章 [***Part-3: 传统文本数据方法***](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41)，你可能看到我使用了一些实际的机器学习任务特征，如聚类。我们将利用其他顶级语料库，尝试实现相同的目标。首先，我们将基于语料库构建一个简单的
    Word2Vec 模型，并可视化嵌入。'
- en: '![](../Images/8b3bc365fee36478c6c1a7c8621bf9c2.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b3bc365fee36478c6c1a7c8621bf9c2.png)'
- en: Visualizing word2vec word embeddings on our toy corpus
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的玩具语料库上可视化 word2vec 词嵌入
- en: Remember that our corpus is extremely small so to get meaninful word embeddings
    and for the model to get more context and semantics, more data helps. Now what
    is a word embedding in this scenario? It’s typically a dense vector for each word
    as depicted in the following example for the word ***sky***.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们的语料库非常小，因此为了获得有意义的词嵌入并让模型获得更多的上下文和语义，更多的数据是有帮助的。那么在这种情况下，什么是词嵌入？它通常是每个词的密集向量，如下例所示，表示词***sky***。
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now suppose we wanted to cluster the eight documents from our toy corpus, we
    would need to get the document level embeddings from each of the words present
    in each document. One strategy would be to average out the word embeddings for
    each word in a document. This is an extremely useful strategy and you can adopt
    the same for your own problems. Let’s apply this now on our corpus to get features
    for each document.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们想要对来自玩具语料库的八个文档进行聚类，我们需要从每个文档中每个词的文档级嵌入中获取信息。一种策略是计算每个文档中每个词的词嵌入的平均值。这是一种非常有用的策略，你可以将其应用到你自己的问题中。现在让我们在我们的语料库上应用这个方法，获取每个文档的特征。
- en: '![](../Images/29c8deae1fd070bcbcf525afbcec1bae.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29c8deae1fd070bcbcf525afbcec1bae.png)'
- en: Document level embeddings
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 文档级别的嵌入
- en: Now that we have our features for each document, let’s cluster these documents
    using the [***Affinity Propagation***](https://en.wikipedia.org/wiki/Affinity_propagation) algorithm,
    which is a clustering algorithm based on the concept of *“message passing”* between
    data points and does not need the number of clusters as an explicit input which
    is often required by partition-based clustering algorithms.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经获得了每个文档的特征，让我们使用[***亲和传播***](https://en.wikipedia.org/wiki/Affinity_propagation)算法对这些文档进行聚类。该算法基于数据点之间的*“消息传递”*概念，不需要显式输入簇的数量，而这是基于分区的聚类算法通常需要的。
- en: '![](../Images/b39b4c68ab91b60f63ed7e20aa842781.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b39b4c68ab91b60f63ed7e20aa842781.png)'
- en: Clusters assigned based on our document features from word2vec
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们从 word2vec 获得的文档特征进行簇的分配
- en: We can see that our algorithm has clustered each document into the right group
    based on our Word2Vec features. Pretty neat! We can also visualize how each document
    in positioned in each cluster by using [***Principal Component Analysis (PCA)***](https://en.wikipedia.org/wiki/Principal_component_analysis) to
    reduce the feature dimensions to 2-D and then visualizing the same (by color coding
    each cluster).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们的算法已经根据 Word2Vec 特征将每个文档归入了正确的组，非常棒！我们还可以通过使用[***主成分分析 (PCA)***](https://en.wikipedia.org/wiki/Principal_component_analysis)将特征维度降到
    2-D 并可视化（通过为每个簇进行颜色编码）来可视化每个文档在每个簇中的位置。
- en: '![](../Images/640851dacfaad50813cd37baaf08bd8f.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/640851dacfaad50813cd37baaf08bd8f.png)'
- en: Visualizing our document clusters
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化我们的文档簇
- en: Everything looks to be in order as documents in each cluster are closer to each
    other and far apart from other clusters.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一切看起来都很有序，因为每个簇中的文档彼此更接近，而与其他簇距离较远。
- en: '**Bio: [Dipanjan Sarkar](https://www.linkedin.com/in/dipanzan)** is a Data
    Scientist @Intel, an author, a mentor @Springboard, a writer, and a sports and
    sitcom addict.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介： [Dipanjan Sarkar](https://www.linkedin.com/in/dipanzan)** 是 Intel 的数据科学家，作者，Springboard
    的导师，作家，体育和情景喜剧迷。'
- en: '[Original](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa).
    Reposted with permission.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)。转载经许可。'
- en: '**Related:**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Text Data Preprocessing: A Walkthrough in Python](/2018/03/text-data-preprocessing-walkthrough-python.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本数据预处理：Python中的操作指南](/2018/03/text-data-preprocessing-walkthrough-python.html)'
- en: '[A General Approach to Preprocessing Text Data](/2017/12/general-approach-preprocessing-text-data.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[处理文本数据的一般方法](/2017/12/general-approach-preprocessing-text-data.html)'
- en: '[A Framework for Approaching Textual Data Science Tasks](/2017/11/framework-approaching-textual-data-tasks.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[处理文本数据科学任务的框架](/2017/11/framework-approaching-textual-data-tasks.html)'
- en: '* * *'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速通道进入网络安全领域。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT 需求'
- en: '* * *'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题
- en: '[Applying Descriptive and Inferential Statistics in Python](https://www.kdnuggets.com/applying-descriptive-and-inferential-statistics-in-python)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Python 中应用描述性和推断性统计](https://www.kdnuggets.com/applying-descriptive-and-inferential-statistics-in-python)'
- en: '[Working With Sparse Features In Machine Learning Models](https://www.kdnuggets.com/2021/01/sparse-features-machine-learning-models.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在机器学习模型中处理稀疏特征](https://www.kdnuggets.com/2021/01/sparse-features-machine-learning-models.html)'
- en: '[5 Tasks To Automate With Python](https://www.kdnuggets.com/2021/06/5-tasks-automate-python.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 Python 自动化的 5 个任务](https://www.kdnuggets.com/2021/06/5-tasks-automate-python.html)'
- en: '[Data Representation for Natural Language Processing Tasks](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理任务中的数据表示](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
- en: '[HuggingGPT: The Secret Weapon to Solve Complex AI Tasks](https://www.kdnuggets.com/2023/05/hugginggpt-secret-weapon-solve-complex-ai-tasks.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HuggingGPT：解决复杂 AI 任务的秘密武器](https://www.kdnuggets.com/2023/05/hugginggpt-secret-weapon-solve-complex-ai-tasks.html)'
- en: '[5 Coding Tasks ChatGPT Can''t Do](https://www.kdnuggets.com/5-coding-tasks-chatgpt-cant-do)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5 个 ChatGPT 无法完成的编码任务](https://www.kdnuggets.com/5-coding-tasks-chatgpt-cant-do)'
