# AI和深度学习，简明解释

> 原文：[https://www.kdnuggets.com/2017/07/ai-deep-learning-explained-simply.html/2](https://www.kdnuggets.com/2017/07/ai-deep-learning-explained-simply.html/2)

**有缺陷的自动化增加了（而不是消灭）人类工作岗位**。在我发布这篇文章的两天后，我的个人资料突然被封锁。谷歌搜索“LinkedIn账户限制”可以了解到这种情况发生在很多人身上，仅仅因为活动过多，即使不是在发送消息。我只是出于好奇，查看了那些点击“我喜欢”这篇文章的几百个用户的个人资料（感谢大家）。然后，一个天真的规则“*x*页面在*y*时间内打开”让AI决定我也是一个AI，属于“网页机器人”类型（程序浏览网站的所有页面以复制其内容）。它没有任何“减速”警告就封锁了我，毕竟警告机器人有什么用呢？

**我不是机器人，我发誓我是真人**。计算“*x*页面在*y*时间内打开”会捕捉到许多机器人，但也会出现“假阳性”：活动高峰时的好奇人类。这也让LinkedIn的工作人员感到沮丧：信任AI还是用户？我不得不发送身份证明，这花费了几天时间。这种基于规则的“AI”创造了额外的人力支持工作，仅仅处理不必要的“AI”错误，而这些错误人类本来不会犯。想想：“将所有来自尼日利亚的邮件标记为垃圾邮件”或“将所有长胡子的人标记为恐怖分子”。你可以通过添加参数、过去活动等来改进规则，但永远无法达到人类或经过良好训练的机器学习的准确度。小心“自动化”或“AI”声明：大多数仍然是过于简单的规则，没有任何深度学习。微软以260亿美元收购了LinkedIn，肯定会将这块旧系统升级为真正的机器学习。但在那之前，不要浏览LinkedIn太快！

**如果没有人能预测某事，机器学习通常也无法预测**。许多人用多年市场价格变化训练机器学习，但这些机器学习**无法预测市场**。机器学习会根据过去学习的因素和趋势来猜测事情的发展。然而，股票和经济趋势经常像随机一样变化。当旧数据变得不那么相关或迅速变得错误时，机器学习会失败。任务或规则必须保持不变，或最多偶尔更新，以便你可以重新训练。例如，学习驾驶、玩扑克、以某种风格绘画、根据健康数据预测疾病、翻译语言是机器学习的任务：旧的示例在近期仍然有效。

**机器学习可以在数据中发现因果关系，但无法发现不存在的东西**。例如，在奇怪的研究“使用面部图像的犯罪推断自动化”中，机器学习在标记的面部照片（其中一些可能是尚未被发现的罪犯）上进行训练。作者声称，机器学习能够仅通过面部照片捕捉到新的坏人，但“感觉”进一步研究会驳斥生理特征学的有效性（种族主义）。实际上，他们的数据集存在偏见：一些白领罪犯伪装成诚实的人，嘲笑这一点。机器学习学会了只能找到的关系：快乐或愤怒的嘴巴、领带的类型。那些微笑并穿白领衣物的人被分类为诚实，悲伤且穿黑领衣物的人被评为罪犯。机器学习作者试图通过面部来判断人们（这不是科学！没有相关性），但未能看到机器学习学会了通过衣物（社会地位）来判断。机器学习放大了不公正的偏见：穿便宜衣物的街头小偷（可能皮肤较黑）被发现和判刑的频率比腐败的政客和高层企业欺诈者更高。这个机器学习会将所有街头小偷送入监狱，而不是一个白领罪犯，如果没有说明街头小偷被发现的频率比白领高*x%多的话。如果说明了，再次，它会做出随机或没有决策，这不是科学。一个教训是：机器学习不像成人那样经历我们的世界。机器学习无法知道数据之外的东西，包括“显而易见”的事情，例如：火灾损害越严重，送去的消防车越多。机器学习会注意到：火灾现场的消防员越多，第二天的损害越大，因此消防车造成了火灾损害。结果：机器学习会因纵火罪将消防员送入监狱，原因是：“95%的相关性”！

![](../Images/53e8919e8836c68766c74e8746fe27ca.png)

*(机器学习无法找到不存在的相关性，例如：面部与犯罪性。但是这个数据集存在偏见：没有笑容的白领罪犯！机器学习会学习这种偏见)*

**机器学习可以预测人类无法预测的情况**。例如，由纽约M. Sinai医院用70万患者数据训练的Deep Patient，可以预测精神分裂症的发生：没有人知道原因！只有机器学习能做到这一点：人类无法通过学习机器学习来做到这一点。这是一个问题：无论是投资、医疗、司法还是军事决策，你可能希望**知道** **人工智能如何得出结论，但你却做不到**。你无法知道为什么机器学习拒绝了你的贷款、建议法官判你入狱或将工作分配给其他人。机器学习是否公平？是否受到种族、性别等的偏见影响？机器学习的计算结果虽然可见，但数量太多，无法总结成易于理解的形式。机器学习像先知一样说：“你们人类无法理解，即使我展示给你们数学，你们也无法理解，所以要有信心！你们测试了我的过去预测，这些预测是正确的！”

**人类也从未完全解释他们的决策**：我们给出听起来合理但始终**不完整、过于简化**的理由。例如：“我们入侵伊拉克是因为其拥有大规模杀伤性武器”看起来是对的，但还有几十个其他原因。这看起来很错，即使机器学习是对的：“我们轰炸了那个村庄，因为一个声誉良好的机器学习模型说他们是恐怖分子”。它只是缺乏解释。人们从机器学习中几乎总能得到正确答案后，会开始编造虚假的解释，只为公众接受机器学习的预测。一些人会秘密使用机器学习，并将创意归功于自己。

**机器学习结果的好坏仅取决于你用来训练机器学习的数据**。在机器学习中，你很少编写软件，这些软件由 Google（Keras、Tensorflow）、Microsoft 等提供，算法也是开源的。机器学习是一门由实验定义的不可预测的科学，而非理论。你大部分时间都花在准备训练数据和研究结果上，然后进行大量的修改，大多是凭猜测，并重新尝试。机器学习的训练数据过少或不准确将导致错误的结果。谷歌图像错误地将非洲裔美国人分类为猩猩，而微软的Tay机器人在 Twitter 上仅经过几小时的训练后就学会了纳粹、色情和仇恨言论。问题在于数据，而非软件。

**不良偏见隐含在人类生成的数据中**：一个在 Google 新闻上训练的机器学习模型将“父亲是医生，就像母亲是护士”联系到性别偏见。如果直接使用它，可能会优先考虑男性求职者而非女性求职者。执法机器学习模型可能会基于肤色进行歧视。在特朗普竞选期间，一些机器学习模型可能减少了对“墨西哥”餐馆的推荐，这是一种副作用，因为它阅读了许多关于墨西哥移民的负面帖子，即使没有人具体抱怨墨西哥食品或餐馆。你不能简单地将互联网数据复制到你的机器学习中，并期望它最终会平衡。**训练一个明智的机器学习模型是昂贵的**：你需要人类来审查和“去偏见”那些错误或邪恶的内容，但这些内容自然存在于媒体中。

![自动驾驶汽车](../Images/1fe4b408380fa65ec825765f72c9f9a0.png)

***(照片：詹姆斯·布赖德将一辆自动驾驶汽车困在一个意想不到的圈子里)***

**机器学习受限于缺乏通用智能和先验常识**。即使将所有专门的机器学习结合起来，或对所有内容进行训练，它在通用AI任务上仍会失败，例如理解语言。你不能像与真实的人一样与Siri、Alexa或Cortana讨论每个话题：它们只是助手。在2011年，IBM Watson在*危险边缘*电视问答中比人类回答得更快，但把加拿大和美国搞混了。机器学习可以生成长文本的有用摘要，包括情感分析（意见和情绪识别），但不如人类工作可靠。聊天机器人无法理解过多问题。目前没有任何AI可以完成每个人都能轻松做到的事情：在顾客感到沮丧或讽刺时猜测并相应调整语气。没有像电影中的通用AI。但我们可以获得一些看起来像科幻小说的AI模块，它们在狭义（特定）任务上战胜人类。新的地方在于，“狭义”可以包括创造性或所谓的仅有人类才能完成的任务：绘画（风格、几何，象征性或概念性的较少），作曲，创作，猜测，欺骗，虚假情感等，这些都令人难以置信地似乎不需要通用AI。

**没有人知道如何构建通用人工智能**。这很好：我们得到的是超人类的专门（窄域AI）工人，但不会有任何终结者或《黑客帝国》会自行决定随时消灭我们。不幸的是，人类现在可以训练机器来伤害我们，例如，一个恐怖分子教会自动驾驶卡车撞击行人。一个拥有通用智能的AI可能会自我销毁，而不是听从恐怖分子的命令。

**AI伦理将被破解和非法重编程**。当前的机器学习，既不是通用的也不是有意识的AI，将始终遵循人类给出的命令（训练数据）：不要指望AI会有良心的反对者。每个政府都将必须制定法律，详细说明自动驾驶汽车是在优先保护乘客还是行人。例如，两个孩子突然跑到一辆只有一个乘客的汽车前面，为了避免撞到孩子，这辆车只能选择一个致命的选项，比如悬崖。民意调查显示，大多数人更愿意拥有一辆撞死人而不是伤害自己的汽车。大多数人还未考虑这些极其罕见的事件，但当第一起事件发生时，即便仅仅是每十亿辆车中一次，他们也会过度反应并质疑政治家。在那些指示汽车为了拯救多个行人而牺牲单一乘客的国家，车主会要求黑客秘密重编程汽车，以始终保护乘客。但在破解的AI补丁中，可能也会安装隐藏的AI恶意软件和病毒！

**教一个人很容易**：对于大多数任务，你只需给出十几个例子，让他/她尝试几次。但机器学习需要成千上万倍的标记数据：只有人类可以从少量数据中学习。机器学习必须尝试更多次：如果真实世界的实验是必需的（无法像棋类、围棋等那样完全模拟），你将不得不撞毁成千上万辆真实的汽车，伤害或致死成千上万名真实患者，等等，才能完成训练。与人类不同的是，机器学习**过拟合**：它记住了训练数据的过于具体的细节，而不是一般模式。因此，它在真实任务中对从未见过的数据（即使只是与训练数据稍有不同）表现不佳。目前的机器学习缺乏人类的普遍智能，后者会对每种情况建模，并将其与之前的经验关联，从而从极少的例子或试错中学习，记住的只是一般性内容，忽略不相关的内容，避免尝试那些可以预测为失败的情况。

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业的快车道

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析能力

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你组织的IT需求

* * *

### 更多相关主题

+   [停止学习数据科学以寻找目标，并通过找到目标来…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)

+   [学习数据科学统计的最佳资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)

+   [一个90亿美元的AI失败，深入分析](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)

+   [建立一个稳固的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)

+   [使用管道编写清晰的Python代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)

+   [成功的数据科学家具备的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)
