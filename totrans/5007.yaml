- en: How to Rank 10% in Your First Kaggle Competition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在你的第一次Kaggle比赛中排名前10%
- en: 原文：[https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2)
- en: '**Outlier**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**离群点**'
- en: '[![Outlier Example](../Images/70eadff4905dbdee3a23ff64ef3219a8.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example_outlier.png)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[![离群点示例](../Images/70eadff4905dbdee3a23ff64ef3219a8.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example_outlier.png)'
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三推荐课程
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 加速你的网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的IT组织'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The plot shows some scaled coordinates data. We can see that there are some
    outliers in the top-right corner. Exclude them and the distribution looks good.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图中显示了一些缩放坐标数据。我们可以看到在右上角有一些离群点。排除它们后，分布看起来很好。
- en: '**Dummy Variables**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**虚拟变量**'
- en: For categorical variables, a common practice is **[One-hot Encoding](https://en.wikipedia.org/wiki/One-hot)**.
    For a categorical variable with `n` possible values, we create a group of `n` dummy
    variables. Suppose a record in the data takes one value for this variable, then
    the corresponding dummy variable is set to `1` while other dummies in the same
    group are all set to `0`.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类变量，常见的做法是**[独热编码](https://en.wikipedia.org/wiki/One-hot)**。对于一个具有`n`种可能值的分类变量，我们创建一组`n`个虚拟变量。假设数据中的记录对于这个变量取了一个值，则对应的虚拟变量设为`1`，而同组中的其他虚拟变量都设为`0`。
- en: '[![Dummies Example](../Images/f7891877228b4993c0d084ec1c553387.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example-dummies.png)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[![虚拟变量示例](../Images/f7891877228b4993c0d084ec1c553387.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example-dummies.png)'
- en: In this example, we transform `DayOfWeek` into 7 dummy variables.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将`DayOfWeek`转化为7个虚拟变量。
- en: Note that when the categorical variable can take many values (hundreds or more),
    this might not work well. It’s difficult to find a general solution to that, but
    I’ll discuss one scenario in the next section.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当分类变量可以取许多值（数百个或更多）时，这可能效果不佳。虽然很难找到一个通用的解决方案，但我将在下一节中讨论一种情况。
- en: '**Feature Engineering**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征工程**'
- en: Some describe the essence of Kaggle competitions as **feature engineering supplemented
    by model tuning and ensemble learning**. Yes, that makes a lot of sense. **Feature
    engineering gets your very far.** Yet it is how well you know about the domain
    of given data that decides how far you can go. For example, in a competition where
    data is mainly consisted of texts, Natural Language Processing teachniques are
    a must. The approach of constructing useful features is something we all have
    to continuously learn in order to do better.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人将Kaggle比赛的本质描述为**特征工程辅以模型调优和集成学习**。是的，这很有道理。**特征工程能让你走得很远**。然而，你对给定数据领域的了解程度决定了你能走多远。例如，在数据主要由文本组成的比赛中，自然语言处理技术是必不可少的。构建有用特征的方法是我们所有人必须不断学习的，以便做得更好。
- en: 'Basically, **when you feel that a variable is intuitively useful for the task,
    you can include it as a feature**. But how do you know it actually works? The
    simplest way is to plot it against the target variable like this:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，**当你直观地觉得某个变量对任务有用时，可以将其作为特征包含进去**。但如何确定它确实有效呢？最简单的方法是将它与目标变量绘制在一起，如下所示：
- en: '[![Checking Feature Validity](../Images/d9b85e951f68e16d26e5899549ae2247.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-vis_check_feat.png)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[![检查特征有效性](../Images/d9b85e951f68e16d26e5899549ae2247.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-vis_check_feat.png)'
- en: '**Feature Selection**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征选择**'
- en: 'Generally speaking, **we should try to craft as many features as we can and
    have faith in the model’s ability to pick up the most significant features**.
    Yet there’s still something to gain from feature selection beforehand:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，**我们应该尽可能多地创建特征，并相信模型挑选最重要特征的能力**。然而，在特征选择之前仍然有些好处：
- en: Less features mean faster training
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征较少意味着训练更快
- en: Some features are linearly related to others. This might put a strain on the
    model.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有些特征与其他特征之间是线性相关的。这可能对模型造成压力。
- en: By picking up the most important features, we can use interactions between them
    as new features. Sometimes this gives surprising improvement.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过选择最重要的特征，我们可以将它们之间的交互作用用作新特征。有时这会带来意外的改善。
- en: The simplest way to inspect feature importance is by fitting a random forest
    model. There are more robust feature selection algorithms (e.g. [this](http://jmlr.org/papers/volume10/tuv09a/tuv09a.pdf))
    which are theoretically superior but not practicable due to the absence of efficient
    implementation. You can combat noisy data (to an extent) simply by increasing
    number of trees used in a random forest.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 检查特征重要性的最简单方法是拟合随机森林模型。还有更稳健的特征选择算法（例如[this](http://jmlr.org/papers/volume10/tuv09a/tuv09a.pdf)），这些算法在理论上更优，但由于缺乏高效实现而不切实际。你可以通过增加随机森林中使用的树的数量来在一定程度上对抗噪声数据。
- en: This is important for competitions in which data is **[anonymized](https://en.wikipedia.org/wiki/Data_anonymization)** because
    you won’t waste time trying to figure out the meaning of a variable that’s of
    no significance.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于数据**[匿名化](https://en.wikipedia.org/wiki/Data_anonymization)**的竞赛很重要，因为你不会浪费时间去弄清楚一个没有意义的变量。
- en: '**Feature Encoding**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征编码**'
- en: Sometimes raw features have to be converted to some other formats for them to
    work properly.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有时原始特征必须转换为其他格式才能正常工作。
- en: For example, suppose we have a categorical variable which can take more than
    10K different values. Then naively creating dummy variables is not a feasible
    option. An acceptable solution is to create dummy variables for only a subset
    of the values (e.g. values that constitute 95% of the feature importance) and
    assign everything else to an ‘others’ class.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个类别变量，它可以取超过1万种不同的值。那么，简单地创建虚拟变量不是一个可行的选项。一个可接受的解决方案是仅为这些值的一个子集（例如，构成特征重要性95%的值）创建虚拟变量，并将其他所有值分配到“其他”类别。
- en: '**Updated on Oct 28th, 2016: **For the scenario described above, another possible
    solution is to use **Factorized Machines**. Please refer to [this post](https://www.kaggle.com/c/expedia-hotel-recommendations/forums/t/21607/1st-place-solution-summary) by
    Kaggle user “idle_speculation” for details.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**更新于2016年10月28日：** 对于上述场景，另一种可能的解决方案是使用**因子分解机**。有关详细信息，请参阅Kaggle用户“idle_speculation”的[this
    post](https://www.kaggle.com/c/expedia-hotel-recommendations/forums/t/21607/1st-place-solution-summary)。'
- en: '**Model Selection**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型选择**'
- en: 'When the features are set, we can start training models. Kaggle competitions
    usually favor**tree-based models**:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当特征设置好之后，我们可以开始训练模型。Kaggle竞赛通常偏爱**基于树的模型**：
- en: '**Gradient Boosted Trees**'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度提升树**'
- en: Random Forest
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: Extra Randomized Trees
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 额外随机树
- en: 'The following models are slightly worse in terms of general performance, but
    are suitable as base models in ensemble learning (will be discussed later):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下模型在总体表现上稍差，但作为集成学习中的基础模型（稍后讨论）是合适的：
- en: SVM
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Linear Regression
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归
- en: Logistic Regression
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Neural Networks
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络
- en: Note that this does not apply to computer vision competitions which are pretty
    much dominated by neural network models.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这不适用于计算机视觉竞赛，这些竞赛几乎完全被神经网络模型主导。
- en: All these models are implemented in **[Sklearn](http://scikit-learn.org/)**.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些模型都在**[Sklearn](http://scikit-learn.org/)**中实现。
- en: Here I want to emphasize the greatness of **[Xgboost](https://github.com/dmlc/xgboost)**.
    The outstanding performance of gradient boosted trees and Xgboost’s efficient
    implementation makes it very popular in Kaggle competitions. Nowadays almost every
    winner uses Xgboost in one way or another.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我想强调**[Xgboost](https://github.com/dmlc/xgboost)**的伟大。梯度提升树的卓越表现和Xgboost的高效实现使其在Kaggle竞赛中非常受欢迎。现在几乎每位获胜者都会以某种方式使用Xgboost。
- en: '**Updated on Oct 28th, 2016: **Recently Microsoft open sourced **[LightGBM](https://github.com/Microsoft/LightGBM)**,
    a potentially better library than Xgboost for gradient boosting.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**更新于2016年10月28日：** 最近微软开源了**[LightGBM](https://github.com/Microsoft/LightGBM)**，这是一个可能比Xgboost更优秀的梯度提升库。'
- en: By the way, for Windows users installing Xgboost could be a painstaking process.
    You can refer to [this post](https://dnc1994.com/2016/03/installing-xgboost-on-windows/) by
    me if you run into problems.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How LinkedIn Uses Machine Learning To Rank Your Feed](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Get Your First Job in Data Science without Any Work Experience](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[It''s alive! Build your first robots with Python and some cheap,…](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[From Zero to Hero: Create Your First ML Model with PyTorch](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deploying Your First Machine Learning Model](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
