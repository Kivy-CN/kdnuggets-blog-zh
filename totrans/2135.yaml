- en: A Simple Guide to Running LlaMA 2 Locally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/a-simple-guide-to-running-llama-2-locally](https://www.kdnuggets.com/a-simple-guide-to-running-llama-2-locally)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![A Simple Guide to Running LlaMA 2 Locally](../Images/883675b48c3a97d28a3e88422e51c36e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: New open source models like LLaMA 2 have become quite advanced and are free
    to use. You can use them commercially or fine-tune them on your own data to develop
    specialized versions. With their ease of use, you can now run them locally on
    your own device.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we will learn how to download the necessary files and the LLaMA
    2 model to run the CLI program and interact with an AI assistant. The setup is
    simple enough that even non-technical users or students can get it running by
    following a few basic steps.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading Llama.cpp for GPU machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To install llama.cpp locally, the simplest method is to download the pre-built
    executable from the [llama.cpp releases](https://github.com/ggerganov/llama.cpp/releases).
  prefs: []
  type: TYPE_NORMAL
- en: To install it on Windows 11 with the NVIDIA GPU, we need to first download the
    `llama-master-eb542d3-bin-win-cublas-[version]-x64.zip` file. After downloading,
    extract it in the directory of your choice. It is recommended to create a new
    folder and extract all the files in it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will download the cuBLAS drivers `cudart-llama-bin-win-[version]-x64.zip`
    and extract them in the main directory. For using the GPU acceleration, you have
    two options: `cuBLAS` for NVIDIA GPUs and `clBLAS` for AMD GPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** The [version] is the version of the CUDA installed on your local
    system. You can check it by running `nvcc --version` in the terminal.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![A Simple Guide to Running LlaMA 2 Locally](../Images/10cb80bf9308a540d8091469e5ded2c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Downloading the Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To begin, create a folder named “Models” in the main directory. Within the Models
    folder, create a new folder named “llama2_7b”. Next, download the LLaMA 2 model
    file from the [Hugging Face hub](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/tree/main).
    You can choose any version you prefer, but for this guide, we will be downloading
    the `llama-2-7b-chat.Q5_K_M.gguf` file. Once the download is complete, move the
    file into the “llama2_7b” folder you just created.
  prefs: []
  type: TYPE_NORMAL
- en: '![A Simple Guide to Running LlaMA 2 Locally](../Images/179c11a6d205ee29449a4cabc9fe2ced.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Note:** To avoid any errors, please make sure to download only the `.gguf`
    model files before running the mode.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Initiating the AI Assistant CLI Program
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can now open the terminal in the main directory. By right clicking and selecting
    “Open in Terminal” option. You can also open PowerShell and the us “cd” to change
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: '![A Simple Guide to Running LlaMA 2 Locally](../Images/ff829824e239b1cf01c38405a1a9a925.png)'
  prefs: []
  type: TYPE_IMG
- en: Copy and paste the command below and press “Enter”. We are executing the `main.exe`
    file with model directory location, gpu, color, and system prompt arguments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![A Simple Guide to Running LlaMA 2 Locally](../Images/f7fe5edbb7186863caff0fa21045a46d.png)'
  prefs: []
  type: TYPE_IMG
- en: Our llama.ccp CLI program has been successfully initialized with the system
    prompt. It tells us it's a helpful AI assistant and shows various commands to
    use.
  prefs: []
  type: TYPE_NORMAL
- en: Using LLaMA 2 Locally in PowerShell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s test out the LLaMA 2 in the PowerShell by providing the prompt. We have
    asked a simple question about the age of the earth.
  prefs: []
  type: TYPE_NORMAL
- en: The answer is accurate. Let’s ask a follow up question about earth.
  prefs: []
  type: TYPE_NORMAL
- en: '![A Simple Guide to Running LlaMA 2 Locally](../Images/55f3e00537fbfdb182d0bbb22badcd7f.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the model has provided us with multiple interesting facts about
    our planet.
  prefs: []
  type: TYPE_NORMAL
- en: '![A Simple Guide to Running LlaMA 2 Locally](../Images/a9fc624c1ce0a9d85ccc6ae41ca36eaa.png)'
  prefs: []
  type: TYPE_IMG
- en: You can ask the AI assistant to generate code and an explanation in the terminal,
    which you can easily copy and use in your IDE.
  prefs: []
  type: TYPE_NORMAL
- en: '![A Simple Guide to Running LlaMA 2 Locally](../Images/204eec95e1379ed6bf6d7d9dd119ad47.png)'
  prefs: []
  type: TYPE_IMG
- en: Perfect.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Running Llama 2 locally provides a powerful yet easy-to-use chatbot experience 
    that is customized to your needs. By following this simple guide, you can learn
    to build your own private chatbot set up in no time without needing to rely on
    paid services.
  prefs: []
  type: TYPE_NORMAL
- en: The main benefits of running LlaMA 2 locally are full control over your data
    and conversations as well as no usage limits. You can chat with your bot as much
    as you want and even tweak it to improve responses.
  prefs: []
  type: TYPE_NORMAL
- en: While less convenient than an instantly available cloud AI API, local setup
    brings peace of mind regarding data privacy.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.polywork.com/kingabzpro)****[Abid Ali Awan](https://www.polywork.com/kingabzpro)****
    ([@1abidaliawan](https://www.linkedin.com/in/1abidaliawan)) is a certified data
    scientist professional who loves building machine learning models. Currently,
    he is focusing on content creation and writing technical blogs on machine learning
    and data science technologies. Abid holds a Master''s degree in technology management
    and a bachelor''s degree in telecommunication engineering. His vision is to build
    an AI product using a graph neural network for students struggling with mental
    illness.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Llama, Llama, Llama: 3 Simple Steps to Local RAG with Your Content](https://www.kdnuggets.com/3-simple-steps-to-local-rag-with-your-content)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Easiest Way of Running Llama 3 Locally](https://www.kdnuggets.com/easiest-way-of-running-llama-3-locally)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ollama Tutorial: Running LLMs Locally Made Super Simple](https://www.kdnuggets.com/ollama-tutorial-running-llms-locally-made-super-simple)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using Groq Llama 3 70B Locally: Step by Step Guide](https://www.kdnuggets.com/using-groq-llama-3-70b-locally-step-by-step-guide)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Run an LLM Locally with LM Studio](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Get Up and Running with SQL - A List of Free Learning Resources](https://www.kdnuggets.com/2022/10/get-running-sql-list-free-learning-resources.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
