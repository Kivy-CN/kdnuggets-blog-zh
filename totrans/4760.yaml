- en: Multi-Class Text Classification Model Comparison and Selection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多类别文本分类模型比较与选择
- en: 原文：[https://www.kdnuggets.com/2018/11/multi-class-text-classification-model-comparison-selection.html/2](https://www.kdnuggets.com/2018/11/multi-class-text-classification-model-comparison-selection.html/2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/11/multi-class-text-classification-model-comparison-selection.html/2](https://www.kdnuggets.com/2018/11/multi-class-text-classification-model-comparison-selection.html/2)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](/2018/11/multi-class-text-classification-model-comparison-selection.html?page=2#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](/2018/11/multi-class-text-classification-model-comparison-selection.html?page=2#comments)'
- en: Word2vec and Logistic Regression
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Word2vec与逻辑回归
- en: '[Word2vec](https://en.wikipedia.org/wiki/Word2vec), like [doc2vec](https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e),
    belongs to the text preprocessing phase. Specifically, to the part that transforms
    a text into a row of numbers. Word2vec is a type of mapping that allows words
    with similar meaning to have similar vector representation.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[Word2vec](https://en.wikipedia.org/wiki/Word2vec)像[doc2vec](https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)一样，属于文本预处理阶段。具体来说，是将文本转换为一排数字的部分。Word2vec是一种映射方式，可以使意义相似的词具有相似的向量表示。'
- en: 'The idea behind Word2vec is rather simple: we want to use the surrounding words
    to represent the target words with a Neural Network whose hidden layer encodes
    the word representation.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec背后的想法相当简单：我们希望使用周围的词来表示目标词，通过一个神经网络，其隐藏层编码词表示。
- en: '* * *'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三名课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你在IT领域的工作'
- en: '* * *'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: First we load a word2vec model. It has been pre-trained by Google on a [100
    billion word Google News corpus](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们加载一个word2vec模型。该模型由Google在[1000亿字的Google新闻语料库](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit)上进行预训练。
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We may want to explore some vocabularies.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能想要探索一些词汇。
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/41e409166baa7ad336aa150deeaef613.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41e409166baa7ad336aa150deeaef613.png)'
- en: Figure 9
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图9
- en: BOW based approaches that includes averaging, summation, weighted addition.
    The common way is to average the two word vectors. Therefore, we will follow the
    most common way.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基于BOW的方法包括平均、求和、加权相加。常见的方法是对两个词向量取平均。因此，我们将遵循最常见的方法。
- en: We will tokenize the text and apply the tokenization to “post” column, and apply
    word vector averaging to tokenized text.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对文本进行分词，并将分词应用于“post”列，然后对分词后的文本应用词向量平均。
- en: Its time to see how logistic regression classifiers performs on these word-averaging
    document features.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候看看逻辑回归分类器在这些词平均文档特征上的表现了。
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/b1151caad1eb1ec7c8150c27b8e07f94.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1151caad1eb1ec7c8150c27b8e07f94.png)'
- en: Figure 10
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图10
- en: It was disappointing, worst we have seen so far.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这让人失望，是我们迄今为止见过的最糟糕的。
- en: '**Doc2vec and Logistic Regression**'
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**Doc2vec与逻辑回归**'
- en: The same idea of [word2vec](https://en.wikipedia.org/wiki/Word2vec) can be extended
    to documents where instead of learning feature representations for words, we learn
    it for sentences or documents. To get a general idea of a [word2vec](https://en.wikipedia.org/wiki/Word2vec),
    think of it as a mathematical average of the word vector representations of all
    the words in the document. [Doc2Vec](https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e) extends
    the idea of [word2vec](https://en.wikipedia.org/wiki/Word2vec), however words
    can only capture so much, there are times when we need relationships between documents
    and not just words.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[word2vec](https://en.wikipedia.org/wiki/Word2vec)的相同思想可以扩展到文档中，在这里我们学习的是句子或文档的特征表示，而不是词。要对[word2vec](https://en.wikipedia.org/wiki/Word2vec)有一个大致了解，可以将其视为文档中所有词向量表示的数学平均值。[Doc2Vec](https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)扩展了[word2vec](https://en.wikipedia.org/wiki/Word2vec)的思想，然而词语只能捕捉到一定程度，有时我们需要的是文档之间的关系，而不仅仅是词。'
- en: The way to train doc2vec model for our Stack Overflow questions and tags data
    is very similar with when we train [Multi-Class Text Classification with Doc2vec
    and Logistic Regression](https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的Stack Overflow问题和标签数据，训练doc2vec模型的方法与训练[多类别文本分类与Doc2vec和逻辑回归](https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4)时非常相似。
- en: First, we label the sentences. [Gensim’s Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html) implementation
    requires each document/paragraph to have a label associated with it. and we do
    this by using the `TaggedDocument` method. The format will be “TRAIN_i” or “TEST_i”
    where “i” is a dummy index of the post.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们对句子进行标记。[Gensim的Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)实现要求每个文档/段落都必须有一个相关联的标签，我们通过使用`TaggedDocument`方法来实现。格式将为“TRAIN_i”或“TEST_i”，其中“i”是帖子的虚拟索引。
- en: 'According to [Gensim doc2vec tutorial](https://github.com/RaRe-Technologies/gensim/blob/ca0dcaa1eca8b1764f6456adac5719309e0d8e6d/docs/notebooks/doc2vec-IMDB.ipynb),
    its doc2vec class was trained on the entire data, and we will do the same. Let’s
    have a look what the tagged document looks like:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[Gensim doc2vec教程](https://github.com/RaRe-Technologies/gensim/blob/ca0dcaa1eca8b1764f6456adac5719309e0d8e6d/docs/notebooks/doc2vec-IMDB.ipynb)，其doc2vec类是在整个数据集上训练的，我们也将进行相同的操作。让我们看看标记文档的样子：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/5f82f0a31a878fbe540d4d4bc9f0b132.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f82f0a31a878fbe540d4d4bc9f0b132.png)'
- en: Figure 11
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图11
- en: 'When training the doc2vec, we will vary the following parameters:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练doc2vec时，我们将调整以下参数：
- en: '`dm=0` , distributed bag of words (DBOW) is used.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dm=0`，使用分布式词袋（DBOW）。'
- en: '`vector_size=300` , 300 vector dimensional feature vectors.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vector_size=300`，300维的特征向量。'
- en: '`negative=5` , specifies how many “noise words” should be drawn.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative=5`，指定应该抽取多少个“噪声词”。'
- en: '`min_count=1`, ignores all words with total frequency lower than this.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_count=1`，忽略所有总频率低于此的词。'
- en: '`alpha=0.065` , the initial learning rate.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha=0.065`，初始学习率。'
- en: We initialize the model and train for 30 epochs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们初始化模型并训练30个epoch。
- en: Next, we get vectors from trained doc2vec model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从训练好的doc2vec模型中获取向量。
- en: Finally, we get a logistic regression model trained by the doc2vec features.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们得到一个由doc2vec特征训练的逻辑回归模型。
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/e4f217a1204b49c9cda60ad67ad47cbb.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4f217a1204b49c9cda60ad67ad47cbb.png)'
- en: Figure 12
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图12
- en: We achieve an accuracy score of 80% which is 1% higher than SVM.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得了80%的准确率，比SVM高出1%。
- en: BOW with Keras
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Keras的BOW
- en: Finally, we are going to do a text classification with [Keras](https://keras.io/) which
    is a Python Deep Learning library.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用[Keras](https://keras.io/)进行文本分类，Keras是一个Python深度学习库。
- en: 'The following code were largely taken from a [Google workshop](https://github.com/tensorflow/workshops/blob/master/extras/keras-bag-of-words/keras-bow-model.ipynb).
    The process is like this:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码大部分来自于[Google工作坊](https://github.com/tensorflow/workshops/blob/master/extras/keras-bag-of-words/keras-bow-model.ipynb)。过程如下：
- en: Separate the data into training and test sets.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据分成训练集和测试集。
- en: Use `tokenizer` methods to count the unique words in our vocabulary and assign
    each of those words to indices.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`tokenizer`方法来计算我们词汇中的唯一词，并将每个词分配到索引。
- en: Calling `fit_on_texts()` automatically creates a word index lookup of our vocabulary.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用`fit_on_texts()`会自动创建我们的词汇的词索引查找。
- en: We limit our vocabulary to the top words by passing a `num_words` param to the
    tokenizer.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过向分词器传递`num_words`参数来限制我们的词汇为前几个词。
- en: With our tokenizer, we can now use the `texts_to_matrix` method to create the
    training data that we’ll pass our model.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用我们的分词器，我们现在可以使用 `texts_to_matrix` 方法来创建训练数据，然后传递给我们的模型。
- en: We feed a one-hot vector to our model.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们向模型输入一个 one-hot 向量。
- en: After we transform our features and labels in a format Keras can read, we are
    ready to build our text classification model.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们将特征和标签转换为 Keras 可以读取的格式后，我们就准备好构建文本分类模型了。
- en: When we build our model, all we need to do is tell Keras the shape of our input
    data, output data, and the type of each layer. keras will look after the rest.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们构建模型时，我们只需要告诉 Keras 输入数据的形状、输出数据的形状和每层的类型。Keras 会处理其余的部分。
- en: When training the model, we’ll call the `fit()` method, pass it our training
    data and labels, batch size and epochs.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练模型时，我们会调用 `fit()` 方法，传入训练数据和标签、批量大小和训练轮数。
- en: '![](../Images/7516a2d9bd5e03c5f15735ca30d1adb8.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7516a2d9bd5e03c5f15735ca30d1adb8.png)'
- en: Figure 13
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13
- en: 'The accuracy is:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/18a33d8393caab65bd5d17840f686145.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18a33d8393caab65bd5d17840f686145.png)'
- en: Figure 14
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14
- en: So, which model is the best for this particular data set? I will leave it to
    you to decide.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，哪个模型最适合这个特定的数据集呢？我会留给你去决定。
- en: '[Jupyter notebook](https://github.com/susanli2016/NLP-with-Python/blob/master/Text%20Classification%20model%20selection.ipynb) can
    be found on [Github](https://github.com/susanli2016/NLP-with-Python/blob/master/Text%20Classification%20model%20selection.ipynb).
    Have a productive day!'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[Jupyter notebook](https://github.com/susanli2016/NLP-with-Python/blob/master/Text%20Classification%20model%20selection.ipynb)
    可以在 [Github](https://github.com/susanli2016/NLP-with-Python/blob/master/Text%20Classification%20model%20selection.ipynb)
    上找到。祝您一天愉快！'
- en: '**References:**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: '[https://github.com/RaRe-Technologies/movie-plots-by-genre/blob/master/ipynb_with_output/Document%20classification%20with%20word%20embeddings%20tutorial%20-%20with%20output.ipynb](https://github.com/RaRe-Technologies/movie-plots-by-genre/blob/master/ipynb_with_output/Document%20classification%20with%20word%20embeddings%20tutorial%20-%20with%20output.ipynb)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/RaRe-Technologies/movie-plots-by-genre/blob/master/ipynb_with_output/Document%20classification%20with%20word%20embeddings%20tutorial%20-%20with%20output.ipynb](https://github.com/RaRe-Technologies/movie-plots-by-genre/blob/master/ipynb_with_output/Document%20classification%20with%20word%20embeddings%20tutorial%20-%20with%20output.ipynb)'
- en: '[https://github.com/tensorflow/workshops/blob/master/extras/keras-bag-of-words/keras-bow-model.ipynb](https://github.com/tensorflow/workshops/blob/master/extras/keras-bag-of-words/keras-bow-model.ipynb)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/tensorflow/workshops/blob/master/extras/keras-bag-of-words/keras-bow-model.ipynb](https://github.com/tensorflow/workshops/blob/master/extras/keras-bag-of-words/keras-bow-model.ipynb)'
- en: '[https://datascience.stackexchange.com/questions/20076/word2vec-vs-sentence2vec-vs-doc2vec](https://datascience.stackexchange.com/questions/20076/word2vec-vs-sentence2vec-vs-doc2vec)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://datascience.stackexchange.com/questions/20076/word2vec-vs-sentence2vec-vs-doc2vec](https://datascience.stackexchange.com/questions/20076/word2vec-vs-sentence2vec-vs-doc2vec)'
- en: '**Bio: [Susan Li](https://www.linkedin.com/in/susanli/)** is changing the world,
    one article at a time. She is a Sr. Data Scientist, located in Toronto, Canada.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[Susan Li](https://www.linkedin.com/in/susanli/)** 正在以一篇文章改变世界。她是一位高级数据科学家，工作地点在加拿大多伦多。'
- en: '[Original](https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568).
    Reposted with permission.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568)。经许可转载。'
- en: '**Related:**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Machine Learning for Text Classification Using SpaCy in Python](/2018/09/machine-learning-text-classification-using-spacy-python.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 SpaCy 进行文本分类的机器学习](/2018/09/machine-learning-text-classification-using-spacy-python.html)'
- en: '[Multi-Class Text Classification with Scikit-Learn](/2018/08/multi-class-text-classification-scikit-learn.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Scikit-Learn 的多分类文本分类](/2018/08/multi-class-text-classification-scikit-learn.html)'
- en: '[Named Entity Recognition and Classification with Scikit-Learn](/2018/10/named-entity-recognition-classification-scikit-learn.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Scikit-Learn 进行命名实体识别和分类](/2018/10/named-entity-recognition-classification-scikit-learn.html)'
- en: More On This Topic
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[A Comparison of Machine Learning Algorithms in Python and R](https://www.kdnuggets.com/2023/06/machine-learning-algorithms-python-r.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 和 R 中的机器学习算法比较](https://www.kdnuggets.com/2023/06/machine-learning-algorithms-python-r.html)'
- en: '[Linear Regression Model Selection: Balancing Simplicity and Complexity](https://www.kdnuggets.com/2023/02/linear-regression-model-selection-balancing-simplicity-complexity.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性回归模型选择：平衡简洁性和复杂性](https://www.kdnuggets.com/2023/02/linear-regression-model-selection-balancing-simplicity-complexity.html)'
- en: '[What is Text Classification?](https://www.kdnuggets.com/2022/07/text-classification.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是文本分类？](https://www.kdnuggets.com/2022/07/text-classification.html)'
- en: '[Best Architecture for Your Text Classification Task: Benchmarking…](https://www.kdnuggets.com/2023/04/best-architecture-text-classification-task-benchmarking-options.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[您文本分类任务的最佳架构：基准测试…](https://www.kdnuggets.com/2023/04/best-architecture-text-classification-task-benchmarking-options.html)'
- en: '[ChatGPT vs Google Bard: A Comparison of the Technical Differences](https://www.kdnuggets.com/2023/03/chatgpt-google-bard-comparison-technical-differences.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatGPT 与 Google Bard：技术差异比较](https://www.kdnuggets.com/2023/03/chatgpt-google-bard-comparison-technical-differences.html)'
- en: '[A Deep Dive into GPT Models: Evolution & Performance Comparison](https://www.kdnuggets.com/2023/05/deep-dive-gpt-models.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深入探讨 GPT 模型：演变与性能比较](https://www.kdnuggets.com/2023/05/deep-dive-gpt-models.html)'
