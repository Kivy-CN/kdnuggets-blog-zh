["```py\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Assuming heart_data is already loaded\nX = heart_data.drop('target', axis=1)\ny = heart_data['target']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Define regularization values to explore\nregularization_values = [0.001, 0.01, 0.1]\n\n# Placeholder for storing performance metrics\nperformance_metrics = []\n\n# Iterate over regularization values for L1 and L2\nfor C_value in regularization_values:\n    # Train and evaluate L1 model\n    log_reg_l1 = LogisticRegression(penalty='l1', C=C_value, solver='liblinear')\n    log_reg_l1.fit(X_train_scaled, y_train)\n    y_pred_l1 = log_reg_l1.predict(X_test_scaled)\n    accuracy_l1 = accuracy_score(y_test, y_pred_l1)\n    report_l1 = classification_report(y_test, y_pred_l1)\n    performance_metrics.append(('L1', C_value, accuracy_l1))\n\n    # Train and evaluate L2 model\n    log_reg_l2 = LogisticRegression(penalty='l2', C=C_value, solver='liblinear')\n    log_reg_l2.fit(X_train_scaled, y_train)\n    y_pred_l2 = log_reg_l2.predict(X_test_scaled)\n    accuracy_l2 = accuracy_score(y_test, y_pred_l2)\n    report_l2 = classification_report(y_test, y_pred_l2)\n    performance_metrics.append(('L2', C_value, accuracy_l2))\n\n# Print the performance metrics for all models\nprint(\"Model Performance Evaluation:\")\nprint(\"--------------------------------\")\nfor metric in performance_metrics:\n    reg_type, C_value, accuracy = metric\n    print(f\"Regularization: {reg_type}, C: {C_value}, Accuracy: {accuracy:.2f}\") \n```", "```py\nfrom tensorflow.keras.regularizers import l1_l2\nimport numpy as np\n\n# Define a list/grid of L1 and L2 regularization values\nl1_values = [0.001, 0.01, 0.1]\nl2_values = [0.001, 0.01, 0.1]\n\n# Placeholder for storing performance metrics\nperformance_metrics = []\n\n# Iterate over all combinations of L1 and L2 values\nfor l1_val in l1_values:\n    for l2_val in l2_values:\n        # Define model with the current combination of L1 and L2\n        model = Sequential([\n            Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],), kernel_regularizer=l1_l2(l1=l1_val, l2=l2_val)),\n            Dropout(0.5),\n            Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=l1_val, l2=l2_val)),\n            Dropout(0.5),\n            Dense(1, activation='sigmoid')\n        ])\n\n        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n        # Train the model\n        history = model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=100, batch_size=10, verbose=0)\n\n        # Evaluate the model\n        loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n\n        # Store the performance along with the regularization values\n        performance_metrics.append((l1_val, l2_val, accuracy))\n\n# Find the best performing model\nbest_performance = max(performance_metrics, key=lambda x: x[2])\nbest_l1, best_l2, best_accuracy = best_performance\n\n# After the loop, to print all performance metrics\nprint(\"All Model Performances:\")\nprint(\"L1 Value | L2 Value | Accuracy\")\nfor metrics in performance_metrics:\n    print(f\"{metrics[0]:<8} | {metrics[1]:<8} | {metrics[2]:.3f}\")\n\n# After finding the best performance, to print the best model details\nprint(\"\\nBest Model Performance:\")\nprint(\"----------------------------\")\nprint(f\"Best L1 value: {best_l1}\")\nprint(f\"Best L2 value: {best_l2}\")\nprint(f\"Best accuracy: {best_accuracy:.3f}\") \n```"]