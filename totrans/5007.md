# 如何在你的第一次Kaggle比赛中排名前10%

> 原文：[https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2)

**离群点**

[![离群点示例](../Images/70eadff4905dbdee3a23ff64ef3219a8.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example_outlier.png)

* * *

## 我们的前三推荐课程

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 加速你的网络安全职业生涯

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的IT组织

* * *

图中显示了一些缩放坐标数据。我们可以看到在右上角有一些离群点。排除它们后，分布看起来很好。

**虚拟变量**

对于分类变量，常见的做法是**[独热编码](https://en.wikipedia.org/wiki/One-hot)**。对于一个具有`n`种可能值的分类变量，我们创建一组`n`个虚拟变量。假设数据中的记录对于这个变量取了一个值，则对应的虚拟变量设为`1`，而同组中的其他虚拟变量都设为`0`。

[![虚拟变量示例](../Images/f7891877228b4993c0d084ec1c553387.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example-dummies.png)

在这个例子中，我们将`DayOfWeek`转化为7个虚拟变量。

注意，当分类变量可以取许多值（数百个或更多）时，这可能效果不佳。虽然很难找到一个通用的解决方案，但我将在下一节中讨论一种情况。

**特征工程**

有些人将Kaggle比赛的本质描述为**特征工程辅以模型调优和集成学习**。是的，这很有道理。**特征工程能让你走得很远**。然而，你对给定数据领域的了解程度决定了你能走多远。例如，在数据主要由文本组成的比赛中，自然语言处理技术是必不可少的。构建有用特征的方法是我们所有人必须不断学习的，以便做得更好。

基本上，**当你直观地觉得某个变量对任务有用时，可以将其作为特征包含进去**。但如何确定它确实有效呢？最简单的方法是将它与目标变量绘制在一起，如下所示：

[![检查特征有效性](../Images/d9b85e951f68e16d26e5899549ae2247.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-vis_check_feat.png)

**特征选择**

一般来说，**我们应该尽可能多地创建特征，并相信模型挑选最重要特征的能力**。然而，在特征选择之前仍然有些好处：

+   特征较少意味着训练更快

+   有些特征与其他特征之间是线性相关的。这可能对模型造成压力。

+   通过选择最重要的特征，我们可以将它们之间的交互作用用作新特征。有时这会带来意外的改善。

检查特征重要性的最简单方法是拟合随机森林模型。还有更稳健的特征选择算法（例如[this](http://jmlr.org/papers/volume10/tuv09a/tuv09a.pdf)），这些算法在理论上更优，但由于缺乏高效实现而不切实际。你可以通过增加随机森林中使用的树的数量来在一定程度上对抗噪声数据。

这对于数据**[匿名化](https://en.wikipedia.org/wiki/Data_anonymization)**的竞赛很重要，因为你不会浪费时间去弄清楚一个没有意义的变量。

**特征编码**

有时原始特征必须转换为其他格式才能正常工作。

例如，假设我们有一个类别变量，它可以取超过1万种不同的值。那么，简单地创建虚拟变量不是一个可行的选项。一个可接受的解决方案是仅为这些值的一个子集（例如，构成特征重要性95%的值）创建虚拟变量，并将其他所有值分配到“其他”类别。

**更新于2016年10月28日：** 对于上述场景，另一种可能的解决方案是使用**因子分解机**。有关详细信息，请参阅Kaggle用户“idle_speculation”的[this post](https://www.kaggle.com/c/expedia-hotel-recommendations/forums/t/21607/1st-place-solution-summary)。

**模型选择**

当特征设置好之后，我们可以开始训练模型。Kaggle竞赛通常偏爱**基于树的模型**：

+   **梯度提升树**

+   随机森林

+   额外随机树

以下模型在总体表现上稍差，但作为集成学习中的基础模型（稍后讨论）是合适的：

+   支持向量机

+   线性回归

+   逻辑回归

+   神经网络

请注意，这不适用于计算机视觉竞赛，这些竞赛几乎完全被神经网络模型主导。

所有这些模型都在**[Sklearn](http://scikit-learn.org/)**中实现。

在这里，我想强调**[Xgboost](https://github.com/dmlc/xgboost)**的伟大。梯度提升树的卓越表现和Xgboost的高效实现使其在Kaggle竞赛中非常受欢迎。现在几乎每位获胜者都会以某种方式使用Xgboost。

**更新于2016年10月28日：** 最近微软开源了**[LightGBM](https://github.com/Microsoft/LightGBM)**，这是一个可能比Xgboost更优秀的梯度提升库。

顺便提一下，对于 Windows 用户来说，安装 Xgboost 可能是一个费力的过程。如果遇到问题，你可以参考我写的 [this post](https://dnc1994.com/2016/03/installing-xgboost-on-windows/)。

### 相关话题

+   [LinkedIn 如何使用机器学习来排名你的动态](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)

+   [如何在没有工作经验的情况下获得数据科学领域的第一份工作](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)

+   [使用 TensorFlow 和 Keras 构建和训练你的第一个神经网络](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)

+   [它活过来了！用 Python 和一些便宜的组件构建你的第一个机器人](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)

+   [从零到英雄：使用 PyTorch 创建你的第一个 ML 模型](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)

+   [部署你的第一个机器学习模型](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)
