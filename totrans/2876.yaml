- en: What is Hierarchical Clustering?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/09/hierarchical-clustering.html](https://www.kdnuggets.com/2019/09/hierarchical-clustering.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**What is Clustering??**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clustering **is a technique that groups similar objects such that the objects
    in the same group are more similar to each other than the objects in the other
    groups. The group of similar objects is called a **Cluster.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/d864c96c4c7666187cbf8c49c37b343e.png)'
  prefs: []
  type: TYPE_IMG
- en: Clustered data points
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 5 popular clustering algorithms that data scientists need to know:'
  prefs: []
  type: TYPE_NORMAL
- en: '**K-Means Clustering**: To know more click [here](https://towardsdatascience.com/introduction-to-image-segmentation-with-k-means-clustering-83fd0a9e2fc3).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Hierarchical Clustering**: We’ll discuss this algorithm here in detail.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Mean-Shift Clustering:** To know more click [here](https://spin.atomicobject.com/2015/05/26/mean-shift-clustering/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Density-Based Spatial Clustering of Applications with Noise (DBSCAN): **To
    know more click [here](https://en.wikipedia.org/wiki/DBSCAN).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Expectation-Maximization (EM) Clustering using Gaussian Mixture Models (GMM): **To
    know more click [here](https://towardsdatascience.com/gaussian-mixture-models-d13a5e915c8e).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Hierarchical Clustering Algorithm**'
  prefs: []
  type: TYPE_NORMAL
- en: Also called **Hierarchical cluster analysis** or **HCA **is an unsupervised
    clustering algorithm which involves creating clusters that have predominant ordering
    from top to bottom.
  prefs: []
  type: TYPE_NORMAL
- en: 'For e.g: All files and folders on our hard disk are organized in a hierarchy.'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm groups similar objects into groups called ***clusters***. The
    endpoint is a set of clusters or groups*, *where each cluster is distinct from
    each other cluster, and the objects within each cluster are broadly similar to
    each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'This clustering technique is divided into two types:'
  prefs: []
  type: TYPE_NORMAL
- en: Agglomerative Hierarchical Clustering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divisive Hierarchical Clustering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Agglomerative Hierarchical Clustering**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Agglomerative Hierarchical Clustering is the most common type of hierarchical
    clustering used to group objects in clusters based on their similarity. It’s also
    known as AGNES (Agglomerative Nesting). It's a “[bottom-up](https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design)”
    approach: ***each observation starts in its own cluster, and pairs of clusters
    are merged as one moves up the hierarchy.***
  prefs: []
  type: TYPE_NORMAL
- en: '**How does it work?**'
  prefs: []
  type: TYPE_NORMAL
- en: Make each data point a single-point cluster → forms N clusters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the two closest data points and make them one cluster → forms N-1 clusters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the two closest clusters and make them one cluster → Forms N-2 clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step-3 until you are left with only one cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Have a look at the visual representation of Agglomerative Hierarchical Clustering
    for better understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/2d5c84fed59033b3130bd242b7e92738.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Agglomerative Hierarchical Clustering](https://gfycat.com/somelonelycaterpillar)'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several ways to measure the distance between clusters in order to
    decide the rules for clustering, and they are often called Linkage Methods. Some
    of the common linkage methods are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Complete-linkage**: the distance between two clusters is defined as the *longest* distance
    between two points in each cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Single-linkage**: the distance between two clusters is defined as the *shortest* distance
    between two points in each cluster. This linkage may be used to detect high values
    in your dataset which may be outliers as they will be merged at the end.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Average-linkage**: the distance between two clusters is defined as the average
    distance between each point in one cluster to every point in the other cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Centroid-linkage:** finds the centroid of cluster 1 and centroid of cluster
    2, and then calculates the distance between the two before merging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of linkage method entirely depends on you and there is no hard and
    fast method that will always give you good results. Different linkage methods
    lead to different clusters.
  prefs: []
  type: TYPE_NORMAL
- en: The point of doing all this is to demonstrate the way hierarchical clustering
    works, it maintains a memory of how we went through this process and that memory
    is stored in **Dendrogram**.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is a Dendrogram?**'
  prefs: []
  type: TYPE_NORMAL
- en: A Dendrogram is a type of tree diagram showing hierarchical relationships between
    different sets of data.
  prefs: []
  type: TYPE_NORMAL
- en: As already said a Dendrogram contains the memory of hierarchical clustering
    algorithm, so just by looking at the Dendrgram you can tell how the cluster is
    formed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/21be7560a8fe5a22873d0b7907c2d922.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Dendrogram](https://giphy.com/explore/dendrogram)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:- **'
  prefs: []
  type: TYPE_NORMAL
- en: Distance between data points represents dissimilarities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Height of the blocks represents the distance between clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So you can observe from the above figure that initially P5 and P6 which are
    closest to each other by any other point are combined into one cluster followed
    by P4 getting merged into the same cluster(C2). Then P1and P2 gets combined into
    one cluster followed by P0 getting merged into the same cluster(C4). Now P3 gets
    merged in cluster C2 and finally, both clusters get merged into one.
  prefs: []
  type: TYPE_NORMAL
- en: '**Parts of a Dendrogram**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/61b86c9b3b38eb3d02057b488973efc2.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Pic Credit](https://www.statisticshowto.datasciencecentral.com/hierarchical-clustering/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A dendrogram can be a column graph (as in the image below) or a row graph.
    Some dendrograms are circular or have a fluid-shape, but the software will usually
    produce a row or column graph. No matter what the shape, the basic graph comprises
    the same parts:'
  prefs: []
  type: TYPE_NORMAL
- en: The ***Clades*** are the branch and are arranged according to how similar (or
    dissimilar) they are. Clades that are close to the same height are similar to
    each other; clades with different heights are dissimilar — **the greater the difference
    in height, the more dissimilarity.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each clade has one or more ***leaves***.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leaves A, B, and C are more similar to each other than they are to leaves D,
    E, or F.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leaves D and E are more similar to each other than they are to leaves A, B,
    C, or F.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leaf F is substantially different from all of the other leaves.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A clade can theoretically have an infinite amount of leaves. However, the more
    leaves you have, the harder the graph will be to read with the naked eye.
  prefs: []
  type: TYPE_NORMAL
- en: '**One question that might have intrigued you by now is how do you decide when
    to stop merging the clusters?**'
  prefs: []
  type: TYPE_NORMAL
- en: You cut the dendrogram tree with a horizontal line at a height where the line
    can traverse the maximum distance up and down without intersecting the merging
    point.
  prefs: []
  type: TYPE_NORMAL
- en: For example in the below figure L3 can traverse maximum distance up and down
    without intersecting the merging points. So we draw a horizontal line and the
    number of verticle lines it intersects is the optimal number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/e468c33d0dae5aa5b033ffad73e4c21b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Choosing the optimal number of clusters.](https://www.google.com/imgres?imgurl=https%3A%2F%2Fars.els-cdn.com%2Fcontent%2Fimage%2F3-s2.0-B978012811654800004X-f04-03-9780128116548.jpg&imgrefurl=https%3A%2F%2Fwww.sciencedirect.com%2Ftopics%2Fcomputer-science%2Fagglomerative-algorithm&docid=4v7-F4YYs4wofM&tbnid=ZDsxb9kaKxWmWM%3A&vet=12ahUKEwizmMT5vtTkAhWHad4KHXc5ByQ4rAIQMygvMC96BAgBEDE..i&w=314&h=226&bih=691&biw=1440&q=dendrogram%20explained&ved=2ahUKEwizmMT5vtTkAhWHad4KHXc5ByQ4rAIQMygvMC96BAgBEDE&iact=mrc&uact=8)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of Clusters in this case = 3.**'
  prefs: []
  type: TYPE_NORMAL
- en: Divisive Hierarchical Clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Divisive* or DIANA(DIvisive ANAlysis Clustering) is a top-down clustering
    method where we assign all of the observations to a single cluster and then partition
    the cluster to two least similar clusters. Finally, we proceed recursively on
    each cluster until there is one cluster for each observation. So this clustering
    approach is exactly opposite to Agglomerative clustering.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/fcda2a4b265cb4b2bf061a6726d9e62c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Pic credit](https://www.researchgate.net/figure/Conceptual-dendrogram-for-agglomerative-and-divisive-Hierarchical-based-clustering-19_fig2_321399805)'
  prefs: []
  type: TYPE_NORMAL
- en: There is evidence that divisive algorithms produce more accurate hierarchies
    than agglomerative algorithms in some circumstances but is conceptually more complex.
  prefs: []
  type: TYPE_NORMAL
- en: In both agglomerative and divisive hierarchical clustering, users need to specify
    the desired number of clusters as a termination condition(when to stop merging).
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the goodness of Clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Well, there are many measures to do this, perhaps the most popular one is the `Dunn's
    Index`. Dunn's index is the ratio between the minimum inter-cluster distances
    to the maximum intra-cluster diameter. The diameter of a cluster is the distance
    between its two furthermost points. In order to have well separated and compact
    clusters you should aim for a higher Dunn's index.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's implement one use case scenario using Agglomerative Hierarchical clustering
    algorithm. The data set consist of customer details of one particular shopping
    mall along with their spending score.
  prefs: []
  type: TYPE_NORMAL
- en: You can download the dataset from [here](https://www.kaggle.com/shwetabh123/mall-customers).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by importing 3 basic libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure](../Images/daaa2201747eb4c09b8c92b61d618025.png)'
  prefs: []
  type: TYPE_IMG
- en: Original Dataset
  prefs: []
  type: TYPE_NORMAL
- en: So our goal is to cluster customers based on their spending score.
  prefs: []
  type: TYPE_NORMAL
- en: Out of all the features, `CustomerID` and `Genre` are irrelevant fields and
    can be dropped and create a matrix of independent variables by select only `Age`and `Annual
    Income`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need to choose the number of clusters and for doing this we’ll use
    Dendrograms.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3f388b0ef4b8de84cbd26d87f31da321.png)'
  prefs: []
  type: TYPE_IMG
- en: As we have already discussed to choose the number of clusters we draw a horizontal
    line to the longest line that traverses maximum distance up and down without intersecting
    the merging points. So we draw a horizontal line and the number of verticle lines
    it intersects is the optimal number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, it's 5\. So let's fit our Agglomerative model with 5 clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Visualize the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure](../Images/5aca25e413a6f4f2840bf998959e4520.png)'
  prefs: []
  type: TYPE_IMG
- en: A cluster of customers based on their Annual Income and spending score.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hierarchical clustering is a very useful way of segmentation. The advantage
    of not having to pre-define the number of clusters gives it quite an edge over
    k-Means. However, it doesn't work well when we have huge amount of data.
  prefs: []
  type: TYPE_NORMAL
- en: Well, this comes to the end of this article. I hope you guys have enjoyed reading
    it. Please share your thoughts/comments/doubts in the comment section.
  prefs: []
  type: TYPE_NORMAL
- en: You can reach me out over [LinkedIn](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/?source=post_page---------------------------) for
    any query.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36fac5b8825e21f2e39f62ab62445132.png)'
  prefs: []
  type: TYPE_IMG
- en: Thanks for reading !!!
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Nagesh Singh Chauhan](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)**
    is a Big data developer at CirrusLabs. He has over 4 years of working experience
    in various sectors like Telecom, Analytics, Sales, Data Science having specialisation
    in various Big data components.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/swlh/what-is-hierarchical-clustering-c04e9972e002).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Predict Age and Gender Using Convolutional Neural Network and OpenCV](/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introduction to Image Segmentation with K-Means clustering](/2019/08/introduction-image-segmentation-k-means-clustering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Classifying Heart Disease Using K-Nearest Neighbors](/2019/07/classifying-heart-disease-using-k-nearest-neighbors.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Unveiling Hidden Patterns: An Introduction to Hierarchical Clustering](https://www.kdnuggets.com/unveiling-hidden-patterns-an-introduction-to-hierarchical-clustering)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Clustering Unleashed: Understanding K-Means Clustering](https://www.kdnuggets.com/2023/07/clustering-unleashed-understanding-kmeans-clustering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Use MultiIndex for Hierarchical Data Organization in Pandas](https://www.kdnuggets.com/how-to-use-multiindex-for-hierarchical-data-organization-in-pandas)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[DBSCAN Clustering Algorithm in Machine Learning](https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What is K-Means Clustering and How Does its Algorithm Work?](https://www.kdnuggets.com/2023/05/kmeans-clustering-algorithm-work.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Clustering with scikit-learn: A Tutorial on Unsupervised Learning](https://www.kdnuggets.com/2023/05/clustering-scikitlearn-tutorial-unsupervised-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
