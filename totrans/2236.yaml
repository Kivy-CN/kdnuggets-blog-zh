- en: 'From Zero to Hero: Create Your First ML Model with PyTorch'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零到英雄：使用 PyTorch 创建你的第一个 ML 模型
- en: 原文：[https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)
- en: '![From Zero to Hero: Create Your First ML Model with PyTorch](../Images/aac9a7fec562df82c6895dad92e6871c.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![从零到英雄：使用 PyTorch 创建你的第一个 ML 模型](../Images/aac9a7fec562df82c6895dad92e6871c.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Motivation
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织中的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: PyTorch is the most widely used Python-based Deep Learning framework. It provides
    tremendous support for all machine learning architectures and data pipelines.
    In this article, we go through all the framework basics to get you started with
    implementing your algorithms.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 是最广泛使用的基于 Python 的深度学习框架。它对所有机器学习架构和数据管道提供了极大的支持。在本文中，我们将讲解框架的基础知识，以帮助你开始实现自己的算法。
- en: 'All machine learning implementations have 4 major steps:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所有机器学习实现都有 4 个主要步骤：
- en: '**Data Handling**'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据处理**'
- en: '**Model Architecture**'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型架构**'
- en: '**Training Loop**'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练循环**'
- en: '**Evaluation**'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估**'
- en: We go through all these steps while implementing our own MNIST image classification
    model in PyTorch. This will familiarize you with the general flow of a machine-learning
    project.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现我们自己的 MNIST 图像分类模型时，我们会经历所有这些步骤。这将帮助你熟悉机器学习项目的一般流程。
- en: Imports
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: torch.nn module provides support for neural network architectures and has built-in
    implementations for popular layers such as Dense Layers, Convolutional Neural
    Networks, and many more.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: torch.nn 模块提供了对神经网络架构的支持，并且内置了对常用层如密集层、卷积神经网络等的实现。
- en: torch.optim provides implementations for optimizers such as Stochastic Gradient
    Descent and Adam.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: torch.optim 提供了优化器的实现，如随机梯度下降和 Adam。
- en: Other utility modules are available for data handling support and transformations.
    We will go through each in more detail later.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其他实用模块可用于数据处理支持和转换。稍后我们将详细介绍每个模块。
- en: Declare Hyperparameters
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 声明超参数
- en: Each hyperparameter will be explained further where appropriate. However, it
    is a best practice to declare them at the top of our file for ease of change and
    understanding.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 每个超参数将在适当的地方进一步解释。然而，最好在文件顶部声明它们，以便于更改和理解。
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Data Loading and Transforms
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据加载和转换
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: MNIST is a popular image classification dataset, provided by default in PyTorch.
    It consists of grayscale images of 10 hand-written digits from 0 to 9\. Each image
    is of size 28 pixels by 28 pixels, and the dataset contains 60000 training and
    10000 testing images.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 是一个流行的图像分类数据集，在 PyTorch 中默认提供。它包含 10 个从 0 到 9 的手写数字的灰度图像。每张图像的尺寸为 28 像素
    x 28 像素，数据集包含 60000 张训练图像和 10000 张测试图像。
- en: We load the training and testing dataset separately, denoted by the train argument
    in the MNIST initialization function. The root argument declares the directory
    in which the dataset is to be downloaded.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分别加载训练和测试数据集，通过 MNIST 初始化函数中的 train 参数来表示。root 参数声明了数据集下载的目录。
- en: 'However, we also pass an additional transform argument. For PyTorch, all inputs
    and outputs are supposed to be in Torch.Tensor format. This is equivalent to a
    numpy.ndarray in numpy. This tensor format provides additional support for data
    manipulation. However, the MNIST data we load from is in the PIL.Image format.
    We need to transform the images into PyTorch-compatible tensors. Accordingly,
    we pass the following transforms:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们还传递了一个额外的转换参数。对于 PyTorch，所有输入和输出应该是 Torch.Tensor 格式。这相当于 numpy 中的 numpy.ndarray。这个张量格式提供了额外的数据操作支持。然而，我们加载的
    MNIST 数据是 PIL.Image 格式。我们需要将图像转换为 PyTorch 兼容的张量。因此，我们传递了以下转换：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The ToTensor() transform converts images to tensor format. Next, we pass an
    additional Lambda transform. The Lambda function allows us to implement custom
    transforms. Here we declare a function to flatten the input. The images are of
    size 28x28, but, we flatten them i.e. convert them to a single-dimensional array
    of size 28x28 or 784\. This will be important later when we implement our model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ToTensor() 转换将图像转换为张量格式。接下来，我们传递一个额外的 Lambda 转换。Lambda 函数允许我们实现自定义转换。在这里，我们声明一个函数来展平输入。图像的大小为
    28x28，但我们将其展平，即转换为大小为 28x28 或 784 的一维数组。稍后在实现模型时，这一点将很重要。
- en: The Compose function sequentially combines all the transforms. Firstly, the
    data is converted to tensor format and then flattened to a one-dimensional array.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Compose 函数按顺序组合所有转换。首先，数据被转换为张量格式，然后展平为一维数组。
- en: Dividing our Data into Batches
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据划分为批次
- en: For computational and training purposes, we can not pass the complete dataset
    into the model at once. We need to divide our dataset into mini-batches that will
    be fed to the model in sequential order. This allows faster training and adds
    randomness to our dataset, which can assist in stable training.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 出于计算和训练目的，我们不能一次将整个数据集传递到模型中。我们需要将数据集划分为小批量，按顺序喂给模型。这可以加快训练速度，并为我们的数据集添加随机性，这有助于稳定训练。
- en: PyTorch provides built-in support for batching our data. The DataLoader class
    from torch. utils module can create batches of data, given a torch dataset module.
    As above, we already have the dataset loaded.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 提供了对数据批处理的内置支持。来自 torch.utils 模块的 DataLoader 类可以创建数据批次，给定一个 torch 数据集模块。如上所述，我们已经加载了数据集。
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We pass the dataset to our dataloader, and our batch_size hyperparameter as
    initialization arguments. This creates an iterable data loader, so we can easily
    iterate over each batch using a simple for loop.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据集传递给我们的数据加载器，并将 batch_size 超参数作为初始化参数。这创建了一个可迭代的数据加载器，因此我们可以使用简单的 for 循环轻松遍历每个批次。
- en: Our initial image was of size (784, ) with a single associated label. The batching
    then combines different images and labels in a batch. For example, if we have
    a batch size of 64, the input size in a batch will become (64, 784) and we will
    have 64 associated labels for each batch.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初始图像的大小为 (784, )，带有一个相关标签。批处理将不同的图像和标签组合在一起。例如，如果我们有一个批量大小为 64，那么批次中的输入大小将变为
    (64, 784)，并且我们将为每个批次有 64 个相关标签。
- en: We also shuffle the training batch, which changes the images within a batch
    for each epoch. It allows for stable training and faster convergence of our model
    parameters.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还会打乱训练批次，这会在每个 epoch 更改批次中的图像。这有助于稳定训练并加快模型参数的收敛。
- en: Defining our Classification Model
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义我们的分类模型
- en: We use a simple implementation consisting of 3 hidden layers. Although simple,
    this can give you a general understanding of combining different layers for more
    complex implementations.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个由 3 个隐藏层组成的简单实现。虽然简单，但这可以让你对将不同层组合在一起以实现更复杂的实现有一个大致了解。
- en: As described above, we have an input tensor of size (784, ) and 10 different
    output classes, one for each digit from 0-9.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，我们有一个大小为 (784, ) 的输入张量和 10 个不同的输出类，每个类对应一个从 0 到 9 的数字。
- en: '**** For model implementation, we can ignore the batch dimension.**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**** 对于模型实现，我们可以忽略批次维度。**'
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Firstly, the model must inherit from the torch.nn.Module class. This provides
    basic functionality for neural network architectures. We then must implement two
    methods, __init__ and forward.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，模型必须继承自 torch.nn.Module 类。这为神经网络架构提供了基本功能。然后，我们必须实现两个方法，__init__ 和 forward。
- en: In the __init__ method, we declare all layers the model will use. We use Linear
    (also called Dense) layers provided by PyTorch. The first layer maps the input
    to 512 neurons. We can pass input_size as a model parameter, so we can later use
    it for input of different sizes as well. The second layer maps the 512 neurons
    to 256\. The third hidden layer maps the 256 neurons from the previous layer to
    128\. The final layer then finally reduces to the output size. Our output size
    will be a tensor of size (10, ) because we are predicting ten different numbers.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在 __init__ 方法中，我们声明了模型将使用的所有层。我们使用 PyTorch 提供的线性（也称为 Dense）层。第一层将输入映射到 512 个神经元。我们可以将
    input_size 作为模型参数传递，这样我们以后也可以用于不同大小的输入。第二层将 512 个神经元映射到 256。第三个隐藏层将前一层的 256 个神经元映射到
    128。最后一层将最终减少到输出大小。我们的输出大小将是一个大小为 (10, ) 的张量，因为我们正在预测十个不同的数字。
- en: '![From Zero to Hero: Create Your First ML Model with PyTorch](../Images/892683a58550b21fc5cd9d81645de015.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![从零到英雄：使用 PyTorch 创建你的第一个机器学习模型](../Images/892683a58550b21fc5cd9d81645de015.png)'
- en: Image by Author
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Moreover, we initialize a ReLU activation layer for non-linearity in our model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们初始化一个 ReLU 激活层以增加模型的非线性。
- en: The forward function receives images and we provide code for processing the
    input. We use the layers declared and sequentially pass our input through each
    layer, with an intermediate ReLU activation layer.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 前向函数接收图像，我们提供处理输入的代码。我们使用声明的层，并依次将输入通过每一层，其中包含一个中间的 ReLU 激活层。
- en: In our main code, we can then initialize the model providing it with the input
    and output size for our dataset.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的主代码中，我们可以初始化模型，提供数据集的输入和输出大小。
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once initialized, we change the model device (which can be either CUDA GPU or
    CPU). We checked for our device when we initialized the hyperparameters. Now,
    we have to manually change the device for our tensors and model layers.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦初始化完成，我们将更改模型设备（可以是 CUDA GPU 或 CPU）。在初始化超参数时，我们检查了设备。现在，我们必须手动更改张量和模型层的设备。
- en: Training Loop
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练循环
- en: Firstly, we must declare our loss function and optimizer that will be used to
    optimize our model parameters.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须声明用于优化模型参数的损失函数和优化器。
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Firstly, we must declare our loss function and optimizer that will be used to
    optimize our model parameters.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须声明用于优化模型参数的损失函数和优化器。
- en: We use the Cross-Entropy Loss that is primarily used for multi-label classification
    models. It first applies softmax to the predictions and calculates the given target
    labels and predicted values.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用交叉熵损失，主要用于多标签分类模型。它首先对预测值应用 softmax，并计算给定的目标标签和预测值。
- en: Adam optimizer is the most-used optimizer function that allows stable gradient
    descent toward convergence. It is the default optimizer choice nowadays and provides
    satisfactory results. **We pass our model parameters as an argument that denotes
    the weights that will be optimized.**
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Adam 优化器是最常用的优化函数，它允许稳定的梯度下降以趋向收敛。它现在是默认的优化器选择，并提供令人满意的结果。**我们将模型参数作为参数传递，这些参数表示将被优化的权重。**
- en: For our training loop, we build step-by-step and fill in missing portions as
    we gain an understanding.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的训练循环，我们逐步构建并填补缺失的部分，以加深理解。
- en: 'As a starting point, we iterate over the complete dataset multiple times (called
    epoch), and optimize our model each time. However, we have divided our data into
    batches. Then, for every epoch, we must iterate over each batch as well. The code
    for this will look as below:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 作为起点，我们多次遍历完整的数据集（称为 epoch），并每次优化模型。然而，我们将数据分成了多个批次。因此，对于每个 epoch，我们还必须遍历每个批次。代码如下所示：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, we can train the model given a single input batch. Our batch consists of
    images and labels. Firstly, we must separate each of these. Our model only requires
    images as input to make predictions. We then compare the predictions with the
    true labels, to estimate our model’s performance.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用单个输入批次训练模型。我们的批次包括图像和标签。首先，我们必须将这些分开。我们的模型只需要图像作为输入来进行预测。然后，我们将预测结果与真实标签进行比较，以估计模型的性能。
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We pass the batch of images directly to the model that will be processed by
    the forward function defined within the model. Once we have our predictions, we
    can optimize our model weights.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将一批图像直接传递给模型，这些图像将通过模型内部定义的前向函数进行处理。一旦获得预测结果，我们可以优化模型的权重。
- en: 'The optimization code looks as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 优化代码如下所示：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Using the above code, we can compute all the backpropagation gradients and optimize
    the model weights using the Adam optimizer. All the above codes combined can train
    our model toward convergence.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述代码，我们可以计算所有的反向传播梯度，并使用Adam优化器优化模型权重。将所有上述代码结合起来，可以使我们的模型逐步收敛。
- en: 'Complete training loop looks as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的训练循环如下所示：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The loss gradually decreases and reaches close to 0\. Then, we can evaluate
    the model on the test dataset we declared initially.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 损失值逐渐减少并接近0。然后，我们可以在我们最初声明的测试数据集上评估模型。
- en: Evaluating our Model Performace
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估我们的模型性能
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Similar to the training loop, we iterate over each batch in the test dataset
    for evaluation. We generate predictions for the inputs. However, for evaluation,
    we only need the label with the highest probability. The argmax function provides
    this functionality to obtain the index of the value with the highest value in
    our predictions array.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练循环类似，我们在测试数据集上逐批进行迭代以进行评估。我们为输入生成预测。然而，对于评估，我们只需要具有最高概率的标签。`argmax`函数提供了这个功能，以获取预测数组中值最高的索引。
- en: For the accuracy score, we can then compare if the predicted label matches the
    true target label. We then compute the accuracy of the number of correct labels
    divided by the total predicted labels.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于准确性评分，我们可以比较预测标签是否与真实目标标签匹配。然后，我们计算正确标签的数量除以总预测标签的准确率。
- en: Results
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: I only trained the model for five epochs and achieved a test accuracy of over
    96 percent, as compared to 10 percent accuracy before training. The image below
    shows the model predictions after training five epochs.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我只训练了五个周期，并取得了超过96%的测试准确率，相比之下，训练前准确率为10%。下图显示了训练五个周期后的模型预测结果。
- en: '![From Zero to Hero: Create Your First ML Model with PyTorch](../Images/eb5127ca41a590e3e22753e37e78bab3.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![从零到英雄：用PyTorch创建你的第一个ML模型](../Images/eb5127ca41a590e3e22753e37e78bab3.png)'
- en: There you have it. You have now implemented a model from scratch that can differentiate
    hand-written digits just from image pixel values.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，你已经从零开始实现了一个可以仅通过图像像素值区分手写数字的模型。
- en: This in no way is a comprehensive guide to PyTorch but it does provide you with
    a general understanding of structure and data flow in a machine learning project.
    This is nonetheless sufficient knowledge to get you started with implementing
    state-of-the-art architectures in deep learning.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这绝不是PyTorch的全面指南，但它确实为你提供了机器学习项目中结构和数据流的一般理解。尽管如此，这些知识足以让你开始实施深度学习中的最先进架构。
- en: Complete Code
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完整代码
- en: 'The complete code is as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码如下：
- en: '**model.py:**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**model.py：**'
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**main.py**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**main.py**'
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**[Muhammad Arham](https://www.linkedin.com/in/muhammad-arham-a5b1b1237/)**
    is a Deep Learning Engineer working in Computer Vision and Natural Language Processing.
    He has worked on the deployment and optimizations of several generative AI applications
    that reached the global top charts at Vyro.AI. He is interested in building and
    optimizing machine learning models for intelligent systems and believes in continual
    improvement.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**[穆罕默德·阿赫曼](https://www.linkedin.com/in/muhammad-arham-a5b1b1237/)** 是一名从事计算机视觉和自然语言处理的深度学习工程师。他曾在Vyro.AI工作，负责多个生成AI应用的部署和优化，这些应用在全球排行榜上名列前茅。他对构建和优化智能系统的机器学习模型感兴趣，并相信持续改进。'
- en: More On This Topic
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Breaking the Data Barrier: How Zero-Shot, One-Shot, and Few-Shot…](https://www.kdnuggets.com/2023/08/breaking-data-barrier-zeroshot-oneshot-fewshot-learning-transforming-machine-learning.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[突破数据障碍：如何通过零样本、单样本和少样本学习…](https://www.kdnuggets.com/2023/08/breaking-data-barrier-zeroshot-oneshot-fewshot-learning-transforming-machine-learning.html)'
- en: '[Zero-shot Learning, Explained](https://www.kdnuggets.com/2022/12/zeroshot-learning-explained.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[零样本学习详解](https://www.kdnuggets.com/2022/12/zeroshot-learning-explained.html)'
- en: '[A Bug That Can Make You a Data Science Hero](https://www.kdnuggets.com/2022/03/bug-make-data-science-hero.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一个能让你成为数据科学英雄的bug](https://www.kdnuggets.com/2022/03/bug-make-data-science-hero.html)'
- en: '[Deploying Your First Machine Learning Model](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[部署你的第一个机器学习模型](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)'
- en: '[Step-by-Step Tutorial to Building Your First Machine Learning Model](https://www.kdnuggets.com/step-by-step-tutorial-to-building-your-first-machine-learning-model)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[逐步教程：构建你的第一个机器学习模型](https://www.kdnuggets.com/step-by-step-tutorial-to-building-your-first-machine-learning-model)'
- en: '[PyTorch Tips to Boost Your Productivity](https://www.kdnuggets.com/2023/08/pytorch-tips-boost-productivity.html)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提升生产力的 PyTorch 技巧](https://www.kdnuggets.com/2023/08/pytorch-tips-boost-productivity.html)'
