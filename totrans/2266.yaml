- en: Advanced Feature Selection Techniques for Machine Learning Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型的高级特征选择技术
- en: 原文：[https://www.kdnuggets.com/2023/06/advanced-feature-selection-techniques-machine-learning-models.html](https://www.kdnuggets.com/2023/06/advanced-feature-selection-techniques-machine-learning-models.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/06/advanced-feature-selection-techniques-machine-learning-models.html](https://www.kdnuggets.com/2023/06/advanced-feature-selection-techniques-machine-learning-models.html)
- en: '![Advanced Feature Selection Techniques for Machine Learning Models](../Images/9e2e5df65e4ca529fa348b159fe56a8a.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习模型的高级特征选择技术](../Images/9e2e5df65e4ca529fa348b159fe56a8a.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Machine learning is undeniably the shining star of the new era. It forms the
    backbone of various major technologies that have become integral to our daily
    lives, such as facial recognition (supported by Convolutional Neural Networks
    or CNN), speech recognition (leveraging CNN and Recurrent Neural Networks or RNN),
    and the increasingly popular chatbots like ChatGPT (powered by Reinforcement Learning
    from Human Feedback, RLHF).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习无疑是新时代的明星。它构成了各种主要技术的基础，这些技术已经成为我们日常生活的不可或缺的一部分，如面部识别（由卷积神经网络或CNN支持）、语音识别（利用CNN和递归神经网络或RNN）以及日益流行的聊天机器人，如ChatGPT（由人类反馈强化学习，RLHF驱动）。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织在IT方面'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Numerous methods are available today to enhance the performance of a machine
    learning model. These methods can give your project a competitive edge by delivering
    superior performance.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有许多方法可以提升机器学习模型的性能。这些方法能通过提供卓越的表现为你的项目带来竞争优势。
- en: 'In this discussion, we''ll delve into the realm of feature selection techniques.
    But before we proceed, let''s clarify: what exactly is feature selection?'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次讨论中，我们将深入探讨特征选择技术。但在继续之前，让我们澄清一下：什么是特征选择？
- en: What Is the Feature Selection?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是特征选择？
- en: Feature selection is the process of choosing the best features for your model.
    This process might differ from one technique to another, but the main goal is
    to find out which features have more impact on your model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择是选择对你的模型最有利的特征的过程。这一过程可能因技术而异，但主要目标是找出对你的模型影响最大的特征。
- en: Why Should We Do Feature Selection?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们为什么要进行特征选择？
- en: Because sometimes, having too many features might harm your machine learning
    model. How?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因为有时候，特征过多可能会对你的机器学习模型产生负面影响。怎么回事呢？
- en: There might be too many different reasons. For example, these features might
    be related to each other, which can cause multicollinearity, ruining your model’s
    performance.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有很多不同的原因。例如，这些特征可能相互关联，导致多重共线性，从而破坏模型的性能。
- en: Another potential issue is related to computational power. The presence of too
    many features necessitates more computational power to execute the task concurrently,
    which could require more resources and, consequently, increased costs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个潜在问题与计算能力有关。特征过多需要更多的计算能力来同时执行任务，这可能需要更多资源，因此增加成本。
- en: Certainly, there may be other reasons as well. But these examples should give
    you a general idea of the potential problems. However, there's one more important
    aspect to understand before we delve further into this topic.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，也可能还有其他原因。但这些例子应该能给你一个大致的了解。不过，在我们进一步探讨这个话题之前，还有一个重要的方面需要理解。
- en: Which Feature Selection Method Will Be Better for My Model?
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 哪种特征选择方法对我的模型更好？
- en: Yes, that is a great question and should be answered before beginning the project.
    But it’s not easy to give a generic answer.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这是个很好的问题，应该在开始项目之前得到回答。但很难给出一个通用的答案。
- en: The choice of feature selection model relies on the type of data you have and
    the aim of your project.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择模型的选择依赖于你拥有的数据类型和项目的目标。
- en: For example, filter-based methods such as the chi-squared test or mutual information
    gain are typically used for feature selection in categorical data. The wrapper-based
    methods like forward or backward selection are suitable for numerical data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，像卡方检验或互信息增益这样的过滤方法通常用于分类数据的特征选择。像前向选择或后向选择这样的包裹方法适用于数值数据。
- en: Yet, it's good to know that many feature selection methods can handle both categorical
    and numerical data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，值得了解的是，许多特征选择方法可以处理分类数据和数值数据。
- en: For example, lasso regression, decision trees, and random forest can handle
    both types of data quite well.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，lasso回归、决策树和随机森林都可以很好地处理这两种数据类型。
- en: In terms of supervised and unsupervised feature selection, supervised methods
    like recursive feature elimination or decision trees are good for labeled data.
    Unsupervised methods like principal component analysis (PCA) or independent component
    analysis (ICA) are used for unlabeled data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 就监督特征选择和无监督特征选择而言，监督方法如递归特征消除或决策树适用于有标签的数据。无监督方法如主成分分析（PCA）或独立成分分析（ICA）用于无标签的数据。
- en: Ultimately, the choice of feature selection method should be based on the specific
    characteristics of your data and the goals of your project.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，特征选择方法的选择应基于数据的具体特征和项目的目标。
- en: Take a look at the overview of the topics we’ll discuss in the article. Make
    yourself familiar with it, and let’s start with the supervised feature selection
    techniques.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 查看一下我们将在文章中讨论的主题概述。熟悉它，然后让我们开始讨论监督特征选择技术。
- en: '![Advanced Feature Selection Techniques for Machine Learning Models](../Images/04ff4fa49074eb8300bc01daa19f5d4e.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习模型的高级特征选择技术](../Images/04ff4fa49074eb8300bc01daa19f5d4e.png)'
- en: Image by Author
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 1\. Supervised Feature Selection Techniques
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. 监督特征选择技术
- en: Feature selection strategies in supervised learning aim to discover the most
    relevant features for predicting the target variable by using the relationship
    between the input features and the target variable. These strategies might help
    improve model performance, reduce overfitting, and lower the computational cost
    of training the model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习中的特征选择策略旨在通过利用输入特征与目标变量之间的关系来发现最相关的特征。这些策略可能有助于提高模型性能，减少过拟合，并降低模型训练的计算成本。
- en: Here’s the overview of the supervised feature selection techniques we’ll talk
    about.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将讨论的监督特征选择技术的概述。
- en: '![Advanced Feature Selection Techniques for Machine Learning Models](../Images/efb1f95ca4f5c7a5adaca9d28957a827.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习模型的高级特征选择技术](../Images/efb1f95ca4f5c7a5adaca9d28957a827.png)'
- en: Image by Author
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 1.1 Filter-Based Approach
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 基于过滤的方法
- en: Filter-based feature selection approaches are based on data intrinsic attributes
    such as feature correlation or statistics. These approaches assess the value of
    each characteristic alone or in pairs without taking into account the performance
    of a particular learning algorithm.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 基于过滤的方法依赖于数据的固有属性，如特征相关性或统计数据。这些方法评估每个特征单独或成对的价值，而不考虑特定学习算法的表现。
- en: Filter-based approaches are computationally efficient and may be used with a
    variety of learning algorithms. However, because they do not account for the interaction
    between the features and the learning method, they may not always capture the
    ideal feature subset for a certain algorithm.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 基于过滤的方法计算效率高，可以与多种学习算法配合使用。然而，由于它们没有考虑特征与学习方法之间的交互，它们可能无法总是捕捉到特定算法的理想特征子集。
- en: Take a look at the overview of the filter-based approaches, and then we’ll discuss
    each.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 查看基于过滤的方法的概述，然后我们将逐一讨论每种方法。
- en: '![Advanced Feature Selection Techniques for Machine Learning Models](../Images/f133b65f927ab6d188b8614c5522492a.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习模型的高级特征选择技术](../Images/f133b65f927ab6d188b8614c5522492a.png)'
- en: Image by Author
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Information Gain
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信息增益
- en: Information Gain is a statistic that measures the reduction in entropy (uncertainty)
    for a specific feature by dividing the data according to that characteristic.
    It is often used in decision tree algorithms and also has useful features. The
    higher a feature's information gain, the more useful it is for decision-making.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益是一种统计量，通过根据特定特征对数据进行划分来测量熵（不确定性）的减少。它常用于决策树算法，并且具有有用的特征。特征的信息增益越高，它在决策中越有用。
- en: Now, let’s apply information gain by using a prebuilt diabetes dataset.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过使用预构建的糖尿病数据集来应用信息增益。
- en: The diabetes dataset contains physiological features related to predicting the
    progression of diabetes.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 糖尿病数据集包含与预测糖尿病进展相关的生理特征。
- en: 'age: Age in years'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'age: 年龄（岁）'
- en: 'sex: Gender (1 = male, 0 = female)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'sex: 性别（1 = 男性，0 = 女性）'
- en: 'BMI: Body mass index, calculated as weight in kilograms divided by the square
    of height in meters'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'BMI: 体质指数，计算方法为体重（千克）除以身高（米）的平方'
- en: 'bp: Average blood pressure (mm Hg)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'bp: 平均血压（mm Hg）'
- en: 's1, s2, s3, s4, s5, s6: Blood serum measurements of six different blood chemicals
    (including glucose),'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 's1、s2、s3、s4、s5、s6: 六种不同血液化学物质（包括葡萄糖）的血清测量'
- en: The following code demonstrates how to apply the Information Gain method. This
    code uses the diabetes dataset from the sklearn library as an example.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了如何应用信息增益方法。此代码使用来自sklearn库的糖尿病数据集作为示例。
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The primary objective of this code is to calculate feature importance scores
    based on Information Gain, which helps identify the most relevant features for
    the predictive model. By determining these scores, you can make informed decisions
    about which features to include or exclude from your analysis, ultimately leading
    to improved model performance, reduced overfitting, and faster training times.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码的主要目标是基于信息增益计算特征重要性分数，这有助于确定对预测模型最相关的特征。通过确定这些分数，你可以对分析中应包含或排除哪些特征做出明智的决策，从而提高模型性能，减少过拟合，并加快训练时间。
- en: To achieve this, this code calculates Information Gain scores for each feature
    in the dataset and stores them in a dictionary.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，这段代码计算了数据集中每个特征的信息增益分数，并将其存储在字典中。
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The features are then sorted in descending order according to their scores.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后根据它们的分数将特征按降序排序。
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We will visualize the sorted feature importance scores as a horizontal bar chart,
    allowing you to easily compare the relevance of different features for the given
    task.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把排序后的特征重要性分数可视化为水平条形图，以便你能够轻松比较不同特征在给定任务中的相关性。
- en: This visualization is particularly helpful when deciding which features to retain
    or discard while building a machine-learning model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这种可视化在决定在构建机器学习模型时保留或丢弃哪些特征时特别有用。
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let’s see the whole code.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看完整的代码。
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here is the output.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果。
- en: '![Advanced Feature Selection Techniques for Machine Learning Models](../Images/9893b636aa166abd62834f6936aa83f0.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习模型的高级特征选择技术](../Images/9893b636aa166abd62834f6936aa83f0.png)'
- en: The output shows the feature importance scores calculated using the Information
    Gain method for each feature in the diabetes dataset. The features are sorted
    in descending order based on their scores, which indicate their relative importance
    in predicting the target variable.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果显示了使用信息增益方法计算的每个特征的重要性分数。特征按照分数的降序排列，这些分数表示它们在预测目标变量中的相对重要性。
- en: 'The results are as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: Body mass index (bmi) has the highest importance score (0.174), indicating that
    it has the most significant influence on the target variable in the diabetes dataset.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 体质指数（bmi）具有最高的重要性分数（0.174），表明它对糖尿病数据集中目标变量的影响最大。
- en: Serum measurement 5 (s5) follows with a score of 0.153, making it the second
    most important feature.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 血清测量 5（s5）以0.153的分数紧随其后，是第二重要的特征。
- en: Serum measurement 6 (s6), serum measurement 4 (s4), and blood pressure (bp)
    have moderate importance scores, ranging from 0.104 to 0.065.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 血清测量 6（s6）、血清测量 4（s4）和血压（bp）具有中等的重要性分数，范围从0.104到0.065。
- en: The remaining features, such as serum measurements 1, 2, and 3 (s1, s2, s3),
    sex, and age, have relatively lower importance scores, indicating that they contribute
    less to the predictive power of the model.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其余特征，如血清测量 1、2 和 3（s1、s2、s3）、性别和年龄的重要性分数相对较低，表明它们对模型的预测能力贡献较小。
- en: By analyzing these feature importance scores, you can decide which features
    to include or exclude from your analysis to improve the performance of your machine
    learning model. In this case, you might consider retaining features with higher
    importance scores, such as bmi and s5, while potentially removing or further investigating
    features with lower scores, such as age and s2.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析这些特征重要性得分，你可以决定哪些特征应该包含在分析中，哪些特征应排除，以提高机器学习模型的性能。在这种情况下，你可能考虑保留重要性得分较高的特征，如bmi和s5，同时可能去除或进一步调查得分较低的特征，如age和s2。
- en: Chi-square Test
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卡方检验
- en: The Chi-square test is a statistical test used to assess the relationship between
    two categorical variables. It is used in feature selection to analyze the relationship
    between a categorical feature and the target variable. A greater Chi-square score
    shows a stronger link between the feature and the target, showing that the feature
    is more important for the classification job.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 卡方检验是一种统计检验，用于评估两个分类变量之间的关系。它在特征选择中用于分析分类特征与目标变量之间的关系。较大的卡方得分表明特征与目标之间的关联更强，显示该特征在分类任务中更为重要。
- en: While the Chi-square test is a commonly used feature selection method, it is
    typically used for categorical data, where the features and target variables are
    discrete.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管卡方检验是一种常用的特征选择方法，但它通常用于分类数据，其中特征和目标变量是离散的。
- en: Fisher’s Score
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 费舍尔得分
- en: Fisher's Discriminant Ratio, commonly known as Fisher's Score, is a feature
    selection approach that ranks features based on their ability to differentiate
    various classes in a dataset. It may be used for continuous features in a classification
    problem.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 费舍尔判别比率，通常称为费舍尔得分，是一种特征选择方法，根据特征区分数据集中不同类别的能力来对特征进行排名。它可以用于分类问题中的连续特征。
- en: Fisher's Score is calculated as the ratio of between-class and within-class
    variance. A higher Fisher's Score implies the characteristic is more discriminative
    and valuable for classification.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 费舍尔得分的计算为类别间方差与类别内方差的比率。较高的费舍尔得分意味着特征更具区分性，对分类更有价值。
- en: To use Fisher's Score for feature selection, compute a score for each continuous
    feature and rank them according to their scores. The model considers features
    with a higher Fisher's Score more important.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用费舍尔得分进行特征选择，计算每个连续特征的得分并根据得分对其进行排名。模型认为费舍尔得分较高的特征更重要。
- en: Missing Value Ratio
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺失值比率
- en: The Missing Value Ratio is a straightforward feature selection method that makes
    decisions based on the number of missing values in a feature.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失值比率是一种简单的特征选择方法，它根据特征中缺失值的数量做出决策。
- en: Features having a significant proportion of missing values may be uninformative
    and may harm the model's performance. You can filter out features with too many
    missing values by specifying a threshold for the acceptable missing value ratio.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失值比例较高的特征可能无信息量，并可能影响模型的性能。通过设置接受的缺失值比率阈值，可以过滤掉缺失值过多的特征。
- en: 'To use the Missing Value Ratio for feature selection, follow these steps:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用缺失值比率进行特征选择，请按照以下步骤操作：
- en: Calculate the missing value ratio for each feature by dividing the number of
    missing values by the total number of instances in the dataset.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将缺失值的数量除以数据集中实例的总数来计算每个特征的缺失值比率。
- en: Set a threshold for the acceptable missing value ratio (e.g., 0.8, meaning that
    a feature should have at most 80% of its values missing to be considered).
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个可接受的缺失值比率阈值（例如，0.8，意味着一个特征最多允许80%的值缺失才被考虑）。
- en: Filter out features that have a missing value ratio above the threshold.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过滤掉缺失值比率高于阈值的特征。
- en: 1.2 Wrapper-Based Approach
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 基于包装的方法
- en: Wrapper-based feature selection approaches include assessing the importance
    of features using a specific machine learning algorithm. They seek the best subset
    of features by experimenting with various feature combinations and evaluating
    their performance with the selected method.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 基于包装的特征选择方法包括使用特定的机器学习算法评估特征的重要性。它们通过尝试各种特征组合并使用所选方法评估其性能来寻找最佳特征子集。
- en: Because of the huge amount of available feature subsets, wrapper-based approaches
    can be computationally costly, especially when working with high-dimensional datasets.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可用特征子集的数量庞大，基于包装的特征选择方法可能计算成本高，尤其是在处理高维数据集时。
- en: However, they often outperform filter-based approaches because they consider
    the relationship between features and the learning algorithm.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它们通常比基于过滤的方法表现更好，因为它们考虑了特征与学习算法之间的关系。
- en: '![Advanced Feature Selection Techniques for Machine Learning Models](../Images/54d7b724520384c074a01426c8c7e18c.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习模型的高级特征选择技术](../Images/54d7b724520384c074a01426c8c7e18c.png)'
- en: Image by Author
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Forward Selection
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前向选择
- en: In forward selection, you start with an empty feature set and iteratively add
    features to the set. At each step, you evaluate the model's performance with the
    current feature set and the additional feature. The feature that results in the
    best performance improvement is added to the set.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向选择中，你从一个空的特征集开始，并逐步添加特征。在每一步中，你评估当前特征集和新增特征的模型性能。那些带来最佳性能提升的特征将被添加到特征集中。
- en: The process continues until no significant improvement in performance is observed,
    or a predefined number of features is reached.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程持续进行，直到观察到性能没有显著提升，或达到预定义的特征数量为止。
- en: The following code demonstrates the application of forward selection, a wrapper-based
    supervised feature selection technique.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了前向选择的应用，这是一种基于包装的监督特征选择技术。
- en: The example uses the breast cancer dataset from the sklearn library. The Breast
    Cancer dataset, also known as the Wisconsin Diagnostic Breast Cancer (WDBC) dataset,
    is a commonly used pre-built dataset for classification. And here, the main objective
    is building predictive models for diagnosing breast cancer as either malignant
    (cancerous) or benign (non-cancerous).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 示例使用了来自sklearn库的乳腺癌数据集。乳腺癌数据集，也被称为威斯康星诊断乳腺癌（WDBC）数据集，是一个常用的预构建分类数据集。在这里，主要目标是构建用于诊断乳腺癌为恶性（癌性）或良性（非癌性）的预测模型。
- en: For the sake of our model, we will select a different number of features to
    see how performance changes accordingly, but first, let’s load the libraries,
    dataset, and variables.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了我们模型的需要，我们将选择不同数量的特征以观察性能的变化，但首先，让我们加载库、数据集和变量。
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The objective of the code is to identify an optimal subset of features for a
    logistic regression model using forward selection. This technique starts with
    an empty set of features and iteratively adds the features that improve the model's
    performance based on a specified evaluation metric. In this case, the metric used
    is accuracy.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的目标是通过前向选择识别出适用于逻辑回归模型的最佳特征子集。这种技术从一个空的特征集开始，迭代地添加那些基于指定评估指标提高模型性能的特征。在这种情况下，使用的评估指标是准确性。
- en: The next part of the code employs the SequentialFeatureSelector from the mlxtend
    library to perform forward selection. It is configured with a logistic regression
    model, the desired number of features, and 5-fold cross-validation. The forward
    selection object is fitted to the training data, and the selected features are
    printed.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的下一部分使用mlxtend库中的SequentialFeatureSelector来执行前向选择。它配置了一个逻辑回归模型、所需的特征数量和5折交叉验证。前向选择对象被拟合到训练数据中，所选特征将被打印出来。
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Additionally, we need to evaluate the performance of the selected features on
    the testing set and visualize the model's performance with different feature subsets
    in a line chart.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要评估所选特征在测试集上的表现，并通过折线图可视化模型在不同特征子集下的表现。
- en: The chart will show the cross-validated accuracy as a function of the number
    of features, providing insights into the trade-off between model complexity and
    predictive performance.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图表将展示交叉验证的准确性作为特征数量的函数，提供有关模型复杂性与预测性能之间权衡的见解。
- en: By analyzing the output and the chart, you can determine the optimal number
    of features to include in your model, ultimately improving its performance and
    reducing overfitting.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析输出和图表，你可以确定在模型中包含的最佳特征数量，从而最终提高其性能并减少过拟合。
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here is the whole code.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这是完整的代码。
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Advanced Feature Selection Techniques for Machine Learning Models](../Images/f041ce47529262e6db88c8f9b5897813.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习模型的高级特征选择技术](../Images/f041ce47529262e6db88c8f9b5897813.png)'
- en: 'The output of the forward selection code demonstrates that the algorithm has
    identified a subset of 5 features that yield the best accuracy (0.9548) for the
    logistic regression model on the breast cancer dataset. These selected features
    are identified by their indices: 0, 1, 4, 21, and 22.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 前向选择代码的输出结果显示，该算法识别了5个特征的子集，这些特征在乳腺癌数据集上的逻辑回归模型中产生了最佳准确率（0.9548）。这些选定的特征通过其索引进行标识：0、1、4、21和22。
- en: 'The line graph provides additional insights into the performance of the model
    with different numbers of features. It shows that:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 线形图提供了有关不同特征数量下模型性能的额外见解。它显示：
- en: With just 1 feature, the model achieves an accuracy of around 91%.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用1个特征，模型的准确率约为91%。
- en: Adding a second feature increases the accuracy to 94%.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加第二个特征将准确率提高至94%。
- en: With 3 features, the accuracy further improves to 95%.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用3个特征，准确率进一步提高至95%。
- en: Including 4 features pushes the accuracy slightly above 95%.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含4个特征使准确率稍微超过95%。
- en: Beyond 4 features, the improvements in accuracy become less significant. This
    information can help you make informed decisions about the trade-offs between
    model complexity and predictive performance. Based on these results, you might
    decide to use only 3 or 4 features in your model to balance accuracy and simplicity.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 超过4个特征后，准确率的提高变得不那么显著。这些信息可以帮助你做出关于模型复杂性和预测性能之间权衡的明智决定。基于这些结果，你可能决定在模型中仅使用3或4个特征，以平衡准确性和简洁性。
- en: Backward Selection
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 后向选择
- en: The opposite of forward selection is backward selection. You begin with the
    entire feature set and gradually eliminate features from it.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 前向选择的对立方法是后向选择。你从整个特征集开始，并逐渐消除其中的特征。
- en: At each phase, you measure the model's performance with the current feature
    set minus the feature to be deleted.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个阶段，你需要测量当前特征集减去要删除的特征后的模型性能。
- en: The feature that causes the least amount of performance reduction gets eliminated
    from the set.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 造成性能下降最小的特征被从特征集中删除。
- en: The procedure is repeated until there is no substantial increase in performance
    or a preset number of features is reached.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程重复进行，直到性能没有实质性提升或达到预设的特征数量。
- en: Backward and forward selections are categorized as a sequential feature selection;
    you can learn more [here](https://scikit-learn.org/stable/modules/feature_selection.html#recursive-feature-elimination).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 后向选择和前向选择被归类为顺序特征选择；你可以在[这里](https://scikit-learn.org/stable/modules/feature_selection.html#recursive-feature-elimination)了解更多信息。
- en: Exhaustive Feature Selection
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 穷尽特征选择
- en: Exhaustive feature selection compares the performance of all possible feature
    subsets and chooses the best-performing subset. This approach is computationally
    demanding, especially for large datasets, yet it ensures the best feature subset.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 穷尽特征选择比较所有可能的特征子集的性能，并选择表现最好的子集。这种方法计算量大，特别是对于大数据集，但确保了最佳的特征子集。
- en: Recursive Feature Elimination
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 递归特征消除
- en: Recursive feature elimination starts with the whole feature set and eliminates
    features repeatedly depending on their relevance as judged by the learning algorithm.
    The least important feature is removed at each step, and the model is retrained.
    The method is repeated until a predetermined number of features are achieved.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 递归特征消除从整个特征集开始，并根据学习算法判断的相关性反复消除特征。在每一步，最不重要的特征被移除，模型被重新训练。该方法重复进行，直到达到预定数量的特征。
- en: 1.3 Embedded Approach
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 嵌入式方法
- en: Embedded feature selection approaches include the feature selection process
    as part of the learning algorithm.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入式特征选择方法将特征选择过程作为学习算法的一部分。
- en: This implies that throughout the training phase, the learning algorithm not
    only optimizes the model parameters but also picks the most important characteristics.
    Embedded methods can be more effective than wrapper methods since they do not
    require an external feature selection procedure.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在训练阶段，学习算法不仅优化模型参数，还选择最重要的特征。嵌入式方法比包装方法更有效，因为它们不需要外部特征选择过程。
- en: '![Advanced Feature Selection Techniques for Machine Learning Models](../Images/6e3b5d2f996af040b51870b82eee9b73.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习模型的高级特征选择技术](../Images/6e3b5d2f996af040b51870b82eee9b73.png)'
- en: Image by Author
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: Regularization
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正则化
- en: Regularization is a method that adds a penalty term to the loss function to
    prevent overfitting in machine learning models.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是一种向损失函数添加惩罚项的方法，以防止机器学习模型中的过拟合。
- en: Regularization methods, such as lasso (L1 regularization) and ridge (L2 regularization),
    can be used in conjunction with feature selection to decrease the coefficients
    of less significant features towards zero, thereby picking a subset of the most
    relevant features.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化方法，如lasso（L1正则化）和ridge（L2正则化），可以与特征选择结合使用，以减少不重要特征的系数接近零，从而选择出最相关特征的子集。
- en: Random Forest Importance
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林重要性
- en: Random forest is an ensemble learning approach that combines the predictions
    of several decision trees. Random forest computes a feature significance score
    for each feature as part of the tree-building process, which may be used to order
    features based on their relevance. The model considers features with higher significance
    ratings to be more significant.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一种集成学习方法，它结合了多个决策树的预测。随机森林在构建树的过程中计算每个特征的重要性评分，这些评分可以用来根据特征的相关性进行排序。模型将具有更高重要性评分的特征视为更重要。
- en: If you want to learn more about the random forest, here is the article “[Decision
    Tree and Random Forest Algorithm](https://www.stratascratch.com/blog/decision-tree-and-random-forest-algorithm-explained/?utm_source=blog&utm_medium=click&utm_campaign=kdn+ml+feature+selection)”,
    which also explains the decision tree algorithm too.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于随机森林的信息，下面的文章“[决策树和随机森林算法](https://www.stratascratch.com/blog/decision-tree-and-random-forest-algorithm-explained/?utm_source=blog&utm_medium=click&utm_campaign=kdn+ml+feature+selection)”也解释了决策树算法。
- en: The following example uses the Covertype dataset, which includes information
    about different types of forest cover.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例使用了Covertype数据集，该数据集包含有关不同类型森林覆盖的信息。
- en: The aim of the Covertype dataset is to predict the forest cover type (the dominant
    tree species) within the Roosevelt National Forest of Northern Colorado.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Covertype数据集的目标是预测**罗斯福国家森林**中森林覆盖类型（主导树种）。
- en: The primary goal of the below code is to determine the importance of features
    using a random forest classifier. By evaluating the contribution of each feature
    to the overall classification performance, this method helps identify the most
    relevant features for building a predictive model.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 下面代码的主要目标是使用随机森林分类器来确定特征的重要性。通过评估每个特征对整体分类性能的贡献，这种方法有助于识别构建预测模型的最相关特征。
- en: '[PRE9]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Then, we create a RandomForestClassifier object and fit it to the training data.
    It then extracts the feature importances from the trained model and sorts them
    in descending order. The top 10 features are selected based on their importance
    scores and displayed in a ranking.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个`RandomForestClassifier`对象，并将其拟合到训练数据上。接着，它从训练好的模型中提取特征重要性，并按降序排序。前10个特征根据其重要性评分被选择并显示在排名中。
- en: '[PRE10]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Additionally, the code visualizes the top 10 feature importances using a horizontal
    bar chart.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，代码通过水平条形图可视化了前10个特征的重要性。
- en: '[PRE11]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This visualization allows for easy comparison of the importance scores and aids
    in making informed decisions about which features to include or exclude from your
    analysis.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 该可视化允许轻松比较重要性评分，并有助于在决定包含或排除哪些特征时做出明智的选择。
- en: By examining the output and the chart, you can select the most relevant features
    for your predictive model, which can help improve its performance, reduce overfitting,
    and accelerate training times.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查输出和图表，你可以选择最相关的特征用于你的预测模型，这有助于提高模型性能，减少过拟合，并加快训练时间。
- en: Here is the whole code.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这是完整代码。
- en: '[PRE12]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here is the output.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果。
- en: '![Advanced Feature Selection Techniques for Machine Learning Models](../Images/a504e222d0cfdf0d9df4c963e3516f85.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![高级特征选择技术用于机器学习模型](../Images/a504e222d0cfdf0d9df4c963e3516f85.png)'
- en: The output of the Random Forest Importance method displays the top 10 features
    ranked by their importance in predicting the forest cover type in the Covertype
    dataset.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林重要性方法的输出显示了根据其在预测森林覆盖类型中的重要性排名的前10个特征。
- en: It reveals that Elevation has the highest importance score (0.2423) among all
    features in predicting the forest cover type. This suggests that elevation plays
    a critical role in determining the dominant tree species in the Roosevelt National
    Forest.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 它揭示了**海拔**在所有特征中具有最高的重要性评分（0.2423），在预测森林覆盖类型中作用最大。这表明海拔在确定**罗斯福国家森林**中的主导树种方面起着关键作用。
- en: Other features with relatively high importance scores include Horizontal_Distance_To_Roadways
    (0.1158) and Horizontal_Distance_To_Fire_Points (0.1100). These indicate that
    proximity to roadways and fire ignition points also significantly impacts forest
    cover types.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 其他具有较高重要性分数的特征包括Horizontal_Distance_To_Roadways（0.1158）和Horizontal_Distance_To_Fire_Points（0.1100）。这些特征表明，靠近道路和火点也显著影响森林覆盖类型。
- en: The remaining features in the top 10 list have relatively lower importance scores,
    but they still contribute to the overall predictive performance of the model.
    These features mainly relate to hydrological factors, slope, aspect, and hillshade
    indices.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 排名前10的特征中，其余特征的重要性分数相对较低，但它们仍然对模型的整体预测性能有所贡献。这些特征主要与水文因素、坡度、方位和山阴影指数相关。
- en: In summary, the results highlight the most important factors affecting the distribution
    of forest cover types in the Roosevelt National Forest, which can be used to build
    a more effective and efficient predictive model for forest cover type classification.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，结果突出了影响罗斯福国家森林区森林覆盖类型分布的最重要因素，这些因素可以用于构建更有效、更高效的森林覆盖类型分类预测模型。
- en: 2\. Unsupervised Feature Selection Techniques
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 无监督特征选择技术
- en: When there is no target variable available, unsupervised feature selection approaches
    can be used in order to reduce the dimensionality of the dataset while keeping
    its underlying structure. These methods often include changing the initial feature
    space into a new lower-dimensional space in which the changed features capture
    the majority of the variation in the data.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 当没有目标变量可用时，可以使用无监督特征选择方法来降低数据集的维度，同时保持其基本结构。这些方法通常包括将初始特征空间转换为一个新的低维空间，其中变化后的特征捕捉数据中的大部分变异。
- en: '![Advanced Feature Selection Techniques for Machine Learning Models](../Images/c2aa4559a90aa654b22281f044305fe7.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习模型的高级特征选择技术](../Images/c2aa4559a90aa654b22281f044305fe7.png)'
- en: Image by Author
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 2.1 Principal Component Analysis (PCA)
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 主成分分析（PCA）
- en: PCA is a linear dimensionality reduction method that converts the original feature
    space into a new orthogonal space defined by principal components. These components
    are linear combinations of the original features chosen to capture the highest
    level of variance in the data.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一种线性降维方法，将原始特征空间转换为由主成分定义的新正交空间。这些组件是原始特征的线性组合，旨在捕捉数据中的最高方差。
- en: PCA may be used to pick the top k principal components representing most of
    the variation, thus lowering the dataset's dimensionality.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: PCA可用于选择代表大部分变异的前k个主成分，从而降低数据集的维度。
- en: To show you how this works in practice, we will work with the Wine dataset.
    This is a widely used dataset for classification and feature selection tasks in
    machine learning and consists of 178 samples, each representing a different wine
    originating from three different cultivars in the same region in Italy.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了向您展示这如何在实践中运作，我们将使用葡萄酒数据集。这是一个广泛用于分类和特征选择任务的机器学习数据集，包含178个样本，每个样本代表来自意大利同一地区三种不同品种的不同葡萄酒。
- en: The goal of working with the Wine dataset is usually to build a predictive model
    that can accurately classify a wine sample into one of the three cultivars based
    on its chemical properties.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 使用葡萄酒数据集的目标通常是构建一个预测模型，该模型可以根据化学属性将葡萄酒样本准确地分类为三种品种之一。
- en: The following code demonstrates the application of Principal Component Analysis
    (PCA), an unsupervised feature selection technique, on the Wine dataset.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了无监督特征选择技术主成分分析（PCA）在葡萄酒数据集上的应用。
- en: These components(principal components) capture the most variance in the data
    while minimizing the information loss.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件（主成分）捕捉数据中最多的方差，同时最小化信息损失。
- en: The code starts by loading the Wine dataset, which consists of 13 features describing
    the chemical properties of different wine samples.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 代码首先加载葡萄酒数据集，该数据集包含描述不同葡萄酒样本化学性质的13个特征。
- en: '[PRE13]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: These features are then standardized using the StandardScaler to ensure that
    PCA is not affected by varying scales of the input features.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用StandardScaler对这些特征进行标准化，以确保PCA不会受到输入特征尺度变化的影响。
- en: '[PRE14]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Next, PCA is performed on the standardized data using the PCA class from the
    sklearn.decomposition module.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用sklearn.decomposition模块中的PCA类对标准化数据进行PCA。
- en: '[PRE15]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The explained variance ratio for each principal component is calculated, indicating
    the proportion of the total variance in the data that each component explains.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 每个主成分的解释方差比例都被计算出来，表示每个主成分解释的数据总方差的比例。
- en: '[PRE16]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Finally, two plots are generated to visualize the explained variance ratio and
    the cumulative explained variance by the principal components.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，生成两个图来可视化主成分的解释方差比例和累计解释方差。
- en: The first plot shows the explained variance ratio for each individual principal
    component, while the second plot illustrates how the cumulative explained variance
    increases as more principal components are included.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个图展示了每个单独主成分的解释方差比例，而第二个图则说明了随着更多主成分的加入，累计解释方差是如何增加的。
- en: These plots help determine the optimal number of principal components to use
    in the model, balancing the trade-off between dimensionality reduction and information
    retention.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图帮助确定模型中使用的主成分的最佳数量，在维度减少和信息保留之间取得平衡。
- en: '[PRE17]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Let’s see the whole code.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看完整的代码。
- en: '[PRE18]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here is the output.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果。
- en: '![Advanced Feature Selection Techniques for Machine Learning Models](../Images/3de8d47d77c9c73424bdba6654384dda.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习模型的高级特征选择技术](../Images/3de8d47d77c9c73424bdba6654384dda.png)'
- en: The graph on the left shows that the explained variance ratio decreases as the
    number of principal components increases. This is a typical behavior observed
    in PCA because principal components are ordered by the amount of variance they
    explain.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧图表显示解释方差比例随着主成分数量的增加而减少。这是PCA中观察到的典型行为，因为主成分是按解释方差的多少排序的。
- en: The first principal component (feature) captures the highest variance, the second
    principal component captures the second highest amount, and so on. As a result,
    the explained variance ratio decreases with each subsequent principal component.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个主成分（特征）捕捉到最高的方差，第二个主成分捕捉到第二高的方差，依此类推。因此，解释方差比例随着每个后续主成分的增加而减少。
- en: This is one of the main reasons PCA is used for dimensionality reduction.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这是PCA用于维度减少的主要原因之一。
- en: The second graph on the right shows the cumulative explained variance and helps
    you determine how many principal components (features) to select to represent
    the percentage of your data. The x-axis represents the number of principal components,
    and the y-axis shows the cumulative explained variance. As you move along the
    x-axis, you can see how much of the total variance is retained when you include
    that many principal components.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的第二张图展示了累计解释方差，并帮助你确定选择多少主成分（特征）来表示数据的百分比。x轴表示主成分的数量，y轴显示累计解释方差。当你沿x轴移动时，可以看到在包含这么多主成分时保留了多少总方差。
- en: In this example, you can see that selecting around 3 or 4 principal components
    already captures more than 80% of the total variance, and around 8 principal components
    capture over 90% of the total variance.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，你可以看到选择大约3或4个主成分已经捕捉了超过80%的总方差，而大约8个主成分捕捉了超过90%的总方差。
- en: You can choose the number of principal components based on your desired trade-off
    between dimensionality reduction and the variance you want to retain.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以根据希望在维度减少和保留方差之间的权衡选择主成分的数量。
- en: In this example, we did use Sci-kit to learn to apply PCA, and here you can
    find the official document.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们确实使用了Sci-kit来学习应用PCA，官方文档可以在这里找到。
- en: 2.2 Independent Component Analysis (ICA)
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 独立成分分析（ICA）
- en: ICA is a method for dividing a multidimensional signal into its components.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ICA是一种将多维信号分解为其成分的方法。
- en: In the context of feature selection, ICA can be used to convert the original
    feature space into a new space characterized by statistically independent components.
    You may decrease the dimensionality of the dataset while keeping the underlying
    structure by picking the top k independent components.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征选择的背景下，ICA可以用来将原始特征空间转换为一个由统计独立成分组成的新空间。通过选择前k个独立成分，你可以在保持底层结构的同时减少数据集的维度。
- en: 2.3 Non-Negative Matrix Factorization (NMF)
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 非负矩阵分解（NMF）
- en: The non-negative matrix factor (NMF) is a dimensionality reduction approach
    that approximates a non-negative data matrix as the product of two lower-dimensional
    non-negative matrices.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 非负矩阵分解（NMF）是一种维度减少方法，通过将一个非负数据矩阵近似为两个低维非负矩阵的乘积来实现。
- en: NMF can be used in the context of feature selection to extract a new set of
    basic features that capture the important structure of the original data. You
    may minimize the dimensionality of the dataset while maintaining the non-negativity
    limitation by picking the top k basis features.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: NMF 可用于特征选择的背景下，提取一组新的基本特征，以捕捉原始数据的重要结构。通过选择前 k 个基础特征，你可以在保持非负性限制的同时最小化数据集的维度。
- en: 2.4 t-distributed Stochastic Neighbor Embedding (t-SNE)
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 t-分布随机邻域嵌入（t-SNE）
- en: t-SNE is a nonlinear dimensionality reduction method that tries to preserve
    the dataset's structure by reducing the difference between pairwise probability
    distributions in high and low-dimensional locations.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE 是一种非线性降维方法，它试图通过减少高维和低维位置之间的配对概率分布差异来保持数据集的结构。
- en: t-SNE may be applied in feature selection to project the original feature space
    into a lower-dimensional space that maintains the structure of the data, allowing
    enhanced visualization and evaluation.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE 可以应用于特征选择，将原始特征空间投影到一个低维空间中，从而保持数据的结构，允许更好的可视化和评估。
- en: You can find more information about both unsupervised algorithms and t-SNE here
    “[Unsupervised Learning Algorithms](https://www.stratascratch.com/blog/overview-of-machine-learning-algorithms-unsupervised-learning/?utm_source=blog&utm_medium=click&utm_campaign=kdn+ml+feature+selection)”.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到更多关于无监督算法和 t-SNE 的信息 “[无监督学习算法](https://www.stratascratch.com/blog/overview-of-machine-learning-algorithms-unsupervised-learning/?utm_source=blog&utm_medium=click&utm_campaign=kdn+ml+feature+selection)”。
- en: 2.5 Autoencoder
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 自编码器
- en: An autoencoder, a kind of artificial neural network, learns to encode input
    data into a lower-dimensional representation and then decode it back to the original
    version. The autoencoder's lower-dimensional representation can be used to produce
    another set of features that capture the underlying structure of the original
    data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一种人工神经网络，它学习将输入数据编码成低维表示，然后再将其解码回原始版本。自编码器的低维表示可以用来生成另一组特征，以捕捉原始数据的潜在结构。
- en: Final Words
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语
- en: In conclusion, feature selection is vital in machine learning. It helps reduce
    the data's dimensionality, minimize the risk of overfitting, and improve the model's
    overall performance. Choosing the right feature selection method depends on the
    specific problem, dataset, and modeling requirements.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，特征选择在机器学习中至关重要。它有助于减少数据的维度，最小化过拟合的风险，并提高模型的整体性能。选择合适的特征选择方法取决于具体的问题、数据集和建模要求。
- en: This article covered a wide range of feature selection techniques, including
    supervised and unsupervised methods.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 本文涵盖了广泛的特征选择技术，包括监督和无监督方法。
- en: Supervised techniques, such as filter-based, wrapper-based, and embedded approaches,
    use the relationship between features and the target variable to identify the
    most important features.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 监督技术，如基于滤波器、基于包装和嵌入的方法，利用特征与目标变量之间的关系来识别最重要的特征。
- en: Unsupervised techniques, like PCA, ICA, NMF, t-SNE, and autoencoders, focus
    on the intrinsic structure of the data to reduce dimensionality without considering
    the target variable.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督技术，如 PCA、ICA、NMF、t-SNE 和自编码器，专注于数据的内在结构，以在不考虑目标变量的情况下降低维度。
- en: When selecting the appropriate feature selection method for your model, it's
    vital to consider the characteristics of your data, the underlying assumptions
    of each technique, and the computational complexity involved.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在为你的模型选择合适的特征选择方法时，考虑数据的特征、每种技术的基本假设和涉及的计算复杂性是至关重要的。
- en: By carefully selecting and applying the right feature selection technique, you
    can significantly enhance the performance, leading to better insights and decision-making.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仔细选择和应用正确的特征选择技术，你可以显著提升性能，从而获得更好的洞察力和决策能力。
- en: '**[Nate Rosidi](https://www.stratascratch.com)** is a data scientist and in
    product strategy. He''s also an adjunct professor teaching analytics, and is the
    founder of [StrataScratch](https://www.stratascratch.com/), a platform helping
    data scientists prepare for their interviews with real interview questions from
    top companies. Connect with him on [Twitter: StrataScratch](https://twitter.com/StrataScratch)
    or [LinkedIn](https://www.linkedin.com/in/nathanrosidi/).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**[内特·罗西迪](https://www.stratascratch.com)** 是一名数据科学家，专注于产品策略。他还是一位副教授，教授分析学，并且是
    [StrataScratch](https://www.stratascratch.com/) 的创始人，这个平台帮助数据科学家准备面试，提供来自顶级公司的真实面试问题。可以在
    [Twitter: StrataScratch](https://twitter.com/StrataScratch) 或 [LinkedIn](https://www.linkedin.com/in/nathanrosidi/)
    上联系他。'
- en: More On This Topic
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解更多主题
- en: '[Alternative Feature Selection Methods in Machine Learning](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中的替代特征选择方法](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
- en: '[Feature Selection: Where Science Meets Art](https://www.kdnuggets.com/2021/12/feature-selection-science-meets-art.html)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[特征选择：科学与艺术的结合](https://www.kdnuggets.com/2021/12/feature-selection-science-meets-art.html)'
- en: '[Feature Store Summit 2022: A free conference on Feature Engineering](https://www.kdnuggets.com/2022/10/hopsworks-feature-store-summit-2022-free-conference-feature-engineering.html)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[特征商店峰会 2022：关于特征工程的免费会议](https://www.kdnuggets.com/2022/10/hopsworks-feature-store-summit-2022-free-conference-feature-engineering.html)'
- en: '[Top 5 Free Resources for Learning Advanced SQL Techniques](https://www.kdnuggets.com/top-5-free-resources-for-learning-advanced-sql-techniques)'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习高级 SQL 技巧的 5 个免费资源](https://www.kdnuggets.com/top-5-free-resources-for-learning-advanced-sql-techniques)'
- en: '[10 Advanced Git Techniques](https://www.kdnuggets.com/10-advanced-git-techniques)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10 种高级 Git 技巧](https://www.kdnuggets.com/10-advanced-git-techniques)'
- en: '[3 Research-Driven Advanced Prompting Techniques for LLM Efficiency…](https://www.kdnuggets.com/3-research-driven-advanced-prompting-techniques-for-llm-efficiency-and-speed-optimization)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3 种研究驱动的高级提示技术以提高 LLM 效率…](https://www.kdnuggets.com/3-research-driven-advanced-prompting-techniques-for-llm-efficiency-and-speed-optimization)'
