# 回到基础，第二部分：梯度下降

> 原文：[`www.kdnuggets.com/2023/03/back-basics-part-dos-gradient-descent.html`](https://www.kdnuggets.com/2023/03/back-basics-part-dos-gradient-descent.html)

欢迎来到我们***回到基础***系列的第二部分。在 [第一部分](https://medium.com/towards-data-science/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46)中，我们介绍了如何使用线性回归和成本函数来找到适合我们房价数据的最佳拟合线。然而，我们也发现测试多个*截距*值可能是繁琐且低效的。在这一部分中，我们将深入探讨梯度下降，这是一种强大的技术，可以帮助我们找到完美的*截距*并优化我们的模型。我们将探讨其背后的数学原理，并了解如何将其应用于我们的线性回归问题。

梯度下降是一种强大的优化算法，***旨在快速而高效地找到曲线的最小点。***最好的可视化方式是想象你站在山顶，山谷中等着你的是一个装满黄金的宝箱。

* * *

## 我们的前三大课程推荐

![](img/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](img/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](img/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你组织的 IT 需求

* * *

![回到基础，第二部分：线性回归、成本函数和梯度下降](img/5d19f0944f791eb0a9d6f16b1facfe4f.png)

然而，山谷的确切位置未知，因为外面非常黑暗，你什么也看不见。此外，你希望在其他人之前到达山谷（因为你想独占所有的宝藏）。梯度下降帮助你在地形中导航，并***高效而快速***地到达这个*最优*点。在每个点，它会告诉你该走多少步以及需要朝哪个方向走。

同样，梯度下降可以通过使用算法中规定的步骤应用于我们的线性回归问题。为了可视化寻找最小值的过程，让我们绘制**MSE**曲线。我们已经知道曲线的方程是：

![回到基础，第二部分：线性回归、成本函数和梯度下降](img/737e52874ea32b77d4d6154e51555832.png)

曲线的方程是用来计算 MSE 的方程

从[上一篇文章](https://towardsdatascience.com/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46#e9d3)中，我们知道我们问题中的**MSE**方程是：

![回到基础，第二部分：线性回归、成本函数和梯度下降](img/b5eb66fcef8e423ecf363808f778b8c6.png)

如果我们放大，可以看到通过在上述方程中代入大量的*截距*值，可以找到一条类似我们山谷的**MSE**曲线。所以我们可以代入 10,000 个*截距*值，以获得如下曲线：

![回到基础，第二部分：线性回归、成本函数和梯度下降](img/6829eca33ad9d3c887892fd49d136e6b.png)

实际上，我们无法知道 MSE 曲线的具体样子

目标是达到这个**MSE**曲线的底部，我们可以通过以下步骤实现：

## 步骤 1：从截距值的随机初始猜测开始

在这种情况下，我们假设对*截距*值的初始猜测是 0。

## 步骤 2：计算此点的 MSE 曲线的梯度

曲线在某一点的*梯度*由该点的切线（即线仅在该点触及曲线的另一种说法）表示。例如，在点 A，**MSE** 曲线的*梯度*可以用红色切线表示，当截距等于 0 时。

![回到基础，第二部分：线性回归、成本函数和梯度下降](img/c1d5e0542e80349a3afd325459352d65.png)

截距 = 0 时 MSE 曲线的梯度

为了确定*梯度*的值，我们应用微积分知识。具体来说，*梯度*等于在给定点上曲线相对于*截距*的导数。这表示为：

![回到基础，第二部分：线性回归、成本函数和梯度下降](img/d5d5a9ed724645ca6957ec078416236e.png)

> 注意：如果你对导数不熟悉，我推荐观看这个[Khan Academy 视频](https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-1-new/ab-2-6a/v/derivative-properties-and-polynomial-derivatives)以供参考。否则，你可以略过下一部分，仍然能跟上文章的其余部分。

我们计算***MSE 曲线的导数***如下：

![回到基础，第二部分：线性回归、成本函数和梯度下降](img/776088318fa18a2c2d17d391277c7dfe.png)

现在，为了找到***点 A 的梯度***，我们将点 A 处的*截距*值代入上面的方程中。由于*截距* = 0，点 A 处的导数为：

![回到基础，第二部分：线性回归、成本函数和梯度下降](img/67db4fea0b447240477d8ff003773e9f.png)

所以当*截距* = 0 时，*梯度* = -190

> **注意：** 当我们接近最优值时，梯度值接近于零。在最优值处，梯度等于零。相反，离最优值越远，梯度就越大。

![回到基础，第二部分：线性回归，成本函数和梯度下降](img/31ba9dc3a9f7fd31faeca6e279b1fae6.png)

从中我们可以推断，步长应该与*梯度*有关，因为它告诉我们是采取小步还是大步。这意味着当曲线的*梯度*接近 0 时，我们应该采取小步，因为我们接近最优值。而当*梯度*较大时，我们应该采取更大的步伐，以更快地达到最优值。

> **注意：** 然而，如果我们采取一个非常大的步骤，可能会跳过最优点。因此我们需要小心。

![回到基础，第二部分：线性回归，成本函数和梯度下降](img/83feba1c3c30a9ae017ea84520ec0d4a.png)

## 步骤 3：使用梯度和学习率计算步长并更新截距值

由于我们看到***步长***和*梯度*是成比例的，因此*步长*由将*梯度*乘以一个预定的常数值——即***学习率***——来确定。

![回到基础，第二部分：线性回归，成本函数和梯度下降](img/dd61f74bee009d27cdc05eebc65dbbc6.png)

*学习率*控制*步长*的大小，并确保所采取的步骤既不过大也不过小。

> 实际上，学习率通常是一个小的正数，大约是 0.001。但对于我们的问题，让我们将其设置为 0.1。

因此，当截距为 0 时，*步长 = 梯度 * x * 学习率* = -190*0.1 = -19*。

根据我们上面计算的*步长*，我们使用以下等效公式中的任何一个来更新*截距*（即改变我们当前位置）：

![回到基础，第二部分：线性回归，成本函数和梯度下降](img/c2d6b9191a2cc8c8a311fdfe637632ef.png)

要在此步骤中找到新的*截距*，我们代入相关值…

![回到基础，第二部分：线性回归，成本函数和梯度下降](img/bfdea6e65612c731551a91113de67952.png)

…并且发现新的*截距* = 19。

现在将这个值代入**均方误差**（**MSE**）方程中，我们发现当*截距*为 19 时，**MSE**为 8064.095。我们注意到，通过一步大的调整，我们更接近了最优值，并减少了**MSE**。

![回到基础，第二部分：线性回归，成本函数和梯度下降](img/bd224aa9af161fda1ef3a1fea6d57077.png)

即使我们查看图表，也可以看到新的*截距*为 19 的直线比旧的*截距*为 0 的直线更好地拟合了我们的数据：

![回到基础，第二部分：线性回归，成本函数和梯度下降](img/dac783aa1dc1c8852610d966da3fe552.png)

## 步骤 4：重复步骤 2–3

我们使用更新后的*截距*值重复步骤 2 和 3。

例如，由于此迭代中新*截距*值为 19，根据[步骤 2](https://medium.com/towards-data-science/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd#6ab4)，我们将计算这个新点的梯度：

![回到基础，第 Dos 部分：线性回归，成本函数和梯度下降](img/d62bfd120ad7554a8260635b17c84b4d.png)

我们发现**MSE**曲线在截距值 19 处的*梯度*是 -152（如下面插图中的红色切线所示）。

![回到基础，第 Dos 部分：线性回归，成本函数和梯度下降](img/a9c9701640ea3e7d14fa5b8db8c323eb.png)

接下来，根据[步骤 3](https://medium.com/towards-data-science/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd#67b0)，我们来计算*步长*：

![回到基础，第 Dos 部分：线性回归，成本函数和梯度下降](img/a4603c5f6fb177ed11418ac5814bd0e4.png)

随后，更新*截距*值：

![回到基础，第 Dos 部分：线性回归，成本函数和梯度下降](img/8715781f8c423fa69ffd05f4488d00f7.png)

现在我们可以将具有 19 的旧*截距*的线与具有新截距 34.2 的新线进行比较…

![回到基础，第 Dos 部分：线性回归，成本函数和梯度下降](img/f4bfe4b7beb6b83bdbdfaf11f59901b2.png)

…我们可以看到新线更好地拟合了数据。

总体而言，**MSE**正在变小…

![回到基础，第 Dos 部分：线性回归，成本函数和梯度下降](img/f618bd0080bb66d326a5826a7d63eed6.png)

…我们的*步长*正在变小：

![回到基础，第 Dos 部分：线性回归，成本函数和梯度下降](img/15c541dcac99271f58f96df770e10797.png)

我们重复这个过程，直到我们收敛到最优解：

![回到基础，第 Dos 部分：线性回归，成本函数和梯度下降](img/2b5b643559ce6e7ac8f7d06d8e7a1293.png)

随着我们接近曲线的最小点，我们观察到*步长*变得越来越小。经过 13 步，梯度下降算法估计*截距*值为 95。如果我们有一只水晶球，这将被确认是**MSE**曲线的最小点。显然，这种方法相比于我们在[上一篇文章](https://medium.com/towards-data-science/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46)中看到的蛮力方法更加高效。

现在我们拥有了*截距*的最优值，线性回归模型是：

![回到基础，第 Dos 部分：线性回归，成本函数和梯度下降](img/61244bb741c82f91f42c98bb02a639c0.png)

线性回归线看起来是这样的：

![回到基础，第二部分：线性回归、成本函数和梯度下降](img/5527a5bf8aa4e535e29e6a878f42e46a.png)

最佳拟合线的截距为 95，斜率为 0.069。

最后，回到我们朋友马克的问题——他应该以多少价格出售他的 2400 平方英尺的房子？

![回到基础，第二部分：线性回归、成本函数和梯度下降](img/ba1f1c05b426df495c7bae00bed79768.png)

将房屋面积 2400 平方英尺代入上述方程…

![回到基础，第二部分：线性回归、成本函数和梯度下降](img/5bd96e8a9a7ebe978819d16426e395f0.png)

…然后就完成了。我们可以告诉我们那位过度担忧的朋友马克，基于他邻里的 3 栋房子，他应该将房子售价定在大约$260,600。

现在我们对这些概念有了充分的理解，让我们快速进行问答环节，解答任何悬而未决的问题。

## 为什么找到梯度实际上有效？

为了说明这一点，考虑一个场景，我们试图达到曲线 C 的最小点*x**。我们当前位于*x*左侧的点 A：

![回到基础，第二部分：线性回归、成本函数和梯度下降](img/c445f16302e50d42ce0b4d94f6bcfa6e.png)

如果我们对点 A 处的曲线相对于*x*求导，得到的值为负（这意味着*梯度*向下倾斜）。我们还观察到，我们需要向右移动才能到达*x**。因此，我们需要增加*x*以到达最小值*x*。

![回到基础，第二部分：线性回归、成本函数和梯度下降](img/0e4d00a3fc662b0d31e8163a471d6378.png)

红线，即梯度，向下倾斜 => 负梯度

由于*dC(x)/dx*是负的，*x-??*dC(x)/dx*将大于*x*，因此向*x*方向移动。

类似地，如果我们在最小点 x*右侧的点 A，那么我们得到一个**正的*梯度***（*梯度*向上倾斜），*dC(x)/dx*。

![回到基础，第二部分：线性回归、成本函数和梯度下降](img/c6c8d53b45f3bcfa2f039351007c88e5.png)

红线，即梯度，向上倾斜 => 正梯度

所以*x-??*dC(x)/dx*将小于*x*，因此向*x*方向移动。

## 梯度下降如何知道何时停止？

梯度下降在*步长（Step Size）*非常接近 0 时停止。正如之前讨论的那样，在最小值点，*梯度*为 0，当我们接近最小值时，*梯度*也接近 0。因此，当某一点的*梯度*接近 0 或接近最小值点时，*步长*也会接近 0，这表明算法已经达到了最优解。

![回到基础，第二部分：线性回归、成本函数和梯度下降](img/b351ad5b83c9a126f86ca31bd7c0d72a.png)

当我们接近最小值点时，梯度接近 0，随之步长（Step Size）也接近 0。

> 实际中，最小步长 = 0.001 或更小

![基础知识回顾，第二部分：线性回归、成本函数和梯度下降](img/76186f8d24893a15731b57b79e5db9af.png)

也就是说，梯度下降还包括一个在放弃之前的步数限制，称为*最大步数*。

> 实际中，最大步数 = 1000 或更多

即使*步长*大于*最小步长*，如果已经超过了*最大步数*，梯度下降仍然会停止。

## 如果最小点更难识别会怎样？

到目前为止，我们一直在处理易于识别最小点的曲线（这些曲线被称为***凸性***）。但如果我们有一条不那么漂亮的曲线（技术上称为***非凸性***）并且看起来像这样：

![基础知识回顾，第二部分：线性回归、成本函数和梯度下降](img/95362b61547e16eaa6527163af4d0472.png)

在这里，我们可以看到点 B 是*全局最小值*（实际最小值），而点 A 和 C 是*局部最小值*（可能被误认为是*全局最小值*但实际上不是）。因此，如果一个函数有多个*局部最小值*和一个*全局最小值*，并不能保证梯度下降会找到*全局最小值*。此外，找到哪个局部最小值将取决于初始猜测的位置（如在[第 1 步](https://towardsdatascience.com/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd#87cf)的梯度下降中所示）。

![基础知识回顾，第二部分：线性回归、成本函数和梯度下降](img/c935384e980297c511af9d0eeaa79146.png)

以上面的非凸曲线为例，如果初始猜测在区块 A 或区块 C，梯度下降将分别宣称最小点在局部最小值 A 或 C，而实际上在 B。只有当初始猜测在区块 B 时，算法才能找到全局最小值 B。

**现在的问题是——我们如何做出一个好的初始猜测？**

*简单的回答：*试错法。可以算是。

*复杂的回答：*从上图可以看出，如果我们对*x*的最小猜测为 0，因为它位于区块 A，将会导致局部最小值 A。因此，如你所见，0 在大多数情况下可能不是一个好的初始猜测。一个常见的做法是基于 x 所有可能值范围的均匀分布应用一个随机函数。此外，如果可行，可以运行算法使用不同的初始猜测并比较它们的结果，这可以提供有关猜测是否存在显著差异的见解。这有助于更高效地识别全局最小值。

好的，我们快到了。最后一个问题。

## 如果我们尝试找到多个最优值会怎样？

到目前为止，我们只关注于找到最佳的截距值，因为我们神奇地知道线性回归的*slope*值为 0.069。但如果没有水晶球，不知道最佳的*slope*值该怎么办？这时我们需要同时优化斜率和截距值，分别表示为*x?* 和 *x?*。

为此，我们必须利用偏导数，而不仅仅是导数。

> 注意：偏导数的计算方式与普通导数相同，但由于我们需要优化多个变量，因此表示方式不同。要了解更多，请阅读这篇[文章](https://www.mathsisfun.com/calculus/derivatives-partial.html)或观看这个[视频](https://www.youtube.com/watch?v=JAf_aSIJryg)。

然而，这个过程仍然与优化单一值的过程相似。成本函数（如 **MSE**）仍需定义，并且必须应用梯度下降算法，但需要额外的一步，即找到 x? 和 x? 的偏导数。

**步骤 1：对 x₀ 和 x₁ 进行初始猜测**

**步骤 2：在这些点上找到相对于 x₀ 和 x₁ 的偏导数**

![回归基础，第二部分：线性回归、成本函数和梯度下降](img/a7339e26b9ad52d4263bc524b7b94e3f.png)

**步骤 3：根据偏导数和学习率同时更新 x₀ 和 x₁**

![回归基础，第二部分：线性回归、成本函数和梯度下降](img/a2f81be7d310c4a5c9cfd8b68e863f78.png)

**步骤 4：重复步骤 2-3，直到达到最大步骤数或步骤大小小于最小步骤大小**

*我们可以将这些步骤推广到 3、4 甚至 100 个值进行优化。*

总结来说，梯度下降是一种强大的优化算法，能有效帮助我们达到最佳值。梯度下降算法可以应用于许多其他优化问题，使其成为数据科学家工具箱中的基础工具。现在进入更大更好的算法吧！

**[Shreya Rao](https://www.linkedin.com/in/shreyarao24/)** 用通俗易懂的语言讲解和说明机器学习算法。

[原文](https://towardsdatascience.com/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd)。已获得许可转载。

### 更多相关话题

+   [关于梯度下降和成本函数的 5 个概念](https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html)

+   [梯度下降：山地徒步者的优化数学指南](https://www.kdnuggets.com/gradient-descent-the-mountain-trekker-guide-to-optimization-with-mathematics)

+   [回归基础第 1 周：Python 编程与数据科学基础](https://www.kdnuggets.com/back-to-basics-week-1-python-programming-data-science-foundations)

+   [回归基础第 3 周：机器学习简介](https://www.kdnuggets.com/back-to-basics-week-3-introduction-to-machine-learning)

+   [回到基础第 4 周：高级主题与部署](https://www.kdnuggets.com/back-to-basics-week-4-advanced-topics-and-deployment)

+   [回到基础附加周：云部署](https://www.kdnuggets.com/back-to-basics-bonus-week-deploying-to-the-cloud)
