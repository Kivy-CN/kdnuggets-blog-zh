- en: Concepts You Should Know Before Getting Into Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在深入了解Transformer之前应该了解的概念
- en: 原文：[https://www.kdnuggets.com/2023/01/concepts-know-getting-transformer.html](https://www.kdnuggets.com/2023/01/concepts-know-getting-transformer.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/01/concepts-know-getting-transformer.html](https://www.kdnuggets.com/2023/01/concepts-know-getting-transformer.html)
- en: 1. Input Embedding
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. 输入嵌入
- en: Neural networks learn through numbers, so each word will be mapped to vectors
    to represent a particular word. The embedding layer can be thought of as a lookup
    table that stores word embeddings and retrieves them using indices.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通过数字进行学习，因此每个单词都会被映射到向量中以表示特定的单词。嵌入层可以被认为是一个查找表，用于存储单词嵌入并通过索引检索它们。
- en: '![Concepts You should know before Getting into Transformer](../Images/67feca653a9f1a4d43414738c0e6b6f2.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![在深入了解Transformer之前应该了解的概念](../Images/67feca653a9f1a4d43414738c0e6b6f2.png)'
- en: Words that have the same meaning will be close in terms of euclidian distance/cosine
    similarity. for example, in the below word representation, “Saturday”,” Sunday”,
    and” Monday” is associated with the same concept, so we can see that the words
    are resulting similar.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 具有相同含义的单词在欧氏距离/余弦相似度方面会很接近。例如，在下面的单词表示中，“Saturday”，“Sunday”和“Monday”与相同的概念相关，因此我们可以看到这些单词的结果是相似的。
- en: '![Concepts You should know before Getting into Transformer](../Images/201bac3fee572f0382e3a1605ae44365.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![在深入了解Transformer之前应该了解的概念](../Images/201bac3fee572f0382e3a1605ae44365.png)'
- en: 2. Positional Encoding
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 位置编码
- en: The determining the position of the word, Why do we need to determine the position
    of word? because, the transformer encoder has no recurrence like recurrent neural
    networks,we must add some information about the positions into the input embeddings.
    This is done using positional encoding. The authors of the paper used the following
    functions to model the position of a word.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 确定单词的位置的原因，为什么我们需要确定单词的位置？因为，transformer编码器没有像递归神经网络那样的递归，我们必须在输入嵌入中添加一些关于位置的信息。这是通过位置编码来完成的。论文的作者使用了以下函数来建模单词的位置。
- en: '![Concepts You should know before Getting into Transformer](../Images/e86a3b77649ce1de4a74d3a9e149b254.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![在深入了解Transformer之前应该了解的概念](../Images/e86a3b77649ce1de4a74d3a9e149b254.png)'
- en: We will try to explain positional Encoding.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试解释位置编码。
- en: '![Concepts You should know before Getting into Transformer](../Images/8c335ba900729d8d71f07c0f03a72a52.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![在深入了解Transformer之前应该了解的概念](../Images/8c335ba900729d8d71f07c0f03a72a52.png)'
- en: Here “pos” refers to the position of the “word” in the sequence. P0 refers to
    the position embedding of the first word; “d” means the size of the word/token
    embedding. In this example d=5\. Finally, “i” refers to each of the 5 individual
    dimensions of the embedding (i.e. 0, 1,2,3,4)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这里“pos”指的是序列中“单词”的位置。P0 指的是第一个单词的位置嵌入；“d”表示单词/标记嵌入的大小。在这个例子中，d=5。最后，“i”指的是嵌入的5个单独维度（即0、1、2、3、4）。
- en: if “i” vary in the equation above, you will get a bunch of curves with varying
    frequencies. Reading off the position embedding values against different frequencies,
    giving different values at different embedding dimensions for P0 and P4.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果上面的方程中的“i”发生变化，你将得到一系列频率不同的曲线。通过读取不同频率下的位置嵌入值，会在不同的嵌入维度中为P0和P4提供不同的值。
- en: 3. Scaled Dot-Product Attention
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 缩放点积注意力
- en: '![Concepts You should know before Getting into Transformer](../Images/092ad7293c501dd51b03be1655769cda.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![在深入了解Transformer之前应该了解的概念](../Images/092ad7293c501dd51b03be1655769cda.png)'
- en: In this **query, Q** represents a vector word, the **keys K** are all other
    words in the sentence, and **value V** represents the vector of the word.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个**查询 Q**中，表示一个向量单词，**键 K**是句子中的所有其他单词，而**值 V**表示单词的向量。
- en: The purpose of attention is to calculate the importance of the key term compared
    to the query term related to the same person/thing or concept.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力的目的是计算关键术语与查询术语相比的重要性，关键术语与相同的人/事物或概念相关。
- en: In our case, V is equal to Q.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，V 等于 Q。
- en: '**The attention mechanism gives us the importance of the word in a sentence.**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意力机制给出了句子中单词的重要性。**'
- en: '![Concepts You should know before Getting into Transformer](../Images/11e009ff8e36ce45e26988feaa571c1f.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![在深入了解Transformer之前应该了解的概念](../Images/11e009ff8e36ce45e26988feaa571c1f.png)'
- en: When we compute the normalized dot product between the query and the keys, we
    get a tensor that represents the relative importance of each other word for the
    query.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们计算查询和键之间的归一化点积时，我们得到一个张量，表示每个其他单词对查询的相对重要性。
- en: '![Concepts You should know before Getting into Transformer](../Images/6d70bf90709da9833ad5a3f4f45e9709.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![在进入Transformer之前你应该了解的概念](../Images/6d70bf90709da9833ad5a3f4f45e9709.png)'
- en: When computing the dot product between Q and K.T, we try to estimate how the
    vectors (i.e words between query and keys) are aligned and return a weight for
    each word in the sentence.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当计算Q和K.T之间的点积时，我们尝试估计向量（即查询和键之间的词）是如何对齐的，并为句子中的每个词返回一个权重。
- en: Then, we normalize the result squared of d_k and The softmax function regularizes
    the terms and rescales them between 0 and 1.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们对d_k的平方结果进行归一化，softmax函数对项进行正则化并将其重新缩放到0和1之间。
- en: Finally, we multiply the result( i.e weights) by the value (i.e all words) to
    reduce the importance of non-relevant words and focus only on the most important
    words.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将结果（即权重）乘以值（即所有词汇），以减少不相关词汇的重要性，并仅关注最重要的词汇。
- en: 4. Residual Connections
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. 残差连接
- en: The multi-headed attention output vector is added to the original positional
    input embedding. This is called a residual connection/skip connection. The output
    of the residual connection goes through layer normalization. The normalized residual
    output is passed through a pointwise feed-forward network for further processing.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力输出向量被添加到原始位置输入嵌入中。这被称为残差连接/跳跃连接。残差连接的输出经过层归一化。归一化后的残差输出经过逐点前馈网络进行进一步处理。
- en: '![Concepts You should know before Getting into Transformer](../Images/e270c87fda8f765ea81be84c3e8ea92a.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![在进入Transformer之前你应该了解的概念](../Images/e270c87fda8f765ea81be84c3e8ea92a.png)'
- en: 5. Mask
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5. 掩码
- en: The mask is a matrix that’s the same size as the attention scores filled with
    values of 0’s and negative infinities.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码是一个与注意力分数大小相同的矩阵，填充了0和负无穷值。
- en: '![Concepts You should know before Getting into Transformer](../Images/7d8fa80c3bb2bdc1dd4e0a5fadb3cff1.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![在进入Transformer之前你应该了解的概念](../Images/7d8fa80c3bb2bdc1dd4e0a5fadb3cff1.png)'
- en: The reason for the mask is that once you take the softmax of the masked scores,
    the negative infinities get zero, leaving zero attention scores for future tokens.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用掩码的原因是，一旦对掩码后的分数进行softmax操作，负无穷将变为零，从而使未来的词汇注意力分数为零。
- en: This tells the model to put no focus on those words.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉模型对那些词不加关注。
- en: 6. Softmax function
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6. softmax函数
- en: The purpose of the softmax function is to grab real numbers(positive and negative)
    and turn them into positive numbers which sum to 1.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: softmax函数的目的是将真实数（正数和负数）转换为总和为1的正数。
- en: '![Concepts You should know before Getting into Transformer](../Images/dd21079a15c752d55a2ef69f4a0e0b3c.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![在进入Transformer之前你应该了解的概念](../Images/dd21079a15c752d55a2ef69f4a0e0b3c.png)'
- en: '**[Ravikumar Naduvin](https://www.linkedin.com/in/ravikumar-mn/)** is busy
    in building and understanding NLP tasks using PyTorch.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Ravikumar Naduvin](https://www.linkedin.com/in/ravikumar-mn/)** 正忙于使用PyTorch构建和理解NLP任务。'
- en: '[Original](https://ravikumarmn.github.io/concepts-you-should-know-before-getting-into/).
    Reposted with permission.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://ravikumarmn.github.io/concepts-you-should-know-before-getting-into/)。经许可转载。'
- en: '* * *'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT工作'
- en: '* * *'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[5 Concepts You Should Know About Gradient Descent and Cost Function](https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于梯度下降和代价函数的5个概念](https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html)'
- en: '[7 SQL Concepts You Should Know For Data Science](https://www.kdnuggets.com/2022/11/7-sql-concepts-needed-data-science.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中你应该知道的7个SQL概念](https://www.kdnuggets.com/2022/11/7-sql-concepts-needed-data-science.html)'
- en: '[KDnuggets™ News 22:n03, Jan 19: A Deep Look Into 13 Data…](https://www.kdnuggets.com/2022/n03.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets™ 新闻 22:n03，1月19日：深入探讨13个数据…](https://www.kdnuggets.com/2022/n03.html)'
- en: '[What Google Recommends You do Before Taking Their Machine Learning…](https://www.kdnuggets.com/2021/10/google-recommends-before-machine-learning-data-science-course.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[谷歌建议你在参加他们的机器学习课程之前做什么…](https://www.kdnuggets.com/2021/10/google-recommends-before-machine-learning-data-science-course.html)'
- en: '[Read This Before You Take Any Free Data Science Course](https://www.kdnuggets.com/read-this-before-you-take-any-free-data-science-course)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在参加任何免费的数据科学课程之前请先阅读此文](https://www.kdnuggets.com/read-this-before-you-take-any-free-data-science-course)'
- en: '[KDnuggets News, April 13: Python Libraries Data Scientists Should…](https://www.kdnuggets.com/2022/n15.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，4月13日：数据科学家应关注的Python库…](https://www.kdnuggets.com/2022/n15.html)'
