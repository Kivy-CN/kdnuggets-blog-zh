- en: 'A Top Machine Learning Algorithm Explained: Support Vector Machines (SVM)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一种顶级机器学习算法解析：支持向量机（SVM）
- en: 原文：[https://www.kdnuggets.com/2020/03/machine-learning-algorithm-svm-explained.html](https://www.kdnuggets.com/2020/03/machine-learning-algorithm-svm-explained.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/03/machine-learning-algorithm-svm-explained.html](https://www.kdnuggets.com/2020/03/machine-learning-algorithm-svm-explained.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '![](../Images/91db1982754222467ae9d4c78622811c.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91db1982754222467ae9d4c78622811c.png)'
- en: One of the most prevailing and exciting supervised learning models with associated
    learning algorithms that analyse data and recognise patterns is Support Vector
    Machines (SVMs). It is used for solving both regression and classification problems.
    However, it is mostly used in solving classification problems. SVMs were first
    introduced by B.E. Boser et al. in 1992 and has become popular due to success
    in handwritten digit recognition in 1994\. Before the emergence of Boosting Algorithms,
    for example, XGBoost and AdaBoost, SVMs had been commonly used.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）是最流行且令人兴奋的监督学习模型之一，其关联的学习算法分析数据并识别模式。它用于解决回归和分类问题。然而，它主要用于解决分类问题。SVMs首次由B.E.
    Boser等人于1992年引入，并因1994年手写数字识别的成功而变得流行。在Boosting算法（如XGBoost和AdaBoost）出现之前，SVMs曾被广泛使用。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的IT工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: If you want to have a consolidated foundation of Machine Learning algorithms,
    you should definitely have it in your arsenal. The algorithm of SVMs is powerful,
    but the concepts behind are not as complicated as you think.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要一个全面的机器学习算法基础，你应该把它纳入你的工具箱。SVMs的算法非常强大，但其背后的概念并不像你想象的那么复杂。
- en: Problem with Logistic Regression
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑回归的问题
- en: In my previous article, I have explained clearly what Logistic Regression is
    ([link](https://towardsdatascience.com/from-linear-to-logistic-regression-explained-step-by-step-11d7f0a9c29)).
    It helps solve classification problems separating the instances into two classes.
    However, there is an infinite number of decision boundaries, and Logistic Regression
    only picks an arbitrary one.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前的文章中，我已经清楚地解释了逻辑回归是什么（[链接](https://towardsdatascience.com/from-linear-to-logistic-regression-explained-step-by-step-11d7f0a9c29)）。它帮助解决分类问题，将实例分为两类。然而，存在无数个决策边界，而逻辑回归只选择了一个任意的边界。
- en: '![](../Images/39279a875d0df753a4fba76f717f44dc.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39279a875d0df753a4fba76f717f44dc.png)'
- en: For point C, since it’s far away from the decision boundary, we are quite certain
    to classify it as 1 (green).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于点C，由于它远离决策边界，我们可以相当确定地将其分类为1（绿色）。
- en: For point A, even though we classify it as 1 for now, since it is pretty close
    to the decision boundary, if the boundary moves a little to the right, we would
    mark point A as “0” instead. Hence, we’re much more confident about our prediction
    at C than at A
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于点A，尽管我们现在将其分类为1，由于它非常接近决策边界，如果边界稍微向右移动，我们将把点A标记为“0”。因此，我们对点C的预测信心远高于对点A的预测。
- en: Logistic Regression doesn’t care whether the instances are close to the decision
    boundary. Therefore, the decision boundary it picks may not be optimal. As we
    can see from the above graph, if a point is far from the decision boundary, we
    may be more confident in our predictions. Therefore, the optimal decision boundary
    should be able to maximize the distance between the decision boundary and all
    instances. i.e., maximize the margins. That’s why the SVM algorithm is important!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归不关心实例是否接近决策边界。因此，它选择的决策边界可能不是最优的。从上面的图可以看出，如果一个点远离决策边界，我们对预测可能更有信心。因此，最优的决策边界应该能够最大化决策边界与所有实例之间的距离，即最大化边距。这就是为什么SVM算法很重要！
- en: What is Support Vector Machines (SVMs)?
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是支持向量机（SVM）？
- en: Given a set of training examples, each marked as belonging to one or the other
    of two categories, an SVM training algorithm builds a model that assignsnew examples
    to one category or the other, making it a non-probabilistic binary linear classifier.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组标记为属于两个类别之一的训练样本，SVM训练算法构建一个模型，将新的样本分配到一个类别或另一个类别，从而使其成为一个非概率性的二元线性分类器。
- en: The objective of applying SVMs is to find the best line in two dimensions or
    the best hyperplane in more than two dimensions in order to help us separate our
    space into classes. The hyperplane (line) is found through the **maximum margin,** i.e.,
    the maximum distance between data points of both classes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 应用支持向量机（SVM）的目标是找到二维中的最佳直线或多维中的最佳超平面，以帮助我们将空间划分为不同的类别。这个超平面（直线）是通过**最大边距**来确定的，即两个类别的数据点之间的最大距离。
- en: '*Don’t you think the definition and idea of SVM look a bit abstract? No worries,
    let me explain in details.*'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*你不觉得支持向量机（SVM）的定义和概念有些抽象吗？别担心，让我详细解释一下。*'
- en: '**Support Vector Machines**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机**'
- en: 'Imagine the labelled training set are two classes of data points (two dimensions):
    Alice and Cinderella. To separate the two classes, there are so many possible
    options of hyperplanes that separate correctly. As shown in the graph below, we
    can achieve exactly the same result using different hyperplanes (L1, L2, L3).
    However, if we add new data points, the consequence of using various hyperplanes
    will be very different in terms of classifying new data point into the right group
    of class.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 想象标记的训练集是两类数据点（二维）：爱丽丝和灰姑娘。为了分隔这两类，有很多可能的超平面可以正确地进行分隔。如下面的图所示，我们可以使用不同的超平面（L1、L2、L3）来获得完全相同的结果。然而，如果我们添加新的数据点，使用不同超平面的结果在将新数据点分类到正确类别时会有很大差异。
- en: '![](../Images/406bd07c65610e1a5eaa38508158192e.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/406bd07c65610e1a5eaa38508158192e.png)'
- en: '*Different hyperplanes (L1, L2, L3).*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*不同的超平面（L1、L2、L3）。*'
- en: How can we decide a separating line for the classes? Which hyperplane shall
    we use?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何为类别决定一个分隔直线？我们应该使用哪个超平面？
- en: '![](../Images/bfc565312d220ed098edf7569016af83.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfc565312d220ed098edf7569016af83.png)'
- en: '**Support Vector, Hyperplane, and Margin**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量、超平面和边距**'
- en: The vector points closest to the hyperplane are known as the **support vector
    points **because only these two points are contributing to the result of the algorithm,
    and other points are not. If a data point is not a support vector, removing it
    has no effect on the model. On the other hand, deleting the support vectors will
    then change the position of the hyperplane.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与超平面最接近的向量点被称为**支持向量点**，因为只有这两个点对算法的结果有贡献，其他点没有。如果一个数据点不是支持向量，那么删除它对模型没有影响。另一方面，删除支持向量会改变超平面的位置。
- en: The dimension of the hyperplane depends upon the number of features. If the
    number of input features is 2, then the hyperplane is just a line. If the number
    of input features is 3, then the hyperplane becomes a two-dimensional plane. It
    becomes difficult to imagine when the number of features exceeds 3.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 超平面的维度取决于特征的数量。如果输入特征的数量为2，则超平面只是一个直线。如果输入特征的数量为3，则超平面变为二维平面。当特征数量超过3时，就很难想象了。
- en: The distance of the vectors from the hyperplane is called the ***margin,***
    which is a separation of a line to the closest class points. We would like to
    choose a hyperplane that maximises the margin between classes. The graph below
    shows what good margin and bad margin are.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 向量与超平面的距离称为***边距***，它是直线与最近类别点的分隔。我们希望选择一个能最大化类别间边距的超平面。下面的图展示了什么是好的边距和坏的边距。
- en: '![](../Images/1c1a8111c00614ef6c8ba2c307c9137f.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c1a8111c00614ef6c8ba2c307c9137f.png)'
- en: '![](../Images/ed84e1285cc734e9851df80c831d7e3d.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed84e1285cc734e9851df80c831d7e3d.png)'
- en: '**Hard Margin**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**硬边际**'
- en: If the training data is linearly separable, we can select two parallel hyperplanes
    that separate the two classes of data, so that the distance between them is as
    large as possible.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练数据是线性可分的，我们可以选择两个平行的超平面来分开两个类别的数据，从而使它们之间的距离尽可能大。
- en: '**Soft Margin**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**软边际**'
- en: As most of the real-world data are not fully linearly separable, we will allow
    some margin violation to occur, which is called soft margin classification. It
    is better to have a large margin, even though some constraints are violated. Margin
    violation means choosing a hyperplane, which can allow some data points to stay
    in either the incorrect side of the hyperplane and between the margin and the
    correct side of the hyperplane.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大多数现实世界的数据并不是完全线性可分的，我们将允许一些边际违规发生，这称为软边际分类。即便一些约束被违反，拥有一个较大的边际更为优越。边际违规意味着选择一个超平面，这可以允许一些数据点位于超平面错误的一侧或在边际与超平面的正确侧之间。
- en: In order to find the **maximal margin**, we need to maximize the margin between
    the data points and the hyperplane. In the following session, I will share the
    mathematical concepts behind this algorithm.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到**最大边际**，我们需要最大化数据点与超平面之间的边际。在接下来的环节中，我将分享这个算法背后的数学概念。
- en: '![](../Images/df9fc3ca480269b642c12eeefb862ee7.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df9fc3ca480269b642c12eeefb862ee7.png)'
- en: Linear Algebra Revisited
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性代数回顾
- en: Before we move on, let’s review some concepts in Linear Algebra.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们复习一下线性代数中的一些概念。
- en: '![](../Images/b9f16893706b282b46ff91456d49f453.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9f16893706b282b46ff91456d49f453.png)'
- en: '![](../Images/831ada4b7d56ffed1f9254873c51efdc.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/831ada4b7d56ffed1f9254873c51efdc.png)'
- en: '**Maximising the Margin**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**最大化边际**'
- en: 'You probably learned that an equation of a line is** y=ax+b**. However, you
    will often find that the equation of a hyperplane is defined by:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能学过直线的方程是**y=ax+b**。然而，你会发现超平面的方程通常被定义为：
- en: '![](../Images/c36f0aebf4bc6549f652d47469e6fdc7.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c36f0aebf4bc6549f652d47469e6fdc7.png)'
- en: '*The two equations are just two different ways of expressing the same thing.*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*这两个方程只是用不同的方式表达相同的内容。*'
- en: For** Support Vector Classifier** (SVC), we use ????T????+???? where ???? is
    the weight vector, and ???? is the bias.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**支持向量分类器**（SVC），我们使用 ????T????+???? 其中 ???? 是权重向量，???? 是偏置。
- en: '![](../Images/a892e732e8b60e8dcb203f8e597164e3.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a892e732e8b60e8dcb203f8e597164e3.png)'
- en: You can see that the name of the variables in the hyperplane equation are **w **and **x,**
    whichmeans they are vectors! A vector has magnitude (size) and direction, which
    works perfectly well in 3 or more dimensions. Therefore, the application of “**vector”** is
    used in the SVMs algorithm.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到超平面方程中的变量名称是**w**和**x**，这意味着它们是向量！向量具有大小（尺寸）和方向，这在三维或更多维度中表现得非常好。因此，“**向量**”的应用被用于SVM算法中。
- en: '![](../Images/de8274f5698d8253d78493d27ea4c41d.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de8274f5698d8253d78493d27ea4c41d.png)'
- en: '![](../Images/896a1dbe6fdba5bf8d21f3282dc4eb8b.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/896a1dbe6fdba5bf8d21f3282dc4eb8b.png)'
- en: '*The equation of calculating the Margin.*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*计算边际的方程。*'
- en: '**Cost Function and Gradient Updates**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**代价函数和梯度更新**'
- en: Maximizing-Margin is equivalent to Minimizing Loss.
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 最大化边际等同于最小化损失。
- en: In the SVM algorithm, we are looking to maximize the **margin** between the
    data points and the hyperplane. The loss function that helps maximize the margin
    is **hinge loss**.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在SVM算法中，我们寻求最大化**边际**，即数据点与超平面之间的边际。帮助最大化边际的损失函数是**铰接损失**。
- en: '![](../Images/ef1c7a5848fbf5414ade5e0fda3d16f9.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef1c7a5848fbf5414ade5e0fda3d16f9.png)'
- en: '*λ=1/C (C is always used for regularization coefficient).*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*λ=1/C（C始终用于正则化系数）。*'
- en: The function of the first term, **hinge loss,** is topenalize misclassifications.
    It measures the error due to misclassification (or data points being closer to
    the classification boundary than the margin). The second term is the regularization
    term, which is a technique to avoid overfitting by penalizing large coefficients
    in the solution vector. The λ(lambda) is the regularization coefficient, and its
    major role is to determine the trade-off between increasing the margin size and
    ensuring that the xi lies on the correct side of the margin.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个术语**铰接损失**的函数是用来惩罚错误分类的。它衡量由于错误分类（或数据点离分类边界比边际更近）导致的错误。第二个术语是正则化项，它是一种通过惩罚解向量中的大系数来避免过拟合的技术。λ（lambda）是正则化系数，它的主要作用是确定增加边际大小和确保xi位于边际正确侧之间的权衡。
- en: “Hinge” describes the fact that the error is 0 if the data point is classified
    correctly (and is not too close to the decision boundary).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: “铰链”描述了错误为 0 的情况，如果数据点被正确分类（且不太靠近决策边界）。
- en: '![](../Images/56f845d28a04c8b2f2ec8b93a4180f56.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56f845d28a04c8b2f2ec8b93a4180f56.png)'
- en: When the true class is -1 (as in your example), the hinge loss looks like this
    in the graph.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当真实类别为 -1（如你的示例中）时，铰链损失在图中看起来是这样的。
- en: We need to minimise the above loss function to find the max-margin classifier.
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们需要最小化上述损失函数，以找到最大间隔分类器。
- en: 'We can derive the formula for the margin from the **hinge-loss**. If a data
    point is on the margin of the classifier, the hinge-loss is exactly zero. Hence,
    on the margin, we have:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从 **铰链损失** 中推导出间隔的公式。如果一个数据点在分类器的间隔上，铰链损失为零。因此，在间隔上，我们有：
- en: '![](../Images/500b5c42ac6cd4ac554ae76a1d06a6f2.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/500b5c42ac6cd4ac554ae76a1d06a6f2.png)'
- en: '*Note that ????i is either +1 or -1.*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意到 ????i 是 +1 或 -1。*'
- en: 'Therefore, we have:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有：
- en: '![](../Images/acd2e6cec0a25204058e9f0492113d35.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/acd2e6cec0a25204058e9f0492113d35.png)'
- en: 'Our objective function is then:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标函数是：
- en: '![](../Images/522973f17b5a9080a222c383f77193bb.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/522973f17b5a9080a222c383f77193bb.png)'
- en: '*To minimize such an objection function, we should then use Lagrange Multiplier.*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*为了最小化这种目标函数，我们应该使用拉格朗日乘子。*'
- en: '![](../Images/3b8afc784ae1860a54a4e562712078d8.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b8afc784ae1860a54a4e562712078d8.png)'
- en: '**Classifying non-linear data**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类非线性数据**'
- en: What about data points are not linearly separable?
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，对于数据点不是线性可分的情况呢？
- en: '![](../Images/0a7afb4b1e1e40981433a7c2cc60f601.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a7afb4b1e1e40981433a7c2cc60f601.png)'
- en: '*Non-linear separate.*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*非线性分离。*'
- en: 'SVM has a technique called the [**kernel**](https://en.wikipedia.org/wiki/Kernel_method)** trick**.
    These are functions that take low dimensional input space and transform it into
    a higher-dimensional space, i.e., it converts not separable problem to separable
    problem. It is mostly useful in non-linear separation problems. This is shown
    as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 有一个称为 [**核函数**](https://en.wikipedia.org/wiki/Kernel_method)** 技巧**。这些函数将低维输入空间转换为高维空间，即将不可分问题转换为可分问题。它主要用于非线性分离问题。如下所示：
- en: '**Mapping to a Higher Dimension**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**映射到更高维度**'
- en: '![](../Images/6d80782f04cdb36746d79e83200cba24.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d80782f04cdb36746d79e83200cba24.png)'
- en: '![](../Images/2f4be06f70d62e727f8003a9053e7e7b.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f4be06f70d62e727f8003a9053e7e7b.png)'
- en: '**Some Frequently Used Kernels**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**一些常用的核函数**'
- en: '![](../Images/df3ce05a5e3fd991a0ebb47c100c0da2.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df3ce05a5e3fd991a0ebb47c100c0da2.png)'
- en: Why SVMs
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么使用 SVM
- en: Solve the data points are not linearly separable
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解决数据点不是线性可分的情况
- en: Effective in a higher dimension.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在更高维度下有效。
- en: 'Suitable for small data set: effective when the number of features is more
    than training examples.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 适用于小数据集：当特征数量多于训练样本时有效。
- en: 'Overfitting problem: The hyperplane is affected by only the support vectors,
    so SVMs are not robust to the outliner.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过拟合问题：超平面仅受支持向量的影响，因此 SVM 对异常值不具鲁棒性。
- en: 'Summary: Now you should know'
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结：现在你应该知道
- en: Problem with Logistic Regression
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归的问题
- en: What is Support Vector, Hyperplane, and Margin
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是支持向量、超平面和间隔
- en: How to find the maximised margin using hinge-loss
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用铰链损失找到最大化的间隔
- en: How to deal with non-linear separable data using different kernels
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用不同的核函数处理非线性可分的数据
- en: '[Original](https://towardsdatascience.com/one-of-the-top-machine-learning-algorithms-for-supervised-learning-support-vector-machines-svms-fc45ac0667f4).
    Reposted with permission.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/one-of-the-top-machine-learning-algorithms-for-supervised-learning-support-vector-machines-svms-fc45ac0667f4)。转载已获许可。'
- en: '**Related:**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[A Friendly Introduction to Support Vector Machines](https://www.kdnuggets.com/2019/09/friendly-introduction-support-vector-machines.html)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[支持向量机的友好介绍](https://www.kdnuggets.com/2019/09/friendly-introduction-support-vector-machines.html)'
- en: '[Support Vector Machine (SVM) Tutorial: Learning SVMs From Examples](https://www.kdnuggets.com/2017/08/support-vector-machines-learning-svms-examples.html)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[支持向量机 (SVM) 教程：通过示例学习 SVM](https://www.kdnuggets.com/2017/08/support-vector-machines-learning-svms-examples.html)'
- en: '[What is a Support Vector Machine, and Why Would I Use it?](https://www.kdnuggets.com/2017/02/yhat-support-vector-machine.html)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是支持向量机，它的用途是什么？](https://www.kdnuggets.com/2017/02/yhat-support-vector-machine.html)'
- en: More On This Topic
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Support Vector Machines: An Intuitive Approach](https://www.kdnuggets.com/2022/08/support-vector-machines-intuitive-approach.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[支持向量机：一种直观的方法](https://www.kdnuggets.com/2022/08/support-vector-machines-intuitive-approach.html)'
- en: '[A Gentle Introduction to Support Vector Machines](https://www.kdnuggets.com/2023/07/gentle-introduction-support-vector-machines.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[支持向量机的温和入门](https://www.kdnuggets.com/2023/07/gentle-introduction-support-vector-machines.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学统计的最佳资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目的，并寻找目的来…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功数据科学家的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[是什么让Python成为初创公司的理想编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
