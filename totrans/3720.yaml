- en: Converting Text Documents to Token Counts with CountVectorizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/10/converting-text-documents-token-counts-countvectorizer.html](https://www.kdnuggets.com/2022/10/converting-text-documents-token-counts-countvectorizer.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We interact with machines on a daily basis – whether it's asking “OK Google,
    set the alarm for 6 AM” or “Alexa, play my favorite playlist”. But these machines
    do not understand natural language. So what happens when we talk to a device?
    It needs to convert the speech i.e. text to numbers for processing the information
    and learning the context. In this post, you will learn one of the most popular
    tools to convert the language to numbers using CountVectorizer. [Scikit-learn’s
    CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)
    is used to recast and preprocess corpora of text to a token count vector representation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting Text Documents to Token Counts with CountVectorizer](../Images/fc067cd21dca6b31af364682a86f4fc8.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Source](https://img.freepik.com/free-vector/human-hand-typing-computer-with-different-symbols-located-near-cup-coffee-side-view-open-laptop-flat-vector-illustration-new-technologies-millennials-work-concept_74855-21931.jpg?w=1480&t=st=1665332617~exp=1665333217~hmac=75d8c5ae364cabb50f74dbc02bc175292145468ab3c10b2f506adff151278f11)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take an example of a book title from a popular kids' book to illustrate
    how CountVectorizer works.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: There are six unique words in the vector; thus the length of the vector representation
    is six. The vector represents the frequency of occurrence of each token/word in
    the text.
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting Text Documents to Token Counts with CountVectorizer](../Images/aa0ff5a87c7c089c1e76b8275f32a5be.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's add another document to our corpora to witness how the dimension of the
    resulting matrix increases.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The CountVectorizer would produce the below output, where the matrix becomes
    a 2 X 13 from 1 X 6 by adding one more document.
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting Text Documents to Token Counts with CountVectorizer](../Images/10bd82304842443ba4df35a023c1fea0.png)'
  prefs: []
  type: TYPE_IMG
- en: Each column in the matrix represents a unique token (word) in the dictionary
    formed by a union of all tokens from the corpus of documents, while each row represents
    a document. The above example has two book titles i.e. documents represented by
    two rows where each cell contains a value identifying the corresponding word count
    in the document. As a result of such representation, certain cells have zero value
    wherever the token is absent in the corresponding document.
  prefs: []
  type: TYPE_NORMAL
- en: Notably, it becomes unmanageable to store huge matrices in memory with the increasing
    size of the corpora. Thus, CountVectorizer stores them as a sparse matrix, a compressed
    form of the full-blown matrix discussed above.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-On!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's pick the Harry Potter series of eight movies and one Indiana Jones movie
    for this demo. This would help us understand some important attributes of CountVectorizer.
  prefs: []
  type: TYPE_NORMAL
- en: Start with importing Pandas library and CountVectorizer from Sklearn > feature_extraction
    > text.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Declare the documents as a list of strings.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Vectorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Initialize the CountVectorizer object with lowercase=True (default value) to
    convert all documents/strings into lowercase. Next, call fit_transform and pass
    the list of documents as an argument followed by adding column and row names to
    the data frame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Good news! The documents are converted to numbers. But, a close look shows that
    “Harry Potter and the Order of the Phoenix” is similar to “Indiana Jones and the
    Raiders of the Lost Ark” as compared to other Harry Potter movies - at least at
    the first glance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting Text Documents to Token Counts with CountVectorizer](../Images/c6fd74a880e106bbbd3cf1d5aa1d74d5.png)'
  prefs: []
  type: TYPE_IMG
- en: You must be wondering if tokens like ‘and’, ‘the’, and ‘of’ add any information
    to our feature set. That takes us to our next step i.e. removing stop words.
  prefs: []
  type: TYPE_NORMAL
- en: stop_words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Uninformative tokens like ‘and’, ‘the’, and ‘of’ are called stop words. It is
    important to remove stop words as they impact the document's similarity and unnecessarily
    expand the column dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Argument ‘stop_words’ removes such preidentified stop words – specifying ‘english’
    removes English-specific stop words. You can also explicitly add a list of stop
    words i.e. stop_words = [‘and’, ‘of’, ‘the’].
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Looks better! Now the row vectors look more meaningful.
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting Text Documents to Token Counts with CountVectorizer](../Images/4bc31a8008f051607bac5c87b88c1cb1.png)'
  prefs: []
  type: TYPE_IMG
- en: max_df
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Words like “Harry” and “Potter” aren’t “stop words” but are quite common and
    add little information to the Count Matrix. Hence, you can add max_df argument
    to stem repetitive words as features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Below output demonstrates that stop words as well as “harry” and “potter” are
    removed from columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting Text Documents to Token Counts with CountVectorizer](../Images/6572c77d22e3d8fc438cc45c58ffde9d.png)'
  prefs: []
  type: TYPE_IMG
- en: min_df
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is exactly opposite to max_df and signifies the least number of documents
    (or proportion and percentage) that should have the particular feature.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Here the below columns (words) are present in at least two documents.
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting Text Documents to Token Counts with CountVectorizer](../Images/e7a006bf406429d69c168750e6f31a72.png)'
  prefs: []
  type: TYPE_IMG
- en: max_features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It represents the topmost occurring features/words/columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Top four commonly occurring words are chosen below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting Text Documents to Token Counts with CountVectorizer](../Images/f1dff0e8706c7f132871f6d592a61937.png)'
  prefs: []
  type: TYPE_IMG
- en: binary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The binary argument replaces all positive occurrences of words by ‘1’ in a document.
    It signifies the presence or absence of a word or token instead of frequency and
    is useful in analysis like sentiment or product review.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon comparing with the previous output, the frequency table of the column
    named “the” is capped to ‘1’ in the result shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting Text Documents to Token Counts with CountVectorizer](../Images/4b36f2b5d183f6a654b4c7c685626dbc.png)'
  prefs: []
  type: TYPE_IMG
- en: vocabulary_
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It returns the position of columns and is used to map algorithm results to interpretable
    words.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The output of the above code is shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The tutorial discussed the importance of pre-processing text aka vectorizing
    it as an input into machine learning algorithms. The post also demonstrated sklearn’s
    implementation of CountVectorizer with various input parameters on a small set
    of documents.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Vidhi Chugh](https://vidhi-chugh.medium.com/)** is an award-winning AI/ML
    innovation leader and an AI Ethicist. She works at the intersection of data science,
    product, and research to deliver business value and insights. She is an advocate
    for data-centric science and a leading expert in data governance with a vision
    to build trustworthy AI solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Classifying Long Text Documents Using BERT](https://www.kdnuggets.com/2022/02/classifying-long-text-documents-bert.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Convert Text Documents to a TF-IDF Matrix with tfidfvectorizer](https://www.kdnuggets.com/2022/09/convert-text-documents-tfidf-matrix-tfidfvectorizer.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Ways of Converting Unstructured Data into Structured Insights with LLMs](https://www.kdnuggets.com/5-ways-of-converting-unstructured-data-into-structured-insights-with-llms)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Converting JSONs to Pandas DataFrames: Parsing Them the Right Way](https://www.kdnuggets.com/converting-jsons-to-pandas-dataframes-parsing-them-the-right-way)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GPT4All is the Local ChatGPT for your Documents and it is Free!](https://www.kdnuggets.com/2023/06/gpt4all-local-chatgpt-documents-free.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Use ChatGPT to Convert Text into a PowerPoint Presentation](https://www.kdnuggets.com/2023/08/chatgpt-convert-text-powerpoint-presentation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
