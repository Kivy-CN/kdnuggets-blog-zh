# 使用 Python 实现你自己的 k-最近邻算法

> 原文：[https://www.kdnuggets.com/2016/01/implementing-your-own-knn-using-python.html](https://www.kdnuggets.com/2016/01/implementing-your-own-knn-using-python.html)

**由娜塔莎·拉季谢娃**。

*编辑者注*：娜塔莎活跃于[剑桥编码学院](https://cambridgecoding.com/)，该学院将在2016年2月20日至21日举办[Python 数据科学训练营](https://cambridgecoding.com/datascience-bootcamp)，在这里你可以学习针对现实世界问题的最先进机器学习技术。

在机器学习中，你可能经常希望构建能够根据一组相关值将事物分类的预测器。例如，可以根据以前患者的数据为患者提供诊断。分类可能涉及在类别之间构建高度非线性的边界，就像[下图](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)中红色、绿色和蓝色类别的情况一样：

![kNN 边界](../Images/4322b5f9ae66be35375b65bec3b6c05f.png)

已经开发了许多用于自动分类的算法，其中常见的包括随机森林、支持向量机、朴素贝叶斯分类器和多种神经网络。为了了解分类是如何工作的，我们以一个简单的分类算法——k-最近邻（kNN）——为例，并在 Python 2 中从头开始构建它。如果你刚开始接触 Python，你可以使用主要的[命令式编码风格](http://latentflip.com/imperative-vs-declarative/)，而不是带有[lambda 函数](http://www.secnetix.de/olli/Python/lambda_functions.hawk)和[列表推导](http://www.secnetix.de/olli/Python/list_comprehensions.hawk)的声明式/函数式风格，以保持简单。在这里，我们将介绍后者的方法。kNN 通过将新实例与最相似的情况分组来进行分类。在这里，你将使用 kNN 在流行（虽然理想化的）鸢尾花数据集上进行分类，该数据集包含三种鸢尾花的花朵测量数据。我们的任务是根据花朵测量数据预测一组花朵的物种标签。由于你将根据一组已知正确的分类构建预测器，kNN 是一种监督式机器学习（尽管有些混淆地说，在 kNN 中没有明确的训练阶段；见[懒惰学习](https://en.wikipedia.org/wiki/Lazy_learning)）。kNN 任务可以分解为编写 3 个主要函数：

1\. 计算任何两点之间的距离

2\. 根据这些成对距离找到最近的邻居

3\. 基于最近邻列表对类别标签进行多数投票

下图中的步骤提供了你在代码中需要完成的任务的高级概述。

[![kNN 算法](../Images/39dbb98036b866c0b324bcb95cf9762c.png)](https://cambridgecoding.files.wordpress.com/2016/01/knn2.jpg)

**算法**

简而言之，你希望构建一个脚本，对于每一个需要分类的输入，搜索整个训练集中的 k 个最相似实例。最相似实例的类别标签应通过多数投票进行总结，并作为对测试案例的预测返回。

完整的代码在文章末尾。现在，让我们分别查看不同的部分并解释它们的功能。

加载数据并分割成训练集和测试集 为了开始，你将使用一些辅助函数：虽然我们可以自己[下载鸢尾花数据](https://archive.ics.uci.edu/ml/datasets/Iris)并使用[csv.reader](https://docs.python.org/2/library/csv.html)加载它，但你也可以直接从 scikit-learn 快速获取鸢尾花数据。此外，你可以使用 train_test_split 函数进行 60/40 的训练/测试分割，但你也可以自己随机分配行（见此类型的实现）。在机器学习中，训练/测试分割用于减少过拟合——在整个数据集上训练模型往往会导致模型对数据的噪声和特性进行过度拟合，而不是对真实的、潜在的趋势进行拟合。你仅在训练集上进行任何模型调优（例如选择邻居数量 k）——测试集作为一个独立的、未经触及的数据集，用于测试最终模型的性能。

```py
from sklearn.datasets import load_iris
from sklearn import cross_validation
import numpy as np

# load dataset and partition in training and testing sets
iris = load_iris()
X_train, X_test, y_train, y_test = cross_validation.train_test_split(iris.data, iris.target, test_size=0.4, random_state=1)

# reformat train/test datasets for convenience
train = np.array(zip(X_train,y_train))
test = np.array(zip(X_test, y_test))

```

这里是鸢尾花数据集的概述、数据分割以及索引的快速指南。

[![分割鸢尾花数据集](../Images/8099def34ac1f68142c2841020df09ca.png)](https://cambridgecoding.files.wordpress.com/2016/01/knn3.jpg)

### 更多相关主题

+   [从理论到实践：构建 k 最近邻分类器](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)

+   [分类的最近邻](https://www.kdnuggets.com/2022/04/nearest-neighbors-classification.html)

+   [Scikit-learn 中的 K 最近邻](https://www.kdnuggets.com/2022/07/knearest-neighbors-scikitlearn.html)

+   [LangChain 101: 构建你自己的 GPT 驱动应用](https://www.kdnuggets.com/2023/04/langchain-101-build-gptpowered-applications.html)

+   [用 LlamaIndex 构建你自己的 PandasAI](https://www.kdnuggets.com/build-your-own-pandasai-with-llamaindex)

+   [用 ChatGPT 的 GPTs 创建你自己的 GPT](https://www.kdnuggets.com/make-your-own-gpts-with-chatgpts-gpts)
