["```py\n## Libraries\n\nimport tensorflow as tf\n\nmodel = tf.keras.models.Sequential()\nDense = tf.keras.layers.Dense\nDropout = tf.keras.layers.Dropout\nLSTM = tf.keras.layers.LSTM\n\n## Dataset\nmnist_data = tf.keras.datasets.mnist # mnist is a dataset of 28x28 images of handwritten digits and their labels with 60,000 rows of data\n\n## Create train and test data\n(x_train, y_train),(x_test, y_test) = mnist_data.load_data()\n\nx_train = x_train/255.0 # Normalize training data features\nx_test = x_test/255.0 # Normalize training data labels\n\n# The images are 28x28 pixels of unassigned integers in the range of 0 to 255\\. The above #normalization code is not necessary and can still be passed on to compile. \n# However, the #accuracy will be much worse of at around 20% best case scenario and loss at over 90%. The #training time will also increase.\n\nmodel.add(LSTM(256, activation='relu', return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(256, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation='softmax'))\n\noptimizer = tf.keras.optimizers.Adam(lr=1e-4, decay=1e-6)\n\n# Compile model\nmodel.compile(\n   loss='sparse_categorical_crossentropy',\n   optimizer=optimizer,\n   metrics=['accuracy'],\n)\n\n# The specification of loss=’sparse_categorical_crossentropy’ is very important here as our targets are # integers and not one-hot encoded categories.\nmodel.fit(x_train,\n   y_train,\n   epochs=4,\n   validation_data=(x_test, y_test))\n```", "```py\nEpoch 1/4\n60000/60000 [==============================] - 278s 5ms/sample - loss: 0.9960 - acc: 0.6611 - val_loss: 0.2939 - val_acc: 0.9013\n\nEpoch 2/4\n60000/60000 [==============================] - 276s 5ms/sample - loss: 0.2955 - acc: 0.9107 - val_loss: 0.1523 - val_acc: 0.9504\n\nEpoch 3/4\n60000/60000 [==============================] - 273s 5ms/sample - loss: 0.1931 - acc: 0.9452 - val_loss: 0.1153 - val_acc: 0.9641\n\nEpoch 4/4\n60000/60000 [==============================] - 270s 4ms/sample - loss: 0.1489 - acc: 0.9581 - val_loss: 0.1076 - val_acc: 0.9696\n```"]