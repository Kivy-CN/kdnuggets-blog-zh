- en: 'Data Science Basics: An Introduction to Ensemble Learners'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2016/11/data-science-basics-intro-ensemble-learners.html](https://www.kdnuggets.com/2016/11/data-science-basics-intro-ensemble-learners.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Algorithm selection can be challenging for machine learning newcomers. Often
    when building classifiers, especially for beginners, an approach is adopted to
    problem solving which considers single instances of single algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: However, in a given scenario, it may prove more useful to chain or group classifiers
    together, using the techniques of voting, weighting, and combination to pursue
    the most accurate classifier possible. Ensemble learners are classifiers which
    provide this functionality in a variety of ways.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: This post will provide an overview of bagging, boosting, and stacking, arguably
    the most used and well-known of the basic ensemble methods. They are not, however,
    the **only** options. [Random Forests](/tag/random-forests) is another example
    of an ensemble learner, which uses numerous decision trees in a single predictive
    model, and which is often overlooked and treated as a "regular" algorithm. There
    are other approaches to selecting effective algorithms as well, treated below.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bagging operates by simple concept: build a number of models, observe the results
    of these models, and settle on the majority result. I recently had an issue with
    the rear axle assembly in my car: I wasn''t sold on the diagnosis of the dealership,
    and so I took it to 2 other garages, both of which agreed the issue was something
    different than the dealership suggested. *Voila*. Bagging in action.'
  prefs: []
  type: TYPE_NORMAL
- en: I only visited 3 garages in my example, but you could imagine that accuracy
    would likely increase if I had visited tens or hundreds of garages, especially
    if my car's problem was one of a very complex nature. This holds true for bagging,
    and the bagged classifier often is significantly more accurate than single constituent
    classifiers. Also note that the type of constituent classifier used are inconsequential;
    the resulting model can be made up of any single classifier type.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging is short for *bootstrap aggregation*, so named because it takes a number
    of samples from the dataset, with each sample set being regarded as a bootstrap
    sample. The results of these bootstrap samples are then aggregated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bagging TL;DR:'
  prefs: []
  type: TYPE_NORMAL
- en: Operates via equal weighting of models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Settles on result using majority voting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Employs multiple instances of same classifier for one dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Builds models of smaller datasets by sampling with replacement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Works best when classifier is unstable (decision trees, for example), as this
    instability creates models of differing accuracy and results to draw majority
    from
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging can hurt stable model by introducing artificial variability from which
    to draw inaccurate conclusions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Bagging](../Images/da14f46784242ccf6a49c3d7ea114504.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Boosting**'
  prefs: []
  type: TYPE_NORMAL
- en: Boosting is similar to bagging, but with one conceptual modification. Instead
    of assigning equal weighting to models, boosting assigns varying weights to classifiers,
    and derives its ultimate result based on weighted voting.
  prefs: []
  type: TYPE_NORMAL
- en: Thinking again of my car problem, perhaps I had been to one particular garage
    numerous times in the past, and trusted their diagnosis slightly more than others.
    Also suppose that I was not a fan of previous interactions with the dealership,
    and that I trusted their insight less. The weights I assigned would be reflective.
  prefs: []
  type: TYPE_NORMAL
- en: 'Boosting TL;DR:'
  prefs: []
  type: TYPE_NORMAL
- en: Operates via weighted voting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithm proceeds iteratively; new models are influenced by previous ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New models become experts for instances classified incorrectly by earlier models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Can** be used without weights by using resampling, with probability determined
    by weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Works well if classifiers are not too complex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also works well with weak learners
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AdaBoost](https://en.wikipedia.org/wiki/AdaBoost) (Adaptive Boosting) is a
    popular boosting algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitBoost](https://en.wikipedia.org/wiki/LogitBoost) (derived from AdaBoost)
    is another, which uses additive logistic regression, and handles multi-class problems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Boosting](../Images/658f99083c93080b92c1f86c7f94ecf7.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Stacking**'
  prefs: []
  type: TYPE_NORMAL
- en: Stacking is a bit different from the previous 2 techniques as it trains multiple
    single classifiers, as opposed to various incarnations of the same learner. While
    bagging and boosting would use numerous models built using various instances of
    the same classification algorithm (eg. decision tree), stacking builds its models
    using different classification algorithms (perhaps decision trees, logistic regression,
    an ANNs, or some other combination).
  prefs: []
  type: TYPE_NORMAL
- en: A combiner algorithm is then trained to make ultimate predictions using the
    predictions of other algorithms. This combiner can be any ensemble technique,
    but logistic regression is often found to be an adequate and simple algorithm
    to perform this combining. Along with classification, stacking can also be employed
    in unsupervised learning tasks such as density estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stacking TL;DR:'
  prefs: []
  type: TYPE_NORMAL
- en: Trains multiple learners (as opposed to bagging/boosting which train a single
    learner)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each learner uses a subset of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A "combiner" is trained on a validation segment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacking uses a meta learner (as opposed to bagging/boosting which use voting
    schemes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Difficult to analyze theoretically ("black magic")
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Level-1 → meta learner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Level-0 → base classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can also be used for numeric prediction (regression)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The best algorithms to use for base models are smooth, global learners
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Stacking](../Images/2f18adb49b096f6887425f5b3aa4d528.png)'
  prefs: []
  type: TYPE_IMG
- en: While the above ensemble learners are likely the most well-known and -utilized,
    numerous other options exist. Besides stacking, a variety of other meta learners
    exist as well.
  prefs: []
  type: TYPE_NORMAL
- en: An easy mistake for data science newcomers to make is to underestimate the complexity
    of the algorithm landscape, thinking that a decision tree is a decision tree,
    a neural network is a neural network, etc. Besides overlooking the fact that there
    is significant difference between various specific implementations of algorithm
    families (look at the vast amount of research being done in neural networks over
    the past few years for explicit evidence), the idea that various algorithms can
    be used in tandem (via ensemble or intervening meta-learners) to accomplish greater
    accuracy in a given task, or even to tackle tasks which, alone, would not be solvable,
    does not even register. Any form of recent "artificial intelligence" you can think
    of takes this approach. But that's a different conversation completely.
  prefs: []
  type: TYPE_NORMAL
- en: '**Related**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Data Science Basics: Data Mining vs. Statistics](/2016/09/data-science-basics-data-mining-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Science Basics: 3 Insights for Beginners](/2016/09/data-science-basics-3-insights-beginners.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning Key Terms, Explained](/2016/05/machine-learning-key-terms-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[When Would Ensemble Techniques be a Good Choice?](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ensemble Learning with Examples](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Back to Basics Week 3: Introduction to Machine Learning](https://www.kdnuggets.com/back-to-basics-week-3-introduction-to-machine-learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Back to Basics Week 1: Python Programming & Data Science Foundations](https://www.kdnuggets.com/back-to-basics-week-1-python-programming-data-science-foundations)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mastering Python for Data Science: Beyond the Basics](https://www.kdnuggets.com/mastering-python-for-data-science-beyond-the-basics)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
