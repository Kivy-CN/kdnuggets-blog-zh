- en: Unveiling Unsupervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭示无监督学习
- en: 原文：[https://www.kdnuggets.com/unveiling-unsupervised-learning](https://www.kdnuggets.com/unveiling-unsupervised-learning)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/unveiling-unsupervised-learning](https://www.kdnuggets.com/unveiling-unsupervised-learning)
- en: '![Unveiling Unsupervised Learning](../Images/ef0e31549bbc4dde78a53ff85e3a7fb5.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![揭示无监督学习](../Images/ef0e31549bbc4dde78a53ff85e3a7fb5.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: What Is Unsupervised Learning?
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是无监督学习？
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全领域的职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In machine learning, **unsupervised learning** is a paradigm that involves training
    an algorithm on an *unlabeled* dataset. So there’s no supervision or labeled outputs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，**无监督学习**是一种在*未标记*数据集上训练算法的范式。因此，没有监督或标记输出。
- en: In unsupervised learning, the goal is to discover patterns, structures, or relationships
    within the data itself, rather than predicting or classifying based on labeled
    examples. It involves exploring the inherent structure of the data to gain insights
    and make sense of complex information.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，目标是发现数据本身的模式、结构或关系，而不是基于标记的示例进行预测或分类。这涉及探索数据的内在结构，以获取洞察并理解复杂信息。
- en: This guide will introduce you to unsupervised learning. We’ll start by going
    over the differences between supervised and unsupervised learning—to lay the ground
    for the remainder of the discussion. We’ll then cover the key unsupervised learning
    techniques and the popular algorithms within them.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南将介绍无监督学习。我们将首先探讨监督学习和无监督学习之间的差异，为后续讨论奠定基础。接着，我们将介绍关键的无监督学习技术及其中的流行算法。
- en: Supervised vs. Unsupervised Learning
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习与无监督学习
- en: 'Supervised and unsupervised machine learning are two different approaches used
    in the field of artificial intelligence and data analysis. Here''s a brief summary
    of their key differences:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习和无监督学习是人工智能和数据分析领域中使用的两种不同方法。以下是它们关键差异的简要总结：
- en: Training Data
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练数据
- en: In supervised learning, the algorithm is trained on a **labeled dataset**, where
    input data is paired with corresponding desired output (labels or target values).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，算法在**标记数据集**上进行训练，其中输入数据与对应的期望输出（标签或目标值）配对。
- en: Unsupervised learning, on the other hand, involves working with an **unlabeled
    dataset**, where there are no predefined output labels.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习则涉及处理**未标记的数据集**，其中没有预定义的输出标签。
- en: Objective
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标
- en: The goal of supervised learning algorithms is to **learn a relationship**—**a
    mapping**—from the input to the output space. Once the mapping is learned, we
    can use the model to predict the output values or class label for unseen data
    points.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法的目标是**学习关系**——**映射**——从输入到输出空间。一旦学会了这种映射，我们可以使用模型预测未见数据点的输出值或类别标签。
- en: In unsupervised learning, the goal is to **find patterns**, structures, or relationships
    within the data, often for clustering data points into groups, exploratory analysis
    or feature extraction.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，目标是**发现模式**、结构或关系，通常用于将数据点聚类到组中、进行探索性分析或特征提取。
- en: Common Tasks
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见任务
- en: '**Classification** (assigning a class label—one of the many predefined categories—to
    a previously unseen data point) and **regression** (predicting continuous values)
    are common tasks in supervised learning.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类**（为一个未见过的数据点分配一个类别标签——众多预定义类别之一）和**回归**（预测连续值）是监督学习中的常见任务。'
- en: '**Clustering** (grouping similar data points) and **dimensionality reduction**
    (reducing the number of features while preserving important information) are common
    tasks in unsupervised learning. We’ll discuss these in greater detail shortly.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚类**（将相似的数据点分组）和**降维**（在保留重要信息的同时减少特征数量）是无监督学习中的常见任务。我们将在稍后详细讨论这些内容。'
- en: When To Use
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时使用
- en: Supervised learning is widely used when the desired output is known and well-defined,
    such as spam email detection, image classification, and medical diagnosis.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督学习广泛用于期望输出已知且定义明确的情况，例如垃圾邮件检测、图像分类和医学诊断。
- en: Unsupervised learning is used when there is limited or no prior knowledge about
    the data and the objective is to uncover hidden patterns or gain insights from
    the data itself.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习在对数据没有或仅有有限的先验知识时使用，其目标是揭示隐藏的模式或从数据本身中获得洞察。
- en: 'Here’s a summary of the differences:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是差异的总结：
- en: '![Unveiling Unsupervised Learning](../Images/ff5a48170413c9ac9e58712a4791acd8.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![揭示无监督学习](../Images/ff5a48170413c9ac9e58712a4791acd8.png)'
- en: Supervised vs. Unsupervised Learning | Image by Author
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督学习与无监督学习 | 图片由作者提供
- en: '**Summing up**: Supervised learning focuses on learning from labeled data to
    make predictions or classifications, while unsupervised learning seeks to discover
    patterns and relationships within unlabeled data. Both approaches have their own
    applications—based on the nature of the data and the problem at hand.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结**：有监督学习侧重于从标记数据中学习，以进行预测或分类，而无监督学习则试图发现未标记数据中的模式和关系。两种方法各有其应用——根据数据的性质和当前问题。'
- en: Unsupervised Learning Techniques
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习技术
- en: As discussed, in unsupervised learning, we have the input data and are tasked
    with finding meaningful patterns or representations within that data. Unsupervised
    learning algorithms do so by identifying similarities, differences, and relationships
    among the data points without being provided with predefined categories or labels.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如讨论所述，在无监督学习中，我们拥有输入数据并被要求在这些数据中寻找有意义的模式或表示。无监督学习算法通过识别数据点之间的相似性、差异和关系来实现这一点，而不提供预定义的类别或标签。
- en: 'For this discussion, we’ll go over the two main unsupervised learning techniques:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本讨论中，我们将介绍两种主要的无监督学习技术：
- en: Clustering
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类
- en: Dimensionality Reduction
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降维
- en: What Is Clustering?
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是聚类？
- en: Clustering involves grouping similar data points together into clusters based
    on some *similarity measure*. The algorithm aims to find natural groups or categories
    within the data where data points in the same cluster are *more similar* to each
    other than to those in other clusters.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类涉及根据某种*相似性度量*将相似的数据点分组到一起。算法旨在找到数据中的自然组或类别，其中同一簇中的数据点*彼此更相似*，而与其他簇中的数据点相比则不那么相似。
- en: Once we have the dataset grouped into different clusters we can essentially
    label them. And if needed, we can perform supervised learning on the clustered
    dataset.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将数据集分成不同的簇，我们实际上可以对它们进行标记。如有需要，我们可以在聚类后的数据集上执行有监督学习。
- en: What Is Dimensionality Reduction?
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是降维？
- en: Dimensionality reduction refers to techniques that reduce the number of features—dimensions—in
    the data while preserving important information. High-dimensional data can be
    complex and difficult to work with, so dimensionality reduction helps in simplifying
    the data for analysis.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 降维是指减少数据中特征——维度——数量的技术，同时保留重要信息。高维数据可能复杂且难以处理，因此降维有助于简化数据以便进行分析。
- en: Both clustering and dimensionality reduction are powerful techniques in unsupervised
    learning, providing valuable insights and simplifying complex data for further
    analysis or modeling.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类和降维都是无监督学习中强大的技术，提供了宝贵的洞察力，并简化了复杂数据以便于进一步分析或建模。
- en: In the remainder of the article, let's review important clustering and dimensionality
    reduction algorithms.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文的剩余部分，让我们回顾重要的聚类和降维算法。
- en: 'Clustering Algorithms: An Overview'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类算法：概述
- en: As discussed, clustering is a fundamental technique in unsupervised learning
    that involves grouping similar data points together into clusters, where data
    points within the same cluster are more similar to each other than to those in
    other clusters. Clustering helps identify natural divisions within the data, which
    can provide insights into patterns and relationships.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，聚类是无监督学习中的一种基本技术，它涉及将相似的数据点分组到簇中，在同一簇内的数据点彼此之间比与其他簇中的数据点更相似。聚类有助于识别数据中的自然划分，这可以提供关于模式和关系的见解。
- en: 'There are various algorithms used for clustering, each with its own approach
    and characteristics:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种算法用于聚类，每种算法都有其自身的方法和特性：
- en: K-Means Clustering
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-Means 聚类
- en: K-Means clustering is a simple, robust, and commonly used algorithm. It partitions
    the data into a predefined number of clusters (K) by iteratively updating cluster
    centroids based on the mean of data points within each cluster.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: K-Means 聚类是一个简单、稳健且常用的算法。它通过根据每个簇中数据点的均值迭代更新簇质心，将数据划分为预定义数量的簇（K）。
- en: It iteratively refines cluster assignments until convergence.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过迭代精细化簇分配，直到收敛。
- en: 'Here’s how the K-Means clustering algorithm works:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 K-Means 聚类算法的工作原理：
- en: Initialize K cluster centroids.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 K 个簇的质心。
- en: Assign each data point—based on the chosen distance metric—to the nearest cluster
    centroid.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据选择的距离度量，将每个数据点分配到最近的簇质心。
- en: Update centroids by computing the mean of data points in each cluster.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过计算每个簇中数据点的均值来更新质心。
- en: Repeat steps 2 and 3 until convergence or a defined number of iterations.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 2 和 3，直到收敛或达到定义的迭代次数。
- en: Hierarchical Clustering
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层次聚类
- en: Hierarchical clustering creates a tree-like structure—a **dendrogram**—of data
    points, capturing similarities at multiple levels of granularity. Agglomerative
    clustering is the most commonly used hierarchical clustering algorithm. It starts
    with individual data points as separate clusters and gradually merges them based
    on a linkage criterion, such as distance or similarity.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类创建一个树状结构——一个**树形图**——的数据点，捕捉不同粒度层次的相似性。凝聚层次聚类是最常用的层次聚类算法。它从单独的数据点作为独立簇开始，并根据连接准则（如距离或相似性）逐渐合并它们。
- en: 'Here’s how the agglomerative clustering algorithm works:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是凝聚层次聚类算法的工作原理：
- en: 'Start with `n` clusters: each data point as its own cluster.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `n` 个簇开始：每个数据点作为自己的簇。
- en: Merge closest data points/clusters into a larger cluster.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将最近的数据点/簇合并为一个更大的簇。
- en: Repeat 2\. until a single cluster remains or a defined number of clusters is
    reached.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复 2 直到剩下一个簇或达到定义的簇数量。
- en: The result can be interpreted with the help of a dendrogram.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果可以借助树形图进行解释。
- en: Density-Based Spatial Clustering of Applications with Noise (DBSCAN)
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于密度的空间聚类（DBSCAN）
- en: DBSCAN identifies clusters based on the density of data points in a neighborhood.
    It can find arbitrarily shaped clusters and can also identify noise points and
    detect outliers.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN 根据邻域中数据点的密度识别簇。它可以发现任意形状的簇，并且还可以识别噪声点和检测离群点。
- en: 'The algorithm involves the following (simplified to include the key steps):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法包括以下（简化以包含关键步骤）：
- en: Select a data point and find its neighbors within a specified radius.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个数据点，并找到其指定半径内的邻居。
- en: If the point has sufficient neighbors, expand the cluster by including the neighbors
    of its neighbors.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果点有足够的邻居，通过包括其邻居的邻居来扩展簇。
- en: Repeat for all points, forming clusters connected by density.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对所有点重复此操作，形成由密度连接的簇。
- en: 'Dimensionality Reduction Algorithms: An Overview'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维算法概述
- en: Dimensionality reduction is the process of reducing the number of features (dimensions)
    in a dataset while retaining essential information. High-dimensional data can
    be complex, computationally expensive, and is prone to overfitting. Dimensionality
    reduction algorithms help simplify data representation and visualization.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 降维是减少数据集中特征（维度）数量的过程，同时保留重要信息。高维数据可能复杂、计算开销大且容易过拟合。降维算法有助于简化数据表示和可视化。
- en: Principal Component Analysis (PCA)
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）
- en: '[Principal Component Analysis](/2023/05/principal-component-analysis-pca-scikitlearn.html)—or
    PCA—transforms data into a new coordinate system to maximize variance along the
    principal components. It reduces data dimensions while preserving as much variance
    as possible.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how you can perform PCA for dimensionality reduction:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Compute the covariance matrix of the input data.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform eigenvalue decomposition on the covariance matrix. Compute the eigenvectors
    and eigenvalues of the covariance matrix.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort eigenvectors by eigenvalues in descending order.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Project data onto the eigenvectors to create a lower-dimensional representation.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: t-Distributed Stochastic Neighbor Embedding (t-SNE)
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first time I used t-SNE was to visualize word embeddings. t-SNE is used
    for visualization by reducing high-dimensional data to a lower-dimensional representation
    while maintaining local pairwise similarities.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how t-SNE works:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Construct probability distributions to measure pairwise similarities between
    data points in high-dimensional and low-dimensional spaces.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Minimize the divergence between these distributions using gradient descent.
    Iteratively move data points in the lower-dimensional space, adjusting their positions
    to minimize the cost function.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In addition, there are deep learning architectures such as autoencoders that
    can be used for dimensionality reduction. Autoencoders are neural networks designed
    to encode and then decode data, effectively learning a compressed representation
    of the input data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Some Applications of Unsupervised Learning
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s explore some applications of unsupervised learning. Here are some examples:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Customer Segmentation
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In marketing, businesses use unsupervised learning to segment their customer
    base into groups with similar behaviors and preferences. This helps tailor marketing
    strategies, campaigns, and product offerings. For example, retailers categorize
    customers into groups such as "budget shoppers," "luxury buyers," and "occasional
    purchasers."
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Document Clustering
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can run a clustering algorithm on a corpus of documents. This helps group
    similar documents together, aiding in document organization, search, and retrieval.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly Detection
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unsupervised learning can be used to identify rare and unusual patterns—anomalies—in
    data. Anomaly detection has applications in fraud detection and network security
    to detect unusual—anomalous—behavior. Detecting fraudulent credit card transactions
    by identifying unusual spending patterns is a practical example.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Image Compression
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clustering can be used for image compression to transform images from high-dimensional
    color space to a much lower dimensional color space. This reduces image storage
    and transmission size by representing similar pixel regions with a single centroid.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Social Network Analysis
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can analyze social network data—based on user interactions—to uncover communities,
    influencers, and patterns of interaction.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Topic Modeling
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In natural language processing, the task of topic modeling is used to extract
    topics from a collection of text documents. This helps categorize and understand
    the main themes—topics—within a large text corpus.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理领域，主题建模任务用于从文本集合中提取主题。这有助于对大文本语料库中的主要主题进行分类和理解。
- en: Say, we have a corpus of news articles and we don’t have the documents and their
    corresponding categories beforehand. So we can perform topic modeling on the collection
    of news articles to identify topics such as politics, technology, and entertainment.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，我们有一批新闻文章的语料库，并且我们事先没有这些文档及其对应的类别。我们可以对这批新闻文章进行主题建模，以识别诸如政治、科技和娱乐等主题。
- en: Genomic Data Analysis
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基因组数据分析
- en: Unsupervised learning also has applications in biomedical and genomic data analysis.
    Examples include clustering genes based on their expression patterns to discover
    potential associations with specific diseases.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习在生物医学和基因组数据分析中也有应用。例如，通过根据基因的表达模式对基因进行聚类，以发现与特定疾病的潜在关联。
- en: Conclusion
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: I hope this article helped you understand the basics of unsupervised learning.
    The next time you work with a real-world dataset, try to figure out the learning
    problem at hand. And try to assess if it can be modeled as a supervised or an
    unsupervised learning problem.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章能帮助你理解无监督学习的基础知识。下次你处理实际数据集时，尝试找出当前的学习问题，并评估是否可以将其建模为监督学习或无监督学习问题。
- en: If you’re working with a dataset with high-dimensional features, try to apply
    dimensionality reduction before building the machine learning model. Keep learning!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在处理具有高维特征的数据集，尝试在构建机器学习模型之前应用降维技术。继续学习！
- en: '**[Bala Priya C](https://www.linkedin.com/in/bala-priya/)** is a developer
    and technical writer from India. She likes working at the intersection of math,
    programming, data science, and content creation. Her areas of interest and expertise
    include DevOps, data science, and natural language processing. She enjoys reading,
    writing, coding, and coffee! Currently, she''s working on learning and sharing
    her knowledge with the developer community by authoring tutorials, how-to guides,
    opinion pieces, and more.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Bala Priya C](https://www.linkedin.com/in/bala-priya/)** 是一位来自印度的开发者和技术写作专家。她喜欢在数学、编程、数据科学和内容创作的交集处工作。她的兴趣和专长领域包括DevOps、数据科学和自然语言处理。她喜欢阅读、写作、编程和咖啡！目前，她正在通过撰写教程、操作指南、观点文章等，与开发者社区分享她的知识。'
- en: More On This Topic
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Unsupervised Disentangled Representation Learning in Class…](https://www.kdnuggets.com/2023/01/unsupervised-disentangled-representation-learning-class-imbalanced-dataset-elastic-infogan.html)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在类别不平衡数据集上进行无监督解缠表示学习](https://www.kdnuggets.com/2023/01/unsupervised-disentangled-representation-learning-class-imbalanced-dataset-elastic-infogan.html)'
- en: '[Exploring Unsupervised Learning Metrics](https://www.kdnuggets.com/2023/04/exploring-unsupervised-learning-metrics.html)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[探索无监督学习指标](https://www.kdnuggets.com/2023/04/exploring-unsupervised-learning-metrics.html)'
- en: '[Clustering with scikit-learn: A Tutorial on Unsupervised Learning](https://www.kdnuggets.com/2023/05/clustering-scikitlearn-tutorial-unsupervised-learning.html)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用scikit-learn进行聚类：无监督学习教程](https://www.kdnuggets.com/2023/05/clustering-scikitlearn-tutorial-unsupervised-learning.html)'
- en: '[Hands-On with Unsupervised Learning: K-Means Clustering](https://www.kdnuggets.com/handson-with-unsupervised-learning-kmeans-clustering)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[无监督学习实战：K-Means聚类](https://www.kdnuggets.com/handson-with-unsupervised-learning-kmeans-clustering)'
- en: '[Unveiling the Potential of CTGAN: Harnessing Generative AI for…](https://www.kdnuggets.com/2023/04/unveiling-potential-ctgan-harnessing-generative-ai-synthetic-data.html)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[揭示CTGAN的潜力：利用生成式AI生成合成数据](https://www.kdnuggets.com/2023/04/unveiling-potential-ctgan-harnessing-generative-ai-synthetic-data.html)'
- en: '[Unveiling Midjourney 5.2: A Leap Forward in AI Image Generation](https://www.kdnuggets.com/2023/06/unveiling-midjourney-52-leap-forward.html)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[揭示Midjourney 5.2：AI图像生成的飞跃](https://www.kdnuggets.com/2023/06/unveiling-midjourney-52-leap-forward.html)'
