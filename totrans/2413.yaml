- en: A Beginner’s Guide to Q Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q学习初学者指南
- en: 原文：[https://www.kdnuggets.com/2022/06/beginner-guide-q-learning.html](https://www.kdnuggets.com/2022/06/beginner-guide-q-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/06/beginner-guide-q-learning.html](https://www.kdnuggets.com/2022/06/beginner-guide-q-learning.html)
- en: '![A Beginner''s Guide to Q Learning](../Images/7fd8fd599940431a0cdca357f7474ae8.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Q学习初学者指南](../Images/7fd8fd599940431a0cdca357f7474ae8.png)'
- en: '[Alexander Andrews](https://unsplash.com/@alex_andrews) via Unsplash'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[亚历山大·安德鲁斯](https://unsplash.com/@alex_andrews) 通过 Unsplash'
- en: In order for you to understand what Q learning is, you need some knowledge of
    Reinforcement Learning.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解 Q 学习，你需要一些强化学习的知识。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三名课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Reinforcement Learning** branches from Machine Learning, it aims to train
    a model that returns an optimum solution using a sequence of solutions that have
    been created for a specific problem.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习** 源于机器学习，它旨在通过一系列为特定问题创建的解决方案来训练一个返回最佳解决方案的模型。'
- en: The model will have a variety of solutions and when it chooses the right one,
    a reward signal is generated. If the model performs closer to the goal, a positive
    reward is generated; however, if the model performs further away from the goal,
    a negative reward is generated.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 模型会有多种解决方案，当选择正确的方案时，会生成奖励信号。如果模型的表现接近目标，则生成正奖励；如果模型的表现远离目标，则生成负奖励。
- en: Reinforcement Learning consists of two types of algorithms
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习包括两种算法
- en: '**Model-free:** This excludes the dynamics of the environment to estimate the
    optimal policy'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**无模型的：** 这排除环境的动态性来估计最佳策略'
- en: '**Model-based:** This includes the dynamics of the environment to estimate
    the optimal policy.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**基于模型的：** 这包括环境的动态性来估计最佳策略。'
- en: What is Q-Learning?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 Q-Learning？
- en: '**Q-Learning** is a model-free reinforcement learning algorithm. It tries to
    find the next best action that can maximize the reward, randomly. The algorithm
    updates the value function based on an equation, making it a **value-based** learning
    algorithm.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q-Learning** 是一种无模型的强化学习算法。它尝试随机找出可以最大化奖励的最佳行动。该算法基于一个方程更新价值函数，使其成为一种**基于价值**的学习算法。'
- en: It’s like when you’re trying to find a solution to your current situation, to
    ensure you reap maximum benefits. The model can generate its own rules and even
    operate outside of the policy provided. Implying that there is no need for a policy,
    making it an off-policy learner.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 就像当你试图找到当前情况的解决方案，以确保你获得最大的利益一样。模型可以生成自己的规则，甚至在提供的策略之外运行。这意味着不需要策略，使其成为一种离策略学习者。
- en: An **off-policy learner** is when the model learns the value of the optimal
    policy regardless of the agent’s actions. An **on-policy learner** is when the
    model learns the value of the policy carried out by the agent, finding an optimal
    policy.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**离策略学习者** 是指模型学习最佳策略的价值而不管代理的行动。而 **在策略学习者** 是指模型学习代理所执行的策略的价值，找到最佳策略。'
- en: Let’s look at advertisements based on a recommendation system. Advertisements
    no-a-days are based on your search history or previous watches. However, Q-Learning
    can go a step further by optimizing the ad recommendation system to recommend
    products that are known to be frequently bought together. The reward signal is
    if the user purchases or clicks on the suggested product.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看基于推荐系统的广告。如今的广告基于你的搜索历史或之前的观看记录。然而，Q-Learning 可以更进一步，通过优化广告推荐系统来推荐那些经常被一起购买的产品。奖励信号是用户是否购买或点击了推荐的产品。
- en: ‘Q’ in Q-Learning stands for Quality. It represents how effective a given action
    is in gaining rewards.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习中的‘Q’代表质量。它表示给定动作在获得奖励方面的有效性。
- en: Bellman Equation
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝尔曼方程
- en: The Bellman Equation was named after Richard E. Bellman who was known as the
    father of Dynamic programming. Dynamic Programming aims to simplify complex problems/tasks,
    by breaking them down into smaller problems and then solving these smaller problems
    recursively before attacking the larger problem.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程以理查德·E·贝尔曼的名字命名，他被称为动态规划的奠基人。动态规划旨在通过将复杂问题/任务分解成更小的问题，然后递归解决这些小问题，从而简化复杂问题。
- en: 'The **Bellman Equation** determines the value of a particular state and comes
    to a conclusion on  how valuable it is in that state. The Q-function uses the
    Bellman equation and uses two inputs: the state (s) and the action (a).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**贝尔曼方程**确定特定状态的值，并得出该状态的价值。Q函数使用贝尔曼方程，并使用两个输入：状态(s)和动作(a)。'
- en: 'How do we know which action to take if we know all the expected rewards for
    every action? You can choose the sequence of actions that generate the best rewards
    which we can represent as the Q value. Using this equation:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们知道每个动作的所有期望奖励，怎么知道选择哪个动作？你可以选择生成最佳奖励的动作序列，我们可以将其表示为Q值。使用这个方程：
- en: '![Bellman Equation](../Images/290b2262c96c6d736f883046cbf52273.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![贝尔曼方程](../Images/290b2262c96c6d736f883046cbf52273.png)'
- en: Q(s, a) stands for the Q Value that has been yielded at state ‘s’ and selecting
    action ‘a’.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q(s, a)代表在状态‘s’下执行动作‘a’所得到的Q值。
- en: This is calculated by r(s, a) which stands for the immediate reward received
    + the best Q Value from state ‘s’.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这通过r(s, a)计算，其中r(s, a)代表立即获得的奖励 + 状态‘s’的最佳Q值。
- en: '![Equation](../Images/3ed6fc16fb7ad00b363f9819c310dc0b.png) is the discount
    factor that controls and determines the importance to the current state'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![方程](../Images/3ed6fc16fb7ad00b363f9819c310dc0b.png) 是折扣因子，控制和决定当前状态的重要性'
- en: The equation consists of the current state, the learning rate, the discount
    factor, the reward linked to that particular state, and the maximum expected reward.
    These are used to find the next state of the agent.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 该方程包括当前状态、学习率、折扣因子、与特定状态相关的奖励以及最大期望奖励。这些用于确定代理的下一个状态。
- en: What is a Q-table?
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是Q表？
- en: You can imagine the different paths and solutions that Q-learning provides.
    Therefore, in order to manage and determine which is the best, we use Q-table.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以想象Q学习所提供的不同路径和解决方案。因此，为了管理和确定哪个是最好的，我们使用Q表。
- en: '**Q-table** is just a simple lookup table. It is created so we can calculate
    as well as manage the maximum expected future rewards. We can easily identify
    the best action for each state in the environment. Using the Bellman Equation
    at each state, we get the expected future state and reward, then save it in a
    table to make comparisons to other states.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q表**只是一个简单的查找表。它的创建是为了便于计算和管理最大期望的未来奖励。我们可以轻松识别环境中每个状态的最佳动作。通过在每个状态下使用贝尔曼方程，我们获得期望的未来状态和奖励，然后将其保存在表中，以便与其他状态进行比较。'
- en: 'For example:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '| **State:** | **Action:** |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| **状态：** | **动作：** |'
- en: '|  | ![up](../Images/bcb34682015d5ea4cc26d99c09f29582.png) | ![down](../Images/debf225a26fbacce2c4ec53046250054.png)
    | ![right](../Images/0da7cb1170753229a83fadd201a3e542.png) | ![ left](../Images/3b29475fcaa612f8eac811c79149eb41.png)
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | ![上](../Images/bcb34682015d5ea4cc26d99c09f29582.png) | ![下](../Images/debf225a26fbacce2c4ec53046250054.png)
    | ![右](../Images/0da7cb1170753229a83fadd201a3e542.png) | ![左](../Images/3b29475fcaa612f8eac811c79149eb41.png)
    |'
- en: '| 0 | 0 | 1 | 0 | 0 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 1 | 0 | 0 |'
- en: '| .. |  |  |  |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| .. |  |  |  |  |'
- en: '| 250 | -2.07469 | -2.34655 | -1.99878 | -2.03458 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 250 | -2.07469 | -2.34655 | -1.99878 | -2.03458 |'
- en: '| .. |  |  |  |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| .. |  |  |  |  |'
- en: '| 500 | 11.47930 | 7.23467 | 13.47290 | 9.53478 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 500 | 11.47930 | 7.23467 | 13.47290 | 9.53478 |'
- en: Q-learning Process
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q学习过程
- en: 1\. Initialize the Q-Table
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 初始化Q表
- en: The first step is creating the Q-table
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建Q表
- en: n = number of actions
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: n = 动作数量
- en: m = number of states
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: m = 状态数量
- en: For example, n could be left, right, up, or down, whilst m could be start, idle,
    correct move, wrong move, and end in a game.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，n可以是左、右、上或下，而m可以是在游戏中的开始、空闲、正确动作、错误动作和结束。
- en: 2\. Choose and Perform an Action
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 选择并执行一个动作
- en: Our Q-table should all have 0’s as no action has been performed. We then choose
    an action and update it in our Q-table in the correct section. This states that
    the action has been performed.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Q表应该全部为0，因为尚未执行任何动作。然后，我们选择一个动作并在Q表中正确的部分更新它。这表示动作已被执行。
- en: 3\. Calculate the Q-Value using Bellman Equation
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 使用贝尔曼方程计算Q值
- en: Using the Bellman Equation, calculate the value of the actual reward and the
    Q-value for the action just performed.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝尔曼方程，计算实际奖励的值以及刚刚执行的动作的 Q 值。
- en: 4\. Continue Steps 2 and 3
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 继续步骤 2 和 3
- en: Repeat Steps 2 and 3 till an episode ends or the Q-table is filled.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 重复步骤 2 和 3 直到一个回合结束或 Q 表被填满。
- en: '![flow chart of q learning](../Images/4f3743ac9dded346f3b4b2cb7b6ec51d.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![q 学习流程图](../Images/4f3743ac9dded346f3b4b2cb7b6ec51d.png)'
- en: 'Source: Authors image'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者图片
- en: Conclusion
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'This is a simple guide for beginners, if you want a better understanding of
    Q-learning and Reinforcement Learning, have a read of this book: [Reinforcement
    Learning: An Introduction (2nd Edition)](https://www.amazon.com/dp/0262039249/)
    by Richard S. Sutton and Andrew G. Barto.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的初学者指南，如果你想更好地理解 Q 学习和强化学习，可以阅读这本书：[强化学习：导论（第2版）](https://www.amazon.com/dp/0262039249/)
    作者为 Richard S. Sutton 和 Andrew G. Barto。
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and Freelance Technical Writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**[尼莎·阿里亚](https://www.linkedin.com/in/nisha-arya-ahmed/)** 是一名数据科学家和自由技术写作人。她特别感兴趣于提供数据科学职业建议或教程，以及数据科学相关的理论知识。她还希望探索人工智能如何对人类寿命产生好处。作为一个渴望学习的人，她寻求拓宽自己的技术知识和写作技能，同时帮助指导他人。'
- en: More On This Topic
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[A Beginner''s Guide to End to End Machine Learning](https://www.kdnuggets.com/2021/12/beginner-guide-end-end-machine-learning.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[初学者的端到端机器学习指南](https://www.kdnuggets.com/2021/12/beginner-guide-end-end-machine-learning.html)'
- en: '[Essential Machine Learning Algorithms: A Beginner''s Guide](https://www.kdnuggets.com/2021/05/essential-machine-learning-algorithms-beginners.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[重要的机器学习算法：初学者指南](https://www.kdnuggets.com/2021/05/essential-machine-learning-algorithms-beginners.html)'
- en: '[A Beginner''s Guide to the Top 10 Machine Learning Algorithms](https://www.kdnuggets.com/a-beginner-guide-to-the-top-10-machine-learning-algorithms)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[初学者的十大机器学习算法指南](https://www.kdnuggets.com/a-beginner-guide-to-the-top-10-machine-learning-algorithms)'
- en: '[Beginner’s Guide to Machine Learning with Python](https://www.kdnuggets.com/beginners-guide-to-machine-learning-with-python)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 进行机器学习的初学者指南](https://www.kdnuggets.com/beginners-guide-to-machine-learning-with-python)'
- en: '[Beginner’s Guide to Machine Learning Testing With DeepChecks](https://www.kdnuggets.com/beginners-guide-to-machine-learning-testing-with-deepchecks)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 DeepChecks 进行机器学习测试的初学者指南](https://www.kdnuggets.com/beginners-guide-to-machine-learning-testing-with-deepchecks)'
- en: '[Beginner’s Guide to Careers in AI and Machine Learning](https://www.kdnuggets.com/beginners-guide-to-careers-in-ai-and-machine-learning)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[初学者的 AI 和机器学习职业指南](https://www.kdnuggets.com/beginners-guide-to-careers-in-ai-and-machine-learning)'
