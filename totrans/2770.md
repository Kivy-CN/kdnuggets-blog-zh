# 机器学习模型的超参数优化

> 原文：[https://www.kdnuggets.com/2020/05/hyperparameter-optimization-machine-learning-models.html](https://www.kdnuggets.com/2020/05/hyperparameter-optimization-machine-learning-models.html)

[评论](#comments)![图](../Images/e60360ab2907502e8bfd8adc536d14bf.png)

[致谢](https://swisscognitive.ch/2020/01/13/machine-learning-in-adversity/)

### 引言

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析能力

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的 IT

* * *

模型优化是实现机器学习解决方案的最艰巨挑战之一。整个机器学习和深度学习理论的分支都致力于模型的优化。

机器学习中的超参数优化旨在找到一组超参数，这些参数在验证集上能提供最佳性能。与模型参数不同，超参数由机器学习工程师在训练之前设置。随机森林中的树木数量是一个超参数，而神经网络中的权重是训练期间学习到的模型参数。我喜欢将超参数视为需要调整的模型设置，以便模型能够最佳地解决机器学习问题。

一些模型超参数的例子包括：

+   训练神经网络的 `learning rate`。

+   `**C**` 和 `**????**` 支持向量机的超参数。

+   `**k**` 在 k-最近邻算法中。

超参数优化寻找一组超参数组合，返回一个最佳模型，从而减少预定义的损失函数，并提高在给定独立数据上的准确性。

![图](../Images/1a76857e56c4740636dc8f28879c8971.png)

[分类模型及其相应的超参数。](https://www.computer.org/csdl/journal/ts/2018/06/07990590/13rRUx0gerA)

### 超参数优化方法

超参数可以直接影响机器学习算法的训练。因此，为了实现最大性能，了解如何优化它们非常重要。以下是一些优化超参数的常见策略：

### **1\. 手动超参数调整**

传统上，超参数通过试错手动调整。这**仍然**很常见，并且经验丰富的工程师可以“猜测”出能够为ML模型提供非常高准确率的参数值。然而，仍在不断寻找更好、更快和更自动化的超参数优化方法。

### 2\. 网格搜索

网格搜索可以说是最基础的超参数调优方法。使用这种技术，我们简单地为所有提供的超参数值的每一个可能组合构建一个模型，评估每个模型，然后选择产生最佳结果的架构。

![](../Images/0d65f74594b1f825f12beb47db397245.png)

网格搜索不仅仅适用于一种模型类型，而是可以跨机器学习应用，以计算给定模型的最佳参数。

例如，配备RBF核的典型软边界SVM分类器有至少两个需要优化的超参数，以在未见数据上获得良好性能：正则化常数*C*和一个核超参数γ。这两个参数都是连续的，因此为了执行网格搜索，需要为每个参数选择一个有限的“合理”值，假设

![](../Images/e78be57e455f5dfa0eac00be4c135791.png)

网格搜索然后在这两个集合的c[笛卡尔积](https://www.wikiwand.com/en/Cartesian_product)中对每一对(*C*, γ)训练一个SVM，并在保留的验证集上评估它们的表现（或者在训练集上进行内部交叉验证，此情况下每对参数会训练多个SVM）。最后，网格搜索算法输出在验证过程中获得最高分的设置。

**它在Python中如何工作？**

[PRE0]

这里是使用来自`sklearn`库的`[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)`的网格搜索的Python实现。

[PRE1]

适应网格搜索：

[PRE2]

在网格搜索中运行的方法：

[PRE3]

然后使用在网格搜索中选择的最佳超参数值集，如上所示，在实际模型中应用。

网格搜索的一个**缺点**是当涉及到维度时，当超参数的数量指数级增长时会遭遇困难。然而，并没有保证搜索会产生完美的解决方案，因为它通常通过在正确的集合周围进行别名来找到一个解。

### 3\. 随机搜索

通常一些超参数比其他的更为重要。进行随机搜索而不是网格搜索可以更精确地发现重要参数的良好值。

![](../Images/8292a0cb03eeaa9f5cf4f473a2581516.png)

随机搜索设置一个超参数值的网格，并选择随机组合来训练模型和评分。这允许你明确控制尝试的参数组合数量。搜索迭代的次数是基于时间或资源设置的。Scikit Learn提供了`RandomizedSearchCV`函数来实现这一过程。

尽管`RandomizedSearchCV`可能不会找到像`GridSearchCV`那样准确的结果，但它惊人地更常选择最佳结果，并且用`GridSearchCV`所需时间的*一小部分*。在相同资源下，随机搜索甚至可以超越网格搜索。这可以在下图中通过使用连续参数来可视化。

随机搜索找到最佳参数的机会相对较高，因为随机搜索模式下，模型可能会被训练在优化的参数上而没有任何混叠。随机搜索最适合低维数据，因为找到正确集合所需的时间较少，迭代次数也较少。对于维度较少的情况，随机搜索是最好的参数搜索技术。

在深度学习算法的情况下，它超越了网格搜索。

![图示](../Images/b1b1600a34c9a27779b36bbb9d9a02cc.png)

[致谢](https://community.alteryx.com/t5/Data-Science-Blog/Hyperparameter-Tuning-Black-Magic/ba-p/449289)

在上图中，如果你有两个参数，5x6 网格搜索只检查每个参数的5个不同值（左侧图中的六行五列），而随机搜索则检查每个参数的14个不同值。

**它在 Python 中是如何工作的？**

[PRE4]

这里是使用`sklearn`库的`RandomizedSearchCV`进行网格搜索的 Python 实现。

[PRE5]

随机搜索拟合：

[PRE6]

网格搜索运行方法：

[PRE7]

### 4\. 贝叶斯优化

前两种方法执行了各自的实验，构建了具有不同超参数值的模型，并记录了每个模型的性能。由于每个实验都是独立进行的，因此很容易并行化这个过程。然而，由于每个实验都是独立的，我们无法利用一个实验的信息来改进下一个实验。贝叶斯优化属于*基于模型的序列优化*（SMBO）算法的一类，它允许我们利用前一次迭代的结果来改进下一个实验的采样方法。

这反过来限制了模型需要为验证而训练的次数，因为只有那些预计会产生更高验证得分的设置才会被传递进行评估。

贝叶斯优化通过构建一个后验分布（高斯过程），来最佳描述你想要优化的函数。随着观察数量的增加，后验分布会得到改善，算法对哪些参数空间的区域值得探索以及哪些不值得探索变得更加确定。

我们可以在下面的图像中看到这一点：

![图示](../Images/23d29016155406c3b70ff0644e409dd3.png)

来源：[bayesian-optimization](https://github.com/fmfn/BayesianOptimization)

当你不断迭代时，算法在探索和利用的需求之间取得平衡，考虑到它对目标函数的了解。在每一步，都会将高斯过程拟合到已知样本（之前探索过的点），然后结合探索策略（如 UCB（上置信界）或 EI（期望改进）），来确定下一个应该探索的点。

使用贝叶斯优化，我们可以更智能地探索参数空间，从而减少完成这一过程所需的时间。

你可以查看下面的贝叶斯优化的 Python 实现：

[**thuijskens/bayesian-optimization**](https://github.com/thuijskens/bayesian-optimization/blob/master/ipython-notebooks/svm-optimization.ipynb)

### 5\. 基于梯度的优化

它特别用于神经网络的情况。它计算超参数的梯度，并使用梯度下降算法优化它们。

计算梯度是问题中最小的部分。至少在先进的 [自动微分](https://en.wikipedia.org/wiki/Automatic_differentiation) 软件时代。（**以通用方式为所有 sklearn 分类器实现这一点，当然是不容易的**）。

![图示](../Images/43a7177eaea30f7a4c2b619f758eb304.png)

[致谢](https://www.mathworks.com/matlabcentral/fileexchange/27631-derivative-based-optimization)

虽然已经有一些人使用过这种理念的研究，但他们仅限于一些特定且明确的问题（例如 SVM 调优）。此外，可能有许多假设，因为：

> 为什么这不是一个好主意？

**1\. 超参数优化通常是不平滑的**

+   GD 非常喜欢平滑的函数，因为零梯度没有帮助。

+   每一个由离散集合定义的超参数（例如选择 l1 或 l2 惩罚）都会引入不平滑的表面。

**2\. 超参数优化通常是非凸的**

+   梯度下降的整个收敛理论假设底层问题是凸的。

+   最佳情况：你得到一些局部最小值（可能非常差）。

+   最坏的情况：梯度下降甚至无法收敛到局部最小值。

要获取 Python 实现和更多关于梯度下降优化算法的信息 [点击这里。](https://hackernoon.com/implementing-different-variants-of-gradient-descent-optimization-algorithm-in-python-using-numpy-809e7ab3bab4)

### **6\. 进化优化**

进化优化遵循一种受生物进化概念启发的过程，由于自然进化是一个在不断变化的环境中进行的动态过程，因此它们也非常适合动态优化问题。

![](../Images/801484956ac7dd0a3414aecce35df824.png)

进化算法通常用于寻找无法通过其他技术轻松解决的良好近似解。优化问题通常没有精确的解决方案，因为找到最优解决方案可能过于耗时和计算密集。然而，在这种情况下，进化算法是理想的，因为它们可以用来找到一个接近最优的解决方案，这通常是足够的。

进化算法的一个优点是它们能够开发出没有任何人为误解或偏见的解决方案，这意味着它们可以产生一些我们自己可能永远无法想到的惊人想法。

你可以在[这里](https://en.wikipedia.org/wiki/Evolutionary_algorithm)了解更多关于进化算法的内容。你也可以在[这里](https://github.com/MorvanZhou/Evolutionary-Algorithm)查看 Python 实现。

> 一般来说，每当你想优化超参数调优时，就想想网格搜索和随机搜索！

### 结论

在这篇文章中，我们了解到找到超参数的正确值可能是一个令人沮丧的任务，并可能导致机器学习模型的欠拟合或过拟合。我们看到如何通过使用网格搜索、随机搜索和其他算法来克服这一障碍——这些算法优化超参数调优，以节省时间并消除随机猜测导致的过拟合或欠拟合的可能性。

好了，这篇文章就到此为止**。**希望你们喜欢阅读这篇文章，欢迎在评论区分享你们的意见/想法/反馈。

感谢阅读！！！

**简介：[纳戈什·辛格·乔汉](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)** 是一位数据科学爱好者。对大数据、Python 和机器学习感兴趣。

[原文](https://medium.com/swlh/hyperparameter-optimization-for-machine-learning-models-12582f00ae52)。转载已获许可。

**相关：**

+   [如何在 3 个简单步骤中对任何 Python 脚本进行超参数调优](/2020/04/hyperparameter-tuning-python.html)

+   [使用 Biopython 进行冠状病毒 COVID-19 基因组分析](/2020/04/coronavirus-covid-19-genome-analysis-biopython.html)

+   [实用的超参数优化](/2020/02/practical-hyperparameter-optimization.html)

### 更多相关主题

+   [超参数优化：10 个顶级 Python 库](https://www.kdnuggets.com/2023/01/hyperparameter-optimization-10-top-python-libraries.html)

+   [使用 TPOT 进行机器学习管道优化](https://www.kdnuggets.com/2021/05/machine-learning-pipeline-optimization-tpot.html)

+   [使用网格搜索和随机搜索进行超参数调优](https://www.kdnuggets.com/2022/10/hyperparameter-tuning-grid-search-random-search-python.html)

+   [超参数调优：GridSearchCV 和 RandomizedSearchCV 解释](https://www.kdnuggets.com/hyperparameter-tuning-gridsearchcv-and-randomizedsearchcv-explained)

+   [SQL 查询优化技巧](https://www.kdnuggets.com/2023/03/sql-query-optimization-techniques.html)

+   [数据库优化：探索 SQL 中的索引](https://www.kdnuggets.com/2023/07/database-optimization-exploring-indexes-sql.html)
