- en: 'RAG vs Finetuning: Which Is the Best Tool to Boost Your LLM Application?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG 与微调：哪个是提升您的 LLM 应用的最佳工具？
- en: 原文：[https://www.kdnuggets.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application](https://www.kdnuggets.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application](https://www.kdnuggets.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application)
- en: '![RAG vs Finetuning: Which Is the Best Tool to Boost Your LLM Application?](../Images/4a9cdca48c03b38c0ecbe7d5894b1d61.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![RAG 与微调：哪个是提升您的 LLM 应用的最佳工具？](../Images/4a9cdca48c03b38c0ecbe7d5894b1d61.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Prologue
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的快车道'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您组织中的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'As the wave of interest in Large Language Models (LLMs) surges, many developers
    and organisations are busy building applications harnessing their power. However,
    when the pre-trained LLMs out of the box don’t perform as expected or hoped, the
    question on how to improve the performance of the LLM application. And eventually
    we get to the point of where we ask ourselves: Should we use [Retrieval-Augmented
    Generation](https://arxiv.org/abs/2005.11401) (RAG) or model finetuning to improve
    the results?'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对大型语言模型（LLMs）的兴趣激增，许多开发者和组织忙于构建利用其强大功能的应用程序。然而，当现成的预训练 LLM 的表现未达到预期时，如何提升
    LLM 应用的性能成为一个问题。最终我们会问自己：我们应该使用[检索增强生成](https://arxiv.org/abs/2005.11401)（RAG）还是模型微调来改善结果？
- en: 'Before diving deeper, let’s demystify these two methods:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨之前，让我们解开这两种方法的神秘面纱：
- en: '**RAG**: This approach integrates the power of retrieval (or searching) into
    LLM text generation. It combines a retriever system, which fetches relevant document
    snippets from a large corpus, and an LLM, which produces answers using the information
    from those snippets. In essence, RAG helps the model to “look up” external information
    to improve its responses.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**RAG**：这种方法将检索（或搜索）的力量整合到 LLM 文本生成中。它结合了一个检索系统，从大型语料库中提取相关文档片段，以及一个 LLM，利用这些片段中的信息生成回答。实质上，RAG
    帮助模型“查找”外部信息以改善其回应。'
- en: '![RAG vs Finetuning: Which Is the Best Tool to Boost Your LLM Application?](../Images/4385d321c30dc36b47d8ba379eb8148d.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![RAG 与微调：哪个是提升您的 LLM 应用的最佳工具？](../Images/4385d321c30dc36b47d8ba379eb8148d.png)'
- en: Image by Author
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: '**Finetuning**: This is the process of taking a pre-trained LLM and further
    training it on a smaller, specific dataset to adapt it for a particular task or
    to improve its performance. By finetuning, we are adjusting the model’s weights
    based on our data, making it more tailored to our application’s unique needs.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**微调**：这是在较小的特定数据集上进一步训练预训练 LLM 的过程，以使其适应特定任务或提升其性能。通过微调，我们根据我们的数据调整模型的权重，使其更加符合我们应用的独特需求。'
- en: '![RAG vs Finetuning: Which Is the Best Tool to Boost Your LLM Application?](../Images/040e9b6d708df60dcae9372abcb76700.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![RAG 与微调：哪个是提升您的 LLM 应用的最佳工具？](../Images/040e9b6d708df60dcae9372abcb76700.png)'
- en: Image by Author
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Both RAG and finetuning serve as powerful tools in enhancing the performance
    of LLM-based applications, but they address different aspects of the optimisation
    process, and this is crucial when it comes to choosing one over the other.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 和微调都作为增强基于 LLM 的应用性能的强大工具，但它们关注优化过程中的不同方面，这在选择使用哪一种工具时至关重要。
- en: 'Previously, I would often suggest to organisations that they experiment with
    RAG before diving into finetuning. This was based on my perception that both approaches
    achieved similar results but varied in terms of complexity, cost, and quality.
    I even used to illustrate this point with diagrams such as this one:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，我通常会建议组织在深入微调之前先尝试RAG。这是基于我认为这两种方法实现了类似的结果，但在复杂性、成本和质量上有所不同。我甚至用这样的图示来说明这一点：
- en: '![RAG vs Finetuning: Which Is the Best Tool to Boost Your LLM Application?](../Images/26546386b946f85cacef089a7a757a70.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![RAG与微调：哪个是提升LLM应用的最佳工具？](../Images/26546386b946f85cacef089a7a757a70.png)'
- en: Image by Author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'In this diagram, various factors like complexity, cost, and quality are represented
    along a single dimension. The takeaway? RAG is simpler and less expensive, but
    its quality might not match up. My advice usually was: start with RAG, gauge its
    performance, and if found lacking, shift to finetuning.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，复杂性、成本和质量等各种因素沿着一个维度表示。要点是？RAG更简单且成本更低，但其质量可能无法匹配。我的建议通常是：从RAG开始，评估其性能，如果表现不佳，再转向微调。
- en: However, my perspective has since evolved. I believe it’s an oversimplification
    to view RAG and finetuning as two techniques that achieve the same result, just
    where one is just cheaper and less complex than the other. They are fundamentally
    distinct — instead of *co-linear* they are actually *orthogonal *— and serve different
    requirements of an LLM application.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我的观点已经发生了变化。我认为将RAG和微调视为实现相同结果的两种技术，只不过一种比另一种更便宜、更简单，是一种过于简化的看法。它们在本质上是不同的——它们实际上是*正交*的，而不是*共线*的——并且服务于LLM应用的不同需求。
- en: 'To make this clearer, consider a simple real-world analogy: When posed with
    the question, “Should I use a knife or a spoon to eat my meal?”, the most logical
    counter-question is: “Well, what are you eating?” I asked friends and family this
    question and everyone instinctively replied with that counter-question, indicating
    that they don’t view the knife and spoon as interchangeable, or one as an inferior
    variant of the other.'
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了更清楚地说明这一点，可以考虑一个简单的现实世界类比：当被问到“我应该用刀子还是勺子来吃饭？”时，最合逻辑的反问是：“你在吃什么？”我问过朋友和家人这个问题，每个人都本能地用那个反问来回答，表明他们并不认为刀子和勺子是可以互换的，或者其中一个是另一个的劣等变体。
- en: What is this about?
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这到底是关于什么的？
- en: In this blog post, we’ll dive deep into the nuances that differentiate RAG and
    finetuning across various dimensions that are, in my opinion, crucial in determining
    the optimal technique for a specific task. Moreover, we’ll be looking at some
    of the most popular use cases for LLM applications and use the dimensions established
    in the first part to identify which technique might be best suited for which use
    case. In the last part of this blog post we will identify additional aspects that
    should be considered when building LLM applications. Each one of those might warrant
    its own blog post and therefore we can only touch briefly on them in the scope
    of this post.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我们将深入探讨在各个维度上区分RAG和微调的细微差别，这些维度在我看来对确定特定任务的最佳技术至关重要。此外，我们还会查看一些最受欢迎的LLM应用案例，并利用第一部分中建立的维度来识别哪种技术可能最适合哪个用例。在博客的最后部分，我们将识别在构建LLM应用时应考虑的额外方面。每一个方面可能都值得单独写一篇博客，因此在这篇文章中我们只能简要提及它们。
- en: Why should you care?
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么你需要关心这些？
- en: 'Choosing the right technique for adapting large language models can have a
    major impact on the success of your NLP applications. Selecting the wrong approach
    can lead to:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的技术来调整大型语言模型对NLP应用的成功有重大影响。选择错误的方法可能会导致：
- en: Poor model performance on your specific task, resulting in inaccurate outputs.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在特定任务上的模型性能差，导致输出不准确。
- en: Increased compute costs for model training and inference if the technique is
    not optimized for your use case.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果技术未针对您的用例进行优化，模型训练和推理的计算成本会增加。
- en: Additional development and iteration time if you need to pivot to a different
    technique later on.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果之后需要转向不同的技术，会增加额外的开发和迭代时间。
- en: Delays in deploying your application and getting it in front of users.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署应用程序和将其展示给用户的延迟。
- en: A lack of model interpretability if you choose an overly complex adaptation
    approach.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果选择了过于复杂的适应方法，可能缺乏模型可解释性。
- en: Difficulty deploying the model to production due to size or computational constraints.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于大小或计算限制，部署模型到生产环境中的困难。
- en: The nuances between RAG and finetuning span model architecture, data requirements,
    computational complexity, and more. **Overlooking these details can derail your
    project timeline and budget.**
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: RAG和微调之间的细微差别涵盖了模型架构、数据需求、计算复杂性等方面。**忽视这些细节可能会影响项目的时间线和预算。**
- en: This blog post aims to prevent wasted effort by clearly laying out when each
    technique is advantageous. With these insights, you can hit the ground running
    with the right adaptation approach from day one. The detailed comparison will
    equip you to make the optimal technology choice to achieve your business and AI
    goals. **This guide to selecting the right tool for the job will set your project
    up for success.**
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本博客文章旨在通过清晰地阐述每种技术何时具有优势来防止浪费精力。通过这些见解，你可以从第一天开始就用正确的适配方法取得进展。详细的比较将使你能够做出最佳技术选择，实现业务和AI目标。**本指南将帮助你选择合适的工具，为你的项目成功奠定基础。**
- en: So let’s dive in!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们深入探讨吧！
- en: Key considerations for boosting performance
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升性能的关键考虑因素
- en: Before we choose RAG vs Fintuning, we should assess the requirements of our
    LLM project along some dimensions and ask ourselves a few questions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择RAG还是微调之前，我们应该根据一些维度评估LLM项目的要求，并问自己几个问题。
- en: Does our use case require access to external data sources?
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的用例是否需要访问外部数据源？
- en: When choosing between finetuning an LLM or using RAG, one key consideration
    is whether the application requires access to external data sources. If the answer
    is yes, RAG is likely the better option.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择微调LLM还是使用RAG时，一个关键考虑因素是应用是否需要访问外部数据源。如果答案是肯定的，RAG可能是更好的选择。
- en: RAG systems are, by definition, designed to augment an LLM’s capabilities by
    retrieving relevant information from knowledge sources before generating a response.
    This makes this technique well-suited for applications that need to query databases,
    documents, or other structured/unstructured data repositories. The retriever and
    generator components can be optimised to leverage these external sources.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: RAG系统本质上设计用于通过在生成响应前从知识源中检索相关信息来增强LLM的能力。这使得这种技术非常适合需要查询数据库、文档或其他结构化/非结构化数据存储的应用。检索器和生成器组件可以被优化以利用这些外部资源。
- en: In contrast, while it is possible to finetune an LLM to learn some external
    knowledge, doing so requires a large labelled dataset of question-answer pairs
    from the target domain. This dataset must be updated as the underlying data changes,
    making it impractical for frequently changing data sources. The finetuning process
    also does not explicitly model the retrieval and reasoning steps involved in querying
    external knowledge.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，虽然可以通过微调LLM来学习一些外部知识，但这样做需要大量来自目标领域的标注数据集。这个数据集必须随着基础数据的变化而更新，这使得它在频繁变化的数据源中不切实际。微调过程也没有明确建模在查询外部知识时涉及的检索和推理步骤。
- en: So in summary, if our application needs to leverage external data sources, using
    a RAG system will likely be more effective and scalable than attempting to “bake
    in” the required knowledge through finetuning alone.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，如果我们的应用需要利用外部数据源，那么使用RAG系统通常比仅通过微调“嵌入”所需知识更有效且可扩展。
- en: Do we need to modify the model’s behaviour, writing style, or domain-specific
    knowledge?
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们是否需要修改模型的行为、写作风格或领域特定知识？
- en: Another very important aspect to consider is how much we need the model to adjust
    its behaviour, its writing style, or tailor its responses for domain-specific
    applications.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常重要的方面是我们需要模型调整其行为、写作风格或为领域特定应用定制响应的程度。
- en: Finetuning excels in its ability to adapt an LLM’s behaviour to specific nuances,
    tones, or terminologies. If we want the model to sound more like a medical professional,
    write in a poetic style, or use the jargon of a specific industry, finetuning
    on domain-specific data allows us to achieve these customisations. This ability
    to influence the model’s behaviour is essential for applications where alignment
    with a particular style or domain expertise is vital.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 微调在于能够将LLM的行为适应特定的细微差别、语调或术语。如果我们希望模型听起来更像医疗专业人士，采用诗意风格，或使用特定行业的术语，微调领域特定数据可以实现这些定制。影响模型行为的能力对于需要与特定风格或领域专业知识一致的应用至关重要。
- en: RAG, while powerful in incorporating external knowledge, primarily focuses on
    information retrieval and doesn’t inherently adapt its linguistic style or domain-specificity
    based on the retrieved information. It will pull relevant content from the external
    data sources but might not exhibit the tailored nuances or domain expertise that
    a finetuned model can offer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 虽然在融入外部知识方面非常强大，但主要关注于信息检索，并不会根据检索到的信息固有地调整其语言风格或领域特异性。它会从外部数据源中提取相关内容，但可能不会表现出微调模型所能提供的量身定制的细微差别或领域专业知识。
- en: So, if our application demands specialised writing styles or deep alignment
    with domain-specific vernacular and conventions, finetuning presents a more direct
    route to achieving that alignment. It offers the depth and customisation necessary
    to genuinely resonate with a specific audience or expertise area, ensuring the
    generated content feels authentic and well-informed.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果我们的应用程序需要专业的写作风格或与特定领域的术语和惯例深度对齐，微调提供了更直接的途径来实现这种对齐。它提供了必要的深度和定制，以真正与特定受众或专业领域产生共鸣，确保生成的内容感觉真实且信息丰富。
- en: Quick recap
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速回顾
- en: These two aspects are by far the most important ones to consider when deciding
    which method to use to boost LLM application performance. Interestingly, they
    are, in my opinion, orthogonal and can be used independently (and also be combined).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定使用哪种方法来提升 LLM 应用性能时，这两个方面是迄今为止最重要的方面。有趣的是，在我看来，它们是正交的，可以独立使用（也可以结合使用）。
- en: '![RAG vs Finetuning: Which Is the Best Tool to Boost Your LLM Application?](../Images/f98da1a2300a5db0adeb8c08cf267999.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![RAG vs Finetuning: Which Is the Best Tool to Boost Your LLM Application?](../Images/f98da1a2300a5db0adeb8c08cf267999.png)'
- en: Image by Author
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'However, before diving into the use cases, there are a few more key aspects
    we should consider before choosing a method:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在深入使用案例之前，我们应该考虑一些关键方面：
- en: How crucial is it to suppress hallucinations?
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抑制幻觉的重要性有多大？
- en: One downside of LLMs is their tendency to hallucinate — making up facts or details
    that have no basis in reality. This can be highly problematic in applications
    where accuracy and truthfulness are critical.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的一个缺点是它们容易产生幻觉——编造没有现实依据的事实或细节。这在准确性和真实性至关重要的应用中可能会非常成问题。
- en: Finetuning can help reduce hallucinations to some extent by grounding the model
    in a specific domain’s training data. However, the model may still fabricate responses
    when faced with unfamiliar inputs. Retraining on new data is required to continuously
    minimise false fabrications.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 微调可以通过将模型固定在特定领域的训练数据中来帮助减少幻觉。然而，当面临不熟悉的输入时，模型仍可能会编造回应。需要在新数据上重新训练，以持续最小化虚假的编造。
- en: In contrast, RAG systems are inherently less prone to hallucination because
    they ground each response in retrieved evidence. The retriever identifies relevant
    facts from the external knowledge source before the generator constructs the answer.
    This retrieval step acts as a fact-checking mechanism, reducing the model’s ability
    to confabulate. The generator is constrained to synthesise a response supported
    by the retrieved context.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，RAG 系统天生更不容易产生幻觉，因为它们将每个响应基于检索到的证据。检索器在生成器构造答案之前会从外部知识源中识别相关事实。这个检索步骤充当了事实检查机制，减少了模型的编造能力。生成器被限制在检索到的上下文支持的回应中合成答案。
- en: So in applications where suppressing falsehoods and imaginative fabrications
    is vital, RAG systems provide in-built mechanisms to minimise hallucinations.
    The retrieval of supporting evidence prior to response generation gives RAG an
    advantage in ensuring factually accurate and truthful outputs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在抑制虚假和虚构内容至关重要的应用中，RAG 系统提供了内置的机制来最小化幻觉。响应生成之前的证据检索使 RAG 在确保事实准确和真实输出方面具有优势。
- en: How much labelled training data is available?
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有多少标注的训练数据可用？
- en: When deciding between RAG and finetuning, a crucial factor to consider is the
    volume of domain- or task-specific, labelled training data at our disposal.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RAG 和微调之间做出决定时，一个关键因素是我们手头拥有的领域或任务特定的标注训练数据的量。
- en: Finetuning an LLM to adapt to specific tasks or domains is heavily dependent
    on the quality and quantity of the labelled data available. A rich dataset can
    help the model deeply understand the nuances, intricacies, and unique patterns
    of a particular domain, allowing it to generate more accurate and contextually
    relevant responses. However, if we’re working with a limited dataset, the improvements
    from finetuning might be marginal. In some cases, a scant dataset might even lead
    to overfitting, where the model performs well on the training data but struggles
    with unseen or real-world inputs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对LLM进行微调以适应特定任务或领域，严重依赖于可用标记数据的质量和数量。丰富的数据集可以帮助模型深入理解特定领域的细微差别、复杂性和独特模式，从而生成更准确和上下文相关的响应。然而，如果我们处理的是有限的数据集，微调带来的改进可能会很有限。在某些情况下，稀缺的数据集甚至可能导致过拟合，即模型在训练数据上表现良好，但在未见或现实世界输入中表现不佳。
- en: On the contrary, RAG systems are independent from training data because they
    leverage external knowledge sources to retrieve relevant information. Even if
    we don’t have an extensive labelled dataset, a RAG system can still perform competently
    by accessing and incorporating insights from its external data sources. The combination
    of retrieval and generation ensures that the system remains informed, even when
    domain-specific training data is sparse.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，RAG系统独立于训练数据，因为它们利用外部知识源来检索相关信息。即使我们没有一个广泛的标记数据集，RAG系统仍能通过访问和整合其外部数据源的见解而表现出色。检索和生成的结合确保了系统在特定领域训练数据稀缺时仍保持知情。
- en: In essence, if we have a wealth of labelled data that captures the domain’s
    intricacies, finetuning can offer a more tailored and refined model behaviour.
    But in scenarios where such data is limited, a RAG system provides a robust alternative,
    ensuring the application remains data-informed and contextually aware through
    its retrieval capabilities.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，如果我们拥有捕捉领域复杂性的丰富标记数据，微调可以提供更量身定制和精细化的模型行为。但在这种数据有限的情况下，RAG系统提供了一个稳健的替代方案，通过其检索能力确保应用保持数据驱动和上下文敏感。
- en: How static/dynamic is the data?
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据的静态/动态特性如何？
- en: Another fundamental aspect to consider when choosing between RAG and finetuning
    is the dynamic nature of our data. How frequently is the data updated, and how
    imperative is it for the model to stay current?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 选择RAG和微调之间的另一个基本方面是数据的动态特性。数据更新的频率如何？模型保持最新状态有多重要？
- en: Finetuning an LLM on a specific dataset means the model’s knowledge becomes
    a static snapshot of that data at the time of training. If the data undergoes
    frequent updates, changes, or expansions, this can quickly render the model outdated.
    To keep the LLM current in such dynamic environments, we’d have to retrain it
    frequently, a process that can be both time-consuming and resource-intensive.
    Additionally, each iteration requires careful monitoring to ensure that the updated
    model still performs well across different scenarios and hasn’t developed new
    biases or gaps in understanding.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对特定数据集进行LLM微调意味着模型的知识在训练时成为该数据的静态快照。如果数据经常更新、变化或扩展，这可能很快使模型过时。为了在这种动态环境中保持LLM的最新状态，我们必须频繁地对其进行再训练，这一过程既耗时又资源密集。此外，每次迭代都需要仔细监控，以确保更新后的模型在不同场景下仍表现良好，并且没有产生新的偏见或理解上的缺口。
- en: In contrast, RAG systems inherently possess an advantage in environments with
    dynamic data. Their retrieval mechanism constantly queries external sources, ensuring
    that the information they pull in for generating responses is up-to-date. As the
    external knowledge bases or databases update, the RAG system seamlessly integrates
    these changes, maintaining its relevance without the need for frequent model retraining.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，RAG系统在动态数据环境中固有的优势。它们的检索机制不断查询外部来源，确保用于生成响应的信息是最新的。随着外部知识库或数据库的更新，RAG系统无缝地整合这些变化，保持其相关性，而无需频繁的模型再训练。
- en: In summary, if we’re grappling with a rapidly evolving data landscape, RAG offers
    an agility that’s hard to match with traditional finetuning. By always staying
    connected to the most recent data, RAG ensures that the responses generated are
    in tune with the current state of information, making it an ideal choice for dynamic
    data scenarios.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，如果我们面对的是快速变化的数据环境，RAG提供了传统微调难以匹敌的灵活性。通过始终保持与最新数据的连接，RAG确保生成的响应与当前信息状态保持一致，使其成为动态数据场景的理想选择。
- en: How transparent/interpretable does our LLM app need to be?
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的LLM应用需要多大的透明度/可解释性？
- en: The last aspect to consider is the degree to which we need insights into the
    model’s decision-making process.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个需要考虑的方面是我们需要多少关于模型决策过程的洞察。
- en: Finetuning an LLM, while incredibly powerful, operates like a black box, making
    the reasoning behind its responses more opaque. As the model internalises the
    information from the dataset, it becomes challenging to discern the exact source
    or reasoning behind each response. This can make it difficult for developers or
    users to trust the model’s outputs, especially in critical applications where
    understanding the “why” behind an answer is vital.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然微调LLM非常强大，但它的操作类似于黑箱，使得其响应背后的推理变得更加不透明。随着模型从数据集中内化信息，识别每个响应的确切来源或推理变得困难。这可能使开发者或用户难以信任模型的输出，特别是在需要理解回答“为什么”的关键应用中。
- en: RAG systems, on the other hand, offer a level of transparency that’s not typically
    found in solely finetuned models. Given the two-step nature of RAG — retrieval
    and then generation — users can peek into the process. The retrieval component
    allows for the inspection of which external documents or data points are selected
    as relevant. This provides a tangible trail of evidence or reference that can
    be evaluated to understand the foundation upon which a response is built. The
    ability to trace back a model’s answer to specific data sources can be invaluable
    in applications that demand a high degree of accountability or when there’s a
    need to validate the accuracy of the generated content.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，RAG系统提供了一种通常在单纯的微调模型中找不到的透明度。由于RAG的两步特性——检索和生成——用户可以窥见这一过程。检索组件允许检查哪些外部文档或数据点被选为相关内容。这提供了可以评估的实质证据或参考，以理解响应构建的基础。在需要高度问责或验证生成内容准确性的应用中，追溯模型答案到特定数据源的能力是无价的。
- en: In essence, if transparency and the ability to interpret the underpinnings of
    a model’s responses are priorities, RAG offers a clear advantage. By breaking
    down the response generation into distinct stages and allowing insight into its
    data retrieval, RAG fosters greater trust and understanding in its outputs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，如果透明度和解释模型响应基础的能力是优先考虑的，RAG提供了明确的优势。通过将响应生成分解为不同阶段并允许对数据检索的深入了解，RAG促进了对其输出的更大信任和理解。
- en: Summary
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Choosing between RAG and finetuning becomes more intuitive when considering
    these dimensions. If we need lean towards accessing external knowledge and valuing
    transparency, RAG is our go-to. On the other hand, if we’re working with stable
    labelled data and aim to adapt the model more closely to specific needs, finetuning
    is the better choice.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑这些维度时，选择RAG和微调变得更为直观。如果我们需要依赖外部知识并重视透明度，RAG是我们的首选。另一方面，如果我们处理的是稳定的标注数据并希望将模型更紧密地调整到特定需求，微调是更好的选择。
- en: '![RAG vs Finetuning: Which Is the Best Tool to Boost Your LLM Application?](../Images/e6ea79fc3c31b1216d50da3a1746bb25.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![RAG vs Finetuning: Which Is the Best Tool to Boost Your LLM Application?](../Images/e6ea79fc3c31b1216d50da3a1746bb25.png)'
- en: Image by Author
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: In the following section, we’ll see how we can assess popular LLM use cases
    based on these criteria.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将看到如何根据这些标准评估流行的LLM使用案例。
- en: Use cases
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用案例
- en: 'Let’s look at some popular use cases and how the above framework can be used
    to choose the right method:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一些流行的使用案例以及上述框架如何帮助选择合适的方法：
- en: Summarisation (in a specialised domain and/or a specific style)
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结（在专业领域和/或特定风格中）
- en: '**1\. External knowledge required?** For the task of summarizing in the style
    of previous summaries, the primary data source would be the previous summaries
    themselves. If these summaries are contained within a static dataset, there’s
    little need for continuous external data retrieval. However, if there’s a dynamic
    database of summaries that frequently updates and the goal is to continually align
    the style with the newest entries, RAG might be useful here.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 是否需要外部知识？** 对于以往总结风格的任务，主要的数据来源是以往的总结。如果这些总结包含在一个静态数据集中，则无需持续检索外部数据。然而，如果有一个经常更新的动态总结数据库，并且目标是不断将风格与最新条目对齐，那么RAG可能会在这里发挥作用。'
- en: '**2\. Model adaptation required?** The core of this use case revolves around
    adapting to a specialised domain or a and/or a specific writing style. Finetuning
    is particularly adept at capturing stylistic nuances, tonal variations, and specific
    domain vocabularies, making it an optimal choice for this dimension.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 需要模型适应吗？** 这个用例的核心围绕适应专业领域或特定写作风格。微调特别擅长捕捉风格细节、语气变化和特定领域的词汇，使其成为这个维度的*最佳选择*。'
- en: '**3\. Crucial to minimise hallucinations?** Hallucinations are problematic
    in most LLM applications, including summarisation. However, in this use case,
    the text to be summarised is typically provided as context. This makes hallucinations
    less of a concern compared to other use cases. The source text constrains the
    model, reducing imaginative fabrications. So while factual accuracy is always
    desirable, suppressing hallucinations is a lower priority for summarisation given
    the contextual grounding.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 是否关键要最小化幻觉？** 幻觉在大多数 LLM 应用中都是问题，包括总结。然而，在这个用例中，被总结的文本通常作为背景提供。这使得幻觉相较于其他用例的关注度较低。源文本限制了模型，减少了虚构。因此，虽然事实准确性总是重要的，但在有背景限制的情况下，抑制幻觉的优先级较低。'
- en: '**4\. Training data available?** If there’s a substantial collection of previous
    summaries that are labelled or structured in a way that the model can learn from
    them, finetuning becomes a very attractive option. On the other hand, if the dataset
    is limited, and we’re leaning on external databases for stylistic alignment, RAG
    could play a role, although its primary strength isn’t style adaptation.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 有可用的训练数据吗？** 如果有大量的标记或结构化的先前总结，模型可以从中学习，那么微调会变得非常有吸引力。另一方面，如果数据集有限，并且我们依赖外部数据库来进行风格对齐，RAG
    可能会发挥作用，尽管它的主要优势不在于风格适应。'
- en: '**5\. How dynamic is the data?** If the database of previous summaries is static
    or updates infrequently, the finetuned model’s knowledge will likely remain relevant
    for a longer time. However, if the summaries update frequently and there’s a need
    for the model to align with the newest stylistic changes continuously, RAG might
    have an edge due to its dynamic data retrieval capabilities.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**5\. 数据有多动态？** 如果先前总结的数据库是静态的或更新不频繁，微调模型的知识可能会保持较长时间的相关性。然而，如果总结频繁更新并且需要模型不断对齐最新的风格变化，RAG
    可能具有优势，因为其动态数据检索能力。'
- en: '**6\. Transparency/Interpretability required?** The primary goal here is stylistic
    alignment, so the “why” behind a particular summarisation style might be less
    critical than in other use cases. That said, if there’s a need to trace back and
    understand which previous summaries influenced a particular output, RAG offers
    a bit more transparency. Still, this might be a secondary concern for this use
    case.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**6\. 需要透明性/可解释性吗？** 主要目标是风格对齐，因此对特定总结风格的“为什么”可能没有其他用例那样关键。不过，如果需要追溯并了解哪些先前总结影响了特定输出，RAG
    提供了更多的透明性。尽管如此，这对这个用例来说可能是一个次要关注点。'
- en: '**Recommendation:** For this use case ***finetuning*** appears to be the more
    fitting choice. The primary objective is stylistic alignment, a dimension where
    finetuning shines. Assuming there’s a decent amount of previous summaries available
    for training, finetuning an LLM would allow for deep adaptation to the desired
    style, capturing the nuances and intricacies of the domain. However, if the summaries
    database is extremely dynamic and there’s value in tracing back influences, considering
    a hybrid approach or leaning towards RAG could be explored.'
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**推荐：** 对于这个用例，***微调*** 似乎是更合适的选择。主要目标是风格对齐，而微调在这一点上表现突出。假设有足够的先前总结可用于训练，微调
    LLM 将允许深入适应所需风格，捕捉领域的细微差别和复杂性。然而，如果总结数据库极其动态且追溯影响有价值，可以考虑混合方法或倾向于 RAG。'
- en: Question/answering system on organisational knowledge (i.e. external data)
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组织知识的问答系统（即外部数据）
- en: '**1\. External knowledge required?** A question/answering system relying on
    organisational knowledge bases inherently requires access to external data, in
    this case, the org’s internal databases and document stores. The system’s effectiveness
    hinges on its ability to tap into and retrieve relevant information from these
    sources to answer queries. Given this, RAG stands out as the more suitable choice
    for this dimension, as it’s designed to augment LLM capabilities by retrieving
    pertinent data from knowledge sources.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 需要外部知识？** 依赖于组织知识库的问题/回答系统本质上需要访问外部数据，在这种情况下，是组织的内部数据库和文档存储。系统的有效性取决于它从这些来源中提取和检索相关信息以回答查询的能力。因此，RAG在这个维度上更为适合，因为它设计用来通过从知识源中检索相关数据来增强大模型的能力。'
- en: '**2\. Model adaptation required?** Depending on the organization and its field,
    there might be a requirement for the model to align with specific terminologies,
    tones, or conventions. While RAG focuses primarily on information retrieval, finetuning
    can help the LLM adjust its responses to the company’s internal vernacular or
    the nuances of its domain. Thus, for this dimension, depending on the specific
    requirements finetuning might play a role.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 是否需要模型适配？** 根据组织及其领域，可能需要模型符合特定的术语、语气或惯例。虽然RAG主要关注信息检索，微调可以帮助大模型调整其回答以适应公司的内部术语或其领域的细微差别。因此，根据具体要求，微调可能在这个维度上发挥作用。'
- en: '**3\. Crucial to minimise hallucinations?** Hallucinations are a major concern
    in this use case, due to the knowledge-cutoff of LLMs. If the model is unable
    to answer a question based on the data it has been trained on, it will almost
    certainly revert to (partially or entirely) making up a plausible but incorrect
    answer.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 关键在于最小化虚假信息？** 虚假信息在这个应用场景中是一个主要问题，因为大模型的知识截止日期。如果模型无法基于其训练的数据回答问题，它几乎肯定会回退到（部分或完全）编造一个看似合理但不正确的答案。'
- en: '**4\. Training data available?** If the organization has a structured and labeled
    dataset of previously answered questions, this can bolster the finetuning approach.
    However, not all internal databases are labeled or structured for training purposes.
    In scenarios where the data isn’t neatly labeled or where the primary focus is
    on retrieving accurate and relevant answers, RAG’s ability to tap into external
    data sources without needing a vast labeled dataset makes it a compelling option.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 是否有可用的训练数据？** 如果组织有一个结构化且标记好的之前回答过的问题的数据集，这可以增强微调的方法。然而，并非所有内部数据库都被标记或结构化以用于训练。在数据没有被整齐标记或主要关注准确和相关答案的情况下，RAG能够从外部数据源中提取数据而不需要大量标记数据集，这使其成为一个有吸引力的选择。'
- en: '**5\. How dynamic is the data?** Internal databases and document stores in
    organisations can be highly dynamic, with frequent updates, changes, or additions.
    If this dynamism is characteristic of the organisation’s knowledge base, RAG offers
    a distinct advantage. It continually queries the external sources, ensuring its
    answers are based on the latest available data. Finetuning would require regular
    retraining to keep up with such changes, which might be impractical.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**5\. 数据的动态性如何？** 组织内部的数据库和文档存储可能非常动态，具有频繁的更新、变化或添加。如果这种动态性是组织知识库的特征，RAG提供了独特的优势。它不断查询外部来源，确保其答案基于最新的数据。微调则需要定期重新训练以跟上这些变化，这可能是不切实际的。'
- en: '**6\. Transparency/Interpretability required?** For internal applications,
    especially in sectors like finance, healthcare, or legal, understanding the reasoning
    or source behind an answer can be paramount. Since RAG provides a two-step process
    of retrieval and then generation, it inherently offers a clearer insight into
    which documents or data points influenced a particular answer. This traceability
    can be invaluable for internal stakeholders who might need to validate or further
    investigate the sources of certain answers.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**6\. 需要透明度/可解释性？** 对于内部应用，特别是在金融、医疗或法律等领域，理解答案背后的推理或来源可能至关重要。由于RAG提供了检索和生成的两步过程，它本质上提供了对哪些文档或数据点影响了特定答案的更清晰的洞察。这种可追溯性对于需要验证或进一步调查某些答案来源的内部利益相关者非常宝贵。'
- en: '**Recommendation:** For this use case ***a RAG system*** seems to be the more
    fitting choice. Given the need for dynamic access to the organisation’s evolving
    internal databases and the potential requirement for transparency in the answering
    process, RAG offers capabilities that align well with these needs. However, if
    there’s a significant emphasis on tailoring the model’s linguistic style or adapting
    to domain-specific nuances, incorporating elements of finetuning could be considered.'
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**推荐：** 对于这种使用场景，***RAG 系统*** 似乎是更合适的选择。考虑到需要动态访问组织不断变化的内部数据库，以及可能需要在回答过程中的透明度，RAG
    提供的功能与这些需求非常契合。然而，如果特别强调模型语言风格的定制或适应特定领域的细微差别，可以考虑引入微调元素。'
- en: Customer Support Automation (i.e. automated chatbots or help desk solutions
    providing instant responses to customer inquiries)
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 客户支持自动化（即提供即时响应的自动聊天机器人或帮助台解决方案）
- en: '**1\. External knowledge required? **Customer support often necessitates access
    to external data, especially when dealing with product details, account-specific
    information, or troubleshooting databases. While many queries can be addressed
    with general knowledge, some might require pulling data from company databases
    or product FAQs. Here, RAG’s capability to retrieve pertinent information from
    external sources would be beneficial. However, it’s worth noting that a lot of
    customer support interactions are also based on predefined scripts or knowledge,
    which can be effectively addressed with a finetuned model.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**1. 是否需要外部知识？** 客户支持通常需要访问外部数据，特别是在处理产品详情、账户特定信息或故障排除数据库时。虽然许多查询可以通过一般知识解决，但有些可能需要从公司数据库或产品常见问题中提取数据。在这里，RAG
    从外部来源检索相关信息的能力将非常有益。然而，值得注意的是，许多客户支持互动也基于预定义的脚本或知识，这些可以通过微调模型有效处理。'
- en: '**2\. Model adaptation required?** Customer interactions demand a certain tone,
    politeness, and clarity, and might also require company-specific terminologies.
    Finetuning is especially useful for ensuring the LLM adapts to the company’s voice,
    branding, and specific terminologies, ensuring a consistent and brand-aligned
    customer experience.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**2. 需要模型适应吗？** 客户互动需要特定的语气、礼貌和清晰度，并且可能需要公司特定的术语。微调对于确保大型语言模型（LLM）适应公司的声音、品牌和特定术语尤其有用，从而确保一致且符合品牌的客户体验。'
- en: '**3\. Crucial to minimise hallucinations?** For customer support chatbots,
    avoiding false information is essential to maintain user trust. Finetuning alone
    leaves models prone to hallucinations when faced with unfamiliar queries. In contrast,
    RAG systems suppress fabrications by grounding responses in retrieved evidence.
    This reliance on sourced facts allows RAG chatbots to minimise harmful falsehoods
    and provide users with reliable information where accuracy is vital.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**3. 是否关键于最小化虚假信息？** 对于客户支持聊天机器人，避免虚假信息对保持用户信任至关重要。单靠微调会让模型在面对不熟悉的查询时容易产生虚假信息。相比之下，RAG
    系统通过将回应基于检索到的证据来抑制虚假信息。这种对来源事实的依赖使得 RAG 聊天机器人能够最小化有害的虚假信息，并在准确性至关重要的地方提供可靠的信息。'
- en: '**4\. Training data available?** If a company has a history of customer interactions,
    this data can be invaluable for finetuning. A rich dataset of previous customer
    queries and their resolutions can be used to train the model to handle similar
    interactions in the future. If such data is limited, RAG can provide a fallback
    by retrieving answers from external sources like product documentation.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**4. 是否有培训数据？** 如果公司有客户互动的历史记录，这些数据对微调来说非常宝贵。丰富的历史客户查询及其解决方案数据可以用来训练模型，以处理未来类似的互动。如果这样的数据有限，RAG
    可以通过从外部来源（如产品文档）检索答案提供备用方案。'
- en: '**5\. How dynamic is the data?** Customer support might need to address queries
    about new products, updated policies, or changing service terms. In scenarios
    where the product line up, software versions, or company policies are frequently
    updated, RAG’s ability to dynamically pull from the latest documents or databases
    is advantageous. On the other hand, for more static knowledge domains, finetuning
    can suffice.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**5. 数据动态性如何？** 客户支持可能需要处理有关新产品、更新的政策或变化的服务条款的查询。在产品线、软件版本或公司政策经常更新的情况下，RAG
    从最新文档或数据库动态提取信息的能力是有利的。另一方面，对于更静态的知识领域，微调可能就足够了。'
- en: '**6\. Transparency/Interpretability required?** While transparency is essential
    in some sectors, in customer support, the primary focus is on accurate, fast,
    and courteous responses. However, for internal monitoring, quality assurance,
    or addressing customer disputes, having traceability regarding the source of an
    answer could be beneficial. In such cases, RAG’s retrieval mechanism offers an
    added layer of transparency.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**6\. 需要透明度/可解释性吗？** 虽然在某些领域透明度是必需的，但在客户支持中，主要关注点是准确、快速和礼貌的回应。然而，对于内部监控、质量保证或处理客户争议，关于答案来源的可追溯性可能会有帮助。在这种情况下，RAG的检索机制提供了额外的透明度层。'
- en: '**Recommendation:** For customer support automation a ***hybrid approach*** might
    be optimal. Finetuning can ensure that the chatbot aligns with the company’s branding,
    tone, and general knowledge, handling the majority of typical customer queries.
    RAG can then serve as a complementary system, stepping in for more dynamic or
    specific inquiries, ensuring the chatbot can pull from the latest company documents
    or databases and thereby minimising hallucinations. By integrating both approaches,
    companies can provide a comprehensive, timely, and brand-consistent customer support
    experience.'
  id: totrans-106
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**推荐：** 对于客户支持自动化，***混合方法***可能是最优的。微调可以确保聊天机器人与公司的品牌、语调和一般知识一致，处理大多数典型的客户查询。RAG可以作为补充系统，处理更动态或特定的询问，确保聊天机器人可以从最新的公司文档或数据库中获取信息，从而减少幻觉。通过整合这两种方法，公司可以提供全面、及时且品牌一致的客户支持体验。'
- en: '![RAG vs Finetuning: Which Is the Best Tool to Boost Your LLM Application?](../Images/01c5095559eb6418d4cc1d162039a9d2.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![RAG与微调：哪个是提升您的LLM应用的最佳工具？](../Images/01c5095559eb6418d4cc1d162039a9d2.png)'
- en: Image by Author
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图片作者
- en: Additional aspects to consider
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 额外需要考虑的方面
- en: 'As mentioned above, there are other factors that should be considered when
    deciding between RAG and finetuning (or both). We can’t possibly dive deep into
    them, as all of them are multi-faceted and don’t have clear answers like some
    of the aspects above (for example, if there is no training data the finetuning
    is just simply not possible). But that doesn’t mean we should disregard them:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，在决定使用RAG还是微调（或两者结合）时，还应考虑其他因素。我们无法深入探讨这些因素，因为它们都是多方面的，并且不像上述某些方面那样有明确的答案（例如，如果没有训练数据，微调根本不可行）。但这并不意味着我们应该忽视它们：
- en: Scalability
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可扩展性
- en: As an organisation grows and its needs evolve, how scalable are the methods
    in question? RAG systems, given their modular nature, might offer more straightforward
    scalability, especially if the knowledge base grows. On the other hand, frequently
    finetuning a model to cater to expanding datasets can be computationally demanding.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 随着组织的成长和需求的变化，所讨论的方法的可扩展性如何？由于RAG系统具有模块化的特点，它们可能提供更直接的可扩展性，特别是当知识库增长时。另一方面，频繁地调整模型以适应扩展的数据集可能会计算上较为繁重。
- en: Latency and Real-time Requirements
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 延迟和实时要求
- en: If the application requires real-time or near-real-time responses, consider
    the latency introduced by each method. RAG systems, which involve retrieving data
    before generating a response, might introduce more latency compared to a finetuned
    LLM that generates responses based on internalised knowledge.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果应用程序需要实时或近实时的响应，请考虑每种方法引入的延迟。RAG系统涉及在生成响应之前检索数据，可能会比基于内化知识生成响应的微调LLM引入更多的延迟。
- en: Maintenance and Support
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维护和支持
- en: Think about the long-term. Which system aligns better with the organisation's
    ability to provide consistent maintenance and support? RAG might require upkeep
    of the database and the retrieval mechanism, while finetuning would necessitate
    consistent retraining efforts, especially if the data or requirements change.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑长期情况。哪个系统更符合组织提供一致维护和支持的能力？RAG可能需要维护数据库和检索机制，而微调则需要持续的再训练工作，特别是当数据或需求变化时。
- en: Robustness and Reliability
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 鲁棒性和可靠性
- en: How robust is each method to different types of inputs? While RAG systems can
    pull from external knowledge sources and might handle a broad array of questions,
    a well finetuned model might offer more consistency in certain domains.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 每种方法对不同类型输入的鲁棒性如何？虽然RAG系统可以从外部知识来源中提取信息，可能处理各种问题，但经过良好微调的模型在某些领域可能提供更一致的表现。
- en: Ethical and Privacy Concerns
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理和隐私问题
- en: Storing and retrieving from external databases might raise privacy concerns,
    especially if the data is sensitive. On the other hand, a finetuned model, while
    not querying live databases, might still produce outputs based on its training
    data, which could have its own ethical implications.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 从外部数据库存储和检索数据可能引发隐私问题，特别是当数据敏感时。另一方面，虽然微调的模型不查询实时数据库，但仍可能基于其训练数据生成输出，这也可能具有伦理问题。
- en: Integration with Existing Systems
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与现有系统的集成
- en: Organisations might already have certain infrastructure in place. The compatibility
    of RAG or finetuning with existing systems — be it databases, cloud infrastructures,
    or user interfaces — can influence the choice.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 组织可能已经具备某些基础设施。RAG 或微调与现有系统的兼容性——无论是数据库、云基础设施还是用户界面——都可能影响选择。
- en: User Experience
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户体验
- en: Consider the end-users and their needs. If they require detailed, reference-backed
    answers, RAG could be preferable. If they value speed and domain-specific expertise,
    a finetuned model might be more suitable.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑最终用户及其需求。如果他们需要详细的、以参考为基础的回答，RAG 可能更为合适。如果他们重视速度和领域特定的专业知识，微调模型可能更适合。
- en: Cost
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本
- en: Finetuning can get expensive, especially for really large models. But in the
    past few months costs have gone down significantly thanks to parameter efficient
    techniques like [QLoRA](https://github.com/artidoro/qlora). Setting up RAG can
    be a large initial investment — covering the integration, database access, maybe
    even licensing fees — but then there’s also the regular maintenance of that external
    knowledge base to think about.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 微调可能会变得昂贵，特别是对于非常大的模型。但在过去几个月中，得益于如 [QLoRA](https://github.com/artidoro/qlora)
    等参数高效技术，成本已显著降低。设置 RAG 可能需要大量的初始投资——包括集成、数据库访问，甚至许可费用——但还需考虑到对外部知识库的常规维护。
- en: Complexity
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复杂性
- en: Finetuning can get complex quickly. While many providers now offer one-click
    finetuning where we just need to provide the training data, keeping track of model
    versions and ensuring that the new models still perform well across the board
    is challenging. RAG, on the other hand, can also get complex quickly. There’s
    the setup of multiple components, making sure the database stays fresh, and ensuring
    the pieces — like retrieval and generation — fit together just right.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 微调可能会迅速变得复杂。虽然现在许多供应商提供一键微调，我们只需要提供训练数据，但跟踪模型版本并确保新模型在各方面表现良好是具有挑战性的。另一方面，RAG
    也可能迅速变得复杂。需要设置多个组件，确保数据库保持新鲜，并确保各个部分——如检索和生成——完美契合。
- en: Conclusion
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: As we’ve explored, choosing between RAG and finetuning requires a nuanced evaluation
    of an LLM application’s unique needs and priorities. There is no one-size-fits-all
    solution; success lies in aligning the optimisation method with the specific requirements
    of the task. By assessing key criteria — the need for external data, adapting
    model behaviour, training data availability, data dynamics, result transparency,
    and more — organisations can make an informed decision on the best path forward.
    In certain cases, a hybrid approach leveraging both RAG and finetuning may be
    optimal.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所探讨的，选择 RAG 和微调之间需要对 LLM 应用的独特需求和优先级进行细致评估。没有一刀切的解决方案；成功在于将优化方法与任务的具体要求对齐。通过评估关键标准——对外部数据的需求、适应模型行为、训练数据的可用性、数据动态、结果透明度等——组织可以对最佳前进路径做出明智的决策。在某些情况下，利用
    RAG 和微调的混合方法可能是最佳选择。
- en: The key is avoiding assumptions that one method is universally superior. Like
    any tool, their suitability depends on the job at hand. Misalignment of approach
    and objectives can hinder progress, while the right method accelerates it. As
    an organisation evaluates options for boosting LLM applications, it must resist
    oversimplification and not view RAG and finetuning as interchangeable and choose
    the tool that empowers the model to fulfil its capabilities aligned to the needs
    of the use case. The possibilities these methods unlock are astounding but possibility
    alone isn’t enough — execution is everything. The tools are here — now let’s put
    them to work.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于避免假设某种方法是普遍优越的。像任何工具一样，它们的适用性取决于具体的任务。方法和目标的错位会阻碍进展，而正确的方法则会加速进展。当组织评估提升
    LLM 应用的选项时，必须抵制过度简化的倾向，不应将 RAG 和微调视为可互换的工具，而是选择能够使模型发挥其能力并与用例需求对齐的工具。这些方法所解锁的可能性令人惊叹，但仅有可能性是不够的——执行才是关键。工具已经在这里——现在让我们开始使用它们。
- en: '[Original](https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7).
    Reposted with permission.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7)。已获许可转载。'
- en: '**[](https://www.linkedin.com/in/heikohotz/)**[Heiko Hotz](https://www.linkedin.com/in/heikohotz/)****
    is the Founder of NLP London, an AI consultancy helping organizations implement
    natural language processing and conversational AI. With over 15 years of experience
    in the tech industry, Heiko is an expert in leveraging AI and machine learning
    to solve complex business challenges.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**[](https://www.linkedin.com/in/heikohotz/)**[Heiko Hotz](https://www.linkedin.com/in/heikohotz/)****
    是 NLP London 的创始人，该公司是一家人工智能咨询公司，帮助组织实施自然语言处理和对话 AI。Heiko 拥有超过 15 年的技术行业经验，是利用
    AI 和机器学习解决复杂商业挑战的专家。'
- en: More On This Topic
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关话题
- en: '[8 Ways to Improve Your Search Application this Week](https://www.kdnuggets.com/2022/09/corise-8-ways-improve-search-application-week.html)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[本周提升你搜索应用的 8 种方法](https://www.kdnuggets.com/2022/09/corise-8-ways-improve-search-application-week.html)'
- en: '[Which is Best: Data Science Bootcamp vs Degree vs Online Course](https://www.kdnuggets.com/2022/09/best-data-science-bootcamp-degree-online-course.html)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[哪一个更好：数据科学训练营 vs 学位 vs 在线课程](https://www.kdnuggets.com/2022/09/best-data-science-bootcamp-degree-online-course.html)'
- en: '[Some Kick Ass Prompt Engineering Techniques to Boost our LLM Models](https://www.kdnuggets.com/some-kick-ass-prompt-engineering-techniques-to-boost-our-llm-models)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一些厉害的提示工程技巧来提升我们的LLM模型](https://www.kdnuggets.com/some-kick-ass-prompt-engineering-techniques-to-boost-our-llm-models)'
- en: '[6 Best Free Online Courses to Learn Python and Boost Your Career](https://www.kdnuggets.com/2022/11/corise-6-best-free-online-courses-python-boost-career.html)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6 个最佳免费在线课程来学习Python并提升你的职业生涯](https://www.kdnuggets.com/2022/11/corise-6-best-free-online-courses-python-boost-career.html)'
- en: '[ETL vs ELT: Which One is Right for Your Data Pipeline?](https://www.kdnuggets.com/2023/03/etl-elt-one-right-data-pipeline.html)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ETL与ELT：哪个更适合你的数据管道？](https://www.kdnuggets.com/2023/03/etl-elt-one-right-data-pipeline.html)'
- en: '[Web LLM: Bring LLM Chatbots to the Browser](https://www.kdnuggets.com/2023/05/webllm-bring-llm-chatbots-browser.html)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Web LLM：将LLM聊天机器人带入浏览器](https://www.kdnuggets.com/2023/05/webllm-bring-llm-chatbots-browser.html)'
