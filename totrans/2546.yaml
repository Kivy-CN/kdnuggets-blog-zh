- en: 10 Machine Learning Model Training Mistakes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/07/10-machine-learning-model-training-mistakes.html](https://www.kdnuggets.com/2021/07/10-machine-learning-model-training-mistakes.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Sandeep Uttamchandani, Ph.D.](https://www.linkedin.com/in/sandeepuc/),
    Both a Product/Software Builder (VP of Engg) & Leader in operating enterprise-wide
    Data/AI initiatives (CDO)**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mistakes header](../Images/70a570ad807374cd4dd6269c8abf20d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Tumisu](https://pixabay.com/users/tumisu-148124/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1966448)
    from [Pixabay](https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1966448)
  prefs: []
  type: TYPE_NORMAL
- en: ML model training is the most time-consuming and resource-expensive part of
    the overall model-building journey. Training by definition is iterative, but somewhere
    during the iterations, mistakes seep into the mix. In this article, I share the
    ten deadly sins during ML model training — these are the most common as well as
    the easiest to overlook.
  prefs: []
  type: TYPE_NORMAL
- en: Ten Deadly Sins of ML Model Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**1\. Blindly increasing the number of epochs when the model is not converging**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During model training, there are scenarios when the loss-epoch graph keeps bouncing
    around and does not seem to converge irrespective of the number of epochs. There
    is no silver bullet as there are multiple root causes to investigate — bad training
    examples, missing truths, changing data distributions, too high a learning rate.
    The most common one I have seen is bad training examples related to a combination
    of anomalous data and incorrect labels.
  prefs: []
  type: TYPE_NORMAL
- en: 2. **Not shuffling the training dataset**
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes there are scenarios where the model seems to be converging, but suddenly
    the loss value increases significantly, i.e., loss value reduces and then increases
    significantly with epochs. There are multiple reasons for this kind of exploding
    loss. The most common one I have seen is outliers in the data that are not evenly
    distributed/shuffled in the data. Shuffling, in general, is an important step
    including for patterns where the loss is showing a repeating step function behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 3. **In multiclass classification, not prioritizing specific per-class metrics
    accuracy**
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For multiclass prediction problems, instead of tracking just the overall classification
    accuracy, it is often useful to prioritize the accuracy of specific classes and
    iteratively work on improving the model class by class. For instance, in classifying
    different forms of fraudulent transactions, focus on increasing the recall of
    specific classes (such as foreign transactions) based on business needs.
  prefs: []
  type: TYPE_NORMAL
- en: 4. **Assuming specificity will lead to lower model accuracy**
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of building a generic model, imagine building a model for a specific
    geographic region or specific user persona. Specificity will make the data more
    sparse but can lead to better accuracy for those specific problems. It is important
    to explore the specificity and sparsity trade-off during tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 5. **Ignoring prediction bias**
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prediction bias is the difference between the average of predictions and the
    average of labels in the dataset. Prediction bias serves as an early indicator
    of model issues. A big nonzero prediction bias is indicative of a bug somewhere
    in the model. There’s an interesting [Facebook paper](https://research.fb.com/wp-content/uploads/2016/11/practical-lessons-from-predicting-clicks-on-ads-at-facebook.pdf) in
    the context of ad CTR. Typically, the bias is useful to measure across prediction
    buckets.
  prefs: []
  type: TYPE_NORMAL
- en: 6. **Calling it a success just on model accuracy numbers**
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Accuracy of 95% means 95 of 100 predictions were correct. Accuracy is a flawed
    metric with a class imbalance in the dataset. Instead investigate deeply into
    metrics, such as precision/recall and how it correlates to overall user metrics
    such as spam detection, tumor classification, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 7. **Not understanding the impact of regularization lambda**
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Lambda is a key parameter in striking the balance between simplicity and training-data
    fit. High lambda → simple model → possibly underfitting. Low lambda → complex
    model → potential overfitting your data (won’t be able to generalize to new data).
    The ideal value of lambda is one that generalizes well to previously unseen data:
    data-dependent and requires analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Using the same test set over and over
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The more the same data is used for parameter and hyperparameter settings, the
    lesser confidence that the results will actually generalize. It is important to
    collect more data and keep adding to the test and validation sets.
  prefs: []
  type: TYPE_NORMAL
- en: '**9\. Not paying attention to initiation value in neural networks**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given non-convex optimization in NN, [initialization matters](https://www.deeplearning.ai/ai-notes/initialization/).
  prefs: []
  type: TYPE_NORMAL
- en: 10\. Assuming wrong labels always need to be fixed
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When wrong labels are detected, it is tempting to jump in and get them fixed.
    It is important to first analyze misclassified examples for the root cause. Oftentimes,
    errors due to incorrect labels may be a very small percentage. There might be
    a bigger opportunity to better train for specific data slices that might be the
    predominant root cause.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, avoiding these mistakes puts you significantly ahead of most other
    teams. Incorporate these as a checklist in your process.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Sandeep Uttamchandani, Ph.D.](https://www.linkedin.com/in/sandeepuc/)**:
    Data + AI/ML -- Both a Product/Software Builder (VP of Engg) & Leader in operating
    enterprise-wide Data/AI initiatives (CDO) | O''Reilly Book Author | Founder -
    DataForHumanity (non-profit)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://betterprogramming.pub/10-deadly-sins-of-ml-model-training-a5046c1f5094).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[How to Determine if Your Machine Learning Model is Overtrained](/2021/05/how-determine-machine-learning-model-overtrained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Write and train your own custom machine learning models using PyCaret](/2021/05/pycaret-write-train-custom-machine-learning-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to break a model in 20 days — a tutorial on production model analytics](/2021/03/break-model-20-days-tutorial-production-model-analytics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How To Use Synthetic Data To Overcome Data Shortages For Machine…](https://www.kdnuggets.com/2022/03/synthetic-data-overcome-data-shortages-machine-learning-model-training.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Speed Up XGBoost Model Training](https://www.kdnuggets.com/2021/12/speed-xgboost-model-training.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Software Mistakes and Tradeoffs: New book by Tomasz Lelek and…](https://www.kdnuggets.com/2021/12/manning-software-mistakes-tradeoffs-book.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mistakes That Newbie Data Scientists Should Avoid](https://www.kdnuggets.com/2022/06/mistakes-newbie-data-scientists-avoid.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3 Mistakes That Could Be Affecting the Accuracy of Your Data Analytics](https://www.kdnuggets.com/2023/03/3-mistakes-could-affecting-accuracy-data-analytics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Avoid These 5 Common Mistakes Every Novice in AI Makes](https://www.kdnuggets.com/avoid-these-5-common-mistakes-every-novice-in-ai-makes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
