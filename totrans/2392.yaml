- en: Decision Trees vs Random Forests, Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/08/decision-trees-random-forests-explained.html](https://www.kdnuggets.com/2022/08/decision-trees-random-forests-explained.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees and random forests are two of the most popular predictive models
    for supervised learning. These models can be used for both classification and
    regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, I will explain the difference between decision trees and random
    forests. By the end of the article, you should be familiar with the following
    concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: How does the decision tree algorithm work?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Components of a decision tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pros and cons of the decision tree algorithm?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What does bagging mean, and how does the random forest algorithm work?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which algorithm is better in terms of speed and performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees are highly interpretable machine learning models that allow us
    to stratify or segment data. They allow us to continuously split data based on
    specific parameters until a final decision is made.
  prefs: []
  type: TYPE_NORMAL
- en: How does the decision tree algorithm work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Take a look at the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision Trees vs Random Forests, Explained](../Images/30839068c44d7adef3ee93bb66180fcf.png)'
  prefs: []
  type: TYPE_IMG
- en: This dataset consists of only four variables?—?“Day”, ‘‘Temperature’’, ‘‘Wind’’,
    and ‘‘Play?’’. Depending on the temperature and wind on any given day, the outcome
    is binary - either to go out and play or stay home.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s build a decision tree using this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision Trees vs Random Forests, Explained](../Images/3d32f94954dd9e6333a894782b060c9a.png)'
  prefs: []
  type: TYPE_IMG
- en: The example above is one that is simple, but encapsulates exactly how a decision
    tree splits on different data points until an outcome is obtained.
  prefs: []
  type: TYPE_NORMAL
- en: From the visualization above, notice that the decision tree splits on the variable
    “Temperature” first. It also stops splitting when the temperature is extreme,
    and says that we should not go out to play.
  prefs: []
  type: TYPE_NORMAL
- en: It is only if the temperature is mild that it starts to split on the second
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'These observations lead to the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How does a decision tree decide on the first variable to split on?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to decide when to stop splitting?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the order used to construct a decision tree?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Entropy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Entropy is a metric that measures the impurity of a split in a decision tree.
    It determines how the decision tree chooses to partition data. Entropy values
    range from 0 to 1\. A value of 0 indicates a pure split and a value of 1 indicates
    an impure split.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the decision tree above, recall the tree had stopped splitting when the
    temperature was extreme:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision Trees vs Random Forests, Explained](../Images/b47e25071ea03307a5e5fcacbf4525fd.png)'
  prefs: []
  type: TYPE_IMG
- en: This is because when the temperature is extreme, the outcome of “Play?” is always
    “No.” This means that we have a pure split with 100% of data points in a single
    class. The entropy value of this split is 0, and the decision tree will stop splitting
    on this node.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees will always select the feature with the **lowest entropy** to
    be the first node.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, since the variable “Temperature” had a lower entropy value than
    “Wind”, this was the first split of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: Watch [this](https://www.youtube.com/watch?v=1IQOtJ4NI_0) YouTube video to learn
    more about how entropy is calculated and how it is used in decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Information Gain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Information gain measures the reduction in entropy when building a decision
    tree.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees can be constructed in a number of different ways. The tree needs
    to find a feature to split on first, second, third, etc. Information gain is a
    metric that tells us the best possible tree that can be constructed to minimize
    entropy.
  prefs: []
  type: TYPE_NORMAL
- en: The best tree is one with the highest information gain.
  prefs: []
  type: TYPE_NORMAL
- en: If you’d like to learn more about how to calculate information gain and use
    it to build the best possible decision tree, you can watch [this](https://www.youtube.com/watch?v=FuTRucXB9rA)
    YouTube video.
  prefs: []
  type: TYPE_NORMAL
- en: 'Components of a decision tree:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Decision Trees vs Random Forests, Explained](../Images/97538c4659bdec844d102b9fc636cc74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Root node: A root node is at the top of the decision tree, and is the variable
    from which the dataset starts dividing. The root node is the feature that provides
    us with the best split of data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Internal nodes: These are the nodes that split the data after the root node.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Leaf nodes: Finally, these are nodes at the bottom of the decision tree after
    which no further splits are possible.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Branches: Branches connect one node to another and are used to represent the
    outcome of a test.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pros and Cons of the Decision Tree Algorithm:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you understand how decision trees work, let’s take a look at some advantages
    and disadvantages of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Pros
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decision trees are simple and easy to interpret.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They can be used for classification and regression problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They can partition data that isn’t linearly separable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decision trees are prone to overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even a small change in the training dataset can make a huge difference in the
    logic of decision trees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random Forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the biggest drawbacks of the decision tree algorithm is that it is prone
    to overfitting. This means that the model is overly complex and has **high variance.**
    A model like this will have high training accuracy but will not generalize well
    to other datasets.
  prefs: []
  type: TYPE_NORMAL
- en: How does the random forest algorithm work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The random forest algorithm solves the above challenge by combining the predictions
    made by multiple decision trees and returning a single output. This is done using
    an extension of a technique called **bagging**, or **bootstrap aggregation**.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging is a procedure that is applied to reduce the variance of machine learning
    models. It works by averaging a set of observations to reduce variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how bagging works:'
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we had more than one training dataset, we could train multiple decision trees
    on each dataset and average the results.
  prefs: []
  type: TYPE_NORMAL
- en: However, since we usually only have one training dataset in most real-world
    scenarios, a statistical technique called **bootstrap** is used to sample the
    dataset with replacement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, multiple decision trees are created, and each tree is trained on a different
    data sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision Trees vs Random Forests, Explained](../Images/586e5360990898829b981920a41c3645.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that three bootstrap samples have been created from the training dataset
    above. The random forest algorithm goes a step further than bagging and randomly
    samples features so only a subset of variables are used to build each tree.
  prefs: []
  type: TYPE_NORMAL
- en: Each decision tree will render different predictions based on the data sample
    they were trained on.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this step, the prediction of each decision tree will be combined to come
    up with a single output.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of a classification problem, a majority class prediction is made:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision Trees vs Random Forests, Explained](../Images/6a2800f970f006e6206a7a4e57de325f.png)'
  prefs: []
  type: TYPE_IMG
- en: In regression problems, the predictions of all decision trees will be averaged
    to come up with a single value.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we randomly sample variables in the random forest algorithm?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the random forest algorithm, it is not only rows that are randomly sampled,
    but variables too.
  prefs: []
  type: TYPE_NORMAL
- en: This is because if we were to build multiple decision trees with the same features,
    every tree will be similar and highly correlated with each other, potentially
    yielding the same result. This will again lead to the issue of high variance.
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees vs. Random Forests - Which One Is Better and Why?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Random forests typically perform better than decision trees due to the following
    reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Random forests solve the problem of overfitting because they combine the output
    of multiple decision trees to come up with a final prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you build a decision tree, a small change in data leads to a huge difference
    in the model’s prediction. With a random forest, this problem does not arise since
    the data is sampled many times before generating a prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In terms of speed, however, the random forests are slower since more time is
    taken to construct multiple decision trees. Adding more trees to a random forest
    model will improve its accuracy to a certain extent, but also increases computation
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, decision trees are also easier to interpret than random forests since
    they are straightforward. It is easy to visualize a decision tree and understand
    how the algorithm reached its outcome. A random forest is harder to deconstruct
    since it is more complex and combines the output of multiple decision trees to
    make a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Natassha Selvaraj](https://www.natasshaselvaraj.com/)** is a self-taught
    data scientist with a passion for writing. You can connect with her on [LinkedIn](https://www.linkedin.com/in/natassha-selvaraj-33430717a/).'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning from Scratch: Decision Trees](https://www.kdnuggets.com/2022/11/machine-learning-scratch-decision-trees.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Generalized and Scalable Optimal Sparse Decision Trees(GOSDT)](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Demystifying Decision Trees for the Real World](https://www.kdnuggets.com/demystifying-decision-trees-for-the-real-world)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Random Forest vs Decision Tree: Key Differences](https://www.kdnuggets.com/2022/02/random-forest-decision-tree-key-differences.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Decision Tree Algorithm, Explained](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
