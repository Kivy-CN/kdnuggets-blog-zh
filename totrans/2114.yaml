- en: 'Mistral 7B-V0.2: Fine-Tuning Mistral’s New Open-Source LLM with Hugging Face'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/mistral-7b-v02-fine-tuning-mistral-new-open-source-llm-with-hugging-face](https://www.kdnuggets.com/mistral-7b-v02-fine-tuning-mistral-new-open-source-llm-with-hugging-face)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Mistral 7B-V0.2: Fine-Tuning Mistral’s New Open-Source LLM with Hugging Face](../Images/48bfc0ce427cb35b9a82f42561e5b59e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Mistral AI, one of the world’s leading AI research companies, has recently released
    the base model for [**Mistral 7B v0.2**](https://anakin.ai/blog/mistral-7b-v0-2-base-model/).
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: This open-source language model was unveiled during the company’s hackathon
    event on March 23, 2024.
  prefs: []
  type: TYPE_NORMAL
- en: The Mistral 7B models have 7.3 billion parameters, making them extremely powerful.
    They outperform Llama 2 13B and Llama 1 34B on almost all benchmarks.  The latest
    V0.2 model introduces a 32k context window among other advancements, enhancing
    its ability to process and generate text.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the version that was recently announced is the base model of the
    instruction-tuned variant, “Mistral-7B-Instruct-V0.2,” which was released earlier
    last year.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, I will show you how to access and fine-tune this language
    model on Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Hugging Face’s AutoTrain Feature
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be fine-tuning the Mistral 7B-v0.2 base model using Hugging Face’s AutoTrain
    functionality.
  prefs: []
  type: TYPE_NORMAL
- en: '[Hugging Face](https://huggingface.co/) is renowned for democratizing access
    to machine learning models, allowing everyday users to develop advanced AI solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: AutoTrain, a feature of Hugging Face, automates the process of model training,
    making it accessible and efficient.
  prefs: []
  type: TYPE_NORMAL
- en: It helps users select the best parameters and training techniques when fine-tuning
    models, which is a task that can otherwise be daunting and time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-Tuning the Mistral-7B Model with AutoTrain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are 5 steps to fine-tuning your Mistral-7B model:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Setting up the environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You must first create an account with Hugging Face, and then create a model
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this, simply follow the steps provided in this [link](https://huggingface.co/docs/hub/en/repositories-getting-started)
    and come back to this tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: We will be training the model in Python. When it comes to selecting a notebook
    environment for training, you can use [Kaggle Notebooks](https://www.kaggle.com/docs/notebooks)
    or [Google Colab](https://colab.google/), both of which provide free access to
    GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: If the training process takes too long, you might want to switch to a cloud
    platform like AWS Sagemaker or Azure ML.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, perform the following pip installs before you start coding along to
    this tutorial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Preparing your dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this tutorial, we will be using the [Alpaca dataset](https://huggingface.co/datasets/tatsu-lab/alpaca/viewer/default/train?p=520)
    on Hugging Face, which looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mistral 7B-V0.2: Fine-Tuning Mistral’s New Open-Source LLM with Hugging Face](../Images/dfc4a9431798c2b1a531e84d9364ef91.png)'
  prefs: []
  type: TYPE_IMG
- en: We will fine-tune the model on pairs of instructions and outputs and assess
    its ability to respond to the given instruction in the evaluation process.
  prefs: []
  type: TYPE_NORMAL
- en: 'To access and prepare this dataset, run the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The first function will load the Alpaca dataset using the “datasets” library
    and clean it to ensure that we aren’t including any empty instructions. The second
    function structures your data in a format that AutoTrain can understand.
  prefs: []
  type: TYPE_NORMAL
- en: After running the above code, the dataset will be loaded, formatted, and saved
    in the specified path. When you open your formatted dataset, you should see a
    single column labeled “formatted_text.”
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Setting up your training environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you’ve successfully prepared the dataset, let’s proceed to set up your
    model training environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, you must define the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a breakdown of the above specifications:'
  prefs: []
  type: TYPE_NORMAL
- en: You can specify any *project_name*. This is where all your project and training
    files will be stored.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *model_name* parameter is the model you’d like to fine-tune. In this case,
    I’ve specified a path to the **Mistral-7B v0.2 base model** on Hugging Face.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *hf_token* variable must be set to your Hugging Face token, which can be
    obtained by navigating to [this link](https://huggingface.co/settings/tokens).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your *repo_id* must be set to the Hugging Face model repository that you created
    in the first step of this tutorial. For example, my repository ID is *NatasshaS/Model2.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4\. Configuring model parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before fine-tuning our model, we must define the training parameters, which
    control aspects of model behavior such as training duration and regularization.
  prefs: []
  type: TYPE_NORMAL
- en: These parameters influence key aspects like how long the model trains, how it
    learns from the data, and how it avoids overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can set the following parameters for your model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 5\. Setting environment variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s now prepare our training environment by setting some environment variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'This step ensures that the AutoTrain feature uses the desired settings to fine-tune
    the model, such as our project name and training preferences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 6\. Initiate model training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, let’s start training the model using the *autotrain* command. This
    step involves specifying your model, dataset, and training configurations, as
    displayed below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Make sure to change the *data-path* to where your training dataset is located.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Evaluating the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once your model has finished training, you should see a folder appear in your
    directory with the same title as your project name.
  prefs: []
  type: TYPE_NORMAL
- en: 'In my case, this folder is titled “*mistralai,”* as seen in the image below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mistral 7B-V0.2: Fine-Tuning Mistral’s New Open-Source LLM with Hugging Face](../Images/e650269f16bb1815dfccc3be3ad0b343.png)'
  prefs: []
  type: TYPE_IMG
- en: Within this folder, you can find files that encompass your model weights, hyperparameters,
    and architecture details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now check whether this fine-tuned model is able to respond accurately
    to a question in our dataset. To achieve this, we first need to run the following
    lines of code to generate 5 sample inputs and outputs from our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a response that looks like this, showcasing 5 sample data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We are going to type one of the above instructions into the model and check
    if it generates accurate output. Here is a function to provide an instruction
    to the model and get a response from it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, enter a question into this function as displayed below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Your model should generate a response that is identical to its corresponding
    output in the training dataset, as displayed below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Please note that the response may seem incomplete or cut off because of the
    number of tokens we’ve specified. Feel free to adjust the “max_length” value to
    allow for a more extended response.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-Tuning Mistral-7B V0.2 - Next Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’ve come this far, congratulations!
  prefs: []
  type: TYPE_NORMAL
- en: You have successfully fine-tuned a state-of-the-art language model, leveraging
    the power of Mistral 7B v-0.2 alongside Hugging Face’s capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: But the journey doesn’t end here.
  prefs: []
  type: TYPE_NORMAL
- en: As a next step, I recommend experimenting with different datasets or tweaking
    certain training parameters to optimize model performance. Fine-tuning models
    on a larger scale will enhance their utility, so try experimenting with bigger
    datasets or varying formats, such as PDFs and text files.
  prefs: []
  type: TYPE_NORMAL
- en: Such experience becomes invaluable when working with real-world data in organizations,
    which is often messy and unstructured.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://linktr.ee/natasshaselvaraj)**[Natassha Selvaraj](https://linktr.ee/natasshaselvaraj)**
    is a self-taught data scientist with a passion for writing. Natassha writes on
    everything data science-related, a true master of all data topics. You can connect
    with her on [LinkedIn](https://www.linkedin.com/in/natassha-selvaraj-33430717a/)
    or check out her [YouTube channel](https://www.youtube.com/@natassha_ds).'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How to Finetune Mistral AI 7B LLM with Hugging Face AutoTrain](https://www.kdnuggets.com/how-to-finetune-mistral-ai-7b-llm-with-hugging-face-autotrain)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[RAG vs Finetuning: Which Is the Best Tool to Boost Your LLM Application?](https://www.kdnuggets.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Web LLM: Bring LLM Chatbots to the Browser](https://www.kdnuggets.com/2023/05/webllm-bring-llm-chatbots-browser.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top 10 Machine Learning Demos: Hugging Face Spaces Edition](https://www.kdnuggets.com/2022/05/top-10-machine-learning-demos-hugging-face-spaces-edition.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A community developing a Hugging Face for customer data modeling](https://www.kdnuggets.com/2022/08/objectiv-community-developing-hugging-face-customer-data-modeling.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Build AI Chatbot in 5 Minutes with Hugging Face and Gradio](https://www.kdnuggets.com/2023/06/build-ai-chatbot-5-minutes-hugging-face-gradio.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
