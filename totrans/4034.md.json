["```py\nimport pandas as pd\n\ndata = pd.read_csv('training.csv', encoding='ISO-8859-1', header=None)\ncolumn_names = ['target', 'ids', 'date', 'flag', 'user', 'text']\ndata.columns = column_names\nhead = data.head()\ninfo = data.info()\ndescribe = data.describe()\nhead, info, describe \n```", "```py\n# Separate positive and negative tweets based on the 'target' column\npositive_tweets = data[data['target'] == 4]['text']\nnegative_tweets = data[data['target'] == 0]['text']\n\n# Sample some positive and negative tweets to create word clouds\nsample_positive_text = \" \".join(text for text in positive_tweets.sample(frac=0.1, random_state=23))\nsample_negative_text = \" \".join(text for text in negative_tweets.sample(frac=0.1, random_state=23))\n\n# Generate word cloud images for both positive and negative sentiments\nwordcloud_positive = WordCloud(width=800, height=400, max_words=200, background_color=\"white\").generate(sample_positive_text)\nwordcloud_negative = WordCloud(width=800, height=400, max_words=200, background_color=\"white\").generate(sample_negative_text)\n\n# Display the generated image using matplotlib\nplt.figure(figsize=(15, 7.5))\n\n# Positive word cloud\nplt.subplot(1, 2, 1)\nplt.imshow(wordcloud_positive, interpolation='bilinear')\nplt.title('Positive Tweets Word Cloud')\nplt.axis(\"off\")\n\n# Negative word cloud\nplt.subplot(1, 2, 2)\nplt.imshow(wordcloud_negative, interpolation='bilinear')\nplt.title('Negative Tweets Word Cloud')\nplt.axis(\"off\")\n\nplt.show() \n```", "```py\n# Since we need to use a smaller dataset due to resource constraints, let's sample 100k tweets\n# Balanced sampling: 50k positive and 50k negative\nsample_size_per_class = 50000\n\npositive_sample = data[data['target'] == 4].sample(n=sample_size_per_class, random_state=23)\nnegative_sample = data[data['target'] == 0].sample(n=sample_size_per_class, random_state=23)\n\n# Combine the samples into one dataset\nbalanced_sample = pd.concat([positive_sample, negative_sample])\n\n# Check the balance of the sampled data\nbalanced_sample['target'].value_counts() \n```", "```py\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n\n# Train and test split\nX_train, X_val, y_train, y_val = train_test_split(balanced_sample['text'], balanced_sample['target'], test_size=0.2, random_state=23)\n\n# After vectorizing the text data using TF-IDF\nX_train_vectorized = vectorizer.fit_transform(X_train)\nX_val_vectorized = vectorizer.transform(X_val)\n\n# Convert the sparse matrix to a dense matrix\nX_train_vectorized = X_train_vectorized.todense()\nX_val_vectorized = X_val_vectorized.todense()\n\n# Convert labels to one-hot encoding\nencoder = LabelEncoder()\ny_train_encoded = to_categorical(encoder.fit_transform(y_train))\ny_val_encoded = to_categorical(encoder.transform(y_val))\n\n# Define a simple neural network model\nmodel = Sequential()\nmodel.add(Dense(512, input_shape=(X_train_vectorized.shape[1],), activation='relu'))\nmodel.add(Dense(2, activation='softmax'))  # 2 because we have two classes\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model over epochs\nhistory = model.fit(X_train_vectorized, y_train_encoded, epochs=10, batch_size=128, \n                    validation_data=(X_val_vectorized, y_val_encoded), verbose=1)\n\n# Plotting the model accuracy over epochs\nplt.figure(figsize=(10, 6))\nplt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')\nplt.title('Model Accuracy over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\nplt.show() \n```"]