- en: 'Support Vector Machines: A Concise Technical Overview'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2016/09/support-vector-machines-concise-technical-overview.html](https://www.kdnuggets.com/2016/09/support-vector-machines-concise-technical-overview.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Classification is concerned with building a model that separates data into distinct
    classes. This model is built by inputting a set of training data for which the
    classes are pre-labeled in order for the algorithm to learn from. The model is
    then used by inputting a different dataset for which the classes are withheld,
    allowing the model to predict their class membership based on what it has learned
    from the training set. Well-known classification schemes include decision trees
    and Support Vector Machines, among a whole host of others. As this type of algorithm
    requires explicit class labeling, classification is a form of supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, Support Vector Machines (SVMs) are a particular classification
    strategy. SMVs work by transforming the training dataset into a higher dimension,
    which is then inspected for the optimal separation boundary, or boundaries, between
    classes. In SVMs, these boundaries are referred to as hyperplanes, which are identified
    by locating support vectors, or the instances that most essentially define classes,
    and their margins, which are the lines parallel to the hyperplane defined by the
    shortest distance between a hyperplane and its support vectors. Consequently,
    SVMs are able to classify both linear and nonlinear data.
  prefs: []
  type: TYPE_NORMAL
- en: The grand idea with SVMs is that, with a high enough number of dimensions, a
    hyperplane separating a particular class from all others can always be found,
    thereby delineating dataset member classes. When repeated a sufficient number
    of times, enough hyperplanes can be generated to separate all classes in *n*-dimensional
    space. Importantly, SVMs look not just for any separating hyperplane but the **maximum-margin**
    hyperplane, being that which resides equidistance from respective class support
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '![SVM](../Images/d5f9a91b5d6ed87326db5bc95500daf4.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Fig.1: Maximum-Margin Hyperplane and the Support Vectors.**'
  prefs: []
  type: TYPE_NORMAL
- en: When data is linearly-separable, there are many separating lines that could
    be chosen. Such a hyperplane can be expressed as
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/364a39a508b7052231835d606e63da3e.png)'
  prefs: []
  type: TYPE_IMG
- en: where **W** is a vector of weights, *b* is a scalar bias, and X are the training
    data (of the form (x[1], x[2] )). If our bias, *b*, is thought of as an additional
    weight, the equation can be expressed as
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/87db00ed9a21b0870f5fd9380e1e2eec.png)'
  prefs: []
  type: TYPE_IMG
- en: which can, in turn, be rewritten as a pair of linear inequalities, solving to
    greater or less than zero, either of which satisfied indicate that a particular
    point lies above or below the hyperplane, respectively. Finding the maximum-margin
    hyperplane, or the hyperplane that resides equidistance from the support vectors,
    is done by combining the linear inequalities into a single equation and transforming
    them into a constrained quadratic optimization problem, using a Lagrangian formulation
    and solving using [Karush-Kuhn-Tucker conditions](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions).
  prefs: []
  type: TYPE_NORMAL
- en: Following this transformation, the maximum-margin hyperplane can be expressed
    in the form
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/e5b7422f4c8f0ccfaaec65e3a45bf2eb.png)'
  prefs: []
  type: TYPE_IMG
- en: where *b* and *α[i]* are learned parameters, *n* is the number of support vectors,
    *i* is a support vector instance, *t* is the vector of training instances, *y[i]*
    is the class value of a particular training instance of vector *t*, and *a(i)*
    is the vector of support vectors. Once the maximum-margin hyperplane is identified
    and training is complete, only the support vectors are relevant to the model as
    they define the maximum-margin hyperplane; all of the other training instances
    can be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: When data is not linearly-separable, the data is first transformed into a higher
    dimensional space using some function, and this space is then searched for the
    hyperplane, another quadratic optimization problem. In order to circumvent the
    additional computational complexity that increased dimensionality brings, all
    calculations can be performed on the original input data, which is quite likely
    of lower dimensionality. This higher computational complexity is based on the
    fact that, in higher dimensionality, the dot product of an instance and each of
    the support vectors would need to be calculated for each instance classification.
    A kernel function, which maps an instance to feature space created by a particular
    function it applies instances to, is used on the original, lower-dimensionality
    data. A common kernel function is the polynomial kernel, which expressed
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/66125590acc35a340f97de70f2cefccd.png)'
  prefs: []
  type: TYPE_IMG
- en: computes the dot product of 2 vectors, and raises that result to the power of
    *n*.
  prefs: []
  type: TYPE_NORMAL
- en: SVMs are powerful, well-used classification algorithms, which garnered a substantial
    amount of research attention prior to the deep learning boom we are currently
    in. Despite the fact that, to the onlooker, they may no longer be bleeding edge
    algorithms, they certainly have had great success in particular domains, [and
    remain some of the most popular classification algorithms](/2016/09/poll-algorithms-used-data-scientists.html)
    in the toolkits of machine learning practitioners and data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: What has been presented above is a very high-level overview of SVMs. Support
    Vector Machines are a complex classifier, more of which can be found by [starting
    here](/tag/support-vector-machines).
  prefs: []
  type: TYPE_NORMAL
- en: '**Related**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Data Mining History: The Invention of Support Vector Machines](/2016/07/guyon-data-mining-history-svm-support-vector-machines.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Support Vector Machines: A Simple Explanation](/2016/07/support-vector-machines-simple-explanation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Select Support Vector Machine Kernels](/2016/06/select-support-vector-machine-kernels.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Support Vector Machines: An Intuitive Approach](https://www.kdnuggets.com/2022/08/support-vector-machines-intuitive-approach.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Gentle Introduction to Support Vector Machines](https://www.kdnuggets.com/2023/07/gentle-introduction-support-vector-machines.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How Semantic Vector Search Transforms Customer Support Interactions](https://www.kdnuggets.com/how-semantic-vector-search-transforms-customer-support-interactions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python Vector Databases and Vector Indexes: Architecting LLM Apps](https://www.kdnuggets.com/2023/08/python-vector-databases-vector-indexes-architecting-llm-apps.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Ethics of AI: Navigating the Future of Intelligent Machines](https://www.kdnuggets.com/2023/04/ethics-ai-navigating-future-intelligent-machines.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[With Data Privacy learn to implement technical privacy solutions…](https://www.kdnuggets.com/2022/04/manning-data-privacy-learn-implement-technical-privacy-solutions-tools-scale.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
