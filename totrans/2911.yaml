- en: 'Opening Black Boxes: How to leverage Explainable Machine Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/08/open-black-boxes-explainable-machine-learning.html](https://www.kdnuggets.com/2019/08/open-black-boxes-explainable-machine-learning.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst/), Emset**.'
  prefs: []
  type: TYPE_NORMAL
- en: As Machine Learning and AI are becoming more and more popular, an increasing
    number of organizations is adopting this new technology. Predictive modeling is
    helping processes becoming more efficient but also allow users to gain benefits.
    One can predict how much you are likely going to be earning based on your professional
    skills and experience. The output could simply be a number, but users typically
    want to know why that value is given!
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will demonstrate some methods for creating explainable predictions
    and guide you into opening these black-box models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/313ed8b941464d1b36522ff8b99f30db.png)'
  prefs: []
  type: TYPE_IMG
- en: The data used in this article is the US Adult Income data set, which is typically
    used to predict whether somebody makes less than 50K or more than 50, a simple
    binary classification task. You can get the data [here](https://www.kaggle.com/johnolafenwa/us-census-data),
    or you can follow along with the notebook [here](https://github.com/MaartenGr/InterpretableML).
  prefs: []
  type: TYPE_NORMAL
- en: The data is relatively straightforward with information with respect to an individuals’
    Relationship, Occupation, Race, Gender, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/683718f5d319cd68e46d7d1116649495.png)'
  prefs: []
  type: TYPE_IMG
- en: Modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The categorical variables are one-hot encoded, and the target is set to either
    0 (≤50K) or 1 (>50K). Now let’s say that we would like to use a model that is
    known for its great performance on classification tasks, but is highly complex
    and the output difficult to interpret. This model would be LightGBM which, together
    with CatBoost and XGBoost, is often used in both classification and regression
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by simply fitting the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: I should note that preferably you would have a train/test split with additional
    holdout data to prevent any overfitting you would do.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, I quickly check the performance of the model using 10-fold cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, scores_accuracy gives an average accuracy across 10-folds of
    87% while scores_balanced gives 80%. As it turns out, the target variable is imbalanced,
    with 25% of the target belonging to 1 and 75% to 0\. Thus, choosing the correct
    validation measure is highly important as it may falsely indicate a good model.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have created the model, next would be explaining what it exactly
    has done. Since LightGBM works with highly efficient gradient boosting decision
    trees, interpretation of the output can be difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Partial Dependency Plots (PDP)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Partial Dependency Plots (DPD) show the effect a feature has on the outcome
    of a predictive based model. It marginalizes the model output over the distribution
    of features to extract the importance of the feature of interest. The package,
    PDPbox, that I used can be found [here](https://github.com/SauceCat/PDPbox).
  prefs: []
  type: TYPE_NORMAL
- en: '**Assumptions**'
  prefs: []
  type: TYPE_NORMAL
- en: This importance calculation is based on an important assumption, namely that
    the feature of interest is not correlated with all other features (except for
    the target). The reason for this is that it will show data points that are likely
    to be impossible. For example, weight and height are correlated, but the PDP might
    show the effect of a large weight and very small height on the target while that
    combination is highly unlikely. This can be partially resolved by showing a rug
    at the bottom of your PDP.
  prefs: []
  type: TYPE_NORMAL
- en: '**Correlation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we check the correlation between features to make sure that there are
    no problems there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/60bb33e90dd4bc99b3dae6fb4f12957b.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that there is no strong correlation present among the features. However,
    I will do some one-hot encoding, later on, to prepare the data for modeling, which
    could lead to the creation of correlated features.
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous variables**'
  prefs: []
  type: TYPE_NORMAL
- en: 'PDP plots can be used to visualize the impact of a variable on the output across
    all data points. Let’s first start with an obvious one, the effect of a continuous
    variable, namely *capital_gain*, on the target:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3a4b24499ee4089c1915fe3aad26d54a.png)'
  prefs: []
  type: TYPE_IMG
- en: The x-axis shows the values capital_gain can take, and the y-axis indicates
    the effect it can have on the probability of the binary classification. It is
    clear that as one’s *capital_gain *increases their chance of making <50K increases
    with it. Note that the rug of data points at the bottom helps identify data points
    that do not appear often.
  prefs: []
  type: TYPE_NORMAL
- en: '**One-hot encoded variables**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, what if you have categorical variables that you one-hot encoded? You would
    likely want to see the effect of the categories individually without having to
    plot them separately. With PDPbox, you can show the effect on multiple binary
    categorical variables at the same time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1f2a8c0afee9ce8416afcc2c1c66e290.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, you can clearly see that the likelihood of making more is positively affected
    by being either in a managerial position or that of technology. The chance decreases
    if you are working in the fishing industry.
  prefs: []
  type: TYPE_NORMAL
- en: '**Interaction between variables**'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the interaction between variables might be problematic as PDP will
    create values for the combination, which are unlikely to be possible if the variables
    are highly correlated.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5d5167ae19832b3753cbd36e65aa82d1.png)'
  prefs: []
  type: TYPE_IMG
- en: This matrix tells you that an individual is likely to make more if they are
    around 49 years old and work roughly 50 hours a week. I should note that it is
    important to keep in mind the actual distribution of all interactions in your
    data set. There is a chance that this plot will show you interesting interactions
    that will rarely or never happen.
  prefs: []
  type: TYPE_NORMAL
- en: Local Interpretable Model-agnostic Explanations (LIME)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LIME basically tries to step away from deriving the importance of global features
    and instead approximates the importance of features for local predictions. It
    does so by taking the row (or set of data points) from which to predict and generate
    fake data based on that row. It then calculates the similarity between the fake
    data and the real data and approximates the effect of the changes based on the
    similarity between the fake and real data. The package that I used can be found [here](https://github.com/marcotcr/lime).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e929e6ebcb6cbdb81d8ddeadfb30f189.png)'
  prefs: []
  type: TYPE_IMG
- en: '*The output of LIME for a single row.*'
  prefs: []
  type: TYPE_NORMAL
- en: The output shows the effect of the top 5 variables on the prediction probability.
    This helps in identifying why your model makes a certain prediction but also allows
    for explanations to users.
  prefs: []
  type: TYPE_NORMAL
- en: '**Disadvantage**'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the neighborhood (kernel width) around which LIME tries to find different
    values for the initial row is, to an extent, a hyperparameter that can be optimized.
    At times you want a larger neighborhood depending on the data. It is a bit of
    trial and error to find the right kernel width as it might hurt the interpretability
    of explanations.
  prefs: []
  type: TYPE_NORMAL
- en: SHapley Additive exPlanations (SHAP)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A (fairly) recent development has been the implementation of Shapley values
    into machine learning applications. In its essence, SHAP uses game theory to track
    the *marginal contributions *of each variable. For each variable, it randomly
    samples other values from the data set and calculates the change in your model
    score. These changes are then averaged for each variable to create a summary score,
    but also gives information on how important certain variables are for a specific
    data point.
  prefs: []
  type: TYPE_NORMAL
- en: Click [here](https://github.com/slundberg/shap) for the package that I used
    in the analyses. For a more in-depth explanation of the theoretical background
    of SHAP, click [here](https://towardsdatascience.com/one-feature-attribution-method-to-supposedly-rule-them-all-shapley-values-f3e04534983d).
  prefs: []
  type: TYPE_NORMAL
- en: '**Three axioms of interpretability**'
  prefs: []
  type: TYPE_NORMAL
- en: 'SHAP has been well-received due to its ability to satisfy the three axioms
    of interpretability:'
  prefs: []
  type: TYPE_NORMAL
- en: Any feature that has no effect on the predicted value should have a Shapley
    value of 0 (Dummy)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If two features add the same value to the prediction, then their Shapley values
    should be the same (Substitutability)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have two or more predictions that you would want to merge you should
    be able to simply add the Shapley values that were calculated on the individual
    predictions (Additivity)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Binary Classification**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what the result would be if we were to calculate the Shapley values
    for a single row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f20722d81563bf308ce241839154c08c.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Shapley values for a single data point.*'
  prefs: []
  type: TYPE_NORMAL
- en: This plot shows a base value that is used to indicate the direction of the prediction.
    Seeing as most of the targets are 0, it isn’t strange to see that the base value
    is negative.
  prefs: []
  type: TYPE_NORMAL
- en: The red bar shows how much the probability that the target is 1 (>50K) is increased
    if its *Education_num* is 13\. Higher education typically leads to making more.
  prefs: []
  type: TYPE_NORMAL
- en: The blue bars show that these variables decrease the probability, with *Age*having
    the biggest effect. This makes sense as younger people typically make less.
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Shapley works intuitively a bit better when it concerns regression (continuous
    variable) rather than binary classification. Just to show an example, let’s train
    a model to predict age from the same data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/284c3c397552aa096e96fe0e57c541e3.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Shapley values for a single data point.*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can quickly observe that if you are never married, the predicted Age
    is lowered by roughly 8 years. This helps a bit more with explaining the prediction
    compared to a classification task since you are directly talking about the value
    of the target instead of its probability.
  prefs: []
  type: TYPE_NORMAL
- en: '**One-hot encoded features**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Additivity axiom allows for summing Shapley values for each feature over
    all data points to create the mean absolute Shapley value. In other words, it
    gives global feature importances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e9b93c3774dbe2de1b9d998f466553da.png)'
  prefs: []
  type: TYPE_IMG
- en: However, you can immediately see the problem using Shapley values for one-hot
    encoded features, they are shown for each one-hot encoded feature instead of what
    they originally represented.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, the Additivity axiom allows the Shapley values for each one-hot
    encoded generated feature to be summed as a representation of the Shapley value
    for the entire feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to sum up all the Shapley values for the one-hot encoded features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that all the Shapley values are average across all features and summed
    for the one-hot encoded features we can plot the resulting feature importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/381d21c7fd64b721b0e565944e50af1e.png)'
  prefs: []
  type: TYPE_IMG
- en: We can now see that *Occupation *is way more important than the original Shapley
    summary plot showed. Thus, make sure to use the Additivity to your advantage when
    explaining the importance of features to your users. They are likely to be more
    interested in how important *Occupation *is rather than a specific *Occupation*.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although this is not the first article to talk about Interpretable and Explainable
    ML, I hope this helped you understand how these technologies can be used when
    developing your model.
  prefs: []
  type: TYPE_NORMAL
- en: There has been a significant buzz around SHAP, and I hope that demonstrating
    the Additivity axiom using one-hot encoded features gives more intuition on how
    to use such a method.
  prefs: []
  type: TYPE_NORMAL
- en: Notebook with code can be found [here](https://github.com/MaartenGr/InterpretableML).
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio:** [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst/)
    is a Data Scientist | I/O Psychologist | Clinical Psychologist | Co-Founder at
    Emset.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/opening-black-boxes-how-to-leverage-explainable-machine-learning-dd4ab439998e).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[A Data Science Playbook for explainable ML/xAI](https://www.kdnuggets.com/2019/07/domino-xai-data-science-explainable.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“Please, explain.” Interpretability of machine learning models](https://www.kdnuggets.com/2019/05/interpretability-machine-learning-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[An introduction to explainable AI, and why we need it](https://www.kdnuggets.com/2019/04/introduction-explainable-ai.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Black Friday Deal - Master Machine Learning for Less with DataCamp](https://www.kdnuggets.com/2022/11/datacamp-black-friday-deal-master-machine-learning-less-datacamp.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Better Leverage Data Science for Business Growth](https://www.kdnuggets.com/2022/08/better-leverage-data-science-business-growth.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using RAPIDS cuDF to Leverage GPU in Feature Engineering](https://www.kdnuggets.com/2023/06/rapids-cudf-leverage-gpu-feature-engineering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How To Leverage Docker Cache for Optimizing Build Speeds](https://www.kdnuggets.com/how-to-leverage-docker-cache-for-optimizing-build-speeds)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Quest for Model Confidence: Can You Trust a Black Box?](https://www.kdnuggets.com/the-quest-for-model-confidence-can-you-trust-a-black-box)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Closing the Gap Between Human Understanding and Machine Learning:…](https://www.kdnuggets.com/2023/06/closing-gap-human-understanding-machine-learning-explainable-ai-solution.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
