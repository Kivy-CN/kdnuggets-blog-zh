- en: Understanding Decision Trees for Classification in Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Python 中的决策树分类
- en: 原文：[https://www.kdnuggets.com/2019/08/understanding-decision-trees-classification-python.html](https://www.kdnuggets.com/2019/08/understanding-decision-trees-classification-python.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/08/understanding-decision-trees-classification-python.html](https://www.kdnuggets.com/2019/08/understanding-decision-trees-classification-python.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Michael Galarnyk](https://www.linkedin.com/in/michaelgalarnyk/), Data
    Scientist**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：[Michael Galarnyk](https://www.linkedin.com/in/michaelgalarnyk/)，数据科学家**'
- en: '![Figure](../Images/1279716ceb53cae5f0ee4c396b8bdbf3.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/1279716ceb53cae5f0ee4c396b8bdbf3.png)'
- en: This tutorial goes into extreme detail about how decision trees work.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将详细讲解决策树的工作原理。
- en: Decision trees are a popular supervised learning method for a variety of reasons.
    Benefits of decision trees include that they can be used for both regression and
    classification, they are easy to interpret and they don’t require feature scaling.
    They have several flaws including being prone to overfitting. This tutorial covers
    decision trees for classification also known as classification trees.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一种受欢迎的监督学习方法，原因有很多。决策树的优点包括可以用于回归和分类，易于解释，不需要特征缩放。然而，它们也有一些缺陷，包括容易过拟合。本文涵盖了用于分类的决策树，也称为分类树。
- en: 'Additionally, this tutorial will cover:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，本教程还将涵盖：
- en: The anatomy of classification trees (depth of a tree, root nodes, decision nodes,
    leaf nodes/terminal nodes).
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类树的结构（树的深度、根节点、决策节点、叶节点/终端节点）。
- en: How classification trees make predictions
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类树如何进行预测
- en: How to use scikit-learn (Python) to make classification trees
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 scikit-learn (Python) 制作分类树
- en: Hyperparameter tuning
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数调整
- en: As always, the code used in this tutorial is available on my [github](https://github.com/mGalarnyk/Python_Tutorials/blob/master/Statistics/boxplot/box_plot.ipynb) ([anatomy](https://github.com/mGalarnyk/Python_Tutorials/blob/master/Sklearn/CART/Dt_Classification/ClassificationTreeAnatomy.ipynb), [predictions](https://github.com/mGalarnyk/Python_Tutorials/blob/master/Sklearn/CART/Dt_Classification/ClassificationTreesUsingPython.ipynb)).
    With that, let’s get started!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，本教程中使用的代码可以在我的 [github](https://github.com/mGalarnyk/Python_Tutorials/blob/master/Statistics/boxplot/box_plot.ipynb)
    上找到（[结构](https://github.com/mGalarnyk/Python_Tutorials/blob/master/Sklearn/CART/Dt_Classification/ClassificationTreeAnatomy.ipynb)，[预测](https://github.com/mGalarnyk/Python_Tutorials/blob/master/Sklearn/CART/Dt_Classification/ClassificationTreesUsingPython.ipynb)）。好了，让我们开始吧！
- en: What are Classification Trees?
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是分类树？
- en: '**C**lassification **a**nd **R**egression **T**rees (CART) is a term introduced
    by Leo Breiman to refer to the Decision Tree algorithm that can be learned for
    classification or regression predictive modeling problems. This post covers classification
    trees.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**C**lassification **a**nd **R**egression **T**rees (CART) 是 Leo Breiman 提出的术语，指可以用于分类或回归预测建模问题的决策树算法。本文讨论分类树。'
- en: Classification Trees
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类树
- en: Classification trees are essentially a series of questions designed to assign
    a classification. The image below is a classification tree trained on the IRIS
    dataset (flower species). Root (brown) and decision (blue) nodes contain questions
    which split into subnodes. The root node is just the topmost decision node. In
    other words, it is where you start traversing the classification tree. The leaf
    nodes (green), also called terminal nodes, are nodes that don’t split into more
    nodes. Leaf nodes are where classes are assigned by majority vote.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 分类树本质上是一系列问题，用于分配分类。下图是一个在 IRIS 数据集（花卉种类）上训练的分类树。根节点（棕色）和决策节点（蓝色）包含的问题会分裂成子节点。根节点就是最顶层的决策节点。换句话说，它是你开始遍历分类树的地方。叶节点（绿色），也称为终端节点，是不会再分裂成更多节点的节点。叶节点是通过多数投票来分配类别的地方。
- en: '![Figure](../Images/65a3459e504df08826b476c86386cd7f.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/65a3459e504df08826b476c86386cd7f.png)'
- en: Classification tree to classification one of three flower species (IRIS Dataset)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 分类树对三种花卉种类（IRIS 数据集）的分类
- en: '**How to use a Classification Tree**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**如何使用分类树**'
- en: To use a classification tree, start at the root node (brown), and traverse the
    tree until you reach a leaf (terminal) node. Using the classification tree in
    the the image below, imagine you had a flower with a petal length of 4.5 cm and
    you wanted to classify it. Starting at the root node, you would first ask “Is
    the petal length (cm) ≤ 2.45”? The length is greater than 2.45 so that question
    is False. Proceed to the next decision node and ask, “Is the petal length (cm)
    ≤ 4.95”? This is True so you could predict the flower species as versicolor. This
    is just one example.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分类树时，从根节点（棕色）开始，遍历树直到到达叶子（终端）节点。使用下面图中的分类树，假设你有一朵花，花瓣长度为4.5 cm，你想对其进行分类。从根节点开始，你首先会问“花瓣长度（cm）≤
    2.45”？长度大于2.45，因此这个问题为假。继续到下一个决策节点，问“花瓣长度（cm）≤ 4.95”？这是对的，所以你可以预测花的种类为versicolor。这只是一个例子。
- en: '![](../Images/1279716ceb53cae5f0ee4c396b8bdbf3.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1279716ceb53cae5f0ee4c396b8bdbf3.png)'
- en: '**How are Classification Trees Grown? (Non Math Version)**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类树是如何生长的？（非数学版本）**'
- en: A classification tree learns a sequence of if then questions with each question
    involving one feature and one split point. Look at the partial tree below (A),
    the question, “petal length (cm) ≤ 2.45” splits the data into two branches based
    on some value (2.45 in this case). The value between the nodes is called a split
    point. A good value (one that results in largest information gain) for a split
    point is one that does a good job of separating one class from the others. Looking
    at part B of the figure below, all the points to the left of the split point are
    classified as setosa while all the points to the right of the split point are
    classified as versicolor.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 分类树学习的是一系列的“如果…那么…”问题，每个问题涉及一个特征和一个拆分点。看看下面的部分树（A），问题“花瓣长度（cm）≤ 2.45”根据某个值（在此例中为2.45）将数据拆分为两个分支。节点之间的值称为拆分点。一个好的拆分点值（即产生最大信息增益的值）是能够很好地将一个类别与其他类别分开的值。看下面的图B，拆分点左侧的所有点被分类为setosa，而拆分点右侧的所有点被分类为versicolor。
- en: '![](../Images/d3c64db5d8c0282bacb3b24928fa1ad4.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3c64db5d8c0282bacb3b24928fa1ad4.png)'
- en: The figure shows that setosa was correctly classified for all 38 points. It
    is a pure node. Classification trees don’t split on pure nodes. It would result
    in no further information gain. However, impure nodes can split further. Notice
    the rightside of figure B shows that many points are misclassified as versicolor.
    In other words, it contains points that are of two different classes (virginica
    and versicolor). Classification trees are a greedy algorithm which means by default
    it will continue to split until it has a pure node. Again, the algorithm chooses
    the best split point (we will get into mathematical methods in the next section)
    for the impure node.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图中显示所有38个点的setosa分类都正确。它是一个纯节点。分类树不会在纯节点上进行拆分。这会导致没有进一步的信息增益。然而，非纯节点可以进一步拆分。注意图B的右侧显示许多点被错误分类为versicolor。换句话说，它包含了两种不同类别的点（virginica和versicolor）。分类树是一个贪婪算法，这意味着它默认会继续拆分直到得到一个纯节点。算法再次为非纯节点选择最佳的拆分点（我们将在下一节中深入探讨数学方法）。
- en: '![](../Images/80f24154f01eb353c78f83d847b112c7.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80f24154f01eb353c78f83d847b112c7.png)'
- en: In the image above, the tree has a maximum depth of 2\. Tree depth is a measure
    of how many splits a tree can make before coming to a prediction. This process
    could be continued further with more splitting until the tree is as pure as possible.
    The problem with many repetitions of this process is that this can lead to a very
    deep classification tree with many nodes. This often leads to overfitting on the
    training dataset. Luckily, most classification tree implementations allow you
    to control for the maximum depth of a tree which reduces overfitting. For example,
    Python’s scikit-learn allows you to preprune decision trees. In other words, you
    can set the maximum depth to stop the growth of the decision tree past a certain
    depth. For a visual understanding of maximum depth, you can look at the image
    below.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图像中，树的最大深度为2。树的深度是衡量一棵树在做出预测之前能进行多少次拆分的指标。这个过程可以继续进行更多的拆分，直到树尽可能纯净。许多重复这一过程的问题是，这可能会导致一棵非常深的分类树，拥有许多节点。这通常会导致对训练数据集的过拟合。幸运的是，大多数分类树实现允许你控制树的最大深度，从而减少过拟合。例如，Python的scikit-learn允许你预剪枝决策树。换句话说，你可以设置最大深度来停止决策树超过某个深度。要直观理解最大深度，你可以查看下面的图像。
- en: '![Figure](../Images/c15dd3aa17a5097083f415558381cd2b.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/c15dd3aa17a5097083f415558381cd2b.png)'
- en: Classification trees of different depths fit on the IRIS dataset.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 不同深度的分类树在IRIS数据集上的拟合情况。
- en: The Selection Criterion
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择标准
- en: '![](../Images/0f29aac1bae7f47ec6e8e678787e5e36.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f29aac1bae7f47ec6e8e678787e5e36.png)'
- en: This section answers how information gain and two criterion gini and entropy
    are calculated.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回答了信息增益以及Gini和熵这两个标准的计算方法。
- en: This section is really about understanding what is a good split point for root/decision
    nodes on classification trees. Decision trees split on the feature and corresponding
    split point that results in the largest information gain (IG) for a given criterion
    (gini or entropy in this example). Loosely, we can define information gain as
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的重点是理解在分类树上对根节点/决策节点的良好分割点。决策树会在特征和相应的分割点上进行分裂，以获得给定标准（本例中的Gini或熵）的最大信息增益（IG）。大致上，我们可以将信息增益定义为
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For a clearer understanding of parent and children, look at the decision tree
    below.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地理解父节点和子节点，请查看下方的决策树。
- en: '![](../Images/10124ea94f8e023bed0fe6a75d155971.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10124ea94f8e023bed0fe6a75d155971.png)'
- en: A more proper formula for information gain formula is below.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 更为恰当的信息增益公式如下。
- en: '![](../Images/6a295347c0ad85b49c81d1feed0af966.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a295347c0ad85b49c81d1feed0af966.png)'
- en: Since classification trees have binary splits, the formula can be simplified
    into the formula below.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于分类树有二元分裂，因此公式可以简化为如下公式。
- en: '![](../Images/08e00b983acb7d02e836bad0651867a6.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08e00b983acb7d02e836bad0651867a6.png)'
- en: Two common criterion `I`, used to measure the impurity of a node are Gini index
    and entropy.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 两个常见的`I`标准，用于测量节点的 impurity 是Gini指数和熵。
- en: '![](../Images/aed1afe2dbc527a99c2c1a89d7946119.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aed1afe2dbc527a99c2c1a89d7946119.png)'
- en: For the sake of understanding these formulas a bit better, the image below shows
    how information gain was calculated for a decision tree with Gini criterion.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这些公式，下图展示了如何使用Gini标准计算决策树的信息增益。
- en: '![](../Images/8d6d9a72ce75262c2c562dc2029a8bdc.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d6d9a72ce75262c2c562dc2029a8bdc.png)'
- en: The image below shows how information gain was calculated for a decision tree
    with entropy.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了如何计算决策树的熵信息增益。
- en: '![](../Images/717e47477f736322937f65a4002364b2.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/717e47477f736322937f65a4002364b2.png)'
- en: I am not going to go into more detail on this as it should be noted that different
    impurity measures (Gini index and entropy) [usually yield similar results](https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf).
    The graph below shows that Gini index and entropy are very similar impurity criterion.
    I am guessing one of the reasons why Gini is the default value in scikit-learn
    is that entropy might be a little slower to compute (because it makes use of a
    logarithm).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我不打算深入探讨这一点，因为需要指出的是，不同的 impurity 测量（Gini指数和熵）[通常会产生相似的结果](https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf)。下图展示了Gini指数和熵是非常相似的
    impurity 标准。我猜测Gini作为Scikit-learn中的默认值之一的原因可能是熵的计算可能稍慢（因为它使用了对数）。
- en: '![Figure](../Images/2b36fd282c42280a9cc175015eb65141.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/2b36fd282c42280a9cc175015eb65141.png)'
- en: Different impurity measures (Gini index and entropy) [usually yield similar
    results](https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf).
    Thanks to [Data Science StackExchange](https://datascience.stackexchange.com/questions/10228/gini-impurity-vs-entropy) and [Sebastian
    Raschka](https://twitter.com/rasbt) for the inspiration for this graph.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的 impurity 测量（Gini指数和熵）[通常会产生相似的结果](https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf)。感谢[Data
    Science StackExchange](https://datascience.stackexchange.com/questions/10228/gini-impurity-vs-entropy)和[Sebastian
    Raschka](https://twitter.com/rasbt)对本图的启发。
- en: Before finishing this section, I should note that are various decision tree
    algorithms that differ from each other. Some of the more popular algorithms are
    ID3, C4.5, and CART. Scikit-learn uses an [optimized version of the CART algorithm](http://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart).
    You can learn about it’s time complexity [here](http://scikit-learn.org/stable/modules/tree.html#complexity).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成本节之前，我需要指出，存在各种不同的决策树算法，它们彼此有所不同。一些更受欢迎的算法包括ID3、C4.5和CART。Scikit-learn使用了[CART算法的优化版本](http://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart)。你可以在[这里](http://scikit-learn.org/stable/modules/tree.html#complexity)了解其时间复杂度。
- en: Classification Trees using Python
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Python的分类树
- en: The previous sections went over the theory of classification trees. One of the
    reasons why it is good to learn how to make decision trees in a programming language
    is that working with data can help in understanding the algorithm.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的部分讲述了分类树的理论。学习如何在编程语言中制作决策树的一个原因是，处理数据可以帮助理解算法。
- en: Load the Dataset
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载数据集
- en: The Iris dataset is one of datasets scikit-learn comes with that do not require
    the downloading of any file from some external website. The code below loads the
    iris dataset.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Iris 数据集是 scikit-learn 提供的一个数据集，无需从外部网站下载任何文件。下面的代码加载了 iris 数据集。
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Figure](../Images/8f7cea2bde50ecfe1af75c5983f582d5.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/8f7cea2bde50ecfe1af75c5983f582d5.png)'
- en: Original Pandas df (features + target)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 原始 Pandas df（特征 + 目标）
- en: Splitting Data into Training and Test Sets
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将数据拆分为训练集和测试集
- en: The code below puts 75% of the data into a training set and 25% of the data
    into a test set.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码将 75% 的数据放入训练集，将 25% 的数据放入测试集。
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Figure](../Images/7d3764ca625fb8c22e62cdca6532a91c.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/7d3764ca625fb8c22e62cdca6532a91c.png)'
- en: The colors in the image indicate which variable (X_train, X_test, Y_train, Y_test)
    the data from the dataframe df went to for this particular train test split.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图片中的颜色表示数据框 df 的数据在这个特定的训练测试分割中属于哪个变量（X_train, X_test, Y_train, Y_test）。
- en: Note, one of the benefits of Decision Trees is that you don’t have to standardize
    your data unlike [PCA](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60) and
    logistic regression which are [sensitive to effects of not standardizing your
    data](http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，决策树的一个好处是你无需像[PCA](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)和逻辑回归那样对数据进行标准化，因为这两者对[数据未标准化的影响比较敏感](http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py)。
- en: Scikit-learn 4-Step Modeling Pattern
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Scikit-learn 四步建模模式
- en: '**Step 1:** Import the model you want to use'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1:**  导入你想要使用的模型'
- en: In scikit-learn, all machine learning models are implemented as Python classes
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，所有机器学习模型都是作为 Python 类实现的。
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Step 2:** Make an instance of the Model'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2:**  创建模型实例'
- en: In the code below, I set the `max_depth = 2` to preprune my tree to make sure
    it doesn’t have a depth greater than 2\. I should note the next section of the
    tutorial will go over how to choose an optimal `max_depth` for your tree.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我将 `max_depth = 2` 设置为提前修剪我的树，以确保它的深度不超过 2。我应该指出教程的下一部分将介绍如何为你的树选择最佳的
    `max_depth`。
- en: Also note that in my code below, I made `random_state = 0` so that you can get
    the same results as me.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，在我下面的代码中，我设置了 `random_state = 0`，这样你可以得到和我一样的结果。
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Step 3:** Train the model on the data'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3:**  在数据上训练模型'
- en: The model is learning the relationship between X(sepal length, sepal width,
    petal length, and petal width) and Y(species of iris)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 模型正在学习 X（萼片长度、萼片宽度、花瓣长度和花瓣宽度）和 Y（鸢尾花的种类）之间的关系
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Step 4:** Predict labels of unseen (test) data'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 4:**  预测未见（测试）数据的标签'
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Remember, a prediction is just the majority class of the instances in a leaf
    node.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，预测仅仅是叶节点中实例的多数类别。
- en: Measuring Model Performance
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测量模型性能
- en: While there are other ways of measuring model performance (precision, recall,
    F1 Score, [ROC Curve](https://towardsdatascience.com/receiver-operating-characteristic-curves-demystified-in-python-bd531a4364d0),
    etc), we are going to keep this simple and use accuracy as our metric.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管还有其他测量模型性能的方法（如精确率、召回率、F1 分数，[ROC 曲线](https://towardsdatascience.com/receiver-operating-characteristic-curves-demystified-in-python-bd531a4364d0)等），我们将保持简单，使用准确率作为我们的指标。
- en: 'Accuracy is defined as:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率定义为：
- en: '(fraction of correct predictions): correct predictions / total number of data
    points'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: （正确预测的比例）：正确预测数 / 数据点总数
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Tuning the Depth of a Tree
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调整树的深度
- en: Finding the optimal value for`max_depth` is one way way to tune your model.
    The code below outputs the accuracy for decision trees with different values for `max_depth`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 找到 `max_depth` 的最佳值是一种调整模型的方法。下面的代码输出了具有不同 `max_depth` 值的决策树的准确率。
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Since the graph below shows that the best accuracy for the model is when the
    parameter `max_depth` is greater than or equal to 3, it might be best to choose
    the least complicated model with `max_depth = 3`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于下面的图表显示，当参数 `max_depth` 大于或等于 3 时，模型的准确率最佳，因此选择 `max_depth = 3` 的最简单模型可能是最好的选择。
- en: '![Figure](../Images/6b374458fabce5b952a6e233f7e6a67e.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: I choose max_depth =3 as it it seems to be an accurate model and not the most
    complicated.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: It is important to keep in mind that `max_depth` is not the same thing as depth
    of a decision tree. `max_depth` is a way to preprune a decision tree. In other
    words, if a tree is already as pure as possible at a depth, it will not continue
    to split. The image below shows decision trees with `max_depth` values of 3, 4,
    and 5\. Notice that the trees with a `max_depth` of 4 and 5 are identical. They
    both have a depth of 4.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/911049a033b2de4b61830401eb81b236.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: Notice how we have we have two of the exact same trees.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: If you ever wonder what the depth of your trained decision tree is, you can
    use the `get_depth` method. Additionally, you can get the number of leaf nodes
    for a trained decision tree by using the `get_n_leaves` method.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: While this tutorial has covered changing selection criterion (Gini index, entropy,
    etc) and `max_depth` of a tree, keep in mind that you can also tune minimum samples
    for a node to split (`min_samples_leaf`), max number of leaf nodes (`max_leaf_nodes`),
    and more.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Feature Importance
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One advantage of classification trees is that they are relatively easy to interpret.
    Classification trees in scikit-learn allow you to calculate feature importance
    which is the total amount that gini index or entropy decrease due to splits over
    a given feature. Scikit-learn outputs a number between 0 and 1 for each feature.
    All feature importances are normalized to sum to 1\. The code below shows feature
    importances for each feature in a decision tree model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Figure](../Images/f2d0662304902618333fa4e671964ef5.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: In the example above (for a particular train test split of iris), the petal
    width has the highest feature importance weight. We can confirm by looking at
    the corresponding decision tree.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/3f8f902adbb1d014e099e449f749cf78.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: The only two features this decision tree splits on are petal width (cm) and
    petal length (cm),
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that if a feature has a low feature importance value, it doesn’t
    necessarily mean that the feature isn’t important for prediction, it just means
    that the particular feature wasn’t chosen at a particularly early level of the
    tree. It could also be that the feature could be identical or highly correlated
    with another informative feature. Feature importance values also don’t tell you
    which class they are very predictive for or relationships between features which
    may influence prediction. It is important to note that when performing cross validation
    or similar, you can use an average of the feature importance values from multiple
    train test splits.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Concluding Remarks
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While this post only went over decision trees for classification, feel free
    to see my other post Decision Trees for Regression (Python). **C**lassification
    **a**nd **R**egression **T**rees (CART) are a relatively old technique (1984)
    that is the basis for more sophisticated techniques. One of the primary weaknesses
    of decision trees is that they usually aren’t the most accurate algorithm. This
    is partially because decision trees are a high variance algorithm, meaning that
    different splits in the training data can lead to very different trees. If you
    have any questions or thoughts on the tutorial, feel free to reach out in the
    comments below or through [Twitter](https://twitter.com/GalarnykMichael).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这篇文章只讨论了用于分类的决策树，但你可以查看我另一篇文章《决策树用于回归（Python）》。**C**lassification **a**nd
    **R**egression **T**rees (CART) 是一种相对较旧的技术（1984年），是更复杂技术的基础。决策树的主要弱点之一是它们通常不是最准确的算法。这部分是因为决策树是一种高方差算法，这意味着训练数据中的不同分裂可以导致非常不同的树。如果你对教程有任何问题或想法，请随时在下面的评论中或通过
    [Twitter](https://twitter.com/GalarnykMichael) 联系我。
- en: '**Bio: [Michael Galarnyk](https://www.linkedin.com/in/michaelgalarnyk/)** is
    a Data Scientist and Corporate Trainer. He currently works at Scripps Translational
    Research Institute. You can find him on Twitter (https://twitter.com/GalarnykMichael),
    Medium (https://medium.com/@GalarnykMichael), and GitHub (https://github.com/mGalarnyk).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介: [迈克尔·加拉尼克](https://www.linkedin.com/in/michaelgalarnyk/)** 是一名数据科学家和企业培训师。他目前在斯克里普斯转化研究所工作。你可以在
    Twitter (https://twitter.com/GalarnykMichael)、Medium (https://medium.com/@GalarnykMichael)
    和 GitHub (https://github.com/mGalarnyk) 上找到他。'
- en: '[Original](https://towardsdatascience.com/understanding-decision-trees-for-classification-python-9663d683c952).
    Reposted with permission.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/understanding-decision-trees-for-classification-python-9663d683c952)。已获得许可转载。'
- en: '**Related:**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关:**'
- en: '[Decision Trees — An Intuitive Introduction](/2019/02/decision-trees-introduction.html)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[决策树——直观介绍](/2019/02/decision-trees-introduction.html)'
- en: '[How to Build a Data Science Portfolio](/2018/07/build-data-science-portfolio.html)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何建立数据科学作品集](/2018/07/build-data-science-portfolio.html)'
- en: '[Random Forests vs Neural Networks: Which is Better, and When?](/2019/06/random-forest-vs-neural-network.html)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[随机森林与神经网络：哪个更好，何时使用？](/2019/06/random-forest-vs-neural-network.html)'
- en: '* * *'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行IT管理'
- en: '* * *'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Machine Learning from Scratch: Decision Trees](https://www.kdnuggets.com/2022/11/machine-learning-scratch-decision-trees.html)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从零开始的机器学习：决策树](https://www.kdnuggets.com/2022/11/machine-learning-scratch-decision-trees.html)'
- en: '[Decision Trees vs Random Forests, Explained](https://www.kdnuggets.com/2022/08/decision-trees-random-forests-explained.html)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[决策树与随机森林的解释](https://www.kdnuggets.com/2022/08/decision-trees-random-forests-explained.html)'
- en: '[Generalized and Scalable Optimal Sparse Decision Trees(GOSDT)](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[广义和可扩展的最优稀疏决策树（GOSDT）](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
- en: '[Demystifying Decision Trees for the Real World](https://www.kdnuggets.com/demystifying-decision-trees-for-the-real-world)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[揭开现实世界决策树的面纱](https://www.kdnuggets.com/demystifying-decision-trees-for-the-real-world)'
- en: '[Understanding Classification Metrics: Your Guide to Assessing Model…](https://www.kdnuggets.com/understanding-classification-metrics-your-guide-to-assessing-model-accuracy)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解分类指标：评估模型的指南](https://www.kdnuggets.com/understanding-classification-metrics-your-guide-to-assessing-model-accuracy)'
- en: '[Understanding by Implementing: Decision Tree](https://www.kdnuggets.com/2023/02/understanding-implementing-decision-tree.html)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过实现理解：决策树](https://www.kdnuggets.com/2023/02/understanding-implementing-decision-tree.html)'
