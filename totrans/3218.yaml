- en: Top 10 Machine Learning Algorithms for Beginners
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/10/top-10-machine-learning-algorithms-beginners.html/2](https://www.kdnuggets.com/2017/10/top-10-machine-learning-algorithms-beginners.html/2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'V. Unsupervised learning algorithms:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**6\. Apriori**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Apriori algorithm is used in a transactional database to mine frequent
    itemsets and then generate association rules. It is popularly used in market basket
    analysis, where one checks for combinations of products that frequently co-occur
    in the database. In general, we write the association rule for ‘if a person purchases
    item X, then he purchases item Y’ as : X -> Y.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: if a person purchases milk and sugar, then he is likely to purchase
    coffee powder. This could be written in the form of an association rule as: {milk,sugar}
    -> coffee powder. Association rules are generated after crossing the threshold
    for support and confidence.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/644f10b1b1ef53a08eec4a0458b93880.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Formulae for support, confidence and lift for the association rule
    X->Y. [Source](http://chem-eng.utoronto.ca/~datamining/dmc/association_rules.htm)The
    Support measure helps prune the number of candidate itemsets to be considered
    during frequent itemset generation. This support measure is guided by the Apriori
    principle. The Apriori principle states that if an itemset is frequent, then all
    of its subsets must also be frequent.'
  prefs: []
  type: TYPE_NORMAL
- en: '**7\. K-means**'
  prefs: []
  type: TYPE_NORMAL
- en: K-means is an iterative algorithm that groups similar data into clusters.It
    calculates the centroids of k clusters and assigns a data point to that cluster
    having least distance between its centroid and the data point.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fffb3ddbea2ab8b4c211f642866fbe29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Steps of the K-means algorithm. [Source](https://www.packtpub.com/books/content/clustering-and-other-unsupervised-learning-methods)*Step
    1: k-means initialization:*'
  prefs: []
  type: TYPE_NORMAL
- en: a) Choose a value of k. Here, let us take k=3.
  prefs: []
  type: TYPE_NORMAL
- en: b) Randomly assign each data point to any of the 3 clusters.
  prefs: []
  type: TYPE_NORMAL
- en: c) Compute cluster centroid for each of the clusters. The red, blue and green
    stars denote the centroids for each of the 3 clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 2: Associating each observation to a cluster:*'
  prefs: []
  type: TYPE_NORMAL
- en: Reassign each point to the closest cluster centroid. Here, the upper 5 points
    got assigned to the cluster with the blue colour centroid. Follow the same procedure
    to assign points to the clusters containing the red and green colour centroid.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 3: Recalculating the centroids:*'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the centroids for the new clusters. The old centroids are shown by
    gray stars while the new centroids are the red, green and blue stars.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 4: Iterate, then exit if unchanged.*'
  prefs: []
  type: TYPE_NORMAL
- en: Repeat steps 2-3 until there is no switching of points from one cluster to another.
    Once there is no switching for 2 consecutive steps, exit the k-means algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '**8\. PCA**'
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis (PCA) is used to make data easy to explore and
    visualize by reducing the number of variables. This is done by capturing the maximum
    variance in the data into a new co-ordinate system with axes called ‘principal
    components’. Each component is a linear combination of the original variables
    and is orthogonal to one another. Orthogonality between components indicates that
    the correlation between these components is zero.
  prefs: []
  type: TYPE_NORMAL
- en: The first principal component captures the direction of the maximum variability
    in the data. The second principal component captures the remaining variance in
    the data but has variables uncorrelated with the first component. Similarly, all
    successive principal components (PC3, PC4 and so on) capture the remaining variance
    while being uncorrelated with the previous component.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d44c1bcdae7f89b678e8c00568b3fdc4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The 3 original variables (genes) are reduced to 2 new variables termed
    principal components (PC''s). [Source](http://www.nlpca.org/pca_principal_component_analysis.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'VI. Ensemble learning techniques:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ensembling means combining the results of multiple learners (classifiers) for
    improved results, by voting or averaging. Voting is used during classification
    and averaging is used during regression. The idea is that ensembles of learners
    perform better than single learners.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 3 types of ensembling algorithms: Bagging, Boosting and Stacking.
    We are not going to cover ‘stacking’ here, but if you’d like a detailed explanation
    of it, let me know in the comments section below, and I can write a separate blog
    on it.'
  prefs: []
  type: TYPE_NORMAL
- en: '**9\. Bagging with Random Forests**'
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest (multiple learners) is an improvement over bagged decision trees
    (a single learner).
  prefs: []
  type: TYPE_NORMAL
- en: 'Bagging: The first step in bagging is to create multiple models with datasets
    created using the Bootstrap Sampling method. In Bootstrap Sampling, each generated
    trainingset is composed of random subsamples from the original dataset. Each of
    these trainingsets is of the same size as the original dataset, but some records
    repeat multiple times and some records do not appear at all. Then, the entire
    original dataset is used as the testset. Thus, if the size of the original dataset
    is N, then the size of each generated trainingset is also N, with the number of
    unique records being about (2N/3); the size of the testset is also N.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second step in bagging is to create multiple models by using the same algorithm
    on the different generated trainingsets. In this case, let us discuss Random Forest.
    Unlike a decision tree, where each node is split on the best feature that minimizes
    error, in random forests, we choose a random selection of features for constructing
    the best split. The reason for randomness is: even with bagging, when decision
    trees choose a best feature to split on, they end up with similar structure and
    correlated predictions. But bagging after splitting on a random subset of features
    means less correlation among predictions from subtrees.'
  prefs: []
  type: TYPE_NORMAL
- en: The number of features to be searched at each split point is specified as a
    parameter to the random forest algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in bagging with Random Forest, each tree is constructed using a random
    sample of records and each split is constructed using a random sample of predictors.
  prefs: []
  type: TYPE_NORMAL
- en: '**10\. Boosting with AdaBoost**'
  prefs: []
  type: TYPE_NORMAL
- en: a) Bagging is a parallel ensemble because each model is built independently.
    On the other hand, boosting is a sequential ensemble where each model is built
    based on correcting the misclassifications of the previous model.
  prefs: []
  type: TYPE_NORMAL
- en: b) Bagging mostly involves ‘simple voting’, where each classifier votes to obtain
    a final outcome– one that is determined by the majority of the parallel models;
    boosting involves ‘weighted voting’, where each classifier votes to obtain a final
    outcome which is determined by the majority– but the sequential models were built
    by assigning greater weights to misclassified instances of the previous models.
  prefs: []
  type: TYPE_NORMAL
- en: Adaboost stands for Adaptive Boosting.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/480921b711d02ec9e399316c1876264d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Adaboost for a decision tree. [Source](https://sebastianraschka.com/faq/docs/bagging-boosting-rf.html)In
    Figure 9, steps 1, 2, 3 involve a weak learner called a decision stump (a 1-level
    decision tree making a prediction based on the value of only 1 input feature;
    a decision tree with its root immediately connected to its leaves). The process
    of constructing weak learners continues until a user-defined number of weak learners
    has been constructed or until there is no further improvement while training.
    Step 4 combines the 3 decision stumps of the previous models (and thus has 3 splitting
    rules in the decision tree).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 1: Start with 1 decision tree stump to make a decision on 1 input variable:*'
  prefs: []
  type: TYPE_NORMAL
- en: The size of the data points show that we have applied equal weights to classify
    them as a circle or triangle. The decision stump has generated a horizontal line
    in the top half to classify these points. We can see that there are 2 circles
    incorrectly predicted as triangles. Hence, we will assign higher weights to these
    2 circles and apply another decision stump.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 2: Move to another decision tree stump to make a decision on another
    input variable:*'
  prefs: []
  type: TYPE_NORMAL
- en: We observe that the size of the 2 misclassified circles from the previous step
    is larger than the remaining points. Now, the 2^(nd) decision stump will try to
    predict these 2 circles correctly.
  prefs: []
  type: TYPE_NORMAL
- en: As a result of assigning higher weights, these 2 circles have been correctly
    classified by the vertical line on the left. But this has now resulted in misclassifying
    the 3 circles at the top. Hence, we will assign higher weights to these 3 circles
    at the top and apply another decision stump.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 3: Train another decision tree stump to make a decision on another input
    variable.*'
  prefs: []
  type: TYPE_NORMAL
- en: The 3 misclassified circles from the previous step are larger than the rest
    of the data points. Now, a vertical line to the right has been generated to classify
    the circles and triangles.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 4: Combine the decision stumps:*'
  prefs: []
  type: TYPE_NORMAL
- en: We have combined the separators from the 3 previous models and observe that
    the complex rule from this model classifies data points correctly as compared
    to any of the individual weak learners.
  prefs: []
  type: TYPE_NORMAL
- en: 'VII. Conclusion:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To recap, we have learnt:'
  prefs: []
  type: TYPE_NORMAL
- en: 5 supervised learning techniques- Linear Regression, Logistic Regression, CART,
    Naïve Bayes, KNN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3 unsupervised learning techniques- Apriori, K-means, PCA.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2 ensembling techniques- Bagging with Random Forests, Boosting with XGBoost.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In my next blog, we will learn about a technique that has become a rage at Kaggle
    competitions - the XGBoost algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Top Algorithms and Methods used by Data Scientists](/2016/09/poll-algorithms-used-data-scientists.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 10 Algorithms Machine Learning Engineers need to know](/2016/08/10-algorithms-machine-learning-engineers.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top 10 Data Mining Algorithms, Explained](/2015/05/top-10-data-mining-algorithms-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
