- en: Evaluating Object Detection Models Using Mean Average Precision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/03/evaluating-object-detection-models-using-mean-average-precision.html](https://www.kdnuggets.com/2021/03/evaluating-object-detection-models-using-mean-average-precision.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate object detection models like R-CNN and [YOLO](https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/),
    the **mean average precision (mAP)** is used. The mAP compares the ground-truth
    bounding box to the detected box and returns a score. The higher the score, the
    more accurate the model is in its detections.
  prefs: []
  type: TYPE_NORMAL
- en: In my last article we looked in detail at the [confusion matrix, model accuracy,
    precision, and recall](https://blog.paperspace.com/deep-learning-metrics-precision-recall-accuracy).
    We used the Scikit-learn library to calculate these metrics as well. Now we'll
    extend our discussion to see how precision and recall are used to calculate the
    mAP.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the sections covered in this tutorial:'
  prefs: []
  type: TYPE_NORMAL
- en: From Prediction Score to Class Label
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision-Recall Curve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average Precision (AP)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intersection over Union (IoU)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean Average Precision (mAP) for Object Detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: '**From Prediction Score to Class Label**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section we'll do a quick review of how a class label is derived from
    a prediction score.
  prefs: []
  type: TYPE_NORMAL
- en: Given that there are two classes, *Positive* and *Negative*, here are the ground-truth
    labels of 10 samples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: When these samples are fed to the model it returns the following prediction
    scores. Based on these scores, how do we classify the samples (i.e. assign a class
    label to each sample)?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To convert the scores into a class label, **a threshold is used**. When the
    score is equal to or above the threshold, the sample is classified as one class.
    Otherwise, it is classified as the other class. Let's agree that a sample is *Positive* if
    its score is above or equal to the threshold. Otherwise, it is *Negative*. The
    next block of code converts the scores into class labels with a threshold of **0.5**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now both the ground-truth and predicted labels are available in the `y_true` and `y_pred` variables.
    Based on these labels, the [confusion matrix, precision, and recall](https://blog.paperspace.com/deep-learning-metrics-precision-recall-accuracy/) can
    be calculated.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: After this quick review of calculating the precision and recall, in the next
    section we'll discuss creating the precision-recall curve.
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision-Recall Curve**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From the definition of both the precision and recall given in [Part 1](https://blog.paperspace.com/deep-learning-metrics-precision-recall-accuracy/),
    remember that the higher the precision, the more confident the model is when it
    classifies a sample as *Positive*. The higher the recall, the more positive samples
    the model correctly classified as *Positive*.
  prefs: []
  type: TYPE_NORMAL
- en: When a model has high recall but low precision, then the model classifies most
    of the positive samples correctly but it has many false positives (i.e. classifies
    many *Negative* samples as *Positive*). When a model has high precision but low
    recall, then the model is accurate when it classifies a sample as *Positive* but
    it may classify only some of the positive samples.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Due to the importance of both precision and recall, there is a **precision-recall
    curve** the shows the tradeoff between the precision and recall values for different
    thresholds. This curve helps to select the best threshold to maximize both metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some inputs needed to create the precision-recall curve:'
  prefs: []
  type: TYPE_NORMAL
- en: The ground-truth labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The prediction scores of the samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some thresholds to convert the prediction scores into class labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next block of code creates the `y_true` list to hold the ground-truth labels,
    the `pred_scores` list for the prediction scores, and finally the `thresholds` list
    for different threshold values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here are the thresholds saved in the `thresholds` list. Because there are 10
    thresholds, 10 values for precision and recall will be created.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The next function named `precision_recall_curve()` accepts the ground-truth
    labels, prediction scores, and thresholds. It returns two equal-length lists representing
    the precision and recall values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The next code calls the `precision_recall_curve()` function after passing the
    three previously prepared lists. It returns the `precisions` and `recalls` lists
    that hold all the values of the precisions and recalls, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here are the returned values in the `precisions` list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here is the list of  values in the `recalls` list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Given the two lists of equal lengths, it is possible to plot their values in
    a 2D plot as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The precision-recall curve is shown in the next figure. Note that as the recall
    increases, the precision decreases. The reason is that when the number of positive
    samples increases (high recall), the accuracy of classifying each sample correctly
    decreases (low precision). This is expected, as the model is more likely to fail
    when there are many samples.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/485ef820cd31ff4ef7c85f2abce540ba.png)'
  prefs: []
  type: TYPE_IMG
- en: The precision-recall curve makes it easy to decide the point where both the
    precision and recall are high. According to the previous figure, the best point
    is `(recall, precision)=(0.778, 0.875)`.
  prefs: []
  type: TYPE_NORMAL
- en: Graphically deciding the best values for both the precision and recall might
    work using the previous figure because the curve is not complex. A better way
    is to use a metric called the `f1` score, which is calculated according to the
    next equation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55be9dd9d291f369c35c1c9908412e0f.png)'
  prefs: []
  type: TYPE_IMG
- en: The `f1` metric measures the balance between precision and recall. When the
    value of `f1` is high, this means both the precision and recall are high. A lower `f1` score
    means a greater imbalance between precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: According to the previous example, the `f1` is calculated according to the code
    below. According to the values in the `f1` list, the highest score is `0.82352941`.
    It is the 6th element in the list (i.e. index 5). The 6th elements in the `recalls` and `precisions` lists
    are `0.778` and `0.875`, respectively. The corresponding threshold value is `0.45`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The next figure shows, in blue, the location of the point that corresponds to
    the best balance between the recall and the precision. In conclusion, the best
    threshold to balance the precision and recall is `0.45` at which the precision
    is `0.875` and the recall is `0.778`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9d90ccf37d25964e2c5bb5fe5eb218d1.png)'
  prefs: []
  type: TYPE_IMG
- en: After the precision-recall curve is discussed, the next section discusses how
    to calculate the **average precision**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Average Precision (AP)**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **average precision (AP)** is a way to summarize the precision-recall curve
    into a single value representing the average of all precisions. The AP is calculated
    according to the next equation. Using a loop that goes through all precisions/recalls,
    the difference between the current and next recalls is calculated and then multiplied
    by the current precision. In other words, the AP is the weighted sum of precisions
    at each threshold where the weight is the increase in recall.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b4fb8d00f4ebb4cfde3e78063126b80e.png)'
  prefs: []
  type: TYPE_IMG
- en: It is important to append the recalls and precisions lists by 0 and 1, respectively.
    For example, if the `recalls` list is0.8,0.60.8,0.6, then it should have 0 appended
    to be0.8,0.6,0.00.8,0.6,0.0\. The same happens for the `precisions` list but have
    1 rather than 0 appended (e.g.0.8,0.2,1.00.8,0.2,1.0).
  prefs: []
  type: TYPE_NORMAL
- en: Given that both `recalls` and `precisions` are NumPy arrays, the previous equation
    is modeled according to the next Python line.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here is the complete code that calculates the AP.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This is all about the average precision. Here is a summary of the steps to
    calculate the AP:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate the **prediction scores** using the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the **prediction scores** to **class labels**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the **confusion matrix**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the **precision** and **recall** metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the **precision-recall curve**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measure the **average precision**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next section talks about the **intersection over union (IoU)** which is
    how an object detection generates the prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '**Intersection over Union (IoU)**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To train an object detection model, usually, there are 2 inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: An image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ground-truth bounding boxes for each object in the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model predicts the bounding boxes of the detected objects. It is expected
    that the predicted box will not match exactly the ground-truth box. The next figure
    shows a cat image. The ground-truth box of the object is in red while the predicted
    one is in yellow. Based on the visualization of the 2 boxes, is the model made
    a good prediction with a high match score?
  prefs: []
  type: TYPE_NORMAL
- en: It is difficult to subjectively evaluate the model predictions. For example,
    someone may conclude that there is a 50% match while someone else notices that
    there is a 60% match.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3db505280cbb6dd521581b900e6e1d7c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image](https://pixabay.com/photos/cat-young-animal-curious-wildcat-2083492) without
    labels from [Pixabay](https://pixabay.com/photos/cat-young-animal-curious-wildcat-2083492) by [susannp4](https://pixabay.com/users/susannp4-1777190)'
  prefs: []
  type: TYPE_NORMAL
- en: A better alternative is to use a quantitative measure to score how the ground-truth
    and predicted boxes match. This measure is the **intersection over union (IoU)**.
    The IoU helps to know if a region has an object or not.
  prefs: []
  type: TYPE_NORMAL
- en: The IoU is calculated according to the next equation by dividing the area of
    intersection between the 2 boxes by the area of their union. The higher the IoU,
    the better the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/65c006c1e81f14f0dffb941e8ddf7fcf.png)'
  prefs: []
  type: TYPE_IMG
- en: The next figure shows 3 cases with different IoUs. Note that the IoUs at the
    top of each case are objectively measured and may differ a bit from the reality
    but it makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: For case A, the predicted box in yellow is so far from being aligned on the
    red ground-truth box and thus the IoU score is **0.2** (i.e. there is only a 20%
    overlap between the 2 boxes).
  prefs: []
  type: TYPE_NORMAL
- en: For case B, the intersection area between the 2 boxes is larger but the 2 boxes
    are still not aligned well and thus the IoU score is **0.5**.
  prefs: []
  type: TYPE_NORMAL
- en: For case C, the coordinates of the 2 boxes are so close and thus their IoU is **0.9** (i.e.
    there is a 90% overlap between the 2 boxes).
  prefs: []
  type: TYPE_NORMAL
- en: Note that the IoU is 0.0 when there is a 0% overlap between the predicted and
    ground-truth boxes. The IoU is **1.0** when the 2 boxes fit each other 100%.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7bb321e12b722d04b585731116b71d26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To calculate the IoU for an image, here is a function named `intersection_over_union()`.
    It accepts the following 2 parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`gt_box`: Ground-truth bounding box.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`pred_box`: Predicted bounding box.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It calculates the intersection and union between the 2 boxes in the `intersection` and `union` variables,
    respectively. Moreover, the IoU is calculated in the `iou` variable. It returns
    all of these 3 variables.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The bounding box passed to the function is a list of 4 elements which are:'
  prefs: []
  type: TYPE_NORMAL
- en: The x-axis of the top-left corner.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The y-axis of the top-left corner.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Width.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Height.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here are the ground-truth and predicted bounding boxes of the car image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Given that the image is named `cat.jpg`, here is the complete that draws the
    bounding boxes over the image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The next figure shows the image with the bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3081d86d1a81996b3a304c922d647b98.png)'
  prefs: []
  type: TYPE_IMG
- en: To calculate the IoU, just call the `intersection_over_union()` function. Based
    on the bounding boxes, the IoU score is `0.54`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The IoU score **0.54** means there is a 54% overlap between the ground-truth
    and predicted bounding boxes. Looking at the boxes, someone may visually feel
    it is good enough to conclude that the model detected the cat object. Someone
    else may feel the model is not yet accurate as the predicted box does not fit
    the ground-truth box well.
  prefs: []
  type: TYPE_NORMAL
- en: To objectively judge whether the model predicted the box location correctly
    or not, a **threshold** is used. If the model predicts a box with an IoU score
    greater than or equal to the **threshold**, then there is a high overlap between
    the predicted box and one of the ground-truth boxes. This means the model was
    able to detect an object successfully. The detected region is classified as **Positive** (i.e.
    contains an object).
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, when the IoU score is smaller than the threshold, then the
    model made a bad prediction as the predicted box does not overlap with the ground-truth
    box. This means the detected region is classified as **Negative** (i.e. does not
    contain an object).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/66f31d5e3d700c35d5db1fab90bd1ace.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's have an example to clarify how the IoU scores help to classify a region
    as an object or not. Assume the object detection model is fed by the next image
    where there are 2 target objects with their ground-truth boxes in red and the
    predicted boxes are in yellow.
  prefs: []
  type: TYPE_NORMAL
- en: The next code reads the image (given it is named `pets.jpg`), draws the boxes,
    and calculates the IoU for each object. The IoU for the left object is **0.76** while
    the other object has an IoU score of **0.26**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Given that the IoU threshold is 0.6, then only the regions with IoU scores greater
    than or equal to 0.6 are classified as **Positive** (i.e. having objects). Thus,
    the box with IoU score 0.76 is Positive while the other box with IoU of 0.26 is **Negative**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d66ca57f3e5d41315a023aabd18ea039.png)'
  prefs: []
  type: TYPE_IMG
- en: Image without Labels from [hindustantimes.com](https://www.hindustantimes.com/rf/image_size_960x540/HT/p2/2019/05/25/Pictures/_b336a8c4-7e7b-11e9-8a88-8b84fe2ad6da.png)
  prefs: []
  type: TYPE_NORMAL
- en: If the threshold changed to be **0.2** rather than 0.6, then both predictions
    are **Positive**. If the threshold is **0.8**, then both predictions are **Negative**.
  prefs: []
  type: TYPE_NORMAL
- en: As a summary, the IoU score measures how close is the predicted box to the ground-truth
    box. It ranges from 0.0 to 1.0 where 1.0 is the optimal result. When the IoU is
    greater than the threshold, then the box is classified as **Positive** as it surrounds
    an object. Otherwise, it is classified as **Negative**.
  prefs: []
  type: TYPE_NORMAL
- en: The next section shows how to benefit from the IoUs to calculate the mean average
    precision (mAP) for an object detection model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean Average Precision (mAP) for Object Detection**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Usually, the object detection models are evaluated with different IoU thresholds
    where each threshold may give different predictions from the other thresholds.
    Assume that the model is fed by an image that has 10 objects distributed across
    2 classes. How to calculate the mAP?
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the mAP, start by calculating the AP for each class. The mean of
    the APs for all classes is the mAP.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming that the dataset used has only 2 classes. For the first class, here
    are the ground-truth labels and predicted scores in the `y_true` and `pred_scores` variables,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Here are the `y_true` and `pred_scores` variables of the second class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The list of IoU thresholds starts from 0.2 to 0.9 with 0.25 step.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: To calculate the AP for a class, just feed its `y_true` and `pred_scores` variables
    to the next code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: For the first class, here is its precision-recall curve. Based on this curve,
    the AP is `0.949`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4291b5f650a2af303082cb38dbe712b.png)'
  prefs: []
  type: TYPE_IMG
- en: The precision-recall curve of the second class is shown below. Its AP is `0.958`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e8d11fad5d3e86f59a810c177fa82ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Based on the APs of the 2 classes (0.949 and 0.958), the mAP of the object detection
    model is calculated according to the next equation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/365f7ef8e3a3e4bbdb735aaad8aabed7.png)'
  prefs: []
  type: TYPE_IMG
- en: Based on this equation, the mAP is `0.9535`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '**Conclusion**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This tutorial discussed how to calculate the mean average precision (mAP) for
    an object detection model. We started by discussing how to convert a prediction
    score to a class label. Using different thresholds, a precision-recall curve is
    created. From that curve, the average precision (AP) is measured.
  prefs: []
  type: TYPE_NORMAL
- en: For an object detection model, the threshold is the intersection over union
    (IoU) that scores the detected objects. Once the AP is measured for each class
    in the dataset, the mAP is calculated.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Ahmed Gad](https://www.linkedin.com/in/ahmedfgad/)** received his B.Sc.
    degree with excellent with honors in information technology from the Faculty of
    Computers and Information (FCI), Menoufia University, Egypt, in July 2015\. For
    being ranked first in his faculty, he was recommended to work as a teaching assistant
    in one of the Egyptian institutes in 2015 and then in 2016 to work as a teaching
    assistant and a researcher in his faculty. His current research interests include
    deep learning, machine learning, artificial intelligence, digital signal processing,
    and computer vision.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://blog.paperspace.com/mean-average-precision/). Reposted with
    permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Evaluating Deep Learning Models: The Confusion Matrix, Accuracy, Precision,
    and Recall](/2021/02/evaluating-deep-learning-models-confusion-matrix-accuracy-precision-recall.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Working With The Lambda Layer in Keras](/2021/01/working-lambda-layer-keras.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Create Custom Real-time Plots in Deep Learning](/2020/12/create-custom-real-time-plots-deep-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Idiot''s Guide to Precision, Recall, and Confusion Matrix](https://www.kdnuggets.com/2020/01/guide-precision-recall-confusion-matrix.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Classification Metrics Walkthrough: Logistic Regression with…](https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Confusion Matrix, Precision, and Recall Explained](https://www.kdnuggets.com/2022/11/confusion-matrix-precision-recall-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, November 16: How LinkedIn Uses Machine Learning •…](https://www.kdnuggets.com/2022/n45.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets™ News 22:n09, Mar 2: Telling a Great Data Story: A…](https://www.kdnuggets.com/2022/n09.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Is the Difference Between SQL and Object-Relational Mapping (ORM)?](https://www.kdnuggets.com/2022/02/difference-sql-object-relational-mapping-orm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
