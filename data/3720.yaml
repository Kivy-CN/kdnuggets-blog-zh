- en: Converting Text Documents to Token Counts with CountVectorizer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CountVectorizer 将文本文件转换为令牌计数
- en: 原文：[https://www.kdnuggets.com/2022/10/converting-text-documents-token-counts-countvectorizer.html](https://www.kdnuggets.com/2022/10/converting-text-documents-token-counts-countvectorizer.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/10/converting-text-documents-token-counts-countvectorizer.html](https://www.kdnuggets.com/2022/10/converting-text-documents-token-counts-countvectorizer.html)
- en: We interact with machines on a daily basis – whether it's asking “OK Google,
    set the alarm for 6 AM” or “Alexa, play my favorite playlist”. But these machines
    do not understand natural language. So what happens when we talk to a device?
    It needs to convert the speech i.e. text to numbers for processing the information
    and learning the context. In this post, you will learn one of the most popular
    tools to convert the language to numbers using CountVectorizer. [Scikit-learn’s
    CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)
    is used to recast and preprocess corpora of text to a token count vector representation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们每天都在与机器互动——无论是询问“OK Google，设置早上6点的闹钟”还是“Alexa，播放我喜欢的播放列表”。但这些机器并不理解自然语言。那么当我们与设备交谈时会发生什么？它需要将语音即文本转换为数字，以便处理信息并学习上下文。在这篇文章中，你将学习一种使用
    CountVectorizer 将语言转换为数字的流行工具。[Scikit-learn 的 CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)
    用于将文本语料库重新转换并预处理为令牌计数向量表示。
- en: '![Converting Text Documents to Token Counts with CountVectorizer](../Images/fc067cd21dca6b31af364682a86f4fc8.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![使用 CountVectorizer 将文本文件转换为令牌计数](../Images/fc067cd21dca6b31af364682a86f4fc8.png)'
- en: '[Source](https://img.freepik.com/free-vector/human-hand-typing-computer-with-different-symbols-located-near-cup-coffee-side-view-open-laptop-flat-vector-illustration-new-technologies-millennials-work-concept_74855-21931.jpg?w=1480&t=st=1665332617~exp=1665333217~hmac=75d8c5ae364cabb50f74dbc02bc175292145468ab3c10b2f506adff151278f11)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://img.freepik.com/free-vector/human-hand-typing-computer-with-different-symbols-located-near-cup-coffee-side-view-open-laptop-flat-vector-illustration-new-technologies-millennials-work-concept_74855-21931.jpg?w=1480&t=st=1665332617~exp=1665333217~hmac=75d8c5ae364cabb50f74dbc02bc175292145468ab3c10b2f506adff151278f11)'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的 IT 工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: How does it work?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: Let's take an example of a book title from a popular kids' book to illustrate
    how CountVectorizer works.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以一本流行儿童书的书名为例来说明 CountVectorizer 是如何工作的。
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: There are six unique words in the vector; thus the length of the vector representation
    is six. The vector represents the frequency of occurrence of each token/word in
    the text.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 向量中有六个独特的词，因此向量表示的长度为六。该向量表示每个令牌/词在文本中的出现频率。
- en: '![Converting Text Documents to Token Counts with CountVectorizer](../Images/aa0ff5a87c7c089c1e76b8275f32a5be.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![使用 CountVectorizer 将文本文件转换为令牌计数](../Images/aa0ff5a87c7c089c1e76b8275f32a5be.png)'
- en: Let's add another document to our corpora to witness how the dimension of the
    resulting matrix increases.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们添加另一个文档到我们的语料库中，以观察结果矩阵的维度如何增加。
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The CountVectorizer would produce the below output, where the matrix becomes
    a 2 X 13 from 1 X 6 by adding one more document.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: CountVectorizer 将生成以下输出，通过添加一个文档，矩阵从 1 X 6 变为 2 X 13。
- en: '![Converting Text Documents to Token Counts with CountVectorizer](../Images/10bd82304842443ba4df35a023c1fea0.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![使用 CountVectorizer 将文本文件转换为令牌计数](../Images/10bd82304842443ba4df35a023c1fea0.png)'
- en: Each column in the matrix represents a unique token (word) in the dictionary
    formed by a union of all tokens from the corpus of documents, while each row represents
    a document. The above example has two book titles i.e. documents represented by
    two rows where each cell contains a value identifying the corresponding word count
    in the document. As a result of such representation, certain cells have zero value
    wherever the token is absent in the corresponding document.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵中的每一列表示由语料库中的所有令牌组成的字典中的唯一令牌（单词），而每一行表示一个文档。上述示例有两个书名，即由两行表示的文档，每个单元格包含一个值，标识文档中对应的单词计数。由于这种表示方式，某些单元格在对应文档中令牌缺失时会有零值。
- en: Notably, it becomes unmanageable to store huge matrices in memory with the increasing
    size of the corpora. Thus, CountVectorizer stores them as a sparse matrix, a compressed
    form of the full-blown matrix discussed above.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，随着语料库规模的增加，将巨大的矩阵存储在内存中变得不可管理。因此，CountVectorizer 将它们存储为稀疏矩阵，这是一种压缩形式的完整矩阵。
- en: Hands-On!
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实操！
- en: Let's pick the Harry Potter series of eight movies and one Indiana Jones movie
    for this demo. This would help us understand some important attributes of CountVectorizer.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们选择《哈利·波特》系列的八部电影和一部《印第安纳·琼斯》电影进行演示。这将帮助我们了解 CountVectorizer 的一些重要属性。
- en: Start with importing Pandas library and CountVectorizer from Sklearn > feature_extraction
    > text.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先导入 Pandas 库和来自 Sklearn > feature_extraction > text 的 CountVectorizer。
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Declare the documents as a list of strings.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 将文档声明为字符串列表。
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Vectorization
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量化
- en: Initialize the CountVectorizer object with lowercase=True (default value) to
    convert all documents/strings into lowercase. Next, call fit_transform and pass
    the list of documents as an argument followed by adding column and row names to
    the data frame.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化 CountVectorizer 对象时设置 lowercase=True（默认值）以将所有文档/字符串转换为小写。接下来，调用 fit_transform
    并将文档列表作为参数传递，然后将列和行名称添加到数据框中。
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Good news! The documents are converted to numbers. But, a close look shows that
    “Harry Potter and the Order of the Phoenix” is similar to “Indiana Jones and the
    Raiders of the Lost Ark” as compared to other Harry Potter movies - at least at
    the first glance.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息！文档已转换为数字。但仔细观察会发现，“哈利·波特与凤凰令”与“印第安纳·琼斯与失落的圣杯”相比，与其他哈利·波特电影相似——至少在初步观察时是这样。
- en: '![Converting Text Documents to Token Counts with CountVectorizer](../Images/c6fd74a880e106bbbd3cf1d5aa1d74d5.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![使用 CountVectorizer 将文本文件转换为令牌计数](../Images/c6fd74a880e106bbbd3cf1d5aa1d74d5.png)'
- en: You must be wondering if tokens like ‘and’, ‘the’, and ‘of’ add any information
    to our feature set. That takes us to our next step i.e. removing stop words.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你一定在想像 ‘and’、‘the’ 和 ‘of’ 这样的令牌是否对我们的特征集添加了任何信息。这引导我们到下一步，即去除停用词。
- en: stop_words
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: stop_words
- en: Uninformative tokens like ‘and’, ‘the’, and ‘of’ are called stop words. It is
    important to remove stop words as they impact the document's similarity and unnecessarily
    expand the column dimension.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 像 ‘and’、‘the’ 和 ‘of’ 这样的无用令牌称为停用词。去除停用词很重要，因为它们会影响文档的相似性，并不必要地扩展列的维度。
- en: Argument ‘stop_words’ removes such preidentified stop words – specifying ‘english’
    removes English-specific stop words. You can also explicitly add a list of stop
    words i.e. stop_words = [‘and’, ‘of’, ‘the’].
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 ‘stop_words’ 删除这些预先识别的停用词——指定 ‘english’ 会删除特定于英语的停用词。你还可以显式添加一个停用词列表，即 stop_words
    = [‘and’, ‘of’, ‘the’]。
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Looks better! Now the row vectors look more meaningful.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来更好！现在行向量看起来更有意义。
- en: '![Converting Text Documents to Token Counts with CountVectorizer](../Images/4bc31a8008f051607bac5c87b88c1cb1.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![使用 CountVectorizer 将文本文件转换为令牌计数](../Images/4bc31a8008f051607bac5c87b88c1cb1.png)'
- en: max_df
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: max_df
- en: Words like “Harry” and “Potter” aren’t “stop words” but are quite common and
    add little information to the Count Matrix. Hence, you can add max_df argument
    to stem repetitive words as features.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 像 “Harry” 和 “Potter” 这样的单词不是“停用词”，但它们相当常见，对 Count Matrix 添加的信息很少。因此，你可以添加 max_df
    参数来将重复的单词作为特征进行处理。
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Below output demonstrates that stop words as well as “harry” and “potter” are
    removed from columns:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出演示了停用词以及“harry”和“potter”从列中被删除：
- en: '![Converting Text Documents to Token Counts with CountVectorizer](../Images/6572c77d22e3d8fc438cc45c58ffde9d.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![使用 CountVectorizer 将文本文件转换为令牌计数](../Images/6572c77d22e3d8fc438cc45c58ffde9d.png)'
- en: min_df
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: min_df
- en: It is exactly opposite to max_df and signifies the least number of documents
    (or proportion and percentage) that should have the particular feature.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 它正好与 max_df 相反，表示应具有特定特征的最小文档数量（或比例和百分比）。
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here the below columns (words) are present in at least two documents.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列（单词）在至少两个文档中出现。
- en: '![Converting Text Documents to Token Counts with CountVectorizer](../Images/e7a006bf406429d69c168750e6f31a72.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![使用 CountVectorizer 将文本文档转换为令牌计数](../Images/e7a006bf406429d69c168750e6f31a72.png)'
- en: max_features
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: max_features
- en: It represents the topmost occurring features/words/columns.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 它代表最常出现的特征/单词/列。
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Top four commonly occurring words are chosen below.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 选择了下列四个最常出现的单词。
- en: '![Converting Text Documents to Token Counts with CountVectorizer](../Images/f1dff0e8706c7f132871f6d592a61937.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![使用 CountVectorizer 将文本文档转换为令牌计数](../Images/f1dff0e8706c7f132871f6d592a61937.png)'
- en: binary
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: binary
- en: The binary argument replaces all positive occurrences of words by ‘1’ in a document.
    It signifies the presence or absence of a word or token instead of frequency and
    is useful in analysis like sentiment or product review.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: binary 参数将文档中所有正出现的单词替换为‘1’。它表示单词或令牌的存在与否，而不是频率，并且在情感分析或产品评论等分析中非常有用。
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Upon comparing with the previous output, the frequency table of the column
    named “the” is capped to ‘1’ in the result shown below:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与先前的输出进行比较后，名为“the”的列的频率表在下方结果中被限制为‘1’：
- en: '![Converting Text Documents to Token Counts with CountVectorizer](../Images/4b36f2b5d183f6a654b4c7c685626dbc.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![使用 CountVectorizer 将文本文档转换为令牌计数](../Images/4b36f2b5d183f6a654b4c7c685626dbc.png)'
- en: vocabulary_
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: vocabulary_
- en: It returns the position of columns and is used to map algorithm results to interpretable
    words.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回列的位置，并用于将算法结果映射到可解释的单词。
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The output of the above code is shown below.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下所示。
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Summary
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The tutorial discussed the importance of pre-processing text aka vectorizing
    it as an input into machine learning algorithms. The post also demonstrated sklearn’s
    implementation of CountVectorizer with various input parameters on a small set
    of documents.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程讨论了文本预处理的重要性，即将其矢量化作为机器学习算法的输入。文章还展示了 sklearn 实现 CountVectorizer 的各种输入参数在一小组文档上的应用。
- en: '**[Vidhi Chugh](https://vidhi-chugh.medium.com/)** is an award-winning AI/ML
    innovation leader and an AI Ethicist. She works at the intersection of data science,
    product, and research to deliver business value and insights. She is an advocate
    for data-centric science and a leading expert in data governance with a vision
    to build trustworthy AI solutions.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Vidhi Chugh](https://vidhi-chugh.medium.com/)** 是一位获奖的 AI/ML 创新领袖和 AI 伦理学家。她在数据科学、产品和研究的交汇点上工作，以提供业务价值和见解。她倡导以数据为中心的科学，是数据治理领域的领先专家，致力于构建值得信赖的
    AI 解决方案。'
- en: More On This Topic
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题
- en: '[Classifying Long Text Documents Using BERT](https://www.kdnuggets.com/2022/02/classifying-long-text-documents-bert.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 BERT 对长文本文档进行分类](https://www.kdnuggets.com/2022/02/classifying-long-text-documents-bert.html)'
- en: '[Convert Text Documents to a TF-IDF Matrix with tfidfvectorizer](https://www.kdnuggets.com/2022/09/convert-text-documents-tfidf-matrix-tfidfvectorizer.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 tfidfvectorizer 将文本文档转换为 TF-IDF 矩阵](https://www.kdnuggets.com/2022/09/convert-text-documents-tfidf-matrix-tfidfvectorizer.html)'
- en: '[5 Ways of Converting Unstructured Data into Structured Insights with LLMs](https://www.kdnuggets.com/5-ways-of-converting-unstructured-data-into-structured-insights-with-llms)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5 种将非结构化数据转换为结构化见解的方法](https://www.kdnuggets.com/5-ways-of-converting-unstructured-data-into-structured-insights-with-llms)'
- en: '[Converting JSONs to Pandas DataFrames: Parsing Them the Right Way](https://www.kdnuggets.com/converting-jsons-to-pandas-dataframes-parsing-them-the-right-way)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[将 JSON 转换为 Pandas DataFrames：正确解析 JSON](https://www.kdnuggets.com/converting-jsons-to-pandas-dataframes-parsing-them-the-right-way)'
- en: '[GPT4All is the Local ChatGPT for your Documents and it is Free!](https://www.kdnuggets.com/2023/06/gpt4all-local-chatgpt-documents-free.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT4All 是您文档的本地 ChatGPT，而且是免费的！](https://www.kdnuggets.com/2023/06/gpt4all-local-chatgpt-documents-free.html)'
- en: '[How to Use ChatGPT to Convert Text into a PowerPoint Presentation](https://www.kdnuggets.com/2023/08/chatgpt-convert-text-powerpoint-presentation.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 ChatGPT 将文本转换为 PowerPoint 演示文稿](https://www.kdnuggets.com/2023/08/chatgpt-convert-text-powerpoint-presentation.html)'
