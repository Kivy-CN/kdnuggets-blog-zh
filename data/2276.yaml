- en: Exploring Unsupervised Learning Metrics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索无监督学习指标
- en: 原文：[https://www.kdnuggets.com/2023/04/exploring-unsupervised-learning-metrics.html](https://www.kdnuggets.com/2023/04/exploring-unsupervised-learning-metrics.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/04/exploring-unsupervised-learning-metrics.html](https://www.kdnuggets.com/2023/04/exploring-unsupervised-learning-metrics.html)
- en: '![Exploring Unsupervised Learning Metrics](../Images/2f60ef6cdd25fafa7f251367fc11a8b4.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![探索无监督学习指标](../Images/2f60ef6cdd25fafa7f251367fc11a8b4.png)'
- en: Image by [rawpixel](https://www.freepik.com/free-vector/global-communication-background-business-network-vector-design_19585255.htm#query=clustering&position=2&from_view=search&track=sph)
    on [Freepik](https://www.freepik.com/)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [rawpixel](https://www.freepik.com/free-vector/global-communication-background-business-network-vector-design_19585255.htm#query=clustering&position=2&from_view=search&track=sph)
    提供，来源于 [Freepik](https://www.freepik.com/)
- en: Unsupervised learning is a branch of machine learning where the models learn
    patterns from the available data rather than provided with the actual label. We
    let the algorithm come up with the answers.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习是机器学习的一个分支，其中模型从可用数据中学习模式，而不是提供实际标签。我们让算法自己得出答案。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织IT工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In unsupervised learning, there are two main techniques; clustering and dimensionality
    reduction. The clustering technique uses an algorithm to learn the pattern to
    segment the data. In contrast, the dimensionality reduction technique tries to
    reduce the number of features by keeping the actual information intact as much
    as possible.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，有两种主要技术：聚类和降维。聚类技术使用算法学习模式以分割数据。而降维技术则试图减少特征数量，同时尽可能保留实际信息。
- en: An example algorithm for clustering is K-Means, and for dimensionality reduction
    is PCA. These were the most used algorithm for unsupervised learning. However,
    we rarely talk about the metrics to evaluate unsupervised learning.  As useful
    as it is, we still need to evaluate the result to know if the output is precise.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于聚类的示例算法是K-Means，用于降维的算法是PCA。这些是无监督学习中使用最多的算法。然而，我们很少讨论评估无监督学习的指标。尽管它很有用，我们仍然需要评估结果以了解输出是否准确。
- en: This article will discuss the metrics used to evaluate unsupervised machine
    learning algorithms and will be divided into two sections; Clustering algorithm
    metrics and dimensionality reduction metrics. Let’s get into it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将讨论用于评估无监督机器学习算法的指标，并分为两个部分：聚类算法指标和降维指标。让我们深入了解。
- en: Clustering Algorithm Metrics
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类算法指标
- en: We would not discuss in detail about the clustering algorithm as it’s not the
    main point of this article. Instead, we would focus on examples of the metrics
    used for the evaluation and how to assess the result.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会详细讨论聚类算法，因为这不是本文的重点。相反，我们将重点关注用于评估的指标示例以及如何评估结果。
- en: This article will use the [Wine Dataset from Kaggle](https://www.kaggle.com/datasets/harrywang/wine-dataset-for-clustering)
    as our dataset example. Let’s read the data first and use the K-Means algorithm
    to segment the data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将使用 [Kaggle的葡萄酒数据集](https://www.kaggle.com/datasets/harrywang/wine-dataset-for-clustering)
    作为数据集示例。首先读取数据，然后使用K-Means算法对数据进行分段。
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: I initiate the cluster as 4, which means we segment the data into 4 clusters.
    Is it the right number of clusters? Or is there any more suitable cluster number?
    Commonly, we can use the technique called the **elbow method** to find the appropriate
    cluster. Let me show the code below.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我将集群数量初始化为4，这意味着我们将数据分成4个集群。这是正确的集群数量吗？还是有更合适的集群数量？通常，我们可以使用称为**肘部法则**的技术来找到适当的集群数量。让我在下面展示代码。
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Exploring Unsupervised Learning Metrics](../Images/786421ebff712af35e633c515a215698.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![探索无监督学习指标](../Images/786421ebff712af35e633c515a215698.png)'
- en: In the elbow method, we use WCSS or Within-Cluster Sum of Squares to calculate
    the sum of squared distances between data points and the respective cluster centroids
    for various k (clusters). The best k value is expected to be the one with the
    most decrease of WCSS or the elbow in the picture above, which is 2.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在肘部法中，我们使用 WCSS 或簇内平方和来计算数据点与各自簇的质心之间的平方距离之和，针对不同的 k（簇）。最佳的 k 值预计是 WCSS 减少最多的值，或者上图中的肘部，即
    2。
- en: However, we can expand the elbow method to use other metrics to find the best
    k. How about the algorithm automatically finding the cluster number without relying
    on the centroid? Yes, we can also evaluate them using similar metrics.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我们可以扩展肘部法来使用其他指标来找到最佳的 k 值。如何让算法自动找到簇的数量而不依赖于质心呢？是的，我们也可以使用类似的指标来评估它们。
- en: As a note, we can assume a centroid as the data mean for each cluster even though
    we don’t use the K-Means algorithm. So, any algorithm that did not rely on the
    centroid while segmenting the data could still use any metric evaluation that
    relies on the centroid.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 作为备注，我们可以假设一个簇的质心是数据均值，即使我们没有使用 K-Means 算法。因此，任何不依赖于质心的算法在进行数据分割时，仍然可以使用依赖于质心的任何指标进行评估。
- en: Silhouette Coefficient
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 轮廓系数
- en: Silhouette is a technique in clustering to measure the similarity of data within
    the cluster compared to the other cluster. The Silhouette coefficient is a numerical
    representation ranging from -1 to 1\. Value 1 means each cluster completely differed
    from the others, and value  -1 means all the data was assigned to the wrong cluster.
    0 means there are no meaningful clusters from the data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓系数是一种聚类技术，用于衡量簇内数据与其他簇数据的相似性。轮廓系数是一个从 -1 到 1 的数值表示。值为 1 表示每个簇与其他簇完全不同，而值为
    -1 表示所有数据都被分配到了错误的簇中。值为 0 表示数据中没有有意义的簇。
- en: We could use the following code to calculate the Silhouette coefficient.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码来计算轮廓系数。
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Silhouette Coefficient: 0.562'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓系数：0.562
- en: We can see that our segmentation above has a positive Silhouette Coefficient,
    which means there is the degree of separation between the clusters, although some
    overlapping still happens.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，以上的分割具有正的轮廓系数，这意味着各个簇之间有一定的分离度，尽管仍然会发生一些重叠。
- en: Calinski-Harabasz Index
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Calinski-Harabasz 指数
- en: The Calinski-Harabasz Index or Variance Ratio Criterion is an index that is
    used to evaluate cluster quality by measuring the ratio of between-cluster dispersion
    to within-cluster dispersion. Basically, we measured the differences between the
    sum squared distance of the data between the cluster and data within the internal
    cluster.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Calinski-Harabasz 指数或方差比准则是一个用于评估簇质量的指标，通过测量簇间离散度与簇内离散度的比率来评估。基本上，我们测量了簇之间的数据平方距离总和与簇内部的数据距离的差异。
- en: The higher the Calinski-Harabasz Index score, the better, which means the clusters
    were well separated. However, there are no upper limits for the score means that
    this metric is better for evaluating different k numbers rather than interpreting
    the result as it is.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Calinski-Harabasz 指数的分数越高越好，这意味着簇之间的分离度越好。然而，分数没有上限，这意味着这个指标更适合用于评估不同的 k 值，而不是直接解释结果。
- en: Let’s use the Python code to calculate the Calinski-Harabasz Index score.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Python 代码来计算 Calinski-Harabasz 指数分数。
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Calinski-Harabasz Index: 708.087'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Calinski-Harabasz 指数：708.087
- en: One other consideration for the Calinski-Harabasz Index score is that the score
    is sensitive to the number of clusters. A higher number of clusters could lead
    to a higher score as well. So it’s a good idea to use other metrics alongside
    the Calinski-Harabasz Index to validate the result.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对 Calinski-Harabasz 指数的另一个考虑是该指数对簇的数量非常敏感。更多的簇可能会导致更高的分数。因此，最好将其他指标与 Calinski-Harabasz
    指数一起使用，以验证结果。
- en: Davies-Bouldin Index
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Davies-Bouldin 指数
- en: The Davies-Bouldin Index is a clustering evaluation metric measured by calculating
    the average similarity between each cluster and its most similar one. The ratio
    of within-cluster distances to between-cluster distances calculates the similarity.
    This means the further apart the clusters and the less dispersed would lead to
    better scores.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Davies-Bouldin 指数是一种聚类评估指标，通过计算每个簇与其最相似簇之间的平均相似性来测量。这个比率计算簇内距离与簇间距离的比值。这意味着簇之间越远且分散越少，会导致更好的分数。
- en: In contrast with our previous metrics, the Davies-Bouldin Index aims to have
    a lower score as much as possible. The lower the score was, the more separated
    each cluster was. Let’s use a Python example to calculate the score.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的指标相比，戴维斯-鲍尔丁指数旨在尽可能降低得分。得分越低，每个聚类的分离度越高。让我们使用Python示例来计算这个得分。
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Davies-Bouldin Index: 0.544'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 戴维斯-鲍尔丁指数：0.544
- en: We can’t say that the above score is good or bad because similar to the previous
    metrics, we still need to evaluate the result by using various metrics as support.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能仅凭上述得分来判断其好坏，因为类似于之前的指标，我们仍然需要使用各种指标来支持结果评估。
- en: Dimensionality Reduction Metrics
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维指标
- en: Unlike clustering, dimensionality reduction aims to reduce the number of features
    while preserving the original information as much as possible. Because of that,
    many of the evaluation metrics in dimensionality reduction were all about information
    preservation. Let’s reduce dimensionality with PCA and see how the metric works.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与聚类不同，降维的目标是减少特征数量，同时尽可能保留原始信息。因此，许多降维评估指标都关注信息保留。让我们使用PCA来减少维度，并看看这些指标的效果。
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the above example, we fit the PCA to the data, but we haven’t reduced the
    number of the feature yet. Instead, we want to evaluate the dimensionality reduction
    and variance trade-off with the **Cumulative Explained Variance**. It’s the common
    metric for dimensionality reduction to see how information remains with each feature
    reduction.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，我们对数据进行了主成分分析（PCA），但尚未减少特征的数量。相反，我们希望使用**累计解释方差**来评估降维和方差之间的权衡。这是一个常见的降维指标，用于查看在每次减少特征时信息的保留情况。
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Exploring Unsupervised Learning Metrics](../Images/781c57c460988a9e7466edfbbaa1deda.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![探索无监督学习指标](../Images/781c57c460988a9e7466edfbbaa1deda.png)'
- en: We can see from the above chart the amount of PC retained compared to the explained
    variance. As a rule of thumb, we often choose around 90-95% retained when we try
    to make dimensionality reduction, so around 14 features are reduced to 8 if we
    follow the chart above.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述图表中，我们可以看到保留的主成分数量与解释方差的关系。作为经验法则，我们通常选择保留约90-95%的主成分进行降维，因此如果遵循上述图表，大约14个特征会减少到8个。
- en: Let’s look at the other metrics to validate our dimensionality reduction.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看其他指标以验证我们的降维效果。
- en: Trustworthiness
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可信度
- en: Trustworthiness is a measurement of the dimensionality reduction technique quality.
    This metric measured how well the reduced dimension preserved the original data
    nearest neighbor.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 可信度是对降维技术质量的衡量指标。这个指标衡量的是降维后数据的最近邻保持了多少原始数据的特征。
- en: Basically, the metric tries to see how well the dimension reduction technique
    preserved the data in maintaining the original data's local structure.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这个指标试图查看降维技术在保持原始数据的局部结构方面做得如何。
- en: The Trustworthiness metric ranges between 0 to 1, where values closer to 1 are
    means the neighbor that is close to reduced dimension data points are mostly close
    as well in the original dimension.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 可信度指标范围从0到1，其中值越接近1，表示在降维数据点附近的邻居在原始维度中也大多接近。
- en: Let’s use the Python code to calculate the Trustworthiness metric.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Python代码计算可信度指标。
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Trustworthiness: 0.87'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 可信度：0.87
- en: Sammon’s Mapping
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 萨蒙映射
- en: Sammon’s mapping is a non-linear dimensionality reduction technique to preserve
    the high-dimensionality pairwise distance when being reduced. The objective is
    to use Sammon’s Stress function to calculate the pairwise distance between the
    original data and the reduction space.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 萨蒙映射是一种非线性降维技术，旨在在降维过程中保留高维度的成对距离。其目标是使用萨蒙应力函数来计算原始数据和降维空间之间的成对距离。
- en: The lower Sammon’s stress function score, the better because it indicates better
    pairwise preservation. Let’s try to use the Python code example.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 萨蒙应力函数的得分越低越好，因为它表示成对保留情况越好。让我们尝试使用Python代码示例。
- en: First, we would install an additional package for Sammon’s Mapping.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要安装一个额外的萨蒙映射包。
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Then we would use the following code to calculate the Sammon’s stress.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将使用以下代码计算萨蒙的应力。
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Sammon''s Stress: 1e-05'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 萨蒙应力：1e-05
- en: The result shown a low Sammon’s Score which means the data preservation was
    there.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示了一个较低的萨蒙得分，这意味着数据的保留情况良好。
- en: Conclusion
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'Unsupervised learning is a machine learning branch that tries to learn the
    pattern from the data. Compared to supervised learning, the output evaluation
    might not discuss much. In this article, we try to learn a few unsupervised learning
    metrics, including:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习是机器学习的一个分支，它尝试从数据中学习模式。与监督学习相比，输出评估可能不会有太多讨论。本文中，我们尝试学习一些无监督学习指标，包括：
- en: Within-Cluster Sum Square
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 簇内平方和
- en: Silhouette Coefficient
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 轮廓系数
- en: Calinski-Harabasz Index
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Calinski-Harabasz 指数
- en: Davies-Bouldin Index
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Davies-Bouldin 指数
- en: Cumulative Explained Variance
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 累积解释方差
- en: Trustworthiness
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可信度
- en: Sammon’s Mapping
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sammon 映射
- en: '**[Cornellius Yudha Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**
    is a data science assistant manager and data writer. While working full-time at
    Allianz Indonesia, he loves to share Python and Data tips via social media and
    writing media.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Cornellius Yudha Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**
    是一位数据科学助理经理和数据撰稿人。在全职工作于 Allianz Indonesia 的同时，他喜欢通过社交媒体和写作媒体分享 Python 和数据技巧。'
- en: More On This Topic
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题更多内容
- en: '[Unsupervised Disentangled Representation Learning in Class…](https://www.kdnuggets.com/2023/01/unsupervised-disentangled-representation-learning-class-imbalanced-dataset-elastic-infogan.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[无监督分离表示学习在类别不平衡数据集中的应用](https://www.kdnuggets.com/2023/01/unsupervised-disentangled-representation-learning-class-imbalanced-dataset-elastic-infogan.html)'
- en: '[Clustering with scikit-learn: A Tutorial on Unsupervised Learning](https://www.kdnuggets.com/2023/05/clustering-scikitlearn-tutorial-unsupervised-learning.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 scikit-learn 进行聚类：无监督学习教程](https://www.kdnuggets.com/2023/05/clustering-scikitlearn-tutorial-unsupervised-learning.html)'
- en: '[Unveiling Unsupervised Learning](https://www.kdnuggets.com/unveiling-unsupervised-learning)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[揭示无监督学习](https://www.kdnuggets.com/unveiling-unsupervised-learning)'
- en: '[Hands-On with Unsupervised Learning: K-Means Clustering](https://www.kdnuggets.com/handson-with-unsupervised-learning-kmeans-clustering)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[无监督学习实践：K均值聚类](https://www.kdnuggets.com/handson-with-unsupervised-learning-kmeans-clustering)'
- en: '[Machine Learning Evaluation Metrics: Theory and Overview](https://www.kdnuggets.com/machine-learning-evaluation-metrics-theory-and-overview)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习评估指标：理论与概述](https://www.kdnuggets.com/machine-learning-evaluation-metrics-theory-and-overview)'
- en: '[More Performance Evaluation Metrics for Classification Problems You…](https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分类问题的更多性能评估指标](https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html)'
