- en: N-gram Language Modeling in Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理中的N-gram语言建模
- en: 原文：[https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织进行IT管理'
- en: '* * *'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Language modeling is used to determine the probability of the word’s sequence.
    This modeling has a large number of applications i.e. recognition of speech, filtering
    of spam, etc. [1].
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 语言建模用于确定单词序列的概率。这种建模有大量应用，如语音识别、垃圾邮件过滤等[1]。
- en: Natural Language Processing (NLP)
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）
- en: Natural language processing (NLP) is the convergence of [artificial intelligence](https://algoscale.com/artificial-intelligence-solution-providers/)
    (AI) and linguistics. It is used to make the computers understand the words or
    statements that are written in human languages. NLP has been developed for making
    the work and communication with the computer easy and satisfying. As all the computer
    users cannot be well known by the specific languages of machines so NLP works
    better with the users who cannot have time for learning the new languages of machines.
    We can define language as a set of rules or symbols. Symbols are combined to convey
    the information. They are tyrannized by the set of rules. NLP is classified into
    two portions that are natural language understanding and natural language generation
    which evolves the tasks for understanding and generating the text. The classifications
    of NLP are shown in Figure 1 [2].
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是[人工智能](https://algoscale.com/artificial-intelligence-solution-providers/)（AI）与语言学的融合。它用于使计算机理解用人类语言书写的单词或陈述。NLP的开发旨在使与计算机的工作和沟通变得更简单、更令人满意。由于所有计算机用户无法完全掌握机器的特定语言，NLP在那些没有时间学习新机器语言的用户中表现更好。我们可以将语言定义为一组规则或符号。符号组合用于传达信息，它们受规则集合的支配。NLP被分为两个部分：自然语言理解和自然语言生成，这涉及理解和生成文本的任务。NLP的分类如图1所示[2]。
- en: '![Classifications of NLP](../Images/e4d57c96867cfe433c6267ea4cec9cfa.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![NLP的分类](../Images/e4d57c96867cfe433c6267ea4cec9cfa.png)'
- en: Figure 1 Classifications of NLP
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1 NLP的分类
- en: Methods of the Language Modeling
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言建模的方法
- en: 'Language modelings are classified as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 语言建模的分类如下：
- en: '**Statistical language modelings**: In this modeling, there is the development
    of probabilistic models. This probabilistic model predicts the next word in a
    sequence. For example N-gram language modeling. This modeling can be used for
    disambiguating the input. They can be used for selecting a probable solution.
    This modeling depends on the theory of probability. Probability is to predict
    how likely something will occur.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**统计语言建模**：在这种建模中，开发了概率模型。这个概率模型预测序列中的下一个单词。例如N-gram语言建模。该建模可用于消歧输入。它们可以用于选择可能的解决方案。这种建模依赖于概率理论。概率是预测某事发生的可能性。'
- en: '**Neural language modelings:** Neural language modeling gives better results
    than the classical methods both for the standalone models and when the models
    are incorporated into the larger models on the challenging tasks i.e. speech recognitions
    and machine translations. One method of performing neural language modeling is
    by word embedding [1].'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经语言建模**：神经语言建模比经典方法在独立模型以及当模型融入更大模型中，如语音识别和机器翻译等挑战性任务中，均能取得更好的结果。执行神经语言建模的一种方法是通过词嵌入[1]。'
- en: N-gram Modelling in NLP
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP中的N-gram建模
- en: N-gram is a sequence of the N-words in the modeling of NLP. Consider an example
    of the statement for modeling. “I love reading history books and watching documentaries”.
    In one-gram or unigram, there is a one-word sequence. As for the above statement,
    in one gram it can be “I”, “love”, “history”, “books”, “and”, “watching”, “documentaries”.
    In two-gram or the bi-gram, there is the two-word sequence i.e. “I love”, “love
    reading”, or “history books”. In the three-gram or the tri-gram, there are the
    three words sequences i.e. “I love reading”, “history books,” or “and watching
    documentaries” [3]. The illustration of the N-gram modeling i.e. for N=1,2,3 is
    given below in Figure 2 [5].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: N-gram 是在自然语言处理建模中用于表示 N 个词序列的模型。以建模的语句为例：“我喜欢阅读历史书籍和观看纪录片”。在一元模型（unigram）中，序列由一个词构成。对于上述语句，在一元模型中可以是“I”、“love”、“history”、“books”、“and”、“watching”、“documentaries”。在二元模型（bi-gram）中，序列由两个词构成，即“I
    love”、“love reading”或“history books”。在三元模型（tri-gram）中，序列由三个词构成，即“I love reading”、“history
    books”或“and watching documentaries”[3]。N-gram 模型的示意图（N=1,2,3）见下图 Figure 2 [5]。
- en: '![Uni-gram, Bi-gram, and Tri-gram Model'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![一元、二元和三元模型'
- en: '](../Images/733ed2884d98b58703bdfe6f8c9112da.png)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](../Images/733ed2884d98b58703bdfe6f8c9112da.png)'
- en: Figure 2 Uni-gram, Bi-gram, and Tri-gram Model
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2 一元、二元和三元模型
- en: 'For N-1 words, the N-gram modeling predicts most occurred words that can follow
    the sequences. The model is the probabilistic language model which is trained
    on the collection of the text. This model is useful in applications i.e. speech
    recognition, and machine translations. A simple model has some limitations that
    can be improved by smoothing, interpolations, and back off. So, the N-gram language
    model is about finding probability distributions over the sequences of the word.
    Consider the sentences i.e. "There was heavy rain" and "There was heavy flood".
    By using experience, it can be said that the first statement is good. The N-gram
    language model tells that the "heavy rain" occurs more frequently than the "heavy
    flood". So, the first statement is more likely to occur and it will be then selected
    by this model. In the one-gram model, the model usually relies on that which word
    occurs often without pondering the previous words. In 2-gram, only the previous
    word is considered for predicting the current word. In 3-gram, two previous words
    are considered. In the N-gram language model the following probabilities are calculated:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 N-1 个词，N-gram 模型预测可以跟随序列的最常出现的词。该模型是一种概率语言模型，基于文本集合进行训练。该模型在应用中非常有用，例如语音识别和机器翻译。简单的模型有一些局限性，可以通过平滑、插值和回退进行改进。因此，N-gram
    语言模型的目标是找到词序列的概率分布。考虑以下句子：“There was heavy rain”和“There was heavy flood”。通过经验可以说，第一个句子更好。N-gram
    语言模型会指出“heavy rain”出现的频率高于“heavy flood”。因此，第一个句子更可能出现，并会被该模型选择。在一元模型中，模型通常依赖于词的频率，而不考虑之前的词。在二元模型中，仅考虑前一个词来预测当前词。在三元模型中，考虑两个前一个词。在
    N-gram 语言模型中计算以下概率：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As it is not practical to calculate the conditional probability but by using the
    “*Markov Assumptions”*, this is approximated to the bi-gram model as [4]:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算条件概率并不实际，但通过使用“*Markov Assumptions*”，可以近似为二元模型[4]：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Applications of the N-gram Model in NLP
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: N-gram 模型在自然语言处理中的应用
- en: In speech recognition, the input can be noisy. This noise can make a wrong speech
    to the text conversion. The N-gram language model corrects the noise by using
    probability knowledge. Likewise, this model is used in machine translations for
    producing more natural statements in target and specified languages. For spelling
    error corrections, the dictionary is useless sometimes. For instance, "in about
    fifteen minutes" 'minuets' is a valid word according to the dictionary but it
    is incorrect in the phrase. The N-gram language model can rectify this type of
    error.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在语音识别中，输入可能会受到噪声的干扰。这种噪声可能导致错误的语音到文本的转换。N-gram 语言模型通过使用概率知识来纠正噪声。同样，这种模型也用于机器翻译，以生成目标语言中更自然的语句。在拼写错误纠正方面，字典有时会无用。例如，“in
    about fifteen minutes”中的“minuets”在字典中是一个有效的词，但在短语中是不正确的。N-gram 语言模型可以纠正这种类型的错误。
- en: The N-gram language model is generally at the word levels. It is also used at
    the character levels for doing the stemming i.e. for separating the root words
    from a suffix. By looking at the N-gram model, the languages can be classified
    or differentiated between the US and UK spellings. Many applications get benefit
    from the N-gram model including tagging of part of the speech, natural language
    generations, word similarities, and sentiments extraction. [4].
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: N-gram语言模型通常应用于词级别。它也用于字符级别进行词干提取，即分离根词和后缀。通过查看N-gram模型，可以对语言进行分类或区分美国和英国的拼写。许多应用程序从N-gram模型中受益，包括词性标注、自然语言生成、词语相似性和情感提取[4]。
- en: Limitations of N-gram Model in NLP
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: N-gram模型在NLP中的局限性
- en: The N-gram language model has also some limitations. There is a problem with the
    out of vocabulary words. These words are during the testing but not in the training.
    One solution is to use the fixed vocabulary and then convert out vocabulary words
    in the training to pseudowords. When implemented in the sentiment analysis, the
    bi-gram model outperformed the uni-gram model but the number of the features is
    then doubled. So, the scaling of the N-gram model to the larger data sets or moving
    to the higher-order needs better feature selection approaches. The N-gram model
    captures the long-distance context poorly. It has been shown after every 6-grams,
    the gain of performance is limited.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: N-gram语言模型也存在一些局限性。存在词汇外问题。这些词在测试时出现但在训练时未出现。一种解决方案是使用固定词汇表，然后将训练中的词汇外词转换为伪词。当应用于情感分析时，二元模型优于单元模型，但特征数量翻倍。因此，将N-gram模型扩展到更大的数据集或提高阶数需要更好的特征选择方法。N-gram模型对长距离上下文的捕捉较差。研究显示，每6-gram之后，性能提升有限。
- en: References
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (N-Gram Language Modelling with NLTK, 30 May, 2021)
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (N-Gram语言建模与NLTK, 2021年5月30日)
- en: (Diksha Khurana, Aditya Koli, Kiran Khatter, Sukhdev Singh, August 2017)
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (Diksha Khurana, Aditya Koli, Kiran Khatter, Sukhdev Singh, 2017年8月)
- en: (Mohdsanadzakirizvi, August 8, 2019)
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (Mohdsanadzakirizvi, 2019年8月8日)
- en: (N-Gram Model, March 29)
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (N-Gram模型, 3月29日)
- en: (N-Gram)
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (N-Gram)
- en: '**[Neeraj Agarwal](https://www.linkedin.com/in/neeagl/)** is a founder of [Algoscale](https://www.linkedin.com/company/algoscale),
    a data consulting company covering data engineering, applied AI, data science,
    and product engineering. He has over 9 years of experience in the field and has
    helped a wide range of organizations from start-ups to Fortune 100 companies ingest
    and store enormous amounts of raw data in order to translate it into actionable
    insights for better decision-making and faster business value.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Neeraj Agarwal](https://www.linkedin.com/in/neeagl/)** 是 [Algoscale](https://www.linkedin.com/company/algoscale)
    的创始人，这是一家涵盖数据工程、应用人工智能、数据科学和产品工程的数据咨询公司。他在该领域有超过9年的经验，帮助了从初创公司到财富100强公司的广泛组织摄取和存储大量原始数据，从而将其转化为可操作的见解，以便做出更好的决策和更快的商业价值。'
- en: More On This Topic
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题
- en: '[Natural Language Processing Key Terms, Explained](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理关键术语解释](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
- en: '[Data Representation for Natural Language Processing Tasks](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理任务的数据表示](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
- en: '[Transfer Learning for Image Recognition and Natural Language Processing](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[图像识别和自然语言处理的迁移学习](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
- en: '[How to Start Using Natural Language Processing With PyTorch](https://www.kdnuggets.com/2022/04/start-natural-language-processing-pytorch.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何开始使用PyTorch进行自然语言处理](https://www.kdnuggets.com/2022/04/start-natural-language-processing-pytorch.html)'
- en: '[A Gentle Introduction to Natural Language Processing](https://www.kdnuggets.com/2022/06/gentle-introduction-natural-language-processing.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理的温和介绍](https://www.kdnuggets.com/2022/06/gentle-introduction-natural-language-processing.html)'
- en: '[Natural Language Processing with spaCy](https://www.kdnuggets.com/2023/01/natural-language-processing-spacy.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用spaCy进行自然语言处理](https://www.kdnuggets.com/2023/01/natural-language-processing-spacy.html)'
