- en: 'LSTMs Rise Again: Extended-LSTM Models Challenge the Transformer Superiority'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTMs 再次崛起：扩展LSTM模型挑战变换器的优势
- en: 原文：[https://www.kdnuggets.com/lstms-rise-again-extended-lstm-models-challenge-the-transformer-superiority](https://www.kdnuggets.com/lstms-rise-again-extended-lstm-models-challenge-the-transformer-superiority)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/lstms-rise-again-extended-lstm-models-challenge-the-transformer-superiority](https://www.kdnuggets.com/lstms-rise-again-extended-lstm-models-challenge-the-transformer-superiority)
- en: '![LSTMs Rise Again](../Images/159eea8cbfb38f1606c4fa351a3de0ad.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![LSTMs 再次崛起](../Images/159eea8cbfb38f1606c4fa351a3de0ad.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: LSTMs were initially introduced in the early 1990s by authors [Sepp Hochreiter](https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en)
    and [Jurgen Schmidhuber](https://scholar.google.com/citations?user=gLnCTgIAAAAJ&hl=en).
    The original model was extremely compute-expensive and it was in the mid-2010s
    when RNNs and LSTMs gained attention. With more data and better GPUs available,
    LSTM networks became the standard method for language modeling and they became
    the backbone for the first large language model. That was the case until the release
    of [Attention-Based Transformer Architecture](https://arxiv.org/abs/1706.03762)
    in 2017\. LSTMs were gradually outdone by the Transformer architecture which is
    now the standard for all recent Large Language Models including ChatGPT, Mistral,
    and Llama.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM最初由[Sepp Hochreiter](https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en)和[Jurgen
    Schmidhuber](https://scholar.google.com/citations?user=gLnCTgIAAAAJ&hl=en)于1990年代初期引入。最初的模型计算成本极高，直到2010年代中期，RNN和LSTM才受到关注。随着数据量的增加和更好的GPU的出现，LSTM网络成为语言建模的标准方法，并成为第一个大型语言模型的基础。这种情况一直持续到2017年[基于注意力的变换器架构](https://arxiv.org/abs/1706.03762)的发布。LSTM逐渐被变换器架构取代，后者现在是所有最近的大型语言模型，包括ChatGPT、Mistral和Llama的标准。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能。'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的IT工作。'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: However, the recent release of the **[xLSTM paper](https://arxiv.org/abs/2405.04517)**
    by the original LSTM author Sepp Hochreiter has caused a major stir in the research
    community. The results show comparative pre-training results to the latest LLMs
    and it has raised a question if LSTMs can once again take over Natural Language
    Processing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由原始LSTM作者**[Sepp Hochreiter](https://arxiv.org/abs/2405.04517)** 最近发布的xLSTM论文在研究界引起了巨大轰动。结果显示其预训练结果与最新的LLMs进行了比较，并且提出了一个问题：LSTM是否可以再次在自然语言处理领域取得突破。
- en: High-Level Architecture Overview
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级架构概述
- en: 'The original LSTM network had some major limitations that limited its usability
    for larger contexts and deeper models. Namely:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的LSTM网络有一些主要的局限性，这些局限性限制了它在更大上下文和更深层模型中的可用性。具体来说：
- en: LSTMs were sequential models that made it hard to parallelize training and inference.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM是顺序模型，这使得训练和推理难以并行化。
- en: They had limited storage capabilities and all information had to be compressed
    into a single cell state.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们的存储能力有限，所有信息必须压缩到一个单独的单元状态中。
- en: The recent xLSTM network introduces new sLSTM and mLSTM blocks to address both
    these shortcomings. Let us take a birds-eye view of the model architecture and
    see the approach used by the authors.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最近发布的xLSTM网络引入了新的sLSTM和mLSTM模块，以解决这些不足之处。让我们从整体上了解一下模型架构，并看看作者采用了什么方法。
- en: Short Review of Original LSTM
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原始LSTM的简短评述
- en: 'The LSTM network used a hidden state and cell state to counter the vanishing
    gradient problem in the vanilla RNN networks. They also added the forget, input
    and output sigmoid gates to control the flow of information. The equations are
    as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM网络使用了隐藏状态和单元状态来解决普通RNN网络中的梯度消失问题。他们还添加了忘记、输入和输出的sigmoid门来控制信息流动。方程如下：
- en: '![LSTM Equation](../Images/9401b41e35bec3728ce22f18db903377.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![LSTM方程](../Images/9401b41e35bec3728ce22f18db903377.png)'
- en: Image from [Paper](https://arxiv.org/abs/2405.04517)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [论文](https://arxiv.org/abs/2405.04517)
- en: The cell state (ct) passed through the LSTM cell with minor linear transformations
    that helped preserve the gradient across large input sequences.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 细胞状态（ct）经过LSTM单元时经过了微小的线性变换，帮助在较长的输入序列中保持梯度。
- en: The xLSTM model modifies these equations in the new blocks to remedy the known
    limitations of the model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: xLSTM模型在新模块中修改了这些方程，以补救模型已知的局限性。
- en: sLSTM Block
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: sLSTM模块
- en: 'The block modifies the sigmoid gates and uses the exponential function for
    the input and forget gate. As quoted by the authors, this can improve the storage
    issues in LSTM and still allow multiple memory cells allowing memory mixing within
    each head but not across head. The modified sLSTM block equation is as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 该模块修改了sigmoid门，并对输入门和遗忘门使用了指数函数。正如作者所引述的，这可以改善LSTM中的存储问题，并且仍然允许多个记忆单元在每个头内混合记忆，但不跨头。修改后的sLSTM模块方程如下：
- en: '![sLSTM Equation](../Images/d2abcf38ff255621e3659e5d35a26b36.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![sLSTM方程](../Images/d2abcf38ff255621e3659e5d35a26b36.png)'
- en: Image from [Paper](https://arxiv.org/abs/2405.04517)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [论文](https://arxiv.org/abs/2405.04517)
- en: Moreover, as the exponential function can cause large values, the gate values
    are normalized and stabilized using log functions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于指数函数可能导致大值，门值使用对数函数进行归一化和稳定。
- en: mLSTM Block
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: mLSTM模块
- en: 'To counter the parallelizability and storage issues in the LSTM network, the
    xLSTM modifies the cell state from a 1-dimensional vector to a 2-dimensional square
    matrix. They store a decomposed version as key and value vectors and use the same
    exponential gating as the sLSTM block. The equations are as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决LSTM网络中的并行性和存储问题，xLSTM将细胞状态从1维向量修改为2维方阵。他们存储了作为键和值向量的分解版本，并使用与sLSTM模块相同的指数门控。方程如下：
- en: '![mLSTM Equation](../Images/8fff002f2873a8f54c668208a7d318fa.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![mLSTM方程](../Images/8fff002f2873a8f54c668208a7d318fa.png)'
- en: Image from [Paper](https://arxiv.org/abs/2405.04517)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [论文](https://arxiv.org/abs/2405.04517)
- en: Architecture Diagram
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 架构图
- en: '![xLSTM Architecture Diagram](../Images/eb03b32b1d213056a0f3ece07c260e13.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![xLSTM架构图](../Images/eb03b32b1d213056a0f3ece07c260e13.png)'
- en: Image from [Paper](https://arxiv.org/abs/2405.04517)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [论文](https://arxiv.org/abs/2405.04517)
- en: The overall xLSTM architecture is a sequential combination of mLSTM and sLSTM
    blocks in different proportions. As the diagram shows, the xLSTM block can have
    any memory cell. The different blocks are stacked together with layer normalizations
    to form a deep network of residual blocks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 整体xLSTM架构是mLSTM和sLSTM模块按不同比例顺序组合而成。如图所示，xLSTM模块可以具有任意记忆单元。不同的模块通过层归一化堆叠在一起，形成深度残差网络。
- en: Evaluation Results and Comparison
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估结果与比较
- en: The authors train the xLSTM network on language model tasks and compare the
    perplexity *(lower is better)* of the trained model with the current Transformer-based
    LLMs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在语言模型任务上训练xLSTM网络，并将训练模型的困惑度 *(越低越好)* 与当前的基于Transformer的LLMs进行比较。
- en: The authors first train the models on 15B tokens from SlimPajama. The results
    showed that xLSTM outperform all other models in the validation set with the lowest
    perplexity score.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 作者首先在SlimPajama的15B令牌上训练模型。结果表明，xLSTM在验证集上的困惑度得分最低，优于所有其他模型。
- en: '![xLSTM Evaluation and Comparison](../Images/601c3fa61e44dba4d5701227f48d4eb5.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![xLSTM评估与比较](../Images/601c3fa61e44dba4d5701227f48d4eb5.png)'
- en: Image from [Paper](https://arxiv.org/abs/2405.04517)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [论文](https://arxiv.org/abs/2405.04517)
- en: Sequence Length Extrapolation
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 序列长度外推
- en: 'The authors also analyze performance when the test time sequence length exceeds
    the context length the model was trained on. They trained all models on a sequence
    length of 2048 and the below graph shows the validation perplexity with changes
    in token position:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作者还分析了测试时序列长度超过模型训练时上下文长度的表现。他们在2048的序列长度上训练了所有模型，下面的图展示了随着令牌位置变化的验证困惑度：
- en: '![xLSTM equence Length Extrapolation](../Images/19523ecbdaaaac64f805fbf8f96065ac.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![xLSTM序列长度外推](../Images/19523ecbdaaaac64f805fbf8f96065ac.png)'
- en: Image from [Paper](https://arxiv.org/abs/2405.04517)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [论文](https://arxiv.org/abs/2405.04517)
- en: The graph shows that even for much longer sequences, xLSTM networks maintain
    a stable perplexity score and perform better than any other model for much longer
    context lengths.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示，即使对于更长的序列，xLSTM网络仍能保持稳定的困惑度得分，并在更长的上下文长度中表现优于其他任何模型。
- en: Scaling xLSMT to Larger Model Sizes
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展xLSTM到更大模型尺寸
- en: The authors further train the model on 300B tokens from the SlimPajama dataset.
    The results show that even for larger model sizes, xLSTM scales better than the
    current Transformer and Mamba architecture.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 作者进一步在300B个来自SlimPajama数据集的标记上训练模型。结果表明，即使对于更大的模型，xLSTM的扩展性也优于当前的Transformer和Mamba架构。
- en: '![Scaling xLSMT](../Images/48ffcf08af07141efd683e72144e3295.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![Scaling xLSMT](../Images/48ffcf08af07141efd683e72144e3295.png)'
- en: Image from [Paper](https://arxiv.org/abs/2405.04517)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于[论文](https://arxiv.org/abs/2405.04517)
- en: Wrapping Up
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: That might have been difficult to understand and that is okay! Nonetheless,
    you should now understand why this research paper has got all the attention recently.
    It has been shown to perform at least as well as the recent large language models
    if not better. It is proven to be scalable for larger models and can be a serious
    competitor for all recent LLMs built on Transformers. Only time will tell if LSTMs
    will regain their glory once again, but for now, we know that the xLSTM architecture
    is here to challenge the superiority of the renowned Transformers architecture.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能很难理解，不过没关系！尽管如此，你现在应该明白为什么这篇研究论文最近受到了如此关注。它的表现至少与近期的大型语言模型一样好，甚至更优。已证明它可以扩展到更大的模型，并且可以成为所有近期基于Transformers构建的LLMs的严肃竞争者。只有时间会证明LSTMs是否会重新获得辉煌，但现在我们知道xLSTM架构在挑战著名的Transformers架构的优势。
- en: '**[](https://www.linkedin.com/in/kanwal-mehreen1/)**[Kanwal Mehreen](https://www.linkedin.com/in/kanwal-mehreen1/)****
    Kanwal is a machine learning engineer and a technical writer with a profound passion
    for data science and the intersection of AI with medicine. She co-authored the
    ebook "Maximizing Productivity with ChatGPT". As a Google Generation Scholar 2022
    for APAC, she champions diversity and academic excellence. She''s also recognized
    as a Teradata Diversity in Tech Scholar, Mitacs Globalink Research Scholar, and
    Harvard WeCode Scholar. Kanwal is an ardent advocate for change, having founded
    FEMCodes to empower women in STEM fields.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**[](https://www.linkedin.com/in/kanwal-mehreen1/)**[Kanwal Mehreen](https://www.linkedin.com/in/kanwal-mehreen1/)**
    Kanwal是一位机器学习工程师和技术作家，对数据科学以及人工智能与医学的交叉领域充满深厚的热情。她共同撰写了电子书《利用ChatGPT最大化生产力》。作为2022年亚太地区Google一代学者，她倡导多样性和学术卓越。她还被誉为Teradata技术多样性学者、Mitacs全球研究学者和哈佛WeCode学者。Kanwal是变革的热心倡导者，创办了FEMCodes以赋能STEM领域的女性。'
- en: More On This Topic
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[If I Had To Start Learning Data Science Again, How Would I Do It?](https://www.kdnuggets.com/2020/08/start-learning-data-science-again.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如果我必须重新开始学习数据科学，我会怎么做？](https://www.kdnuggets.com/2020/08/start-learning-data-science-again.html)'
- en: '[R vs Python (Again): A Human Factor Perspective](https://www.kdnuggets.com/2022/01/r-python-human-factor-perspective.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[R与Python（再次）：一个人因视角](https://www.kdnuggets.com/2022/01/r-python-human-factor-perspective.html)'
- en: '[There and Back Again… a RAPIDS Tale](https://www.kdnuggets.com/2023/06/back-again-rapids-tale.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从那里和回来……一个RAPIDS故事](https://www.kdnuggets.com/2023/06/back-again-rapids-tale.html)'
- en: '[Why Emily Ekdahl chose co:rise to level up her job performance as a…](https://www.kdnuggets.com/2022/08/corise-emily-ekdahl-chose-corise-level-job-performance-machine-learning-engineer.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么Emily Ekdahl选择co:rise来提升她作为…的工作表现](https://www.kdnuggets.com/2022/08/corise-emily-ekdahl-chose-corise-level-job-performance-machine-learning-engineer.html)'
- en: '[Drag, Drop, Analyze: The Rise of No-Code Data Science](https://www.kdnuggets.com/drag-drop-analyze-the-rise-of-nocode-data-science)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[拖拽、分析：无代码数据科学的崛起](https://www.kdnuggets.com/drag-drop-analyze-the-rise-of-nocode-data-science)'
- en: '[The Rise and Fall of Prompt Engineering: Fad or Future?](https://www.kdnuggets.com/the-rise-and-fall-of-prompt-engineering-fad-or-future)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提示工程的兴起与衰落：时尚还是未来？](https://www.kdnuggets.com/the-rise-and-fall-of-prompt-engineering-fad-or-future)'
