- en: More Performance Evaluation Metrics for Classification Problems You Should Know
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html](https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Evaluating a model is a major part of building an effective machine learning
    model. The most frequent classification evaluation metric that we use should be
    ‘**Accuracy**’. You might believe that the model is good when the accuracy rate
    is 99%! However, it is not always true and can be misleading in some situations.
    I’m going to explain the 4 aspects as shown below in this article:'
  prefs: []
  type: TYPE_NORMAL
- en: The Confusion Matrix for a 2-class classification problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key classification metrics: *Accuracy, Recall, Precision, and F1- Score*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference between *Recall and Precision in specific cases*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision Thresholds and Receiver Operating Characteristic (ROC) curve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Flow of Machine Learning Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In any binary classification task, we model can only achieve two results, either
    our model is **correct** or **incorrect **in the prediction where we only have
    two classes. Imagine we now have a classification task to predict if an image
    is a dog or cat. In supervised learning, we first **fit/train **a model on training
    data, then **test** the model on **testing data**. Once we have the model’s predictions
    from the **X_test** data, we compare it to the** true y_values** (the correct
    labels).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4563f2583050a1b1671bbc374b1be28.png)'
  prefs: []
  type: TYPE_IMG
- en: We feed the image of dog into our trained model before the model prediction.
    The model predicts that this is a dog, and then we compare the prediction to the
    correct label. If we compare the prediction to the label of “dog,” it is correct.
    However, if it predicts that this image is a cat, this comparison to the correct
    label would be incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: We repeat this process for all the images in our X test data. Eventually, we
    will have a count of correctly matched and a count of incorrect matches. The key
    realisation is that not all incorrect or correct matches hold **equal value **in
    reality. Therefore a single metric won’t tell the whole story.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, accuracy is one of the common evaluation metrics in classification
    problems, that is the total number of correct predictions divided by the total
    number of predictions made for a dataset. Accuracy is useful when the target class
    is *well balanced* but is not a good choice with unbalanced classes. Imagine we
    had 99 images of the dog and only 1 image of a cat in our training data, our model
    would be simply a line that always predicted dog, and therefore we got 99% accuracy.
    Data is always imbalanced in reality, such as Spam email, credit card fraud, and
    medical diagnosis. Hence, if we want to have a full picture of the model evaluation,
    other metrics such as recall and precision should also be considered.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion Matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluation of the performance of a classification model is based on the counts
    of test records correctly and incorrectly predicted by the model. The confusion
    matrix provides a more insightful picture which is not only the performance of
    a predictive model, but also which classes are being predicted correctly and incorrectly,
    and what type of errors are being made. To illustrate, we can see how the 4 classification
    metrics are calculated (TP, FP, FN, TN), and our predicted value compared to the
    actual value in a confusion matrix is clearly presented in the below confusion
    matrix table.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d5f3e3de47423d7565d725b846eb01d.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Possible Classification Outcomes: TP, FP, FN, TN.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*The confusion matrix is useful for measuring Recall (also known as Sensitivity),
    Precision, Specificity, Accuracy, and, most importantly, the AUC-ROC Curve.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Do you feel confused when you were reading the table? That’s expected. I was
    also before. Let me put it in an interesting scenario in terms of pregnancy analogy
    to explain the terms of TP, FP, FN, TN. We can then understand Recall, Precision,
    Specificity, Accuracy, and, most importantly, the AUC-ROC Curve.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88b6a0ff716a6bf5b4af5e4fb6cc4a24.png)'
  prefs: []
  type: TYPE_IMG
- en: '*[image source](https://dzone.com/articles/understanding-the-confusion-matrix)*'
  prefs: []
  type: TYPE_NORMAL
- en: The Equations of 4 Key Classification Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/8fa9dab027b54588244d29ee3b7acc2b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Recall versus Precision**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/590c61e5345073ed8a891b794f61e1c7.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Precision** is the ratio of *True Positives* to all the positives predicted
    by the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Low precision: the more False positives the model predicts, the lower the precision.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall (Sensitivity)**is the ratio of *True Positives* to all the positives
    in your Dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Low recall: the more False Negatives the model predicts, the lower the recall.'
  prefs: []
  type: TYPE_NORMAL
- en: '*The idea of recall and precision seems to be abstract. Let me illustrate the
    difference in three real cases.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/caffd27813ca620b178fbec91fbeb25d.png)'
  prefs: []
  type: TYPE_IMG
- en: the result of TP will be that the COVID 19 residents diagnosed with COVID-19.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the result of TN will be that healthy residents are with good health.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the result of FP will be that those actually healthy residents are predicted
    as COVID 19 residents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the result of FN will be that those actual COVID 19 residents are predicted
    as the healthy residents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In case 1, which scenario do you think will have the highest cost?
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that if we predict COVID-19 residents as healthy patients and they do
    not need to quarantine, there would be a massive number of COVID-19 infections.
    The cost of f*alse negatives *is much higher than the cost of f*alse positives.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/27c47014ea641c459da7e6a1bdb23040.png)'
  prefs: []
  type: TYPE_IMG
- en: the result of TP will be that spam emails are placed in the spam folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the result of TN will be that important emails are received.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the result of FP will be that important emails are placed in the spam folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the result of FN will be that spam emails are received.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In case 2, which scenario do you think will have the highest cost?
  prefs: []
  type: TYPE_NORMAL
- en: Well, since missing important emails will clearly be more of a problem than
    receiving spam, we can say that in this case, FP will have a higher cost than
    FN.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05234286f46dea9a2fff74e5cfc4edd6.png)'
  prefs: []
  type: TYPE_IMG
- en: the result of TP will be that bad loans are correctly predicted as bad loans.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the result of TN will be that good loans are correctly predicted as good loans.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the result of FP will be that (actual) good loans are incorrectly predicted
    as bad loans.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the result of FN will be that (actual) bad loans are incorrectly predicted as
    good loans.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In case 3, which scenario do you think will have the highest cost?
  prefs: []
  type: TYPE_NORMAL
- en: The banks would lose a bunch amount of money if the actual bad loans are predicted
    as good loans due to loans not being repaid. On the other hand, banks won't be
    able to make more revenue if the actual good loans are predicted as bad loans.
    Therefore, the cost of *False Negatives *is much higher than the cost of *False
    Positives. *Imagine that.
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b0c7d23958f29d84cfc490697ab5f3e2.png)'
  prefs: []
  type: TYPE_IMG
- en: In practice, the cost of false negatives is not the same as the cost of false
    positives, depending on the different specific cases. It is evident that not only
    should we calculate accuracy, but we should also evaluate our model using other
    metrics, for example, *Recall *and *Precision*.
  prefs: []
  type: TYPE_NORMAL
- en: Combining Precision and Recall
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the above three cases, we want to maximize either recall or precision at
    the expense of the other metric. For example, in the case of a good or bad loan
    classification, we would like to decrease FN to increase recall. However, in cases
    where we want to find an optimal blend of precision and recall, we can combine
    the two metrics using the [F1 score](https://en.wikipedia.org/wiki/F1_score).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6cc7f8f2835ab8e0ce9b2674d62faf94.png)'
  prefs: []
  type: TYPE_IMG
- en: F-Measure provides a single score that balances both the concerns of precision
    and recall in one number. A good F1 score means that you have low false positives
    and low false negatives, so you’re correctly identifying real threats, and you
    are not disturbed by false alarms. An F1 score is considered perfect when it’s
    1, while the model is a total failure when it’s 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af4424d44698efd4190dad446b7b782f.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision Threshold
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ROC is a major visualization technique for presenting the performance of a classification
    model. It summarizes the trade-off between the true positive rate (tpr) and false
    positive rate (fpr) for a predictive model using different probability thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ba9645520e1252a292cc220de108b25.png)'
  prefs: []
  type: TYPE_IMG
- en: '*The equation of tpr and fpr.*'
  prefs: []
  type: TYPE_NORMAL
- en: The true positive rate (tpr) is the recall and the false positive rate (FPR)
    is the probability of a false alarm.
  prefs: []
  type: TYPE_NORMAL
- en: A ROC curve plots the true positive rate (tpr) versus the false positive rate
    (fpr) as a function of the model’s threshold for classifying a positive. Given
    that **c **is a constant known as decision threshold, the below ROC curve suggests
    that by default c=0.5, when c=0.2, both tpr and fpr increase. When c=0.8, both
    tpr and fpr decrease. In general, tpr and fpr increase as c decrease. In the extreme
    case when c=1, all cases are predicted as negative; tpr=fpr=0\. On the other hand,
    when c=0, all cases are predicted as positive; tpr=fpr=1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/19f60956fb7baa2b0c3430dafd9bbf63.png)'
  prefs: []
  type: TYPE_IMG
- en: '*ROC chart.*'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can assess the performance of the model by the area under the ROC
    curve (**AUC**). As a rule of thumb, 0.9–1=excellent; 0.8-.09=good; 0.7–0.8=fair;
    0.6–0.7=poor; 0.50–0.6=fail.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Confusion Matrix for a 2-class classification problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key classification metrics: *Accuracy, Recall, Precision, and F1- Score*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference between *Recall and Precision in specific cases*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision Thresholds and Receiver Operating Characteristic (ROC) curve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[Clare Liu](https://www.linkedin.com/in/clareliuchungyan/)** is a Data Scientist
    at Fintech (bank) industry, based in HK. Passionate in resolving mystery about
    data science and machine learning. Join me on the self-learning journey.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Machine Learning Evaluation Metrics: Theory and Overview](https://www.kdnuggets.com/machine-learning-evaluation-metrics-theory-and-overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Classification Metrics Walkthrough: Logistic Regression with…](https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding Classification Metrics: Your Guide to Assessing Model…](https://www.kdnuggets.com/understanding-classification-metrics-your-guide-to-assessing-model-accuracy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Want to Use Your Data Skills to Solve Global Problems? Here’s What…](https://www.kdnuggets.com/2022/04/jhu-want-data-skills-solve-global-problems.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, April 13: Python Libraries Data Scientists Should…](https://www.kdnuggets.com/2022/n15.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Science Projects That Can Help You Solve Real World Problems](https://www.kdnuggets.com/2022/11/data-science-projects-help-solve-real-world-problems.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
