- en: Training a Neural Network to Write Like Lovecraft
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个神经网络以模仿洛夫克拉夫特的写作风格
- en: 原文：[https://www.kdnuggets.com/2019/07/training-neural-network-write-like-lovecraft.html](https://www.kdnuggets.com/2019/07/training-neural-network-write-like-lovecraft.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/07/training-neural-network-write-like-lovecraft.html](https://www.kdnuggets.com/2019/07/training-neural-network-write-like-lovecraft.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '![Header image](../Images/8a5bd5048297de93e1bdb25f52dbc3d9.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![标题图](../Images/8a5bd5048297de93e1bdb25f52dbc3d9.png)'
- en: LSTM Neural Networks have seen a lot of use in the recent years, both for text
    and music generation, and for Time Series Forecasting.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM神经网络近年来被广泛应用于文本和音乐生成以及时间序列预测。
- en: Today, I’ll teach you how to train a LSTM Neural Network for text generation,
    so that it can write with H. P. Lovecraft’s style.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我将教你如何训练一个LSTM神经网络进行文本生成，以便它可以以H. P. 洛夫克拉夫特的风格写作。
- en: In order to train this LSTM, we’ll be using TensorFlow’s Keras API for Python.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练这个LSTM，我们将使用TensorFlow的Keras API for Python。
- en: I learned about this subject from this awesome [LSTM Neural Networks tutorial](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).
    My code follows this [Text Generation tutorial](https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/)‘s
    closely.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我从这个很棒的 [LSTM神经网络教程](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
    学习到了这个主题。我的代码紧随这个 [文本生成教程](https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/)。
- en: I’ll show you my Python examples and results as usual, but first, let’s do some
    explaining.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我将像往常一样展示我的Python示例和结果，但首先，让我们先做一些解释。
- en: What are LSTM Neural Networks?
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是LSTM神经网络？
- en: The most vanilla, run-of-the-mill Neural Network, called a Multi-Layer-Perceptron,
    is just a composition of fully connected layers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最普通的、最基本的神经网络，称为多层感知器，就是完全连接层的组合。
- en: In these models, the input is a vector of features, and each subsequent layer
    is a set of “neurons”.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些模型中，输入是特征的向量，每一层是一个“神经元”集合。
- en: Each neuron performs an affine (linear) transformation to the previous layer’s
    output, and then applies some non-linear function to that result.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元对前一层的输出进行仿射（线性）变换，然后对结果应用某种非线性函数。
- en: The output of a layer’s neurons, a new vector, is fed to the next layer, and
    so on.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一层的神经元的输出，即一个新的向量，被传递到下一层，依此类推。
- en: '![Figure](../Images/5f5762b2af5557ca5580f784a15daba8.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/5f5762b2af5557ca5580f784a15daba8.png)'
- en: '[Source](https://www.researchgate.net/figure/A-hypothetical-example-of-Multilayer-Perceptron-Network_fig4_303875065)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://www.researchgate.net/figure/A-hypothetical-example-of-Multilayer-Perceptron-Network_fig4_303875065)'
- en: A LSTM (Long Short-term Memory) Neural Network is just another kind of [Artificial
    Neural Network](http://www.datastuff.tech/machine-learning/autoencoder-deep-learning-tensorflow-eager-api-keras/),
    which falls in the category of Recurrent Neural Networks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM（长短期记忆）神经网络只是另一种 [人工神经网络](http://www.datastuff.tech/machine-learning/autoencoder-deep-learning-tensorflow-eager-api-keras/)，属于递归神经网络的范畴。
- en: What makes LSTM Neural Networks different from regular Neural Networks is, they
    have LSTM cells as neurons in some of their layers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM神经网络与普通神经网络不同之处在于，它们在某些层中使用LSTM单元作为神经元。
- en: Much like [Convolutional Layers](http://www.datastuff.tech/machine-learning/convolutional-neural-networks-an-introduction-tensorflow-eager/) help
    a Neural Network learn about image features, LSTM cells help the Network learn
    about temporal data, something which other Machine Learning models traditionally
    struggled with.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 [卷积层](http://www.datastuff.tech/machine-learning/convolutional-neural-networks-an-introduction-tensorflow-eager/)
    帮助神经网络学习图像特征一样，LSTM单元帮助网络学习时间序列数据，这是其他机器学习模型传统上难以处理的。
- en: How do LSTM cells work? I’ll explain it now, though I highly recommend you give
    those tutorials a chance too.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM单元是如何工作的？我现在将解释，不过我强烈建议你也看看那些教程。
- en: How do LSTM cells work?
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LSTM单元是如何工作的？
- en: An LSTM layer will contain many LSTM cells.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一个LSTM层将包含许多LSTM单元。
- en: Each LSTM cell in our Neural Network will only look at a single column of its
    inputs, and also at the previous column’s LSTM cell’s output.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们神经网络中的每个LSTM单元只会查看输入的单列，以及前一列的LSTM单元输出。
- en: Normally, we feed our LSTM Neural Network a whole matrix as its input, where
    each column corresponds to something that “comes before” the next column.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们将整个矩阵作为输入喂给LSTM神经网络，每列对应于“前面”某个内容的下一个列。
- en: 'This way, each LSTM cell will have **two different input vectors**: the previous
    LSTM cell’s output (which gives it some information about the previous input column)
    and its own input column.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，每个LSTM单元将有**两个不同的输入向量**：前一个LSTM单元的输出（为其提供有关前一个输入列的一些信息）和它自己的输入列。
- en: 'LSTM Cells in action: an intuitive example.'
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LSTM单元的实际运作：一个直观的例子。
- en: For instance, if we were training an LSTM Neural Network to predict stock exchange
    values, we could feed it a vector with a stock’s closing price in the last three
    days.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们训练一个LSTM神经网络来预测股票市场值，我们可以给它输入一个包含过去三天股票收盘价的向量。
- en: The first LSTM cell, in that case, would use the first day as input, and send
    some extracted features to the next cell.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，第一个LSTM单元将使用第一天的数据作为输入，并将一些提取的特征传递给下一个单元。
- en: That second cell would look at the second day’s price, and also at whatever
    the previous cell learned from yesterday, before generating new inputs for the
    next cell.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 那个第二个单元会查看第二天的价格，并且还会查看前一个单元从昨天学到的内容，然后生成下一个单元的新输入。
- en: After doing this for each cell, the last one will actually have a lot of temporal
    information. It will receive, from the previous one, what it learned from yesterday’s
    closing price, and from the previous two (through the other cells’ extracted information).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在对每个单元做完这些操作后，最后一个单元实际上会有很多时间信息。它将接收来自前一个单元的昨日收盘价的学习结果，以及来自前两个单元的信息（通过其他单元提取的信息）。
- en: You can experiment with different time windows, and also change how many units
    (neurons) will look at each day’s data, but this is the general idea.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试不同的时间窗口，也可以改变有多少单元（神经元）查看每天的数据，但这是大致的思路。
- en: 'How LSTM Cells work: the Math.'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LSTM单元如何工作：数学原理。
- en: The actual math behind what each cell extracts from the previous one is a bit
    more involved.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，每个单元从前一个单元提取的内容的数学原理要复杂一些。
- en: '**Forget Gate**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**忘记门**'
- en: The “forget gate” is a sigmoid layer, that regulates how much the previous cell’s
    outputs will influence this one’s.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: “忘记门”是一个sigmoid层，它调节前一个单元的输出对当前单元的影响程度。
- en: It takes as input both the previous cell’s “hidden state” (another output vector),
    and the actual inputs from the previous layer.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 它以前一个单元的“隐藏状态”（另一个输出向量）和来自前一层的实际输入作为输入。
- en: 'Since it is a sigmoid, it will return a vector of “probabilities”: values between
    0 and 1.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它是一个sigmoid函数，它将返回一个“概率”向量：值在0和1之间。
- en: They will **multiply the previous cell’s outputs** to regulate how much influence
    they hold, creating this cell’s state.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 它们将**前一个单元的输出进行乘法运算**，以调节它们所持有的影响力，从而创建该单元的状态。
- en: For instance, in a drastic case, the sigmoid may return a vector of zeroes,
    and the whole state would be multiplied by 0 and thus discarded.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在极端情况下，sigmoid可能返回一个零向量，整个状态将被乘以0，从而被丢弃。
- en: This may happen if this layer sees a very big change in the inputs distribution,
    for example.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果该层看到输入分布的变化非常大，例如，可能会发生这种情况。
- en: '**Input Gate**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入门**'
- en: Unlike the forget gate, the input gate’s output is added to the previous cell’s
    outputs (after they’ve been multiplied by the forget gate’s output).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与忘记门不同，输入门的输出会被添加到前一个单元的输出中（在它们被忘记门的输出乘法运算后）。
- en: 'The input gate is the dot product of two different layers’ outputs, though
    they both take the same input as the forget gate (previous cell’s hidden state,
    and previous layer’s outputs):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 输入门是两个不同层输出的点积，尽管它们都使用与忘记门相同的输入（前一个单元的隐藏状态和前一层的输出）：
- en: A **sigmoid unit**, regulating how much the new information will impact this
    cell’s output.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**sigmoid单元**，调节新信息对该单元输出的影响程度。
- en: A **tanh unit**, which actually extracts the new information. Notice tanh takes
    values between -1 and 1.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**tanh单元**，实际提取新信息。请注意，tanh的取值范围在-1和1之间。
- en: The **product of these two units** (which could, again, be 0, or be exactly
    equal to the tanh output, or anything in between) is added to this neuron’s cell
    state.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**这两个单元的乘积**（可能再次是0，也可能完全等于tanh的输出，或介于两者之间）被添加到该神经元的单元状态中。'
- en: '**The LSTM cell’s outputs**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**LSTM单元的输出**'
- en: The cell’s state is what the next LSTM cell will receive as input, along with
    this cell’s hidden state.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 单元的状态是下一个LSTM单元将接收的输入，连同该单元的隐藏状态一起。
- en: The hidden state will be **another tanh unit** applied to this neuron’s state,
    multiplied by another **sigmoid unit** that takes the previous layer’s and cell’s
    outputs (just like the forget gate).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏状态将是**另一个tanh单元**，应用于该神经元的状态，再乘以另一个**sigmoid单元**，该单元接受前一层和单元的输出（就像忘记门一样）。
- en: 'Here’s a visualization of what each LSTM cell looks like, borrowed from the
    tutorial I just linked:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我刚刚链接的教程中借用的每个LSTM单元的可视化图像。
- en: '![Figure](../Images/eda83ef89ac424ffec6707be8f329176.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/eda83ef89ac424ffec6707be8f329176.png)'
- en: Source: [Text Generating LSTMs](https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 来源： [文本生成LSTM](https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/)
- en: Now that we’ve covered the theory, let’s move on to some practical uses!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经覆盖了理论部分，让我们转到一些实际应用吧！
- en: As usual, all of the code is [available on GitHub](https://github.com/StrikingLoo/LoveCraftLSTM) if
    you want to try it out, or you can just follow along and see the gists.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，所有代码都可以在[GitHub上找到](https://github.com/StrikingLoo/LoveCraftLSTM)，如果你想尝试一下，或者你可以继续跟随并查看要点。
- en: Training LSTM Neural Networks with TensorFlow Keras
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用TensorFlow Keras训练LSTM神经网络
- en: For this task, I used this [dataset containing 60 Lovecraft tales](https://github.com/vilmibm/lovecraftcorpus).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，我使用了这个[包含60篇洛夫克拉夫特故事的数据集](https://github.com/vilmibm/lovecraftcorpus)。
- en: Since he wrote most of his work in the 20s, and he died in 1937, it’s now mostly
    in the public domain, so it wasn’t that hard to get.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于他大部分作品是在20年代写的，他在1937年去世，因此现在大部分作品已进入公有领域，所以获取这些作品并不困难。
- en: I thought training a Neural Network to write like him would be an interesting
    challenge.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为训练一个神经网络来模仿他的写作风格将是一个有趣的挑战。
- en: 'This is because, on the one hand, he had a very distinct style (with abundant
    purple prose: using weird words and elaborate language), but on the other he used
    a very complex vocabulary, and a Network may have trouble understanding it.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为，一方面，他有一种非常独特的风格（充满华丽的修辞：使用奇怪的词汇和复杂的语言），但另一方面，他使用了非常复杂的词汇，网络可能难以理解。
- en: 'For instance, here’s a random sentence from the first tale in the dataset:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这里是数据集中的第一个故事中的一句随机句子：
- en: At night the subtle stirring of the black city outside, the sinister scurrying
    of rats in the wormy partitions, and the creaking of hidden timbers in the centuried
    house, were enough to give him a sense of strident pandemonium
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 夜晚，黑暗城市的细微躁动、蛀虫隔板中老鼠的阴险奔走，以及百年老屋中隐蔽木材的吱吱声，足以让他感受到刺耳的混乱。
- en: If I can get a Neural Network to write “pandemonium”, then I’ll be impressed.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我能让神经网络写出“pandemonium”，那么我会很惊讶。
- en: Preprocessing our data
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理我们的数据
- en: In order to train an LSTM Neural Network to generate text, we must first preprocess
    our text data so that it can be consumed by the network.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练LSTM神经网络生成文本，我们必须首先预处理文本数据，以便网络可以处理。
- en: In this case, since a Neural Network takes vectors as input, we need a way to
    convert the text into vectors.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，由于神经网络接受向量作为输入，我们需要一种将文本转换为向量的方法。
- en: For these examples, I decided to train my LSTM Neural Networks to predict the
    next M characters in a string, taking as input the previous N ones.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些示例，我决定训练我的LSTM神经网络来预测字符串中的下一个M个字符，以之前的N个字符作为输入。
- en: To be able to feed it the N characters, I did a one-hot encoding of each one
    of them, so that the network’s input is a matrix of CxN elements, where C is the
    total number of different characters on my dataset.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够输入N个字符，我对每个字符进行了独热编码，使得网络的输入是一个CxN的矩阵，其中C是数据集中不同字符的总数。
- en: First, we read the text files and concatenate all of their contents.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们读取文本文件并将它们的内容连接起来。
- en: We limit our characters to be alphanumerical, plus a few punctuation marks.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将字符限制为字母数字和一些标点符号。
- en: We can then proceed to one-hot encode the strings into matrices, where every
    element of the *j*-th column is a 0 except for the one corresponding to the *j*-th
    character in the corpus.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将字符串进行独热编码，转化为矩阵，其中每个*j*列的元素都是0，除了对应于语料库中的*j*字符的那个元素。
- en: In order to do this, we first define a dictionary that assigns an index to each
    character.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们首先定义一个字典，将每个字符分配一个索引。
- en: Notice how, if we wished to sample our data, we could just make the variable *slices*smaller.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果我们希望对数据进行采样，我们可以将变量*slices*调整得更小。
- en: I also chose a value for *SEQ_LENGTH* of 50, making the network receive 50 characters
    and try to predict the next 50.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我还为*SEQ_LENGTH*选择了50的值，使网络接收50个字符并尝试预测接下来的50个字符。
- en: Training our LSTM Neural Network
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练我们的LSTM神经网络
- en: In order to train the Neural Network, we must first define it.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练神经网络，我们必须首先定义它。
- en: This Python code creates an LSTM Neural Network with two LSTM layers, each with
    100 units.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这段Python代码创建了一个具有两个LSTM层的LSTM神经网络，每层有100个单元。
- en: Remember each unit has one cell for each character in the input sequence, thus
    50.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，每个单元对输入序列中的每个字符都有一个单元，因此有50个。
- en: Here *VOCAB_SIZE* is just the amount of characters we’ll use, and *TimeDistributed*is
    a way of applying a given layer to each different cell, maintaining temporal ordering.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里*VOCAB_SIZE*只是我们将使用的字符数量，而*TimeDistributed*是一种将给定层应用于每个不同单元格的方式，保持时间顺序。
- en: For this model, I actually tried many different learning rates to test convergence
    speed vs overfitting.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个模型，我实际上尝试了许多不同的学习率来测试收敛速度与过拟合。
- en: 'Here’s the code for training:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是训练的代码：
- en: What you are seeing is what had the best performance in terms of loss minimization.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你看到的是在损失最小化方面表现最好的结果。
- en: However, with a binary_cross_entropy of 0.0244 in the final epoch (after 500
    epochs), here’s what the model’s output looked like.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在最后一个周期（500个周期后）中，binary_cross_entropy为0.0244，模型的输出如下。
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: There are many **good things** about this output, and **many bad ones** as well.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出有很多**好处**，也有**很多坏处**。
- en: The way the spacing is set up, with words mostly between 2 and 5 characters
    long with some longer outliers, is pretty similar to the actual word length distribution
    in the corpus.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 间距设置的方式，单词大多在2到5个字符之间，偶尔有较长的例外，这与语料库中的实际单词长度分布非常相似。
- en: I also noticed the **letters** ‘T’, ‘E’ and ‘I’ were **appearing very commonly**,
    whereas ‘y’ or ‘x’ were **less frequent**.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我还注意到**字母**‘T’，‘E’和‘I’**非常常见**，而‘y’或‘x’则**较少见**。
- en: When I looked at **letter relative frequencies** in the sampled output versus
    the corpus, they were pretty similar. It’s the **ordering** that’s **completely
    off**.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当我查看**字母相对频率**在样本输出与语料库中的比较时，它们相当相似。问题在于**排序**完全**错乱**。
- en: There is also something to be said about how **capital letters only appear after
    spaces**, as is usually the case in English.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一点需要指出的是，**大写字母仅在空格后出现**，这通常是英语中的情况。
- en: To generate these outputs, I simply asked the model to predict the next 50 characters
    for different 50 character subsets in the corpus. If it’s this bad with training
    data, I figured testing or random data wouldn’t be worth checking.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成这些输出，我只是让模型预测语料库中不同50个字符子集的下一个50个字符。如果训练数据如此糟糕，我觉得测试或随机数据也不会值得检查。
- en: 'The nonsense actually reminded me of one of H. P. Lovecraft’s most famous tales,
    “Call of Cthulhu”, where people start having hallucinations about this cosmic,
    eldritch being, and they say things like:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这些无意义的内容实际上让我想起了H. P. Lovecraft最著名的故事之一，“克苏鲁的呼唤”，其中人们开始对这个宇宙的、古怪的存在产生幻觉，他们会说：
- en: Ph’nglui mglw’nafh *Cthulhu R’lyeh* wgah’nagl fhtagn.
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Ph’nglui mglw’nafh *Cthulhu R’lyeh* wgah’nagl fhtagn.
- en: Sadly the model wasn’t overfitting that either, it was clearly **underfitting**.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 可惜模型也没有过拟合，它明显**欠拟合**。
- en: 'So I tried to make its task smaller, and the model bigger: 125 units, predicting
    only 30 characters.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我试图让任务更小，模型更大：125个单元，预测仅30个字符。
- en: Bigger model, smaller problem. Any results?
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更大的模型，更小的问题。有什么结果吗？
- en: With this smaller model, after another 500 epochs, some patterns began to emerge.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个更小的模型下，再经过500个训练周期后，一些模式开始出现。
- en: Even though the loss function wasn’t that much smaller (at 210), the character’s
    frequency remained similar to the corpus’.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管损失函数（210）没有小太多，字符的频率仍然与语料库相似。
- en: 'The ordering of characters improved a lot though: here’s a random sample from
    its output, see if you can spot some words.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，字符的排序改善了很多：这是它输出的一个随机样本，看看你能否发现一些单词。
- en: '[PRE1]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Tech, the, and, was… **small words** are where it’s at! It also realized many
    words ended with **common suffixes** like -ing, -ed, and -tion.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 技术、the 和 and，was… **小词**是重点！它还意识到许多单词以**常见后缀**如-ing，-ed和-tion结尾。
- en: Out of 10000 words, 740 were “*the*“, 37 ended in “*tion*” (whereas only 3 contained
    without ending in it), and 115 ended in –*ing*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在10000个单词中，740个是“*the*”，37个以“*tion*”结尾（而只有3个是没有以此结尾的），115个以-*ing*结尾。
- en: Other common words were “than” and “that”, though the model was clearly still
    unable to produce English sentences.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其他常见单词包括“than”和“that”，尽管模型显然仍然无法生成英文句子。
- en: Even bigger model
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更大的模型
- en: This gave me hopes. The Neural Network was clearly learning *something*, just
    not enough.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我希望。神经网络显然在学习*某些东西*，只是还不够多。
- en: 'So I did what you do when your model underfits: I tried an even bigger Neural
    Network.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我做了当模型欠拟合时应该做的事情：我尝试了一个更大的神经网络。
- en: Take into account, I’m running this on my laptop.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑，我是在我的笔记本电脑上运行这个。
- en: With a modest 16GB of RAM and an i7 processor, these models take hours to learn.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 即使配备了16GB的内存和i7处理器，这些模型仍需数小时才能完成学习。
- en: So I set the amount of units to 150, and tried my hand again at 50 characters.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我将单位数量设置为150，再次尝试了50个字符。
- en: I figured maybe giving it a smaller time window was making things harder for
    the Network.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我想也许给它更小的时间窗口会让网络更难处理。
- en: Here’s what the model’s output was like, after a few hours of training.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这是模型在经过几个小时训练后的输出情况。
- en: '[PRE2]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Pure nonsense, except a lot of “the” and “and”s.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 完全是无意义的，除了大量的“the”和“and”。
- en: It was actually saying “the” more often than the previous one, but it hadn’t
    learned about gerunds yet (no -ing).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，它说“the”的频率比之前更高，但它还没有学会动名词（没有 -ing）。
- en: Interestingly, many words here ended with “-ed” which means it was kinda grasping
    the idea of the **past tense**.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这里许多单词以“-ed”结尾，这意味着它有点儿掌握了**过去时**的概念。
- en: I let it go at it a few hundred more epochs (to a total of 750).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我让它继续训练了几百个周期（共750个）。
- en: 'The output didn’t change too much, still a lot of “the”, “a” and “an”, and
    still no bigger structure. Here’s another sample:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 输出变化不大，仍然是大量的“the”、“a”和“an”，并且没有更大的结构。这里是另一个样本：
- en: '[PRE3]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: An interesting thing that emerged here though, was the use of prepositions and
    pronouns.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 但有趣的是，出现了对介词和代词的使用。
- en: The network wrote “I”, “you”, “she”, “we”, “of” and other similar words a few
    times. All in all, **prepositions and pronouns** amounted to about **10% of the
    total sampled words**.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 网络写了几次“I”，“you”，“she”，“we”，“of”等类似的词。总的来说，**介词和代词**大约占**总采样单词的10%**。
- en: This was an improvement, as the Network was clearly learning low-entropy words.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个改进，因为网络显然在学习低熵的单词。
- en: However, it was still far from generating coherent English texts.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它仍然远未生成连贯的英语文本。
- en: I let it train 100 more epochs, and then killed it.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我让它再训练了100个周期，然后停止了。
- en: Here’s its last output.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的最后输出。
- en: '[PRE4]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: I knew it was doing its best, but it wasn’t really going anywhere, at least
    not quickly enough.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道它已经尽力而为，但它确实没有取得实质性的进展，至少不够快。
- en: I thought of accelerating convergence speed with **Batch Normalization**.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我考虑通过**批量归一化**来加快收敛速度。
- en: However, I read on StackOverflow that BatchNorm is not supposed to be used with
    LSTM Neural Networks.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我在StackOverflow上读到，BatchNorm不应该与LSTM神经网络一起使用。
- en: If any of you is more experienced with LSTM nets, please let me know if that’s
    right in the comments!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你们中有人对LSTM网络更有经验，请在评论中告诉我这是否正确！
- en: At last, I tried this same task with 10 characters as input and 10 as output.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我尝试了用10个字符作为输入，10个字符作为输出来执行相同的任务。
- en: 'I guess the model wasn’t getting enough context to predict things well enough
    though: the results were awful.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我想模型没有获得足够的上下文来很好地预测事物：结果非常糟糕。
- en: I considered the experiment finished for now.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我考虑目前实验已经结束了。
- en: Conclusions
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: While it is clear, looking at other people’s work, that an LSTM Neural Network *could*learn
    to write like Lovecraft, I don’t think my PC is powerful enough to train a big
    enough model in a reasonable time.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管从其他人的工作中可以看出，LSTM神经网络*可能*学会像洛夫克拉夫特一样写作，但我认为我的电脑不够强大，无法在合理的时间内训练出足够大的模型。
- en: Or maybe it just needs more data than I had.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，也许它只是需要比我拥有的更多的数据。
- en: In the future, I’d like to repeat this experiment with a word-based approach
    instead of a character-based one.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 未来，我希望用基于词的方式重复这个实验，而不是基于字符的方式。
- en: I checked, and about 10% of the words in the corpus appear only once.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我检查了一下，语料库中大约10%的单词只出现过一次。
- en: Is there any good practice I should follow if I removed them before training?
    Like replacing all nouns with the same one, sampling from [clusters](http://www.datastuff.tech/machine-learning/k-means-clustering-unsupervised-learning-for-recommender-systems/),
    or something? Please let me know! I’m sure many of you are more experienced with
    LSTM neural networks than I.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我在训练之前删除了它们，有什么好的做法吗？比如用相同的名词替换所有名词，从[簇](http://www.datastuff.tech/machine-learning/k-means-clustering-unsupervised-learning-for-recommender-systems/)中采样，或者其他方法？请告诉我！我相信你们中的许多人在LSTM神经网络方面比我更有经验。
- en: Do you think this would have worked better with a different architecture? Something
    I should have handled differently? Please also let me know, I want to learn more
    about this.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为如果使用不同的架构会更好吗？有什么我应该以不同方式处理的地方吗？请告诉我，我想了解更多关于这方面的知识。
- en: Did you find any rookie mistakes on my code? Do you think I’m an idiot for not
    trying XYZ? Or did you actually find my experiment enjoyable, or maybe you even
    learned something from this article?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 你在我的代码中发现了什么新手错误吗？你认为我因为没尝试XYZ而显得很愚蠢吗？还是你实际上觉得我的实验很有趣，或者你甚至从这篇文章中学到了什么？
- en: Contact me on [Twitter](https://www.twitter.com/strikingloo), [LinkedIn](http://linkedin.com/in/luciano-strika), [Medium](https://medium.com/@strikingloo) or [Dev.to](http://www.dev.to/strikingloo) if
    you want to discuss that, or any related topic.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如需讨论此事或相关话题，请在[Twitter](https://www.twitter.com/strikingloo)、[LinkedIn](http://linkedin.com/in/luciano-strika)、[Medium](https://medium.com/@strikingloo)或[Dev.to](http://www.dev.to/strikingloo)与我联系。
- en: '*If you want to become a Data scientist, or learn something new, check out
    my [Machine Learning Reading List](http://www.datastuff.tech/data-science/3-machine-learning-books-that-helped-me-level-up-as-a-data-scientist/)!*'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你想成为数据科学家或学习新东西，查看我的[机器学习阅读清单](http://www.datastuff.tech/data-science/3-machine-learning-books-that-helped-me-level-up-as-a-data-scientist/)!*'
- en: '**Bio: [Luciano Strika](http://www.datastuff.tech/author/strikingloo/)** is
    a computer science student at Buenos Aires University, and a data scientist at
    MercadoLibre. He also writes about machine learning and data on [www.datastuff.tech](http://www.datastuff.tech/).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[Luciano Strika](http://www.datastuff.tech/author/strikingloo/)** 是布宜诺斯艾利斯大学的计算机科学学生，同时也是MercadoLibre的数据科学家。他还在[www.datastuff.tech](http://www.datastuff.tech/)上撰写有关机器学习和数据的文章。'
- en: '[Original](http://www.datastuff.tech/machine-learning/lstm-how-to-train-neural-networks-to-write-like-lovecraft).
    Reposted with permission.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](http://www.datastuff.tech/machine-learning/lstm-how-to-train-neural-networks-to-write-like-lovecraft)。经许可转载。'
- en: '**Related:**'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Autoencoders: Deep Learning with TensorFlow’s Eager Execution](/2019/05/autoencoders-deep-learning-with-tensorflows-eager-execution.html)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动编码器：使用TensorFlow的Eager Execution进行深度学习](/2019/05/autoencoders-deep-learning-with-tensorflows-eager-execution.html)'
- en: '[3 Machine Learning Books that Helped me Level Up as a Data Scientist](/2019/05/3-machine-learning-books-helped-level-up-data-scientist.html)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3本提升我作为数据科学家技能的机器学习书籍](/2019/05/3-machine-learning-books-helped-level-up-data-scientist.html)'
- en: '[Understanding Backpropagation as Applied to LSTM](/2019/05/understanding-backpropagation-applied-lstm.html)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解反向传播在LSTM中的应用](/2019/05/understanding-backpropagation-applied-lstm.html)'
- en: '* * *'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三名课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的快车道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT需求'
- en: '* * *'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用管道编写干净的Python代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成为优秀数据科学家所需的5项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个初学者数据科学家都应掌握的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2021年最佳ETL工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
- en: '[Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用TensorFlow和Keras构建和训练你的第一个神经网络](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，并寻找目标来…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
