- en: Model Evaluation Metrics in Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/05/model-evaluation-metrics-machine-learning.html](https://www.kdnuggets.com/2020/05/model-evaluation-metrics-machine-learning.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: Predictive models have become a trusted advisor to many businesses and for a
    good reason. These models can “foresee the future”, and there are many different
    methods available, meaning any industry can find one that fits their particular
    challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we talk about predictive models, we are talking either about a **regression
    model** (continuous output) or a **classification model** (nominal or binary output).
    In classification problems, we use two types of algorithms (dependent on the kind
    of output it creates):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Class output**: Algorithms like SVM and KNN create a class output. For instance,
    in a binary classification problem, the outputs will be either 0 or 1\. However,
    today we have algorithms that can convert these class outputs to probability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Probability output**: Algorithms like Logistic Regression, Random Forest,
    Gradient Boosting, Adaboost, etc. give probability outputs. Converting probability
    outputs to class output is just a matter of creating a threshold probability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While data preparation and training a machine learning model is a key step in
    the machine learning pipeline, it’s equally important to measure the performance
    of this trained model. How well the model generalizes on the unseen data is what
    defines adaptive vs non-adaptive machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: By using different metrics for performance evaluation, we should be in a position
    to improve the overall predictive power of our model before we roll it out for
    production on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Without doing a proper evaluation of the ML model using different metrics, and
    depending only on accuracy, it can lead to a problem when the respective model
    is deployed on unseen data and can result in poor predictions.
  prefs: []
  type: TYPE_NORMAL
- en: This happens because, in cases like these, our models don’t learn but instead
    memorize;hence, they cannot generalize well on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Model Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let us now define the evaluation metrics for evaluating the performance of a
    machine learning model, which is an integral component of any data science project.
    It aims to estimate the generalization accuracy of a model on the future (unseen/out-of-sample)
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion Matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A confusion matrix is a matrix representation of the prediction results of any
    binary testing that is often used to **describe the performance of the classification
    model** (or “classifier”) on a set of test data for which the true values are
    known.
  prefs: []
  type: TYPE_NORMAL
- en: The confusion matrix itself is relatively simple to understand, but the related
    terminology can be confusing.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/238722baa8343d76a6f5ae4014af4dbe.png)'
  prefs: []
  type: TYPE_IMG
- en: Confusion matrix with 2 class labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each prediction can be one of the four outcomes, based on how it matches up
    to the actual value:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positive (TP): **Predicted True and True in reality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negative (TN): **Predicted False and False in reality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positive (FP):** Predicted True and False in reality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negative (FN):** Predicted False and True in reality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let us understand this concept using hypothesis testing.
  prefs: []
  type: TYPE_NORMAL
- en: A **Hypothesis** is speculation or theory based on insufficient evidence that
    lends itself to further testing and experimentation. With further testing, a hypothesis
    can usually be proven true or false.
  prefs: []
  type: TYPE_NORMAL
- en: A **Null Hypothesis** is a hypothesis that says there is no statistical significance
    between the two variables in the hypothesis. It is the hypothesis that the researcher
    is trying to disprove.
  prefs: []
  type: TYPE_NORMAL
- en: We would always reject the null hypothesis when it is false, and we would accept
    the null hypothesis when it is indeed true.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Even though hypothesis tests are meant to be reliable, **there are two types
    of errors that can occur.**
  prefs: []
  type: TYPE_NORMAL
- en: These errors are known as **Type 1 and Type II errors.**
  prefs: []
  type: TYPE_NORMAL
- en: For example, when examining the effectiveness of a drug, the null hypothesis
    would be that the drug does not affect a disease.
  prefs: []
  type: TYPE_NORMAL
- en: '**Type I Error:- **equivalent to False Positives(FP).'
  prefs: []
  type: TYPE_NORMAL
- en: The first kind of error that is possible involves the rejection of a null hypothesis
    that is true.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go back to the example of a drug being used to treat a disease. If we
    reject the null hypothesis in this situation, then we claim that the drug does
    have some effect on a disease. But if the null hypothesis is true, then, in reality,
    the drug does not combat the disease at all. The drug is falsely claimed to have
    a positive effect on a disease.
  prefs: []
  type: TYPE_NORMAL
- en: '**Type II Error:- **equivalent to False Negatives(FN).'
  prefs: []
  type: TYPE_NORMAL
- en: The other kind of error that occurs when we accept a false null hypothesis.
    This sort of error is called a type II error and is also referred to as an error
    of the second kind.
  prefs: []
  type: TYPE_NORMAL
- en: If we think back again to the scenario in which we are testing a drug, what
    would a type II error look like? A type II error would occur if we accepted that
    the drug hs no effect on disease, but in reality, it did.
  prefs: []
  type: TYPE_NORMAL
- en: A sample python implementation of the Confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/32ef98a849a9bf9dadcf60b17ff8df47.png)'
  prefs: []
  type: TYPE_IMG
- en: Confusion matrix with 3 class labels.
  prefs: []
  type: TYPE_NORMAL
- en: The diagonal elements represent the number of points for which the predicted
    label is equal to the true label, while anything off the diagonal was mislabeled
    by the classifier. Therefore, the higher the diagonal values of the confusion
    matrix the better, indicating many correct predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, the classifier predicted all the 13 setosa and 18 virginica plants
    in the test data perfectly. However, it incorrectly classified 4 of the versicolor
    plants as virginica.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also a list of rates that are often computed from a confusion matrix
    for a binary classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Accuracy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Overall, how often is the classifier correct?
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy = (TP+TN)/total
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When our classes are roughly equal in size, we can use accuracy**, **which will
    give us correctly classified values.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy is a common evaluation metric for classification problems. It’s the
    number of correct predictions made as a ratio of all predictions made.
  prefs: []
  type: TYPE_NORMAL
- en: '**Misclassification Rate(Error Rate):** Overall, how often is it wrong. Since
    accuracy is the percent we correctly classified (success rate), it follows that
    our error rate (the percentage we got wrong) can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Misclassification Rate = (FP+FN)/total
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We use the sklearn module to compute the accuracy of a classification task,
    as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: The classification accuracy is **88%** on the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Precision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When it predicts yes, how often is it correct?
  prefs: []
  type: TYPE_NORMAL
- en: Precision=TP/predicted yes
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When we have a class imbalance, accuracy can become an unreliable metric for
    measuring our performance. For instance, if we had a 99/1 split between two classes,
    A and B, where the rare event, B, is our positive class, we could build a model
    that was 99% accurate by just saying everything belonged to class A. Clearly,
    we shouldn’t bother building a model if it doesn’t do anything to identify class
    B; thus, we need different metrics that will discourage this behavior. For this,
    we use precision and recall instead of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Recall or Sensitivity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When it’s actually yes, how often does it predict yes?
  prefs: []
  type: TYPE_NORMAL
- en: True Positive Rate = TP/actual yes
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Recall gives us the **true positive rate** (**TPR**), which is the ratio of
    true positives to everything positive.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of the 99/1 split between classes A and B, the model that classifies
    everything as A would have a recall of 0% for the positive class, B (precision
    would be undefined — 0/0). Precision and recall provide a better way of evaluating
    model performance in the face of a class imbalance. They will correctly tell us
    that the model has little value for our use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like accuracy, both precision and recall are easy to compute and understand
    but require thresholds. Besides, precision and recall only consider half of the
    confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91ec868e492267fb92b1daed648438bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 4\. F1 Score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The F1 score is the [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean) of
    the [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall),
    where an F1 score reaches its best value at 1 (perfect precision and recall) and
    worst at 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77f68629d1a37e25bca6efa9b7204007.png)'
  prefs: []
  type: TYPE_IMG
- en: '***Why harmonic mean?*** Since the harmonic mean of a list of numbers skews
    strongly toward the least elements of the list, it tends (compared to the arithmetic
    mean) to mitigate the impact of large outliers and aggravate the impact of small
    ones.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An F1 score punishes extreme values more. Ideally, an F1 Score could be an
    effective evaluation metric in the following classification scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '*When FP and FN are equally costly — meaning they miss on true positives or
    find false positives — both impact the model almost the same way, as in our cancer
    detection classification example*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Adding more data doesn’t effectively change the outcome effectively*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TN is high (like with flood predictions, cancer predictions, etc.)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A sample python implementation of the F1 score.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0386367949b03779167d8b46213b15b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 5\. Specificity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When it’s no, how often does it predict no?
  prefs: []
  type: TYPE_NORMAL
- en: True Negative Rate=TN/actual no
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is the **true negative rate** or the proportion of true negatives to everything
    that should have been classified as negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that, together, specificity and sensitivity consider the full confusion
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1fa5a1519c294b00d3fe48bfcbf3574.png)'
  prefs: []
  type: TYPE_IMG
- en: 6\. Receiver Operating Characteristics (ROC) Curve
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Measuring the area under the ROC curve is also a very useful method for evaluating
    a model. By plotting the true positive rate (sensitivity) versus the false-positive
    rate (1 — specificity), we get the **Receiver Operating Characteristic** (**ROC**) **curve**.
    This curve allows us to visualize the trade-off between the true positive rate
    and the false positive rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are examples of good ROC curves. The dashed line would be random
    guessing (no predictive value) and is used as a baseline; anything below that
    is considered worse than guessing. We want to be toward the top-left corner:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d04f5b222c9909d1cb36c50e47631161.png)'
  prefs: []
  type: TYPE_IMG
- en: A sample python implementation of the ROC curves.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/beff968a8224e5e7c16acd4fe6153e37.png)'
  prefs: []
  type: TYPE_IMG
- en: In the example above, the AUC is relatively close to 1 and greater than 0.5\.
    A perfect classifier will have the ROC curve go along the Y-axis and then along
    the X-axis.
  prefs: []
  type: TYPE_NORMAL
- en: Log Loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Log Loss is the most important classification metric based on probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/4525d39c13e7dc150aed104370ae1e9e.png)'
  prefs: []
  type: TYPE_IMG
- en: As the **predicted probability** of the **true class** gets **closer to zero**,
    the **loss increases exponentially**
  prefs: []
  type: TYPE_NORMAL
- en: It measures the performance of a classification model where the prediction input
    is a probability value between 0 and 1\. Log loss increases as the predicted probability
    diverge from the actual label. The goal of any machine learning model is to minimize
    this value. As such, smaller log loss is better, with a perfect model having a
    log loss of 0.
  prefs: []
  type: TYPE_NORMAL
- en: A sample python implementation of the Log Loss.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Jaccard Index
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Jaccard Index is one of the simplest ways to calculate and find out the accuracy
    of a classification ML model. Let’s understand it with an example. Suppose we
    have a labeled test set, with labels as –
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: And our model has predicted the labels as –
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/667302f21a8d59e942ab7771b3436eb0.png)'
  prefs: []
  type: TYPE_IMG
- en: The above Venn diagram shows us the labels of the test set and the labels of
    the predictions, and their intersection and union.
  prefs: []
  type: TYPE_NORMAL
- en: Jaccard Index or Jaccard similarity coefficient is a statistic used in understanding
    the similarities between sample sets. The measurement emphasizes the similarity
    between finite sample sets and is formally defined as the size of the intersection
    divided by the size of the union of the two labeled sets, with formula as –
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed169e327c593c70f7820fcb39920d2b.png)'
  prefs: []
  type: TYPE_IMG
- en: '![Figure](../Images/c2d7428abbdb789e23a8de8841b4db80.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Jaccard Index or Intersection over Union(IoU)](https://commons.wikimedia.org/wiki/File:Intersection_over_Union_-_visual_equation.png)'
  prefs: []
  type: TYPE_NORMAL
- en: So, for our example, we can see that the intersection of the two sets is equal
    to 8 (since eight values are predicted correctly) and the union is **10 + 10–8
    = 12**. So, the Jaccard index gives us the accuracy as –
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/948566c9337719113a09c9a1547bce4b.png)'
  prefs: []
  type: TYPE_IMG
- en: So, the accuracy of our model, according to **Jaccard Index**, becomes 0.66,
    or 66%.
  prefs: []
  type: TYPE_NORMAL
- en: Higher the Jaccard index higher the accuracy of the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: A sample python implementation of the Jaccard index.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Kolomogorov Smirnov chart
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: K-S or Kolmogorov-Smirnov chart measures the performance of classification models.
    More accurately, K-S is a measure of the degree of separation between positive
    and negative distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/4f71119b16403ed7cdcec55d961cede5.png)'
  prefs: []
  type: TYPE_IMG
- en: '[The cumulative frequency for the observed and hypothesized distributions is
    plotted against the ordered frequencies. The vertical double arrow indicates the
    maximal vertical difference](https://www.sciencedirect.com/topics/medicine-and-dentistry/kolmogorov-smirnov-test).'
  prefs: []
  type: TYPE_NORMAL
- en: The `K-S is 100` if the scores partition the population into two separate groups
    in which one group contains all the positives and the other all the negatives.
    On the other hand, If the model cannot differentiate between positives and negatives,
    then it is as if the model selects cases randomly from the population. The `K-S
    would be 0`.
  prefs: []
  type: TYPE_NORMAL
- en: In most classification models the K-S will fall between 0 and 100, and that
    the higher the value the better the model is at separating the positive from negative
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: The K-S may also be used to test whether two underlying one-dimensional probability
    distributions differ. It is a very efficient way to determine if two samples are
    significantly different from each other.
  prefs: []
  type: TYPE_NORMAL
- en: A sample python implementation of the Kolmogorov-Smirnov.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6746fe34f97779b5ade9c549dd64480.png)'
  prefs: []
  type: TYPE_IMG
- en: The Null hypothesis used here assumes that the numbers follow the normal distribution.
    It returns statistics and p-value. If the **p-value is < alpha**, we reject the
    Null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: '*Alpha is defined as the probability of rejecting the null hypothesis given
    the null hypothesis(H*0*) is true. For most of the practical applications, alpha
    is chosen as 0.05.*'
  prefs: []
  type: TYPE_NORMAL
- en: Gain and Lift Chart
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gain or Lift is a measure of the effectiveness of a classification model calculated
    as the ratio between the results obtained with and without the model. Gain and
    lift charts are visual aids for evaluating the performance of classification models.
    However, in contrast to the confusion matrix that evaluates models on the whole
    population gain or lift chart evaluates model performance in a portion of the
    population.
  prefs: []
  type: TYPE_NORMAL
- en: The higher the lift (i.e. the further up it is from the baseline), the better
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: The following gains chart, run on a validation set, shows that with 50% of the
    data, the model contains 90% of targets, Adding more data adds a negligible increase
    in the percentage of targets included in the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/dddcc35be3fae9094d555ba1d1e2cf8a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Gain/lift chart](https://www.datasciencecentral.com/profiles/blogs/comparing-model-evaluation-techniques-part-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Lift charts are often shown as a **cumulative lift chart**, which is also known
    as a **gains chart**. Therefore, gains charts are sometimes (perhaps confusingly)
    called “lift charts”, but they are more accurately *cumulative *lift charts.
  prefs: []
  type: TYPE_NORMAL
- en: It is one of their most common uses is in marketing, to decide if a prospective
    client is worth calling.
  prefs: []
  type: TYPE_NORMAL
- en: Gini Coefficient
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Gini coefficient or Gini Index is a popular metric for imbalanced class
    values. The coefficient ranges from 0 to 1 where 0 represents perfect equality
    and 1 represents perfect inequality. Here, if the value of an index is higher,
    then the data will be more dispersed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gini coefficient can be computed from the area under the ROC curve using the
    following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: Gini Coefficient = (2 * ROC_curve) — 1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Understanding how well a machine learning model is going to perform on unseen
    data is the ultimate purpose behind working with these evaluation metrics. Metrics
    like accuracy, precision, recall are good ways to evaluate classification models
    for balanced datasets, but if the data is imbalanced and there’s a class disparity,
    then other methods like ROC/AUC, Gini coefficient perform better in evaluating
    the model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Well, this concludes this article**. **I hope you guys have enjoyed reading
    it, feel free to share your comments/thoughts/feedback in the comment section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Thanks for reading !!!**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Nagesh Singh Chauhan](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)**
    is a Big data developer at CirrusLabs. He has over 4 years of working experience
    in various sectors like Telecom, Analytics, Sales, Data Science having specialisation
    in various Big data components.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://levelup.gitconnected.com/model-evaluation-metrics-in-machine-learning-8988739236fc).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Dimensionality Reduction with Principal Component Analysis (PCA)](/2020/05/dimensionality-reduction-principal-component-analysis.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hyperparameter Optimization for Machine Learning Models](/2020/05/hyperparameter-optimization-machine-learning-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[DBSCAN Clustering Algorithm in Machine Learning](/2020/04/dbscan-clustering-algorithm-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Machine Learning Evaluation Metrics: Theory and Overview](https://www.kdnuggets.com/machine-learning-evaluation-metrics-theory-and-overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
