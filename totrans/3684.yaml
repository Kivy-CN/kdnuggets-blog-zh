- en: 'Introducing TPU v4: Googles Cutting Edge Supercomputer for Large Language Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/04/introducing-tpu-v4-googles-cutting-edge-supercomputer-large-language-models.html](https://www.kdnuggets.com/2023/04/introducing-tpu-v4-googles-cutting-edge-supercomputer-large-language-models.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Introducing TPU v4: Googles Cutting Edge Supercomputer for Large Language
    Models](../Images/0dad33ae6b45aeffa128995ae7739c43.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Editor
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning and artificial intelligence seem to be growing at a rapid rate
    that some of us can even keep up with. As these machine-learning models get better
    at what they do, they will require better infrastructure and hardware support
    to keep them going. The advancement of machine learning has a direct lead to scaling
    computing performance. So let’s learn more about TPU v4.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: What is TPU v4?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TPU stands for Tensor Processing Unit and they were designed for machine learning
    and deep learning applications. TPU was invented by Google and was constructed
    in a way that it has the ability to be able to handle the high computational needs
    of machine learning and artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: When Google designed the TPU, they created it as a domain-specific architecture,
    which means they designed it as a matrix processor, instead of it being a general-purpose
    processor so that it specializes in neural network workloads. This solves Google's
    issue of memory access problem which slows down GPUs and CPUs, causing them to
    use more processing power.
  prefs: []
  type: TYPE_NORMAL
- en: So there’s been TPU v2, v3, and now v4\. So what’s v2 all about?
  prefs: []
  type: TYPE_NORMAL
- en: 'The TPU v2 chip contains two TensorCores, four MXUs, a vector unit, and a scalar
    unit. See the image below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing TPU v4: Googles Cutting Edge Supercomputer for Large Language
    Models](../Images/259c8fa83f027f061868b432cfacae63.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Google](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm)
  prefs: []
  type: TYPE_NORMAL
- en: Optical Circuit Switches (OCSes)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TPU v4 is the first supercomputer to deploy reconfigurable optical circuit switches.
    Optical circuit switches (OCS) are considered to be more effective. They reduce
    congestion found in previous networks because they are transmitted as they occur.
    OCS improves scalability, availability, modularity, deployment, security, power,
    performance, and more.
  prefs: []
  type: TYPE_NORMAL
- en: OCSes and other optical components in TPU v4 make up less than 5% of TPU v4’s
    system cost and less than 5% of system power.
  prefs: []
  type: TYPE_NORMAL
- en: SparseCores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TPU v4 also is the first supercomputer with hardware support for embedding.
    Neural networks train well on dense vectors, and embeddings are the most effective
    way to transform categorical feature values into dense vectors. TPU v4 include
    third-generation SparseCores, which are dataflow processes that accelerate machine
    learning models that are reliant on embedding.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the embedding function can translate a word in English, which would
    be considered a large categorical space into a smaller dense space of a 100-vector
    representation of each word. Embedding is a key element to Deep Learning Recommendation
    Models (DLRMs), which are part of our everyday lives and are used in advertising,
    search ranking, YouTube, and more.
  prefs: []
  type: TYPE_NORMAL
- en: The image below shows the performance of recommendation models on CPUs, TPU
    v3, TPU v4 (using SparseCore), and TPU v4 with embeddings in CPU memory (not using
    SparseCore). As you can see the TPU v4 SparseCore is 3X faster than TPU v3 on
    recommendation models, and 5–30X faster than systems using CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing TPU v4: Googles Cutting Edge Supercomputer for Large Language
    Models](../Images/70bd04dcc3d9db52513a04d61349bb72.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Google](https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains)
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TPU v4 outperforms TPU v3 by 2.1x and has an improved performance/Watt by 2.7x.
    TPU v4 is 4x larger at 4096 chips, making it 10x faster. The implementation and
    flexibility of OCS are also major help for large language models.
  prefs: []
  type: TYPE_NORMAL
- en: The performance and availability of TPU v4 supercomputers are being heavily
    considered to improve large language models such as LaMDA, MUM, and PaLM. PaLM,
    the 540B-parameter model was trained on TPU v4 for over 50 days and had a remarkable
    57.8% hardware floating point performance.
  prefs: []
  type: TYPE_NORMAL
- en: TPU v4 also has multidimensional model-partitioning techniques that enable low-latency,
    high-throughput inference for large language models.
  prefs: []
  type: TYPE_NORMAL
- en: Energy Efficiency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With more laws and regulations being put in place for companies globally to
    do better to improve their overall energy efficiency, TPU v4 is doing a decent
    job. TPU v4s inside Google Cloud use ~2-6x less energy and produce ~20x less CO2e
    than contemporary DSAs in typical on-premise data centres.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning Workloads Changes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So now you know a bit more about TPU v4, you might be wondering how fast machine
    learning workloads actually change with TPU v4.
  prefs: []
  type: TYPE_NORMAL
- en: The below table shows the workload by deep neural network model type and the
    % TPUs used. Over 90% of training at Google is on TPUs, and this table shows the
    fast change in production workloads at Google.
  prefs: []
  type: TYPE_NORMAL
- en: There is a drop in recurrent neural networks (RNN), as this is because RNNs
    process the input all at once rather than sequentially, in comparison to transforms
    which are known for natural language translation and text summarization.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing TPU v4: Googles Cutting Edge Supercomputer for Large Language
    Models](../Images/22ec13f5b8689ecfcfc1bbf86ee8bf38.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To learn more about TPU v4 capabilities, read the research paper [TPU v4: An
    Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support
    for Embeddings](https://arxiv.org/pdf/2304.01433.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping it up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Last year, TPU v4 supercomputers were available to AI researchers and developers
    at Google Cloud’s ML cluster in Oklahoma. The author of this [paper](https://arxiv.org/pdf/2304.01433.pdf)
    claims that the TPU v4 is faster and uses less power than Nvidia A100\. However,
    they have not been able to compare the TPU v4 to the newer Nvidia H100 GPUs due
    to their limited availability and its 4nm architecture, whereas TPU v4 has a 7nm
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: What do you think TPU v4 is capable of, its limitations, and is it better than
    Nvidia A100 GPU?
  prefs: []
  type: TYPE_NORMAL
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist,
    Freelance Technical Writer and Community Manager at KDnuggets. She is particularly
    interested in providing Data Science career advice or tutorials and theory based
    knowledge around Data Science. She also wishes to explore the different ways Artificial
    Intelligence is/can benefit the longevity of human life. A keen learner, seeking
    to broaden her tech knowledge and writing skills, whilst helping guide others.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Introducing Healthcare-Specific Large Language Models from John Snow Labs](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cutting Down Implementation Time by Integrating Jupyter and KNIME](https://www.kdnuggets.com/2021/12/cutting-implementation-time-integrating-jupyter-knime.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Open Source Large Language Models](https://www.kdnuggets.com/2022/09/john-snow-top-open-source-large-language-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Learn About Large Language Models](https://www.kdnuggets.com/2023/03/learn-large-language-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are Large Language Models and How Do They Work?](https://www.kdnuggets.com/2023/05/large-language-models-work.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AI: Large Language & Visual Models](https://www.kdnuggets.com/2023/06/ai-large-language-visual-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
