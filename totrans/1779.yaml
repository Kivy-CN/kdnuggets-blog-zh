- en: 17 More Must-Know Data Science Interview Questions and Answers, Part 3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/03/17-data-science-interview-questions-answers-part-3.html](https://www.kdnuggets.com/2017/03/17-data-science-interview-questions-answers-part-3.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Editor''s note:** This is the third and final part of this post. See also:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**17 More Must-Know Data Science Interview Questions and Answers, Part 1**](/2017/02/17-data-science-interview-questions-answers.html)'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**17 More Must-Know Data Science Interview Questions and Answers, Part 2**](/2017/02/17-data-science-interview-questions-answers-part-2.html)'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This post contains answers to:'
  prefs: []
  type: TYPE_NORMAL
- en: Q13\. What makes a good data visualization?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q14\. What are some of the common data quality issues when dealing with Big
    Data? What can be done to avoid them or to mitigate their impact?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q15\. In an A/B test, how can we ensure that assignment to the various buckets
    is truly random?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q16\. How would you conduct an A/B test on an opt-in feature?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q17\. How to determine the influence of a Twitter user?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Q13\. What makes a good data visualization?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Gregory Piatetsky](/author/gregory-piatetsky) answers:**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: This answer contains excerpts from the recent post [What makes a good
    data visualization – a Data Scientist perspective](/2017/03/what-makes-good-data-visualization.html).*'
  prefs: []
  type: TYPE_NORMAL
- en: Data Science is more than just building predictive models - it is also about
    explaining the models and using them to help people to understand data and make
    decisions. Data visualization is an integral part of presenting data in a convincing
    way.
  prefs: []
  type: TYPE_NORMAL
- en: There is a ton of research of good data visualization and how people best perceive
    information - see work by [Stephen Few](https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/data-visualization-for-human-perception)
    and many others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Guidelines on improving human perception include:'
  prefs: []
  type: TYPE_NORMAL
- en: position data along a common scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: bars are more effective than circles or squares in communicating size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: color is more discernible than shape in scatterplots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: avoid pie chart unless it is for showing proportions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: avoid 3D charts and reduce chartjunk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sunburst visualization is more effective for hierarchical plots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use small multiples (even though animation looks cool, it is less effective
    for understanding changing data.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See [39 studies about human perception](https://medium.com/@kennelliott/39-studies-about-human-perception-in-30-minutes-4728f9e31a73#.irdxe7jip),
    by Washington Post graphics editor for a lot more detail.
  prefs: []
  type: TYPE_NORMAL
- en: From Data Science point of view, what makes visualization important is highlighting
    the key aspects of data - what are the most important variables, what is their
    relative importance, what are the changes and trends.
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart Junk](../Images/cbb010b950917bac215ccbe3b3b65540.png)'
  prefs: []
  type: TYPE_IMG
- en: Data visualization should be visually appealing but not at the expense of loading
    a chart with unnecessary junk, like in this extreme example on the right.
  prefs: []
  type: TYPE_NORMAL
- en: '**How do we make a good data visualization?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, choose the right type of chart for your data:'
  prefs: []
  type: TYPE_NORMAL
- en: Line Charts to track changes or trends over time and show the relationship between
    two or more variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bar Charts to compare quantities of different categories.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scatter Plots show joint variation of two data items.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pie Charts to compare parts of a whole - used them sparingly since people have
    hard time comparing the area of pie slices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can show additional variables on a 2-D plot using color, shape, and size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use interactive dashboards to allow experiments with key variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example of visualization of US Presidential Elections, 1976-2016,
    that shows multiple variables at once: the electoral college votes difference
    (y-axis), the % popular vote difference (X-axis), the size of the popular vote
    (circle area), winner party (color), and winner name and year (label). See my
    post on [What makes a good data visualization](/2017/03/what-makes-good-data-visualization.html)
    for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualization Us Elections 1976 2016](../Images/7ba5d10e741dd06143308b56263154f1.png)'
  prefs: []
  type: TYPE_IMG
- en: '**US Presidential Elections, 1976-2016**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs: []
  type: TYPE_NORMAL
- en: '[What makes a good visualization](http://www.informationisbeautiful.net/visualizations/what-makes-a-good-data-visualization/),
    David McCandless, Information is Beautiful'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Data Visualization Best Practices](https://www.gooddata.com/blog/5-data-visualization-best-practices),
    GoodData'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39 studies about human perception in 30 minutes](https://medium.com/@kennelliott/39-studies-about-human-perception-in-30-minutes-4728f9e31a73#.irdxe7jip),
    Kenn Elliott'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Visualization for Human Perception](https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/data-visualization-for-human-perception),
    landmark work by Stephen Few (key ideas summarized [here](https://viscomvibz.wordpress.com/2012/02/27/data-visualization-for-human-perception/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Q14\. What are some of the common data quality issues when dealing with Big
    Data? What can be done to avoid them or to mitigate their impact?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**[Anmol Rajpurohit](https://twitter.com/hey_anmol) answers:**'
  prefs: []
  type: TYPE_NORMAL
- en: The most common data quality issues observed when dealing with Big Data can
    be best understood in terms of the key characteristics of Big Data – Volume, Velocity,
    Variety, Veracity, and Value.
  prefs: []
  type: TYPE_NORMAL
- en: '**Volume:**'
  prefs: []
  type: TYPE_NORMAL
- en: In the traditional data warehouse environment, comprehensive data quality assessment
    and reporting was at least possible (if not, ideal). However, in the Big Data
    projects the scale of data makes it impossible. Thus, the data quality measurements
    can at best be approximations (i.e. need to be described in probability and confidence
    intervals, and not in terms of absolute values). We also need to re-define most
    of the data quality metrics based on the specific characteristics of the Big Data
    project so that those metrics can have a clear meaning, be measured (good approximation)
    and be used for evaluating the alternative strategies for data quality improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the great volume of underlying data, it is not uncommon to find out
    that some desired data was not captured or is not available for other reasons
    (such as high cost, delay in getting it, etc.). It is ironical but true that data
    availability continues to be a prominent data quality concern in the Big Data
    era.
  prefs: []
  type: TYPE_NORMAL
- en: '**Velocity:**'
  prefs: []
  type: TYPE_NORMAL
- en: The tremendous pace of data generation and collection makes it incredibly hard
    to monitor data quality within a reasonable overhead on time and resources (storage,
    compute, human effort, etc.). So, by the time data quality assessment completes,
    the output might be outdated and of little use, particularly if the Big Data project
    is to serve any real-time or near real-time business needs. In such scenarios,
    you would need to re-define data quality metrics so that they are relevant as
    well as feasible in the real-time context.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling can help you gain speed for the data quality efforts, but this comes
    at the cost of a bias (which eventually makes the end result less useful) because
    of the fact that samples are rarely an accurate representation of the entire data.
    Lesser samples will give higher speed, but with a bigger bias.
  prefs: []
  type: TYPE_NORMAL
- en: Another impact of velocity is that you might have to do data quality assessments
    on-the-fly, i.e. somewhere plugged-in within the data collection/transfer/storage
    processes; as the critical time-constraint does not give you the privilege of
    making a copy of a selected data subset, storing it elsewhere and running data
    quality assessments on it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Variety:**'
  prefs: []
  type: TYPE_NORMAL
- en: One of the biggest data quality issues in Big Data is that the data includes
    several data types (structured, semi-structured, and unstructured) coming in from
    different data sources. Thus, often a single data quality metric will not be applicable
    for the entire data and you would need to separately define data quality metrics
    for each data type. Moreover, assessing and improving the data quality of unstructured
    or semi-structured data is way more tricky and complex than that of structured
    data. For example, when mining the physician notes from medical records across
    the world (related to a particular medical condition) even if the language (and
    the grammar) is same the meaning might be very different due to local dialects
    and slang. This leads to low data interpretability, another data quality measure.
  prefs: []
  type: TYPE_NORMAL
- en: Data from different sources often has serious semantic differences. For example,
    “profit” can have widely varied definitions across the business units of an organization
    or external agencies. Thus, the fields with identical names may not mean the same
    thing. This problem is made worse by the lack of adequate and consistent meta-data
    from each data source. In order to make sense of data, you need reliable metadata
    (such as to make sense of sales numbers from a store, you need other information
    such as date-time, items purchased, coupons used, etc.). Usually, a lot of these
    data sources are outside an organization and thus, it is very hard to ensure good
    metadata for such data.
  prefs: []
  type: TYPE_NORMAL
- en: Another common issue is syntactic inconsistencies. For example, “time-stamp”
    values from different sources would be incompatible unless they are captured along
    with the time zone information.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16f3b109ac88dd3b1b05de9e4cb25598.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image source](http://informationcatalyst.com/index.php/vision-experience/big-data-value/).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Veracity:**'
  prefs: []
  type: TYPE_NORMAL
- en: Veracity, one of the most overlooked Big Data characteristics, is directly related
    to data quality, as it refers to the inherent biases, noise and abnormality in
    data. Because of veracity, the data values might not be exact real values, rather
    they might be approximations. In other words, the data might have some inherent
    impreciseness and uncertainty. Besides data inaccuracies, Veracity also includes
    data consistency (defined by the statistical reliability of data) and data trustworthiness
    (based on data origin, data collection and processing methods, security infrastructure,
    etc.). These data quality issues in turn impact data integrity and data accountability.
  prefs: []
  type: TYPE_NORMAL
- en: While the other V’s are relatively well-defined and can be easily measured,
    Veracity is a complex theoretical construct with no standard approach for measurement.
    In a way this reflects how complex the topic of “data quality” is within the Big
    Data context.
  prefs: []
  type: TYPE_NORMAL
- en: Data users and data providers are often different organizations with very different
    goals and operational procedures. Thus, it is no surprise that their notions of
    data quality are very different. In many cases, the data providers have no clue
    about the business use cases of data users (data providers might not even care
    about it, unless they are getting paid for the data). This disconnect between
    data source and data use is one of the prime reasons behind the data quality issues
    symbolized by Veracity.
  prefs: []
  type: TYPE_NORMAL
- en: '**Value:**'
  prefs: []
  type: TYPE_NORMAL
- en: The Value characteristic connects directly to the end purpose. Organizations
    are harnessing Big Data for many diverse business pursuits, and those pursuits
    are the real drivers of how data quality is defined, measured, and improved.
  prefs: []
  type: TYPE_NORMAL
- en: A common and old definition of data quality is that it is the “fitness of use”
    for the data consumer. This means that data quality is dependent on what you plan
    to do with the data. Thus, for a given data two different organizations with different
    business goals will most likely have widely different measurements of data quality.This
    nuance is often not well understood – data quality is a “relative” term. A Big
    Data project might involve incomplete and inconsistent data, however, it is possible
    that those data quality issues do not impact the utility of data towards the business
    goal. In such a case, the business would say that the data quality is great (and
    will not be interested in investing in data quality improvements). For example,
    for a producer of mashed potato cans a batch of small potatoes would be of same
    quality as a batch of big potatoes. However, for a fast food restaurant making
    fries, the quality of the two batches would be radically different.
  prefs: []
  type: TYPE_NORMAL
- en: The Value aspect also brings in the “cost-benefit” perspective to data quality
    – whether it would be worth to resolve a given data quality issue, which issues
    should be resolved on priority, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '**Putting it all together:**'
  prefs: []
  type: TYPE_NORMAL
- en: Data quality in Big Data projects is a very complex topic, where the theory
    and practice often differ. I haven’t come across any standard theory yet that
    is widely-accepted. Rather, I see little interest in the industry towards this
    goal.In practice, data quality does play an important role in the design of Big
    Data architecture. All the data quality efforts must start from a solid understanding
    of high-priority business use cases, and use that insight to navigate various
    trade-offs (samples given below) to optimize the quality of the final output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sample trade-offs related to data quality:'
  prefs: []
  type: TYPE_NORMAL
- en: Is it worth improving the timeliness of data at the expense of data completeness
    and/or inadequate assessment of accuracy?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should we select data for cleaning based on cost of cleaning effort or based
    on how frequently the data is used or based on its relative importance within
    the data models consuming it? Or, a combination of those factors? What sort of
    combination?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is it a good idea to improve data accuracy through getting rid of incomplete
    or erroneous data? While removing some data, how do we ensure that no bias is
    getting introduced?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the magnanimous scope of work and very limited resources (relatively!),
    one common way for data quality efforts on Big Data projects is to adopt the baseline
    approach, in which, the data users are surveyed to identify and document the bare
    minimum data quality needed to ensure that the business processes they support
    are not disrupted. These minimum satisfactory levels of data quality are referred
    to as the baseline, and the data quality efforts are focused on ensuring that
    data quality for each data does not fall beyond its baseline level. It looks like
    a good starting point and you may later move into more advanced endeavors (based
    on business needs and available budget).
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary of Recommendations to improve data quality in Big Data projects:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Identify and prioritize the business use cases** (then, use them to define
    data quality metrics, measurement methodology, improvement goals, etc.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on a strong understanding of the business use cases and the Big Data architecture
    implemented to achieve them, **design and implement an optimal layer of data governance**
    (data definitions, metadata requirements, data ownership, data flow diagrams,
    etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document baseline quality levels for key data** (think of “critical-path”
    diagram and “throughput-bottleneck” assessment)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Define ROI for data quality efforts** (in order to create feedback loop on
    the ROI metric to improve efficiency and to sustain funding for data quality efforts)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integrate data quality efforts** (to achieve efficiency through minimizing
    redundancy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automate data quality monitoring** (to reduce cost as well as to let employees
    stay focused on complex tasks)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Do not rely on machine learning to automatically take care of poor data quality**
    (machine learning is science and not magic!)'
  prefs: []
  type: TYPE_NORMAL
- en: 3 more interesting answers on the next page - read on ...
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[20 Questions (with Answers) to Detect Fake Data Scientists: ChatGPT…](https://www.kdnuggets.com/2023/01/20-questions-detect-fake-data-scientists-chatgpt-1.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20 Questions (with Answers) to Detect Fake Data Scientists: ChatGPT…](https://www.kdnuggets.com/2023/02/20-questions-detect-fake-data-scientists-chatgpt-2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Data Analytics Interview Questions & Answers](https://www.kdnuggets.com/2022/09/7-data-analytics-interview-questions-answers.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Python Interview Questions & Answers](https://www.kdnuggets.com/2022/09/5-python-interview-questions-answers.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
