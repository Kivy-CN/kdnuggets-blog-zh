- en: Recreating Fingerprints using Convolutional Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/03/recreating-fingerprints-using-convolutional-autoencoders.html](https://www.kdnuggets.com/2020/03/recreating-fingerprints-using-convolutional-autoencoders.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)![Figure](../Images/ad6146b9d7ada2b0c5689c5886abcfbd.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Source](https://www.pinterest.com/pin/151152131219099327/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Biometrics** is the technical term for body measurements and calculations.
    It refers to metrics related to human characteristics. Biometrics authentication
    (or realistic authentication) is used in computer science as a form of identification
    and access control. It is also used to identify individuals in groups that are
    under surveillance.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Biometric authentication systems are classified into two types such as Physiological
    Biometrics and Behavioral Biometrics. Physiological biometrics mainly include
    face recognition, fingerprint, hand geometry, iris recognition, and DNA. Whereas
    behavioral biometrics include keystroke, signature and voice recognition.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/ba1145d548e1bb164b5b028bf5e83358.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Types of Biometrics](https://www.researchgate.net/figure/Various-types-of-biometric-modalities_fig1_321082969)'
  prefs: []
  type: TYPE_NORMAL
- en: Fingerprints are the most reliable human characteristics that can be used for
    personal identification and has been widely used in biometric authentication systems
    due to its uniqueness and consistency. Fingerprint recognition systems play a
    crucial role in many situations where a person needs to be verified or identified
    with high confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the tremendous progress made in Automatic Fingerprint Identification
    Systems (AFIS), highly efficient and accurate fingerprint matching remains a critical
    challenge.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fingerprints: As Unique as You'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine you misplaced your smartphone and start panicking because there’s a
    lot of personal information on that phone. You’re worried because you don’t want
    whoever picks it up to be able to access it. But then you remember that you secured
    it so that no one could use it, should this scenario ever arise. The only person
    your phone will unlock for is you, and it knows it’s you because you use your
    fingerprint.
  prefs: []
  type: TYPE_NORMAL
- en: '**Fingerprints** and also toe prints can be used to identify a single individual
    because they are unique to each person and they do not change over time. Amazingly,
    even identical twins have fingerprints that are different from each other, and
    none of your fingers have the same print as the others. Fingerprints consist of **ridges**,
    which are the raised lines, and **furrows**, which are the valleys between those
    lines. And it’s the pattern of those ridges and furrows that are different for
    everyone.'
  prefs: []
  type: TYPE_NORMAL
- en: The patterns of the ridges are what is imprinted on a surface when your finger
    touches it. If you get fingerprinted the ridges are printed on the paper and can
    be used to match fingerprints you might leave elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: '**Human fingerprints are detailed, almost unique, difficult to modify, and
    durable over the life of an individual, making them suitable as long-term markers
    of human identity.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Characteristics of Fingerprints
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first encounter with fingerprints makes them look complicated. They may
    leave you wondering how forensic and law enforcement people make use of them.
    Fingerprints may look complicated, but the fact is that they have general ridge
    patterns and each person’s fingerprint is unique making it possible to systematically
    classify them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fingerprints have three basic ridge patterns: “Arch”, “Loop” and “Whorl/Core”.'
  prefs: []
  type: TYPE_NORMAL
- en: Arches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/72c6d8bb1c4095745b1a049319323184.png)'
  prefs: []
  type: TYPE_IMG
- en: In this pattern type, ridges enter on one side and exit on the other side. 5%
    of the total world’s population is believed to have arches in their fingerprints.
  prefs: []
  type: TYPE_NORMAL
- en: Loops
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/83378601656fb880fe89e6fc480f7f4f.png)'
  prefs: []
  type: TYPE_IMG
- en: This pattern type has ridges entering on one side and exiting on the same side.
    60–65% of the world’s population is believed to have loops in their fingerprints.
  prefs: []
  type: TYPE_NORMAL
- en: Whorls/Core
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/7236dd1728cc38e53814c3a7d255cca8.png)'
  prefs: []
  type: TYPE_IMG
- en: Consists of circles, more than one loop, or a mixture of pattern type. 30–35%
    of the world’s population is believed to have whorls in their fingerprints.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **uniqueness** of a fingerprint is exclusively determined by the local
    ridge characteristics and their relationships. The ridges and valleys in a fingerprint
    alternate, flowing in a local constant direction. The two most prominent local
    ridge characteristics are: 1) **ridge ending** and, 2) **ridge bifurcation**.
    A ridge ending is defined as the point where a ridge ends abruptly. A ridge bifurcation
    is defined as the point where a ridge forks or diverges into branch ridges. Collectively,
    these features are called ***minutiae.***'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/5f87e23f9fbee2f9f369d212006a26ef.png)  ![Figure](../Images/35324c16f4f1afeae027911f624f9376.png)'
  prefs: []
  type: TYPE_IMG
- en: Minutiae points
  prefs: []
  type: TYPE_NORMAL
- en: The set of minutiae points is considered to be the most distinctive feature
    for fingerprint representation and is widely used in fingerprint matching. It
    was believed that the minutiae set do not contain sufficient information to reconstruct
    the original fingerprint image from which minutiae were extracted. However, recent
    studies have shown that it is indeed possible to reconstruct fingerprint images
    from their minutiae representations.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Convolutional Autoencoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After having an overview of the fingerprint, its features, it is time to utilize
    our newly developed skill to build a Neural network that is capable of recreating
    or reconstructing fingerprint images.
  prefs: []
  type: TYPE_NORMAL
- en: So, first of all, we’ll explore the dataset including what kind of images it
    has, how to read the images, how to create an array of the images, exploring the
    fingerprint images and finally preprocessing them to be able to feed them in the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: I have used convolutional autoencoder for training the model. Next, we will
    visualize the training and validation loss plot and finally predict the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Here I’m assuming you guys are comfortable with Convolutional Neural Networks
    and AutoEncoders. Anyway, I’ll try to explain them as a one-liner.
  prefs: []
  type: TYPE_NORMAL
- en: A **Convolutional neural network** (CNN) is a neural network that has one or
    more convolutional layers and is used mainly for image processing, classification,
    and segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: '**OK. Whats an AutoEncoder?**'
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders are a family of Neural Networks for which the input is the same
    as the output. They work by compressing the input into a latent-space representationand
    then reconstructing the output from this representation. Check [this out for more.](http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/)
  prefs: []
  type: TYPE_NORMAL
- en: '**Now whats a Convolutional Autoencoders?**'
  prefs: []
  type: TYPE_NORMAL
- en: The convolution operator allows filtering an input signal in order to extract
    some part of its content. Autoencoders in their traditional formulation do not
    take into account the fact that a signal can be seen as a sum of other signals.
    Convolutional Autoencoders, instead, use the convolution operator to exploit this
    observation. They learn to encode the input in a set of simple signals and then
    try to reconstruct the input from them. For more [check this out.](https://pgaleone.eu/neural-networks/2016/11/24/convolutional-autoencoders/)
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/7fb5d9d2f85dcbb24378add7221a21d8.png)'
  prefs: []
  type: TYPE_IMG
- en: A convolution between a 4x4x1 input and a 3x3x1 convolutional filter.
  prefs: []
  type: TYPE_NORMAL
- en: The result is a 2x2x1 activation map. [Source](https://github.com/vdumoulin/conv_arithmetic)
  prefs: []
  type: TYPE_NORMAL
- en: The dataset that I’m using is the FVC2002 fingerprint dataset. It consists of
    4 different sensor fingerprints namely Low-cost Optical Sensor, Low-cost Capacitive
    Sensor, Optical Sensor and Synthetic Generator, each sensor having varying image
    sizes. The dataset has 320 images, 80 images per sensor.
  prefs: []
  type: TYPE_NORMAL
- en: '[Download dataset](http://bias.csr.unibo.it/fvc2002/databases.asp).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Load the required libraries:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Load the dataset:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now convert these images into float32 array.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Once you have the data loaded properly, you are all set to analyze it in order
    to get some intuition about the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Exploration:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: From the above output, you can see that the data has a shape of 320 x 224 x
    224 since there are 320 samples each of the 224 x 224-dimensional matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at a first 5 images in our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure](../Images/edb065cc813b0e104904e0f25f59daec.png)  ![Figure](../Images/4b1dfc787b85ba397749d714cd249533.png)  ![Figure](../Images/dc06d851be4a2a3468f881616a894705.png)![Figure](../Images/8c6146f550eff0a83de7e76e4e4a32b8.png)  ![Figure](../Images/046f442c10ea86db8dc9a3fe1140b5ff.png)'
  prefs: []
  type: TYPE_IMG
- en: Dataset
  prefs: []
  type: TYPE_NORMAL
- en: As we can see that the fingerprints are not very clear, it will be interesting
    to see if the convolutional autoencoder is able to learn the features and is able
    to reconstruct these images properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The images of the dataset are grayscale images with pixel values ranging from
    0 to 255 having a dimension of 224 x 224, so before we feed the data into the
    model, it is very important to preprocess it. We’ll first convert each 224 x 224
    image of the dataset into a matrix of size 224 x 224 x 1\. 1 for Grayscale image,
    which we can then feed into the Neural Network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Next, we want to make sure to check the data type of the NumPy array; it should
    be in **float32** format, if not you will need to convert it into this format,
    you also have to rescale the pixel values in range 0–1.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If we verify we should get `dtype('float32')`
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, rescale the data with the maximum pixel value of the images in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Let’s verify the maximum and minimum value of data which should be 0.0 and 1.0
    after rescaling it!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If we verify we should get `(1.0, 0.0)`
  prefs: []
  type: TYPE_NORMAL
- en: 'In order for your model to generalize well, you split the data into two parts:
    training and a validation set. You will train your model on 80% of the data and
    validate it on 20% of the remaining training data.'
  prefs: []
  type: TYPE_NORMAL
- en: This will also help you in reducing the chances of overfitting, as you will
    be validating your model on data it would not have seen in the training phase.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We don’t need training and testing labels that’s why we will pass the training
    images twice. Our training images will both act as the input as well as the ground
    truth similar to the labels you have in the classification task.
  prefs: []
  type: TYPE_NORMAL
- en: Now we are all set to define the network and feed the data into the network.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Convolutional Autoencoder**'
  prefs: []
  type: TYPE_NORMAL
- en: The images are of size 224 x 224 x 1 or a 50,176-dimensional vector. We convert
    the image matrix to an array, rescale it between 0 and 1, reshape it so that it’s
    of size 224 x 224 x 1, and feed this as an input to the network.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we will use a batch size of 128 using a higher batch size of 256 or 512
    is also preferable it all depends on the system you train your model. It contributes
    heavily in determining the learning parameters and affects the prediction accuracy.
    We will train your network for 300 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As you might already know well before, the autoencoder is divided into two
    parts: there are encoder and a decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Encoder**'
  prefs: []
  type: TYPE_NORMAL
- en: The first layer will have 32 filters of size 3 x 3, followed by a downsampling
    (max-pooling) layer,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second layer will have 64 filters of size 3 x 3, followed by another downsampling
    layer,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final layer of encoder will have 128 filters of size 3 x 3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoder**'
  prefs: []
  type: TYPE_NORMAL
- en: The first layer will have 128 filters of size 3 x 3 followed by an upsampling
    layer,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second layer will have 64 filters of size 3 x 3 followed by another upsampling
    layer,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final layer of encoder will have one filter of size 3 x 3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The max-pooling layer will downsample the input by two times each time you use
    it, while the upsampling layer will upsample the input by two times each time
    it is used.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**: The number of filters, the filter size, the number of layers, number
    of epochs you train your model, are all hyperparameters and should be decided
    based on your own intuition, you are free to try new experiments by tweaking with
    these hyperparameters and measure the performance of your model. And that is how
    you will slowly learn the art of deep learning!'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that you also have to specify the loss type via the argument loss. In
    this case, that’s the mean squared error, since the loss after every batch will
    be computed between the batch of predicted output and the ground truth using mean
    squared error pixel by pixel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Let’s visualize the layers that you created in the above step by using the summary
    function; this will show a number of parameters (weights and biases) in each layer
    and also the total parameters in your model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1b5aea710a7485aa3e01d758d7d777c6.png)'
  prefs: []
  type: TYPE_IMG
- en: It’s finally time to train the model with Keras’ fit() function! The model trains
    for 300 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Finally! You trained the model on the fingerprint dataset for 300 epochs, Now,
    let’s plot the loss plot between training and validation data to visualize the
    model performance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/785eb6e6086e93f40ab081ef92e782ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, you can see that the validation loss and the training loss both are
    in sync. It shows that your model is not overfitting: the validation loss is decreasing
    and not increasing,'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, you can say that your model’s generalization capability is good.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it’s time to reconstruct the test images using the predict() function
    of Keras and see how well your model is able to reconstruct the test data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Test images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d8dd22170276dd5a39cc63f16fc55298.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Recreated images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6500cc0bee861a3749f5130f9121ec6.png)'
  prefs: []
  type: TYPE_IMG
- en: From the above figures, you can observe that your model did a fantastic job
    of reconstructing the test images that you predicted using the model. At least
    visually, the test and the reconstructed images look almost similar.
  prefs: []
  type: TYPE_NORMAL
- en: You can also find the code in my Github.
  prefs: []
  type: TYPE_NORMAL
- en: '**[nageshsinghc4/Recreating-Fingerprints-using-Convolutional-Autoencoders](https://github.com/nageshsinghc4/Recreating-Fingerprints-using-Convolutional-Autoencoders)**'
  prefs: []
  type: TYPE_NORMAL
- en: Build a Neural network that is capable of recreating or reconstructing fingerprint
    images. The dataset that I'm using…
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It was never prevised that fingerprint science, which was used for catching
    criminals, would be used for unlocking mobile phones and authenticating payments.
    Phones instantly unlock when a finger registered with it, is put on the sensor,
    but it refuses to recognize the finger next to it, that’s when the uniqueness
    of fingerprints is practically felt.
  prefs: []
  type: TYPE_NORMAL
- en: Well, that’s all for this article hope you guys have enjoyed reading it and
    I’ll be glad if the article is of any help. Feel free to share your comments/thoughts/feedback
    in the comment section.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!!!
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Nagesh Singh Chauhan](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)**
    is a Big data developer at CirrusLabs. He has over 4 years of working experience
    in various sectors like Telecom, Analytics, Sales, Data Science having specialisation
    in various Big data components.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/recreating-fingerprints-using-convolutional-autoencoders-5c576e479d4f).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Neural Networks 201: All About Autoencoders](/2019/11/all-about-autoencoders.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stock Market Forecasting Using Time Series Analysis](/2020/01/stock-market-forecasting-time-series-analysis.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Exoplanet Hunting Using Machine Learning](/2020/01/exoplanet-hunting-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Image Classification with Convolutional Neural Networks (CNNs)](https://www.kdnuggets.com/2022/05/image-classification-convolutional-neural-networks-cnns.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Comprehensive Guide to Convolutional Neural Networks](https://www.kdnuggets.com/2023/06/comprehensive-guide-convolutional-neural-networks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a Convolutional Neural Network with PyTorch](https://www.kdnuggets.com/building-a-convolutional-neural-network-with-pytorch)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Automate Microsoft Excel and Word Using Python](https://www.kdnuggets.com/2021/08/automate-microsoft-excel-word-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Determine the Best Fitting Data Distribution Using Python](https://www.kdnuggets.com/2021/09/determine-best-fitting-data-distribution-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using Datawig, an AWS Deep Learning Library for Missing Value Imputation](https://www.kdnuggets.com/2021/12/datawig-aws-deep-learning-library-missing-value-imputation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
