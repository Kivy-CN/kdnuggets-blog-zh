- en: An Introduction to NLP and 5 Tips for Raising Your Game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/09/introduction-nlp-5-tips-raising-your-game.html](https://www.kdnuggets.com/2020/09/introduction-nlp-5-tips-raising-your-game.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: For those working around Data Science, Machine Learning and/or Artificial Intelligence,
    NLP is probably one of the most exciting fields to work in.
  prefs: []
  type: TYPE_NORMAL
- en: '*NLP stands for Natural Language Processing and it’s about the interactions
    between computers and human languages. Programming algorithms capable of processing
    and analyzing large amounts of natural language data.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The underlying objective may vary, but the overall goal is to get to conclusions
    about human behaviour…our intentions when writing something, what we were thinking
    or feeling when we do it, the category of an item we were writing about, and some
    other stuff like chatbots, market segmentation of customers, find duplicates and
    similarities in between elements, virtual assistants (like Siri or Alexa) and
    much more stuff.
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, NLP as a subject didn’t appear much time ago, it was just in 1960
    when Alan Turing published an article called “Computing Machinery and Intelligence”
    which proposed what is now called the ‘Turing test’. The paper introduced the
    question ‘Can machines think?’ and the test proves a machine’s ability to exhibit
    intelligent behaviour equivalent to, or indistinguishable from, that of a human.
    Three participants are necessary for running the test, where a player C, the evaluator,
    is given the task of trying to determine which player — A or B — is a computer
    and which is a human.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/12bd2a60b6b24c26c85617612e7633f4.png)'
  prefs: []
  type: TYPE_IMG
- en: The Japanese robot Pepper, made by Aldebaran Robotics
  prefs: []
  type: TYPE_NORMAL
- en: The evaluator would then judge natural language conversations between a human
    and a machine designed to generate human-like responses, knowing that one of the
    two partners in conversation is a machine. The conversation would be limited to
    a text-only channel and the results do not depend on the machine’s ability to
    give correct answers to questions, only how closely its answers resemble those
    a human would give. If at the end of the test the evaluator cannot reliably tell
    the machine from the human, the machine is said to have passed the test.
  prefs: []
  type: TYPE_NORMAL
- en: Starting from there, in the past years, the field has evolved exponentially,
    going from hand-coded systems using a set of rules, to a more sophisticated statistical
    NLP. And in this context, some companies are doing some pretty exciting stuff
    in the field. For example, if you’re an Android user you’re probably familiar
    with **Swiftkey**, a startup using text prediction designed to boost the accuracy,
    fluency and speed of users’ writing. Swiftkey learns from our writing, predicting
    favourite words, emojis and even expressions. Another startup,** SignAll**, converts
    sign language into text. Helping individuals who are deaf communicate with those
    who don’t know sign language.
  prefs: []
  type: TYPE_NORMAL
- en: And the fact is that nowadays the expansion of some open source libraries using
    Python, Tensorflow, Keras and others, has made NLP accessible and each day more
    and more businesses are using it. Some of them hiring other companies specifically
    specialized in the subject, but some others are hiring Data Scientists and Data
    Analyst in order to build their own solutions.
  prefs: []
  type: TYPE_NORMAL
- en: If any of these is your case, whether you are the company or the data specialist,
    in the next lines I will introduce some of my learning while working with NLP.
    Lucky for you, all of them are mistake-based tips! So hopefully, you will be able
    to avoid them in advance, not as it happened to me :)
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Find the right type of vectorization for you
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In NLP usually, after lots and lots and lots (and probably lots more) of data
    cleaning, the magic starts with something called vectorization. This tool, technique,
    or however you want to call it, take a bunch of text, usually called documents,
    and transforms them in vectors according to the words appearing within each document.
    Take the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/eba200a25aa4fce271629f3a64542c37.png)'
  prefs: []
  type: TYPE_IMG
- en: Example created by the author using images from [https://www.oreilly.com](https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html)
  prefs: []
  type: TYPE_NORMAL
- en: 'In the example above we are using a tool known as **Count Vectorizer or Bag
    of Words**. This kind of vectorization usually discards grammar, order, and structure
    in the text. It is a great option since it keeps track of all words appearing
    within the documents and their simple way of processing them by just counting
    it’s easily understandable and gives us a clear picture of the most important
    words overall. However, it presents two main problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data sparsity**: when counting all appearances throughout documents, we can
    easily end with a matrix composed of vectors full of zeros since of course, each
    document will only contain a small amount of all the possible words. We’ll talk
    more about this later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The future itself**: a Count Vectorizer outputs a fixed-sized matrix with
    all words (or those of certain frequencies) appearing in our current documents.
    This could be a problem if we receive further documents in the future and we don’t
    know the words we might find.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tricky documents**: what happens if we have a document in which a specific
    word appears so many times, that it ends looking as it is the most common word
    throughout all the documents instead of just a word appearing lots of time in
    just one document?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To solve the first problem and the second problem we could use a **Hashing Vectorize**,
    which converts a collection of text documents to a matrix of occurrences calculated
    with the [hashing trick](https://en.wikipedia.org/wiki/Feature_hashing). Each
    word is mapped to a feature with the use of a hash function that converts it to
    a number. If we encounter that word again in the text, it will be converted to
    the same hash, allowing us to count word occurrences without retaining a dictionary
    in memory. The main drawback of this trick is that it’s not possible to compute
    the inverse transform, and thus we lose information on what words the important
    features correspond to.
  prefs: []
  type: TYPE_NORMAL
- en: To solve the third problem mentioned above, we could use **term frequency-inverse
    document frequency (tf-idf) vectorizer**. A tf-idf score tells us which words
    are most discriminating between documents. Words that occur a lot in one document
    but don’t occur in many documents contain a great deal of discriminating power.
    The inverse document frequency is a measure of how much information the word provides,
    that is, whether the term is common or rare across all documents. Enhancing terms
    highly specific of a particular document while suppressing terms that are common
    to most documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sklearn has implementations for all these three types of vectorization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Countvectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hashingvectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Tfidfvectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2\. Personalize stop words and be aware of the language in your data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using stop words when doing any kind of vectorization is a key step for getting
    reliable results. Passing a list of stop words to our algorithm we’re telling
    to it: ‘please ignore all these words if you find any…I don’t want to have them
    in my output matrix’. Skelarn does include a default list of stop words for us
    to use, just by passing the word ‘english’ to the ‘stop_words’ hyperparameter.
    However, there are several limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: It only includes basic words like “and”, “the”, “him”, which are presumed to
    be uninformative in representing the content of a text and which may be removed
    to avoid them being construed as a signal for prediction. However, if for example,
    you were processing descriptions of houses scraped from rental agencies websites,
    you would probably want to remove all words that d not make to the description
    of the property itself. Words as ‘opportunity’, ‘offer’, ‘amazing’, ‘great’, ‘now’
    and stuff like that
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And what has been for me the greatest drawback being a Spanish speaker and
    working with machine learning problems in that language: It’s only available in
    English'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So whether you want to enrich the default list of words in English to improve
    your output matrix, or you want to use a list in some other language, you can
    pass Sklearn’s algorithm a personalized list of stop words by using the hyperparameter
    ‘stop_words’. By the way, [here’s a GitHub repository](https://github.com/Alir3z4/stop-words) with
    an impressive number of lists in several languages.
  prefs: []
  type: TYPE_NORMAL
- en: Before jumping into the next point, just have in mind that sometimes you won’t
    want to use any stop words at all. For example, if you’re dealing with numbers,
    even the default English list of stop words within Sklearn includes all single
    numbers from 0 to 9\. So it’s important for you to ask yourself whether or not
    you’re working on an NLP problem that needs stop words.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Use stemmer for ‘grouping’ similar words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Text normalization is the process of converting slightly different versions
    of words with essentially equivalent meaning into the same features. In some cases,
    it might be sensible to consider all possible variants of a possible word, but
    whether you’re working in English or any other language, sometimes you’ll also
    want to do some kind of pre-processing to your document in order to represent
    in the same way words that underly the same meaning. For example, consultant,
    consulting, consult, consultative and consultants could all be expressed as just
    ‘consultant’. See the next table for more examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/7c89acddcd3de9f9a3915d64adc25030.png)'
  prefs: []
  type: TYPE_IMG
- en: Source: [https://www.wolfram.com](https://www.wolfram.com/language/11/text-and-language-processing/generate-and-verify-stemmed-words.html) —
    Generate and verify stemmed words
  prefs: []
  type: TYPE_NORMAL
- en: 'For doing this, we could use stemming. Stemmers remove morphological affixes
    from words, leaving only the word stem. Luckily for us, [NLTK library for Python](https://www.nltk.org/api/nltk.stem.html) contains
    several robust stemmers. And if you want to incorporate your specific-language
    stemmer, or other, into your vectorizer algorithm, you can just use the following
    bunch of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can easily change this for using HashingVectorizer or TfidfVectorizer, just
    by changing the algorithm given to the class.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Avoid using Pandas DataFrames
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This piece of advice is short and sweet: if you’e working in an NLP project
    with any data larger than 5–10k thousand rows, avoid using DataFrames. Just vectorizing
    a big number of documents using Pandas returns a massive matrix that makes handling
    very slow, but also, lots of times, Natural Language Processing projects involve
    stuff like measuring distances, what tends to be very slow since it needs to compare
    elements against each other. And even though I’m myself a heavy user of Pandas’
    DataFrames, for this kind of stuff I would recommend using Numpy Arrays or Sparse
    Matrices.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also. mind that you can always get your sparse matrix to an array just by using
    the ‘.toarray()’ function and vice-versa, from array to sparse matrix using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'By the way, if your dealing with time issues, remember you can time your code
    using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '5\. Data sparsity: make your output matrix usable'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Image for post](../Images/47f4911c29c2175f0a67e7db20befb2c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As said before, one of the biggest problems while working with NLP is the issue
    of data sparsity…ending with matrices of dozens of thousands of columns full of
    zeros, that make it impossible for us to apply certain stuff afterwards. Here
    are a couple of tips of things I have used in the past for dealing with this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '**When using TfidfVectorizer or CountVectorizer using the hyperparameter ‘max_features’**.
    For example, you could print out the words frequencies across documents and then
    set a certain threshold for them. Imagine you have set a threshold of 50, and
    your data corpus consists of 100 words. After looking at the words frequencies
    20 words occur less than 50 times. Thus, you set max_features=80 and you are good
    to go. If max_features is set to None, then the whole corpus is considered during
    the transformation. Otherwise, if you pass, say, 5 to max_features, that would
    mean creating a feature matrix out of the most 5 frequent words across text documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Setting up a number of ‘n_features’ in HashingVectorizer.** This hyperparameter
    sets the number of features/columns in the output matrix. Small numbers of features
    are likely to cause hash collisions, but large numbers will cause larger coefficient
    dimensions in linear learners. The number is up to you and what you need.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Using dimensionality reduction**. Techniques as Principal Component Analysis
    take an output matrix with dozens of thousands of columns into a much smaller
    set capturing the variance on the original matrix could be a great idea. Just
    mind analyzing how much this dimensionality reduction affects your final results,
    to check if it’s actually useful and also to select the number of dimensions to
    be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I really really hope all these learnings I have had might help in your NLP project.
    More stories about NLP will come in the future, but if you enjoy this story don’t
    forget to check out some of my last articles, like [how to divide your data into
    train and test set assuring representativeness](https://medium.com/better-programming/how-to-divide-data-into-train-and-test-assuring-representativeness-c4c12c215d79?source=friends_link&sk=389e0ee2de25f5e76d31d8430e7ccc4b),
    s[urvivorship bias in Data Science](https://towardsdatascience.com/survivorship-bias-in-data-science-and-machine-learning-4581419b3bca?source=friends_link&sk=fd531decb8e07af98c506c4038c53bf9) and [using
    a cluster in the cloud for Data Science projects in 4 simple steps](https://medium.com/swlh/using-a-cluster-in-the-cloud-for-data-science-projects-in-4-simple-steps-9ee067238448?source=friends_link&sk=39832abc5b65eca87eb1089f934e87e1).
    All of them and more available within [my Medium profile](https://medium.com/@g.ferreiro.volpi).
  prefs: []
  type: TYPE_NORMAL
- en: And **if you want to receive my latest articles directly on your email, just **[**subscribe
    to my newsletter**](https://gmail.us3.list-manage.com/subscribe?u=8190cded0d5e26657d9bc54d7&id=3e942158a2)** :)**
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: 'And a special mention to the following sources I used throughout the story:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*https://blog.ventureradar.com/2015/09/22/top-5-companies-innovating-with-natural-language-processing/*](https://blog.ventureradar.com/2015/09/22/top-5-companies-innovating-with-natural-language-processing/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*https://en.wikipedia.org/wiki/Natural_language_processing*](https://en.wikipedia.org/wiki/Natural_language_processing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*https://en.wikipedia.org/wiki/Turing_test*](https://en.wikipedia.org/wiki/Turing_test)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*https://www.forbes.com/sites/bernardmarr/2019/06/03/5-amazing-examples-of-natural-language-processing-nlp-in-practice/#365b59901b30*](https://www.forbes.com/sites/bernardmarr/2019/06/03/5-amazing-examples-of-natural-language-processing-nlp-in-practice/#365b59901b30)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html*](https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*https://stackoverflow.com/questions/46118910/scikit-learn-vectorizer-max-features*](https://stackoverflow.com/questions/46118910/scikit-learn-vectorizer-max-features)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bio:** After 5+ years of experience in eCommerce and Marketing across multiple
    industries, **[Gonzalo Ferreiro Volpi](https://www.linkedin.com/in/gferreirovolpi/)**
    pivoted into the world of Data Science and Machine Learning, and currently works
    at Ravelin Technology using a combination of machine learning and human insights
    to tackle fraud in eCommerce.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/an-introduction-to-nlp-and-5-tips-for-raising-your-game-639636188ddf).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Accelerated Natural Language Processing: A Free Course From Amazon](/2020/08/accelerated-nlp-free-amazon-machine-learning-university.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4 Free Math Courses to do and Level up your Data Science Skills](/2020/06/4-free-maths-courses.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The NLP Model Forge: Generate Model Code On Demand](/2020/08/nlp-model-forge.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[5 Tips to Step Up Your Data Science Game Right Away](https://www.kdnuggets.com/5-tips-to-step-up-your-data-science-game-right-away)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Step up your Python game with Fast Python for Data Science!](https://www.kdnuggets.com/2022/06/manning-step-python-game-fast-python-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Future-Proof Your Data Game: Top Skills Every Data Scientist Needs in 2023](https://www.kdnuggets.com/futureproof-your-data-game-top-skills-every-data-scientist-needs-in-2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Transforming AI with LangChain: A Text Data Game Changer](https://www.kdnuggets.com/2023/08/transforming-ai-langchain-text-data-game-changer.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python f-Strings Magic: 5 Game-Changing Tricks Every Coder Needs to Know](https://www.kdnuggets.com/python-fstrings-magic-5-gamechanging-tricks-every-coder-needs-to-know)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kickstart Your NLP Journey with These 5 Free Courses](https://www.kdnuggets.com/kickstart-your-nlp-journey-with-these-5-free-courses)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
