# 如何在第一次Kaggle竞赛中排名前10%

> 原文：[https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2)

**异常值**

[![异常值示例](../Images/70eadff4905dbdee3a23ff64ef3219a8.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example_outlier.png)

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析能力

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织IT

* * *

图表显示了一些缩放坐标数据。我们可以看到在右上角有一些异常值。排除它们后，分布看起来很好。

**虚拟变量**

对于分类变量，常见的做法是**[独热编码](https://en.wikipedia.org/wiki/One-hot)**。对于一个有`n`种可能值的分类变量，我们创建一组`n`个虚拟变量。假设数据中的一条记录对这个变量的取值是某个值，则对应的虚拟变量被设置为`1`，而同组中的其他虚拟变量都设置为`0`。

[![虚拟变量示例](../Images/f7891877228b4993c0d084ec1c553387.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example-dummies.png)

在这个例子中，我们将`DayOfWeek`转换成7个虚拟变量。

注意，当分类变量可以取许多值（数百个或更多）时，这可能效果不好。很难找到一种通用的解决方案，但我会在下一节中讨论一个场景。

**特征工程**

有些人将Kaggle竞赛的本质描述为**特征工程补充模型调优和集成学习**。是的，这确实很有道理。**特征工程能让你走得很远。** 然而，决定你能走多远的是你对给定数据领域的了解程度。例如，在一个数据主要由文本组成的竞赛中，自然语言处理技术是必不可少的。构建有用特征的方法是我们必须不断学习以做得更好的。

基本上，**当你觉得一个变量对任务直观上有用时，你可以将其作为特征包括在内**。但你怎么知道它实际上有效？最简单的方法是像这样将其与目标变量绘制出来：

[![检查特征有效性](../Images/d9b85e951f68e16d26e5899549ae2247.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-vis_check_feat.png)

**特征选择**

一般来说，**我们应该尽可能多地创建特征，并相信模型能够挑选出最重要的特征**。然而，在此之前进行特征选择仍然有一些收获：

+   特征少意味着训练更快

+   一些特征与其他特征线性相关。这可能会给模型带来压力。

+   通过挑选最重要的特征，我们可以将它们之间的交互用作新特征。有时这会带来意想不到的改善。

检查特征重要性的最简单方法是拟合一个随机森林模型。还有更稳健的特征选择算法（例如，[这个](http://jmlr.org/papers/volume10/tuv09a/tuv09a.pdf)），虽然在理论上更优越，但由于缺乏高效实现而不具实用性。你可以通过增加随机森林中使用的树的数量来对抗噪声数据（在一定程度上）。

这在数据**[匿名化](https://en.wikipedia.org/wiki/Data_anonymization)**的竞赛中很重要，因为你不会浪费时间去弄清楚没有意义的变量的含义。

**特征编码**

有时原始特征必须转换为其他格式才能正常工作。

例如，假设我们有一个可以取超过10K不同值的分类变量。然后，简单地创建虚拟变量并不是一个可行的选择。一个可接受的解决方案是仅为值的一个子集（例如，占特征重要性95%的值）创建虚拟变量，其余的分配给‘其他’类别。

**更新于 2016年10月28日：** 对于上述场景，另一个可能的解决方案是使用**因子化机器**。有关详细信息，请参阅Kaggle用户“idle_speculation”的[这篇帖子](https://www.kaggle.com/c/expedia-hotel-recommendations/forums/t/21607/1st-place-solution-summary)。

**模型选择**

当特征设置好后，我们可以开始训练模型。Kaggle 比赛通常偏爱**基于树的模型**：

+   **梯度提升树**

+   随机森林

+   额外的随机树

以下模型在总体表现上略逊一筹，但作为集成学习中的基础模型是合适的（稍后会讨论）：

+   支持向量机

+   线性回归

+   逻辑回归

+   神经网络

请注意，这不适用于计算机视觉竞赛，这些竞赛几乎完全由神经网络模型主导。

所有这些模型都在**[Sklearn](http://scikit-learn.org/)**中实现。

在这里，我想强调**[Xgboost](https://github.com/dmlc/xgboost)**的伟大。梯度提升树的出色表现和Xgboost的高效实现使其在Kaggle比赛中非常受欢迎。如今，几乎每个赢家都以某种方式使用Xgboost。

**更新于 2016年10月28日：** 最近微软开源了**[LightGBM](https://github.com/Microsoft/LightGBM)**，这是一个可能比Xgboost更好的梯度提升库。

顺便说一下，对于Windows用户来说，安装Xgboost可能是一个痛苦的过程。如果你遇到问题，可以参考我写的[这篇文章](https://dnc1994.com/2016/03/installing-xgboost-on-windows/)。

### 相关话题更多内容

+   [LinkedIn如何利用机器学习来排名你的动态](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)

+   [如何在没有任何工作经验的情况下获得你的第一份数据科学工作](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)

+   [使用TensorFlow和Keras构建并训练你的第一个神经网络](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)

+   [用Python和一些便宜的组件制作你的第一个机器人！](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)

+   [从零到英雄：使用PyTorch创建你的第一个机器学习模型](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)

+   [部署你的第一个机器学习模型](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)
