- en: 'Decision Tree Pruning: The Hows and Whys'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树修剪：如何和为什么
- en: 原文：[https://www.kdnuggets.com/2022/09/decision-tree-pruning-hows-whys.html](https://www.kdnuggets.com/2022/09/decision-tree-pruning-hows-whys.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/09/decision-tree-pruning-hows-whys.html](https://www.kdnuggets.com/2022/09/decision-tree-pruning-hows-whys.html)
- en: '![Decision Tree Pruning: The Hows and Whys](../Images/ad5cf415445386976861e940e07248ed.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![决策树修剪：如何和为什么](../Images/ad5cf415445386976861e940e07248ed.png)'
- en: '[Devin H](https://unsplash.com/@devin_photography?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    via Unsplash'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Devin H](https://unsplash.com/@devin_photography?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    via Unsplash'
- en: Let’s recap Decision Trees.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下决策树。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - 提升您的数据分析水平'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - 支持您的组织IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Decision Trees are a non-parametric supervised learning method that can be used
    for classification and regression tasks. The goal is to build a model that can
    make predictions on the value of a target variable by learning simple decision
    rules inferred from the data features.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一种非参数的监督学习方法，可用于分类和回归任务。目标是通过学习从数据特征中推断出的简单决策规则，建立一个能够预测目标变量值的模型。
- en: Decision Trees are made up of
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树由以下部分组成
- en: Root Node - the very top of the decision tree and is the ultimate decision you’re
    trying to make.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根节点 - 决策树的最顶部，是你尝试做出的**最终决策**。
- en: Internal Nodes - this branches off from the Root Node, and represent different
    options
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内部节点 - 从根节点分支出来，表示不同的选项。
- en: Leaf Nodes - these are attached at the end of the branches and represent possible
    outcomes for each action.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 叶子节点 - 这些节点附加在分支的末端，表示每个动作的可能结果。
- en: Just like any other machine learning algorithm, the most annoying thing that
    can happen is overfitting. And Decision Trees are one of the machine learning
    algorithms that are susceptible to overfitting.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何其他机器学习算法一样，最烦人的事情就是过拟合。决策树就是容易过拟合的机器学习算法之一。
- en: Overfitting is when a model completely fits the training data and struggles
    or fails to generalize the testing data. This happens when the model memorizes
    noise in the training data and fails to pick up essential patterns which can help
    them with the test data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是指模型完全拟合训练数据，却难以或无法泛化测试数据。这发生在模型记住了训练数据中的噪声，而没有捕捉到帮助处理测试数据的重要模式。
- en: One of the techniques you can use to reduce overfitting in Decision Trees is
    Pruning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 降低决策树过拟合的技术之一是修剪。
- en: What is Decision Tree Pruning and Why is it Important?
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是决策树修剪，它为何重要？
- en: Pruning is a technique that removes the parts of the Decision Tree which prevent
    it from growing to its full depth. The parts that it removes from the tree are
    the parts that do not provide the power to classify instances. A Decision tree
    that is trained to its full depth will highly likely lead to overfitting the training
    data - therefore Pruning is important.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪是一种移除决策树中阻止其完全生长的部分的技术。它移除的部分是不提供分类实例能力的部分。一个完全生长的决策树很可能会导致过拟合训练数据，因此修剪是重要的。
- en: In simpler terms, the aim of Decision Tree Pruning is to construct an algorithm
    that will perform worse on training data but will generalize better on test data.
    Tuning the hyperparameters of your Decision Tree model can do your model a lot
    of justice and save you a lot of time and money.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，决策树修剪的目的是构建一个在训练数据上表现较差但在测试数据上泛化更好的算法。调整决策树模型的超参数可以大大改善模型表现，节省时间和金钱。
- en: How do you Prune a Decision Tree?
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何修剪决策树？
- en: 'There are two types of pruning: Pre-pruning and Post-pruning. I will go through
    both of them and how they work.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝有两种类型：预剪枝和后剪枝。我将介绍它们以及它们是如何工作的。
- en: Pre-pruning
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预剪枝
- en: The pre-pruning technique of Decision Trees is tuning the hyperparameters prior
    to the training pipeline. It involves the heuristic known as ‘early stopping’
    which stops the growth of the decision tree - preventing it from reaching its
    full depth.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的预剪枝技术是在训练管道之前调整超参数。它涉及到一种被称为‘早期停止’的启发式方法，该方法会停止决策树的生长 - 防止其达到完整深度。
- en: It stops the tree-building process to avoid producing leaves with small samples.
    During each stage of the splitting of the tree, the cross-validation error will
    be monitored. If the value of the error does not decrease anymore - then we stop
    the growth of the decision tree.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 它会停止树的构建过程，以避免生成样本量较小的叶子。在树的每个分裂阶段，将会监控交叉验证误差。如果误差值不再减少 - 那么我们就停止决策树的生长。
- en: 'The hyperparameters that can be tuned for early stopping and preventing overfitting
    are:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 可以调节的超参数以进行早期停止和防止过拟合是：
- en: '`max_depth`, `min_samples_leaf`, and `min_samples_split`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_depth`、`min_samples_leaf` 和 `min_samples_split`'
- en: These same parameters can also be used to tune to get a robust model. However,
    you should be cautious as early stopping can also lead to underfitting.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些相同的参数也可以用于调整以获得稳健模型。然而，你应该谨慎，因为早期停止也可能导致欠拟合。
- en: Post-pruning
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 后剪枝
- en: Post-pruning does the opposite of pre-pruning and allows the Decision Tree model
    to grow to its full depth. Once the model grows to its full depth, tree branches
    are removed to prevent the model from overfitting.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 后剪枝与预剪枝相反，允许决策树模型生长到其完整深度。一旦模型生长到其完整深度，树枝将被移除以防止模型过拟合。
- en: The algorithm will continue to partition data into smaller subsets until the
    final subsets produced are similar in terms of the outcome variable. The final
    subset of the tree will consist of only a few data points allowing the tree to
    have learned the data to the T. However, when a new data point is introduced that
    differs from the learned data - it may not get predicted well.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 算法将继续将数据划分为更小的子集，直到最终产生的子集在结果变量方面相似。树的最终子集将仅包含少量数据点，从而使树能够完全学习数据。然而，当引入与学习数据不同的新数据点时
    - 它可能无法得到良好的预测。
- en: 'The hyperparameter that can be tuned for post-pruning and preventing overfitting
    is: `ccp_alpha`'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 可用于后剪枝和防止过拟合的超参数是：`ccp_alpha`
- en: '`ccp` stands for Cost Complexity Pruning and can be used as another option
    to control the size of a tree. A higher value of `ccp_alpha` will lead to an increase
    in the number of nodes pruned.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`ccp` 代表成本复杂度剪枝，可以用作控制树大小的另一种选项。较高的`ccp_alpha`值将导致更多节点被修剪。'
- en: 'Cost complexity pruning (post-pruning) steps:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 成本复杂度剪枝（后剪枝）步骤：
- en: Train your Decision Tree model to its full depth
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将决策树模型训练到其完整深度
- en: Compute the ccp_alphas value using `cost_complexity_pruning_path()`
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`cost_complexity_pruning_path()`计算`ccp_alphas`值
- en: Train your Decision Tree model with different `ccp_alphas` values and compute
    train and test performance scores
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用不同的`ccp_alphas`值训练你的决策树模型，并计算训练和测试性能分数
- en: Plot the train and test scores for each value of `ccp_alphas` values.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制每个`ccp_alphas`值的训练和测试分数。
- en: This hyperparameter can also be used to tune to get the best fit models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个超参数也可以用于调整以获得最佳拟合模型。
- en: Tips
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示
- en: 'Here are some tips you can apply when Decision Tree Pruning:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些你可以在决策树剪枝时应用的提示：
- en: If the node gets very small, do not continue to split
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果节点变得非常小，则不要继续分裂
- en: Minimum error (cross-validation) pruning without early stopping is a good technique
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在没有早期停止的情况下，最小误差（交叉验证）剪枝是一种好的技术
- en: Build a full-depth tree and work backward by applying a statistical test during
    each stage
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个完整深度的树，然后通过在每个阶段应用统计检验来向后工作
- en: Prune an interior node and raise the sub-tree beneath it up one level
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修剪一个内部节点，并将其下方的子树上移一级
- en: Conclusion
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, I have gone over the two types of pruning techniques and their
    uses. Decision Trees are highly susceptible to overfitting - therefore pruning
    is a vital phase for the algorithm. If you would like to know more about post-pruning
    decision trees with cost complexity pruning, click on this [link](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#:~:text=Cost%20complexity%20pruning%20provides%20another,the%20number%20of%20nodes%20pruned.).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我已经详细讲解了两种修剪技术及其用途。决策树非常容易出现过拟合——因此修剪是算法中一个至关重要的阶段。如果你想了解更多关于使用成本复杂度修剪的后期修剪决策树的内容，请点击这个[链接](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#:~:text=Cost%20complexity%20pruning%20provides%20another,the%20number%20of%20nodes%20pruned/)。
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and Freelance Technical Writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)**是一名数据科学家和自由技术写作员。她特别关注于提供数据科学职业建议或教程以及数据科学的理论知识。她还希望探索人工智能如何或可以有益于人类寿命的不同方式。作为一个热衷学习者，她寻求拓宽自己的技术知识和写作技能，同时帮助指导他人。'
- en: More On This Topic
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Simplifying Decision Tree Interpretability with Python & Scikit-learn](https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用Python和Scikit-learn简化决策树可解释性](https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html)'
- en: '[Decision Tree Algorithm, Explained](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[决策树算法，解释](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)'
- en: '[Understanding by Implementing: Decision Tree](https://www.kdnuggets.com/2023/02/understanding-implementing-decision-tree.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过实现理解：决策树](https://www.kdnuggets.com/2023/02/understanding-implementing-decision-tree.html)'
- en: '[Telling a Great Data Story: A Visualization Decision Tree](https://www.kdnuggets.com/2021/02/telling-great-data-story-visualization-decision-tree.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[讲述一个伟大的数据故事：一个可视化决策树](https://www.kdnuggets.com/2021/02/telling-great-data-story-visualization-decision-tree.html)'
- en: '[Random Forest vs Decision Tree: Key Differences](https://www.kdnuggets.com/2022/02/random-forest-decision-tree-key-differences.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[随机森林与决策树：主要区别](https://www.kdnuggets.com/2022/02/random-forest-decision-tree-key-differences.html)'
- en: '[KDnuggets™ News 22:n09, Mar 2: Telling a Great Data Story: A…](https://www.kdnuggets.com/2022/n09.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets™ 新闻 22:n09, 3月2日: 讲述一个伟大的数据故事：A…](https://www.kdnuggets.com/2022/n09.html)'
