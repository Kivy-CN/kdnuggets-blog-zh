- en: Introduction to Local Interpretable Model-Agnostic Explanations (LIME)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《局部可解释模型无关解释（LIME）介绍》
- en: 原文：[https://www.kdnuggets.com/2016/08/introduction-local-interpretable-model-agnostic-explanations-lime.html](https://www.kdnuggets.com/2016/08/introduction-local-interpretable-model-agnostic-explanations-lime.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/08/introduction-local-interpretable-model-agnostic-explanations-lime.html](https://www.kdnuggets.com/2016/08/introduction-local-interpretable-model-agnostic-explanations-lime.html)
- en: '**By Marco Tulio Ribeiro*, Sameer Singh^, and Carlos Guestrin*.**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：Marco Tulio Ribeiro*、Sameer Singh^ 和 Carlos Guestrin*。**'
- en: '* University of Washington'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '* 华盛顿大学'
- en: ^ University of Califronia, Irvine
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ^ 加州大学欧文分校
- en: Machine learning is at the core of many recent advances in science and technology.
    With computers [beating professionals in games like Go](https://deepmind.com/alpha-go.html),
    many people have started asking if machines would also make for better [drivers](https://www.google.com/selfdrivingcar/)or
    even better doctors.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是许多近期科学和技术进步的核心。随着计算机在[围棋等游戏中战胜专业选手](https://deepmind.com/alpha-go.html)，许多人开始询问机器是否也会成为更好的[司机](https://www.google.com/selfdrivingcar/)甚至更好的医生。
- en: In many applications of machine learning, users are asked to trust a model to
    help them make decisions. A doctor will certainly not operate on a patient simply
    because “the model said so.” Even in lower-stakes situations, such as when choosing
    a movie to watch from Netflix, a certain measure of trust is required before we
    surrender hours of our time based on a model. Despite the fact that many machine
    learning models are black boxes, understanding the rationale behind the model's
    predictions would certainly help users decide when to trust or not to trust their
    predictions. An example is shown in Figure 1, in which a model predicts that a
    certain patient has the flu. The prediction is then explained by an "explainer"
    that highlights the symptoms that are most important to the model. With this information
    about the rationale behind the model, the doctor is now empowered to trust the
    model—or not.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多机器学习应用中，用户被要求信任模型来帮助他们做决策。医生肯定不会仅仅因为“模型说了”就对患者进行手术。即使在较低风险的情况下，例如从Netflix中选择一部电影观看，我们也需要一定程度的信任，然后才会根据模型的推荐花费几个小时的时间。尽管许多机器学习模型是黑箱模型，但了解模型预测背后的理由肯定会帮助用户决定是否信任这些预测。图1展示了一个例子，其中模型预测某位患者感染了流感。然后由一个“解释器”解释该预测，突出模型认为最重要的症状。通过了解模型背后的理由，医生现在可以决定是否信任模型——或不信任。
- en: '[![Explaining individual predictions to a human decision-maker](../Images/7dfadf0c6bcefc6b7f93ac09df8bee1e.png)](https://d3ansictanv2wj.cloudfront.net/figure1-a9533a3fb9bb9ace6ee96b4cdc9b6bcb.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[![向人类决策者解释个体预测](../Images/7dfadf0c6bcefc6b7f93ac09df8bee1e.png)](https://d3ansictanv2wj.cloudfront.net/figure1-a9533a3fb9bb9ace6ee96b4cdc9b6bcb.jpg)'
- en: '*Figure 1. Explaining individual predictions to a human decision-maker. Source:
    Marco Tulio Ribeiro.*In a sense, every time an engineer uploads a machine learning
    model to production, the engineer is implicitly trusting that the model will make
    sensible predictions. Such assessment is usually done by looking at held-out accuracy
    or some other aggregate measure. However, as anyone who has ever used machine
    learning in a real application can attest, such metrics can be very misleading.
    Sometimes data that shouldn''t be available accidentally leaks into the training
    and into the held-out data (e.g., looking into the future). Sometimes the model
    makes mistakes that are too embarrassing to be acceptable. These and many other
    tricky problems indicate that understanding the model''s predictions can be an
    additional useful tool when deciding if a model is trustworthy or not, because
    humans often have good intuition and business intelligence that is hard to capture
    in evaluation metrics. Assuming a “pick step” in which certain representative
    predictions are selected to be explained to the human would make the process similar
    to the one illustrated in Figure 2.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1.  向人类决策者解释个体预测。 来源：Marco Tulio Ribeiro。* 从某种意义上说，每次工程师将机器学习模型上传到生产环境时，工程师都在隐含地信任该模型会做出合理的预测。这种评估通常是通过查看持出准确性或其他汇总指标来完成的。然而，正如任何曾经在实际应用中使用过机器学习的人都可以证明的那样，这些指标可能非常具有误导性。有时不应可用的数据意外地泄漏到训练和持出数据中（例如，提前查看未来）。有时模型会犯下令人尴尬的错误，这些错误是不可接受的。这些以及许多其他棘手的问题表明，在决定模型是否值得信赖时，理解模型的预测可能是一个额外有用的工具，因为人类通常拥有难以在评估指标中捕捉到的良好直觉和商业智慧。假设一个“选择步骤”，在这个步骤中选择某些具有代表性的预测以向人类解释，会使过程类似于图
    2 所示的过程。'
- en: '[![Explaining a model to a human decision-maker](../Images/ca1507e7b92934a5cf58507d7727675a.png)](https://d3ansictanv2wj.cloudfront.net/figure2-802e0856e423b6bf8862843102243a8b.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[![向人类决策者解释模型](../Images/ca1507e7b92934a5cf58507d7727675a.png)](https://d3ansictanv2wj.cloudfront.net/figure2-802e0856e423b6bf8862843102243a8b.jpg)'
- en: '*Figure 2. Explaining a model to a human decision-maker. Source: Marco Tulio
    Ribeiro.*In ["Why Should I Trust You?" Explaining the Predictions of Any Classifier](http://arxiv.org/pdf/1602.04938.pdf),
    a joint work by [Marco Tulio Ribeiro](http://homes.cs.washington.edu/~marcotcr/), [Sameer
    Singh](http://sameersingh.org/), and [Carlos Guestrin](https://homes.cs.washington.edu/~guestrin/) (to
    appear in ACM''s [Conference on Knowledge Discovery and Data Mining](http://www.kdd.org/kdd2016/) -[-](http://www.kdd.org/kdd2016/)KDD2016),
    we explore precisely the question of trust and explanations. We propose Local
    Interpretable Model-Agnostic Explanations (LIME), a technique to explain the predictions
    of *any* machine learning classifier, and evaluate its usefulness in various tasks
    related to trust.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2. 向人类决策者解释模型。 来源：Marco Tulio Ribeiro。* 在[《我为什么应该信任你？解释任何分类器的预测》](http://arxiv.org/pdf/1602.04938.pdf)中，由[Marco
    Tulio Ribeiro](http://homes.cs.washington.edu/~marcotcr/)、[Sameer Singh](http://sameersingh.org/)和[Carlos
    Guestrin](https://homes.cs.washington.edu/~guestrin/)共同完成（即将在 ACM 的[知识发现与数据挖掘会议](http://www.kdd.org/kdd2016/)上发表），我们准确地探讨了信任和解释的问题。我们提出了局部可解释模型无关解释（LIME）技术，这是一种解释*任何*机器学习分类器预测的方法，并评估其在与信任相关的各种任务中的有用性。'
- en: Intuition behind LIME
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LIME 背后的直觉
- en: Because we want to be model-agnostic, what we can do to learn the behavior of
    the underlying model is to perturb the input and see how the predictions change.
    This turns out to be a benefit in terms of interpretability, because we can perturb
    the input by changing components that make sense to humans (e.g., words or parts
    of an image), even if the model is using much more complicated components as features
    (e.g., word embeddings).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们希望保持模型的独立性，所以我们可以通过扰动输入并观察预测结果的变化来学习基础模型的行为。这在可解释性方面是一种优势，因为我们可以通过改变对人类有意义的组件（例如，单词或图像的一部分）来扰动输入，即使模型使用的是更复杂的特征组件（例如，词嵌入）。
- en: We generate an explanation by approximating the underlying model by an interpretable
    one (such as a linear model with only a few non-zero coefficients), learned on
    perturbations of the original instance (e.g., removing words or hiding parts of
    the image). The key intuition behind LIME is that it is much easier to approximate
    a black-box model by a simple model *locally* (in the neighborhood of the prediction
    we want to explain), as opposed to trying to approximate a model globally. This
    is done by weighting the perturbed images by their similarity to the instance
    we want to explain. Going back to our example of a flu prediction, the three highlighted
    symptoms may be a faithful approximation of the black-box model for patients who
    look like the one being inspected, but they probably do not represent how the
    model behaves for all patients.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过用可解释的模型（例如，只有少数非零系数的线性模型）来逼近潜在模型，从而生成解释，该模型是在原始实例的扰动上学习的（例如，删除单词或隐藏图像的部分）。LIME的关键直觉在于，在我们要解释的预测的邻域内，用简单模型*局部*逼近黑箱模型比全局逼近要容易得多。这是通过根据扰动图像与我们要解释的实例的相似性来加权扰动图像来实现的。回到我们的流感预测示例，三种突出的症状可能是对黑箱模型的忠实近似，适用于与被检查的患者相似的患者，但它们可能并不代表模型对所有患者的行为。
- en: See Figure 3 for an example of how LIME works for image classification. Imagine
    we want to explain a classifier that predicts how likely it is for the image to
    contain a tree frog. We take the image on the left and divide it into interpretable
    components (contiguous superpixels).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见图3，了解LIME在图像分类中的工作原理。假设我们想解释一个分类器，该分类器预测图像中包含树蛙的可能性。我们取左侧的图像，并将其划分为可解释的组件（连续超像素）。
- en: '[![Transforming an image into interpretable components](../Images/36d4700080d6db03a84ea0fee05bd526.png)](https://d3ansictanv2wj.cloudfront.net/figure3-2cea505fe733a4713eeff3b90f696507.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[![将图像转化为可解释的组件](../Images/36d4700080d6db03a84ea0fee05bd526.png)](https://d3ansictanv2wj.cloudfront.net/figure3-2cea505fe733a4713eeff3b90f696507.jpg)'
- en: '*Figure 3. Transforming an image into interpretable components. Sources: Marco
    Tulio Ribeiro, [Pixabay](https://pixabay.com/en/love-valentine-s-day-pose-heart-903178/).*As
    illustrated in Figure 4, we then generate a data set of perturbed instances by
    turning some of the interpretable components “off” (in this case, making them
    gray). For each perturbed instance, we get the probability that a tree frog is
    in the image according to the model. We then learn a simple (linear) model on
    this data set, which is locally weighted—that is, we care more about making mistakes
    in perturbed instances that are more similar to the original image. In the end,
    we present the superpixels with highest positive weights as an explanation, graying
    out everything else.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3. 将图像转化为可解释的组件。来源：Marco Tulio Ribeiro，[Pixabay](https://pixabay.com/en/love-valentine-s-day-pose-heart-903178/).*如图4所示，我们接着通过将一些可解释的组件“关闭”（在这种情况下，将其变为灰色）来生成一个扰动实例的数据集。对于每个扰动实例，我们根据模型获得图像中是否有树蛙的概率。然后，我们在这个数据集上学习一个简单的（线性）模型，该模型是局部加权的——也就是说，我们更关心在与原始图像更相似的扰动实例中犯错。最终，我们将具有最高正权重的超像素作为解释，其他部分则变灰。'
- en: '[![Explaining a prediction with LIME](../Images/e59c638c0e76de599a57f43ef6a885d4.png)](https://d3ansictanv2wj.cloudfront.net/figure4-99d9ea184dd35876e0dbae81f6fce038.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[![使用LIME解释预测](../Images/e59c638c0e76de599a57f43ef6a885d4.png)](https://d3ansictanv2wj.cloudfront.net/figure4-99d9ea184dd35876e0dbae81f6fce038.jpg)'
- en: '*Figure 4. Explaining a prediction with LIME. Sources: Marco Tulio Ribeiro, [Pixabay](https://pixabay.com/en/love-valentine-s-day-pose-heart-903178/).*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4. 使用LIME解释预测。来源：Marco Tulio Ribeiro，[Pixabay](https://pixabay.com/en/love-valentine-s-day-pose-heart-903178/).*'
- en: Examples
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例
- en: We used LIME to explain a myriad of classifiers (such as [random forests](https://en.wikipedia.org/wiki/Random_forest),[support
    vector machines (SVM)](https://en.wikipedia.org/wiki/Support_vector_machine),
    and [neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network))
    in the text and image domains. Here are a few examples of the generated explanations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用LIME来解释各种分类器（例如，[随机森林](https://en.wikipedia.org/wiki/Random_forest)，[支持向量机(SVM)](https://en.wikipedia.org/wiki/Support_vector_machine)以及[神经网络](https://en.wikipedia.org/wiki/Artificial_neural_network)）在文本和图像领域的结果。以下是一些生成的解释示例。
- en: 'First, an example from text classification. The famous [20 newsgroups data
    set](http://qwone.com/~jason/20Newsgroups/) is a benchmark in the field, and has
    been used to compare different models in several papers. We take two classes that
    are hard to distinguish because they share many words: Christianity and atheism.
    Training a random forest with 500 trees, we get a test set accuracy of 92.4%,
    which is surprisingly high. If accuracy was our only measure of trust, we would
    definitely trust this classifier. However, let’s look at an explanation in Figure
    5 for an arbitrary instance in the test set (a one liner in Python with[our open
    source package](https://github.com/marcotcr/lime)):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，一个文本分类的例子。著名的[20个新闻组数据集](http://qwone.com/~jason/20Newsgroups/)是该领域的一个基准，已被用来比较几篇论文中的不同模型。我们选择了两个难以区分的类别，因为它们共享许多词汇：基督教和无神论。通过训练一个拥有500棵树的随机森林，我们得到一个92.4%的测试集准确率，这出乎意料地高。如果准确率是我们唯一的信任度衡量标准，我们肯定会信任这个分类器。然而，让我们看看图5中对测试集中任意实例的解释（使用[我们的开源包](https://github.com/marcotcr/lime)的一行Python代码）：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![Explanation for a prediction in the 20 newsgroups data set](../Images/1b8744c7b6325991e013951aaa09340b.png)](https://d3ansictanv2wj.cloudfront.net/figure5-cd7d3c5128549df1e957bf8f9f93bb2b.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[![20个新闻组数据集中的预测解释](../Images/1b8744c7b6325991e013951aaa09340b.png)](https://d3ansictanv2wj.cloudfront.net/figure5-cd7d3c5128549df1e957bf8f9f93bb2b.jpg)'
- en: '*Figure 5. Explanation for a prediction in the 20 newsgroups data set. Source:
    Marco Tulio Ribeiro.*This is a case in which the classifier predicts the instance
    correctly, but for the wrong reasons. Additional exploration shows us that the
    word "posting" (part of the email header) appears in 21.6% of the examples in
    the training set but only two times in the class “Christianity.” This is also
    the case in the test set, where the word appears in almost 20% of the examples
    but only twice in “Christianity.” This kind of artifact in the data set makes
    the problem much easier than it is in the real world, where we wouldn''t expect
    such patterns to occur. These insights become easy once you understand what the
    models are actually doing, which in turn leads to models that generalize much
    better.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5. 20个新闻组数据集中的预测解释。来源：Marco Tulio Ribeiro.*这是一个分类器正确预测实例但理由错误的案例。进一步探讨表明，单词“posting”（电子邮件头部的一部分）在训练集中的21.6%的示例中出现，但在“基督教”类别中仅出现两次。在测试集中也是如此，单词几乎出现在20%的示例中，但在“基督教”中仅出现两次。这种数据集中的伪影使得问题比实际情况简单得多，因为我们不会预期出现这样的模式。一旦理解了模型实际做了什么，这些见解就变得容易，这反过来又导致模型具有更好的泛化能力。'
- en: 'As a second example, we explain [Google''s Inception neural network](https://github.com/google/inception) on
    arbitrary images. In this case, illustrated in Figure 6, the classifier predicts
    “tree frog” as the most likely class, followed by "pool table" and "balloon" with
    lower probabilities. The explanation reveals that the classifier primarily focuses
    on the frog''s face as an explanation for the predicted class. It also sheds light
    on why "pool table" has non-zero probability: the frog''s hands and eyes bear
    a resemblance to billiard balls, especially on a green background. Similarly,
    the heart bears a resemblance to a red balloon.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第二个例子，我们解释了[Google的Inception神经网络](https://github.com/google/inception)对任意图像的处理。在这种情况下，如图6所示，分类器预测“树蛙”是最可能的类别，其次是“台球桌”和“气球”，后者的概率较低。解释显示，分类器主要关注青蛙的面部作为预测类别的依据。它还揭示了为什么“台球桌”有非零概率：青蛙的手和眼睛与台球有相似之处，尤其是在绿色背景下。同样，心脏与红色气球也有相似之处。
- en: '[![Explanation for a prediction from Inception](../Images/1d97052096ab8d53d15988f05e4a327e.png)](https://d3ansictanv2wj.cloudfront.net/Figure-6-c8db425eefec7cff5a3cf035a40d8841.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[![Inception的预测解释](../Images/1d97052096ab8d53d15988f05e4a327e.png)](https://d3ansictanv2wj.cloudfront.net/Figure-6-c8db425eefec7cff5a3cf035a40d8841.jpg)'
- en: '*Figure 6. Explanation for a prediction from Inception. The top three predicted
    classes are "tree frog," "pool table," and "balloon." Sources: Marco Tulio Ribeiro,
    Pixabay ([frog](https://pixabay.com/en/love-valentine-s-day-pose-heart-903178/), [billiards](https://pixabay.com/en/billiards-dutch-colors-pool-cue-1263076/), [hot
    air balloon](https://pixabay.com/en/hot-air-balloon-balloon-1378998/)).*In the
    experiments in [our research paper](http://arxiv.org/abs/1602.04938), we demonstrate
    that both machine learning experts and lay users greatly benefit from explanations
    similar to Figures 5 and 6 and are able to choose which models generalize better,
    improve models by changing them, and get crucial insights into the models'' behavior.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6. 对Inception预测的解释。前三个预测类别是“树蛙”、“台球桌”和“气球”。来源：马尔科·图利奥·里贝罗，Pixabay（[青蛙](https://pixabay.com/en/love-valentine-s-day-pose-heart-903178/)，[台球](https://pixabay.com/en/billiards-dutch-colors-pool-cue-1263076/)，[热气球](https://pixabay.com/en/hot-air-balloon-balloon-1378998/)）。*在[我们的研究论文](http://arxiv.org/abs/1602.04938)中，我们展示了机器学习专家和普通用户如何从类似于图5和图6的解释中受益，并能够选择哪些模型具有更好的泛化能力，通过改变模型来改进模型，并获得对模型行为的关键见解。'
- en: Conclusion
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: Trust is crucial for effective human interaction with machine learning systems,
    and we think explaining individual predictions is an effective way of assessing
    trust. LIME is an efficient tool to facilitate such trust for machine learning
    practitioners and a good choice to add to their tool belts (did we mention we
    have [an open source project](https://github.com/marcotcr/lime)?), but there is
    still plenty of work to be done to better explain machine learning models. We're
    excited to see where this research direction will lead us. The video below provides
    an overview of LIME, with more details available in [our paper](http://arxiv.org/abs/1602.04938).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 信任对有效的人机互动至关重要，我们认为解释个体预测是评估信任的一种有效方式。LIME是一个高效的工具，能够为机器学习从业者提供这种信任，并且是添加到他们工具箱中的不错选择（我们提到过我们有[一个开源项目](https://github.com/marcotcr/lime)吗？），但仍然需要大量工作来更好地解释机器学习模型。我们对这个研究方向的前景感到兴奋。下面的视频提供了LIME的概述，更多细节请见[我们的论文](http://arxiv.org/abs/1602.04938)。
- en: '**[Marco Tulio Ribeiro](https://homes.cs.washington.edu/~marcotcr/)** is a
    PhD student at the University of Washington working under Carlos Guestrin. His
    research focus is making it easier for humans to understand and interact with
    machine learning models.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**[马尔科·图利奥·里贝罗](https://homes.cs.washington.edu/~marcotcr/)** 是华盛顿大学的博士生，师从卡洛斯·格斯特林。他的研究重点是让人们更容易理解和互动机器学习模型。'
- en: '**[Carlos Guestrin](https://twitter.com/guestrin)** is the CEO of Turi, Inc.,
    and the Amazon Professor of Machine Learning in Computer Science & Engineering
    at the University of Washington. A world-recognized leader in the field of Machine
    Learning, Carlos was named one of the 2008 “Brilliant 10" by Popular Science Magazine,
    received the 2009 IJCAI Computers and Thought Award for his contributions to Artificial
    Intelligence, and a Presidential Early Career Award for Scientists and Engineers
    (PECASE).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**[卡洛斯·格斯特林](https://twitter.com/guestrin)** 是Turi, Inc.的首席执行官，同时也是华盛顿大学计算机科学与工程系的亚马逊机器学习教授。作为机器学习领域公认的世界领袖，卡洛斯曾被《流行科学》杂志评为2008年“杰出10人”之一，因其对人工智能的贡献获得了2009年IJCAI计算机与思想奖，并获得了总统早期职业科学家和工程师奖（PECASE）。'
- en: '**[Dr. Sameer Singh](http://sameersingh.org/)** is an Assistant Professor of
    Computer Science at University of California, Irvine, conducting research on large-scale,
    interactive machine learning and natural language processing.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**[萨米尔·辛格博士](http://sameersingh.org/)** 是加州大学欧文分校计算机科学助理教授，从事大规模互动机器学习和自然语言处理的研究。'
- en: '[Original](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime).
    Reposted with permission.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime)。经许可转载。'
- en: '**Related:**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Contest Winner: Winning the AutoML Challenge with Auto-sklearn](/2016/08/winning-automl-challenge-auto-sklearn.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[比赛获胜者：使用Auto-sklearn赢得AutoML挑战赛](/2016/08/winning-automl-challenge-auto-sklearn.html)'
- en: '[Approaching (Almost) Any Machine Learning Problem](/2016/08/approaching-almost-any-machine-learning-problem.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[接近（几乎）任何机器学习问题](/2016/08/approaching-almost-any-machine-learning-problem.html)'
- en: '[TPOT: A Python Tool for Automating Data Science](/2016/05/tpot-python-automating-data-science.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TPOT：一个用于自动化数据科学的Python工具](/2016/05/tpot-python-automating-data-science.html)'
- en: '* * *'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的 IT 工作'
- en: '* * *'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Explain NLP Models with LIME](https://www.kdnuggets.com/2022/01/explain-nlp-models-lime.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 LIME 解释 NLP 模型](https://www.kdnuggets.com/2022/01/explain-nlp-models-lime.html)'
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 PyTorch 解释神经网络](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
- en: '[GPT4All is the Local ChatGPT for your Documents and it is Free!](https://www.kdnuggets.com/2023/06/gpt4all-local-chatgpt-documents-free.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT4All 是您文档的本地 ChatGPT，并且是免费的！](https://www.kdnuggets.com/2023/06/gpt4all-local-chatgpt-documents-free.html)'
- en: '[LangChain + Streamlit + Llama: Bringing Conversational AI to Your…](https://www.kdnuggets.com/2023/08/langchain-streamlit-llama-bringing-conversational-ai-local-machine.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LangChain + Streamlit + Llama: 将对话 AI 带到您的本地计算机…](https://www.kdnuggets.com/2023/08/langchain-streamlit-llama-bringing-conversational-ai-local-machine.html)'
- en: '[Octoparse 8.5: Empowering Local Scraping and More](https://www.kdnuggets.com/2022/02/octoparse-85-empowering-local-scraping.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Octoparse 8.5: 授权本地抓取及更多](https://www.kdnuggets.com/2022/02/octoparse-85-empowering-local-scraping.html)'
- en: '[7 Steps to Running a Small Language Model on a Local CPU](https://www.kdnuggets.com/7-steps-to-running-a-small-language-model-on-a-local-cpu)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在本地 CPU 上运行小型语言模型的 7 个步骤](https://www.kdnuggets.com/7-steps-to-running-a-small-language-model-on-a-local-cpu)'
