- en: Top 10 Must-Know Machine Learning Algorithms for Data Scientists – Part 1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/04/top-10-must-know-machine-learning-algorithms-data-scientists-1.html](https://www.kdnuggets.com/2021/04/top-10-must-know-machine-learning-algorithms-data-scientists-1.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)![Simple ML](../Images/a5441b69289f9fd2607606e638f57b99.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: [A Comprehensive Survey on Graph Neural Networks](https://arxiv.org/abs/1901.00596v4)'
  prefs: []
  type: TYPE_NORMAL
- en: Wading through the vast array of information for data science newcomers on [machine
    learning algorithms](https://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html)
    can be a difficult and time-consuming process. Figuring out which algorithms are
    widely used and which are simply novel or interesting is not just an academic
    exercise; determining where to concentrate your time and focus during the early
    days of study can determine the difference between getting your career off to
    a quick start and experiencing an extended ground delay.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: How, exactly, does one discriminate between immediately useful algorithms worthy
    of attention and, well, not so much? Determining how to come up with an objective
    list of machine learning algorithms of authority is inherently difficult, but
    it seems that going directly to practitioners for their feedback may be the optimal
    approach. Such a process presents a whole host of difficulties of its own, as
    could be easily imagined, and results of such surveys are few and far between.
  prefs: []
  type: TYPE_NORMAL
- en: KDnuggets, however, [conducted such a survey](https://www.kdnuggets.com/2019/04/top-data-science-machine-learning-methods-2018-2019.html)
    within the last few years, asking respondents "Which Data Science / Machine Learning
    methods and algorithms did you use in 2018/2019 for a real-world application?"
    Of course, as alluded to above, such surveys are hindered by self-selection, lack
    of verifiability of participants, trust in the actual responses, etc., but this
    survey represents the most recent, most far-reaching, and ultimately the best
    source we have available to us.
  prefs: []
  type: TYPE_NORMAL
- en: And, so, this is the source we will use to identify the top 10 machine learning
    algorithms being used, and, as such, the top 10 must-know algorithms for data
    scientists. The first 5 of these top 10 must-know algorithms are introduced below,
    with a brief overview of what the algorithms are and how they work. We will followup
    with part 2 in the coming weeks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: we have skipped over entries which do not map to machine learning algorithms
    (e.g. "visualization", "descriptive statistics", "text analytics")
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we have treated the entry "regression" separately as both "linear regression"
    and "logistic regression"
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we have swapped the entry "ensemble methods" for "bagging", a particular ensemble
    method, as a few other ensemble methods are also individually represented in this
    list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we have skipped over any neural network entries, as these techniques combine
    architecture with a variety of different algorithms to achieve their goals, aspects
    which are beyond the scope of this discussion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1\. Linear Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regression is a time-tested manner for approximating relationships among a given
    collection of data, and the recipient of unhelpful naming via [unfortunate circumstances](https://en.wikipedia.org/wiki/Regression_analysis#History).
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression is a simple algebraic tool which attempts to find the “best”
    (straight, for the purposes of this discussion) line fitting 2 or more attributes,
    with one attribute (**simple** linear regression), or a combination of several
    (**multiple** linear regression), being used to predict another, the class attribute.
    A set of training instances is used to compute the linear model, with one attribute,
    or a set of attributes, being plotted against another. The model then attempts
    to identify where new instances would lie on the regression line, given a particular
    class attribute.
  prefs: []
  type: TYPE_NORMAL
- en: 'The relationship between predictor and response variables (*x* and *y*, respectively)
    can be expressed via the following equation, with which everyone reading this
    is undoubtedly familiar: ![Equation](../Images/ef82f76aa240383ae202e5777644a284.png).'
  prefs: []
  type: TYPE_NORMAL
- en: '*m* and *b* are the regression coefficients, and denote line slope and y-intercept,
    respectively. I suggest you check your elementary school algebra notes if you
    are having trouble recalling :)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation for multiple linear regression is generalized for *n* attributes
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/2228a2c2a643f05fb98f4fdefcbe888c.png)'
  prefs: []
  type: TYPE_IMG
- en: 2\. Decision Trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In machine learning, decision trees have been used for decades as effective
    and easily understandable data classifiers (contrast that with the numerous blackbox
    classifiers in existence). Over the years, a number of decision tree algorithms
    have resulted from research, 3 of the most important, influential, and well-used
    being:'
  prefs: []
  type: TYPE_NORMAL
- en: Iterative Dichotimiser 3 (ID3) - Ross Quinlan's precursor to the C4.5
  prefs: []
  type: TYPE_NORMAL
- en: Iterative Dichotimiser 3 (ID3) - Ross Quinlan's precursor to the C4.5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C4.5 - one of the most popular classifiers of all time, also from Quinlan
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CART - independently invented around the same time as C4.5, also still very
    popular
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ID3, C4.5, and CART all adopt a top-down, recursive, divide-and-conquer approach
    to decision tree induction.
  prefs: []
  type: TYPE_NORMAL
- en: Over the years, C4.5 has become a benchmark against which the performance of
    newer classification algorithms are often measured. Quinlan’s original implementation
    contains proprietary code; however, various open-source versions have been coded
    over the years, including the (at one time very popular) Weka Machine Learning
    Toolkit's J48 decision tree algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model- (or tree-) building aspect of decision tree classification algorithms
    are composed of 2 main tasks: tree induction and tree pruning. **Tree induction**
    is the task of taking a set of pre-classified instances as input, deciding which
    attributes are best to split on, splitting the dataset, and recursing on the resulting
    split datasets until all training instances are categorized. **Tree pruning**
    involves reducing the size of decision trees by eliminating branches which are
    redundant or non-essential to the classification process.'
  prefs: []
  type: TYPE_NORMAL
- en: While building our tree, the goal is to split on the attributes which create
    the purest child nodes possible, which would keep to a minimum the number of splits
    that would need to be made in order to classify all instances in our dataset.
    This purity is generally measured by one of a number of different attribute selection
    measures.
  prefs: []
  type: TYPE_NORMAL
- en: There are 3 prominent attribute selection measures for decision tree induction,
    each paired with one of the 3 prominent decision tree classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Information gain - used in the ID3 algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gain ratio - used in the C4.5 algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gini index - used in the CART algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In ID3, purity is measured by the concept of information gain, based on the
    work of Claude Shannon, which relates to how much would need to be known about
    a previously-unseen instance in order for it to be properly classified. In practice,
    this is measured by comparing entropy, or the amount of information needed to
    classify a single instance of a current dataset partition, to the amount of information
    to classify a single instance if the current dataset partition were to be further
    partitioned on a given attribute.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important takeaways from this discussion should be that decision
    tree is a classification strategy as opposed to some single, well-defined classification
    algorithm. While we have had a brief look at 3 separate decision tree algorithm
    implementations, there are a variety of ways to configure different aspects of
    each. Indeed, any algorithm which seeks to classify data, and takes a top-down,
    recursive, divide-and-conquer approach to crafting a tree-based graph for subsequent
    instance classification, regardless of any other particulars (including attribution
    split selection methods and optional tree-pruning approach) would be considered
    a decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. *k*-means Clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*k*-means is a simple, yet often effective, approach to clustering. Traditionally,
    *k* data points from a given dataset are randomly chosen as cluster centers, or
    centroids, and all training instances are plotted and added to the closest cluster.
    After all instances have been added to clusters, the centroids, representing the
    mean of the instances of each cluster are re-calculated, with these re-calculated
    centroids becoming the new centers of their respective clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, all cluster membership is reset, and all instances of the training
    set are re-plotted and re-added to their closest, possibly re-centered, cluster.
    This iterative process continues until there is no change to the centroids or
    their membership, and the clusters are considered settled.
  prefs: []
  type: TYPE_NORMAL
- en: 'Convergence is achieved once the re-calculated centroids match the previous
    iteration’s centroids, or are within some preset margin. The measure of distance
    is generally Euclidean in *k*-means, which, given 2 points in the form of (*x,
    y*), can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/6caecd241af5ba356579714b862cb7ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Of technical note, especially in the era of parallel computing, iterative clustering
    in *k*-means is serial in nature; however, the distance calculations within an
    iteration need not be. Therefore, for sets of a significant size, distance calculations
    are a worthy target for parallelization in the *k*-means clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Also, while we have just described a particular method of clustering using cluster
    mean values and Euclidean distance, it should not be difficult to imagine a variety
    of possible other methods using, say, cluster median values or any number of other
    distance metrics (cosine, Manhattan, Chebyshev, etc.)
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Bagging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a given scenario, it may prove more useful than not to to chain or group
    classifiers together, using the techniques of voting, weighting, or combination
    to pursue the most accurate classifier possible. Ensemble learners are classifiers
    which provide this functionality in a variety of ways. Bagging is an example of
    an ensemble learner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bagging operates by simple concept: build a number of models, observe the results
    of these models, and settle on the majority result. I recently had an issue with
    the rear axle assembly in my car: I wasn''t sold on the diagnosis of the dealership,
    and so I took it to 2 other garages, both of which agreed the issue was something
    different than the dealership suggested. Voila. Bagging in action.'
  prefs: []
  type: TYPE_NORMAL
- en: I only visited 3 garages in my example, but you could imagine that accuracy
    would likely increase if I had visited tens or hundreds of garages, especially
    if my car's problem was one of a more complex nature. This holds true for bagging,
    and the bagged classifier often is significantly more accurate than single constituent
    classifiers. Also note that the type of constituent classifier used are inconsequential;
    the resulting model can be made up of any single classifier type.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging is short for *bootstrap aggregation*, so named because it takes a number
    of samples from the dataset, with each sample set being regarded as a bootstrap
    sample. The results of these bootstrap samples are then aggregated.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Support Vector Machines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned, Support Vector Machines (SVMs) are a particular classification
    strategy. SMVs work by transforming the training dataset into a higher dimension,
    which is then inspected for the optimal separation boundary, or boundaries, between
    classes. In SVMs, these boundaries are referred to as hyperplanes, which are identified
    by locating support vectors, or the instances that most essentially define classes,
    and their margins, which are the lines parallel to the hyperplane defined by the
    shortest distance between a hyperplane and its support vectors. Consequently,
    SVMs are able to classify both linear and nonlinear data.
  prefs: []
  type: TYPE_NORMAL
- en: The grand idea with SVMs is that, with a high enough number of dimensions, a
    hyperplane separating a particular class from all others can always be found,
    thereby delineating dataset member classes. When repeated a sufficient number
    of times, enough hyperplanes can be generated to separate all classes in *n*-dimensional
    space. Importantly, SVMs look not just for any separating hyperplane but the maximum-margin
    hyperplane, being that which resides equidistance from respective class support
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Central to SVMs is the kernel trick, which enables comparison between original
    data and potential higher dimensionality feature space transformations of this
    data in order to determine if such transformation facilitates the separation of
    data classes, which would put us in a position where hyperplanes could separate
    classes. The kernel trick is essential as it can make potentially otherwise intractable
    transformation computations feasible, with these comparisons of original and high-dimensional
    feature space date enabled by pairwise similarity comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: When data is linearly-separable, there are many separating lines that could
    be chosen. Such a hyperplane can be expressed as
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/e8afe37e7dc375b373dea975c6278eba.png)'
  prefs: []
  type: TYPE_IMG
- en: where *W* is a vector of weights, b is a scalar bias, and X are the training
    data (of the form (*x[1], x[2]*, ...)). If our bias, *b*, is thought of as an
    additional weight, the equation can be expressed as
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/a94de6f6481000d3f547e2a04a772acb.png)'
  prefs: []
  type: TYPE_IMG
- en: which can, in turn, be rewritten as a pair of linear inequalities, solving to
    greater or less than zero, either of which satisfied indicate that a particular
    point lies above or below the hyperplane, respectively. Finding the maximum-margin
    hyperplane, or the hyperplane that resides equidistance from the support vectors,
    is done by combining the linear inequalities into a single equation and transforming
    them into a constrained quadratic optimization problem, using a Lagrangian formulation
    and solving using Karush-Kuhn-Tucker conditions, which is beyond what we will
    discuss here.
  prefs: []
  type: TYPE_NORMAL
- en: So there you have 5 must-know algorithms for data scientists, with *must-know*
    being defined as the most utilized in practice, and the most utilized informed
    by the results of a recent KDnuggets survey. We will followup with the second
    part of this list in the coming weeks. Until then, we hope you find this list
    and simple explanations helpful, and that you are able to branch out to learn
    more about each of these form here.
  prefs: []
  type: TYPE_NORMAL
- en: '**Related**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[How to Explain Key Machine Learning Algorithms at an Interview](/2020/10/explain-machine-learning-algorithms-interview.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 10 Algorithms Machine Learning Engineers Need to Know](/2016/08/10-algorithms-machine-learning-engineers.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top 10 Machine Learning Algorithms for Beginners](/2017/10/top-10-machine-learning-algorithms-beginners.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[KDnuggets News, June 22: Primary Supervised Learning Algorithms…](https://www.kdnuggets.com/2022/n25.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20 Questions (with Answers) to Detect Fake Data Scientists: ChatGPT…](https://www.kdnuggets.com/2023/01/20-questions-detect-fake-data-scientists-chatgpt-1.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20 Questions (with Answers) to Detect Fake Data Scientists: ChatGPT…](https://www.kdnuggets.com/2023/02/20-questions-detect-fake-data-scientists-chatgpt-2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Beginner''s Guide to the Top 10 Machine Learning Algorithms](https://www.kdnuggets.com/a-beginner-guide-to-the-top-10-machine-learning-algorithms)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Primary Supervised Learning Algorithms Used in Machine Learning](https://www.kdnuggets.com/2022/06/primary-supervised-learning-algorithms-used-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Essential Machine Learning Algorithms: A Beginner''s Guide](https://www.kdnuggets.com/2021/05/essential-machine-learning-algorithms-beginners.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
