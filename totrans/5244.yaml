- en: Scalable Select of Random Rows in SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2018/04/scalable-select-random-rows-sql.html](https://www.kdnuggets.com/2018/04/scalable-select-random-rows-sql.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By Pavel Tiunov, [Statsbot](https://statsbot.co/)**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Header image](../Images/65a6d842e8e568f47b9eef4a27cf66c3.png)'
  prefs: []
  type: TYPE_IMG
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re new to the big data world and also migrating from tools like Google
    Analytics or Mixpanel for your web analytics, you probably noticed performance
    differences. Google Analytics can show you predefined reports in seconds, while
    the same query for the same data in your data warehouse can take several minutes
    or even more. Such performance boosts are achieved by selecting random rows or
    the [sampling technique](https://en.wikipedia.org/wiki/Sampling_(statistics)).
    Let’s learn how to select random rows in SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Sample and Population
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we start to work on sampling implementation, it is worth mentioning some
    sampling fundamentals. Sampling is based on a subset selection of individuals
    from some population to describe this population’s properties. So if you have
    some event data, you can select a subset of unique users and their events to calculate
    metrics that describe all users’ behavior. On the other hand, if you select a
    subset of events, it won’t describe the behavior of the population precisely.
  prefs: []
  type: TYPE_NORMAL
- en: Given this fact, we assume all data sets to which sampling is applied have unique
    user identifiers. Moreover, if you have both anonymous user identifiers and authorized
    user identifiers, we assume using the last one as it delivers more accurate results.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting Random Rows in SQL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There’re [plenty of different methods](https://en.wikipedia.org/wiki/Sampling_(statistics)#Sampling_methods) you
    can use to select a subset of users. We’ll consider only two of those as the most
    obvious and simple to implement: simple random sampling and systematic sampling.'
  prefs: []
  type: TYPE_NORMAL
- en: Simple random sampling can be implemented as giving a unique number to each
    user in a range from 0 to N-1 and then selecting X random numbers from 0 to N-1\.
    N denotes the total number of users here and X is the sample size. Although this
    method is very simple for understanding, it’s a little bit tricky to implement
    in an SQL environment, mostly because random number generator outputs don’t scale
    very well when sample sizes are equal to billions. Also it isn’t very clear as
    to how to get evenly distributed samples over time.
  prefs: []
  type: TYPE_NORMAL
- en: Bearing this in mind, we’ll use systematic sampling which can overcome these
    obstacles from an SQL implementation perspective. Simple systematic sampling may
    be implemented as selecting one user from each M users at a specified interval.
    In this case, sample size will be equal to N / M. Selecting one user out of M
    while preserving uniform distribution across sample buckets is the main challenge
    of this approach. Let’s see how this can be implemented in SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Sequence Generated User Identifiers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You’re lucky if your user IDs are integers generated as a strict sequence without
    gaps like those generated by `AUTO_INCREMENT` primary key fields. In this case,
    you can implement systematic sampling as simple as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This SQL query and all SQL queries below are in Standard BigQuery SQL. In this
    example, we’re selecting one user out of 10, which is a 10% sample. 7 is the random
    number of the sampling bucket and it can be any number from 0 to 9\. We use MOD
    operation to create sampling buckets which stand for the remainder of a division
    by 10 in this particular case. It’s really simple to show that if `user_id`is
    a strict integer sequence, then user counts are uniformly distributed across all
    sampling buckets when user count is high enough.
  prefs: []
  type: TYPE_NORMAL
- en: 'To estimate the event count, for example, you can write something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Please note the multiplication by 10 in this query. Given the fact that we use
    a 10% sample, all estimated additive measures should be scaled by 1/10% in order
    to match real values.
  prefs: []
  type: TYPE_NORMAL
- en: 'One can question how precise this sampling approach is for a specific dataset.
    You can estimate it by checking how uniform the distribution is within sampling
    buckets. To do that you can query something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s consider some example results of this query:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c62f4fe96b983935965418997009139e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can calculate the mean and confidence interval for an alpha coefficient
    equal to 0.01 for these sampling bucket sizes. The [confidence interval](https://en.wikipedia.org/wiki/Confidence_interval#Basic_steps) will
    be equal to 0.01% of the sampling bucket average size. This means that with 99%
    probability the sampling bucket sizes differ not more than 0.01%. Different metrics
    calculated with these sampling buckets couple with this statistic but don’t inherit
    it. So, in order to calculate precision for event count estimation, you can calculate
    the event count for each sample as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Then calculate absolute and relative confidence intervals for these event counts
    as in the case of user count to get event count estimate precision.
  prefs: []
  type: TYPE_NORMAL
- en: String Identifiers and Other User Identifiers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the case that you have string user identifiers or integers, but not a strict
    sequence instead of integer sequence identifiers, you need a way to distribute
    all user IDs between different sampling buckets in a uniform manner. [This](https://en.wikipedia.org/wiki/Hash_function#Uniformity) can
    be done by hash functions. Not all hash functions can get you uniform distribution
    under different circumstances. You can check [smhasher](https://github.com/rurban/smhasher/tree/master/doc) test
    suite results to check how good a particular hash function is at this.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in BigQuery you can use the `FARM_FINGERPRINT` hash function to
    prepare a sample for select:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`FARM_FINGERPRINT` can be replaced with any suitable hash function, such as
    `xxhash64` in Presto or even the combination of `md5` and `strtol` in Redshift.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As in the case of sequence-generated user identifiers you can check uniformity
    statistics as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Common Pitfalls
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most often, sampling can reduce your SQL query execution time by more than 5-10
    times without being harmful in terms of precision, but there are still cases where
    you should be cautious. This mostly belongs to a sampling bias problem because
    of sample size. It can occur when dispersion of metrics you’re interested in between
    samples is too high even if the sample size is big enough.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you can be interested in some rare event count such as an enterprise
    demo request being some B2C site with a huge amount of traffic. If the enterprise
    demo request count is about 100 events per month and your monthly active users
    is about 1M, then sampling can lead you in a situation where the enterprise demo
    request count estimate with a 10% sample produces significant errors. Generally
    speaking, sampling random rows in SQL should be avoided in this case or [more
    sophisticated methods](https://en.wikipedia.org/wiki/Sampling_(statistics)#Stratified_sampling) should
    be used instead.
  prefs: []
  type: TYPE_NORMAL
- en: As always don’t hesitate to leave your comments or [contact our team](https://statsbot.co/form-trial?utm_source=statsbotblog&utm_medium=article&utm_campaign=random_rows) to
    get help.
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://statsbot.co/blog/select-random-rows-sql?utm_source=kdnuggets&utm_medium=post&utm_campaign=random-select).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Calculating Customer Lifetime Value: SQL Example](/2018/02/calculating-customer-lifetime-value-sql-example.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SQL Window Functions Tutorial for Business Analysis](/2017/12/sql-window-functions-tutorial-business-analytics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Guide for Customer Retention Analysis with SQL](/2017/12/guide-customer-retention-analysis-sql.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How to Select Rows and Columns in Pandas Using [ ], .loc, iloc, .at…](https://www.kdnuggets.com/2019/06/select-rows-columns-pandas.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a Scalable ETL with SQL + Python](https://www.kdnuggets.com/2022/04/building-scalable-etl-sql-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Generalized and Scalable Optimal Sparse Decision Trees(GOSDT)](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Build a Scalable Data Architecture with Apache Kafka](https://www.kdnuggets.com/2023/04/build-scalable-data-architecture-apache-kafka.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Process a DataFrame with Millions of Rows in Seconds](https://www.kdnuggets.com/2022/01/process-dataframe-millions-rows-seconds.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3 Ways to Append Rows to Pandas DataFrames](https://www.kdnuggets.com/2022/08/3-ways-append-rows-pandas-dataframes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
