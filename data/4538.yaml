- en: Build Pipelines with Pandas Using pdpipe
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 pdpipe 构建 Pandas 管道
- en: 原文：[https://www.kdnuggets.com/2019/12/build-pipelines-pandas-pdpipe.html](https://www.kdnuggets.com/2019/12/build-pipelines-pandas-pdpipe.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/12/build-pipelines-pandas-pdpipe.html](https://www.kdnuggets.com/2019/12/build-pipelines-pandas-pdpipe.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '![](../Images/35dc4b78af29432b47da1a6c94187323.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35dc4b78af29432b47da1a6c94187323.png)'
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引言
- en: Pandas is an amazing library in the Python ecosystem for data analytics and
    machine learning. They form the perfect bridge between the data world, where Excel/CSV
    files and SQL tables live, and the modeling world where Scikit-learn or TensorFlow
    perform their magic.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 是 Python 生态系统中一个了不起的数据分析和机器学习库。它们在数据世界（如 Excel/CSV 文件和 SQL 表）与建模世界（如
    Scikit-learn 或 TensorFlow 的魔力）之间架起了完美的桥梁。
- en: A data science flow is most often a sequence of steps — datasets must be cleaned,
    scaled, and validated before they can be ready to be used by that powerful machine
    learning algorithm.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学流程通常是一系列步骤——数据集必须经过清洗、缩放和验证，然后才能被强大的机器学习算法使用。
- en: These tasks can, of course, be done with many single-step functions/methods
    that are offered by packages like Pandas but a more elegant way is to use a pipeline.
    In almost all cases, a pipeline reduces the chance of error and saves time by
    automating repetitive tasks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这些任务当然可以通过 Pandas 等包提供的许多单步函数/方法完成，但更优雅的方式是使用管道。在几乎所有情况下，管道通过自动化重复任务来减少错误的机会并节省时间。
- en: In the data science world, great examples of packages with pipeline features
    are — [dplyr in R language](https://dplyr.tidyverse.org/), and [Scikit-learn in
    the Python ecosystem](https://scikit-learn.org/stable/modules/compose.html).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学领域，具有管道功能的优秀包示例包括——[R 语言中的 dplyr](https://dplyr.tidyverse.org/)，以及[Python
    生态系统中的 Scikit-learn](https://scikit-learn.org/stable/modules/compose.html)。
- en: A data science flow is most often a sequence of steps — datasets must be cleaned,
    scaled, and validated before they can be ready to be used
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据科学流程通常是一系列步骤——数据集必须经过清洗、缩放和验证，然后才能准备好使用。
- en: Following is a great article about their use in a machine-learning workflow.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一篇关于在机器学习工作流程中使用管道的精彩文章。
- en: '[**Managing Machine Learning Workflows with Scikit-learn Pipelines Part 1:
    A Gentle Introduction**](https://www.kdnuggets.com/2017/12/managing-machine-learning-workflows-scikit-learn-pipelines-part-1.html?source=post_page-----cade6128cd31----------------------)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[**使用 Scikit-learn Pipelines 管理机器学习工作流程 第1部分：温和入门**](https://www.kdnuggets.com/2017/12/managing-machine-learning-workflows-scikit-learn-pipelines-part-1.html?source=post_page-----cade6128cd31----------------------)'
- en: Are you familiar with Scikit-learn Pipelines? They are an extremely simple yet
    very useful tool for managing machine ...
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你对 Scikit-learn Pipelines 熟悉吗？它们是管理机器学习的极简而非常有用的工具...
- en: Pandas also offer a `**.pipe**` method which can be used for similar purposes
    with user-defined functions. However, in this article, we are going to discuss
    a wonderful little library called [**pdpipe**](https://github.com/shaypal5/pdpipe),
    which specifically addresses this pipelining issue with Pandas DataFrame.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 还提供了一个`**.pipe**`方法，可以用于类似的目的与用户定义的函数。然而，在本文中，我们将讨论一个叫做[**pdpipe**](https://github.com/shaypal5/pdpipe)的绝妙小库，它专门解决了
    Pandas DataFrame 的管道化问题。
- en: In almost all cases, a pipeline reduces the chance of error and saves time by
    automating repetitive tasks
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在几乎所有情况下，管道通过自动化重复任务来减少错误的机会并节省时间。
- en: Pipelining with Pandas
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Pandas 进行管道化
- en: The example [Jupyter notebook can be found here in my Github repo](https://github.com/tirthajyoti/Machine-Learning-with-Python/blob/master/Pandas%20and%20Numpy/pdpipe-example.ipynb).
    Let’s see how we can build useful pipelines with this library.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 示例[Jupyter notebook 可以在我的 GitHub 仓库中找到](https://github.com/tirthajyoti/Machine-Learning-with-Python/blob/master/Pandas%20and%20Numpy/pdpipe-example.ipynb)。让我们看看如何利用这个库构建有用的管道。
- en: The dataset
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集
- en: For the demonstration purpose, we will use a [dataset of US Housing prices](https://www.kaggle.com/vedavyasv/usa-housing) (downloaded
    from Kaggle). We can load the dataset in Pandas and show its summary statistics
    as follows,
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示，我们将使用一个[美国住房价格数据集](https://www.kaggle.com/vedavyasv/usa-housing)（从 Kaggle
    下载）。我们可以在 Pandas 中加载数据集，并显示其摘要统计信息如下，
- en: '![](../Images/fec6e833a0de5a47dadd841113509ca2.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fec6e833a0de5a47dadd841113509ca2.png)'
- en: However, the dataset also has an ‘Address’ field which contains text data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据集中也有一个包含文本数据的“Address”字段。
- en: '![](../Images/82af45ead8d94494050e8834814a9587.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82af45ead8d94494050e8834814a9587.png)'
- en: Adding a size qualifier column
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加大小限定符列
- en: For the demo, we add a column to the dataset qualifying the size of the house,
    with the following code,
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示，我们向数据集添加了一列来标示房子的大小，代码如下，
- en: '![](../Images/bec265b90a72104255320186bb0880b7.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bec265b90a72104255320186bb0880b7.png)'
- en: The dataset looks like following after this,
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在处理后如下所示，
- en: '![](../Images/0600cabbbf97f788eecefe6f6485a709.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0600cabbbf97f788eecefe6f6485a709.png)'
- en: The simplest pipeline — one operation
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最简单的管道 — 一个操作
- en: We start with the simplest possible pipeline, consisting of just one operation
    (don’t worry, we will add complexity soon enough).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从最简单的管道开始，仅包含一个操作（别担心，我们很快会增加复杂性）。
- en: Let’s say the machine learning team and the domain experts say that they think
    we can safely ignore the `Avg. Area House Age` data for modeling. Therefore, we
    will drop this column from the dataset.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 假设机器学习团队和领域专家认为我们可以安全地忽略 `Avg. Area House Age` 数据用于建模。因此，我们将从数据集中删除此列。
- en: For this task, we create a pipeline object `drop_age` with the `ColDrop` method
    from [**pdpipe**](https://github.com/shaypal5/pdpipe) and pass the DataFrame to
    this pipeline.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此任务，我们创建了一个管道对象 `drop_age`，使用来自 [**pdpipe**](https://github.com/shaypal5/pdpipe)
    的 `ColDrop` 方法，并将 DataFrame 传递给该管道。
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The resulting DataFrame, as expected, looks like following,
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 结果 DataFrame，如预期的那样，如下所示，
- en: '![](../Images/98a36d37e0343974e7ee173153763177.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98a36d37e0343974e7ee173153763177.png)'
- en: Chain stages of pipeline simply by adding
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过添加来链式链接管道的各个阶段
- en: Pipelines are useful and practical only when we are able to multiple stages.
    There are multiple methods by which you can do that in [**pdpipe**](https://github.com/shaypal5/pdpipe).
    However, the simplest and most intuitive approach is to use the + operator. It
    is like hand-joining to pipes!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 管道只有在我们能够进行多个阶段时才有用和实用。你可以使用多种方法在 [**pdpipe**](https://github.com/shaypal5/pdpipe)
    中实现这一点。然而，最简单且直观的方法是使用 + 运算符。这就像是手动连接管道！
- en: Let’s say, apart from dropping the age column, we also want to one-hot-encode
    the `House_size` column so that a classification or regression algorithm can be
    run on the dataset easily.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，除了删除年龄列，我们还希望对 `House_size` 列进行一热编码，以便可以轻松地在数据集上运行分类或回归算法。
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: So, we created a pipeline object first with the `ColDrop` method to drop the `Avg.
    Area House Age` column. Thereafter, we just simply added the `OneHotEncode` method
    to this pipeline object with the usual Python `+=` syntax.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们首先创建了一个管道对象，使用 `ColDrop` 方法删除 `Avg. Area House Age` 列。然后，我们简单地将 `OneHotEncode`
    方法添加到该管道对象中，使用通常的 Python `+=` 语法。
- en: The resulting DataFrame looks like the following. Note the additional indicator
    columns `House_size_Medium` and `House_size_Small` created from the one-hot-encoding
    process.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 结果 DataFrame 如下所示。注意由一热编码过程创建的附加指示器列 `House_size_Medium` 和 `House_size_Small`。
- en: '![](../Images/f28f16ec5fc6c724925932d3fa92e28b.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f28f16ec5fc6c724925932d3fa92e28b.png)'
- en: Drop some rows based on their values
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 根据其值删除某些行
- en: Next, we may want to remove rows of data based on their values. Specifically,
    we may want to drop all the data where the house price is less than 250,000\.
    We have the`ApplybyCol` method to apply any user-defined function to the DataFrame
    and also a method `ValDrop` to drop rows based on a specific value. We can easily
    chain these methods to our pipeline to selectively drop rows (we are still adding
    to our existing `pipeline` object which already does the other jobs of column
    dropping and one-hot-encoding).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可能希望根据其值删除数据行。具体来说，我们可能希望删除所有房价低于 250,000 的数据。我们有 `ApplybyCol` 方法可以将任何用户定义的函数应用到
    DataFrame，还有一个方法 `ValDrop` 用于根据特定值删除行。我们可以轻松地将这些方法链接到我们的管道中，以选择性地删除行（我们仍在向现有的
    `pipeline` 对象添加内容，该对象已经完成了列删除和一热编码的其他工作）。
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The first method tags the rows based on the value in the `Price` column by applying
    the user-defined function `price_tag()`,
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法通过应用用户定义的函数 `price_tag()`，根据 `Price` 列中的值标记行，
- en: '![](../Images/160d29fd4df1f23cf5a9212239a4768f.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/160d29fd4df1f23cf5a9212239a4768f.png)'
- en: The second method looks for the string `drop` in the `Price_tag` column and
    drops those rows that match. And finally, the third method removes the `Price_tag` column,
    cleaning up the DataFrame. After all, this `Price_tag` column was only needed
    temporarily, to tag specific rows, and should be removed after it served its purpose.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法查找 `Price_tag` 列中的字符串 `drop`，并删除匹配的行。最后，第三种方法删除 `Price_tag` 列，清理 DataFrame。毕竟，这个
    `Price_tag` 列只是暂时需要的，用于标记特定的行，并在完成其用途后应被删除。
- en: All of this is done by simply chaining stages of operations on the same pipeline!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都是通过简单地链接同一管道上的操作阶段完成的！
- en: At this point, we can look back and see what our pipeline does to the DataFrame
    right from the beginning,
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可以回顾一下我们的管道从一开始对数据框做了什么，
- en: drops a specific column
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除特定列
- en: one-hot-encodes a categorical data column for modeling
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对类别数据列进行独热编码以进行建模
- en: tags data based on a user-defined function
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于用户定义的函数标记数据
- en: drops rows based on the tag
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于标签删除行
- en: drops the temporary tagging column
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除临时标记列
- en: All of this — using the following five lines of code,
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些 — 使用以下五行代码，
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Scikit-learn and NLTK stages
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Scikit-learn 和 NLTK 阶段
- en: There are many more useful and intuitive DataFrame manipulation methods available
    for DataFrame manipulation. However, we just wanted to show that even some operations
    from Scikit-learn and NLTK package are included in [**pdpipe**](https://github.com/shaypal5/pdpipe) for
    making awesome pipelines.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多更多有用且直观的数据框操作方法可供使用。然而，我们只是想展示即使是来自 Scikit-learn 和 NLTK 包的一些操作也被包含在 [**pdpipe**](https://github.com/shaypal5/pdpipe) 中，以便创建出色的管道。
- en: Scaling estimator from Scikit-learn
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Scikit-learn 的缩放估算器
- en: One of the most common tasks for building machine learning models is the scaling
    of the data. Scikit-learn offers a few different types of scaling such as Min-Max
    scaling, or Standardization based scaling (where mean of a data set is subtracted
    followed by division by standard deviation).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 构建机器学习模型的最常见任务之一是数据缩放。Scikit-learn 提供了几种不同类型的缩放，例如 Min-Max 缩放或基于标准化的缩放（其中从数据集中减去均值，然后除以标准差）。
- en: We can directly chain such scaling operations in a pipeline. Following code
    demonstrates the use,
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接在管道中链接这样的缩放操作。以下代码演示了这种用法，
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here we applied the `[StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)`[ estimator](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) from
    the Scikit-learn package to transform the data for clustering or neural network
    fitting. We can selectively exclude columns which do not need such scaling as
    we have done here for the indicator columns `House_size_Medium` and `House_size_Small`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们应用了来自 Scikit-learn 包的 `[`[StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)`]`[ 估算器](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)
    来转换数据以进行聚类或神经网络拟合。我们可以选择性地排除不需要此类缩放的列，如我们在这里对指标列 `House_size_Medium` 和 `House_size_Small`
    所做的那样。
- en: And voila! We get the scaled DataFrame,
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！我们得到了缩放后的数据框，
- en: '![](../Images/9c395fb9f3a24f89c8681e0bdd1c6a89.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c395fb9f3a24f89c8681e0bdd1c6a89.png)'
- en: Tokenizer from NLTK
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NLTK 的分词器
- en: We note that the Address field in our DataFrame is pretty useless right now.
    However, if we can extract zip code or State from those strings, they might be
    useful for some kind of visualization or machine learning task.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，目前我们数据框中的地址字段几乎没有用处。然而，如果我们能够从这些字符串中提取邮政编码或州名，它们可能对某种可视化或机器学习任务有用。
- en: We can use a [Word Tokenizer](https://www.guru99.com/tokenize-words-sentences-nltk.html) for
    this purpose. NLTK is a popular and powerful Python library for text mining and
    natural language processing (NLP) and offers a range of tokenizer methods. Here,
    we can use one such tokenizer to split up the text in the address field and extract
    the name of the state from that. We recognize that the name of the state is the
    penultimate word in the address string. Therefore, following chained pipeline
    will do the job for us,
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用一个 [词汇分词器](https://www.guru99.com/tokenize-words-sentences-nltk.html) 来实现这个目的。NLTK
    是一个流行且强大的 Python 库，用于文本挖掘和自然语言处理（NLP），并提供了一系列分词器方法。在这里，我们可以使用其中一个分词器来拆分地址字段中的文本，并从中提取州名。我们认识到，州名是地址字符串中的倒数第二个单词。因此，以下链式管道将为我们完成这项工作，
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The resulting DataFrame looks like following,
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 结果数据框如下所示，
- en: '![](../Images/423cabcab7c4c503e70de4a239380d78.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/423cabcab7c4c503e70de4a239380d78.png)'
- en: Summary
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: If we summarize all the operations shown in this demo, it looks like the following,
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们总结一下演示中展示的所有操作，它看起来像以下内容，
- en: '![](../Images/dc42570defa14208de7f4f03da36a377.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc42570defa14208de7f4f03da36a377.png)'
- en: All of these operations may be used frequently on similar types of datasets
    and it will be wonderful to have a simple set of sequential code blocks to execute
    as a pre-processing operation before the dataset is ready for the next level of
    modeling.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些操作可能会在类似类型的数据集上频繁使用，拥有一组简单的顺序代码块来执行数据集预处理操作将会非常棒，以便在数据集准备好进行下一阶段建模之前进行处理。
- en: Pipelining is the key to achieve that uniform set of sequential code blocks.
    Pandas is the most widely used Python library for such data pre-processing tasks
    in a machine learning/data science team and [**pdpipe**](https://github.com/shaypal5/pdpipe) provides
    a simple yet powerful way to build pipelines with Pandas-type operations which
    can be directly applied to the Pandas DataFrame objects.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 管道化是实现那一组统一的顺序代码块的关键。Pandas是机器学习/数据科学团队中用于数据预处理任务的最广泛使用的Python库，[**pdpipe**](https://github.com/shaypal5/pdpipe)提供了一种简单而强大的方式来构建管道，使用类似Pandas的操作，这些操作可以直接应用于Pandas
    DataFrame对象。
- en: '[Explore this library on your own](https://github.com/shaypal5/pdpipe) and
    build more powerful pipelines for your specific data science task.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[自行探索此库](https://github.com/shaypal5/pdpipe)并为你的特定数据科学任务构建更强大的管道。'
- en: If you have any questions or ideas to share, please contact the author at [**tirthajyoti[AT]gmail.com**](mailto:tirthajyoti@gmail.com).
    Also, you can check the author’s [**GitHub**](https://github.com/tirthajyoti?tab=repositories)** repositories **for
    code, ideas, and resources in machine learning and data science. If you are, like
    me, passionate about AI/machine learning/data science, please feel free to [add
    me on LinkedIn](https://www.linkedin.com/in/tirthajyoti-sarkar-2127aa7/) or [follow
    me on Twitter](https://twitter.com/tirthajyotiS).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何问题或想法分享，请通过[**tirthajyoti[AT]gmail.com**](mailto:tirthajyoti@gmail.com)联系作者。此外，你还可以查看作者的[**GitHub**](https://github.com/tirthajyoti?tab=repositories)** 代码库**，了解有关机器学习和数据科学的代码、想法和资源。如果你像我一样，对AI/机器学习/数据科学充满热情，请随时[在LinkedIn上添加我](https://www.linkedin.com/in/tirthajyoti-sarkar-2127aa7/)或[在Twitter上关注我](https://twitter.com/tirthajyotiS)。
- en: '[Original](https://towardsdatascience.com/https-medium-com-tirthajyoti-build-pipelines-with-pandas-using-pdpipe-cade6128cd31).
    Reposted with permission.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/https-medium-com-tirthajyoti-build-pipelines-with-pandas-using-pdpipe-cade6128cd31)。已获得许可转载。'
- en: '**Related:**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[How to Speed up Pandas by 4x with one line of code](/2019/11/speed-up-pandas-4x.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何用一行代码将Pandas加速4倍](/2019/11/speed-up-pandas-4x.html)'
- en: '[5 Great New Features in Latest Scikit-learn Release](/2019/12/5-features-scikit-learn-release-highlights.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[最新Scikit-learn版本中的5个重要新特性](/2019/12/5-features-scikit-learn-release-highlights.html)'
- en: '[Data Pipelines, Luigi, Airflow: Everything you need to know](/2019/03/data-pipelines-luigi-airflow-everything-need-know.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据管道、Luigi、Airflow：你需要了解的一切](/2019/03/data-pipelines-luigi-airflow-everything-need-know.html)'
- en: '* * *'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行IT管理'
- en: '* * *'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成为一名优秀数据科学家所需的5项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个初学者数据科学家应掌握的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2021年最佳ETL工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用管道编写清晰的Python代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，并寻找目标以……](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一项 $9B 的 AI 失败，解析](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
