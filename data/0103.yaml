- en: 'GPT-4: 8 Models in One; The Secret is Out'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-4：8个模型合而为一；秘密曝光
- en: 原文：[https://www.kdnuggets.com/2023/08/gpt4-8-models-one-secret.html](https://www.kdnuggets.com/2023/08/gpt4-8-models-one-secret.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/08/gpt4-8-models-one-secret.html](https://www.kdnuggets.com/2023/08/gpt4-8-models-one-secret.html)
- en: The GPT4 model has been THE groundbreaking model so far, available to the general
    public either for free or through their commercial portal (for public beta use).
    It has worked wonders in igniting new project ideas and use-cases for many entrepreneurs
    but the secrecy about the number of parameters and the model was killing all enthusiasts
    who were betting on the first 1 trillion parameter model to 100 trillion parameter
    claims!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4模型迄今为止是最具突破性的模型，向公众开放，无论是免费还是通过其商业门户（用于公开测试）。它在激发许多企业家的新项目想法和应用场景方面效果显著，但关于参数数量和模型的保密性使所有对第一个1万亿参数模型或100万亿参数模型的猜测者感到沮丧！
- en: The cat is out of the bag
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 秘密已经泄露
- en: Well, the cat is out of the bag (Sort of). On June 20th, [George Hotz](https://twitter.com/swyx/status/1671272883379908608),
    founder of self-driving startup Comma.ai leaked that GPT-4 isn’t a single monolithic
    dense model (like GPT-3 and GPT-3.5) but a mixture of 8 x 220-billion-parameter
    models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，秘密已经泄露（某种程度上）。6月20日，[George Hotz](https://twitter.com/swyx/status/1671272883379908608)，自驾车初创公司Comma.ai的创始人，泄露了GPT-4并不是一个单一的整体密集模型（像GPT-3和GPT-3.5那样），而是由8个2200亿参数模型组成的混合体。
- en: Later that day, [Soumith Chintala](https://twitter.com/soumithchintala/status/1671267150101721090),
    co-founder of PyTorch at Meta, reaffirmed the leak.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当天稍晚些时候，[Soumith Chintala](https://twitter.com/soumithchintala/status/1671267150101721090)，Meta的PyTorch联合创始人，重申了这一泄密信息。
- en: Just the day before, [Mikhail Parakhin](https://twitter.com/MParakhin/status/1670666605427298304),
    Microsoft Bing AI lead, had also hinted at this.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 就在前一天，[Mikhail Parakhin](https://twitter.com/MParakhin/status/1670666605427298304)，微软必应AI负责人，也对此有所暗示。
- en: 'GPT 4: Not a Monolith'
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-4：非单一实体
- en: What do all the tweets mean? The GPT-4 is not a single large model but a union/ensemble
    of 8 smaller models sharing the expertise. Each of these models is rumored to
    be 220 Billion parameters.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些推文意味着什么？GPT-4并不是一个单一的大型模型，而是由8个小型模型组成的联合体/集成体，这些小模型共享专长。每个模型据说有2200亿个参数。
- en: '![GPT-4: 8 Models in One; The Secret is Out](../Images/adc4c3c626cdea18f09ee3feca386139.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![GPT-4: 8个模型合而为一；秘密曝光](../Images/adc4c3c626cdea18f09ee3feca386139.png)'
- en: The methodology is called a mixture of experts' model paradigms (linked below).
    It's a well-known methodology also called as hydra of model. It reminds me of
    Indian mythology I will go with Ravana.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被称为专家混合模型范式（详见下方链接）。这是一种著名的方法，也被称为模型的多头怪物。它让我想起了印度神话中的拉瓦那。
- en: Please take it with a grain of salt that it is not official news but significantly
    high-ranking members in the AI community have spoken/hinted towards it. Microsoft
    is yet to confirm any of these.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 请谨慎对待这些信息，因为这不是官方消息，但在人工智能领域的许多高级成员已对此有所言谈或暗示。微软尚未确认这些内容。
- en: What is a Mixture of Experts paradigm?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是专家混合范式？
- en: Now that we have spoken about the mixture of experts, let's take a little bit
    of a dive into what that thing is. The Mixture of Experts is an ensemble learning
    technique developed specifically for neural networks. It differs a bit from the
    general ensemble technique from the conventional machine learning modeling (that
    form is a generalized form). So you can consider that the Mixture of Experts in
    LLMs is a special case for ensemble methods.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们谈到了专家混合，那么让我们稍微深入了解一下这是什么。专家混合是一种专门为神经网络开发的集成学习技术。它与传统机器学习建模中的一般集成技术有所不同（这种形式是广义形式）。所以你可以认为在LLM中的专家混合是一种特殊的集成方法。
- en: In short, in this method, a task is divided into subtasks, and experts for each
    subtask are used to solve the models. It is a way to divide and conquer approach
    while creating decision trees. One could also consider it as meta-learning on
    top of the expert models for each separate task.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在这种方法中，一个任务被分解成子任务，然后利用每个子任务的专家来解决模型。这是一种将任务分解并征服的方法，同时创建决策树。也可以将其视为对每个单独任务的专家模型的元学习。
- en: A smaller and better model can be trained for each sub-task or problem type.
    A meta-model learns to use which model is better at predicting a particular task.
    Meta learner/model acts as a traffic cop. The sub-tasks may or may not overlap,
    which means that a combination of the outputs can be merged together to come up
    with the final output.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 可以为每个子任务或问题类型训练一个更小、更好的模型。元模型学习使用哪个模型更适合预测特定任务。元学习者/模型充当交通警察。子任务可能会有重叠，也可能没有，这意味着可以将输出组合在一起以得到最终结果。
- en: '***For the concept-descriptions from MOE to Pooling, all credits to the great
    blog by Jason Brownlee (***[***https://machinelearningmastery.com/mixture-of-experts/***](https://machinelearningmastery.com/mixture-of-experts/)***).
    If you like what you read below, please please subscribe to Jason’s blog and buy
    a book or two to support his amazing work!***'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '***关于从MOE到Pooling的概念描述，所有荣誉归于Jason Brownlee的精彩博客（***[***https://machinelearningmastery.com/mixture-of-experts/***](https://machinelearningmastery.com/mixture-of-experts/)***）。如果你喜欢下面的内容，请订阅Jason的博客并购买一两本书以支持他的精彩工作！***'
- en: '**Mixture of experts**, MoE or ME for short, is an ensemble learning technique
    that implements the idea of training experts on subtasks of a predictive modeling
    problem.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**专家模型**，简称MoE或ME，是一种集成学习技术，实施了在预测建模问题的子任务上训练专家的理念。'
- en: '*In the neural network community, several researchers have examined the decomposition
    methodology. […] Mixture–of–Experts (ME) methodology that decomposes the input
    space, such that each expert examines a different part of the space. […] A gating
    network is responsible for combining the various experts.*'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*在神经网络社区，几位研究人员已研究了分解方法论。[…] Mixture–of–Experts (ME) 方法将输入空间分解，使得每个专家检查空间的不同部分。[…]
    门控网络负责组合不同的专家。*'
- en: — Page 73, [Pattern Classification Using Ensemble Methods](https://amzn.to/2zxc0F7),
    2010.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: — 第73页，[使用集成方法的模式分类](https://amzn.to/2zxc0F7)，2010年。
- en: 'There are four elements to the approach, they are:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法有四个要素，它们是：
- en: Division of a task into subtasks.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将任务分解为子任务。
- en: Develop an expert for each subtask.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个子任务开发一个专家。
- en: Use a gating model to decide which expert to use.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用门控模型来决定使用哪个专家。
- en: Pool predictions and gating model output to make a prediction.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 汇总预测和门控模型输出以进行预测。
- en: The figure below, taken from Page 94 of the 2012 book “[Ensemble Methods](https://amzn.to/2XZzrjG),”
    provides a helpful overview of the architectural elements of the method.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下图摘自2012年书籍《[集成方法](https://amzn.to/2XZzrjG)》第94页，提供了该方法的架构元素的有用概述。
- en: How Do 8 Smaller Models in GPT4 Work?
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT4中的8个小模型如何工作？
- en: The secret “Model of Experts” is out, let's understand why GPT4 is so good!
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: “专家模型”的秘密揭晓了，让我们了解一下为什么GPT4如此出色！
- en: ithinkbot.com
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ithinkbot.com
- en: '![GPT-4: 8 Models in One; The Secret is Out](../Images/43649f70f4c867fef5175e0c10255984.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![GPT-4: 8 Models in One; The Secret is Out](../Images/43649f70f4c867fef5175e0c10255984.png)'
- en: Example of a Mixture of Experts Model with Expert Members and a Gating Network
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 具有专家成员和门控网络的专家模型示例
- en: 'Taken from: Ensemble Methods'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：集成方法
- en: Subtasks
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 子任务
- en: The first step is to divide the predictive modeling problem into subtasks. This
    often involves using domain knowledge. For example, an image could be divided
    into separate elements such as background, foreground, objects, colors, lines,
    and so on.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将预测建模问题划分为子任务。这通常涉及使用领域知识。例如，可以将图像分解为背景、前景、对象、颜色、线条等独立元素。
- en: '*… ME works in a divide-and-conquer strategy where a complex task is broken
    up into several simpler and smaller subtasks, and individual learners (called
    experts) are trained for different subtasks.*'
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*… ME采用分而治之的策略，将复杂任务分解为几个更简单、更小的子任务，个体学习者（称为专家）被训练以处理不同的子任务。*'
- en: — Page 94, [Ensemble Methods](https://amzn.to/2XZzrjG), 2012.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: — 第94页，[集成方法](https://amzn.to/2XZzrjG)，2012年。
- en: For those problems where the division of the task into subtasks is not obvious,
    a simpler and more generic approach could be used. For example, one could imagine
    an approach that divides the input feature space by groups of columns or separates
    examples in the feature space based on distance measures, inliers, and outliers
    for a standard distribution, and much more.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些任务分解不明显的问题，可以使用更简单和更通用的方法。例如，可以设想一种方法，通过列的组来划分输入特征空间，或根据距离度量、异常值、标准分布等将特征空间中的示例分开，等等。
- en: '*… in ME, a key problem is how to find the natural division of the task and
    then derive the overall solution from sub-solutions.*'
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*…在ME中，一个关键问题是如何找到任务的自然划分，然后从子解决方案中得出总体解决方案。*'
- en: — Page 94, [Ensemble Methods](https://amzn.to/2XZzrjG), 2012.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: — 第94页， [集成方法](https://amzn.to/2XZzrjG)，2012年。
- en: Expert Models
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 专家模型
- en: Next, an expert is designed for each subtask.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为每个子任务设计一个专家。
- en: The mixture of experts approach was initially developed and explored within
    the field of artificial neural networks, so traditionally, experts themselves
    are neural network models used to predict a numerical value in the case of regression
    or a class label in the case of classification.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 专家混合方法最初是在人工神经网络领域中开发和探索的，因此传统上，专家本身是神经网络模型，用于在回归情况下预测数值，或在分类情况下预测类别标签。
- en: '*It should be clear that we can “plug in” any model for the expert. For example,
    we can use neural networks to represent both the gating functions and the experts.
    The result is known as a mixture density network.*'
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*应该清楚，我们可以为专家“插入”任何模型。例如，我们可以使用神经网络来表示门控函数和专家。结果被称为混合密度网络。*'
- en: '— Page 344, [Machine Learning: A Probabilistic Perspective](https://amzn.to/2YrVLmp),
    2012.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: — 第344页， [机器学习：概率视角](https://amzn.to/2YrVLmp)，2012年。
- en: Experts each receive the same input pattern (row) and make a prediction.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 专家们接收相同的输入模式（行）并做出预测。
- en: Gating Model
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 门控模型
- en: A model is used to interpret the predictions made by each expert and to aid
    in deciding which expert to trust for a given input. This is called the gating
    model, or the gating network, given that it is traditionally a neural network
    model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型用于解释每个专家所做的预测，并帮助决定在给定输入时信任哪个专家。这被称为门控模型，或门控网络，因为它通常是一个神经网络模型。
- en: The gating network takes as input the input pattern that was provided to the
    expert models and outputs the contribution that each expert should have in making
    a prediction for the input.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 门控网络以提供给专家模型的输入模式为输入，并输出每个专家在对输入进行预测时应有的贡献。
- en: '*… the weights determined by the gating network are dynamically assigned based
    on the given input, as the MoE effectively learns which portion of the feature
    space is learned by each ensemble member*'
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*…由门控网络确定的权重是基于给定输入动态分配的，因为MoE有效地学习了每个集成成员学习的特征空间的部分*'
- en: — Page 16, [Ensemble Machine Learning](https://amzn.to/2C7syo5), 2012.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: — 第16页， [集成机器学习](https://amzn.to/2C7syo5)，2012年。
- en: The gating network is key to the approach and effectively, the model learns
    to choose the type subtask for a given input and, in turn, the expert to trust
    to make a strong prediction.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 门控网络是该方法的关键，有效地，模型学会选择给定输入的子任务类型，并进而选择信任哪个专家以做出强有力的预测。
- en: '*Mixture-of-experts can also be seen as a classifier selection algorithm, where
    individual classifiers are trained to become experts in some portion of the feature
    space.*'
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*混合专家也可以被视为一个分类器选择算法，其中单个分类器被训练成为特征空间某部分的专家。*'
- en: — Page 16, [Ensemble Machine Learning](https://amzn.to/2C7syo5), 2012.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: — 第16页， [集成机器学习](https://amzn.to/2C7syo5)，2012年。
- en: When neural network models are used, the gating network and the experts are
    trained together such that the gating network learns when to trust each expert
    to make a prediction. This training procedure was traditionally implemented using [expectation
    maximization](https://machinelearningmastery.com/expectation-maximization-em-algorithm/) (EM).
    The gating network might have a softmax output that gives a probability-like confidence
    score for each expert.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用神经网络模型时，门控网络和专家一起训练，以便门控网络学习何时信任每个专家进行预测。这种训练过程传统上使用 [期望最大化](https://machinelearningmastery.com/expectation-maximization-em-algorithm/) (EM)
    实现。门控网络可能具有一个softmax输出，为每个专家提供类似概率的置信度分数。
- en: '*In general, the training procedure tries to achieve two goals: for given experts,
    to find the optimal gating function; for a given gating function, to train the
    experts on the distribution specified by the gating function.*'
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*一般来说，训练过程试图实现两个目标：对于给定的专家，找到最优的门控函数；对于给定的门控函数，在门控函数指定的分布上训练专家。*'
- en: — Page 95, [Ensemble Methods](https://amzn.to/2XZzrjG), 2012.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: — 第95页， [集成方法](https://amzn.to/2XZzrjG)，2012年。
- en: Pooling Method
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化方法
- en: Finally, the mixture of expert models must make a prediction, and this is achieved
    using a pooling or aggregation mechanism. This might be as simple as selecting
    the expert with the largest output or confidence provided by the gating network.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，专家模型的混合必须进行预测，这通过池化或聚合机制来实现。这可能是选择输出最大或由门控网络提供的信心最大的专家。
- en: Alternatively, a weighted sum prediction could be made that explicitly combines
    the predictions made by each expert and the confidence estimated by the gating
    network. You might imagine other approaches to making effective use of the predictions
    and gating network output.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，可以做出一个加权和预测，明确结合每个专家做出的预测和由门控网络估计的信心。你可以想象其他方法来有效利用预测和门控网络输出。
- en: '*The pooling/combining system may then choose a single classifier with the
    highest weight, or calculate a weighted sum of the classifier outputs for each
    class, and pick the class that receives the highest weighted sum.*'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*池化/组合系统可能会选择权重最高的单个分类器，或计算每个类别的分类器输出的加权和，并选择接收最高加权和的类别。*'
- en: — Page 16, [Ensemble Machine Learning](https://amzn.to/2C7syo5), 2012.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: — 第16页，[集成机器学习](https://amzn.to/2C7syo5)，2012。
- en: Switch Routing
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 切换路由
- en: We should also briefly discuss the switch routing approach differs from the
    MoE paper. I am bringing it up as it seems like Microsoft has used a switch routing
    than a Model of Experts to save some computational complexity, but I am happy
    to be proven wrong. When there are more than one expert's models, they may have
    a non-trivial gradient for the routing function (which model to use when). This
    decision boundary is controlled by the switch layer.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应简要讨论切换路由方法与MoE论文的不同之处。我提到这一点是因为微软似乎使用了切换路由而不是专家模型来节省计算复杂度，但我乐于接受纠正。当有多个专家模型时，它们可能对路由函数（什么时候使用哪个模型）有非平凡的梯度。这一决策边界由切换层控制。
- en: The benefits of the switch layer are threefold.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 切换层的好处有三方面。
- en: Routing computation is reduced if the token is being routed only to a single
    expert model
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果令牌仅路由到一个专家模型，则路由计算减少。
- en: The batch size (expert capacity) can be at least halved since a single token
    goes to a single model
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批处理大小（专家容量）可以至少减半，因为单个令牌只去一个模型。
- en: The routing implementation is simplified and communications are reduced.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 路由实现被简化，通信也减少了。
- en: The overlap of the same token to more than 1 expert model is called as the Capacity
    factor. Following is a conceptual depiction of how routing with different expert
    capacity factors works
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 同一令牌重叠到多个专家模型中被称为容量因子。以下是不同专家容量因子的路由工作原理的概念图
- en: '![GPT-4: 8 Models in One; The Secret is Out](../Images/6bf5e35af339d7ad184aaf6f25b171e6.png)illustration
    of token routing dynamics. Each expert processes a fixed batch-size'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![GPT-4: 8 Models in One; The Secret is Out](../Images/6bf5e35af339d7ad184aaf6f25b171e6.png)示意图展示了令牌路由的动态。每个专家处理一个固定的批处理大小'
- en: of tokens modulated by the capacity factor. Each token is routed to the expert
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由容量因子调制的令牌。每个令牌被路由到专家
- en: with the highest router probability, but each expert has a fixed batch size
    of
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 每个专家都有固定的批处理大小，但具有最高路由概率的专家可能会溢出。
- en: (total tokens/num experts) × capacity factor. If the tokens are unevenly dis-
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: （总令牌数/专家数）×容量因子。如果令牌分布不均，
- en: patched, then certain experts will overflow (denoted by dotted red lines), resulting
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果打补丁，则某些专家会溢出（用虚线红线表示），导致
- en: in these tokens not being processed by this layer. A larger capacity factor
    allevi-
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些令牌没有被该层处理的情况下。较大的容量因子缓解了
- en: ates this overflow issue but also increases computation and communication costs
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这种溢出问题得到了缓解，但也增加了计算和通信成本。
- en: (depicted by padded white/empty slots). (source [https://arxiv.org/pdf/2101.03961.pdf](https://arxiv.org/pdf/2101.03961.pdf))
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: （由填充的白色/空白插槽描绘）。 （来源 [https://arxiv.org/pdf/2101.03961.pdf](https://arxiv.org/pdf/2101.03961.pdf)）
- en: When compared with the MoE, findings from the MoE and Switch paper suggest that
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 与MoE相比，MoE和切换论文的发现表明
- en: Switch transformers outperform carefully tuned dense models and MoE transformers
    on a speed-quality basis.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换变换器在速度-质量基础上优于精心调优的密集模型和MoE变换器。
- en: Switch transformers have a smaller compute futprint than MoE
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换变换器比MoE具有更小的计算足迹。
- en: Switch transformers perform better at lower capacity factors (1–1.25).
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换变换器在较低的容量因子（1–1.25）下表现更好。
- en: '![GPT-4: 8 Models in One; The Secret is Out](../Images/e0bbf8c21655eb52300d42b844e5d205.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![GPT-4: 8 Models in One; The Secret is Out](../Images/e0bbf8c21655eb52300d42b844e5d205.png)'
- en: Concluding thoughts
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结思考
- en: Two caveats, first, that this is all coming from hearsay, and second, my understanding
    of these concepts is fairly feeble, so I urge readers to take it with a boulder
    of salt.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个警告：首先，这些都是听说的，其次，我对这些概念的理解相当薄弱，因此我建议读者对此持有极大的怀疑态度。
- en: But what did Microsoft achieve by keeping this architecture hidden? Well, they
    created a buzz, and suspense around it. This might have helped them to craft their
    narratives better. They kept innovation to themselves and avoided others catching
    up to them sooner. The whole idea was likely a usual Microsoft gameplan of thwarting
    competition while they invest 10B into a company.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 但微软通过隐藏这个架构实现了什么呢？他们制造了轰动效应和悬念。这可能帮助他们更好地塑造叙事。他们将创新留给自己，避免了其他人更早赶上他们。整个想法可能是微软惯有的策略：在投资100亿美元到一家公司时阻碍竞争。
- en: GPT-4 performance is great, but it was not an innovative or breakthrough design.
    It was an amazingly clever implementation of the methods developed by engineers
    and researchers topped up by an enterprise/capitalist deployment. OpenAI has neither
    denied or agreed to these claims ([https://thealgorithmicbridge.substack.com/p/gpt-4s-secret-has-been-revealed](https://thealgorithmicbridge.substack.com/p/gpt-4s-secret-has-been-revealed)),
    which makes me think that this architecture for GPT-4 is more than likely the
    reality (which is great!). Just not cool! We all want to know and learn.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4的表现很出色，但这不是一个创新或突破性的设计。这是对工程师和研究人员开发的方法的极其聪明的实施，结合了企业/资本家的部署。OpenAI既没有否认也没有同意这些说法（[https://thealgorithmicbridge.substack.com/p/gpt-4s-secret-has-been-revealed](https://thealgorithmicbridge.substack.com/p/gpt-4s-secret-has-been-revealed)），这让我认为GPT-4的架构更有可能就是现实（这很好！）。只是没那么酷！我们都想知道并学习。
- en: A huge credit goes to [Alberto Romero](https://medium.com/u/7ba6be8a3022?source=post_page-----e3d16fd1eee0--------------------------------)
    for bringing this news to the surface and investigating it further by reaching
    out to OpenAI (who did not respond as per the last update. I saw his article on
    Linkedin but the same has been published on Medium too.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[Alberto Romero](https://medium.com/u/7ba6be8a3022?source=post_page-----e3d16fd1eee0--------------------------------)为将这一消息公之于众并通过联系OpenAI进一步调查做出了巨大贡献（根据最新消息，OpenAI没有回应。我在LinkedIn上看到他的文章，但同样也发布在Medium上）。'
- en: '**[Dr. Mandar Karhade, MD. PhD.](https://www.linkedin.com/in/mandarkarhade/)**
    Sr. Director of Advanced Analytics and Data Strategy @Avalere Health. Mandar is
    an experienced Physician Scientist working on the cutting edge implementations
    of the AI to the Life Sciences and Health Care industry for 10+ years. Mandar
    is also part of AFDO/RAPS helping to regulate implantations of AI to the Healthcare.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Mandar Karhade 博士，医学博士](https://www.linkedin.com/in/mandarkarhade/)** 高级分析与数据战略总监
    @Avalere Health。Mandar 是一位经验丰富的医学科学家，拥有超过10年的前沿AI技术在生命科学和医疗保健行业中的应用经验。Mandar 还参与了AFDO/RAPS，帮助规范AI在医疗保健中的应用。'
- en: '[Original](https://medium.com/towards-artificial-intelligence/gpt-4-8-models-in-one-the-secret-is-out-e3d16fd1eee0).
    Reposted with permission.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/towards-artificial-intelligence/gpt-4-8-models-in-one-the-secret-is-out-e3d16fd1eee0)。经许可转载。'
- en: More On This Topic
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[HuggingGPT: The Secret Weapon to Solve Complex AI Tasks](https://www.kdnuggets.com/2023/05/hugginggpt-secret-weapon-solve-complex-ai-tasks.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HuggingGPT：解决复杂AI任务的秘密武器](https://www.kdnuggets.com/2023/05/hugginggpt-secret-weapon-solve-complex-ai-tasks.html)'
- en: '[Getting Started with LLMOps: The Secret Sauce Behind Seamless Interactions](https://www.kdnuggets.com/getting-started-with-llmops-the-secret-sauce-behind-seamless-interactions)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LLMOps入门：无缝互动背后的秘密武器](https://www.kdnuggets.com/getting-started-with-llmops-the-secret-sauce-behind-seamless-interactions)'
- en: '[The Not-so-Sexy SQL Concepts to Make You Stand Out](https://www.kdnuggets.com/2022/02/not-so-sexy-sql-concepts-stand-out.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[那些不那么性感的SQL概念，让你脱颖而出](https://www.kdnuggets.com/2022/02/not-so-sexy-sql-concepts-stand-out.html)'
- en: '[Map out your journey towards SAS Certification](https://www.kdnuggets.com/2022/11/sas-map-journey-towards-sas-certification.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[规划你获得SAS认证的旅程](https://www.kdnuggets.com/2022/11/sas-map-journey-towards-sas-certification.html)'
- en: '[Don''t Miss Out! Enroll in FREE Courses Before 2023 Ends](https://www.kdnuggets.com/dont-miss-out-enroll-in-free-courses-before-2023-ends)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[不要错过！在2023年结束前报名免费课程](https://www.kdnuggets.com/dont-miss-out-enroll-in-free-courses-before-2023-ends)'
- en: '[3 New Prompt Engineering Resources to Check Out](https://www.kdnuggets.com/3-new-prompt-engineering-resources)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3个新的Prompt Engineering资源](https://www.kdnuggets.com/3-new-prompt-engineering-resources)'
