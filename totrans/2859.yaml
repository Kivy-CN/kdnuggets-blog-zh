- en: 'Transfer Learning Made Easy: Coding a Powerful Technique'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习变得简单：编码一种强大的技术
- en: 原文：[https://www.kdnuggets.com/2019/11/transfer-learning-coding.html](https://www.kdnuggets.com/2019/11/transfer-learning-coding.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/11/transfer-learning-coding.html](https://www.kdnuggets.com/2019/11/transfer-learning-coding.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: Artificial Intelligence for the Average User
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 面向普通用户的人工智能
- en: Artificial intelligence (A.I.) is shaping up to be the [*most powerful and transformative
    technology to sweep the globe and touch all facets of life*](https://www.strategyand.pwc.com/uk/en/transformative-power-ai/transformative-power-artificial-intelligence.html) –
    economics, healthcare, finance, industry, socio-cultural interactions, etc. –
    in an unprecedented manner. This is even more important with developments in transfer
    learning and machine learning capabilities.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能（A.I.）正在成为[*最强大且具有变革性的全球性技术*](https://www.strategyand.pwc.com/uk/en/transformative-power-ai/transformative-power-artificial-intelligence.html)，以前所未有的方式影响全球经济、医疗、金融、工业、社会文化互动等各个方面。这一点在迁移学习和机器学习能力的发展中尤为重要。
- en: Already, we are using A.I. technologies on a daily basis, and it is impacting
    our lives and choices whether we consciously know it or not. From our Google search
    and Navigation, Netflix movie recommendations, Amazon purchase suggestions, voice
    assistants for daily tasks like Siri or Alexa, Facebook community building, medical
    diagnoses, credit score calculations, and mortgage decision making, etc., A.I.
    is only going to grow in adoption.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在日常生活中使用人工智能技术，无论我们是否意识到，它都在影响我们的生活和选择。从我们的Google搜索和导航、Netflix电影推荐、Amazon购买建议、日常任务的语音助手如Siri或Alexa、Facebook社区建设、医疗诊断、信用评分计算和按揭决策等，人工智能的应用只会不断增长。
- en: Most modern A.I. systems are currently being powered by a family of algorithms
    or techniques called [deep learning](https://machinelearningmastery.com/what-is-deep-learning/),
    which basically trains and builds [deep layers of neural networks](https://blog.exxactcorp.com/deep-learning-vs-machine-learning-vs-data-science-how-do-they-differ/) with
    various architectural configurations.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代人工智能系统目前由一类算法或技术驱动，即[深度学习](https://machinelearningmastery.com/what-is-deep-learning/)，它基本上训练和构建[具有不同架构配置的深层神经网络](https://blog.exxactcorp.com/deep-learning-vs-machine-learning-vs-data-science-how-do-they-differ/)。
- en: '![](../Images/208231dc1b5052d184e745923aeea657.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/208231dc1b5052d184e745923aeea657.png)'
- en: '*Image source: Fjodor van Veen – asimovinstitute.org.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片来源：Fjodor van Veen – asimovinstitute.org.*'
- en: After 50+ years of ebbs and flows, the deep learning revolution has caught the
    steam and looks unstoppable – fueled by Big Data technologies, innovation in hardware,
    and algorithms. Therefore, [deep learning networks are promising to impact and
    fundamentally alter](https://elitedatascience.com/machine-learning-impact) how
    we, humans, live, work, and play for at least the next few decades to come.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 经过50多年的起伏波动，深度学习革命已获得动力，看起来势不可挡——这得益于大数据技术、硬件创新和算法。因此，[深度学习网络有望影响并从根本上改变](https://elitedatascience.com/machine-learning-impact)我们人类在未来几十年的生活、工作和娱乐方式。
- en: '*So, we can finally see the promise of A.I. for everyone on the planet!*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*所以，我们终于可以看到人工智能为全球每个人带来的希望！*'
- en: '**However... there is a catch.**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**然而……有一个陷阱。**'
- en: Deep Learning is Expensive and the Focus is Narrow
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习昂贵且专注范围狭窄
- en: Deep learning networks tend to be resource-hungry and computationally expensive.
    Unlike traditional statistical learning models (such as regression, decision trees,
    or support vector machines), they tend to contain millions of parameters and therefore
    need a lot of training data to avoid overfitting.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习网络往往需要大量资源和计算成本。与传统的统计学习模型（如回归、决策树或支持向量机）不同，深度学习网络往往包含数百万个参数，因此需要大量的训练数据以避免过拟合。
- en: Therefore, deep learning models are trained with **massive amounts of high-dimensional
    raw data** such as images, unstructured text, or audio signals. Also, they employ **millions
    of vectorized computation** (e.g., matrix multiplication) over and over, to optimize
    the huge parameter set to fit the data. Moreover, they are built with a [large
    number of hyperparameters](https://towardsdatascience.com/hyperparameters-in-deep-learning-927f7b2084dd) (e.g.
    number of layers, neurons per layer, optimizer algorithm settings, etc.), and
    it often takes many weeks or months for a team of highly trained researchers to
    create a state-of-the-art model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，深度学习模型会使用**大量的高维原始数据**，如图像、非结构化文本或音频信号进行训练。此外，它们还反复进行**数百万次向量化计算**（例如矩阵乘法），以优化庞大的参数集以适应数据。而且，它们的构建涉及到一个[大量的超参数](https://towardsdatascience.com/hyperparameters-in-deep-learning-927f7b2084dd)（例如层数、每层的神经元数量、优化算法设置等），通常需要一个高水平研究团队花费数周或数月时间来创建一个最先进的模型。
- en: '*All of these lead to a great demand on the computational power needed to train
    and robust and high-performance deep learning model, optimized for a given task.*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有这些都对训练和优化特定任务的鲁棒且高性能的深度学习模型所需的计算能力提出了巨大的需求。*'
- en: Let’s say we can afford to train a great model after spending a huge amount
    of computational resources. **Don’t we want to re-use this model for the maximum
    number and variety of tasks and reap the benefit of our investment many times
    over**?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们可以在花费大量计算资源后训练出一个优秀的模型。**难道我们不希望将这个模型用于尽可能多的任务，并多次收获我们投资的回报吗？**
- en: '*Herein lies the problem.*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*这就是问题所在。*'
- en: Deep learning algorithms, so far, have been **traditionally designed to work
    in isolation**. These algorithms are trained to [solve specific tasks](https://bdtechtalks.com/2018/04/23/strong-ai-vs-weak-ai-deep-learning/).
    The models, in most cases, have to be **rebuilt from scratch** once the feature-space
    distribution changes.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，深度学习算法**传统上被设计为孤立工作**。这些算法被训练来[解决特定任务](https://bdtechtalks.com/2018/04/23/strong-ai-vs-weak-ai-deep-learning/)。在大多数情况下，一旦特征空间分布发生变化，模型就必须**从头开始重建**。
- en: But, this does not make sense, especially when it is compared to how we humans
    currently utilize our limited computation speed.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但这没有意义，尤其是与我们人类目前利用有限计算速度的方式相比。
- en: Humans have an [inherent ability to transfer knowledge across tasks](https://www.nap.edu/read/9853/chapter/6#52).
    What we acquire as knowledge while learning about one task, we utilize in the
    same way to solve related tasks. If the similarity between the tasks or domains
    is high, we are able to cross-utilize our ‘learned’ knowledge better.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 人类具有[跨任务转移知识的固有能力](https://www.nap.edu/read/9853/chapter/6#52)。我们在学习一个任务时获得的知识，可以以相同的方式来解决相关任务。如果任务或领域之间的相似性很高，我们能够更好地跨任务利用我们的“已学”知识。
- en: '*Transfer learning is the idea of overcoming the isolated learning paradigms
    and utilizing the knowledge acquired for one task to solve related ones, as applied
    to machine learning, and in particular, to the domain of deep learning.*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*转移学习的理念是克服孤立学习范式，并利用在一个任务中获得的知识来解决相关任务，这在机器学习，特别是在深度学习领域中得到了应用。*'
- en: '![](../Images/c6555dd2aeee750bd0f47985e3600e0d.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6555dd2aeee750bd0f47985e3600e0d.png)'
- en: '*Image source: [A Comprehensive Hands-on Guide to Transfer Learning with Real-World
    Applications in Deep Learning](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a).*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片来源：[转移学习综合实用指南：在深度学习中的实际应用](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a)。*'
- en: Transfer Learning for Deep Learning Networks
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习网络的转移学习
- en: There is a myriad of strategies to follow for the transfer learning process
    in the deep learning setting, and multiple important things to consider and engineering
    decisions to make – similarity of datasets and domains, supervised or unsupervised
    setting, how much retraining to be done, etc.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习环境中，转移学习过程有许多策略可供选择，还有多个重要因素需要考虑和工程决策需要做出——数据集和领域的相似性、监督或非监督设置、需要多少重新训练等。
- en: 'However, to put it very simply, we can assume that for transfer learning:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，简单来说，我们可以假设对于转移学习：
- en: We need to take a pre-trained deep learning model
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要使用一个预训练的深度学习模型
- en: Re-use all or certain portions of it
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新使用全部或某些部分
- en: Apply it to our new domain of interest for a particular machine learning problem–classification
    or regression.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将其应用于我们新感兴趣的领域中的特定机器学习问题——分类或回归。
- en: In this way, we are able to avoid a large portion of the huge computational
    effort of training and optimizing a large deep learning model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以避免训练和优化大型深度学习模型所需的大量计算工作。
- en: '*In the end, a trained deep learning model is just a collection of millions
    of real numbers in a particular data structure format, which can be used readily
    for prediction/inference, the task we are really interested in, as the consumers
    of the model.*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*最终，一个训练好的深度学习模型只是一组在特定数据结构格式下的数百万个实数，这些可以直接用于预测/推断，这是我们作为模型使用者真正感兴趣的任务。*'
- en: But remember that a pre-trained model might have been trained using a particular
    classification in mind, i.e. its output vector and computation graph is suited
    for prediction of a particular task only.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 但请记住，预训练模型可能是针对特定分类进行训练的，即其输出向量和计算图只适用于预测某一特定任务。
- en: 'Therefore, a widely used strategy in transfer learning is to:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，迁移学习中广泛使用的策略是：
- en: Load the weights matrices of a pre-trained model except for the weights of the
    very last layers near the O/P,
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载预训练模型的权重矩阵，但不包括离输出层最近的最后几层的权重，
- en: Hold those weights fixed, i.e. untrainable
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持这些权重固定，即不可训练
- en: Attach new layers suitable for the task at hand, and train the model with new
    data
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 附加适合当前任务的新层，并使用新数据训练模型
- en: '![](../Images/4513886ec76fb18b9b3ca3a77712aeaa.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4513886ec76fb18b9b3ca3a77712aeaa.png)'
- en: '*Fig: The transfer learning strategy for deep learning networks, as we will
    explore here.*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*图：我们将在这里探索的深度学习网络迁移学习策略。*'
- en: This way, we don’t have to train the whole model, we get to **repurpose the
    model for our specific machine learning task**, yet can leverage the learned structures
    and patterns of the data, contained in the fixed weights, which are loaded from
    the pre-trained, optimized model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们无需训练整个模型，就可以**将模型重新用于我们的特定机器学习任务**，同时利用从预训练、优化模型中加载的固定权重中包含的数据结构和模式。
- en: A Hands-On Example You Can Run on Your Laptop
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个你可以在笔记本电脑上运行的实用示例
- en: Let’s get our hands dirty and build a simple code demo to demonstrate the power
    of transfer learning, shall we?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们动手构建一个简单的代码演示，展示迁移学习的力量，好吗？
- en: '**Why not follow a traditional approach?**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么不遵循传统方法？**'
- en: Now, traditionally, tutorials on this topic have focused on learning from famous,
    high-performance deep learning networks, such as [VGGNet-16](https://neurohive.io/en/popular-networks/vgg16/), [ResNet-50](https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624),
    or [Inception-V3/V4](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202),
    etc. These networks were trained on the massive [ImageNet database](http://www.image-net.org/),
    and secured top places in the [annual ImageNet competition – ILSVRC](http://www.image-net.org/challenges/LSVRC/),
    thereby positioning themselves as the golden benchmark models for image classification
    tasks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，这个主题的教程集中在从著名的高性能深度学习网络中学习，例如[VGGNet-16](https://neurohive.io/en/popular-networks/vgg16/)、[ResNet-50](https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624)
    或[Inception-V3/V4](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)
    等。这些网络在庞大的[ImageNet数据库](http://www.image-net.org/)上进行了训练，并在[年度ImageNet比赛 - ILSVRC](http://www.image-net.org/challenges/LSVRC/)中获得了前列，从而成为图像分类任务的黄金基准模型。
- en: However, the issue with these networks is that they contain a large number of
    complex layers, and not easy to understand when you are starting to learn deep
    learning concepts.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些网络的问题在于它们包含大量复杂的层，当你开始学习深度学习概念时，不容易理解。
- en: '*Therefore, if you want to code up a transfer learning example from scratch,
    it may be beneficial, from a self-learning and confidence-building point of view,
    to try an independent example first. You can train a deep learning model first,
    transfer its learning to another seed network, and then show the performance on
    a standard classification task.*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*因此，如果你想从零开始编写一个迁移学习的示例，从自学和建立信心的角度来看，先尝试一个独立的示例可能会更有益。你可以先训练一个深度学习模型，将其学习转移到另一个种子网络，然后展示在标准分类任务中的性能。*'
- en: '**What are we going to do in this tutorial?**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们在这个教程中要做什么？**'
- en: 'In this article, we will demonstrate the transfer learning concept in a very
    simple setting using Python, working with the Keras package (TensorFlow backend).
    We will take the well-known CIFAR-10 dataset and do the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将使用 Python 和 Keras 包（TensorFlow 后端）在一个非常简单的环境中演示迁移学习概念。我们将使用著名的 CIFAR-10
    数据集并进行以下操作：
- en: Create a Keras neural net by stacking a set of classificiation_layers on top
    of a set of feature_layers
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在一组特征层之上堆叠一组分类层来创建一个 Keras 神经网络
- en: Train the resulting network on a partial CIFAR-10 dataset, consisting of examples
    from the first 5 categories (0…4).
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在由前 5 类 (0…4) 的示例组成的部分 CIFAR-10 数据集上训练生成的网络。
- en: Freeze the feature_layers and stack a new set of fully-connected layers on top
    of them, thereby creating another conv net
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冻结特征层并在其上堆叠一组新的全连接层，从而创建另一个卷积网络
- en: Train this new conv net on the examples from the rest of the categories (5..9)
    of CIFAR-10, tuning weights only for those densely connected layers
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 CIFAR-10 的其余类别 (5..9) 的示例上训练这个新的卷积网络，只调整那些密集连接层的权重
- en: The entire code is open-source and [can be found here](https://github.com/tirthajyoti/Deep-learning-with-Python/blob/master/Notebooks/Transfer_learning_CIFAR.ipynb).
    We show only some essential parts of the code here in this article.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 整个代码是开源的，[可以在这里找到](https://github.com/tirthajyoti/Deep-learning-with-Python/blob/master/Notebooks/Transfer_learning_CIFAR.ipynb)。我们在本文中只展示了部分重要的代码。
- en: '**Code Along**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码示例**'
- en: 'We start by importing the necessary libraries and functions:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入必要的库和函数：
- en: from time import time
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: from time import time
- en: import keras
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: import keras
- en: from keras.datasets import mnist,cifar10
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: from keras.datasets import mnist,cifar10
- en: from keras.models import Sequential
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: from keras.models import Sequential
- en: from keras.layers import Dense, Dropout, Activation, Flatten
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: from keras.layers import Dense, Dropout, Activation, Flatten
- en: from keras.layers import Conv2D, MaxPooling2D
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: from keras.layers import Conv2D, MaxPooling2D
- en: from keras.optimizers import Adam
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: from keras.optimizers import Adam
- en: from keras import backend as K
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: from keras import backend as K
- en: import matplotlib.pyplot as plt
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: import random
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: import random
- en: 'Next, we decide some architectural choices for the deep learning models. We
    will use the convolutional neural net (CNN), as it is the most suited for image
    classification task:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们决定一些深度学习模型的架构选择。我们将使用卷积神经网络 (CNN)，因为它最适合图像分类任务：
- en: number of convolutional filters to use
  id: totrans-66
  prefs:
  - PREF_OL
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要使用的卷积滤波器数量
- en: filters = 64
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: filters = 64
- en: size of pooling area for max pooling
  id: totrans-68
  prefs:
  - PREF_OL
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大池化的池化区域大小
- en: pool_size = 2
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: pool_size = 2
- en: convolution kernel size
  id: totrans-70
  prefs:
  - PREF_OL
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积核大小
- en: kernel_size = 3
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: kernel_size = 3
- en: Next, we split the dataset into training and validation sets, and create two
    datasets – one with class labels below 5 and one with 5 and above. Why do we do
    that?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将数据集拆分为训练集和验证集，并创建两个数据集——一个包含标签低于 5 的类别，另一个包含 5 及以上的类别。为什么要这样做？
- en: The whole CIFAR-10 dataset has 10 categories of images in a very small size.** We
    will have two neural networks**. One will be pre-trained, and the learning will
    be transferred to a second network. But, we will not use all the categories of
    image to train both networks. **The first one will be trained only using first
    5 categories of images and this learning will help the second network to learn
    the last 5 categories of images faster**.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 整个 CIFAR-10 数据集有 10 类图像，尺寸非常小。**我们将有两个神经网络**。一个将被预训练，学习将转移到第二个网络。但是，我们不会使用所有的图像类别来训练这两个网络。**第一个网络将仅使用前
    5 类图像进行训练，这一学习将帮助第二个网络更快地学习最后 5 类图像**。
- en: Therefore,
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: '![](../Images/2d79361e3f786d6866899a439699885b.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d79361e3f786d6866899a439699885b.png)'
- en: '*All 10 categories of images in the CIFAR-10 dataset.*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*CIFAR-10 数据集中的 10 类图像。*'
- en: Here are some random images from the first 5 categories, which the first neural
    network will ‘see’ and be trained on. The categories are – **airplane, automobile,
    bird, cat,** or **deer.**
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是前 5 类的一些随机图像，第一个神经网络将“看到”并进行训练。这些类别是——**飞机、汽车、鸟、猫**或**鹿**。
- en: '![](../Images/37af3bf1cfccd33cb5a0ec7f18282f57.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37af3bf1cfccd33cb5a0ec7f18282f57.png)'
- en: '*Fig: First 5 categories of images, seen only by the first neural network.*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*图：前 5 类图像，仅被第一个神经网络看到。*'
- en: But we are actually interested in building a neural net for the last 5 categories
    of images – **dog, frog, horse, sheep, **or** truck**.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们实际上感兴趣的是为最后 5 类图像构建一个神经网络——**狗、青蛙、马、羊**或**卡车**。
- en: '![](../Images/8d2624dfcc6e5e50b7d33aa383dd7dcf.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d2624dfcc6e5e50b7d33aa383dd7dcf.png)'
- en: '*Fig: Last 5 categories of images, seen only by the second neural network.*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*图：最后 5 类图像，仅被第二个神经网络看到。*'
- en: 'Next, we define two groups/types of layers: feature (convolutions) and classification
    (dense).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义两组/类型的层：特征层（卷积）和分类层（密集）。
- en: Again, do not be bothered about the implementation details of these code snippets.
    You can learn the details from any standard tutorial on Keras package. The idea
    is to understand the concept.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，请不要被这些代码片段的实现细节困扰。你可以从任何标准的Keras教程中了解细节。关键是理解概念。
- en: 'The feature layers:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 特征层：
- en: feature_layers = [
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征层 = [
- en: '**Conv2D**(filters, kernel_size,'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Conv2D**(filters, kernel_size,'
- en: padding='valid',
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: padding='valid',
- en: input_shape=input_shape),
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: input_shape=input_shape),
- en: '**Activation**(''relu''),'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Activation**(''relu''),'
- en: '**Conv2D**(filters, kernel_size),'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Conv2D**(filters, kernel_size),'
- en: '**Activation**(''relu''),'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Activation**(''relu''),'
- en: '**MaxPooling2D**(pool_size=pool_size),'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**MaxPooling2D**(pool_size=pool_size),'
- en: '**Dropout**(25),'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Dropout**(25),'
- en: '**Flatten**(),'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Flatten**(),'
- en: ']'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ']'
- en: 'The dense classification layer:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 密集分类层：
- en: classification_layers = [
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分类层 = [
- en: '**Dense**(128),'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Dense**(128),'
- en: '**Activation**(''relu''),'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Activation**(''relu''),'
- en: '**Dropout**(25),'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Dropout**(25),'
- en: '**Dense**(num_classes),'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Dense**(num_classes),'
- en: '**Activation**(''softmax'')'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Activation**(''softmax'')'
- en: ']'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ']'
- en: Next, we Create the complete model by stacking together **feature_layers** and **classification_layers**.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过将**特征层**和**分类层**堆叠在一起创建完整模型。
- en: model_1 = **Sequential**(feature_layers + classification_layers)
  id: totrans-106
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: model_1 = **Sequential**(特征层 + 分类层)
- en: 'We then define a function for training the model (not shown) and just train
    the model for certain number of epochs to reach a good enough performance:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义一个用于训练模型的函数（未显示），并训练模型若干轮以达到足够好的性能：
- en: '**train_model**(model_1,'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**train_model**(model_1,'
- en: (x_train_lt5, y_train_lt5),
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (x_train_lt5, y_train_lt5),
- en: (x_test_lt5, y_test_lt5), num_classes)
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (x_test_lt5, y_test_lt5), 类别数)
- en: 'We can show how the accuracy of the network evolved over training epochs:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以展示网络在训练轮次中的准确度演变：
- en: '![](../Images/e35416277e8b1316d8d807f8a6b4cb6d.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e35416277e8b1316d8d807f8a6b4cb6d.png)'
- en: '*Fig: Validation set accuracy over epochs while training the first network.*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*图：训练第一个网络时验证集准确度随轮次变化。*'
- en: Next, we **freeze feature layers and rebuild the model**.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们**冻结特征层并重建模型**。
- en: This freezing of feature layers is at the heart of transfer learning. This allows
    re-use of the pre-trained model for classification tasks because users can just
    stack up new fully-connected layers on top of the pre-trained feature layers and
    get good performance.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 冻结特征层是迁移学习的核心。这允许重新使用预训练模型进行分类任务，因为用户可以在预训练特征层上堆叠新的全连接层，从而获得良好的性能。
- en: 'We will create a fresh new model called **model_2** with the **untrainable** **feature_layers** and **trainable** **classification_layers**. We
    show the summary in the figure below:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个名为**model_2**的新模型，具有**不可训练**的**特征层**和**可训练**的**分类层**。我们在下图中展示了摘要：
- en: 'for l in feature_layers:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于特征层中的每一层：
- en: trainable = False
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可训练 = False
- en: model_2 = **Sequential**(feature_layers + classification_layers)
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: model_2 = **Sequential**(特征层 + 分类层)
- en: '![](../Images/0e2f59f900013b67c13f41b64fa377c1.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e2f59f900013b67c13f41b64fa377c1.png)'
- en: '*Fig: The model summary of the second network showing the fixed and trainable
    weights. The fixed weights are transferred directly from the first network.*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*图：第二个网络的模型摘要，显示了固定和可训练的权重。固定权重直接从第一个网络转移过来。*'
- en: Now we train the second model and observe **how it takes less overall time and
    still gets equal or higher performance**.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们训练第二个模型并观察**它总体上所需时间更少且性能相当或更高**。
- en: '**train_model**(model_2,'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**train_model**(model_2,'
- en: (x_train_gte5, y_train_gte5),(x_test_gte5, y_test_gte5), num_classes)
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (x_train_gte5, y_train_gte5),(x_test_gte5, y_test_gte5), 类别数)
- en: The accuracy of the second model is even higher than the first model, although
    this may not be the case all the time, and depends on the model architecture and
    dataset.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个模型的准确率甚至高于第一个模型，尽管这并不总是如此，且取决于模型架构和数据集。
- en: '![](../Images/f74b9408a1d84fe41383f4fb5a738b53.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f74b9408a1d84fe41383f4fb5a738b53.png)'
- en: '*Fig: Validation set accuracy over epochs while training the second network.*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*图：训练第二个网络时验证集准确度随轮次变化。*'
- en: 'The time taken for training two models are shown below:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 两个模型的训练时间如下所示：
- en: '![](../Images/5dec718a5302933f8930708ff6d0b97d.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5dec718a5302933f8930708ff6d0b97d.png)'
- en: '*Fig: Training time for the two networks.*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*图：两个网络的训练时间。*'
- en: What Did We Achieve?
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们达成了什么？
- en: Not only did the **model_2** train faster than **model_1**, it also started
    at a higher baseline accuracy and achieved better final accuracy for the same
    number of epochs and identical hyperparameters (learning rate, optimizer, batch
    size, etc.). And it achieved this training on images that were not seen by **model_1**.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅**model_2**的训练速度比**model_1**更快，而且它还以更高的基准准确度开始，并在相同的训练轮次和相同的超参数（学习率、优化器、批量大小等）下达到了更好的最终准确度。而且，它在**model_1**未见过的图像上完成了训练。
- en: This means that although **model_1** was trained on images of – **airplane,
    automobile, bird, cat,** or **deer** – it’s learned weights, when transferred
    to **model_2**, helped **model_2** achieve excellent performance on the classification
    of completely different categories of images – **dog, frog, horse, sheep, **or** truck**.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着虽然**model_1**是在**airplane、automobile、bird、cat**或**deer**的图像上训练的，但它学到的权重在转移到**model_2**时，帮助**model_2**在完全不同类别的图像分类中表现出色——**dog、frog、horse、sheep**或**truck**。
- en: Isn’t that amazing? And you can now build this kind of transfer learning with
    so few lines of codes. Again, the entire code is open-source and [can be found
    here](https://github.com/tirthajyoti/Deep-learning-with-Python/blob/master/Notebooks/Transfer_learning_CIFAR.ipynb).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 难道这不令人惊讶吗？现在，你可以用这么少的代码行来构建这种转移学习。再次强调，所有代码都是开源的，[可以在这里找到](https://github.com/tirthajyoti/Deep-learning-with-Python/blob/master/Notebooks/Transfer_learning_CIFAR.ipynb)。
- en: '[Original](https://blog.exxactcorp.com/transfer-learning-made-easy/). Reposted
    with permission.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://blog.exxactcorp.com/transfer-learning-made-easy/)。经许可转载。'
- en: '**Related:**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Building NLP Classifiers Cheaply With Transfer Learning and Weak Supervision](https://www.kdnuggets.com/2019/03/building-nlp-classifiers-cheaply-transfer-learning-weak-supervision.html)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用迁移学习和弱监督廉价构建 NLP 分类器](https://www.kdnuggets.com/2019/03/building-nlp-classifiers-cheaply-transfer-learning-weak-supervision.html)'
- en: '[Recycling Deep Learning Models with Transfer Learning](https://www.kdnuggets.com/2015/08/recycling-deep-learning-representations-transfer-ml.html)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用迁移学习回收深度学习模型](https://www.kdnuggets.com/2015/08/recycling-deep-learning-representations-transfer-ml.html)'
- en: '[The State of Transfer Learning in NLP](https://www.kdnuggets.com/2019/09/state-transfer-learning-nlp.html)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NLP 中的迁移学习现状](https://www.kdnuggets.com/2019/09/state-transfer-learning-nlp.html)'
- en: '* * *'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的 IT 工作'
- en: '* * *'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[TensorFlow for Computer Vision - Transfer Learning Made Easy](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow 在计算机视觉中的应用 - 迁移学习简化版](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)'
- en: '[Parallel Processing in Prompt Engineering: The Skeleton-of-Thought…](https://www.kdnuggets.com/parallel-processing-in-prompt-engineering-the-skeleton-of-thought-technique)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提示工程中的并行处理：思维骨架…](https://www.kdnuggets.com/parallel-processing-in-prompt-engineering-the-skeleton-of-thought-technique)'
- en: '[LLaMA 3: Meta’s Most Powerful Open-Source Model Yet](https://www.kdnuggets.com/llama-3-metas-most-powerful-open-source-model-yet)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LLaMA 3：Meta 迄今为止最强大的开源模型](https://www.kdnuggets.com/llama-3-metas-most-powerful-open-source-model-yet)'
- en: '[Machine Learning Made Simple for Data Analysts with BigQuery ML](https://www.kdnuggets.com/machine-learning-made-simple-for-data-analysts-with-bigquery-ml)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用 BigQuery ML 简化数据分析师的机器学习](https://www.kdnuggets.com/machine-learning-made-simple-for-data-analysts-with-bigquery-ml)'
- en: '[The Mistake Every Data Scientist Has Made at Least Once](https://www.kdnuggets.com/2022/09/mistake-every-data-scientist-made-least.html)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个数据科学家至少犯过的一个错误](https://www.kdnuggets.com/2022/09/mistake-every-data-scientist-made-least.html)'
- en: '[Combining Pandas DataFrames Made Simple](https://www.kdnuggets.com/2022/09/combining-pandas-dataframes-made-simple.html)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[简化 Pandas DataFrames 的合并](https://www.kdnuggets.com/2022/09/combining-pandas-dataframes-made-simple.html)'
