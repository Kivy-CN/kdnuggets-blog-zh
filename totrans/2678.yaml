- en: Doing the impossible? Machine learning with less than one example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/11/machine-learning-less-than-one-example.html](https://www.kdnuggets.com/2020/11/machine-learning-less-than-one-example.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e18f71d0759fc5c1745c0815d776efed.png)'
  prefs: []
  type: TYPE_IMG
- en: '*“Less-than-one-shot learning” enables machine learning algorithms to classify
    N labels with less than N training examples.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If I told you to imagine something between a horse and a bird—say, a flying
    horse—would you need to see a concrete example? Such a creature does not exist,
    but nothing prevents us from using our imagination to create one: the Pegasus.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d717c55796dfc05b6fc7b6bf5b00934d.png)'
  prefs: []
  type: TYPE_IMG
- en: The human mind has all kinds of mechanisms to create new concepts by combining
    abstract and concrete knowledge it has of the real world. We can imagine existing
    things that we might have never seen (a horse with a long neck—a giraffe), as
    well as things that do not exist in real life (a winged serpent that breathes
    fire—a dragon). This cognitive flexibility allows us to learn new things with
    few and sometimes no new examples.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, [machine learning](https://bdtechtalks.com/2017/08/28/artificial-intelligence-machine-learning-deep-learning/) and [deep
    learning](https://bdtechtalks.com/2019/02/15/what-is-deep-learning-neural-networks/),
    the current leading fields of artificial intelligence, are known to require many
    examples to learn new tasks, even when they are related to things they already
    know.
  prefs: []
  type: TYPE_NORMAL
- en: Overcoming this challenge has led to a host of research work and innovation
    in machine learning. And although we are still far from creating artificial intelligence
    that can replicate [the brain’s capacity for understanding](https://bdtechtalks.com/2020/07/13/ai-barrier-meaning-understanding/),
    the progress in the field is remarkable.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, [transfer learning](https://bdtechtalks.com/2019/06/10/what-is-transfer-learning/) is
    a technique that enables developers to finetune an artificial neural network for
    a new task without the need for many training examples. Few-shot and [one-shot
    learning](https://bdtechtalks.com/2020/08/12/what-is-one-shot-learning/) enable
    a machine learning model trained on one task to perform a related task with a
    single or very few new examples. For instance, if you have an image classifier
    trained to detect volleyballs and soccer balls, you can use one-shot learning
    to add basketball to the list of classes it can detect.
  prefs: []
  type: TYPE_NORMAL
- en: A new technique dubbed “less-than-one-shot learning” (or LO-shot learning),
    recently developed by AI scientists at the University of Waterloo, takes one-shot
    learning to the next level. The idea behind LO-shot learning is that to train
    a machine learning model to detect M classes, you need less than one sample per
    class. The technique, introduced in [a paper published in the arXiv preprocessor](https://arxiv.org/abs/2009.08449),
    by [Ilia Sucholutsky](https://ilia10000.github.io/) and [Matthias Schonlau](https://uwaterloo.ca/statistics-and-actuarial-science/people-profiles/matthias-schonlau),
    is still in its early stages but shows promise and can be useful in various scenarios
    where there is not enough data or too many classes.
  prefs: []
  type: TYPE_NORMAL
- en: The k-NN classifier
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/407814ec1d5b83e6cc6d2180e4aad3af.png)'
  prefs: []
  type: TYPE_IMG
- en: '*The k-NN machine learning algorithm classifies data by finding the closest
    instances.*'
  prefs: []
  type: TYPE_NORMAL
- en: The LO-shot learning technique proposed by the researchers applies to the “k-nearest
    neighbors” machine learning algorithm. K-NN can be used for both classification
    (determining the category of an input) or regression (predicting the outcome of
    an input) tasks. But for the sake of this discussion, we’ll stick to classification.
  prefs: []
  type: TYPE_NORMAL
- en: As the name implies, k-NN classifies input data by comparing it to its *k *nearest
    neighbors (*k *is an adjustable parameter). Say you want to create a k-NN machine
    learning model that classifies handwritten digits. First, you provide it with
    a set of labeled images of digits. Then, when you provide the model with a new,
    unlabeled image, it will determine its class by looking at its nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if you set *k *to 5, the machine learning model will find the
    five most similar digit photos for each new input. If, say, three of them belong
    to the class “7,” then it will classify the image as the digit seven.
  prefs: []
  type: TYPE_NORMAL
- en: k-NN is an “instance-based” machine learning algorithm. As you provide it with
    more labeled examples of each class, its accuracy improves, but its performance
    degrades because each new sample adds new comparisons operations.
  prefs: []
  type: TYPE_NORMAL
- en: In their LO-shot learning paper, the researchers showed that you could achieve
    accurate results with k-NN while providing fewer examples than classes. “We propose
    ‘less than one’-shot learning (LO-shot learning), a setting where a model must
    learn *N* new classes given only *M < N* examples, less than one example per class,”
    the AI researchers write. “At first glance, this appears to be an impossible task,
    but we both theoretically and empirically demonstrate feasibility.”
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning with less than one example per class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The classic k-NN algorithm provides “hard labels,” which means for every input,
    it provides exactly one class to which it belongs. Soft labels, on the other hand,
    provide the probability that an input belongs to each of the output classes (e.g.,
    there’s a 20% chance it’s a “2”, 70% chance it’s a “5,” and a 10% chance it’s
    a “3”).
  prefs: []
  type: TYPE_NORMAL
- en: In their work, the AI researchers at the University of Waterloo explored whether
    they could use soft labels to generalize the capabilities of the k-NN algorithm.
    The proposition of LO-shot learning is that soft label prototypes should allow
    the machine learning model to classify *N *classes with less than *N *labeled
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: The technique builds on previous work the researchers had done on [soft labels
    and data distillation](https://arxiv.org/abs/1910.02551). “Dataset distillation
    is a process for producing small synthetic datasets that train models to the same
    accuracy as training them on the full training set,” Ilia Sucholutsky, co-author
    of the paper, told *TechTalks*. “Before soft labels, dataset distillation was
    able to represent datasets like MNIST using as few as one example per class. I
    realized that adding soft labels meant I could actually represent MNIST using
    less than one example per class.”
  prefs: []
  type: TYPE_NORMAL
- en: '[MNIST](http://yann.lecun.com/exdb/mnist/) is a database of images of handwritten
    digits often used in training and testing machine learning models. Sucholutsky
    and his colleague Matthias Schonlau managed to achieve above-90 percent accuracy
    on MNIST with just five synthetic examples on the [convolutional neural network](https://bdtechtalks.com/2020/01/06/convolutional-neural-networks-cnn-convnets/) LeNet.'
  prefs: []
  type: TYPE_NORMAL
- en: “That result really surprised me, and it’s what got me thinking more broadly
    about this LO-shot learning setting,” Sucholutsky said.
  prefs: []
  type: TYPE_NORMAL
- en: Basically, LO-shot uses soft labels to create new classes by partitioning the
    space between existing classes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f6bf799d37192639d351108ea722fb60.png)'
  prefs: []
  type: TYPE_IMG
- en: '*LO-shot learning uses soft labels to partition the space between existing
    classes.*'
  prefs: []
  type: TYPE_NORMAL
- en: In the example above, there are two instances to tune the machine learning model
    (shown with black dots). A classic k-NN algorithm would split the space between
    the two dots between the two classes. But the “soft-label prototype k-NN” (SLaPkNN)
    algorithm, as the OL-shot learning model is called, creates a new space between
    the two classes (the green area), which represents a new label (think horse with
    wings). Here we have achieved *N *classes with *N-1* samples.
  prefs: []
  type: TYPE_NORMAL
- en: In the paper, the researchers show that LO-shot learning can be scaled up to
    detect *3N-2* classes using *N* labels and even beyond.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/180a91d5144256abca75295a6bfa6261.png)'
  prefs: []
  type: TYPE_IMG
- en: '*LO-shot learning can be extended to obtain multiple classes per instance.
    Left: 10 classes obtained from four instances. Right: 13 classes obtained from
    five instances.*'
  prefs: []
  type: TYPE_NORMAL
- en: In their experiments, Sucholutsky and Schonlau found that with the right configurations
    for the soft labels, LO-shot machine learning can provide reliable results even
    when you have noisy data.
  prefs: []
  type: TYPE_NORMAL
- en: “I think LO-shot learning can be made to work from other sources of information
    as well—similar to how many zero-shot learning methods do—but soft labels are
    the most straightforward approach,” Sucholutsky said, adding that there are already
    several methods that can find the right soft labels for LO-shot machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: While the paper displays the power of LO-shot learning with the k-NN classifier,
    Sucholutsky says the technique applies to other machine learning algorithms as
    well. “The analysis in the paper focuses specifically on k-NN just because it’s
    easier to analyze, but it should work for any classification model that can make
    use of soft labels,” Sucholutsky said. The researchers will soon release a more
    comprehensive paper that shows the application of LO-shot learning to deep learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: New venues for machine learning research
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/c13bb4a384a89236bc87fe06934c78b6.png)'
  prefs: []
  type: TYPE_IMG
- en: “For instance-based algorithms like k-NN, the efficiency improvement of LO-shot
    learning is quite large, especially for datasets with a large number of classes,”
    Susholutsky said. “More broadly, LO-shot learning is useful in any kind of setting
    where a classification algorithm is applied to a dataset with a large number of
    classes, especially if there are few, or no, examples available for some classes.
    Basically, most settings where zero-shot learning or few-shot learning are useful,
    LO-shot learning can also be useful.”
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a [computer vision](https://bdtechtalks.com/2019/01/14/what-is-computer-vision/) system
    that must identify thousands of objects from images and video frames can benefit
    from this machine learning technique, especially if there are no examples available
    for some of the objects. Another application would be to tasks that naturally
    have soft-label information, like [natural language processing](https://bdtechtalks.com/2018/02/20/ai-machine-learning-nlg-nlp/) systems
    that perform sentiment analysis (e.g., a sentence can be both sad and angry simultaneously).
  prefs: []
  type: TYPE_NORMAL
- en: In their paper, the researchers describe “less than one”-shot learning as “a
    viable new direction in machine learning research.”
  prefs: []
  type: TYPE_NORMAL
- en: “We believe that creating a soft-label prototype generation algorithm that specifically
    optimizes prototypes for LO-shot learning is an important next step in exploring
    this area,” they write.
  prefs: []
  type: TYPE_NORMAL
- en: “Soft labels have been explored in several settings before. What’s new here
    is the extreme setting in which we explore them,” Susholutsky said. “I think it
    just wasn’t a directly obvious idea that there is another regime hiding between
    one-shot and zero-shot learning.”
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://bdtechtalks.com/2020/10/01/less-than-one-shot-machine-learning/).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Introduction to the K-nearest Neighbour Algorithm Using Examples](https://www.kdnuggets.com/2020/04/introduction-k-nearest-neighbour-algorithm-using-examples.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AI Papers to Read in 2020](https://www.kdnuggets.com/2020/09/ai-papers-read-2020.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13 must-read papers from AI experts](https://www.kdnuggets.com/2020/05/13-must-read-papers-ai-experts.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Machine Learning Algorithms Explained in Less Than 1 Minute Each](https://www.kdnuggets.com/2022/07/machine-learning-algorithms-explained-less-1-minute.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, July 20: Machine Learning Algorithms Explained in…](https://www.kdnuggets.com/2022/n29.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Multi-modal deep learning in less than 15 lines of code](https://www.kdnuggets.com/2023/01/predibase-multi-modal-deep-learning-less-15-lines-code.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Become a Business Intelligence Analyst in Less Than 6 Months](https://www.kdnuggets.com/become-a-business-intelligence-analyst-in-less-than-6-months)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Why You Need To Learn More Than One Programming Language!](https://www.kdnuggets.com/2022/06/need-learn-one-programming-language.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Black Friday Deal - Master Machine Learning for Less with DataCamp](https://www.kdnuggets.com/2022/11/datacamp-black-friday-deal-master-machine-learning-less-datacamp.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
