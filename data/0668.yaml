- en: How to do Everything in Computer Vision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在计算机视觉中做一切
- en: 原文：[https://www.kdnuggets.com/2019/02/everything-computer-vision.html](https://www.kdnuggets.com/2019/02/everything-computer-vision.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/02/everything-computer-vision.html](https://www.kdnuggets.com/2019/02/everything-computer-vision.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)![Figure](../Images/8e51a1b5b1c74e6f15f1ee81b3c1b006.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)![图](../Images/8e51a1b5b1c74e6f15f1ee81b3c1b006.png)'
- en: Mask-RCNN doing object detection and instance segmentation
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Mask-RCNN进行对象检测和实例分割
- en: 'Want to do Computer Vision? Deep Learning is the way to go these days. Large
    scale datasets plus the representational power of deep Convolutional Neural Networks
    (CNNs) make for super accurate and robust models. Only one challenge still remains:
    how to design your model.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 想做计算机视觉吗？如今深度学习是实现这一目标的途径。大规模的数据集加上深度卷积神经网络（CNN）的表现能力，构建了超级准确和稳健的模型。唯一仍然存在的挑战是：如何设计你的模型。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的IT需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'With a field as broad and complex as computer vision, the solution isn’t always
    clear. The many standard tasks in computer vision all require special consideration:
    classification, detection, segmentation, pose estimation, enhancement and restoration,
    and action recognition. Although the state-of-the-art networks used for each of
    them exhibit common patterns, they’ll all still need their own unique design twist.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉这样一个广泛而复杂的领域中，解决方案并不总是明确的。计算机视觉中的许多标准任务都需要特别考虑：分类、检测、分割、姿态估计、增强与修复以及动作识别。虽然用于每种任务的最先进网络都展示了共同的模式，但它们仍然需要独特的设计调整。
- en: So how can we build models for all of those different tasks?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何为所有这些不同的任务构建模型呢？
- en: Let me show you how to do everything in Computer Vision with Deep Learning!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我向你展示如何用深度学习做计算机视觉中的一切！
- en: Classification
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类
- en: The most well known of them all! Image classification networks start with an
    input of *fixed size. *The input image can have any number of channels, but is
    usually 3 for an RGB image. When you design your network, the resolution can technically
    be any size as long as it is large enough to support the amount of downsampling
    you will do throughout the network. For example, if you downsample 4 times within
    the network, then your input needs to at least be 4² = 16 x 16 pixels in size.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名的！图像分类网络以*固定大小的输入*开始。输入图像可以有任意数量的通道，但RGB图像通常为3。当你设计网络时，分辨率在技术上可以是任何大小，只要足够大以支持网络中所有的下采样。例如，如果你在网络中下采样4次，那么你的输入图像大小至少需要为4²
    = 16 x 16像素。
- en: As you go deeper into the network the spatial resolution will decrease as we
    try to squeeze all of that information and get down to a 1-dimensional vector
    representation. To insure that the network always has the capacity to carry forward
    all of the information it extracts, we increase the number of feature maps proportionally
    to the depth to accommodate the reduction in spatial resolution. I.e we are losing
    spatial information in our downsampling process, and to accommodate the loss we
    expand our feature maps to increase our semantic information.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你深入网络，空间分辨率会降低，因为我们试图将所有信息压缩成一个一维向量表示。为了确保网络始终有能力传递它提取的所有信息，我们会根据深度增加特征图的数量，以适应空间分辨率的降低。即，我们在下采样过程中丢失了空间信息，为了弥补这种损失，我们扩展特征图以增加语义信息。
- en: After a certain amount of downsampling that you have selected, the feature maps
    are vectorized and fed into a series of fully connected layers. The last layer
    has as many outputs as there are classes in your dataset.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择的一定量下采样后，特征图被矢量化并输入到一系列全连接层中。最后一层的输出数量与数据集中类别的数量相同。
- en: '![](../Images/591b3afafa3c1ba51183474506ac49c5.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/591b3afafa3c1ba51183474506ac49c5.png)'
- en: Object Detection
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标检测
- en: 'Object detectors come in 2 flavours: one-stage and two-stage. Both of them
    start out with “anchor boxes”; these are default bounding boxes. Our detector
    is going to predict the *difference* between those boxes and the ground-truth,
    rather than predicting the boxes directly.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测有两种类型：一阶段和两阶段。它们都以“锚框”开始；这些是默认的边界框。我们的探测器将预测这些框与真实框之间的*差异*，而不是直接预测框。
- en: 'In a two-stage detector we naturally have two networks: a box proposal network
    and a classification network. The box proposal network proposes coordinates for
    bounding boxes where it thinks there is a high likelihood that objects are there;
    again these are *relative* to the anchor boxes. The classification network then
    takes each of these bounding boxes and classifies the potential object that lies
    within it.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个两阶段探测器中，我们自然有两个网络：一个框提议网络和一个分类网络。框提议网络提出了边界框的坐标，这些坐标是基于它认为对象可能存在的高概率区域；这些坐标相对于锚框是*相对的*。分类网络则对每个边界框中的潜在对象进行分类。
- en: In a one-stage detector, the proposal and classifier networks are fused into
    one single stage. The network directly predicts both the bounding box coordinates
    and the class which resides within that box. Because the two stages are fused
    together, one-stage detectors tend to be faster than two-stage. But the two-stage
    detectors have higher accuracy due to the separation of the two tasks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在一阶段探测器中，提议和分类网络被融合成一个单独的阶段。网络直接预测边界框坐标和框内的类别。由于两个阶段融合在一起，一阶段探测器通常比两阶段的更快。但由于任务的分离，两阶段探测器的准确性更高。
- en: '![Figure](../Images/969ef8b51646f839a7a9648728c4337a.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/969ef8b51646f839a7a9648728c4337a.png)'
- en: The Faster-RCNN two-stage object detection architecture![Figure](../Images/0be8d94083d3a339242c26d3f45aa040.png)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Faster-RCNN 两阶段目标检测架构 ![图示](../Images/0be8d94083d3a339242c26d3f45aa040.png)
- en: The SSD one-stage object detection architecture
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: SSD 一阶段目标检测架构
- en: Segmentation
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分割
- en: Segmentation is one of the more unique tasks in computer vision in that the
    networks need to learn both low- and high-level information. Low-level information
    to accurately segment each area and object in the image by the pixel, and high-level
    information to directly classify those pixels. This leads to networks being designed
    to combine the information from earlier layers and high-resolution (low-level
    spatial information) with deeper layers and low-resolution (high-level semantic
    information).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 分割是计算机视觉中较为独特的任务之一，因为网络需要学习低层次和高层次的信息。低层次信息用于准确地按像素分割图像中的每个区域和对象，而高层次信息则用于直接分类这些像素。这导致网络被设计为将早期层和高分辨率（低层次空间信息）的信息与更深层次和低分辨率（高层次语义信息）的信息结合起来。
- en: As we can see below, we first run our image through a standard classification
    network. We then extract features from each stage of the network, thus using information
    from a range of low-to-high. Each information level is processed independently
    before combining them all together in turn. As the information is combined, we
    upsample the feature maps to eventually get to the full image resolution.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，我们首先通过标准分类网络处理图像。然后，我们从网络的每一阶段提取特征，从而利用从低到高的一系列信息。每个信息层级在合并之前会独立处理。当信息被合并时，我们上采样特征图，以最终达到完整的图像分辨率。
- en: To learn more details about how segmentation with deep learning works, check
    out [this article](https://towardsdatascience.com/semantic-segmentation-with-deep-learning-a-guide-and-code-e52fc8958823).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于深度学习分割的细节，请查看 [这篇文章](https://towardsdatascience.com/semantic-segmentation-with-deep-learning-a-guide-and-code-e52fc8958823)。
- en: '![Figure](../Images/3c3bf26f47d1e4fc2a532b728a7deab7.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/3c3bf26f47d1e4fc2a532b728a7deab7.png)'
- en: The GCN Segmentation architecture
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: GCN 分割架构
- en: Pose Estimation
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 姿态估计
- en: 'Pose estimation models need to accomplish 2 tasks: (1) detect keypoints in
    an image for each body part (2) find out how to properly connect those keypoints.
    This is done in three stages:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 姿态估计模型需要完成两个任务：（1）检测图像中每个身体部位的关键点（2）找出如何正确连接这些关键点。这分为三个阶段：
- en: (1) Extract features from the image using a standard classification network
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 使用标准分类网络从图像中提取特征
- en: (2) Given those features, train a sub-network to predict a set of 2D heatmaps.
    Each heatmap is associated with a particular keypoint and contains confidence
    values for each image pixel about whether a keypoint likely exists there or not
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 给定这些特征，训练一个子网络来预测一组2D热图。每个热图与特定关键点相关，并包含关于每个图像像素是否可能存在关键点的置信度值。
- en: (3) Again given the features from the classification network, we train a sub-network
    to predict a set of 2D vector fields, where each vector field encodes the degree
    of association between the keypoints. Keypoints with high association are then
    said to be connected.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 再次利用分类网络的特征，我们训练一个子网络来预测一组2D矢量场，每个矢量场编码了关键点之间的关联程度。高关联的关键点被认为是连接在一起的。
- en: Training the model in this way with the sub-networks will jointly optimise detecting
    the keypoints and connecting them together.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式训练模型与子网络将共同优化关键点的检测和连接。
- en: '![Figure](../Images/e271162f9bf7faad065120ef15091627.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/e271162f9bf7faad065120ef15091627.png)'
- en: The OpenPose Pose Estimation architecture
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: OpenPose姿态估计架构
- en: Enhancement and Restoration
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 增强与恢复
- en: Enhancement and restoration networks are their own unique beast. We don’t do *any* downsampling
    with these since what we are really concerned about is high-pixel / spatial accuracy.
    Downsampling would really kill this information since it would reduce how many
    pixels we have for spatial accuracy. Instead, all processing is done at the full
    image resolution.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 增强和恢复网络是它们自己独特的存在。我们不对这些进行*任何*下采样，因为我们真正关心的是高像素/空间准确性。下采样会降低空间准确性，因为它减少了用于空间准确性的像素数。相反，所有处理都在全图像分辨率下进行。
- en: We begin by passing the image we want to enhance / restore to our network without
    any modification, at full-resolution. The network simply consists of a stack of
    many convolutions and activation function. These blocks are typically inspired
    and occasionally direct copies of those originally developed for image classification
    such as [Residual Blocks](https://arxiv.org/pdf/1512.03385.pdf), [Dense Blocks](https://arxiv.org/pdf/1608.06993.pdf), [Squeeze
    Excitation Blocks](https://arxiv.org/pdf/1709.01507.pdf), etc. There is no activation
    function on the last layer, not even sigmoid or softmax, since we want to predict
    the image pixels directly and do not require any probabilities or scores.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将要增强/恢复的图像以全分辨率传递给我们的网络，不做任何修改。该网络仅由多个卷积和激活函数堆叠而成。这些模块通常受到启发，并且偶尔直接复制自最初为图像分类开发的模块，如[Residual
    Blocks](https://arxiv.org/pdf/1512.03385.pdf)、[Dense Blocks](https://arxiv.org/pdf/1608.06993.pdf)、[Squeeze
    Excitation Blocks](https://arxiv.org/pdf/1709.01507.pdf)等。最后一层没有激活函数，甚至没有sigmoid或softmax，因为我们希望直接预测图像像素，而不需要任何概率或分数。
- en: That’s about all there is to these types of networks! Lots of processing at
    the image’s full-resolution to achieve high spatial accuracy, using the same convolutions
    that have been proven to work with other tasks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是这些类型网络的全部内容！在图像的全分辨率下进行大量处理，以实现高空间准确性，使用已被证明在其他任务中有效的相同卷积。
- en: '![Figure](../Images/4241804302e82822e35bd2ecaa6d868c.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/4241804302e82822e35bd2ecaa6d868c.png)'
- en: The EDSR Super-Resolution architecture
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: EDSR超分辨率架构
- en: Action Recognition
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动作识别
- en: Action Recognition is one of the few applications that specifically requires *video
    data* to work well. To classify an action we need to have knowledge of the change
    in the scene that is taking place over time; this naturally leads us to requiring
    videos. Our network must be trained to learn both *spatial* and *temporal* information
    i.e changes in *space* and *time*. The perfect network for this is a *3D-CNN*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 动作识别是少数几个特定需要*视频数据*才能良好工作的应用之一。要分类一个动作，我们需要了解场景随时间发生的变化，这自然要求我们需要视频。我们的网络必须学会*空间*和*时间*信息，即*空间*和*时间*的变化。最适合这个任务的网络是*3D-CNN*。
- en: 'A 3D-CNN is, as the name of course suggests, a Convolutional Net that uses
    3D convolutions! They differ from regular CNNs by the fact that the convolutions
    are applied in 3-dimensions: width, height, and *time*. Thus, each output pixel
    is predicted from calculations that are based on both the pixels around it and
    the pixels in previous and subsequent frames in the same locations!'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 3D-CNN，顾名思义，是一种使用3D卷积的卷积网络！它们与常规CNN的不同之处在于卷积应用于3个维度：宽度、高度和*时间*。因此，每个输出像素的预测基于其周围像素和相同位置的前后帧中的像素的计算！
- en: '![Figure](../Images/39e99ebba274c96308b31a606822f0d3.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/39e99ebba274c96308b31a606822f0d3.png)'
- en: Passing images in a large batch directly
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 直接传递大批量图像
- en: 'The video frames can be passed in several ways:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 视频帧可以通过几种方式传递：
- en: (1) In a large batch directly, such as in the first figure. Since we are passing
    a sequence of frames, both spatial and temporal information is available
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 直接在大批量中，如第一张图所示。由于我们传递的是一系列帧，因此同时提供了空间和时间信息。
- en: '![Figure](../Images/e5e704adf682f38d75b8971205ff2a50.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/e5e704adf682f38d75b8971205ff2a50.png)'
- en: Single frame + optical flow (left). Video + optical flow (right)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 单帧 + 光流（左）。视频 + 光流（右）
- en: (2) We could also pass a single image frame in one stream (spatial information
    of the data) and its corresponding optical flow representation from the video
    (temporal information of the data). We’ll extract features from both using regular
    2D CNNs, before combining them to be passed to our 3D CNN, which combines both
    types of information
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 我们也可以在一个流中传递单个图像帧（数据的空间信息）及其对应的视频光流表示（数据的时间信息）。我们会使用常规的2D CNN从这两者中提取特征，然后将它们结合后传递给我们的3D
    CNN，后者结合了这两种信息。
- en: (3) Pass our sequence of frames to one 3D CNN and the optical flow representation
    of the video to another 3D CNN. Both of the data streams have spatial and temporal
    information available. This would be the slowest option but also potentially the
    most accurate, given that we are doing specific processing for two different representations
    of our video that both contain all of our information.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 将我们的帧序列传递给一个3D CNN，将视频的光流表示传递给另一个3D CNN。这两个数据流都有空间和时间信息。这将是最慢的选项，但也可能是最准确的，因为我们对视频的两种不同表示进行特定处理，而这两种表示包含了所有信息。
- en: All of these networks output the action classification of the video.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些网络输出视频的动作分类。
- en: Like to learn?
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 喜欢学习吗？
- en: Follow me on[ twitter](https://twitter.com/GeorgeSeif94) where I post all about
    the latest and greatest AI, Technology, and Science!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在[ twitter](https://twitter.com/GeorgeSeif94)上关注我，我会发布关于最新和最伟大的AI、技术和科学的内容！
- en: '**Bio: [George Seif](https://towardsdatascience.com/@george.seif94)** is a
    Certified Nerd and AI / Machine Learning Engineer.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[George Seif](https://towardsdatascience.com/@george.seif94)** 是一位认证的极客和AI/机器学习工程师。'
- en: '[Original](https://towardsdatascience.com/how-to-do-everything-in-computer-vision-2b442c469928).
    Reposted with permission.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/how-to-do-everything-in-computer-vision-2b442c469928)。经许可转载。'
- en: '**Related:**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Solve any Image Classification Problem Quickly and Easily](/2018/12/solve-image-classification-problem-quickly-easily.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[快速轻松解决任何图像分类问题](/2018/12/solve-image-classification-problem-quickly-easily.html)'
- en: '[An Introduction to Scikit Learn: The Gold Standard of Python Machine Learning](/2019/02/introduction-scikit-learn-gold-standard-python-machine-learning.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Scikit Learn简介：Python机器学习的黄金标准](/2019/02/introduction-scikit-learn-gold-standard-python-machine-learning.html)'
- en: '[How to Setup a Python Environment for Machine Learning](/2019/02/setup-python-environment-machine-learning.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何为机器学习设置Python环境](/2019/02/setup-python-environment-machine-learning.html)'
- en: More On This Topic
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[TensorFlow for Computer Vision - Transfer Learning Made Easy](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow在计算机视觉中的应用 - 转移学习轻松实现](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)'
- en: '[Discover the World of Computer Vision: Introducing MLM''s Latest…](https://www.kdnuggets.com/2024/01/mlm-discover-the-world-of-computer-vision-ebook)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[探索计算机视觉的世界：介绍MLM的最新……](https://www.kdnuggets.com/2024/01/mlm-discover-the-world-of-computer-vision-ebook)'
- en: '[5 Applications of Computer Vision](https://www.kdnuggets.com/2022/03/5-applications-computer-vision.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[计算机视觉的5个应用](https://www.kdnuggets.com/2022/03/5-applications-computer-vision.html)'
- en: '[6 Things You Need To Know About Data Management And Why It Matters…](https://www.kdnuggets.com/2022/05/6-things-need-know-data-management-matters-computer-vision.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于数据管理你需要知道的 6 件事及其重要性…](https://www.kdnuggets.com/2022/05/6-things-need-know-data-management-matters-computer-vision.html)'
- en: '[KDnuggets News March 9, 2022: Build a Machine Learning Web App in 5…](https://www.kdnuggets.com/2022/n10.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻 2022 年 3 月 9 日：在 5 天内构建一个机器学习 Web 应用](https://www.kdnuggets.com/2022/n10.html)'
- en: '[DINOv2: Self-Supervised Computer Vision Models by Meta AI](https://www.kdnuggets.com/2023/05/dinov2-selfsupervised-computer-vision-models-meta-ai.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DINOv2：Meta AI 的自监督计算机视觉模型](https://www.kdnuggets.com/2023/05/dinov2-selfsupervised-computer-vision-models-meta-ai.html)'
