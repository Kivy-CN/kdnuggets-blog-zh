["```py\nimport timefrom sklearn import datasets, svm\nfrom skdist.distribute.search import DistGridSearchCV\nfrom pyspark.sql import SparkSession # instantiate spark session\nspark = (   \n    SparkSession    \n    .builder    \n    .getOrCreate()    \n    )\nsc = spark.sparkContext # the digits dataset\ndigits = datasets.load_digits()\nX = digits[\"data\"]\ny = digits[\"target\"] # create a classifier: a support vector classifier\nclassifier = svm.SVC()\nparam_grid = {\n    \"C\": [0.01, 0.01, 0.1, 1.0, 10.0, 20.0, 50.0], \n    \"gamma\": [\"scale\", \"auto\", 0.001, 0.01, 0.1], \n    \"kernel\": [\"rbf\", \"poly\", \"sigmoid\"]\n    }\nscoring = \"f1_weighted\"\ncv = 10# hyperparameter optimization\nstart = time.time()\nmodel = DistGridSearchCV(    \n    classifier, param_grid,     \n    sc=sc, cv=cv, scoring=scoring,\n    verbose=True    \n    )\nmodel.fit(X,y)\nprint(\"Train time: {0}\".format(time.time() - start))\nprint(\"Best score: {0}\".format(model.best_score_))------------------------------\nSpark context found; running with spark\nFitting 10 folds for each of 105 candidates, totalling 1050 fits\nTrain time: 3.380601406097412\nBest score: 0.981450024203508\n```"]