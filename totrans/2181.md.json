["```py\nimport pandas as pd\nimport sklearn\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n```", "```py\ndata = pd.read_csv('path')\n```", "```py\ndata.head()\n```", "```py\ndata.info()\n```", "```py\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 8 columns):\n  #     Column     Non-Null  Count   Dtype\n- - -   - - - -    - - - - - - - -   - - - -\n  0     loc1       10000 non-null     object\n  1     loc2       10000 non-null     object\n  2     para1      10000 non-null     int64\n  3     dow        10000 non-null     object\n  4     para2      10000 non-null     int64\n  5     para3      10000 non-null     float64\n  6     para4      10000 non-null     float64\n  7     price      10000 non-null     float64\n dtypes:   float64(3),   int64(2),   object(3)\n memory  usage:  625.1+ KB \n```", "```py\ndata.describe()\n```", "```py\ndata[\"loc1\"].value_counts()\n```", "```py\nloc1\n2\t1607\n0\t1486\n1\t1223\n7\t1081\n3\t945\n5\t846\n4\t773\n8\t727\n9\t690\n6\t620\nS\t  1\nT\t  1\nName:  count,  dtype:  int64 \n```", "```py\ndata = data[(data[\"loc1\"] != \"S\") & (data[\"loc1\"] != \"T\")]\n```", "```py\ndata[\"loc2\"] = pd.to_numeric(data[\"loc2\"], errors='coerce')\ndata[\"loc1\"] = pd.to_numeric(data[\"loc1\"], errors='coerce')\ndata.dropna(inplace=True) \n```", "```py\n# Assuming data is already loaded and 'dow' column contains day names\n# Map 'dow' to numeric codes\ndays_of_week = {'Mon': 1, 'Tue': 2, 'Wed': 3, 'Thu': 4, 'Fri': 5, 'Sat': 6, 'Sun': 7}\ndata['dow'] = data['dow'].map(days_of_week)\n\n# Invert the days_of_week dictionary\nweek_days = {v: k for k, v in days_of_week.items()}\n\n# Convert dummy variable columns to integer type\ndow_dummies = pd.get_dummies(data['dow']).rename(columns=week_days).astype(int)\n\n# Drop the original 'dow' column\ndata.drop('dow', axis=1, inplace=True)\n\n# Concatenate the dummy variables\ndata = pd.concat([data, dow_dummies], axis=1)\n\ndata.head() \n```", "```py\nX = data.drop('price', axis=1)  # Assuming 'price' is the target variable\ny = data['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```", "```py\n# Initialize the models\nmodels = {\n    \"Multiple Linear Regression\": LinearRegression(),\n    \"Decision Tree Regression\": DecisionTreeRegressor(random_state=42),\n    \"Support Vector Regression\": SVR()\n}\n\n# Dictionary to store the results\nresults = {}\n\n# Fit the models and evaluate\nfor name, model in models.items():\n    model.fit(X_train, y_train)  # Train the model\n    y_pred = model.predict(X_test)  # Predict on the test set\n\n    # Calculate performance metrics\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n\n    # Store results\n    results[name] = {'MSE': mse, 'R^2 Score': r2}\n\n# Print the results\nfor model_name, metrics in results.items():\n    print(f\"{model_name} - MSE: {metrics['MSE']}, R^2 Score: {metrics['R^2 Score']}\") \n```", "```py\nMultiple Linear Regression - MSE: 35143.23011545407, R^2 Score: 0.5825954700994046\nDecision Tree Regression - MSE: 44552.00644904675, R^2 Score: 0.4708451884787034\nSupport Vector Regression - MSE: 73965.02477382126, R^2 Score: 0.12149975134965318 \n```", "```py\nimport matplotlib.pyplot as plt\nfrom math import sqrt\n\n# Calculate RMSE for each model from the stored MSE and prepare for plotting\nrmse_values = [sqrt(metrics['MSE']) for metrics in results.values()]\nmodel_names = list(results.keys())\n\n# Create a horizontal bar graph for RMSE\nplt.figure(figsize=(10, 5))\nplt.barh(model_names, rmse_values, color='skyblue')\nplt.xlabel('Root Mean Squared Error (RMSE)')\nplt.title('Comparison of RMSE Across Regression Models')\nplt.show() \n```"]