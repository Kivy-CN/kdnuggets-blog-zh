- en: Applying Data Science to Cybersecurity Network Attacks & Events
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/09/applying-data-science-cybersecurity-network-attacks-events.html](https://www.kdnuggets.com/2019/09/applying-data-science-cybersecurity-network-attacks-events.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Aakash Sharma](https://www.linkedin.com/in/aakashsharma21/), Data Scientist**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: The cyber world is such a vast concept to comprehend. At the time, I decided
    I wanted to get into cybersecurity during my undergrad in college. What intrigued
    me was understanding the concepts of malware, network security, penetration testing
    & the encryption aspect that really plays a role in what cybersecurity really
    is.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Being able to protect the infrastructure is important but interesting nonetheless.
    Sure there is coding but I never really learned how we can implement code into
    cybersecurity principles. This is what I really want to know next that could stretch
    my knowledge in information technology & computer science. I learned more about
    coding especially in Python. I dabbled a little in Scala & I already had a good
    foundation of Sequel & Java applications during my undergrad that learning it
    during my boot camp allowed me to feel a little more comfortable with it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: The data science immersive program taught me how to gather data through Sequel,
    JSON, HTML or web-scrapping applications where I went into cleaning the data &
    then applying Python related code for statistical analysis. I then as able to
    model the data to either find trends, make predictions, or provide suggestions/recommendations.
    I wanted to apply this to my background in Cisco NetaCad, cybersecurity principles
    & software development
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/b1cbeb14e9d610cf85aa10111f51dd87.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: FCC Logo
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: I then wanted to relate this to my cybersecurity background. I decided to gather
    data from Data.org regarding a ton of cyber attacks regarding the Federal Communications
    Commission (FCC). In this blog post, I decided to write about how I was able to
    connect my data science knowledge to my cybersecurity background in real world
    industries. I will provide some background of the project, a code along & some
    insight into what the FCC could do to better understand the data as I did. This
    could be useful for future situations or other government related cybersecurity
    projects.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: So the FCC, to the best of my knowledge, follows the CSRIC Best Practices Search
    Tool which allows you to search CSRIC’s collection of Best Practices using a variety
    of criteria including Network Type, Industry Role, Keywords, Priority Levels &
    BP Number.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: The Communications Security, Reliability & Interoperability Council’s (CSRIC)
    mission is to provide recommendations to the FCC to ensure, among other things,
    optimal security & reliability of communications systems, including telecommunications,
    media & public safety.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'CSRIC’s members focus on a range of public safety & homeland security-related
    communications matters, including: (1) the reliability & security of communications
    systems & infrastructure, particularly mobile systems; (2) 911, Enhanced 911 (E911),
    & Next Generation 911 (NG911); & (3) emergency alerting.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: The CSRIC’s recommendations will address the prevention & remediation of detrimental
    cyber events, the development of best practices to improve overall communications
    reliability, the availability & performance of communications services & emergency
    alerting during natural disasters, terrorist attacks, cyber security attacks or
    other events that result in exceptional strain on the communications infrastructure,
    the rapid restoration of communications services in the event of widespread or
    major disruptions & the steps communications providers can take to help secure
    end-users & servers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: The first step for me in tackling this project was understanding what I needed
    to accomplish & what kind of direction this project had me taking. I had remembered
    that I needed to provide recommendations to the FCC to ensure optimal security
    & reliability of communication systems in telecommunications, media & public safety.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: There are different approaches I had taken into account when attempting this
    project. The 1st is diving straight in in order to better understand the data
    itself where I focused on just the priority of the event & applying a ton of machine
    learning classification models to it. The 2nd approach was being able to apply
    natural language processing techniques on the description of the event & see how
    that correlates to the priority of the event.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: With this we can make predictions & following that, recommendations to better
    prevent, understand or control the event. My idea is if we can focus on the more
    critical events by fixing the less complex ones, we can save enough assets to
    further improve the systems to combat the more complex ones.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'What is needed:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: A Python IDE
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine Learning & Statistical Packages
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strong knowledge in Data Science Concepts
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some knowledge in cybersecurity & network related concepts
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin! Let’s first start by importing any packages we intend on using,
    I usually copy & paste a list of useful modules that has helped me with previous
    data science projects.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We then need to load in the data & view it by:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we have our data we can explore, clean & understand the data. Below,
    I have provided a function for basic data exploratory analysis. We want to do
    this to fully understand the data we’re dealing with & what the overall goal needs
    to be met.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始探索、清理和理解数据。下面，我提供了一个用于基本数据探索分析的函数。我们这样做是为了充分理解我们处理的数据以及需要达成的整体目标。
- en: The following function will allow us to view any null values, replace any blank
    spaces with an underscore, reformat the data frame index, see the data types for
    each column, display any duplicated data, describes the statistical analysis of
    the data & checks the shape.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数将允许我们查看任何空值，用下划线替换任何空白字符，重新格式化数据框的索引，查看每列的数据类型，显示任何重复的数据，描述数据的统计分析，并检查数据的形状。
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Based off of the data we say that this was a unbalanced classification problem!
    What we need to do next is eliminate any “NaN” or null values & somewhat balance
    our class. Based off of our exploratory data analysis, we can see that most of
    the data has “NaN” values in the object type columns. We can fix this with the
    following function below!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据，我们说这是一个不平衡的分类问题！我们接下来需要做的是消除任何“NaN”或空值，并在一定程度上平衡我们的类别。根据我们的探索性数据分析，我们可以看到大多数数据在对象类型的列中有“NaN”值。我们可以使用下面的函数来解决这个问题！
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Figure](../Images/00875b80a3ceb8555178033818446813.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/00875b80a3ceb8555178033818446813.png)'
- en: Checking the amount of values in the Priorities column
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 检查“Priorities”列中的值数量
- en: Looking at the priority column(s), we have a object related column that ranks
    the severity of the event to important, highly important & critical. Another column
    that corresponds to that priority column ranks them 1–3, with 1 being important,
    2 as highly important & 3 being critical. Based off of the variety of priorities,
    we see that the data is unbalanced. We fix this by renaming our column for better
    understand & then balancing it where we focus on the highly important & critical
    events.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 查看优先级列，我们有一个与对象相关的列，用于按重要性、非常重要和关键来排序事件的严重程度。另一个与优先级列对应的列将它们排序为1到3，其中1为重要，2为非常重要，3为关键。基于优先级的多样性，我们看到数据是不平衡的。我们通过重新命名列以便更好地理解，然后对其进行平衡，重点关注非常重要和关键事件来解决这个问题。
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Figure](../Images/bbc7ce6635439f561897a56d1e140338.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/bbc7ce6635439f561897a56d1e140338.png)'
- en: Result of the above code after we’ve balanced the class
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们平衡了类别之后，上述代码的结果
- en: '**First Approach (Understanding the Data)**'
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**第一种方法（理解数据）**'
- en: In this next section I will discuss my initial approach in understanding the
    priorities column. I learned fast that this approach was not the best but was
    very informative in where I should look elsewhere when looking at the priorities
    of the attack in making recommendations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我将讨论我在理解优先级列方面的初步方法。我很快发现这种方法并不是最好的，但它在我寻找攻击优先级的推荐时提供了很有用的信息。
- en: My next step was understanding which columns correlated best for patterns &
    trends with my priorities column. It seemed that all columns that worked well
    were binary! The description column were text related & machines don’t like handling
    text objects. With the below code, we can see which columns are the most positively
    correlated & most negatively correlated with our predictor column.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我的下一步是理解哪些列与我的优先级列的模式和趋势最相关。似乎所有表现良好的列都是二进制的！描述列是与文本相关的，而机器处理文本对象不太喜欢。使用下面的代码，我们可以看到哪些列与我们的预测列最正相关，哪些列最负相关。
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Figure](../Images/63555a72fc10d4e0f147599a5698eda0.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/63555a72fc10d4e0f147599a5698eda0.png)'
- en: Most Negatively Correlated![Figure](../Images/87dc709d7e3fba09a31ae8e2e6793512.png)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最负相关的![图像](../Images/87dc709d7e3fba09a31ae8e2e6793512.png)
- en: Most Positively Correlated
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最正相关的
- en: Now we can start making our models to see what we’ve done will give us accurate
    predictions or enough information for recommendations. Let’s first start with
    a train-test split with these correlated columns serving as our features & our
    priorities column serving as our predictor variable.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始构建模型，以查看我们所做的是否能提供准确的预测或足够的信息用于推荐。首先从训练-测试拆分开始，这些相关列作为特征，我们的优先级列作为预测变量。
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that we’ve train-test split our features, let’s apply grid search to find
    the best parameters or features for full accuracy on a **Naive Bayes Classifier,
    Random Forest Classifier, Adaboost/Gradient Boost Classifier** & a **Keras Neural
    Network**! But what do these classifier models even mean?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: In simple terms, a **Naive Bayes** classifier assumes that the presence of a
    particular feature in a class is unrelated to the presence of any other feature.
    Let’s see it on our data!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Random Forest Classifier** creates a set of decision trees from a randomly
    selected subset of the training set, which then aggregates the votes from different
    decision trees to decide the final class of the test object. Each individual tree
    in the random forest spits out a class prediction and the class with the most
    votes becomes our model’s prediction. Let’s model it!'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Figure](../Images/091f19d0b69e88e11a54841c6c23f8b7.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: Graph of our Adaboost Model, it’s overfit!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '**AdaBoost** is short for Adaptive Boosting. It is basically a machine learning
    algorithm that is used as a classifier. Whenever you have a large amount of data
    and you want divide it into different categories, we need a good classification
    algorithm to do it. Hence the word ‘boosting’, as in it boosts other algorithms!'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Neural networks** are a set of algorithms, modeled loosely after the human
    brain, that are designed to recognize patterns. They interpret sensory data through
    a kind of machine perception, labeling or clustering raw input. We can even apply
    regularization to combat the **over fitting** issue!'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/1d09ab2060c853b6e6407693f94189ed.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: Our regularized Neural Network, it’s overfit!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![Figure](../Images/113883da063f2e405f4c0c7d3b679bb6.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: The Accuracy of Our Models
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: What is this telling us?! Based off of the training score & test score we can
    see that our model is **over fit** & is not great at making predictions or analyzing
    trends. What does it mean that our model is **over fit**? We have **high variance** & **low
    bias**! **High variance** can cause an algorithm to model the random noise in
    the training data, rather than the intended outputs. We still can’t make recommendations
    off of this since it really doesn’t give us much information!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Second Approach (Natural Language Processing) — The Best Route
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: My 2nd approach was focusing on the description column. After my 1st approach,
    I wanted to see how the priority of the attack correlated with what was given
    in the description. The description column gave us a short explanation of what
    happened & a suggested FCC compliant resolution in what they might do in stopping
    that similar event.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: In order to better understand the description column, I needed to apply natural
    language processing (NLP) since computers & statistical models dislike handling
    text & words. But we can get around this! My approach is similar when cleaning
    the data & balancing the priorities column, however I have applied some NLP concepts
    to better understand the description, analyze it, make recommendations & even
    predict what the next event would be based off of the occurrence of words specific
    to the events.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the concepts include:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-processing **is the technique of converting raw data into a clean data
    set.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regex, **regular expressionis a string of text that allows you to create
    patterns that help match, locate & manage text. Another way to clean the text.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lemmatizing** is the process of grouping together the inflected forms of
    a word so they can be analyzed as a single term.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stemming** is the process of reducing inflected words to their stem, base
    or root form.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Countvectorize **counts the word frequencies.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TFIDFVectorizer** is the value of a word increases proportionally to count,
    but is offset by the frequency of the word in the corpus.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start off by applying regex concepts to our already cleaned data. We also
    want to strip or clean out common words that are useful but are sporadic in every
    single description.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now that we’ve cleaned our data, we should apply some pre-processing techniques
    to better understand the words given to use in every description of the event.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We then want to apply countvectorize on our stemmed, lemmatized & tokenized
    description words in order to control common stop words in the English language,
    we can do this with the following code. We then can see what the most common words
    are in the entire data set.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![Figure](../Images/f5c1a6526d8e53db4a11ca3ad0dc150a.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: Top Word Count in the Entire Data Set
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: We can see that most of the words that popped up are network or security related.
    We can use this information to better understand the scope of what these events
    are! Are the network attacks? Are they network warehouse related? Etc.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: But what if we wanted more information? We can group the descriptions based
    on the urgency or severity of the importance of the event. Maybe it’s nothing
    serious so it’s ranked a 0 (Not Important) or really bad ranked as 1 (Really Important).
    We can do this with the below code based on our pre-processed columns. We can
    then visualize what the most common really important words are.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![Figure](../Images/a5b0249a84ab149056d3d6a93b280bc3.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: Top Word Count of the Really IMPORTANT words
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can start modeling regression & classification metrics on the tokenized
    data. Let’s start by applying a logistic regression model through a pipeline where
    can apply a grid search tool in order to tune our BEST features or BEST parameters.
    Let’s establish our X variable, our features! We will use the words or features
    within the tokenized column within the processed data frame we created above.
    The processed data frame is an entirely NEW data frame that contains our tokenized,
    stemmed & lemmatized columns.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Here I decided to focus on the tokenized columns because this specific column
    functioned the best for parameter tuning & accuracy. To cut on time length for
    this blog post I decided to focus on the tokenized, what works best, as well!
    Let’s train-test split it as well.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now let’s created our **pipeline** that uses grid search concepts for the best
    hyper parameters. Once the grid search has fit (this can take awhile!) we can
    pull out a variety of information and useful objects from the grid search object.
    Often, we’ll want to apply several transformers to a data set & then finally build
    a model. If you do all of these steps independently, your code when predicting
    on test data can be messy. It’ll also be prone to errors. Luckily, we’ll have **pipelines**!
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Here we will be applying a logistic model which can take into consideration
    LASSO & Ridge penalties.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '**You should:**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Fit and validate the accuracy of a default logistic regression on the data.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform a gridsearch over different regularization strengths, Lasso & Ridge
    penalties.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the accuracy on the test set of your optimized logistic regression to
    the baseline accuracy & the default model.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Look at the best parameters found. What was chosen? What does this suggest about
    our data?
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Look at the (non-zero, if Lasso was selected as best) coefficients & associated
    predictors for your optimized model. What appears to be the most important predictors?
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now we can apply the pipeline on our logistic regression model within a grid
    search object. Notice the countvectorize model being instantiated. We did this
    because we want see how that factors into our accuracy & importance of the words
    we associated with our network attacks.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Figure](../Images/decbbb88ade18117d56b64ad0fc88c23.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: Our Improved Accuracy of our Logistic Model
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: So what can infer from this? Well it looks like we’ve seen a massive increase
    in model accuracy for a training data as well as 10% increase in accuracy for
    our testing data. However, the model is still **over fit**! But still great job!
    What were our best parameters? We could use this information if we want to tune
    future logistic models! The below code will show us that!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Figure](../Images/d40ca1a01a557a1969435a75fbb8c5a8.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: Our BEST Hyper Parameters from our Logistic Gridsearch within the Pipeline
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: A **regression** model that uses **L1** regularization technique is called Lasso **Regression** and
    model which uses **L2** is called Ridge **Regression**. From our best hyper parameters,
    our models favors a Ridge Regression technique. Now we want to make predictions
    off of our logistic regression & be able to make suggestions. How do we go upon
    doing that? Let’s look at the coefficients associated with our features that will
    predict the outcomes of the best y variable.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Figure](../Images/dc670ed8a1c26c818a2d5dc00edfceec.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: Predictions for Suggestions Based off Occurrence & Importance of Words
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Recommendations & Conclusions
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that I’ve finished modeling & analyzing the data, I can now make suggestions
    to the FCC as well as any other data scientist or data analyst that plans on doing
    a project similar to mine.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: For future data scientists doing a similar project. Get more data, a better
    sample of data, increase/decrease complexity of model & **regularize**. This can
    help combat the issue of **over fitting** your data as I experienced through this
    project. Understand unbalanced classification problems. This can lead to your
    main direction in solving the problem.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: For the FCC’s CSRIC’s best practices, my best recommendation would be to fix
    the simple issues first so they don’t happen to often & soak up your resources.
    This can allow them to focus on the more important & complex events or attacks.
    Based off what I could predict & analyzing the given data.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01f7dec0f4c03bc987d72b6290a162a2.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: 'Simple problems:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Different Cables
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Color Code Cables
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better ventilation of warehouse
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increase power capacity
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better hardware
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spacing of antennas
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/b66fa8ec368b2c891e0a298da5eb282c.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: 'Moderate problems:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Utilize Network Surveillance
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide secure electrical software where feasible
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find thresholds for new hardware & software
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Virus protection
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/ce63df369ea4719967572ba1ca60e2d1.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: 'Complex:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Minimize single points of failure & software faults
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Device Management Architecture
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secure networks/encrypted systems
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bio: [Aakash Sharma](https://www.linkedin.com/in/aakashsharma21/)** is a
    Data Scientist, Data Analyst, Cybersecurity Network Engineer, Full Stack Software
    Developer, and Machine Learning & A.I. Enthusiast.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/reddit-post-classification-b70258d6affe).
    Reposted with permission.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[Machine Learning Security](/2019/01/machine-learning-security.html)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PySyft and the Emergence of Private Deep Learning](/2019/06/pysyft-emergence-deep-learning.html)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top 5 domains Big Data analytics helps to transform](/2018/11/top-5-domains-big-data-analytics.html)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[GPT-4 is Vulnerable to Prompt Injection Attacks on Causing Misinformation](https://www.kdnuggets.com/2023/05/gpt4-vulnerable-prompt-injection-attacks-causing-misinformation.html)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Be prepared to manage the threat with an MS in Cybersecurity from…](https://www.kdnuggets.com/2022/07/baypath-prepared-manage-threat-ms-cybersecurity.html)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[准备好通过网络安全硕士学位来管理威胁…](https://www.kdnuggets.com/2022/07/baypath-prepared-manage-threat-ms-cybersecurity.html)'
- en: '[Be prepared to manage the threat with an MS in Cybersecurity from…](https://www.kdnuggets.com/2022/12/baypath-prepared-manage-threat-ms-cybersecurity.html)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[准备好通过网络安全硕士学位来管理威胁…](https://www.kdnuggets.com/2022/12/baypath-prepared-manage-threat-ms-cybersecurity.html)'
- en: '[Applying Descriptive and Inferential Statistics in Python](https://www.kdnuggets.com/applying-descriptive-and-inferential-statistics-in-python)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Python 中应用描述性和推断统计](https://www.kdnuggets.com/applying-descriptive-and-inferential-statistics-in-python)'
- en: '[AI-Automated Cybersecurity: What to Automate?](https://www.kdnuggets.com/ai-automated-cybersecurity-what-to-automate)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AI 自动化网络安全：应该自动化什么？](https://www.kdnuggets.com/ai-automated-cybersecurity-what-to-automate)'
- en: '[Learn Deep Learning by Building 15 Neural Network Projects in 2022](https://www.kdnuggets.com/2022/01/15-neural-network-projects-build-2022.html)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过构建 15 个神经网络项目学习深度学习（2022 年）](https://www.kdnuggets.com/2022/01/15-neural-network-projects-build-2022.html)'
