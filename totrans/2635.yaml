- en: Machine learning is going real-time
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习正在变得实时
- en: 原文：[https://www.kdnuggets.com/2021/01/machine-learning-real-time.html](https://www.kdnuggets.com/2021/01/machine-learning-real-time.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/01/machine-learning-real-time.html](https://www.kdnuggets.com/2021/01/machine-learning-real-time.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Chip Huyen](https://huyenchip.com/), ML Production at Snorkel AI, Teaching
    at Stanford**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Chip Huyen](https://huyenchip.com/)**, Snorkel AI的ML生产，斯坦福大学的讲师。'
- en: '![Real Time](../Images/58a4de630b7ab8bbc230e0e05f8d8a0a.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![实时](../Images/58a4de630b7ab8bbc230e0e05f8d8a0a.png)'
- en: After talking to machine learning and infrastructure engineers at major Internet
    companies across the US, Europe, and China, I noticed two groups of companies.
    One group has made significant investments (hundreds of millions of dollars) into
    infrastructure to allow real-time machine learning and has already seen returns
    on their investments. Another group still wonders if there’s value in real-time
    ML.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在与美国、欧洲和中国主要互联网公司的机器学习和基础设施工程师交谈后，我注意到两类公司。一类公司在基础设施上投入了大量资金（数亿美元），以实现实时机器学习，并已看到投资回报。另一类公司仍在怀疑实时机器学习是否有价值。
- en: There seems to be little consensus on what real-time ML means, and there hasn’t
    been a lot of in-depth discussion on how it’s done in the industry. In this post,
    I want to share what I’ve learned after talking to about a dozen companies that
    are doing it.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 关于实时机器学习的定义似乎没有共识，并且在业界关于如何实现实时机器学习的讨论不多。在这篇文章中，我想分享我与大约十几家公司交谈后所学到的。
- en: There are two levels of real-time machine learning that I’ll go over in this
    post.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将讨论实时机器学习的两个级别。
- en: 'Level 1: Your ML system makes predictions in real-time (online predictions).'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Level 1: 你的机器学习系统实时进行预测（在线预测）。'
- en: 'Level 2: Your system can incorporate new data and update your model in real-time
    (online learning).'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Level 2: 你的系统可以实时接收新数据并更新模型（在线学习）。'
- en: I use “model” to refer to the machine learning model and “system” to refer to
    the infrastructure around it, including data pipeline and monitoring systems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用“模型”来指代机器学习模型，用“系统”来指代围绕它的基础设施，包括数据管道和监控系统。
- en: 'Level 1: Online predictions - your system can make predictions in real-time'
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Level 1: 在线预测 - 你的系统可以实时进行预测'
- en: '***Real-time** here is defined to be in the order of milliseconds to seconds.*'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***实时**在这里被定义为毫秒到秒的范围内。*'
- en: '**Use cases**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**用例**'
- en: Latency matters, especially for user-facing applications. In 2009, Google’s
    experiments demonstrated that [increasing web search latency 100 to 400 ms reduces
    the daily number of searches per user by 0.2% to 0.6%](https://services.google.com/fh/files/blogs/google_delayexp.pdf).
    In 2019, [Booking.com found that an increase of 30% in latency cost about 0.5%
    in conversion rates — “a relevant cost for our business.”](https://blog.acolyer.org/2019/10/07/150-successful-machine-learning-models/)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟很重要，特别是对于面向用户的应用程序。在2009年，谷歌的实验表明，[将网页搜索延迟增加100到400毫秒会使每个用户的每日搜索次数减少0.2%到0.6%](https://services.google.com/fh/files/blogs/google_delayexp.pdf)。在2019年，[Booking.com发现延迟增加30%会使转化率减少约0.5%——“这是对我们业务的相关成本。”](https://blog.acolyer.org/2019/10/07/150-successful-machine-learning-models/)
- en: No matter how great your ML models are, if they take just milliseconds too long
    to make predictions, users are going to click on something else.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你的机器学习模型多么出色，如果它们在进行预测时只慢了几毫秒，用户就会点击其他内容。
- en: '**Problems with batch predictions**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**批量预测的问题**'
- en: One non-solution is to avoid making predictions online. You can generate predictions
    in a batch offline, store them (e.g., in SQL tables), and pull out pre-computed
    predictions when needed.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非解决方案是避免在线进行预测。你可以在离线批量生成预测，存储它们（例如，在SQL表中），并在需要时提取预先计算的预测。
- en: This can work when the input space is finite – you know exactly how many possible
    inputs to make predictions for. One example is when you need to generate movie
    recommendations for your users – you know exactly how many users there are. So
    you predict a set of recommendations for each user periodically, such as every
    few hours.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入空间是有限的时，这种方法可以工作——你确切知道要为多少个可能的输入进行预测。例如，当你需要为用户生成电影推荐时，你确切知道有多少用户。因此，你会定期为每个用户预测一组推荐，例如每隔几个小时。
- en: To make their user input space finite, many apps make their users choose from
    categories instead of entering wild queries. For example, if you go to TripAdvisor,
    you first have to pick a predefined metropolis area instead of being able to enter
    just any location.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用户输入空间有限，许多应用程序让用户从类别中选择，而不是输入任意查询。例如，如果你去 TripAdvisor，你首先必须选择一个预定义的大都市区域，而不是可以输入任何地点。
- en: This approach has many limitations. TripAdvisor results are okay within their
    predefined categories, such as **“Restaurants”** in **“San Francisco,”** but are
    pretty bad when you try to enter wild queries like **“high rating Thai restaurants
    in Hayes Valley.”**
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有很多限制。TripAdvisor 的结果在其预定义的类别中，如**“餐馆”**在**“旧金山”**，是可以接受的，但当你尝试输入诸如**“海斯谷的高评分泰国餐馆”**这样的查询时，效果就很差。
- en: '![](../Images/23367ba14d3d6d5bcc84a86eddbc7ce9.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23367ba14d3d6d5bcc84a86eddbc7ce9.png)'
- en: Limitations caused by batch predictions exist even in more technologically progressive
    companies like Netflix. Say you’ve been watching a lot of horrors lately, so when
    you first log into Netflix, horror movies dominate recommendations. But you’re
    feeling bright today, so you search “comedy” and start browsing the comedy category.
    Netflix should learn and show you more comedy in your list of their recommendations,
    right? But it can’t update the list until the next time batch recommendations
    are generated.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在像 Netflix 这样技术先进的公司中，批量预测也会造成限制。比如你最近看了很多恐怖片，所以当你首次登录 Netflix 时，恐怖片会主导推荐列表。但今天你心情很好，于是搜索了“喜剧”并开始浏览喜剧类别。Netflix
    应该会学习并在你的推荐列表中显示更多喜剧内容，对吧？但它无法在下次批量推荐生成之前更新列表。
- en: In the two examples above, batch predictions lead to decreases in user experience
    (which is tightly coupled with user engagement/retention), not catastrophic failures.
    Other examples are ad ranking, Twitter’s trending hashtag ranking, Facebook’s
    newsfeed ranking, estimating the time of arrival, etc.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述两个例子中，批量预测会导致用户体验下降（这与用户参与度/保留率紧密相关），而不是灾难性的失败。其他例子包括广告排名、Twitter 的热门话题排名、Facebook
    的新闻推送排名、到达时间估算等。
- en: There are also many applications that, without online predictions, would lead
    to catastrophic failures or just wouldn’t work. Examples include high-frequency
    trading, autonomous vehicles, voice assistants, unlocking your phones using face/fingerprints,
    fall detection for elderly care, fraud detection, etc. Being able to detect a
    fraudulent transaction that happened 3 hours ago is still better than not detecting
    it at all, but being able to detect it in real-time can prevent it from going
    through.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多应用，如果没有在线预测，就会导致灾难性的失败或根本无法工作。例如高频交易、自动驾驶汽车、语音助手、通过面部/指纹解锁手机、老年人跌倒检测、欺诈检测等。能够检测到3小时前发生的欺诈交易仍然比完全无法检测要好，但实时检测可以防止交易完成。
- en: Switching from batch predictions to real-time predictions allows you to use
    dynamic features to make more relevant predictions. Static features are information
    that changes slowly or rarely – age, gender, job, neighborhood, etc. Dynamic features
    are features based on what’s happening right now – what you’re watching, what
    you’ve just liked, etc. Knowing a user’s interests right now will allow your systems
    to make recommendations much more relevant to them.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从批量预测转向实时预测可以让你利用动态特征做出更相关的预测。静态特征是那些变化缓慢或很少变化的信息——如年龄、性别、职业、邻里等。动态特征是基于当前发生的事情的特征——如你正在观看的内容、你刚刚点赞的内容等。了解用户当前的兴趣将使你的系统能够做出更符合他们需求的推荐。
- en: '![](../Images/d4becf5482ba17f49624c25d3a5cf789.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4becf5482ba17f49624c25d3a5cf789.png)'
- en: '**Solutions**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案**'
- en: 'For your system to be able to make online predictions, it has to have two components:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使系统能够进行在线预测，它必须具有两个组件：
- en: 'Fast inference: a model that can make predictions in the order of milliseconds.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 快速推理：一种可以在毫秒级别内做出预测的模型。
- en: 'Real-time pipeline: a pipeline that can process data, input it into the model,
    and return a prediction in real-time.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实时管道：一种可以实时处理数据、将其输入模型并返回预测的管道。
- en: '**Fast inference**'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**快速推理**'
- en: 'When a model is too big and taking too long to make predictions, there are
    three approaches:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个模型过大且预测时间过长时，有三种方法可以解决：
- en: '**Make models faster (inference optimization)**'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**加快模型速度（推理优化）**'
- en: E.g., fusing operations, distributing computations, memory footprint optimization,
    writing high-performance kernels targeting specific hardware, etc.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，融合操作、分配计算、内存占用优化、编写针对特定硬件的高性能内核等。
- en: '**Make models smaller (model compression)**'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**缩小模型（模型压缩）**'
- en: Originally, this family of techniques is to make models smaller to make them
    fit on edge devices. Making models smaller often makes them run faster. The most
    common, general technique for model compression is quantization, e.g., using 16-bit
    floats (half precision) or 8-bit integers (fixed-point) instead of 32-bit floats
    (full precision) to represent your model weights. In the extreme case, some have
    attempted 1-bit representation (binary weight neural networks), e.g., [BinaryConnect](https://arxiv.org/abs/1511.00363) and [Xnor-Net](https://arxiv.org/abs/1603.05279).
    The authors of Xnor-Net spun off Xnor.ai, a startup focused on model compression,
    which was [acquired by Apple for a reported $200M](https://www.geekwire.com/2020/exclusive-apple-acquires-xnor-ai-edge-ai-spin-paul-allens-ai2-price-200m-range/).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，这些技术的目的是将模型缩小以适应边缘设备。缩小模型通常会使其运行更快。模型压缩的最常见、通用技术是量化，例如，使用16位浮点数（半精度）或8位整数（定点）来表示模型权重，而不是32位浮点数（全精度）。在极端情况下，一些人尝试了1位表示（二进制权重神经网络），例如，[BinaryConnect](https://arxiv.org/abs/1511.00363)和[Xnor-Net](https://arxiv.org/abs/1603.05279)。Xnor-Net的作者创办了Xnor.ai，这是一家专注于模型压缩的初创公司，后来被[苹果以2亿美元的价格收购](https://www.geekwire.com/2020/exclusive-apple-acquires-xnor-ai-edge-ai-spin-paul-allens-ai2-price-200m-range/)。
- en: Another popular technique is [knowledge distillation](https://arxiv.org/abs/1503.02531) –
    a small model (student) is trained to mimic a larger model or an ensemble of models
    (teacher). Even though the student is often trained with a pre-trained teacher,
    both may also be trained at the same time. One example of a distilled network
    used in production is [DistilBERT](https://arxiv.org/abs/1910.01108), which reduces
    the size of a BERT model by 40% while retaining 97% of its language understanding
    capabilities and being 60% faster.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行技术是[知识蒸馏](https://arxiv.org/abs/1503.02531)——一个小模型（学生）被训练来模仿一个更大的模型或模型集（老师）。尽管学生通常是在一个预训练的老师指导下进行训练，但两者也可以同时训练。一个在生产中使用的蒸馏网络的例子是[DistilBERT](https://arxiv.org/abs/1910.01108)，它将BERT模型的大小减少了40%，同时保留了97%的语言理解能力，并且速度提升了60%。
- en: Other techniques include pruning (finding parameters least useful to predictions
    and setting them to 0) and low-rank factorization (replacing the over-parametric
    convolution filters with compact blocks to both reduce the number of parameters
    and increase speed). See [A Survey of Model Compression and Acceleration for Deep
    Neural Networks](https://arxiv.org/abs/1710.09282) (Cheng et al. 2017) for a detailed
    analysis.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其他技术包括剪枝（找出对预测最无用的参数并将其设置为0）和低秩分解（用紧凑块替换过度参数化的卷积滤波器，以减少参数数量并提高速度）。有关详细分析，请参见[A
    Survey of Model Compression and Acceleration for Deep Neural Networks](https://arxiv.org/abs/1710.09282)（Cheng
    et al. 2017）。
- en: The number of research papers on model compression is growing. Off-the-shelf
    utilities are proliferating. Awesome Open Source has a list of [The Top 40 Model
    Compression Open Source Projects](https://awesomeopensource.com/projects/model-compression).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 关于模型压缩的研究论文数量在增长。现成的工具也在增加。Awesome Open Source列出了[前40个模型压缩开源项目](https://awesomeopensource.com/projects/model-compression)。
- en: '**Make hardware faster**'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提升硬件速度**'
- en: This is another research area that is booming. Big companies and startups alike
    are in a race to develop hardware that allows large ML models to do inference,
    even training, faster both on the cloud and especially on devices. IDC forecasts
    that by 2020, the combination of edge and mobile devices doing inferencing will [total
    3.7 billion units, with a further 116 million units doing training](https://www.arm.com/-/media/global/solutions/artificial-intelligence/ai-ml-on-cpu-whitepaper.pdf?revision=17a2b30b-0f5a-4a42-8681-3d9f3f94e513).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一个蓬勃发展的研究领域。大公司和初创公司都在竞相开发能够加快大型ML模型推理速度的硬件，包括云端和设备上的训练。IDC预测，到2020年，边缘和移动设备的推理总量将达到[37亿台，另外还有1.16亿台用于训练](https://www.arm.com/-/media/global/solutions/artificial-intelligence/ai-ml-on-cpu-whitepaper.pdf?revision=17a2b30b-0f5a-4a42-8681-3d9f3f94e513)。
- en: '**Real-time pipeline**'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时管道**'
- en: Suppose you have a ride-sharing app and want to detect fraudulent transactions,
    e.g., payments using stolen credit cards. When the true credit owner discovers
    unauthorized payments, they’ll dispute with their bank, and you’ll have to refund
    the charges. To maximize profits, fraudsters might call multiple rides either
    in succession or from multiple accounts. In 2019, merchants estimated fraudulent
    transactions account for an average of [27% of their annual online sales](https://network.americanexpress.com/globalnetwork/dam/jcr:09c34553-b4a2-43ca-bf3e-47cbc911ea51/American%20Express%202019%20Digital%20Payments%20Survey_Insights%20Paper.pdf).
    The longer it takes for you to detect the stolen credit card, the more money you’ll
    lose.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个共享出行应用，想要检测欺诈交易，例如，使用盗取的信用卡进行支付。当真正的信用卡持有人发现未经授权的支付时，他们会与银行争议，你将不得不退还费用。为了最大化利润，欺诈者可能会连续打车或从多个账户中打车。2019年，商家估计欺诈交易占其年在线销售额的平均[27%](https://network.americanexpress.com/globalnetwork/dam/jcr:09c34553-b4a2-43ca-bf3e-47cbc911ea51/American%20Express%202019%20Digital%20Payments%20Survey_Insights%20Paper.pdf)。你检测到盗用信用卡的时间越长，你损失的钱就越多。
- en: To detect whether a transaction is fraudulent, looking at that transaction alone
    isn’t enough. You need to at least look into the recent history of the user involved
    in that transaction, their recent trips and activities in-app, the credit card’s
    recent transactions, and other transactions happening around the same time.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要检测交易是否欺诈，仅仅查看该交易是不够的。你至少需要查看涉及该交易的用户的近期历史记录、他们在应用中的近期行程和活动、信用卡的近期交易，以及其他同时发生的交易。
- en: 'To quickly access these types of information, you want to keep as much of them
    in-memory as possible. Every time an event you care about happens – a user choosing
    a location, booking a trip, contacting a driver, canceling a trip, adding a credit
    card, removing a credit card, etc. – information about that event goes into your
    in-memory storage. It stays there for as long as they are useful (usually in the
    order of days), then either goes into permanent storage (e.g., S3) or is discarded.
    The most common tool for this is [Apache Kafka](https://github.com/apache/kafka),
    with alternatives such as Amazon Kinesis. Kafka is a stream storage: it stores
    data as it streams.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了快速访问这些信息，你希望尽可能将它们保存在内存中。每次发生你关心的事件——用户选择位置、预订行程、联系司机、取消行程、添加信用卡、删除信用卡等——该事件的信息都会存入你的内存存储中。这些信息会在有用的时间内保留（通常是几天），然后要么转入永久存储（例如S3），要么被丢弃。最常用的工具是[Apache
    Kafka](https://github.com/apache/kafka)，还有像Amazon Kinesis这样的替代品。Kafka是一种流存储：它在数据流动时存储数据。
- en: Streaming data is different from static data – data that already exists somewhere
    in its entirety, such as CSV files. When reading from CSV files, you know when
    the job is finished. Streams of data never finish.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 流数据与静态数据不同——静态数据是指已经存在的完整数据，例如CSV文件。当从CSV文件中读取时，你知道任务何时完成。而数据流是不断进行的。
- en: Once you’ve had a way to manage streaming data, you want to extract features
    to input into your ML models. On top of features from streaming data, you might
    also need features from static data (when was this account created, what’s the
    user’s rating, etc.). You need a tool that allows you to process streaming data
    as well as static data and join them together from various data sources.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了管理流数据的方法，你就会想要提取特征以输入到你的机器学习模型中。除了流数据的特征，你可能还需要静态数据的特征（例如，该账户何时创建，用户的评分等）。你需要一个工具，它允许你处理流数据和静态数据，并将它们从各种数据源中结合起来。
- en: '**Stream processing vs. batch processing**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**流处理与批处理**'
- en: People generally use “batch processing” to refer to static data processing because
    you can process them in batches. This is opposed to “stream processing,” which
    processes each event as it arrives. Batch processing is **efficient** – you can
    leverage tools like MapReduce to process large amounts of data. Stream processing
    is **fast** because you can process each piece of data as soon as it comes. Robert
    Metzger, a PMC member at Apache Flink, disputed that streaming processing could
    be as efficient as batch processing because [batch is a special case of streaming](https://www.ververica.com/blog/batch-is-a-special-case-of-streaming).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 人们通常使用“批处理”来指代静态数据处理，因为你可以批量处理这些数据。这与“流处理”相对，流处理是在每个事件到达时进行处理。批处理是**高效的**——你可以利用像
    MapReduce 这样的工具处理大量数据。流处理是**快速的**，因为你可以在数据到达时立即处理它。Apache Flink 的 PMC 成员 Robert
    Metzger 争辩说，流处理不可能像批处理一样高效，因为[批处理是流处理的一种特殊情况](https://www.ververica.com/blog/batch-is-a-special-case-of-streaming)。
- en: Processing stream data is more difficult because the data amount is unbounded,
    and the data comes in at variable rates and speeds. It’s easier to make a stream
    processor do batch processing than making a batch processor do stream processing.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 处理流数据更为困难，因为数据量是无限的，并且数据以不稳定的速率和速度到达。让流处理器进行批处理要比让批处理器进行流处理容易。
- en: Apache Kafka has some capacity for stream processing, and some companies use
    this capacity on top of their Kafka stream storage, but Kafka stream processing
    is limited in its ability to deal with various data sources. There have been efforts
    to extend SQL, the popular query language intended for static data tables, to
    handle data streams [[1](http://cs.brown.edu/~ugur/streamsql.pdf), [2](https://en.wikipedia.org/wiki/StreamSQL)].
    However, the most popular tool for stream processing is [Apache Flink](https://github.com/apache/flink),
    with native support for batch processing.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka 具有一定的流处理能力，一些公司在其 Kafka 流存储之上使用了这种能力，但 Kafka 流处理在处理各种数据源的能力上有限。已有努力扩展
    SQL，这种流行的查询语言旨在处理静态数据表，以处理数据流 [[1](http://cs.brown.edu/~ugur/streamsql.pdf), [2](https://en.wikipedia.org/wiki/StreamSQL)]。然而，最受欢迎的流处理工具是[Apache
    Flink](https://github.com/apache/flink)，它原生支持批处理。
- en: In the early days of machine learning production, many companies built their
    ML systems on top of their existing MapReduce/Spark/Hadoop data pipeline. When
    these companies want to do real-time inference, they need to build a separate
    pipeline for streaming data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习生产的早期阶段，许多公司将他们的机器学习系统建立在现有的 MapReduce/Spark/Hadoop 数据管道之上。当这些公司希望进行实时推理时，他们需要建立一个独立的流数据管道。
- en: Having two different pipelines to process your data is a common cause for bugs
    in ML production, e.g., the changes in one pipeline aren’t correctly replicated
    in the other, leading to two pipelines extracting two different sets of features.
    This is especially common if the two pipelines are maintained by two different
    teams, e.g., the development team maintains the batch pipeline for training while
    the deployment team maintains the stream pipeline for inference. Companies including [Uber](https://www.infoq.com/presentations/sql-streaming-apache-flink/) and [Weibo](https://www.youtube.com/watch?v=WQ520rWgd9A&ab_channel=FlinkForward) have
    made major infrastructure overhaul to unify their batch and stream processing
    pipelines with Flink.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有两个不同的管道来处理数据是机器学习生产中常见的错误来源，例如，一个管道中的变化未能正确地复制到另一个管道，导致两个管道提取了不同的特征集。如果这两个管道由两个不同的团队维护，则尤其常见，例如，开发团队维护用于训练的批处理管道，而部署团队维护用于推理的流处理管道。包括[Uber](https://www.infoq.com/presentations/sql-streaming-apache-flink/)和[微博](https://www.youtube.com/watch?v=WQ520rWgd9A&ab_channel=FlinkForward)在内的公司已经进行了重大基础设施改造，以使用
    Flink 统一它们的批处理和流处理管道。
- en: '**Event-driven vs. request-driven**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**事件驱动 vs. 请求驱动**'
- en: The software world has gone microservices in the last decade. The idea is to
    break your business logic into small components – each component is a self-contained
    service – that can be maintained independently. The owner of each component can
    update to and test that component quickly without having to consult the rest of
    the system.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，软件世界已经转向微服务。这个概念是将业务逻辑拆分成小组件——每个组件都是一个独立的服务——可以独立维护。每个组件的所有者可以快速更新和测试该组件，而不必咨询系统的其他部分。
- en: Microservices often go hand-in-hand with REST, a set of methods that let these
    microservices communicate. REST APIs are request-driven. A client (service) sends
    requests to tell its server exactly what to do via methods such as POST and GET,
    and its server responds with the results. A server has to listen to the request
    for the request to register.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务通常与 REST 一起使用，REST 是一组方法，用于让这些微服务进行通信。REST API 是请求驱动的。客户端（服务）通过 POST 和 GET
    等方法向服务器发送请求，指示服务器执行特定操作，服务器则以结果响应。服务器必须监听请求才能使请求注册。
- en: 'Because in a request-driven world, data is handled via requests to different
    services, no one has an overview of how data flows through the entire system.
    Consider a simple system with 3 services:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在请求驱动的世界中，数据通过向不同服务发出的请求进行处理，没有人能够概览数据如何流经整个系统。考虑一个包含 3 个服务的简单系统：
- en: A manages drivers availability
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A 管理司机可用性
- en: B manages ride demand
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: B 管理乘车需求
- en: C predicts the best possible price to show customers each time they request
    a ride
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C 预测每次客户请求乘车时的最佳价格
- en: 'Because prices depend on availability and demands, service C’s output depends
    on the outputs from service A and B. First, this system requires inter-service
    communication: C needs to ping A and B for predictions, A needs to ping B to know
    whether to mobilize more drivers and ping C to know what price incentive to give
    them. Second, there’d be no easy way to monitor how changes in A or B logics affect
    the performance of service C, or to map the data flow to debug if service C’s
    performance suddenly goes down.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于价格依赖于可用性和需求，服务 C 的输出依赖于服务 A 和 B 的输出。首先，该系统需要服务间通信：C 需要向 A 和 B 请求预测，A 需要向 B
    请求以了解是否需要调动更多司机，并向 C 请求以了解应给予司机什么价格激励。其次，监控 A 或 B 逻辑的变化如何影响服务 C 的性能，或者映射数据流以调试服务
    C 性能突然下降的问题，将变得非常困难。
- en: With only 3 services, things are already getting complicated. Imagine having
    hundreds, if not thousands, of services like what major Internet companies have.
    Inter-service communication would blow up. Sending data as JSON blobs over HTTP
    – the way REST requests are commonly made – is also slow. Inter-service data transfer
    can become a bottleneck, slowing down the entire system.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅是 3 个服务，事情已经变得复杂。想象一下拥有数百个，甚至上千个服务的情况，就像大型互联网公司那样。服务间通信将会爆炸。以 JSON 数据块通过 HTTP
    发送数据——这是 REST 请求的常见方式——也很慢。服务间数据传输可能成为瓶颈，减缓整个系统的速度。
- en: 'Instead of having 20 services ping service A for data, what if whenever an
    event happens within service A, this event is broadcasted to a stream, and whichever
    service wants data from A can subscribe to that stream and pick out what it needs?
    What if there’s a stream all services can broadcast their events and subscribe
    to? This model is called pub/sub: publish & subscribe. This is what solutions
    like Kafka allow you to do. Since all data flows through a stream, you can set
    up a dashboard to monitor your data and its transformation across your system.
    Because it’s based on events broadcasted by services, this architecture is event-driven.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不让 20 个服务向服务 A 请求数据，而是每当服务 A 内发生事件时，这个事件被广播到一个流中，而任何需要 A 数据的服务可以订阅这个流并提取所需的内容，这会怎样？如果所有服务都可以广播它们的事件并订阅一个流，这种模型被称为
    pub/sub：发布 & 订阅。这就是像 Kafka 这样的解决方案所允许的。由于所有数据都通过流进行传输，你可以设置一个仪表板来监控数据及其在系统中的变化。由于它基于服务广播的事件，这种架构是事件驱动的。
- en: '![](../Images/4ceecb97d4d7ef3d243f242fa13277c7.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ceecb97d4d7ef3d243f242fa13277c7.png)'
- en: '*[Beyond Microservices: Streams, State and Scalability](https://www.infoq.com/presentations/microservices-streams-state-scalability/) (Gwen
    Shapira, QCon 2019).*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*[超越微服务：流、状态与可扩展性](https://www.infoq.com/presentations/microservices-streams-state-scalability/) (Gwen
    Shapira, QCon 2019)。*'
- en: Request-driven architecture works well for systems that rely more on logics
    than on data. Event-driven architecture works better for systems that are data-heavy.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请求驱动的架构适用于依赖于逻辑而非数据的系统。事件驱动的架构更适合数据密集型的系统。
- en: '**Challenges**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**挑战**'
- en: Many companies are switching from batch processing to stream processing, from
    request-driven architecture to event-driven architecture. My impression from talking
    to major Internet companies in the US and China is that this change is still slow
    in the US but much faster in China. The adoption of streaming architecture is
    tied to the popularity of Kafka and Flink. Robert Metzger told me that he observed
    more machine learning workloads with Flink in Asia than in the US. Google Trends
    for “Apache Flink” is consistent with this observation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司正在从批处理转向流处理，从请求驱动架构转向事件驱动架构。我与美国和中国主要互联网公司交流后的印象是，美国的这种变化仍然较慢，而中国的速度要快得多。流处理架构的采用与Kafka和Flink的流行程度密切相关。罗伯特·梅茨格告诉我，他观察到亚洲的Flink机器学习工作负载比美国更多。Google
    Trends中“Apache Flink”的数据与这一观察结果一致。
- en: '![](../Images/57fdf04eedadbcf1c20c288a28d4c694.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57fdf04eedadbcf1c20c288a28d4c694.png)'
- en: There are many reasons why streaming isn’t more popular.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理不够流行有很多原因。
- en: '**Companies don’t see the benefits of streaming**'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**公司看不到流处理的好处**'
- en: Their system isn’t at a scale where inter-service communication is a bottleneck.
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们的系统规模尚未达到服务间通信成为瓶颈的程度。
- en: They don’t have applications that benefit from online predictions.
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们没有从在线预测中受益的应用程序。
- en: They have applications that might benefit from online predictions, but they
    don’t know that yet because they have never done online predictions before.
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们有可能从在线预测中受益的应用程序，但由于他们之前从未进行过在线预测，因此尚未意识到这一点。
- en: '**High initial investment in infrastructure**'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**高初期基础设施投资**'
- en: Infrastructure updates are expensive and can jeopardize existing applications.
    Managers might not be willing to invest in upgrading their infra to allow online
    predictions.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基础设施更新成本高昂，可能会危及现有应用程序。经理们可能不愿意投资升级基础设施以支持在线预测。
- en: '**Mental shift**'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**心理转变**'
- en: Switching from batch processing to stream processing requires a mental shift.
    With batch processing, you know when a job is done. With stream processing, it’s
    never done. You can make rules such as get the average of all data points in the
    last 2 minutes, but what if an event that happened 2 minutes ago got delayed and
    hasn’t entered the stream yet? With batch processing, you can have well-defined
    tables and join them, but in streaming, there are no tables to join, then what
    does it mean to do a join operation on two streams?
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从批处理切换到流处理需要心理转变。使用批处理，你知道一个任务什么时候完成。使用流处理，它永远不会完成。你可以制定规则，比如获取过去2分钟内所有数据点的平均值，但如果发生在2分钟前的事件被延迟了，还没有进入流中呢？使用批处理，你可以有定义明确的表并将其连接起来，但在流处理中没有表可以连接，那么对两个流进行连接操作意味着什么？
- en: '**Python incompatibility**'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Python不兼容**'
- en: Python is the lingua franca of machine learning, whereas Kafka and Flink run
    on Java and Scala. Introducing streaming might create language incompatibility
    in the workflows. Apache Beam provides a Python interface on top of Flink for
    communicating with streams, but you’d still need people who can work with Java/Scala.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Python是机器学习的通用语言，而Kafka和Flink运行在Java和Scala上。引入流处理可能会在工作流程中造成语言不兼容。Apache Beam在Flink之上提供了一个Python接口用于与流进行通信，但你仍然需要能够使用Java/Scala的人。
- en: '**Higher processing cost**'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更高的处理成本**'
- en: Batch processing means you can use your computing resources more efficiently.
    If your hardware is capable of processing 1000 data points at a time, it’s wasteful
    to use it to process only 1 data point at a time.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 批处理意味着你可以更有效地利用计算资源。如果你的硬件能够一次处理1000个数据点，那么仅处理1个数据点就是浪费。
- en: 'Level 2: Online learning - your system can incorporate new data and update
    in real-time'
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第二级：在线学习 - 你的系统可以实时整合新数据并进行更新
- en: '***Real-time** here is defined to be in the order of minutes*'
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***实时**在这里定义为以分钟为单位*'
- en: '**Defining "online learning"**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义“在线学习”**'
- en: 'I used “online learning” instead of “online training” because the latter term
    is contentious. By definition, online training means learning from each incoming
    data point. Very, very few companies actually do this because:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用“在线学习”而不是“在线训练”，因为后者的定义有争议。根据定义，在线训练意味着从每个进入的数据点中学习。实际上，极少有公司做到这一点，因为：
- en: This method suffers from catastrophic forgetting – neural networks abruptly
    forget previously learned information upon learning new information.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法遭遇灾难性遗忘——神经网络在学习新信息时会突然忘记先前学到的信息。
- en: It can be more expensive to run a learning step on only one data point than
    on a batch (this can be mitigated by having hardware just powerful enough to process
    exactly one data point).
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在仅用一个数据点进行学习的步骤可能比批量处理更昂贵（这可以通过拥有足够强大的硬件来处理恰好一个数据点来缓解）。
- en: Even if a model is learning with each incoming data point, it doesn’t mean the
    new weights are deployed after each data point. With our current limited understanding
    of how ML algorithms learn, the updated model needs to be evaluated first to see
    how well it does.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 即使一个模型在每次接收到数据点时都在学习，也不意味着每个数据点后都会部署新的权重。由于我们目前对机器学习算法的理解有限，更新的模型需要先进行评估，以查看其效果如何。
- en: For most companies that do so-called online training, their models learn in
    micro-batches and are evaluated after a certain period of time. Only after its
    performance is evaluated to be satisfactory is the model deployed wider. For Weibo,
    their iteration cycle from learning to deploying model updates is 10 minutes.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数进行所谓在线训练的公司，它们的模型在微批量中进行学习，并在一定时间后进行评估。只有在模型性能被评估为满意后，才会进行更广泛的部署。对于微博而言，它们的学习到部署模型更新的迭代周期为10分钟。
- en: '![](../Images/2b26a0ab4dcd1b6e180e6e7347bb5be9.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b26a0ab4dcd1b6e180e6e7347bb5be9.png)'
- en: '*[Machine learning with Flink in Weibo](https://www.youtube.com/watch?v=WQ520rWgd9A) (Qian
    Yu, Flink Forward 2020).*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*[微博中的Flink机器学习](https://www.youtube.com/watch?v=WQ520rWgd9A) (Qian Yu, Flink
    Forward 2020).*'
- en: '**Use cases**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**用例**'
- en: TikTok is incredibly addictive. Its secret lies in its [recommendation systems](https://newsroom.tiktok.com/en-us/how-tiktok-recommends-videos-for-you) that
    can learn your preferences quickly and suggest videos that you are likely to watch
    next, giving its users an incredible scrolling experience. It’s possible because
    ByteDance, the company behind TikTok, has set up a mature infrastructure that
    allows their recommendation systems to learn their user preferences (“user profiles”
    in their lingo) in real-time.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: TikTok 非常令人上瘾。它的秘密在于其 [推荐系统](https://newsroom.tiktok.com/en-us/how-tiktok-recommends-videos-for-you) 可以迅速学习你的偏好并建议你可能会观看的下一个视频，为用户提供了令人惊叹的滚动体验。这是因为
    TikTok 的背后公司字节跳动建立了成熟的基础设施，使其推荐系统能够实时学习用户偏好（在他们的术语中称为“用户画像”）。
- en: Recommendation systems are perfect candidates for online learning. They have
    natural labels – if a user clicks on a recommendation, it’s a correct prediction.
    Not all recommendation systems need online learning. User preferences for items
    like houses, cars, flights, hotels are unlikely to change from a minute to the
    next, so it would make little sense for systems to continually learn. However,
    user preferences for online content – videos, articles, news, tweets, posts, memes
    – can change very quickly (“I just read that octopi sometimes punch fish for no
    reason, and now I want to see a video of it”). As preferences for online content
    change in real-time, ad systems also need to be updated in real-time to show relevant
    ads.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统是在线学习的完美候选者。它们具有自然标签——如果用户点击了推荐内容，那就是正确的预测。并非所有的推荐系统都需要在线学习。用户对房屋、汽车、航班、酒店等物品的偏好不太可能在短时间内改变，因此系统持续学习意义不大。然而，用户对在线内容（如视频、文章、新闻、推文、帖子、迷因）的偏好可以迅速变化（“我刚刚读到章鱼有时无缘无故地打鱼，现在我想看一个相关的视频”）。随着在线内容偏好的实时变化，广告系统也需要实时更新以展示相关广告。
- en: Online learning is crucial for systems to adapt to rare events. Consider online
    shopping on Black Friday. Because Black Friday happens only once a year, there’s
    no way Amazon or other e-commerce sites can get enough historical data to learn
    how users are going to behave that day, so their systems need to continually learn
    on that day to adapt.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习对系统适应稀有事件至关重要。以黑色星期五的在线购物为例。由于黑色星期五一年只有一次，亚马逊或其他电商网站无法获得足够的历史数据来学习用户当天的行为，因此它们的系统需要在当天持续学习以适应。
- en: Or consider a Twitter search when someone famous tweets something stupid. For
    example, as soon as the news about “Four Seasons Total Landscaping” went live,
    many people were going to search “total landscaping.” If your system doesn’t immediately
    learn that “total landscaping” here refers to the press conference, your users
    are going to get a lot of gardening recommendations.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 或者考虑一下当某个名人发推特时的Twitter搜索。例如，当“四季总景观”的新闻一发布，很多人都会去搜索“总景观”。如果你的系统没有立即学习到“总景观”这里指的是新闻发布会，你的用户会得到很多园艺推荐。
- en: Online learning can also help with the cold start problem. A user just joined
    your app, and you have no information on them yet. If you don’t have the capacity
    for any form of online learning, you’ll have to serve your users generic recommendations
    until the next time your model is trained offline.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习也可以帮助解决冷启动问题。一个用户刚刚加入你的应用程序，而你还没有关于他们的信息。如果你没有任何形式的在线学习能力，你将不得不为用户提供通用推荐，直到下一次离线训练模型时。
- en: '**Solutions**'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案**'
- en: Since online learning is still fairly new and most companies who are doing it
    aren’t talking publicly about it in detail yet, there’s no standard solution.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在线学习仍然比较新，而且大多数实施它的公司尚未公开详细信息，所以没有标准解决方案。
- en: Online learning doesn’t mean “no batch training.” The companies that have most
    successfully used online learning also train their models offline in parallel
    and then combine the online version with the offline version.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习并不意味着“没有批量训练”。那些最成功地使用在线学习的公司通常也会将模型离线训练与在线版本结合起来。
- en: '**Challenges**'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**挑战**'
- en: There are many challenges facing online learning, both theoretical and practical.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习面临许多挑战，包括理论上的和实际上的。
- en: '**Theoretical**'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理论**'
- en: Online learning flips a lot of what we’ve learned about machine learning on
    its head. In introductory machine learning classes, students are probably taught
    different versions of “train your model with a sufficient number of epochs until
    convergence.” In online learning, there’s no epoch – your model sees each data
    point only once. There’s no such thing as convergence either. Your underlying
    data distribution keeps on shifting. There’s nothing stationary to converge to.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习颠覆了我们对机器学习的许多传统认识。在初级机器学习课程中，学生们可能会学习到“用足够的轮次训练模型直到收敛”的不同版本。然而，在在线学习中，没有轮次——你的模型每次只看到一个数据点。也没有所谓的收敛。你的基础数据分布不断变化，没有任何静态的东西可以收敛。
- en: Another theoretical challenge for online learning is model evaluation. In traditional
    batch training, you evaluate your models on stationary held out test sets. If
    a new model performs better than the existing model on the same test set, we say
    the new model is better. However, the goal of online learning is to adapt your
    model to constantly changing data. If your updated model is trained to adapt to
    data now, and we know that data now is different from data in the past, it wouldn’t
    make sense to use old data to test your updated model.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习的另一个理论挑战是模型评估。在传统的批量训练中，你会在静态的保留测试集上评估模型。如果一个新模型在相同的测试集上表现优于现有模型，我们会说新模型更好。然而，在线学习的目标是使模型适应不断变化的数据。如果你的更新模型是为了适应现在的数据，而我们知道现在的数据与过去的数据不同，那么用旧数据来测试更新后的模型就没有意义了。
- en: Then how do we know that the model trained on data from the last 10 minutes
    is better than the model trained on data from 20 minutes ago? We have to compare
    these two models on current data. Online training demands online evaluation, but
    serving a model that hasn’t been tested on users sounds like a recipe for disaster.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何知道过去10分钟的数据训练的模型比20分钟前的数据训练的模型更好呢？我们必须在当前数据上比较这两个模型。在线训练要求在线评估，但提供一个未经用户测试的模型似乎是灾难的配方。
- en: Many companies do it anyway. New models are first subjected to offline tests
    to make sure they aren’t disastrous, then evaluated online in parallel with the
    existing models via a complex A/B testing system. Only when a model is shown to
    be better than an existing model in some metrics the company cares about that
    it can be deployed wider. (Don’t get me started on choosing a metric for online
    evaluation).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司还是会这样做。新的模型首先会进行离线测试，以确保它们不会造成灾难，然后与现有模型通过复杂的A/B测试系统在线评估。只有当一个模型在公司关心的一些指标上表现优于现有模型时，它才能被广泛部署。（别让我开始谈选择在线评估指标的问题）。
- en: '**Practical**'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实际**'
- en: There are not yet standard infrastructures for online training. Some companies
    have converged to streaming architecture with [parameter servers](https://web.eecs.umich.edu/~mosharaf/Readings/Parameter-Server.pdf),
    but other than that, companies that do online training that I’ve talked to have
    to build a lot of their infrastructures in house. I’m reluctant to discuss this
    online since some companies asked me to keep this information confidential because
    they’re building solutions for them – it’s their competitive advantage.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 目前还没有标准的在线培训基础设施。一些公司已经趋向于使用[参数服务器](https://web.eecs.umich.edu/~mosharaf/Readings/Parameter-Server.pdf)的流式架构，但除了这些之外，我谈到的进行在线培训的公司必须在内部建立许多基础设施。我不愿意在线讨论这个问题，因为一些公司要求我保密这些信息，因为他们正在为其构建解决方案——这是他们的竞争优势。
- en: The MLOps race between the US and China
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 中美MLOps竞争
- en: I’ve read a lot about the AI race between the US and China, but most comparisons
    seem to focus on the number of [research papers, patents, citations, and funding](https://datainnovation.org/2019/08/who-is-winning-the-ai-race-china-the-eu-or-the-united-states/).
    Only after I’ve started talking to both American and Chinese companies about real-time
    machine learning that I noticed a staggering difference in their MLOps infrastructures.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我阅读了很多关于中美人工智能竞争的文章，但大多数比较似乎集中在[研究论文、专利、引用和资金](https://datainnovation.org/2019/08/who-is-winning-the-ai-race-china-the-eu-or-the-united-states/)的数量上。只有在我开始与中美两国公司讨论实时机器学习后，我才发现他们的MLOps基础设施存在惊人的差异。
- en: Few American Internet companies have attempted online learning, and even among
    these companies, online learning is used for simple models such as logistic regression.
    My impression from both talking directly to Chinese companies and talking with
    people who have worked with companies in both countries is that online learning
    is more common in China, and Chinese engineers are more eager to make the jump.
    You can see some of the conversations [here](https://twitter.com/chipro/status/1337077324936663040) and [here](https://www.linkedin.com/posts/chiphuyen_mlops-machinelearning-activity-6742844916705177600-taRd).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 很少有美国互联网公司尝试在线学习，即使在这些公司中，在线学习也仅用于如逻辑回归等简单模型。通过直接与中国公司交流以及与在中美两国公司工作过的人交谈，我的印象是在线学习在中国更为普遍，中国工程师更愿意尝试这种技术。你可以在[这里](https://twitter.com/chipro/status/1337077324936663040)和[这里](https://www.linkedin.com/posts/chiphuyen_mlops-machinelearning-activity-6742844916705177600-taRd)查看一些对话。
- en: '![](../Images/11a1db19b83783be99737342f17b6bea.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11a1db19b83783be99737342f17b6bea.png)'
- en: Conclusion
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: Machine learning is going real-time, whether you’re ready or not. While the
    majority of companies are still debating whether there’s value in online inference
    and online learning, some of those who do it correctly have already seen returns
    on investment, and their real-time algorithms might be a major contributing factor
    that helps them stay ahead of their competitors.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习正趋向实时，无论你是否准备好。虽然大多数公司仍在争论在线推理和在线学习的价值，但那些正确实施的公司已经看到了投资回报，他们的实时算法可能是帮助他们领先竞争对手的主要因素。
- en: I have a lot more thoughts on real-time machine learning, but this post is already
    long. If you’re interested in chatting about this, shoot me an email.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我对实时机器学习有很多想法，但这篇文章已经很长。如果你有兴趣讨论这个话题，可以给我发邮件。
- en: Acknowledgments
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 致谢
- en: This post is a synthesis of many conversations with the following wonderful
    engineers and academics. I’d like to thank Robert Metzger, Neil Lawrence, Savin
    Goyal, Zhenzhong Xu, Ville Tuulos, Dat Tran, Han Xiao, Hien Luu, Ledio Ago, Peter
    Skomoroch, Piero Molino, Daniel Yao, Jason Sleight, Becket Qin, Tien Le, Abraham
    Starosta, Will Deaderick, Caleb Kaiser, Miguel Ramos.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章是许多与以下优秀工程师和学者对话的总结。我想感谢Robert Metzger、Neil Lawrence、Savin Goyal、Zhenzhong
    Xu、Ville Tuulos、Dat Tran、Han Xiao、Hien Luu、Ledio Ago、Peter Skomoroch、Piero Molino、Daniel
    Yao、Jason Sleight、Becket Qin、Tien Le、Abraham Starosta、Will Deaderick、Caleb Kaiser、Miguel
    Ramos。
- en: '[Original](https://huyenchip.com/2020/12/27/real-time-machine-learning.html).
    Reposted with permission.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://huyenchip.com/2020/12/27/real-time-machine-learning.html)。经许可转载。'
- en: '**Bio:** [Chip Huyen](https://twitter.com/chipro) is a writer and computer
    scientist. She works to bring the best engineering practices to machine learning
    research and production. She writes about culture, people, and tech.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：** [Chip Huyen](https://twitter.com/chipro) 是一位作家和计算机科学家。她致力于将最佳工程实践带入机器学习研究和生产中。她还撰写关于文化、人和科技的文章。'
- en: '**Related:**'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[The ravages of concept drift in stream learning applications and how to deal
    with it](https://www.kdnuggets.com/2019/12/ravages-concept-drift-stream-learning-applications.html)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[概念漂移在流学习应用中的破坏及应对方法](https://www.kdnuggets.com/2019/12/ravages-concept-drift-stream-learning-applications.html)'
- en: '[Recommendation Engines and Real-time personalization – download guidebook](https://www.kdnuggets.com/2017/10/dataiku-recommendation-engines-real-time-personalization-download-guidebook.html)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[推荐引擎与实时个性化 – 下载指南](https://www.kdnuggets.com/2017/10/dataiku-recommendation-engines-real-time-personalization-download-guidebook.html)'
- en: '[How to Use MLOps for an Effective AI Strategy](https://www.kdnuggets.com/2021/01/mlops-effective-ai-strategy.html)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用MLOps制定有效的AI战略](https://www.kdnuggets.com/2021/01/mlops-effective-ai-strategy.html)'
- en: '* * *'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 为你的组织提供IT支持'
- en: '* * *'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[How to Stay on Top of What''s Going on in the AI World](https://www.kdnuggets.com/2022/03/stay-top-going-ai-world.html)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何跟上人工智能领域的最新动态](https://www.kdnuggets.com/2022/03/stay-top-going-ai-world.html)'
- en: '[Sentiment Analysis in Python: Going Beyond Bag of Words](https://www.kdnuggets.com/sentiment-analysis-in-python-going-beyond-bag-of-words)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python中的情感分析：超越词袋模型](https://www.kdnuggets.com/sentiment-analysis-in-python-going-beyond-bag-of-words)'
- en: '[5 Machine Learning Skills Every Machine Learning Engineer Should…](https://www.kdnuggets.com/2023/03/5-machine-learning-skills-every-machine-learning-engineer-know-2023.html)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个机器学习工程师都应该掌握的5项机器学习技能…](https://www.kdnuggets.com/2023/03/5-machine-learning-skills-every-machine-learning-engineer-know-2023.html)'
- en: '[KDnuggets News, December 14: 3 Free Machine Learning Courses for…](https://www.kdnuggets.com/2022/n48.html)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，12月14日：3个免费的机器学习课程…](https://www.kdnuggets.com/2022/n48.html)'
- en: '[A Solid Plan for Learning Data Science, Machine Learning, and Deep Learning](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学、机器学习和深度学习的稳固计划](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
- en: '[AI, Analytics, Machine Learning, Data Science, Deep Learning…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能、分析、机器学习、数据科学、深度学习…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
