- en: What are the Assumptions of XGBoost?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/08/assumptions-xgboost.html](https://www.kdnuggets.com/2022/08/assumptions-xgboost.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![What are the Assumptions of XGBoost?](../Images/640a6c40bc036eae9847f9795f190915.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Faye Cornish](https://unsplash.com/@fcornish) via Unsplash'
  prefs: []
  type: TYPE_NORMAL
- en: Before we get into the assumptions of XGBoost, I will do an overview of the
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost stands for Extreme Gradient Boosting and is a supervised learning algorithm
    and falls under the gradient-boosted decision tree (GBDT) family of machine learning
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive into boosting first.
  prefs: []
  type: TYPE_NORMAL
- en: From Boosting to XGBoost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Boosting is the method of combining a set of weak learners into a strong learner
    to reduce the level of training errors. Boosting helps to deal with bias-variance
    trade-off making it more effective. There are different types of boosting algorithms,
    such as AdaBoost (Adaptive Boosting), Gradient Boosting, XGBoost, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s go into XGBoost
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, XGBoost is an extension to gradient boosted decision trees
    (GBM) and is well known for its speed and performance.
  prefs: []
  type: TYPE_NORMAL
- en: Predictions are made based on combining a set of simpler, weaker models - these
    models are decision trees that are created in sequential form. These models make
    predictions based on evaluating other decision trees through if-then-else true/false
    feature questions, using these to assess and estimate the probability of producing
    a correct decision.
  prefs: []
  type: TYPE_NORMAL
- en: 'It consists of these three elements:'
  prefs: []
  type: TYPE_NORMAL
- en: A loss function that is to be optimized.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A weak learner to make predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An additive model to add to the weak learners to reduce errors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Features of XGBoost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are 3 features of XGBoost:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Gradient Tree Boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The tree ensemble model needs to be trained in an additive manner. Meaning that
    it is an iterative and sequential process where decision trees are added one step
    at a time. There are a fixed number of trees added and with each iteration, there
    should be a reduction in loss function value.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Regularized Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regularized Learning helps to minimize the loss function and prevent overfitting
    or underfitting from occurring - helping to smooth out the final learnt weight.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Shrinkage and Feature Subsampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These two techniques are used to further prevent overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Shrinkage reduces the level of influence each tree has on the overall model
    and allows room for future trees to potentially improve it.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Subsampling is something that you may have seen in the Random Forest
    algorithm. The features are in the column section of the data and not only does
    it prevent overfitting, but it also speeds up computations of the parallel algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost Hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'XGBoost hyperparameters are divided into 4 groups:'
  prefs: []
  type: TYPE_NORMAL
- en: General parameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Booster parameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learning task parameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Command line parameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: General parameters, Booster parameters and Task parameters are set before running
    the XGBoost model. The Command line parameters are only used in the console version
    of XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: If the parameters are not tuned properly, it can easily lead to overfitting.
    However, it is difficult to tune the parameters of an XGBoost model.
  prefs: []
  type: TYPE_NORMAL
- en: Stay tuned for an upcoming article about Tuning XGBoost Hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: What Are the Assumptions of XGBoost?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main assumptions of XGBoost are:'
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost may assume that encoded integer values for each input variable have
    an ordinal relationship
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost assume that your data may not be complete (i.e. it can deal with missing
    values)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As it DOES NOT assume that all values are present, the algorithm can handle
    missing values by default. When working with tree based algorithms, missing values
    are learned during the training phase. This then leads to the fact that:'
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost can handle sparsity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost manages only numeric vectors, therefore if you have categorical variables
    they will need to be converted into numeric variables.
  prefs: []
  type: TYPE_NORMAL
- en: You will have to transform a dense dataframe with few zeroes in the matrix to
    a very sparse matrix which has lots of zero in the matrix. This means that XGBoost
    has the ability to convert variables into a sparse matrix format as an input.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this blog, you have come to understand: how boosting relates to XGBoost;
    the features of XGBoost; how it reduces the loss function value and overfitting.
    We have briefly gone over the 4 hyperparameters which will be followed up with
    an article which solely focuses on Tuning XGBoost Hyperparameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and Freelance Technical Writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How to Speed Up XGBoost Model Training](https://www.kdnuggets.com/2021/12/speed-xgboost-model-training.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Tuning XGBoost Hyperparameters](https://www.kdnuggets.com/2022/08/tuning-xgboost-hyperparameters.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Leveraging XGBoost for Time-Series Forecasting](https://www.kdnuggets.com/2023/08/leveraging-xgboost-timeseries-forecasting.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[WTF is the Difference Between GBM and XGBoost?](https://www.kdnuggets.com/wtf-is-the-difference-between-gbm-and-xgboost)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
