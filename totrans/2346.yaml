- en: Ensemble Learning with Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Ensemble Learning with Examples](../Images/3b8286cd091c64374aef205988db0dab.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: What is Ensemble Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning combines multiple models to obtain better model performance.
    It helps you improve robustness and provide a generalized model. In short, it
    combines different decisions from the model to improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we will learn about various ensemble methods with examples.
    We will be using the [Heart Attack Analysis & Prediction](https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset)
    dataset from Kaggle. Our focus is to find the patents with higher and lower chances
    of heart attack using various features.
  prefs: []
  type: TYPE_NORMAL
- en: '**Output :**'
  prefs: []
  type: TYPE_NORMAL
- en: 0 = less chance of heart attack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 = more chance of heart attack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Averaging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start by importing the required utility and machine learning packages.
  prefs: []
  type: TYPE_NORMAL
- en: Simple Average
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will take a simple average by adding all of the models' output and dividing
    it by the total number of models.
  prefs: []
  type: TYPE_NORMAL
- en: Loaded the heart.csv file using pandas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The target is the “output” feature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drop “output” to create training features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scaled the features using StandardScaler()
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the data into train and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build all three model objects. We are using RandomForestClassifier, LogisticRegression,
    and SGDClassifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training the model on a train set and predicting the output using a test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the average formula and rounding the output to show only 1s and 0s.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Displaying accuracy and AUC metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our model has performed well on default hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Weighted Average
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the weighted average, we give the highest weight to best-performing models
    and the lowest weight to lower-performing models while taking an average.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **model_1** | **model_2** | **model_3** |'
  prefs: []
  type: TYPE_TB
- en: '| **Weightage** | 30% | 60% | 10% |'
  prefs: []
  type: TYPE_TB
- en: The code below shows that by giving more weightage to **model_2,** we have achieved
    better performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** while giving weightage, make sure they all add up to 1\. For example,
    0.3 + 0.6 + 0.1 = 1'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Max Voting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Max Voting is generally used for classification problems. In this method, each
    model makes a prediction and votes for each sample. From the sample class, only
    the highest-voted class is included in the final predictive class.
  prefs: []
  type: TYPE_NORMAL
- en: '![Ensemble Learning with Examples](../Images/b0ec751c949da1bf325fd079623b8a7f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [rasbt.github.io](http://rasbt.github.io/mlxtend/user_guide/classifier/EnsembleVoteClassifier/)
  prefs: []
  type: TYPE_NORMAL
- en: 'In the example below, I have added a fourth model KNeighborsClassifier. We
    will use Scikit-Learn’s VotingClassifier and add three classifiers: RandomForestClassifier,
    LogisticRegression, and KNeighborsClassifier.'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, we got better results than the simple average.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Stacking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stacking combines multiple base models via meta-model (meta-classifier or meta-regression).
    The base models are trained on a full dataset. The meta-model is trained on the
    features from base models(outputs). The base models and the meta-model are generally
    different. In short, meta-models help base models to find useful features to achieve
    high performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Ensemble Learning with Examples](../Images/a944ca4e7cd30b427e03b02410333a6a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [opengenus.org](https://iq.opengenus.org/content/images/2019/08/stacking.PNG)
  prefs: []
  type: TYPE_NORMAL
- en: In the example, we are going to use Random forest, Logistic regression, and
    AdaBoost classifier for base models (estimators) and GradientBoosting classifier
    as meta-model (final_estimator).
  prefs: []
  type: TYPE_NORMAL
- en: Stacking the models did not produce better results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Bagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bagging, also known as Bootstrap Aggregating, is an ensemble method to improve
    the stability and accuracy of machine learning models. It is used for minimizing
    variance and overfitting. Generally, it is applied to decision tree methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![Ensemble Learning with Examples](../Images/570d620f6e7244bcaf1398f42b05c5ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Bagging by [Fernando López](https://towardsdatascience.com/ensemble-learning-bagging-boosting-3098079e5422)
  prefs: []
  type: TYPE_NORMAL
- en: Bagging randomly creates a subset of training data to get a fair distribution
    of the whole dataset and train the models on it. The final output is created by
    combining all of the output from the base model.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we will use one type of model and train it on various subsets of
    training data. A subset is also known as a bag hence the bagging algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In the example, we are using Scikit-learn's BaggingClassifier and utilizing
    Logistic regression as the base estimator.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging algorithm has produced the best and reliable result.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Boosting combines weak classifiers to create a strong classifier. It reduces
    biases and improves the single model performance.
  prefs: []
  type: TYPE_NORMAL
- en: You can achieve high performance by training weak classifiers in series. The
    first model is training on training data, then the second model is trained to
    correct the error present in the first model. It is an iterative algorithm that
    adjusts weights based on the previous model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Ensemble Learning with Examples](../Images/2c12322a42716fd36ddbe12ac238a859.png)'
  prefs: []
  type: TYPE_IMG
- en: Boosting by [Fernando López](https://towardsdatascience.com/ensemble-learning-bagging-boosting-3098079e5422)
  prefs: []
  type: TYPE_NORMAL
- en: The boosting algorithm gives more weightage to the observations that previous
    models have predicted accurately. This process continues until all training data
    sets are predicted correctly or it has reached the maximum number of models.
  prefs: []
  type: TYPE_NORMAL
- en: In the example, we will be using the AdaBoost classifier. The adaptive boosting
    algorithm (AdaBoost) is the OG technique of combining weak classifiers into a
    single powerful classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though ensemble learning should be applied to all machine learning applications,
    in the case of large neural networks, you need to consider throughput, computation,
    and cost of operation to decide whether to train and serve multiple models or
    not.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we have learned the importance of ensemble learning. Furthermore,
    we have learned about averaging, max voting, stacking, bagging, and boosting with
    code examples.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you like the tutorial, and if you have any questions regarding the best
    possible technique, do comment in the section below.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Abid Ali Awan](https://www.polywork.com/kingabzpro)** ([@1abidaliawan](https://twitter.com/1abidaliawan))
    is a certified data scientist professional who loves building machine learning
    models. Currently, he is focusing on content creation and writing technical blogs
    on machine learning and data science technologies. Abid holds a Master''s degree
    in Technology Management and a bachelor''s degree in Telecommunication Engineering.
    His vision is to build an AI product using a graph neural network for students
    struggling with mental illness.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[When Would Ensemble Techniques be a Good Choice?](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Picking Examples to Understand Machine Learning Model](https://www.kdnuggets.com/2022/11/picking-examples-understand-machine-learning-model.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SQL LIKE Operator Examples](https://www.kdnuggets.com/2022/09/sql-like-operator-examples.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Solid Plan for Learning Data Science, Machine Learning, and Deep Learning](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AI, Analytics, Machine Learning, Data Science, Deep Learning…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
