- en: Stacking Models for Improved Predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/02/stacking-models-imropved-predictions.html](https://www.kdnuggets.com/2017/02/stacking-models-imropved-predictions.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**By Burak Himmetoglu, UC Santa Barbara.**'
  prefs: []
  type: TYPE_NORMAL
- en: If you have ever competed in a Kaggle competition, you are probably familiar
    with the use of combining different predictive models for improved accuracy which
    will creep your score up in the leader board. While it is widely used, there are
    only a few resources that I am aware of where a clear description is available
    (One that I know of is [here](http://mlwave.com/kaggle-ensembling-guide/), and
    there is also a [caret package extension](https://cran.r-project.org/web/packages/caretEnsemble/vignettes/caretEnsemble-intro.html)
    for it). Therefore,  I will try to workout a simple example here to illustrate
    how different models can be combined. The example I have chosen is the [House
    Prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)
    competition from Kaggle. This is a regression problem and given lots of features
    about houses, one is expected to predict their prices on a test set. I will use
    three different regression methods to create predictions (XGBoost, Neural Networks,
    and Support Vector Regression) and stack them up to produce a final prediction.
    I assume that the reader is familiar with R, Xgboost and caret packages, as well
    as support vector regression and neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main idea of constructing a predictive model by combining different models
    can be schematically illustrated as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![workflow](../Images/3af5043b368b3b859dfdfb58d330a072.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let me describe the key points in the figure:'
  prefs: []
  type: TYPE_NORMAL
- en: Initial training data (**X**) has *m* observations, and *n* features (so it
    is *m x n*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are M different models that are trained on X (by some method of training,
    like cross-validation) before hand.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each model provides predictions for the outcome (y) which are then cast into
    a second level training data (Xl2) which is now *m x M*. Namely, the M predictions
    become features for this second level data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A second level model (or models) can then be trained on this data to produce
    the final outcomes which will be used for predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are several ways that the second level data (Xl2) can be built. Here,
    I will discuss **stacking**, which works great for small or medium size data sets.
    Stacking uses a similar idea to k-folds cross validation to create **out-of-sample**
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The key word here is **out-of-sample**, since if we were to use predictions
    from the M models that are **fit to all the training data**, then the second level
    model will be biased towards the best of M models. This will be of no use.
  prefs: []
  type: TYPE_NORMAL
- en: As an illustration of this point, let’s say that **model 1** has lower training
    accuracy, than **model 2** on the training data. There may however be data points where
    **model 1** performs better, but for some reason it performs terribly on others
    (see figure below). Instead, **model 2** may have a better overall performance
    on all the data points, but it has worse performance on the very set of points
    where **model 1** is better. The idea is to combine these two models where they
    perform the best. This is why creating out-of-sample predictions have a higher
    chance of capturing distinct regions where each model performs the best.
  prefs: []
  type: TYPE_NORMAL
- en: '![models](../Images/5176f9842aa4d78e0ac86d823a58a174.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, let me describe what I mean by stacking. The idea is to divide the training
    set into several pieces like you would do in k-folds cross validation. For each
    fold, the rest of the folds are used to obtain a predictions using all the models
    1…M. The best way to explain this is by the figure below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![stacking](../Images/b485907a53455f6ecfe2584bd46e091b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we divide our training data into N folds, and hold the Nth fold out for
    validation (i.e. the holdout fold). Suppose we have M number of models (we will
    later use M=3). As the figure shows, prediction for each fold (Fj) is obtained
    from a fit using the rest of the folds and collected in an out-of-sample predictions
    matrix (Xoos). Namely, the level 2 training data Xl2 is Xoos. This is repeated
    for each of the models. The out-of -sample prediction matrix (Xoos) will then
    be used in a second level training (by some method of choice) to obtain the final
    predictions for all the data points. There are several points to note:'
  prefs: []
  type: TYPE_NORMAL
- en: We have not simply stacked the predictions on all the training data from the
    M models column-by-column to create a second level training data, due to the problem
    mentioned above (the fact that the second level training will simply choose the
    best of the M models).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using out-of-sample predictions, we still have a large data to train the
    second level model. We just need to train on Xoos and predict on the holdout fold
    (Nth). This is in contrast to model ensembles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, each model (1...M) can be trained on the (N-1) folds and a prediction on
    the holdout fold (Nth) can be made. There is nothing new here. But what we do is
    that, using the second level model which is trained on Xoos, we will obtain predictions
    on the holdout data. We want that the **predictions from the second level training
    be better than each of the M predictions from the original models**. If not, we
    will have to restructure the way we combine models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let me illustrate what I just wrote with a concrete example. For the case of
    the House Prices data, I have used 10 folds of division of the training data.
    The first 9 is used for building Xoos, and 10th is the holdout data for validation.
    I trained three level 1 models: XGBoost, neural network, support vector regression.
    For level 2, I used a linear elasticnet model (i.e. LASSO + Ridge regression).
    Below are the root-mean-squared errors (RMSE) of each of the models evaluated
    on the holdout fold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As clear from this data, the stacked model has slightly lower RMSE than the
    rest. This may look too small of a change, but when Kaggle leaderships are involved,
    such small differences matter a lot!
  prefs: []
  type: TYPE_NORMAL
- en: '![stackplot](../Images/25c59f2fc6d8329b603e2437a4418cf3.png)'
  prefs: []
  type: TYPE_IMG
- en: Graphically, once can see that the circled data point is a prediction which
    is worse in XGBoost (which is the best model when trained on all the training
    data), but neural network and support vector regression does better for that specific
    point. In the stacked model, that data point is placed close to where it is for neural
    network and support vector regression. Of course you can also see some cases where
    using just XGboost is better than stacking (like some of the lower lying points).
    However, the overall predictive accuracy of the stacked model is better.
  prefs: []
  type: TYPE_NORMAL
- en: One final complication that will further boost your score: If you have spare
    computational time, you can create repeated stacks. This will further reduce the
    variance of your predictions (something reminiscent of bagging).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s create a 10 folds stacking not just once, but 10 times!
    (say by caret’s createMultiFolds function). This will give us multiple level 2
    predictions, which can then be average over. For example, below are the RMSE values
    on the holdout data (rmse1: XGBoost, rmse2: Neural Network, rmse3: Support Vector
    Regression), for 20 different random 10-folds created. Averaging the final predictions
    from the level 2 predictions on these Xoos’s (i.e. Xoos from stack1, Xoos from
    stack2, …, Xoos from stack10), would further improve your score.'
  prefs: []
  type: TYPE_NORMAL
- en: '![table](../Images/1c5f65587d7163510e8cb27031ccdfec.png)'
  prefs: []
  type: TYPE_IMG
- en: Once we verify that stacking results in better predictions than each of the
    models, then we re-run the whole machinery once again, without keeping Nth fold
    as holdout data. We create Xoos from all the folds, and the the second level training
    uses Xoos to predict the test set which Kaggle provides us with. Hopefully, this
    will creep your score up in the leader board!.
  prefs: []
  type: TYPE_NORMAL
- en: 'Final word: You can find the scripts from my [Github repo](https://github.com/bhimmetoglu/kaggle_101/tree/master/HousePrices).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: If you have a classification problem, you can still use the same procedure
    to stack class probabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Burak Himmetoglu](https://burakhimmetoglu.com/)** is a Data Scientist
    and High Performance Computing (HPC) specialist with Ph.D. in physics. He has
    strong mathematical modeling, data analysis, and programming background, and is
    passionate about applying academic skills to solve difficult business problems
    and develop data products.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://burakhimmetoglu.com/2016/12/01/stacking-models-for-improved-predictions/).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Data Science Basics: An Introduction to Ensemble Learners](/2016/11/data-science-basics-intro-ensemble-learners.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Random Forests in Python](/2016/12/random-forests-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top R Packages for Machine Learning](/2017/02/top-r-packages-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[The Importance of Permutation in Neural Network Predictions](https://www.kdnuggets.com/2022/12/importance-permutation-neural-network-predictions.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Making Predictions: A Beginner''s Guide to Linear Regression in Python](https://www.kdnuggets.com/2023/06/making-predictions-beginner-guide-linear-regression-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chatting with the Future: Predictions for AI in the Next Decade](https://www.kdnuggets.com/2023/04/chatting-future-predictions-ai-next-decade.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Why Do Machine Learning Models Die In Silence?](https://www.kdnuggets.com/2022/01/machine-learning-models-die-silence.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Explain NLP Models with LIME](https://www.kdnuggets.com/2022/01/explain-nlp-models-lime.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
