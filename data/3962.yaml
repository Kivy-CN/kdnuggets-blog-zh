- en: Cleaning and Preprocessing Text Data in Pandas for NLP Tasks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Pandas中清理和预处理文本数据以用于NLP任务
- en: 原文：[https://www.kdnuggets.com/cleaning-and-preprocessing-text-data-in-pandas-for-nlp-tasks](https://www.kdnuggets.com/cleaning-and-preprocessing-text-data-in-pandas-for-nlp-tasks)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/cleaning-and-preprocessing-text-data-in-pandas-for-nlp-tasks](https://www.kdnuggets.com/cleaning-and-preprocessing-text-data-in-pandas-for-nlp-tasks)
- en: '![NLP (Natural Language Processing)](../Images/5ae96669735b9ef013b0b884100fcb7a.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![NLP（自然语言处理）](../Images/5ae96669735b9ef013b0b884100fcb7a.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Cleaning and preprocessing data is often one of the most daunting, yet critical
    phases in building AI and Machine Learning solutions fueled by data, and text
    data is not the exception.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 清理和预处理数据通常是构建由数据驱动的AI和机器学习解决方案中最令人畏惧但又至关重要的阶段，而文本数据也不例外。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: This tutorial breaks the ice in tackling the challenge of preparing text data
    for NLP tasks such as those Language Models (LMs) can solve. By encapsulating
    your text data in pandas DataFrames, the below steps will help you get your text
    ready for being digested by NLP models and algorithms.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程破冰地解决了为NLP任务（如语言模型（LMs）可以解决的任务）准备文本数据的挑战。通过将你的文本数据封装在pandas DataFrames中，以下步骤将帮助你准备文本数据，以便被NLP模型和算法处理。
- en: Load the Data into a Pandas DataFrame
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据加载到Pandas DataFrame中
- en: To keep this tutorial simple and focused on understanding the necessary text
    cleaning and preprocessing steps, let's consider a small sample of four single-attribute
    text data instances that will be moved into a pandas DataFrame instance. We will
    from now on apply every preprocessing step on this DataFrame object.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持本教程简单明了并专注于理解必要的文本清理和预处理步骤，让我们考虑一个包含四个单属性文本数据实例的小样本，这些实例将被移动到一个pandas DataFrame实例中。从现在起，我们将在这个DataFrame对象上应用每个预处理步骤。
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Output:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Handle Missing Values
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: 'Did you notice the ''None'' value in one of the example data instances? This
    is known as a missing value. Missing values are commonly collected for various
    reasons, often accidental. The bottom line: you need to handle them. The simplest
    approach is to simply detect and remove instances containing missing values, as
    done in the code below:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你注意到示例数据实例中的'None'值了吗？这被称为缺失值。缺失值由于各种原因通常会被收集，往往是偶然的。关键点是：你需要处理这些值。最简单的方法是检测并删除包含缺失值的实例，如下代码所示：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Output:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Normalize the Text to Make it Consistent
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规范化文本以保持一致
- en: Normalizing text implies standardizing or unifying elements that may appear
    under different formats across different instances, for instance, date formats,
    full names, or case sensitiveness. The simplest approach to normalize our text
    is to convert all of it to lowercase, as follows.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 规范化文本意味着标准化或统一那些在不同实例中可能以不同格式出现的元素，例如日期格式、全名或大小写敏感性。规范化文本的最简单方法是将所有文本转换为小写，如下所示。
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Output:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Remove Noise
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 去除噪音
- en: Noise is unnecessary or unexpectedly collected data that may hinder the subsequent
    modeling or prediction processes if not handled adequately. In our example, we
    will assume that punctuation marks like "!" are not needed for the subsequent
    NLP task to be applied, hence we apply some noise removal on it by detecting punctuation
    marks in the text using a regular expression. The 're' Python package is used
    for working and performing text operations based on regular expression matching.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 噪音是指不必要或意外收集的数据，如果处理不当，可能会妨碍后续的建模或预测过程。在我们的示例中，我们假设像“！”这样的标点符号在后续的NLP任务中是不需要的，因此我们通过使用正则表达式检测文本中的标点符号来进行噪音去除。Python的're'包用于基于正则表达式匹配进行文本操作。
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Output:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Tokenize the Text
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对文本进行分词
- en: Tokenization is arguably the most important text preprocessing step -along with
    encoding text into a numerical representation- before using NLP and language models.
    It consists in splitting each text input into a vector of chunks or tokens. In
    the simplest scenario, tokens are associated with words most of the time, but
    in some cases like compound words, one word might lead to multiple tokens. Certain
    punctuation marks (if they were not previously removed as noise) are also sometimes
    identified as standalone tokens.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 分词可以说是进行 NLP 和语言模型使用前最重要的文本预处理步骤之一 - 以及将文本编码为数字表示 - 它包括将每个文本输入拆分成一系列片段或标记。在最简单的情况下，标记通常与单词相关，但在复合词等某些情况下，一个单词可能会导致多个标记。某些标点符号（如果它们之前没有被作为噪音去除）有时也会被识别为独立的标记。
- en: 'This code splits each of our three text entries into individual words (tokens)
    and adds them as a new column in our DataFrame, then displays the updated data
    structure with its two columns. The simplified tokenization approach applied is
    known as simple whitespace tokenization: it just uses whitespaces as the criterion
    to detect and separate tokens.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将我们三个文本条目中的每一个分割成单独的单词（标记），并将其添加为 DataFrame 中的新列，然后显示更新后的数据结构及其两列。应用的简化分词方法被称为简单的空格分词：它仅使用空格作为检测和分隔标记的标准。
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Output:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Remove Stop Words
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 移除停用词
- en: 'Once the text is tokenized, we filter out unnecessary tokens. This is typically
    the case of stop words, like articles "a/an, the", or conjunctions, which do not
    add actual semantics to the text and should be removed for later efficient processing.
    This process is language-dependent: the code below uses the NLTK library to download
    a dictionary of English stop words and filter them out from the token vectors.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文本被分词，我们会过滤掉不必要的标记。这通常是停用词的情况，比如冠词“a/an, the”或连词，这些词对文本没有实际语义贡献，应被移除以便后续有效处理。这个过程依赖于语言：下面的代码使用
    NLTK 库下载英语停用词字典，并从标记向量中过滤掉它们。
- en: '[PRE10]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Output:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE11]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Stemming and Lemmatization
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词干提取和词形还原
- en: Almost there! Stemming and lemmatization are additional text preprocessing steps
    that might sometimes be used depending on the specific task at hand. Stemming
    reduces each token (word) to its base or root form, whilst lemmatization further
    reduces it to its lemma or base dictionary form depending on the context, e.g.
    "best" -> "good". For simplicity, we will only apply stemming in this example,
    by using the PorterStemmer implemented in the NLTK library, aided by the wordnet
    dataset of word-root associations. The resulting stemmed words are saved in a
    new column in the DataFrame.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 快完成了！词干提取和词形还原是可能根据具体任务使用的附加文本预处理步骤。词干提取将每个标记（单词）还原为其基础或根形式，而词形还原则进一步将其还原为词形或基础词典形式，取决于上下文，例如“best”
    -> “good”。为了简化起见，我们在这个示例中将只应用词干提取，使用 NLTK 库中实现的 PorterStemmer，借助 wordnet 数据集的词根关联。得到的词干词保存在
    DataFrame 中的新列中。
- en: '[PRE12]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Output:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE13]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Convert Text into Numerical Representations
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将文本转换为数字表示
- en: Last but not least, computer algorithms including AI/ML models do not understand
    human language but numbers, hence we need to map our word vectors into numerical
    representations, commonly known as embedding vectors, or simply embedding. The
    below example converts tokenized text in the 'tokens' column and uses a TF-IDF
    vectorization approach (one of the most popular approaches in the good old days
    of classical NLP) to transform the text into numerical representations.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，计算机算法包括 AI/ML 模型并不理解人类语言，而是数字，因此我们需要将我们的词向量映射到数字表示中，通常称为嵌入向量，或简称为嵌入。下面的示例将“tokens”列中的分词文本转换为
    TF-IDF 向量化方法（这是经典 NLP 的黄金时代中最受欢迎的方法之一）来将文本转换为数字表示。
- en: '[PRE14]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Output:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE15]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: And that's it! As unintelligible as it may seem to us, this numerical representation
    of our preprocessed text is what intelligent systems including NLP models do understand
    and can handle exceptionally well for challenging language tasks like classifying
    sentiment in text, summarizing it, or even translating it to another language.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！尽管对我们来说可能显得难以理解，这种我们预处理文本的数字表示就是智能系统，包括 NLP 模型，能够理解并且在挑战性语言任务如文本情感分类、总结或甚至翻译成另一种语言时表现得异常出色的内容。
- en: The next step would be feeding these numerical representations to our NLP model
    to let it do its magic.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将这些数字表示输入到我们的 NLP 模型中，让它施展魔法。
- en: '[](https://www.linkedin.com/in/ivanpc/)****[Iván Palomares Carrascosa](https://www.linkedin.com/in/ivanpc/)****
    is a leader, writer, speaker, and adviser in AI, machine learning, deep learning
    & LLMs. He trains and guides others in harnessing AI in the real world.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.linkedin.com/in/ivanpc/)****[伊万·帕洛马雷斯·卡拉斯科萨](https://www.linkedin.com/in/ivanpc/)****
    是一位在人工智能、机器学习、深度学习和大型语言模型方面的领导者、作家、演讲者和顾问。他培训和指导他人在实际应用中利用人工智能。'
- en: More On This Topic
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Learn Data Cleaning and Preprocessing for Data Science with This Free eBook](https://www.kdnuggets.com/2023/08/learn-data-cleaning-preprocessing-data-science-free-ebook.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据清理和预处理以进行数据科学，这本免费电子书](https://www.kdnuggets.com/2023/08/learn-data-cleaning-preprocessing-data-science-free-ebook.html)'
- en: '[7 Steps to Mastering Data Cleaning and Preprocessing Techniques](https://www.kdnuggets.com/2023/08/7-steps-mastering-data-cleaning-preprocessing-techniques.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握数据清理和预处理技术的7个步骤](https://www.kdnuggets.com/2023/08/7-steps-mastering-data-cleaning-preprocessing-techniques.html)'
- en: '[Harnessing ChatGPT for Automated Data Cleaning and Preprocessing](https://www.kdnuggets.com/2023/08/harnessing-chatgpt-automated-data-cleaning-preprocessing.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用ChatGPT进行自动化数据清理和预处理](https://www.kdnuggets.com/2023/08/harnessing-chatgpt-automated-data-cleaning-preprocessing.html)'
- en: '[Data Representation for Natural Language Processing Tasks](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理任务的数据表示](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
- en: '[5 Tasks To Automate With Python](https://www.kdnuggets.com/2021/06/5-tasks-automate-python.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用Python自动化的5个任务](https://www.kdnuggets.com/2021/06/5-tasks-automate-python.html)'
- en: '[HuggingGPT: The Secret Weapon to Solve Complex AI Tasks](https://www.kdnuggets.com/2023/05/hugginggpt-secret-weapon-solve-complex-ai-tasks.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HuggingGPT：解决复杂AI任务的秘密武器](https://www.kdnuggets.com/2023/05/hugginggpt-secret-weapon-solve-complex-ai-tasks.html)'
