- en: Build a Serverless News Data Pipeline using ML on AWS Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/11/build-serverless-news-data-pipeline-ml-aws-cloud.html](https://www.kdnuggets.com/2021/11/build-serverless-news-data-pipeline-ml-aws-cloud.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Maria Zentsova](https://www.linkedin.com/in/maria-zentsova-6058b6ab/),
    Senior Data Analyst at Wood Mackenzie**'
  prefs: []
  type: TYPE_NORMAL
- en: As an analyst, I spend a lot of time tracking news and industry updates. Thinking
    about this problem on my maternity leave, I've decided to build a simple [app](https://www.sustinero.com/)
    to track news on green tech and renewable energy.  Using AWS Lambda and other
    AWS services like EventBridge, SNS, DynamoDB, and Sagemaker it's very easy to
    get started and build a prototype in a couple of days.
  prefs: []
  type: TYPE_NORMAL
- en: The app is powered by a series of serverless Lambda functions and a text summarization
    Machine Learning model deployed as SageMaker endpoint. Every 24 hours AWS EventBridge
    rule triggers Lambda function to fetch feeds from the DynamoDB database.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/1b41cc9dd533268335c40fb8cf393aaa.png)'
  prefs: []
  type: TYPE_IMG
- en: These feeds are then sent as SNS topics to trigger multiple lambdas to parse
    feeds and extract news URLs. Each website updates its RSS feeds daily with just
    a couple of articles at most, so this way we won't send a lot of traffic, which
    might result in consuming too many resources of any particular news publication.
  prefs: []
  type: TYPE_NORMAL
- en: The big problem, however, is to extract the full text of the article, because
    every website is different. Luckily for us, libraries such as goose3 solve this
    problem by applying ML methods to extract the body of the page. I can't store
    the full text of the article due to copyright, that's why I apply a HuggingFace
    text summarization transformer [model](https://huggingface.co/facebook/bart-large-cnn)
    to generate a short summary.
  prefs: []
  type: TYPE_NORMAL
- en: Here is a detailed guide on how to build your own news aggregation pipeline
    powered by ML.
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Set up IAM role with necessary permissions.**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although this data pipeline is very simple, it connects a number of AWS resources.
    To grant our functions access to all the resources it needs, we need to set up
    [IAM role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html). This
    role assigns our function permissions to use other resources in the cloud, such
    as DynamoDB, Sagemaker, CloudWatch, and SNS. For security reasons, it is best
    not to give full AWS administrative access to our IAM role, and only allow it
    to use the resources it needs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/7c666a6a9d80f8bdd5d6c7f4faea8d6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '**2\. Fetch RSS feeds from DynamoDB in an RSS Dispatcher Lambda**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With AWS Lambda one can do almost anything, it's a very powerful serverless
    compute service and is great for short tasks. The main advantage for me is that
    it's very easy to access other services in the AWS ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: I store all RSS feeds in a DynamoDB table and it's really easy to access it
    from the lambda using [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/dynamodb.html)
    library. Once I fetch all the feeds from the database, I send them as SNS messages
    to trigger feed parsing lambdas.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 3\. **Creating Layers with necessary libraries**
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To use some specific libraries in AWS Lambdas, you need to import them as a
    [Layer](https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html).
    To prepare the library for import, it needs to be in a python.zip archive, which
    then we can upload on AWS and use in the function. To create a layer, just cd
    in a python folder, run the pip install command, zip it and it's ready to be uploaded.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: However, I had some difficulty deploying the goose3 library as a layer. After
    a short investigation, it turns out some libraries like LXML need to be compiled
    in a Lambda-like environment, which is Linux. So if the library was compiled,
    for instance, on Windows and then imported into the function, the error will occur.
    To solve this problem, before creating an archive we need to install the library
    on Linux.
  prefs: []
  type: TYPE_NORMAL
- en: There are two ways to do it. First, to install on [simulated lambda environment](https://aws.amazon.com/premiumsupport/knowledge-center/lambda-layer-simulated-docker/)
    with docker. For me, the easiest way was to use the AWS [sam build](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-cli-command-reference-sam-build.html)
    command. Once the function is built, I just copy the required packages from the
    build folder and upload them as a layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**4\. Launch Lambda functions to parse a feed**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we send news URLs to SNS as topics, we could trigger multiple lambdas to
    go and fetch news articles from the RSS feeds. Some RSS feeds are different, but
    the feed parser library allows us to work with different formats. Our URL is a
    part of the event object, so we need to extract it by key.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**5\. Creating and deploying text summarization model on Sagemaker**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sagemaker is a service that makes it easy to write, train and deploy ML models
    on AWS. HuggingFace has [partnered](https://aws.amazon.com/blogs/machine-learning/aws-and-hugging-face-collaborate-to-simplify-and-accelerate-adoption-of-natural-language-processing-models/)
    with AWS to make it even easier to deploy its models to the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Here I write a simple text summarization model in Jupiter notebook and deploy
    it using deploy() command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Once it is deployed, we could get endpoint information from Sagemaker -> Inference
    -> Endpoint configuration and use it in our lamdas.
  prefs: []
  type: TYPE_NORMAL
- en: '**6\. Get the full text of the article, summarize it and store results in DynamoDB**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We shouldn't store full text due to copyright, that's why all the processing
    work happens in one lambda. I launch text processing lambda, once URL made its
    way to the DynamoDB table. To achieve this, I set up a DynamoDB item creation,
    as a trigger to launch a lambda. I set up batch size of one, so that lambda deals
    only with one article at a time.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/c9b8da0d517979b663d0a037f7053934.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here is how using AWS  tools, we've built and deployed a simple serverless data
    pipeline to read the latest news. If you have any ideas, how it could be improved
    or have any questions do not hesitate to contact me.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Maria Zentsova](https://www.linkedin.com/in/maria-zentsova-6058b6ab/)**
    is a Senior Data Analyst at Wood Mackenzie. She works on data collection and analysis,
    ETL pipelines and data exploration tools.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[How I Redesigned over 100 ETL into ELT Data Pipelines](/2021/11/redesigned-over-100-etl-elt-data-pipelines.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best Ways for Data Professionals to Market AWS Skills in 2022](/2021/11/best-ways-data-professionals-market-aws-skills.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Prefect: How to Write and Schedule Your First ETL Pipeline with Python](/2021/08/prefect-write-schedule-etl-pipeline-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[11 Best Practices of Cloud and Data Migration to AWS Cloud](https://www.kdnuggets.com/2023/04/11-best-practices-cloud-data-migration-aws-cloud.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Data Science Technologies You Need to Build Your Supply Chain Pipeline](https://www.kdnuggets.com/2022/01/6-data-science-technologies-need-build-supply-chain-pipeline.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using Datawig, an AWS Deep Learning Library for Missing Value Imputation](https://www.kdnuggets.com/2021/12/datawig-aws-deep-learning-library-missing-value-imputation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AWS AI & ML Scholarship Program Overview](https://www.kdnuggets.com/2022/09/aws-ai-ml-scholarship-program-overview.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Setup and use JupyterHub (TLJH) on AWS EC2](https://www.kdnuggets.com/2023/01/setup-jupyterhub-tljh-aws-ec2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top 8 GenAI Courses for AWS to Take Now](https://www.kdnuggets.com/top-8-genai-courses-for-aws-to-take-now)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
