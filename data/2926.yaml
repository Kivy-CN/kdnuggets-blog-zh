- en: Why Machine Learning is vulnerable to adversarial attacks and how to fix it
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么机器学习容易受到对抗性攻击以及如何解决这个问题
- en: 原文：[https://www.kdnuggets.com/2019/06/machine-learning-adversarial-attacks.html](https://www.kdnuggets.com/2019/06/machine-learning-adversarial-attacks.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/06/machine-learning-adversarial-attacks.html](https://www.kdnuggets.com/2019/06/machine-learning-adversarial-attacks.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: The AI Safety debate rages following the lead from top researchers in the field
    alongside many of today’s technological visionaries, such as those propping up
    the [Future of Life Institute](http://futureoflife.org/team) and the [Machine
    Intelligence Research Institute](http://intelligence.org/team). Through the media,
    this conversation may appear to sit in a cloud of worry about speculative future-bots
    that will wipe out humanity.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: AI安全辩论跟随领域内顶尖研究者及许多当今技术愿景者的脚步进行，例如支持[生命未来研究所](http://futureoflife.org/team)和[机器智能研究所](http://intelligence.org/team)的人士。通过媒体，这一对话可能看起来像是在担忧那些将消灭人类的未来机器人。
- en: However, real inklings of how we can easily lose mastery over our AI creations
    [are observed](https://arxiv.org/pdf/1606.06565.pdf) in practical problems related
    to unintended behaviors from poorly designed machine learning systems. Among these
    potential “AI accidents” is the case of [adversarial techniques](https://www.kdnuggets.com/2018/10/adversarial-examples-explained.html).
    This approach takes, for instance, a trained classifier model that performs well
    with identifying inputs compared to how a person would classify. Then, a new input
    comes along that includes subtle yet maliciously crafted data that causes the
    model to behave very poorly. What is troublesome is that the type of poor behavior
    is not a reduction in the *statistical performance* of the model. With such a
    manipulated input, the model may still classify the object with very high confidence,
    but the classification is *no longer* what a human anticipates.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们在实际问题中观察到有关如何轻易失去对我们AI创作的掌控的真实迹象，这些问题与设计不良的机器学习系统的不良行为相关。潜在的“AI事故”之一就是[对抗性技术](https://www.kdnuggets.com/2018/10/adversarial-examples-explained.html)的案例。这种方法以一个训练好的分类模型为例，该模型在识别输入时表现良好，与人类的分类方式相比。但随后出现一个新输入，其中包含微妙但恶意伪造的数据，这导致模型表现非常糟糕。令人担忧的是，这种糟糕的行为不是模型的*统计性能*下降。使用这样的操控输入，模型可能仍以非常高的信心分类对象，但分类结果*不再是*人类所预期的。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 加速进入网络安全职业的快车道'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析水平'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行IT工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Top-performing and competition-winning image classifiers have been fooled by
    [adversarial attacks](https://www.kdnuggets.com/2019/03/breaking-neural-networks-adversarial-attacks.html).
    If you look closely at these scenarios, you may find an interesting catch. The
    offending noise is based on the cost function of the model suggesting that a malicious
    attacker needs to have the model in hand to figure out how to build the adversarial
    input. Simply protecting the model from attackers could be a straightforward solution
    to the problem.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 表现优异并赢得比赛的图像分类器已经被[对抗性攻击](https://www.kdnuggets.com/2019/03/breaking-neural-networks-adversarial-attacks.html)所欺骗。如果你仔细查看这些场景，可能会发现一个有趣的发现。令人困扰的噪声基于模型的成本函数，这表明恶意攻击者需要手中掌握模型以弄清楚如何构建对抗输入。简单地保护模型免受攻击者的攻击可能是解决问题的直接方法。
- en: Not so fast.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 不要急于行动。
- en: In 2017, researchers at Penn State, including Nicolas Papernot and Goodfellow,
    [showed](https://arxiv.org/pdf/1602.02697.pdf) that remote adversarial attacks
    could be performed on ML algorithms considered as a “black-box” – in other words,
    without access to the model’s training data or its code. This approach uses the
    [idea of transferability](https://arxiv.org/pdf/1605.07277.pdf) where a sample
    that is trained to mislead one model will likely be able to mislead another model
    (i.e., one targeted for attack).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，宾州州立大学的研究人员，包括 Nicolas Papernot 和 Goodfellow，[展示了](https://arxiv.org/pdf/1602.02697.pdf)远程对抗攻击可以对被视为“黑箱”的机器学习算法进行——换句话说，无需访问模型的训练数据或代码。这种方法使用了[可转移性](https://arxiv.org/pdf/1605.07277.pdf)的概念，即训练来误导一个模型的样本可能会误导另一个模型（即被攻击的目标模型）。
- en: Papernot’s team created local image classifier models and trained them to see
    how modified samples would result in misclassifications. They then took these
    same adversarial samples (images of stop signs modified in ways imperceptible
    to human vision) and sent them off to remotely hosted deep neural networks accessible
    through APIs from Amazon and Google. The same tricky images tricked the remote
    models as well. They even reported misleading remotely hosted models that included
    adversarial defense strategies.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Papernot 的团队创建了局部图像分类器模型，并训练它们以查看修改样本如何导致错误分类。然后，他们将这些对抗样本（以人眼不可察觉的方式修改的停车标志图像）发送给通过亚马逊和谷歌的
    API 访问的远程深度神经网络。这些棘手的图像也欺骗了远程模型。他们甚至报告了包括对抗性防御策略的误导性远程托管模型。
- en: (Want to try to break your models with adversarial examples? Try out the [cleverhans](https://github.com/tensorflow/cleverhans)
    library developed by Papernot et al.)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: (想尝试用对抗样本破坏你的模型吗？试试由 Papernot 等人开发的[cleverhans](https://github.com/tensorflow/cleverhans)库。)
- en: All this is a big deal, [so what should we do about it](https://www.kdnuggets.com/2019/01/machine-learning-security.html)?
    If an attacker can control (or confuse) the classification results of, say, a
    computer vision model driving your autonomous vehicle to work without needing
    direct access to the code, then a stop sign may be easily classified as a yield
    sign. So much for getting to work on time or alive. While a clear problem that
    AI developers must appreciate, this still represents a *defined challenge* that
    should be tractable and solvable. It’s just one more way *humans* ingeniously
    hack into other *human* systems to cause *human* issues against which *humans*
    must defend.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都非常重要，[我们该如何应对](https://www.kdnuggets.com/2019/01/machine-learning-security.html)？如果攻击者能够控制（或混淆）例如，驾驶你自动驾驶车辆的计算机视觉模型的分类结果，而不需要直接访问代码，那么停车标志可能会被轻易地分类为让行标志。这样一来，准时到达工作地点或者安全到达就变得不那么可能。虽然这是一个
    AI 开发者必须认识到的明确问题，但它仍然代表着一个*明确的挑战*，应该是可以处理和解决的。这只是人类巧妙地入侵其他*人类*系统以造成*人类*问题的一种方式，对抗这些问题依然需要*人类*的防御。
- en: What if this case of adversarial samples is an iceberg tip of something more?
    Something inherent in the way machines process information? Something beyond human
    perception?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这种对抗样本的情况只是冰山一角呢？是否存在某种机器处理信息的内在机制？是否有超出人类感知的因素？
- en: Just recently, a group from MIT posted [a study](https://arxiv.org/pdf/1905.02175.pdf)
    hypothesizing that previously published observations of the vulnerability of machine
    learning to adversarial techniques are the direct consequence of inherent patterns
    within standard data sets. While incomprehensible to humans, these exist as natural
    features that are fundamentally used by supervised learning algorithms.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，麻省理工学院的一组研究人员发布了[一项研究](https://arxiv.org/pdf/1905.02175.pdf)，假设先前对机器学习易受对抗性技术影响的观察是标准数据集中固有模式的直接结果。虽然这些模式对人类来说难以理解，但它们作为自然特征存在，主要被监督学习算法使用。
- en: In this paper, Andrew Ilyas et al. describe a notion of robust and non-robust
    features existing within a data set that *both* provide useful signals for classification.
    In hindsight, this might not be surprising when we recall that classifiers typically
    are trained only to maximize accuracy. Which features a model uses to obtain the
    highest accuracy – as determined by a human’s supervisory labeling – is left to
    the model to sort through with brute force. What looks to us like a person’s nose
    in an input image is just another combination of 0s and 1s compared to a chin
    from the perspective of the algorithm.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，Andrew Ilyas等人描述了数据集中存在的稳健特征和非稳健特征的概念，这些特征*都*为分类提供了有用的信号。回顾起来，当我们记得分类器通常只被训练以最大化准确性时，这可能并不令人惊讶。模型使用哪些特征来获得最高准确性——由人类监督标记确定——留给模型用蛮力来处理。对我们来说像人鼻的输入图像，在算法的角度下，只是与下巴的另一个0和1的组合。
- en: To demonstrate how these human-perceptible and non-perceptible features support
    accurate classification, the MIT team constructed a framework to separate the
    features into distinct training data sets. Both are run through the training and
    are sufficient to classify as expected. The models are seen, as illustrated in
    Figure 1, to use non-robust features in *meaningful ways* just as the robust features,
    even though they look like garbled information to us.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示这些人类可感知和不可感知的特征如何支持准确分类，麻省理工学院团队构建了一个框架，将特征分离到不同的训练数据集中。两者都经过训练，并能按预期进行分类。如图1所示，模型被发现以*有意义的方式*使用非稳健特征，就像使用稳健特征一样，尽管这些特征对我们来说看起来像是乱码信息。
- en: '![](../Images/e3a143e102045c2966404c87fe678a73.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3a143e102045c2966404c87fe678a73.png)'
- en: '**Figure 1.** An image of a frog is separated into “robust” features used to
    train a classifier to accurately identify it as a frog and “non-robust” features
    that are also used to train a classifier to identify frog accurately, but not
    when adversarial perturbations are included in the input. These non-robust features
    used by the model are *inherent* to the image and not manipulated or inserted
    by an attacker [from [Ilyas et al.](https://arxiv.org/pdf/1905.02175.pdf) with
    permission].'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1.** 一只青蛙的图像被分解为“稳健”的特征，用于训练分类器准确识别为青蛙，以及“非稳健”的特征，这些特征也用于训练分类器准确识别青蛙，但当输入中包含对抗性扰动时则不适用。模型使用的这些非稳健特征是*固有的*图像特征，并非攻击者操控或插入的
    [来自 [Ilyas et al.](https://arxiv.org/pdf/1905.02175.pdf) 的许可]。'
- en: Let’s be clear on this point again.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次明确这一点。
- en: Legitimate data is comprised of human-imperceptible features that impact training.
    These weird features that don’t appear to connect to the appropriate human classification
    are not artifacts or overfittings but are naturally occurring patterns that are
    sufficient *on their own* to result in a learned classifier.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 合法数据由对人类不可感知的特征组成，这些特征影响训练。这些看似与适当的人类分类无关的奇怪特征不是伪影或过拟合，而是自然发生的模式，*单独*就足以导致分类器的学习。
- en: In other words, patterns useful for classification that we as humans expect
    to see might be found by the algorithm. However, *other* patterns are being identified
    that make no sense to us – but they are still patterns that support the algorithm
    in making good classifications.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，算法可能会发现那些我们作为人类期望看到的有用分类模式。然而，*其他*模式也在被识别，它们对我们来说毫无意义——但它们仍然是支持算法进行良好分类的模式。
- en: Dimitris Tsipras from the MIT team is excited about their result as “it convincingly
    demonstrates that models can indeed learn imperceptible well-generalizing features.”
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 麻省理工学院的Dimitris Tsipras对他们的结果感到兴奋，称“它有力地证明了模型确实可以学习不可感知的良好泛化特征。”
- en: While these patterns don’t look like anything meaningful to us, the ML classifier
    is finding them and using them.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些模式对我们看起来毫无意义，但机器学习分类器正在发现并利用它们。
- en: When facing adversarial inputs (e.g., small perturbations in the new data),
    this same set of nonsensical features breaks down leading to incorrect classification.
    Now, it’s not only some third-party attacker inserting misleading data into a
    sample, but it’s a consequence of the non-robust features existing in the real
    data that makes the classifier vulnerable to an adversary’s taunting.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 面对对抗性输入（例如，新数据中的小扰动），这一套无意义的特征会崩溃，从而导致错误分类。现在，不仅仅是某个第三方攻击者将误导数据插入样本中，而是非稳健特征在真实数据中存在，使得分类器容易受到对手的挑衅。
- en: Recalling the notion of transferability mentioned above from Papernot et al.,
    it’s also these non-robust features existing across datasets that make them sensitive
    to adversarial examples trained on other models. An adversary trained to break
    one model will likely break another because it’s messing with the inherent non-robust
    features found in both models.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆一下 Papernot 等人提到的可转移性概念，这些跨数据集存在的非鲁棒特性使得它们对其他模型训练出的对抗样本敏感。一个训练来攻破某个模型的对手很可能也会攻破另一个模型，因为它正干扰着两个模型中固有的非鲁棒特性。
- en: So, the notion that supervised machine learning is vulnerable to adversarial
    examples is a direct consequence of the inherent properties of how algorithms
    look at the data. The algorithm is performing just fine chugging away at whichever
    patterns of features lead it to the best accuracy. If an adversary messes with
    its classification result, then the model still thinks it’s performing quite well.
    The fact that the result is not what we as humans would perceive is our problem.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，监督学习对抗对抗样本的脆弱性是算法如何看待数据的固有属性的直接结果。算法在处理任何特征模式时表现得很好，导向最佳准确率。如果一个对手干扰了它的分类结果，那么模型仍然认为它表现得相当好。结果与我们作为人类所感知的不一致，是我们的问题。
- en: To have a bit more human touch, Tsipras said that since “models might rely on
    all sorts of features of the data, we need to explicitly enforce priors on what
    we want the model to learn.”
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加一点人性，Tsipras 说，“模型可能依赖于数据的各种特征，我们需要明确地对模型学习的内容施加先验。”
- en: In other words, if we want to create models that are *interpretable* as far
    as we are concerned, then we need to encode our expected perceptions into the
    training process and not leave the algorithm to its own, non-human, devices.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果我们希望创建在我们看来是*可解释的*模型，那么我们需要将我们期望的感知编码到训练过程中，而不是让算法依赖于其自身的非人类装置。
- en: Well, so what if the algorithm is seeing things *differently* than we do as
    humans? Why is this unexpected? Is it scary? Is it something we don’t want to
    see happening?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果算法看到的事物与我们人类*不同*，这有什么问题？这有什么意外之处吗？这令人害怕吗？这是我们不想看到的事情吗？
- en: We know ML processes our world differently than us. Just compare to the foundational
    notion in NLP where the meanings of words can be extracted not from a memorized
    dictionary (as we humans do) but from exploring the nearby terms in a human-written
    text (such as in [Word2vec](https://www.tensorflow.org/tutorials/representation/word2vec)).
    It should already be appreciated that algorithms running on silicon see information
    differently than synapses firing in brains.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，机器学习处理世界的方式与我们不同。只需对比 NLP 中的基础概念，即单词的意义可以通过探索人类编写的文本中的邻近词汇（如 [Word2vec](https://www.tensorflow.org/tutorials/representation/word2vec)）来提取，而不是通过记忆词典（如我们人类所做的）。我们应该已经意识到，运行在硅片上的算法以不同于大脑中突触的方式看待信息。
- en: However, it’s this potential for an alternate, although equally effective, viewpoint
    that can lead AI down different paths of intention than humans. It’s what might
    make us lose our control.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正是这种替代性、尽管同样有效的观点，可能使得 AI 在意图上走上与人类不同的道路。这可能让我们失去控制。
- en: From the work of Ilyas et al., a key focus, then, should be to embed human biases
    into ML algorithms to always nudge them into the direction we expect from *our*
    viewpoint. For their future work, Tsipras said, “we are actively working on utilizing
    robustness as a tool to bias models towards learning features that are more meaningful
    to humans.”
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Ilyas 等人的研究，一个关键的关注点应该是将人类偏见嵌入到机器学习算法中，以便始终将其引导到我们期望的*视角*中。Tsipras 说，他们正在积极利用鲁棒性作为工具，以将模型偏向学习对人类更有意义的特征。
- en: Building our models to be more human-aligned by including priors during development
    and training may be the crucial starting point to ensuring we don’t lose our controlling
    grasp on the future of AI.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发和训练过程中通过包括先验来使我们的模型更符合人类的期望，可能是确保我们不失去对未来 AI 控制的关键起点。
- en: So, consider keeping a little touch of “human” in your next machine learning
    project.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在你的下一个机器学习项目中，考虑保留一点“人性”。
- en: …
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: 'Resources cited in this article:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本文引用的资源：
- en: Amodei, D. et al. “Concrete Problems in AI Safety.” [arXiv.org](https://arxiv.org/abs/1606.06565),
    June 2016.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Amodei, D. 等. “AI 安全中的具体问题。” [arXiv.org](https://arxiv.org/abs/1606.06565),
    2016年6月。
- en: Tanay, T. “Adversarial Examples, Explained.” [KDnuggets](https://www.kdnuggets.com/2018/10/adversarial-examples-explained.html),
    October 2018.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Tanay, T. “对抗样本的解释。” [KDnuggets](https://www.kdnuggets.com/2018/10/adversarial-examples-explained.html)，2018年10月。
- en: Jain, A. “Breaking neural networks with adversarial attacks.” [KDnuggets](https://www.kdnuggets.com/2019/03/breaking-neural-networks-adversarial-attacks.html),
    March 2019.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jain, A. “用对抗攻击破解神经网络。” [KDnuggets](https://www.kdnuggets.com/2019/03/breaking-neural-networks-adversarial-attacks.html)，2019年3月。
- en: Papernot, N. et al. “Practical Black-Box Attacks against Machine Learning.”
    [arXiv.org](https://arxiv.org/abs/1602.02697), February 2016.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Papernot, N. 等. “针对机器学习的实际黑箱攻击。” [arXiv.org](https://arxiv.org/abs/1602.02697)，2016年2月。
- en: 'Papernot, N. et al. “Transferability in Machine Learning: from Phenomena to
    Black-Box Attacks using Adversarial Samples.” [arXiv.org](https://arxiv.org/abs/1605.07277),
    May 2016.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Papernot, N. 等. “机器学习中的可迁移性：从现象到使用对抗样本的黑箱攻击。” [arXiv.org](https://arxiv.org/abs/1605.07277)，2016年5月。
- en: Jost, Z. “Machine Learning Security.” [KDnuggets](https://www.kdnuggets.com/2019/01/machine-learning-security.html),
    January 2019.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jost, Z. “机器学习安全性。” [KDnuggets](https://www.kdnuggets.com/2019/01/machine-learning-security.html)，2019年1月。
- en: Ilyas, A. et al. “Adversarial Examples Are Not Bugs, They Are Features.” [arXiv.org](https://arxiv.org/abs/1905.02175),
    May 2019.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ilyas, A. 等. “对抗样本不是错误，它们是特性。” [arXiv.org](https://arxiv.org/abs/1905.02175)，2019年5月。
- en: '**Related:**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[3 Big Problems with Big Data and How to Solve Them](https://www.kdnuggets.com/2019/04/3-big-problems-big-data.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大数据的3个重大问题及其解决方法](https://www.kdnuggets.com/2019/04/3-big-problems-big-data.html)'
- en: '[Machine Learning Meets Humans – Insights from HUML 2016](https://www.kdnuggets.com/2017/01/machine-learning-humans-huml-2016.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习与人类的相遇——来自HUML 2016的见解](https://www.kdnuggets.com/2017/01/machine-learning-humans-huml-2016.html)'
- en: '[Key Takeaways from AI Conference SF, Day 2: AI and Security, Adversarial Examples,
    Innovation](https://www.kdnuggets.com/2018/10/key-takeaways-aiconf-san-francisco-day2.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AI会议SF第2天的关键要点：AI与安全、对抗样本、创新](https://www.kdnuggets.com/2018/10/key-takeaways-aiconf-san-francisco-day2.html)'
- en: More On This Topic
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[GPT-4 is Vulnerable to Prompt Injection Attacks on Causing Misinformation](https://www.kdnuggets.com/2023/05/gpt4-vulnerable-prompt-injection-attacks-causing-misinformation.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT-4易受提示注入攻击导致误导信息](https://www.kdnuggets.com/2023/05/gpt4-vulnerable-prompt-injection-attacks-causing-misinformation.html)'
- en: '[10 Most Common Data Quality Issues and How to Fix Them](https://www.kdnuggets.com/2022/11/10-common-data-quality-issues-fix.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10个最常见的数据质量问题及其解决方法](https://www.kdnuggets.com/2022/11/10-common-data-quality-issues-fix.html)'
- en: '[What is Adversarial Machine Learning?](https://www.kdnuggets.com/2022/03/adversarial-machine-learning.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是对抗性机器学习？](https://www.kdnuggets.com/2022/03/adversarial-machine-learning.html)'
- en: '[Machine Learning Algorithms - What, Why, and How?](https://www.kdnuggets.com/2022/09/machine-learning-algorithms.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习算法——什么、为什么以及如何？](https://www.kdnuggets.com/2022/09/machine-learning-algorithms.html)'
- en: '[Why Do Machine Learning Models Die In Silence?](https://www.kdnuggets.com/2022/01/machine-learning-models-die-silence.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么机器学习模型在沉默中消亡？](https://www.kdnuggets.com/2022/01/machine-learning-models-die-silence.html)'
- en: '[Machine learning does not produce value for my business. Why?](https://www.kdnuggets.com/2021/12/machine-learning-produce-value-business.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习没有为我的业务创造价值。为什么？](https://www.kdnuggets.com/2021/12/machine-learning-produce-value-business.html)'
