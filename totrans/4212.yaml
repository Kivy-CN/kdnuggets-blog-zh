- en: An Introduction to Reinforcement Learning with OpenAI Gym, RLlib, and Google
    Colab
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 OpenAI Gym、RLlib 和 Google Colab 进行强化学习简介
- en: 原文：[https://www.kdnuggets.com/2021/09/intro-reinforcement-learning-openai-gym-rllib-colab.html](https://www.kdnuggets.com/2021/09/intro-reinforcement-learning-openai-gym-rllib-colab.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/09/intro-reinforcement-learning-openai-gym-rllib-colab.html](https://www.kdnuggets.com/2021/09/intro-reinforcement-learning-openai-gym-rllib-colab.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Michael Galarnyk](https://www.linkedin.com/in/michaelgalarnyk/) & [Sven
    Mika](https://www.anyscale.com/blog?author=sven-mika)**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Michael Galarnyk](https://www.linkedin.com/in/michaelgalarnyk/) 和 [Sven
    Mika](https://www.anyscale.com/blog?author=sven-mika)**'
- en: This tutorial will use reinforcement learning (RL) to help balance a virtual
    CartPole. The [video](https://www.youtube.com/watch?v=XiigTGKZfks&t=106s) above
    from PilcoLearner shows the results of using RL in a real-life CartPole environment.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将使用强化学习（RL）来帮助平衡一个虚拟的 CartPole。上面的 [视频](https://www.youtube.com/watch?v=XiigTGKZfks&t=106s)
    来自 PilcoLearner，展示了在真实 CartPole 环境中使用 RL 的结果。
- en: One possible definition of reinforcement learning (RL) is a computational approach
    to learning how to maximize the total sum of rewards when interacting with an
    environment. While a definition is useful, this tutorial aims to illustrate what
    reinforcement learning is through images, code, and video examples and along the
    way introduce reinforcement learning terms like agents and environments.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）的一个可能定义是，一种计算方法，用于学习如何在与环境交互时最大化奖励的总和。虽然定义是有用的，但本教程旨在通过图像、代码和视频示例来展示强化学习是什么，并在此过程中介绍像代理和环境这样的强化学习术语。
- en: 'In particular, this tutorial explores:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，本教程探讨：
- en: What is Reinforcement Learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是强化学习
- en: The OpenAI Gym CartPole Environment
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI Gym CartPole 环境
- en: The Role of Agents in Reinforcement Learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理在强化学习中的作用
- en: How to Train an Agent by using the Python Library RLlib
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 Python 库 RLlib 训练代理
- en: How to use a GPU to Speed Up Training
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 GPU 加速训练
- en: Hyperparameter Tuning with Ray Tune
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Ray Tune 进行超参数调优
- en: What is Reinforcement Learning
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是强化学习
- en: As a [previous post noted](https://www.anyscale.com/blog/reinforcement-learning-with-rllib-in-the-unity-game-engine),
    machine learning (ML), a sub-field of AI, uses neural networks or other types
    of mathematical models to learn how to interpret complex patterns. Two areas of
    ML that have recently become very popular due to their high level of maturity
    are supervised learning (SL), in which neural networks learn to make predictions
    based on large amounts of data, and reinforcement learning (RL), where the networks
    learn to make good action decisions in a trial-and-error fashion, using a simulator.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 [之前的帖子提到的](https://www.anyscale.com/blog/reinforcement-learning-with-rllib-in-the-unity-game-engine)，机器学习（ML），作为
    AI 的一个子领域，利用神经网络或其他类型的数学模型来学习如何解释复杂的模式。由于其成熟度高，最近非常流行的两个 ML 领域是监督学习（SL），在这个领域中，神经网络学习基于大量数据进行预测，以及强化学习（RL），在该领域中，网络通过模拟器以试错的方式学习做出良好的行动决策。
- en: '![intro-to-rl-1](../Images/f4070fd03af2cf9672e423792ec19925.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![intro-to-rl-1](../Images/f4070fd03af2cf9672e423792ec19925.png)'
- en: RL is the tech behind mind-boggling successes such as DeepMind’s AlphaGo Zero
    and the StarCraft II AI (AlphaStar) or OpenAI’s DOTA 2 AI (“OpenAI Five”). Note
    that there are [many impressive uses of reinforcement learning](https://www.anyscale.com/blog/best-reinforcement-learning-talks-from-ray-summit-2021) and
    the reason why it is so powerful and promising for real-life decision making problems
    is because RL is capable of learning continuously — sometimes even in ever changing
    environments — starting with no knowledge of which decisions to make whatsoever
    (random behavior).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: RL 是 DeepMind 的 AlphaGo Zero 和 StarCraft II AI（AlphaStar）或 OpenAI 的 DOTA 2 AI（“OpenAI
    Five”）等令人惊叹的成功背后的技术。请注意，[强化学习的许多令人印象深刻的应用](https://www.anyscale.com/blog/best-reinforcement-learning-talks-from-ray-summit-2021)
    以及它在现实决策问题中如此强大和有前景的原因是，RL 能够不断学习——有时甚至在不断变化的环境中——从完全没有做出决策的知识（随机行为）开始。
- en: Agents and Environments
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理和环境
- en: '![agents-and-environments](../Images/c4030151c211208821ed55d1d5fc4a13.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![agents-and-environments](../Images/c4030151c211208821ed55d1d5fc4a13.png)'
- en: The diagram above shows the interactions and communications between an agent
    and an environment. In reinforcement learning, one or more agents interact within
    an environment which may be either a simulation like CartPole in this tutorial
    or a connection to real-world sensors and actuators. At each step, the agent receives
    an observation (i.e., the state of the environment), takes an action, and usually
    receives a reward (the frequency at which an agent receives a reward depends on
    a given task or problem). Agents learn from repeated trials, and a sequence of
    those is called an episode — the sequence of actions from an initial observation
    up to either a “success” or “failure” causing the environment to reach its “done”
    state. The learning portion of an RL framework trains a policy about which actions
    (i.e., sequential decisions) cause agents to maximize their long-term, cumulative
    rewards.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了代理与环境之间的交互和通信。在强化学习中，一个或多个代理在一个环境中进行交互，这个环境可能是像本教程中的 CartPole 这样的模拟，也可能是连接到现实世界传感器和执行器的环境。在每一步，代理接收一个观察（即环境的状态），采取一个动作，并通常接收一个奖励（代理接收奖励的频率取决于给定任务或问题）。代理通过重复试验进行学习，这些试验的序列称为一个
    episode——从初始观察到“成功”或“失败”的一系列动作，使环境达到“完成”状态。强化学习框架的学习部分训练一个策略，以确定哪些动作（即顺序决策）可以使代理最大化其长期的累积奖励。
- en: The OpenAI Gym Cartpole Environment
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAI Gym Cartpole 环境
- en: '![Figure](../Images/fb66617e97423db91b7a05be0ca7dab1.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/fb66617e97423db91b7a05be0ca7dab1.png)'
- en: CartPole
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: CartPole
- en: The problem we are trying to solve is trying to keep a pole upright. Specifically,
    the pole is attached by an un-actuated joint to a cart, which moves along a frictionless
    track. The pendulum starts upright, and the goal is to prevent it from falling
    over by increasing and reducing the cart's velocity.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要解决的问题是保持杆子直立。具体来说，杆子通过一个未驱动的关节连接到一个沿着无摩擦轨道移动的小车上。摆锤从直立位置开始，目标是通过增加或减少小车的速度来防止它倒下。
- en: '![policy ](../Images/76939bcc741d03593c2fcc04fa4e1a03.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![policy](../Images/76939bcc741d03593c2fcc04fa4e1a03.png)'
- en: Rather than code this environment from scratch, this tutorial will use [OpenAI
    Gym](http://gym.openai.com/) which is a toolkit that provides a wide variety of
    simulated environments (Atari games, board games, 2D and 3D physical simulations,
    and so on). Gym makes no assumptions about the structure of your agent (what pushes
    the cart left or right in this cartpole example), and is compatible with any numerical
    computation library, such as numpy.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与其从头编写这个环境的代码，不如使用 [OpenAI Gym](http://gym.openai.com/)，这是一个提供各种模拟环境的工具包（例如
    Atari 游戏、棋盘游戏、2D 和 3D 物理模拟等）。Gym 不对代理的结构做任何假设（在这个 cartpole 示例中是什么推动小车向左或向右），并且与任何数值计算库（如
    numpy）兼容。
- en: The code below loads the cartpole environment.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码加载了 cartpole 环境。
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let's now start to understand this environment by looking at the action space.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过查看动作空间来理解这个环境。
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![env.action_space](../Images/4f953445588d3239751a5922010d343a.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![env.action_space](../Images/4f953445588d3239751a5922010d343a.png)'
- en: 'The output Discrete(2) means that there are two actions. In cartpole, 0 corresponds
    to "push cart to the left" and 1 corresponds to "push cart to the right". Note
    that in this particular example, standing still is not an option. In reinforcement
    learning, the agent produces an action output and this action is sent to an environment
    which then reacts. The environment produces an observation (along with a reward
    signal, not shown here) which we can see below:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 Discrete(2) 表示有两个动作。在 cartpole 中，0 对应于“将小车推向左侧”，而 1 对应于“将小车推向右侧”。注意，在这个特定的示例中，静止不动不是一个选项。在强化学习中，代理产生一个动作输出，这个动作被发送到环境中，环境随后作出反应。环境生成一个观察（以及一个奖励信号，此处未显示），我们可以看到下面的内容：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![env.reset ()](../Images/33891c8ea8faa80c232aaed5c512eed7.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![env.reset ()](../Images/33891c8ea8faa80c232aaed5c512eed7.png)'
- en: The observation is a vector of dim=4, containing the cart's x position, cart
    x velocity, the pole angle in radians (1 radian = 57.295 degrees), and the angular
    velocity of the pole. The numbers shown above are the initial observation after
    starting a new episode (`env.reset()`). With each timestep (and action), the observation
    values will change, depending on the state of the cart and pole.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 观察是一个维度为 4 的向量，包括小车的 x 位置、小车的 x 速度、杆子的角度（以弧度表示，1 弧度 = 57.295 度）以及杆子的角速度。上面显示的数字是开始新一集后的初始观察（`env.reset()`）。每个时间步（和动作）后，观察值将会变化，具体取决于小车和杆子的状态。
- en: Training an Agent
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练代理
- en: In reinforcement learning, the goal of the agent is to produce smarter and smarter
    actions over time. It does so with a policy. In deep reinforcement learning, this
    policy is represented with a neural network. Let's first interact with the gym
    environment without a neural network or machine learning algorithm of any kind.
    Instead we'll start with random movement (left or right). This is just to understand
    the mechanisms.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，代理的目标是随着时间的推移产生越来越聪明的动作。它通过策略来实现这一点。在深度强化学习中，这一策略通过神经网络来表示。我们首先与gym环境进行互动，不使用神经网络或任何机器学习算法。相反，我们从随机运动（左或右）开始。这只是为了理解机制。
- en: '![policy random movement](../Images/8cffa24f22da5da6a05db5329f856048.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![策略随机移动](../Images/8cffa24f22da5da6a05db5329f856048.png)'
- en: The code below resets the environment and takes 20 steps (20 cycles), always
    taking a random action and printing the results.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码重置环境并执行20步（20个周期），每次都采取随机动作并打印结果。
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![steps](../Images/b12cc3182b86e800c621b97f36132b17.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![步骤](../Images/b12cc3182b86e800c621b97f36132b17.png)'
- en: Sample output. There are multiple conditions for episode termination in cartpole.
    In the image, the episode is terminated because it is over 12 degrees (0.20944
    rad). Other conditions for episode termination are cart position is more than
    2.4 (center of the cart reaches the edge of the display), episode length is greater
    than 200, or the solved requirement which is when the average return is greater
    than or equal to 195.0 over 100 consecutive trials.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 示例输出。cartpole 中有多个回合终止条件。在图像中，回合因超过12度（0.20944弧度）而终止。其他回合终止条件包括小车位置超过2.4（即小车中心到达显示屏边缘）、回合长度超过200，或解决要求，即平均回报在100次连续试验中大于或等于195.0。
- en: 'The printed output above shows the following things:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 上面打印的输出显示了以下内容：
- en: step (how many times it has cycled through the environment). In each timestep,
    an agent chooses an action, and the environment returns an observation and a reward
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤（环境已经循环了多少次）。在每个时间步中，代理选择一个动作，环境返回一个观察结果和一个奖励。
- en: observation of the environment [x cart position, x cart velocity, pole angle
    (rad), pole angular velocity]
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境的观察 [x 小车位置, x 小车速度, 杆角度（弧度），杆角速度]
- en: reward achieved by the previous action. The scale varies between environments,
    but the goal is always to increase your total reward. The reward is 1 for every
    step taken for cartpole, including the termination step. After it is 0 (step 18
    and 19 in the image).
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励是通过先前的动作获得的。这个尺度在不同环境中有所不同，但目标始终是增加你的总奖励。对于cartpole，每一步的奖励是1，包括终止步骤。之后奖励为0（图中的第18和19步）。
- en: done is a boolean. It indicates whether it's time to reset the environment again.
    Most tasks are divided up into well-defined episodes, and done being True indicates
    the episode has terminated. In cart pole, it could be that the pole tipped too
    far (more than 12 degrees/0.20944 radians), position is more than 2.4 meaning
    the center of the cart reaches the edge of the display, episode length is greater
    than 200, or the solved requirement which is when the average return is greater
    than or equal to 195.0 over 100 consecutive trials.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: done 是一个布尔值。它指示是否需要重新设置环境。大多数任务被划分为定义明确的回合，done 为 True 表示回合已经结束。在 cart pole
    中，这可能是由于杆子倾斜得太远（超过12度/0.20944弧度）、位置超过2.4（即小车中心到达显示屏边缘）、回合长度超过200，或解决要求，即平均回报在100次连续试验中大于或等于195.0。
- en: info which is diagnostic information useful for debugging. It is empty for this
    cartpole environment.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: info 是有用的诊断信息，用于调试。对于这个 cartpole 环境，它是空的。
- en: While these numbers are useful outputs, a video might be clearer. If you are
    running this code in Google Colab,  it is important to note that there is no display
    driver available for generating videos. However, it is possible to install a virtual
    display driver to get it to work.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些数字是有用的输出，但视频可能更清晰。如果你在 Google Colab 中运行此代码，请注意，生成视频时没有可用的显示驱动程序。然而，可以安装虚拟显示驱动程序以使其正常工作。
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The next step is to start an instance of the virtual display.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是启动虚拟显示实例。
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: OpenAI gym has a VideoRecorder wrapper that can record a video of the running
    environment in MP4 format. The code below is the same as before except that it
    is for 200 steps and is recording.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI gym 有一个 VideoRecorder 包装器，可以将运行环境录制为 MP4 格式的视频。以下代码与之前相同，只是它录制了200步。
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![user warning](../Images/bacf6b8d7d383ca0f38b53d49dfa3ad2.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![用户警告](../Images/bacf6b8d7d383ca0f38b53d49dfa3ad2.png)'
- en: Usually you end the simulation when done is 1 (True). The code above let the
    environment keep on going after a termination condition was reached. For example,
    in CartPole, this could be when the pole tips over, pole goes off-screen, or reaches
    other termination conditions.
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通常，当 done 为 1（True）时，你会结束仿真。上面的代码让环境在达到终止条件后继续运行。例如，在 CartPole 中，这可能是当杆子倾斜、杆子消失在屏幕外，或达到其他终止条件时。
- en: The code above saved the video file into the Colab disk. In order to display
    it in the notebook, you need a helper function.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将视频文件保存到了 Colab 磁盘中。为了在笔记本中显示它，你需要一个辅助函数。
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The code below renders the results. You should get a video similar to the one
    below.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码渲染了结果。你应该能获得一个类似于下面的视频。
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Playing the video demonstrates that randomly choosing an action is not a good
    policy for keeping the CartPole upright.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 播放视频显示，随机选择动作并不是保持 CartPole 直立的好策略。
- en: How to Train an Agent using Ray's RLlib
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用 Ray 的 RLlib 训练智能体
- en: The previous section of the tutorial had our agent make random actions disregarding
    the observations and rewards from the environment. The goal of having an agent
    is to produce smarter and smarter actions over time and random actions don't accomplish
    that. To make an agent make smarter actions over time, itl needs a better policy.
    In deep reinforcement learning, the policy is represented with a neural network.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程的前一部分让我们的智能体随机执行动作，忽略了来自环境的观察和奖励。智能体的目标是随着时间的推移产生越来越智能的动作，而随机动作无法实现这一目标。为了让智能体随着时间的推移产生更智能的动作，它需要一个更好的策略。在深度强化学习中，策略是通过神经网络来表示的。
- en: '![policy deep-rl](../Images/bcf88c97a0e018a6555b2e832af00d3e.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![policy deep-rl](../Images/bcf88c97a0e018a6555b2e832af00d3e.png)'
- en: 'This tutorial will use the [RLlib library](https://docs.ray.io/en/master/rllib.html) to
    train a smarter agent. RLlib has many advantages like:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将使用 [RLlib 库](https://docs.ray.io/en/master/rllib.html) 来训练一个更智能的智能体。RLlib
    具有许多优势，比如：
- en: Extreme flexibility. It allows you to customize every aspect of the RL cycle.
    For instance, this section of the tutorial will make a custom neural network policy
    using PyTorch (RLlib also has native support for TensorFlow).
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极大的灵活性。它允许你定制 RL 周期的每一个方面。例如，本教程的这一部分将使用 PyTorch 制作一个自定义神经网络策略（RLlib 也原生支持 TensorFlow）。
- en: Scalability. Reinforcement learning applications can be quite compute intensive
    and often need to scale-out to a cluster for faster training. RLlib not only has
    first-class support for GPUs, but it is also built on [Ray](https://ray.io/) which
    makes scaling Python programs from a laptop to a cluster easy.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展性。强化学习应用程序可能会非常消耗计算资源，并且通常需要扩展到集群以加速训练。RLlib 不仅对 GPU 提供一流的支持，还建立在 [Ray](https://ray.io/) 之上，这使得从笔记本电脑到集群的
    Python 程序扩展变得简单。
- en: A unified API and support for offline, model-based, model-free, multi-agent
    algorithms, and more (these algorithms won’t be explored in this tutorial).
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统一的 API 以及对离线、基于模型、无模型、多智能体算法等的支持（这些算法在本教程中不会被探讨）。
- en: Being part of the [Ray Project ecosystem](https://github.com/ray-project/ray).
    One advantage of this is that RLlib can be run with other libraries in the ecosystem
    like [Ray Tune](https://docs.ray.io/en/master/tune/index.html), a library for
    experiment execution and hyperparameter tuning at any scale (more on this later).
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为 [Ray Project 生态系统](https://github.com/ray-project/ray)的一部分。这有一个好处，即 RLlib
    可以与生态系统中的其他库一起运行，如 [Ray Tune](https://docs.ray.io/en/master/tune/index.html)，这是一个用于实验执行和超参数调整的库，支持任何规模（稍后会详细介绍）。
- en: While some of these features won't be fully utilized in this post, they are
    highly useful for when you want to do something more complicated and solve real
    world problems. You can learn about some impressive use-cases of RLlib [here](https://www.anyscale.com/blog/best-reinforcement-learning-talks-from-ray-summit-2021).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些功能中的一些在本帖中不会被完全利用，但它们对于当你想做一些更复杂的事情和解决实际问题时非常有用。你可以在 [这里](https://www.anyscale.com/blog/best-reinforcement-learning-talks-from-ray-summit-2021) 了解一些
    RLlib 的令人印象深刻的用例。
- en: To get started with RLlib, you need to first install it.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用 RLlib，你需要首先安装它。
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now you can train a PyTorch model using the Proximal Policy Optimization (PPO)
    algorithm. It is a very well rounded, one size fits all type of algorithm which
    you can learn more about [here](https://docs.ray.io/en/master/rllib-algorithms.html#proximal-policy-optimization-ppo).
    The code below uses a neural network consisting of a single hidden layer of 32
    neurons and linear activation functions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以使用 Proximal Policy Optimization (PPO) 算法训练一个 PyTorch 模型。这是一种非常全面的、适合所有场景的算法，你可以在[这里](https://docs.ray.io/en/master/rllib-algorithms.html#proximal-policy-optimization-ppo)了解更多。下面的代码使用了一个包含
    32 个神经元和线性激活函数的单隐层神经网络。
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This code should produce quite a bit of output. The final entry should look
    something like this:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码应该会产生相当多的输出。最终条目应该类似于：
- en: '![status](../Images/55c8f51dc6dba6aecf920e54657ea47f.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![状态](../Images/55c8f51dc6dba6aecf920e54657ea47f.png)'
- en: The entry shows it took 35 iterations, running over 258 seconds, to solve the
    environment. This will be different each time, but will probably be about 7 seconds
    per iteration (258 / 35 = 7.3). Note that if you like to learn the Ray API and
    see what commands like ray.shutdown and ray.init do, you can [check out this tutorial](https://www.anyscale.com/blog/writing-your-first-distributed-python-application-with-ray).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 该条目显示解决环境需要 35 次迭代，运行了 258 秒。每次可能会有所不同，但每次迭代大约需要 7 秒（258 / 35 = 7.3）。如果你希望了解
    Ray API 并查看类似 ray.shutdown 和 ray.init 的命令，你可以[查看这个教程](https://www.anyscale.com/blog/writing-your-first-distributed-python-application-with-ray)。
- en: How to use a GPU to Speed Up Training
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用 GPU 加速训练
- en: While the rest of the tutorial utilizes CPUs, it is important to note that you
    can speed up model training by using a GPU in Google Colab. This can be done by
    selecting **Runtime > Change runtime type** and set hardware accelerator to **GPU**.
    Then select **Runtime > Restart and run all**.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管教程的其余部分使用了 CPU，但值得注意的是，你可以通过在 Google Colab 中使用 GPU 来加速模型训练。可以通过选择**运行时 > 更改运行时类型**并将硬件加速器设置为**GPU**来完成。然后选择**运行时
    > 重启并运行所有**。
- en: '![status 2](../Images/35133d87395977e42ff8f7e55d01a004.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![状态 2](../Images/35133d87395977e42ff8f7e55d01a004.png)'
- en: Notice that, although the number of training iterations might be about the same,
    the time per iteration has come down significantly (from 7 seconds to 5.5 seconds).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到，尽管训练迭代的次数可能差不多，但每次迭代的时间显著减少（从 7 秒减少到 5.5 秒）。
- en: Creating a Video of the Trained Model in Action
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建训练模型运行的视频
- en: RLlib provides a Trainer class which holds a policy for environment interaction.
    Through the trainer interface, a policy can be trained, action computed, and checkpointed.
    While the analysis object returned from *ray.tune.run* earlier did not contain
    any trainer instances, it has all the information needed to reconstruct one from
    a saved checkpoint because  *checkpoint_at_end=True* was passed as a parameter.
    The code below shows this.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: RLlib 提供了一个 Trainer 类，用于持有环境交互的策略。通过训练器接口，可以训练策略、计算动作和保存检查点。虽然之前从*ray.tune.run*返回的分析对象没有包含任何训练器实例，但它拥有从保存的检查点重建一个所需的所有信息，因为传递了*checkpoint_at_end=True*作为参数。下面的代码展示了这一点。
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Let’s now create another video, but this time choose the action recommended
    by the trained model instead of acting randomly.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们再创建一个视频，但这次选择训练模型推荐的动作，而不是随机行动。
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This time, the pole balances nicely which means the agent has solved the cartpole
    environment!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，杆子平衡得很好，这意味着代理已经解决了 cartpole 环境！
- en: Hyperparameter Tuning with Ray Tune
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Ray Tune 进行超参数调整
- en: '![Figure](../Images/c8485cf99eafb673e010078db6660b81.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/c8485cf99eafb673e010078db6660b81.png)'
- en: The Ray Ecosystem
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 生态系统
- en: RLlib is a reinforcement learning library that is part of the Ray Ecosystem.
    Ray is a highly scalable universal framework for parallel and distributed python.
    It is very general and that generality is important for supporting its library
    ecosystem. The ecosystem covers everything from [training](https://docs.ray.io/en/latest/tune/index.html),
    to [production serving](https://docs.ray.io/en/master/serve/index.html), to [data
    processing](https://www.anyscale.com/blog/data-processing-support-in-ray) and
    more. You can use multiple libraries together and build applications that do all
    of these things.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: RLlib 是一个强化学习库，是 Ray 生态系统的一部分。Ray 是一个高度可扩展的通用框架，用于并行和分布式 Python。它非常通用，这种通用性对于支持其库生态系统至关重要。生态系统涵盖了从[训练](https://docs.ray.io/en/latest/tune/index.html)、到[生产服务](https://docs.ray.io/en/master/serve/index.html)、到[数据处理](https://www.anyscale.com/blog/data-processing-support-in-ray)等各方面。你可以将多个库一起使用，构建执行这些操作的应用程序。
- en: This part of the tutorial utilizes [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) which
    is another library in the Ray Ecosystem. It is a library for experiment execution
    and hyperparameter tuning at any scale. While this tutorial will only use grid
    search, note that Ray Tune also gives you access to more efficient hyperparameter
    tuning algorithms like population based training, BayesOptSearch, and HyperBand/ASHA.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分教程使用了[Ray Tune](https://docs.ray.io/en/latest/tune/index.html)，这是 Ray 生态系统中的另一个库。它是一个用于实验执行和超参数调优的库，支持任何规模。虽然本教程只使用网格搜索，但请注意
    Ray Tune 还提供了更高效的超参数调优算法，如基于种群的训练、BayesOptSearch 和 HyperBand/ASHA。
- en: Let’s now try to find hyperparameters that can solve the CartPole environment
    in the fewest timesteps.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试找出能够在最少时间步数内解决 CartPole 环境的超参数。
- en: 'Enter the following code, and be prepared for it to take a while to run:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 输入以下代码，准备好可能需要一段时间来运行：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: By asking for 12 CPU cores by passing in num_cpus=12 to ray.init, four trials
    get run in parallel across three cpus each. If this doesn’t work, perhaps Google
    has changed the VMs available on Colab. Any value of three or more should work.
    If Colab errors by running out of RAM, you might need to do **Runtime > Factory
    reset runtime**, followed by **Runtime > Run all**. Note that there is an area
    in the top right of the Colab notebook showing the RAM and disk use.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通过传递 num_cpus=12 到 ray.init 请求 12 个 CPU 核心时，四个试验将在三个 CPU 上并行运行。如果这不起作用，可能是 Google
    更改了 Colab 上可用的虚拟机。任何三个或更多的值都应该有效。如果 Colab 由于内存不足而出错，你可能需要执行**Runtime > Factory
    reset runtime**，然后**Runtime > Run all**。注意 Colab 笔记本右上角有显示 RAM 和磁盘使用情况的区域。
- en: Specifying num_samples=5 means that you will get five random samples for the
    learning rate. For each of those, there are two values for the size of the hidden
    layer, and two values for the activation function. So, there will be 5 * 2 * 2
    = 20 trials, shown with their statuses in the output of the cell as the calculation
    runs.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 指定 num_samples=5 意味着你将获得五个随机样本用于学习率。对于每一个样本，有两个隐藏层大小的值和两个激活函数的值。因此，将会有 5 * 2
    * 2 = 20 个试验，试验状态将在单元格的输出中显示。
- en: Note that Ray prints the current best configuration as it goes. This includes
    all the default values that have been set, which is a good place to find other
    parameters that could be tweaked.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 Ray 在运行过程中会打印当前的最佳配置。这包括所有已设置的默认值，这是找到可以调整的其他参数的好地方。
- en: 'After running this, the final output might be similar to the following output:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 运行后，最终输出可能类似于以下输出：
- en: 'INFO tune.py:549 -- Total run time: 3658.24 seconds (3657.45 seconds for the
    tuning loop).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: INFO tune.py:549 -- 总运行时间：3658.24 秒（调优循环的时间为 3657.45 秒）。
- en: 'Best hyperparameters found: {''env'': ''CartPole-v0'', ''framework'': ''torch'',
    ''model'': {''fcnet_hiddens'': [64], ''fcnet_activation'': ''relu''}, ''lr'':
    0.006733929096170726};'''''''
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '找到的最佳超参数：{''env'': ''CartPole-v0'', ''framework'': ''torch'', ''model'': {''fcnet_hiddens'':
    [64], ''fcnet_activation'': ''relu''}, ''lr'': 0.006733929096170726};'''''''
- en: So, of the twenty sets of hyperparameters, the one with 64 neurons, the ReLU
    activation function, and a learning rate around 6.7e-3 performed best.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在二十组超参数中，具有 64 个神经元、ReLU 激活函数和大约 6.7e-3 学习率的组合表现最好。
- en: Conclusion
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: '![Figure](../Images/96b0fd8599b56a82579732a4fa9ecfdd.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/96b0fd8599b56a82579732a4fa9ecfdd.png)'
- en: Neural MMO is an environment modeled from Massively Multiplayer Online games
    — a genre supporting hundreds to thousands of concurrent players. You can learn
    how Ray and RLlib help enable some key features of this and other projects [here](https://www.anyscale.com/blog/best-reinforcement-learning-talks-from-ray-summit-2021).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Neural MMO 是一个从大规模多人在线游戏建模的环境——这种游戏类型支持数百到数千名同时在线玩家。你可以通过点击[这里](https://www.anyscale.com/blog/best-reinforcement_learning_talks_from_ray_summit_2021)了解
    Ray 和 RLlib 如何帮助实现这些和其他项目的关键功能。
- en: This tutorial illustrated what reinforcement learning is by introducing reinforcement
    learning terminology, by showing how agents and environments interact, and by
    demonstrating these concepts through code and video examples. If you would like
    to learn more about reinforcement learning, check out the [RLlib tutorial by Sven
    Mika](https://www.anyscale.com/events/2021/06/24/hands-on-reinforcement-learning-with-rays-rllib).
    It is a great way to learn about RLlib’s best practices, multi-agent algorithms,
    and much more. If you would like to keep up to date with all things RLlib and
    Ray, consider [following @raydistributed on twitter](https://twitter.com/raydistributed) and [sign
    up for the Ray newsletter](https://anyscale.us5.list-manage.com/subscribe?u=524b25758d03ad7ec4f64105f&id=d94e960a03).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程通过介绍强化学习术语、展示代理与环境的互动，并通过代码和视频示例演示这些概念，从而阐明了强化学习的内容。如果你想深入了解强化学习，建议查看 [Sven
    Mika 的 RLlib 教程](https://www.anyscale.com/events/2021/06/24/hands-on-reinforcement-learning-with-rays-rllib)。这是学习
    RLlib 最佳实践、多代理算法及更多内容的绝佳方式。如果你希望随时了解 RLlib 和 Ray 的所有最新动态，考虑 [关注 @raydistributed
    的 Twitter](https://twitter.com/raydistributed) 并 [注册 Ray 新闻通讯](https://anyscale.us5.list-manage.com/subscribe?u=524b25758d03ad7ec4f64105f&id=d94e960a03)。
- en: '[Original](https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google).
    Reposted with permission.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google)
    已获授权转载。'
- en: '**Related:**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Getting Started with Reinforcement Learning](/2021/04/getting-started-reinforcement-learning.html)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[强化学习入门](/2021/04/getting-started-reinforcement-learning.html)'
- en: '[Facebook Launches One of the Toughest Reinforcement Learning Challenges in
    History](/2021/06/facebook-launches-toughest-reinforcement-learning-challenges.html)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Facebook 推出历史上最具挑战性的强化学习挑战之一](/2021/06/facebook-launches-toughest-reinforcement-learning-challenges.html)'
- en: '[10 Real-Life Applications of Reinforcement Learning](/2021/04/10-real-life-applications-reinforcement-learning.html)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[强化学习的 10 个实际应用](/2021/04/10-real-life-applications-reinforcement-learning.html)'
- en: '* * *'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT'
- en: '* * *'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Running Redis on Google Colab](https://www.kdnuggets.com/2022/01/running-redis-google-colab.html)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Google Colab 上运行 Redis](https://www.kdnuggets.com/2022/01/running-redis-google-colab.html)'
- en: '[From Google Colab to a Ploomber Pipeline: ML at Scale with GPUs](https://www.kdnuggets.com/2022/03/google-colab-ploomber-pipeline-ml-scale-gpus.html)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从 Google Colab 到 Ploomber 管道：使用 GPU 进行大规模机器学习](https://www.kdnuggets.com/2022/03/google-colab-ploomber-pipeline-ml-scale-gpus.html)'
- en: '[Fine Tuning LLAMAv2 with QLora on Google Colab for Free](https://www.kdnuggets.com/fine-tuning-llamav2-with-qlora-on-google-colab-for-free)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Google Colab 上免费微调 LLAMAv2 与 QLora](https://www.kdnuggets.com/fine-tuning-llamav2-with-qlora-on-google-colab-for-free)'
- en: '[Running Mixtral 8x7b On Google Colab For Free](https://www.kdnuggets.com/running-mixtral-8x7b-on-google-colab-for-free)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Google Colab 上免费运行 Mixtral 8x7b](https://www.kdnuggets.com/running-mixtral-8x7b-on-google-colab-for-free)'
- en: '[RAPIDS cuDF for Accelerated Data Science on Google Colab](https://www.kdnuggets.com/2023/01/rapids-cudf-accelerated-data-science-google-colab.html)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RAPIDS cuDF 在 Google Colab 上加速数据科学](https://www.kdnuggets.com/2023/01/rapids-cudf-accelerated-data-science-google-colab.html)'
- en: '[OpenAI’s Whisper API for Transcription and Translation](https://www.kdnuggets.com/2023/06/openai-whisper-api-transcription-translation.html)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAI 的 Whisper API 用于转录和翻译](https://www.kdnuggets.com/2023/06/openai-whisper-api-transcription-translation.html)'
