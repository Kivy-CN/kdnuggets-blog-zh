- en: 'Clustering with scikit-learn: A Tutorial on Unsupervised Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/05/clustering-scikitlearn-tutorial-unsupervised-learning.html](https://www.kdnuggets.com/2023/05/clustering-scikitlearn-tutorial-unsupervised-learning.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Clustering with scikit-learn: A Tutorial on Unsupervised Learning](../Images/b86ba87744e6c80502bec063c6ee062f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Clustering is a popular unsupervised machine learning technique, meaning it
    is used for datasets where the target variable or outcome variable is not provided.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In unsupervised learning, algorithms are tasked with catching the patterns and
    relationships within data without any pre-existing knowledge or guidance.
  prefs: []
  type: TYPE_NORMAL
- en: What does clustering do? It groups similar data points, enabling us to discover
    hidden patterns and relationships within our data.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will explore the different clustering algorithms available
    and their respective use cases, along with important evaluation metrics to assess
    the quality of clustering results.
  prefs: []
  type: TYPE_NORMAL
- en: We will also demonstrate how to develop multiple clustering algorithms at once
    using the popular Python library scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will highlight some of the most famous real-life applications that
    used clustering, discussing the algorithms used and the evaluation metrics employed.
  prefs: []
  type: TYPE_NORMAL
- en: But first, let’s get familiar with the clustering algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: What are the Clustering Algorithms?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Below you will find an overview of the clustering algorithms and short definitions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering with scikit-learn: A Tutorial on Unsupervised Learning](../Images/940ae1dc6a569e725132ac247cef1cd2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: According to scikit-learn official documentation, there are 11 different clustering
    algorithms:  K-Means, Affinity propagation, Mean Shift, Special Clustering, Hierarchical
    Clustering, Agglomerative Clustering, DBScan, Optics, Gaussian Mixture, Birch,
    Bisecting K-Means.
  prefs: []
  type: TYPE_NORMAL
- en: '[Here](https://scikit-learn.org/stable/modules/clustering.html#clustering)
    you can find the official documentation.'
  prefs: []
  type: TYPE_NORMAL
- en: This section will explore the 5 most famous and important clustering algorithms.
    They are K-Means, Mean-Shift, DBScan, Gaussian Mixture, and Hierarchical Clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Before diving deeper, let’s look at the following graph. It shows how these
    five algorithms work on six differently structured datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering with scikit-learn: A Tutorial on Unsupervised Learning](../Images/0bcfe93fee62fd50217fc1cd654635e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Clustering Algorithms - Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: In the [scikit-learn documentation](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html),
    you will find similar graphs which inspired the image above. I limited it to the
    five most famous clustering algorithms and added the dataset's structure along
    the algorithm name, e.g., K-Means - Noisy Moons or K-Means Varied.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are six different datasets shown, all generated by using scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Noisy Circles:** This dataset consists of a large circle containing a smaller
    circle that is not perfectly centered. The data also has random Gaussian noise
    added to it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noisy Moons:** This dataset consists of two interleaving half-moon shapes
    that are not linearly separable. The data also has random Gaussian noise added
    to it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blobs:** This dataset consists of randomly generated blobs that are relatively
    uniform in size and shape. The dataset contains three blobs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No Structure:** This dataset consists of randomly generated data points with
    no inherent structure or clustering pattern.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anisotropicly Distributed:** This dataset consists of randomly generated
    data points that are anisotropically distributed. The data points are generated
    with a specific transformation matrix to make them elongated along certain axes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Varied:** This dataset consists of randomly generated blobs with varied variances.
    The dataset contains three blobs, each with a different standard deviation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seeing the plots and how each algorithm works on them will help us compare how
    well our algorithms perform on each dataset. This may help you in your data project
    if your data have the same structure as in these graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s dig deeper into these five algorithms, starting with the K-Means algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: K-Means
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Clustering with scikit-learn: A Tutorial on Unsupervised Learning](../Images/ffefd7b582ec70aa7df0aebbb28c4887.png)'
  prefs: []
  type: TYPE_IMG
- en: K-Means 2D | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering with scikit-learn: A Tutorial on Unsupervised Learning](../Images/bf81918b942555beb2292df55e50fd6f.png)'
  prefs: []
  type: TYPE_IMG
- en: K Means 3D | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: K-Means is a popular clustering algorithm that partitions a dataset into K distinct
    clusters, where K is a hyperparameter specified by the user.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm works by assigning each data point to the nearest cluster centroid
    and then recalculating the centroid based on the average of all the data points
    in that cluster.
  prefs: []
  type: TYPE_NORMAL
- en: This process continues until the centroids no longer move or a specified maximum
    number of iterations is reached.
  prefs: []
  type: TYPE_NORMAL
- en: It has also been used in various real-life applications, such as customer segmentation
    in e-commerce, disease clustering in healthcare, and image compression in computer
    vision.
  prefs: []
  type: TYPE_NORMAL
- en: To see the real-life applications of K-Means or other algorithms, continue reading.
    We will get to it in the later sections.
  prefs: []
  type: TYPE_NORMAL
- en: DBScan
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Clustering with scikit-learn: A Tutorial on Unsupervised Learning](../Images/8de3494c09f041a20ae000b8efad224d.png)'
  prefs: []
  type: TYPE_IMG
- en: DBScan | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: DBScan (Density-Based Spatial Clustering of Applications with Noise) is a density-based
    clustering algorithm that identifies clusters as high-density regions separated
    by low-density regions.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm groups together point that are close to each other based on a
    density threshold and a minimum number of points.
  prefs: []
  type: TYPE_NORMAL
- en: DBScan is often used in outlier detection, spatial clustering, and image segmentation,
    where the goal is to identify distinct clusters in the data while also handling
    noisy or outlier data points.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Clustering with scikit-learn: A Tutorial on Unsupervised Learning](../Images/682c33f0e069426fb4d86fd57143bc4c.png)'
  prefs: []
  type: TYPE_IMG
- en: Hierarchical Clustering | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical Clustering is a clustering algorithm that builds a tree-like structure
    of clusters by merging or splitting clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the approach taken, the algorithm can be agglomerative (bottom-up)
    or divisive (like the graph above).
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical Clustering has been used in various real-life applications such
    as social network analysis, image segmentation, and ecological research, where
    the goal is to identify meaningful relationships between clusters and subclusters.
  prefs: []
  type: TYPE_NORMAL
- en: Mean-Shift Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Clustering with scikit-learn: A Tutorial on Unsupervised Learning](../Images/ef77715493326f95d741b11f5255109b.png)'
  prefs: []
  type: TYPE_IMG
- en: Mean-Shift Clustering | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Mean-shift clustering is a non-parametric algorithm that doesn't require prior
    assumptions about the shape or number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm works by shifting each data point towards the local mean (x in
    the above graph) until convergence, where the kernel density function estimates
    the local mean.
  prefs: []
  type: TYPE_NORMAL
- en: The mean-shift algorithm identifies clusters as high-density regions in the
    feature space.
  prefs: []
  type: TYPE_NORMAL
- en: Mean-shift clustering has been used in real-life applications such as image
    segmentation, object tracking in video surveillance, and [anomaly detection](https://www.stratascratch.com/blog/machine-learning-algorithms-explained-anomaly-detection/?utm_source=blog&utm_medium=click&utm_campaign=kdn+clustering+unsupervised+learning)
    in network intrusion detection, where the goal is to identify distinct regions
    or patterns in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian Mixture Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Clustering with scikit-learn: A Tutorial on Unsupervised Learning](../Images/65fb3a8e31358038c73e9495d08d855d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Gaussian Mixture Model in the moon-shaped data set | Image by Author![Clustering
    with scikit-learn: A Tutorial on Unsupervised Learning](../Images/d66db08ceabfe04abbe8b9f81fb9462d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: K Means Clustering Model in the moon-shaped data set | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian Mixture is a probabilistic clustering algorithm that models the distribution
    of data points using a mixture of Gaussian distributions. The algorithm fits a
    set of Gaussian distributions to the data, where each Gaussian corresponds to
    a separate cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian Mixture has been used in various real-life applications such as speech
    recognition, gene expression analysis, and face recognition, where the goal is
    to model the underlying distribution of the data and identify clusters based on
    the fitted Gaussian distributions.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the graph above, the Gaussian mixture has a better capability
    of capturing trends in elliptical data points as above and drawing elliptical
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, each clustering algorithm has its unique strengths and weaknesses.
    The choice of algorithm depends on the problem at hand and the dataset's characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the nuances of each algorithm and its use cases is crucial for
    achieving accurate and meaningful results.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering Evaluation Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After applying the algorithm, you need to evaluate its performance to see whether
    there is room for improvement or change the algorithm if the performance of your
    algorithm does not meet the criteria. To do that, you should use evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s an overview of the most popular ones.
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering with scikit-learn: A Tutorial on Unsupervised Learning](../Images/de76f99ec499d990675696d2f5dff256.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'These are not all, of course. You can get the complete list on scikit-learn.
    It lists the following evaluation metrics: Rand Index, Mutual Information based
    scores, Silhouette Coefficient, Fowlkes-Mallows scores, Homogeneity, Completeness,
    V-measure, Calinski-Harabasz Index, Davies-Bouldin Index, Contingency Matrix,
    Pair Confusion Matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Here](https://scikit-learn.org/stable/modules/clustering.html#clustering)
    you can see the official documents.'
  prefs: []
  type: TYPE_NORMAL
- en: We will stick with the popular ones and start with the Rand Index.
  prefs: []
  type: TYPE_NORMAL
- en: Rand Index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Rand Index evaluates the similarity between the true cluster labels and
    the predicted cluster labels.
  prefs: []
  type: TYPE_NORMAL
- en: The index ranges from 0 to 1, with 1 indicating a perfect match between the
    true and predicted labels.
  prefs: []
  type: TYPE_NORMAL
- en: The Rand Index is often used in image segmentation, text clustering, and document
    clustering, where the true labels of the data are known.
  prefs: []
  type: TYPE_NORMAL
- en: Silhouette Coefficient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Silhouette Coefficient measures the quality of clustering based on how well-separated
    the clusters are and how similar are the data points within each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The coefficient ranges from -1 to 1, with 1 indicating a well-separated and
    compact cluster and -1 indicating an incorrect clustering.
  prefs: []
  type: TYPE_NORMAL
- en: The Silhouette Coefficient is often used in market segmentation, customer profiling,
    and product recommendation, where the goal is to identify meaningful clusters
    based on customer behavior and preferences.
  prefs: []
  type: TYPE_NORMAL
- en: Fowlkes-Mallows scores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Fowlkes-Mallows index is named after two researchers, Edward Fowlkes and
    S.G. Mallows, who proposed the metric in 1983.
  prefs: []
  type: TYPE_NORMAL
- en: The index measures the similarity between a clustering algorithm's true and
    predicted labels.
  prefs: []
  type: TYPE_NORMAL
- en: The score ranges from 0 to 1, with 1 indicating a perfect match between the
    true and predicted labels.
  prefs: []
  type: TYPE_NORMAL
- en: The Fowlkes-Mallows score is often used in image segmentation, text clustering,
    and document clustering, where the true labels of the data are known.
  prefs: []
  type: TYPE_NORMAL
- en: Davies-Bouldin Index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Davies-Bouldin Index is named after two researchers, David L. Davies and
    Donald W. Bouldin, who proposed the metric in 1979.
  prefs: []
  type: TYPE_NORMAL
- en: The index ranges from 0 to infinity, with lower values indicating better clustering
    quality.
  prefs: []
  type: TYPE_NORMAL
- en: It is handy for identifying the optimal number of clusters in the data and for
    detecting cases where the clusters overlap or are too similar to each other. However,
    the index assumes that the clusters are spherical and have similar densities,
    which may not always hold in real-world datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The Davies-Bouldin Index is often used in market segmentation, customer profiling,
    and product recommendation, where the goal is to identify meaningful clusters
    based on customer behavior and preferences.
  prefs: []
  type: TYPE_NORMAL
- en: Calinski-Harabasz Index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Calinski-Harabasz Index is named after T. Calinski and J. Harabasz, who
    proposed the metric in 1974.
  prefs: []
  type: TYPE_NORMAL
- en: The Calinski-Harabasz Index measures the quality of clustering based on how
    well-separated the clusters are and how well the data points within each cluster
    are similar to each other.
  prefs: []
  type: TYPE_NORMAL
- en: The index ranges from 0 to infinity, with higher values indicating better clustering.
  prefs: []
  type: TYPE_NORMAL
- en: The Calinski-Harabasz Index is often used in market segmentation, customer profiling,
    and product recommendation, where the goal is to identify meaningful clusters
    based on customer behavior and preferences.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Clustering with scikit-learn: A Tutorial on Unsupervised Learning](../Images/853188381b0cb4016331ac5d79a6a165.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: For the Rand Index, Silhouette Coefficient, and Fowlkes-Mallows scores, higher
    values indicate better clustering performance.
  prefs: []
  type: TYPE_NORMAL
- en: The best score is 1.
  prefs: []
  type: TYPE_NORMAL
- en: For Davies-Bouldin Index, lower values indicate better clustering performance.
  prefs: []
  type: TYPE_NORMAL
- en: The best score is 0.
  prefs: []
  type: TYPE_NORMAL
- en: For Calinski-Harabasz Index, the highest scores indicate better performance.
  prefs: []
  type: TYPE_NORMAL
- en: The best score is ∞. (infinity.)
  prefs: []
  type: TYPE_NORMAL
- en: In theory, the best score for the Calinski-Harabasz (CH) Index would be infinity,
    as it would indicate an extremely high between-cluster dispersion compared to
    the within-cluster dispersion. However, achieving an infinite CH Index value is
    not realistic in practice.
  prefs: []
  type: TYPE_NORMAL
- en: There is no fixed upper bound for the best score, as it depends on the specific
    data and clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Don’t forget: there is no algorithm or script which is perfect. If you achieve
    the best scores with any of these evaluation metrics, it’s quite likely your model
    is overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: How to Develop Multiple Clustering Algorithms at Once With scikit-learn?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The purpose here is to apply multiple clustering algorithms to the Iris dataset
    and calculate their performance using different evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Here we will use the IRIS dataset.
  prefs: []
  type: TYPE_NORMAL
- en: You can find this dataset [here](https://archive.ics.uci.edu/ml/datasets/iris).
  prefs: []
  type: TYPE_NORMAL
- en: The iris dataset is a famous multi-class classification dataset that contains
    150 samples of iris flowers, each having four features (length and width of sepals
    and petals).
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering with scikit-learn: A Tutorial on Unsupervised Learning](../Images/612e90b34d5c306110f2a628231861e9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Machine Learning in R for beginners](https://www.datacamp.com/community/tutorials/machine-learning-in-r)
  prefs: []
  type: TYPE_NORMAL
- en: There are three classes in the dataset representing three types of iris flowers.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is commonly used for machine learning and pattern recognition tasks,
    particularly for testing and comparing different [classification algorithms](https://www.stratascratch.com/blog/overview-of-machine-learning-algorithms-classification/?utm_source=blog&utm_medium=click&utm_campaign=kdn+clustering+unsupervised+learning).
    It was introduced by British statistician and biologist Ronald Fisher in 1936.
  prefs: []
  type: TYPE_NORMAL
- en: Here we will write the code, which imports the necessary libraries to load the
    dataset, implements five clustering algorithms (DBSCAN, K-Means, Hierarchical
    Clustering, Gaussian Mixture Model, and Mean Shift), and evaluates their performance
    using five metrics.
  prefs: []
  type: TYPE_NORMAL
- en: To do that, we will add the evaluation metrics and algorithms in the dictionaries
    and apply them with two for loops each other.
  prefs: []
  type: TYPE_NORMAL
- en: But we have an exception here. The **rand_score** and **fowlkes_mallows_score**
    functions compare clustering results with true labels, so we will add the **if-else
    block** to provide that.
  prefs: []
  type: TYPE_NORMAL
- en: Then we will add these results to the data frame to do further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here is the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering with scikit-learn: A Tutorial on Unsupervised Learning](../Images/1d90f8f90ddb9442c4fdffbaa5ca69be.png)'
  prefs: []
  type: TYPE_IMG
- en: Prediction DataFrame | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s make a visualization to see the result better. Here the aim is to
    create visuals of the clustering algorithm's evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: The following code pivots the data to have algorithms as columns and metrics
    as rows and then generates bar charts for each metric. This allows for easy comparison
    of the clustering algorithms' performance across different evaluation measures.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here is the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering with scikit-learn: A Tutorial on Unsupervised Learning](../Images/2f18af9a20aa92829646aad6cb366d7c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the Mean Shift algorithm performs the best according to the Silhouette
    Score and Davies Bouldin Score.
  prefs: []
  type: TYPE_NORMAL
- en: The K-Means algorithm performs the best according to the Calinski Harabasz Score,
    and GMM performs the best according to the Rand Score and Fowlkes-Mallows Score.
  prefs: []
  type: TYPE_NORMAL
- en: There is no clear winner among the clustering algorithms, as each one performs
    well on different metrics.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of the best algorithm depends on the specific requirements and the
    importance assigned to each evaluation metric in your clustering problem.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering Real-Life Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let’s see the real-life examples of both our algorithms and evaluation
    metrics to grasp the logic even further.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s the overview of the examples we’ll talk about in detail.
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering with scikit-learn: A Tutorial on Unsupervised Learning](../Images/972c3d2614ebd9a056c3dcbcf4f2fac6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Super Market Chain Personalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Clustering with scikit-learn: A Tutorial on Unsupervised Learning](../Images/f4b3799950da5654b6fde265d9a6bf28.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-life example**: A supermarket chain wants to create personalized marketing
    campaigns for its customers. They use K-Means clustering to segment customers
    based on their purchasing habits, demographics, and store visit frequency. These
    segments help the company tailor its marketing messages to engage better and serve
    its customers.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithm:** K-Means clustering'
  prefs: []
  type: TYPE_NORMAL
- en: K-Means is chosen because it is a simple, efficient, and widely-used clustering
    algorithm that works well with large datasets. It can quickly identify patterns
    and create distinct customer segments based on the input features.
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluation metrics:** Silhouette Score'
  prefs: []
  type: TYPE_NORMAL
- en: The Silhouette Score is used to evaluate the quality of customer segmentation
    by measuring how well each data point fits within its assigned cluster compared
    to other clusters. This helps ensure the clusters are compact and well-separated,
    which is essential for creating effective personalized marketing campaigns.
  prefs: []
  type: TYPE_NORMAL
- en: Fraudulent Transaction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Clustering with scikit-learn: A Tutorial on Unsupervised Learning](../Images/adfd49556ef14f99235136dfb81e5d4c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-life example**: A credit card company wants to detect fraudulent transactions.
    They use DBSCAN to cluster transactions based on factors like transaction amount,
    time, and location. Unusual transactions that don''t fit into any cluster are
    flagged as potential frauds for further investigation.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithm:** DBSCAN'
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN is chosen because it is a density-based clustering algorithm that can
    identify clusters of varying shapes and sizes, as well as detect noise points
    that do not belong to any cluster. This makes it suitable for detecting unusual
    patterns or outliers, such as potentially fraudulent transactions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluation metrics:** Silhouette Score'
  prefs: []
  type: TYPE_NORMAL
- en: The Silhouette Score is chosen as an evaluation metric in this case because
    it helps assess the effectiveness of DBSCAN by separating normal transactions
    from potential outliers representing fraud.
  prefs: []
  type: TYPE_NORMAL
- en: A higher Silhouette Score indicates that the clusters of regular transactions
    are well separated from each other and the noise points (outliers). This separation
    makes it easier to identify and flag suspicious transactions that deviate significantly
    from normal patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Cancer Genomics Relation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Clustering with scikit-learn: A Tutorial on Unsupervised Learning](../Images/5e4d4ab765bfbe4f23aabb53260eb6eb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-life example**: Researchers studying cancer genomics want to understand
    the relationships between different types of cancer cells. They use Hierarchical
    Clustering to group cells based on their gene expression patterns. The resulting
    clusters help them identify commonalities and differences between cancer types
    and develop targeted therapies.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithm**: Agglomerative Hierarchical Clustering'
  prefs: []
  type: TYPE_NORMAL
- en: Agglomerative Hierarchical Clustering is chosen because it creates a tree-like
    structure (dendrogram) that allows researchers to visualize and interpret the
    relationships between cancer cells at multiple levels of granularity. This approach
    can reveal nested subgroups of cells and helps researchers understand the hierarchical
    organization of cancer types based on their gene expression patterns.
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluation metrics**: Calinski-Harabasz Index'
  prefs: []
  type: TYPE_NORMAL
- en: The Calinski-Harabasz Index is chosen in this case because it measures the ratio
    of between-cluster dispersion to within-cluster dispersion. For cancer genomics,
    it helps researchers evaluate the clustering quality in terms of how distinct
    and well-separated the groups of cancer cells are based on their gene expression
    patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous Car
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Clustering with scikit-learn: A Tutorial on Unsupervised Learning](../Images/d065fcf84da57950cd29cdcf02083f70.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-life example**: A self-driving car company wants to improve the car''s
    ability to identify objects in its surroundings. They use Mean-Shift Clustering
    to segment images captured by the car''s cameras into different regions based
    on color and texture, which helps the car recognize and track objects like pedestrians
    and other vehicles.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithm**: Mean Shift Clustering'
  prefs: []
  type: TYPE_NORMAL
- en: Mean Shift clustering is chosen because it is a non-parametric, density-based
    algorithm that can automatically adapt to the underlying structure and scale of
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: This makes it particularly suitable for image segmentation tasks, where the
    number of clusters or regions may not be known in advance, and the shapes of the
    regions can be complex and irregular.
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluation metrics**: Fowlkes-Mallows Score (FMS)'
  prefs: []
  type: TYPE_NORMAL
- en: The Fowlkes-Mallows Score is chosen in this case because it measures the similarity
    between two clusterings, typically comparing the algorithm's output to a ground-truth
    clustering.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of self-driving cars, the FMS can be used to assess how well
    the Mean Shift clustering algorithm segments the images compared to human-labeled
    segmentations.
  prefs: []
  type: TYPE_NORMAL
- en: News Recommendation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Clustering with scikit-learn: A Tutorial on Unsupervised Learning](../Images/0b50b4b60373daaae7dd97f598d463d8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-life example**: An online news platform wants to group articles into
    topics to improve content recommendations for its users. They use Gaussian Mixture
    Models to cluster articles based on features extracted from their texts, such
    as word frequency and term co-occurrence. By identifying distinct topics, the
    platform can recommend articles more relevant to a user''s interests.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithm**: Gaussian Mixture Model (GMM) clustering'
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian Mixture Models are chosen because they are a probabilistic, generative
    approach that can model complex, overlapping clusters. This is particularly useful
    for text data, where articles may belong to multiple topics or have shared features.
    GMM can capture these nuances and provide a soft clustering, assigning each article
    a probability of belonging to each topic.
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluation metrics**: Silhouette Coefficient'
  prefs: []
  type: TYPE_NORMAL
- en: The Silhouette Coefficient is chosen because it measures the compactness and
    separation of the clusters, helping assess the quality of the topic assignments.
  prefs: []
  type: TYPE_NORMAL
- en: A higher Silhouette Coefficient indicates that the articles within a topic are
    more similar to each other and distinct from other topics, which is important
    for accurate content recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to know more about Unsupervised algorithms, here you can collect
    more information on “[Unsupervised Learning Algorithms](https://www.stratascratch.com/blog/overview-of-machine-learning-algorithms-unsupervised-learning/?utm_source=blog&utm_medium=click&utm_campaign=kdn+clustering+unsupervised+learning)”.
    Also, check out “[Supervised vs Unsupervised Learning](https://www.stratascratch.com/blog/supervised-vs-unsupervised-learning/?utm_source=blog&utm_medium=click&utm_campaign=kdn+clustering+unsupervised+learning)”
    the two approaches that we should know in the world of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, clustering is an essential unsupervised learning technique used
    to find similarities or patterns in data without prior knowledge of class labels.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed different clustering algorithms, including K-Means, Mean Shift,
    DBScan, Gaussian Mixture, and Hierarchical Clustering, along with their use cases
    and real-life applications.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we explored various evaluation metrics, including the Silhouette
    Coefficient, Calinski-Harabasz Index, and Davies-Bouldin Index, which help us
    assess the quality of clustering results.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned how to develop multiple clustering algorithms simultaneously
    using scikit-learn and evaluated them using the metrics we had already discovered.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we discussed some popular applications that utilized clustering algorithms
    to solve real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: If you still have questions, here is an article explaining [Clustering and its
    algorithms](https://www.stratascratch.com/blog/machine-learning-algorithms-explained-clustering/?utm_source=blog&utm_medium=click&utm_campaign=kdn+clustering+unsupervised+learning).
  prefs: []
  type: TYPE_NORMAL
- en: Clustering has a wide range of applications, from customer segmentation in marketing
    to image recognition in computer vision, and it is an essential tool for discovering
    hidden patterns and insights in data.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Nate Rosidi](https://www.stratascratch.com)** is a data scientist and in
    product strategy. He''s also an adjunct professor teaching analytics, and is the
    founder of [StrataScratch](https://www.stratascratch.com/), a platform helping
    data scientists prepare for their interviews with real interview questions from
    top companies. Connect with him on [Twitter: StrataScratch](https://twitter.com/StrataScratch)
    or [LinkedIn](https://www.linkedin.com/in/nathanrosidi/).'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Hands-On with Unsupervised Learning: K-Means Clustering](https://www.kdnuggets.com/handson-with-unsupervised-learning-kmeans-clustering)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Clustering Unleashed: Understanding K-Means Clustering](https://www.kdnuggets.com/2023/07/clustering-unleashed-understanding-kmeans-clustering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unsupervised Disentangled Representation Learning in Class…](https://www.kdnuggets.com/2023/01/unsupervised-disentangled-representation-learning-class-imbalanced-dataset-elastic-infogan.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Exploring Unsupervised Learning Metrics](https://www.kdnuggets.com/2023/04/exploring-unsupervised-learning-metrics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unveiling Unsupervised Learning](https://www.kdnuggets.com/unveiling-unsupervised-learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[DBSCAN Clustering Algorithm in Machine Learning](https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
