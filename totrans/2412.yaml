- en: How Activation Functions Work in Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的激活函数如何工作
- en: 原文：[https://www.kdnuggets.com/2022/06/activation-functions-work-deep-learning.html](https://www.kdnuggets.com/2022/06/activation-functions-work-deep-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/06/activation-functions-work-deep-learning.html](https://www.kdnuggets.com/2022/06/activation-functions-work-deep-learning.html)
- en: 'Let''s start with a definition of **activation function**:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从**激活函数**的定义开始：
- en: '* * *'
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '* * *'
- en: ''
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_BQ
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前3个课程推荐
- en: ''
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: ''
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: ''
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行 IT'
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '* * *'
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**“In artificial neural networks, each neuron forms a weighted sum of its inputs
    and passes the resulting scalar value through a function referred to as an activation
    function.”**'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**“在人工神经网络中，每个神经元对其输入形成加权总和，并通过被称为激活函数的函数传递结果的标量值。”**'
- en: —[Definition from Wikipedia](https://en.wikipedia.org/wiki/Activation_function)
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —[维基百科定义](https://en.wikipedia.org/wiki/Activation_function)
- en: Sounds a little complicated? Don’t worry! After reading this article, you will
    have a better understanding of activation functions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来有点复杂？别担心！阅读本文后，你将对激活函数有更好的理解。
- en: '![How Activation Functions Work in Deep Learning](../Images/8d4304aeeb1d39b7ac754f055981f1e6.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习中的激活函数如何工作](../Images/8d4304aeeb1d39b7ac754f055981f1e6.png)'
- en: In humans, our brain receives input from the outside world, performs processing
    on the neuron receiving input and activates the neuron tail to generate required
    decisions. Similarly, in neural networks, we provide input as images, sounds,
    numbers, etc., and processing is performed on the artificial neuron, with an algorithm
    activating the correct final neuron layer to generate results.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人类而言，我们的大脑接收来自外部世界的信息，对接收输入的神经元进行处理，并激活神经元尾部以生成所需的决策。同样，在神经网络中，我们提供图像、声音、数字等作为输入，对人工神经元进行处理，通过算法激活正确的最终神经元层以生成结果。
- en: Why do we need activation functions?
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们为什么需要激活函数？
- en: An activation function determines if a neuron should be **activated or not activated**.
    This implies that it will use some simple mathematical operations to determine
    if the neuron’s input to the network is relevant or not relevant in the prediction
    process.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数决定了一个神经元是否应该**激活或不激活**。这意味着它将使用一些简单的数学运算来确定神经元对网络的输入在预测过程中是否相关。
- en: The ability to introduce **non-linearity** to an artificial neural network and
    generate output from a collection of input values fed to a layer is the purpose
    of the activation function.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数的目的是向人工神经网络引入**非线性**，并从输入值集合中生成输出。
- en: Types of Activation functions
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数的类型
- en: 'Activation functions can be divided into three types:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数可以分为三种类型：
- en: Linear Activation Function
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 线性激活函数
- en: Binary Step Function
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 二值阶跃函数
- en: Non-linear Activation Functions
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 非线性激活函数
- en: Linear Activation Function
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性激活函数
- en: The linear activation function, often called the **identity activation function**,
    is proportional to the input. The range of the linear activation function will
    be (-∞ to ∞). The linear activation function simply adds up the weighted total
    of the inputs and returns the result.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 线性激活函数，通常称为**恒等激活函数**，与输入成正比。线性激活函数的范围为（-∞到∞）。线性激活函数只是将输入的加权总和相加并返回结果。
- en: '![How Activation Functions Work in Deep Learning](../Images/15a61eb4276c282836fc2e706c9442b6.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习中的激活函数如何工作](../Images/15a61eb4276c282836fc2e706c9442b6.png)'
- en: Linear Activation Function — Graph
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 线性激活函数——图示
- en: 'Mathematically, it can be represented as:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，它可以表示为：
- en: '![How Activation Functions Work in Deep Learning](../Images/151bf2885a57ef72a7ac4c54ba18e9c8.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习中的激活函数如何工作](../Images/151bf2885a57ef72a7ac4c54ba18e9c8.png)'
- en: Linear Activation Function — Equation
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 线性激活函数 — 方程
- en: '**Pros and Cons**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**优缺点**'
- en: It is not a binary activation because the linear activation function only delivers
    a range of activations. We can surely connect a few neurons together, and if there
    are multiple activations, we can calculate the max (or soft max) based on that.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这不是一个二元激活函数，因为线性激活函数仅提供一系列激活值。我们可以将几个神经元连接在一起，如果有多个激活值，我们可以基于此计算最大值（或软最大值）。
- en: The derivative of this activation function is a constant. That is to say, the
    gradient is unrelated to the x (input).
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该激活函数的导数是一个常数。也就是说，梯度与x（输入）无关。
- en: Binary Step Activation Function
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 二元阶跃激活函数
- en: A **threshold value** determines whether a neuron should be activated or not
    activated in a binary step activation function.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一个**阈值**决定了在二元阶跃激活函数中神经元是否应该被激活。
- en: The activation function compares the input value to a threshold value. If the
    input value is greater than the threshold value, the neuron is activated. It’s
    disabled if the input value is less than the threshold value, which means its
    output isn’t sent on to the next or hidden layer.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数将输入值与阈值进行比较。如果输入值大于阈值，则激活神经元。如果输入值小于阈值，则禁用神经元，这意味着其输出不会传递到下一层或隐藏层。
- en: '![How Activation Functions Work in Deep Learning](../Images/10e4ae23fa8708e44ca18e6a0e10b212.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![激活函数在深度学习中的工作原理](../Images/10e4ae23fa8708e44ca18e6a0e10b212.png)'
- en: Binary Step Function — Graph
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 二元阶跃函数 — 图
- en: 'Mathematically, the binary activation function can be represented as:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，二元激活函数可以表示为：
- en: '![How Activation Functions Work in Deep Learning](../Images/47bd118991d87fcd6c5dfe70168733df.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![激活函数在深度学习中的工作原理](../Images/47bd118991d87fcd6c5dfe70168733df.png)'
- en: Binary Step Activation Function — Equation
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 二元阶跃激活函数 — 方程
- en: '**Pros and Cons**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**优缺点**'
- en: It cannot provide multi-value outputs — for example, it cannot be used for multi-class
    classification problems.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不能提供多值输出——例如，它不能用于多类分类问题。
- en: The step function’s gradient is zero, which makes the back propagation procedure
    difficult.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步进函数的梯度为零，这使得反向传播过程变得困难。
- en: Non-linear Activation Functions
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非线性激活函数
- en: The non-linear activation functions are the most-used activation functions.
    They make it uncomplicated for an artificial neural network model to adapt to
    a variety of data and to differentiate between the outputs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性激活函数是最常用的激活函数。它们使得人工神经网络模型能够适应各种数据并区分输出。
- en: Non-linear activation functions allow the stacking of multiple layers of neurons,
    as the output would now be a non-linear combination of input passed through multiple
    layers. Any output can be represented as a functional computation output in a
    neural network.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性激活函数允许堆叠多个神经元层，因为输出将是通过多个层的输入的非线性组合。任何输出都可以表示为神经网络中的功能计算输出。
- en: These activation functions are mainly divided basis on their range and curves.
    The remainder of this article will outline the major non-linear activiation functions
    used in neural networks.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些激活函数主要根据其范围和曲线进行分类。本文余下部分将概述神经网络中使用的主要非线性激活函数。
- en: 1\. Sigmoid
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. Sigmoid
- en: 'Sigmoid accepts a number as input and returns a number between 0 and 1\. It’s
    simple to use and has all the desirable qualities of activation functions: nonlinearity,
    continuous differentiation, monotonicity, and a set output range.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 接受一个数字作为输入，并返回一个介于 0 和 1 之间的数字。它简单易用，具备激活函数的所有理想特性：非线性、连续可微性、单调性和固定输出范围。
- en: This is mainly used in **binary classification problems.** This sigmoid function
    gives the probability of an existence of a particular class.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这主要用于**二元分类问题**。这个Sigmoid函数给出了特定类别存在的概率。
- en: '![How Activation Functions Work in Deep Learning](../Images/1dd0239de89470cbb6c12ddd05c294c2.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![激活函数在深度学习中的工作原理](../Images/1dd0239de89470cbb6c12ddd05c294c2.png)'
- en: Sigmoid Activation Function — Graph
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 激活函数 — 图
- en: 'Mathematically, it can be represented as:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，它可以表示为：
- en: '![How Activation Functions Work in Deep Learning](../Images/a354aae8799bdd4770a600624273cb60.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![激活函数在深度学习中的工作原理](../Images/a354aae8799bdd4770a600624273cb60.png)'
- en: Sigmoid Activation Function — Equation
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 激活函数 — 方程
- en: '**Pros and Cons**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**优缺点**'
- en: It is non-linear in nature. Combinations of this function are also non-linear,
    and it will give an analogue activation, unlike binary step activation function.
    It has a smooth gradient too, and It’s good for a classifier type problem.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它本质上是非线性的。该函数的组合也具有非线性特性，并且它将提供类比激活，不像二进制步进激活函数。它也有平滑的梯度，对于分类问题来说效果很好。
- en: The output of the activation function is always going to be in the range (0,1)
    compared to (-∞, ∞) of linear activation function. As a result, we’ve defined
    a range for our activations.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数的输出始终会在 (0,1) 范围内，而线性激活函数的范围是 (-∞, ∞)。因此，我们为激活定义了一个范围。
- en: Sigmoid function gives rise to a problem of **“Vanishing gradients”** and Sigmoids
    saturate and kill gradients.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid 函数引发了**“梯度消失”**的问题，Sigmoid 会饱和并杀死梯度。
- en: Its output **isn’t zero centred**, and it makes the gradient updates go too
    far in different directions. The output value is between zero and one, so it makes
    optimization harder.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的输出**不是零中心的**，这会导致梯度更新在不同方向上过度移动。输出值在零和一之间，使得优化变得更加困难。
- en: The network either refuses to learn more or is extremely slow.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络要么拒绝学习更多内容，要么变得非常慢。
- en: 2.TanH (Hyperbolic Tangent)
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. TanH（双曲正切）
- en: TanH compress a real-valued number to the range **[-1, 1]**. It’s non-linear,
    But it’s different from Sigmoid,and its output is **zero-centered**. The main
    advantage of this is that the negative inputs will be mapped strongly to the negative
    and zero inputs will be mapped to almost zero in the graph of TanH.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: TanH 将一个实值数压缩到范围**[-1, 1]**。它是非线性的，但与 Sigmoid 不同，其输出是**零中心的**。其主要优点在于，负输入将被强烈映射到负值，而零输入在
    TanH 图中将被映射到接近零的位置。
- en: '![How Activation Functions Work in Deep Learning](../Images/95bba17f69f7a53fd4b21393d23585e5.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习中的激活函数如何工作](../Images/95bba17f69f7a53fd4b21393d23585e5.png)'
- en: TanH Activation Function — Graph
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: TanH 激活函数 — 图
- en: 'Mathematically, TanH function can be represented as:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，TanH 函数可以表示为：
- en: '![How Activation Functions Work in Deep Learning](../Images/fb603f8d328bf76b6ef20935478456c5.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习中的激活函数如何工作](../Images/fb603f8d328bf76b6ef20935478456c5.png)'
- en: TanH Activation Function — Equation
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: TanH 激活函数 — 方程
- en: '**Pros and Cons**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点与缺点**'
- en: TanH also has the vanishing gradient problem, but the gradient is stronger for
    TanH than sigmoid (derivatives are steeper).
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TanH 也存在梯度消失问题，但 TanH 的梯度比 sigmoid 更强（导数更陡）。
- en: TanH is zero-centered, and gradients do not have to move in a specific direction.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TanH 是零中心的，梯度不必朝着特定方向移动。
- en: 3.ReLU (Rectified Linear Unit)
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. ReLU（修正线性单元）
- en: ReLU stands for Rectified Linear Unit and is one of the most commonly used activation
    function in the applications. It’s solved the problem of vanishing gradient because
    the maximum value of the gradient of ReLU function is one. It also solved the
    problem of saturating neuron, since the slope is never zero for ReLU function.
    The range of ReLU is between **0 and infinity.**
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 代表修正线性单元，是应用中最常用的激活函数之一。它解决了梯度消失的问题，因为 ReLU 函数的梯度最大值为一。它也解决了神经元饱和的问题，因为
    ReLU 函数的斜率从未为零。ReLU 的范围在**0和无限之间**。
- en: '![How Activation Functions Work in Deep Learning](../Images/0369c70833265a0315e80122dfe52e5f.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习中的激活函数如何工作](../Images/0369c70833265a0315e80122dfe52e5f.png)'
- en: ReLU Activation Function — Graph
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 激活函数 — 图
- en: 'Mathematically, it can be represented as:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，它可以表示为：
- en: '![How Activation Functions Work in Deep Learning](../Images/0e329f7594f219053f638c84a2d05baa.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习中的激活函数如何工作](../Images/0e329f7594f219053f638c84a2d05baa.png)'
- en: ReLU Activation Function — Equation
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 激活函数 — 方程
- en: '**Pros and Cons**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点与缺点**'
- en: Since only a certain number of neurons are activated, the ReLU function is far
    more computationally efficient when compared to the sigmoid and TanH functions.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于只有一定数量的神经元被激活，ReLU 函数在计算上比 sigmoid 和 TanH 函数更高效。
- en: ReLU accelerates the convergence of gradient descent towards the global minimum
    of the loss function due to its linear, non-saturating property.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于其线性、非饱和的特性，ReLU 加速了梯度下降向损失函数全局最小值的收敛。
- en: One of its limitations is that **it should only be used within hidden layers**
    of an artificial neural network model.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其一个局限性是**它应该仅在人工神经网络模型的隐藏层中使用**。
- en: Some gradients can be fragile during training.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，一些梯度可能会很脆弱。
- en: In other words, For activations in the region (x<0) of ReLu, the gradient will
    be 0 because of which the weights will not get adjusted during descent. That means,
    those neurons, which go into that state will stop responding to variations in
    input (simply because the gradient is 0, nothing changes.) This is called the
    **dying ReLu problem**.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 换句话说，对于 ReLU 的 (x<0) 区域的激活，梯度将为 0，因此权重在下降过程中不会调整。这意味着那些进入该状态的神经元将停止响应输入的变化（仅仅因为梯度为
    0，没有变化）。这就是**死 ReLU 问题**。
- en: 4.Leaky ReLU
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. Leaky ReLU
- en: Leaky ReLU is an upgraded version of the ReLU activation function to solve the
    dying ReLU problem, as it has a small positive slope in the negative area. But,
    the consistency of the benefit across tasks is presently ambiguous.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU 是 ReLU 激活函数的升级版，用于解决 ReLU 死亡问题，因为它在负区域有一个小的正斜率。但是，目前不同任务上的好处一致性尚不明确。
- en: '![How Activation Functions Work in Deep Learning](../Images/d9ad66067d83564315ce55cf1946de49.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习中激活函数的工作原理](../Images/d9ad66067d83564315ce55cf1946de49.png)'
- en: Leaky ReLU Activation Function — Graph
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU 激活函数 — 图
- en: Mathematically, it can be represented as,
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上表示为，
- en: '![How Activation Functions Work in Deep Learning](../Images/ad8ffedd0c6939c1bc789804bbfa5910.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习中激活函数的工作原理](../Images/ad8ffedd0c6939c1bc789804bbfa5910.png)'
- en: Leaky ReLU Activation Function — Equation
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU 激活函数 — 方程
- en: '**Pros and Cons**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**优缺点**'
- en: The advantages of Leaky ReLU are the same as that of ReLU, in addition to the
    fact that it does enable back propagation, even for negative input values.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leaky ReLU 的优点与 ReLU 相同，此外它还允许反向传播，即使对于负输入值也是如此。
- en: Making minor modification of negative input values, the gradient of the left
    side of the graph comes out to be a real (non-zero) value. As a result, there
    would be no more dead neurons in that area.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过对负输入值进行微小修改，图形左侧的梯度将变成真实（非零）值。因此，该区域内不会再有死神经元。
- en: The predictions may not be steady for negative input values.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于负输入值，预测可能不稳定。
- en: 5.ELU (Exponential Linear Units)
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5. ELU（指数线性单元）
- en: ELU is also one of the variations of ReLU which also solves the dead ReLU problem.
    ELU, just like leaky ReLU also considers negative values by introducing a new
    alpha parameter and multiplying it will another equation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ELU 也是 ReLU 的一种变体，它也解决了死 ReLU 问题。ELU 就像 Leaky ReLU 一样，通过引入一个新的 alpha 参数并与另一个方程相乘来考虑负值。
- en: ELU is slightly more computationally expensive than leaky ReLU, and it’s very
    similar to ReLU except negative inputs. They are both in identity function shape
    for positive inputs.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ELU 在计算上比 Leaky ReLU 略贵，而且它与 ReLU 非常相似，只是在负输入值方面有所不同。对于正输入值，它们的形状都是单位函数。
- en: '![How Activation Functions Work in Deep Learning](../Images/b2f9279c8d4987ceab71751a00d71528.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习中激活函数的工作原理](../Images/b2f9279c8d4987ceab71751a00d71528.png)'
- en: ELU Activation Function-Graph
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ELU 激活函数-图
- en: 'Mathematically, it can be represented as:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上表示为：
- en: '![How Activation Functions Work in Deep Learning](../Images/a71669c3b75ad8457e02a84b81f41dfc.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习中激活函数的工作原理](../Images/a71669c3b75ad8457e02a84b81f41dfc.png)'
- en: ELU Activation Function — Equation
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ELU 激活函数 — 方程
- en: '**Pros and Cons**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**优缺点**'
- en: ELU is a strong alternative to ReLU. Different from the ReLU, ELU can produce
    negative outputs.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ELU 是 ReLU 的一个强有力的替代方案。与 ReLU 不同，ELU 可以产生负输出。
- en: Exponential operations are there in ELU, So it increases the computational time.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ELU 中有指数运算，因此增加了计算时间。
- en: No learning about the ‘a’ value takes place, and exploding gradient problem.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不涉及对 'a' 值的学习，并且存在梯度爆炸问题。
- en: 6\. Softmax
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6. Softmax
- en: A combination of many sigmoids is referred to as the Softmax function. It determines
    relative probability. Similar to the sigmoid activation function, the Softmax
    function returns the probability of each class/labels. **In multi-class classification,
    softmax activation function is most commonly** **used for the last layer of the
    neural network.**
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 多个 sigmoid 的组合称为 Softmax 函数。它确定相对概率。类似于 sigmoid 激活函数，Softmax 函数返回每个类别/标签的概率。**在多分类问题中，softmax
    激活函数最常用于神经网络的最后一层。**
- en: The softmax function gives the probability of the current class with respect
    to others. This means that it also considers the possibility of other classes
    too.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 函数给出了当前类别相对于其他类别的概率。这意味着它还考虑了其他类别的可能性。
- en: '![How Activation Functions Work in Deep Learning](../Images/ca087644f0ff080de42f7998ff705f24.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习中激活函数的工作原理](../Images/ca087644f0ff080de42f7998ff705f24.png)'
- en: Softmax Activation Function — Graph
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 激活函数 — 图
- en: 'Mathematically, it can be represented as:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度看，它可以表示为：
- en: '![How Activation Functions Work in Deep Learning](../Images/778fe4f04a08fdf86f75f65b951e180e.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习中激活函数的工作原理](../Images/778fe4f04a08fdf86f75f65b951e180e.png)'
- en: Softmax Activation Function — Equation
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 激活函数 — 方程
- en: '**Pros and Cons**'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**优缺点**'
- en: It mimics the one encoded label better than the absolute values.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它比绝对值更好地模拟了编码标签。
- en: We would lose information if we used absolute (modulus) values, but the exponential
    takes care of this on its own.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们使用绝对值（模值），信息会丢失，但指数函数会自动处理这一点。
- en: The softmax function **should be used for multi-label classification** and regression
    task as well.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**softmax 函数也应用于多标签分类**和回归任务。'
- en: 7\. Swish
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7\. Swish
- en: Swish allows for the propagation of a few numbers of negative weights, whereas
    ReLU sets all non-positive weights to zero. This is a crucial property that determines
    the success of non-monotonic smooth activation functions, such as Swish’s, in
    progressively deep neural networks.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Swish 允许少量负权重的传播，而 ReLU 将所有非正权重设置为零。这是决定非单调平滑激活函数（如 Swish）在逐层深度神经网络中成功的关键特性。
- en: It’s a self-gated activation function created by Google researchers.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 它是由谷歌研究人员创建的自门控激活函数。
- en: '![How Activation Functions Work in Deep Learning](../Images/10c2ae768b3b654d151f1fdc1293d310.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习中激活函数的工作原理](../Images/10c2ae768b3b654d151f1fdc1293d310.png)'
- en: Swish Activation Function — Graph
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Swish 激活函数 — 图
- en: 'Mathematically, it can be represented as:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度看，它可以表示为：
- en: '![How Activation Functions Work in Deep Learning](../Images/4cf850d754d377b9c455429f5aef4209.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习中激活函数的工作原理](../Images/4cf850d754d377b9c455429f5aef4209.png)'
- en: Swish Activation Function — Equation
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Swish 激活函数 — 方程
- en: '**Pros and Cons**'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**优缺点**'
- en: Swish is a smooth activation function that means that it does not suddenly change
    direction like ReLU does near x equal to zero. Rather, it smoothly bends from
    0 towards values < 0 and then upwards again.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swish 是一个平滑的激活函数，这意味着它不像 ReLU 在 x 等于零附近那样突然改变方向。相反，它平滑地从 0 向小于 0 的值弯曲，然后再次向上。
- en: Non-positive values were zeroed out in ReLU activation function. Negative numbers,
    on the other hand, may be valuable for detecting patterns in the data. Because
    of the sparsity, large negative numbers are wiped out, resulting in a win-win
    situation.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 ReLU 激活函数中，非正值被置为零。而负数可能对检测数据中的模式有价值。由于稀疏性，大的负数被消除，从而形成双赢局面。
- en: The swish activation function being non-monotonous enhances the term of input
    data and weight to be learnt.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swish 激活函数由于非单调性，增强了输入数据和权重的学习。
- en: Slightly more computationally expensive and More problems with the algorithm
    will probably arise given time.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算上略显昂贵，并且随着时间的推移，算法可能会出现更多问题。
- en: Important Considerations
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重要考虑事项
- en: 'While choosing the proper activation function, the following problems and issues
    must be considered:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择合适的激活函数时，必须考虑以下问题和事项：
- en: '**Vanishing gradient** is a common problem encountered during neural network
    training. Like a sigmoid activation function, some activation functions have a
    small output range (0 to 1). So a huge change in the input of the sigmoid activation
    function will create a small modification in the output. Therefore, the derivative
    also becomes small. These activation functions are only used for shallow networks
    with only a few layers. When these activation functions are applied to a multi-layer
    network, the gradient may become too small for expected training.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度消失**是神经网络训练过程中常见的问题。像 sigmoid 激活函数一样，一些激活函数的输出范围很小（0 到 1）。因此，sigmoid 激活函数的输入发生巨大变化时，输出的修改很小。因此，导数也变得很小。这些激活函数仅用于只有几层的浅层网络。当这些激活函数应用于多层网络时，梯度可能变得过小，无法完成预期训练。'
- en: '**Exploding gradients** are situations in which massive incorrect gradients
    build during training, resulting in huge updates to neural network model weights.
    When there are exploding gradients, an unstable network might form, and training
    cannot be completed. Due to exploding gradients, the weights’ values can potentially
    grow to the point where they overflow, resulting in loss in **NaN** values.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度爆炸**是训练过程中产生大规模错误梯度的情况，导致神经网络模型权重的巨大更新。当发生梯度爆炸时，网络可能变得不稳定，训练无法完成。由于梯度爆炸，权重值可能增长到溢出的程度，从而导致**NaN**值的丢失。'
- en: Final Takeaways
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终总结
- en: All hidden layers generally use the same activation functions. **ReLU** activation
    function **should only** be used in the hidden layer for better results.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有隐藏层通常使用相同的激活函数。**ReLU** 激活函数 **应仅** 在隐藏层中使用，以获得更好的结果。
- en: Sigmoid and TanH activation functions **should not be utilized** in hidden layers
    due to the vanishing gradient, since they make the model more susceptible to problems
    during training.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于梯度消失问题，**不应在** 隐藏层中使用 sigmoid 和 TanH 激活函数，因为它们使模型在训练过程中更容易出现问题。
- en: Swish function is used in artificial neural networks having a depth **more than
    40 layers.**
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swish 函数用于深度 **超过 40 层** 的人工神经网络。
- en: Regression problems should use linear activation functions
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归问题应使用线性激活函数
- en: Binary classification problems should use the sigmoid activation function
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二分类问题应使用 sigmoid 激活函数
- en: Multiclass classification problems shold use the softmax activation function
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多分类问题应使用 softmax 激活函数
- en: Neural network architecture and their usable activation functions,
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络架构及其可用的激活函数，
- en: 'Convolutional Neural Network (CNN): ReLU activation function'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络 (CNN)：ReLU 激活函数
- en: 'Recurrent Neural Network (RNN): TanH or sigmoid activation functions'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络 (RNN)：TanH 或 sigmoid 激活函数
- en: You can find countless additional articles evaluating activation function comparisons.
    I suggest you get your hands dirty and practise well.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以找到无数的文章评估激活函数的比较。我建议你亲自动手实践。
- en: If you have any questions, feel free to ask me on [LinkedIn](https://www.linkedin.com/in/parthibanmarimuthu/)
    or [Twitter](https://twitter.com/parthibharathiv).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何问题，可以在 [LinkedIn](https://www.linkedin.com/in/parthibanmarimuthu/) 或 [Twitter](https://twitter.com/parthibharathiv)
    上随时问我。
- en: '**[Parthiban Marimuthu](https://www.linkedin.com/in/parthibanmarimuthu/)**
    ([@parthibharathiv](https://twitter.com/parthibharathiv)) lives in Chennai, India,
    and works at Spritle Software. Parthiban studies and transforms data science prototypes,
    designs machine learning systems, and researches and implements appropriate ML
    algorithms and tools.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Parthiban Marimuthu](https://www.linkedin.com/in/parthibanmarimuthu/)**
    ([@parthibharathiv](https://twitter.com/parthibharathiv)) 生活在印度钦奈，在 Spritle Software
    工作。Parthiban 研究并转化数据科学原型，设计机器学习系统，并研究和实现适当的 ML 算法和工具。'
- en: More On This Topic
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Unveiling Neural Magic: A Dive into Activation Functions](https://www.kdnuggets.com/unveiling-neural-magic-a-dive-into-activation-functions)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[揭开神经魔法的面纱：深入了解激活函数](https://www.kdnuggets.com/unveiling-neural-magic-a-dive-into-activation-functions)'
- en: '[KDnuggets™ News 22:n03, Jan 19: A Deep Look Into 13 Data…](https://www.kdnuggets.com/2022/n03.html)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets™ 新闻 22:n03，1月19日：深入了解 13 个数据…](https://www.kdnuggets.com/2022/n03.html)'
- en: '[KDnuggets News, August 3: 10 Most Used Tableau Functions • Is…](https://www.kdnuggets.com/2022/n31.html)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，8月3日：10 个最常用的 Tableau 函数 • 是…](https://www.kdnuggets.com/2022/n31.html)'
- en: '[How Does Logistic Regression Work?](https://www.kdnuggets.com/2022/07/logistic-regression-work.html)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[逻辑回归是如何工作的？](https://www.kdnuggets.com/2022/07/logistic-regression-work.html)'
- en: '[Learn How Different Data Visualizations Work](https://www.kdnuggets.com/2022/09/datacamp-learn-different-data-visualizations-work.html)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[了解不同的数据可视化如何工作](https://www.kdnuggets.com/2022/09/datacamp-learn-different-data-visualizations-work.html)'
- en: '[What are Large Language Models and How Do They Work?](https://www.kdnuggets.com/2023/05/large-language-models-work.html)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大型语言模型是什么？它们是如何工作的？](https://www.kdnuggets.com/2023/05/large-language-models-work.html)'
