- en: Schedule & Run ETLs with Jupysql and GitHub Actions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Jupysql和GitHub Actions安排和运行ETL
- en: 原文：[https://www.kdnuggets.com/2023/05/schedule-run-etls-jupysql-github-actions.html](https://www.kdnuggets.com/2023/05/schedule-run-etls-jupysql-github-actions.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/05/schedule-run-etls-jupysql-github-actions.html](https://www.kdnuggets.com/2023/05/schedule-run-etls-jupysql-github-actions.html)
- en: '![Schedule & Run ETLs with Jupysql and GitHub Actions](../Images/4f0d1e0ad6d0308517fba43087bd543c.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![使用Jupysql和GitHub Actions安排和运行ETL](../Images/4f0d1e0ad6d0308517fba43087bd543c.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'In this blog you''ll achieve:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本博客中你将实现：
- en: Have a basic understanding of ETLs and JupySQL
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 了解ETL和JupySQL的基本概念
- en: Use the public Penguins dataset and perform ETL.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用公共企鹅数据集并执行ETL。
- en: Schedule the ETL we've built on GitHub actions.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在GitHub Actions上安排我们构建的ETL。
- en: '* * *'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织IT'
- en: '* * *'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Introduction
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In this brief yet informative guide, we aim to provide you with a comprehensive
    understanding of the fundamental concepts of ETL (Extract, Transform, Load) and
    JupySQL, a flexible and versatile tool that allows for seamless SQL-based ETL
    from Jupyter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这份简短而信息丰富的指南中，我们旨在为你提供对ETL（提取、转换、加载）和JupySQL这一灵活且多功能工具的基本概念的全面理解，该工具允许从Jupyter中无缝地进行基于SQL的ETL操作。
- en: Our primary focus will be on demonstrating how to effectively execute ETLs through
    JupySQL, the popular and powerful Python library designed for SQL interaction,
    while also highlighting the benefits of automating the ETL process through scheduling
    a full example ETL notebook via GitHub actions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要关注点将是演示如何通过JupySQL这一流行且强大的Python库有效地执行ETL操作，同时强调通过GitHub Actions安排完整的ETL笔记本的自动化ETL过程的好处。
- en: But first, what is an ETL?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 但首先，什么是ETL？
- en: Now, let's dive into the details. `ETL` (Extract, Transform, Load) crucial process
    in data management that involves the extraction of data from various sources,
    the transformation of the extracted data into a usable format, and loading the
    transformed data into a target database or data warehouse. It is an essential
    process for data analysis, data science, data integration, and data migration,
    among other purposes. On the other hand, JupySQL is a widely-used Python library
    that simplifies the interaction with databases through the power of SQL queries.
    By using JupySQL, data scientists and analysts can easily execute SQL queries,
    manipulate data frames, and interact with databases from their Jupyter notebooks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解细节。`ETL`（提取、转换、加载）是数据管理中至关重要的过程，它涉及从各种来源提取数据，将提取的数据转换为可用格式，并将转换后的数据加载到目标数据库或数据仓库中。它是数据分析、数据科学、数据集成和数据迁移等多个目的的关键过程。另一方面，JupySQL是一个广泛使用的Python库，它通过SQL查询的强大功能简化了与数据库的交互。通过使用JupySQL，数据科学家和分析师可以轻松执行SQL查询，操作数据框，并在Jupyter笔记本中与数据库交互。
- en: Why ETLs are important?
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么ETL很重要？
- en: ETLs play a significant role in data analytics and business intelligence. They
    help businesses to collect data from various sources, including social media,
    web pages, sensors, and other internal and external systems. By doing this, businesses
    can obtain a holistic view of their operations, customers, and market trends.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ETL在数据分析和商业智能中扮演着重要角色。它们帮助企业从各种来源收集数据，包括社交媒体、网页、传感器以及其他内部和外部系统。通过这样做，企业可以获得其运营、客户和市场趋势的整体视图。
- en: After extracting data, ETLs transform it into a structured format, such as a
    relational database, which allows businesses to analyze and manipulate data easily.
    By transforming data, ETLs can clean, validate, and standardize it, making it
    easier to understand and analyze.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 提取数据后，ETL 将其转换为结构化格式，例如关系数据库，这使企业能够轻松分析和操作数据。通过转换数据，ETL 可以清理、验证和标准化数据，使其更易于理解和分析。
- en: Finally, ETLs load the data into a database or data warehouse, where businesses
    can access it easily. By doing this, ETLs enable businesses to access accurate
    and up-to-date information, allowing them to make informed decisions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，ETL 将数据加载到数据库或数据仓库中，企业可以轻松访问这些数据。通过这样做，ETL 使企业能够访问准确和最新的信息，从而做出明智的决策。
- en: What is JupySQL?
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是 JupySQL？
- en: '[JupySQL](https://github.com/ploomber/jupysql) is an extension for Jupyter
    Notebooks that allows you to interact with databases using SQL queries. It provides
    a convenient way to access databases and data warehouses directly from Jupyter
    Notebooks, allowing you to perform complex data manipulations and analyses.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[JupySQL](https://github.com/ploomber/jupysql) 是 Jupyter 笔记本的一个扩展，允许您使用 SQL
    查询与数据库进行交互。它提供了一种方便的方式，直接从 Jupyter 笔记本访问数据库和数据仓库，使您能够执行复杂的数据操作和分析。'
- en: JupySQL supports multiple database management systems, including SQLite, MySQL,
    PostgreSQL, DuckDB, Oracle, Snowflake and more (check out our integrations section
    on the left to learn more). You can connect to databases using standard connection
    strings or through the use of environment variables.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: JupySQL 支持多种数据库管理系统，包括 SQLite、MySQL、PostgreSQL、DuckDB、Oracle、Snowflake 等（查看左侧的集成部分了解更多信息）。您可以使用标准连接字符串或通过环境变量连接到数据库。
- en: Why JupySQL?
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么选择 JupySQL？
- en: JupySQL, a powerful tool, facilitates direct SQL query interaction with databases
    inside Jupyter notebooks. With a view to carrying out efficient and accurate data
    extraction and transformation processes, there are several critical factors to
    consider when performing ETLs via JupySQL. JupySQL provides users with the necessary
    tools to interact with data sources and perform data transformations with ease.
    To save valuable time and effort while guaranteeing consistency and reliability,
    automating the ETL process through scheduling a full ETL notebook via GitHub actions
    can be a game-changer. By utilizing JupySQL, users can achieve the best of both
    worlds, data interactivity (Jupyter) and ease of usage and SQL connectivity (JupySQL),
    thereby streamlining the data management process and allowing data scientists
    and analysts to concentrate on their core competencies - generating valuable insights
    and reports.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: JupySQL 是一个强大的工具，便于在 Jupyter 笔记本中直接使用 SQL 查询与数据库进行交互。为了高效和准确地执行数据提取和转换过程，在通过
    JupySQL 执行 ETL 时有几个关键因素需要考虑。JupySQL 为用户提供了与数据源交互并轻松进行数据转换的必要工具。为了节省宝贵的时间和精力，同时确保一致性和可靠性，通过
    GitHub Actions 调度完整的 ETL 笔记本来自动化 ETL 过程可能是一个改变游戏规则的选择。通过利用 JupySQL，用户可以实现数据交互性（Jupyter）和易用性及
    SQL 连接性（JupySQL）的最佳结合，从而简化数据管理过程，使数据科学家和分析师能够专注于他们的核心能力——生成有价值的见解和报告。
- en: Getting started with JupySQL
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用 JupySQL
- en: 'To use JupySQL, you need to install it using pip. You can run the following
    command:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 JupySQL，您需要使用 pip 安装它。您可以运行以下命令：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once installed, you can load the extension in Jupyter notebooks using the following
    command:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 安装后，您可以使用以下命令在 Jupyter 笔记本中加载扩展：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After loading the extension, you can connect to a database using the following
    command:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 加载扩展后，您可以使用以下命令连接到数据库：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For example, to connect to a local DuckDB database, you can use the following
    command:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要连接到本地 DuckDB 数据库，您可以使用以下命令：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Performing ETL using JupySQL
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 JupySQL 执行 ETL
- en: 'To perform ETLs using JupySQL, we will follow the standard ETL process, which
    involves the following steps:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 JupySQL 执行 ETL 时，我们将遵循标准 ETL 流程，其中包括以下步骤：
- en: Extract data
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取数据
- en: Transform data
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换数据
- en: Load data
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据
- en: Extract data
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取数据
- en: Extract data
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取数据
- en: 'To extract data using JupySQL, we need to connect to the source database and
    execute a query to retrieve the data. For example, to extract data from a MySQL
    database, we can use the following command:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 JupySQL 提取数据，我们需要连接到源数据库并执行查询以检索数据。例如，要从 MySQL 数据库中提取数据，我们可以使用以下命令：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This command connects to the MySQL database using the specified connection string
    and retrieves all the data from the "mytable" table. The data is stored in the
    "data" variable as a Pandas DataFrame.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令使用指定的连接字符串连接到 MySQL 数据库，并从“mytable”表中检索所有数据。数据存储在名为“data”的变量中，作为 Pandas DataFrame。
- en: '**Note**: We can also use `%%sql df <<` to save the data into the df variable'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：我们还可以使用 `%%sql df <<` 将数据保存到 df 变量中'
- en: 'Since we''ll be running locally via DuckDB we can simply Extract a public dataset
    and start working immediately. We''re going to get our sample dataset (we will
    work with the Penguins datasets via a csv file):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将通过 DuckDB 在本地运行，我们可以直接提取一个公共数据集并立即开始工作。我们将获取我们的样本数据集（我们将通过 csv 文件处理企鹅数据集）：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And we can get a sample of the data to check we''re connected and we can query
    the data:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以获取数据的样本，以检查我们是否已连接并且可以查询数据：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Transform data
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换数据
- en: 'After extracting data, it''s often necessary to transform it into a format
    that''s more suitable for analysis. This step may include cleaning data, filtering
    data, aggregating data, and combining data from multiple sources. Here are some
    common data transformation techniques:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 提取数据后，通常需要将其转换为更适合分析的格式。此步骤可能包括数据清理、数据过滤、数据聚合和合并来自多个来源的数据。以下是一些常见的数据转换技术：
- en: '**Cleaning data**: Data cleaning involves removing or fixing errors, inconsistencies,
    or missing values in the data. For example, you might remove rows with missing
    values, replace missing values with the mean or median value, or fix typos or
    formatting errors.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清理数据**：数据清理涉及删除或修复数据中的错误、不一致性或缺失值。例如，你可能会删除缺失值的行，将缺失值替换为均值或中位数，或修正错别字或格式错误。'
- en: '**Filtering data**: Data filtering involves selecting a subset of data that
    meets specific criteria. For example, you might filter data to only include records
    from a specific date range, or records that meet a certain threshold.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过滤数据**：数据过滤涉及选择符合特定标准的数据子集。例如，你可以过滤数据，仅包含特定日期范围的记录，或符合某一阈值的记录。'
- en: '**Aggregating data**: Data aggregation involves summarizing data by calculating
    statistics such as the sum, mean, median, or count of a particular variable. For
    example, you might aggregate sales data by month or by product category.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合数据**：数据聚合涉及通过计算统计数据（如某个变量的总和、均值、中位数或计数）来总结数据。例如，你可以按月或按产品类别聚合销售数据。'
- en: '**Combining data**: Data combination involves merging data from multiple sources
    to create a single dataset. For example, you might combine data from different
    tables in a relational database, or combine data from different files.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合并数据**：数据合并涉及将来自多个来源的数据合并为一个数据集。例如，你可以合并关系数据库中不同表的数据，或合并不同文件中的数据。'
- en: In JupySQL, you can use Pandas DataFrame methods to perform data transformations
    or native SQL. For example, you can use the rename method to rename columns, the
    dropna method to remove missing values, and the astype method to convert data
    types. I'll demonstrate how to do it either with pandas or SQL.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在 JupySQL 中，你可以使用 Pandas DataFrame 方法进行数据转换，也可以使用原生 SQL。例如，你可以使用 rename 方法重命名列，使用
    dropna 方法删除缺失值，使用 astype 方法转换数据类型。我将演示如何使用 pandas 或 SQL 来完成这些操作。
- en: 'Note: You can use either `%sql` or `%%sql`, check out the difference between
    the two [here](https://jupysql.ploomber.io/en/latest/community/developer-guide.html?highlight=%25sql%20vs%20%25%25sql#magics-e-g-sql-sql-etc)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意：你可以使用 `%sql` 或 `%%sql`，了解它们之间的区别，请查看 [这里](https://jupysql.ploomber.io/en/latest/community/developer-guide.html?highlight=%25sql%20vs%20%25%25sql#magics-e-g-sql-sql-etc)
- en: 'Here''s an example of how to use Pandas and the JupySQL alternatives to transform
    data:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个如何使用 Pandas 和 JupySQL 替代方案进行数据转换的示例：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In our example we''ll use simple transformations, in a similar manner to the
    above code. We''ll clean our data from NAs and will split a column (species) into
    3 individual columns (named for each species):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们将使用简单的转换方式，类似于上述代码。我们将清理数据中的 NA，并将一列（species）拆分成 3 列（分别命名为每种物种）：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Load data
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据
- en: 'After transforming the data, we need to load it into a destination database
    or data warehouse. We can use `ipython-sql` to connect to the destination database
    and execute SQL queries to load the data. For example, to load data into a PostgreSQL
    database, we can use the following command:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 转换数据后，我们需要将数据加载到目标数据库或数据仓库中。我们可以使用 `ipython-sql` 连接到目标数据库并执行 SQL 查询以加载数据。例如，要将数据加载到
    PostgreSQL 数据库中，我们可以使用以下命令：
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This command connects to the PostgreSQL database using the specified connection
    string, drops the "mytable" table if it exists, creates a new table with the specified
    columns and data types, and loads the data from the CSV file.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令使用指定的连接字符串连接到 PostgreSQL 数据库，删除“mytable”表（如果存在），创建一个具有指定列和数据类型的新表，并从 CSV
    文件中加载数据。
- en: Since our use case is using DuckDB locally we can simply save the newly created
    `transformed_df` into a csv file, but we can also use the snipped above to save
    it into our DB or DWH depending on our use case.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的用例是本地使用 DuckDB，我们可以简单地将新创建的 `transformed_df` 保存为 CSV 文件，但我们也可以使用上面的代码片段根据我们的用例将其保存到数据库或数据仓库中。
- en: 'Run the following step to save the new data as a CSV file:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下步骤以将新数据保存为 CSV 文件：
- en: '[PRE17]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We can see a new file called `transformed_data.csv` was created for us. In the
    next step we'll see how we can automate this process and consume the final file
    via GitHub.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到一个名为 `transformed_data.csv` 的新文件被创建了。在下一步中，我们将看到如何自动化这个过程并通过 GitHub 消耗最终文件。
- en: Scheduling on GitHub actions
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GitHub actions 上的调度
- en: The last step in our process is executing the complete notebook via GitHub actions.
    To do that we can use `ploomber-engine` which lets you schedule notebooks, along
    with other notebook capabilities such as profiling, debugging etc. If needed we
    can pass external parameters to our notebook and make it a generic template.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们流程中的最后一步是通过 GitHub actions 执行完整的 notebook。为此，我们可以使用 `ploomber-engine`，它允许你调度
    notebooks，以及其他 notebook 功能，如分析、调试等。如果需要，我们可以向 notebook 传递外部参数，并将其制作成通用模板。
- en: 'Note: Our notebook file is loading a public dataset and saves it after ETL
    locally, we can easily change it to consume any dataset, and load it to S3, visualize
    the data as a dashboard and more.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意：我们的 notebook 文件正在加载一个公共数据集，并在本地 ETL 后保存，我们可以轻松地更改为使用任何数据集，并将其加载到 S3，作为仪表盘可视化数据等。
- en: For our example we can use this sample ci.yml file (this is what sets the github
    workflow in your repository), and put it in our repository, the final file should
    be located under `.github/workflows/ci.yml`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，我们可以使用这个示例 ci.yml 文件（这是设置 GitHub 工作流的文件），并将其放入我们的代码库中，最终文件应位于 `.github/workflows/ci.yml`
    下。
- en: 'Content of the `ci.yml` file:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`ci.yml` 文件的内容：'
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this example CI, I've also added a scheduled trigger, this job will run nightly
    at 4 am.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个 CI 示例中，我还添加了一个计划触发器，这个任务将每晚凌晨 4 点运行。
- en: Conclusion
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: ETLs are an essential process for data analytics and business intelligence.
    They help businesses to collect, transform, and load data from various sources,
    making it easier to analyze and make informed decisions. JupySQL is a powerful
    tool that allows you to interact with databases using SQL queries directly in
    Jupyter notebooks. Combined with Github actions we can create powerful workflows
    that can be scheduled and help us get the data to its final stage.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ETL 是数据分析和商业智能的重要过程。它们帮助企业从各种来源收集、转换和加载数据，使分析和做出明智决策变得更加容易。JupySQL 是一个强大的工具，允许你直接在
    Jupyter notebooks 中使用 SQL 查询与数据库交互。结合 GitHub actions，我们可以创建强大的工作流，这些工作流可以被调度并帮助我们将数据处理到最终阶段。
- en: By using JupySQL, you can perform ETLs easily and efficiently, allowing you
    to extract, transform, and load data in a structured format while Github actions
    allocate compute and set the environment.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 JupySQL，你可以轻松高效地执行 ETL，允许你以结构化格式提取、转换和加载数据，同时 GitHub actions 分配计算资源并设置环境。
- en: '**[Ido Michael](https://www.linkedin.com/in/ido-michael/)** co-founded Ploomber
    to help data scientists build faster. He''d been working at AWS leading data engineering/science
    teams. Single handedly he built 100’s of data pipelines during those customer
    engagements together with his team. Originally from Israel, he came to NY for
    his MS at Columbia University. He focused on building Ploomber after he constantly
    found that projects dedicated about 30% of their time just to refactor the dev
    work (prototype) into a production pipeline.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Ido Michael](https://www.linkedin.com/in/ido-michael/)** 共同创办了 Ploomber，旨在帮助数据科学家更快地构建。他曾在
    AWS 领导数据工程/科学团队。在这些客户互动中，他和团队一起单独构建了数百个数据管道。来自以色列的他来到纽约攻读哥伦比亚大学的硕士学位。他专注于构建 Ploomber，因为他发现项目通常将大约
    30% 的时间用于将开发工作（原型）重构为生产管道。'
- en: More On This Topic
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[GitHub Actions For Machine Learning Beginners](https://www.kdnuggets.com/github-actions-for-machine-learning-beginners)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GitHub Actions For Machine Learning Beginners](https://www.kdnuggets.com/github-actions-for-machine-learning-beginners)'
- en: '[Best Practices for Building ETLs for ML](https://www.kdnuggets.com/best-practices-for-building-etls-for-ml)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为 ML 构建 ETL 的最佳实践](https://www.kdnuggets.com/best-practices-for-building-etls-for-ml)'
- en: '[Getting Started with PyTest: Effortlessly Write and Run Tests in Python](https://www.kdnuggets.com/getting-started-with-pytest-effortlessly-write-and-run-tests-in-python)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[开始使用 PyTest：轻松编写和运行 Python 测试](https://www.kdnuggets.com/getting-started-with-pytest-effortlessly-write-and-run-tests-in-python)'
- en: '[Distribute and Run LLMs with llamafile in 5 Simple Steps](https://www.kdnuggets.com/distribute-and-run-llms-with-llamafile-in-5-simple-steps)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 llamafile 以 5 个简单步骤分发和运行 LLM](https://www.kdnuggets.com/distribute-and-run-llms-with-llamafile-in-5-simple-steps)'
- en: '[Learn How to Run Alpaca-LoRA on Your Device in Just a Few Steps](https://www.kdnuggets.com/2023/05/learn-run-alpacalora-device-steps.html)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习如何在您的设备上仅需几个步骤运行 Alpaca-LoRA](https://www.kdnuggets.com/2023/05/learn-run-alpacalora-device-steps.html)'
- en: '[Run an LLM Locally with LM Studio](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 LM Studio 在本地运行 LLM](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio)'
