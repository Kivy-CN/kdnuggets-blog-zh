- en: The Gentlest Introduction to Tensorflow – Part 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《最温和的 Tensorflow 入门 – 第二部分》
- en: 原文：[https://www.kdnuggets.com/2016/08/gentlest-introduction-tensorflow-part-2.html/2](https://www.kdnuggets.com/2016/08/gentlest-introduction-tensorflow-part-2.html/2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/08/gentlest-introduction-tensorflow-part-2.html/2](https://www.kdnuggets.com/2016/08/gentlest-introduction-tensorflow-part-2.html/2)
- en: Training Variation
  id: totrans-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练变化。
- en: '**Stochastic, Mini-batch, Batch**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机、小批量、批量**'
- en: 'In the training above, we feed a single datapoint at each epoch. This is known
    as *stochastic* gradient descent. We can feed a bunch of datapoints at each epoch,
    which is known as *mini-batch* gradient descent, or even feed all the datapoints
    at each epoch, known as *batch* gradient descent. See the graphical comparison
    below and note the 2 differences between the 3 diagrams:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述训练中，我们在每个时期喂入一个数据点。这被称为*随机*梯度下降。我们可以在每个时期喂入一批数据点，这被称为*小批量*梯度下降，甚至可以在每个时期喂入所有数据点，这被称为*批量*梯度下降。请参见下面的图形比较，并注意三张图之间的两个差异：
- en: The number of datapoints (upper-right of each diagram) fed to TF.Graph at each
    epoch
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个时期喂入 TF.Graph 的数据点数量（每个图的右上角）。
- en: The number of datapoints for the gradient descent optimizer to consider when
    tweaking W, b to reduce cost (bottom-right of each diagram)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降优化器在调整 W、b 以降低成本时需要考虑的数据点数量（每个图的右下角）。
- en: '![Image](../Images/b79b5679f1ceeb28278aaab148182ea2.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../Images/b79b5679f1ceeb28278aaab148182ea2.png)'
- en: '*Stochastic gradient descent*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机梯度下降*'
- en: '![Image](../Images/33557bec9cb92d4fdb6e45778da7e05d.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../Images/33557bec9cb92d4fdb6e45778da7e05d.png)'
- en: '*Mini-batch gradient descent*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*小批量梯度下降*'
- en: '![Image](../Images/427edda0188c06a578eea475cdd2f31c.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../Images/427edda0188c06a578eea475cdd2f31c.png)'
- en: '*Batch gradient descent*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*批量梯度下降*'
- en: 'The number of datapoints used at each epoch has 2 implications. With more datapoints:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 每个时期使用的数据点数量有两个影响。使用更多的数据点：
- en: Computational resource (subtractions, squares, and additions) needed to calculate
    the cost and perform gradient descent increases
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算资源（减法、平方和加法）用于计算成本和执行梯度下降的需求增加。
- en: Speed at which the model can learn and generalize increases
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型学习和泛化的速度增加。
- en: 'The pros and cons of doing stochastic, mini-batch, batch gradient descent can
    be summarized in the diagram below:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随机、小批量、批量梯度下降的优缺点可以在下面的图表中总结：
- en: '![Image](../Images/4dd4d20bd311d1735880a43acfa54df3.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../Images/4dd4d20bd311d1735880a43acfa54df3.png)'
- en: '*Pros and cons of stochastic, mini-batch & batch gradient descent*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机、小批量和批量梯度下降的优缺点*'
- en: 'To switch between stochastic/mini-batch/batch gradient descent, we just need
    to prepare the datapoints into different batch sizes before feeding them into
    the training step [D], i.e., use the snippet below for[C]:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要在随机/小批量/批量梯度下降之间切换，我们只需要在将数据点输入训练步骤 [D] 之前准备不同的批量大小，即使用下面的代码片段用于 [C]：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Learn Rate Variation**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习率变化**'
- en: Learn rate is how big an increment/decrement we want gradient descent to tweak
    W, b, once it decides whether to increment/decrement them. With a small learn
    rate, we will proceed slowly but surely towards minimal cost, but with a larger
    learn rate, we can reach the minimal cost faster, but at the risk of ‘overshooting’,
    and never finding it.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率是指我们希望梯度下降调整 W、b 时增减的幅度。使用较小的学习率，我们会缓慢而稳妥地接近最小成本，但使用较大的学习率，我们可以更快地达到最小成本，但存在“超调”的风险，从而无法找到最小值。
- en: To overcome this, many ML practitioners use a large learn rate initially (with
    the assumption that initial cost is far away from minimum), and then decrease
    the learn rate gradually after each epoch.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个问题，许多机器学习从业者在初始阶段使用较大的学习率（假设初始成本离最小值很远），然后在每个时期后逐渐降低学习率。
- en: TF provides 2 ways to do so as wonderfully explained in this StackOverflow [thread](http://stackoverflow.com/questions/33919948/how-to-set-adaptive-learning-rate-for-gradientdescentoptimizer),
    but here is the summary.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: TF 提供了两种方法，如此 StackOverflow [讨论](http://stackoverflow.com/questions/33919948/how-to-set-adaptive-learning-rate-for-gradientdescentoptimizer)
    中精彩解释，但这里是总结。
- en: Use Gradient Descent Optimizer Variants
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度下降优化器变体。
- en: TF comes with various gradient descent optimizer, which supports learn rate
    variation, such as [tf.train.AdagradientOptimizer](http://www.tensorflow.org/api_docs/python/train.html#AdagradOptimizer),
    and[tf.train.AdamOptimizer](http://www.tensorflow.org/api_docs/python/train.html#AdamOptimizer).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: TF 提供了多种梯度下降优化器，支持学习率变化，例如 [tf.train.AdagradientOptimizer](http://www.tensorflow.org/api_docs/python/train.html#AdagradOptimizer)
    和 [tf.train.AdamOptimizer](http://www.tensorflow.org/api_docs/python/train.html#AdamOptimizer)。
- en: Use *tf.placeholder* for Learn Rate
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*tf.placeholder*来设置学习率
- en: As you have learned previously, if we declare a *tf.placeholder*, in this case
    for learn rate, and use it within the *tf.train.GradientDescentOptimizer*, we
    can feed a different value to it at each training epoch, much like how we feed
    different datapoints to x, y_, which are also *tf.placeholders*, at each epoch.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你之前学到的，如果我们声明一个*tf.placeholder*，在这种情况下用于学习率，并在*tf.train.GradientDescentOptimizer*中使用它，我们可以在每个训练epoch中向其提供不同的值，就像我们在每个epoch中向x、y_（它们也是*tf.placeholders*）提供不同的数据点一样。
- en: 'We need 2 slight modifications:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要2个小修改：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Wrapping Up
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: We illustrated what machine learning ‘training’ is, and how to perform it using
    Tensorflow with just model & cost definitions, and looping through the training
    step, which feeds datapoints into the gradient descent optimizer. We also discussed
    the common variations in training, namely changing the size of datapoints the
    model uses for learning at each epoch, and varying the learn rate of gradient
    descent optimizer.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了什么是机器学习的“训练”，以及如何仅通过模型和成本定义，循环进行训练步骤来使用Tensorflow，这些步骤将数据点输入到梯度下降优化器中。我们还讨论了训练中的常见变体，即每个epoch中模型用于学习的数据点大小的变化，以及梯度下降优化器的学习率的变化。
- en: '**Coming Up Next**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**接下来**'
- en: Set up Tensor Board to visualize TF execution to detect problems in our model,
    cost function, or gradient descent
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置Tensor Board以可视化TF执行，检测模型、成本函数或梯度下降中的问题
- en: Perform linear regression with multiple features
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多个特征进行线性回归
- en: '**Resources**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源**'
- en: The code on [Github](https://github.com/nethsix/gentle_tensorflow/blob/master/code/linear_regression_one_feature_using_mini_batch_with_tensorboard.py)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码可以在 [Github](https://github.com/nethsix/gentle_tensorflow/blob/master/code/linear_regression_one_feature_using_mini_batch_with_tensorboard.py)
    上找到
- en: The slides on [SlideShare](http://www.slideshare.net/KhorSoonHin/gentlest-intro-to-tensorflow-part-2-62006222)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于此主题的幻灯片可在 [SlideShare](http://www.slideshare.net/KhorSoonHin/gentlest-intro-to-tensorflow-part-2-62006222)
    上查看
- en: The video on [YouTube](https://www.youtube.com/watch?v=Trc52FvMLEg)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频可在 [YouTube](https://www.youtube.com/watch?v=Trc52FvMLEg) 上观看
- en: '**Bio: [Soon Hin Khor](https://twitter.com/neth_6)**, Ph.D is using tech to
    make the world more caring, and responsible. Contributor of ruby-tensorflow. Co-organizer
    for Tokyo Tensorflow meetup.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介: [Soon Hin Khor](https://twitter.com/neth_6)**，博士，致力于通过技术使世界更加关怀和负责任。ruby-tensorflow的贡献者。东京Tensorflow
    meetup的共同组织者。'
- en: '[Original](https://medium.com/@khor/gentlest-introduction-to-tensorflow-part-2-ed2a0a7a624f).
    Reposted with permission.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/@khor/gentlest-introduction-to-tensorflow-part-2-ed2a0a7a624f).
    经许可转载。'
- en: '**Related:**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容:**'
- en: '[The Good, Bad & Ugly of TensorFlow](/2016/05/good-bad-ugly-tensorflow.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow的优点、缺点和不足](/2016/05/good-bad-ugly-tensorflow.html)'
- en: '[Multi-Task Learning in Tensorflow: Part 1](/2016/07/multi-task-learning-tensorflow-part-1.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tensorflow中的多任务学习：第1部分](/2016/07/multi-task-learning-tensorflow-part-1.html)'
- en: '[7 Steps to Understanding Deep Learning](/2016/01/seven-steps-deep-learning.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解深度学习的7个步骤](/2016/01/seven-steps-deep-learning.html)'
- en: '* * *'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在的组织的IT需求'
- en: '* * *'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[TensorFlow for Computer Vision - Transfer Learning Made Easy](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow用于计算机视觉 - 转移学习变得简单](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)'
- en: '[PyTorch or TensorFlow? Comparing popular Machine Learning frameworks](https://www.kdnuggets.com/2022/02/packt-pytorch-tensorflow-comparing-popular-machine-learning-frameworks.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch还是TensorFlow？比较流行的机器学习框架](https://www.kdnuggets.com/2022/02/packt-pytorch-tensorflow-comparing-popular-machine-learning-frameworks.html)'
- en: '[The "Hello World" of Tensorflow](https://www.kdnuggets.com/2022/05/hello-world-tensorflow.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tensorflow的“Hello World”](https://www.kdnuggets.com/2022/05/hello-world-tensorflow.html)'
- en: '[A Guide to Train an Image Classification Model Using Tensorflow](https://www.kdnuggets.com/2022/12/guide-train-image-classification-model-tensorflow.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 TensorFlow 训练图像分类模型指南](https://www.kdnuggets.com/2022/12/guide-train-image-classification-model-tensorflow.html)'
- en: '[Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 TensorFlow 和 Keras 构建并训练你的第一个神经网络](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
- en: '[Free TensorFlow 2.0 Complete Course](https://www.kdnuggets.com/2023/02/free-tensorflow-20-complete-course.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[免费 TensorFlow 2.0 完整课程](https://www.kdnuggets.com/2023/02/free-tensorflow-20-complete-course.html)'
