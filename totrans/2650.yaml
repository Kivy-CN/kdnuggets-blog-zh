- en: Can you trust AutoML?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你能信任AutoML吗？
- en: 原文：[https://www.kdnuggets.com/2020/12/trust-automl.html](https://www.kdnuggets.com/2020/12/trust-automl.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/12/trust-automl.html](https://www.kdnuggets.com/2020/12/trust-automl.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Ioannis Tsamardinos](https://www.linkedin.com/in/ioannistsamardinos/),
    (JADBio), Iordanis Xanthopoulos (Univ. of Crete, Greece), Vassilis Christophides
    (ENSEA, France)**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由[Ioannis Tsamardinos](https://www.linkedin.com/in/ioannistsamardinos/)、(JADBio)、Iordanis
    Xanthopoulos (克里特大学，希腊)、Vassilis Christophides (ENSEA，法国)**。'
- en: '![](../Images/0a257cd9dd24de6599059e72307802be.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a257cd9dd24de6599059e72307802be.png)'
- en: '*Image via Shutterstock under license.*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片来源于Shutterstock，许可下使用。*'
- en: What is AutoML?
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是AutoML？
- en: Automated Machine Learning, or AutoML, promises to automate the machine learning
    process end-to-end. It is a fast-rising, ultra-hot sub-field of machine learning
    and data science. AutoML platforms try hundreds or even thousands of different
    ML pipelines, called ***configurations***, combining algorithms for the different
    analysis steps (transformations, imputation, feature selection, and modeling)
    and their corresponding hyper-parameters. They often deliver models that beat
    the experts and win competitions, all with a few mouse clicks or lines of code.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化机器学习（AutoML）承诺端到端自动化机器学习过程。它是机器学习和数据科学领域快速崛起的超热门子领域。AutoML平台尝试数百种甚至数千种不同的机器学习管道，称为***配置***，将不同分析步骤（变换、插补、特征选择和建模）及其相应的超参数结合在一起。它们通常提供击败专家并赢得竞赛的模型，只需几个鼠标点击或几行代码。
- en: The Need to Estimate Performance
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 需要估计性能
- en: However, obtaining a model is never enough! One cannot trust the model for predictions
    unless there are guarantees of predictive performance. Is the model almost always
    accurate or closer to random guessing? Can we depend on it for life and death
    decisions in the clinic or business decisions that cost money? Having reliable
    estimates of out-of-sample (on new, unseen samples) performance is paramount.
    A good analyst would provide them for you, and so should an AutoML tool. Do existing
    AutoML libraries and platforms return reliable estimates of the performance of
    their models?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，获取一个模型永远不够！除非有预测性能的保证，否则不能信任模型的预测。这个模型几乎总是准确的，还是更接近随机猜测？我们能依赖它来做生命攸关的临床决策，或成本高昂的商业决策吗？可靠的样本外（在新样本上）性能估计至关重要。一个优秀的分析师会为你提供这些，而AutoML工具也应如此。现有的AutoML库和平台是否能提供其模型性能的可靠估计？
- en: What’s the size of the test set?
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试集的大小是多少？
- en: 'Well, some AutoML tools do not return any estimates at all! Take, for example,
    what is arguable, the most popular AutoML library: auto sklearn. Its authors suggest
    that to estimate performance, you hold out a separate, untainted, and unseen by
    the library test set strictly to estimate the final model''s performance. What
    tools like auto sklearn essentially offer is a Combined Algorithm Selection and
    Hyper-parameter optimization (**CASH**) or Hyper-Parameter Optimization (**HPO**)
    engine. They may be extremely useful and population but only partially implement
    the AutoML vision. After all, if you are required to decide on the optimal test
    size, write the code to apply the model, and the code to do the estimation, then
    the tool is not fully automatic. Deciding on the optimal test size is not trivial.
    It requires machine learning skills and knowledge. It requires an expert. Should
    you leave out 10%, 20%, or 30% of the data? Should you partition randomly or stratify
    the partition according to the outcome class. What if you have 1,000,000 samples?
    What percentage would you leave out? What if you have 1,000,000 samples and the
    positive class has a prevalence of 1 out of 100 000? What if you have a censored
    outcome like in survival analysis? How much is enough for your test set so that
    estimation is accurate? How do you calculate the confidence intervals of your
    estimate? Once you let the user decide on the test set estimation, you cannot
    claim anymore that your tool “democratizes” machine learning to non-expert analysts.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，一些AutoML工具根本不返回任何估计！以一个有争议的、最受欢迎的AutoML库为例：auto sklearn。其作者建议，为了估计性能，你需要保留一个独立的、未被库看到的测试集，严格用于估计最终模型的性能。像auto
    sklearn这样的工具本质上提供的是一个综合算法选择和超参数优化（**CASH**）或超参数优化（**HPO**）引擎。它们可能非常有用且流行，但只是部分实现了AutoML的愿景。毕竟，如果你需要决定最佳测试集大小、编写应用模型的代码以及进行估计的代码，那么这个工具就不是完全自动化的。决定最佳测试集大小并非易事。它需要机器学习技能和知识。它需要专家。你应该留下10%、20%还是30%的数据？你应该随机分割还是根据结果类别进行分层分割？如果你有1,000,000个样本呢？你会留下多少比例？如果你有1,000,000个样本，并且正类的发生率是1/100,000呢？如果你有像生存分析中的截尾结果呢？你的测试集需要多少才能保证估计的准确性？你如何计算估计的置信区间？一旦你让用户决定测试集估计，你就不能再声称你的工具“使”机器学习对非专家分析师“民主化”。
- en: Losing Samples to Estimation
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 样本丢失对估计的影响
- en: Having a separate hold-out test, however, creates an additional, deeper, more
    serious problem. The held-out data are “lost to estimation.” It could be precious
    data that we had to pay handsomely to collect or measure, data that we had to
    wait a significant time to obtain, and we just use them to gauge the effectiveness
    of a model. They are not employed to improve the model and our predictions. In
    many applications, we may not have the luxury to leave out a sizable test set!
    We need to analyze SARS-CoV-2 (COVID-19) data as soon as we observe the first
    few deaths – not after we wait long enough to have 1000 victims to “spare” just
    for performance estimation. When a new competitor product is out, we need to analyze
    our most recent data and react immediately, not wait until we have enough clients
    dropping out. “Small” sample data are important too. They are out there, and they
    will always be out there. ***Big Data may be computationally challenging, but
    small sample data are statistically challenging***. AutoML platforms and libraries
    that require an additional test set are out of the picture for the analysis of
    small sample size data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，拥有一个单独的测试集会产生一个额外、更深层次、更严重的问题。保留的数据“丧失了估计”的机会。那些可能是我们花费高昂成本收集或测量的宝贵数据，可能是我们等待了很长时间才获得的数据，我们只是用它们来评估模型的效果。它们没有被用于改进模型和我们的预测。在许多应用中，我们可能没有留下一个相当大测试集的奢侈条件！我们需要在观察到首批死亡后立即分析SARS-CoV-2（COVID-19）数据，而不是等到我们有1000个“可支配”的受害者来进行性能估计。当有新竞争对手产品推出时，我们需要立即分析我们最新的数据并做出反应，而不是等到我们有足够的客户流失。即使是“少量”的样本数据也很重要。它们存在，并且会一直存在。***大数据可能在计算上具有挑战性，但小样本数据在统计上具有挑战性***。需要额外测试集的AutoML平台和库不适合小样本数据的分析。
- en: From Estimating Performance of Models to Estimating Performance of Configurations
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从估计模型性能到估计配置性能
- en: 'Other AutoML tools do return performance estimates based on your input data
    only. They do not require you to select test set sizes, truly automating the procedure
    and paying justice to their name. Here is the idea: the final model is trained
    on **all the input data**. On average, since classifiers improve with sample size,
    this will be the best model. But then, we do not have any samples for estimation.
    Well, cross-validation and similar protocols (e.g., repeated hold-out) use proxy
    models to estimate the performance of the final model. They train many other models
    produced with the same configuration. So, ***they do not directly estimate the
    performance of the prediction model; they estimate the performance of the configuration
    (learning method, pipeline) that produces the model***. But, can you trust their
    estimates? Which brings us to the so-called “winner’s curse.”'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 其他 AutoML 工具确实只根据你的输入数据返回性能估计。它们不要求你选择测试集的大小，真正实现了程序的自动化，并且名副其实。其理念是：最终模型是在**所有输入数据**上训练的。通常来说，由于分类器在样本量增加时会提高，这将是最佳模型。但这样，我们就没有任何样本用于估计了。好吧，交叉验证和类似的协议（例如，重复保留法）使用代理模型来估计最终模型的性能。它们会训练许多使用相同配置生成的其他模型。因此，***它们并不直接估计预测模型的性能，而是估计生成模型的配置（学习方法、流程）的性能***。但，你能相信它们的估计吗？这引出了所谓的“赢家的诅咒”。
- en: Estimating Performance of a Single Configuration
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单个配置的性能估计
- en: We will illustrate an interesting statistical phenomenon with a small, simulated
    experiment. First, we will try to estimate the performance of a **single configuration**,
    consisting of a classifier with specific hyper-parameters, say 1-Nearest Neighbor.
    I simulate a test set of 50 samples for a binary outcome with 50–50% distribution.
    I assume my classifier has an 85% chance of providing the correct prediction,
    i.e., an accuracy of 85%. The next figure, panel (a), shows the accuracy estimate
    I obtained during my first try. I then generate a second (b) and a third (c) test
    set and corresponding estimates.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个小型模拟实验来说明一个有趣的统计现象。首先，我们尝试估计**单个配置**的性能，这个配置由一个具有特定超参数的分类器组成，比如1-最近邻。我模拟了一个二元结果的测试集，包含50个样本，分布为50-50%。我假设我的分类器有85%的概率提供正确的预测，即85%的准确率。下图面板（a）显示了我第一次尝试时获得的准确率估计。然后，我生成了第二个（b）和第三个（c）测试集以及相应的估计。
- en: '![](../Images/6663f6a330687c6ba23111b1f7ead6a1.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6663f6a330687c6ba23111b1f7ead6a1.png)'
- en: Sometimes my model is a bit lucky on the specific test set, and I estimate a
    performance higher than 0.85\. In fact, there is one simulation when the model
    got right in almost all 50 samples and derived a 95%-100% accuracy. Other times,
    the model will be unlucky on the specific test set, and I will underestimate the
    performance, but on average, the estimate will be correct and unbiased. I will
    not be fooling myself, my clients, or my fellow scientists.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，我的模型在特定测试集上稍微幸运一些，我会估计出高于0.85的性能。实际上，有一次模拟中，模型在几乎所有50个样本中都预测正确，得出了95%-100%的准确率。其他时候，模型在特定测试集上的表现会不佳，我会低估性能，但平均来说，估计将是正确且无偏的。我不会欺骗自己、我的客户或我的同行科学家。
- en: The Winners’ Curse (or Why You Systematically Overestimate)
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '胜者的诅咒（或你为什么系统性地高估）  '
- en: Now, let us simulate what AutoML libraries and platforms actually do, which
    is **try thousands of configurations**. For simplicity, I presume I have just
    8 configurations (instead of thousands) of 3 different algorithms (K-NN, Decision
    Tree, and Simple Bayes) matched with some hyper-parameter values. Each time I
    apply *all of them*, I estimate their performance on the same test set, *select
    the best one* as my final model, and *report the estimated performance of the
    winning configuration*. In the simulation, I assume all configurations lead to
    equally predictive models with 85% accuracy.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们模拟 AutoML 库和平台实际上做的事情，即**尝试成千上万的配置**。为简单起见，我假设我只有8个配置（而不是成千上万），这些配置由3种不同的算法（K-NN、决策树和简单贝叶斯）与一些超参数值匹配。每次我应用*所有这些配置*，我都会在相同的测试集上估计它们的性能，*选择表现最佳的一个*作为我的最终模型，并*报告获胜配置的估计性能*。在模拟中，我假设所有配置都导致具有85%准确率的同等预测模型。
- en: '![](../Images/e16c0bcdc75f8b7e0075173ac09616bd.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e16c0bcdc75f8b7e0075173ac09616bd.png)'
- en: What happens after the 1000 simulations? On average, my estimate is 0.916, while
    the true accuracy is 0.85. **I systematically overestimate.** Do not kid yourself;
    you will get the same results with cross-validation. Why does this happen? The
    phenomenon is related to the “winner’s curse” in statistics, where an effect size
    optimized among many models tends to be overestimated. In machine learning, the
    phenomenon was first discovered by David Jensen and named “the multiple induction
    problem” [Jensen, Cohen 2000]. But, let’s face it, “winner’s curse” is cooler
    (sorry, David). Conceptually the phenomenon is similar to multiple hypothesis
    testing. We need to “adjust” the produced *p*-values using a Bonferroni of False
    Discovery Rate procedure. Similarly, when we cross-validate numerous configurations,
    we need to “adjust” the cross-validated performance of the winning configuration
    for multiple tries. So, ***when you cross-validate a single configuration, you
    obtain an accurate estimate of performance ***(actually, it is a conservative
    estimate of the model trained on all data); ***when you cross-validate multiple
    configurations and report the estimate of the winning one, you overestimate performance.***
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在1000次模拟后会发生什么？平均而言，我的估计是0.916，而真实的准确率是0.85。**我系统性地过高估计。** 别自欺欺人；你通过交叉验证也会得到相同的结果。这是为什么呢？这一现象与统计学中的“赢家的诅咒”有关，其中在许多模型中优化的效应量往往会被过高估计。在机器学习中，这一现象首次由David
    Jensen发现，并被命名为“多重诱导问题”[Jensen, Cohen 2000]。不过，说实话，“赢家的诅咒”听起来更酷（对不起，David）。从概念上讲，这一现象类似于多重假设检验。我们需要使用Bonferroni或虚假发现率程序来“调整”产生的*p*-值。同样，当我们交叉验证多个配置时，我们需要对获胜配置的交叉验证表现进行“调整”以应对多次尝试。因此，***当你对单一配置进行交叉验证时，你会得到一个准确的性能估计***（实际上，这是对在所有数据上训练的模型的保守估计）；***当你对多个配置进行交叉验证并报告获胜配置的估计时，你会过高估计性能。***
- en: Does it Really Matter?
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这真的重要吗？
- en: Is this overestimation really significant, or just of academic curiosity? Well,
    when you have a large and balanced dataset, it is unnoticeable. On small sample
    datasets or very imbalanced datasets where you have just a few samples for one
    of the classes, it can become quite significant. It can easily reach 15–20 AUC
    points [Tsamardinos et al. 2020], which means your true AUC being equal to random-guessing
    (0.50), and you report 0.70, considered respectable and publishable performance
    for some domains. The overestimation is higher (a) the smaller the sample size
    for some of the classes, (b) the more configurations you try, (c) the more independent
    the models produced by your configurations, and (d) the closer to random guessing
    is your winning model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这种过高估计是否真的重要，还是只是学术上的好奇？好吧，当你拥有一个大且平衡的数据集时，这种过高估计是无足轻重的。在小样本数据集或非常不平衡的数据集上，比如某个类别只有几个样本时，它可能会变得相当显著。它很容易达到15–20
    AUC点[Tsamardinos et al. 2020]，这意味着你实际的AUC等同于随机猜测（0.50），而你报告的是0.70，这在某些领域被认为是令人尊敬且可以发表的表现。过高估计会更高：
    (a) 某些类别的样本量越小，(b) 你尝试的配置越多，(c) 你配置所产生的模型越独立，(d) 你的最佳模型越接近随机猜测。
- en: Can Overestimation Be Fixed? Can We Beat Winner’s Curse?
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过高估计可以修复吗？我们能否战胜“赢家的诅咒”？
- en: 'So, how do we fix it? There are at least three methodologies. The first one
    is — you guessed it — to hold out a separate test set, let us call it Estimation
    Set. The test sets during cross-validation of each configuration are employed
    for selecting the best one (namely for *tuning*) and are “seen” by each configuration
    multiple times. The Estimation Set will be employed only once, just to estimate
    the performance of the final model, so there is no “winner’s curse.” But, then,
    of course, we fall back into the same problem as before “losing samples to estimation.”
    The second method is to do **nested cross-validation **[Tsamardinos et al., 2018].
    The idea is the following: we consider the selection of the winning model as part
    of our learning procedure. We now have a **single procedure** (meta-configuration
    if you’d like) that tries numerous configurations, selects the best one using
    cross-validation, and trains the final model on all input data using the winning
    configuration. **We now cross-validate our single learning procedure** (which
    internally also performs cross-validation on each configuration; hence, the term
    nested). Nested cross-validation is accurate (see Figure 2 in [Tsamardinos et
    al. 2018]) but quite computationally expensive. It trains O(*C*⋅*K*2) models,
    where *C* is the number of configurations we try and *K* the number of folds in
    our cross-validations. But fortunately, there is a better way; the **Bootstrap
    Bias Corrected CV** or **BBC-CV **method [Tsamardinos et al. 2018]. BBC-CV is
    as accurate in estimation as nested cross-validation (Figure 2 in [Tsamardinos
    et al. 2018]) and one order of magnitude faster, namely trains O(C⋅K) models. ***BBC-CV
    essentially removes the need for having a separate Estimation Set***.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们该如何解决这个问题呢？至少有三种方法。第一种方法是——你猜对了——保留一个单独的测试集，我们称之为估计集。在每个配置的交叉验证过程中，测试集用于选择最佳配置（即进行
    *调优*），并且每个配置都会多次“看到”这些测试集。估计集仅会被使用一次，用于估计最终模型的性能，因此不存在“赢家的诅咒”。但当然，我们又会回到之前的问题，“样本用于估计的损失”。第二种方法是进行
    **嵌套交叉验证** [Tsamardinos et al., 2018]。其思路如下：我们将选择获胜模型视为学习过程的一部分。我们现在有一个 **单一过程**（如果你愿意，可以称之为元配置），它尝试许多配置，使用交叉验证选择最佳配置，并在所有输入数据上使用获胜配置训练最终模型。**我们现在对我们的单一学习过程进行交叉验证**（这个过程内部也对每个配置进行交叉验证，因此称为嵌套）。嵌套交叉验证是准确的（见
    [Tsamardinos et al. 2018] 的图 2），但计算开销相当大。它训练 O(*C*⋅*K*2) 个模型，其中 *C* 是我们尝试的配置数量，*K*
    是交叉验证的折数。但幸运的是，还有一种更好的方法；那就是 **自助法偏差修正交叉验证**（**BBC-CV**）方法 [Tsamardinos et al.
    2018]。BBC-CV 在估计准确性上与嵌套交叉验证相当（见 [Tsamardinos et al. 2018] 的图 2），并且快一个数量级，即训练 O(C⋅K)
    个模型。***BBC-CV 本质上消除了需要单独的估计集***。
- en: So, Can We Trust AutoML?
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 那么，我们可以信任 AutoML 吗？
- en: 'Back to our motivating question: can we trust AutoML? Despite all the work
    on AutoML, dozens of papers, competitions, challenges, and comparative evaluations, **nobody
    has checked so far whether these tools return accurate estimates**? Nobody, that
    is, until a couple of our recent works [Xanthopoulos 2020, Tsamardinos et al.
    2020]. In the following figure, we compare two AutoML tools, namely TPOT [Olson
    et al. 2016] and our own Just Add Data Bio, or JADBio [Tsamardinos et al. 2020]
    ([www.jadbio.com](http://www.jadbio.com/)) for short. TPOT is a commonly used,
    freeware AutoML library, and JADBio is our commercial AutoML platform employing
    BBC-CV. JADBio was designed with these estimation considerations in mind and specifically
    to encompass small-sample, high-dimensional biological data. Given that models
    learned from biological data may be employed in critical clinical applications,
    e.g., to predict therapy response and optimize treatment, we were really striving
    to return accurate performance estimates.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们激励性的问题：我们可以信任 AutoML 吗？尽管 AutoML 已经取得了大量的工作，包括数十篇论文、竞赛、挑战和比较评估，**至今还没有人检查过这些工具是否返回了准确的估计**。没有人，直到我们最近的一些工作
    [Xanthopoulos 2020, Tsamardinos et al. 2020]。在下图中，我们比较了两个 AutoML 工具，即 TPOT [Olson
    et al. 2016] 和我们自己的 Just Add Data Bio，简称 JADBio [Tsamardinos et al. 2020] ([www.jadbio.com](http://www.jadbio.com/))。TPOT
    是一个常用的免费 AutoML 库，而 JADBio 是我们采用 BBC-CV 的商业 AutoML 平台。JADBio 设计时考虑到了这些估计因素，并特别适用于小样本、高维生物数据。鉴于从生物数据中学习到的模型可能用于关键的临床应用，例如预测治疗反应和优化治疗，我们确实在努力返回准确的性能估计。
- en: We tried both systems on 100 binary classification problems from openml.org,
    spanning a wide range of sample size, feature size, and balance ratios (details
    in [Xanthopoulos 2020]). Each dataset was split in half, one to feed the AutoML
    tool and obtain a model and a self-assessed estimate (estimate of the model assessed
    by the AutoML platform, called Train Estimate), and one on which to apply the
    model (estimate of the model on the test set, called Test Estimate). The metric
    of performance was the AUC. Each point is an experiment on one dataset. Red dots
    are points below the diagonal, where the Test Estimate is lower than the training
    estimate, i.e., overestimated performances. The black dots are points above the
    diagonal, where performance is underestimated.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在来自 openml.org 的 100 个二分类问题上尝试了这两种系统，涵盖了广泛的样本大小、特征大小和平衡比例（详细信息见 [Xanthopoulos
    2020]）。每个数据集被分成两半，一部分用于输入 AutoML 工具以获取模型和自我评估估计（AutoML 平台评估的模型估计，称为 Train Estimate），另一部分用于应用模型（测试集上的模型估计，称为
    Test Estimate）。性能的度量指标是 AUC。每个点是对一个数据集的实验。红点是对角线下方的点，其中 Test Estimate 低于训练估计，即高估的性能。黑点是对角线以上的点，其中性能被低估。
- en: '![](../Images/3139daaee6c8857ba36c932ded04c1bb.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3139daaee6c8857ba36c932ded04c1bb.png)'
- en: TPOT seriously overestimates performance. There are datasets where the tool
    estimates model’s performance to be 1.0! In one of these cases (lowest redpoint),
    the actual performance on the test is less than 0.6, dangerously close to random
    guessing (0.5 AUC). It is obvious that you should be skeptical of TPOT’s self-assessed
    estimate. You need to lose samples to a hold-out test set. JADBio, on the other
    hand, systematically underestimates performance somewhat. The highest black point
    in the middle of its graph gets close to 1.00 AUC on the test but estimated performance
    at less than 0.70 AUC. On average, JADBio’s estimates are closer to the diagonal.
    The datasets in this experiment have more than 100 samples and some over tens
    of thousands. In other, more challenging experiments, we tried our system JADBio
    on 370+ omics datasets with sample sizes as small as 40 and the average number
    of features at 80,000+. JADBio, using the BBC-CV, does not overestimate performance
    [Tsamardinos et al. 2020]. Unfortunately, it is not only TPOT that exhibits this
    behavior. Additional similar evaluations and preliminary results indicate that
    it could be a common problem with AutoML tools [Xanthopoulos 2020].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: TPOT 严重高估了性能。有些数据集上，这个工具估计模型的性能为 1.0！在其中一个案例（最低的红点）中，实际测试性能不到 0.6，危险地接近随机猜测（0.5
    AUC）。显然，你应该对 TPOT 自我评估的估计持怀疑态度。你需要将样本分配到保留测试集中。另一方面，JADBio 系统性地低估了性能。其图中间的最高黑点在测试中的
    AUC 接近 1.00，但估计的性能低于 0.70 AUC。平均而言，JADBio 的估计更接近对角线。这些实验中的数据集有超过 100 个样本，有些数据集超过几万。在其他更具挑战性的实验中，我们在
    370 多个组学数据集上尝试了 JADBio，样本量最小为 40，特征平均数为 80,000+。JADBio 使用 BBC-CV 并不会高估性能 [Tsamardinos
    et al. 2020]。不幸的是，不仅 TPOT 展现了这种行为。额外的类似评估和初步结果表明，这可能是 AutoML 工具的普遍问题 [Xanthopoulos
    2020]。
- en: Don’t trust AND verify
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不要盲目相信，而要验证。
- en: In conclusion, first, beware of the self-assessment of performance of AutoML
    tools. It is so easy to overestimate performance when you automate analysis and
    generate thousands of models. Try your favorite tool on some datasets where you
    completely withhold part of the data. You need to do it multiple times to gauge
    the average behavior. Try challenging tasks like small-sample or highly-imbalanced
    data. The amount of overestimation may depend on meta-features (dataset characteristics)
    like the percentage of missing values, percentage of discrete features, number
    of values of the discrete features, and others. So, try datasets with similar
    characteristics like the ones you typically analyze! Second, even if you do not
    use AutoML and you code the analysis yourself, you may still produce numerous
    models and report the unadjusted, uncorrected cross-validated estimate of the
    winning one. The “winner’s curse” is ever-present to haunt you. You would not
    want to report a highly inflated, over-optimistic performance to your client,
    boss, professor, or paper reviewers, would you? … Or, would you?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，首先，要警惕AutoML工具性能的自我评估。当你自动化分析并生成成千上万的模型时，很容易高估性能。尝试在一些数据集上使用你喜欢的工具，其中你完全保留了一部分数据。你需要多次进行这种尝试，以评估平均行为。尝试具有挑战性的任务，比如小样本或高度不平衡的数据。过度估计的程度可能取决于元特征（数据集特征），例如缺失值的百分比、离散特征的百分比、离散特征的值的数量等。因此，尝试具有类似特征的数据集，类似于你通常分析的数据集！其次，即使你不使用AutoML而是自己编写分析代码，你仍然可能会产生大量模型，并报告未经调整、未经修正的交叉验证估计的最佳模型。
    “赢家诅咒”总是存在并困扰着你。你不想向你的客户、老板、教授或论文评审者报告一个高度夸大的、不切实际的性能，对吧？ … 或者，你会吗？
- en: References
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Jensen D.D. and Cohen P.R. (2000) *[*Multiple Comparisons in Induction Algorithms*](https://arizona.pure.elsevier.com/en/publications/multiple-comparisons-in-induction-algorithms)*.
    Mach. Learn., 38, 309–338.*'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Jensen D.D. 和 Cohen P.R. (2000) *[*归纳算法中的多重比较*](https://arizona.pure.elsevier.com/en/publications/multiple-comparisons-in-induction-algorithms)*.
    Mach. Learn., 38, 309–338.*'
- en: '*Tsamardinos I., Charonyktakis P., Lakiotaki K., Borboudakis G., Zenklusen
    J.C., Juhl H., Chatzaki E. and Lagani V. (2020) *[*Just Add Data: Automated Predictive
    Modeling and BioSignature Discovery.*](https://www.biorxiv.org/content/10.1101/2020.05.04.075747v1)* bioRxiv,
    10.1101/2020.05.04.075747*'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Tsamardinos I., Charonyktakis P., Lakiotaki K., Borboudakis G., Zenklusen
    J.C., Juhl H., Chatzaki E. 和 Lagani V. (2020) *[*仅需添加数据：自动预测建模与生物标志发现*](https://www.biorxiv.org/content/10.1101/2020.05.04.075747v1)* bioRxiv,
    10.1101/2020.05.04.075747*'
- en: '*Tsamardinos I., Greasidou E. and Borboudakis G. (2018) *[*Bootstrapping the
    out-of-sample predictions for efficient and accurate cross-validation.*](https://link.springer.com/article/10.1007/s10994-018-5714-4)* Mach.
    Learn., 107, 1895–1922.*'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Tsamardinos I., Greasidou E. 和 Borboudakis G. (2018) *[*引导外样本预测以实现高效准确的交叉验证*](https://link.springer.com/article/10.1007/s10994-018-5714-4)* Mach.
    Learn., 107, 1895–1922.*'
- en: '*Iordanis Xanthopoulos, M.Sc. Thesis, Computer Science Department, University
    of Crete, 2020*'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Iordanis Xanthopoulos, 硕士论文, 计算机科学系, 克里特大学, 2020*'
- en: '*Olson R.S., Bartley N., Urbanowicz R.J., and Moore J.H. (2016)*[* Evaluation
    of a tree-based pipeline optimization tool for automating data science.*](https://arxiv.org/abs/1603.06212)* In
    GECCO 2016 — Proceedings of the 2016 Genetic and Evolutionary Computation Conference.
    ACM Press, New York, New York, USA, pp. 485–492.*'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Olson R.S., Bartley N., Urbanowicz R.J. 和 Moore J.H. (2016)*[* 评估一个基于树的管道优化工具用于自动化数据科学*](https://arxiv.org/abs/1603.06212)* 在GECCO
    2016 — 2016年遗传与进化计算会议论文集。ACM Press, 纽约, 美国, 页码485–492.*'
- en: '[Original](https://ioannis-tsamardinos.medium.com/can-you-trust-automl-3a02332e66a0).
    Reposted with permission.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://ioannis-tsamardinos.medium.com/can-you-trust-automl-3a02332e66a0)。经许可转载。'
- en: '**Bio:** [Dr. Tsamardinos](https://twitter.com/tsamardgr) acquired his Ph.D.
    in 2001 from Pittsburgh University, USA. He worked as an Assistant Professor at
    the Department of Biomedical Informatics, Vanderbilt University until 2006, when
    he joint the University of Crete. His research interests are Artificial Intelligence
    and Philosophy of AI, Artificial Intelligence in Biomedicine, Machine Learning,
    Causal Inference, and Induction, Learning from Biomedical Data, Feature and Variable
    Selection for Classification, Bioinformatics, Planning, Applications of Machine
    Learning in Biomedical Informatics.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：** [Tsamardinos博士](https://twitter.com/tsamardgr) 于2001年获得美国匹兹堡大学博士学位。2006年，他在范德比尔特大学生物医学信息学系担任助理教授，随后加入克里特大学。他的研究兴趣包括人工智能与人工智能哲学、生物医学中的人工智能、机器学习、因果推断与归纳、从生物医学数据中学习、分类的特征与变量选择、生物信息学、规划、机器学习在生物医学信息学中的应用。'
- en: '[Iordanis Xanthopoulos](https://www.linkedin.com/in/iordanis-xanthopoulos-72943571/?originalSubdomain=gr)
    is a Postgraduate Research Fellow at University of Crete.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[Iordanis Xanthopoulos](https://www.linkedin.com/in/iordanis-xanthopoulos-72943571/?originalSubdomain=gr)
    是克里特大学的研究生研究员。'
- en: '[Vassilis Christophides](https://who.rocq.inria.fr/Vassilis.Christophides/)
    is at ENSEA, France.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[Vassilis Christophides](https://who.rocq.inria.fr/Vassilis.Christophides/)
    在法国ENSEA。'
- en: '**Related:**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Algorithms for Advanced Hyper-Parameter Optimization/Tuning](https://www.kdnuggets.com/2020/11/algorithms-for-advanced-hyper-parameter-optimization-tuning.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[高级超参数优化/调整算法](https://www.kdnuggets.com/2020/11/algorithms-for-advanced-hyper-parameter-optimization-tuning.html)'
- en: '[When Will AutoML replace Data Scientists? Poll Results and Analysis](https://www.kdnuggets.com/2020/03/poll-automl-replace-data-scientists-results.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AutoML何时会取代数据科学家？调查结果与分析](https://www.kdnuggets.com/2020/03/poll-automl-replace-data-scientists-results.html)'
- en: '[5 Techniques to Prevent Overfitting in Neural Networks](https://www.kdnuggets.com/2019/12/5-techniques-prevent-overfitting-neural-networks.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[防止神经网络过拟合的5种技术](https://www.kdnuggets.com/2019/12/5-techniques-prevent-overfitting-neural-networks.html)'
- en: '* * *'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前3个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织IT'
- en: '* * *'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[The Quest for Model Confidence: Can You Trust a Black Box?](https://www.kdnuggets.com/the-quest-for-model-confidence-can-you-trust-a-black-box)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[对模型信心的追求：你能信任黑箱吗？](https://www.kdnuggets.com/the-quest-for-model-confidence-can-you-trust-a-black-box)'
- en: '[Trust in AI is Priceless](https://www.kdnuggets.com/2022/08/trust-ai-priceless.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[对人工智能的信任是无价的](https://www.kdnuggets.com/2022/08/trust-ai-priceless.html)'
- en: '[In Data We Trust: Data Centric AI](https://www.kdnuggets.com/2022/10/data-trust-data-centric-ai.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我们信任数据：数据驱动的人工智能](https://www.kdnuggets.com/2022/10/data-trust-data-centric-ai.html)'
- en: '[Sky''s the Limit: Learn how JetBlue uses Monte Carlo and Snowflake…](https://www.kdnuggets.com/2022/12/monte-carlo-jetblue-snowflake-build-trust-improve-model-accuracy.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[无限可能：了解JetBlue如何利用蒙特卡洛和Snowflake…](https://www.kdnuggets.com/2022/12/monte-carlo-jetblue-snowflake-build-trust-improve-model-accuracy.html)'
- en: '[The Top AutoML Frameworks You Should Consider in 2023](https://www.kdnuggets.com/2023/05/best-automl-frameworks-2023.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2023年你应该考虑的顶级AutoML框架](https://www.kdnuggets.com/2023/05/best-automl-frameworks-2023.html)'
- en: '[9 Professional Certificates That Can Take You Onto a Degree... If…](https://www.kdnuggets.com/9-professional-certificates-that-can-take-you-onto-a-degree-if-you-really-want-to)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9个专业证书可以帮助你获得学位…如果…](https://www.kdnuggets.com/9-professional-certificates-that-can-take-you-onto-a-degree-if-you-really-want-to)'
