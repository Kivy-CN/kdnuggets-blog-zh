- en: Cleaning and Preprocessing Text Data in Pandas for NLP Tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/cleaning-and-preprocessing-text-data-in-pandas-for-nlp-tasks](https://www.kdnuggets.com/cleaning-and-preprocessing-text-data-in-pandas-for-nlp-tasks)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![NLP (Natural Language Processing)](../Images/5ae96669735b9ef013b0b884100fcb7a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning and preprocessing data is often one of the most daunting, yet critical
    phases in building AI and Machine Learning solutions fueled by data, and text
    data is not the exception.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial breaks the ice in tackling the challenge of preparing text data
    for NLP tasks such as those Language Models (LMs) can solve. By encapsulating
    your text data in pandas DataFrames, the below steps will help you get your text
    ready for being digested by NLP models and algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Load the Data into a Pandas DataFrame
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To keep this tutorial simple and focused on understanding the necessary text
    cleaning and preprocessing steps, let's consider a small sample of four single-attribute
    text data instances that will be moved into a pandas DataFrame instance. We will
    from now on apply every preprocessing step on this DataFrame object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Handle Missing Values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Did you notice the ''None'' value in one of the example data instances? This
    is known as a missing value. Missing values are commonly collected for various
    reasons, often accidental. The bottom line: you need to handle them. The simplest
    approach is to simply detect and remove instances containing missing values, as
    done in the code below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Normalize the Text to Make it Consistent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Normalizing text implies standardizing or unifying elements that may appear
    under different formats across different instances, for instance, date formats,
    full names, or case sensitiveness. The simplest approach to normalize our text
    is to convert all of it to lowercase, as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Remove Noise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Noise is unnecessary or unexpectedly collected data that may hinder the subsequent
    modeling or prediction processes if not handled adequately. In our example, we
    will assume that punctuation marks like "!" are not needed for the subsequent
    NLP task to be applied, hence we apply some noise removal on it by detecting punctuation
    marks in the text using a regular expression. The 're' Python package is used
    for working and performing text operations based on regular expression matching.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Tokenize the Text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tokenization is arguably the most important text preprocessing step -along with
    encoding text into a numerical representation- before using NLP and language models.
    It consists in splitting each text input into a vector of chunks or tokens. In
    the simplest scenario, tokens are associated with words most of the time, but
    in some cases like compound words, one word might lead to multiple tokens. Certain
    punctuation marks (if they were not previously removed as noise) are also sometimes
    identified as standalone tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'This code splits each of our three text entries into individual words (tokens)
    and adds them as a new column in our DataFrame, then displays the updated data
    structure with its two columns. The simplified tokenization approach applied is
    known as simple whitespace tokenization: it just uses whitespaces as the criterion
    to detect and separate tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Remove Stop Words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the text is tokenized, we filter out unnecessary tokens. This is typically
    the case of stop words, like articles "a/an, the", or conjunctions, which do not
    add actual semantics to the text and should be removed for later efficient processing.
    This process is language-dependent: the code below uses the NLTK library to download
    a dictionary of English stop words and filter them out from the token vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Stemming and Lemmatization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Almost there! Stemming and lemmatization are additional text preprocessing steps
    that might sometimes be used depending on the specific task at hand. Stemming
    reduces each token (word) to its base or root form, whilst lemmatization further
    reduces it to its lemma or base dictionary form depending on the context, e.g.
    "best" -> "good". For simplicity, we will only apply stemming in this example,
    by using the PorterStemmer implemented in the NLTK library, aided by the wordnet
    dataset of word-root associations. The resulting stemmed words are saved in a
    new column in the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Convert Text into Numerical Representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Last but not least, computer algorithms including AI/ML models do not understand
    human language but numbers, hence we need to map our word vectors into numerical
    representations, commonly known as embedding vectors, or simply embedding. The
    below example converts tokenized text in the 'tokens' column and uses a TF-IDF
    vectorization approach (one of the most popular approaches in the good old days
    of classical NLP) to transform the text into numerical representations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: And that's it! As unintelligible as it may seem to us, this numerical representation
    of our preprocessed text is what intelligent systems including NLP models do understand
    and can handle exceptionally well for challenging language tasks like classifying
    sentiment in text, summarizing it, or even translating it to another language.
  prefs: []
  type: TYPE_NORMAL
- en: The next step would be feeding these numerical representations to our NLP model
    to let it do its magic.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.linkedin.com/in/ivanpc/)****[Iván Palomares Carrascosa](https://www.linkedin.com/in/ivanpc/)****
    is a leader, writer, speaker, and adviser in AI, machine learning, deep learning
    & LLMs. He trains and guides others in harnessing AI in the real world.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Learn Data Cleaning and Preprocessing for Data Science with This Free eBook](https://www.kdnuggets.com/2023/08/learn-data-cleaning-preprocessing-data-science-free-ebook.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Steps to Mastering Data Cleaning and Preprocessing Techniques](https://www.kdnuggets.com/2023/08/7-steps-mastering-data-cleaning-preprocessing-techniques.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Harnessing ChatGPT for Automated Data Cleaning and Preprocessing](https://www.kdnuggets.com/2023/08/harnessing-chatgpt-automated-data-cleaning-preprocessing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Representation for Natural Language Processing Tasks](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Tasks To Automate With Python](https://www.kdnuggets.com/2021/06/5-tasks-automate-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[HuggingGPT: The Secret Weapon to Solve Complex AI Tasks](https://www.kdnuggets.com/2023/05/hugginggpt-secret-weapon-solve-complex-ai-tasks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
