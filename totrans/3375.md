# 深度学习研究综述：强化学习

> 原文：[https://www.kdnuggets.com/2016/11/deep-learning-research-review-reinforcement-learning.html](https://www.kdnuggets.com/2016/11/deep-learning-research-review-reinforcement-learning.html)

![](../Images/21b4ad6273c43fa6a8bd2bd337284dbb.png)

*这是名为深度学习研究综述的新系列的第二部分。每隔几周，我将总结和解释深度学习特定子领域的研究论文。本周的重点是强化学习。*

[*上次*](https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-1-Generative-Adversarial-Nets) *是生成对抗网络，如果你错过了*

### **强化学习简介**

**机器学习的三大类**

在深入讨论论文之前，让我们首先谈谈什么是**强化学习**。机器学习的领域可以分为三大类。

1.  监督学习

1.  无监督学习

1.  强化学习

![](../Images/7a5ff597262bfbaecade1e2614986dc5.png)

第一类，**监督学习**，可能是你最熟悉的。它依赖于基于一组包含输入及其对应标签的训练数据来创建一个函数或模型。卷积神经网络就是一个很好的例子，因为图像是输入，输出是图像的分类（狗、猫等）。

**无监督学习** 旨在通过聚类分析方法在数据中发现某种结构。一个最著名的机器学习聚类算法K-Means就是无监督学习的一个例子。

**强化学习** 的任务是学习在特定情况/环境下采取什么行动，以最大化奖励信号。监督学习和强化学习之间有一个有趣的区别，那就是这个奖励信号只是告诉你代理采取的行动（或输入）是好还是坏，并不会告诉你*最佳*行动是什么。相比之下，卷积神经网络（CNN）中的每个图像输入的对应标签则明确指示了每个输入的输出应该是什么。另一个强化学习的独特组成部分是，代理的行动会影响它随后接收到的数据。例如，代理向左移动而不是向右移动意味着代理在下一个时间步将从环境中接收到不同的输入。让我们先看一个例子。

**强化学习问题**

首先，让我们考虑一下强化学习问题中包含了什么。设想一个小机器人在一个小房间里。我们没有编程让这个机器人移动、行走或采取任何行动。它只是站在那里。这个机器人是我们的**代理**。

![](../Images/5612d33f51f345bd3f80fd3b221d4df9.png)

正如我们之前提到的，强化学习的核心在于尝试理解最优的决策/动作方式，以最大化某些**奖励** **R**。这个奖励是一个反馈信号，用于指示代理在给定时间步的表现如何。代理在每个时间步采取的**动作 A**是奖励（告诉代理当前表现如何的信号）和**状态 S**（描述代理所处环境的状态）的函数。环境状态到动作的映射称为我们的**策略 P**。策略基本上定义了代理在特定时间、特定情况下的行为方式。现在，我们还有一个**价值函数 V**，它衡量每个位置的好坏。与奖励不同的是，奖励信号指示的是即时的好坏，而价值函数则更能指示在长期来看处于该状态/位置的好坏。最后，我们还有一个**模型 M**，它是代理对环境的表征。这是代理对环境如何表现的模型。

![](../Images/7cde482d39514ca446c63adbf7dd676b.png)

**马尔可夫决策过程**

那么，现在让我们回想一下我们在小房间里的机器人（代理）。我们的**奖励函数**取决于我们希望代理完成什么任务。假设我们希望它移动到房间的一个角落，在那里它将获得奖励。当机器人到达这个点时，它将获得 +25 的奖励，每经过一个时间步则会得到 -1 的惩罚。我们基本上希望机器人尽快到达角落。代理可以采取的**动作**有向北、向南、向东或向西移动。代理的**策略**可以是一个简单的策略，其中行为是代理总是移动到具有更高价值函数的位置。对吧？具有高价值函数的位置 = 在该位置处于较长回报中是好的。

现在，这整个 RL 环境可以用**马尔可夫决策过程**来描述。对于那些之前没有听说过这个术语的人，MDP 是一个建模代理决策的框架。它包含一个有限的状态集（以及这些状态的价值函数）、一个有限的动作集、一种策略和一个奖励函数。我们的价值函数可以分成两个部分。

1.  **状态-价值函数 V**：在状态 S 中并遵循策略 π 的预期回报。这个回报通过查看每个未来时间步的奖励总和来计算（gamma 指的是一个常数折扣因子，这意味着第 10 个时间步的奖励权重会稍低于第 1 个时间步的奖励）。

![](../Images/29868060c2213b587f9057bd5e41c833.png)

1.  **动作-价值函数 Q**：在状态 S 中，遵循策略 π 并采取动作 a 的预期回报（方程式将与上述相同，只是我们有一个额外条件，即 At = a）。

既然我们已经有了所有组件，我们该如何处理这个 MDP 呢？当然，我们希望解决它。通过解决 MDP，你将能够找到最大化代理在环境中任何状态下期望获得的奖励量的最优行为（策略）。

**解决 MDP**

我们可以通过动态规划特别是使用 **策略迭代** 来解决 MDP 并获得最优策略（还有另一种技术叫做值迭代，但现在不讨论）。这个想法是我们从某个初始策略 π1 开始，评估该策略的状态价值函数。我们通过 **贝尔曼期望方程** 来完成这一步骤。

![](../Images/efc5925eb0ccb99a4fcd1e748c63df11.png)

这个方程基本上表明，给定我们遵循策略 π 的情况下，我们的价值函数可以分解为即时奖励 Rt+1 和后继状态 St+1 的价值函数的期望回报总和。如果仔细考虑，这等同于我们在上一节中使用的价值函数定义。使用这个方程就是我们的 **策略评估** 组件。为了得到更好的策略，我们使用 **策略** 改进步骤，其中我们仅对价值函数采取贪婪的行动。换句话说，代理选择最大化价值的动作。

![](../Images/d6dbe01172b839fa39441614dd9d0708.png)

现在，为了获得 *最优* 策略，我们重复这两个步骤，一个接一个，直到我们收敛到最优策略 π*。

**当你没有 MDP 时**

策略迭代非常有效，但它仅在我们拥有一个给定的 MDP 时才有效。MDP 实质上告诉你环境如何运作，而在现实世界中，这些信息往往不会被提供。当没有给定 MDP 时，我们使用模型无关的方法，这些方法直接从代理与环境的经验/互动中得到价值函数和策略。我们将执行相同的策略评估和策略改进步骤，只是没有 MDP 提供的信息。

我们的方法是，通过优化状态价值函数来改进策略，而是优化动作价值函数 Q。记得我们是如何将状态价值函数分解为即时奖励和后继状态价值函数的总和的吗？我们可以对 Q 函数做同样的处理。

![](../Images/655495d0bbcdba93e724a2c83da5b86c.png)

现在，我们将经历相同的策略评估和策略改进过程，只是我们将状态值函数V替换为动作值函数Q。现在，我将跳过关于评估/改进步骤的具体变化。要理解MDP自由评估和改进方法，如蒙特卡罗学习、时间差分学习和SARSA等主题将需要整个博客来讨论（不过，如果你感兴趣的话，请听听David Silver的[Lecture 4](https://www.youtube.com/watch?v=PnHCvfgC_ZA)和[Lecture 5](https://www.youtube.com/watch?v=0g4j2k_Ggc4)）。现在，我将直接跳到价值函数逼近以及AlphaGo和Atari论文中讨论的方法，希望这能给你一些现代RL技术的概述。**主要的收获是我们想找到能最大化我们的动作值函数Q的最优策略π***。

**价值函数逼近**

所以，如果你回顾我们到目前为止所学的内容，我们以一种相对简单的方式处理了问题。看看上面的Q方程。我们在接受特定状态S和动作A之后，计算一个基本上告诉我们期望回报的数字。现在假设我们的代理移动了1毫米到右边。这意味着我们有了一个全新的状态S’，现在我们需要为这个新状态计算Q值。在现实世界的RL问题中，有数以百万计的状态，因此我们的价值函数需要理解泛化，即我们不需要为每个可能的状态存储一个完全不同的价值函数。解决方案是使用**Q值函数逼近**，它能够泛化到未知状态。

所以，我们想要的是一个函数，我们称之为Qhat，它能够在给定某个状态S和某个动作A的情况下，提供Q值的大致估计。

![](../Images/96caca4b660041c06c312793e33f7876.png)

这个函数将接受S、A和一个老旧的权重向量W（看到W你就会知道我们要引入一些梯度下降*）。它将计算x（即表示S和A的特征向量）与W之间的点积。我们改进这个函数的方式是通过计算真实Q值（假设现在它是已知的）与逼近函数输出之间的损失*。

*![](../Images/61c07904b90863d6af0b7ae4b910caac.png)

在计算损失之后，我们使用梯度下降来找到最小值，此时我们将获得最优的W向量。这个函数逼近的想法在稍后查看论文时将非常关键。

**再说一件事**

在讨论论文之前，想提一件最后的事。一个有趣的强化学习话题是探索与利用。**利用** 是代理利用已知的知识，然后采取它知道会产生最大奖励的行动。这听起来不错，对吧？代理将总是根据当前知识做出最佳行动。然而，这句话中有一个关键短语。*当前知识*。如果代理没有足够地探索状态空间，它不可能知道是否真的在采取最佳行动。以探索状态空间为主要目的的行动被称为**探索**。

这个想法很容易与现实生活中的例子相关联。假设你今晚要选择去哪个餐馆就餐。你（作为代理）知道自己喜欢墨西哥菜，所以在强化学习术语中，去墨西哥餐馆将是一个能最大化你奖励或幸福/满足感的行动。然而，还有一个意大利菜的选择，你以前从未尝试过。它可能比墨西哥菜更好，也可能差很多。这种在利用代理的过去知识和尝试新事物以期发现更大奖励之间的权衡，是强化学习中的一个主要挑战（在我们的日常生活中也是如此）。

**其他强化学习资源**

哇，这信息量很大。不过，这绝不是该领域的全面概述。如果你想要更深入的强化学习概述，我强烈推荐这些资源。

+   David Silver（来自Deepmind）强化学习 [视频讲座](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT)

    +   我在强化学习课程中的 [个人笔记](https://docs.google.com/document/d/1TjmYDOxQzOQ0jd0lUiFOVQ1hNAmtwKfSAACck9dR7a8/edit?usp=sharing)

+   Sutton 和 Barto 的 [强化学习教材](https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf)（如果你决定深入学习这个子领域，这真的是终极宝典）

+   Andrej Karpathy 的 [博客文章](http://karpathy.github.io/2016/05/31/rl/) 关于强化学习（如果你想轻松入门强化学习，并查看一个非常出色的实践例子，可以从这篇开始）

+   [UC Berkeley CS 188](http://ai.berkeley.edu/lecture_videos.html) 讲座 8-11

+   [Open AI Gym](https://gym.openai.com/): 当你对强化学习感到舒适时，尝试用Open AI创建的这个强化学习工具包创建自己的代理

* * *

## 我们的三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业的快车道。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的IT

* * *

### 更多相关内容

+   [HuggingFace推出了免费的深度强化学习课程](https://www.kdnuggets.com/2022/05/huggingface-launched-free-deep-reinforcement-learning-course.html)

+   [人工智能、分析、机器学习、数据科学、深度学习……](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)

+   [实践强化学习课程第3部分：SARSA](https://www.kdnuggets.com/2022/01/handson-reinforcement-learning-course-part-3-sarsa.html)

+   [实践强化学习课程，第1部分](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-course-part-1.html)

+   [实践强化学习课程，第2部分](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-part-2.html)

+   [强化学习新手指南](https://www.kdnuggets.com/2022/05/reinforcement-learning-newbies.html)*
