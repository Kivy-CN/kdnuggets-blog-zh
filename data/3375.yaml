- en: 'Deep Learning Research Review: Reinforcement Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习研究综述：强化学习
- en: 原文：[https://www.kdnuggets.com/2016/11/deep-learning-research-review-reinforcement-learning.html](https://www.kdnuggets.com/2016/11/deep-learning-research-review-reinforcement-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/11/deep-learning-research-review-reinforcement-learning.html](https://www.kdnuggets.com/2016/11/deep-learning-research-review-reinforcement-learning.html)
- en: '![](../Images/21b4ad6273c43fa6a8bd2bd337284dbb.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21b4ad6273c43fa6a8bd2bd337284dbb.png)'
- en: '*This is the 2nd installment of a new series called Deep Learning Research
    Review. Every couple weeks or so, I’ll be summarizing and explaining research
    papers in specific subfields of deep learning. This week focuses on Reinforcement
    Learning.*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是名为深度学习研究综述的新系列的第二部分。每隔几周，我将总结和解释深度学习特定子领域的研究论文。本周的重点是强化学习。*'
- en: '[*Last time*](https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-1-Generative-Adversarial-Nets)
    *was Generative Adversarial Networks ICYMI*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[*上次*](https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-1-Generative-Adversarial-Nets)
    *是生成对抗网络，如果你错过了*'
- en: '**Introduction to Reinforcement Learning**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**强化学习简介**'
- en: '**3 Categories of Machine Learning**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习的三大类**'
- en: Before getting into the papers, let’s first talk about what **reinforcement
    learning** is. The field of machine learning can be separated into 3 main categories.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入讨论论文之前，让我们首先谈谈什么是**强化学习**。机器学习的领域可以分为三大类。
- en: Supervised Learning
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised Learning
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Reinforcement Learning
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 强化学习
- en: '![](../Images/7a5ff597262bfbaecade1e2614986dc5.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a5ff597262bfbaecade1e2614986dc5.png)'
- en: The first category, **supervised learning**, is the one you may be most familiar
    with. It relies on the idea of creating a function or model based on a set of
    training data, which contains inputs and their corresponding labels. Convolutional
    Neural Networks are a great example of this, as the images are the inputs and
    the outputs are the classifications of the images (dog, cat, etc).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第一类，**监督学习**，可能是你最熟悉的。它依赖于基于一组包含输入及其对应标签的训练数据来创建一个函数或模型。卷积神经网络就是一个很好的例子，因为图像是输入，输出是图像的分类（狗、猫等）。
- en: '**Unsupervised learning** seeks to find some sort of structure within data
    through methods of cluster analysis. One of the most well-known ML clustering
    algorithms, K-Means, is an example of unsupervised learning.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**无监督学习** 旨在通过聚类分析方法在数据中发现某种结构。一个最著名的机器学习聚类算法K-Means就是无监督学习的一个例子。'
- en: '**Reinforcement learning** is the task of learning what actions to take, given
    a certain situation/environment, so as to maximize a reward signal. The interesting
    difference between supervised and reinforcement learning is that this reward signal
    simply tells you whether the action (or input) that the agent takes is good or
    bad. It doesn’t tell you anything about what the *best* action is. Contrast this
    to CNNs where the corresponding label for each image input is a definite instruction
    of what the output should be for each input.  Another unique component of RL is
    that an agent’s actions will affect the subsequent data it receives. For example,
    an agent’s action of moving left instead of right means that the agent will receive
    different input from the environment at the next time step. Let’s look at an example
    to start off.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习** 的任务是学习在特定情况/环境下采取什么行动，以最大化奖励信号。监督学习和强化学习之间有一个有趣的区别，那就是这个奖励信号只是告诉你代理采取的行动（或输入）是好还是坏，并不会告诉你*最佳*行动是什么。相比之下，卷积神经网络（CNN）中的每个图像输入的对应标签则明确指示了每个输入的输出应该是什么。另一个强化学习的独特组成部分是，代理的行动会影响它随后接收到的数据。例如，代理向左移动而不是向右移动意味着代理在下一个时间步将从环境中接收到不同的输入。让我们先看一个例子。'
- en: '**The RL Problem**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习问题**'
- en: So, let’s first think about what have in a reinforcement learning problem. Let’s
    imagine a tiny robot in a small room. We haven’t programmed this robot to move
    or walk or take any action. It’s just standing there. This robot is our **agent**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们考虑一下强化学习问题中包含了什么。设想一个小机器人在一个小房间里。我们没有编程让这个机器人移动、行走或采取任何行动。它只是站在那里。这个机器人是我们的**代理**。
- en: '![](../Images/5612d33f51f345bd3f80fd3b221d4df9.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5612d33f51f345bd3f80fd3b221d4df9.png)'
- en: Like we mentioned before, reinforcement learning is all about trying to understand
    the optimal way of making decisions/actions so that we maximize some **reward** **R**.
    This reward is a feedback signal that just indicates how well the agent is doing
    at a given time step. The **action A** that an agent takes at every time step
    is a function of both the reward (signal telling the agent how well it’s currently
    doing) and the **state S**, which is a description of the environment the agent
    is in. The mapping from environment states to actions is called our **policy P**.
    The policy basically defines the agent’s way of behaving at a certain time, given
    a certain situation. Now, we also have a **value function V** which is a measure
    of how good each position is. This is different from the reward in that the reward
    signal indicates what is good in the immediate sense, while the value function
    is more indicative of how good it is to be in this state/position in the long
    run. Finally, we also have a **model M** which is the agent’s representation of
    the environment. This is the agent’s model of how it thinks that the environment
    is going to behave.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，强化学习的核心在于尝试理解最优的决策/动作方式，以最大化某些**奖励** **R**。这个奖励是一个反馈信号，用于指示代理在给定时间步的表现如何。代理在每个时间步采取的**动作
    A**是奖励（告诉代理当前表现如何的信号）和**状态 S**（描述代理所处环境的状态）的函数。环境状态到动作的映射称为我们的**策略 P**。策略基本上定义了代理在特定时间、特定情况下的行为方式。现在，我们还有一个**价值函数
    V**，它衡量每个位置的好坏。与奖励不同的是，奖励信号指示的是即时的好坏，而价值函数则更能指示在长期来看处于该状态/位置的好坏。最后，我们还有一个**模型
    M**，它是代理对环境的表征。这是代理对环境如何表现的模型。
- en: '![](../Images/7cde482d39514ca446c63adbf7dd676b.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7cde482d39514ca446c63adbf7dd676b.png)'
- en: '**Markov Decision Process**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**马尔可夫决策过程**'
- en: So, let’s now think back to our robot (the agent) in the small room. Our **reward
    function** is dependent on what we want the agent to accomplish. Let’s say that
    we want it to move to one of the corners of the room where it will receive a reward.
    The robot will get a +25 when it reaches this point, and will get a -1 for every
    time step it takes to get there. We basically want the robot to get the corner
    as fast as possible. The **actions** the agent can take are moving north, south,
    east, or west. The agent’s **policy** can be a simple one, where the behavior
    is that the agent will always move to the location with the higher value function.
    Makes sense right? A position with a high value function = good to be in this
    position (with regards to long term reward).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，现在让我们回想一下我们在小房间里的机器人（代理）。我们的**奖励函数**取决于我们希望代理完成什么任务。假设我们希望它移动到房间的一个角落，在那里它将获得奖励。当机器人到达这个点时，它将获得
    +25 的奖励，每经过一个时间步则会得到 -1 的惩罚。我们基本上希望机器人尽快到达角落。代理可以采取的**动作**有向北、向南、向东或向西移动。代理的**策略**可以是一个简单的策略，其中行为是代理总是移动到具有更高价值函数的位置。对吧？具有高价值函数的位置
    = 在该位置处于较长回报中是好的。
- en: Now, this whole RL environment can be described with a **Markov Decision Process**.
    For those that haven’t heard the term before, an MDP is a framework for modeling
    an agent’s decision making. It contains a finite set of states (and value functions
    for those states), a finite set of actions, a policy, and a reward function. Our
    value function can be split into 2 terms.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这整个 RL 环境可以用**马尔可夫决策过程**来描述。对于那些之前没有听说过这个术语的人，MDP 是一个建模代理决策的框架。它包含一个有限的状态集（以及这些状态的价值函数）、一个有限的动作集、一种策略和一个奖励函数。我们的价值函数可以分成两个部分。
- en: '**State-value function V**: The expected return from being in a state S and
    following a policy π. This return is calculated by looking at summation of the
    reward at each future time step (The gamma refers to a constant discount factor,
    which means that the reward at time step 10 is weighted a little less than the
    reward at time step 1).'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**状态-价值函数 V**：在状态 S 中并遵循策略 π 的预期回报。这个回报通过查看每个未来时间步的奖励总和来计算（gamma 指的是一个常数折扣因子，这意味着第
    10 个时间步的奖励权重会稍低于第 1 个时间步的奖励）。'
- en: '![](../Images/29868060c2213b587f9057bd5e41c833.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29868060c2213b587f9057bd5e41c833.png)'
- en: '**Action-value function Q**: The expected return from being in a state S, following
    a policy π, and taking an action a (Equation will be same as above except that
    we have an additional condition that At = a).'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**动作-价值函数 Q**：在状态 S 中，遵循策略 π 并采取动作 a 的预期回报（方程式将与上述相同，只是我们有一个额外条件，即 At = a）。'
- en: Now that we have all the components, what do we do with this MDP? Well, we want
    to solve it, of course. By solving an MDP, you’ll be able to find the optimal
    behavior (policy) that maximizes the amount of reward the agent can expect to
    get from any state in the environment.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经有了所有组件，我们该如何处理这个 MDP 呢？当然，我们希望解决它。通过解决 MDP，你将能够找到最大化代理在环境中任何状态下期望获得的奖励量的最优行为（策略）。
- en: '**Solving the MDP**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决 MDP**'
- en: We can solve an MDP and get the optimum policy through the use of dynamic programming
    and specifically through the use of **policy iteration **(there is another technique
    called value iteration, but won’t go into that right now). The idea is that we
    take some initial policy π1 and evaluate the state value function for that policy.
    The way we do this is through the **Bellman expectation equation**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过动态规划特别是使用 **策略迭代** 来解决 MDP 并获得最优策略（还有另一种技术叫做值迭代，但现在不讨论）。这个想法是我们从某个初始策略
    π1 开始，评估该策略的状态价值函数。我们通过 **贝尔曼期望方程** 来完成这一步骤。
- en: '![](../Images/efc5925eb0ccb99a4fcd1e748c63df11.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efc5925eb0ccb99a4fcd1e748c63df11.png)'
- en: This equation basically says that our value function, given that we’re following
    policy π, can be decomposed into the expected return sum of the immediate reward
    Rt+1 and the value function of the successor state St+1\. If you think about it
    closely, this is equivalent to the value function definition we used in the previous
    section. Using this equation is our **policy evaluation**component. In order to
    get a better policy, we use a **policy **improvement step where we simply act
    greedily with respect to the value function. In other words, the agent takes the
    action that maximizes value.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程基本上表明，给定我们遵循策略 π 的情况下，我们的价值函数可以分解为即时奖励 Rt+1 和后继状态 St+1 的价值函数的期望回报总和。如果仔细考虑，这等同于我们在上一节中使用的价值函数定义。使用这个方程就是我们的
    **策略评估** 组件。为了得到更好的策略，我们使用 **策略** 改进步骤，其中我们仅对价值函数采取贪婪的行动。换句话说，代理选择最大化价值的动作。
- en: '![](../Images/d6dbe01172b839fa39441614dd9d0708.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6dbe01172b839fa39441614dd9d0708.png)'
- en: Now, in order to get the *optimal* policy, we repeat these 2 steps, one after
    the other, until we converge to optimal policy π*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了获得 *最优* 策略，我们重复这两个步骤，一个接一个，直到我们收敛到最优策略 π*。
- en: '**When You’re Not Given an MDP**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**当你没有 MDP 时**'
- en: Policy iteration is great and all, but it only works when we have a given MDP.
    The MDP essentially tells you how the environment works, which realistically is
    not going to be given in real world scenarios. When not given an MDP, we use model
    free methods that go directly from the experience/interactions of the agents and
    the environment to the value functions and policies. We’re going to be doing the
    same steps of policy evaluation and policy improvement, just without the information
    given by the MDP.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 策略迭代非常有效，但它仅在我们拥有一个给定的 MDP 时才有效。MDP 实质上告诉你环境如何运作，而在现实世界中，这些信息往往不会被提供。当没有给定 MDP
    时，我们使用模型无关的方法，这些方法直接从代理与环境的经验/互动中得到价值函数和策略。我们将执行相同的策略评估和策略改进步骤，只是没有 MDP 提供的信息。
- en: The way we do this is instead of improving our policy by optimizing over the
    state value function, we’re going to optimize over the action value function Q.
    Remember how we decomposed the state value function into the sum of immediate
    reward and value function of the successor state? Well, we can do the same with
    our Q function.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法是，通过优化状态价值函数来改进策略，而是优化动作价值函数 Q。记得我们是如何将状态价值函数分解为即时奖励和后继状态价值函数的总和的吗？我们可以对
    Q 函数做同样的处理。
- en: '![](../Images/655495d0bbcdba93e724a2c83da5b86c.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/655495d0bbcdba93e724a2c83da5b86c.png)'
- en: Now, we’re going to go through the same process of policy evaluation and policy
    improvement, except we replace our state value function V with our action value
    function Q. Now, I’m going to skip over the details of what changes with the evaluation/improvement
    steps. To understand MDP free evaluation and improvement methods, topics such
    as Monte Carlo Learning, Temporal Difference Learning, and SARSA would require
    whole blogs just themselves (If you are interested, though, please take a listen
    to David Silver’s [Lecture 4](https://www.youtube.com/watch?v=PnHCvfgC_ZA) and [Lecture
    5](https://www.youtube.com/watch?v=0g4j2k_Ggc4)). Right now, however, I’m going
    to jump ahead to value function approximation and the methods discussed in the
    AlphaGo and Atari Papers, and hopefully that should give a taste of modern RL
    techniques. **The main takeaway is that we want to find the optimal policy π* that
    maximizes our action value function Q.**
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将经历相同的策略评估和策略改进过程，只是我们将状态值函数V替换为动作值函数Q。现在，我将跳过关于评估/改进步骤的具体变化。要理解MDP自由评估和改进方法，如蒙特卡罗学习、时间差分学习和SARSA等主题将需要整个博客来讨论（不过，如果你感兴趣的话，请听听David
    Silver的[Lecture 4](https://www.youtube.com/watch?v=PnHCvfgC_ZA)和[Lecture 5](https://www.youtube.com/watch?v=0g4j2k_Ggc4)）。现在，我将直接跳到价值函数逼近以及AlphaGo和Atari论文中讨论的方法，希望这能给你一些现代RL技术的概述。**主要的收获是我们想找到能最大化我们的动作值函数Q的最优策略π***。
- en: '**Value Function Approximation**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**价值函数逼近**'
- en: So, if you think about everything we’ve learned up until this point, we’ve treated
    our problem in a relatively simplistic way. Look at the above Q equation. We’re
    taking in a specific state S and action A, and then computing a number that basically
    tells us what the expected return is. Now let’s imagine that our agent moves 1
    millimeter to the right. This means we have a whole new state S’, and now we’re
    going to have to compute a Q value for that. In real world RL problems, there
    are millions and millions of states so it’s important that our value functions
    understand generalization in that we don’t have to store a completely separate
    value function for every possible state. The solution is to use a **Q value function
    approximation **that is able to generalize to unknown states.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你回顾我们到目前为止所学的内容，我们以一种相对简单的方式处理了问题。看看上面的Q方程。我们在接受特定状态S和动作A之后，计算一个基本上告诉我们期望回报的数字。现在假设我们的代理移动了1毫米到右边。这意味着我们有了一个全新的状态S’，现在我们需要为这个新状态计算Q值。在现实世界的RL问题中，有数以百万计的状态，因此我们的价值函数需要理解泛化，即我们不需要为每个可能的状态存储一个完全不同的价值函数。解决方案是使用**Q值函数逼近**，它能够泛化到未知状态。
- en: So, what we want is some function, let’s call is Qhat, that gives a rough approximation
    of the Q value given some state S and some action A.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们想要的是一个函数，我们称之为Qhat，它能够在给定某个状态S和某个动作A的情况下，提供Q值的大致估计。
- en: '![](../Images/96caca4b660041c06c312793e33f7876.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96caca4b660041c06c312793e33f7876.png)'
- en: This function is going to take in S, A, and a good old weight vector W (Once
    you see that W, you already know we’re bringing in some gradient descent *). It
    is going to compute the dot product between x (which is just a feature vector
    that represents S and A) and W. The way we’re going to improve this function is
    by calculating the loss between the true Q value (let’s just assume that it’s
    given to us for now) and the output of the approximate function.*
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将接受S、A和一个老旧的权重向量W（看到W你就会知道我们要引入一些梯度下降*）。它将计算x（即表示S和A的特征向量）与W之间的点积。我们改进这个函数的方式是通过计算真实Q值（假设现在它是已知的）与逼近函数输出之间的损失*。
- en: '*![](../Images/61c07904b90863d6af0b7ae4b910caac.png)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*![](../Images/61c07904b90863d6af0b7ae4b910caac.png)'
- en: After we compute the loss, we use gradient descent to find the minimum value,
    at which point we will have our optimal W vector. This idea of function approximation
    is going to be very key when taking a look at the papers a little later.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算损失之后，我们使用梯度下降来找到最小值，此时我们将获得最优的W向量。这个函数逼近的想法在稍后查看论文时将非常关键。
- en: '**Just One More Thing**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**再说一件事**'
- en: Before getting to the papers, just wanted to touch on one last thing. An interesting
    discussion with the topic of reinforcement learning is that of exploration vs
    exploitation. **Exploitation** is the agent’s process of taking what it already
    knows, and then making the actions that it knows will produce the maximum reward.
    This sounds great, right? The agent will always be making the best action based
    on its current knowledge. However, there is a key phrase in that statement. *Current
    knowledge*. If the agent hasn’t explored enough of the state space, it can’t possibly
    know whether it is really taking the best possible action. This idea of taking
    actions with the main purpose of exploring the state space is called **exploration**.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论论文之前，想提一件最后的事。一个有趣的强化学习话题是探索与利用。**利用** 是代理利用已知的知识，然后采取它知道会产生最大奖励的行动。这听起来不错，对吧？代理将总是根据当前知识做出最佳行动。然而，这句话中有一个关键短语。*当前知识*。如果代理没有足够地探索状态空间，它不可能知道是否真的在采取最佳行动。以探索状态空间为主要目的的行动被称为**探索**。
- en: This idea can be easily related to a real world example. Let’s say you have
    a choice of what restaurant to eat at tonight. You (acting as the agent) know
    that you like Mexican food, so in RL terms, going to a Mexican restaurant will
    be the action that maximizes your reward, or happiness/satisfaction in this case.
    However, there is also a choice of Italian food, which you’ve never had before.
    There’s a possibility that it could be better than Mexican food, or could be a
    lot worse. This tradeoff between whether to exploit an agent’s past knowledge
    vs trying something new in hope of discovering a greater reward is one of the
    major challenges in reinforcement learning (and in our daily lives tbh).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法很容易与现实生活中的例子相关联。假设你今晚要选择去哪个餐馆就餐。你（作为代理）知道自己喜欢墨西哥菜，所以在强化学习术语中，去墨西哥餐馆将是一个能最大化你奖励或幸福/满足感的行动。然而，还有一个意大利菜的选择，你以前从未尝试过。它可能比墨西哥菜更好，也可能差很多。这种在利用代理的过去知识和尝试新事物以期发现更大奖励之间的权衡，是强化学习中的一个主要挑战（在我们的日常生活中也是如此）。
- en: '**Other Resources for Learning RL**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**其他强化学习资源**'
- en: Phew. That was a lot of info. By no means, however, was that a comprehensive
    overview of the field. If you’d like a more in-depth overview of RL, I’d strongly
    recommend these resources.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，这信息量很大。不过，这绝不是该领域的全面概述。如果你想要更深入的强化学习概述，我强烈推荐这些资源。
- en: David Silver (from Deepmind) Reinforcement Learning [Video Lectures](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: David Silver（来自Deepmind）强化学习 [视频讲座](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT)
- en: My [personal notes](https://docs.google.com/document/d/1TjmYDOxQzOQ0jd0lUiFOVQ1hNAmtwKfSAACck9dR7a8/edit?usp=sharing) from
    the RL course
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我在强化学习课程中的 [个人笔记](https://docs.google.com/document/d/1TjmYDOxQzOQ0jd0lUiFOVQ1hNAmtwKfSAACck9dR7a8/edit?usp=sharing)
- en: Sutton and Barto’s [Reinforcement Learning Textbook](https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf) (This
    is really the holy grail if you are determined to learn the ins and outs of this
    subfield)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 和 Barto 的 [强化学习教材](https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf)（如果你决定深入学习这个子领域，这真的是终极宝典）
- en: Andrej Karpathy’s [Blog Post](http://karpathy.github.io/2016/05/31/rl/) on RL
    (Start with this one if you want to ease into RL and want to see a really well
    done practical example)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrej Karpathy 的 [博客文章](http://karpathy.github.io/2016/05/31/rl/) 关于强化学习（如果你想轻松入门强化学习，并查看一个非常出色的实践例子，可以从这篇开始）
- en: '[UC Berkeley CS 188](http://ai.berkeley.edu/lecture_videos.html) Lectures 8-11'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[UC Berkeley CS 188](http://ai.berkeley.edu/lecture_videos.html) 讲座 8-11'
- en: '[Open AI Gym](https://gym.openai.com/): When you feel comfortable with RL,
    try creating your own agents with this reinforcement learning toolkit that Open
    AI created'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Open AI Gym](https://gym.openai.com/): 当你对强化学习感到舒适时，尝试用Open AI创建的这个强化学习工具包创建自己的代理'
- en: '* * *'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的快车道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT'
- en: '* * *'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[HuggingFace Has Launched a Free Deep Reinforcement Learning Course](https://www.kdnuggets.com/2022/05/huggingface-launched-free-deep-reinforcement-learning-course.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HuggingFace推出了免费的深度强化学习课程](https://www.kdnuggets.com/2022/05/huggingface-launched-free-deep-reinforcement-learning-course.html)'
- en: '[AI, Analytics, Machine Learning, Data Science, Deep Learning…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能、分析、机器学习、数据科学、深度学习……](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
- en: '[Hands-on Reinforcement Learning Course Part 3: SARSA](https://www.kdnuggets.com/2022/01/handson-reinforcement-learning-course-part-3-sarsa.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实践强化学习课程第3部分：SARSA](https://www.kdnuggets.com/2022/01/handson-reinforcement-learning-course-part-3-sarsa.html)'
- en: '[Hands-On Reinforcement Learning Course, Part 1](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-course-part-1.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实践强化学习课程，第1部分](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-course-part-1.html)'
- en: '[Hands-On Reinforcement Learning Course, Part 2](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-part-2.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实践强化学习课程，第2部分](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-part-2.html)'
- en: '[Reinforcement Learning for Newbies](https://www.kdnuggets.com/2022/05/reinforcement-learning-newbies.html)*'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[强化学习新手指南](https://www.kdnuggets.com/2022/05/reinforcement-learning-newbies.html)*'
