- en: How to Use GPT for Generating Creative Content with Hugging Face Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GPT 生成创意内容的方法
- en: 原文：[https://www.kdnuggets.com/how-to-use-gpt-for-generating-creative-content-with-hugging-face-transformers](https://www.kdnuggets.com/how-to-use-gpt-for-generating-creative-content-with-hugging-face-transformers)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/how-to-use-gpt-for-generating-creative-content-with-hugging-face-transformers](https://www.kdnuggets.com/how-to-use-gpt-for-generating-creative-content-with-hugging-face-transformers)
- en: '![How to Use GPT for Generating Creative Content with Hugging Face Transformers](../Images/7c2590f4db8e141e3008c7f98ba82fe5.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![如何使用 GPT 生成创意内容与 Hugging Face Transformers](../Images/7c2590f4db8e141e3008c7f98ba82fe5.png)'
- en: '## Introduction'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '## 介绍'
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的 IT 工作'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: GPT, short for Generative Pre-trained Transformer, is a family of transformer-based
    language models. Known as an example of an early transformer-based model capable
    of generating coherent text, OpenAI's GPT-2 was one of the initial triumphs of
    its kind, and can be used as a tool for a variety of applications, including helping
    write content in a more creative way. The Hugging Face Transformers library is
    a library of pretrained models that simplifies working with these sophisticated
    language models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: GPT，即生成预训练变换器，是一类基于变换器的语言模型。作为一种早期基于变换器的模型，它能够生成连贯的文本，OpenAI 的 GPT-2 是此类模型的初期成功案例之一，可用于各种应用，包括以更具创意的方式撰写内容。Hugging
    Face Transformers 库是一个预训练模型库，它简化了与这些复杂语言模型的交互。
- en: The generation of creative content could be valuable, for example, in the world
    of data science and machine learning, where it might be used in a variety of ways
    to spruce up dull reports, create synthetic data, or simply help to guide the
    telling of a more interesting story. This tutorial will guide you through using
    GPT-2 with the Hugging Face Transformers library to generate creative content.
    Note that we use the GPT-2 model here for its simplicity and manageable size,
    but swapping it out for another generative model will follow the same steps.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 创意内容的生成可能会很有价值，例如，在数据科学和机器学习的世界中，它可以用于各种方式来美化枯燥的报告、创建合成数据，或简单地帮助讲述更有趣的故事。本教程将指导你如何使用
    GPT-2 和 Hugging Face Transformers 库来生成创意内容。请注意，我们在这里使用 GPT-2 模型是由于其简便和适中的大小，但替换为其他生成模型将遵循相同的步骤。
- en: Setting Up the Environment
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置环境
- en: Before getting started, we need to set up our environment. This will involve
    installing and importing the necessary libraries and importing the required packages.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们需要设置我们的环境。这将涉及安装和导入必要的库及所需的包。
- en: 'Install the necessary libraries:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 安装必要的库：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the required packages:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的包：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can learn about Huging Face Auto Classes and AutoModels [here](https://huggingface.co/docs/transformers/model_doc/auto).
    Moving on.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [这里](https://huggingface.co/docs/transformers/model_doc/auto) 了解 Hugging
    Face 的自动类和自动模型。继续阅读。
- en: Loading the Model and Tokenizer
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载模型和分词器
- en: Next, we will load the model and tokenizer in our script. The model in this
    case is GPT-2, while the tokenizer is responsible for converting text into a format
    that the model can understand.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在脚本中加载模型和分词器。在这个案例中，模型是 GPT-2，而分词器负责将文本转换为模型可以理解的格式。
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that changing the model_name above can swap in different Hugging Face language
    models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，更改上面的 model_name 可以切换不同的 Hugging Face 语言模型。
- en: Preparing Input Text for Generation
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为生成准备输入文本
- en: In order to have our model generate text, we need to provide the model with
    an initial input, or prompt. This prompt will be tokenized by the tokenizer.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的模型生成文本，我们需要提供模型一个初始输入或提示。这个提示将被分词器进行分词处理。
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that the `return_tensors='pt'` argument ensures that PyTorch tensors are
    returned.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`return_tensors='pt'` 参数确保返回 PyTorch 张量。
- en: Generating Creative Content
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成创造性内容
- en: Once the input text has been tokenized and prepared for input into the model,
    we can then use the model to generate creative content.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦输入文本被标记化并准备好输入模型，我们就可以使用模型生成创造性内容。
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Customizing Generation with Advanced Settings
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用高级设置自定义生成
- en: For added creativity, we can adjust the temperature and use top-k sampling and
    top-p (nucleus) sampling.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加创造性，我们可以调整温度，并使用 top-k 采样和 top-p（核）采样。
- en: 'Adjusting the temperature:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 调整温度：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Using top-k sampling and top-p sampling:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 top-k 采样和 top-p 采样：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Practical Examples of Creative Content Generation
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创造性内容生成的实际示例
- en: Here are some practical examples of using GPT-2 to generate creative content.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些使用 GPT-2 生成创造性内容的实际示例。
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Summary
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Experimenting with different parameters and settings can significantly impact
    the quality and creativity of the generated content. GPT, especially the newer
    versions of which we are all aware, has tremendous potential in creative fields,
    enabling data scientists to generate engaging narratives, synthetic data, and
    more. For further reading, consider exploring the Hugging Face documentation and
    other resources to deepen your understanding and expand your skills.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过尝试不同的参数和设置可以显著影响生成内容的质量和创造性。GPT，特别是我们都熟知的较新版本，在创造性领域具有巨大的潜力，使数据科学家能够生成引人入胜的叙述、合成数据等。进一步阅读可考虑探索
    Hugging Face 文档及其他资源，以加深理解和扩展技能。
- en: By following this guide, you should now be able to harness the power of GPT-3
    and Hugging Face Transformers to generate creative content for various applications
    in data science and beyond.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循本指南后，你现在应该能够利用 GPT-3 和 Hugging Face Transformers 的强大功能，为数据科学及其他领域生成创造性内容。
- en: 'For additional information on these topics, check out the following resources:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多相关信息，请查看以下资源：
- en: '[Hugging Face Transformers Documentation](https://huggingface.co/transformers/)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hugging Face Transformers 文档](https://huggingface.co/transformers/)'
- en: '[PyTorch Documentation](https://pytorch.org/docs/stable/index.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch 文档](https://pytorch.org/docs/stable/index.html)'
- en: '[Generative Pre-trained Transformer (Wikipedia)](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[生成预训练变换器（维基百科）](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer)'
- en: '[](https://www.linkedin.com/in/mattmayo13/)****[Matthew Mayo](https://www.kdnuggets.com/wp-content/uploads/./profile-pic.jpg)****
    ([**@mattmayo13**](https://twitter.com/mattmayo13)) holds a master''s degree in
    computer science and a graduate diploma in data mining. As managing editor of
    [KDnuggets](https://www.kdnuggets.com/) & [Statology](https://www.statology.org/),
    and contributing editor at [Machine Learning Mastery](https://machinelearningmastery.com/),
    Matthew aims to make complex data science concepts accessible. His professional
    interests include natural language processing, language models, machine learning
    algorithms, and exploring emerging AI. He is driven by a mission to democratize
    knowledge in the data science community. Matthew has been coding since he was
    6 years old.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.linkedin.com/in/mattmayo13/)****[Matthew Mayo](https://www.kdnuggets.com/wp-content/uploads/./profile-pic.jpg)****
    ([**@mattmayo13**](https://twitter.com/mattmayo13)) 拥有计算机科学硕士学位和数据挖掘研究生文凭。作为 [KDnuggets](https://www.kdnuggets.com/)
    和 [Statology](https://www.statology.org/) 的主编，以及 [Machine Learning Mastery](https://machinelearningmastery.com/)
    的贡献编辑，Matthew 旨在使复杂的数据科学概念变得易于理解。他的职业兴趣包括自然语言处理、语言模型、机器学习算法以及探索新兴的人工智能。他致力于在数据科学社区中普及知识。Matthew
    从6岁起便开始编程。'
- en: More On This Topic
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[How to Fine-Tune BERT for Sentiment Analysis with Hugging Face Transformers](https://www.kdnuggets.com/how-to-fine-tune-bert-sentiment-analysis-hugging-face-transformers)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 Hugging Face Transformers 微调 BERT 进行情感分析](https://www.kdnuggets.com/how-to-fine-tune-bert-sentiment-analysis-hugging-face-transformers)'
- en: '[Building a Recommendation System with Hugging Face Transformers](https://www.kdnuggets.com/building-a-recommendation-system-with-hugging-face-transformers)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Hugging Face Transformers 构建推荐系统](https://www.kdnuggets.com/building-a-recommendation-system-with-hugging-face-transformers)'
- en: '[Using Hugging Face Transformers for Emotion Detection in Text](https://www.kdnuggets.com/using-hugging-face-transformers-for-emotion-detection-in-text)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Hugging Face Transformers 进行文本情感检测](https://www.kdnuggets.com/using-hugging-face-transformers-for-emotion-detection-in-text)'
- en: '[How to Build and Train a Transformer Model from Scratch with…](https://www.kdnuggets.com/how-to-build-and-train-a-transformer-model-from-scratch-with-hugging-face-transformers)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何从头开始构建和训练一个 Transformer 模型…](https://www.kdnuggets.com/how-to-build-and-train-a-transformer-model-from-scratch-with-hugging-face-transformers)'
- en: '[How to Translate Languages with MarianMT and Hugging Face Transformers](https://www.kdnuggets.com/how-to-translate-languages-with-marianmt-and-hugging-face-transformers)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 MarianMT 和 Hugging Face Transformers 进行语言翻译](https://www.kdnuggets.com/how-to-translate-languages-with-marianmt-and-hugging-face-transformers)'
- en: '[Meet Gorilla: UC Berkeley and Microsoft’s API-Augmented LLM…](https://www.kdnuggets.com/2023/06/meet-gorilla-uc-berkeley-microsoft-apiaugmented-llm-outperforms-gpt4-chatgpt-claude.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[认识 Gorilla：加州大学伯克利分校和微软的 API 增强型 LLM…](https://www.kdnuggets.com/2023/06/meet-gorilla-uc-berkeley-microsoft-apiaugmented-llm-outperforms-gpt4-chatgpt-claude.html)'
