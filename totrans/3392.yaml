- en: How to Rank 10% in Your First Kaggle Competition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Outlier**'
  prefs: []
  type: TYPE_NORMAL
- en: '[![Outlier Example](../Images/70eadff4905dbdee3a23ff64ef3219a8.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example_outlier.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The plot shows some scaled coordinates data. We can see that there are some
    outliers in the top-right corner. Exclude them and the distribution looks good.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dummy Variables**'
  prefs: []
  type: TYPE_NORMAL
- en: For categorical variables, a common practice is **[One-hot Encoding](https://en.wikipedia.org/wiki/One-hot)**.
    For a categorical variable with `n` possible values, we create a group of `n` dummy
    variables. Suppose a record in the data takes one value for this variable, then
    the corresponding dummy variable is set to `1` while other dummies in the same
    group are all set to `0`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![Dummies Example](../Images/f7891877228b4993c0d084ec1c553387.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example-dummies.png)'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we transform `DayOfWeek` into 7 dummy variables.
  prefs: []
  type: TYPE_NORMAL
- en: Note that when the categorical variable can take many values (hundreds or more),
    this might not work well. It’s difficult to find a general solution to that, but
    I’ll discuss one scenario in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature Engineering**'
  prefs: []
  type: TYPE_NORMAL
- en: Some describe the essence of Kaggle competitions as **feature engineering supplemented
    by model tuning and ensemble learning**. Yes, that makes a lot of sense. **Feature
    engineering gets your very far.** Yet it is how well you know about the domain
    of given data that decides how far you can go. For example, in a competition where
    data is mainly consisted of texts, Natural Language Processing teachniques are
    a must. The approach of constructing useful features is something we all have
    to continuously learn in order to do better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, **when you feel that a variable is intuitively useful for the task,
    you can include it as a feature**. But how do you know it actually works? The
    simplest way is to plot it against the target variable like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![Checking Feature Validity](../Images/d9b85e951f68e16d26e5899549ae2247.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-vis_check_feat.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature Selection**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally speaking, **we should try to craft as many features as we can and
    have faith in the model’s ability to pick up the most significant features**.
    Yet there’s still something to gain from feature selection beforehand:'
  prefs: []
  type: TYPE_NORMAL
- en: Less features mean faster training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some features are linearly related to others. This might put a strain on the
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By picking up the most important features, we can use interactions between them
    as new features. Sometimes this gives surprising improvement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The simplest way to inspect feature importance is by fitting a random forest
    model. There are more robust feature selection algorithms (e.g. [this](http://jmlr.org/papers/volume10/tuv09a/tuv09a.pdf))
    which are theoretically superior but not practicable due to the absence of efficient
    implementation. You can combat noisy data (to an extent) simply by increasing
    number of trees used in a random forest.
  prefs: []
  type: TYPE_NORMAL
- en: This is important for competitions in which data is **[anonymized](https://en.wikipedia.org/wiki/Data_anonymization)** because
    you won’t waste time trying to figure out the meaning of a variable that’s of
    no significance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature Encoding**'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes raw features have to be converted to some other formats for them to
    work properly.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose we have a categorical variable which can take more than
    10K different values. Then naively creating dummy variables is not a feasible
    option. An acceptable solution is to create dummy variables for only a subset
    of the values (e.g. values that constitute 95% of the feature importance) and
    assign everything else to an ‘others’ class.
  prefs: []
  type: TYPE_NORMAL
- en: '**Updated on Oct 28th, 2016: **For the scenario described above, another possible
    solution is to use **Factorized Machines**. Please refer to [this post](https://www.kaggle.com/c/expedia-hotel-recommendations/forums/t/21607/1st-place-solution-summary) by
    Kaggle user “idle_speculation” for details.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Selection**'
  prefs: []
  type: TYPE_NORMAL
- en: 'When the features are set, we can start training models. Kaggle competitions
    usually favor**tree-based models**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient Boosted Trees**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random Forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extra Randomized Trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following models are slightly worse in terms of general performance, but
    are suitable as base models in ensemble learning (will be discussed later):'
  prefs: []
  type: TYPE_NORMAL
- en: SVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural Networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that this does not apply to computer vision competitions which are pretty
    much dominated by neural network models.
  prefs: []
  type: TYPE_NORMAL
- en: All these models are implemented in **[Sklearn](http://scikit-learn.org/)**.
  prefs: []
  type: TYPE_NORMAL
- en: Here I want to emphasize the greatness of **[Xgboost](https://github.com/dmlc/xgboost)**.
    The outstanding performance of gradient boosted trees and Xgboost’s efficient
    implementation makes it very popular in Kaggle competitions. Nowadays almost every
    winner uses Xgboost in one way or another.
  prefs: []
  type: TYPE_NORMAL
- en: '**Updated on Oct 28th, 2016: **Recently Microsoft open sourced **[LightGBM](https://github.com/Microsoft/LightGBM)**,
    a potentially better library than Xgboost for gradient boosting.'
  prefs: []
  type: TYPE_NORMAL
- en: By the way, for Windows users installing Xgboost could be a painstaking process.
    You can refer to [this post](https://dnc1994.com/2016/03/installing-xgboost-on-windows/) by
    me if you run into problems.
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How LinkedIn Uses Machine Learning To Rank Your Feed](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Get Your First Job in Data Science without Any Work Experience](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[It''s alive! Build your first robots with Python and some cheap,…](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[From Zero to Hero: Create Your First ML Model with PyTorch](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deploying Your First Machine Learning Model](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
