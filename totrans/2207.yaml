- en: 'Overview of PEFT: State-of-the-art Parameter-Efficient Fine-Tuning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/overview-of-peft-stateoftheart-parameterefficient-finetuning](https://www.kdnuggets.com/overview-of-peft-stateoftheart-parameterefficient-finetuning)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Overview of PEFT: State-of-the-art Parameter-Efficient Fine-Tuning](../Images/aff0878bf10b35d37fe07ff658443928.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: What is PEFT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: As large language models (LLMs) such as GPT-3.5, LLaMA2, and PaLM2 grow ever
    larger in scale, fine-tuning them on downstream natural language processing (NLP)
    tasks becomes increasingly computationally expensive and memory intensive.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter-Efficient Fine-Tuning (PEFT) methods address these issues by only
    fine-tuning a small number of extra parameters while freezing most of the pretrained
    model. This prevents catastrophic forgetting in large models and enables fine-tuning
    with limited compute.
  prefs: []
  type: TYPE_NORMAL
- en: PEFT has proven effective for tasks like image classification and text generation
    while using just a fraction of the parameters. The small tuned weights can simply
    be added to the original pretrained weights.
  prefs: []
  type: TYPE_NORMAL
- en: You can even fine tune LLMs on the free version of Google Colab using 4-bit
    quantization and PEFT techniques QLoRA.
  prefs: []
  type: TYPE_NORMAL
- en: The modular nature of PEFT also allows the same pretrained model to be adapted
    for multiple tasks by adding small task-specific weights, avoiding the need to
    store full copies.
  prefs: []
  type: TYPE_NORMAL
- en: The [PEFT](https://github.com/huggingface/peft) library integrates popular PEFT
    techniques like LoRA, Prefix Tuning, AdaLoRA, Prompt Tuning, MultiTask Prompt
    Tuning, and LoHa with Transformers and Accelerate. This provides easy access to
    cutting-edge large language models with efficient and scalable fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: What is LoRA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this tutorial, we will be using the most popular parameter-efficient fine-tuning
    (PEFT) technique called [LoRA](https://arxiv.org/abs/2106.09685) (Low-Rank Adaptation
    of Large Language Models). LoRA is a technique that significantly speeds up the
    fine-tuning process of large language models while consuming less memory.
  prefs: []
  type: TYPE_NORMAL
- en: The key idea behind LoRA is to represent weight updates using two smaller matrices
    achieved through low-rank decomposition. These matrices can be trained to adapt
    to new data while minimizing the overall number of modifications. The original
    weight matrix remains unchanged and doesn't undergo any further adjustments. The
    final results are obtained by combining both the original and the adapted weights.
  prefs: []
  type: TYPE_NORMAL
- en: There are several advantages to using LoRA. Firstly, it greatly enhances the
    efficiency of fine-tuning by reducing the number of trainable parameters. Additionally,
    LoRA is compatible with various other parameter-efficient methods and can be combined
    with them. Models fine-tuned using LoRA demonstrate performance comparable to
    fully fine-tuned models.  Importantly, LoRA doesn't introduce any additional inference
    latency since adapter weights can be seamlessly merged with the base model.
  prefs: []
  type: TYPE_NORMAL
- en: Use Cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many use cases of PEFT, from language models to Image classifiers.
    You can check all of the use case tutorials on official [documentation](https://huggingface.co/docs/peft/task_guides/image_classification_lora).
  prefs: []
  type: TYPE_NORMAL
- en: '[StackLLaMA: A hands-on guide to train LLaMA with RLHF](https://huggingface.co/blog/stackllama)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Finetune-opt-bnb-peft](https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o?usp=sharing)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Efficient flan-t5-xxl training with LoRA and Hugging Face](https://www.philschmid.de/fine-tune-flan-t5-peft)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[DreamBooth fine-tuning with LoRA](https://huggingface.co/docs/peft/task_guides/dreambooth_lora)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Image classification using LoRA](https://huggingface.co/docs/peft/task_guides/image_classification_lora)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training the LLMs using PEFT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to load and wrap our transformer model using
    the `bitsandbytes` and `peft` library. We will also cover loading the saved fine-tuned
    QLoRA model and running inferences with it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we will install all the necessary libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Then, we will import the essential modules and name the base model ([Llama-2-7b-chat-hf](https://huggingface.co/NousResearch/Llama-2-7b-chat-hf))
    to fine-tune it using the [mlabonne/guanaco-llama2-1k](https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k)
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: PEFT Configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Create PEFT configuration that we will use to wrap or train our model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 4 bit Quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Loading LLMs on consumer or Colab GPUs poses significant challenges. However,
    we can overcome this issue by implementing a 4-bit quantization technique with
    an NF4 type configuration using BitsAndBytes. By employing this approach, we can
    effectively load our model, thereby conserving memory and preventing machine crashes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Wrapping Base Transformers Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To make our model parameter efficient, we will wrap the base transformer model
    using `get_peft_model`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Our trainable parameters are fewer than those of the base model, allowing us
    to use less memory and fine-tune the model faster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to train the model. You can do that by following the [4-bit
    quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
    guide.
  prefs: []
  type: TYPE_NORMAL
- en: Saving the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After training, you can either save the model adopter locally.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Or, push it to the Hugging Face Hub.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the model adopter is just 134MB whereas the base LLaMA 2 7B model
    is around 13GB.
  prefs: []
  type: TYPE_NORMAL
- en: '![Overview of PEFT: State-of-the-art Parameter-Efficient Fine-Tuning](../Images/980ded1e7c785ce7884c611929a51e7e.png)'
  prefs: []
  type: TYPE_IMG
- en: Loading the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To run the model Inference, we have to first load the model using 4-bit precision
    quantization and then merge trained PEFT weights with the base (LlaMA 2) model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For running the inference we have to write the prompt in guanaco-llama2-1k dataset
    style(*“<s>[INST] {prompt} [/INST]”*). Otherwise you will get responses in different
    languages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The output seems perfect.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**Note:** If you are facing difficulties while loading the model in Colab,
    you can check out my notebook: [Overview of PEFT](https://colab.research.google.com/drive/1qh6pAJczmC1GOw0ES-zVjPmFzspikq0m?usp=sharing).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parameter-Efficient Fine-Tuning techniques like LoRA enable efficient fine-tuning
    of large language models using only a fraction of parameters. This avoids expensive
    full fine-tuning and enables training with limited compute resources. The modular
    nature of PEFT allows adapting models for multiple tasks. Quantization methods
    like 4-bit precision can further reduce memory usage. Overall, PEFT opens up large
    language model capabilities to a much wider audience.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.polywork.com/kingabzpro)****[Abid Ali Awan](https://www.polywork.com/kingabzpro)****
    ([@1abidaliawan](https://www.linkedin.com/in/1abidaliawan)) is a certified data
    scientist professional who loves building machine learning models. Currently,
    he is focusing on content creation and writing technical blogs on machine learning
    and data science technologies. Abid holds a Master''s degree in technology management
    and a bachelor''s degree in telecommunication engineering. His vision is to build
    an AI product using a graph neural network for students struggling with mental
    illness.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Explainable Forecasting and Nowcasting with State-of-the-art Deep…](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[RAG vs Finetuning: Which Is the Best Tool to Boost Your LLM Application?](https://www.kdnuggets.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Current State of Data Science Careers](https://www.kdnuggets.com/2022/10/current-state-data-science-careers.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, November 2: The Current State of Data Science…](https://www.kdnuggets.com/2022/n43.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Feature Selection: Where Science Meets Art](https://www.kdnuggets.com/2021/12/feature-selection-science-meets-art.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Art of Prompt Engineering: Decoding ChatGPT](https://www.kdnuggets.com/2023/06/art-prompt-engineering-decoding-chatgpt.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
