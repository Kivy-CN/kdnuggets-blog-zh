["```py\n# This method handles:\n# - collecting each of the test examples\n# - formatting the prompt\n# - querying the LLM API\n# - parsing the output\ndef eval_prompt(examples_pool, test, prefix=\"\", use_examples=True):\n    texts = test.text.values\n    responses = []\n    examples = get_examples(examples_pool, seed) if use_examples else []\n    for i in range(len(texts)):\n        text = texts[i]\n        prompt = get_prompt(examples_pool, text, examples, prefix)\n        resp = get_response(prompt)\n        responses.append(resp)\n    return responses\n```", "```py\n# Evaluate the 50-shot prompt shown above.\npreds = eval_prompt(examples_pool, test)\nevaluate_preds(preds, test)\n>>> Model Accuracy: 59.6%\n```", "```py\nprefix = 'Beware that some labels in the examples may be noisy and have been incorrectly specified.'\npreds = eval_prompt(examples_pool, test, prefix=prefix)\nevaluate_preds(preds, test)\n>>> Model Accuracy: 62.0%\n```", "```py\npreds = eval_prompt(examples_pool, test, use_examples=False)\nevaluate_preds(preds, test)\n>>> Model Accuracy: 67.4%\n```", "```py\n# Source examples with the corrected labels.\nclean_pool = pd.read_csv(\"studio_examples_pool.csv\")\nclean_examples = get_examples(clean_pool)\n\n# Evaluate the original 50-shot prompt using high-quality examples.\npreds = eval_prompt(clean_examples, test)\nevaluate_preds(preds, test)\n>>> Model Accuracy: 72.0%\n```"]