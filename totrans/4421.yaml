- en: Which methods should be used for solving linear regression?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决线性回归应该使用哪些方法？
- en: 原文：[https://www.kdnuggets.com/2020/09/solving-linear-regression.html](https://www.kdnuggets.com/2020/09/solving-linear-regression.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/09/solving-linear-regression.html](https://www.kdnuggets.com/2020/09/solving-linear-regression.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Ahmad Bin Shafiq](https://medium.com/@ahmadbinshafiq), Machine Learning
    Student**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Ahmad Bin Shafiq](https://medium.com/@ahmadbinshafiq)，机器学习学生**。'
- en: Linear Regression is a supervised machine learning algorithm. It predicts a linear
    relationship between an independent variable (y), based on the given dependant
    variables (x), such that the independent variable (y) has the **lowest cost**.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是一种监督式机器学习算法。它预测一个独立变量（y）与给定的依赖变量（x）之间的线性关系，以使得独立变量（y）具有**最低成本**。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的 IT 需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Different approaches to solve linear regression models
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决线性回归模型的不同方法
- en: There are many different methods that we can apply to our linear regression
    model in order to make it more efficient. But we will discuss the most common
    of them here.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对线性回归模型应用许多不同的方法以提高其效率。但在这里，我们将讨论其中最常见的一些。
- en: Gradient Descent
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度下降
- en: Least Square Method / Normal Equation Method
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最小二乘法 / 正规方程法
- en: Adams Method
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 亚当方法
- en: Singular Value Decomposition (SVD)
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 奇异值分解（SVD）
- en: Okay, so let’s begin…
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们开始吧……
- en: Gradient Descent
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度下降
- en: One of the most common and easiest methods for beginners to solve linear regression
    problems is gradient descent.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于初学者而言，解决线性回归问题最常见且最简单的方法之一就是梯度下降。
- en: '**How Gradient Descent works**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度下降的工作原理**'
- en: Now, let's suppose we have our data plotted out in the form of a scatter graph,
    and when we apply a cost function to it, our model will make a prediction. Now
    this prediction can be very good, or it can be far away from our ideal prediction
    (meaning its cost will be high). So, in order to minimize that cost (error), we
    apply gradient descent to it.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们的数据以散点图的形式绘制出来，当我们对其应用一个成本函数时，我们的模型会做出预测。这个预测可能非常好，也可能与我们的理想预测相距甚远（即成本很高）。因此，为了最小化这个成本（错误），我们应用梯度下降。
- en: Now, gradient descent will slowly converge our hypothesis towards a global minimum,
    where the **cost** would be lowest. In doing so, we have to manually set the value
    of **alpha, **and the slope of the hypothesis changes with respect to our alpha’s
    value. If the value of alpha is large, then it will take big steps. Otherwise,
    in the case of small alpha, our hypothesis would converge slowly and through small
    baby steps.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，梯度下降会逐渐使我们的假设收敛到全局最小值，在那里**成本**最低。在此过程中，我们必须手动设置**alpha**的值，且假设的斜率会根据alpha的值而变化。如果alpha的值很大，那么梯度下降将会采取大的步伐。否则，在小的alpha情况下，我们的假设将会慢慢收敛，通过小的步骤。
- en: '![](../Images/05c736b21cefb5041ec721443fcb0400.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05c736b21cefb5041ec721443fcb0400.png)'
- en: '*Hypothesis converging towards a global minimum. Image from [Medium](https://medium.com/@ahmadbinshafiq/linear-regression-simplified-for-beginners-dcd3afe0b23f).*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设收敛到全局最小值。图片来源于 [Medium](https://medium.com/@ahmadbinshafiq/linear-regression-simplified-for-beginners-dcd3afe0b23f)。*'
- en: The Equation for Gradient Descent is
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的方程是
- en: '![](../Images/c61371274f47e7d11e00674987d93d89.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c61371274f47e7d11e00674987d93d89.png)'
- en: '*Source: [Ruder.io](https://ruder.io/optimizing-gradient-descent/index.html#batchgradientdescent).*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*来源：[Ruder.io](https://ruder.io/optimizing-gradient-descent/index.html#batchgradientdescent)。*'
- en: '**Implementing Gradient Descent in Python**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**在 Python 中实现梯度下降**'
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/2134bd14e00ac4b5f024dfa631837691.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2134bd14e00ac4b5f024dfa631837691.png)'
- en: '*Model after Gradient Descent.*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*梯度下降后的模型。*'
- en: Here first, we have created our dataset, and then we looped over all our training
    examples in order to minimize our cost of hypothesis.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建了我们的数据集，然后遍历所有训练样本，以最小化假设的成本。
- en: '**Pros:**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点：**'
- en: Important advantages of Gradient Descent are
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降法的重要优点包括
- en: Less Computational Cost as compared to SVD or ADAM
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相比于SVD或ADAM，计算成本较低
- en: Running time is O(kn²)
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行时间为O(kn²)
- en: Works well with more number of features
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在特征数量较多的情况下效果很好
- en: '**Cons:**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点：**'
- en: Important cons of Gradient Descent are
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降法的重要缺点是
- en: Need to choose some learning rate **α**
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要选择一些学习率**α**
- en: Needs many iterations to converge
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要多次迭代才能收敛
- en: Can be stuck in Local Minima
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能会陷入局部最小值
- en: If not proper Learning Rate **α**, then it might not converge.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果学习率**α**不合适，可能无法收敛。
- en: Least Square Method
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最小二乘法
- en: The least-square method, also known as the **normal equation,** is also one
    of the most common approaches to solving linear regression models easily. But,
    this one needs to have some basic knowledge of linear algebra.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最小二乘法，也称为**正规方程**，是解决线性回归模型的最常见方法之一。但这个方法需要对线性代数有一些基本了解。
- en: '**How the least square method works**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**最小二乘法的工作原理**'
- en: In normal LSM, we solve directly for the value of our coefficient. In short,
    in one step, we reach our optical minimum point, or we can say only in one step
    we fit our hypothesis to our data with the lowest cost possible.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在普通LSM中，我们直接求解系数的值。简而言之，通过一步，我们达到光学最小点，或者我们可以说仅一步我们将假设拟合到数据中，成本最低。
- en: '![](../Images/de26c7903434f41e202cdfdc438bfa58.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de26c7903434f41e202cdfdc438bfa58.png)'
- en: '*Before and after applying LSM to our dataset. Image from [Medium](https://towardsdatascience.com/complete-guide-to-linear-regression-in-python-d95175447255).*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*在应用LSM之前和之后的数据集。图片来自 [Medium](https://towardsdatascience.com/complete-guide-to-linear-regression-in-python-d95175447255)。*'
- en: The equation for LSM is
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: LSM的方程是
- en: '![](../Images/46f6be2f627273f634faebbbd21e17ee.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46f6be2f627273f634faebbbd21e17ee.png)'
- en: '**Implementing LSM in Python**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**在Python中实现LSM**'
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/37cf628c5f022a6500bd42f2a2e07d28.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37cf628c5f022a6500bd42f2a2e07d28.png)'
- en: Here first we have created our dataset and then minimized the cost of our hypothesis
    using the
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们创建了我们的数据集，然后使用最小二乘法最小化我们的假设成本。
- en: '*b = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*b = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)*'
- en: code, which is equivalent to our equation.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 代码，与我们的方程式等效。
- en: '**Pros:**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点：**'
- en: 'Important advantages of LSM are:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: LSM的重要优点包括：
- en: No Learning Rate
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无需学习率
- en: No Iterations
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无需迭代
- en: Feature Scaling Not Necessary
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征缩放不是必需的
- en: Works really well when the Number of Features is less.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在特征数量较少时效果非常好。
- en: '**Cons:**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点：**'
- en: 'Important cons are:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的缺点有：
- en: Is computationally expensive when the dataset is big.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据集很大时计算成本较高。
- en: Slow when Number of Features is more
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当特征数量较多时较慢
- en: Running Time is O(n³)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行时间为O(n³)
- en: Sometimes, your X transpose X is non-invertible, i.e., a singular matrix with
    no inverse. You can use *np.linalg.pinv* instead of *np.linalg.inv* to overcome
    this problem.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时你的X转置X是不可逆的，即没有逆的奇异矩阵。你可以使用*np.linalg.pinv*代替*np.linalg.inv*来克服这个问题。
- en: Adam’s Method
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Adam方法
- en: ADAM, which stands for Adaptive Moment Estimation, is an optimization algorithm
    that is widely used in Deep Learning.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ADAM，即自适应矩估计，是一种在深度学习中广泛使用的优化算法。
- en: It is an iterative algorithm that works well on noisy data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个迭代算法，对噪声数据效果很好。
- en: It is the combination of RMSProp and Mini-batch Gradient Descent algorithms.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这是RMSProp和小批量梯度下降算法的结合。
- en: In addition to storing an exponentially decaying average of past squared gradients
    like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of
    past gradients, similar to momentum.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 除了像Adadelta和RMSprop一样存储过去平方梯度的指数衰减平均值外，Adam还保留了过去梯度的指数衰减平均值，类似于动量。
- en: 'We compute the decaying averages of past and past squared gradients respectively
    as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算过去和过去平方梯度的衰减平均值如下：
- en: '![](../Images/d8d0ce73722f8ff946b44afce3bed82b.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8d0ce73722f8ff946b44afce3bed82b.png)'
- en: '*Credit: [Ruder.io](https://ruder.io/optimizing-gradient-descent/index.html#adam).*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*致谢： [Ruder.io](https://ruder.io/optimizing-gradient-descent/index.html#adam)。*'
- en: As *mt* and *vt* are initialized as vectors of 0’s, the authors of Adam observe
    that they are biased towards zero, especially during the initial time steps, and
    especially when the decay rates are small (i.e., β1β1 and β2β2 are close to 1).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于*mt*和*vt*初始化为0向量，Adam的作者观察到它们在初始时间步骤特别偏向于零，尤其是当衰减率较小（即β1β1和β2β2接近1）时。
- en: 'They counteract these biases by computing bias-corrected first and second-moment
    estimates:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b1d19391beb6059d447bedfd93aac64.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: '*Credit: [Ruder.io](https://ruder.io/optimizing-gradient-descent/#adam).*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'They then update the parameters with:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72bdfcb71d105944ac3fc874029e46e4.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: '*Credit: [Ruder.io](https://ruder.io/optimizing-gradient-descent/#adam).*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: You can learn the theory behind Adam [here](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c) or [here](https://ruder.io/optimizing-gradient-descent/#adam).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '**Pseudocode for Adam** is'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7d9cefe677a4dec1b2411846d9b89eb.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: '*Source: [Arxiv Adam](https://arxiv.org/pdf/1412.6980v9.pdf).*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see it’s code in Pure Python.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now Let’s find the actual graph of Linear Regression and values for slope and
    intercept for our dataset.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/052dd2e3a15d0ee688af908dd00bfd1a.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: Now let us see the Linear Regression line using the Seaborn *regplot *function.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/17b5a51be7715cb2d005f456aebaff9b.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: Let us code Adam Optimizer now in pure Python.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we have implemented all the equations mentioned in the pseudocode above
    using an object-oriented approach and some helper functions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Let us now set the hyperparameters for our model.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: And finally, the training process.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/edf2910abbf73579207f6226f5774d44.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: Now, if we compare the *Final theta* values to the slope and intercept values,
    calculated earlier using *scipy.stats.mstat.linregress*, they are almost 99% equal
    and can be 100% equal by adjusting the hyperparameters.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let us plot it.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/35e7578a0e22a853df15714a5103be67.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: And we can see that our plot is similar to plot obtained using *sns.regplot*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros:**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Straightforward to implement.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computationally efficient.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Little memory requirements.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invariant to diagonal rescale of the gradients.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Well suited for problems that are large in terms of data and/or parameters.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appropriate for non-stationary objectives.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appropriate for problems with very noisy/or sparse gradients.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyper-parameters have intuitive interpretation and typically require little
    tuning.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons:**'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Adam and RMSProp are highly sensitive to certain values of the learning rate
    (and, sometimes, other hyper-parameters like the batch size), and they can catastrophically
    fail to converge if e.g., the learning rate is too high. (Source: [stackexchange](https://ai.stackexchange.com/questions/11455/when-should-we-use-algorithms-like-adam-as-opposed-to-sgd))
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singular Value Decomposition
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Singular value decomposition shortened as SVD is one of the famous and most
    widely used dimensionality reduction methods in linear regression.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: SVD is used (amongst other uses) as a preprocessing step to reduce the number
    of dimensions for our learning algorithm. SVD decomposes a matrix into a product
    of three other matrices (U, S, V).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/99d90e99fc9d0ec6c74a128cb72ae814.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: Once our matrix has been decomposed, the coefficients for our hypothesis can
    be found by calculating the pseudoinverse of the input matrix **X** and multiplying
    that by the output vector **y**. After that, we fit our hypothesis to our data,
    and that gives us the lowest cost.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的矩阵被分解，假设的系数可以通过计算输入矩阵 **X** 的伪逆并将其乘以输出向量 **y** 来找到。之后，我们将假设拟合到数据中，这将给我们最低的成本。
- en: '**Implementing SVD in Python**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**在 Python 中实现奇异值分解**'
- en: '[PRE9]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/0474bf1c816645c5a8f9d172b29df722.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0474bf1c816645c5a8f9d172b29df722.png)'
- en: Though it is not converged very well, it is still pretty good.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它没有很好地收敛，但仍然相当不错。
- en: Here first, we have created our dataset and then minimized the cost of our hypothesis
    using b = np.linalg.pinv(X).dot(y), which is the equation for SVD.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建了数据集，然后使用 `b = np.linalg.pinv(X).dot(y)` 这个 SVD 方程来最小化假设的成本。
- en: '**Pros:**'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点：**'
- en: Works better with higher dimensional data
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在高维数据上效果更好
- en: Good for gaussian type distributed data
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于高斯类型分布的数据
- en: Really stable and efficient for a small dataset
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于小数据集非常稳定和高效
- en: While solving linear equations for linear regression, it is more stable and
    the preferred approach.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决线性回归的线性方程时，这种方法更稳定，也是首选方法。
- en: '**Cons:**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点：**'
- en: Running time is O(n³)
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行时间为 O(n³)
- en: Multiple risk factors
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多重风险因素
- en: Really sensitive to outliers
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对异常值非常敏感
- en: May get unstable with a very large dataset
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能在非常大的数据集上变得不稳定
- en: Learning Outcome
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习成果
- en: As of now, we have learned and implemented gradient descent, LSM, ADAM, and
    SVD. And now, we have a very good understanding of all of these algorithms, and
    we also know what are the pros and cons.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习并实现了梯度下降、最小二乘法、ADAM 和奇异值分解。现在，我们对这些算法有了很好的理解，也知道了它们的优缺点。
- en: One thing we noticed was that the ADAM optimization algorithm was the most accurate,
    and according to the actual ADAM research paper, ADAM outperforms almost all other
    optimization algorithms.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，ADAM 优化算法是最准确的，根据实际的 ADAM 研究论文，ADAM 的表现优于几乎所有其他优化算法。
- en: '**Related:**'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Linear to Logistic Regression, Explained Step by Step](https://www.kdnuggets.com/2020/03/linear-logistic-regression-explained.html)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性到逻辑回归，逐步解释](https://www.kdnuggets.com/2020/03/linear-logistic-regression-explained.html)'
- en: '[A Beginner’s Guide to Linear Regression in Python with Scikit-Learn](https://www.kdnuggets.com/2019/03/beginners-guide-linear-regression-python-scikit-learn.html)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Scikit-Learn 进行线性回归的初学者指南](https://www.kdnuggets.com/2019/03/beginners-guide-linear-regression-python-scikit-learn.html)'
- en: '[Linear Regression In Real Life](https://www.kdnuggets.com/2018/08/linear-regression-real-life.html)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性回归在现实生活中](https://www.kdnuggets.com/2018/08/linear-regression-real-life.html)'
- en: More On This Topic
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[3 Reasons Why You Should Use Linear Regression Models Instead of…](https://www.kdnuggets.com/2021/08/3-reasons-linear-regression-instead-neural-networks.html)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你应该使用线性回归模型而不是……的 3 个理由](https://www.kdnuggets.com/2021/08/3-reasons-linear-regression-instead-neural-networks.html)'
- en: '[Which Metric Should I Use? Accuracy vs. AUC](https://www.kdnuggets.com/2022/10/metric-accuracy-auc.html)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我应该使用哪个指标？准确率与 AUC](https://www.kdnuggets.com/2022/10/metric-accuracy-auc.html)'
- en: '[Comparing Linear and Logistic Regression](https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[比较线性回归与逻辑回归](https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html)'
- en: '[Linear vs Logistic Regression: A Succinct Explanation](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性回归与逻辑回归：简洁的解释](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
- en: '[KDnuggets News 22:n12, March 23: Best Data Science Books for…](https://www.kdnuggets.com/2022/n12.html)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻 22:n12, 3 月 23 日：最佳数据科学书籍](https://www.kdnuggets.com/2022/n12.html)'
- en: '[Linear Regression for Data Science](https://www.kdnuggets.com/2022/07/linear-regression-data-science.html)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中的线性回归](https://www.kdnuggets.com/2022/07/linear-regression-data-science.html)'
