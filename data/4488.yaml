- en: The Benefits & Examples of Using Apache Spark with PySpark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Apache Spark 和 PySpark 的好处与示例
- en: 原文：[https://www.kdnuggets.com/2020/04/benefits-apache-spark-pyspark.html](https://www.kdnuggets.com/2020/04/benefits-apache-spark-pyspark.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/04/benefits-apache-spark-pyspark.html](https://www.kdnuggets.com/2020/04/benefits-apache-spark-pyspark.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**What is Apache Spark? **'
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**什么是 Apache Spark?**'
- en: '[Apache Spark](https://spark.apache.org/) is one of the hottest new trends
    in the technology domain. It is the framework with probably the highest potential
    to realize the fruit of the marriage between Big Data and Machine Learning.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[Apache Spark](https://spark.apache.org/)是技术领域中最炙手可热的趋势之一。它可能是实现大数据与机器学习结合成果的潜力最大的框架。'
- en: It runs fast (up to 100x faster than traditional [Hadoop MapReduce](https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm) due
    to in-memory operation, offers robust, distributed, fault-tolerant data objects
    (called [RDD](https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.htm)),
    and integrates beautifully with the world of machine learning and graph analytics
    through supplementary packages like [Mlib](https://spark.apache.org/mllib/) and [GraphX](https://spark.apache.org/graphx/).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 它运行速度很快（比传统的[Hadoop MapReduce](https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm)快最多100倍，因其内存操作），提供强大、分布式、容错的数据对象（称为[RDD](https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.htm)），并通过像[Mlib](https://spark.apache.org/mllib/)和[GraphX](https://spark.apache.org/graphx/)等附加包与机器学习和图分析领域完美集成。
- en: '* * *'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您组织的 IT'
- en: '* * *'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![The Benefits & Examples of Using Apache Spark with PySpark in Python | Apache
    Spark](../Images/30cb3911f9df0322fd922f31ff23852a.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Apache Spark 和 PySpark 在 Python 中的好处与示例 | Apache Spark](../Images/30cb3911f9df0322fd922f31ff23852a.png)'
- en: Spark is implemented on [Hadoop/HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html) and
    written mostly in [Scala](https://www.scala-lang.org/), a functional programming
    language, similar to Java. In fact, Scala needs the latest Java installation on
    your system and runs on JVM. However, for most beginners, Scala is not a language
    that they learn first to venture into the world of data science. Fortunately,
    Spark provides a wonderful Python integration, called **PySpark**, which lets
    Python programmers to interface with the Spark framework and learn how to manipulate
    data at scale and work with objects and algorithms over a distributed file system.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是在[Hadoop/HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html)上实现的，主要用[Scala](https://www.scala-lang.org/)编写，Scala
    是一种类似于 Java 的函数式编程语言。事实上，Scala 需要系统上最新的 Java 安装，并运行在 JVM 上。然而，对于大多数初学者来说，Scala
    不是他们学习数据科学领域的第一语言。幸运的是，Spark 提供了一个极好的 Python 集成，称为**PySpark**，它允许 Python 程序员与
    Spark 框架进行接口，并学习如何在大规模下处理数据以及在分布式文件系统上处理对象和算法。
- en: In this article, we will learn the basics of PySpark. There are a lot of concepts
    (constantly evolving and introduced), and therefore, we just focus on fundamentals
    with a few simple examples. Readers are encouraged to build on these and explore
    more on their own.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将学习 PySpark 的基础知识。虽然有很多概念（不断发展和引入），因此，我们仅关注基础知识，并通过几个简单的示例进行讲解。鼓励读者在此基础上进一步探索。
- en: '**The Short History of Apache Spark**'
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**Apache Spark 的简短历史**'
- en: Apache Spark started as a research project at the UC Berkeley AMPLab in 2009,
    and was open sourced in early 2010\. It was a class project at UC Berkeley. Idea
    was to build a cluster management framework, which can support different kinds
    of cluster computing systems. Many of the ideas behind the system were presented
    in various research papers over the years. After being released, Spark grew into
    a broad developer community, and moved to the Apache Software Foundation in 2013\.
    Today, the project is developed collaboratively by a community of hundreds of
    developers from hundreds of organizations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 起初是 UC Berkeley AMPLab 在 2009 年的一个研究项目，并在 2010 年初开源。它是 UC Berkeley
    的一个课程项目。其理念是构建一个集群管理框架，能够支持不同类型的集群计算系统。多年来，系统背后的许多理念在各种研究论文中得到了展示。发布后，Spark 发展成为一个庞大的开发者社区，并在
    2013 年迁移至 Apache 软件基金会。如今，该项目由来自数百个组织的数百名开发者共同开发。
- en: '**Spark is Not a Programming Language**'
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**Spark 不是编程语言**'
- en: One thing to remember is that Spark is not a programming language like Python
    or Java. It is a general-purpose distributed data processing engine, suitable
    for use in a wide range of circumstances. It is particularly useful for big data
    processing both at scale and with high speed.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的一点是，Spark 不是像 Python 或 Java 这样的编程语言。它是一个通用的分布式数据处理引擎，适用于各种情况。它特别适合大数据处理，无论是大规模还是高速度的处理。
- en: Application developers and data scientists generally incorporate Spark into
    their applications to rapidly query, analyze, and transform data at scale. Some
    of the tasks that are most frequently associated with Spark, include, – ETL and
    SQL batch jobs across large data sets (often of terabytes of size), – processing
    of streaming data from IoT devices and nodes, data from various sensors, financial
    and transactional systems of all kinds, and – machine learning tasks for e-commerce
    or IT applications.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序开发者和数据科学家通常将 Spark 集成到他们的应用程序中，以快速查询、分析和转换大规模数据。与 Spark 最常关联的一些任务包括：– 对大数据集（通常是几个
    TB 大小）进行 ETL 和 SQL 批处理作业，– 处理来自 IoT 设备和节点的流数据、各种传感器的数据、各类金融和事务系统的数据，以及 – 用于电子商务或
    IT 应用的机器学习任务。
- en: 'At its core, Spark builds on top of the Hadoop/HDFS framework for handling
    distributed files. It is mostly implemented with Scala, a functional language
    variant of Java. There is a core Spark data processing engine, but on top of that,
    there are many libraries developed for SQL-type query analysis, distributed machine
    learning, large-scale graph computation, and streaming data processing. Multiple
    programming languages are supported by Spark in the form of easy interface libraries:
    Java, Python, Scala, and R.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，Spark 构建在 Hadoop/HDFS 框架之上，用于处理分布式文件。它主要用 Scala 实现，Scala 是 Java 的一种函数式语言变体。Spark
    拥有一个核心的数据处理引擎，但在其之上，还有许多用于 SQL 类型查询分析、分布式机器学习、大规模图计算和流数据处理的库。Spark 支持多种编程语言，通过易于使用的接口库提供：Java、Python、Scala
    和 R。
- en: '**Spark Uses the MapReduce Paradigm for Distributed Processing**'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**Spark 使用 MapReduce 范式进行分布式处理**'
- en: The basic idea of distributed processing is to divide the data chunks into small
    manageable pieces (including some filtering and sorting), bring the computation
    close to the data i.e. use small nodes of a large cluster for specific jobs and
    then re-combine them back. The dividing portion is called the ‘Map’ action and
    the recombination is called the ‘Reduce’ action. Together, they make the famous
    ‘MapReduce’ paradigm, which was introduced by Google around 2004 (see the [original
    paper here](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式处理的基本思想是将数据块划分为小的可管理部分（包括一些过滤和排序），将计算靠近数据，即使用大型集群中的小节点进行特定作业，然后再将它们重新组合。划分部分称为“Map”操作，重新组合部分称为“Reduce”操作。它们共同形成了著名的“MapReduce”范式，该范式由
    Google 于 2004 年左右引入（请参见[原始论文](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)）。
- en: For example, if a file has 100 records to be processed, 100 mappers can run
    together to process one record each. Or maybe 50 mappers can run together to process
    two records each. After all the mappers complete processing, the framework shuffles
    and sorts the results before passing them on to the reducers. A reducer cannot
    start while a mapper is still in progress. All the map output values that have
    the same key are assigned to a single reducer, which then aggregates the values
    for that key.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一个文件有 100 条记录需要处理，100 个映射器可以一起运行来处理每条记录。或者 50 个映射器可以一起运行来处理每两个记录。所有映射器完成处理后，框架会在将结果传递给归约器之前对其进行洗牌和排序。一个归约器在映射器仍在进行时无法启动。所有具有相同键的映射输出值都会分配给一个归约器，该归约器随后会聚合该键的值。
- en: '**How to Setup PySpark**'
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**如何设置 PySpark**'
- en: If you’re already familiar with Python and libraries such as Pandas and Numpy,
    then PySpark is a great extension/framework to learn in order to create more scalable,
    data-intensive analyses and pipelines by utilizing the power of Spark in the background.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经熟悉 Python 及 Pandas 和 Numpy 等库，那么 PySpark 是一个很好的扩展/框架，可以通过利用后台 Spark 的强大功能来创建更具可扩展性的数据密集型分析和管道。
- en: The exact process of installing and setting up PySpark environment (on a standalone
    machine) is somewhat involved and can vary slightly depending on your system and
    environment. The goal is to get your regular Jupyter data science environment
    working with Spark in the background using the PySpark package.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 安装和设置 PySpark 环境（在独立机器上）的具体过程相当复杂，可能会因系统和环境的不同而有所变化。目标是使你常规的 Jupyter 数据科学环境在后台使用
    PySpark 包来运行 Spark。
- en: '[**This article**](https://medium.com/free-code-camp/how-to-set-up-pyspark-for-your-jupyter-notebook-7399dd3cb389) on
    Medium provides more details on the step-by-step setup process.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[**这篇文章**](https://medium.com/free-code-camp/how-to-set-up-pyspark-for-your-jupyter-notebook-7399dd3cb389)
    提供了有关逐步设置过程的更多细节。'
- en: '![](../Images/881536150793aa5d361af9092960c036.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/881536150793aa5d361af9092960c036.png)'
- en: Alternatively, you can use Databricks setup for practicing Spark. This company
    was created by the original creators of Spark and have an excellent ready-to-launch
    environment to do distributed analysis with Spark.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以使用 Databricks 设置来练习 Spark。这家公司由 Spark 的原始创建者创建，拥有一个优秀的即开即用环境来进行分布式分析。
- en: But the idea is always the same. You are distributing (and replicating) your
    large dataset in small fixed chunks over many nodes. You then bring the compute
    engine close to them so that the whole operation is parallelized, fault-tolerant
    and scalable.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 但思想总是相同的。你是将大型数据集分成许多固定的小块在多个节点上进行分发（和复制）。然后将计算引擎靠近这些节点，从而使整个操作并行化、容错并且可扩展。
- en: By working with PySpark and Jupyter notebook, you can learn all these concepts
    without spending anything on AWS or Databricks platform. You can also easily interface
    with SparkSQL and MLlib for database manipulation and machine learning. It will
    be much easier to start working with real-life large clusters if you have internalized
    these concepts beforehand!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 PySpark 和 Jupyter notebook，你可以学习所有这些概念而无需在 AWS 或 Databricks 平台上花费任何费用。你还可以轻松地与
    SparkSQL 和 MLlib 进行接口以进行数据库操作和机器学习。如果你事先掌握了这些概念，那么开始使用现实世界的大型集群将会容易得多！
- en: '**Resilient Distributed Dataset (RDD) and SparkContext**'
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**弹性分布式数据集 (RDD) 和 SparkContext**'
- en: Many Spark programs revolve around the concept of a resilient distributed dataset
    (RDD), which is a fault-tolerant collection of elements that can be operated on
    in parallel. SparkContext resides in the Driver program and manages the distributed
    data over the worker nodes through the cluster manager. The good thing about using
    PySpark is that all this complexity of data partitioning and task management is
    handled automatically at the back and the programmer can focus on the specific
    analytics or machine learning job itself.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 Spark 程序围绕弹性分布式数据集（RDD）的概念展开，RDD 是一个容错的元素集合，可以并行操作。SparkContext 存在于 Driver
    程序中，并通过集群管理器管理分布式数据。使用 PySpark 的好处是数据分区和任务管理的复杂性在后台自动处理，程序员可以专注于具体的分析或机器学习任务。
- en: '![The Benefits & Examples of Using Apache Spark with PySpark in Python | Rdd-1](../Images/5bb0f6532941bb2fa5a40915e804a5df.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Apache Spark 与 PySpark 在 Python 中的好处与示例 | Rdd-1](../Images/5bb0f6532941bb2fa5a40915e804a5df.png)'
- en: '*rdd-1*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*rdd-1*'
- en: There are two ways to create RDDs–parallelizing an existing collection in your
    driver program, or referencing a dataset in an external storage system, such as
    a shared file- system, HDFS, HBase, or any data source offering a Hadoop InputFormat.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 RDD 有两种方式——在你的驱动程序中并行化现有集合，或引用外部存储系统中的数据集，如共享文件系统、HDFS、HBase 或任何提供 Hadoop
    InputFormat 的数据源。
- en: For illustration with a Python-based approach, we will give examples of the
    first type here. We can create a simple Python array of 20 random integers (between
    0 and 10), using Numpy random.randint(), and then create an RDD object as following,
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了用基于 Python 的方法进行说明，我们将在这里给出第一种类型的示例。我们可以使用 Numpy random.randint() 创建一个包含 20
    个随机整数（介于 0 和 10 之间）的简单 Python 数组，然后按照以下方式创建 RDD 对象，
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***Note the ‘4’ in the argument*****. It denotes 4 computing cores (in your
    local machine) to be used for this ****SparkContext**** object**. If we check
    the type of the RDD object, we get the following,'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '***注意参数中的‘4’*****。它表示为这个 ****SparkContext**** 对象使用4个计算核心（在你的本地机器上）。如果我们检查RDD对象的类型，我们会得到以下结果，'
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Opposite to parallelization is the collection (with collect()) which brings
    all the distributed elements and returns them to the head node.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与并行化相对的是收集（使用 collect()），它将所有分布式元素带回头节点。
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: But A is no longer a simple Numpy array. We can use the glom() method to check
    how the partitions are created.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 但是 A 不再是简单的 Numpy 数组。我们可以使用 glom() 方法来检查分区是如何创建的。
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now stop the SC and reinitialize it with 2 cores and see what happens when you
    repeat the process.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在停止 SC，并用 2 个核心重新初始化，看看当你重复这个过程时会发生什么。
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '***The RDD is now distributed over two chunks, not four! ***'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '***RDD 现在分布在两个块上，而不是四个块！***'
- en: '**You have learned about the first step in distributed data analytics i.e. controlling
    how your data is partitioned over smaller chunks for further processing**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**你已经了解了分布式数据分析的第一步，即控制数据如何被划分为更小的块以便进一步处理**'
- en: '**Some Examples of Basic Operations with RDD & PySpark**'
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**RDD 和 PySpark 的基本操作示例**'
- en: '**Count the elements**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算元素个数**'
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**The first element (****first****) and the first few elements (****take****)**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一个元素（****第一个****）和前几个元素（****取****）**'
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Removing duplicates with using ****distinct**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 ****distinct** 进行去重'
- en: '*NOTE*: This operation requires a **shuffle** in order to detect duplication
    across partitions. So, it is a slow operation. Don’t overdo it.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意*: 这个操作需要一个**shuffle**以便检测分区间的重复。因此，它是一个较慢的操作。不要过度使用它。'
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**To sum all the elements use ****reduce**** method**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**要对所有元素求和，使用 ****reduce**** 方法**'
- en: Note the use of a lambda function in this,
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意其中使用了 lambda 函数，
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Or the direct ****sum()**** method**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**或者直接使用 ****sum()**** 方法**'
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Finding maximum element by ****reduce**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过 ****reduce** 查找最大元素'
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Finding longest word in a blob of text**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**在一段文本中查找最长单词**'
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Use ****filter**** for logic-based filtering**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 ****filter**** 进行基于逻辑的过滤**'
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Write regular Python functions to use with ****reduce()**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**编写常规 Python 函数以配合 ****reduce()**'
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note here the x < y does a lexicographic comparison and determines that Macintosh is
    larger than computers!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里 x < y 进行的是词典序比较，并确定 Macintosh 大于 computers！
- en: '**Mapping operation with a lambda function with PySpark**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 PySpark 的 lambda 函数的映射操作**'
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Mapping with a regular Python function in PySpark**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**在 PySpark 中使用常规 Python 函数进行映射**'
- en: '[PRE15]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**groupby**** returns a RDD of grouped elements (iterable) as per a given group
    operation**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**groupby**** 返回一个根据给定分组操作的分组元素（可迭代）RDD**'
- en: In the following example, we use a list-comprehension along with the groupby to
    create a list of two elements, each having a header (the result of the lambda
    function, simple modulo 2 here), and a sorted list of the elements which gave
    rise to that result. You can imagine easily that this kind of seperation can come
    particularly handy for processing data which needs to be binned/canned out based
    on particular operation performed over them.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们使用列表推导式结合 groupby 创建一个包含两个元素的列表，每个元素都有一个标题（lambda 函数的结果，这里是简单的模 2），以及生成该结果的元素的排序列表。你可以很容易地想象这种分离在需要根据特定操作进行分箱/处理的数据处理时特别有用。
- en: '[PRE16]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Using ****histogram**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 ****histogram**'
- en: The histogram() method takes a list of bins/buckets and returns a tuple with
    result of the histogram (binning),
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: histogram() 方法接受一个桶/箱的列表，并返回一个包含直方图（分箱）结果的元组，
- en: '[PRE17]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Set operations**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**集合操作**'
- en: You can also do regular set operations on RDDs like – union(), intersection(), subtract(),
    or cartesian().
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在 RDD 上进行常规集合操作，如 union()、 intersection()、 subtract() 或 cartesian()。
- en: Check out [**this Jupyter notebook**](https://github.com/tirthajyoti/Spark-with-Python/blob/master/SparkContext%20and%20RDD%20Basics.ipynb) for
    more examples.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[**这个Jupyter笔记本**](https://github.com/tirthajyoti/Spark-with-Python/blob/master/SparkContext%20and%20RDD%20Basics.ipynb)以获取更多示例。
- en: '**Lazy evaluation with PySpark (and *****Caching*****)**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**PySpark中的惰性评估（以及**Caching**）**'
- en: Lazy evaluation is an evaluation/computation strategy which prepares a detailed
    step-by-step internal map of the execution pipeline for a computing task, but
    delays the final execution until when it is absolutely needed. This strategy is
    at the heart of Spark for speeding up many parallelized Big Data operations.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 惰性评估是一种评估/计算策略，它为计算任务准备了详细的逐步内部执行流程图，但会将最终执行推迟到绝对需要时。这个策略是Spark加速许多并行大数据操作的核心。
- en: Let’s use two CPU cores for this example,
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用两个CPU核心来做这个例子，
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Make a RDD with 1 million elements**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**创建一个包含100万元素的RDD**'
- en: '[PRE19]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**Some computing function – ****taketime**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**某个计算函数 – **taketime**'
- en: '[PRE20]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Check how much time is taken by taketime function**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**检查taketime函数所需的时间**'
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Remember this result, the taketime() function took a wall time of 31.5 us. Of
    course, the exact number will depend on the machine you are working on.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这个结果，taketime()函数的实际时间为31.5微秒。当然，具体的数字会依赖于你所使用的机器。
- en: '**Now do the map operation on the function**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**现在对函数进行映射操作**'
- en: '[PRE22]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '*How come each **taketime** function takes 45.8 us but the map operation with
    a 1 million elements RDD also took similar time?*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么每个**taketime**函数都需要45.8微秒，但一个包含100万元素的RDD的映射操作也花费了类似的时间？*'
- en: '**Because of lazy evaluation i.e. nothing was computed in the previous step,
    just a plan of execution was made**. The variable interim does not point to a
    data structure, instead it points to a plan of execution, expressed as a dependency
    graph. The dependency graph defines how RDDs are computed from each other.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**由于惰性评估，即在上一步中没有进行任何计算，仅仅制定了执行计划**。变量interim并不指向数据结构，而是指向一个执行计划，表示为依赖图。依赖图定义了RDD如何相互计算。'
- en: '**The actual execution by ****reduce**** method**'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过**reduce**方法的实际执行**'
- en: '[PRE23]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: So, the wall time here is 15.6 seconds. Remember, the taketime() function had
    a wall time of 31.5 us? Therefore, we expect the total time to be on the order
    of ~ 31 seconds for a 1-million array. Because of parallel operation on two cores,
    it took ~ 15 seconds.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这里的实际时间为15.6秒。记住，taketime()函数的时间是31.5微秒？因此，我们预计1百万数组的总时间约为31秒。由于在两个核心上并行操作，它花费了约15秒。
- en: Now, we have not saved (materialized) any intermediate results in interim, so
    another simple operation (e.g. counting elements > 0) will take almost same time.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们没有在interim中保存（物化）任何中间结果，所以另一个简单操作（例如计数元素> 0）将花费几乎相同的时间。
- en: '[PRE24]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '**Caching to reduce computation time on similar operation (spending memory)**'
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**缓存以减少类似操作的计算时间（消耗内存）**'
- en: Remember the dependency graph that we built in the previous step? We can run
    the same computation as before with cache method to tell the dependency graph
    to plan for caching.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 记住我们在上一步中构建的依赖图吗？我们可以使用缓存方法进行相同的计算，以便告诉依赖图规划缓存。
- en: '[PRE25]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The first computation will not improve, but it caches the interim result,
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次计算不会有所改进，但它缓存了中间结果，
- en: '[PRE26]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Now run the same filter method with the help of cached result,
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用缓存结果运行相同的过滤方法，
- en: '[PRE27]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Wow! The compute time came down to less than a second from 12 seconds earlier!
    This way, caching and parallelization with lazy excution, is the core feature
    of programming with Spark.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！计算时间从之前的12秒降到了不到1秒！通过这种方式，使用缓存和惰性执行的并行化是使用Spark编程的核心特性。
- en: '**Dataframe and SparkSQL**'
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**DataFrame和SparkSQL**'
- en: Apart from the RDD, the second key data structure in the Spark framework, is
    the *DataFrame*. If you have done work with Python Pandas or R DataFrame, the
    concept may seem familiar.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 除了RDD，Spark框架中的第二个关键数据结构是*DataFrame*。如果你有使用Python Pandas或R DataFrame的经验，这个概念可能很熟悉。
- en: 'A DataFrame is a distributed collection of rows under named columns. It is
    conceptually equivalent to a table in a relational database, an Excel sheet with
    Column headers, or a data frame in R/Python, but with richer optimizations under
    the hood. DataFrames can be constructed from a wide array of sources such as:
    structured data files, tables in Hive, external databases, or existing RDDs. It
    also shares some common characteristics with RDD:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame是按命名列组织的分布式行集合。它在概念上等同于关系数据库中的表、带有列头的Excel表格或R/Python中的数据框，但在底层具有更丰富的优化。DataFrame可以从各种来源构建，如：结构化数据文件、Hive中的表、外部数据库或现有的RDD。它还与RDD共享一些共同的特性：
- en: 'Immutable in nature : We can create DataFrame / RDD once but can’t change it.
    And we can transform a DataFrame / RDD after applying transformations.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本质上是不可变的：我们可以创建DataFrame / RDD，但不能更改它。我们可以在应用转换后对DataFrame / RDD进行转换。
- en: 'Lazy Evaluations: Which means that a task is not executed until an action is
    performed. Distributed: RDD and DataFrame both are distributed in nature.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 惰性求值：这意味着在执行操作之前不会执行任务。分布式：RDD和DataFrame本质上都是分布式的。
- en: '![The Benefits & Examples of Using Apache Spark with PySpark in Python | DataFrame
    in PySpark](../Images/197e1ce09b894ea1628c7da1c8d3c87d.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![使用Apache Spark与PySpark在Python中的好处和示例 | PySpark中的DataFrame](../Images/197e1ce09b894ea1628c7da1c8d3c87d.png)'
- en: '**Advantages of the DataFrame**'
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**DataFrame的优点**'
- en: DataFrames are designed for processing large collection of structured or semi-structured
    data.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame设计用于处理大量结构化或半结构化数据。
- en: Observations in Spark DataFrame are organised under named columns, which helps
    Apache Spark to understand the schema of a DataFrame. This helps Spark optimize
    execution plan on these queries.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark DataFrame中的观察结果按命名列组织，这帮助Apache Spark理解DataFrame的模式。这有助于Spark优化这些查询的执行计划。
- en: DataFrame in Apache Spark has the ability to handle petabytes of data.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark中的DataFrame能够处理PB级别的数据。
- en: DataFrame has a support for wide range of data format and sources.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame支持多种数据格式和数据源。
- en: It has API support for different languages like Python, R, Scala, Java.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持Python、R、Scala、Java等不同语言的API。
- en: '**DataFrame basics example**'
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**DataFrame基础示例**'
- en: For fundamentals and typical usage examples of DataFrames, please see the following
    Jupyter Notebooks,
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 有关DataFrame的基础知识和典型使用示例，请参见以下Jupyter Notebooks，
- en: '[**Spark DataFrame basics**](https://github.com/tirthajyoti/Spark-with-Python/blob/master/Dataframe_basics.ipynb)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Spark DataFrame基础**](https://github.com/tirthajyoti/Spark-with-Python/blob/master/Dataframe_basics.ipynb)'
- en: '[**Spark DataFrame operations**](https://github.com/tirthajyoti/Spark-with-Python/blob/masterDataFrame_operations_basics.ipynb)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Spark DataFrame操作**](https://github.com/tirthajyoti/Spark-with-Python/blob/master/DataFrame_operations_basics.ipynb)'
- en: '**SparkSQL Helps to Bridge the Gap for PySpark**'
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**SparkSQL帮助弥合PySpark的差距**'
- en: Relational data stores are easy to build and query. Users and developers often
    prefer writing easy-to-interpret, declarative queries in a human-like readable
    language such as SQL. However, as data starts increasing in volume and variety,
    the relational approach does not scale well enough for building Big Data applications
    and analytical systems.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 关系型数据存储易于构建和查询。用户和开发者通常更喜欢用易于理解的声明性查询语言，如SQL。然而，随着数据量和数据种类的增加，关系型方法在构建大数据应用和分析系统时的扩展性不足。
- en: We have had success in the domain of Big Data analytics with Hadoop and the
    MapReduce paradigm. This was powerful, but often slow, and gave users a low-level, **procedural
    programming interface** that required people to write a lot of code for even very
    simple data transformations. However, once Spark was released, it really revolutionized
    the way Big Data analytics was done with a focus on in-memory computing, fault
    tolerance, high-level abstractions, and ease of use.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hadoop和MapReduce范式下，我们在大数据分析领域取得了成功。这很强大，但往往很慢，并且提供了一个低级的**过程编程接口**，要求用户为即使是非常简单的数据转换编写大量代码。然而，一旦Spark发布，它确实彻底革新了大数据分析的方式，专注于内存计算、容错、高级抽象和易用性。
- en: Spark SQL essentially tries to bridge the gap between the two models we mentioned
    previously—the relational and procedural models. Spark SQL works through the DataFrame
    API that can perform relational operations on both external data sources and Spark’s
    built-in distributed collections—at scale!
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL本质上试图弥合我们之前提到的两种模型——关系模型和过程模型之间的差距。Spark SQL通过DataFrame API来处理对外部数据源和Spark的内置分布式集合进行关系操作——在大规模下！
- en: '![The Benefits & Examples of Using Apache Spark with PySpark in Python | Spark
    SQL](../Images/fbbca3e99c3728cb51c0284181460269.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Apache Spark 和 PySpark 在 Python 中的好处与示例 | Spark SQL](../Images/fbbca3e99c3728cb51c0284181460269.png)'
- en: Why is Spark SQL so fast and optimized? The reason is because of a new extensible
    optimizer, **Catalyst**, based on functional programming constructs in Scala.
    Catalyst supports both rule-based and cost-based optimization. While extensible
    optimizers have been proposed in the past, they have typically required a complex
    domain-specific language to specify rules. Usually, this leads to having a significant
    learning curve and maintenance burden. In contrast, Catalyst uses standard features
    of the Scala programming language, such as pattern-matching, to let developers
    use the full programming language while still making rules easy to specify.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么 Spark SQL 会如此快速且优化？原因在于一种新的可扩展优化器**Catalyst**，它基于 Scala 中的函数式编程结构。Catalyst
    支持规则基础优化和成本基础优化。虽然过去曾提出过可扩展的优化器，但它们通常需要复杂的领域特定语言来指定规则。这通常导致有显著的学习曲线和维护负担。相比之下，Catalyst
    使用 Scala 编程语言的标准特性，如模式匹配，让开发者可以使用完整的编程语言，同时仍然使规则的指定变得简单。
- en: 'You can refer to the following Jupyter notebook for an introduction to Database
    operations with SparkSQL:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下 Jupyter 笔记本，了解 SparkSQL 数据库操作的介绍：
- en: '[**SparkSQL database operations basics**](https://github.com/tirthajyoti/Spark-with-Python/blob/master/Dataframe_SQL_query.ipynb)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[**SparkSQL 数据库操作基础**](https://github.com/tirthajyoti/Spark-with-Python/blob/master/Dataframe_SQL_query.ipynb)'
- en: '**How Are You Going to Use PySpark in Your Project? **'
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**你打算如何在项目中使用 PySpark？**'
- en: We covered the fundamentals of the Apache Spark ecosystem and how it works along
    with some basic usage examples of core data structure RDD with the Python interface
    PySpark. Also, DataFrame and SparkSQL were discussed along with reference links
    for example code notebooks.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们涵盖了 Apache Spark 生态系统的基本概念及其工作原理，并提供了一些 PySpark 核心数据结构 RDD 的基本用法示例。此外，还讨论了
    DataFrame 和 SparkSQL，并提供了示例代码笔记本的参考链接。
- en: There is so much more to learn and experiment with Apache Spark being used with
    Python. The [PySpark website is](https://spark.apache.org/docs/latest/api/python/index.html) a
    good reference to have on your radar, and they make regular updates and enhancements–so
    keep an eye on that.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python 的 Apache Spark 有更多的学习和实验空间。可以参考 [PySpark 网站](https://spark.apache.org/docs/latest/api/python/index.html)，它是一个很好的参考资源，并且他们会定期更新和增强功能——请保持关注。
- en: And, if you are interested in doing large-scale, distributed machine learning
    with Apache Spark, then check out the [MLLib portion of the PySpark ecosystem](https://spark.apache.org/mllib/).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对使用 Apache Spark 进行大规模分布式机器学习感兴趣，可以查看 [PySpark 生态系统的 MLLib 部分](https://spark.apache.org/mllib/)。
- en: '[Original](https://blog.exxactcorp.com/the-benefits-examples-of-using-apache-spark-with-pyspark-using-python/).
    Reposted with permission.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://blog.exxactcorp.com/the-benefits-examples-of-using-apache-spark-with-pyspark-using-python/)。经许可转载。'
- en: '**Related:**'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Learn how to use PySpark in under 5 minutes (Installation + Tutorial)](/2019/08/learn-pyspark-installation-tutorial.html)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 5 分钟内学习如何使用 PySpark（安装 + 教程](/2019/08/learn-pyspark-installation-tutorial.html)'
- en: '[Time Series Analysis: A Simple Example with KNIME and Spark](/2019/10/time-series-analysis-simple-example-knime-spark.html)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[时间序列分析：使用 KNIME 和 Spark 的简单示例](/2019/10/time-series-analysis-simple-example-knime-spark.html)'
- en: '[Spark NLP 101: LightPipeline](/2019/11/spark-nlp-101-lightpipeline.html)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Spark NLP 101: LightPipeline](/2019/11/spark-nlp-101-lightpipeline.html)'
- en: More On This Topic
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Data Validation for PySpark Applications using Pandera](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Pandera 进行 PySpark 应用程序的数据验证](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
- en: '[Benefits Of Becoming A Data-First Enterprise](https://www.kdnuggets.com/2022/07/benefits-becoming-datafirst-enterprise.html)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成为数据优先企业的好处](https://www.kdnuggets.com/2022/07/benefits-becoming-datafirst-enterprise.html)'
- en: '[3 Benefits to A/B Testing (+ Where to Get Started)](https://www.kdnuggets.com/2022/08/sphere-3-benefits-ab-testing-get-started.html)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[A/B 测试的 3 大好处 (+ 如何入门)](https://www.kdnuggets.com/2022/08/sphere-3-benefits-ab-testing-get-started.html)'
- en: '[The Benefits of Natural Language AI for Content Creators](https://www.kdnuggets.com/2022/08/benefits-natural-language-ai-content-creators.html)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言 AI 对内容创作者的好处](https://www.kdnuggets.com/2022/08/benefits-natural-language-ai-content-creators.html)'
- en: '[PySpark for Data Science](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PySpark 在数据科学中的应用](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
- en: '[Picking Examples to Understand Machine Learning Model](https://www.kdnuggets.com/2022/11/picking-examples-understand-machine-learning-model.html)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[挑选示例以理解机器学习模型](https://www.kdnuggets.com/2022/11/picking-examples-understand-machine-learning-model.html)'
