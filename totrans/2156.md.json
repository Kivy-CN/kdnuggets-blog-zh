["```py\npip install -U autotrain-advanced\n```", "```py\npip install datasets\n```", "```py\nfrom datasets import load_dataset \n\n# Load the dataset\ndataset = load_dataset(\"tatsu-lab/alpaca\") \ntrain = dataset['train']\n```", "```py\ntrain.to_csv('train.csv', index = False)\n```", "```py\n!autotrain setup\n```", "```py\nproject_name = 'my_autotrain_llm'\nmodel_name = 'tiiuae/falcon-7b'\n```", "```py\npush_to_hub = False\nhf_token = \"YOUR HF TOKEN\"\nrepo_id = \"username/repo_name\"\n```", "```py\nlearning_rate = 2e-4\nnum_epochs = 4\nbatch_size = 1\nblock_size = 1024\ntrainer = \"sft\"\nwarmup_ratio = 0.1\nweight_decay = 0.01\ngradient_accumulation = 4\nuse_fp16 = True\nuse_peft = True\nuse_int4 = True\nlora_r = 16\nlora_alpha = 32\nlora_dropout = 0.045\n```", "```py\nimport os\nos.environ[\"PROJECT_NAME\"] = project_name\nos.environ[\"MODEL_NAME\"] = model_name\nos.environ[\"PUSH_TO_HUB\"] = str(push_to_hub)\nos.environ[\"HF_TOKEN\"] = hf_token\nos.environ[\"REPO_ID\"] = repo_id\nos.environ[\"LEARNING_RATE\"] = str(learning_rate)\nos.environ[\"NUM_EPOCHS\"] = str(num_epochs)\nos.environ[\"BATCH_SIZE\"] = str(batch_size)\nos.environ[\"BLOCK_SIZE\"] = str(block_size)\nos.environ[\"WARMUP_RATIO\"] = str(warmup_ratio)\nos.environ[\"WEIGHT_DECAY\"] = str(weight_decay)\nos.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\nos.environ[\"USE_FP16\"] = str(use_fp16)\nos.environ[\"USE_PEFT\"] = str(use_peft)\nos.environ[\"USE_INT4\"] = str(use_int4)\nos.environ[\"LORA_R\"] = str(lora_r)\nos.environ[\"LORA_ALPHA\"] = str(lora_alpha)\nos.environ[\"LORA_DROPOUT\"] = str(lora_dropout)\n```", "```py\n!autotrain llm \\\n--train \\\n--model ${MODEL_NAME} \\\n--project-name ${PROJECT_NAME} \\\n--data-path data/ \\\n--text-column text \\\n--lr ${LEARNING_RATE} \\\n--batch-size ${BATCH_SIZE} \\\n--epochs ${NUM_EPOCHS} \\\n--block-size ${BLOCK_SIZE} \\\n--warmup-ratio ${WARMUP_RATIO} \\\n--lora-r ${LORA_R} \\\n--lora-alpha ${LORA_ALPHA} \\\n--lora-dropout ${LORA_DROPOUT} \\\n--weight-decay ${WEIGHT_DECAY} \\\n--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n$( [[ \"$USE_FP16\" == \"True\" ]] && echo \"--fp16\" ) \\\n$( [[ \"$USE_PEFT\" == \"True\" ]] && echo \"--use-peft\" ) \\\n$( [[ \"$USE_INT4\" == \"True\" ]] && echo \"--use-int4\" ) \\\n$( [[ \"$PUSH_TO_HUB\" == \"True\" ]] && echo \"--push-to-hub --token ${HF_TOKEN} --repo-id ${REPO_ID}\" )\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = \"my_autotrain_llm\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\n```", "```py\ninput_text = \"Health benefits of regular exercise\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\noutput = model.generate(input_ids)\npredicted_text = tokenizer.decode(output[0], skip_special_tokens=False)\nprint(predicted_text)\n```"]