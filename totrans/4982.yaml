- en: 7 More Steps to Mastering Machine Learning With Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/03/seven-more-steps-machine-learning-python.html/2](https://www.kdnuggets.com/2017/03/seven-more-steps-machine-learning-python.html/2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Step 4: More Ensemble Methods'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first post only dealt with a single ensemble method: Random Forests (RF).
    RF has seen a great deal of success over the years as a top performing classifier,
    but it is certainly not the only ensemble classifier available. We will have a
    look at bagging, boosting, and voting.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Boosting](../Images/658f99083c93080b92c1f86c7f94ecf7.png)'
  prefs: []
  type: TYPE_IMG
- en: Give me a boost.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, read these overviews of ensemble learners, the first being general,
    and the second being as they relate to Scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[An Introduction to Ensemble Learners](/2016/11/data-science-basics-intro-ensemble-learners.html),
    by Matthew Mayo'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ensemble methods in Scikit-learn](http://scikit-learn.org/stable/modules/ensemble.html),
    Scikit-learn documentation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, before moving on to new ensemble methods, get back up to speed on Random
    Forests with a fresh tutorial here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Random Forests in Python](/2016/12/random-forests-python.html), from Yhat'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging, boosting, and voting are all different forms of ensemble classifiers.
    All involve building multiple models; however, what **algorithms** the models
    are built from, the **data** which the models use, and how the results are ultimately
    **combined** differ between schemes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging** builds multiple models from the same classification algorithm,
    while using different (independent) samples of data from the training set -- Scikit-learn
    implements BaggingClassifier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boosting** builds multiple models from the same classification algorithm,
    chaining models one after another in order to boost the learning of each subsequent
    model -- Scikit-learn implements AdaBoost'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Voting** builds multiple models from different classification algorithms,
    and a criteria is used to determine how the models are best combined -- Scikit-learn
    implements VotingClassifier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, why combine models? To approach this question from one specific angle,
    here is an overview of the [bias-variance trade-off](/2016/08/bias-variance-tradeoff-overview.html)
    as related specifically to boosting, from the Scikit-learn documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Single estimator versus bagging: bias-variance decomposition](http://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html),
    Scikit-learn documentation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that you have read some introductory material on ensemble learners in general,
    and have a basic understanding of a few specific ensemble classifiers, follow
    this introduction to implementing the ensemble classifiers in Python using Scikit-learn,
    from Machine Learning Mastery:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Ensemble Machine Learning Algorithms in Python with scikit-learn](http://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/),
    by Jason Brownlee'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 5: Gradient Boosting'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our next step keeps us in the realm of ensemble classifiers, focusing on one
    of the most popular contemporary machine learning algorithms. Gradient boosting
    has made a noticeable impact in machine learning in the recent past, becoming
    (of note) one of the most utilized and successful algorithms in Kaggle competitions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Gradient boosting](../Images/8a508ad2c2ca40aaed56f1ad7dfa9128.png)'
  prefs: []
  type: TYPE_IMG
- en: Give me a gradient boost.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, read an overview of what gradient boosting is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Gradient boosting on Wikipedia](https://en.wikipedia.org/wiki/Gradient_boosting)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, read up on why gradient boosting is the "most winning-est" approach to
    Kaggle competitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Why does Gradient boosting work so well for so many Kaggle problems? on Quora](https://www.quora.com/Why-does-Gradient-boosting-work-so-well-for-so-many-Kaggle-problems)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Kaggle Master Explains Gradient Boosting](https://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/),
    by Ben Gorman'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While Scikit-learn has its own gradient boosting implementation, we will diverge
    somewhat and use the [XGBoost library](https://github.com/dmlc/xgboost), which
    has [been noted](http://www.jmlr.org/proceedings/papers/v42/chen14.pdf) to be
    a faster implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following links provide some additional info on the XGBoost library, as
    well as gradient boosting (out of necessity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[XGBoost on Wikipedia](https://en.wikipedia.org/wiki/Xgboost)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[XGBoost library on Github](https://github.com/dmlc/xgboost)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[XGBoost documentation](https://xgboost.readthedocs.io/en/latest/model.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, follow this tutorial which brings it all together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[A Guide to Gradient Boosted Trees with XGBoost in Python](https://jessesw.com/XG-Boost/),
    by Jesse Steinweg-Woods'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can also follow these more concise examples for reinforcement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[XGBoost Example (Python) on Kaggle](https://www.kaggle.com/cbrogan/titanic/xgboost-example-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Iris Dataset and XGBoost Simple Tutorial](http://ieva.rocks/2016/08/25/iris_dataset_and_xgboost_simple_tutorial/),
    by Ieva Zarina'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 6: More Dimensionality Reduction'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction)
    is the act of reducing the variables being used for model building from its initial
    number to a reduced number, by utilizing processes to obtain a set of **principal
    variables**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 2 major forms of dimensionality reduction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Feature selection](https://en.wikipedia.org/wiki/Feature_selection) - selecting
    a subset of relevant features'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Feature extraction](https://en.wikipedia.org/wiki/Feature_extraction) - building
    an informative and non-redundant set of derived values features'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following is a look at a pair of common feature extraction methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![LDA Iris dataset](../Images/64b868a81463c323186ccc586b238ac0.png)'
  prefs: []
  type: TYPE_IMG
- en: LDA of Iris dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Principal component analysis (PCA)**](https://en.wikipedia.org/wiki/Principal_component_analysis)
    is a statistical procedure that uses an orthogonal transformation to convert a
    set of observations of possibly correlated variables into a set of values of linearly
    uncorrelated variables called principal components. The number of principal components
    is less than or equal to the number of original variables. This transformation
    is defined in such a way that the first principal component has the largest possible
    variance (that is, accounts for as much of the variability in the data as possible)[.]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The above definition comes from the [PCA Wikipedia entry](https://en.wikipedia.org/wiki/Principal_component_analysis),
    which you can read further if interested. However, the following overview-slash-tutorial
    is very thorough:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Principal Component Analysis in 3 Simple Steps](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html),
    by Sebastian Raschka'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Linear discriminant analysis (LDA)**](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)
    is a generalization of Fisher''s linear discriminant, a method used in statistics,
    pattern recognition and machine learning to find a linear combination of features
    that characterizes or separates two or more classes of objects or events. The
    resulting combination may be used as a linear classifier, or, more commonly, for
    dimensionality reduction before later classification.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: LDA is closely related to analysis of variance (ANOVA) and regression analysis,
    which also attempt to express one dependent variable as a linear combination of
    other features or measurements. However, ANOVA uses categorical independent variables
    and a continuous dependent variable, whereas discriminant analysis has continuous
    independent variables and a categorical dependent variable (i.e. the class label)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The above definition is also from Wikipedia. And again, the following read
    is thorough:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Linear Discriminant Analysis – Bit by Bit](http://sebastianraschka.com/Articles/2014_python_lda.html),
    by Sebastian Raschka'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Are you at all confused as to the practical difference between PCA and LDA
    for dimensionality reduction? [Sebastian Raschka clarifies](http://sebastianraschka.com/Articles/2014_python_lda.html):'
  prefs: []
  type: TYPE_NORMAL
- en: Both Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA)
    are linear transformation techniques that are commonly used for dimensionality
    reduction. PCA can be described as an “unsupervised” algorithm, since it “ignores”
    class labels and its goal is to find the directions (the so-called principal components)
    that maximize the variance in a dataset. In contrast to PCA, LDA is “supervised”
    and computes the directions (“linear discriminants”) that will represent the axes
    that that maximize the separation between multiple classes.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For a concise elaboration on this, read the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[What is the difference between LDA and PCA for dimensionality reduction?](https://sebastianraschka.com/faq/docs/lda-vs-pca.html),
    by Sebastian Raschka'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 7: More Deep Learning'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The original 7 Steps... post provided an entry point to neural networks and
    deep learning. If you have made it this far without much trouble and want to solidify
    your understanding of neural networks, and practice implementing a few common
    neural network models, there is no reason not to continue on.
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep learning](../Images/35cca5c97cb4d4033c23fdadcb5820b8.png)'
  prefs: []
  type: TYPE_IMG
- en: The many layers of deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, have a look at a few deep learning foundation materials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Deep Learning Key Terms, Explained](/2016/10/deep-learning-key-terms-explained.html),
    by Matthew Mayo'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Steps to Understanding Deep Learning](/2016/01/seven-steps-deep-learning.html),
    by Matthew Mayo'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, try a few introductory overview-slash-tutorials on [TensorFlow](https://www.tensorflow.org/),
    Google''s "open-source software library for Machine Intelligence," effectively
    a deep learning framework and nearly *de facto* contemporary go-to neural network
    tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[The Gentlest Introduction to Tensorflow – Part 1](/2016/08/gentlest-introduction-tensorflow-part-1.html),
    by Soon Hin Khor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Gentlest Introduction to Tensorflow – Part 2](/2016/08/gentlest-introduction-tensorflow-part-2.html),
    by Soon Hin Khor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Gentlest Introduction to Tensorflow – Part 3](/2017/02/gentlest-introduction-tensorflow-part-3.html),
    by Soon Hin Khor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Gentlest Introduction to Tensorflow – Part 4](/2017/02/gentlest-introduction-tensorflow-part-4.html),
    by Soon Hin Khor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, try your hand at these tutorials directly from the TensorFlow site,
    which implement a few of the most popular and common neural network models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Recurrent Neural Networks](https://www.tensorflow.org/tutorials/recurrent),
    Google TensorFlow tutorial'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Convolutional Neural Networks](https://www.tensorflow.org/tutorials/deep_cnn),
    Google TensorFlow tutorial'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, a 7 Steps... post focusing on deep learning is currently in the works,
    and will focus on the use of a high-level API sitting atop TensorFlow to increase
    the ease and flexibility with which a practitioner can implement models. I will
    also add a link here once complete.
  prefs: []
  type: TYPE_NORMAL
- en: '**Related**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[5 EBooks to Read Before Getting into A Machine Learning Career](/2016/10/5-free-ebooks-machine-learning-career.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Steps to Understanding Deep Learning](/2016/01/seven-steps-deep-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning Key Terms, Explained](/2016/05/machine-learning-key-terms-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
