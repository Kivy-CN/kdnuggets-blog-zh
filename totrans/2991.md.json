["```py\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n```", "```py\nall_variables = shared_vars + a_vars + b_vars\nall_gradients = tf.gradients(loss, all_variables)\n\nshared_subnet_gradients = all_gradients[:len(shared_vars)]\na_gradients = all_gradients[len(shared_vars):len(shared_vars + a_vars)]\nb_gradients = all_gradients[len(shared_vars + a_vars):]\n\nshared_subnet_optimizer = tf.train.AdamOptimizer(shared_learning_rate)\na_optimizer = tf.train.AdamOptimizer(a_learning_rate)\nb_optimizer = tf.train.AdamOptimizer(b_learning_rate)\n\ntrain_shared_op = shared_subnet_optimizer.apply_gradients(zip(shared_subnet_gradients, shared_vars))\ntrain_a_op = a_optimizer.apply_gradients(zip(a_gradients, a_vars))\ntrain_b_op = b_optimizer.apply_gradients(zip(b_gradients, b_vars))\n\ntrain_op = tf.group(train_shared_op, train_a_op, train_b_op)\n```", "```py\nall_gradients = tf.gradients(loss, all_variables, stop_gradients=stop_tensors)\n```"]