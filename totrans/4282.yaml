- en: 'XGBoost Explained: DIY XGBoost Library in Less Than 200 Lines of Python'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost 详解：用少于 200 行 Python 代码实现 DIY XGBoost 库
- en: 原文：[https://www.kdnuggets.com/2021/05/xgboost-explained-diy-xgboost-library-200-lines-python.html](https://www.kdnuggets.com/2021/05/xgboost-explained-diy-xgboost-library-200-lines-python.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/05/xgboost-explained-diy-xgboost-library-200-lines-python.html](https://www.kdnuggets.com/2021/05/xgboost-explained-diy-xgboost-library-200-lines-python.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[comments](#comments)'
- en: '**By [Guillaume Saupin](https://www.linkedin.com/in/guillaume-saupin-5802aa31/),
    CTO at Verteego**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Guillaume Saupin](https://www.linkedin.com/in/guillaume-saupin-5802aa31/)，Verteego
    的 CTO 提供**'
- en: '![](../Images/2af80546d15b24dbb95b3e7f06ad2b1d.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2af80546d15b24dbb95b3e7f06ad2b1d.png)'
- en: Photo by [Jens Lelie](https://unsplash.com/@madebyjens?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Jens Lelie](https://unsplash.com/@madebyjens?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: XGBoost is probably one of the most widely used libraries in data science. Many
    data scientists around the world are using it. It’s a very versatile algorithm
    that can be use to perform classification, regression as well as confidence intervals
    as shown in this [article](https://towardsdatascience.com/confidence-intervals-for-xgboost-cac2955a8fde).
    But how many really understand its underlying principles?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 可能是数据科学中使用最广泛的库之一。全球许多数据科学家都在使用它。它是一个非常通用的算法，可用于执行分类、回归以及置信区间，如本文[文章](https://towardsdatascience.com/confidence-intervals-for-xgboost-cac2955a8fde)所示。但有多少人真正理解它的基本原理呢？
- en: You might think that a Machine Learning algorithm that performs as well as XGBoost
    might be using very complex and advanced mathematics. You probably imagine that
    it’s a masterpiece of software engineering.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为，像 XGBoost 这样表现出色的机器学习算法可能使用了非常复杂和先进的数学方法。你可能会想象它是一个软件工程的杰作。
- en: And you’re partly right. The XGBoost library is a pretty complex one, but if
    you consider only the mathematical formulation of gradient boosting applied to
    decision trees, it’s not that complicated.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你部分正确。XGBoost 库确实相当复杂，但如果仅考虑应用于决策树的梯度提升的数学公式，其实并没有那么复杂。
- en: You will see below in detail how to train decision trees for **regression** using
    the gradient boosting method with less than 200 lines of code.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 下面你将详细了解如何使用少于 200 行代码，通过梯度提升方法训练**回归**决策树。
- en: Decision Tree
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树
- en: 'Before getting into the mathematical details, let’s refresh our memory regarding
    decision trees. The principle is fairly simple: associate a value to a given set
    of features traversing a binary tree. Each node of the binary tree is attached
    to a condition; leaves contain values.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入数学细节之前，让我们回顾一下决策树的基本概念。原理非常简单：将值关联到一组特征，通过遍历二叉树。二叉树的每个节点都附带一个条件；叶子节点包含值。
- en: If the condition is true, we continue the traversal using the left node, if
    not, we use the right one. Once a leaf is reached, we have our prediction.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果条件为真，我们继续使用左节点进行遍历；如果不成立，我们使用右节点。一旦到达叶子节点，我们就得到了预测结果。
- en: 'As often, a picture is worth a thousand words:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 正如常常所说，一图胜千言：
- en: '![](../Images/f681ccf5b863618aa3ba615f5f7ed042.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f681ccf5b863618aa3ba615f5f7ed042.png)'
- en: A three-level decision tree. Image by the author.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 三层决策树。图像由作者提供。
- en: The condition attached to a node can be regarded as a decision, hence the name **Decision
    Tree**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 附加到节点的条件可以视为一个决策，因此得名**决策树**。
- en: 'This kind of structure is pretty old in the history of computer science and
    has been used for decades with success. A basic implementation is given by the
    following lines:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构在计算机科学历史上相当古老，并且在几十年里成功使用过。基本实现如下：
- en: Stacking multiple trees
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 堆叠多个树
- en: Even though Decision Trees have been used with some success in a number of applications,
    like expert systems (before the AI winter) it remains a very basic model that
    cannot handle the complexity usually encountered in real-world data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管决策树在许多应用中取得了一些成功，例如专家系统（在 AI 冬季之前），但它仍然是一个非常基础的模型，无法处理现实世界数据中通常遇到的复杂性。
- en: We usually refer to this kind of estimators as ***weak models***.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将这种类型的估算器称为***弱模型***。
- en: To overcome this limitation, the idea emerged in the nineties to combine multiple *weak
    models* to create a *strong model: ****ensemble learning****.*
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这一局限性，九十年代出现了将多个*弱模型*组合起来以创建一个*强模型：****集成学习****。* 的想法。
- en: This method can be applied to any kind of model, but as Decision Trees are simple,
    fast, generic and easily interpretable models, they are commonly used.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以应用于任何类型的模型，但由于决策树简单、快速、通用且易于解释，因此它们被广泛使用。
- en: Various strategies can be deployed to combine models. We can for instance use
    a weighted sum of each model prediction. Or even better, use a Bayesian approach
    to combine them based on learning.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 可以部署各种策略来组合模型。例如，我们可以使用每个模型预测的加权和。甚至更好的是，使用贝叶斯方法根据学习进行组合。
- en: 'XGBoost and all **boosting** methods use another approach: each new model tries
    to compensate for the errors of the previous one.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 和所有**boosting**方法使用另一种方法：每个新模型尝试补偿前一个模型的错误。
- en: Optimizing decision trees
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化决策树
- en: 'As we have seen above, making predictions using a decision tree is straightforward.
    The job is hardly more complicated when using *ensemble learning*: all we have
    to do is sum contributions of each model.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，使用决策树进行预测很直接。使用*集成学习*时，工作并不复杂：我们只需对每个模型的贡献进行求和即可。
- en: What’s really complicated is building the tree itself! How can we find the best
    condition to apply at each node of our training dataset? This is where math helps
    us. A full derivation can be found in the [XGBoost documentation](https://xgboost.readthedocs.io/en/latest/tutorials/model.html).
    Here, we will focus only on the formulas of interest for this article.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 真正复杂的是构建树本身！我们如何找到在每个训练数据集节点上应用的最佳条件？这就是数学帮助我们的地方。完整的推导可以在[XGBoost 文档](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)中找到。在这里，我们将仅关注本文的感兴趣公式。
- en: 'As always in machine learning, we want to set our model parameters so that
    our model’s predictions on the training set minimize a given objective:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，在机器学习中，我们希望设置模型参数，以便使模型在训练集上的预测最小化给定的目标：
- en: '![](../Images/1c6ea3b60063b729682ac9a687e59dd2.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c6ea3b60063b729682ac9a687e59dd2.png)'
- en: Objective formula. Formula by the author.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 目标公式。公式由作者提供。
- en: 'Please note that this objective is made of two terms:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个目标由两个部分组成：
- en: One to measure the error made by the prediction. It’s the famous loss function *l(y,
    y_hat)*.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种用于测量预测错误的指标。这就是著名的损失函数*l(y, y_hat)*。
- en: The other, *omega*, to control model complexity.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个*omega*，用于控制模型复杂性。
- en: 'As stated in the XGBoost [documentation](https://xgboost.readthedocs.io/en/latest/tutorials/model.html),
    complexity is a very important part of the objective that allows us to tune the
    bias/variance trade-off. Many different functions can be used to define this regularization
    term. XGBoost use:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如 XGBoost [文档](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)所述，复杂性是目标中非常重要的一部分，它允许我们调整偏差/方差权衡。可以使用许多不同的函数来定义这个正则化项。XGBoost
    使用：
- en: '![](../Images/0ce54f0ee55300ffbfdaa119f4139291.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ce54f0ee55300ffbfdaa119f4139291.png)'
- en: Regularization term. Formula by the author.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化项。公式由作者提供。
- en: Here* T* is the total number of leaves, whereas *w_j* are the weights attached
    to each leaf. This means that a large weight and a large number of leaves will
    be penalized.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里* T* 是叶子的总数，而*w_j* 是附加到每个叶子的权重。这意味着较大的权重和较多的叶子会受到惩罚。
- en: 'As the error is often a complex, non-linear function, we linearized it using
    second-order Taylor expansion:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于错误通常是复杂的非线性函数，我们使用二阶泰勒展开对其进行线性化：
- en: '![](../Images/30f9a52c0856023e980b2ed196a90f7a.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30f9a52c0856023e980b2ed196a90f7a.png)'
- en: Second-order expansion of the loss. Formula by the author.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 损失的二阶展开。公式由作者提供。
- en: 'where:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '![](../Images/d15672d7944ba013a374896777a30f45.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d15672d7944ba013a374896777a30f45.png)'
- en: Gaussian and hessian formulas. Formula by the author.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯和 Hessian 公式。公式由作者提供。
- en: The linearization is computed with respect to the prediction term, as we want
    to estimate how the error changes when the prediction changes. Linearization is
    essential, as it will ease the minimization of the error.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 线性化是针对预测项计算的，因为我们希望估计预测变化时错误的变化。线性化是至关重要的，因为它将简化错误的最小化。
- en: Whatwe want to achieve with gradient boosting, is to find the optimal *delta_y_i *that
    will minimize the loss function, i.e. we want to find how to modify the existing
    tree so that the modification improves the prediction.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望通过梯度提升实现的目标是找到最佳的*delta_y_i*，以最小化损失函数，即我们希望找到如何修改现有的树，以使修改改善预测。
- en: 'When dealing with tree models, there are two kinds of parameters to consider:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理树模型时，有两类参数需要考虑：
- en: 'The ones defining the tree in itself: the condition for each node, the depth
    of the tree'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义树本身的参数：每个节点的条件，树的深度
- en: The values attached to each leaf of the tree. These values are the predictions
    themselves.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 附加到每个叶子的值。这些值就是预测值本身。
- en: 'Exploring every tree configurations would be too complex, therefore gradient
    tree boosting methods only consider splitting one node into two leaves. This means
    that we have to optimize three parameters:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 探索每个树的配置将过于复杂，因此梯度提升树方法仅考虑将一个节点拆分为两个叶子。这意味着我们必须优化三个参数：
- en: 'The splitting value: on which condition do we split data'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拆分值：我们根据什么条件来拆分数据
- en: The value attached to the left leaf
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 附加到左侧叶子的值
- en: The value attached to the right leaf
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 附加到右侧叶子的值
- en: 'In the XGBoost documentation, the best value for a leaf *j* with respect to
    the objective is given by:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 XGBoost 文档中，关于目标的叶子 *j* 的最佳值由以下公式给出：
- en: '![](../Images/30ba452986e0c2f8e110335381fe9963.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30ba452986e0c2f8e110335381fe9963.png)'
- en: Optimal leaf value with respect to objective. Formula by the author.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于目标的最优叶子值。作者公式。
- en: where *G_j *is the sum of the gradient of the training points attached to the
    node *j*, and *H_j* is the sum of the hessian of the training points attached
    to the node *j.*
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *G_j* 是附加到节点 *j* 的训练点的梯度之和，*H_j* 是附加到节点 *j* 的训练点的赫希矩阵之和。
- en: 'The reduction in the objective function obtained with this optimal param is:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此最优参数获得的目标函数的减少量是：
- en: '![](../Images/92074bf0fd43651aab5b4918039c0764.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92074bf0fd43651aab5b4918039c0764.png)'
- en: Objective improvement when using optimal weight. Formula by the author.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最优权重时的目标改善。作者公式。
- en: 'Choosing the right split value is done using brut force: we compute the improvement
    for each split value, and we keep the best one.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的拆分值是使用暴力方法完成的：我们计算每个拆分值的改进，并保留最佳的拆分值。
- en: We now have all the required mathematical information to boost an initial tree
    performance with respect to a given objective by adding new leaves.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在拥有所有所需的数学信息，可以通过添加新叶子来提升初始树的性能，以符合给定的目标。
- en: Before doing it concretely, let’s take some time to understand what these formulas
    mean.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在具体实施之前，让我们花些时间理解这些公式的含义。
- en: Grokking gradient boosting
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解梯度提升
- en: Let’s try to get some insight on how weight is computed, and what *G_j* and *H_i* equal
    to. As they are respectively the gradient and hessian of the loss function with
    respect to the prediction, we have to pick a loss function.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解权重是如何计算的，以及 *G_j* 和 *H_i* 的值。由于它们分别是相对于预测的损失函数的梯度和赫希矩阵，我们必须选择一个损失函数。
- en: 'We will focus on the squared error, which is commonly used and is the default
    objective for XGBoost:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于平方误差，这是一种常用的损失函数，也是 XGBoost 的默认目标：
- en: '![](../Images/bd91ae15b1e00e8ed1dd2fc927f73301.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd91ae15b1e00e8ed1dd2fc927f73301.png)'
- en: Squared error loss function. Formula by the author
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 平方误差损失函数。作者公式
- en: 'It’s a pretty simple formula, whose gradient is:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当简单的公式，其梯度是：
- en: '![](../Images/9941cb63747e203b1bc9de0f40549606.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9941cb63747e203b1bc9de0f40549606.png)'
- en: The gradient of the loss function. Formula by the author.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的梯度。作者公式。
- en: 'and hessian:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 和赫希矩阵：
- en: '![](../Images/4821077093599e50bb41001008817d81.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4821077093599e50bb41001008817d81.png)'
- en: Hessian of the loss function. Formula by the author.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的赫希矩阵。作者公式。
- en: 'Hence, if we remember the formulas for the optimal weight that maximize error
    reduction:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们记住最大化误差减少的最优权重公式：
- en: '![](../Images/30ba452986e0c2f8e110335381fe9963.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30ba452986e0c2f8e110335381fe9963.png)'
- en: Optimal leaf value with respect to the objective. Formula by the author.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于目标的最优叶子值。作者公式。
- en: We realize that the optimal weight, *i.e. *the value that we add to the previous
    prediction is the opposite of the average error between the previous prediction
    and the real value (when regularization is disabled, i.e. *lambda = 0*). Using
    the squared loss to train Decision Trees with gradient boosting just boils down
    to updating the prediction with the average error in each new node.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们意识到，最优权重，即我们添加到先前预测中的值，是先前预测与实际值之间的平均误差的相反数（当正则化禁用时，即 *lambda = 0*）。使用平方损失来训练带有梯度提升的决策树仅仅归结为在每个新节点中用平均误差更新预测。
- en: We also see that *lambda* has the expected effect, that is ensuring that weights
    are not too large, as weight is inversely proportional to *lambda*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到 *lambda* 具有预期的效果，即确保权重不会过大，因为权重与 *lambda* 成反比。
- en: Training decision trees
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练决策树
- en: Now comes the easy part. Let’s say that we have an existing decision tree that
    ensures predictions with a given error. We want to reduce the error and improve
    the attached objective by splitting one node.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进入简单的部分。假设我们有一个现有的决策树，能够在给定的误差范围内进行预测。我们希望通过分裂一个节点来减少误差并改善附加目标。
- en: 'The algorithm to do so is pretty straightforward:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一点的算法相当直接：
- en: Pick a feature of interest
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个感兴趣的特征
- en: Order data points attached to the current node using values of the selected
    feature
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据所选特征的值对当前节点附加的数据点进行排序
- en: pick a possible split value
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个可能的分裂值
- en: Put data points below this split value in the right node, and the one above
    in the left node
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将此分裂值以下的数据点放入右节点，将以上的数据点放入左节点
- en: compute objective reduction for the parent node, the right node and the left
    one
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算父节点、右节点和左节点的目标减少
- en: If the sum of the objective reduction for left and right nodes is greater than
    the one of the parent node, keep the split value as the best one
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果左节点和右节点的目标减少总和大于父节点的目标减少总和，则将分裂值保持为最佳值
- en: Iterate for each split value
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对每个分裂值进行迭代
- en: Use the best split value if any, and add the two new nodes
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果有最佳分裂值，则使用它，并添加两个新节点
- en: If no splitting improved the objective, don’t add child nodes.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果没有分裂改善目标，则不要添加子节点。
- en: The resulting code creates a DecisionTree class, that is configured by an objective,
    a number of estimators, i.e. the number of trees, and a max depth.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的代码创建了一个DecisionTree类，该类通过一个目标、一组估计器（即树的数量）和一个最大深度进行配置。
- en: 'As promised the code takes less than 200 lines:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 正如承诺的代码少于200行：
- en: The core of the training is coded in the function *_find_best_split*. It essentially
    follows the steps detailed above.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的核心编码在函数*_find_best_split*中。它本质上遵循上述详细步骤。
- en: Note that to support any kind of objective, without the pain of manually calculating
    the gradient and the hessian, we use automatic differentiation and the [jax](https://jax.readthedocs.io/en/latest/index.html) library
    to automate computations.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了支持任何类型的目标，而不必手动计算梯度和Hessian，我们使用自动微分和[jax](https://jax.readthedocs.io/en/latest/index.html)库来自动化计算。
- en: Initially, we start with a tree with only one node whose leaf value leaf is
    zero. As we have mimic XGBoost, we also use a base score, that we set to be the
    mean of the value to predict.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们从一个只有一个节点的树开始，其叶子节点的值为零。由于我们模仿XGBoost，我们还使用一个基准分数，设置为待预测值的平均值。
- en: Also, note that at line 126 we stop tree building if we reach the max depth
    defined when initializing the tree. We could have used other conditions like a
    minimum number of sample for each leaf or a minimum value for the new weight.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意在第126行，我们如果达到初始化树时定义的最大深度则停止树的构建。我们还可以使用其他条件，比如每个叶子节点的最小样本数量或新权重的最小值。
- en: Another very important point is the choice of the feature used for the splitting.
    Here, for the sake of simplicity, features are selected randomly, but we could
    have been using smarter strategies, like using the feature having the greatest
    variance.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常重要的点是选择用于分裂的特征。这里为了简化起见，特征是随机选择的，但我们本可以使用更智能的策略，例如使用方差最大的特征。
- en: Conclusion
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we have seen how gradient boosting works to train decision
    trees. To further improve our understanding, we have written the minimal set of
    lines required to train an ensemble of decision trees and use them for prediction.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们了解了梯度提升如何训练决策树。为了进一步提高我们的理解，我们编写了训练决策树集成和使用它们进行预测所需的最小行数。
- en: Having a deep understanding of the algorithms we use for machine learning is
    absolutely critical. It not only helps us building better models but more importantly
    allows us to bend these models to our needs.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 深刻理解我们用于机器学习的算法是绝对关键的。这不仅有助于我们构建更好的模型，更重要的是使我们能够根据需求调整这些模型。
- en: For instance, in the case of gradient boosting, playing with loss functions
    is an excellent way to increase precision when predicting.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在梯度提升的情况下，调整损失函数是提高预测精度的一个绝佳方法。
- en: '**Bio: [Guillaume Saupin](https://www.linkedin.com/in/guillaume-saupin-5802aa31/)**
    is CTO at Verteego.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**简历： [Guillaume Saupin](https://www.linkedin.com/in/guillaume-saupin-5802aa31/)**
    是 Verteego 的首席技术官。'
- en: '[Original](https://towardsdatascience.com/diy-xgboost-library-in-less-than-200-lines-of-python-69b6bf25e7d9).
    Reposted with permission.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/diy-xgboost-library-in-less-than-200-lines-of-python-69b6bf25e7d9)。经许可转载。'
- en: '**Related:**'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Gradient Boosted Decision Trees – A Conceptual Explanation](/2021/04/gradient-boosted-trees-conceptual-explanation.html)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[梯度提升决策树 – 概念解释](/2021/04/gradient-boosted-trees-conceptual-explanation.html)'
- en: '[XGBoost: What it is, and when to use it](/2020/12/xgboost-what-when.html)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost：它是什么，何时使用](/2020/12/xgboost-what-when.html)'
- en: '[Many Heads Are Better Than One: The Case For Ensemble Learning](/2019/09/ensemble-learning.html)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[众人拾柴火焰高：集成学习的案例](/2019/09/ensemble-learning.html)'
- en: '* * *'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的 3 大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织进行 IT 支持'
- en: '* * *'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Multi-modal deep learning in less than 15 lines of code](https://www.kdnuggets.com/2023/01/predibase-multi-modal-deep-learning-less-15-lines-code.html)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[少于 15 行代码的多模态深度学习](https://www.kdnuggets.com/2023/01/predibase-multi-modal-deep-learning-less-15-lines-code.html)'
- en: '[KDnuggets News, July 20: Machine Learning Algorithms Explained in…](https://www.kdnuggets.com/2022/n29.html)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，7月20日：机器学习算法解释…](https://www.kdnuggets.com/2022/n29.html)'
- en: '[Machine Learning Algorithms Explained in Less Than 1 Minute Each](https://www.kdnuggets.com/2022/07/machine-learning-algorithms-explained-less-1-minute.html)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习算法每个不到 1 分钟解释](https://www.kdnuggets.com/2022/07/machine-learning-algorithms-explained-less-1-minute.html)'
- en: '[Become a Business Intelligence Analyst in Less Than 6 Months](https://www.kdnuggets.com/become-a-business-intelligence-analyst-in-less-than-6-months)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在不到 6 个月的时间内成为商业智能分析师](https://www.kdnuggets.com/become-a-business-intelligence-analyst-in-less-than-6-months)'
- en: '[DIY Automated Machine Learning with Streamlit](https://www.kdnuggets.com/2021/11/diy-automated-machine-learning-app.html)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Streamlit DIY 自动化机器学习](https://www.kdnuggets.com/2021/11/diy-automated-machine-learning-app.html)'
- en: '[Black Friday Deal - Master Machine Learning for Less with DataCamp](https://www.kdnuggets.com/2022/11/datacamp-black-friday-deal-master-machine-learning-less-datacamp.html)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[黑色星期五优惠 - 在 DataCamp 上以更低的价格掌握机器学习](https://www.kdnuggets.com/2022/11/datacamp-black-friday-deal-master-machine-learning-less-datacamp.html)'
