- en: Getting Started with spaCy for Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 spaCy 开始自然语言处理
- en: 原文：[https://www.kdnuggets.com/2018/05/getting-started-spacy-natural-language-processing.html](https://www.kdnuggets.com/2018/05/getting-started-spacy-natural-language-processing.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/05/getting-started-spacy-natural-language-processing.html](https://www.kdnuggets.com/2018/05/getting-started-spacy-natural-language-processing.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: In a series of previous posts, we have looked at some general ideas related
    to textual data science tasks, be they natural language processing, text mining,
    or something different yet closely related. In [the most recent of these posts](/2018/03/text-data-preprocessing-walkthrough-python.html),
    we covered a text data preprocessing walkthrough using Python. More specifically,
    we looked at a text preprocessing walkthrough using Python *and NLTK*. While we
    did not go any further than data preprocessing with NLTK, the toolkit could, theoretically,
    be used for further analysis tasks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的一系列文章中，我们探讨了一些与文本数据科学任务相关的一般性观点，无论是自然语言处理、文本挖掘，还是其他相关的主题。在[这些文章中最新的一篇](/2018/03/text-data-preprocessing-walkthrough-python.html)中，我们介绍了使用
    Python 进行文本数据预处理的流程。更具体地说，我们查看了使用 Python *和 NLTK* 进行的文本预处理流程。虽然我们没有进一步探讨使用 NLTK
    进行的数据预处理以外的内容，但理论上，该工具包可以用于进一步的分析任务。
- en: While NLTK is a great natural language... well, toolkit (hence the name), it
    is not optimized for building production systems. This may or may not be of consequence
    if using NLTK only to preprocess your data, but if you are planning an end-to-end
    NLP solution and are selecting an appropriate tool to build said system, it may
    make sense to preprocess your data with the same.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 NLTK 是一个很棒的自然语言... 好吧，工具包（所以名字叫做 NLTK），但它并不优化于构建生产系统。如果只是用 NLTK 来预处理数据，这可能无关紧要，但如果你计划构建一个端到端的
    NLP 解决方案，并且在选择适当的工具来构建该系统，那么使用同样的工具来预处理数据可能是有意义的。
- en: '![](../Images/f259bf6da4be96051a36006ff848f1cc.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f259bf6da4be96051a36006ff848f1cc.png)'
- en: While NLTK was built with *learning* NLP in mind, [spaCy](https://spacy.io/)
    is specifically designed with the goal of being a useful library for implementing
    production-ready systems.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 NLTK 是在 *学习* NLP 时构建的，[spaCy](https://spacy.io/) 则专门设计用于成为一个实现生产就绪系统的有用库。
- en: spaCy is designed to help you do real work — to build real products, or gather
    real insights. The library respects your time, and tries to avoid wasting it.
    It's easy to install, and its API is simple and productive. We like to think of
    spaCy as the Ruby on Rails of Natural Language Processing.
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: spaCy 旨在帮助你做实际的工作——构建实际的产品，或获取实际的见解。该库尊重你的时间，并试图避免浪费时间。它安装简单，API 简单高效。我们喜欢将
    spaCy 视为自然语言处理的 Ruby on Rails。
- en: spaCy is opinionated, in that it does not allow for as much mixing and matching
    of what could be considered NLP pipeline modules, the argument being that particular
    lemmatizers, for example, are not optimized to play well with particular tokenizers.
    While the tradeoff is less flexibility in **some** aspects of your NLP pipeline,
    the result should be increased performance.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 是有立场的，因为它不允许过多地混合和匹配可能被视为 NLP 流水线模块的内容，理由是特定的词形还原器，例如，并没有优化以良好地配合特定的分词器。尽管这种权衡在
    **某些** 方面减少了灵活性，但最终应该会提高性能。
- en: You can get a quick overview of spaCy and some of its design philosophy and
    choices [in this video](https://www.youtube.com/watch?v=jB1-NukGZm0), a talk given
    by spaCy developers [Matthew Honnibal](https://twitter.com/honnibal) and [Ines
    Montani](https://twitter.com/_inesmontani), from a SF Machine Learning meetup
    co-hosted with the Data Institute at USF.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过[这个视频](https://www.youtube.com/watch?v=jB1-NukGZm0)快速了解 spaCy 及其设计哲学和选择，这是由
    spaCy 开发者[Matthew Honnibal](https://twitter.com/honnibal)和[Ines Montani](https://twitter.com/_inesmontani)在美国旧金山的数据研究所联合主办的机器学习聚会上进行的讲座。
- en: spaCy bills itself as "the best way to prepare text for deep learning." Since
    much of the [previous walkthrough](/2018/03/text-data-preprocessing-walkthrough-python.html)
    did not use NLTK (the task-dependent noise removal as well as a few steps in the
    normalization process), we won't repeat the entire post here using spaCy instead
    of NLTK in particular spots, since that would be a waste of everyone's time. Instead,
    we will investigate how **some** of that same functionality we employed NLTK for
    could be accomplished with spaCy, and then move on to some additional tasks which
    **could** be considered preprocessing, depending on your end goal, but which may
    also bleed into the subsequent steps of [textual data science tasks](/2017/11/framework-approaching-textual-data-tasks.html).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 宣称自己是“为深度学习准备文本的最佳方式。”由于 [之前的演练](/2018/03/text-data-preprocessing-walkthrough-python.html)
    没有使用 NLTK（任务相关的噪声移除以及规范化过程中的一些步骤），我们不会在这里重复整个帖子，只是将 spaCy 替代 NLTK 的特定部分，因为那样会浪费大家的时间。相反，我们将探讨
    **一些** 用于 NLTK 的相同功能如何通过 spaCy 实现，然后进行一些额外的任务，这些任务 **可能** 被认为是预处理，取决于你的最终目标，但也可能会渗透到后续的
    [文本数据科学任务](/2017/11/framework-approaching-textual-data-tasks.html) 中。
- en: Getting Started
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开始使用
- en: Before doing anything, you need to have spaCy installed, as well as its English
    language model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行任何操作之前，你需要先安装 spaCy 以及它的英语语言模型。
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We need a sample of text to use:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个文本样本来使用：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now let's import spaCy, along with displaCy (for visualizing some of spaCy's
    modeling) and a list of English stop words (we'll use them below). We also load
    the English language model as a `Language` object (we will call it 'nlp' out of
    spaCy convention), and then call the nlp object on our sample text, which returns
    a processed `Doc` object (which we cleverly call 'doc').
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们导入 spaCy，以及 displaCy（用于可视化 spaCy 的模型）和一份英语停用词列表（我们将在下面使用）。我们还加载英语语言模型作为
    `Language` 对象（我们将按照 spaCy 的惯例称之为 'nlp'），然后对我们的样本文本调用 nlp 对象，这将返回一个处理过的 `Doc` 对象（我们巧妙地称之为
    'doc'）。
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And there you have it. From the [spaCy documentation](https://spacy.io/usage/spacy-101):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。从 [spaCy 文档](https://spacy.io/usage/spacy-101)：
- en: Even though a `Doc` is processed – e.g. split into individual words and annotated
    – it still holds **all information of the original text**, like whitespace characters.
    You can always get the offset of a token into the original string, or reconstruct
    the original by joining the tokens and their trailing whitespace. This way, you'll
    never lose any information when processing text with spaCy.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 即使 `Doc` 已经被处理——例如，拆分成单独的单词并进行了注释——它仍然保留了**原始文本的所有信息**，例如空白字符。你始终可以获取一个词元在原始字符串中的偏移量，或者通过连接词元及其后面的空白字符来重建原始文本。这样，在使用
    spaCy 处理文本时，你将不会丢失任何信息。
- en: This is core to spaCy's design philosophy; I encourage you to watch [this video](https://www.youtube.com/watch?v=jB1-NukGZm0).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 spaCy 设计理念的核心；我鼓励你观看 [这个视频](https://www.youtube.com/watch?v=jB1-NukGZm0)。
- en: Now let's try a few things. Note how little code is required to perform said
    things.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试一些操作。注意执行这些操作所需的代码量非常少。
- en: Tokenization
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分词
- en: 'Printing out tokens of our sample is straightforward:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出我们样本的词元是很简单的：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Recall from above that a `Doc` object is processed as it is passed to the `Language`
    object, and so tokenization has already been completed at this point; we are just
    accessing those existing tokens. From the [documentation](https://spacy.io/api/doc):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面回顾一下，`Doc` 对象在传递给 `Language` 对象时就已经处理完毕，因此在这个阶段分词已经完成；我们只是访问那些现有的词元。从 [文档](https://spacy.io/api/doc)：
- en: A Doc is a sequence of `Token` objects. Access sentences and named entities,
    export annotations to numpy arrays, losslessly serialize to compressed binary
    strings. The `Doc` object holds an array of `TokenC` structs. The Python-level
    `Token` and `Span` objects are views of this array, i.e. they don't own the data
    themselves.
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`Doc` 是一系列 `Token` 对象。访问句子和命名实体，将注释导出为 numpy 数组，进行无损压缩序列化到二进制字符串中。`Doc` 对象持有一个
    `TokenC` 结构体的数组。Python 层面的 `Token` 和 `Span` 对象是该数组的视图，即它们不拥有数据本身。'
- en: We didn't discuss the specifics of spaCy's implementation, but getting to know
    what its underlying code looks like -- specifically how it was written (and why)
    -- gives you the insight needed to understand why it's so very fast. For a third
    time, I encourage you to watch [this video](https://www.youtube.com/watch?v=jB1-NukGZm0).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有讨论 spaCy 实现的具体细节，但了解其底层代码的样子——尤其是它是如何编写的（以及为什么）——能帮助你理解它为何如此快速。我再一次鼓励你观看
    [这个视频](https://www.youtube.com/watch?v=jB1-NukGZm0)。
- en: 'Though it is not necessary, given differences in design, if you want to pull
    these tokens out into a list of their own (similar to how we did in the NLTK walkthrough):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不是必要的，但考虑到设计上的差异，如果你想将这些 tokens 提取到自己的列表中（类似于我们在 NLTK 演练中所做的）：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Identifying Stop Words
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 识别停用词
- en: 'Let''s identify stop words. We imported the word list above, so it''s just
    a matter of iterating through the tokens stored in the `Doc` object and performing
    a comparison:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们识别停用词。我们在上面导入了词汇表，所以只需迭代存储在 `Doc` 对象中的 tokens，并进行比较：
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that we did not touch the tokens list created above, and rely only on the
    pre-processed `Doc` object.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们没有触及上面创建的 tokens 列表，而只是依赖于预处理的 `Doc` 对象。
- en: Part-of-speech Tagging
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词性标注
- en: A `Doc` object holds `Token` objects, and you can read the `Token` [documentation](https://spacy.io/api/token)
    for an idea of what data each token already possesses before we even ask for it,
    and for specific info on what is being shown below.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `Doc` 对象包含 `Token` 对象，你可以查看 `Token` 的 [文档](https://spacy.io/api/token) 以了解每个
    token 在我们请求之前已经包含的数据，以及下面显示的具体信息。
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'While this is a lot of info as is, and could be useful in all sorts of ways
    for a given NLP goal, let''s simply visualize it using [displaCy](https://spacy.io/usage/visualizers)
    for a more concise view:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这本身信息量很大，并且对于特定的 NLP 目标可能以各种方式有用，但我们可以通过 [displaCy](https://spacy.io/usage/visualizers)
    来进行可视化，以获得更简洁的视图：
- en: '[PRE11]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![POS visualized](../Images/121e0dc7f6e5229b9afaab8f83a11158.png)](https://image.ibb.co/haQoeS/spacy_pos_visualized.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[![POS 可视化](../Images/121e0dc7f6e5229b9afaab8f83a11158.png)](https://image.ibb.co/haQoeS/spacy_pos_visualized.jpg)'
- en: Named Entity Recognition
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: 'A `Doc` object has already processed named entities as well. Access them with:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `Doc` 对象也已经处理了命名实体。用以下方法访问它们：
- en: '[PRE12]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As can be determined from the code above, first the named entity is output,
    followed by its starting character index in the document, then its ending character
    index, and finally its entity type (label).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的代码可以确定，首先输出命名实体，然后是其在文档中的起始字符索引，接着是结束字符索引，最后是实体类型（标签）。
- en: 'displaCy''s visualization is handy again, here:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: displaCy 的可视化再次显得非常实用：
- en: '[PRE14]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![NER visualized](../Images/e67a700939e14859796d357891c1e984.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![NER 可视化](../Images/e67a700939e14859796d357891c1e984.png)'
- en: 'I reiterate: scroll back over this post and look at how little code was required
    to accomplish the tasks we undertook.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我再强调一遍：回过头来看看这篇帖子，看看我们完成这些任务所需的代码是多么少。
- en: This only scratches the surface of what we may want to accomplish with NLP or
    a tool as powerful as spaCy. As should be obvious, NLP preprocessing tasks, as
    well as those of other data forms (along with text and string manipulation), can
    often be accomplished with a variety of strategies, tools, and libraries. In my
    view, spaCy excels due to its ease of use and API simplicity. I encourage anyone
    who has been following these NLP-related posts I have been writing to check spaCy
    out if they are serious about natural language processing.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅仅触及了我们可能希望通过 NLP 或像 spaCy 这样强大的工具实现的表面。显而易见，NLP 预处理任务以及其他数据形式的任务（包括文本和字符串操作）通常可以通过多种策略、工具和库来完成。在我看来，spaCy
    因其易用性和 API 简单性而表现出色。我鼓励那些一直关注我撰写的 NLP 相关帖子的人，如果他们对自然语言处理认真，可以去了解一下 spaCy。
- en: 'To gain more of an introduction to spaCy, I recommend these resources:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地了解 spaCy，我推荐这些资源：
- en: '[spaCy 101: Everything you need to know](https://spacy.io/usage/spacy-101)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[spaCy 101：你需要了解的一切](https://spacy.io/usage/spacy-101)'
- en: '[Language Processing Pipelines](https://spacy.io/usage/processing-pipelines)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[语言处理管道](https://spacy.io/usage/processing-pipelines)'
- en: '[Increasing data science productivity; founders of spaCy & Prodigy](https://www.youtube.com/watch?v=jB1-NukGZm0)
    (SF Machine Learning meetup talk)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提高数据科学生产力；spaCy 和 Prodigy 的创始人](https://www.youtube.com/watch?v=jB1-NukGZm0)（SF
    机器学习见面会演讲）'
- en: '[Matthew Honnibal - Designing spaCy: Industrial-strength NLP](https://www.youtube.com/watch?v=gJJQs47aUQ0)
    (PyData Berlin 2016 talk)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Matthew Honnibal - 设计 spaCy：工业级 NLP](https://www.youtube.com/watch?v=gJJQs47aUQ0)（PyData
    Berlin 2016 演讲）'
- en: '**Related**:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容**：'
- en: '[Text Data Preprocessing: A Walkthrough in Python](/2018/03/text-data-preprocessing-walkthrough-python.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本数据预处理：Python 中的操作流程](/2018/03/text-data-preprocessing-walkthrough-python.html)'
- en: '[A Framework for Approaching Textual Data Science Tasks](/2017/11/framework-approaching-textual-data-tasks.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[处理文本数据科学任务的框架](/2017/11/framework-approaching-textual-data-tasks.html)'
- en: '[A General Approach to Preprocessing Text Data](/2017/12/general-approach-preprocessing-text-data.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本数据预处理的一般方法](/2017/12/general-approach-preprocessing-text-data.html)'
- en: '* * *'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织在 IT 领域'
- en: '* * *'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解更多相关话题
- en: '[Natural Language Processing with spaCy](https://www.kdnuggets.com/2023/01/natural-language-processing-spacy.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 spaCy 进行自然语言处理](https://www.kdnuggets.com/2023/01/natural-language-processing-spacy.html)'
- en: '[Getting Started with spaCy for NLP](https://www.kdnuggets.com/2022/11/getting-started-spacy-nlp.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[开始使用 spaCy 进行 NLP](https://www.kdnuggets.com/2022/11/getting-started-spacy-nlp.html)'
- en: '[N-gram Language Modeling in Natural Language Processing](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理中的 N-gram 语言建模](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
- en: '[Natural Language Processing Key Terms, Explained](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理关键术语解释](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
- en: '[Data Representation for Natural Language Processing Tasks](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理任务的数据表示](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
- en: '[Transfer Learning for Image Recognition and Natural Language Processing](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[图像识别和自然语言处理的迁移学习](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
