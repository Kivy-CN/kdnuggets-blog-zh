- en: A checklist to track your Data Science progress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/05/checklist-data-science-progress.html](https://www.kdnuggets.com/2021/05/checklist-data-science-progress.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Pascal Janetzky](https://pascaljanetzky.medium.com/), Computer science
    student, sports fan.**'
  prefs: []
  type: TYPE_NORMAL
- en: There are many awesome courses to learn data science. And there are plenty of
    certificates that certify your success.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: But, how do you track your progress? How do you know what you have already achieved
    and what’s still there to get your hands on?
  prefs: []
  type: TYPE_NORMAL
- en: 'The following checklist can help you get an overview of your progress. It’s
    intended for users looking for a general outline of where they are on their journey.
    Rather than emphasizing specific guides, courses, and software packages, it focuses
    on general concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8aa744f86264be3e544544378299e67b.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Image by the author. Available on Notion [here](https://www.notion.so/A-checklist-to-track-your-Data-Science-progress-9de80b1b23c04634904168991247b651),
    as a PDF [here](https://drive.google.com/file/d/1CjuSlqhzjq1rUX5JYvJhx7YWIjxs5tdF/view?usp=sharing),
    and GitHub [here](https://github.com/phrasenmaeher/data-science-checklist).*'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s cover the levels in detail, beginning with the Entry level, continuing
    with the Intermediate level, and ending on the wide fields of the Advanced level.
  prefs: []
  type: TYPE_NORMAL
- en: Entry level
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the place to start. It covers the fundamentals of your further journey.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data handling**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The intention behind this category is to focus on being able to handle the
    most common data types:'
  prefs: []
  type: TYPE_NORMAL
- en: Images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audio/Time-Series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, the datasets at this stage are quite small, and there is no (mental)
    overhead from handling larger-than-memory datasets. Good examples are the classic
    MNIST image dataset ([PyTorch](https://pytorch.org/vision/stable/datasets.html#mnist), [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/mnist)),
    the IMDB reviews text dataset ([PyTorch](https://pytorch.org/text/stable/datasets.html#imdb), [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb)),
    and small audio or time-series datasets. They are only a few hundred MBs at most
    and comfortably fit into the RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Some of these datasets require minimal pre-processing (scaling images, shortening
    sentences), which is usually not more than a few lines of code. Since the number
    of examples is either very low or they are of small size only, you can easily
    do the processing during runtime (as opposed to running separate complex scripts
    in advance).
  prefs: []
  type: TYPE_NORMAL
- en: In summary, this category emphasizes handling small audio/time-series, image,
    and text datasets and applying simple operations to pre-process the data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Networks**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The intention behind this category is to transition from classic machine learning
    towards neural networks and knowing the common building blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: Classic machine learning techniques include Support Vector Machines, Linear
    Regression, and clustering algorithms. Even though the more sophisticated relatives,
    the neural networks, seem to dominate the recent research, they come in handy
    for small problems — or as a baseline. Knowing how to use them is also handy when
    it comes to analyzing the data.
  prefs: []
  type: TYPE_NORMAL
- en: When progressing towards deep learning, dense layers are a good start. I guess
    that they are used in nearly every (classification) model, at least to build the
    output layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second commonly used network type is the Convolutional Neural Network,
    which uses convolution operation at its core. It’s hard to think about any successful
    research that has not used and benefited from this simple operation: You slide
    a kernel over your input and then calculate the inner product between the kernel
    with the patch that it covers. The convolution operation extracts a small set
    of features from every possible location of the input data. It’s no wonder that
    they are very successful in classifying images, where important characteristics
    are scattered all around.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous two network types are mainly used for static inputs, where we
    know the data’s shape in advance. When you have data of varying shapes, take sentences
    as an example, you might look for more flexible approaches. That is where Recurrent
    Neural Networks come in: They can retain information over long periods, which
    enables connecting the “I” in “I, after getting up this morning and dressing,
    took a walk with my dog” and the “took a walk with my dog” to answer the question
    “Who took a walk with the dog?”'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, this category focuses on working with simple Dense, Convolutional,
    and Recurrent neural networks to classify image, text, and time-series data.
  prefs: []
  type: TYPE_NORMAL
- en: '**General**'
  prefs: []
  type: TYPE_NORMAL
- en: The intention behind this category is to learn the general handling of Data
    Science related tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'An important step is to get to know the data itself. This is not limited to
    image data or text data alone but also covers time-series and audio data and any
    further data type. The term exploratory data analysis (thanks to [Andryas Waurzenczak](https://medium.com/@andryaas) for
    pointing this out) describes this step best: Using techniques to find patterns,
    outliers (data points that exceed a common range), substructures, label distributions,
    and also visualizing the data. This might incorporate PCA or dimensionality reduction
    techniques. The datasets that you work with at this point are usually already
    explored in detail (try a search for the dataset''s name to find interesting characteristics),
    but learning this now will pay off once you proceed to custom datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will also learn how to load and save those models above so that you can
    re-use them later. What’s also common is to store data about the data, the *metadata*,
    in separate files. Take a CSV file as an example: The first column stores the
    file paths to your data samples, and the second column stores the class for the
    samples. Being able to parse this is mandatory, but thanks to many libraries,
    that is a straightforward task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you begin training your basic networks on those small datasets, with those
    architectures above, you’ll gradually uncover the helpfulness of callbacks. Callbacks
    are code that gets executed during training, and they implement many functionalities:
    Periodically saving your model to make it fail-safe, stopping the training, or
    changing parameters. The most common ones are already built-in with most libraries,
    and a short function call is all it takes to use them!'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, this category has you learn to handle the tasks around running neural
    networks on small datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Intermediate level
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In my opinion, this is where the great fun begins, and you’ll reach it faster
    than you might expect! I found the shift to happen quite naturally: You look for
    a more efficient way to process data — and before you realize it, you have written
    a custom pipeline for a large dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data handling**'
  prefs: []
  type: TYPE_NORMAL
- en: The intention behind this stage is to be able to handle larger or more complex
    datasets which might require special treatment in the form of augmentation and
    custom pre-processing pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, the datasets tend to become larger and might no longer fit into
    your RAM. Efficient pre-processing becomes more important, as you won’t let your
    hardware idle around. Also, you might have to write your own generators when you
    handle samples of unequal shapes, fetch samples from a database, or do custom
    pre-processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Or you work with complex and unbalanced datasets, where the majority of your
    samples is of one class, and only a minority of samples is of the desired class.
    There are a few helpful techniques you’ll learn: Augmenting the data to generate
    more samples of the minor class or downsampling the major class.'
  prefs: []
  type: TYPE_NORMAL
- en: For both dataset types, custom pipelines become more important. When you quickly
    want to iterate settings, for example, cropping an image to 32x32 rather than
    50x50, you rarely want to start long-running scripts. Incorporating such possibilities
    into your pipeline enables quick experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the datasets tend to become more complex (think unbalanced) and
    larger (think up to 30—40 GBs) and require more sophisticated handling.
  prefs: []
  type: TYPE_NORMAL
- en: '**Custom projects**'
  prefs: []
  type: TYPE_NORMAL
- en: This is the heart of the Intermediate level. The intention behind this is to
    work on custom projects and thereby learning and using many of the other categories’
    items.
  prefs: []
  type: TYPE_NORMAL
- en: The first level might have led you through training a network on the MNIST datasets.
    On this level, you’ll naturally focus more on your own data and how to parse it.
    I have only listed the three major domains audio, images, and text, but there
    are many more.
  prefs: []
  type: TYPE_NORMAL
- en: By working on your own projects, you connect what you have learned earlier to
    solve your challenges.
  prefs: []
  type: TYPE_NORMAL
- en: '**Training**'
  prefs: []
  type: TYPE_NORMAL
- en: The intention behind this category is to learn more about training neural networks.
    This category is the largest category on the Intermediate level, which is due
    to the focus on more advanced topics.
  prefs: []
  type: TYPE_NORMAL
- en: A first step to customizing training is using transfer learning and fine-tuning.
    Before, you will usually have used the standard methods to train and test your
    models, but now you might require something more powerful. That’s where it comes
    in handy to simply re-use models that other practitioners trained. You load their
    models and carefully train them on your own datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In case you miss some feature, this is the point where you begin to write custom
    training loops and custom callbacks. Want to print some statistics every 20 batches?
    Write a custom callback. Want to accumulate gradients over 20 batches? Write a
    custom training loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'More complex loops might need more resources: multi-GPU training is coming!
    However, it’s not simply adding a second, a third, or even more resources; you
    have to fetch the data fast enough. Keep in mind that multi-device training increases
    the complexity. Most of the background work is already done by PyTorch and TensorFlow,
    which handle this case with a few minor code changes only, but the front-end part
    is left to you. (But fret not, there are many guides out there to help you get
    this done!)'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are like me and do not have access to multiple GPUs, then training on
    the cloud is a possible next step. All major cloud services provide you with the
    requested resources. At first, there will be some (mental) overhead while transitioning
    from local setups to cloud computing. After some attempts, this gets way easier.
    (When I started to run scripts on a Kubernetes cluster, things were very overwhelming:
    What is a Dockerfile? How to run jobs? How to use local resources? How to get
    data in and out? After experimenting around, I got used to it, and now it’s a
    few simple commands to get my scripts running).'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you are already in the cloud, why not try TPU training? TPUs are Google’s
    custom-made chips. And they are fast: A year ago, I worked with a team to classify
    a large text corpus, around 20,000 documents, and each document had about 10 pages
    of text. We ran our first experiments on a CPU only, and one epoch took 8 hours.
    In the next step, we improved pre-processing, caching, and used a single GPU:
    The time went down to 15 minutes, which was a huge leap. After reading TensorFlow’s
    documentation and playing around with our code, I then managed to make TPU training
    possible, and one epoch went down to *15 seconds*. So, I encourage you to upgrade
    your pipeline and run your training on TPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, this category focuses on expanding the actual training parts towards
    more complex training loops and custom callbacks.
  prefs: []
  type: TYPE_NORMAL
- en: '**General**'
  prefs: []
  type: TYPE_NORMAL
- en: The intention behind this category is to learn more about what’s possible beyond
    using the normal architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you are dealing with custom datasets, it becomes important to get
    a grasp on problem thinking. Let me explain it this way: Say you are working with
    an audio dataset. You have done your initial data analysis, and it turns out that
    your data is not only imbalanced, but the samples are of different lengths. Some
    are only three seconds long, others 20 seconds and more. Further, you want to
    classify only segments of the audio, not the whole clip as is. And lastly, this
    is for an industry project, so restrictions apply. Bringing all this together
    is what’s required from you.'
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, you are not alone at this. Data analysis is ticked already, and pre-processing
    custom data is ticked too. It’s the industrial part that gets the main attention
    here. Check what other folks have done (and published on GitHub), ask your people,
    and experiment with different techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Say you want a network layer that normalizes your data following a custom scheme.
    Looking into your library’s documentation, you don’t find anything similar already
    implemented. Now is the time to implement such functionality yourself. As before, [TensorFlow](https://www.tensorflow.org/tutorials/customization/custom_layers) and
    PyTorch (which makes this an even simpler approach) provide some good resources
    to begin with.
  prefs: []
  type: TYPE_NORMAL
- en: For a project a while ago, I wanted to write a custom embedding layer. After
    finding no implementation, I decided to tackle this by writing a custom one. Now,
    that was a challenge! After many trial and error loops, I finally came up with
    a working TensorFlow layer. (Ironically, in the end, it did not prove better than
    the existing ones.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Another large field in Deep Learning opens up now: Generative networks. Before,
    you mainly worked with classifying existing data, but nothing is holding you back
    from also generating it. That’s exactly what generative networks do, and a large
    part of their recent success comes from one single paper: [Generative Adversarial
    Networks](https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf).
    The researchers really came up with something clever: Instead of training one
    network, two networks are trained, and both get better over time. Examining their
    workings in detail is out of scope here, but Joseph and Baptiste Rocca did a terrific
    job explaining them [here](https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29).'
  prefs: []
  type: TYPE_NORMAL
- en: Besides only using a wider variety of models, you’ll also want to track your
    experiments. This is helpful when you want to comprehend how your model’s metrics
    are influenced by its parameters. Speaking of which, you might also start a parameter
    search, which aims to find the best set of parameter which optimizes a target
    metric.
  prefs: []
  type: TYPE_NORMAL
- en: As in the last part, you will definitely use more advanced models. Besides those
    generative networks, there are also huge language models. Have you ever heard
    of Transformers (not the movie)? I bet you have, and now comes the time to use
    them for your own projects. Or try getting access to GPT-3, and discuss AI with
    it.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, this category expands beyond the normal models and explores further
    techniques around.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced level
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the last stage, though the boundaries to the previous stage are admittedly
    blurry. Congratulations on getting here! There is only one category for you to
    explore since the items build on your previous experience.
  prefs: []
  type: TYPE_NORMAL
- en: '**General**'
  prefs: []
  type: TYPE_NORMAL
- en: Even if you have come this far, there are still further things to explore.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first item is *huge datasets*. While starting the training process is a
    routine task for you now, the focus here is on making training fast, despite huge
    datasets. With *huge* I mean datasets of several hundred GBs and more. Think ImageNet
    scale or [The Pile](https://github.com/EleutherAI/the-pile) scale. Tackling them
    requires thinking through your storage options, data fetching, and pre-processing
    pipelines. And not only the size increases, but also the complexity: A multi-model
    dataset of a few hundred GBs, where each image is accompanied by a textual and
    auditive description? Now that requires some thinking.'
  prefs: []
  type: TYPE_NORMAL
- en: When working with such enormous datasets, it might be required to run a multi-worker
    training setup. The GPUs are no longer installed on a single machine but distributed
    across multiple machines, the workers. Managing the distribution of your workload
    and your data becomes necessary. Thankfully, [PyTorch](https://pytorch.org/tutorials/intermediate/dist_tuto.html) and [TensorFlow](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras?hl=en) have
    some resources on this, too.
  prefs: []
  type: TYPE_NORMAL
- en: After you have spent hours setting up your models, why not deploy them? Use
    some libraries that let you build a nice GUI easily (such as [streamlit.io](http://streamlit.io/))
    and deploy your models so that other practitioners can see your work live.
  prefs: []
  type: TYPE_NORMAL
- en: When you have also done that, get into Reinforcement Learning. Placing it in
    the advanced section might not be 100 percent correct (the basics can be grasped
    quite early), but it’s mostly distinct from the fields you have worked on so far.
    As always, there are [some](https://www.tensorflow.org/agents/overview) [resources](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf) [available](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) to
    get you started.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have gained some vast experience and knowledge, it’s time to do
    research. Have you noticed anything on your journey that could be improved? That
    could be the first thing to work on. Or contribute to other research projects
    by providing code to their repositories. Collaborate with other enthusiasts and
    keep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'That might be the last thing that’s left here: Staying up-to-date. Data Science
    is a fast-moving field, with many new exciting things coming up day after day.
    Keeping track of what matters is hard, so choose some newsletters (I read Andrew
    Ng and deeplearning.ai’s [The Batch](https://www.deeplearning.ai/the-batch/))
    to get condensed information. To narrow down, you can also use Andrew Karpathy’s
    Arxiv [Sanity Preserver](http://www.arxiv-sanity.com/), which helps you filter
    papers that meet your interests.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, this category focuses on exploring Reinforcement Learning as a completely
    new field, contributing to research, and staying up-to-date—the last two points
    are never truly ticked.
  prefs: []
  type: TYPE_NORMAL
- en: Where to go next?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can find the checklist on Notion [here](https://www.notion.so/A-checklist-to-track-your-Data-Science-progress-9de80b1b23c04634904168991247b651) and
    a PDF [here](https://drive.google.com/file/d/1CjuSlqhzjq1rUX5JYvJhx7YWIjxs5tdF/view?usp=sharing).
    Make a copy, customize it, and then gradually tick it off.
  prefs: []
  type: TYPE_NORMAL
- en: Please leave any suggestions or remarks on [GitHub](https://github.com/phrasenmaeher/data-science-checklist).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are looking for specific resources:'
  prefs: []
  type: TYPE_NORMAL
- en: Most of the entry-level is covered by deeplearning.ai’s [TensorFlow Developer
    Professional Certificate](https://www.coursera.org/professional-certificates/tensorflow-in-practice)
    (try the TF exam afterward to see what you have already learned!)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parts of the entry and intermediate and advanced levels are covered by Berkley’s [Full
    Stack Deep Learning](https://fall2019.fullstackdeeplearning.com/#who-is-this-for)
    course
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some parts of the intermediate and advanced levels are covered by DeepMind’s [Advanced
    Deep Learning & Reinforcement Learning](https://www.youtube.com/playlist?list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs)
    lecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parts of the intermediate and advanced levels are covered by deeplearning.ai’s [TensorFlow:
    Data and Deployment Specialization](https://www.coursera.org/specializations/tensorflow-data-and-deployment) and [TensorFlow:
    Advanced Techniques Specialization](https://www.coursera.org/specializations/tensorflow-advanced-techniques)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are looking for a more concrete list you can check Daniel Bourke’s roadmap [here](https://whimsical.com/machine-learning-roadmap-2020-CA7f3ykvXpnJ9Az32vYXva).
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*… is missing.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Leave a comment and let me know. Keep in mind that this checklist is intended
    as a general overview and does not contain *working with NumPy/pandas/TensorFlow/…* or
    similar specific items.
  prefs: []
  type: TYPE_NORMAL
- en: '*Isn’t there an overlap between some categories?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yes, definitely. There are no distinct dividing lines between some items from
    different categories. For example, when you work with images, you might have stored
    them on disk, and a CSV file holds all the file paths and labels. You can thus
    tick two items in one go.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, there is sometimes an overlap between two items within the same category.
    This happens when they mostly go hand-in-hand but are still somewhat different.
  prefs: []
  type: TYPE_NORMAL
- en: '*Is there an order?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'I originally tried to maintain order within each category (e.g., within *Training*),
    with the easiest task at the top and the harder tasks at the bottom. But there
    is not always a clear difficulty ranking between two items. Take *transfer learning* and *fine-tuning* as
    two examples: Both are tightly intertwined, and there’s no clear order.'
  prefs: []
  type: TYPE_NORMAL
- en: '*What’s so special about TPUs to give them a separate place?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: While it’s thankfully easy to go to [Colab](https://colab.research.google.com/) and
    select a TPU, it’s crucial to get the data ready first. My intention behind this
    point is not to simply having run a model with the help of TPUs, but to have made
    your pipelines very efficient so that data gets to the TPU on time. The focus
    is thus more on efficient pre-processing rather than on speeding up your computations.
  prefs: []
  type: TYPE_NORMAL
- en: And it’s admittedly quite cool to run your code on TPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '*The list is quite long. How can I complete it?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Two short answers:'
  prefs: []
  type: TYPE_NORMAL
- en: — You don’t have to; I am myself far away from this point. Use it as a mere
    guideline for your next endeavors.
  prefs: []
  type: TYPE_NORMAL
- en: — You don’t have to complete one level or category before advancing to the next
    one.
  prefs: []
  type: TYPE_NORMAL
- en: 'And a longer answer:'
  prefs: []
  type: TYPE_NORMAL
- en: Take one day per week, block all other things out.
  prefs: []
  type: TYPE_NORMAL
- en: On the other days, you can do your studies. Take a course on Wednesday, on Thursday,
    and Friday. Studying on the weekends is a bonus.
  prefs: []
  type: TYPE_NORMAL
- en: Then, on Monday, you repeat what you have learned over the last days, using
    the weekend to consolidate things.
  prefs: []
  type: TYPE_NORMAL
- en: And on your blocked Tuesday, you apply your new knowledge to your own projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'These projects don’t have to be big:'
  prefs: []
  type: TYPE_NORMAL
- en: Learned something about efficient data processing? Set up a pipeline for your
    own data (and tick *custom pipelines*).
  prefs: []
  type: TYPE_NORMAL
- en: Learned how to classify images? Take some images of stuff in your room, and
    classify them (and tick *custom image project*).
  prefs: []
  type: TYPE_NORMAL
- en: Learned about a specific network architecture? Use a library of your choice
    and simply re-implement it (and tick *advanced architectures* or *convolutional
    neural networks* or both).
  prefs: []
  type: TYPE_NORMAL
- en: Use these short standalone projects to get your hands on a broad range of topics.
    The more small topics you work on, the easier you can get started with those large
    projects that seemed daunting at first—because they only consist of many small
    steps you have already taken.
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/a-checklist-to-track-your-data-science-progress-bf92e878edf2).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[How to organize your data science project in 2021](https://www.kdnuggets.com/2021/04/how-organize-your-data-science-project-2021.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to frame the right questions to be answered using data](https://www.kdnuggets.com/2021/03/right-questions-answered-using-data.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Advice to aspiring Data Scientists – your most common questions answered](https://www.kdnuggets.com/2021/01/advice-aspiring-data-scientists.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
