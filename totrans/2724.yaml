- en: 'Essential Data Science Tips: How to Use One-Vs-Rest and One-Vs-One for Multi-Class
    Classification'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/08/one-vs-rest-one-multi-class-classification.html](https://www.kdnuggets.com/2020/08/one-vs-rest-one-multi-class-classification.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By Thomas Glare, Machine Learning professional and writer**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eddab375a6fe2bcdb19f51b4eb2cea5d.png)'
  prefs: []
  type: TYPE_IMG
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Only a few classification models aid multi-class classification. Specific algorithms,
    including logistic regression and perceptron, work best with binary classification
    and do not support more than two classes of classification tasks. The best alternative
    for solving [multi-class classification problems](https://en.wikipedia.org/wiki/Multiclass_classification)
    is splitting the multi-class datasets into multiple binary assemblies of data
    that can fit the binary classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms used in [binary classification](https://www.coursera.org/lecture/analytics-excel/introduction-to-binary-classification-TUihw)
    problems cannot work with multi-class tasks. Therefore, heuristic methods, such
    as one-vs-one and one-vs-rest, are used to split multi-class problems into multiple
    binary datasets and train the binary classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Binary vs. Multi-Class Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Classification problems are common in machine learning. In most cases, developers
    prefer using a supervised machine-learning approach to predict class tables for
    a given dataset. Unlike regression, classification involves designing the classifier
    model and training it to input and categorize the test dataset. For that, you
    can divide the dataset into either binary or multi-class modules.
  prefs: []
  type: TYPE_NORMAL
- en: As the name suggests, binary classification involves solving a problem with
    only two class labels. This makes it easy to filter the data, apply classification
    algorithms, and train the model to predict outcomes. On the other hand, multi-class
    classification is applicable when there are more than two class labels in the
    input train data. The technique enables developers to categorize the test data
    into multiple binary class labels.
  prefs: []
  type: TYPE_NORMAL
- en: That said, while binary classification requires only one classifier model, the
    one used in the multi-class approach depends on the classification technique.
    Below are the two models of the multi-class classification algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: One-Vs-Rest Classification Model for Multi-Class Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Also known as one-vs-all, the one-vs-rest model is a defined heuristic method
    that leverages a binary classification algorithm for multi-class classifications.
    The technique involves splitting a multi-class dataset into multiple sets of binary
    problems. Following this, a binary classifier is trained to handle each binary
    classification model with the most confident one making predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, with a multi-class classification problem with red, green, and
    blue datasets, binary classification can be categorized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem one: red vs. green/blue'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Problem two: blue vs. green/red'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Problem three: green vs. blue/red'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The only challenge of using this model is that you should create a model for
    every class. The three classes require three models from the above datasets, which
    can be challenging for large sets of data with million rows, slow models, such
    as neural networks and datasets with a significant number of classes.
  prefs: []
  type: TYPE_NORMAL
- en: The one-vs-rest approach requires individual models to prognosticate the probability-like
    score. The class index with the largest score is then used to predict a class.
    As such, it is commonly used for classification algorithms that can naturally
    predict scores or numerical class membership such as perceptron and logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: One-Vs-One Classification Model for Multi-Class Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Like the one-vs-all model, the one-vs-one is another excellent heuristic method
    that takes advantage of the binary classification algorithm for classifying multi-class
    datasets. It also splits multi-class datasets into binary classification problems.
    However, unlike the one-vs-rest model that breaks datasets into a single binary
    assembly of data for every class, the one-vs-one classification model groups datasets
    into one data file for every class versus every other class.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, [taking into consideration multi-class dataset problems](https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a)
    with four classes — blue, red, green and yellow — the one-vs-one approach splits
    it into the following six binary classification datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem 1: red vs. green'
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem 2: red vs. blue'
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem 3: red vs. yellow'
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem 4: green vs. yellow'
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem 5: blue vs. green'
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem 6: blue vs. yellow'
  prefs: []
  type: TYPE_NORMAL
- en: 'The above are more classification datasets compared to the one-vs-all approach
    explained before. As such, the formula used for calculating the total number of
    binary classification datasets becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of classes x (number of classes – 1)/2
  prefs: []
  type: TYPE_NORMAL
- en: 'From the four multi-class datasets above, this formula gives the expected six
    binary classification problems as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 4 x (4 - 1)/2
  prefs: []
  type: TYPE_NORMAL
- en: (4 x 3) /2
  prefs: []
  type: TYPE_NORMAL
- en: 12/2
  prefs: []
  type: TYPE_NORMAL
- en: =6
  prefs: []
  type: TYPE_NORMAL
- en: While each binary classification model, can predict a single class label, the
    one-vs-one strategy predicts the model with the most votes. If the binary classification
    models predict numerical class memberships accurately, such as probabilities,
    the class with the most sum score is taken as the class label.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, this model is best to support the classification of support vector
    machines and associated kernel-based classification algorithms. This is probably
    because kernel methods don’t scale in proportion to the size of the training dataset.
    Also, using the subsets of training data can revert this effect.
  prefs: []
  type: TYPE_NORMAL
- en: The support vector learning machine implementation within Scikit-learn facilitated
    by the [SVC class](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)
    supports the one-vs-one classification method of multi-class classification. To
    use this, you will have to change the settings on the “decision function shape”
    provision into “ovo.”
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the scikit-learn archive also allows for a separate one-vs-one
    classifier multi-class approach, making the one-vs-one option usable with any
    classifier. It enables this multi-class approach to be used with any other binary
    classifier, such as perceptron, logistic regression, SVM, or different classifiers
    supporting multi-class classification natively.
  prefs: []
  type: TYPE_NORMAL
- en: Bottom Line
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned, using the one-vs-rest multi-class classification option makes
    it challenging to handle large datasets due to a large number of class instances.
    However, the one-vs-one multi-class classification option only splits the primary
    dataset into a single binary classification for each pair of classes.
  prefs: []
  type: TYPE_NORMAL
- en: Although the one-vs-rest approach cannot handle multiple datasets, it trains
    less number of classifiers, making it a faster option and often preferred. On
    the other hand, the one-vs-one approach is less prone to creating an imbalance
    in the dataset due to dominance in specific classes.
  prefs: []
  type: TYPE_NORMAL
- en: That said, which one do you think provides the best approach of the two? Share
    your thoughts in the comments section below.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio:** Thomas Glare is a machine-learning professional who teaches developers
    how to get the best from modern machine learning methods and hands-on tutorials.
    You can find his articles on this [website](https://futureentech.com/write-business-blog-visitors/).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[A Classification Project in Machine Learning: a gentle step-by-step guide](https://www.kdnuggets.com/2020/06/classification-project-machine-learning-guide.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A simple and interpretable performance measure for a binary classifier](https://www.kdnuggets.com/2020/03/interpretable-performance-measure-binary-classifier.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Classify A Rare Event Using 5 Machine Learning Algorithms](https://www.kdnuggets.com/2020/01/classify-rare-event-machine-learning-algorithms.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Classification Metrics Walkthrough: Logistic Regression with…](https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[More Performance Evaluation Metrics for Classification Problems You…](https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introduction to Binary Classification with PyCaret](https://www.kdnuggets.com/2021/12/introduction-binary-classification-pycaret.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Fine-Tuning BERT for Tweets Classification with HuggingFace](https://www.kdnuggets.com/2022/01/finetuning-bert-tweets-classification-ft-hugging-face.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning Algorithms for Classification](https://www.kdnuggets.com/2022/03/machine-learning-algorithms-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Logistic Regression for Classification](https://www.kdnuggets.com/2022/04/logistic-regression-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
