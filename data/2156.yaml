- en: How to Use Hugging Face AutoTrain to Fine-tune LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用 Hugging Face AutoTrain 对LLM进行微调
- en: 原文：[https://www.kdnuggets.com/how-to-use-hugging-face-autotrain-to-finetune-llms](https://www.kdnuggets.com/how-to-use-hugging-face-autotrain-to-finetune-llms)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/how-to-use-hugging-face-autotrain-to-finetune-llms](https://www.kdnuggets.com/how-to-use-hugging-face-autotrain-to-finetune-llms)
- en: '![How to Use Hugging Face AutoTrain to Fine-tune LLMs](../Images/9535954ddf526bd0261f1d923485c8ba.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![如何使用 Hugging Face AutoTrain 对LLM进行微调](../Images/9535954ddf526bd0261f1d923485c8ba.png)'
- en: Image by Editor
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑器提供的图片
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全领域'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - 支持您的组织进行IT工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In recent years, the Large Language Model (LLM) has changed how people work
    and has been used in many fields, such as education, marketing, research, etc.
    Given the potential, LLM can be enhanced to solve our business problems better.
    This is why we could perform LLM fine-tuning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLM）改变了人们的工作方式，并已在教育、营销、研究等多个领域得到应用。鉴于其潜力，LLM 可以被增强以更好地解决我们的业务问题。这就是我们可以进行LLM微调的原因。
- en: We want to fine-tune our LLM for several reasons, including adopting specific
    domain use cases, improving the accuracy, data privacy and security, controlling
    the model bias, and many others. With all these benefits, it’s essential to learn
    how to fine-tune our LLM to have one in production.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望对LLM进行微调的原因包括采用特定领域的用例、提高准确性、数据隐私和安全、控制模型偏见等。鉴于所有这些好处，学习如何对LLM进行微调是非常重要的，以便将其投入生产。
- en: One way to perform LLM fine-tuning automatically is by using [Hugging Face’s
    AutoTrain](https://huggingface.co/docs/autotrain/v0.6.10/index). The HF AutoTrain
    is a no-code platform with Python API  to train state-of-the-art models for various
    tasks such as Computer Vision, Tabular, and NLP tasks. We can use the AutoTrain
    capability even if we don’t understand much about the LLM fine-tuning process.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自动进行LLM微调的一种方法是使用[Hugging Face 的 AutoTrain](https://huggingface.co/docs/autotrain/v0.6.10/index)。HF
    AutoTrain 是一个无代码平台，提供 Python API，用于训练各种任务（如计算机视觉、表格数据和自然语言处理任务）的先进模型。即使我们对LLM微调过程了解不多，我们仍然可以利用
    AutoTrain 的能力。
- en: So, how does it work? Let’s explore further.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，它是如何工作的呢？让我们进一步探索。
- en: Getting Started with AutoTrain
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用 AutoTrain
- en: Even if HF AutoTrain is a no-code solution, we can develop it on top of the
    AutoTrain using Python API. We would explore the code routes as the no-code platform
    isn’t stable for training. However, if you want to use the no-code platform, We
    can create the AutoTrain space using the following [page](https://huggingface.co/new-space?template=autotrain-projects/autotrain-advanced).
    The overall platform will be shown in the image below.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 即使 HF AutoTrain 是一个无代码解决方案，我们仍然可以在 AutoTrain 的基础上使用 Python API 进行开发。我们将探索代码路径，因为无代码平台在训练时并不稳定。然而，如果你想使用无代码平台，我们可以通过以下[页面](https://huggingface.co/new-space?template=autotrain-projects/autotrain-advanced)创建
    AutoTrain 空间。整体平台将在下图中显示。
- en: '![How to Use Hugging Face AutoTrain to Fine-tune LLMs](../Images/9a19c37473b3ada3812e03be6c43a855.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![如何使用 Hugging Face AutoTrain 对LLM进行微调](../Images/9a19c37473b3ada3812e03be6c43a855.png)'
- en: Image by Author
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: To fine-tune the LLM with Python API, we need to install the Python package,
    which you can run using the following code.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Python API 对LLM进行微调，我们需要安装 Python 包，可以使用以下代码运行。
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Also, we would use the Alpaca sample dataset from [HuggingFace](https://huggingface.co/datasets/tatsu-lab/alpaca),
    which required datasets package to acquire.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还会使用来自[HuggingFace](https://huggingface.co/datasets/tatsu-lab/alpaca)的Alpaca样本数据集，该数据集需要通过数据集包进行获取。
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then, use the following code to acquire the data we need.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用以下代码获取我们需要的数据。
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Additionally, we would save the data in the CSV format as we would need them
    for our fine-tuning.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们会将数据保存为CSV格式，因为我们在微调过程中需要它们。
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: With the environment and the dataset ready, let’s try to use HuggingFace AutoTrain
    to fine-tune our LLM.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在环境和数据集准备好之后，让我们尝试使用 HuggingFace AutoTrain 来微调我们的 LLM。
- en: Fine-tuning Procedure and Evaluation
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调过程和评估
- en: I would adapt the fine-tuning process from the AutoTrain example, which we can
    find [here](https://colab.research.google.com/github/huggingface/autotrain-advanced/blob/main/colabs/AutoTrain_LLM.ipynb).
    To start the process, we put the data we would use to fine-tune in the folder
    called data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我将从 AutoTrain 示例中调整微调过程，我们可以在 [这里](https://colab.research.google.com/github/huggingface/autotrain-advanced/blob/main/colabs/AutoTrain_LLM.ipynb)
    找到。为了开始这个过程，我们将要微调的数据放入名为 data 的文件夹中。
- en: '![How to Use Hugging Face AutoTrain to Fine-tune LLMs](../Images/d43661e233bf6f5c8ac7bc62b0def2fc.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![如何使用 Hugging Face AutoTrain 微调 LLM](../Images/d43661e233bf6f5c8ac7bc62b0def2fc.png)'
- en: Image by Author
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: For this tutorial, I try to sample only 100 row data so our training process
    can be much more swifter. After we have our data ready, we could use our Jupyter
    Notebook to fine-tune our model. Make sure the data contain ‘text’ column as the
    AutoTrain would read from that column only.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我尝试仅采样 100 行数据，以便我们的训练过程可以更快。数据准备好后，我们可以使用 Jupyter Notebook 来微调模型。确保数据中包含‘text’列，因为
    AutoTrain 只会从该列读取。
- en: First, let’s run the AutoTrain setup using the following command.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用以下命令运行 AutoTrain 设置。
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, we would provide an information required for AutoTrain to run. For the
    following one is the information about the project name and the pre-trained model
    you want. You can only choose the model that was available in the HuggingFace.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将提供运行 AutoTrain 所需的信息。下面的信息是关于项目名称和你想要的预训练模型。你只能选择 HuggingFace 中可用的模型。
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Then we would add HF information, if you want push your model to teh repository
    or using a private model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将添加 HF 信息，如果你想将模型推送到仓库或使用私有模型。
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Lastly, we would initiate the model parameter information in the variables below.
    You can change them as you like to see if the result is good or not.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将在下面的变量中初始化模型参数信息。你可以根据需要修改这些变量，以查看结果是否良好。
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With all the information is ready, we would set up the environment to accept
    all the information we have set up previously.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有信息准备好之后，我们将设置环境以接受我们之前设置的所有信息。
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To run the AutoTrain in our notebook, we would use the following command.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 notebook 中运行 AutoTrain，我们将使用以下命令。
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If you run the AutoTrain successfully, you should find the following folder
    in your directory with all the model and tokenizer producer by AutoTrain.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你成功运行 AutoTrain，你应该在目录中找到以下文件夹，其中包含 AutoTrain 生成的所有模型和标记器。
- en: '![How to Use Hugging Face AutoTrain to Fine-tune LLMs](../Images/48f5dfbec3ac40fe83b88dfa81ee3637.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![如何使用 Hugging Face AutoTrain 微调 LLM](../Images/48f5dfbec3ac40fe83b88dfa81ee3637.png)'
- en: Image by Author
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: To test the model, we would use the HuggingFace transformers package with the
    following code.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试模型，我们将使用 HuggingFace transformers 包，代码如下。
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Then, we can try to evaluate our model based on the training input we have given.
    For example, we use the "Health benefits of regular exercise" as the input.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以根据提供的训练输入来评估模型。例如，我们使用“规律锻炼的健康益处”作为输入。
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![How to Use Hugging Face AutoTrain to Fine-tune LLMs](../Images/4ba169bff91d6f375bd16de96a9d151f.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![如何使用 Hugging Face AutoTrain 微调 LLM](../Images/4ba169bff91d6f375bd16de96a9d151f.png)'
- en: The result is certainly still could be better, but at least it’s closer to the
    sample data we have provided. We can try to playing around with the pre-trained
    model and the parameter to improve the fine-tuning.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 结果肯定还有提升空间，但至少比我们提供的样本数据更接近。我们可以尝试调整预训练模型和参数来改善微调效果。
- en: Tips for Successful Fine-tuning
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成功微调的技巧
- en: 'There are few best practices that you might want to know to improve the fine-tuning
    process, including:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些最佳实践你可能想要了解，以改善微调过程，包括：
- en: Prepare our dataset with the quality matching the representative task,
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备我们的数据集，确保质量符合代表性任务，
- en: Study the pre-trained model that we used,
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 研究我们使用的预训练模型，
- en: Use an appropriate regularization techniques to avoid overfitting,
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用适当的正则化技术以避免过拟合，
- en: Trying out the learning rate from smaller and gradually become bigger,
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试从较小的学习率开始，逐渐增大，
- en: Use fewer epoch as the training as LLM usually learn the new data quite fast,
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用更少的 epoch 进行训练，因为 LLM 通常对新数据的学习速度较快，
- en: Don’t ignore the computational cost, as it would become higher with bigger data,
    parameter, and model,
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不要忽视计算成本，因为随着数据、参数和模型的增大，计算成本会变得更高，
- en: Make sure you follow the ethical consideration regarding the data you use.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保你遵循有关数据使用的伦理考虑。
- en: Conclusion
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Fine-tuning our Large Language Model is beneficial to our business process,
    especially if there are certain requirements that we required. With the HuggingFace
    AutoTrain, we can boost up our training process and easily using the available
    pre-trained model to fine-tune the model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 微调我们的大型语言模型对我们的业务流程有益，特别是当有特定要求时。通过 HuggingFace AutoTrain，我们可以加快训练过程，并轻松使用现有的预训练模型来进行微调。
- en: '**[](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**[Cornellius Yudha
    Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**** is a data science
    assistant manager and data writer. While working full-time at Allianz Indonesia,
    he loves to share Python and data tips via social media and writing media. Cornellius
    writes on a variety of AI and machine learning topics.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**[](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**[Cornellius Yudha
    Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**** 是数据科学助理经理和数据撰稿人。在全职工作于
    Allianz Indonesia 的同时，他喜欢通过社交媒体和撰写媒体分享 Python 和数据技巧。Cornellius 涉及多种 AI 和机器学习主题的撰写。'
- en: More On This Topic
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[How to Finetune Mistral AI 7B LLM with Hugging Face AutoTrain](https://www.kdnuggets.com/how-to-finetune-mistral-ai-7b-llm-with-hugging-face-autotrain)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 Hugging Face AutoTrain 微调 Mistral AI 7B LLM](https://www.kdnuggets.com/how-to-finetune-mistral-ai-7b-llm-with-hugging-face-autotrain)'
- en: '[How to Use GPT for Generating Creative Content with Hugging Face…](https://www.kdnuggets.com/how-to-use-gpt-for-generating-creative-content-with-hugging-face-transformers)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 GPT 生成创意内容与 Hugging Face…](https://www.kdnuggets.com/how-to-use-gpt-for-generating-creative-content-with-hugging-face-transformers)'
- en: '[How to Use the Hugging Face Tokenizers Library to Preprocess Text Data](https://www.kdnuggets.com/how-to-use-the-hugging-face-tokenizers-library-to-preprocess-text-data)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 Hugging Face Tokenizers 库来预处理文本数据](https://www.kdnuggets.com/how-to-use-the-hugging-face-tokenizers-library-to-preprocess-text-data)'
- en: '[How to Use Hugging Face’s Datasets Library for Efficient Data Loading](https://www.kdnuggets.com/how-to-use-hugging-faces-datasets-library-for-efficient-data-loading)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 Hugging Face 的 Datasets 库进行高效数据加载](https://www.kdnuggets.com/how-to-use-hugging-faces-datasets-library-for-efficient-data-loading)'
- en: '[Top 10 Machine Learning Demos: Hugging Face Spaces Edition](https://www.kdnuggets.com/2022/05/top-10-machine-learning-demos-hugging-face-spaces-edition.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[前 10 大机器学习演示：Hugging Face Spaces 版](https://www.kdnuggets.com/2022/05/top-10-machine-learning-demos-hugging-face-spaces-edition.html)'
- en: '[A community developing a Hugging Face for customer data modeling](https://www.kdnuggets.com/2022/08/objectiv-community-developing-hugging-face-customer-data-modeling.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一个社区正在为客户数据建模开发 Hugging Face](https://www.kdnuggets.com/2022/08/objectiv-community-developing-hugging-face-customer-data-modeling.html)'
