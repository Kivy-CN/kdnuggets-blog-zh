["```py\ngunicorn wsgi:app\n```", "```py\nlocust --host=[http://localhost:8000](http://localhost:8000/) # host of the API you want to load test\n```", "```py\ngunicorn wsgi:app --workers=5\n```", "```py\ngunicorn wsgi:app --threads=20\n```", "```py\ndocker run -p 8501:8501 \\\n--mount type=bind,source=/path/to/my_model/,target=/models/my_model \\\n-e MODEL_NAME=my_model -it tensorflow/serving\n```", "```py\nmax_batch_size { value: 20 }\nbatch_timeout_micros { value: 500000 }\nmax_enqueued_batches { value: 100 }\nnum_batch_threads { value: 8 }\n```", "```py\ndocker run -p 8501:8501 \\\n--mount type=bind,source=/path/to/my_model/,target=/models/my_model \\\n--mount type=bind,source=/path/to/batching.cfg,target=/batching.cfg \\\n-e MODEL_NAME=my_model -it tensorflow/serving \\\n--enable_batching --batching_parameters_file=/batching.cfg\n```"]