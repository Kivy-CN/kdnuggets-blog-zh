- en: Your Guide to Natural Language Processing (NLP)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你在自然语言处理（NLP）方面的指南
- en: 原文：[https://www.kdnuggets.com/2019/05/guide-natural-language-processing-nlp.html](https://www.kdnuggets.com/2019/05/guide-natural-language-processing-nlp.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/05/guide-natural-language-processing-nlp.html](https://www.kdnuggets.com/2019/05/guide-natural-language-processing-nlp.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Diego Lopez Yse](https://twitter.com/LopezYse), Moody''s Operations LATAM**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由[Diego Lopez Yse](https://twitter.com/LopezYse)，穆迪LATAM运营**。'
- en: '![](../Images/fbc813c836c8dc6c3bb83c0cc408e48a.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbc813c836c8dc6c3bb83c0cc408e48a.png)'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Everything we express (either verbally or in written) carries huge amounts of
    information. The topic we choose, our tone, our selection of words, everything
    adds some type of information that can be interpreted and value extracted from
    it. In theory, we can understand and even predict human behaviour using that information.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们表达的每一件事（无论是口头的还是书面的）都携带大量的信息。我们选择的话题、语气、词汇选择，一切都增加了一些可以被解释并从中提取价值的信息。从理论上讲，我们可以利用这些信息来理解甚至预测人类行为。
- en: 'But there is a problem: one person may generate hundreds or thousands of words
    in a declaration, each sentence with its corresponding complexity. If you want
    to scale and analyze several hundreds, thousands or millions of people or declarations
    in a given geography, then the situation is unmanageable.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但有一个问题：一个人可能在声明中生成数百或数千个单词，每句话都有其对应的复杂性。如果你想在特定地理区域内扩展并分析数百、数千或数百万人或声明，那么情况就会变得难以管理。
- en: Data generated from conversations, declarations or even tweets are examples
    of unstructured data. **Unstructured data** doesn’t fit neatly into the traditional
    row and column structure of relational databases, and represent the vast majority
    of data available in the actual world. It is messy and hard to manipulate. Nevertheless,
    thanks to the advances in disciplines like machine learning a big revolution is
    going on regarding this topic. Nowadays it is no longer about trying to interpret
    a text or speech based on its keywords (the old fashioned mechanical way), but
    about understanding the meaning behind those words (the cognitive way). This way
    it is possible to detect figures of speech like irony, or even perform sentiment
    analysis.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从对话、声明甚至推文中生成的数据是非结构化数据的例子。**非结构化数据**不适合传统的关系数据库中的行列结构，代表了现实世界中绝大多数的数据。它是杂乱的，难以操作。然而，得益于机器学习等学科的进步，这个话题正经历一场巨大的革命。如今，不再是尝试基于关键词（传统的机械方式）来解释文本或语音，而是理解这些词汇背后的含义（认知方式）。这样可以检测到修辞手法，如讽刺，甚至进行情感分析。
- en: '***Natural Language Processing**** or NLP is a field of Artificial Intelligence
    that gives the machines the ability to read, understand and derive meaning from
    human languages.*'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***自然语言处理***或NLP是人工智能的一个领域，它赋予机器阅读、理解和推断人类语言含义的能力。'
- en: It is a discipline that focuses on the interaction between data science and
    human language, and is scaling to lots of industries. Today NLP is booming thanks
    to the huge improvements in the access to data and the increase in computational
    power, which are allowing practitioners to achieve meaningful results in areas
    like healthcare, media, finance and human resources, among others.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个专注于数据科学与人类语言之间互动的学科，并且正在扩展到许多行业。如今，由于对数据获取的巨大改进和计算能力的提升，NLP正在蓬勃发展，这使得从业者能够在医疗保健、媒体、金融和人力资源等领域取得有意义的成果。
- en: '**Use Cases of NLP**'
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**NLP的使用案例**'
- en: In simple terms, NLP represents the automatic handling of natural human language
    like speech or text, and although the concept itself is fascinating, the real
    value behind this technology comes from the use cases.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，NLP代表了对自然人类语言（如语音或文本）的自动处理，虽然这个概念本身很迷人，但这种技术的真正价值来自于它的实际应用场景。
- en: 'NLP can help you with lots of tasks and the fields of application just seem
    to increase on a daily basis. Let’s mention some examples:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: NLP可以帮助您完成许多任务，其应用领域似乎每天都在增加。让我们举一些例子：
- en: NLP enables the recognition and **prediction of diseases **based on electronic
    health records and patient’s own speech. This capability is being explored in
    health conditions that go from cardiovascular diseases to depression and even
    schizophrenia. For example, Amazon Comprehend Medical is a service that uses NLP
    to [extract disease conditions](https://www.thenewsminute.com/article/tech-giants-india-join-ai-bandwagon-focus-healthcare-93833),
    medications and treatment outcomes from patient notes, clinical trial reports
    and other electronic health records.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP使得基于电子健康记录和患者自身语音的疾病**识别和预测**成为可能。这项能力正在被探索用于从心血管疾病到抑郁症甚至精神分裂症的健康状况。例如，Amazon
    Comprehend Medical是一项利用NLP来[提取疾病状况](https://www.thenewsminute.com/article/tech-giants-india-join-ai-bandwagon-focus-healthcare-93833)、药物和治疗结果的服务，从患者笔记、临床试验报告和其他电子健康记录中提取信息。
- en: Organizations can determine what customers are saying about a service or product
    by identifying and extracting information in sources like social media. This [**sentiment
    analysis**](https://towardsdatascience.com/sentiment-analysis-concept-analysis-and-applications-6c94d6f58c17)can
    provide a lot of information about customers choices and their decision drivers.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组织可以通过识别和提取社交媒体等来源的信息来确定客户对服务或产品的评价。这种[**情感分析**](https://towardsdatascience.com/sentiment-analysis-concept-analysis-and-applications-6c94d6f58c17)可以提供关于客户选择及其决策驱动因素的大量信息。
- en: '[An inventor at IBM developed a **cognitive assistant**](https://www.theatlantic.com/technology/archive/2016/01/sorry-dave-afraid-i-cant-do-that/431559/)that
    works like a personalized search engine by learning all about you and then remind
    you of a name, a song, or anything you can’t remember the moment you need it to.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[IBM的一位发明家开发了一种**认知助手**](https://www.theatlantic.com/technology/archive/2016/01/sorry-dave-afraid-i-cant-do-that/431559/)，它像个个性化搜索引擎，通过了解您的所有信息，然后在您需要时提醒您一个名字、一首歌或任何您记不起来的东西。'
- en: Companies like Yahoo and Google filter and classify your emails with NLP by
    analyzing text in emails that flow through their servers and **stopping spam**before
    they even enter your inbox.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像Yahoo和Google这样的公司通过分析流经其服务器的电子邮件文本来过滤和分类您的邮件，并**阻止垃圾邮件**，即使在它们到达您的收件箱之前。
- en: To help **identifying fake news**, the [NLP Group at MIT](http://nlp.csail.mit.edu/)developed
    a new system to determine if a source is accurate or politically biased, detecting
    if a news source can be trusted or not.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了帮助**识别虚假新闻**，[麻省理工学院的NLP小组](http://nlp.csail.mit.edu/)开发了一种新系统，用于判断信息源是否准确或具有政治偏见，检测新闻源是否值得信赖。
- en: Amazon’s Alexa and Apple’s Siri are examples of intelligent **voice driven interfaces**that
    use NLP to respond to vocal prompts and do everything like find a particular shop,
    tell us the weather forecast, suggest the best route to the office or turn on
    the lights at home.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊的Alexa和苹果的Siri是**语音驱动接口**的例子，它们使用NLP来响应语音提示，并完成所有任务，如查找特定商店、告诉我们天气预报、建议最佳路线或打开家里的灯。
- en: 'Having an insight into what is happening and what people are talking about
    can be very valuable to [**financial traders**](https://news.efinancialcareers.com/nl-en/331386/charles-elkan-goldman-sachs-machine-learning).
    NLP is being used to track news, reports, comments about possible mergers between
    companies, everything can be then incorporated into a trading algorithm to generate
    massive profits. Remember: buy the rumor, sell the news.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解发生了什么以及人们在谈论什么对[**金融交易员**](https://news.efinancialcareers.com/nl-en/331386/charles-elkan-goldman-sachs-machine-learning)来说非常宝贵。NLP（自然语言处理）被用来追踪新闻、报告、关于公司可能合并的评论，一切都可以被纳入交易算法中以产生巨额利润。记住：买入谣言，卖出新闻。
- en: NLP is also being used in both the search and selection phases of [**talent
    recruitment**](https://www.forbes.com/sites/forbeshumanresourcescouncil/2018/09/27/how-ai-makes-recruiting-more-human/#7531fc116ba4),
    identifying the skills of potential hires and also spotting prospects before they
    become active on the job market.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP也被应用于[**人才招聘**](https://www.forbes.com/sites/forbeshumanresourcescouncil/2018/09/27/how-ai-makes-recruiting-more-human/#7531fc116ba4)的搜索和筛选阶段，识别潜在雇员的技能，并在他们进入就业市场之前发现有潜力的候选人。
- en: Powered by IBM Watson NLP technology, [LegalMation](https://www.legalmation.com/)developed
    a platform to automate routine** litigation tasks** and help legal teams save
    time, drive down costs and shift strategic focus.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由IBM Watson NLP技术支持，[LegalMation](https://www.legalmation.com/)开发了一个平台，自动化日常**诉讼任务**，帮助法律团队节省时间，降低成本，并转移战略重点。
- en: NLP is particularly booming in the **healthcare industry**. This technology
    is improving care delivery, disease diagnosis and bringing costs down while healthcare
    organizations are going through a growing adoption of electronic health records.
    The fact that clinical documentation can be improved means that patients can be
    better understood and benefited through better healthcare. The goal should be
    to optimize their experience, and several organizations are already working on
    this.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: NLP在**医疗行业**特别蓬勃发展。这项技术改善了护理服务、疾病诊断，并降低了成本，同时医疗机构在电子健康记录的采用上也在不断增长。临床文档的改进意味着患者可以通过更好的医疗得到更好的理解和利益。目标应是优化他们的体验，许多组织已经在这方面进行工作。
- en: '![](../Images/d488f02f8a32cceea5f6df26d4df9ef6.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d488f02f8a32cceea5f6df26d4df9ef6.png)'
- en: Number of publications containing the sentence “natural language processing”
    in PubMed in the period 1978–2018\. As of 2018, PubMed comprised more than 29
    million citations for biomedical literature
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: PubMed在1978–2018年期间包含“自然语言处理”这句话的出版物数量。到2018年，PubMed包含了超过2900万条生物医学文献引用。
- en: Companies like [Winterlight Labs](https://winterlightlabs.com/) are making huge
    improvements in the treatment of Alzheimer’s disease by monitoring cognitive impairment
    through speech and they can also support clinical trials and studies for a wide
    range of central nervous system disorders. Following a similar approach, Stanford
    University developed [Woebot](https://woebot.io/), a **chatbot therapist** with
    the aim of helping people with anxiety and other disorders.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 像[Winterlight Labs](https://winterlightlabs.com/)这样的公司，通过语音监测认知障碍在阿尔茨海默病治疗方面取得了巨大进展，他们还可以支持各种中枢神经系统疾病的临床试验和研究。采用类似的方法，斯坦福大学开发了[Woebot](https://woebot.io/)，一个**聊天机器人治疗师**，旨在帮助有焦虑和其他障碍的人。
- en: But serious [controversy](https://www.bmj.com/content/358/bmj.j3159) is around
    the subject. A couple of years ago Microsoft demonstrated that by analyzing large
    samples of search engine queries, they could [identify internet users who were
    suffering from pancreatic cancer](https://www.nytimes.com/2016/06/08/technology/online-searches-can-identify-cancer-victims-study-finds.html) even
    before they have received a diagnosis of the disease. How would users react to
    such diagnosis? And what would happen if you were tested as a false positive?
    (meaning that you can be diagnosed with the disease even though you don’t have
    it). This recalls the case of Google Flu Trends which in 2009 was announced as
    being able to predict influenza but later on vanished due to its low accuracy
    and inability to meet its projected rates.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 但围绕这一主题存在严重的[争议](https://www.bmj.com/content/358/bmj.j3159)。几年前，微软演示了通过分析大量搜索引擎查询样本，他们可以[识别出正在遭受胰腺癌的互联网用户](https://www.nytimes.com/2016/06/08/technology/online-searches-can-identify-cancer-victims-study-finds.html)，即使在他们收到疾病诊断之前。用户对这种诊断会有何反应？如果你被错误地标记为阳性（即使你没有该疾病）会发生什么？这让人想起Google
    Flu Trends，在2009年宣布能够预测流感，但由于准确性低和未能达到预期而消失。
- en: NLP may be the key to an effective clinical support in the future, but there
    are still many challenges to face in the short term.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: NLP可能是未来有效临床支持的关键，但短期内仍面临许多挑战。
- en: '**Basic NLP to impress your non-NLP friends**'
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**基础NLP技术让你的非NLP朋友刮目相看**'
- en: 'The main drawbacks we face these days with NLP relate to the fact that language
    is very tricky. The process of understanding and manipulating language is extremely
    complex, and for this reason it is common to use different techniques to handle
    different challenges before binding everything together. Programming languages
    like Python or R are highly used to perform these techniques, but before diving
    into code lines (that will be the topic of a different article), it’s important
    to understand the concepts beneath them. Let’s summarize and explain some of the
    most frequently used algorithms in NLP when defining the vocabulary of terms:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当前我们面临的主要缺点与自然语言处理（NLP）相关，语言本身非常复杂。理解和处理语言的过程极其复杂，因此通常需要使用不同的技术来应对不同的挑战，然后再将一切结合起来。像Python或R这样的编程语言被广泛用于执行这些技术，但在深入代码行之前（这将是另一篇文章的主题），理解其背后的概念非常重要。让我们总结并解释一些在定义术语词汇表时最常用的NLP算法：
- en: '**Bag of Words**'
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**词袋模型**'
- en: Is a commonly used model that allows you to count all words in a piece of text.
    Basically it creates an occurrence matrix for the sentence or document, disregarding
    grammar and word order. These word frequencies or occurrences are then used as
    features for training a classifier.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 是一种常用模型，允许你计算一段文本中的所有词汇。基本上，它为句子或文档创建一个发生矩阵，忽略语法和词序。这些词汇的频率或出现次数随后被用作训练分类器的特征。
- en: 'To bring a short example I took the first sentence of the song “Across the
    Universe” from The Beatles:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了举一个简单的例子，我取了披头士乐队歌曲《Across the Universe》的第一句：
- en: '*Words are flowing out like endless rain into a paper cup,*'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*词汇像无尽的雨水流入纸杯，*'
- en: ''
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*They slither while they pass, they slip away across the universe*'
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*他们滑过时，他们滑离宇宙*'
- en: 'Now let’s count the words:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算一下词汇数量：
- en: '![](../Images/cda94d22ba9f20d78ed2e98df51abcaa.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cda94d22ba9f20d78ed2e98df51abcaa.png)'
- en: This approach may reflect several downsides like the absence of semantic meaning
    and context, and the facts that stop words (like “the” or “a”) add noise to the
    analysis and some words are not weighted accordingly (“universe” weights less
    than the word “they”).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可能会反映出一些缺点，比如缺乏语义意义和上下文，停用词（如“the”或“a”）会给分析带来噪声，有些词的权重也不相应（“universe” 的权重低于“they”）。
- en: To solve this problem, one approach is to rescale the frequency of words by
    how often they appear in all texts (not just the one we are analyzing) so that
    the scores for frequent words like “the”, that are also frequent across other
    texts, get penalized. This approach to scoring is called **“Term Frequency ****—****Inverse
    Document Frequency****”** **(TFIDF)**, and improves the bag of words by weights.
    Through TFIDF frequent terms in the text are “rewarded” (like the word “they”
    in our example), but they also get “punished” if those terms are frequent in other
    texts we include in the algorithm too. On the contrary, this method highlights
    and “rewards” unique or rare terms considering all texts. Nevertheless, this approach
    still has no context nor semantics.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，一种方法是通过词汇在所有文本中出现的频率（不仅仅是我们正在分析的文本）来重新调整词频，以使像“the”这样的频繁词汇在其他文本中也很常见，得到惩罚。这种评分方法称为**“词频—逆文档频率”**（**TFIDF**），它通过权重改进了词袋模型。通过TFIDF，文本中的频繁词汇（如我们的例子中的“they”）会得到“奖励”，但如果这些词汇在其他文本中也很频繁，它们也会被“惩罚”。相反，这种方法突出了并“奖励”独特或稀有的术语，考虑了所有文本。然而，这种方法仍然没有上下文或语义。
- en: '**Tokenization**'
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**分词**'
- en: 'Is the process of segmenting running text into sentences and words. In essence,
    it’s the task of cutting a text into pieces called *tokens*, and at the same time
    throwing away certain characters, such as punctuation. Following our example,
    the result of tokenization would be:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 是将连续文本划分为句子和单词的过程。实质上，这就是将文本切割成称为*tokens*的片段，同时丢弃某些字符，如标点符号。以我们的例子为例，分词的结果是：
- en: '![](../Images/c229250b850f1aecf4712dcebddb9435.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c229250b850f1aecf4712dcebddb9435.png)'
- en: Pretty simple, right? Well, although it may seem quite basic in this case and
    also in languages like English that separate words by a blank space (called segmented
    languages) not all languages behave the same, and if you think about it, blank
    spaces alone are not sufficient enough even for English to perform proper tokenizations.
    Splitting on blank spaces may break up what should be considered as one token,
    as in the case of certain names (e.g. San Francisco or New York) or borrowed foreign
    phrases (e.g. laissez faire).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 很简单，对吧？虽然在这种情况下以及像英语这样通过空格分隔单词的语言（称为分段语言）中，这可能看起来相当基础，但并非所有语言都是如此。如果你仔细考虑一下，仅靠空格在英语中也不足以进行准确的分词。基于空格的分割可能会将本应视为一个标记的内容拆开，例如某些名字（如旧金山或纽约）或借用的外语短语（如
    laissez faire）。
- en: '**Tokenization can remove punctuation too**, easing the path to a proper word
    segmentation but also triggering possible complications. In the case of periods
    that follow abbreviation (e.g. dr.), the period following that abbreviation should
    be considered as part of the same token and not be removed.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**分词也可以去除标点符号**，这有助于进行准确的单词分割，但也可能引发一些问题。例如，在缩写（如 dr.）后的句点，句点应视为同一标记的一部分，而不是被去除。'
- en: The tokenization process can be particularly problematic when dealing with biomedical
    text domains which contain lots of hyphens, parentheses, and other punctuation
    marks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理包含大量连字符、括号和其他标点符号的生物医学文本领域时，分词过程可能特别棘手。
- en: For deeper details on tokenization, you can find a great explanation in [this
    article](https://www.ibm.com/developerworks/community/blogs/nlp/entry/tokenization?lang=en).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有关分词的更详细信息，你可以在 [这篇文章](https://www.ibm.com/developerworks/community/blogs/nlp/entry/tokenization?lang=en)中找到很好的解释。
- en: '**Stop Words Removal**'
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**停用词删除**'
- en: Includes getting rid of common language articles, pronouns and prepositions
    such as “and”, “the” or “to” in English. In this process some very common words
    that appear to provide little or no value to the NLP objective are filtered and
    excluded from the text to be processed, hence removing widespread and frequent
    terms that are not informative about the corresponding text.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 包括去除常见的语言文章、代词和介词，如英语中的“and”、“the”或“to”。在此过程中，一些非常常见的单词，因为对自然语言处理目标几乎没有价值，被过滤并排除在处理文本之外，从而去除那些广泛且频繁但没有信息量的词。
- en: Stop words can be safely ignored by carrying out a lookup in a pre-defined list
    of keywords, freeing up database space and improving processing time.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查阅预定义的关键词列表，可以安全地忽略停用词，从而节省数据库空间并提高处理速度。
- en: '**There is no universal list of stop words**. These can be pre-selected or
    built from scratch. A potential approach is to begin by adopting pre-defined stop
    words and add words to the list later on. Nevertheless it seems that the general
    trend over the past time has been to go from the use of large standard stop word
    lists to the use of no lists at all.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**没有通用的停用词列表**。这些可以是预先选择的，也可以从头开始建立。一种潜在的方法是从采用预定义的停用词开始，然后逐步添加新的词。然而，似乎最近的总体趋势是从使用大型标准停用词列表转向不使用任何列表。'
- en: The thing is stop words removal can wipe out relevant information and modify
    the context in a given sentence. For example, if we are performing a sentiment
    analysis we might throw our algorithm off track if we remove a stop word like
    “not”. Under these conditions, you might select a minimal stop word list and add
    additional terms depending on your specific objective.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，停用词的删除可能会抹去相关信息并改变句子的上下文。例如，如果我们在进行情感分析时删除像“not”这样的停用词，可能会让我们的算法偏离方向。在这种情况下，你可能需要选择一个最小的停用词列表，并根据具体目标添加额外的术语。
- en: '**Stemming**'
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**词干提取**'
- en: Refers to the process of slicing the end or the beginning of words with the
    intention of removing affixes (lexical additions to the root of the word).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 指的是通过切割单词的前部或后部来去除词缀（词根的附加部分）。
- en: '*Affixes that are attached at the beginning of the word are called prefixes (e.g.
    “astro” in the word “astrobiology”) and the ones attached at the end of the word
    are called suffixes (e.g. “ful” in the word “helpful”).*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*附加在单词开头的词缀称为前缀（例如单词“astrobiology”中的“astro”），而附加在单词结尾的词缀称为后缀（例如单词“helpful”中的“ful”）。*'
- en: The problem is that affixes can create or expand new forms of the same word
    (called *inflectional* affixes), or even create new words themselves (called *derivational* affixes).
    In English, prefixes are always derivational (the affix creates a new word as
    in the example of the prefix “eco” in the word “ecosystem”), but suffixes can
    be derivational (the affix creates a new word as in the example of the suffix
    “ist” in the word “guitarist”) or inflectional (the affix creates a new form of
    word as in the example of the suffix “er” in the word “faster”).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是词缀可以创造或扩展相同词的新形式（称为*屈折*词缀），甚至创造新的词（称为*派生*词缀）。在英语中，前缀总是派生性的（词缀创造了一个新词，例如“ecosystem”中的前缀“eco”），但后缀可以是派生性的（词缀创造了一个新词，例如“guitarist”中的后缀“ist”）或屈折性的（词缀创造了一个新形式的词，例如“faster”中的后缀“er”）。
- en: Ok, so how can we tell the difference and chop the right bit?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何区分并剪切正确的部分呢？
- en: '![](../Images/1608f7938632b5f6fc5c9937e9d17b4e.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1608f7938632b5f6fc5c9937e9d17b4e.png)'
- en: A possible approach is to consider a list of common affixes and rules (Python
    and R languages have different libraries containing affixes and methods) and perform
    stemming based on them, but of course this approach presents limitations. Since
    stemmers use algorithmics approaches, the result of the stemming process may not
    be an actual word or even change the word (and sentence) meaning. To offset this
    effect you can edit those predefined methods by adding or removing affixes and
    rules, but you must consider that you might be improving the performance in one
    area while producing a degradation in another one. Always look at the whole picture
    and test your model’s performance.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能的方法是考虑一个常见词缀和规则的列表（Python 和 R 语言有不同的库包含词缀和方法），并根据这些进行词干提取，但这种方法显然也存在局限性。由于词干提取器使用算法方法，词干提取过程的结果可能不是一个实际的词，甚至可能改变词（和句子）的含义。为了抵消这种效果，你可以通过添加或删除词缀和规则来编辑这些预定义的方法，但你必须考虑你可能会在一个领域提高性能，同时在另一个领域产生退化。始终考虑整体情况并测试模型的性能。
- en: So if stemming has serious limitations, why do we use it? First of all, it can
    be used to correct spelling errors from the tokens. **Stemmers are simple to use
    and run very fast** (they perform simple operations on a string), and if speed
    and performance are important in the NLP model, then stemming is certainly the
    way to go. Remember, we use it with the objective of improving our performance,
    not as a grammar exercise.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果词干提取有严重的局限性，那我们为什么还要使用它呢？首先，它可以用来纠正词汇中的拼写错误。**词干提取器使用简单且运行非常快速**（它们对字符串进行简单操作），如果在
    NLP 模型中速度和性能很重要，那么词干提取无疑是合适的选择。记住，我们使用它的目的是提高性能，而不是作为语法练习。
- en: '**Lemmatization**'
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**词形还原**'
- en: Has the objective of reducing a word to its base form and grouping together
    different forms of the same word. For example, verbs in past tense are changed
    into present (e.g. “went” is changed to “go”) and synonyms are unified (e.g. “best”
    is changed to “good”), hence standardizing words with similar meaning to their
    root. Although it seems closely related to the stemming process, lemmatization
    uses a different approach to reach the root forms of words.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其目的是将一个词简化为其基本形式，并将同一词的不同形式归为一类。例如，过去时动词被转化为现在时（例如，“went”被转化为“go”），同义词被统一（例如，“best”被转化为“good”），从而将具有相似意义的词标准化为其词根。尽管它似乎与词干提取过程密切相关，但词形还原使用不同的方法来达到词根形式。
- en: '*Lemmatization resolves words to their dictionary form (known as **lemma**)
    for which it requires detailed dictionaries in which the algorithm can look into
    and link words to their corresponding lemmas.*'
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*词形还原将单词解析为其词典形式（称为**词元**），这需要详细的词典，算法可以查阅并将单词链接到其对应的词元。*'
- en: For example, the words “*running”, “runs”* and *“ran”* are all forms of the
    word “*run”*, so “*run”* is the lemma of all the previous words.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，“*running*”，“*runs*” 和 “*ran*” 都是“*run*”的不同形式，因此“*run*”是所有这些词的词元。
- en: '![](../Images/db52d4f3acf412f2fc2968068d3a82b8.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db52d4f3acf412f2fc2968068d3a82b8.png)'
- en: Lemmatization also takes into consideration the context of the word in order
    to **solve other problems like disambiguation**, which means it can discriminate
    between identical words that have different meanings depending on the specific
    context. Think about words like “bat” (which can correspond to the animal or to
    the metal/wooden club used in baseball) or “bank” (corresponding to the financial
    institution or to the land alongside a body of water). By providing a part-of-speech
    parameter to a word ( whether it is a noun, a verb, and so on) it’s possible to
    define a role for that word in the sentence and remove disambiguation.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 词形还原也考虑了词的上下文，以**解决其他问题，如歧义消解**，这意味着它可以区分在特定上下文中具有不同含义的相同词汇。想想“bat”一词（可以指动物或用于棒球的金属/木制球棒）或“bank”一词（可以指金融机构或水体旁的土地）。通过为词提供词性参数（例如名词、动词等），可以定义该词在句子中的角色，从而消除歧义。
- en: As you might already pictured, lemmatization is a much more resource-intensive
    task than performing a stemming process. At the same time, since it requires more
    knowledge about the language structure than a stemming approach, it **demands
    more computational power **than setting up or adapting a stemming algorithm.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经想象的那样，词形还原是一个比词干提取过程更为资源密集的任务。同时，由于它比词干提取方法需要更多的语言结构知识，它**需要更多的计算能力**，而不是设置或调整词干提取算法。
- en: '**Topic Modeling**'
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**话题建模**'
- en: Is as a method for uncovering hidden structures in sets of texts or documents.
    In essence it clusters texts to discover latent topics based on their contents,
    processing individual words and assigning them values based on their distribution.
    This technique is based on the assumptions that each document consists of a mixture
    of topics and that each topic consists of a set of words, which means that if
    we can spot these hidden topics we can unlock the meaning of our texts.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种揭示文本或文档中隐藏结构的方法，本质上，它将文本聚类以发现基于其内容的潜在话题，处理单个词并根据其分布分配值。这项技术基于每个文档由一组话题混合组成，每个话题由一组词组成的假设，这意味着如果我们能够识别这些隐藏的话题，就能解锁文本的意义。
- en: 'From the universe of topic modelling techniques, **Latent Dirichlet Allocation
    (LDA)** is probably the most commonly used. This relatively new algorithm (invented
    less than 20 years ago) works as an unsupervised learning method that discovers
    different topics underlying a collection of documents. In **unsupervised learning **methods
    like this one, there is no output variable to guide the learning process and data
    is explored by algorithms to find patterns. To be more specific, LDA finds groups
    of related words by:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在话题建模技术的宇宙中，**潜在狄利克雷分配（LDA）**可能是最常用的。这种相对较新的算法（发明不到20年）作为一种无监督学习方法，发现文档集合中的不同话题。在**无监督学习**方法中，没有输出变量来引导学习过程，数据由算法探索以发现模式。更具体地说，LDA通过以下方式找到相关词的组：
- en: Assigning each word to a random topic, where the user defines the number of
    topics it wishes to uncover. You don’t define the topics themselves (you define
    just the number of topics) and the algorithm will map all documents to the topics
    in a way that words in each document are mostly captured by those imaginary topics.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个词分配给一个随机话题，用户定义希望揭示的话题数量。你不需要定义具体的话题（只定义话题数量），算法会将所有文档映射到这些虚拟话题上，以便每个文档中的词大多数都被这些虚拟话题所捕捉。
- en: The algorithm goes through each word iteratively and reassigns the word to a
    topic taking into considerations the probability that the word belongs to a topic,
    and the probability that the document will be generated by a topic. These probabilities
    are calculated multiple times, until the convergence of the algorithm.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法逐词迭代，重新分配词到某个话题，考虑到该词属于某话题的概率，以及文档由某话题生成的概率。这些概率会被多次计算，直到算法收敛。
- en: Unlike other clustering algorithms like [*K-means*](https://towardsdatascience.com/the-anatomy-of-k-means-c22340543397) that
    perform hard clustering (where topics are disjointed), LDA assigns each document
    to a mixture of topics, which means that each document can be described by one
    or more topics (e.g. Document 1 is described by 70% of topic A, 20% of topic B
    and 10% of topic C) and reflect more realistic results.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他执行硬聚类（主题之间互不重叠）的聚类算法如[*K-means*](https://towardsdatascience.com/the-anatomy-of-k-means-c22340543397)不同，LDA将每个文档分配给一个或多个主题的混合，这意味着每个文档可以由一个或多个主题来描述（例如，文档1由70%的主题A、20%的主题B和10%的主题C来描述），从而反映出更现实的结果。
- en: '![](../Images/99eacccff44debcd7badd73879846b96.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99eacccff44debcd7badd73879846b96.png)'
- en: Topic modeling is extremely useful for classifying texts, building recommender
    systems (e.g. to recommend you books based on your past readings) or even detecting
    trends in online publications.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模对于分类文本、构建推荐系统（例如，根据过去的阅读推荐书籍）或甚至检测在线出版物中的趋势非常有用。
- en: '**How does the future look like?**'
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**未来看起来怎么样？**'
- en: At the moment NLP is battling to detect nuances in language meaning, whether
    due to lack of context, spelling errors or dialectal differences.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，NLP正在努力检测语言意义的细微差别，无论是由于缺乏上下文、拼写错误还是方言差异。
- en: 'On March 2016 Microsoft launched *Tay*, an Artificial Intelligence (AI) chatbot
    released on Twitter as a NLP experiment. The idea was that as more users conversed
    with Tay, the smarter it would get. Well, the result was that after 16 hours Tay
    had to be removed due to its racist and abusive comments:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年3月，微软推出了*Tay*，这是一个作为NLP实验在Twitter上发布的人工智能（AI）聊天机器人。其想法是，随着更多用户与Tay对话，它会变得更聪明。结果是，经过16小时后，Tay因其种族主义和辱骂性评论而被移除：
- en: '![](../Images/47796f4589c4b9f14b3dffde8ac600fd.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47796f4589c4b9f14b3dffde8ac600fd.png)'
- en: '![](../Images/bdf3e47587ee620ca505d589bd9e3a5c.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdf3e47587ee620ca505d589bd9e3a5c.png)'
- en: Microsoft learnt from its own experience and some months later released [*Zo*](https://www.zo.ai/),
    its second generation English-language chatbot that won’t be caught making the
    same mistakes as its predecessor. Zo uses a combination of innovative approaches
    to recognize and generate conversation, and other companies are exploring with
    bots that can remember details specific to an individual conversation.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 微软从自身经验中吸取了教训，并在几个月后发布了[*Zo*](https://www.zo.ai/)，这是其第二代英语语言聊天机器人，不会再犯与其前身相同的错误。Zo使用了一系列创新的方法来识别和生成对话，其他公司也在探索能够记住特定个人对话细节的机器人。
- en: Although the future looks extremely challenging and full of threats for NLP,
    the discipline is developing at a very fast pace (probably like never before)
    and we are likely to reach a level of advancement in the coming years that will
    make complex applications look possible.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管未来对NLP看起来极具挑战性且充满威胁，但这一学科正在以前所未有的速度发展，我们很可能在未来几年达到一种使复杂应用看起来可行的进步水平。
- en: '[Original](https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1).
    Reposted with permission.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1)。经授权转载。'
- en: '**Bio**: [Diego Lopez Yse](https://twitter.com/LopezYse) is an experienced
    professional with a solid international background acquired in different industries
    (biotechnology, software, consultancy, government, agriculture).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介**：[Diego Lopez Yse](https://twitter.com/LopezYse)是一位经验丰富的专业人士，拥有在不同领域（生物技术、软件、咨询、政府、农业）获得的扎实国际背景。'
- en: '**Resources:**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：**'
- en: '[On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning
    education](https://www.kdnuggets.com/education/online.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在线和基于网络：分析、数据挖掘、数据科学、机器学习教育](https://www.kdnuggets.com/education/online.html)'
- en: '[Software for Analytics, Data Science, Data Mining, and Machine Learning](https://www.kdnuggets.com/software/index.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分析、数据科学、数据挖掘和机器学习软件](https://www.kdnuggets.com/software/index.html)'
- en: '**Related:**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Extracting Knowledge from Knowledge Graphs Using Facebook’s Pytorch-BigGraph](https://www.kdnuggets.com/2019/05/extracting-knowledge-graphs-facebook-pytorch-biggraph.html)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Facebook的Pytorch-BigGraph从知识图谱中提取知识](https://www.kdnuggets.com/2019/05/extracting-knowledge-graphs-facebook-pytorch-biggraph.html)'
- en: '[A Complete Exploratory Data Analysis and Visualization for Text Data: Combine
    Visualization and NLP to Generate Insights](https://www.kdnuggets.com/2019/05/complete-exploratory-data-analysis-visualization-text-data.html)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[完整的文本数据探索性分析和可视化：结合可视化和NLP以生成见解](https://www.kdnuggets.com/2019/05/complete-exploratory-data-analysis-visualization-text-data.html)'
- en: '[Build Your First Chatbot Using Python & NLTK](https://www.kdnuggets.com/2019/05/build-chatbot-python-nltk.html)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Python和NLTK构建你的第一个聊天机器人](https://www.kdnuggets.com/2019/05/build-chatbot-python-nltk.html)'
- en: More On This Topic
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解更多相关话题
- en: '[N-gram Language Modeling in Natural Language Processing](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理中的N-gram语言建模](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
- en: '[A Guide to Top Natural Language Processing Libraries](https://www.kdnuggets.com/2023/04/guide-top-natural-language-processing-libraries.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[顶级自然语言处理库指南](https://www.kdnuggets.com/2023/04/guide-top-natural-language-processing-libraries.html)'
- en: '[Natural Language Processing Key Terms, Explained](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理关键术语解释](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
- en: '[Data Representation for Natural Language Processing Tasks](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理任务的数据表示](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
- en: '[Transfer Learning for Image Recognition and Natural Language Processing](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[图像识别和自然语言处理中的迁移学习](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
- en: '[How to Start Using Natural Language Processing With PyTorch](https://www.kdnuggets.com/2022/04/start-natural-language-processing-pytorch.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用PyTorch开始自然语言处理](https://www.kdnuggets.com/2022/04/start-natural-language-processing-pytorch.html)'
