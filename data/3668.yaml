- en: 'Introducing MPT-7B: A New Open-Source LLM'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 MPT-7B：一款全新的开源 LLM
- en: 原文：[https://www.kdnuggets.com/2023/05/introducing-mpt7b-new-opensource-llm.html](https://www.kdnuggets.com/2023/05/introducing-mpt7b-new-opensource-llm.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/05/introducing-mpt7b-new-opensource-llm.html](https://www.kdnuggets.com/2023/05/introducing-mpt7b-new-opensource-llm.html)
- en: '![Introducing MPT-7B: A New Open-Source LLM](../Images/e8dc98d8b617d12aa091933c049049a7.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![介绍 MPT-7B：一款全新的开源 LLM](../Images/e8dc98d8b617d12aa091933c049049a7.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: The Large language models (LLM) are going crazy at the moment. However, as an
    organization, if you do not have the right resources, it can be challenging to
    jump on the large language model wave. Training and deploying large language models
    can be difficult, and you suddenly feel left out. Open-source LLMs, such as the
    LLaMA series from Meta have allowed for LLM resources to be available.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）目前正变得疯狂。然而，作为一个组织，如果没有合适的资源，可能很难跟上大型语言模型的潮流。训练和部署大型语言模型可能会很困难，你会突然感到被甩在了后面。开源
    LLM，如 Meta 的 LLaMA 系列，使 LLM 资源变得可用。
- en: And to add to the open-source collection is [MosaicML Foundations](https://www.mosaicml.com/)'
    latest addition to their series - [MPT-7B](https://huggingface.co/mosaicml/mpt-7b).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，作为开源系列的最新补充是 [MosaicML Foundations](https://www.mosaicml.com/)' 最新款 [MPT-7B](https://huggingface.co/mosaicml/mpt-7b)。
- en: What is MPT-7B?
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 MPT-7B？
- en: 'MPT stands for MosaicML Pretrained Transformer. MPT models are GPT-style decoder-only
    transformers that come with many improvements:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: MPT 代表 MosaicML 预训练变换器。MPT 模型是仅解码的 GPT 风格变换器，具有许多改进：
- en: Performance-optimized layer implementations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能优化层实现
- en: Greater training stability due to architecture changes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于架构更改，训练稳定性更高
- en: No context length limitations
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无上下文长度限制
- en: MPT-7B is a transformer model that has been trained from scratch using 1T tokens
    of text and code. Yes, 1 TRILLION! It was trained on the MosaicML platform, with
    a time frame of 9.5 days with zero human intervention. Costing MosaicML ~$200k.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: MPT-7B 是一种转换器模型，使用 1T tokens 的文本和代码从头开始训练。是的，1 万亿！它在 MosaicML 平台上训练，耗时 9.5 天，完全没有人工干预。费用大约为
    $200k。
- en: It is open-source, making it available for commercial use and the tool will
    be a game changer on how businesses and organizations work with their predictive
    analytics and decision-making process.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 它是开源的，可用于商业用途，这个工具将彻底改变企业和组织在预测分析和决策过程中的工作方式。
- en: 'The main features of MPT-7B are:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: MPT-7B 的主要特点包括：
- en: Licensed for commercial use
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许可用于商业用途
- en: Trained on a large amount of data (1T tokens)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据量巨大（1T tokens）
- en: Can handle extremely long inputs
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以处理极长的输入
- en: Optimized for fast training and inference
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对快速训练和推理进行优化
- en: Highly efficient open-source training code.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效的开源训练代码。
- en: MPT-7B is the base model and has been shown to outperform other open-source
    7B - 20B models. The quality of MPT-7B matches LLaMA-7B. To evaluate the quality
    of MPT-7B, MosaicML Foundation put together 11 open-source benchmarks and evaluated
    them using the industry-standard manner.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: MPT-7B 是基础模型，已被证明优于其他开源 7B - 20B 模型。MPT-7B 的质量与 LLaMA-7B 相匹配。为了评估 MPT-7B 的质量，MosaicML
    Foundation 组织了 11 个开源基准，并以行业标准方式进行评估。
- en: '![Introducing MPT-7B: A New Open-Source LLM](../Images/05981dd7e854b430900b0d303b0aa9b6.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![介绍 MPT-7B：一款全新的开源 LLM](../Images/05981dd7e854b430900b0d303b0aa9b6.png)'
- en: Image by [MosaicML Foundation](https://www.mosaicml.com/blog/mpt-7b)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [MosaicML Foundation](https://www.mosaicml.com/blog/mpt-7b) 提供
- en: 'MosaicML foundations are also releasing three additional fine-tuned models:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: MosaicML 基础模型也发布了三个额外微调的模型：
- en: MPT-7B-Instruct
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MPT-7B-Instruct
- en: MPT-7B-Chat
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MPT-7B-Chat
- en: MPT-7B-StoryWriter-65k+
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MPT-7B-StoryWriter-65k+
- en: MPT-7B-Instruct
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MPT-7B-Instruct
- en: The [MPT-7B-Instruct](https://huggingface.co/mosaicml/mpt-7b-instruct) model
    is for short-form instruction following. With 26,834 dated the 14th of May, MPT-7B-Instruct
    allows you to ask quick and short questions and provides you with an instant response.
    Have a question, and you just want a simple answer - use MPT-7B-Instruct.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[MPT-7B-Instruct](https://huggingface.co/mosaicml/mpt-7b-instruct) 模型用于短形式指令跟随。自5月14日以来，MPT-7B-Instruct
    允许你提出快速简短的问题并立即获得回应。如果你有问题，想要一个简单的答案 - 使用 MPT-7B-Instruct。'
- en: Why is this so great? Typically LLMs are taught to continue generating text
    based on the input that was provided. However, some are looking for LLMs that
    treat their input as an instruction. Instruction finetuning allows LLMs to perform
    instruction-following outputs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这如此出色？通常LLMs会根据提供的输入继续生成文本。然而，有些人希望LLMs将输入视为指令。指令微调使LLMs能够执行指令跟随输出。
- en: MPT-7B-Chat
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MPT-7B-Chat
- en: Yes, we have another chatbot. [MPT-7B-Chat](https://huggingface.co/mosaicml/mpt-7b-chat)
    generates dialogue. For example, if you want the chatbot to generate a speech,
    giving it context it will generate a text in a conversational manner. Or maybe
    you want to write a tweet which paraphrases a paragraph from an article, it can
    generate the dialogue for you!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我们还有另一个聊天机器人。[MPT-7B-Chat](https://huggingface.co/mosaicml/mpt-7b-chat)生成对话。例如，如果您希望聊天机器人生成演讲稿，提供上下文，它将以对话的方式生成文本。或者，您可能想写一条推文，改写文章中的一段内容，它也可以为您生成对话！
- en: Why is this so great? MPT-7B Chat is ready and well-equipped for a variety of
    conversational tasks, delivering more seamless, engaging multi-turn interactions
    for users.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这如此出色？MPT-7B Chat已经准备好并具备了进行各种对话任务的能力，能够为用户提供更无缝、更吸引人的多轮互动。
- en: MPT-7B-StoryWriter-65k+
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MPT-7B-StoryWriter-65k+
- en: This is for the story writers! For those who want to write stories that have
    a long context, [MPT-7B-StoryWriter-65k+](https://huggingface.co/mosaicml/mpt-7b-storywriter)
    is a model designed for exactly that. The model was built by fine-tuning MPT-7B
    with a **context length of 65k tokens**, and it can extrapolate beyond 65k tokens.
    MosaicML Foundation has been able to generate 84k tokens on a single node of A100-80GB
    GPUs.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这是给故事创作者的！对于那些想要编写长篇背景故事的人，[MPT-7B-StoryWriter-65k+](https://huggingface.co/mosaicml/mpt-7b-storywriter)是专门为此设计的模型。该模型通过对MPT-7B进行微调，拥有**65k
    tokens的上下文长度**，并且可以超越65k tokens进行推断。MosaicML Foundation在单节点A100-80GB GPU上成功生成了84k
    tokens。
- en: Why is this so great? This is because most open-source LLMs can only handle
    sequences with up to a few thousand tokens. But just by using a single node of
    8xA100-80GB on the MosaicML platform, you can finetune MPT-7B to handle context
    lengths up to 65k!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这如此出色？因为大多数开源LLMs只能处理最多几千个tokens的序列。但只需在MosaicML平台上使用单节点8xA100-80GB，您就可以微调MPT-7B以处理上下文长度高达65k！
- en: More on How MPT-7B was Built
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于MPT-7B的更多构建细节
- en: The MosaicML team built these models in only a few weeks. In only a few weeks
    they dealt with the data preparation, training, finetuning, and deployment.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: MosaicML团队在仅仅几周内完成了这些模型的构建。他们在几周内处理了数据准备、训练、微调和部署。
- en: The data was sourced from a variety of sources, which all had a billion tokens
    available in each source. The number of effective tokens still got a billion in
    each source! The team used [EleutherAI’s](https://www.eleuther.ai/), [GPT-NeoX](https://aclanthology.org/2022.bigscience-1.9/),
    and [20B tokenizer](https://huggingface.co/docs/transformers/model_doc/gpt_neox),
    allowing them to train on a diverse mix of data, apply consistent space delimitation,
    and more.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 数据来自各种来源，每个来源都有十亿个tokens可用。有效的tokens数量在每个来源中仍然保持在十亿！团队使用了[EleutherAI](https://www.eleuther.ai/)、[GPT-NeoX](https://aclanthology.org/2022.bigscience-1.9/)和[20B
    tokenizer](https://huggingface.co/docs/transformers/model_doc/gpt_neox)，使他们能够在多样化的数据上进行训练，应用一致的空格分隔符等。
- en: All the MPT-7B models were trained on the [MosaicML platform](https://www.mosaicml.com/training),
    using A100-40GB and A100-80GB GPUs from Oracle Cloud.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所有MPT-7B模型都在[MosaicML平台](https://www.mosaicml.com/training)上训练，使用了来自Oracle Cloud的A100-40GB和A100-80GB
    GPU。
- en: 'If you would like to know more about the tools and costs of MPT-7B, have a
    read of the: [MPT-7B Blog](https://www.mosaicml.com/blog/mpt-7b).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于MPT-7B的工具和成本，请阅读：[MPT-7B博客](https://www.mosaicml.com/blog/mpt-7b)。
- en: Wrapping it up
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: The MosaicML platform can be considered as the best starting point for organisations,
    if it be private, commercial or community related to build custom LLMs. Having
    this open-source resource available will allow organisations to feel freer about
    using these tools to improve the current organisational challenges.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: MosaicML平台可以被视为组织建立自定义LLMs的最佳起点，无论是私有的、商业的还是社区相关的。拥有这一开源资源将使组织在使用这些工具以改善当前组织挑战时感到更加自由。
- en: Customers are able to train LLMs on any computing provider, or data source,
    whilst being able to maintain efficiency, privacy and cost transparency.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 客户能够在任何计算提供商或数据源上训练LLMs，同时保持效率、隐私和成本透明度。
- en: What do you think you will be using MPT-7B for? Let us know in the comments
    below
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为你将如何使用MPT-7B？在下面的评论中告诉我们
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist,
    Freelance Technical Writer and Community Manager at KDnuggets. She is particularly
    interested in providing Data Science career advice or tutorials and theory based
    knowledge around Data Science. She also wishes to explore the different ways Artificial
    Intelligence is/can benefit the longevity of human life. A keen learner, seeking
    to broaden her tech knowledge and writing skills, whilst helping guide others.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** 是一名数据科学家、自由技术作家和KDnuggets的社区经理。她特别关注提供数据科学职业建议或教程，以及围绕数据科学的理论知识。她还希望探索人工智能如何有助于延长人类寿命。作为一名热衷于学习的人员，她寻求拓宽技术知识和写作技能，同时帮助指导他人。'
- en: '* * *'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT需求'
- en: '* * *'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Introducing MetaGPT''s Data Interpreter: SOTA Open Source LLM-based…](https://www.kdnuggets.com/metagpt-data-interpreter-open-source-llm-based-data-solutions)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍MetaGPT的数据解释器：SOTA开源LLM基础的…](https://www.kdnuggets.com/metagpt-data-interpreter-open-source-llm-based-data-solutions)'
- en: '[Web LLM: Bring LLM Chatbots to the Browser](https://www.kdnuggets.com/2023/05/webllm-bring-llm-chatbots-browser.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Web LLM：将LLM聊天机器人带到浏览器](https://www.kdnuggets.com/2023/05/webllm-bring-llm-chatbots-browser.html)'
- en: '[Introducing the Testing Library for Natural Language Processing](https://www.kdnuggets.com/2023/04/introducing-testing-library-natural-language-processing.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍自然语言处理的测试库](https://www.kdnuggets.com/2023/04/introducing-testing-library-natural-language-processing.html)'
- en: '[Introducing Healthcare-Specific Large Language Models from John Snow Labs](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[来自John Snow Labs的医疗专用大型语言模型介绍](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
- en: '[Introducing OpenChat: The Free & Simple Platform for Building…](https://www.kdnuggets.com/2023/06/introducing-openchat-free-simple-platform-building-custom-chatbots-minutes.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍OpenChat：一个免费的、简单的平台，用于构建…](https://www.kdnuggets.com/2023/06/introducing-openchat-free-simple-platform-building-custom-chatbots-minutes.html)'
- en: '[Introducing OpenLLM: Open Source Library for LLMs](https://www.kdnuggets.com/2023/07/introducing-openllm-open-source-library-llms.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍OpenLLM：LLMs的开源库](https://www.kdnuggets.com/2023/07/introducing-openllm-open-source-library-llms.html)'
