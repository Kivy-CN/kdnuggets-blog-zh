- en: 5 Apache Spark Best Practices For Data Science
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 个 Apache Spark 数据科学最佳实践
- en: 原文：[https://www.kdnuggets.com/2020/08/5-spark-best-practices-data-science.html](https://www.kdnuggets.com/2020/08/5-spark-best-practices-data-science.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/08/5-spark-best-practices-data-science.html](https://www.kdnuggets.com/2020/08/5-spark-best-practices-data-science.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Zion Badash](https://www.linkedin.com/in/zion-badash-a73826a1/?originalSubdomain=il),
    Data Scientist @ Wix.com**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由[Zion Badash](https://www.linkedin.com/in/zion-badash-a73826a1/?originalSubdomain=il)提供，Wix.com的数据科学家**'
- en: '![Figure](../Images/8ed183f322d32458e9f9effc15e95c27.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8ed183f322d32458e9f9effc15e95c27.png)'
- en: Photo by [chuttersnap](https://unsplash.com/@chuttersnap?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[chuttersnap](https://unsplash.com/@chuttersnap?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '* * *'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - 支持你的组织在
    IT 领域'
- en: '* * *'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Why move to Spark?
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么转向 Spark？
- en: Although we all [talk about Big Data](https://www.facebook.com/dan.ariely/posts/904383595868),
    it usually takes some time in your career until you encounter it. For me at [Wix.com](https://www.wix.com/) it
    came quicker than I thought, having well over 160M users generates a lot of data
    — and with that comes the need for scaling our data processes.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们都在[talk about Big Data](https://www.facebook.com/dan.ariely/posts/904383595868)，但通常在你的职业生涯中需要一些时间才能遇到它。对我来说，在[Wix.com](https://www.wix.com/)这个过程比我想象的要快，拥有超过
    1.6 亿用户会生成大量数据——这也带来了扩展我们数据处理的需求。
- en: While there are other options out there ([Dask](https://dask.org/) for example),
    we decided to go with Spark for 2 main reasons — (1) It’s the current state of
    the art and widely used for Big Data. (2) We had the infrastructure needed for
    Spark inplace.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然还有其他选项（例如，[Dask](https://dask.org/)），但我们决定选择 Spark，主要有两个原因——（1）它是目前的技术前沿，并广泛用于大数据。（2）我们已有了需要的
    Spark 基础设施。
- en: How to write in PySpark for pandas people
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何为熟悉 pandas 的人编写 PySpark
- en: Chances are you’re familiar with pandas, and when I say familiar I mean fluent,
    your mother's tongue :)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能对 pandas 很熟悉，当我说熟悉时，我的意思是非常流利，你的母语 :)
- en: The headline of the following talk says it all — [Data Wrangling with PySpark
    for Data Scientists Who Know Pandas](https://databricks.com/session/data-wrangling-with-pyspark-for-data-scientists-who-know-pandas) and
    it’s a great one.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来讲座的标题一语道破——[Data Wrangling with PySpark for Data Scientists Who Know Pandas](https://databricks.com/session/data-wrangling-with-pyspark-for-data-scientists-who-know-pandas)，这是一个很棒的讲座。
- en: This will be a very good time to note that simply getting the syntax right might
    be a good place to start but you need a lot more for a successful PySpark project,
    you need to understand how Spark works.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在正是指出，仅仅把语法弄对可能是个好的开始，但要成功完成 PySpark 项目，你需要更多的知识，你需要了解 Spark 是如何工作的。
- en: It’s hard to get Spark to work properly, but when it works — it works great!
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让 Spark 正常工作是很困难的，但当它工作时——它表现非常好！
- en: Spark in a nutshell
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark概述
- en: I would only go knee deep here but I recommend visiting the following article
    and reading the MapReduce explanation for a more extensive explanation — [The
    Hitchhikers guide to handle Big Data using Spark](https://towardsdatascience.com/the-hitchhikers-guide-to-handle-big-data-using-spark-90b9be0fe89a).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我这里只会讲到一部分，但我建议访问以下文章，阅读 MapReduce 解释，以获得更全面的解释——[The Hitchhikers guide to handle
    Big Data using Spark](https://towardsdatascience.com/the-hitchhikers-guide-to-handle-big-data-using-spark-90b9be0fe89a)。
- en: The concept we want to understand here is **Horizontal Scaling.**
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里要理解的概念是**水平扩展**。
- en: It’s easier to start with **Vertical Scaling. **If we have a pandas code that
    works great but then the data becomes too big for it, we can potentially move
    to a stronger machine with more memory and hope it manages. This means we still
    have one machine handling the entire data at the same time - we scaled **vertically.**
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**垂直扩展**更容易。如果我们有一个运行良好的 pandas 代码，但数据变得太大，我们可以考虑迁移到具有更多内存的更强大机器上，希望它能处理。这意味着我们仍然有一台机器处理所有数据——我们进行了**垂直扩展**。'
- en: If instead we decided to use MapReduce, and split the data to chunks and let
    different machines handle each chunk — we’re scaling **horizontally**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们决定使用 MapReduce，将数据拆分成块，并让不同的机器处理每个块——我们就是在进行**水平扩展**。
- en: 5 Spark Best Practices
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5 个 Spark 最佳实践
- en: These are the 5 Spark best practices that helped me reduce runtime by 10x and
    scale our project.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是帮助我将运行时间减少 10 倍并扩展项目的 5 个 Spark 最佳实践。
- en: 1 - Start small — Sample the data
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1 - 从小做起 —— 采样数据
- en: If we want to make big data work, we first want to see we’re in the right direction
    using a small chunk of data. In my project I sampled 10% of the data and made
    sure the pipelines work properly, this allowed me to use the SQL section in the
    Spark UI and see the numbers grow through the entire flow, while not waiting too
    long for the process to run.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想让大数据发挥作用，首先要使用小块数据来确认我们在正确的方向上。在我的项目中，我对 10% 的数据进行了采样，并确保管道正常工作，这使我能够在
    Spark UI 中使用 SQL 部分，并观察整个流程中的数字增长，同时不会等待太长时间。
- en: From my experience if you reach your desired runtime with the small sample,
    you can usually scale up rather easy.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我的经验，如果你在小样本上达到了所需的运行时间，通常可以比较容易地进行扩展。
- en: 2 - Understand the basics — Tasks, partitions, cores
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2 - 理解基础知识 —— 任务、分区、核心
- en: 'This is probably the single most important thing to understand when working
    with Spark:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是使用 Spark 时最重要的事情：
- en: '**1 Partition makes for 1 Task that runs on 1 Core**'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1 个分区对应 1 个任务，运行在 1 个核心上**'
- en: 'You have to always be aware of the number of partitions you have - follow the
    number of tasks in each stage and match them with the correct number of cores
    in your Spark connection. A few tips and rules of thumb to help you do this (all
    of them require testing with your case):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须始终注意分区的数量——跟踪每个阶段的任务数量，并将其与 Spark 连接中的核心数量匹配。以下是一些提示和经验法则来帮助你做到这一点（所有这些都需要用你的情况进行测试）：
- en: The ratio between tasks and cores should be around 2–4 tasks for each core.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务与核心的比例应为每个核心 2–4 个任务。
- en: The size of each partition should be about 200MB–400MB, this depends on the
    memory of each worker, tune it to your needs.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个分区的大小应为 200MB–400MB，这取决于每个工作节点的内存，按需进行调整。
- en: 3 - Debugging Spark
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3 - 调试 Spark
- en: Spark works with lazy evaluation, which means it waits until an action is called
    before executing the graph of computation instructions. Examples of actions are `show(),
    count(),...`
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 使用延迟计算，这意味着它在执行计算指令图之前会等待操作的调用。操作的例子有 `show(), count(),...`
- en: This makes it very hard to understand where are the bugs / places that need
    optimization in our code. One practice which I found helpful was splitting the
    code to sections by using `df.cache()` and then use `df.count()` to force Spark
    to compute the df at each section.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得很难理解代码中哪里有错误或需要优化的地方。我发现有用的一种实践是通过使用 `df.cache()` 将代码分成几个部分，然后使用 `df.count()`
    强制 Spark 计算每个部分的 df。
- en: Now, using the Spark UI you can look at the computation of each section and
    spot the problems. It’s important to note that using this practice without using
    the sampling we mentioned in (1) will probably create a very long runtime which
    will be hard to debug.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用 Spark UI 你可以查看每个部分的计算并发现问题。需要注意的是，如果没有使用我们在（1）中提到的采样，这种做法可能会导致非常长的运行时间，这将很难调试。
- en: 4 - Finding and solving skewness
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4 - 查找和解决倾斜
- en: Let’s start with defining skewness. As we mentioned our data is divided to partitions
    and along the transformations the size of each partition would likely change.
    This can create a wide variation in size between partitions which means we have
    a skewness in our data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从定义数据倾斜开始。正如我们提到的，我们的数据被分成多个分区，并且在转换过程中每个分区的大小可能会发生变化。这可能导致分区之间的大小差异很大，这意味着我们的数据存在倾斜。
- en: 'Finding the Skewness can be done by looking at the stage details in the Spark
    UI and looking for a significant difference between the max and median:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 查找倾斜可以通过查看 Spark UI 中的阶段详细信息，并寻找最大值和中位数之间的显著差异来完成：
- en: '![Figure](../Images/4733ff757dca1d055ee94dac6d0ec4d2.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/4733ff757dca1d055ee94dac6d0ec4d2.png)'
- en: The big variance (Median=3s, Max=7.5min) might suggest a skewness in data
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 大的方差（中位数=3秒，最大值=7.5分钟）可能暗示数据中的倾斜
- en: This means that we have a few tasks that were significantly slower than the
    others.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们有一些任务显著比其他任务慢。
- en: Why is this bad — this might cause other stages to wait for these few tasks
    and leave cores waiting while not doing anything.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这不好——这可能导致其他阶段等待这些少量任务，而让核心空闲等待而无所事事。
- en: 'Preferably if you know where the skewness is coming from you can address it
    directly and change the partitioning. If you have no idea / no option to solve
    it directly, try the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你知道倾斜的来源，最好直接解决并更改分区。如果不知道 / 没有直接解决的选项，可以尝试以下方法：
- en: '**Adjusting the ratio between the tasks and cores**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**调整任务和核心之间的比例**'
- en: As we mentioned, by having more tasks than cores we hope that while the longer
    task is running other cores will remain busy with the other tasks. Although this
    is true, the ratio mentioned earlier (2-4:1) can’t really address such a big variance
    between tasks duration. We can try to increase the ratio to 10:1 and see if it
    helps, but there could be other downsides to this approach.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所提到的，通过让任务数量多于核心数量，我们希望在较长任务运行时，其他核心能继续忙于其他任务。虽然这是真的，但前面提到的比例（2-4:1）无法真正解决任务持续时间之间的巨大差异。我们可以尝试将比例提高到
    10:1 看看是否有效，但这种方法可能还有其他缺点。
- en: '**Salting the data**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**对数据进行 Salting**'
- en: 'Salting is repartitioning the data with a random key so that the new partitions
    would be balanced. Here’s a code example for PySpark (using groupby which is the
    usual suspect for causing skewness):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Salting 是使用随机键重新分区数据，以使新的分区平衡。以下是 PySpark 的代码示例（使用 groupby，这通常是导致倾斜的罪魁祸首）：
- en: 5 - Problems with iterative code in Spark
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5 - Spark 中迭代代码的问题
- en: This one was a real tough one. As we mentioned Spark uses lazy evaluation, so
    when running the code — it only builds a computational graph, a DAG. But this
    method can be very problematic when you have an iterative process, because the
    DAG reopens the previous iteration and becomes very big, I mean very very big
    . This might be too big for the driver to keep in memory. This problem is hard
    to locate because the application is stuck, but it appears in the Spark UI as
    if no job is running (which is true) for a long time — until the driver eventually
    crashes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个真正棘手的问题。正如我们提到的，Spark 使用延迟计算，因此在运行代码时——它只是构建了一个计算图，一个 DAG。但当你有一个迭代过程时，这种方法可能非常有问题，因为
    DAG 重新打开先前的迭代并变得非常大，我是说非常非常大。这可能对驱动程序来说太大，无法保存在内存中。这个问题很难定位，因为应用程序被卡住了，但它在 Spark
    UI 中显示为没有作业运行（这是真的）很长时间——直到驱动程序最终崩溃。
- en: This is currently an inherent problem with Spark and the workaround which worked
    for me was using `df.checkpoint() / df.localCheckpoint()` every 5–6 iterations
    (find your number by experimenting a bit). The reason this works is that `checkpoint()` is
    breaking the lineage and the DAG (unlike `cache()`), saving the results and starting
    from the new checkpoint. The downside is that if something bad happened, you don’t
    have the entire DAG for recreating the df.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 目前这是 Spark 的一个固有问题，我使用的解决方法是每 5-6 次迭代使用 `df.checkpoint() / df.localCheckpoint()`（通过实验找到你的数字）。之所以有效，是因为
    `checkpoint()` 打破了血统和 DAG（不同于 `cache()`），保存结果并从新的检查点开始。缺点是，如果发生了不好的情况，你没有完整的 DAG
    来重新创建 df。
- en: Summary
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: As I said before, it takes time to learn how to make Spark do its magic but
    these 5 practices really pushed my project forward and sprinkled some Spark magic
    on my code.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前说的，学习如何让 Spark 发挥魔力需要时间，但这 5 个实践确实推动了我的项目，并在我的代码中洒上了一些 Spark 魔力。
- en: To conclude, this is the post I was looking for (and didn’t find) when I started
    my project — I hope you found it just in time.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，这是我在开始项目时寻找的（但未找到）的文章——希望你正好找到了。
- en: References
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Data Wrangling with PySpark for Data Scientists Who Know Pandas](https://databricks.com/session/data-wrangling-with-pyspark-for-data-scientists-who-know-pandas)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[针对掌握 Pandas 的数据科学家的 PySpark 数据清理](https://databricks.com/session/data-wrangling-with-pyspark-for-data-scientists-who-know-pandas)'
- en: '[The Hitchhikers guide to handle Big Data using Spark](https://towardsdatascience.com/the-hitchhikers-guide-to-handle-big-data-using-spark-90b9be0fe89a)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Spark 处理大数据的指南](https://towardsdatascience.com/the-hitchhikers-guide-to-handle-big-data-using-spark-90b9be0fe89a)'
- en: '[Spark: The Definitive Guide](https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/) —
    chapter 18 about monitoring and debugging is amazing.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Spark: 权威指南](https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/)
    — 第18章关于监控和调试的内容非常精彩。'
- en: '**Bio: [Zion Badash](https://www.linkedin.com/in/zion-badash-a73826a1/?originalSubdomain=il)**
    is a Data Scientist @ Wix.com on the Forecasting Team. His interests include ML,
    Time Series, Spark and everything in between.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介: [Zion Badash](https://www.linkedin.com/in/zion-badash-a73826a1/?originalSubdomain=il)**
    是 Wix.com 的预测团队数据科学家。他的兴趣包括机器学习、时间序列、Spark 以及相关领域的一切。'
- en: '[Original](https://towardsdatascience.com/5-spark-best-practices-61587a35ac15).
    Reposted with permission.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/5-spark-best-practices-61587a35ac15)。经授权转载。'
- en: '**Related:**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关:**'
- en: '[The Benefits & Examples of Using Apache Spark with PySpark](/2020/04/benefits-apache-spark-pyspark.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Apache Spark 和 PySpark 的好处及示例](/2020/04/benefits-apache-spark-pyspark.html)'
- en: '[Apache Spark Cluster on Docker](/2020/07/apache-spark-cluster-docker.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Apache Spark 集群在 Docker 上](/2020/07/apache-spark-cluster-docker.html)'
- en: '[Apache Spark on Dataproc vs. Google BigQuery](/2020/07/apache-spark-dataproc-vs-google-bigquery.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Dataproc 上的 Apache Spark 与 Google BigQuery](/2020/07/apache-spark-dataproc-vs-google-bigquery.html)'
- en: More On This Topic
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解更多相关话题
- en: '[Integrating ChatGPT Into Data Science Workflows: Tips and Best Practices](https://www.kdnuggets.com/2023/05/integrating-chatgpt-data-science-workflows-tips-best-practices.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[将 ChatGPT 融入数据科学工作流程: 提示和最佳实践](https://www.kdnuggets.com/2023/05/integrating-chatgpt-data-science-workflows-tips-best-practices.html)'
- en: '[5 Best Practices for Data Science Team Collaboration](https://www.kdnuggets.com/2023/06/5-best-practices-data-science-team-collaboration.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学团队协作的 5 个最佳实践](https://www.kdnuggets.com/2023/06/5-best-practices-data-science-team-collaboration.html)'
- en: '[5 Python Best Practices for Data Science](https://www.kdnuggets.com/5-python-best-practices-for-data-science)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中的 5 个 Python 最佳实践](https://www.kdnuggets.com/5-python-best-practices-for-data-science)'
- en: '[11 Best Practices of Cloud and Data Migration to AWS Cloud](https://www.kdnuggets.com/2023/04/11-best-practices-cloud-data-migration-aws-cloud.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[迁移到 AWS 云的 11 个最佳实践](https://www.kdnuggets.com/2023/04/11-best-practices-cloud-data-migration-aws-cloud.html)'
- en: '[Data Warehousing and ETL Best Practices](https://www.kdnuggets.com/2023/02/data-warehousing-etl-best-practices.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据仓储和 ETL 最佳实践](https://www.kdnuggets.com/2023/02/data-warehousing-etl-best-practices.html)'
- en: '[Data Visualization Best Practices & Resources for Effective Communication](https://www.kdnuggets.com/2023/04/data-visualization-best-practices-resources-effective-communication.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据可视化最佳实践与有效沟通资源](https://www.kdnuggets.com/2023/04/data-visualization-best-practices-resources-effective-communication.html)'
