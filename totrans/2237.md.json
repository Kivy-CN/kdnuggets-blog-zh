["```py\nbatch = next(iter(train_dataloader)) # Get a single batch\n\n# For all epochs, keep training on the single batch.\nfor epoch in range(num_epochs):\n    inputs, targets = batch    \n    predictions = model.train(inputs)\n```", "```py\n# For all epochs, iterate over all batches of data.\nfor epoch in range(num_epochs):\n    for batch in iter(dataloader):\n        inputs, targets = batch    \n        predictions = model.train(inputs)\n```", "```py\nfrom torch.utils.data import DataLoader\n\ndataset = # Loading Data\ndataloder = DataLoader(dataset, shuffle=True)\n```", "```py\nimport torchvision.transforms as transforms\n\nimage_transforms = transforms.Compose([\n\ttransforms.ToTensor(),\n\t# Normalize the values in our data\n\ttransforms.Normalize(mean=(0.5,), std=(0.5))\n])\n```", "```py\nfor epoch in range(num_epochs):\n\tfor batch in iter(train_dataloader):\n    \tinputs, targets = batch\n    \tpredictions = model(inputs)\n\n    \toptimizer.zero_grad() # Remove all previous gradients\n    \tloss = criterion(targets, predictions)\n    \tloss.backward() # Computes Gradients for model weights\n\n    \t# Clip the gradients of model weights to a specified max_norm value.\n    \ttorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n    \t# Optimize the model weights AFTER CLIPPING\n    \toptimizer.step()\n```", "```py\nfor epoch in range(num_epochs):\n\n\t# Using training Mode when iterating over training dataset\n\tmodel.train()\n\tfor batch in iter(train_dataloader):\n    \t    # Training Code and Loss Optimization\n\n\t# Using Evaluation Mode when checking accuarcy on validation dataset\n\tmodel.eval()\n\tfor batch in iter(val_dataloader):\n    \t    # Only predictions and Loss Calculations. No backpropogation\n    \t    # No Optimzer Step so we do can omit unrequired layers.\n```", "```py\nimport torch\nimport torch.nn as nn\n\n# Inherit from the Module Base Class\nclass Model(nn.Module):\n      def __init__(self, input_size, output_size):\n    \t    # Initialize the Module Parent Class\n    \t    super().__init__()\n\n    \t     self.dense_layers = nn.ModuleList()\n\n    \t    # Add 5 Linear Layers and contain them within a Modulelist\n    \t    for i in range(5):\n        \t    self.dense_layers.append(\n            \t    nn.Linear(input_size, 512)\n        \t    )\n\n    \t    self.output_layer = nn.Linear(512, output_size)\n\n\tdef forward(self, x):\n\n    \t    # Simplifies Foward Propogation.\n     \t    # Instead of repeating a single line for each layer, use a loop\n    \t    for layer in range(len(self.dense_layers)):\n        \tx = layer(x)\n\n    \t    return self.output_layer(x)\n```"]