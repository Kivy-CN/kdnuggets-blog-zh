# 如何从头开始在 PyTorch 中实现 YOLO (v3) 目标检测器：第1部分

> 原文：[https://www.kdnuggets.com/2018/05/implement-yolo-v3-object-detector-pytorch-part-1.html](https://www.kdnuggets.com/2018/05/implement-yolo-v3-object-detector-pytorch-part-1.html)

[评论](#comments)

**由 [Ayoosh Kathuria](https://www.linkedin.com/in/ayoosh-kathuria-44a319132/)，研究实习生**

![头图](../Images/eb33d52c1b672fb81ea7c6f8adab2264.png)

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业道路

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织 IT 工作

* * *

目标检测是一个在深度学习的最新发展中受益匪浅的领域。近年来，人们开发了许多目标检测算法，其中包括 YOLO、SSD、Mask RCNN 和 RetinaNet。

在过去的几个月里，我一直在研究实验室中改进目标检测。这个经历的最大收获之一是认识到学习目标检测的最佳方法是从头实现算法。这正是我们将在本教程中做的。

我们将使用 PyTorch 实现基于 YOLO v3 的目标检测器，这是其中一种更快的目标检测算法。

本教程的代码设计为在 Python 3.5 和 PyTorch **0.4** 上运行。其完整代码可以在这个 [Github 仓库](https://github.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch) 中找到。

本教程分为5个部分：

1.  第1部分（这部分）：理解 YOLO 的工作原理

1.  [第2部分：创建网络架构的层](https://blog.paperspace.com/how-to-implement-a-yolo-v3-object-detector-from-scratch-in-pytorch-part-2/)

1.  [第3部分：实现网络的前向传播](https://blog.paperspace.com/how-to-implement-a-yolo-v3-object-detector-from-scratch-in-pytorch-part-3/)

1.  [第4部分：目标性分数阈值和非极大值抑制](https://blog.paperspace.com/how-to-implement-a-yolo-v3-object-detector-from-scratch-in-pytorch-part-4/)

1.  [第5部分：设计输入和输出管道](https://blog.paperspace.com/how-to-implement-a-yolo-v3-object-detector-from-scratch-in-pytorch-part-5/)

**前提条件**

+   你应该了解卷积神经网络的工作原理。这还包括对残差块、跳跃连接和上采样的了解。

+   了解什么是目标检测、边界框回归、IoU 和非极大值抑制。

+   基础 PyTorch 使用。你应该能够轻松创建简单的神经网络。

我已经在帖子末尾提供了链接，以防你在某方面有所欠缺。

**什么是 YOLO？**

YOLO 代表“你只看一次”。它是一个使用深度卷积神经网络学到的特征来检测物体的检测器。在我们开始动手编写代码之前，我们必须理解 YOLO 是如何工作的。

**全卷积神经网络**

YOLO 仅使用卷积层，使其成为一个全卷积网络（FCN）。它有 75 个卷积层，包括跳跃连接和上采样层。没有使用任何形式的池化，而是使用步幅为 2 的卷积层来下采样特征图。这有助于防止通常归因于池化的低级特征的丢失。

作为一个全卷积网络（FCN），YOLO 对输入图像的大小是不变的。然而，在实际应用中，我们可能会因为各种问题而希望坚持固定的输入大小，这些问题只有在我们实现算法时才会显现出来。

这些问题中一个重要的就是，如果我们想要批量处理图像（批量图像可以通过 GPU 并行处理，从而提升速度），我们需要所有图像具有固定的高度和宽度。这是为了将多个图像拼接成一个大批次（将多个 PyTorch 张量拼接成一个）。

网络通过一个称为**步幅**的因子来下采样图像。例如，如果网络的步幅是 32，那么尺寸为 416 x 416 的输入图像将产生尺寸为 13 x 13 的输出。一般来说，网络中任何层的***步幅*等于该层输出比输入图像小的因子。**

**解释输出**

通常情况下（所有物体检测器的情况都是如此），卷积层学到的特征会传递给一个分类器/回归器，该分类器/回归器进行检测预测（边界框的坐标、类别标签等）。

在 YOLO 中，预测是通过使用 1 x 1 卷积的卷积层来完成的。

现在，首先需要注意的是我们的**输出是一个特征图**。由于我们使用了 1 x 1 的卷积，预测图的大小正好是其之前的特征图的大小。在 YOLO v3（及其后续版本）中，解释这个预测图的方法是每个单元格可以预测固定数量的边界框。

> 尽管在特征图中描述一个单元的技术上正确的术语是*神经元*，但在我们的上下文中，将其称为单元格使其更加直观。

**在深度上，我们在特征图中有（B x (5 + C)）个条目。** B表示每个单元格可以预测的边界框数量。根据论文，每个这些B个边界框可能专门用于检测某种类型的对象。每个边界框都有*5 + C*个属性，这些属性描述了中心坐标、尺寸、对象性分数以及每个边界框的*C*类置信度。YOLO v3为每个单元格预测3个边界框。

**你期望特征图的每个单元格通过其边界框之一来预测一个对象，如果该对象的中心落在该单元格的感受野内。**（感受野是单元格可见的输入图像区域。有关卷积神经网络的更多解释，请参考链接）。

这与YOLO的训练方式有关，其中只有一个边界框负责检测任何给定的对象。首先，我们必须确定这个边界框属于哪个单元格。

为了做到这一点，我们将**输入**图像划分为与最终特征图尺寸相等的网格。

让我们看下面的例子，其中输入图像的尺寸为416 x 416，网络的步幅为32。如前所述，特征图的尺寸将是13 x 13。然后，我们将输入图像划分为13 x 13个单元格。

![yolo-5](../Images/8c0057d5335fa9f881d70482170f6afc.png)

然后，选择包含真实框中心的单元格（*在输入图像上*）作为负责预测对象的单元格。在图像中，它是标记为红色的单元格，包含真实框的中心（标记为黄色）。

现在，红色单元格是网格中第七行第七列的第七个单元格。我们现在将第七行第七列的单元格**在特征图上**（特征图上的对应单元格）指定为负责检测狗的单元格。

现在，这个单元格可以预测三个边界框。哪个会被分配给狗的真实标签？为了理解这一点，我们必须了解锚点的概念。

> *请注意，我们这里讨论的单元格是预测特征图上的一个单元格。我们将**输入图像**划分为网格只是为了确定**预测特征图**中哪个单元格负责预测*

**锚框**

预测边界框的宽度和高度可能有意义，但实际上，这会导致训练过程中的梯度不稳定。相反，大多数现代目标检测器预测对预定义默认边界框的对数空间变换，或者简单地预测偏移量，这些边界框称为**锚点**。

然后，将这些变换应用于锚框以获得预测。YOLO v3有三个锚点，这会导致每个单元格预测三个边界框。

回到之前的问题，负责检测狗的边界框将是其锚点与真实框具有最高IoU的那个。

**做出预测**

以下公式描述了网络输出如何转换以获得边界框预测。

![YOLO 方程](../Images/e289a333ec5b258865fdfe26bf2f7f99.png)

*bx, by, bw, bh* 是我们预测的 x, y 中心坐标、宽度和高度。*tx, ty, tw, th* 是网络输出的值。*cx* 和 *cy* 是网格的左上角坐标。*pw* 和 *ph* 是边界框的锚点尺寸。

**中心坐标**

注意我们将中心坐标预测值传递通过 sigmoid 函数。这会将输出值强制在0和1之间。为什么会这样？请稍等片刻。

通常，YOLO 不预测边界框中心的绝对坐标。它预测的是偏移量，这些偏移量是：

+   相对于预测对象的网格单元格的左上角。

+   通过特征图中单元格的尺寸进行归一化，即1。

例如，考虑我们的狗图像。如果中心的预测是 (0.4, 0.7)，这意味着中心位于13 x 13 特征图上的 (6.4, 6.7) 处。（因为红色单元格的左上角坐标是 (6,6)）。

但等一下，如果预测的 x,y 坐标大于1，比如 (1.2, 0.7)。这意味着中心位于 (7.2, 6.7)。注意到中心现在位于红色单元格的右边的单元格，或第七行第八个单元格。**这破坏了 YOLO 背后的理论**，因为如果我们假设红色框负责预测狗，那么狗的中心必须位于红色单元格中，而不是旁边的单元格中。

因此，为了纠正这个问题，输出会通过 sigmoid 函数，这样可以将输出压缩到0到1的范围内，有效地将中心保持在预测的网格中。

**边界框的尺寸**

边界框的尺寸是通过对输出应用对数空间转换，然后乘以锚点来预测的。

![](../Images/03bf22eecaa530eb1eeca50c8e76f07b.png)

*检测器输出如何转换以获得最终预测。图片来源：[http://christopher5106.github.io/](http://christopher5106.github.io/)*

结果预测的 *bw* 和 *bh* 是通过将输出进行对数空间转换，然后与锚点相乘来进行归一化的。（训练标签是这样选择的）。所以，如果包含狗的框的预测 *bx* 和 *by* 为 (0.3, 0.8)，则13 x 13特征图上的实际宽度和高度为 (13 x 0.3, 13 x 0.8)。

**对象性分数**

对象分数表示一个对象包含在边界框内的概率。对于红色及相邻网格，这个值应接近1，而对于角落的网格，这个值应接近0。

对象性分数也会通过 sigmoid 函数，因为它需要被解释为概率。

**类别置信度**

类别置信度表示检测到的对象属于特定类别（如狗、猫、香蕉、汽车等）的概率。在v3之前，YOLO 使用 softmax 对类别分数进行处理。

然而，这种设计选择在v3中被放弃，作者选择使用sigmoid代替。原因是Softmax的类别分数假设类别是互斥的。简单来说，如果一个对象属于一个类别，那么它就不能属于另一个类别。这对于我们将基于COCO数据库的检测器来说是成立的。

然而，当我们有*女性*和*人*这样的类别时，这种假设可能不成立。这就是为什么作者避免使用Softmax激活的原因。

**不同尺度的预测。**

YOLO v3在3个不同尺度上进行预测。检测层用于在具有**步幅32、16、8**的三个不同大小的特征图上进行检测。这意味着，在输入为416 x 416时，我们在尺度为13 x 13、26 x 26和52 x 52的图像上进行检测。

网络将输入图像进行下采样，直到第一个检测层，在该层使用步幅为32的特征图进行检测。进一步地，层被上采样2倍，并与具有相同特征图大小的前一层的特征图进行拼接。然后在步幅为16的层上进行再次检测。相同的上采样程序重复进行，最终在步幅为8的层上进行检测。

在每个尺度上，每个单元使用3个锚点预测3个边界框，总共使用了9个锚点。（不同尺度的锚点不同）

![](../Images/2f2e5b26f9b0b03ff43fd8eebe3fd82f.png)

作者报告说，这有助于YOLO v3在检测小物体方面表现得更好，这是YOLO早期版本经常被投诉的问题。上采样可以帮助网络学习细粒度特征，这对检测小物体至关重要。

**输出处理**

对于尺寸为416 x 416的图像，YOLO预测((52 x 52) + (26 x 26) + 13 x 13)) x 3 = **10647个边界框**。然而，在我们的图像中，只有一个对象，一只狗。我们如何将10647个检测减少到1个？

**按对象置信度进行阈值处理**

首先，我们根据对象置信度分数过滤框。通常，分数低于阈值的框会被忽略。

**非极大值抑制**

NMS旨在解决同一图像的多次检测问题。例如，红色网格单元的所有3个边界框可能检测到一个框，或相邻的单元可能检测到相同的对象。

![](../Images/b98bbebed7b6b3886094716c258cfd09.png)

如果你不清楚NMS，我提供了一个链接来解释相关内容。

**我们的实现**

YOLO只能检测数据集中存在的类别的对象。我们将使用官方的权重文件进行检测器。这些权重是通过在COCO数据集上训练网络获得的，因此我们可以检测80个对象类别。

第一部分到此为止。此帖子解释了足够多的YOLO算法内容，以使你能够实现检测器。然而，如果你想深入了解YOLO的工作原理、如何训练以及与其他检测器的性能比较，你可以阅读下面提供的原始论文。

本部分到此为止。下一部分中，我们将在[这里](https://blog.paperspace.com/how-to-implement-a-yolo-v3-object-detector-from-scratch-in-pytorch-part-2/)实现组装检测器所需的各种层。

**进一步阅读**

1.  [YOLO V1: 你只需看一次: 统一的实时物体检测](https://arxiv.org/pdf/1506.02640.pdf)

1.  [YOLO V2: YOLO9000: 更好、更快、更强](https://arxiv.org/pdf/1612.08242.pdf)

1.  [YOLO V3: 递增的改进](https://pjreddie.com/media/files/papers/YOLOv3.pdf)

1.  [卷积神经网络](http://cs231n.github.io/convolutional-networks/)

1.  [边界框回归（附录C）](https://arxiv.org/pdf/1311.2524.pdf)

1.  [IoU](https://www.youtube.com/watch?v=DNEm4fJ-rto)

1.  [非极大值抑制](https://www.youtube.com/watch?v=A46HZGR5fMw)

1.  [PyTorch官方教程](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)

**简介: [Ayoosh Kathuria](https://www.linkedin.com/in/ayoosh-kathuria-44a319132/)** 目前是印度国防研究与发展组织的实习生，致力于提升模糊视频中的物体检测。工作之余，他要么在睡觉，要么在吉他上弹奏Pink Floyd。你可以通过[LinkedIn](https://www.linkedin.com/in/ayoosh-kathuria-44a319132/)与他联系，或者在[GitHub](https://github.com/ayooshkathuria)查看他的更多工作。

[原文](https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/)。已获得许可转载。

**相关内容:**

+   [PyTorch入门第1部分: 理解自动微分的工作原理](/2018/04/getting-started-pytorch-understanding-automatic-differentiation.html)

+   [PyTorch张量基础](/2018/05/pytorch-tensor-basics.html)

+   [使用Google Colab、TensorFlow、Keras & PyTorch进行深度学习开发](/2018/02/google-colab-free-gpu-tutorial-tensorflow-keras-pytorch.html)

### 更多相关主题

+   [成为伟大数据科学家的5项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)

+   [每个初学者数据科学家应该掌握的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)

+   [2021年最佳ETL工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)

+   [停止学习数据科学以寻找目标，并通过寻找目标…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)

+   [一个90亿美元的AI失败案例分析](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)

+   [建立一个可靠的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)
