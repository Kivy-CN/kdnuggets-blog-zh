- en: Understanding Machine Learning Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解机器学习算法
- en: 原文：[https://www.kdnuggets.com/2017/10/understanding-machine-learning-algorithms.html](https://www.kdnuggets.com/2017/10/understanding-machine-learning-algorithms.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/10/understanding-machine-learning-algorithms.html](https://www.kdnuggets.com/2017/10/understanding-machine_learning_algorithms.html)
- en: '**By Brett Wujek, SAS.**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：布雷特·伍杰克，SAS。**'
- en: Machine learning is now mainstream.  And given the success companies see deriving
    value from the vast amount of available data, everyone wants in.  But while the
    thought of machine learning can seem overwhelming, it’s not magic, and the basic
    concepts are fairly simple.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习现在已成为主流。鉴于公司从大量可用数据中获得价值的成功，每个人都想参与其中。虽然机器学习的想法可能令人感到压倒，但这并非魔法，基本概念相当简单。
- en: Here I’ll give you a foundation for understanding the ideas behind some of the
    most popular machine learning algorithms.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我将为你提供理解一些最流行的机器学习算法背后思想的基础。
- en: Random Forests
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林
- en: Devised by [Leo Breiman in 2001](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf),
    a random forest is a simple, yet powerful algorithm comprised of an ensemble (or
    collection)of independently trained decision trees.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是由 [Leo Breiman 于 2001 年](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)
    提出的，这是一种简单而强大的算法，由独立训练的决策树集合（或集合）组成。
- en: In a decision tree, the entire data set of observations is recursively partitioned
    into subsets along different branches so that similar observations are grouped
    together at the terminal leaves.  The variable that optimally increases the “purity”
    of the tree directs the partitioning at each split point, grouping observations
    with similar target values.  For any new observation, you can follow the path
    of splitting rules through the tree to predict the target.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树中，整个观察数据集沿着不同的分支递归地划分为子集，以便相似的观察结果在终端叶子处聚集在一起。最优增加树“纯度”的变量在每个分割点指导分割，分组具有相似目标值的观察结果。对于任何新的观察，你可以沿着树中的分割规则路径来预测目标。
- en: '![](../Images/98b7e01c3a28ecaa616770c0987f5bf4.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98b7e01c3a28ecaa616770c0987f5bf4.png)'
- en: But decision trees are unstable. Small changes in training data can lead to
    wildly different trees. Training a group of trees and “ensembling” them to create
    a “random forest” model enables more robust prediction.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但决策树是不稳定的。训练数据的微小变化可能导致完全不同的树。训练一组树并“集成”它们以创建一个“随机森林”模型可以实现更稳健的预测。
- en: 'In a random forest:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机森林中：
- en: The data set used to train each tree is a random sample of the complete data
    set (with replacement), a process known as “bagging” (bootstrap aggregating).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练每棵树的数据集是完整数据集的随机样本（带替代），这一过程称为“袋装”（bootstrap aggregating）。
- en: Input variables considered for splitting each node are a *random subset of all
    variables*, reducing bias toward the most influential factors and allowing secondary
    factors to play a role in the model.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑用于分割每个节点的输入变量是所有变量的*随机子集*，这减少了对最具影响力因素的偏倚，并允许次要因素在模型中发挥作用。
- en: Trees are overfit by letting them grow to a large depth and small leaf size.
    Averaging the predicted probabilities for a large number of overfit trees is more
    robust than using a single, fine-tuned decision tree.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过让树成长到较大的深度和较小的叶子大小来过拟合。对大量过拟合树的预测概率进行平均比使用单棵调整过的决策树更稳健。
- en: Ensembling, or scoring new observations on many trees enables you to obtain
    a consensus for a predicted target value(voting for classification, averaging
    for interval target prediction) with a more robust and generalizable model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 集成，或在许多树上对新观察进行评分，使你能够通过更稳健和具有普遍性模型的共识来获得预测目标值（分类投票，区间目标预测的平均）。
- en: Because random forests are so effective,  researchers have evolved variants
    such as [“similarity forests”](http://www.kdd.org/kdd2017/papers/view/similarity-forests)
    and [“deep embedding forests.”](https://dl.acm.org/citation.cfm?id=3098059)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于随机森林的高效性，研究人员发展了诸如 [“相似性森林”](http://www.kdd.org/kdd2017/papers/view/similarity-forests)
    和 [“深度嵌入森林”](https://dl.acm.org/citation.cfm?id=3098059) 的变体。
- en: Gradient Boosting Machines
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度提升机器
- en: A gradient boosting machine, such as [XGBoost](http://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf),
    is another machine learning algorithm derived from decision trees.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一种梯度提升机器，如 [XGBoost](http://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf)，是另一种源于决策树的机器学习算法。
- en: In the late 90’s, Breiman observed that a model that exhibits a certain level
    of error can serve as a base learner that can be improved by iteratively adding
    models that compensate for the error – a process called “boosting.” Each added
    model is trained to minimize the error by searching in the negative gradient direction
    (thus “gradient” boosting). Boosting is a sequential process since new models
    are added based on the results of previous models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在90年代末，Breiman 观察到，一个表现出一定错误水平的模型可以作为基础学习器，通过迭代添加补偿错误的模型来改进——这一过程称为“提升”。每个新增模型都通过在负梯度方向上搜索来最小化错误（因此称为“梯度”提升）。提升是一个顺序过程，因为新模型是基于之前模型的结果添加的。
- en: Jerome Friedman applied the gradient boosting concept to decision trees in 2001to
    create [Gradient Boosting Machines](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf).
    The popular and effective Adaboost from Freund and Shapire (2001), which uses
    an exponential loss function, is a variant of this approach.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Jerome Friedman 在 2001 年将梯度提升概念应用于决策树，创建了 [Gradient Boosting Machines](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)。Freund
    和 Shapire (2001) 的流行且有效的 Adaboost，使用指数损失函数，是这种方法的一种变体。
- en: '![](../Images/4d3460a4818b8eb9d8198c5068bbb315.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d3460a4818b8eb9d8198c5068bbb315.png)'
- en: In recent years, Tiangi Chen studied all aspects and revisions of the gradient
    boosting machine algorithm to further enhance performance in an implementation
    known as XGBoost, which stands for “extreme gradient boosting.”His modifications
    included using quantile binning in the decision tree training, adding a regularization
    term to the objective to improve generalization, and supporting multiple forms
    of the loss function. Ultimately, the “extreme” nature of XGBoost could be attributed
    to the fact that Chen implemented it specifically to take advantage of the more
    readily available computing power with distributed multi-threaded processing for
    speed and efficiency.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，Tiangi Chen 研究了梯度提升机算法的各个方面及修订，以进一步增强在 XGBoost 实现中的性能，XGBoost 代表“极端梯度提升”。他的修改包括在决策树训练中使用分位数分箱，将正则化项添加到目标函数中以改善泛化能力，并支持多种损失函数。最终，XGBoost
    的“极端”特性可以归因于 Chen 特意实现它，以利用分布式多线程处理的计算能力，从而提升速度和效率。
- en: Support Vector Machines
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Yet another “machine” in the machine learning practitioner’s toolbox, a support
    vector machine (SVM), is one that has been around for many decades.  Invented
    in 1963 by Vladimir Vapnik and Alexey Chervonenkis, SVM is a binary classification
    model that constructs a hyperplane to separate observations into two classes.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）是机器学习从业者工具箱中的另一种“机器”，已有几十年的历史。SVM 由 Vladimir Vapnik 和 Alexey Chervonenkis
    于1963年发明，是一种二分类模型，通过构造超平面将观察值分为两个类别。
- en: The figure below is a simple example with target values for observations represented
    by dots and x’s, which can split as shown.  Since an infinite number of orientations
    for this line (a hyperplane in multiple dimensions)can separate these points,
    we define a buffer zone around this line and formulate an optimization problem
    to maximize this margin. The more we can separate these two classes, the more
    accurate our model will be when scoring new data. The points that define the margin
    are called “support vectors.”
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是一个简单的示例，观察值的目标值用点和叉表示，可以如图所示进行分割。由于无限多个方向的这条线（在多维空间中的超平面）可以分隔这些点，我们在这条线上定义一个缓冲区，并制定一个优化问题以最大化这个边际。我们能够分离这两个类别的越多，我们的模型在对新数据评分时就会越准确。定义边际的点被称为“支持向量”。
- en: In reality, most multi-dimensional, real-world data sets are not perfectly separable.
    Rarely will all observations from one class be on one side and those from the
    other class be on the other side.  SVM  constructs the best separating hyperplane
    by introducing a penalty for observations that fall on the wrong side of the margin
    (indicated by red arrows).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，大多数多维实际数据集并不是完全可分的。很少会有所有来自一个类别的观察值都在一侧，而来自另一个类别的观察值都在另一侧。SVM 通过引入对落在边际错误一侧的观察值的惩罚（用红色箭头表示），来构造最佳的分隔超平面。
- en: '![](../Images/82e478840cffc34c494f7a2987bb8dbf.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82e478840cffc34c494f7a2987bb8dbf.png)'
- en: Fig. Hyperplane separating classes in SVM
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图. SVM 中分隔类别的超平面
- en: Note that margin size governs a tradeoff between correctly classifying training
    data and generalizing to future data. A wide margin means more misclassified training
    points but generalizes better—always keep in mind that the goal is to apply models
    to future data rather than achieve the best fit to existing data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到边距大小在正确分类训练数据和对未来数据进行泛化之间存在权衡。较大的边距意味着更多的训练点被误分类，但泛化效果更好——始终记住目标是将模型应用于未来数据，而不是在现有数据上获得最佳拟合。
- en: Another obstacle to separating data into two classes is that data can rarely
    be separated *linearly* and higher dimensionality must be accounted for in some
    way. The figure below depicts a one-dimensional problem where one class is contained
    in one interval and the other class falls to the left and right.  A straight line
    cannot separate these points effectively, so SVM uses a kernel function to map
    data to a higher-dimensional space.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个将数据分为两类的障碍是数据很少能*线性*分隔，必须以某种方式考虑更高的维度。下图描述了一个一维问题，其中一个类别包含在一个区间内，另一个类别则分布在左侧和右侧。直线无法有效地分隔这些点，因此SVM使用核函数将数据映射到更高维空间。
- en: The penalty and the “kernel  trick” make SVM a powerful classifier. This technique
    is also used in an algorithm called support vector data description for anomaly
    detection.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 惩罚项和“核技巧”使SVM成为一个强大的分类器。这项技术也用于一种称为支持向量数据描述的算法，用于异常检测。
- en: '![](../Images/41f90cb27db6e58caa007b4f07838e9f.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41f90cb27db6e58caa007b4f07838e9f.png)'
- en: Fig. Applying kernel function for easier separation
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图：应用核函数以便于分隔
- en: Neural Networks
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络
- en: Developed in the 1950’s, neural networks are the poster child for machine learning,
    particularly “deep learning.”They form the basis for complex applications such
    as speech-to-text, image classification and object detection. If you jump right
    to the more sophisticated forms used for deep learning, it’s easy to be overwhelmed;
    but the basic functioning of a neural network is not difficult to follow.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在1950年代发展起来，是机器学习特别是“深度学习”的典型代表。它们是复杂应用程序的基础，如语音转文本、图像分类和对象检测。如果你直接跳到用于深度学习的更复杂形式，可能会感到不知所措；但神经网络的基本功能并不难以理解。
- en: A neural network is simply a set of mathematical transformations applied to
    input data as it feeds through a network of interconnected nodes (called neurons)
    organized in layers.  Each input is represented by an input node. These nodes
    are connected to nodes in the next layer, with a weight assigned to each connection,
    loosely representing a general significance of that input. As values travel from
    node to node, they are multiplied by the weights for the connections and an activation
    function is applied. The resulting value for each node is then passed through
    the network to nodes in the next layer. Ultimately, an activation function in
    an output node assigns a value to provide the prediction.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络只是将数学变换应用于输入数据的一个集合，数据在一个由互联节点（称为神经元）组织的网络中传递。每个输入由一个输入节点表示。这些节点连接到下一层的节点，每个连接上分配一个权重，粗略表示该输入的一般重要性。当值从一个节点传递到另一个节点时，它们会被乘以连接的权重，并应用激活函数。每个节点的结果值随后会传递到网络中的下一层节点。最终，输出节点中的激活函数分配一个值来提供预测。
- en: '![](../Images/8b032ecbc673c5bab633d9041f068fa3.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b032ecbc673c5bab633d9041f068fa3.png)'
- en: Training a neural network is all about finding the right values for the weights.
    A first pass of data through the network with initial settings for weights yields
    output values that do not closely match the known output values from the training
    data. A back-propagation process uses gradient information (quantifying how a
    change in weight values affects outputs at each node) to adjust the weights and
    cycles through the network repeatedly until the weights converge to values with
    acceptable error between predicted and actual values.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络的核心是找到权重的正确值。数据首次通过网络时，初始权重设置产生的输出值与训练数据中的已知输出值不匹配。反向传播过程使用梯度信息（量化权重值变化如何影响每个节点的输出）来调整权重，并在网络中反复循环，直到权重收敛到具有可接受误差的值，即预测值与实际值之间的误差。
- en: Because of the ultra-flexibility in the combination of the number of hidden
    layers, the number of neurons in each layer, and the activation functions used,
    neural networks can provide extremely accurate predictions for highly non-linear
    systems. One of my favorite tools for illustrating neural networks is this [cool
    interactive example](https://playground.tensorflow.org/) developed by researchers
    from Google.  Check it out…you won’t be disappointed.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于隐藏层数量、每层神经元数量和所用激活函数的超灵活组合，神经网络可以为高度非线性系统提供极其准确的预测。我最喜欢用来说明神经网络的工具之一是这个由谷歌研究人员开发的[有趣的互动示例](https://playground.tensorflow.org/)。
    快去看看吧…你不会失望的。
- en: With this simple explanation of neural networks serving as a foundation, you
    can now delve into the more advanced variants such as convolutional neural networks,
    recurrent neural networks, and denoising autoencoders.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个简单的神经网络解释作为基础，你现在可以深入研究更高级的变体，如卷积神经网络、递归神经网络和去噪自编码器。
- en: The Magic is Gone
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 魔法已逝
- en: There you have it – the ideas behind four of the most popular machine learning
    algorithms.  While these algorithms build highly predictive models, they’re not
    magic. A grounding in the fundamental concepts will help you understand these
    algorithms–and those extending them in novel ways.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是四种最受欢迎的机器学习算法背后的理念。虽然这些算法构建了高度预测的模型，但它们并不是魔法。对基础概念的掌握将帮助你理解这些算法——以及以新颖方式扩展它们的算法。
- en: '**Bio: [Brett Wujek](https://www.linkedin.com/in/brettwujek/)** is a Senior
    Data Scientist with the R&D team in the SAS Advanced Analytics division.  He helps
    evangelize and guide the direction of advanced analytics development at SAS, particularly
    in the areas of machine learning and data mining.  Prior to joining SAS, Brett
    led the development of process integration and design exploration technologies
    at Dassault Systemes, helping to architect and implement industry-leading computer-aided
    optimization software for product design applications. His formal background is
    in design optimization methodologies, receiving his PhD from the University of
    Notre Dame for his work developing efficient algorithms for multidisciplinary
    design optimization.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介： [布雷特·伍杰克](https://www.linkedin.com/in/brettwujek/)** 是SAS高级分析部门的高级数据科学家。他帮助推广和指导SAS高级分析的发展方向，特别是在机器学习和数据挖掘领域。在加入SAS之前，布雷特在达索系统公司领导了过程集成和设计探索技术的开发，帮助设计和实施了行业领先的计算机辅助优化软件。他的正式背景是设计优化方法论，他在圣母大学获得了博士学位，研究了用于多学科设计优化的高效算法。'
- en: '**Related:**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[**Fundamental Breakthrough in 2 Decade Old Algorithm Redefines Big Data Benchmarks**](/2017/09/minwise-hashing-breakthrough-big-data-benchmarks.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**二十年老算法的突破重新定义了大数据基准**](/2017/09/minwise-hashing-breakthrough-big-data-benchmarks.html)'
- en: '[**Data Science Primer: Basic Concepts for Beginners**](/2017/08/data-science-primer-basic-concepts-for-beginners.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**数据科学基础：初学者的基本概念**](/2017/08/data-science-primer-basic-concepts-for-beginners.html)'
- en: '[**Must-Know: What is the idea behind ensemble learning?**](/2017/05/must-know-ensemble-learning.html)`'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**必须了解：集成学习的理念是什么？**](/2017/05/must-know-ensemble-learning.html)`'
- en: '* * *'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织IT需求'
- en: '* * *'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成为优秀数据科学家所需的5项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每位初学者数据科学家都应该掌握的 6 个预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2021 年最佳 ETL 工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目的，并寻找目的以…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[是什么使得 Python 成为初创公司理想的编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
