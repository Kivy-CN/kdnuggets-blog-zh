["```py\nimport numpy as np\nimport pandas as pd\nx = np.random.normal(loc= 185.0, scale=1.0, size=10000)\nnp.mean(x) \n```", "```py\nimport random\n\nsample_mean = []\n\n# Bootstrap Sampling\nfor i in range(40):\n    y = random.sample(x.tolist(), 5)\n    avg = np.mean(y)\n\n    sample_mean.append(avg)\nnp.mean(sample_mean)\n```", "```py\nfrom sklearn import metrics\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Load the iris dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\ndef bootstrap(X, y, n_samples=2000):\n    models = []\n    precision = []\n    recall = []\n    f1 = []\n    indices_x = []\n    indices_y = []\n\n    for i in range(n_samples):\n        index_x = np.random.choice(X.shape[0], size=X.shape[0], replace=True)\n        indices_x.append(index_x)\n        X_sample = X[index_x, :]\n\n        index_y = np.random.choice(y.shape[0], size=y.shape[0], replace=True)\n        indices_y.append(index_y)\n        y_sample = y[index_y]\n\n        X_train, X_test, y_train, y_test = train_test_split(\n            X_sample, y_sample, test_size=0.2, random_state=42\n        )\n\n        model = DecisionTreeClassifier().fit(X_train, y_train)\n        models.append(model)\n\n        y_pred = model.predict(X_test)\n        precision.append(\n            metrics.precision_score(y_test, y_pred, average=\"macro\")\n        )\n        recall.append(metrics.recall_score(y_test, y_pred, average=\"macro\"))\n        f1.append(metrics.f1_score(y_test, y_pred, average=\"macro\"))\n    # Save the results to a Pandas dataframe\n    pred_df = pd.DataFrame(\n        {\n            \"Precision\": precision,\n            \"Recall\": recall,\n            \"F1\": f1,\n            \"Models\": models,\n            \"Indices_X\": indices_x,\n            \"Indices_Y\": indices_y,\n        }\n    ) \n```", "```py\nmodels, pred_df = bootstrap(X, y)\n```", "```py\npred_df.head()\n```", "```py\npred_df['Model Number'] = pred_df.index,\n```", "```py\npred_df.sort_values(by= \"Precision\", ascending = False).head(5)\n```", "```py\nimport matplotlib.pyplot as plt\n\n# Create a figure and subplots\nfig, (ax2, ax3, ax4) = plt.subplots(1, 3, figsize=(14, 8))\n\nbest_of = pred_df.sort_values(by=\"Precision\", ascending=False).head(5)\n\n# Create the first graph\nbest_of.plot(\n    kind=\"barh\",\n    x=\"Model Number\",\n    y=\"Precision\",\n    color=\"mediumturquoise\",\n    ax=ax2,\n    legend=False,\n)\nax2.set_xlabel(\n    \"Precision Score\",\n    fontstyle=\"italic\",\n    fontsize=14,\n    font=\"Courier New\",\n    fontweight=\"bold\",\n    y=1.1,\n)\nylabel = \"Model\\nName\"\nax2.set_ylabel(ylabel, fontsize=16, font=\"Courier\")\nax2.set_title(\"Precision\", fontsize=16, fontstyle=\"italic\")\n\nfor index, value in enumerate(best_of[\"Precision\"]):\n    ax2.text(\n        value / 2,\n        index,\n        str(round(value, 2)),\n        ha=\"center\",\n        va=\"center\",\n        fontsize=14,\n        font=\"Comic Sans MS\",\n    )\nbest_of = pred_df.sort_values(by=\"Recall\", ascending=False).head(5)\n\n# Create the second graph\nbest_of.plot(\n    kind=\"barh\",\n    x=\"Model Number\",\n    y=\"Recall\",\n    color=\"orange\",\n    ax=ax3,\n    legend=False,\n)\nax3.set_xlabel(\n    \"Recall Score\",\n    fontstyle=\"italic\",\n    fontsize=14,\n    font=\"Courier New\",\n    fontweight=\"bold\",\n)\nax3.set_ylabel(ylabel, fontsize=16, font=\"Courier\")\nax3.set_title(\"Recall\", fontsize=16, fontstyle=\"italic\")\n\nfor index, value in enumerate(best_of[\"Recall\"]):\n    ax3.text(\n        value / 2,\n        index,\n        str(round(value, 2)),\n        ha=\"center\",\n        va=\"center\",\n        fontsize=14,\n        font=\"Comic Sans MS\",\n    )\n# Create the third graph\nbest_of = pred_df.sort_values(by=\"F1\", ascending=False).head(5)\nbest_of.plot(\n    kind=\"barh\",\n    x=\"Model Number\",\n    y=\"F1\",\n    color=\"darkorange\",\n    ax=ax4,\n    legend=False,\n)\nax4.set_xlabel(\n    \"F1 Score\",\n    fontstyle=\"italic\",\n    fontsize=14,\n    font=\"Courier New\",\n    fontweight=\"bold\",\n)\nax4.set_ylabel(ylabel, fontsize=16, font=\"Courier\")\nax4.set_title(\"F1\", fontsize=16, fontstyle=\"italic\")\n\nfor index, value in enumerate(best_of[\"F1\"]):\n    ax4.text(\n        value / 2,\n        index,\n        str(round(value, 2)),\n        ha=\"center\",\n        va=\"center\",\n        fontsize=14,\n        font=\"Comic Sans MS\",\n    )\n# Fit the figure\nplt.tight_layout()\n\nplt.suptitle(\n    \"Bootstrapped Model Performance Metrics\",\n    fontsize=18,\n    y=1.05,\n    fontweight=\"bold\",\n    fontname=\"Comic Sans MS\",\n)\n\n# Show the figure\nplt.show() \n```", "```py\nX_sample = X[pred_df.iloc[397][\"Indices_X\"], :]\ny_sample = y[pred_df.iloc[397][\"Indices_Y\"]]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_sample, y_sample, test_size=0.2, random_state=42\n)\n\nmodel = DecisionTreeClassifier().fit(X_train, y_train)\nmodels.append(model)\n\ny_pred = model.predict(X_test)\nprecision_397 = metrics.precision_score(y_test, y_pred, average=\"macro\")\nrecall_397 = metrics.recall_score(y_test, y_pred, average=\"macro\")\nf1_397 = metrics.f1_score(y_test, y_pred, average=\"macro\")\nprint(\"Precision : {}\".format(precision_397))\nprint(\"Recall : {}\".format(recall_397))\nprint(\"F1 : {}\".format(f1_397)) \n```"]