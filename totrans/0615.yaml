- en: 'How our Obsession with Algorithms Broke Computer Vision: And how Synthetic
    Computer Vision can fix it'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们对算法的痴迷如何破坏了计算机视觉：以及合成计算机视觉如何修复它
- en: 原文：[https://www.kdnuggets.com/2021/10/obsession-algorithms-broke-computer-vision.html](https://www.kdnuggets.com/2021/10/obsession-algorithms-broke-computer-vision.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/10/obsession-algorithms-broke-computer-vision.html](https://www.kdnuggets.com/2021/10/obsession-algorithms-broke-computer-vision.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Paul Pop](https://www.linkedin.com/in/paul-pop/), Co-founder and CEO
    at Neurolabs**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：[Paul Pop](https://www.linkedin.com/in/paul-pop/)，Neurolabs的联合创始人兼首席执行官**'
- en: '![Figure](../Images/1e7e56aacfd8b6b1665e1afc882cfe48.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/1e7e56aacfd8b6b1665e1afc882cfe48.png)'
- en: Synthetic Computer Vision aims to translate what’s in the Virtual world back
    to the Real world. (Image by author)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 合成计算机视觉旨在将虚拟世界中的内容翻译回现实世界。（图片来自作者）
- en: ????️ The Current State of Computer Vision
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ????️ 计算机视觉的现状
- en: '* * *'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的快车道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织IT需求'
- en: '* * *'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: As of today, there has been over $15B worth of investments in over 1,800 Computer
    Vision startups in the past 8 years, according to [Crunchbase](https://www.crunchbase.com/login?redirect_to=%2Fhub%2Fcomputer-vision-companies).
    More than 20 of these companies are currently valued above $1B and there’s a lot
    more to come according to [Forbes](https://www.forbes.com/sites/robtoews/2021/02/28/a-wave-of-billion-dollar-computer-vision-startups-is-coming/).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[Crunchbase](https://www.crunchbase.com/login?redirect_to=%2Fhub%2Fcomputer-vision-companies)的数据，截至目前，过去8年里，计算机视觉领域的初创公司获得了超过150亿美元的投资。超过20家公司目前的估值已超过10亿美元，并且根据[Forbes](https://www.forbes.com/sites/robtoews/2021/02/28/a-wave-of-billion-dollar-computer-vision-startups-is-coming/)的报道，未来还会有更多。
- en: Why are these companies valued so greatly? To put it simply, they are teaching
    computers how to see. By doing so, they are automating tasks that have previously
    been accomplished using human sight.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这些公司如此受重视？简单来说，它们在教计算机如何“看”。通过这样做，它们在自动化以前由人类视觉完成的任务。
- en: This boom followed a 2012 technology **inflection** point in Computer Vision,
    with the advent of Neural Networks — algorithms that mimic the human brain and
    are trained using colossal amounts of human-labelled data. Since 2012, algorithms
    have steadily improved and have become a match for humans in many visual tasks,
    for example counting objects, [lip reading](https://www.technologyreview.com/2016/11/21/69566/ai-has-beaten-humans-at-lip-reading/) or [cancer
    screening](https://www.nature.com/articles/s41598-019-48995-4).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这一繁荣是由于2012年计算机视觉领域的**拐点**，随着神经网络的出现——这些算法模仿人脑，并使用大量人工标注的数据进行训练。自2012年以来，算法不断改进，在许多视觉任务中已能与人类匹敌，例如物体计数，[唇读](https://www.technologyreview.com/2016/11/21/69566/ai-has-beaten-humans-at-lip-reading/)或[癌症筛查](https://www.nature.com/articles/s41598-019-48995-4)。
- en: 'In the 10 years that followed everybody did their part: academia led the way
    with better algorithms; large companies invested in an army of humans who have
    diligently labelled these image datasets. Some of these efforts were even open
    sourced for the benefit of the community, such as [ImageNet](https://www.image-net.org/),
    a 14 million image dataset.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的10年里，每个人都尽了自己的一份力：学术界通过改进算法引领了潮流；大公司则投资了大量人力，勤勉地标注这些图像数据集。其中一些努力甚至被开源，惠及社区，比如[ImageNet](https://www.image-net.org/)，一个包含1400万张图像的数据集。
- en: 'Unfortunately, now as these systems are getting deployed to productions, we
    are hitting a brick wall:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，现在这些系统正在投入生产时，我们遇到了瓶颈：
- en: The labelled **data that we have is unreliable**. A [systematic study from MIT
    researchers](https://venturebeat.com/2021/03/28/mit-study-finds-systematic-labeling-errors-in-popular-ai-benchmark-datasets/) of
    popular ML datasets, found an average error rate of [incorrect labelling of 5.93%](https://l7.curtisnorthcutt.com/label-errors) for
    ImageNet and an average of 3.4% across other datasets.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们拥有的**标记数据是不可靠的**。来自MIT研究人员的[系统研究](https://venturebeat.com/2021/03/28/mit-study-finds-systematic-labeling-errors-in-popular-ai-benchmark-datasets/)发现，ImageNet的平均错误率为[5.93%的错误标记](https://l7.curtisnorthcutt.com/label-errors)，其他数据集的平均错误率为3.4%。
- en: There is **little effort dedicated to solving the data problem**. The intellectual
    efforts of academia are almost entirely focused on algorithm development, ignoring
    the fundamental need for good data — a guesstimate by Andrew Ng puts the ratio
    at [99% algorithm focus vs 1% data](https://www.youtube.com/watch?t=526&v=06-AZXmwHjo&feature=youtu.be).
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**解决数据问题的努力很少**。学术界的智力投入几乎完全专注于算法开发，忽视了对良好数据的基本需求——Andrew Ng的估计比例为[99%的算法关注
    vs 1%的数据](https://www.youtube.com/watch?t=526&v=06-AZXmwHjo&feature=youtu.be)。'
- en: Computer Vision **algorithms don’t generalise** well from one domain to another
    An algorithm trained to detect cars in the south of France will struggle to detect
    the same car in snowy Norway. Likewise a system trained on specific cameras might
    fail with another camera make and model.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算机视觉**算法不能很好地泛化**从一个领域到另一个领域。一个在法国南部训练检测汽车的算法在雪地中的挪威会很难检测到同样的汽车。同样，一个在特定相机上训练的系统可能在另一种相机品牌和型号上表现不佳。
- en: ♟️ Searching for inspiration
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ♟️ 寻找灵感
- en: Already in 1946 Alan Turing suggested chess as a benchmark for computer capabilities,
    which was since throughly researched receiving a lot of media attention.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 早在1946年，艾伦·图灵就建议将棋类作为计算机能力的基准，此后这一领域经过了大量研究，吸引了广泛的媒体关注。
- en: A commonly accepted way to measure performance in chess is through the [Elo
    rating system](https://en.wikipedia.org/wiki/Elo_rating_system), which provides
    a valid comparison of player skills. The graph below shows world champions and
    chess game engines. The human performance is hovering around the 2800 rating for
    the past 50 years, which is then suppressed by computers in 2010.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量棋类表现的一种被广泛接受的方法是通过[Elo评级系统](https://en.wikipedia.org/wiki/Elo_rating_system)，它提供了对玩家技能的有效比较。下图展示了世界冠军和棋类引擎。过去50年里，人类表现徘徊在2800的评级附近，然后在2010年被计算机所压制。
- en: Until the last decade, we humans have designed chess algorithms to play based
    on rules we could design and understand. The Deep Learning revolution allowed
    us to break beyond human understanding, bringing a leap forward — just like it
    has for Computer Vision.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 直到上个十年，我们人类设计的棋类算法基于我们能设计和理解的规则。深度学习革命让我们突破了人类理解的局限，带来了飞跃——就像它在计算机视觉领域一样。
- en: '![Figure](../Images/57cfd50d649de09f6afcb9701341ee18.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/57cfd50d649de09f6afcb9701341ee18.png)'
- en: Chess engine and human ELO ratings (Image by author)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 棋类引擎与人类ELO评级（图片来自作者）
- en: As good as the progress of Deep Learning chess game engines was, it has now
    been suppressed by the next level of chess engine: **AlphaZero** from DeepMind.
    What’s more impressive, is that **AlphaZero did not use any human sourced data** to
    achieve this performance. It was built without any knowledge of historical chess
    games, or any human guidance for finding optimal moves. AlphaZero was the teacher
    and the student — it taught itself how to better play the game by competing against
    itself and learning through the process.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习棋类游戏引擎的进步非常显著，但现在已经被下一个级别的棋类引擎：**AlphaZero**（来自DeepMind）所压制。更令人印象深刻的是，**AlphaZero并未使用任何人类来源的数据**来实现这一性能。它的构建没有任何关于历史棋局的知识，也没有任何寻找最佳走法的人类指导。AlphaZero既是老师也是学生——它通过自我对弈并在过程中学习来教会自己如何更好地玩游戏。
- en: AlphaZero won against [Stockfish 8](https://www.notion.so/neurolabs/chess.com/news/view/updated-alphazero-crushes-stockfish-in-new-1-000-game-match),
    best engine at the time, without losing a single game, keeping that edge even
    when Alpha Zero was given [an order of magnitude less time](https://chess24.com/en/read/news/alphazero-really-is-that-good) to
    compute its next move.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaZero击败了当时最强的棋类引擎[Stockfish 8](https://www.notion.so/neurolabs/chess.com/news/view/updated-alphazero-crushes-stockfish-in-new-1-000-game-match)，且没有输掉一场比赛，即使在AlphaZero被给予[少一个数量级的时间](https://chess24.com/en/read/news/alphazero-really-is-that-good)来计算下一步棋时，依然保持了这一优势。
- en: Considering the remarkable improvements that AlphaZero, one has to wonder: ***Can
    we translate its success in chess to Computer Vision?***
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到AlphaZero的显著进展，人们不禁要问：***我们能否将其在国际象棋中的成功转化为计算机视觉？***
- en: '???? The new wave: Data-Centric AI'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ???? 新潮流：数据中心化 AI
- en: 'Within the new paradigm of [Data Centric AI](https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/?sh=6d7fce8574f5),
    the goal is not to create better algorithms, but increase performance by changing
    the data itself. Even if we disregard the hurdle of obtaining and labelling image
    datasets in the first place, questions still remain around the quality of the
    data: are we uniformly covering all possible use cases? is the data covering edge
    cases?'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在新的[数据中心化 AI](https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/?sh=6d7fce8574f5)范式下，目标不是创造更好的算法，而是通过改变数据本身来提高性能。即使我们忽略获取和标记图像数据集的困难，数据质量的问题仍然存在：我们是否均匀覆盖了所有可能的使用案例？数据是否涵盖了边缘案例？
- en: If we are to follow the path of Data-Centric Computer Vision, one must be in
    control of the data sourcing process. The data needs to be balanced and we need
    to have a good understanding of the parameters that are influencing what a Computer
    Vision model learns.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要跟随数据中心化计算机视觉的路径，就必须控制数据来源过程。数据需要平衡，我们需要充分理解影响计算机视觉模型学习的参数。
- en: 'Let’s take a simple example in which we look at controlling 3 of such parameters:
    camera angle, lighting and occlusions. Can you imagine gathering a real dataset
    in which you have to diligently control the values of only these 3 parameters,
    whilst gathering 1000s of relevant images? With real data, the task is Sisyphean.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个简单的例子来看待控制这三个参数：摄像机角度、光照和遮挡。你能想象在收集上千张相关图像的同时，还要细心控制这三个参数的值吗？使用真实数据，这个任务是艰巨的。
- en: ???? How do we manage data today?
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ???? 我们今天如何管理数据？
- en: In the past 5 years, there we have made tremendous progress in optimising the
    data gathering process and the quality of the data labels. Moreover, we have learned
    to make the most of the datasets, by using a variety of *data augmentation* techniques.
    Given an image in our dataset, we apply mathematical functions to it in order
    to create more variety in our data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去5年里，我们在优化数据收集过程和数据标签质量方面取得了巨大的进步。此外，我们学会了通过使用各种*数据增强*技术来充分利用数据集。给定我们数据集中的一张图像，我们应用数学函数来创造更多的数据多样性。
- en: There are now over 400 companies with a total [market value of $1.3T](https://www.grandviewresearch.com/industry-analysis/data-collection-labeling-market) (a
    little over the market value of Facebook, ) catering to the data needs of our
    latest algorithms.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 目前已有超过400家公司，总[市场价值达到1.3万亿美元](https://www.grandviewresearch.com/industry-analysis/data-collection-labeling-market)（稍高于Facebook的市场价值），专注于满足我们最新算法的数据需求。
- en: But does the current path lead to a dead end? Are we reaching the limits of
    the algorithms built on top of human sourced datasets? Like in chess, as long
    as we’re using human sourced data as input for our algorithms, we’re bound by
    design not to significantly surpass our own abilities.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 但当前的路径是否走到死胡同了？我们是否达到了基于人类来源数据集的算法的极限？就像在国际象棋中，只要我们使用人类来源的数据作为算法的输入，我们在设计上就无法显著超越自己的能力。
- en: '*In chess, the post-Deep Learning breakthrough came once we’ve stopped building
    on suboptimal human data and allowed the machines to build their own data in order
    to optimise what they learn. In computer vision we must do the same, allowing
    the machine to generate the data they need to optimise its own learning.*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*在国际象棋中，深度学习突破的出现是当我们停止依赖次优的人类数据，并允许机器生成自己的数据以优化学习时。在计算机视觉领域，我们也必须如此，允许机器生成其优化学习所需的数据。*'
- en: ???? What’s next for Computer Vision?
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ???? 计算机视觉的下一步是什么？
- en: The truly scalable way of creating training data is through **Virtual Reality
    engines**. In terms of fidelity, the output has [become indistinguishable](https://www.youtube.com/watch?v=S3DEM6XDDTk) from
    the real world, giving full scene control to the user. This allows the user to **generate
    smart data**, that is truly useful for the Computer Vision model to learn. **Synthetic
    Data can become the bedrock needed for the new Data-Centric AI framework**.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 创建训练数据的真正可扩展方式是通过**虚拟现实引擎**。就真实感而言，输出已[变得无法区分](https://www.youtube.com/watch?v=S3DEM6XDDTk)现实世界，给予用户完全的场景控制。这使得用户能够**生成智能数据**，这些数据对计算机视觉模型的学习真正有用。**合成数据可以成为新数据中心
    AI 框架所需的基石**。
- en: We have good reasons to believe that the time for wide adoption of visual Synthetic
    Data is now.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有充分的理由相信，视觉合成数据广泛应用的时机已经到来。
- en: Virtual Reality engines have dedicated components for Synthetic Data generation
    ([NVIDIA IsaacSim](https://developer.nvidia.com/isaac-sim), [Unity Perception](https://github.com/Unity-Technologies/com.unity.perception))
    and the resulting data is not only eye-candy, but also [essential for training
    better](https://www.prnewswire.com/news-releases/survey-of-industry-leaders-shows-synthetic-data-is-essential-to-building-more-capable-ai-models-301377167.html) algorithms.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚拟现实引擎拥有专门的组件用于合成数据生成（[NVIDIA IsaacSim](https://developer.nvidia.com/isaac-sim),
    [Unity Perception](https://github.com/Unity-Technologies/com.unity.perception)），所生成的数据不仅仅是视觉上的盛宴，而且对于训练更好的[算法至关重要](https://www.prnewswire.com/news-releases/survey-of-industry-leaders-shows-synthetic-data-is-essential-to-building-more-capable-ai-models-301377167.html)。
- en: 3D assets are quickly becoming a commodity — newest [iPhone comes with LiDAR](https://www.geoweeknews.com/news/apple-debuts-its-own-api-for-reality-capture-and-3d-object-creation-but-where-s-the-lidar) and
    first generation apps for 3D scans are producing great [results](https://www.linkedin.com/posts/albandenoyel_photogrammetry-objectcapture-3dcapture-activity-6833436374952099840-MPTn).
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3D 资产正迅速成为商品——最新的[iPhone 配备了 LiDAR](https://www.geoweeknews.com/news/apple-debuts-its-own-api-for-reality-capture-and-3d-object-creation-but-where-s-the-lidar)，而第一代
    3D 扫描应用正在产生出色的[结果](https://www.linkedin.com/posts/albandenoyel_photogrammetry-objectcapture-3dcapture-activity-6833436374952099840-MPTn)。
- en: '[The Metaverse is coming and is a big deal](https://www.forbes.com/sites/cathyhackl/2020/07/05/the-metaverse-is-coming--its-a-very-big-deal/).
    If a fraction of the [$60B forecasted growth](https://www.marketsandmarkets.com/Market-Reports/augmented-reality-market-82758548.html) comes
    to fruition, we will live in a world where Virtual Reality will become habitual.
    Digital Twins have real applications today: one example from BMW, the [factory
    of the future](https://www.youtube.com/watch?v=6-DaWgg4zF8) and another is Google’s [Supply
    Chain Twin](https://venturebeat-com.cdn.ampproject.org/c/s/venturebeat.com/2021/09/14/google-launches-digital-twin-tool-for-logistics-and-manufacturing/amp/).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[元宇宙即将到来，并且意义重大](https://www.forbes.com/sites/cathyhackl/2020/07/05/the-metaverse-is-coming--its-a-very-big-deal/)。如果预测中的一部分[$60B
    增长](https://www.marketsandmarkets.com/Market-Reports/augmented-reality-market-82758548.html)变成现实，我们将生活在一个虚拟现实成为日常的世界中。数字双胞胎今天已有实际应用：例如
    BMW 的[未来工厂](https://www.youtube.com/watch?v=6-DaWgg4zF8)和 Google 的[供应链双胞胎](https://venturebeat-com.cdn.ampproject.org/c/s/venturebeat.com/2021/09/14/google-launches-digital-twin-tool-for-logistics-and-manufacturing/amp/)。'
- en: 'The innovators of the industry have started using Virtual Reality to improve
    Computer Vision algorithms: Tesla [is using virtual worlds](https://www.youtube.com/watch?v=6hkiTejoyms) to
    generate edge cases and novel views for driving scenarios.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行业内的创新者已经开始使用虚拟现实来改进计算机视觉算法：特斯拉[正在使用虚拟世界](https://www.youtube.com/watch?v=6hkiTejoyms)来生成边缘案例和新颖的驾驶场景视图。
- en: ????️‍????️ Synthetic Computer Vision (SCV)
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ????️‍????️ 合成计算机视觉 (SCV)
- en: Having access to the right tools to build our own data, we can envision a world
    in which Computer Vision algorithms are developed and trained without the tedious
    process of manual data labelling. [Gartner](https://blogs.gartner.com/andrew_white/2021/07/24/by-2024-60-of-the-data-used-for-the-development-of-ai-and-analytics-projects-will-be-synthetically-generated/)
    predicts that Synthetic Data will be more predominant than real data within the
    next 3 years.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有构建自己数据的正确工具，我们可以设想一个计算机视觉算法在没有繁琐的手动数据标记过程下开发和训练的世界。[Gartner](https://blogs.gartner.com/andrew_white/2021/07/24/by-2024-60-of-the-data-used-for-the-development-of-ai-and-analytics-projects-will-be-synthetically-generated/)
    预测，在未来 3 年内，合成数据将比真实数据更为普遍。
- en: '![https://cdn-images-1.medium.com/max/1200/1*MkwOeoFy4iOGgnL4Hg-RLA.png](../Images/6129f79a62e64989a54c925c03652de6.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![https://cdn-images-1.medium.com/max/1200/1*MkwOeoFy4iOGgnL4Hg-RLA.png](../Images/6129f79a62e64989a54c925c03652de6.png)'
- en: Why not go a step further? What about a world in which **humans are not needed
    to label images** for Computer Vision?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不再进一步呢？如果有一个世界，其中**人类不需要为计算机视觉标注图像**，会怎样？
- en: ???? The future is bright
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ???? 未来光明
- en: With Synthetic Computer Vision, we build in Virtual Reality and deploy for the
    Real world. The same way that AlphaZero taught itself what’s important in chess,
    we let the algorithms decide what they need to see in order to optimally learn.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用合成计算机视觉，我们在虚拟现实中构建模型，然后在现实世界中部署。就像AlphaZero自己学习了什么在国际象棋中重要一样，我们让算法决定它们需要看到什么，以便最佳地学习。
- en: In Synthetic Computer Vision (SCV), we train Computer Vision models using Virtual
    Reality engines and deploy the models in the real world.
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在合成计算机视觉（SCV）中，我们使用虚拟现实引擎训练计算机视觉模型，并将这些模型部署到现实世界中。
- en: ????Beyond RGB images
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ????超越RGB图像
- en: Reality is much more than what the [human eye can see](https://youtu.be/RZdfE_7cde0?t=188).
    The algorithms that we’ve built are mostly focused on what a human can understand
    and label. But it does not have to be like that — we can build algorithms for
    sensors that measure beyond human perception. And we can train these algorithms
    programatically in Virtual Reality, without having doubts over their validity.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现实远比[人眼所能看到的](https://youtu.be/RZdfE_7cde0?t=188)要复杂得多。我们构建的算法主要集中在一个人类可以理解和标注的范围内。但情况不必非如此——我们可以为超出人类感知范围的传感器构建算法。我们可以在虚拟现实中以编程方式训练这些算法，而无需对它们的有效性产生怀疑。
- en: ???? Smarter not harder
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ???? 更聪明，而非更辛苦
- en: Instead of building larger models and using more computational power to solve
    our problems, we can be smart about how we source data from which our algorithms
    learn. Algorithms don't need more of the same data to learn, they need a variety
    of everything.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以聪明地选择数据来源来让算法学习，而不是构建更大的模型和使用更多的计算能力来解决问题。算法不需要更多相同的数据来学习，它们需要各种各样的数据。
- en: Deep Mind showed that AlphaZero was only the start of the road as they’ve applied
    the same principles to [Go](https://www.youtube.com/watch?v=WXuK6gekU1Y), [Starcraft](https://www.youtube.com/watch?v=cUTMhmVh1qs)
    and [protein folding](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology).
    Today, we have all the necessary building blocks to build an *AlphaZero for Computer-Vision*,
    a self-learning system that is not limited by **human input** by design. A system
    that is capable of creating and manipulating virtual scenes through which it teaches
    itself how to solve Visual Automation tasks.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Deep Mind展示了AlphaZero只是起点，他们将相同的原则应用于[围棋](https://www.youtube.com/watch?v=WXuK6gekU1Y)、[星际争霸](https://www.youtube.com/watch?v=cUTMhmVh1qs)和[蛋白质折叠](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology)。如今，我们拥有所有必要的构建模块来构建一个*AlphaZero式的计算机视觉*，这是一个在设计上不受**人类输入**限制的自学习系统。一个能够创建和操控虚拟场景，并通过这些场景自我学习如何解决视觉自动化任务的系统。
- en: ???? The pioneers in Synthetic Data generation
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ???? 合成数据生成领域的先驱
- en: The foundation for Synthetic Computer Vision is provided by the **Synthetic
    Data** it is built upon. There are roughly [30 early stage companies](https://drive.google.com/file/d/1xhgmO9U8WZoYFi7tviUDiyLWz-SYvRFe/view)
    operating in the visual Synthetic Data Generation space. Some are focused on a
    specific use case in one vertical, while the majority are operating horizontally
    across multiple verticals.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 合成计算机视觉的基础是建立在**合成数据**上的。大约有[30家初创公司](https://drive.google.com/file/d/1xhgmO9U8WZoYFi7tviUDiyLWz-SYvRFe/view)在视觉合成数据生成领域运营。其中一些专注于特定垂直领域的用例，而大多数则在多个垂直领域中横向操作。
- en: '![Figure](../Images/8b5c9e05b12ba7c47c117849a73279a2.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/8b5c9e05b12ba7c47c117849a73279a2.png)'
- en: Synthetic Data companies grouped by focus (Image by author).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 按重点分类的合成数据公司（图像作者提供）。
- en: It's 2021 and we are only at the beginning of the road. Keep in mind that Synthetic
    Data is only **one part of the puzzle** that awaits to be solved!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 到了2021年，我们仍处于起步阶段。请记住，合成数据只是**待解决难题的一部分**！
- en: ❓Questions for you, Dear Reader
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ❓亲爱的读者，您有以下问题：
- en: It’s easy to imagine that in 10 years your smartphone will have better capabilities
    than you do for generic visual perception but how are we going to get there?
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 很容易想象，在10年内，你的智能手机在通用视觉感知方面将比你更强大，但我们将如何实现这一点呢？
- en: Are (augmented) data labellers here to stay or simply a stepping stone?
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （增强）数据标注员是会长期存在还是仅仅是一个过渡阶段？
- en: Will labelling move from 2D to the 3D world, or can we do without this approach
    altogether?
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标注工作会从2D世界转移到3D世界吗？还是我们可以完全不采用这种方法？
- en: State-of-the-Art results are achieved using Deep Learning algorithms in Computer
    Vision — can Synthetic Computer Vision enable a new wave of improved algorithms
    which were previously unavailable?
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过深度学习算法在计算机视觉领域取得了**最先进**的成果——合成计算机视觉能否推动一波此前未曾出现的改进算法？
- en: '**Bio: [Paul Pop](https://www.linkedin.com/in/paul-pop/)** is Co-founder and
    CEO at Neurolabs. Background in Computer Science and AI from the University of
    Edinburgh and have been working in Computer Vision for the past decade. Led the
    team that build the Computer Vision player tracking system used in most European
    football leagues today, whilst at Hudl.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介： [Paul Pop](https://www.linkedin.com/in/paul-pop/)** 是Neurolabs的联合创始人兼首席执行官。拥有爱丁堡大学计算机科学与人工智能背景，并在过去十年中从事计算机视觉工作。曾在Hudl领导团队开发了如今在大多数欧洲足球联赛中使用的计算机视觉球员跟踪系统。'
- en: '**Related:**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Open Source Datasets for Computer Vision](/2021/08/open-source-datasets-computer-vision.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[计算机视觉的开源数据集](/2021/08/open-source-datasets-computer-vision.html)'
- en: '[An overview of synthetic data types and generation methods](/2021/02/overview-synthetic-data-types-generation-methods.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[合成数据类型和生成方法概述](/2021/02/overview-synthetic-data-types-generation-methods.html)'
- en: '[Teaching AI to Classify Time-series Patterns with Synthetic Data](/2021/10/teaching-ai-classify-time-series-patterns-synthetic-data.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[教AI用合成数据分类时间序列模式](/2021/10/teaching-ai-classify-time-series-patterns-synthetic-data.html)'
- en: More On This Topic
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Data access is severely lacking in most companies, and 71% believe…](https://www.kdnuggets.com/2023/07/mostly-data-access-severely-lacking-synthetic-data-help.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大多数公司数据访问严重不足，71%的人认为……](https://www.kdnuggets.com/2023/07/mostly-data-access-severely-lacking-synthetic-data-help.html)'
- en: '[10 Most Common Data Quality Issues and How to Fix Them](https://www.kdnuggets.com/2022/11/10-common-data-quality-issues-fix.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10种最常见的数据质量问题及解决方法](https://www.kdnuggets.com/2022/11/10-common-data-quality-issues-fix.html)'
- en: '[A Community for Synthetic Data is Here and This is Why We Need It](https://www.kdnuggets.com/2022/04/community-synthetic-data-need.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[合成数据社区已成立，我们为何需要它](https://www.kdnuggets.com/2022/04/community-synthetic-data-need.html)'
- en: '[High-Fidelity Synthetic Data for Data Engineers and Data Scientists Alike](https://www.kdnuggets.com/2022/tonic-high-fidelity-synthetic-data-engineers-scientists-alike.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[高保真合成数据适用于数据工程师和数据科学家](https://www.kdnuggets.com/2022/tonic-high-fidelity-synthetic-data-engineers-scientists-alike.html)'
- en: '[How to Democratize AI/ML and Data Science with AI-generated Synthetic Data](https://www.kdnuggets.com/2022/11/mostly-ai-democratize-aiml-data-science-aigenerated-synthetic-data.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何通过AI生成的合成数据民主化AI/ML和数据科学](https://www.kdnuggets.com/2022/11/mostly-ai-democratize-aiml-data-science-aigenerated-synthetic-data.html)'
- en: '[6 Things You Need To Know About Data Management And Why It Matters…](https://www.kdnuggets.com/2022/05/6-things-need-know-data-management-matters-computer-vision.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于数据管理的6件事及其重要性](https://www.kdnuggets.com/2022/05/6-things-need-know-data-management-matters-computer-vision.html)'
