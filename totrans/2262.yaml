- en: Calculate Computational Efficiency of Deep Learning Models with FLOPs and MACs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算深度学习模型的计算效率：FLOPs和MACs
- en: 原文：[https://www.kdnuggets.com/2023/06/calculate-computational-efficiency-deep-learning-models-flops-macs.html](https://www.kdnuggets.com/2023/06/calculate-computational-efficiency-deep-learning-models-flops-macs.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/06/calculate-computational-efficiency-deep-learning-models-flops-macs.html](https://www.kdnuggets.com/2023/06/calculate-computational-efficiency-deep-learning-models-flops-macs.html)
- en: What are FLOPs and MACs?
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FLOPs和MACs是什么？
- en: '* * *'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的IT需求'
- en: '* * *'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: FLOPs (Floating Point Operations) and MACs (Multiply-Accumulate Operations)
    are metrics that are commonly used to calculate the computational complexity of
    deep learning models. They are a fast and easy way to understand the number of
    arithmetic operations required to perform a given computation. For example, when
    working with different model architectures such as [MobileNet](https://arxiv.org/pdf/1704.04861.pdf)
    or [DenseNet](https://arxiv.org/abs/1608.06993) for edge devices, people use MACs
    or FLOPs to estimate the model performance. Also, the reason we use the word “estimate”
    is that both metrics are approximations instead of the actual capture of the runtime
    performance model. However, they still can provide very useful insights on energy
    consumption or computational requirements, which is quite useful in edge computing.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: FLOPs（浮点运算）和MACs（乘加运算）是常用来计算深度学习模型计算复杂性的指标。它们是一种快速且简单的方法来理解执行给定计算所需的算术操作数量。例如，在处理像[MobileNet](https://arxiv.org/pdf/1704.04861.pdf)或[DenseNet](https://arxiv.org/abs/1608.06993)这样的边缘设备模型架构时，人们使用MACs或FLOPs来估计模型性能。此外，我们使用“估计”这个词的原因是这两个指标是近似值，而不是实际的运行时性能模型。然而，它们仍能提供关于能耗或计算需求的有用见解，这在边缘计算中非常有用。
- en: '![Calculate Computational Efficiency of Deep Learning Models with FLOPs and
    MACs](../Images/4a5ccca9751b620ab4feb4936c0f27c0.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![使用FLOPs和MACs计算深度学习模型的计算效率](../Images/4a5ccca9751b620ab4feb4936c0f27c0.png)'
- en: 'Figure 1: Comparison of different neural nets using FLOPs from “Densely Connected
    Convolutional Networks”'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：使用来自“密集连接卷积网络”的FLOPs比较不同神经网络
- en: FLOPs specifically refer to the number of floating-point operations, which include
    addition, subtraction, multiplication, and division operations on floating-point
    numbers. These operations are prevalent in many mathematical computations involved
    in machine learning, such as matrix multiplications, activations, and gradient
    calculations. FLOPs are often used to measure the computational cost or complexity
    of a model or a specific operation within a model. This is helpful when we need
    to provide an estimation of the total arithmetic operations required, which is
    generally used in the context of measuring computation efficiency.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: FLOPs特指浮点运算次数，包括对浮点数的加法、减法、乘法和除法操作。这些操作在机器学习中的许多数学计算中很常见，例如矩阵乘法、激活函数和梯度计算。FLOPs通常用于衡量模型或模型中某个操作的计算成本或复杂性。这在我们需要提供所需总算术运算估计时非常有用，这通常用于衡量计算效率的背景下。
- en: MACs, on the other hand, only count the number of multiply-accumulate operations,
    which involve multiplying two numbers and adding the result. This operation is
    fundamental to many linear algebra operations, such as matrix multiplications,
    convolutions, and dot products. MACs are often used as a more specific measure
    of computational complexity in models that heavily rely on linear algebra operations,
    such as convolutional neural networks (CNNs).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: MACs则仅计算乘加操作的次数，这涉及将两个数字相乘并加上结果。这一操作是许多线性代数操作的基础，如矩阵乘法、卷积和点积。MACs常被用作计算模型计算复杂度的更具体的度量，尤其是在严重依赖线性代数操作的模型中，例如卷积神经网络（CNNs）。
- en: One thing worth mentioning here is that FLOPs cannot be the single factor that
    people calculate to get a sense of computation efficiency. Many other factors
    are considered necessary when estimating the model efficiency. For example, how
    parallel the system setup is; what architecture model has(e.g. group convolution
    costs in MACs); what computing platform the model uses(e.g. Cudnn has GPU acceleration
    for deep neural networks and standard operations such as forward or normalization
    are highly tuned).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 值得提到的是，FLOPs不能作为人们计算计算效率的唯一因素。估计模型效率时，还需考虑许多其他因素。例如，系统设置的并行程度；模型的架构（例如，组卷积在MACs中的成本）；模型使用的计算平台（例如，Cudnn为深度神经网络提供GPU加速，标准操作如前向传播或归一化被高度优化）。
- en: Are FLOPS and FLOPs the Same?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FLOPS和FLOPs是一样的吗？
- en: FLOPS with all uppercase is the abbreviation of “floating point operations per
    second”, which refers to the computation speed and is generally used as a measurement
    of hardware performance. The "S" in the "FLOPS" stands for "second" and together
    with "P" (as "per"), it is generally used to represent a rate.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 全部大写的FLOPS是“每秒浮点运算”的缩写，指的是计算速度，通常用于衡量硬件性能。“FLOPS”中的“S”代表“秒”，与“P”（表示“每”）一起，通常用于表示速率。
- en: FLOPs(lowercase “s” stands for plural) on the other hand, refers to the floating-point
    operations. It is commonly used to calculate the computation complexity of an
    algorithm or model. However, in the discussion of AI, sometimes FLOPs can have
    both of the above meanings and it will leave it to the reader to identify the
    exact one it stands for. There have also been some [discussions](https://www.lesswrong.com/posts/XiKidK9kNvJHX9Yte/avoid-the-abbreviation-flops-use-flop-or-flop-s-instead)
    calling for people to abandon the usage of “FLOPs” completely and use “FLOP” instead
    so it’s easier to tell each one apart. In this article, we will continue to use
    FLOPs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: FLOPs（小写“s”表示复数形式）则指的是浮点运算。它通常用于计算算法或模型的计算复杂度。然而，在AI讨论中，FLOPs有时可能具有上述两种含义，具体指代哪一种由读者自行判断。也有一些[讨论](https://www.lesswrong.com/posts/XiKidK9kNvJHX9Yte/avoid-the-abbreviation-flops-use-flop-or-flop-s-instead)呼吁完全放弃“FLOPs”的使用，而改用“FLOP”，以便更容易区分。在本文中，我们将继续使用FLOPs。
- en: Relationship between FLOPs and MACs
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FLOPs与MACs之间的关系
- en: '![Relationship between FLOPs and MACs](../Images/8e823f1de50f773fbacece305385bc29.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![FLOPs与MACs之间的关系](../Images/8e823f1de50f773fbacece305385bc29.png)'
- en: 'Figure 2: Relationship between GMACs and GLOPs([source](https://github.com/sovrasov/flops-counter.pytorch/issues/16#issuecomment-518585837))'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：GMACs与GLOPs之间的关系（[来源](https://github.com/sovrasov/flops-counter.pytorch/issues/16#issuecomment-518585837)）
- en: As mentioned in the previous section, the major difference between FLOPs and
    MACs include what types of arithmetic operations they count and the context in
    which they are used. The general AI community consensus, like the GitHub comment
    in Figure 2, is one MACs equals roughly two FLOPs. For deep neural networks, multiply-accumulate
    operations are heavy in calculations therefore MACs is considered more significant.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，FLOPs和MACs之间的主要区别包括它们计算的算术操作类型和使用的背景。像图2中的GitHub评论一样，AI社区的普遍共识是，一MACs大致等于两个FLOPs。对于深度神经网络，乘加操作的计算负担较重，因此MACs被认为更为重要。
- en: How to Calculate FLOPs?
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何计算FLOPs？
- en: The good thing is there are multiple open-source packages available already
    for calculating FLOPs specifically so you don’t have to implement it from scratch.
    Some of the most popular ones include [flops-counter.pytorch](https://github.com/sovrasov/flops-counter.pytorch)
    and [pytorch-OpCounter](https://github.com/Lyken17/pytorch-OpCounter). There are
    also packages such as [torchstat](https://github.com/Swall0w/torchstat) that gives
    users a general network analyzer based on PyTorch. It is also worth noting that
    for these packages the supported layers and models are limited. So if you are
    running a model that consists of customized network layers you might have to calculate
    FLOPs yourself.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，目前已经有多个开源包可以专门用于计算FLOPs，因此你无需从零开始实现。最受欢迎的包括 [flops-counter.pytorch](https://github.com/sovrasov/flops-counter.pytorch)
    和 [pytorch-OpCounter](https://github.com/Lyken17/pytorch-OpCounter)。还有一些如 [torchstat](https://github.com/Swall0w/torchstat)
    的包，为用户提供基于PyTorch的一般网络分析器。值得注意的是，这些包支持的层和模型是有限的。因此，如果你使用的模型包含自定义网络层，可能需要自己计算FLOPs。
- en: 'Here we show a code example to calculate FLOPs using pytorch-OpCounter and
    a pre-trained alexnet from torchvision:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了一个使用pytorch-OpCounter和torchvision中的预训练alexnet计算FLOPs的代码示例：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Conclusion
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we introduced the definition of FLOPs and MACs, when they are
    usually used and the difference between the two attributes.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了FLOPs和MACs的定义，它们通常在什么情况下使用以及这两个属性之间的区别。
- en: References
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[https://github.com/sovrasov/flops-counter.pytorch](https://github.com/sovrasov/flops-counter.pytorch)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/sovrasov/flops-counter.pytorch](https://github.com/sovrasov/flops-counter.pytorch)'
- en: '[https://github.com/Lyken17/pytorch-OpCounter](https://github.com/Lyken17/pytorch-OpCounter)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/Lyken17/pytorch-OpCounter](https://github.com/Lyken17/pytorch-OpCounter)'
- en: '[https://github.com/Swall0w/torchstat](https://github.com/Swall0w/torchstat)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/Swall0w/torchstat](https://github.com/Swall0w/torchstat)'
- en: '[https://arxiv.org/pdf/1704.04861.pdf](https://arxiv.org/pdf/1704.04861.pdf)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1704.04861.pdf](https://arxiv.org/pdf/1704.04861.pdf)'
- en: '[https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993)'
- en: '**[Danni Li](https://www.linkedin.com/in/danni-li-cs/)** is the current AI
    Resident at Meta. She''s interested in building efficient AI systems and her current
    research focus is on-device ML models. She is also a strong believer of open source
    collaboration and utilizing community support to maximize our potential for innovation.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Danni Li](https://www.linkedin.com/in/danni-li-cs/)** 是Meta的现任AI研究员。她对构建高效的AI系统感兴趣，目前的研究重点是设备上的机器学习模型。她也是开源协作和利用社区支持以最大化创新潜力的坚定信徒。'
- en: More On This Topic
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解更多主题
- en: '[How To Calculate Algorithm Efficiency](https://www.kdnuggets.com/2022/09/calculate-algorithm-efficiency.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何计算算法效率](https://www.kdnuggets.com/2022/09/calculate-algorithm-efficiency.html)'
- en: '[Efficiency Spells the Difference Between Biological Neurons and…](https://www.kdnuggets.com/2022/11/efficiency-spells-difference-biological-neurons-artificial-counterparts.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[效率决定生物神经元与…之间的差异](https://www.kdnuggets.com/2022/11/efficiency-spells-difference-biological-neurons-artificial-counterparts.html)'
- en: '[3 Research-Driven Advanced Prompting Techniques for LLM Efficiency…](https://www.kdnuggets.com/3-research-driven-advanced-prompting-techniques-for-llm-efficiency-and-speed-optimization)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3种基于研究的高级提示技术提升LLM效率…](https://www.kdnuggets.com/3-research-driven-advanced-prompting-techniques-for-llm-efficiency-and-speed-optimization)'
- en: '[5 Python Tips for Data Efficiency and Speed](https://www.kdnuggets.com/5-python-tips-for-data-efficiency-and-speed)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5个提高数据效率和速度的Python技巧](https://www.kdnuggets.com/5-python-tips-for-data-efficiency-and-speed)'
- en: '[Elevate Math Efficiency: Navigating Numpy Array Operations](https://www.kdnuggets.com/elevate-math-efficiency-navigating-numpy-array-operations)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提升数学效率：驾驭Numpy数组操作](https://www.kdnuggets.com/elevate-math-efficiency-navigating-numpy-array-operations)'
- en: '[Maximizing Efficiency in Data Analysis with ChatGPT](https://www.kdnuggets.com/maximizing-efficiency-in-data-analysis-with-chatgpt)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用ChatGPT最大化数据分析效率](https://www.kdnuggets.com/maximizing-efficiency-in-data-analysis-with-chatgpt)'
