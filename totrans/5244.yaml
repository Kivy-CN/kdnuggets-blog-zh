- en: Scalable Select of Random Rows in SQL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2018/04/scalable-select-random-rows-sql.html](https://www.kdnuggets.com/2018/04/scalable-select-random-rows-sql.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: '**By Pavel Tiunov, [Statsbot](https://statsbot.co/)**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '![Header image](../Images/65a6d842e8e568f47b9eef4a27cf66c3.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: If you’re new to the big data world and also migrating from tools like Google
    Analytics or Mixpanel for your web analytics, you probably noticed performance
    differences. Google Analytics can show you predefined reports in seconds, while
    the same query for the same data in your data warehouse can take several minutes
    or even more. Such performance boosts are achieved by selecting random rows or
    the [sampling technique](https://en.wikipedia.org/wiki/Sampling_(statistics)).
    Let’s learn how to select random rows in SQL.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Sample and Population
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we start to work on sampling implementation, it is worth mentioning some
    sampling fundamentals. Sampling is based on a subset selection of individuals
    from some population to describe this population’s properties. So if you have
    some event data, you can select a subset of unique users and their events to calculate
    metrics that describe all users’ behavior. On the other hand, if you select a
    subset of events, it won’t describe the behavior of the population precisely.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Given this fact, we assume all data sets to which sampling is applied have unique
    user identifiers. Moreover, if you have both anonymous user identifiers and authorized
    user identifiers, we assume using the last one as it delivers more accurate results.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Selecting Random Rows in SQL
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There’re [plenty of different methods](https://en.wikipedia.org/wiki/Sampling_(statistics)#Sampling_methods) you
    can use to select a subset of users. We’ll consider only two of those as the most
    obvious and simple to implement: simple random sampling and systematic sampling.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Simple random sampling can be implemented as giving a unique number to each
    user in a range from 0 to N-1 and then selecting X random numbers from 0 to N-1\.
    N denotes the total number of users here and X is the sample size. Although this
    method is very simple for understanding, it’s a little bit tricky to implement
    in an SQL environment, mostly because random number generator outputs don’t scale
    very well when sample sizes are equal to billions. Also it isn’t very clear as
    to how to get evenly distributed samples over time.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Bearing this in mind, we’ll use systematic sampling which can overcome these
    obstacles from an SQL implementation perspective. Simple systematic sampling may
    be implemented as selecting one user from each M users at a specified interval.
    In this case, sample size will be equal to N / M. Selecting one user out of M
    while preserving uniform distribution across sample buckets is the main challenge
    of this approach. Let’s see how this can be implemented in SQL.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Sequence Generated User Identifiers
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You’re lucky if your user IDs are integers generated as a strict sequence without
    gaps like those generated by `AUTO_INCREMENT` primary key fields. In this case,
    you can implement systematic sampling as simple as:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This SQL query and all SQL queries below are in Standard BigQuery SQL. In this
    example, we’re selecting one user out of 10, which is a 10% sample. 7 is the random
    number of the sampling bucket and it can be any number from 0 to 9\. We use MOD
    operation to create sampling buckets which stand for the remainder of a division
    by 10 in this particular case. It’s really simple to show that if `user_id`is
    a strict integer sequence, then user counts are uniformly distributed across all
    sampling buckets when user count is high enough.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'To estimate the event count, for example, you can write something like:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Please note the multiplication by 10 in this query. Given the fact that we use
    a 10% sample, all estimated additive measures should be scaled by 1/10% in order
    to match real values.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'One can question how precise this sampling approach is for a specific dataset.
    You can estimate it by checking how uniform the distribution is within sampling
    buckets. To do that you can query something like:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s consider some example results of this query:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c62f4fe96b983935965418997009139e.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: 'We can calculate the mean and confidence interval for an alpha coefficient
    equal to 0.01 for these sampling bucket sizes. The [confidence interval](https://en.wikipedia.org/wiki/Confidence_interval#Basic_steps) will
    be equal to 0.01% of the sampling bucket average size. This means that with 99%
    probability the sampling bucket sizes differ not more than 0.01%. Different metrics
    calculated with these sampling buckets couple with this statistic but don’t inherit
    it. So, in order to calculate precision for event count estimation, you can calculate
    the event count for each sample as:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Then calculate absolute and relative confidence intervals for these event counts
    as in the case of user count to get event count estimate precision.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然后计算这些事件计数的绝对和相对置信区间，如用户计数的情况，以获得事件计数估计的精度。
- en: String Identifiers and Other User Identifiers
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 字符串标识符和其他用户标识符
- en: In the case that you have string user identifiers or integers, but not a strict
    sequence instead of integer sequence identifiers, you need a way to distribute
    all user IDs between different sampling buckets in a uniform manner. [This](https://en.wikipedia.org/wiki/Hash_function#Uniformity) can
    be done by hash functions. Not all hash functions can get you uniform distribution
    under different circumstances. You can check [smhasher](https://github.com/rurban/smhasher/tree/master/doc) test
    suite results to check how good a particular hash function is at this.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有字符串用户标识符或整数，但不是严格的整数序列标识符，您需要一种均匀地将所有用户 ID 分配到不同采样桶中的方法。 [这](https://en.wikipedia.org/wiki/Hash_function#Uniformity) 可以通过哈希函数完成。并非所有哈希函数在不同情况下都能实现均匀分布。您可以检查
    [smhasher](https://github.com/rurban/smhasher/tree/master/doc) 测试套件的结果，以检查特定哈希函数的效果。
- en: 'For example, in BigQuery you can use the `FARM_FINGERPRINT` hash function to
    prepare a sample for select:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在 BigQuery 中，您可以使用 `FARM_FINGERPRINT` 哈希函数来准备选择样本：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`FARM_FINGERPRINT` can be replaced with any suitable hash function, such as
    `xxhash64` in Presto or even the combination of `md5` and `strtol` in Redshift.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`FARM_FINGERPRINT` 可以替换为任何合适的哈希函数，例如 Presto 中的 `xxhash64`，或者甚至是 Redshift 中的
    `md5` 和 `strtol` 的组合。'
- en: 'As in the case of sequence-generated user identifiers you can check uniformity
    statistics as:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如同序列生成的用户标识符一样，您可以检查均匀性统计数据：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Common Pitfalls
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 常见陷阱
- en: Most often, sampling can reduce your SQL query execution time by more than 5-10
    times without being harmful in terms of precision, but there are still cases where
    you should be cautious. This mostly belongs to a sampling bias problem because
    of sample size. It can occur when dispersion of metrics you’re interested in between
    samples is too high even if the sample size is big enough.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，采样可以将您的 SQL 查询执行时间减少 5-10 倍而不会对精度造成危害，但仍然存在需要谨慎的情况。这主要是由于样本大小引起的采样偏差问题。当您感兴趣的指标在样本之间的离散度过高时，即使样本量足够大，也可能发生这种情况。
- en: For example, you can be interested in some rare event count such as an enterprise
    demo request being some B2C site with a huge amount of traffic. If the enterprise
    demo request count is about 100 events per month and your monthly active users
    is about 1M, then sampling can lead you in a situation where the enterprise demo
    request count estimate with a 10% sample produces significant errors. Generally
    speaking, sampling random rows in SQL should be avoided in this case or [more
    sophisticated methods](https://en.wikipedia.org/wiki/Sampling_(statistics)#Stratified_sampling) should
    be used instead.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可能对某些稀有事件计数感兴趣，比如一个 B2C 网站上的企业演示请求，尽管该网站有大量流量。如果企业演示请求的数量约为每月 100 个事件，而您的月活跃用户约为
    1M，那么采样可能会导致企业演示请求数量的估计误差显著。一般而言，在这种情况下应避免在 SQL 中随机采样行，或使用 [更复杂的方法](https://en.wikipedia.org/wiki/Sampling_(statistics)#Stratified_sampling)。
- en: As always don’t hesitate to leave your comments or [contact our team](https://statsbot.co/form-trial?utm_source=statsbotblog&utm_medium=article&utm_campaign=random_rows) to
    get help.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，请随时留下您的评论或 [联系我们的团队](https://statsbot.co/form-trial?utm_source=statsbotblog&utm_medium=article&utm_campaign=random_rows) 以获取帮助。
- en: '[Original](https://statsbot.co/blog/select-random-rows-sql?utm_source=kdnuggets&utm_medium=post&utm_campaign=random-select).
    Reposted with permission.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://statsbot.co/blog/select-random-rows-sql?utm_source=kdnuggets&utm_medium=post&utm_campaign=random-select)。经许可转载。'
- en: '**Related:**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Calculating Customer Lifetime Value: SQL Example](/2018/02/calculating-customer-lifetime-value-sql-example.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[计算客户终生价值：SQL 示例](/2018/02/calculating-customer-lifetime-value-sql-example.html)'
- en: '[SQL Window Functions Tutorial for Business Analysis](/2017/12/sql-window-functions-tutorial-business-analytics.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[业务分析的 SQL 窗口函数教程](/2017/12/sql-window-functions-tutorial-business-analytics.html)'
- en: '[A Guide for Customer Retention Analysis with SQL](/2017/12/guide-customer-retention-analysis-sql.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQL 客户保留分析指南](/2017/12/guide-customer-retention-analysis-sql.html)'
- en: More On This Topic
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[How to Select Rows and Columns in Pandas Using [ ], .loc, iloc, .at…](https://www.kdnuggets.com/2019/06/select-rows-columns-pandas.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 [ ], .loc, iloc, .at… 在 Pandas 中选择行和列](https://www.kdnuggets.com/2019/06/select-rows-columns-pandas.html)'
- en: '[Building a Scalable ETL with SQL + Python](https://www.kdnuggets.com/2022/04/building-scalable-etl-sql-python.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 SQL + Python 构建可扩展的 ETL](https://www.kdnuggets.com/2022/04/building-scalable-etl-sql-python.html)'
- en: '[Generalized and Scalable Optimal Sparse Decision Trees(GOSDT)](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[广义且可扩展的最优稀疏决策树 (GOSDT)](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
- en: '[How to Build a Scalable Data Architecture with Apache Kafka](https://www.kdnuggets.com/2023/04/build-scalable-data-architecture-apache-kafka.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 Apache Kafka 构建可扩展的数据架构](https://www.kdnuggets.com/2023/04/build-scalable-data-architecture-apache-kafka.html)'
- en: '[How to Process a DataFrame with Millions of Rows in Seconds](https://www.kdnuggets.com/2022/01/process-dataframe-millions-rows-seconds.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在秒级处理包含百万行的 DataFrame](https://www.kdnuggets.com/2022/01/process-dataframe-millions-rows-seconds.html)'
- en: '[3 Ways to Append Rows to Pandas DataFrames](https://www.kdnuggets.com/2022/08/3-ways-append-rows-pandas-dataframes.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[将行附加到 Pandas DataFrames 的 3 种方法](https://www.kdnuggets.com/2022/08/3-ways-append-rows-pandas-dataframes.html)'
