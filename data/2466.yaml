- en: Decision Tree Algorithm, Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树算法解析
- en: 原文：[https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)
- en: '![](../Images/3a3912c533fa3be38427762c11a17e10.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a3912c533fa3be38427762c11a17e10.png)'
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三名课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的IT需求'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Classification is a two-step process, learning step and prediction step, in
    machine learning. In the learning step, the model is developed based on given
    training data. In the prediction step, the model is used to predict the response
    for given data. Decision Tree is one of the easiest and popular classification
    algorithms to understand and interpret.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 分类是机器学习中的一个两步过程，包括学习步骤和预测步骤。在学习步骤中，模型根据给定的训练数据进行开发。在预测步骤中，模型用于预测给定数据的响应。决策树是最容易理解和解释的分类算法之一。
- en: Decision Tree Algorithm
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树算法
- en: Decision Tree algorithm belongs to the family of supervised learning algorithms.
    Unlike other supervised learning algorithms, the decision tree algorithm can be
    used for solving **regression and classification problems** too.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法属于监督学习算法的范畴。与其他监督学习算法不同，决策树算法也可以用来解决**回归和分类问题**。
- en: The goal of using a Decision Tree is to create a training model that can use
    to predict the class or value of the target variable by **learning simple decision
    rules** inferred from prior data(training data).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用决策树的目标是创建一个训练模型，通过**学习从先前数据（训练数据）推断出的简单决策规则**来预测目标变量的类别或值。
- en: In Decision Trees, for predicting a class label for a record we start from the **root** of
    the tree. We compare the values of the root attribute with the record’s attribute.
    On the basis of comparison, we follow the branch corresponding to that value and
    jump to the next node.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树中，为了预测记录的类别标签，我们从**根**开始。我们比较根属性的值与记录的属性。根据比较结果，我们跟随与该值对应的分支，并跳到下一个节点。
- en: Types of Decision Trees
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树的类型
- en: 'Types of decision trees are based on the type of target variable we have. It
    can be of two types:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的类型基于目标变量的类型。可以分为两种类型：
- en: '**Categorical Variable Decision Tree: **Decision Tree which has a categorical
    target variable then it called a **Categorical variable decision tree.**'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分类变量决策树：** 如果决策树具有一个分类目标变量，那么它被称为**分类变量决策树**。'
- en: '**Continuous Variable Decision Tree: **Decision Tree has a continuous target
    variable then it is called **Continuous Variable Decision Tree.**'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**连续变量决策树：** 如果决策树具有一个连续的目标变量，那么它被称为**连续变量决策树**。'
- en: '**Example:-** Let’s say we have a problem to predict whether a customer will
    pay his renewal premium with an insurance company (yes/ no). Here we know that
    the income of customers is a significant variable but the insurance company does
    not have income details for all customers. Now, as we know this is an important
    variable, then we can build a decision tree to predict customer income based on
    occupation, product, and various other variables. In this case, we are predicting
    values for the continuous variables.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** 假设我们有一个问题，要预测客户是否会向保险公司支付续保费用（是/否）。在这里，我们知道客户的收入是一个重要变量，但保险公司没有所有客户的收入细节。既然这是一个重要变量，我们可以建立一个决策树，根据职业、产品和各种其他变量来预测客户的收入。在这种情况下，我们预测的是连续变量的值。'
- en: Important Terminology related to Decision Trees
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与决策树相关的重要术语
- en: '**Root Node: **It represents the entire population or sample and this further
    gets divided into two or more homogeneous sets.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**根节点：** 它代表整个群体或样本，随后被分为两个或更多同质的集合。'
- en: '**Splitting: **It is a process of dividing a node into two or more sub-nodes.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分割：** 这是将节点分成两个或更多子节点的过程。'
- en: '**Decision Node: **When a sub-node splits into further sub-nodes, then it is
    called the decision node.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**决策节点：** 当一个子节点进一步分裂成子节点时，它被称为决策节点。'
- en: '**Leaf / Terminal Node: **Nodes do not split is called Leaf or Terminal node.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**叶节点/终端节点：** 不再分割的节点称为叶节点或终端节点。'
- en: '**Pruning: **When we remove sub-nodes of a decision node, this process is called
    pruning. You can say the opposite process of splitting.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**剪枝：** 当我们移除决策节点的子节点时，这个过程称为剪枝。你可以认为这是分割的反过程。'
- en: '**Branch / Sub-Tree: **A subsection of the entire tree is called branch or
    sub-tree.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分支/子树：** 整棵树的一个子部分称为分支或子树。'
- en: '**Parent and Child Node: **A node, which is divided into sub-nodes is called
    a parent node of sub-nodes whereas sub-nodes are the child of a parent node.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**父节点和子节点：** 一个被分割成子节点的节点称为子节点的父节点，而子节点是父节点的子节点。'
- en: '![](../Images/b8fac40aba9c2297b93f56d82a72a308.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8fac40aba9c2297b93f56d82a72a308.png)'
- en: Decision trees classify the examples by sorting them down the tree from the
    root to some leaf/terminal node, with the leaf/terminal node providing the classification
    of the example.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树通过从根节点将示例向下排序到某个叶节点/终端节点来进行分类，叶节点/终端节点提供了示例的分类。
- en: Each node in the tree acts as a test case for some attribute, and each edge
    descending from the node corresponds to the possible answers to the test case.
    This process is recursive in nature and is repeated for every subtree rooted at
    the new node.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 树中的每个节点都充当某些属性的测试用例，节点下的每条边对应于测试用例的可能答案。这个过程是递归的，并且对每个以新节点为根的子树重复。
- en: Assumptions while creating Decision Tree
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建决策树时的假设
- en: 'Below are some of the assumptions we make while using Decision tree:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们使用决策树时的一些假设：
- en: In the beginning, the whole training set is considered as the **root.**
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在开始时，整个训练集被视为**根节点**。
- en: Feature values are preferred to be categorical. If the values are continuous
    then they are discretized prior to building the model.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征值最好是分类的。如果值是连续的，则在构建模型之前进行离散化。
- en: Records are **distributed recursively** on the basis of attribute values.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录**递归分布**在属性值的基础上。
- en: Order to placing attributes as root or internal node of the tree is done by
    using some statistical approach.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 属性作为树的根节点或内部节点的排序是通过使用一些统计方法完成的。
- en: Decision Trees follow **Sum of Product (SOP) r**epresentation. The Sum of product
    (SOP) is also known as **Disjunctive Normal Form**. For a class, every branch
    from the root of the tree to a leaf node having the same class is conjunction
    (product) of values, different branches ending in that class form a disjunction
    (sum).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树遵循**乘积和（SOP）**表示法。乘积和（SOP）也称为**析取范式**。对于一个类别，从树的根到具有相同类别的叶节点的每条分支都是值的合取（乘积），不同的分支以该类别结束，形成析取（和）。
- en: The primary challenge in the decision tree implementation is to identify which
    attributes do we need to consider as the root node and each level. Handling this
    is to know as the attributes selection. We have different attributes selection
    measures to identify the attribute which can be considered as the root note at
    each level.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树实现中的主要挑战是确定哪些属性需要作为根节点和每一层。处理这个问题的过程称为属性选择。我们有不同的属性选择度量来识别可以在每一层作为根节点的属性。
- en: How do Decision Trees work?
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树是如何工作的？
- en: The decision of making strategic splits heavily affects a tree’s accuracy. The
    decision criteria are different for classification and regression trees.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 制定战略性分割的决策会对树的准确性产生重大影响。分类树和回归树的决策标准不同。
- en: Decision trees use multiple algorithms to decide to split a node into two or
    more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant
    sub-nodes. In other words, we can say that the purity of the node increases with
    respect to the target variable. The decision tree splits the nodes on all available
    variables and then selects the split which results in most homogeneous sub-nodes.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树使用多种算法来决定是否将节点分割成两个或更多子节点。子节点的创建增加了结果子节点的同质性。换句话说，我们可以说节点的纯度随着目标变量的增加而增加。决策树在所有可用变量上分割节点，然后选择导致最同质子节点的分割。
- en: 'The algorithm selection is also based on the type of target variables. Let
    us look at some algorithms used in Decision Trees:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 算法选择也基于目标变量的类型。让我们看一下决策树中使用的一些算法：
- en: '**ID3** → (extension of D3)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**ID3** →（D3 的扩展）'
- en: '**C4.5** → (successor of ID3)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**C4.5** →（ID3 的继任者）'
- en: '**CART** → (Classification And Regression Tree)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**CART** →（分类和回归树）'
- en: '**CHAID** → (Chi-square automatic interaction detection Performs multi-level
    splits when computing classification trees)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAID** →（卡方自动交互检测，在计算分类树时执行多级拆分）'
- en: '**MARS** → (multivariate adaptive regression splines)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**MARS** →（多变量自适应回归样条）'
- en: The ID3 algorithm builds decision trees using a top-down [greedy search ](https://www.hackerearth.com/practice/algorithms/greedy/basics-of-greedy-algorithms/tutorial/)approach
    through the space of possible branches with no backtracking. A greedy algorithm,
    as the name suggests, always makes the choice that seems to be the best at that
    moment.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ID3 算法通过自顶向下的[贪婪搜索](https://www.hackerearth.com/practice/algorithms/greedy/basics-of-greedy-algorithms/tutorial/)方法构建决策树，不进行回溯。正如其名，贪婪算法总是做出在当下看起来最好的选择。
- en: '**Steps in ID3 algorithm:**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**ID3 算法中的步骤：**'
- en: It begins with the original set S as the root node.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它从原始集合 S 作为根节点开始。
- en: On each iteration of the algorithm, it iterates through the very unused attribute
    of the set S and calculates **Entropy(H)** and **Information gain(IG) **of this
    attribute.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在算法的每次迭代中，它遍历集合 S 中未使用的属性，并计算该属性的**熵（H）**和**信息增益（IG）**。
- en: It then selects the attribute which has the smallest Entropy or Largest Information
    gain.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后选择具有最小熵或最大信息增益的属性。
- en: The set S is then split by the selected attribute to produce a subset of the
    data.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后通过所选属性将集合 S 划分为数据子集。
- en: The algorithm continues to recur on each subset, considering only attributes
    never selected before.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法继续在每个子集上递归，仅考虑从未选择过的属性。
- en: Attribute Selection Measures
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 属性选择度量
- en: If the dataset consists of **N** attributes then deciding which attribute to
    place at the root or at different levels of the tree as internal nodes is a complicated
    step. By just randomly selecting any node to be the root can’t solve the issue.
    If we follow a random approach, it may give us bad results with low accuracy.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集包含**N**个属性，则决定将哪个属性放在根节点或树的不同层级作为内部节点是一个复杂的步骤。仅仅随机选择一个节点作为根节点无法解决问题。如果我们遵循随机方法，可能会得到低准确率的不良结果。
- en: 'For solving this attribute selection problem, researchers worked and devised
    some solutions. They suggested using some *criteria* like :'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个属性选择问题，研究人员工作并提出了一些解决方案。他们建议使用一些*标准*，如：
- en: '**Entropy**,'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**熵**，'
- en: '**Information gain,**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**信息增益**，'
- en: '**Gini index,**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**基尼指数**，'
- en: '**Gain Ratio,**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**增益比**，'
- en: '**Reduction in Variance**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**方差减少**'
- en: '**Chi-Square**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**卡方**'
- en: These criteria will calculate values for every attribute. The values are sorted,
    and attributes are placed in the tree by following the order i.e, the attribute
    with a high value(in case of information gain) is placed at the root.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标准将为每个属性计算值。这些值会被排序，属性按照顺序放置在树中，即，具有高值（在信息增益情况下）的属性放置在根节点。
- en: While using Information Gain as a criterion, we assume attributes to be categorical,
    and for the Gini index, attributes are assumed to be continuous.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用信息增益作为标准时，我们假设属性是分类的，而对于基尼指数，则假设属性是连续的。
- en: '**Entropy**'
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**熵**'
- en: Entropy is a measure of the randomness in the information being processed. The
    higher the entropy, the harder it is to draw any conclusions from that information.
    Flipping a coin is an example of an action that provides information that is random.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 熵是处理信息中随机性的度量。熵越高，从信息中得出结论就越困难。掷硬币就是提供随机信息的一个例子。
- en: '![](../Images/e47ddfca8b8db435c48ef441aa56a83f.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e47ddfca8b8db435c48ef441aa56a83f.png)'
- en: From the above graph, it is quite evident that the entropy H(X) is zero when
    the probability is either 0 or 1\. The Entropy is maximum when the probability
    is 0.5 because it projects perfect randomness in the data and there is no chance
    if perfectly determining the outcome.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的图表中可以明显看出，当概率为 0 或 1 时，熵 H(X) 为零。当概率为 0.5 时，熵最大，因为它在数据中投射了完美的随机性，并且无法准确确定结果。
- en: '**ID3 follows the rule — A branch with an entropy of zero is a leaf node and
    A brach with entropy more than zero needs further splitting.**'
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ID3 遵循规则 — 熵为零的分支是叶节点，熵大于零的分支需要进一步拆分。**'
- en: 'Mathematically Entropy for 1 attribute is represented as:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，1个属性的熵表示为：
- en: '![](../Images/57204386d55659ed9b46775794a49db7.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57204386d55659ed9b46775794a49db7.png)'
- en: Where **S → Current state, and Pi → Probability of an event *i *of state S or
    Percentage of class *i* in a node of state S.**
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 **S → 当前状态，Pi → 状态 S 或类 *i* 的事件 *i* 的概率，或状态 S 节点中的类 *i* 的百分比。**
- en: 'Mathematically Entropy for multiple attributes is represented as:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，多属性的熵表示为：
- en: '![](../Images/a1944b5e9919dde88ddd097cdd6b44d6.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1944b5e9919dde88ddd097cdd6b44d6.png)'
- en: where** T→ Current state and X → Selected attribute**
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 **T → 当前状态和 X → 选择的属性**
- en: '**Information Gain**'
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**信息增益**'
- en: Information gain or **IG **is a statistical property that measures how well
    a given attribute separates the training examples according to their target classification.
    Constructing a decision tree is all about finding an attribute that returns the
    highest information gain and the smallest entropy.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益或 **IG** 是一种统计属性，用于测量给定属性根据其目标分类分离训练样本的效果。构建决策树的核心是找到一个能返回最高信息增益和最小熵的属性。
- en: '![Figure](../Images/d88b86d888a94376d44650a42cbea64e.png)[Information Gain](https://becominghuman.ai/decision-trees-in-machine-learning-f362b296594a?gi=a8ffb5170258)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/d88b86d888a94376d44650a42cbea64e.png)[信息增益](https://becominghuman.ai/decision-trees-in-machine-learning-f362b296594a?gi=a8ffb5170258)'
- en: Information gain is a decrease in entropy. It computes the difference between
    entropy before split and average entropy after split of the dataset based on given
    attribute values. ID3 (Iterative Dichotomiser) decision tree algorithm uses information
    gain.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益是熵的减少。它计算了基于给定属性值的分裂前后的熵差。ID3（迭代二分器）决策树算法使用信息增益。
- en: 'Mathematically, IG is represented as:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，IG 表示为：
- en: '![](../Images/4c38c19ad3a40772df5891ecee13348e.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c38c19ad3a40772df5891ecee13348e.png)'
- en: 'In a much simpler way, we can conclude that:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 更简单地说，我们可以得出结论：
- en: '![](../Images/4be9c5fa0c7275f79eee4c7096462fb3.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4be9c5fa0c7275f79eee4c7096462fb3.png)'
- en: '[Information Gain](https://towardsdatascience.com/from-a-single-decision-tree-to-a-random-forest-b9523be65147)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[信息增益](https://towardsdatascience.com/from-a-single-decision-tree-to-a-random-forest-b9523be65147)'
- en: Where “before” is the dataset before the split, K is the number of subsets generated
    by the split, and (j, after) is subset j after the split.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其中“before”是分裂前的数据集，K是分裂生成的子集数量，(j, after)是分裂后的子集 j。
- en: Gini Index
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基尼指数
- en: You can understand the Gini index as a cost function used to evaluate splits
    in the dataset. It is calculated by subtracting the sum of the squared probabilities
    of each class from one. It favors larger partitions and easy to implement whereas
    information gain favors smaller partitions with distinct values.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将基尼指数理解为用来评估数据集分裂的成本函数。它通过从1中减去每个类别的平方概率总和来计算。它偏向于较大的分区且易于实现，而信息增益则偏向于较小的具有不同值的分区。
- en: '![Figure](../Images/6669e4c680091234497fb509d5101542.png)Gini Index'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '![图示](../Images/6669e4c680091234497fb509d5101542.png)基尼指数'
- en: Gini Index works with the categorical target variable “Success” or “Failure”.
    It performs only Binary splits.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼指数适用于类别目标变量“成功”或“失败”。它仅执行二分裂。
- en: Higher value of Gini index implies higher inequality, higher heterogeneity.
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 较高的基尼指数意味着更高的不平等性，更高的异质性。
- en: '**Steps to Calculate Gini index for a split**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算分裂基尼指数的步骤**'
- en: Calculate Gini for sub-nodes, using the above formula for success(p) and failure(q)
    (p²+q²).
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算子节点的基尼，使用上述公式计算成功（p）和失败（q）的值（p²+q²）。
- en: Calculate the Gini index for split using the weighted Gini score of each node
    of that split.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算分裂的基尼指数，使用该分裂每个节点的加权基尼得分。
- en: CART (Classification and Regression Tree) uses the Gini index method to create
    split points.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: CART（分类与回归树）使用基尼指数方法来创建分裂点。
- en: Gain ratio
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 增益比
- en: Information gain is biased towards choosing attributes with a large number of
    values as root nodes. It means it prefers the attribute with a large number of
    distinct values.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益偏向于选择具有大量值的属性作为根节点。这意味着它更倾向于选择具有大量不同值的属性。
- en: C4.5, an improvement of ID3, uses Gain ratio which is a modification of Information
    gain that reduces its bias and is usually the best option. Gain ratio overcomes
    the problem with information gain by taking into account the number of branches
    that would result before making the split. It corrects information gain by taking
    the intrinsic information of a split into account.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: C4.5，作为 ID3 的改进，使用增益比，这是一种信息增益的修改，减少了其偏倚，通常是最佳选择。增益比通过考虑拆分前产生的分支数量来克服信息增益的问题。它通过考虑拆分的固有信息来修正信息增益。
- en: Let us consider if we have a dataset that has users and their movie genre preferences
    based on variables like gender, group of age, rating, blah, blah. With the help
    of information gain, you split at ‘Gender’ (assuming it has the highest information
    gain) and now the variables ‘Group of Age’ and ‘Rating’ could be equally important
    and with the help of gain ratio, it will penalize a variable with more distinct
    values which will help us decide the split at the next level.
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 假设我们有一个数据集，其中包含用户和他们的电影类型偏好，基于性别、年龄组、评分等变量。借助信息增益，你可以在‘性别’上进行拆分（假设它具有最高的信息增益），现在‘年龄组’和‘评分’变量可能同样重要，通过增益比的帮助，它会惩罚具有更多不同值的变量，这将帮助我们在下一个层次决定拆分。
- en: '![Figure](../Images/a732854b47d32f5f3cdf5fed4b9fa605.png)[Gain Ratio](https://towardsdatascience.com/from-a-single-decision-tree-to-a-random-forest-b9523be65147)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/a732854b47d32f5f3cdf5fed4b9fa605.png)[增益比](https://towardsdatascience.com/from-a-single-decision-tree-to-a-random-forest-b9523be65147)'
- en: Where “before” is the dataset before the split, K is the number of subsets generated
    by the split, and (j, after) is subset j after the split.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 其中“before”是拆分前的数据集，K 是拆分生成的子集的数量，(j, after) 是拆分后子集 j。
- en: '**Reduction in Variance**'
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**方差减少**'
- en: '**Reduction in variance** is an algorithm used for continuous target variables
    (regression problems). This algorithm uses the standard formula of variance to
    choose the best split. The split with lower variance is selected as the criteria
    to split the population:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**方差减少** 是一种用于连续目标变量（回归问题）的算法。该算法使用方差的标准公式来选择最佳拆分。选择方差较低的拆分作为拆分总体的标准：'
- en: '![](../Images/bb8247ef89486499b2dbcc281977b898.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb8247ef89486499b2dbcc281977b898.png)'
- en: Above X-bar is the mean of the values, X is actual and n is the number of values.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的 X-bar 是值的均值，X 是实际值，n 是值的数量。
- en: '**Steps to calculate Variance:**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算方差的步骤：**'
- en: Calculate variance for each node.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个节点的方差。
- en: Calculate variance for each split as the weighted average of each node variance.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个拆分的方差，作为每个节点方差的加权平均值。
- en: '**Chi-Square**'
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**卡方**'
- en: The acronym CHAID stands for *Chi*-squared Automatic Interaction Detector. It
    is one of the oldest tree classification methods. It finds out the statistical
    significance between the differences between sub-nodes and parent node. We measure
    it by the sum of squares of standardized differences between observed and expected
    frequencies of the target variable.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 缩写 CHAID 代表 *卡方* 自动交互检测器。它是最古老的树分类方法之一。它找出子节点和父节点之间差异的统计显著性。我们通过目标变量的观察频率和期望频率之间的标准化差异的平方和来衡量它。
- en: It works with the categorical target variable “Success” or “Failure”. It can
    perform two or more splits. Higher the value of Chi-Square higher the statistical
    significance of differences between sub-node and Parent node.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 它适用于分类目标变量“成功”或“失败”。它可以执行两个或更多的拆分。卡方值越高，子节点和父节点之间差异的统计显著性越高。
- en: It generates a tree called CHAID (Chi-square Automatic Interaction Detector).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 它生成一个称为 CHAID（卡方自动交互检测器）的树。
- en: 'Mathematically, Chi-squared is represented as:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，卡方被表示为：
- en: '![](../Images/c6e29fe0d58d993aae89fa9762398c2f.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6e29fe0d58d993aae89fa9762398c2f.png)'
- en: '**Steps to Calculate Chi-square for a split:**'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算拆分卡方的步骤：**'
- en: Calculate Chi-square for an individual node by calculating the deviation for
    Success and Failure both
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过计算成功和失败的偏差来计算单个节点的卡方。
- en: Calculated Chi-square of Split using Sum of all Chi-square of success and Failure
    of each node of the split
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过计算拆分后每个节点的成功和失败的所有卡方值的总和来计算拆分的卡方值。
- en: '**How to avoid/counter Overfitting in Decision Trees?**'
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**如何避免/对抗决策树中的过拟合？**'
- en: The common problem with Decision trees, especially having a table full of columns,
    they fit a lot. Sometimes it looks like the tree memorized the training data set.
    If there is no limit set on a decision tree, it will give you 100% accuracy on
    the training data set because in the worse case it will end up making 1 leaf for
    each observation. Thus this affects the accuracy when predicting samples that
    are not part of the training set.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的常见问题，特别是当表中列很多时，它们的拟合程度很高。有时，看起来树似乎记住了训练数据集。如果决策树没有设置限制，它会在训练数据集上提供100%的准确率，因为在最坏的情况下，它会为每个观察值生成一个叶子节点。因此，这会影响对训练集之外样本的预测准确性。
- en: 'Here are two ways to remove overfitting:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两种去除过拟合的方法：
- en: Pruning Decision Trees.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剪枝决策树。
- en: Random Forest
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机森林
- en: '**Pruning Decision Trees**'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**剪枝决策树**'
- en: The splitting process results in fully grown trees until the stopping criteria
    are reached. But, the fully grown tree is likely to overfit the data, leading
    to poor accuracy on unseen data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 分裂过程会生成完全生长的树，直到达到停止标准。但完全生长的树很可能会过拟合数据，从而在未见数据上的准确性较差。
- en: '![Figure](../Images/1c59703d8f644408230aa4c6ea5e4684.png)[Pruning in action](https://gfycat.com/enchantedyellowishbarasinga)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/1c59703d8f644408230aa4c6ea5e4684.png)[剪枝的实际应用](https://gfycat.com/enchantedyellowishbarasinga)'
- en: 'In **pruning**, you trim off the branches of the tree, i.e., remove the decision
    nodes starting from the leaf node such that the overall accuracy is not disturbed.
    This is done by segregating the actual training set into two sets: training data
    set, D and validation data set, V. Prepare the decision tree using the segregated
    training data set, D. Then continue trimming the tree accordingly to optimize
    the accuracy of the validation data set, V.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在**剪枝**中，你会修剪树的分支，即从叶子节点开始移除决策节点，以确保总体准确性不受影响。这是通过将实际训练集分成两个集合来完成的：训练数据集D和验证数据集V。使用分隔后的训练数据集D来准备决策树。然后继续修剪树，以优化验证数据集V的准确性。
- en: '![Figure](../Images/7bb957cdb54435ba9af5e032a34af199.png)[Pruning](https://www.cs.cmu.edu/~bhiksha/courses/10-601/decisiontrees/)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/7bb957cdb54435ba9af5e032a34af199.png)[剪枝](https://www.cs.cmu.edu/~bhiksha/courses/10-601/decisiontrees/)'
- en: In the above diagram, the ‘Age’ attribute in the left-hand side of the tree
    has been pruned as it has more importance on the right-hand side of the tree,
    hence removing overfitting.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述图示中，树的左侧的‘年龄’属性已被剪枝，因为它在树的右侧更重要，因此去除了过拟合。
- en: '**Random Forest**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林**'
- en: Random Forest is an example of ensemble learning, in which we combine multiple
    machine learning algorithms to obtain better predictive performance.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是集成学习的一个例子，在这种学习中，我们结合多个机器学习算法以获得更好的预测性能。
- en: '**Why the name “Random”?**'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么叫“随机”？**'
- en: 'Two key concepts that give it the name random:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 赋予其“随机”名称的两个关键概念：
- en: A random sampling of training data set when building trees.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在构建树时，随机抽样训练数据集。
- en: Random subsets of features considered when splitting nodes.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分裂节点时考虑的特征随机子集。
- en: A technique known as bagging is used to create an ensemble of trees where multiple
    training sets are generated with replacement.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一种称为“自助法”（bagging）的技术用于创建一个决策树集成模型，其中多个训练集通过有放回的抽样生成。
- en: In the bagging technique, a data set is divided into **N** samples using randomized
    sampling. Then, using a single learning algorithm a model is built on all samples.
    Later, the resultant predictions are combined using voting or averaging in parallel.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在自助法技术中，数据集被划分为**N**个样本，使用随机抽样。然后，使用单一学习算法在所有样本上建立模型。之后，通过投票或平均的方式将结果预测合并在一起。
- en: '![Figure](../Images/9a79dc5b0e44ede04284ddcb44888a0a.png)[Random Forest in
    action](https://towardsdatascience.com/why-random-forests-outperform-decision-trees-1b0f175a0b5)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/9a79dc5b0e44ede04284ddcb44888a0a.png)[随机森林的实际应用](https://towardsdatascience.com/why-random-forests-outperform-decision-trees-1b0f175a0b5)'
- en: Which is better Linear or tree-based models?
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性模型还是基于树的模型哪个更好？
- en: Well, it depends on the kind of problem you are solving.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这要看你正在解决的是什么问题。
- en: If the relationship between dependent & independent variables is well approximated
    by a linear model, linear regression will outperform the tree-based model.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果因变量与自变量之间的关系可以通过线性模型很好地近似，线性回归将优于基于树的模型。
- en: If there is a high non-linearity & complex relationship between dependent &
    independent variables, a tree model will outperform a classical regression method.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果因变量与自变量之间有高度非线性和复杂关系，树模型将优于经典的回归方法。
- en: If you need to build a model that is easy to explain to people, a decision tree
    model will always do better than a linear model. Decision tree models are even
    simpler to interpret than linear regression!
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你需要建立一个容易向人们解释的模型，决策树模型总比线性模型更好。决策树模型甚至比线性回归更易于解释！
- en: Decision Tree Classifier Building in Scikit-learn
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Scikit-learn构建决策树分类器
- en: The dataset that we have is a supermarket data which can be downloaded from [here](https://drive.google.com/open?id=1x1KglkvJxNn8C8kzeV96YePFnCUzXhBS).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们拥有的数据集是超市数据，可以从[这里](https://drive.google.com/open?id=1x1KglkvJxNn8C8kzeV96YePFnCUzXhBS)下载。
- en: Load all the basic libraries.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 加载所有基本库。
- en: '[PRE0]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Load the dataset. It consists of 5 features, `UserID`, `Gender`, `Age`, `EstimatedSalary`
    and `Purchased`.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据集。它包括5个特征：`UserID`、`Gender`、`Age`、`EstimatedSalary` 和 `Purchased`。
- en: '[PRE1]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Figure](../Images/3b51f398bfcc049794918ba1ef132e40.png)Dataset'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '![图](../Images/3b51f398bfcc049794918ba1ef132e40.png) 数据集'
- en: We will take only `Age` and `EstimatedSalary` as our independent variables `X` because
    of other features like `Gender` and `User ID` are irrelevant and have no effect
    on the purchasing capacity of a person. Purchased is our dependent variable `y`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将仅使用 `Age` 和 `EstimatedSalary` 作为我们的自变量 `X`，因为像 `Gender` 和 `User ID` 这样的其他特征与个人的购买能力无关且没有影响。Purchased
    是我们的因变量 `y`。
- en: '[PRE2]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The next step is to split the dataset into training and test.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将数据集拆分为训练集和测试集。
- en: '[PRE3]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Perform feature scaling
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 执行特征缩放
- en: '[PRE4]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Fit the model in the Decision Tree classifier.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树分类器中拟合模型。
- en: '[PRE5]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Make predictions and check accuracy.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 做出预测并检查准确性。
- en: '[PRE6]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The decision tree classifier gave an accuracy of 91%.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树分类器的准确率为91%。
- en: Confusion Matrix
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: '[PRE7]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It means 6 observations have been classified as false.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着6个观察结果被归类为虚假。
- en: '**Let us first visualize the model prediction results.**'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**让我们首先可视化模型预测结果。**'
- en: '[PRE8]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/ddd705134cdcc0f99c1814fe4b72041a.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddd705134cdcc0f99c1814fe4b72041a.png)'
- en: '**Let us also visualize the tree:**'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**让我们也可视化一下这棵树：**'
- en: You can use Scikit-learn’s *export_graphviz* function to display the tree within
    a Jupyter notebook. For plotting trees, you also need to install Graphviz and
    pydotplus.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用Scikit-learn的 *export_graphviz* 函数在Jupyter notebook中显示树。为了绘制树，你还需要安装Graphviz和pydotplus。
- en: '`conda install python-graphviz`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`conda install python-graphviz`'
- en: '`pip install pydotplus`'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`pip install pydotplus`'
- en: '*export_graphviz* function converts decision tree classifier into dot file
    and pydotplus convert this dot file to png or displayable form on Jupyter.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*export_graphviz* 函数将决策树分类器转换为dot文件，pydotplus 将此dot文件转换为png或在Jupyter中可显示的形式。'
- en: '[PRE9]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Figure](../Images/7f1f7d8aa6d9c4a09aa5e79ec82e76a1.png)Decision Tree.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![图](../Images/7f1f7d8aa6d9c4a09aa5e79ec82e76a1.png) 决策树。'
- en: In the decision tree chart, each internal node has a decision rule that splits
    the data. Gini referred to as the Gini ratio, which measures the impurity of the
    node. You can say a node is pure when all of its records belong to the same class,
    such nodes known as the leaf node.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树图表中，每个内部节点都有一个决策规则来分割数据。基尼指数（Gini）被称为基尼比率，用来衡量节点的不纯度。可以说，当所有记录都属于同一类别时，节点是纯的，这种节点称为叶节点。
- en: Here, the resultant tree is unpruned. This unpruned tree is unexplainable and
    not easy to understand. In the next section, let’s optimize it by pruning.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，结果树是未经修剪的。这个未经修剪的树是不可解释的，且不容易理解。在下一节中，我们将通过修剪来优化它。
- en: '**Optimizing the Decision Tree Classifier**'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**优化决策树分类器**'
- en: '**criterion**: optional (default=”gini”) or Choose attribute selection measure:
    This parameter allows us to use the different-different attribute selection measure.
    Supported criteria are “gini” for the Gini index and “entropy” for the information
    gain.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**criterion**：可选（默认值为“gini”）或 选择属性选择度量：此参数允许我们使用不同的属性选择度量。支持的标准有“gini”用于基尼指数和“entropy”用于信息增益。'
- en: '**splitter**: string, optional (default=”best”) or Split Strategy: This parameter
    allows us to choose the split strategy. Supported strategies are “best” to choose
    the best split and “random” to choose the best random split.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**splitter**：字符串， 可选（默认值为“best”）或 分割策略：这个参数允许我们选择分割策略。支持的策略有“best”以选择最佳分割和“random”以选择最佳随机分割。'
- en: '**max_depth**: int or None, optional (default=None) or Maximum Depth of a Tree:
    The maximum depth of the tree. If None, then nodes are expanded until all the
    leaves contain less than min_samples_split samples. The higher value of maximum
    depth causes overfitting, and a lower value causes underfitting (Source).'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**max_depth**：int 或 None，可选（默认=None）或树的最大深度：树的最大深度。如果为 None，则节点扩展直到所有叶子包含的样本数少于
    min_samples_split。较高的最大深度值会导致过拟合，而较低的值会导致欠拟合（来源）。'
- en: In Scikit-learn, optimization of decision tree classifier performed by only
    pre-pruning. The maximum depth of the tree can be used as a control variable for
    pre-pruning.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Scikit-learn 中，决策树分类器的优化仅通过预剪枝进行。树的最大深度可以作为预剪枝的控制变量。
- en: '[PRE10]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Well, the classification rate increased to 94%, which is better accuracy than
    the previous model.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，分类率提高到了94%，比之前的模型准确度更高。
- en: Now let us again visualize the pruned Decision tree after optimization.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在优化后再次可视化剪枝后的决策树。
- en: '[PRE11]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![Figure](../Images/c76148e38059f19cd1326993c049ed16.png)Decision Tree after
    pruning'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '![图像](../Images/c76148e38059f19cd1326993c049ed16.png)剪枝后的决策树'
- en: This pruned model is less complex, explainable, and easy to understand than
    the previous decision tree model plot.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这个剪枝模型比之前的决策树模型图更简单、可解释且易于理解。
- en: Conclusion
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we have covered a lot of details about Decision Tree; It’s
    working, attribute selection measures such as Information Gain, Gain Ratio, and
    Gini Index, decision tree model building, visualization and evaluation on supermarket
    dataset using Python Scikit-learn package and optimizing Decision Tree performance
    using parameter tuning.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们涵盖了关于决策树的许多细节；它的工作原理、属性选择度量如信息增益、增益比和基尼指数，决策树模型的构建、可视化和在超市数据集上使用 Python
    Scikit-learn 包的评估，以及通过参数调整优化决策树性能。
- en: Well, that’s all for this article hope you guys have enjoyed reading it, feel
    free to share your comments/thoughts/feedback in the comment section.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，这篇文章就到此为止，希望大家喜欢阅读，欢迎在评论区分享你的评论/想法/反馈。
- en: '![Figure](../Images/fe29fca4d34cf017cf70bdb1fde78042.png)[Source](https://www.ilikesticker.com/LineStickerAnimation/W433402-Movie-End-Credits-Style-[English-Ver]/en)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/fe29fca4d34cf017cf70bdb1fde78042.png)[来源](https://www.ilikesticker.com/LineStickerAnimation/W433402-Movie-End-Credits-Style-[English-Ver]/en)'
- en: Thanks for reading !!!
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读!!!
- en: '**Bio: [Nagesh Singh Chauhan](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)**
    is a Data Science enthusiast. Interested in Big Data, Python, Machine Learning.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[Nagesh Singh Chauhan](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)**
    是数据科学爱好者。对大数据、Python 和机器学习感兴趣。'
- en: '[Original](https://towardsdatascience.com/decision-tree-algorithm-explained-83beb6e78ef4).
    Reposted with permission.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/decision-tree-algorithm-explained-83beb6e78ef4)。已获转载许可。'
- en: More On This Topic
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Simplifying Decision Tree Interpretability with Python & Scikit-learn](https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 Python 和 Scikit-learn 简化决策树可解释性](https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html)'
- en: '[Understanding by Implementing: Decision Tree](https://www.kdnuggets.com/2023/02/understanding-implementing-decision-tree.html)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过实现理解：决策树](https://www.kdnuggets.com/2023/02/understanding-implementing-decision-tree.html)'
- en: '[Telling a Great Data Story: A Visualization Decision Tree](https://www.kdnuggets.com/2021/02/telling-great-data-story-visualization-decision-tree.html)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[讲述精彩数据故事：可视化决策树](https://www.kdnuggets.com/2021/02/telling-great-data-story-visualization-decision-tree.html)'
- en: '[Random Forest vs Decision Tree: Key Differences](https://www.kdnuggets.com/2022/02/random-forest-decision-tree-key-differences.html)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[随机森林与决策树：关键差异](https://www.kdnuggets.com/2022/02/random-forest-decision-tree-key-differences.html)'
- en: '[KDnuggets™ News 22:n09, Mar 2: Telling a Great Data Story: A…](https://www.kdnuggets.com/2022/n09.html)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets™ 新闻 22:n09，3 月 2 日：讲述精彩数据故事：A…](https://www.kdnuggets.com/2022/n09.html)'
- en: '[A Complete Guide To Decision Tree Software](https://www.kdnuggets.com/2022/08/complete-guide-decision-tree-software.html)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[决策树软件完整指南](https://www.kdnuggets.com/2022/08/complete-guide-decision-tree-software.html)'
