- en: Spark SQL for Real-Time Analytics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL用于实时分析
- en: 原文：[https://www.kdnuggets.com/2015/09/spark-sql-real-time-analytics.html](https://www.kdnuggets.com/2015/09/spark-sql-real-time-analytics.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2015/09/spark-sql-real-time-analytics.html](https://www.kdnuggets.com/2015/09/spark-sql-real-time-analytics.html)
- en: '**By Sumit Pal and Ajit Jaokar, **(FutureText).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**由Sumit Pal和Ajit Jaokar编写，**（FutureText）。'
- en: This article is part of the forthcoming **Data Science for Internet of Things
    Practitioner course** in London. If you want to be a Data Scientist for the Internet
    of Things, this intensive course is ideal for you. We cover complex areas like
    Sensor fusion, Time Series, Deep Learning and others. We work with Apache Spark,
    R language and leading IoT platforms. Please contact [info@futuretext.com](mailto:info@futuretext.com) for
    more details
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是即将推出的**物联网数据科学从业者课程**在伦敦的一部分。如果你想成为物联网的数据科学家，这个密集课程非常适合你。我们涵盖了传感器融合、时间序列、深度学习等复杂领域。我们使用Apache
    Spark、R语言以及领先的物联网平台。有关更多信息，请联系[info@futuretext.com](mailto:info@futuretext.com)。
- en: '**Overview**'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**概述**'
- en: This is the 1^(st) part of a series of 3 part article which discusses SQL with
    Spark for Real Time Analytics for IOT. Part One discusses the technological foundations
    of SQL with Spark. Part two discusses Real Time Analytics with Spark SQL. Finally,
    Part Three discusses an IoT use case for Real Time Analytics with Spark SQL.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这是讨论Spark在物联网实时分析中使用SQL的3部分系列文章的第一部分。第一部分讨论了使用Spark的SQL的技术基础。第二部分讨论了使用Spark
    SQL进行实时分析。最后，第三部分讨论了一个物联网实时分析的用例。
- en: '**Introduction**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: In Part One, we discuss Spark SQL and why it is the preferred method for Real
    Time Analytics. Spark SQL is a module in Apache Spark that integrates relational
    processing with Spark’s functional programming API. Spark SQL has been part of
    Spark Core since version 1.0\. It runs HiveQL/SQL alongside or replacing existing
    hive deployments. It can connect to existing BI Tools. It has bindings in Python,
    Scala and Java. It makes two vital additions to the framework. Firstly, it offers
    a tight integration between relational and procedural processing, with declarative
    DataFrame API that integrates with procedural Spark API. Secondly, it includes
    an extensible optimizer, built with Scala, leveraging its strong pattern matching
    capabilities, which makes it easy to add compos-able rules, control code generation
    and deﬁne extensions.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分中，我们讨论了Spark SQL以及为什么它是实时分析的首选方法。Spark SQL是Apache Spark中的一个模块，它将关系处理与Spark的函数式编程API集成。自1.0版本以来，Spark
    SQL就是Spark Core的一部分。它可以与现有的Hive部署并行运行HiveQL/SQL，或替代现有的Hive部署。它可以连接到现有的BI工具。它在Python、Scala和Java中都有绑定。它为框架提供了两个重要的补充。首先，它提供了关系和过程处理之间的紧密集成，具有与过程式Spark
    API集成的声明性DataFrame API。其次，它包括一个可扩展的优化器，使用Scala构建，利用其强大的模式匹配能力，使得添加可组合规则、控制代码生成和定义扩展变得容易。
- en: '**Objectives and Goals of Spark SQL**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spark SQL的目标和目标**'
- en: While the relational approach has been applied to solving big data problems,
    it is in-sufficient for many big data applications. Relational and procedural
    approaches have until recently remained disjoint, forcing developers to choose
    one paradigm or other. The Spark SQL framework combines both models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管关系方法已被应用于解决大数据问题，但它对许多大数据应用来说仍然不够充分。直到最近，关系和过程方法一直保持分离，迫使开发人员选择其中一种范式。Spark
    SQL框架结合了这两种模型。
- en: As they say, “The fastest way to read data is NOT to read it” at all.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 正如他们所说，“读取数据的最快方式就是根本不读取它”。
- en: Spark SQL supports relational processing both within Spark programs (viaRDDs)
    and on external data sources. It can easily support new data sources, including
    semi-structured data and external databases amenable to query federation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL支持在Spark程序（通过RDDs）和外部数据源上的关系处理。它可以轻松支持新的数据源，包括半结构化数据和适用于查询联合的外部数据库。
- en: Spark SQL helps this philosophy by
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL通过以下方式帮助这一理念
- en: Converting data to more efficient formats (from storage, network and IO perspective)
    by using various columnar formats
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用各种列式格式将数据转换为更高效的格式（从存储、网络和IO的角度来看）
- en: Using data partitioning
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据分区
- en: Skipping data reads using Data statistics
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据统计跳过数据读取
- en: Pushing predicates to Storage System
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将谓词推送到存储系统
- en: Optimization as late as possible when all the information about data pipelines
    is available
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能晚地优化，直到获取有关数据管道的所有信息
- en: Internally, Spark SQL and DataFrame take advantage of the Catalyst query optimizer
    to intelligently plan the execution of queries.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，Spark SQL和DataFrame利用Catalyst查询优化器智能地规划查询的执行。
- en: '**Spark SQL **'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spark SQL**'
- en: Spark SQL can support Batch or Streaming SQL. With RDDs the core Spark Framework
    supports batch workloads. RDDs can point to static data sets and Sparks’s rich
    API can be used to manipulate the RDDs working on batch data sets in memory with
    lazy evaluation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 可以支持批处理或流处理 SQL。使用 RDDs，核心 Spark 框架支持批处理工作负载。RDD 可以指向静态数据集，Spark 的丰富
    API 可用于操作在内存中以惰性求值处理的批数据集。
- en: '**Streaming + SQL and DataFrames**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**流处理 + SQL 和 DataFrames**'
- en: Let us quickly look into what is an RDD in spark core framework and what is
    a DStream which are the basic building blocks for further discussions in the article
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速了解一下在 Spark 核心框架中什么是 RDD，什么是 DStream，这些是本文讨论的基本构建块。
- en: Spark operates on RDDs – resilient distributed dataset – which is an in-memory
    data structure. Each RDD represents a chunk of the data which is partitioned across
    the data nodes in the cluster. RDDs are immutable and a new one is created when
    transformations are applied. RDDs are operated in parallel using transformations/actions
    like mapping, filtering. These operations are performed simultaneously on all
    the partitions in parallel. RDDs are resilient, if a partition is lost due to
    a node crash, it can be reconstructed from the original sources.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 在 RDDs（弹性分布式数据集）上操作，这是一种内存中的数据结构。每个 RDD 代表数据的一部分，这些数据被分区到集群中的数据节点。RDD
    是不可变的，当应用变换时会创建新的 RDD。RDDs 在并行中操作，使用诸如映射、过滤等变换/操作。这些操作在所有分区上同时进行。RDDs 是弹性的，如果由于节点崩溃而丢失某个分区，可以从原始来源重建它。
- en: '![spark-rdd](../Images/76981d00d9f9634e21074e9cdd8f42fc.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![Spark RDD](../Images/76981d00d9f9634e21074e9cdd8f42fc.png)'
- en: Spark Streaming provides as abstraction called DStream (discrete streams) which
    is a continuous stream of data. DStreams are created from input data stream or
    from sources such as Kafka, Flume or by applying operations on other DStreams.
    A DStream is essentially a sequence of RDDs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 提供了一种称为 DStream（离散流）的抽象，它是一个连续的数据流。DStreams 可以从输入数据流或 Kafka、Flume
    等源创建，或通过对其他 DStreams 应用操作创建。DStream 本质上是 RDD 的序列。
- en: '![real-time-rdd](../Images/58e5b962f873f68db85bccce6b212219.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![实时 RDD](../Images/58e5b962f873f68db85bccce6b212219.png)'
- en: RDDs generated by DStreams can be converted to DataFrames and queried using
    SQL. The stream can be exposed to any external application that talks SQL using
    Spark’s JDBC driver. Batches of streaming data are stored in the Spark’s worker
    memory and this can be interactively queried on demand using SQL or Spark’s API.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由 DStreams 生成的 RDD 可以转换为 DataFrames 并使用 SQL 查询。该流可以通过 Spark 的 JDBC 驱动程序暴露给任何外部应用程序，进行
    SQL 查询。流数据的批次存储在 Spark 的工作内存中，可以通过 SQL 或 Spark 的 API 进行交互式查询。
- en: 'StreamSQL is a Spark component that combines Catalyst and Spark Streaming to
    perform SQL queries on DStreams. StreamSQL extends SQL to support streams with
    operations like:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: StreamSQL 是一个 Spark 组件，它结合了 Catalyst 和 Spark Streaming 来对 DStreams 执行 SQL 查询。StreamSQL
    扩展了 SQL 以支持流，具有以下操作：
- en: SELECT against a stream to calculate functions or filter unwanted data (using
    a WHERE clause)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对流进行 SELECT 以计算函数或筛选不需要的数据（使用 WHERE 子句）
- en: JOIN a stream with 1 or more data sets to produce a new stream.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将流与一个或多个数据集进行 JOIN，以生成新的流。
- en: Windowing and Aggregation – Stream can be limited to create finite data sets.
    Windowing allows complex message selection based on field values. Once a finite
    batch is created analytics can be applied.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 窗口化和聚合 – 流可以被限制以创建有限的数据集。窗口化允许基于字段值的复杂消息选择。一旦创建了有限的批次，可以应用分析。
- en: Spark SQL has the following components
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 具有以下组件
- en: '**Components**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**组件**'
- en: Spark SQL Core
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 核心
- en: Execution of queries as RDDs
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以 RDD 执行查询
- en: Reading data sets in multiple file formats Parquet, JSON, Avro etc.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读取多种文件格式的数据集，如 Parquet、JSON、Avro 等。
- en: Reading data sources both SQL and NOSQL Data sources
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读取 SQL 和 NOSQL 数据源
- en: Hive Support
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Hive 支持
- en: HQL, MetaStore, SerDes, UDFs
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HQL、MetaStore、SerDes、UDFs
- en: Catalyst Optimizer
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Catalyst 优化器
- en: it optimizes the Relational algebra + expressions
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它优化了关系代数 + 表达式
- en: it does Query optimization
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它进行查询优化。
- en: '**Problems Spark SQL solves**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spark SQL 解决的问题**'
- en: Spark SQL provides a unified framework with no movement of data outside the
    cluster. No extra modules need to be installed or integrated. It provides a unified
    load/save interface irrespective of the data source and the programming language.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 提供了一个统一的框架，无需将数据移出集群。无需安装或集成额外的模块。它提供了一个统一的加载/保存接口，无论数据源和编程语言是什么。
- en: The example below shows how easy it is to both load data from avro and convert
    it into parquet.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例显示了从 avro 加载数据并将其转换为 parquet 是多么简单。
- en: '`val df = sqlContext.load("mydata.avro", "com.databricks.spark.avro") df.save("mydata.parquet",
    "parquet")`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`val df = sqlContext.load("mydata.avro", "com.databricks.spark.avro") df.save("mydata.parquet",
    "parquet")`'
- en: Spark SQL has a unified framework to solve the same analytics problem both for
    batch & streaming, which has been the Holy Grail in data processing. Frameworks
    have been built which do either one of them and do it well in terms of scalability,
    performance and feature set, but having a unified framework for doing both batch
    and streaming was never feasible, before Spark / Spark SQL led the way. With Spark
    framework the same code (logic) can work either with batch data – RDDs or with
    Streaming Data Sets (DStreams – Discretized Streams). DStream is just a series
    of RDDs. This representation allows batch and streaming workloads to work seamlessly.
    This vastly reduces code maintenance overheads and training developers with 2
    different skill sets.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 具有统一的框架来解决批处理和流处理中的相同分析问题，这一直是数据处理中的圣杯。已有框架可以很好地处理其中之一，并在可扩展性、性能和功能集方面表现良好，但在
    Spark / Spark SQL 之前，拥有一个同时处理批处理和流处理的统一框架是不可行的。通过 Spark 框架，相同的代码（逻辑）可以用于批处理数据
    – RDDs 或流处理数据集（DStreams – 离散化流）。DStream 只是一系列的 RDDs。这种表示方式允许批处理和流处理工作负载无缝工作。这大大减少了代码维护开销，并减少了对两种不同技能集的开发者的培训需求。
- en: '**Reading from JDBC Data Sources**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**从 JDBC 数据源读取**'
- en: Data source for reading from JDBC has been added as built-in source for Spark
    SQL. Spark SQL can extract data from any existing relational databases that supports
    JDBC.  Examples include mysql, postgres, H2, and more.  Reading data from one
    of these systems is as simple as creating virtual table that points to the external
    table.  Data from this table can then be easily read in and joined with any of
    the other sources that Spark SQL supports.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从 JDBC 读取的数据源已作为 Spark SQL 的内置数据源添加。Spark SQL 可以从任何支持 JDBC 的现有关系数据库中提取数据。示例包括
    mysql、postgres、H2 等。从这些系统中的一个读取数据就像创建指向外部表的虚拟表一样简单。然后，可以轻松地读取该表的数据，并与 Spark SQL
    支持的任何其他数据源进行联接。
- en: '**Spark SQL with Data Frames**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spark SQL 与数据框架**'
- en: Data Frames are distributed collection of rows organized into named columns,
    an abstraction for selecting, filtering, aggregating and plotting, structured
    data – it was previously used to be called SchemaRDD.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框架是分布式的行集合，按命名列组织，是一种选择、过滤、聚合和绘制结构化数据的抽象 – 之前称为 SchemaRDD。
- en: DataFrame API can perform relational operations on both external data sources
    and Spark’s built-in distributed collections.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame API 可以对外部数据源和 Spark 的内置分布式集合执行关系操作。
- en: DataFrame offers rich relational/procedural integration within Spark programs.
    DataFrames are collections of structured records that can be manipulated using
    Spark’s procedural API, or using new relational APIs that allow richer optimizations.
    They can be created directly from Spark’s built-in distributed collections of
    objects, enabling relational processing in existing Spark.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 在 Spark 程序中提供了丰富的关系/过程集成功能。DataFrames 是结构化记录的集合，可以使用 Spark 的过程 API
    进行操作，或者使用新的关系 API 进行更丰富的优化。它们可以直接从 Spark 的内置分布式对象集合中创建，从而在现有的 Spark 中进行关系处理。
- en: DataFrames are more convenient and more efﬁcient than Spark’s procedural API.
    They make it easy to compute multiple aggregates in one pass using a SQL statement,
    something that is difﬁcult to express in traditional functional APIs.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrames 比 Spark 的过程 API 更方便和高效。它们使得使用 SQL 语句在一次操作中计算多个聚合变得容易，这在传统的函数式 API
    中很难表达。
- en: Unlike RDDs, DataFrames keep track of their schema and support various relational
    operations that lead to more optimized execution. They infer the schema using
    reflection.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与 RDDs 不同，DataFrames 会跟踪其模式并支持各种关系操作，从而实现更优化的执行。它们通过反射推断模式。
- en: Spark DataFrames are lazy in that each DataFrame object represents a logical
    plan to compute a dataset, but no execution occurs until the user calls a special
    “output operation” such as save. This enables rich optimization across all operations.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Spark DataFrames 是惰性的，每个 DataFrame 对象表示一个计算数据集的逻辑计划，但在用户调用特殊的“输出操作”如保存之前不会执行。这使得所有操作能够进行丰富的优化。
- en: DataFrames evolve Spark’s RDD model, making it faster and easier for Spark developers
    to work with structured data by providing simplified methods for filtering, aggregating,
    and projecting over large datasets. DataFrames are available in Spark’s Java,
    Scala, and Python API.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrames 使 Spark 的 RDD 模型得以演变，提供简化的方法来过滤、聚合和投影大数据集，从而使 Spark 开发者更快、更容易地处理结构化数据。DataFrames
    可在 Spark 的 Java、Scala 和 Python API 中使用。
- en: Spark SQL’s data source API can read and write DataFrames from a wide variety
    of data sources and data formats – Avro, parquet, ORC, JSON, H2.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 的数据源 API 可以从各种数据源和数据格式中读取和写入 DataFrames——包括 Avro、parquet、ORC、JSON、H2。
- en: '*Example of how writing less code– using plain RDDs and using DataFrame APIs
    for SQL*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*减少代码编写的示例——使用普通 RDDs 和 DataFrame API 的 SQL*'
- en: The scala example below shows equivalent code – one using Sparks RDD APIs and
    other one using Spark’s DataFrame API. Let us have an object – People – firstname,
    lastname, age being the attributes and the objective is to get the basic stats
    on age – People – grouped by firstname.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的 Scala 示例展示了等效的代码——一个使用 Spark 的 RDD API，另一个使用 Spark 的 DataFrame API。我们有一个对象——People——包括
    firstname、lastname、age 作为属性，目标是获取按 firstname 分组的年龄基本统计数据。
- en: '`case class People(firstname: String, lastname: String, age: Intger) val people
    = rdd.map(p => (people.firstname, people.age)).cache()`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`case class People(firstname: String, lastname: String, age: Intger) val people
    = rdd.map(p => (people.firstname, people.age)).cache()`'
- en: '`// RDD Code val minAgeByFN = people.reduceByKey( scala.math.min(_, _) ) val
    maxAgeByFN = people.reduceByKey( scala.math.max(_, _) ) val avgAgeByFN = people.mapValues(x
    => (x, 1)) .reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2)) val countByFN = 
    people.mapValues(x => 1).reduceByKey(_ + _)`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`// RDD 代码 val minAgeByFN = people.reduceByKey( scala.math.min(_, _) ) val
    maxAgeByFN = people.reduceByKey( scala.math.max(_, _) ) val avgAgeByFN = people.mapValues(x
    => (x, 1)) .reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2)) val countByFN =
    people.mapValues(x => 1).reduceByKey(_ + _)`'
- en: '`// Data Frame Code df = people.toDF people = df.groupBy("firstname").agg(
    min("age"), max("age"), avg("age"), count("*"))`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`// Data Frame 代码 df = people.toDF people = df.groupBy("firstname").agg( min("age"),
    max("age"), avg("age"), count("*"))`'
- en: Data Frames are faster than using the plain RDDs due to the catalyst optimizer.
    DataFrames provide the same operations as relational query languages like SQL
    and Pig.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Catalyst 优化器，DataFrames 比使用普通 RDDs 更快。DataFrames 提供与 SQL 和 Pig 等关系查询语言相同的操作。
- en: '**Approach to solving Streaming Analytics with Spark SQL**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 Spark SQL 解决流式分析的方案**'
- en: The diagram below shows thinking behind our approach of using Spark SQL for
    doing Real Time Analytics. This follows the well know pattern of Lambda Architecture
    for building Real Time Analytic systems for big streaming data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了我们使用 Spark SQL 进行实时分析的方法。这遵循了构建实时分析系统的 Lambda 架构模式，适用于大规模流数据。
- en: '**Shortcomings of Spark SQL**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spark SQL 的缺点**'
- en: '![lambda-architecture-spark-sql](../Images/f34f18a8fb3c7c2708686cefbea49f34.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![lambda-architecture-spark-sql](../Images/f34f18a8fb3c7c2708686cefbea49f34.png)'
- en: Same as any tool running on a Hadoop cluster – the SLAs do not depend on how
    fast the engine is – but depends on how many other concurrent users are running
    on the system.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何运行在 Hadoop 集群上的工具一样——服务水平协议（SLA）并不依赖于引擎的速度——而是依赖于系统上运行的其他并发用户的数量。
- en: The Spark SQL code is sometimes too pithy for a first user to comprehend what
    is being done. It takes some experience and practice to get used to the coding
    style and implicitness of spark SQL code.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于初次使用者来说，Spark SQL 代码有时过于简洁，难以理解正在做什么。需要一些经验和实践来习惯 Spark SQL 代码的编码风格和隐式性。
- en: '**Conclusion**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**结论**'
- en: Spark SQL as an important evolution of the core Spark API. While Spark’s original
    functional programming API was quite general, it offered only limited opportunities
    for automatic optimization. Spark SQL simultaneously makes Spark accessible to
    more users.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 是核心 Spark API 的重要进化。尽管 Spark 的原始函数式编程 API 相当通用，但它仅提供了有限的自动优化机会。Spark
    SQL 使 Spark 更加易于使用。
- en: This article was an effort to prepare the fundamentals for the next set of articles
    where spark sql is used for doing real time analytics in IOT space.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在为下一组文章奠定基础，这些文章将探讨在物联网领域使用 Spark SQL 进行实时分析。
- en: '**References:**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: StreamSQL – [https://en.wikipedia.org/wiki/StreamSQL](https://en.wikipedia.org/wiki/StreamSQL)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: StreamSQL – [https://en.wikipedia.org/wiki/StreamSQL](https://en.wikipedia.org/wiki/StreamSQL)
- en: StreamSQL – [https://github.com/thunderain-project/StreamSQL](https://github.com/thunderain-project/StreamSQL)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: StreamSQL – [https://github.com/thunderain-project/StreamSQL](https://github.com/thunderain-project/StreamSQL)
- en: '**Bio:** **[Sumit Pal](https://www.linkedin.com/profile/view?id=AAIAAADShS4B8ypiJ59lZDz0YeioS2qlAuFkKtI&trk)** is
    a big data, visualisation and data science consultant. He is also a software architect
    and big data enthusiast and builds end-to-end data-driven analytic systems. Sumit
    has worked for Microsoft (SQL server development team), Oracle (OLAP development
    team) and Verizon (Big Data analytics team) in a career spanning 22 years. Currently,
    he works for multiple clients advising them on their data architectures and big
    data solutions and does hands on coding with Spark, Scala, Java and Python. Sumit
    blogs at  [sumitpal.wordpress.com/](https://sumitpal.wordpress.com/)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**简历：** **[Sumit Pal](https://www.linkedin.com/profile/view?id=AAIAAADShS4B8ypiJ59lZDz0YeioS2qlAuFkKtI&trk)**
    是一位大数据、可视化和数据科学顾问。他还是一名软件架构师和大数据爱好者，构建端到端的数据驱动分析系统。Sumit 曾在微软（SQL 服务器开发团队）、甲骨文（OLAP
    开发团队）和 Verizon（大数据分析团队）工作，拥有 22 年的职业生涯。目前，他为多个客户提供数据架构和大数据解决方案的建议，并进行 Spark、Scala、Java
    和 Python 的实际编码。Sumit 在 [sumitpal.wordpress.com/](https://sumitpal.wordpress.com/)
    撰写博客。'
- en: '**[Ajit Jaokar](https://www.linkedin.com/in/ajitjaokar) ([@AjitJaokar](https://twitter.com/ajitjaokar))**
    does research and consulting for Data Science and the Internet of Things. His
    work is based on his teaching at Oxford University and UPM (Technical University
    of Madrid) and covers IoT, Data Science, Smart cities and Telecoms.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Ajit Jaokar](https://www.linkedin.com/in/ajitjaokar) ([@AjitJaokar](https://twitter.com/ajitjaokar))**
    从事数据科学和物联网的研究与咨询。他的工作基于他在牛津大学和马德里理工大学的教学，涵盖了物联网、数据科学、智能城市和电信领域。'
- en: '**Related:**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Introduction to Big Data with Apache Spark](/2015/06/introduction-big-data-apache-spark.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Apache Spark 介绍大数据](/2015/06/introduction-big-data-apache-spark.html)'
- en: '[50+ Data Science and Machine Learning Cheat Sheets](/2015/07/good-data-science-machine-learning-cheat-sheets.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50+ 数据科学和机器学习备忘单](/2015/07/good-data-science-machine-learning-cheat-sheets.html)'
- en: '[The Big ‘Big Data’ Question: Hadoop or Spark?](/2015/08/big-data-question-hadoop-spark.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大数据的“重大”问题：Hadoop 还是 Spark？](/2015/08/big-data-question-hadoop-spark.html)'
- en: '* * *'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的 IT 部门'
- en: '* * *'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[In-Database Analytics: Leveraging SQL''s Analytic Functions](https://www.kdnuggets.com/2023/07/indatabase-analytics-leveraging-sql-analytic-functions.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据库内分析：利用 SQL 的分析函数](https://www.kdnuggets.com/2023/07/indatabase-analytics-leveraging-sql-analytic-functions.html)'
- en: '[Query Your Pandas DataFrames with SQL](https://www.kdnuggets.com/2021/10/query-pandas-dataframes-sql.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 SQL 查询你的 Pandas DataFrames](https://www.kdnuggets.com/2021/10/query-pandas-dataframes-sql.html)'
- en: '[Data Preparation with SQL Cheatsheet](https://www.kdnuggets.com/2021/05/data-preparation-sql-cheat-sheet.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据准备 SQL 备忘单](https://www.kdnuggets.com/2021/05/data-preparation-sql-cheat-sheet.html)'
- en: '[Getting Started with SQL Cheatsheet](https://www.kdnuggets.com/2022/08/getting-started-sql-cheatsheet.html)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQL 入门备忘单](https://www.kdnuggets.com/2022/08/getting-started-sql-cheatsheet.html)'
- en: '[Handling Missing Values in Time-series with SQL](https://www.kdnuggets.com/2022/09/handling-missing-values-timeseries-sql.html)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 SQL 处理时间序列中的缺失值](https://www.kdnuggets.com/2022/09/handling-missing-values-timeseries-sql.html)'
- en: '[4 Useful Intermediate SQL Queries for Data Science](https://www.kdnuggets.com/2022/12/4-useful-intermediate-sql-queries-data-science.html)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学的 4 个有用的中级 SQL 查询](https://www.kdnuggets.com/2022/12/4-useful-intermediate-sql-queries-data-science.html)'
