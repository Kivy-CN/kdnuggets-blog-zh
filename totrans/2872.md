# 每个数据科学家必须知道的5种分类评估指标

> 原文：[https://www.kdnuggets.com/2019/10/5-classification-evaluation-metrics-every-data-scientist-must-know.html](https://www.kdnuggets.com/2019/10/5-classification-evaluation-metrics-every-data-scientist-must-know.html)

[评论](#comments)

***我们想要优化什么？***大多数企业未能回答这个简单的问题。

***每个商业问题都略有不同，因此应该采取不同的优化方式。***

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业的快车道。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的IT

* * *

我们都创建过分类模型。我们很多时候尝试通过准确率来评估我们的模型。***但我们真的希望准确率作为模型性能的指标吗？***

***如果我们预测的是会撞击地球的小行星数量呢？***

只要说零，你将99%准确。我的模型可能在一定程度上准确，但完全没有价值。在这种情况下我们应该怎么做？

> *设计数据科学项目比建模本身更为重要。*

***这篇文章关于各种评估指标及其使用时机。***

### 1. 准确率、精确度和召回率：

![](../Images/fe2bc86aaaf0cf9b6c68d2768751a9ed.png)

### A. 准确率

准确率是典型的分类指标。它比较容易理解，适用于二分类以及多分类问题。

`准确率 = (TP+TN)/(TP+FP+FN+TN)`

准确率是指在所有被检查的案例中，真正结果的比例。

**何时使用？**

对于平衡良好且没有偏斜或没有类别不平衡的分类问题，准确率是一个有效的评估选择。

**警告**

假设我们的目标类别非常稀疏。我们是否希望准确率作为模型性能的指标？***如果我们预测的是小行星是否会撞击地球？***只要说`不`，你将99%准确。我的模型可能在一定程度上准确，但完全没有价值。

### B. 精确度

首先，我们来谈谈*精确度*，它回答了以下问题：**预测为正例**的比例中有多少是真正的正例？

`精确度 = (TP)/(TP+FP)`

在小行星预测问题中，我们从未预测到真正的正例。

因此，精确度 = 0

**何时使用？**

当我们想要对我们的预测非常确定时，准确率是一个有效的评估指标。例如：如果我们正在建立一个预测是否应该减少某个账户的信用额度的系统，我们希望对我们的预测非常确定，否则可能会导致客户不满意。

**注意事项**

*非常准确意味着我们的模型会遗漏很多信用违约者，从而导致损失。*

### C. 召回率

另一个非常有用的度量是*召回率*，它回答了一个不同的问题：**实际正例**中有多少比例被正确分类？

`召回率 = (TP)/(TP+FN)`

在小行星预测问题中，我们从未预测到真正的正例。

因此，召回率也等于0。

**何时使用？**

当我们想要捕捉尽可能多的正例时，召回率是一个有效的评估指标。例如：如果我们正在建立一个预测一个人是否患有癌症的系统，即使我们不是很确定，我们也希望捕捉到这种疾病。

**注意事项**

***如果我们对所有样本都预测为1，则召回率为1。***

因此，引出了利用准确率与召回率之间的权衡——***F1分数***。

### 2\. F1分数：

这是我最***喜欢的评估指标***，我在分类项目中经常使用它。

F1分数是介于0和1之间的一个数字，是准确率和召回率的调和均值。

![](../Images/d13bce6c7f3757567b2860521e727525.png)

让我们从一个二分类问题开始。***我们在预测小行星是否会撞击地球。***

所以，如果我们对整个训练集都说“不”，我们的准确率是0。那么我们正类的召回率是多少？是零。准确率是多少？超过99%。

因此，F1分数也为0。因此我们了解到，准确率为99%的分类器在我们的情况下基本上没有价值。因此，它解决了我们的问题。

**何时使用？**

我们希望模型具有良好的准确率和召回率。

![图示](../Images/588de63fd8749e1cfea448c9144f6dbe.png)

准确率-召回率权衡

简单来说，***F1分数在你的分类器的准确率和召回率之间保持某种平衡***。如果你的准确率很低，F1分数也会低；如果召回率很低，那么你的F1分数也会低。

> 如果你是一个警察检查员，并且你想抓捕罪犯，你需要确保你抓到的人确实是罪犯（准确率），同时你还想尽可能地捕捉到更多的罪犯（召回率）。F1分数可以平衡这种权衡。

**如何使用？**

你可以使用以下方法计算二分类问题的F1分数：

```py

from sklearn.metrics import f1_score
y_true = [0, 1, 1, 0, 1, 1]
y_pred = [0, 0, 1, 0, 0, 1]
*f1_score(y_true, y_pred)* 

```

这是我用来获取最大化F1分数的最佳阈值的函数之一。以下函数遍历可能的阈值，找到给出最佳F1分数的阈值。

```py
# y_pred is an array of predictions
def bestThresshold(y_true,y_pred):
    best_thresh = None
    best_score = 0
    for thresh in np.arange(0.1, 0.501, 0.01):
        score = f1_score(y_true, np.array(y_pred)>thresh)
        if score > best_score:
            best_thresh = thresh
            best_score = score
    return best_score , best_thresh
```

**注意事项**

F1分数的主要问题是它对准确率和召回率给予了相等的权重。有时候，我们可能需要在评估中加入领域知识，以便我们希望有更多的召回率或更多的准确率。

为了解决这个问题，我们可以通过创建一个加权F1指标来实现，其中beta管理精确度和召回率之间的权衡。

![](../Images/642e02997e7d35b1a68750374c0e6542.png)

在这里，我们给予召回率β倍于精确度的权重。

```py

from sklearn.metrics import fbeta_score
y_true = [0, 1, 1, 0, 1, 1]
y_pred = [0, 0, 1, 0, 0, 1]
fbeta_score(y_true, y_pred,beta=0.5)

```

F1分数也可以用于多类问题。有关详细信息，请查看[这篇](https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1)精彩的博文，作者是[Boaz Shmueli](https://medium.com/u/57ee515c83c5?source=post_page-----aa97784ff226----------------------)。

### 3\. 对数损失/二分类交叉熵

对数损失是二分类器的一个相当好的评估指标，有时在逻辑回归和神经网络中也是优化目标。

一个例子的二分类对数损失由以下公式给出，其中p是预测1的概率。

![](../Images/589f6701243bff934d5bab0dac0f4793.png)

![](../Images/5eb3b742cca4ae0ee23b80a85487d7f6.png)

如你所见，当我们对1的预测相当确定而真实标签也是1时，对数损失降低。

**何时使用？**

*当分类器的输出是预测概率时。****对数损失会考虑到你的预测的不确定性，基于其与实际标签的偏差。***这为我们提供了模型性能的更细致视图。一般来说，最小化对数损失能为分类器带来更高的准确性。

**如何使用？**

```py

from sklearn.metrics import log_loss  
# where y_pred are probabilities and y_true are binary class labels
log_loss(y_true, y_pred, eps=1e-15)

```

**注意事项**

在[不平衡](https://towardsdatascience.com/the-5-sampling-algorithms-every-data-scientist-need-to-know-43c7bc11d17c)数据集中，它是容易受到影响的。你可能需要引入类别权重以对少数类错误进行更多惩罚，或者在平衡数据集后使用它。

### 4\. 分类交叉熵

对数损失也可以推广到多类问题。在多类设置中，分类器必须为所有示例分配每个类别的概率。如果有N个样本属于M个类别，则*分类交叉熵*是-`ylogp` 值的总和：

![](../Images/607c37a49856b1fa9722078f40518ccb.png)

`y_ij` 为1如果样本 `i` 属于类别 `j`，否则为0。

`p_ij` 是我们分类器预测样本 `i` 属于类别 `j` 的概率。

**何时使用？**

*当分类器的输出是多类预测概率时。我们通常在神经网络中使用分类交叉熵。*一般来说，最小化分类交叉熵能为分类器带来更高的准确性。

**如何使用？**

```py

from sklearn.metrics import log_loss  
# Where y_pred is a matrix of probabilities 
# with shape *=(n_samples, n_classes)* and y_true is an array of class labels
log_loss(y_true, y_pred, eps=1e-15)

```

**注意事项：**

在[不平衡](https://towardsdatascience.com/the-5-sampling-algorithms-every-data-scientist-need-to-know-43c7bc11d17c)数据集中，它是容易受到影响的。

### 5\. AUC

AUC是ROC曲线下的面积。

***AUC ROC 表示正类的概率与负类的概率分离得有多好***

***什么是ROC曲线？***

![](../Images/fe2bc86aaaf0cf9b6c68d2768751a9ed.png)

我们从分类器中获得了概率。我们可以使用不同的阈值来绘制灵敏度（TPR）和（1-特异性）（FPR）的曲线，这样我们就得到了ROC曲线。

其中真正的正率或TPR只是我们使用算法捕获的真实比例。

`Sensitivty = TPR(True Positive Rate)= Recall = TP/(TP+FN)`

而假阳性率或FPR只是我们使用算法捕获的虚假比例。

`1- Specificity = FPR(False Positive Rate)= FP/(TN+FP)`

![图](../Images/c1344e1d0fb96baffeb0c4d68b4b55aa.png)

ROC曲线

在这里，我们可以使用ROC曲线来决定阈值。

阈值的选择还将取决于分类器的使用意图。

如果这是一个癌症分类应用，你不希望你的阈值大到0.5。即使一个患者有0.3的癌症概率，你也会将他分类为1。

否则，在信用卡额度减少的应用中，你不希望你的阈值低至0.5。你这里有些担心减少额度对客户满意度的负面影响。

**何时使用？**

AUC是**尺度不变**的。它衡量预测的排序效果，而不是它们的绝对值。例如，如果你作为市场营销人员想找到一个会对营销活动作出反应的用户列表。AUC是一个很好的指标，因为按概率排序的预测就是你创建用户列表以发送营销活动的顺序。

使用AUC的另一个好处是它像对数损失一样**分类阈值不变**。它衡量模型预测的质量，而不管选择了什么分类阈值，这与F1分数或准确度不同，后者依赖于阈值的选择。

**如何使用？**

```py

import numpy as np
from sklearn.metrics import roc_auc_score
y_true = np.array([0, 0, 1, 1])
y_scores = np.array([0.1, 0.4, 0.35, 0.8])
*print(roc_auc_score(y_true, y_scores))*

```

**注意事项**

有时我们需要从模型中获得经过良好校准的概率输出，而AUC无法提供帮助。

### 结论

在创建我们的[machine learning pipeline](https://towardsdatascience.com/6-important-steps-to-build-a-machine-learning-system-d75e3b83686)时，一个重要的步骤是对不同模型进行相互评估。评估指标的选择不当可能会对整个系统造成严重影响。

***因此，始终注意你正在预测什么，以及评估指标的选择可能如何影响/改变你的最终预测。***

此外，评估指标的选择应与业务目标充分对齐，因此有一定的主观性。你也可以自己制定评估指标。

### 持续学习

如果你想了解更多关于如何构建机器学习项目及最佳实践的信息，我推荐他在Coursera上的精彩**第三门课程**，名为“机器学习项目结构化”，该课程属于Coursera的[深度学习专业课程](https://click.linksynergy.com/deeplink?id=lVarvwc5BD0&mid=40328&murl=https%3A%2F%2Fwww.coursera.org%2Fspecializations%2Fdeep-learning)。请务必查看一下。课程讨论了陷阱和许多基本的改进模型的想法。

感谢阅读。我未来也会写更多适合初学者的文章。关注我在[**Medium**](https://medium.com/@rahul_agarwal?source=post_page---------------------------)或订阅我的[**博客**](http://eepurl.com/dbQnuX?source=post_page---------------------------)，以便获取最新内容。像往常一样，我欢迎反馈和建设性的批评，可以通过Twitter与我联系[@mlwhiz](https://twitter.com/MLWhiz?source=post_page---------------------------)

此外，附上一个小免责声明——这篇文章中可能包含一些关联链接，因为分享知识从来不是坏主意。

**个人简介：[Rahul Agarwal](https://www.linkedin.com/in/rahulagwl/)** 是WalmartLabs的高级统计分析师。关注他的Twitter [@mlwhiz](https://twitter.com/MLWhiz)。

[原文](https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226)。经许可转载。

**相关：**

+   [数据科学家应该了解的5种图算法](/2019/09/5-graph-algorithms-data-scientists-know.html)

+   [特征提取的搭便车指南](/2019/06/hitchhikers-guide-feature-extraction.html)

+   [给数据科学家的6条建议](/2019/09/advice-data-scientists.html)

### 更多相关主题

+   [更多分类问题的性能评估指标你…](https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html)

+   [机器学习评估指标：理论与概述](https://www.kdnuggets.com/machine-learning-evaluation-metrics-theory-and-overview)

+   [分类指标演练：逻辑回归与…](https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html)

+   [理解分类指标：评估模型准确性的指南](https://www.kdnuggets.com/understanding-classification-metrics-your-guide-to-assessing-model-accuracy)

+   [KDnuggets新闻，5月25日：每个数据科学家都应该了解的6个Python机器学习工具](https://www.kdnuggets.com/2022/n21.html)

+   [每个数据科学家都应该了解的6个Python机器学习工具](https://www.kdnuggets.com/2022/05/6-python-machine-learning-tools-every-data-scientist-know.html)
