- en: Introduction to Multi-Armed Bandit Problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/01/introduction-multiarmed-bandit-problems.html](https://www.kdnuggets.com/2023/01/introduction-multiarmed-bandit-problems.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A multi-armed bandit (MAB) is a machine learning framework that uses complex
    algorithms to dynamically allocate resources when presented with multiple choices.
    In other words, it’s an advanced form of A/B testing that’s most commonly used
    by data analysts, medicine researchers, and marketing specialists.
  prefs: []
  type: TYPE_NORMAL
- en: Before we delve deeper into the concept of multi-armed bandits, we need to discuss
    reinforcement learning, as well as the exploration vs. exploitation dilemma. Then,
    we can focus on various bandit solutions and practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: What is Reinforcement Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alongside supervised and unsupervised learning, reinforcement learning is one
    of the basic three paradigms of [machine learning](/2022/12/complete-machine-learning-study-roadmap.html).
    Unlike the first two archetypes we mentioned, reinforcement learning focuses on
    rewards and punishments for the agent whenever it interacts with the environment.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to Multi-Armed Bandit Problems](../Images/59e39e159c8b54a489e311cca166ad66.png)'
  prefs: []
  type: TYPE_IMG
- en: Real life examples of reinforcement learning are available to us on a daily
    basis. For instance, if your puppy chews through your computer cables, you may
    scold him and express your dissatisfaction at its actions. By doing so, you will
    teach your dog that destroying cables around the house is not a good thing to
    do. This is negative reinforcement.
  prefs: []
  type: TYPE_NORMAL
- en: Likewise, when your dog performs a trick it learned, you will give it a treat.
    In this case, you’re encouraging its behavior using positive reinforcement.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-armed bandits learn the same way.
  prefs: []
  type: TYPE_NORMAL
- en: Exploration vs. Exploitation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This almost philosophical dilemma exists in all aspects of our life. Should
    you get your coffee from that place you visited on countless occasions after work,
    or should you try the new coffee shop that just opened across the street?
  prefs: []
  type: TYPE_NORMAL
- en: If you choose to **explore**, your daily cup of coffee may turn out to be an
    unpleasant experience. However, if you go ahead and **exploit** what you already
    know and visit the familiar place, you might miss out on the best coffee you would
    have ever tasted.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to Multi-Armed Bandit Problems](../Images/9f11fe6e4f1ccfc09a45c7facb3893f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi-armed bandits tackle these issues in various fields of work and help data
    analysts determine the right course of action.
  prefs: []
  type: TYPE_NORMAL
- en: What are Multi-Armed Bandits?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A one-armed bandit is another name for a slot machine. There is no need to explain
    the meaning behind the nickname.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-armed bandit problems were named after the infamous one-armed bandits.
    However, the acknowledgment was not on the basis of bandits and robbery, but the
    predetermined chances of getting a winning result in the long term when using
    slot machines.
  prefs: []
  type: TYPE_NORMAL
- en: To better understand how this works, let’s use another real life example. This
    time, you’re deciding where to buy your vegetables. There are three choices available.
    The supermarket, local grocery store, and the nearby farmers market. Each has
    different factors to consider, such as pricing and whether the food is organic,
    but we’ll focus solely on food quality for this example.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume you purchased your groceries ten times from each location and you
    were most often satisfied with the vegetables from the farmers market. At some
    point, you will make a conscious decision to purchase your greens exclusively
    from the said market.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to Multi-Armed Bandit Problems](../Images/2319b023e02c5bd71494d52a0aa394ef.png)'
  prefs: []
  type: TYPE_IMG
- en: On rare occasions when the market is closed you may be forced to try the store
    or the supermarket again and the outcome might make you change your mind. The
    best bandit agents are programmed to do the same thing and try the less-rewarding
    options again from time to time and reinforce the data they already have.
  prefs: []
  type: TYPE_NORMAL
- en: The bandit learns in real time and adjusts its parameters accordingly. Compared
    to A/B testing, multi-armed bandits allow you to gather advanced information based
    on an extended period of time rather than making a choice after only a brief period
    of testing.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Armed Bandit Builds & Solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are infinite ways to build multi-armed bandit agents. Pure-exploration
    agents are completely random. They focus on exploration and never exploit any
    of the data they have gathered.
  prefs: []
  type: TYPE_NORMAL
- en: As the name suggests, pure-exploitation agents would always choose the best
    possible solution since they already have all the data to exploit. Being paradoxical
    by nature, this makes them possible in theory only and equally bad as the random
    agents.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we’ll focus on explaining the three most popular MAB agents that are neither
    completely random nor impossible to deploy in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Epsilon-greedy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Epsilon-greedy multi-armed bandits take care of the balance between exploration
    and exploitation by adding the exploration value (epsilon) to the formula. In
    case epsilon equals 0.3, the agent will explore random possibilities 30% of the
    time and focus on exploiting the best average outcome the other 70% of time.
  prefs: []
  type: TYPE_NORMAL
- en: A decay parameter is also included and it reduces epsilon over time. When constructing
    the agent, you may decide to remove epsilon from the equation after a certain
    amount of time or actions taken. This will cause the agent to focus solely on
    exploitation of the data it already gathered and remove random tests from the
    equation.
  prefs: []
  type: TYPE_NORMAL
- en: Upper confidence bound
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These multi-armed bandits are quite similar to the epsilon-greedy agents. However,
    the key difference between the two is an additional parameter included when building
    upper confidence bound bandits.
  prefs: []
  type: TYPE_NORMAL
- en: A variable is included in the equation that forces the bandit to focus on the
    least-explored possibilities from time to time. For example, if you have options
    A, B, C, and D, and option D has only been chosen ten times, while the rest have
    been selected hundreds of times, the bandit will purposefully select D to explore
    the outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, upper confidence bound agents sacrifice some of the resources to
    avoid a huge yet quite improbable mistake of never exploring the best possible
    outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Thompson Sampling (Bayesian)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This agent is built quite disparately from the two we explored above. Being
    by far the most advanced bandit solution on the list, an essay-length article
    would be required to explain how it works with sufficient detail. However, we
    can opt for a less intricate analysis instead.
  prefs: []
  type: TYPE_NORMAL
- en: The Thompson bandit is able to trust certain choices more or less based on how
    often they were picked in the past. For example, we have option A that the agent
    chose a hundred times with an average reward ratio of 0.71\. We also have option
    B that was chosen a total of twenty times with the same average reward radio as
    option A.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the Thompson sampling agent would go for option A a bit more often.
    This is because a higher frequency of choosing a path tends to yield lower average
    rewards. The agent assumes option A is more trustworthy and option B would have
    lower average outcomes if it was chosen more frequently.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Armed Bandit Applications in Real Life
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps the most popular example we can mention here is [Google Analytics](https://analytics.googleblog.com/2013/01/multi-armed-bandit-experiments.html).
    As part of their official documentation they explained how they used multi-armed
    bandits to explore the viability of different search results and how often these
    should be displayed to various visitors.
  prefs: []
  type: TYPE_NORMAL
- en: In 2018, [Netflix held a presentation](https://www.youtube.com/watch?v=kY-BCNHd_dM&ab_channel=DataCouncil)
    at Data Council talking about how they applied multi-armed bandits to determine
    which of their titles should be displayed more often to audiences. Marketers may
    find the demonstration quite interesting, as it explains how different factors
    such as how often titles were replayed or even paused by those watching affected
    multi-armed bandit solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the most important use of multi-armed bandits for humankind is related
    to healthcare. To this day, medical experts use various MAB builds to determine
    the optimal treatment for patients. Bandits also see various applications in clinical
    trials and the exploration of novel treatments.
  prefs: []
  type: TYPE_NORMAL
- en: Key Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multi-armed bandits have been applied successfully in multiple fields of work,
    including marketing, finance, and even health. However, much like reinforcement
    learning itself, they are severely limited by changing environments. If the circumstances
    tend to vary, MABS have to start “learning” all over again every time and become
    a less useful tool.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Alex Popovic](https://www.linkedin.com/in/alex-popovic-24283a169/)** is
    an engineering manager and a writer with a decade of experience as a team leader
    in tech and finance. He now spends time running his own consultancy agency while
    simultaneously exploring topics that he has grown to love, such as data science,
    AI, and high fantasy. You can reach Alex at hello@writeralex.com.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[More Performance Evaluation Metrics for Classification Problems You…](https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Common Data Problems (and Solutions)](https://www.kdnuggets.com/2022/02/common-data-problems-solutions.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4 Factors to Identify Machine Learning Solvable Problems](https://www.kdnuggets.com/2022/04/4-factors-identify-machine-learning-solvable-problems.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Want to Use Your Data Skills to Solve Global Problems? Here’s What…](https://www.kdnuggets.com/2022/04/jhu-want-data-skills-solve-global-problems.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Solving 5 Complex SQL Problems: Tricky Queries Explained](https://www.kdnuggets.com/2022/07/5-hardest-things-sql.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Science Projects That Can Help You Solve Real World Problems](https://www.kdnuggets.com/2022/11/data-science-projects-help-solve-real-world-problems.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
