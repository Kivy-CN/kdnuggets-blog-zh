- en: How to Explain Key Machine Learning Algorithms at an Interview
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在面试中解释关键机器学习算法
- en: 原文：[https://www.kdnuggets.com/2020/10/explain-machine-learning-algorithms-interview.html](https://www.kdnuggets.com/2020/10/explain-machine-learning-algorithms-interview.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/10/explain-machine-learning-algorithms-interview.html](https://www.kdnuggets.com/2020/10/explain-machine-learning-algorithms-interview.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '![](../Images/2b598b47e42ef191873d50e7c2b2d268.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b598b47e42ef191873d50e7c2b2d268.png)'
- en: '*Created by [katemangostar](https://www.freepik.com).*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*由[katemangostar](https://www.freepik.com)创建。*'
- en: Linear Regression
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性回归
- en: Linear Regression involves finding a ‘line of best fit’ that represents a dataset
    using the least squares method. The least squares method involves finding a linear
    equation that minimizes the sum of squared residuals. A residual is equal to the
    actual minus predicted value.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归涉及找到一个“最佳拟合线”，该线使用最小二乘法来表示数据集。最小二乘法涉及找到一个线性方程，以最小化平方残差的总和。残差等于实际值减去预测值。
- en: To give an example, the red line is a better line of best fit than the green
    line because it is closer to the points, and thus, the residuals are smaller.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，红线是比绿线更好的最佳拟合线，因为它更接近数据点，因此残差更小。
- en: '![](../Images/31722631590d909854bc4f2cbf6a9192.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31722631590d909854bc4f2cbf6a9192.png)'
- en: '*Image created by Author.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*图像由作者创建。*'
- en: Ridge Regression
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 岭回归
- en: Ridge regression, also known as L2 Regularization, is a regression technique
    that introduces a small amount of bias to reduce overfitting. It does this by
    minimizing the sum of squared residuals **plus **a penalty, where the penalty
    is equal to lambda times the slope squared. Lambda refers to the severity of the
    penalty.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归，也称为L2正则化，是一种回归技术，通过引入少量的偏差来减少过拟合。它通过最小化平方残差的总和**加上**一个惩罚来实现，其中惩罚等于λ乘以斜率的平方。λ指的是惩罚的严重程度。
- en: '![](../Images/4e2ecf37713d2b405124b71e07babdbe.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e2ecf37713d2b405124b71e07babdbe.png)'
- en: '*Image Created by Author.*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*图像由作者创建。*'
- en: Without a penalty, the line of best fit has a steeper slope, which means that
    it is more sensitive to small changes in X. By introducing a penalty, the line
    of best fit becomes less sensitive to small changes in X. This is the idea behind
    ridge regression.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有惩罚，最佳拟合线的斜率较陡，这意味着它对X的微小变化更敏感。通过引入惩罚，最佳拟合线对X的微小变化变得不那么敏感。这就是岭回归的核心思想。
- en: Lasso Regression
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Lasso回归
- en: Lasso Regression, also known as L1 Regularization, is similar to Ridge regression.
    The only difference is that the penalty is calculated with the absolute value
    of the slope instead.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso回归，也称为L1正则化，与岭回归类似。唯一的区别是惩罚是通过斜率的绝对值来计算的。
- en: '![](../Images/45c9f92fb938f06886b655ff2f640b13.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45c9f92fb938f06886b655ff2f640b13.png)'
- en: Logistic Regression
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Logistic Regression is a classification technique that also finds a ‘line of
    best fit.’ However, unlike linear regression, where the line of best fit is found
    using least squares, logistic regression finds the line (logistic curve) of best
    fit using maximum likelihood. This is done because the *y* value can only be one
    or zero. [*Check out StatQuest’s video to see how the maximum likelihood is calculated*](https://www.youtube.com/watch?v=BfKanl1aSG0).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种分类技术，也找到一个“最佳拟合线”。然而，与使用最小二乘法找到最佳拟合线的线性回归不同，逻辑回归使用最大似然法找到最佳拟合线（逻辑曲线）。这是因为*y*值只能是1或0。[*查看StatQuest的视频，了解如何计算最大似然*](https://www.youtube.com/watch?v=BfKanl1aSG0)。
- en: '![](../Images/03c8e9e7ecd9bc5782ddb7cce468f5c1.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03c8e9e7ecd9bc5782ddb7cce468f5c1.png)'
- en: '*Image Created by Author.*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*图像由作者创建。*'
- en: K-Nearest Neighbours
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K-最近邻
- en: K-Nearest Neighbours is a classification technique where a new sample is classified
    by looking at the nearest classified points, hence ‘K-nearest.’ In the example
    below, if *k=1*, then an unclassified point would be classified as a blue point.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: K-最近邻是一种分类技术，其中通过查看最近的已分类点来对新样本进行分类，因此称为“K-最近”。在下面的示例中，如果*k=1*，则未分类点将被分类为蓝色点。
- en: '![](../Images/f644d68310095ee4c2d6322640b9fcc1.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f644d68310095ee4c2d6322640b9fcc1.png)'
- en: '*Image Created by Author.*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*图像由作者创建。*'
- en: If the value of *k* is too low, then it can be subject to outliers. However,
    if it’s too high, then it may overlook classes with only a few samples.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*k*的值太低，则可能受到异常值的影响。然而，如果值太高，则可能忽略只有少量样本的类别。
- en: Naive Bayes
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: 'The Naive Bayes Classifier is a classification technique inspired by Bayes
    Theorem, which states the following equation:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器是一种受贝叶斯定理启发的分类技术，该定理表示以下方程：
- en: '![](../Images/21511ecbefdcb8581fcaa0a9ccb85cc8.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21511ecbefdcb8581fcaa0a9ccb85cc8.png)'
- en: 'Because of the naive assumption (hence the name) that variables are independent
    given the class, we can rewrite P(X|y) as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于存在天真的假设（因此得名）认为在给定类别的情况下变量是独立的，我们可以将 P(X|y) 重写如下：
- en: '![](../Images/8bc23603cfe8d5f4e118dacdde0a6945.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8bc23603cfe8d5f4e118dacdde0a6945.png)'
- en: Also, since we are solving for *y*, *P(X)* is a constant, which means that we
    can remove it from the equation and introduce a proportionality.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于我们正在求解 *y*，*P(X)* 是一个常数，这意味着我们可以将其从方程中移除并引入一个比例关系。
- en: Thus, the probability of each value of *y* is calculated as the product of the
    conditional probability of *x[n]* given *y*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*y* 的每个值的概率被计算为 *x[n]* 在给定 *y* 时的条件概率的乘积。
- en: Support Vector Machines
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Support Vector Machines are a classification technique that finds an optimal
    boundary, called the hyperplane, which is used to separate different classes.
    The hyperplane is found by maximizing the margin between the classes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机是一种分类技术，它找到一个最佳边界，称为超平面，用于分隔不同类别。通过最大化类别之间的边际来找到超平面。
- en: '![](../Images/4106158905570c86fbbd9b369ff85aa3.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4106158905570c86fbbd9b369ff85aa3.png)'
- en: '*Image Created by Author.*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*由作者创建的图像。*'
- en: Decision Trees
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树
- en: A decision tree is essentially a series of conditional statements that determine
    what path a sample takes until it reaches the bottom. They are intuitive and easy
    to build but tend not to be accurate.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树本质上是一系列条件语句，用于确定样本走的路径，直到到达底部。它们直观且易于构建，但通常不够准确。
- en: '![](../Images/faf27433664fcaed598a28d3212809ea.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/faf27433664fcaed598a28d3212809ea.png)'
- en: Random Forest
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林
- en: Random Forest is an ensemble technique, meaning that it combines several models
    into one to improve its predictive power. Specifically, it builds 1000s of smaller
    decision trees using bootstrapped datasets and random subsets of variables (also
    known as bagging). With 1000s of smaller decision trees, random forests use a
    ‘majority wins’ model to determine the value of the target variable.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一种集成技术，意味着它将多个模型组合成一个，以提高其预测能力。具体来说，它使用自助数据集和变量的随机子集（也称为自助法）构建成千上万的较小决策树。通过成千上万的较小决策树，随机森林使用“多数决定”模型来确定目标变量的值。
- en: '![](../Images/d70f24d4607fd5c44bb550e380f62865.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d70f24d4607fd5c44bb550e380f62865.png)'
- en: For example, if we created one decision tree, the third one, it would predict
    0\. But if we relied on the mode of all 4 decision trees, then the predicted value
    would be 1\. This is the power of random forests.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们创建了一个决策树，即第三棵，它将预测 0。然而，如果我们依赖所有 4 棵决策树的众数，那么预测值将是 1。这就是随机森林的强大之处。
- en: AdaBoost
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AdaBoost
- en: 'AdaBoost is a boosted algorithm that is similar to Random Forests but has a
    couple of significant differences:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 是一种提升算法，与随机森林类似，但有几个显著的不同点：
- en: Rather than a forest of trees, AdaBoost typically makes a forest of stumps (a
    stump is a tree with only one node and two leaves).
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与其说 AdaBoost 构建一片树的森林，不如说它通常构建一片桩的森林（桩是只有一个节点和两个叶子的树）。
- en: Each stump’s decision is not weighted equally in the final decision. Stumps
    with less total error (high accuracy) will have a higher say.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个桩的决策在最终决策中的权重是不等的。总错误较少（高准确性）的桩将具有更高的权重。
- en: The order in which the stumps are created is important, as each subsequent stump
    emphasizes the importance of the samples that were incorrectly classified in the
    previous stump.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 桩的创建顺序很重要，因为每个后续的桩都强调了在前一个桩中被错误分类的样本的重要性。
- en: Gradient Boost
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度提升
- en: Gradient Boost is similar to AdaBoost in the sense that it builds multiple trees
    where each tree is built off of the previous tree. Unlike AdaBoost, which builds
    stumps, Gradient Boost builds trees with usually 8 to 32 leaves.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升与 AdaBoost 相似，因为它构建多个树，每棵树都基于之前的树构建。与 AdaBoost 构建桩（stump）不同，梯度提升构建通常具有 8
    到 32 片叶子的树。
- en: More importantly, Gradient Boost differs from AdaBoost in the way that the decisions
    trees are built. Gradient Boost starts with an initial prediction, usually the
    average. Then, a decision tree is built based on the residuals of the samples.
    A new prediction is made by taking the initial prediction + a learning rate times
    the outcome of the residual tree, and the process is repeated.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，梯度提升与 AdaBoost 的主要区别在于决策树的构建方式。梯度提升从一个初始预测开始，通常是平均值。然后，根据样本的残差构建一棵决策树。通过将初始预测值加上学习率乘以残差树的结果来做出新预测，并重复这一过程。
- en: XGBoost
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XGBoost
- en: XGBoost is essentially the same thing as Gradient Boost, but the main difference
    is how the residual trees are built. With XGBoost, the residual trees are built
    by calculating similarity scores between leaves and the preceding nodes to determine
    which variables are used as the roots and the nodes.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 本质上与梯度提升（Gradient Boost）相同，但主要区别在于残差树的构建方式。使用 XGBoost 时，残差树通过计算叶子与前面节点之间的相似性分数来确定哪些变量用作根节点和节点。
- en: '[Original](https://towardsdatascience.com/how-to-explain-each-machine-learning-model-at-an-interview-499d82f91470).
    Reposted with permission.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/how-to-explain-each-machine-learning-model-at-an-interview-499d82f91470)。已获转载许可。'
- en: '**Related:**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Data Science Internship Interview Questions](https://www.kdnuggets.com/2020/08/data-science-internship-interview-questions.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学实习面试问题](https://www.kdnuggets.com/2020/08/data-science-internship-interview-questions.html)'
- en: '[How to Rock a Virtual Data Interview](https://www.kdnuggets.com/2020/05/pragmatic-rock-virtual-data-interview.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在虚拟数据面试中脱颖而出](https://www.kdnuggets.com/2020/05/pragmatic-rock-virtual-data-interview.html)'
- en: '[The Data Science Interview Study Guide](https://www.kdnuggets.com/2020/01/data-science-interview-study-guide.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学面试学习指南](https://www.kdnuggets.com/2020/01/data-science-interview-study-guide.html)'
- en: '* * *'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三推荐课程
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行IT管理'
- en: '* * *'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成为优秀数据科学家所需的5项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个初学者数据科学家应该掌握的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2021年最佳ETL工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，并通过寻找目标……](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学学习统计学的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功数据科学家的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
