["```py\n# Import libraries\nimport torch\nimport torchvision\nfrom torchvision import transforms, datasets\nImport torch.nn as nn\nImport torch.nn.functional as F\nimport torch.optim as optim\n\n# Create test and training sets\ntrain = datasets.MNIST('', train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor()\n                       ]))\n\ntest = datasets.MNIST('', train=False, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor()\n                       ]))\n\n# This section will shuffle our input/training data so that we have a randomized shuffle of our data and do not risk feeding data with a pattern. Anorther objective here is to send the data in batches. This is a good step to practice in order to make sure the neural network does not overfit our data. NNâ€™s are too prone to overfitting just because of the exorbitant amount of data that is required. For each batch size, the neural network will run a back propagation for new updated weights to try and decrease loss each time.\ntrainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\ntestset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)\n\n# Initialize our neural net\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(28*28, 64)\n        self.fc2 = nn.Linear(64, 64)\n        self.fc3 = nn.Linear(64, 64)\n        self.fc4 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        x = self.fc4(x)\n        return F.log_softmax(x, dim=1)\n\nnet = Net()\n\nprint(net)\n\n### Output:\n### Net(\n###  (fc1): Linear(in_features=784, out_features=64, bias=True)\n###  (fc2): Linear(in_features=64, out_features=64, bias=True)\n###  (fc3): Linear(in_features=64, out_features=64, bias=True)\n###  (fc4): Linear(in_features=64, out_features=10, bias=True)\n###)\n\n# Calculate our loss \nloss_function = nn.CrossEntropyLoss()\noptimizer = optim.Adam(net.parameters(), lr=0.001)\n\nfor epoch in range(5): # we use 5 epochs\n    for data in trainset:  # `data` is a batch of data\n        X, y = data  # X is the batch of features, y is the batch of targets.\n\n        net.zero_grad()  # sets gradients to 0 before calculating loss.\n\n        output = net(X.view(-1,784))  # pass in the reshaped batch (recall they are 28x28 atm, -1 is needed to show that output can be n-dimensions. This is PyTorch exclusive syntax)\n\n        loss = F.nll_loss(output, y)  # calc and grab the loss value\n\n        loss.backward()  # apply this loss backwards thru the network's parameters\n\n        optimizer.step()  # attempt to optimize weights to account for loss/gradients\n    print(loss)  \n\n### Output:\n### tensor(0.6039, grad_fn=)\n### tensor(0.1082, grad_fn=<nlllossbackward>)\n### tensor(0.0194, grad_fn=<nlllossbackward>)\n### tensor(0.4282, grad_fn=<nlllossbackward>)\n### tensor(0.0063, grad_fn=<nlllossbackward>)\n\n# Get the Accuracy\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for data in testset:\n        X, y = data\n        output = net(X.view(-1,784))\n        #print(output)\n        for idx, i in enumerate(output):\n            #print(torch.argmax(i), y[idx])\n            if torch.argmax(i) == y[idx]:\n                correct += 1\n            total += 1\n\nprint(\"Accuracy: \", round(correct/total, 3))\n\n### Output: \n### Accuracy:  0.915</nlllossbackward></nlllossbackward></nlllossbackward></nlllossbackward>\n```"]