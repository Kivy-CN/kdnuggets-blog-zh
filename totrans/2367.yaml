- en: 5 Concepts You Should Know About Gradient Descent and Cost Function
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你应该了解的梯度下降和成本函数的5个概念
- en: 原文：[https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html](https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html](https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html)
- en: '![](../Images/d76ba8086768f959395ebc4c9b2fa781.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d76ba8086768f959395ebc4c9b2fa781.png)'
- en: This picture was taken at the top of Brick Hill (Nam Long Shan, Hong Kong).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图片是在香港的Brick Hill（南朗山）顶端拍摄的。
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在的组织的IT'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Gradient descent is an iterative optimization algorithm used in machine learning
    to minimize a loss function. The loss function describes how well the model will
    perform given the current set of parameters (weights and biases), and gradient
    descent is used to find the best set of parameters. We use gradient descent to
    update the [parameters](https://ml-cheatsheet.readthedocs.io/en/latest/glossary.html#glossary-parameters) of
    our model. For example, parameters refer to coefficients in [Linear Regression](https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html) and [weights](https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#nn-weights) in
    neural networks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种迭代优化算法，用于在机器学习中最小化损失函数。损失函数描述了给定当前参数集（权重和偏置）的模型表现如何，梯度下降用于找到最佳的参数集。我们使用梯度下降来更新我们模型的 [参数](https://ml-cheatsheet.readthedocs.io/en/latest/glossary.html#glossary-parameters)。例如，参数指的是 [线性回归](https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html) 中的系数和神经网络中的 [权重](https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#nn-weights)。
- en: 'In this article, I’ll explain 5 major concepts of gradient descent and cost
    function, including:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将解释梯度下降和成本函数的5个主要概念，包括：
- en: Reason for minimising the Cost Function
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化成本函数的原因
- en: The calculation method of Gradient Descent
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降的计算方法
- en: The function of the learning rate
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率的作用
- en: Batch Gradient Descent (BGD)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量梯度下降（BGD）
- en: Stochastic gradient descent (SGD)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机梯度下降（SGD）
- en: What is the Cost Function?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是成本函数？
- en: 'The primary set-up for learning neural networks is to define a cost function
    (also known as a loss function) that measures how well the network predicts outputs
    on the test set. The goal is to then find a set of weights and biases that minimizes
    the cost. One common function that is often used is the [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error),
    which measures the difference between the actual value of *y* and the estimated
    value of *y* (the prediction). The equation of the below regression line is hθ(*x*)
    = θ + θ1*x*, which has only two parameters: weight (θ1)and bias *(*θ0*)*.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 学习神经网络的主要步骤是定义一个成本函数（也称为损失函数），它衡量网络在测试集上预测输出的效果。目标是找到一组能够最小化成本的权重和偏置。一个常用的函数是 [均方误差](https://en.wikipedia.org/wiki/Mean_squared_error)，它测量实际值*y*与估计值*y*（预测值）之间的差异。下面回归线的方程是
    hθ(*x*) = θ + θ1*x*，它只有两个参数：权重（θ1）和偏置 *（θ0）*。
- en: '![](../Images/ec4316e92785c86355c84c1d29a539e3.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec4316e92785c86355c84c1d29a539e3.png)'
- en: Minimising Cost function
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最小化成本函数
- en: The goal of any Machine Learning model is to minimize the Cost Function.
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 任何机器学习模型的目标是最小化成本函数。
- en: '![](../Images/4ad9fd044086ce7d21139e04b052bc36.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ad9fd044086ce7d21139e04b052bc36.png)'
- en: How to Minimise the Cost Function
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何最小化成本函数
- en: Our goal is to move from the mountain in the top right corner (high cost) to
    the dark blue sea in the bottom left (low cost). In order to get the lowest error
    value, we need to adjust the *weights* ‘*θ0***’** and ‘*θ1*’ to reach the smallest
    possible error. This is because the result of a lower error between the actual
    and the predicted values means the algorithm has done a good job in learning.
    Gradient descent is an efficient optimization algorithm that attempts to find
    a local or global minimum of a function.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是从右上角的山（高成本）移动到左下角的深蓝色海洋（低成本）。为了获得最低的错误值，我们需要调整*权重*‘*θ0***’**和‘*θ1*’以达到最小的误差。这是因为实际值和预测值之间的误差较低意味着算法在学习上表现良好。梯度下降是一种高效的优化算法，旨在找到函数的局部或全局最小值。
- en: Calculating gradient descent
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算梯度下降
- en: Gradient Descent runs iteratively to find the optimal values of the parameters
    corresponding to the minimum value of the given cost function, using calculus.
    Mathematically, the technique of the ‘*derivative*’ is extremely important to
    minimise the cost function because it helps get the minimum point. The derivative
    is a concept from calculus and refers to the slope of the function at a given
    point. We need to know the slope so that we know the direction (sign) to move
    the coefficient values in order to get a lower cost on the next iteration.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降利用微积分迭代地运行，以找到对应于给定成本函数最小值的参数的最优值。在数学上，‘*导数*’的技巧对最小化成本函数极为重要，因为它帮助找到最小点。导数是微积分中的一个概念，指的是函数在某一点的斜率。我们需要知道斜率，以便了解移动系数值的方向（符号），以便在下一次迭代中获得更低的成本。
- en: '![](../Images/26cdb80a0de1c698b94e989586bd6fca.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26cdb80a0de1c698b94e989586bd6fca.png)'
- en: θ1 gradually converges towards a minimum value.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: θ1 会逐渐收敛到最小值。
- en: The derivative of a function (in our case, *J(θ)*) on each parameter (in our
    case weight *θ*) tells us the sensitivity of the function with respect to that
    variable or how changing the variable impacts the function value. Gradient descent,
    therefore***, ***enables the learning process to make corrective updates to the
    learned estimates that move the model toward an optimal combination of parameters
    (*θ*). The cost is calculated for a machine learning algorithm over the entire
    training dataset for each iteration of the gradient descent algorithm. In Gradient
    Descent, one iteration of the algorithm is called one batch, which denotes the
    total number of samples from a dataset that is used for calculating the gradient
    for each iteration.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个函数（在我们这个例子中是*J(θ)*)关于每个参数（在我们这个例子中是权重*θ*）的导数告诉我们函数对该变量的敏感度，或变量的变化如何影响函数值。因此，梯度下降***使得学习过程能够对学习到的估计值进行修正更新，将模型向最优的参数组合（*θ*）靠拢。成本是在每次梯度下降算法迭代中，为整个训练数据集计算的。在梯度下降中，算法的一次迭代称为一个批次，表示用于计算每次迭代的梯度的数据集中的样本总数。
- en: The step of the derivation
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导数的步骤
- en: It would be better if you have some basic understanding of calculus because
    the technique of the partial derivative and the chain rule is being applied in
    this case.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对微积分有一些基本了解会更好，因为在这个过程中应用了偏导数和链式法则。
- en: '![](../Images/eaec5633766b8bcc9dc195f0f5f5a851.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eaec5633766b8bcc9dc195f0f5f5a851.png)'
- en: To solve for the gradient, we iterate through our data points using our new *weight
    ‘θ0**’*** and *bias ‘θ1’* values and compute the partial derivatives. This new
    gradient tells us the slope of our cost function at our current position (current
    parameter values) and the direction we should move to update our parameters. The
    size of our update is controlled by the learning rate.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解析梯度，我们使用新的*权重 ‘θ0**’***和*偏差 ‘θ1’*值对数据点进行迭代，并计算偏导数。这一新的梯度告诉我们在当前位置（当前参数值）函数的斜率，以及我们应该移动的方向来更新参数。更新的大小由学习率控制。
- en: Learning rate (α)
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率（α）
- en: '![](../Images/81e801c6f317ade5295a4bb41ea75969.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81e801c6f317ade5295a4bb41ea75969.png)'
- en: Note that we used ‘:=’ to denote an assignment or an update.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用‘:=’来表示赋值或更新。
- en: The size of these steps is called the **learning rate (α)** that gives us some
    additional control over how large of steps we make. With a large learning rate,
    we can cover more ground each step, but we risk overshooting the lowest point
    since the slope of the hill is constantly changing. With a very low learning rate,
    we can confidently move in the direction of the negative gradient since we are
    recalculating it so frequently. A low learning rate is more precise, but calculating
    the gradient is time-consuming, so it will take us a very long time to get to
    the bottom. The most commonly used rates are: *0.001, 0.003, 0.01, 0.03, 0.1,
    0.3*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s discuss the three variants of gradient descent algorithm. The main
    difference between them is the amount of data we use when computing the gradients
    for each learning step. The trade-off between them is the accuracy of the gradient
    versus the time complexity to perform each parameter’s update (learning step).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic gradient descent (SGD)
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: However, there is a disadvantage of applying a typical Gradient Descent optimization
    technique in our dataset. It becomes computationally very expensive to perform
    because we have to use all of the one million samples for completing one iteration,
    and it has to be done for every iteration until the minimum point is reached.
    This problem can be solved by Stochastic Gradient Descent.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: The word ‘*stochastic’* means a system or a process that is linked with a random
    probability. Stochastic gradient descent uses this idea to speed up the process
    of performing gradient descent. Hence, unlike the typical Gradient Descent optimization,
    instead of using the whole data set for each iteration, we are able to use the
    cost gradient of only 1 example at each iteration (details are shown in the graph
    below). Even though using the whole dataset is really useful for getting to the
    minima in a less noisy or less random manner, the problem arises when our datasets
    get really large.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1493f0189dd890f78d2031c44982d392.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: The two main differences are that the stochastic gradient descent method helps
    us avoid the problem where we find those local extremities or local minimums rather
    than the overall global minimum.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, the stochastic gradient descent method is doing one iteration
    or one row at a time, and therefore, the fluctuations are much higher than the
    batch gradient descent.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Three variants of gradient descent algorithm
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch gradient descent (BGD): **calculate the error for each example in the
    training dataset, but only updates the model after all training examples have
    been evaluated.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic gradient descent (SGD)**: calculate the error and updates the
    model for *each example*in the training dataset.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mini-Batch gradient descent**: split the training dataset into small batches
    that are used to calculate model error and updated model coefficients. (the most
    common implementation of gradient descent used in the field of deep learning)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mini-Batch gradient descent can find **a balance between the robustness of **SGD** and
    the efficiency of **BGD.**'
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**迷你批量梯度下降可以在**SGD**的鲁棒性和**BGD**的效率之间找到平衡。**'
- en: Summary
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'After reading this blog, you now should a better understanding of the 5 concepts
    of Gradient Descent and Cost Function:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完这篇博客后，你现在应该对梯度下降和成本函数的5个概念有了更好的理解：
- en: What is the Cost Function is and how to minimise it?
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本函数是什么，如何最小化它？
- en: How to calculate gradient descent?
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何计算梯度下降？
- en: What is the learning rate?
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是学习率？
- en: What is Batch Gradient Descent (BGD)?
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是批量梯度下降（BGD）？
- en: Why is Stochastic Gradient Descent (SGD) important in machine learning?
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么随机梯度下降（SGD）在机器学习中如此重要？
- en: More On This Topic
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Back To Basics, Part Dos: Gradient Descent](https://www.kdnuggets.com/2023/03/back-basics-part-dos-gradient-descent.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[回到基础，第Dos部分：梯度下降](https://www.kdnuggets.com/2023/03/back-basics-part-dos-gradient-descent.html)'
- en: '[Gradient Descent: The Mountain Trekker''s Guide to Optimization with…](https://www.kdnuggets.com/gradient-descent-the-mountain-trekker-guide-to-optimization-with-mathematics)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[梯度下降：优化的山地旅行者指南…](https://www.kdnuggets.com/gradient-descent-the-mountain-trekker-guide-to-optimization-with-mathematics)'
- en: '[Concepts You Should Know Before Getting Into Transformers](https://www.kdnuggets.com/2023/01/concepts-know-getting-transformer.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在深入了解Transformer之前你应该知道的概念](https://www.kdnuggets.com/2023/01/concepts-know-getting-transformer.html)'
- en: '[7 SQL Concepts You Should Know For Data Science](https://www.kdnuggets.com/2022/11/7-sql-concepts-needed-data-science.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中你应该知道的7个SQL概念](https://www.kdnuggets.com/2022/11/7-sql-concepts-needed-data-science.html)'
- en: '[KDnuggets News, April 13: Python Libraries Data Scientists Should…](https://www.kdnuggets.com/2022/n15.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets新闻，4月13日：数据科学家应该知道的Python库…](https://www.kdnuggets.com/2022/n15.html)'
- en: '[Multi-label NLP: An Analysis of Class Imbalance and Loss Function…](https://www.kdnuggets.com/2023/03/multilabel-nlp-analysis-class-imbalance-loss-function-approaches.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多标签NLP：类别不平衡和损失函数的分析…](https://www.kdnuggets.com/2023/03/multilabel-nlp-analysis-class-imbalance-loss-function-approaches.html)'
