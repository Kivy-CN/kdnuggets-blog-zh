- en: 'Regularization in Logistic Regression: Better Fit and Better Generalization?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归中的正则化：更好的拟合和更好的泛化？
- en: 原文：[https://www.kdnuggets.com/2016/06/regularization-logistic-regression.html](https://www.kdnuggets.com/2016/06/regularization-logistic-regression.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/06/regularization-logistic-regression.html](https://www.kdnuggets.com/2016/06/regularization-logistic-regression.html)
- en: Regularization does NOT improve the performance on the data set that the algorithm
    used to learn the model parameters (feature weights). However, it can improve
    the generalization performance, i.e., the performance on new, unseen data, which
    is exactly what we want.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化不会提高模型用于学习参数（特征权重）的数据集上的性能。然而，它可以提高泛化性能，即在新的、未见过的数据上的表现，这正是我们想要的。
- en: In intuitive terms, we can think of regularization as a penalty against complexity.
    Increasing the regularization strength penalizes "large" weight coefficients --
    our goal is to prevent that our model picks up "peculiarities," "noise," or "imagines
    a pattern where there is none."
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从直观上讲，我们可以把正则化看作是对复杂度的惩罚。增加正则化强度会惩罚“较大”的权重系数——我们的目标是防止模型捕捉到“特异性”、“噪声”或“在不存在模式的地方假设模式”。
- en: Again, we don't want the model to memorize the training dataset, we want a model
    that generalizes well to new, unseen data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们不希望模型记住训练数据集，我们希望模型对新的、未见过的数据有良好的泛化能力。
- en: In more specific terms, we can think of regularization as adding (or increasing
    the) bias if our model suffers from (high) variance (i.e., it overfits the training
    data). On the other hand, too much bias will result in underfitting (a characteristic
    indicator of high bias is that the model shows a "bad" performance for both the
    training and test dataset). We know that our goal in an unregularized model is
    to minimize the cost function, i.e., we want to find the feature weights that
    correspond to the global cost minimum (remember that the logistic cost function
    is convex).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体来说，我们可以把正则化看作是增加（或提高）偏差的过程，如果我们的模型存在（高）方差（即，它过拟合了训练数据）。另一方面，过多的偏差会导致欠拟合（高偏差的一个特征指标是模型在训练和测试数据集上的表现都很“差”）。我们知道，在未正则化的模型中，我们的目标是最小化成本函数，即，我们想找到与全局成本最小值对应的特征权重（记住，逻辑回归的成本函数是凸的）。
- en: '[![](../Images/246f23a1b7db46b1291182f90099e8e3.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/regularized-logistic-regression-performance/unregularized.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/246f23a1b7db46b1291182f90099e8e3.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/regularized-logistic-regression-performance/unregularized.png)'
- en: Now, if we regularize the cost function (e.g., via L2 regularization), we add
    an additional to our cost function (J) that increases as the value of your parameter
    weights (w) increase; keep in mind that the regularization we add a new hyperparameter,
    lambda, to control the regularization strength.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们对成本函数进行正则化（例如，通过L2正则化），我们在成本函数（J）中增加一个额外的项，该项随着参数权重（w）值的增加而增加；请记住，我们添加的正则化还引入了一个新的超参数lambda，用于控制正则化强度。
- en: '[![](../Images/00455a306f38c40c05ede9be3aa95de4.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/regularized-logistic-regression-performance/l2-term.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/00455a306f38c40c05ede9be3aa95de4.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/regularized-logistic-regression-performance/l2-term.png)'
- en: Therefore, our new problem is to minimize the cost function given this added
    constraint.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的新问题是最小化成本函数，给定这个附加的约束。
- en: '[![](../Images/c60871e2ad31b45d65061c2e7dbb97cb.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/regularized-logistic-regression-performance/regularized.png)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/c60871e2ad31b45d65061c2e7dbb97cb.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/regularized-logistic-regression-performance/regularized.png)'
- en: 'Intuitively, we can think of the "sphere" at the coordinate center in the figure
    above as our "budget." Now, our objective is still the same: we want to minimize
    the cost function. However, we are now constraint by the regularization term;
    we want to get as close as possible to the global minimum while staying within
    our "budget" (i.e., the sphere).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从直观上讲，我们可以把图中坐标中心的“球体”看作是我们的“预算”。现在，我们的目标仍然是相同的：我们想要最小化成本函数。然而，我们现在受到正则化项的约束；我们希望在保持在我们的“预算”（即球体）内的同时尽可能接近全局最小值。
- en: '**Bio: [Sebastian Raschka](https://twitter.com/rasbt)** is a ''Data Scientist''
    and Machine Learning enthusiast with a big passion for Python & open source. Author
    of ''[Python Machine Learning](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning)''.
    Michigan State University.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介: [塞巴斯蒂安·拉施卡](https://twitter.com/rasbt)** 是一位‘数据科学家’和机器学习爱好者，对 Python
    和开源有着极大的热情。著作有《[Python 机器学习](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning)》。密歇根州立大学。'
- en: '[Original](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/regularized-logistic-regression-performance.md).
    Reposted with permission.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[原始文档](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/regularized-logistic-regression-performance.md)。经许可转载。'
- en: '**Related:**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关:**'
- en: '[When Does Deep Learning Work Better Than SVMs or Random Forests?](/2016/04/deep-learning-vs-svm-random-forest.html)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习何时优于 SVM 或随机森林？](/2016/04/deep-learning-vs-svm-random-forest.html)'
- en: '[The Development of Classification as a Learning Machine](/2016/04/development-classification-learning-machine.html)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分类作为学习机器的发展](/2016/04/development-classification-learning-machine.html)'
- en: '[Why Implement Machine Learning Algorithms From Scratch?](/2016/05/implement-machine-learning-algorithms-scratch.html)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么从头实现机器学习算法？](/2016/05/implement-machine-learning-algorithms-scratch.html)'
- en: More On This Topic
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Comparing Linear and Logistic Regression](https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[比较线性回归与逻辑回归](https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html)'
- en: '[Classification Metrics Walkthrough: Logistic Regression with…](https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分类指标指南: 逻辑回归与…](https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html)'
- en: '[Tailor ChatGPT to Fit Your Needs with Custom Instructions](https://www.kdnuggets.com/2023/08/tailor-chatgpt-fit-needs-custom-instructions.html)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用自定义指令调整 ChatGPT 以适应您的需求](https://www.kdnuggets.com/2023/08/tailor-chatgpt-fit-needs-custom-instructions.html)'
- en: '[Linear vs Logistic Regression: A Succinct Explanation](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性回归与逻辑回归: 简洁的解释](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
- en: '[KDnuggets News 22:n12, March 23: Best Data Science Books for…](https://www.kdnuggets.com/2022/n12.html)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻 22:n12, 3月 23: 最佳数据科学书籍…](https://www.kdnuggets.com/2022/n12.html)'
- en: '[Logistic Regression for Classification](https://www.kdnuggets.com/2022/04/logistic-regression-classification.html)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分类的逻辑回归](https://www.kdnuggets.com/2022/04/logistic-regression-classification.html)'
