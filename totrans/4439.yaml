- en: 'Recurrent Neural Networks (RNN): Deep Learning for Sequential Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）：用于序列数据的深度学习
- en: 原文：[https://www.kdnuggets.com/2020/07/rnn-deep-learning-sequential-data.html](https://www.kdnuggets.com/2020/07/rnn-deep-learning-sequential-data.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/07/rnn-deep-learning-sequential-data.html](https://www.kdnuggets.com/2020/07/rnn-deep-learning-sequential-data.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: Recurrent Neural Networks (RNN) are a class of Artificial Neural Networks that
    can process a sequence of inputs in [deep learning](https://blog.exxactcorp.com/category/deep-learning/) and
    retain its state while processing the next sequence of inputs. Traditional neural
    networks will process an input and move onto the next one disregarding its sequence.
    Data such as time series have a sequential order that needs to be followed in
    order to understand. Traditional feed-forward networks cannot comprehend this
    as each input is assumed to be independent of each other whereas in a time series
    setting each input is dependent on the previous input.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）是一类人工神经网络，可以在[深度学习](https://blog.exxactcorp.com/category/deep-learning/)中处理一系列输入，并在处理下一序列的输入时保留其状态。传统神经网络会处理一个输入并移动到下一个，忽略其序列。诸如时间序列的数据具有需要遵循的顺序以便理解。传统的前馈网络无法理解这一点，因为每个输入被假定为彼此独立，而在时间序列设置中，每个输入依赖于先前的输入。
- en: '![Figure](../Images/ebdc0da1eb3f72438224bb2b01b4b3f4.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/ebdc0da1eb3f72438224bb2b01b4b3f4.png)'
- en: '[Illustration 1: source](https://www.dummies.com/programming/big-data/data-science/deep-learning-and-recurrent-neural-networks/)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[插图 1: 来源](https://www.dummies.com/programming/big-data/data-science/deep-learning-and-recurrent-neural-networks/)'
- en: '* * *'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT'
- en: '* * *'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In Illustration 1 we see that the neural network (hidden state) *A *takes an *x**t *and
    outputs a value *h**t. *The loop shows how the information is being passed from
    one step to the next. The inputs are the individual letters of ‘JAZZ’ and each
    one is passed on to the network A in the same order it is written (i.e. sequentially).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在插图 1 中，我们看到神经网络（隐藏状态）*A*接收一个*x**t*并输出一个值*h**t*。循环展示了信息是如何从一个步骤传递到下一个步骤的。输入是“JAZZ”的各个字母，每个字母按顺序传递给网络
    A（即顺序处理）。
- en: 'Recurrent Neural Networks can be used for a number of ways such as:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络可以用于多种方式，例如：
- en: Detecting the next word/letter
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测下一个单词/字母
- en: Forecasting financial asset prices in a temporal space
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在时间空间中预测金融资产价格
- en: Action modeling in sports (predict the next action in a sporting event like
    soccer, football, tennis etc)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 体育中的动作建模（预测体育赛事中的下一动作，例如足球、橄榄球、网球等）
- en: Music composition
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音乐创作
- en: Image generation
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像生成
- en: '**RNN vs Autoregressive Models**'
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**RNN 与自回归模型**'
- en: An autoregressive model is when a value from data with a temporal dimension
    are regressed on previous values up to a certain point specified by the user.
    An RNN works the same way but the obvious difference in comparison is that the
    RNN looks at all the data (i.e. it does not require a specific time period to
    be specified by the user.)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归模型是指将具有时间维度的数据的值回归到先前的值，直到用户指定的某个点。RNN的工作方式相同，但明显的区别在于RNN查看所有数据（即不需要用户指定特定的时间段）。
- en: '***Y******t ******= β******0****** + β******1******y******t-1****** + Ɛ******t***'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '***Y******t ******= β******0****** + β******1******y******t-1****** + Ɛ******t***'
- en: The above AR model is an order 1 AR(1) model that takes the immediate preceding
    value to predict the next time period’s value (yt). As this is a linear model,
    it requires certain assumptions of linear regression to hold–especially due to
    the linearity assumption between the dependent and independent variables. In this
    case, ***Y******t ***and ***y******t-1 ***must have a linear relationship. In
    addition there are other checks such as autocorrelation that have to be checked
    to determine the adequate order to forecast ***Y******t******.***
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: An RNN will not require linearity or model order checking. It can automatically
    check the whole dataset to try and predict the next sequence. As demonstrated
    in the image below, a neural network consists of 3 hidden layers with equal weights,
    biases and activation functions and made to predict the output.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/17cf506bfd51128920b1b80d5e555bb9.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: '[Source](https://hackernoon.com/rnn-or-recurrent-neural-network-for-noobs-a9afbb00e860)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: These hidden layers can then be merged to create a single recurrent hidden layer.
    A recurrent neuron now stores all the previous step input and merges that information
    with the current step input.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/13f5e5872ab6f20539c8e81982e67afb.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: '[Source](https://hackernoon.com/rnn-or-recurrent-neural-network-for-noobs-a9afbb00e860)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '****Advantages of an RNN****'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: It can model non-linear temporal/sequential relationships.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No need to specify lags to predict the next value in comparison to and autoregressive
    process.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '****Disadvantages of an RNN****'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Vanishing Gradient Problem
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not suited for predicting long horizons
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vanishing Gradient Problem**'
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As more layers containing activation functions are added, the gradient of the
    loss function approaches zero. The gradient descent algorithm finds the global
    minimum of the cost function of the network. Shallow networks shouldn’t be affected
    by a too small gradient but as the network gets bigger with more hidden layers
    it can cause the gradient to be too small for model training.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Gradients of neural networks are found using the backpropagation algorithm whereby
    you find the derivatives of the network. Using the chain rule, derivatives of
    each layer are found by multiplying down the network. This is where the problem
    lies. Using an activation function like the sigmoid function, the gradient has
    a chance of decreasing as the number of hidden layers increase.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: This issue can cause terrible results after compiling the model. The simple
    solution to this has been to use Long-Short Term Memory models with a ReLU activation
    function.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '**Long-Short Term Memory Models**'
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Long-Short Term Memory Networks are a special type of Recurrent Neural Networks
    that are capable of handling long term dependencies without being affected by
    an unstable gradient.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/b60656880e1eccebad51931e5ae7c94b.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: '[Source](https://www.superdatascience.com/blogs/recurrent-neural-networks-rnn-lstm-variation/)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: The above diagram is a typical RNN except that the repeating module contains
    extra layers that distinguishes itself from an RNN.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图是一个典型的 RNN，只是重复模块包含额外的层，使其与 RNN 区别开来。
- en: The differentiation here is the horizontal line called the ‘cell state’ that
    acts as a conveyor belt of information. The LSTM will remove or add information
    to the cell state by using the 3 gates as illustrated above. The gates are composed
    of a sigmoid function and a point-wise multiplication operation that outputs between
    1 and 0 to describe how much of each component to let through the cell state.
    A value of 1 means to let all the information through while 0 means to completely
    disregard it.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的区分是水平线称为“单元状态”，它充当信息的传送带。LSTM 将通过使用如上所示的 3 个门来删除或添加单元状态中的信息。这些门由一个 sigmoid
    函数和一个逐点乘法操作组成，输出值在 1 和 0 之间，用于描述允许单元状态通过多少成分。值为 1 表示允许所有信息通过，而 0 则表示完全忽略它。
- en: '**The 3 gates are:**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**3 个门是：**'
- en: '**1\. Input gate **'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 输入门**'
- en: This gate is used to discover which value will be used to modify the memory
    using the sigmoid function (by assigning a value between 0 and 1) followed by
    a tanh function which gives a weightage to the value between -1 to 1.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个门用于发现哪些值将用于修改记忆，通过使用 sigmoid 函数（分配介于 0 和 1 之间的值），然后使用 tanh 函数为值赋予 -1 到 1 之间的权重。
- en: '**2\. Forget gate **'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 忘记门**'
- en: The forget gate decides which value to disregard using a sigmoid function by
    using the previous state (*h**t-1*) and the input (*x**t*) by assigning a value
    between 0 and 1 for each value in *c**t-1*.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忘记门使用 sigmoid 函数来决定哪些值被忽略，通过使用前一个状态 (*h**t-1*) 和输入 (*x**t*)，为 *c**t-1* 中的每个值分配一个介于
    0 和 1 之间的值。
- en: '**3\. Output gate **'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 输出门**'
- en: The input of this block is used to decide the output by using a sigmoid function
    to assign a value between 0 and 1 followed by multiplying by a tanh function to
    decide its level of importance by assigning a value between -1 and 1.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该块的输入用于决定输出，通过使用 sigmoid 函数分配介于 0 和 1 之间的值，然后乘以 tanh 函数来决定其重要性级别，分配一个介于 -1 和
    1 之间的值。
- en: '**Coding an RNN – LSTM**'
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**编写 RNN – LSTM**'
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[Original](https://blog.exxactcorp.com/recurrent-neural-networks-rnn-deep-learning-for-sequential-data/).
    Reposted with permission.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://blog.exxactcorp.com/recurrent-neural-networks-rnn-deep-learning-for-sequential-data/)。经许可转载。'
- en: '**Related:**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[LSTM for time series prediction](/2020/04/lstm-time-series-prediction.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LSTM 用于时间序列预测](/2020/04/lstm-time-series-prediction.html)'
- en: '[Deep Learning for NLP: ANNs, RNNs and LSTMs explained!](/2019/08/deep-learning-nlp-explained.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NLP 中的深度学习：ANNs、RNNs 和 LSTMs 解析！](/2019/08/deep-learning-nlp-explained.html)'
- en: '[Understanding Backpropagation as Applied to LSTM](/2019/05/understanding-backpropagation-applied-lstm.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解应用于 LSTM 的反向传播](/2019/05/understanding-backpropagation-applied-lstm.html)'
- en: More On This Topic
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Neural Networks and Deep Learning: A Textbook (2nd Edition)](https://www.kdnuggets.com/2023/07/aggarwal-neural-networks-deep-learning-textbook-2nd-edition.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[神经网络与深度学习：教科书（第2版）](https://www.kdnuggets.com/2023/07/aggarwal-neural-networks-deep-learning-textbook-2nd-edition.html)'
- en: '[Deep Neural Networks Don''t Lead Us Towards AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度神经网络不会引导我们走向AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
- en: '[Explainable Forecasting and Nowcasting with State-of-the-art Deep…](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用最先进的深度学习进行可解释的预测和即时预测](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
- en: '[10 Simple Things to Try Before Neural Networks](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在神经网络之前尝试的 10 件简单事物](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 PyTorch 解释性神经网络](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
- en: '[Image Classification with Convolutional Neural Networks (CNNs)](https://www.kdnuggets.com/2022/05/image-classification-convolutional-neural-networks-cnns.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用卷积神经网络（CNNs）的图像分类](https://www.kdnuggets.com/2022/05/image-classification-convolutional-neural-networks-cnns.html)'
