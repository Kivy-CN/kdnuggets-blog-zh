- en: Audio Data Analysis Using Deep Learning with Python (Part 1)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/02/audio-data-analysis-deep-learning-python-part-1.html](https://www.kdnuggets.com/2020/02/audio-data-analysis-deep-learning-python-part-1.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While much of the literature and buzz on deep learning concerns computer vision
    and natural language processing(NLP), audio analysis — a field that includes automatic
    speech recognition(ASR), digital signal processing, and music classification,
    tagging, and generation — is a growing subdomain of deep learning applications.
    Some of the most popular and widespread machine learning systems, virtual assistants
    Alexa, Siri, and Google Home, are largely products built atop models that can
    extract information from audio signals.
  prefs: []
  type: TYPE_NORMAL
- en: Audio data analysis is about analyzing and understanding audio signals captured
    by digital devices, with numerous applications in the enterprise, healthcare,
    productivity, and smart cities. Applications include customer satisfaction analysis
    from customer support calls, media content analysis and retrieval, medical diagnostic
    aids and patient monitoring, assistive technologies for people with hearing impairments,
    and audio analysis for public safety.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In the first part of this article series, we will talk about all you need to
    know before getting started with the audio data analysis and extract necessary
    features from a sound/audio file. We will also build an Artificial Neural Network(ANN)
    for the music genre classification. In the second part, we will accomplish the
    same by creating the Convolutional Neural Network and will compare their accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Audio file overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications of Audio Processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audio Processing with Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spectrogram
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extraction from Audio signal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Genre classification using Artificial Neural Networks(ANN).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audio file overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The sound excerpts are digital audio files in .wav format. Sound waves are digitized
    by sampling them at discrete intervals known as the sampling rate (typically 44.1kHz
    for CD-quality audio meaning samples are taken 44,100 times per second).
  prefs: []
  type: TYPE_NORMAL
- en: Each sample is the amplitude of the wave at a particular time interval, where
    the bit depth determines how detailed the sample will be also known as the dynamic
    range of the signal (typically 16bit which means a sample can range from 65,536
    amplitude values).
  prefs: []
  type: TYPE_NORMAL
- en: '**What is Sampling and Sampling frequency?**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**In signal processing, sampling is the reduction of a continuous signal into
    a series of discrete values. The sampling frequency or rate is the number of samples
    taken over some fixed amount of time. A high sampling frequency results in less
    information loss but higher computational expense, and low sampling frequencies
    have higher information loss but are fast and cheap to compute.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Figure](../Images/0ae42a89ddb41474f12e1a201e47e5fc.png)'
  prefs: []
  type: TYPE_IMG
- en: A sound wave, in red, represented digitally, in blue (after sampling and 4-bit
    quantisation), with the resulting array shown on the right. Original © Aquegg
    | Wikimedia Commons
  prefs: []
  type: TYPE_NORMAL
- en: Applications of Audio Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'What are the potential applications of audio processing? Here I would list
    a few of them:'
  prefs: []
  type: TYPE_NORMAL
- en: Indexing music collections according to their audio features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommending music for radio channels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarity search for audio files (aka Shazam)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech processing and synthesis — generating artificial voice for conversational
    agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audio Data Handling using Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sound is represented in the form of an **audio** signal having parameters such
    as frequency, bandwidth, decibel, etc. A typical audio signal can be expressed
    as a function of Amplitude and Time.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/ab5d5b6e3239dee7c36f1c2ddf06f771.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Source](https://commons.wikimedia.org/wiki/File:FFT-Time-Frequency-View.png)'
  prefs: []
  type: TYPE_NORMAL
- en: There are devices built that help you catch these sounds and represent it in
    a computer-readable format. Examples of these formats are
  prefs: []
  type: TYPE_NORMAL
- en: wav (Waveform Audio File) format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mp3 (MPEG-1 Audio Layer 3) format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WMA (Windows Media Audio) format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A typical audio processing process involves the extraction of acoustics features
    relevant to the task at hand, followed by decision-making schemes that involve
    detection, classification, and knowledge fusion. Thankfully we have some useful
    python libraries which make this task easier.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python Audio Libraries:**'
  prefs: []
  type: TYPE_NORMAL
- en: Python has some great libraries for audio processing like Librosa and PyAudio.There
    are also built-in modules for some basic audio functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will mainly use two libraries for audio acquisition and playback:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Librosa**'
  prefs: []
  type: TYPE_NORMAL
- en: It is a Python module to analyze audio signals in general but geared more towards
    music. It includes the nuts and bolts to build a MIR(Music information retrieval)
    system. It has been very well [documented](https://librosa.github.io/librosa/) along
    with a lot of examples and tutorials.
  prefs: []
  type: TYPE_NORMAL
- en: '**Installation:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To fuel more audio-decoding power, you can install *ffmpeg* which ships with
    many audio decoders.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. IPython.display.Audio**'
  prefs: []
  type: TYPE_NORMAL
- en: '`[**IPython.display.Audio**](https://ipython.org/ipython-doc/stable/api/generated/IPython.display.html#IPython.display.Audio)` lets
    you play audio directly in a jupyter notebook.'
  prefs: []
  type: TYPE_NORMAL
- en: I have uploaded a random audio file on the below page. Let us now load the file
    in your jupyter console.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Vocaroo | Online voice recorder](https://voca.ro/iMPozIbzB8T)**'
  prefs: []
  type: TYPE_NORMAL
- en: Vocaroo is a quick and easy way to share voice messages over the interwebs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Loading an audio file:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This returns an audio time series as a numpy array with a default sampling rate(sr)
    of 22KHZ mono. We can change this behavior by resampling at 44.1KHz.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: or to disable resampling.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The sample** rate** is the number of samples of audio carried per second, measured
    in Hz or kHz.
  prefs: []
  type: TYPE_NORMAL
- en: '**Playing Audio:**'
  prefs: []
  type: TYPE_NORMAL
- en: Using,`**IPython.display.Audio**` you can play the audio in your jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns an audio widget:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ebc840c4cc68c9e19cfcbf4919a8526.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Visualizing Audio:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can plot the audio array using `[**librosa.display.waveplot**](https://librosa.github.io/librosa/generated/librosa.display.waveplot.html#librosa.display.waveplot)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f2266fe0df29e0c49b39b1bfd897d8ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we have the plot of the amplitude envelope of a waveform.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/d67fa2958bec711253eb0bddc6f53994.png)'
  prefs: []
  type: TYPE_IMG
- en: '[What is Amplitude, Wavelength, and Phase in a signal?](https://gfycat.com/ickyfilthybobolink)'
  prefs: []
  type: TYPE_NORMAL
- en: Spectrogram
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A spectrogram is a visual way of representing the signal strength, or “**loudness**”,
    of a signal over time at various frequencies present in a particular waveform.
    Not only can one see whether there is more or less energy at, for example, 2 Hz
    vs 10 Hz, but one can also see how energy levels vary over time.
  prefs: []
  type: TYPE_NORMAL
- en: A spectrogram is usually depicted as a [heat map](https://en.wikipedia.org/wiki/Heat_map),
    i.e., as an image with the intensity shown by varying the color or brightness.
  prefs: []
  type: TYPE_NORMAL
- en: We can display a spectrogram using. `[**librosa.display.specshow**](https://librosa.github.io/librosa/generated/librosa.display.specshow.html)**.**`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`.stft()` converts data into short term Fourier transform. [STFT](https://www.youtube.com/watch?v=g1_wcbGUcDY) converts
    signals such that we can know the amplitude of the given frequency at a given
    time. Using STFT we can determine the amplitude of various frequencies playing
    at a given time of an audio signal. `.specshow` is used to display a spectrogram.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89fec857ffdd44f2d7965ca58289f224.png)'
  prefs: []
  type: TYPE_IMG
- en: The vertical axis shows frequencies (from 0 to 10kHz), and the horizontal axis
    shows the time of the clip. Since we see that all action is taking place at the
    bottom of the spectrum, we can convert the frequency axis to a logarithmic one.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c787dbd0b0a36b2375823227de8a1e86.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Create an Audio Signal:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Feature extraction from Audio signal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Every audio signal consists of many features. However, we must extract the characteristics
    that are relevant to the problem we are trying to solve. The process of extracting
    features to use them for analysis is called feature extraction. Let us study a
    few of the features in detail.
  prefs: []
  type: TYPE_NORMAL
- en: The **spectral features** (frequency-based **features**), which are obtained
    by converting the time-based signal into the frequency domain using the Fourier
    Transform, like fundamental frequency, frequency components, **spectral** centroid, **spectral** flux, **spectral** density, **spectral** roll-off,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Spectral Centroid**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **spectral centroid** indicates at which frequency the energy of a spectrum
    is centered upon or in other words It indicates where the ” center of mass” for
    a sound is located. This is like a weighted mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4ceac912fc48f96751392968eba12a0.png)'
  prefs: []
  type: TYPE_IMG
- en: where S(k) is the spectral magnitude at frequency bin k, f(k) is the frequency
    at bin k.
  prefs: []
  type: TYPE_NORMAL
- en: '`[**librosa.feature.spectral_centroid**](https://librosa.github.io/librosa/generated/librosa.feature.spectral_centroid.html#librosa.feature.spectral_centroid)` computes
    the spectral centroid for each frame in a signal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`.spectral_centroid` will return an array with columns equal to a number of
    frames present in your sample.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a685695a87a8eb9d92b23d9923c0954.png)'
  prefs: []
  type: TYPE_IMG
- en: There is a rise in the spectral centroid in the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Spectral Rolloff**'
  prefs: []
  type: TYPE_NORMAL
- en: It is a measure of the shape of the signal. It represents the frequency at which
    high frequencies decline to 0\. To obtain it, we have to calculate the fraction
    of bins in the power spectrum where 85% of its power is at lower frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: '`[**librosa.feature.spectral_rolloff**](https://librosa.github.io/librosa/generated/librosa.feature.spectral_rolloff.html#librosa.feature.spectral_rolloff)` computes
    the rolloff frequency for each frame in a signal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e97b4739cade77bd6cede12340dfb1a1.png)'
  prefs: []
  type: TYPE_IMG
- en: '**3\. Spectral Bandwidth**'
  prefs: []
  type: TYPE_NORMAL
- en: The spectral bandwidth is defined as the width of the band of light at one-half
    the peak maximum (or full width at half maximum [FWHM]) and is represented by
    the two vertical red lines and λSB on the wavelength axis.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c030a5676e009ed654935cebf7bb7958.png)'
  prefs: []
  type: TYPE_IMG
- en: '`[**librosa.feature.spectral_bandwidth**](https://librosa.github.io/librosa/generated/librosa.feature.spectral_bandwidth.html#librosa.feature.spectral_bandwidth)` computes
    the order-p spectral bandwidth:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/09b986ff36d794546a104b828a9f264a.png)'
  prefs: []
  type: TYPE_IMG
- en: '**4\. Zero-Crossing Rate**'
  prefs: []
  type: TYPE_NORMAL
- en: A very simple way for measuring the smoothness of a signal is to calculate the
    number of zero-crossing within a segment of that signal. A voice signal oscillates
    slowly — for example, a 100 Hz signal will cross zero 100 per second — whereas
    an unvoiced fricative can have 3000 zero crossings per second.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9a26544369f50a94654bfcd693fed828.png)'
  prefs: []
  type: TYPE_IMG
- en: It usually has higher values for highly percussive sounds like those in metal
    and rock. Now let us visualize it and see how we calculate zero crossing rate.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d77c25e3494c1bfa84e9ab63d00f25fe.png)'
  prefs: []
  type: TYPE_IMG
- en: Zooming in
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6d1e80fd1036c14acab145ce003597ab.png)'
  prefs: []
  type: TYPE_IMG
- en: There appear to be 16 zero crossings. Let’s verify it with Librosa.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**5\. Mel-Frequency Cepstral Coefficients(MFCCs)**'
  prefs: []
  type: TYPE_NORMAL
- en: The Mel frequency cepstral coefficients (MFCCs) of a signal are a small set
    of features (usually about 10–20) which concisely describe the overall shape of
    a spectral envelope. It models the characteristics of the human voice.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/565a91d01bcff556f0ba6c0c790712d5.png)'
  prefs: []
  type: TYPE_IMG
- en: '**6\. Chroma feature**'
  prefs: []
  type: TYPE_NORMAL
- en: A **chroma feature or vector** is typically a 12-element feature vector indicating
    how much energy of each pitch class, {C, C#, D, D#, E, …, B}, is present in the
    signal. In short, It provides a robust way to describe a similarity measure between
    music pieces.
  prefs: []
  type: TYPE_NORMAL
- en: '`[librosa.feature.chroma_stft](https://librosa.github.io/librosa/generated/librosa.feature.chroma_stft.html#librosa.feature.chroma_stft) `is
    used for the computation of Chroma features.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/70f36608314cae8c29f98b34b7a505af.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we understood how we can play around with audio data and extract important
    features using python. In the following section, we are going to use these features
    and build a ANN model for music genre classification.
  prefs: []
  type: TYPE_NORMAL
- en: Music genre classification using ANN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Figure](../Images/b7aba595d99d5b06b0973e10f0a11ee1.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Credits](http://bestanimations.com/Music/Music.html)'
  prefs: []
  type: TYPE_NORMAL
- en: This dataset was used for the well-known paper in genre classification “Musical
    genre classification of audio signals” by G. Tzanetakis and P. Cook in IEEE Transactions
    on Audio and Speech Processing 2002.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset consists of 1000 audio tracks each 30 seconds long. It contains
    10 genres, each represented by 100 tracks. The tracks are all 22050 Hz monophonic
    16-bit audio files in .wav format.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset can be download from [marsyas website.](http://marsyas.info/downloads/datasets.html)
  prefs: []
  type: TYPE_NORMAL
- en: The dataset consists of 10 genres i.e
  prefs: []
  type: TYPE_NORMAL
- en: Blues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classical
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Country
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disco
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hiphop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jazz
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reggae
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rock
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each genre contains 100 songs. Total dataset: 1000 songs.'
  prefs: []
  type: TYPE_NORMAL
- en: Before moving ahead, I would recommend using [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true) for
    doing everything related to Neural networks because it is **free** and provides
    GPUs and TPUs as runtime environments.
  prefs: []
  type: TYPE_NORMAL
- en: '**Roadmap:**'
  prefs: []
  type: TYPE_NORMAL
- en: First of all, we need to convert the audio files into PNG format images(spectrograms).
    From these spectrograms, we have to extract meaningful features, i.e. MFCCs, Spectral
    Centroid, Zero Crossing Rate, Chroma Frequencies, Spectral Roll-off.
  prefs: []
  type: TYPE_NORMAL
- en: Once the features have been extracted, they can be appended into a CSV file
    so that ANN can be used for classification.
  prefs: []
  type: TYPE_NORMAL
- en: If we wanna work with image data instead of CSV we will use CNN(Scope of part
    2).
  prefs: []
  type: TYPE_NORMAL
- en: So let's begin.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Extract and load your data to google drive then mount the drive in Colab.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/4e737bdbf344ad6cf3f75005ab0e2d45.png)'
  prefs: []
  type: TYPE_IMG
- en: Google Colab directory structure after data is loaded.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Import all the required libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Now convert the audio data files into PNG format images or basically extracting
    the Spectrogram for every Audio.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Sample spectrogram of a song having genre as blues.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/5cfb7af68d6c514db34bdb08572995ac.png)'
  prefs: []
  type: TYPE_IMG
- en: spectrogram of a song having genre as Blues
  prefs: []
  type: TYPE_NORMAL
- en: Now since all the audio files got converted into their respective spectrograms
    it’s easier to extract features.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Creating a header for our CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '5\. Extracting features from Spectrogram: We will extract Mel-frequency cepstral
    coefficients (MFCC), Spectral Centroid, Zero Crossing Rate, Chroma Frequencies,
    and Spectral Roll-off.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '6\. Data preprocessing: It involves loading CSV data, label encoding, feature
    scaling and data split into training and test set.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 7\. Building an ANN model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 8\. Fit the model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'After 100 epochs, Accuracy: 0.67'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Well, part 1 ends here. In this article, we did a pretty good analysis of audio
    data. We understood how to extract important features and also implemented Artificial
    Neural Networks(ANN) to classify the music genre.
  prefs: []
  type: TYPE_NORMAL
- en: In part 2, we are going to do the same using Convolutional Neural Networks directly
    on the Spectrogram.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you guys have enjoyed reading it. Please share your thoughts/doubts in
    the comment section.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Audio Data Analysis Using Deep Learning with Python (Part 2)**](https://levelup.gitconnected.com/audio-data-analysis-using-deep-learning-with-python-part-2-4a1f40d3708d)'
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Nagesh Singh Chauhan](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)**
    is a Big data developer at CirrusLabs. He has over 4 years of working experience
    in various sectors like Telecom, Analytics, Sales, Data Science having specialisation
    in various Big data components.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://levelup.gitconnected.com/audio-data-analysis-using-deep-learning-part-1-7f6e08803f60).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Audio File Processing: ECG Audio Using Python](/2020/02/audio-file-processing-ecg-audio-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Basics of Audio File Processing in R](/2020/02/basics-audio-file-processing-r.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Artificial Intelligence Books to Read in 2020](/2020/01/artificial-intelligence-books-read-2020.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
