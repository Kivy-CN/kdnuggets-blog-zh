- en: 'PySpark SQL Cheat Sheet: Big Data in Python'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark SQL 备忘单：Python 大数据
- en: 原文：[https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html/2](https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html/2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html/2](https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html/2)
- en: Filter
  id: totrans-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过滤
- en: 'Filtering your Spark DataFrames is extremely easy: you use the filter() method
    on your DataFrame. Do make sure to pass in the condition on which you’re filtering!'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤你的 Spark DataFrame 非常简单：你在 DataFrame 上使用 filter() 方法。务必传递你要过滤的条件！
- en: '![PySpark cheat sheet](../Images/223c3cc3091b988d6c36c70103a7dfd0.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark 备忘单](../Images/223c3cc3091b988d6c36c70103a7dfd0.png)'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行 IT 工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Sort
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 排序
- en: 'Also sorting your Spark DataFrame is easy peasy: make use of the sort() and
    orderBy() methods, in combination with collect() to retrieve your sorted DataFrame.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对 Spark DataFrame 进行排序也很简单：使用 sort() 和 orderBy() 方法，并结合 collect() 检索排序后的 DataFrame。
- en: '![PySpark cheat sheet](../Images/90419b84e9129e991b74bd6504cd8123.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark 备忘单](../Images/90419b84e9129e991b74bd6504cd8123.png)'
- en: '**Remember:** while show() prints the first n rows to the console, collect()
    returns all the records as a list of [Row](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.Row).
    Make sure that you use the latter when only when you’re working on small DataFrames,
    exactly because of what you have just read: the use of this method will make sure
    that *all* records are returned and will move all records back from the executor
    to the driver.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**记住：** 虽然 show() 将前 n 行打印到控制台，collect() 将所有记录作为 [Row](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.Row)
    返回。请确保仅在处理小 DataFrame 时使用后者，正如你刚刚阅读的：使用此方法将确保*所有*记录都被返回，并将所有记录从执行器移回到驱动程序。'
- en: Missing & Replacing Values
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺失值与替换值
- en: To handle missing values, you can replace, fill or drop them - you can use the
    replace(), fill() and drop() methods to do this.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要处理缺失值，你可以替换、填充或删除它们 - 你可以使用 replace()、fill() 和 drop() 方法来完成。
- en: '**Note:** the replace() method takes two arguments, namely, the value that
    you want to be replacing and the value with which you want to replace the original
    value.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** replace() 方法需要两个参数，即你要替换的值和你希望用来替换原始值的值。'
- en: '![PySpark cheat sheet](../Images/91c90e62240d7b7ad8f2856aaede2e8a.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark 备忘单](../Images/91c90e62240d7b7ad8f2856aaede2e8a.png)'
- en: Repartitioning
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重新分区
- en: 'Repartitioning is something that you’ll find yourself doing from time to time.
    You can use this method to increase or decrease the number of partitions in your
    DataFrame.  However, you might want to consider the way you’re doing this operation.
    Consider the following examples:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 重新分区是你会时不时遇到的操作。你可以使用此方法来增加或减少 DataFrame 中的分区数量。然而，你可能要考虑你执行此操作的方式。考虑以下示例：
- en: '![PySpark cheat sheet](../Images/db874543f832e3f5e449d323028f8813.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark 备忘单](../Images/db874543f832e3f5e449d323028f8813.png)'
- en: You see that both the repartition() and coalesce() methods are used to repartition
    the DataFrame.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现 repartition() 和 coalesce() 方法都用于重新分区 DataFrame。
- en: '**Note** that, while repartition() creates equal-sized data partitions by means
    of a full data shuffle, coalesce() avoids this full shuffle by combining existing
    partitions.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**，尽管 repartition() 方法通过完全数据洗牌创建相等大小的数据分区，coalesce() 方法通过合并现有分区避免了完全洗牌。'
- en: Running SQL Queries Programmatically
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 程序化运行 SQL 查询
- en: '![PySpark cheat sheet](../Images/fb906355148f1cd2844876634560e38b.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark 备忘单](../Images/fb906355148f1cd2844876634560e38b.png)'
- en: '**Registering DataFrames as Views**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**注册 DataFrames 作为视图**'
- en: You register Spark DataFrames as views so that you can query it using “real”
    SQL queries (and not the variants with methods which you saw earlier in this article).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你将 Spark DataFrames 注册为视图，以便可以使用“真实” SQL 查询（而不是你在本文中看到的带有方法的变体）。
- en: The first step in querying your Spark DataFrames with “real” SQL is, of course,
    registering your DataFrames as views. When you’re doing this, it’s important to
    keep the scope of your project in mind - as the name already suggests, createTempView()
    creates a temporary view, while createGlobalTempView() creates a global temporary
    view. The latter is shared among all sessions and is kept alive until the Spark
    application terminates, while the former is session-scoped and will disappear
    if the session that creates it terminates.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用“真实” SQL 查询 Spark DataFrames 的第一步当然是将 DataFrames 注册为视图。在进行此操作时，重要的是要记住项目的范围
    - 正如名称所示，createTempView() 创建一个临时视图，而 createGlobalTempView() 创建一个全局临时视图。后者在所有会话中共享，并在
    Spark 应用程序终止之前保持有效，而前者则是会话范围的，如果创建它的会话终止，它将消失。
- en: Note that you can also use createOrReplaceTempView() to create a temporary view
    if there is none or replace one if there is already a view present.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你还可以使用 createOrReplaceTempView() 来创建一个临时视图（如果没有）或替换一个已有的视图。
- en: '**Query Views**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**查询视图**'
- en: Now that you have made some views, as in the example above, where you made the
    views “people” and “customer”, you can use the sql() function on a SparkSession
    to run SQL queries programmatically - The result of this operation will be another
    DataFrame, as you can see in the examples above.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经创建了一些视图，如上例中的“people”和“customer”视图，你可以在 SparkSession 上使用 sql() 函数以编程方式运行
    SQL 查询 - 此操作的结果将是另一个 DataFrame，如上例所示。
- en: Output
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输出
- en: 'Lastly, there’s the output to consider: your analyses will always have some
    results and you’ll want to save these results to another file, a Pandas DataFrame,
    … This section will cover some of the options that you have available to do this.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，需要考虑输出：你的分析总会有一些结果，你会想将这些结果保存到另一个文件、Pandas DataFrame 等。这一部分将涵盖你可以使用的一些选项。
- en: '![PySpark cheat sheet](../Images/fe06a19dff8edd82cc35474174d8623e.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark 备忘单](../Images/fe06a19dff8edd82cc35474174d8623e.png)'
- en: '**Data Structures**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据结构**'
- en: As already mentioned above, you can convert your Spark DataFrame into an RDD,
    a Pandas DataFrame, an RDD of string, … There are many possible data structures
    to which you can convert your Spark DataFrame but the three listed in the image
    above are probably the most common ones to be used.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，你可以将 Spark DataFrame 转换为 RDD、Pandas DataFrame、字符串 RDD 等。虽然将 Spark DataFrame
    转换为其他数据结构的可能性很多，但图像中列出的三种结构可能是最常用的。
- en: '**Write & Save to Files**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**写入与保存文件**'
- en: Writing and saving the results of your analysis is of key importance whenever
    you’re doing data science - The same holds, of course, when you’re working with
    big data. You’ll want to save your results to JSON or parquet files.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 写入和保存分析结果在进行数据科学时至关重要 - 当然，当你处理大数据时也是如此。你会想将结果保存到 JSON 或 parquet 文件中。
- en: 'Remember: parquet files have a columnar file format available to any project
    in the Hadoop ecosystem. It doesn’t matter which data processing framework, data
    model or programming language you’re using! Parquet files provide efficient data
    compression and encoding schemes with enhanced performance to handle complex data
    in bulk. JSON or JavaScript Object Notation files, on the other hand, have an
    open-standard file format that uses human-readable text to transmit data objects
    consisting of attribute–value pairs and array data types (or any other serializable
    value).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 记住：parquet 文件有一个列式文件格式，适用于 Hadoop 生态系统中的任何项目。不论你使用哪个数据处理框架、数据模型或编程语言！Parquet
    文件提供高效的数据压缩和编码方案，具有增强的性能来处理复杂的大数据。而 JSON 或 JavaScript 对象表示法文件则具有一种开放标准的文件格式，使用人类可读的文本来传输由属性-值对和数组数据类型（或任何其他可序列化值）组成的数据对象。
- en: Stopping SparkSession
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 停止 SparkSession
- en: Before you leave, don’t forget to stop your SparkSession! Remember that you
    have assigned your SparkSession to the spark variable - You’ll need this exact
    variable and the stop() method to correctly stop your SparkSession.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在离开之前，别忘了停止你的 SparkSession！记住你已经将 SparkSession 分配给了 spark 变量 - 你将需要这个确切的变量和
    stop() 方法来正确停止你的 SparkSession。
- en: '![PySpark cheat sheet](../Images/20516c91f72dfe0e86f6b1a3d0c02d4f.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark 备忘单](../Images/20516c91f72dfe0e86f6b1a3d0c02d4f.png)'
- en: This is just the beginning of your journey with Spark SQL. We hope that you
    don’t forget to keep your [cheat sheet](https://www.datacamp.com/community/blog/bokeh-cheat-sheet-python)
    handy, of course, when you continue learning more about this exciting technology,
    its applications and its possibilities.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是你与 Spark SQL 旅程的开始。我们希望你在继续学习这个令人兴奋的技术、其应用及其可能性时，别忘了随身携带你的 [备忘单](https://www.datacamp.com/community/blog/bokeh-cheat-sheet-python)。
- en: '[**DataCamp**](https://www.datacamp.com/) is an online interactive education
    platform that that focuses on building the best learning experience specifically
    for Data Science. Our courses on [R](https://www.datacamp.com/courses?learn=r_programming),
    [Python](https://www.datacamp.com/courses?learn=python_programming), [SQL](https://www.datacamp.com/courses/tech:sql),
    and [Data Science](https://www.datacamp.com/courses) are built around a certain
    topic, and combine video instruction with in-browser coding challenges so that
    you can learn by doing. [You can start every course for free](https://www.datacamp.com/courses),
    whenever you want, wherever you want.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[**DataCamp**](https://www.datacamp.com/) 是一个在线互动教育平台，专注于为数据科学构建最佳学习体验。我们关于
    [R](https://www.datacamp.com/courses?learn=r_programming)、[Python](https://www.datacamp.com/courses?learn=python_programming)、[SQL](https://www.datacamp.com/courses/tech:sql)
    和 [数据科学](https://www.datacamp.com/courses) 的课程围绕特定主题构建，结合了视频教学和浏览器内编码挑战，让你通过实践学习。[你可以随时免费开始每个课程](https://www.datacamp.com/courses)，无论何时何地。'
- en: '[**Karlijn Willems**](https://www.linkedin.com/in/karlijnwillems) is a data
    science journalist and writes for the [DataCamp community](https://www.datacamp.com/community/authors/karlijn-willems),
    focusing on data science education, the latest news and the hottest trends. She
    holds degrees in Literature and Linguistics and Information Management.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Karlijn Willems**](https://www.linkedin.com/in/karlijnwillems) 是一名数据科学记者，为
    [DataCamp 社区](https://www.datacamp.com/community/authors/karlijn-willems) 撰写文章，专注于数据科学教育、最新新闻和热门趋势。她拥有文学和语言学及信息管理的学位。'
- en: '**Related**:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关**：'
- en: '[Keras Cheat Sheet: Deep Learning in Python](/2017/09/datacamp-keras-cheat-sheet-deep-learning-python.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Keras 备忘单：Python 中的深度学习](/2017/09/datacamp-keras-cheat-sheet-deep-learning-python.html)'
- en: '[Pandas Cheat Sheet: Data Science and Data Wrangling in Python](/2017/01/pandas-cheat-sheet.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Pandas 备忘单：Python 中的数据科学和数据整理](/2017/01/pandas-cheat-sheet.html)'
- en: '[Bokeh Cheat Sheet: Data Visualization in Python](/2017/03/bokeh-cheat-sheet.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bokeh 备忘单：Python 中的数据可视化](/2017/03/bokeh-cheat-sheet.html)'
- en: More On This Topic
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[PySpark for Data Science](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PySpark 数据科学](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
- en: '[Data Validation for PySpark Applications using Pandera](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Pandera 对 PySpark 应用进行数据验证](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
- en: '[Data Cleaning with Python Cheat Sheet](https://www.kdnuggets.com/2023/02/data-cleaning-python-cheat-sheet.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 的数据清洗备忘单](https://www.kdnuggets.com/2023/02/data-cleaning-python-cheat-sheet.html)'
- en: '[Best Python Tools for Building Generative AI Applications Cheat Sheet](https://www.kdnuggets.com/2023/08/best-python-tools-generative-ai-cheat-sheet.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[构建生成式 AI 应用的最佳 Python 工具备忘单](https://www.kdnuggets.com/2023/08/best-python-tools-generative-ai-cheat-sheet.html)'
- en: '[Python Control Flow Cheat Sheet](https://www.kdnuggets.com/2022/11/python-control-flow-cheatsheet.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 控制流备忘单](https://www.kdnuggets.com/2022/11/python-control-flow-cheatsheet.html)'
- en: '[KDnuggets News, July 5: A Rotten Data Science Project • 10 AI…](https://www.kdnuggets.com/2023/n24.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，7月5日：一个糟糕的数据科学项目 • 10 AI…](https://www.kdnuggets.com/2023/n24.html)'
