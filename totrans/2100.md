# 量化与LLMs：将模型浓缩到可管理的大小

> 原文：[https://www.kdnuggets.com/quantization-and-llms-condensing-models-to-manageable-sizes](https://www.kdnuggets.com/quantization-and-llms-condensing-models-to-manageable-sizes)

![量化与LLMs：将模型浓缩到可管理的大小](../Images/c200004e40b1a5d03763bbf10e7b6bab.png)

## LLM的规模与复杂性

LLM的惊人能力得益于其庞大的神经网络，这些网络由数十亿个参数组成。这些参数是通过在大量文本语料库上进行训练得到的，并经过微调以使模型尽可能准确和多才多艺。这种复杂性水平需要大量的计算能力来处理和存储。

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 为你的组织提供IT支持

* * *

![量化与LLMs：将模型浓缩到可管理的大小](../Images/cdcb06c3bedff816f35c168293c2ffc0.png)

附图条形图描绘了不同规模语言模型的参数数量。随着模型规模的增大，我们可以看到参数数量显著增加，从具有几百万参数的“小型”语言模型到具有数百亿参数的“大型”模型。

然而，具有1750亿个参数的GPT-4 LLM模型在参数规模上超越了其他模型。GPT-4不仅使用了图表中最多的参数，还驱动了最具知名度的生成型AI模型——ChatGPT。图表中的这一庞大存在代表了同类的其他LLMs，展示了驱动未来AI聊天机器人的需求以及支持如此先进AI系统所需的计算能力。

## 运行LLMs的成本与量化

部署和操作复杂的模型可能会很昂贵，因为它们需要使用[高端GPU](https://www.exxactcorp.com/category/Consumer-Graphics-Cards)、AI加速器以及持续的能源消耗进行云计算。选择本地解决方案可以通过降低成本来节省大量资金，并提高硬件选择的灵活性和系统使用的自由度，但这需要付出维护成本和雇佣专业人员的代价。高成本使得小型企业在训练和运行先进AI方面面临挑战。这时，量化技术就显得非常重要。

## 什么是量化？

量化是一种技术，通过降低模型中每个参数的数值精度，从而减少其内存占用。这类似于将高分辨率图像压缩为较低分辨率，同时保留精髓和最重要的方面，但数据大小有所减少。这种方法使得在硬件条件较差的情况下，也能部署大型语言模型而不会造成显著的性能损失。

ChatGPT的训练和部署使用了数千台NVIDIA DGX系统，数百万美元的硬件以及数万亿美元的基础设施费用。量化可以在性能不逊色（但硬件性能较低）的条件下，实现良好的概念验证甚至完全成熟的部署。

在接下来的部分中，我们将剖析量化的概念、方法及其在缩小大型语言模型对资源的高度依赖与日常技术使用的实际之间差距方面的意义。大型语言模型的变革力量可以在小规模应用中成为主流，为更广泛的受众提供巨大的好处。

## 量化基础

对大型语言模型进行量化是指减少模型中数值值的精度。在神经网络和深度学习模型的上下文中，包括大型语言模型，数值值通常表示为高精度的浮点数（例如32位或16位浮点格式）。有关[浮点精度的更多信息](https://www.exxactcorp.com/blog/hpc/what-is-fp64-fp32-fp16)。

量化通过将这些高精度浮点数转换为较低精度的表示形式，如16位或8位整数，从而提高模型在训练和推理时的内存效率和速度，尽管牺牲了一些精度。因此，模型的训练和推理需要较少的存储空间，消耗更少的内存，并且可以在支持低精度计算的硬件上更快地执行。

## 量化类型

为了增加主题的深度和复杂性，了解量化可以在模型开发和部署生命周期的不同阶段应用至关重要。每种方法都有其独特的优点和权衡，根据用例的具体要求和限制进行选择。

### 1. 静态量化

静态量化是一种在AI模型的训练阶段应用的技术，其中权重和激活被量化为较低的位精度，并应用于所有层。权重和激活提前量化并在整个过程中保持固定。静态量化非常适合于已知系统内存需求的模型部署计划。

+   静态量化的优点

    +   简化部署规划，因为量化参数是固定的。

    +   减少模型的大小，使其更适合边缘设备和实时应用。

+   静态量化的缺点

    +   性能下降是可预测的；因此某些量化部分可能由于广泛的静态方法而遭受更大的损失。

    +   对于静态量化适应性有限，适应不同输入模式的能力差，权重更新不够稳健。

### 2\. 动态量化

动态量化涉及静态量化权重，但在模型推理期间激活会动态量化。权重会提前量化，而激活在数据通过网络时动态量化。这意味着模型的某些部分在不同的精度下执行量化，而不是默认固定量化。

+   动态量化的优点

    +   平衡了模型压缩和运行效率，准确性没有显著下降。

    +   对于激活精度比权重精度更为关键的模型特别有用。

+   动态量化的缺点

    +   相较于静态方法，性能提升不可预测（但这不一定是坏事）。

    +   动态计算意味着比其他方法需要更多的计算开销和更长的训练和推理时间，但仍比没有量化时更轻量。

### 3\. 训练后量化（PTQ）

在这种技术中，量化被纳入到训练过程中。它涉及分析权重和激活的分布，然后将这些值映射到更低的位深。PTQ被部署在资源受限的设备上，如边缘设备和手机。PTQ可以是静态的也可以是动态的。

+   PTQ的优点

    +   可以直接应用于预训练模型，而无需重新训练。

    +   减少模型大小，降低内存需求。

    +   提升了推理速度，实现了部署期间和之后更快的计算。

+   PTQ的缺点

    +   由于权重的近似，可能会导致模型准确性下降。

    +   需要仔细的校准和微调以减轻量化误差。

    +   可能不适用于所有类型的模型，特别是那些对权重精度敏感的模型。

### 4\. 量化感知训练（QAT）

在训练过程中，模型会了解在推理期间将应用的量化操作，并相应调整参数。这使得模型能够学会处理量化引起的误差。

+   QAT的优点

    +   相比PTQ，QAT倾向于保持模型准确性，因为模型训练时考虑了量化误差。

    +   对于对精度敏感的模型更为稳健，即使在较低精度下也能更好地进行推理。

+   QAT的缺点

    +   需要重新训练模型，从而导致训练时间延长。

    +   由于包含量化误差检查，因此计算开销较大。

### 5\. 二进制三进制量化

这些方法将权重量化为两个值（二值）或三个值（三值），代表了量化的最极端形式。权重在训练过程中或之后被限制为二值量化的 +1 和 -1，或三值量化的 +1、0 和 -1。这将大幅减少可能的量化权重值数量，同时仍保持一定的动态性。

+   二值三值量化的优点

    +   最大化模型压缩和推理速度，且内存需求最小。

    +   快速推理和量化计算使得在性能不足的硬件上仍具备实用性。

+   二值三值量化的缺点

    +   高压缩率和降低精度会显著降低准确性。

    +   不适用于所有类型的任务或数据集，并且在处理复杂任务时表现不佳。

## 量化的好处与挑战

![量化前后](../Images/d50020005166b994925cde3b76b3e3db.png)

大型语言模型的量化带来了多种操作上的好处。主要是显著减少了这些模型的内存需求。我们对量化后的模型的目标是内存占用显著减少。更高的效率使得这些模型可以在内存能力较小的平台上部署，并且量化后的模型所需的处理能力减少，直接提高了推理速度和响应时间，从而提升了用户体验。

另一方面，量化也可能会引入一些模型精度的损失，因为它涉及到对真实数字的近似。挑战在于如何在不显著影响模型性能的情况下进行量化。可以通过在量化前后测试模型的精度和完成时间来评估效果、效率和准确性。

通过优化性能和资源消耗之间的平衡，量化不仅扩大了大语言模型的可及性，还促进了更可持续的计算实践。

[**原文**](https://www.exxactcorp.com/blog/deep-learning/what-is-quantization-and-llms)。经许可转载。

**[Kevin Vu](https://blog.exxactcorp.com/)** 负责管理 [Exxact Corp 博客](https://blog.exxactcorp.com/)，并与许多才华横溢的作者合作，这些作者撰写了有关深度学习不同方面的文章。

### 更多相关主题

+   [掌握大型语言模型（LLMs）的 7 个步骤](https://www.kdnuggets.com/7-steps-to-mastering-large-language-models-llms)

+   [生成式 AI 游乐场：Camel-5b 和 Open LLaMA 3B 的 LLMs](https://www.kdnuggets.com/2024/02/intel-generative-ai-playground-llms-with-camel-5b-and-open-llama-3b)

+   [大语言模型、生成式 AI 和深度学习的向量数据库](https://www.kdnuggets.com/vector-database-for-llms-generative-ai-and-deep-learning)

+   [LLMs 的历史与未来](https://www.kdnuggets.com/history-and-future-of-llms)

+   [8个免费的AI和LLMs试验场](https://www.kdnuggets.com/2023/05/8-free-ai-llms-playgrounds.html)

+   [什么是向量数据库，它们为何对LLMs至关重要？](https://www.kdnuggets.com/2023/06/vector-databases-important-llms.html)
