- en: 'Pythia: A Suite of 16 LLMs for In-Depth Research'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Pythia: 一套用于深入研究的 16 个 LLM'
- en: 原文：[https://www.kdnuggets.com/2023/08/pythia-suite-16-llms-indepth-research.html](https://www.kdnuggets.com/2023/08/pythia-suite-16-llms-indepth-research.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/08/pythia-suite-16-llms-indepth-research.html](https://www.kdnuggets.com/2023/08/pythia-suite-16-llms-indepth-research.html)
- en: '![Pythia: A Suite of 16 LLMs for In-Depth Research](../Images/c61095fd3fa8a2c8268d5fed9d416497.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Pythia: 一套用于深入研究的 16 个 LLM](../Images/c61095fd3fa8a2c8268d5fed9d416497.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Today large language models and LLM-powered chatbots like ChatGPT and GPT-4
    have integrated well into our daily lives.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，大型语言模型和如 ChatGPT 和 GPT-4 这样的 LLM 驱动的聊天机器人已广泛融入我们的日常生活中。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: However, decoder-only autoaggressive transformer models have been used extensively
    for generative NLP applications long before LLM applications became mainstream.
    It can be helpful to understand how they evolve during training and how their
    performance changes as they scale.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅解码的自回归变换器模型在 LLM 应用成为主流之前就已经广泛用于生成 NLP 应用。了解它们在训练中的演变以及它们随着规模扩大而性能的变化是很有帮助的。
- en: Pythia, a project by [Eleuther AI](https://www.eleuther.ai/) is a suite of 16
    large language models that provide reproducibility for study, analysis, and further
    research. This article is an introduction to [Pythia](https://github.com/EleutherAI/pythia).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Pythia 是 [Eleuther AI](https://www.eleuther.ai/) 的一个项目，提供了一套 16 个大型语言模型，用于研究、分析和进一步研究的可重复性。本文介绍了
    [Pythia](https://github.com/EleutherAI/pythia)。
- en: What Does the Pythia Suite Offer?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pythia 套件提供了什么？
- en: As mentioned, Pythia is a suite of 16 large language models— decoder-only autoregressive
    transformer models—trained on publicly available dataset. The models in the suite
    have sizes ranging from 70M to 12B parameters.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Pythia 是一套由 16 个大型语言模型组成的套件——仅解码的自回归变换器模型——在公开可用的数据集上进行训练。套件中的模型大小从 7000
    万到 120 亿参数不等。
- en: The entire suite was trained on the same data in the same order. This facilitates
    reproducibility of the training process. So we can not only replicate the training
    pipeline but also analyze the language models and study their behavior in depth.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个套件是在相同的数据和相同的顺序下训练的。这有助于训练过程的可重复性。因此，我们不仅可以复制训练管道，还可以深入分析语言模型并研究其行为。
- en: It also provides facilities for downloading the training data loaders and more
    than 154 model checkpoints for each of the 16 language models.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 套件还提供了下载训练数据加载器和每个 16 个语言模型的 154 个模型检查点的设施。
- en: Training Data and Training Process
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练数据和训练过程
- en: Now let’s delve into the details of the Pythia LLM suite.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解 Pythia LLM 套件的详细信息。
- en: Training Dataset
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练数据集
- en: 'The Pythia LLM suite was trained on the following datasets:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Pythia LLM 套件是在以下数据集上训练的：
- en: '[Pile dataset](https://arxiv.org/abs/2101.00027) with 300B tokens'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Pile 数据集](https://arxiv.org/abs/2101.00027)，包含 3000 亿个标记'
- en: Deduplicated Pile dataset with 207B tokens.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去重的 Pile 数据集，包含 2070 亿个标记。
- en: There are 8 different model sizes with the smallest and largest models having
    70M and 12B parameters, respectively. Other model sizes include 160M, 410M, 1B,
    1.4B, 2.8B, and 6.9B.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 共有 8 种不同的模型尺寸，其中最小和最大的模型分别具有 7000 万和 120 亿个参数。其他模型尺寸包括 1600 万、4100 万、10 亿、14
    亿、28 亿和 69 亿。
- en: Each of these models was trained on both the Pile and the deduplicated Pile
    datasets resulting in a total of 16 models. The following table shows the model
    sizes and a subset of hyperparameters.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型中的每一个都在 Pile 和去重的 Pile 数据集上进行训练，形成了总共 16 个模型。下表展示了模型的大小和部分超参数。
- en: '![Pythia: A Suite of 16 LLMs for In-Depth Research](../Images/4c7cde031787d773503da571915b54d9.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![Pythia: 一套用于深入研究的 16 个 LLM](../Images/4c7cde031787d773503da571915b54d9.png)'
- en: Models and hyperparameters | [Image source](https://arxiv.org/pdf/2304.01373.pdf)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和超参数 | [图像来源](https://arxiv.org/pdf/2304.01373.pdf)
- en: 'For full details of the hyperparameters used, read [Pythia: A Suite for Analyzing
    Large Language Models Across Training and Scaling](https://arxiv.org/abs/2304.01373).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '有关使用的超参数的完整细节，请参阅 [Pythia: A Suite for Analyzing Large Language Models Across
    Training and Scaling](https://arxiv.org/abs/2304.01373)。'
- en: Training Process
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练过程
- en: 'Here’s an overview of the architecture and training process:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是架构和训练过程的概述：
- en: All models have fully dense layers and use [flash attention](https://github.com/HazyResearch/flash-attention).
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有模型都有完全密集的层，并使用 [flash attention](https://github.com/HazyResearch/flash-attention)。
- en: For easier interpretability untied embedding matrices are used.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了更容易解释，使用了未绑定的嵌入矩阵。
- en: A batch size of 1024 is used with sequence length of 2048\. This large batch
    size substantially reduces the wall-clock training time.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用了 1024 的批量大小和 2048 的序列长度。这个大批量大小大大减少了实际训练时间。
- en: The training process also leverages optimization techniques such as data and
    tensor parallelism
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练过程还利用了诸如数据和张量并行等优化技术
- en: For the training process, the [GPT-Neo-X library](https://github.com/EleutherAI/gpt-neox)
    (includes features from the [DeepSpeed](https://www.deepspeed.ai/) library) developed
    by Eleuther AI is used.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练过程，使用了由 Eleuther AI 开发的 [GPT-Neo-X library](https://github.com/EleutherAI/gpt-neox)（包括来自
    [DeepSpeed](https://www.deepspeed.ai/) library 的功能）。
- en: Model Checkpoints
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型检查点
- en: 'There are 154 checkpoints for each model. There’s one checkpoint every 1000
    iterations. In addition, there are checkpoints at log-spaced intervals earlier
    in the training process: 1, 2, 4, 8, 16, 32, 64, 128, 256, and 512.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型有154个检查点。每1000次迭代设置一个检查点。此外，在训练过程的早期阶段，还有按对数间隔设置的检查点：1、2、4、8、16、32、64、128、256
    和 512。
- en: How Does Pythia Compare to Other Language Models?
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pythia 与其他语言模型的比较
- en: The Pythia LLM suite was evaluated against the available language modeling benchmarks
    including [OpenAI’s LAMBADA](https://paperswithcode.com/sota/language-modelling-on-lambada)
    variant. It was found that the performance of Pythia is comparable to the OPT
    and BLOOM language models.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Pythia LLM 套件与现有语言建模基准进行评估，包括 [OpenAI 的 LAMBADA](https://paperswithcode.com/sota/language-modelling-on-lambada)
    变体。结果发现，Pythia 的表现与 OPT 和 BLOOM 语言模型相当。
- en: Advantages and Limitations
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优势和局限性
- en: The key advantage of Pythia LLM suite is the reproducibility. The dataset is
    publicly available, pre-tokenized data loaders, and 154 model checkpoints are
    also publicly available. The full list of hyperparameters has been released, too.
    This makes replicating the model training and analysis simpler.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Pythia LLM 套件的主要优势是可重复性。数据集是公开的，预标记的数据加载器和154个模型检查点也都是公开的。超参数的完整列表也已发布。这使得模型训练和分析的复制变得更简单。
- en: In [1], the authors explain their rationale for choosing an English language
    dataset over a multilingual text corpus. But having reproducible training pipelines
    for multilingual large language models can be helpful. Especially in encouraging
    more research and study of the dynamics of multilingual large language models.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [1] 中，作者解释了他们为何选择英文数据集而非多语言文本语料库的理由。但拥有可重复的多语言大型语言模型训练流程是有帮助的，特别是在鼓励更多的研究和多语言大型语言模型动态的研究方面。
- en: An Overview of Case Studies
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究概述
- en: The research also presents interesting case studies leveraging the reproducibility
    of the training process of large language models in the Pythia suite.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 研究还展示了利用 Pythia 套件中大型语言模型训练过程的可重复性的一些有趣的案例研究。
- en: Gender Bias
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性别偏见
- en: All large language models are prone to bias and misinformation. The study focuses
    on mitigating gender bias by modifying the pretraining data such that a fixed
    percentage has pronouns of a specific gender. This pretraining is also reproducible.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 所有大型语言模型都容易受到偏见和虚假信息的影响。研究集中在通过修改预训练数据来减轻性别偏见，使得固定比例的数据包含特定性别的代词。这种预训练也是可重复的。
- en: Memorization
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 记忆化
- en: Memorization in large language models is also another area that has been widely
    studied. The sequence memorization is modeled as a Poisson point process. The
    study aims at understanding if the location of the specific sequence in the training
    dataset influences memorization. It was observed that the location does not affect
    memorization.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型中的记忆化也是一个广泛研究的领域。序列记忆化被建模为泊松点过程。研究旨在了解特定序列在训练数据集中的位置是否会影响记忆化。观察结果表明，位置不会影响记忆化。
- en: Effect of Pretraining Term Frequencies
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练术语频率的影响
- en: For language models with 2.8B parameters and greater, the occurrence of task-specific
    terms in the pre-training corpus was found to improve the model’s performance
    on tasks such as question answering.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于参数数量为 2.8B 及以上的语言模型，在预训练语料库中出现特定任务术语的情况被发现可以提高模型在问答等任务上的表现。
- en: There is also a correlation between the model size and the performance on more
    involved tasks such as arithmetic and mathematical reasoning.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的规模与在更复杂任务（如算术和数学推理）上的表现之间也存在相关性。
- en: '![Pythia: A Suite of 16 LLMs for In-Depth Research](../Images/972d18fbdb29c398a48f15661db1b227.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![Pythia：一个用于深入研究的 16 个 LLM 套件](../Images/972d18fbdb29c398a48f15661db1b227.png)'
- en: Performance on arithmetic addition task | [Image source](https://arxiv.org/pdf/2304.01373.pdf)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 算术加法任务表现 | [图片来源](https://arxiv.org/pdf/2304.01373.pdf)
- en: Summary and Next Steps
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结与下一步
- en: Let’s sum up the key points in our discussion.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下讨论中的关键点。
- en: Pythia by Eleuther AI is a suite of 16 LLMs trained on publicly available Pile
    and deduplicated Pile datasets.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pythia by Eleuther AI 是一个包含 16 个大型语言模型（LLMs）的套件，这些模型在公开可用的 Pile 数据集和去重后的 Pile
    数据集上进行了训练。
- en: The size of the LLMs range from 70M to 12B parameters.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs 的规模范围从 70M 到 12B 参数不等。
- en: The training data and model checkpoints are open-source and it is possible to
    reconstruct the exact training data loaders. So the LLM suite can be helpful in
    understanding the training dynamics of large language models better.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据和模型检查点是开源的，能够重建精确的训练数据加载器。因此，LLM 套件对于更好地理解大型语言模型的训练动态非常有帮助。
- en: As a next step, you can explore the Pythia suite of models and model checkpoints
    on [Hugging Face Hub](https://huggingface.co/EleutherAI).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步，你可以在 [Hugging Face Hub](https://huggingface.co/EleutherAI) 上探索 Pythia 模型套件和模型检查点。
- en: Reference
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[1] [Pythia: A Suite for Analyzing Large Language Models Across Training and
    Scaling](https://arxiv.org/abs/2304.01373), arXiv, 2023'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [Pythia：分析大型语言模型的训练和扩展的套件](https://arxiv.org/abs/2304.01373)，arXiv，2023'
- en: '**[Bala Priya C](https://www.linkedin.com/in/bala-priya/)** is a developer
    and technical writer from India. She likes working at the intersection of math,
    programming, data science, and content creation. Her areas of interest and expertise
    include DevOps, data science, and natural language processing. She enjoys reading,
    writing, coding, and coffee! Currently, she''s working on learning and sharing
    her knowledge with the developer community by authoring tutorials, how-to guides,
    opinion pieces, and more.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Bala Priya C](https://www.linkedin.com/in/bala-priya/)** 是来自印度的开发人员和技术作家。她喜欢在数学、编程、数据科学和内容创作的交汇处工作。她的兴趣和专业领域包括
    DevOps、数据科学和自然语言处理。她喜欢阅读、写作、编程和喝咖啡！目前，她致力于学习并通过编写教程、操作指南、观点文章等与开发者社区分享她的知识。'
- en: More On This Topic
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Understanding Machine Learning Algorithms: An In-Depth Overview](https://www.kdnuggets.com/understanding-machine-learning-algorithms-an-indepth-overview)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解机器学习算法：深入概述](https://www.kdnuggets.com/understanding-machine-learning-algorithms-an-indepth-overview)'
- en: '[Density Kernel Depth for Outlier Detection in Functional Data](https://www.kdnuggets.com/density-kernel-depth-for-outlier-detection-in-functional-data)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[功能数据中的异常检测密度核深度](https://www.kdnuggets.com/density-kernel-depth-for-outlier-detection-in-functional-data)'
- en: '[From Unstructured to Structured Data with LLMs](https://www.kdnuggets.com/2023/06/predibase-unstructured-structured-data-llms.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用 LLMs 从非结构化到结构化数据](https://www.kdnuggets.com/2023/06/predibase-unstructured-structured-data-llms.html)'
- en: '[Ensuring Reliable Few-Shot Prompt Selection for LLMs](https://www.kdnuggets.com/2023/07/ensuring-reliable-fewshot-prompt-selection-llms.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[确保 LLMs 的可靠少量样本提示选择](https://www.kdnuggets.com/2023/07/ensuring-reliable-fewshot-prompt-selection-llms.html)'
- en: '[Introducing OpenLLM: Open Source Library for LLMs](https://www.kdnuggets.com/2023/07/introducing-openllm-open-source-library-llms.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍 OpenLLM：开源大型语言模型库](https://www.kdnuggets.com/2023/07/introducing-openllm-open-source-library-llms.html)'
- en: '[ReAct, Reasoning and Acting augments LLMs with Tools!](https://www.kdnuggets.com/react-reasoning-and-acting-augments-llms-with-tools)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ReAct：推理和行动增强 LLMs 以使用工具！](https://www.kdnuggets.com/react-reasoning-and-acting-augments-llms-with-tools)'
