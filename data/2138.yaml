- en: Strategies for Optimizing Performance and Costs When Using Large Language Models
    in the Cloud
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用大语言模型在云中优化性能和成本的策略
- en: 原文：[https://www.kdnuggets.com/strategies-for-optimizing-performance-and-costs-when-using-large-language-models-in-the-cloud](https://www.kdnuggets.com/strategies-for-optimizing-performance-and-costs-when-using-large-language-models-in-the-cloud)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/strategies-for-optimizing-performance-and-costs-when-using-large-language-models-in-the-cloud](https://www.kdnuggets.com/strategies-for-optimizing-performance-and-costs-when-using-large-language-models-in-the-cloud)
- en: '![Strategies for Optimizing Performance and Costs When Using Large Language
    Models in the Cloud](../Images/99ce9c90cb6e9401eac8daa858d85add.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![使用大语言模型在云中优化性能和成本的策略](../Images/99ce9c90cb6e9401eac8cdaa858d85add.png)'
- en: Image by [pch.vector](https://www.freepik.com/free-vector/money-income-attraction_9176032.htm#query=cost&position=29&from_view=search&track=sph&uuid=61aa0541-882f-4e86-b49d-e7cc8e315f4b)
    on [Freepik](https://www.freepik.com/)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[pch.vector](https://www.freepik.com/free-vector/money-income-attraction_9176032.htm#query=cost&position=29&from_view=search&track=sph&uuid=61aa0541-882f-4e86-b49d-e7cc8e315f4b)
    在 [Freepik](https://www.freepik.com/)
- en: Large Language Model (LLM) has recently started to find their foot in the business,
    and it will expand even further. As the company began understanding the benefits
    of implementing the LLM, the data team would adjust the model to the business
    requirements.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLM）最近开始在业务中站稳脚跟，并将进一步扩展。随着公司开始理解实施LLM的好处，数据团队将调整模型以满足业务需求。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织进行IT工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The optimal path for the business is to utilize a cloud platform to scale any
    LLM requirements that the business needs. However, many hurdles could hinder LLM
    performance in the cloud and increase the usage cost. It is certainly what we
    want to avoid in the business.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于业务来说，最佳路径是利用云平台来扩展业务所需的任何LLM要求。然而，许多障碍可能会阻碍LLM在云中的性能并增加使用成本。这正是我们希望在业务中避免的。
- en: That’s why this article will try to outline a strategy you could use to optimize
    the performance of LLM in the cloud while taking care of the cost. What’s the
    strategy? Let’s get into it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么本文将尝试概述一种可以用来优化云中LLM性能的策略，同时考虑到成本。策略是什么？让我们深入探讨一下。
- en: 1\. Having a Clear Budget Plan
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 制定清晰的预算计划
- en: We must understand our financial condition before implementing any strategy
    to optimize performance and costs. How much budget we are willing to invest in
    the LLM will become our limit. A higher budget could lead to more significant
    performance results but might not be optimal if it doesn’t support the business.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须在实施任何优化性能和成本的策略之前了解我们的财务状况。我们愿意在LLM上投入的预算将成为我们的限制。更高的预算可能带来更显著的性能结果，但如果不支持业务，可能并不理想。
- en: The budget plan needs extensive discussion with various stakeholders so it would
    not become a waste. Identify the critical focus your business wants to solve and
    assess if LLM is worth investing in.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 预算计划需要与各种利益相关者进行广泛讨论，以避免浪费。确定您的业务希望解决的关键问题，并评估LLM是否值得投资。
- en: The strategy also applies to any solo business or individual. Having a budget
    for the LLM that you are willing to spend would help your financial problem in
    the long run.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这一策略同样适用于任何个人或独立业务。为LLM设定一个您愿意支出的预算，将有助于您长期解决财务问题。
- en: 2\. Decide the Right Model Size and Hardware
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 确定合适的模型规模和硬件
- en: With the advancement of research, there are many kinds of LLMs that we can choose
    to solve our problem. With a smaller parameter model, it would be faster to optimize
    but might not have the best ability to solve your business problems. While a bigger
    model has a more excellent knowledge base and creativity, it costs more to compute.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 随着研究的进展，我们可以选择多种LLM来解决问题。使用较小参数的模型，优化速度更快，但可能没有最佳的业务问题解决能力。而更大的模型具有更丰富的知识库和创造力，但计算成本更高。
- en: There are trade-offs between the performance and cost with the change in the
    LLM size, which we need to take into account when we decide on the model. Do we
    need to have bigger parameter models that have better performance but require
    higher cost, or vice versa? It’s a question we need to ask. So, try to assess
    your needs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定模型时，我们需要考虑LLM大小的性能和成本之间的权衡。我们是需要更大参数的模型，这样性能更好但成本更高，还是相反？这是我们需要考虑的问题。因此，尽量评估你的需求。
- en: Additionally, the cloud Hardware could affect the performance as well. Better
    GPU memory might have a faster response time, allow for more complex models, and
    reduce latency. However, higher memory means higher cost.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，云硬件也可能影响性能。更好的 GPU 内存可能会有更快的响应时间，支持更复杂的模型，并减少延迟。然而，更高的内存意味着更高的成本。
- en: 3\. Choose the Suitable Inference Options
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 选择合适的推理选项
- en: Depending on the cloud platform, there would be many choices for the inferences.
    Comparing your application workload requirements, the option you want to choose
    might be different as well. However, inference could also affect the cost usage
    as the number of resources is different for each option.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 根据云平台的不同，推理选项也会有所不同。根据你的应用程序负载需求，你选择的选项可能也会有所不同。然而，推理还可能影响成本使用，因为每种选项的资源数量不同。
- en: 'If we take an example from [Amazon SageMaker Inferences Options](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html),
    your inference options are:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们以[Amazon SageMaker 推理选项](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html)为例，你的推理选项包括：
- en: '**Real-Time Inference**. The inference processes the response instantly when
    input comes. It’s usually the inferences used in real-time, such as chatbot, translator,
    etc. Because it always requires low latency, the application would need high computing
    resources even in the low-demand period. This would mean that LLM with Real-Time
    inference could lead to higher costs without any benefit if the demand isn’t there.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**实时推理**。这种推理在输入到达时即时处理响应。它通常用于实时应用，如聊天机器人、翻译器等。由于实时推理总是需要低延迟，因此即使在需求低谷期，应用程序也需要高计算资源。这意味着如果需求不足，实时推理的LLM可能导致更高的成本而没有任何好处。'
- en: '**Serverless Inference.** This inference is where the cloud platform scales
    and allocates the resources dynamically as required. The performance might suffer
    as there would be slight latency for each time the resources are initiated for
    each request. But, it’s the most cost-effective as we only pay for what we use.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**无服务器推理**。这种推理是指云平台根据需要动态扩展和分配资源。性能可能会受到影响，因为每次请求时资源启动都会有轻微的延迟。但这是最具成本效益的，因为我们只为所使用的资源付费。'
- en: '**Batch Transform**. The inference is where we process the request in batches.
    This means that the inference is only suitable for offline processes as we don’t
    process the request immediately. It might not be suitable for any application
    that requires an instant process as the delay would always be there, but it doesn’t
    cost much.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**批处理转换**。这种推理是指我们批量处理请求。这意味着推理仅适用于离线处理，因为我们不会立即处理请求。对于任何需要即时处理的应用程序可能不适用，因为延迟总是存在，但成本不高。'
- en: '**Asynchronous Inference**. This inference is suitable for background tasks
    because it runs the inference task in the background while the results are retrieved
    later. Performance-wise, it’s suitable for models that require a long processing
    time as it can handle various tasks concurrently in the background. Cost-wise,
    it could be effective as well because of the better resource allocation.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**异步推理**。这种推理适用于后台任务，因为它在后台运行推理任务，结果稍后检索。从性能角度来看，它适合需要较长处理时间的模型，因为它可以同时处理各种任务。从成本角度来看，由于更好的资源分配，它也可能是有效的。'
- en: Try to assess what your application needs, so you have the most effective inference
    option.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试评估你的应用程序需求，以便选择最有效的推理选项。
- en: 4\. Construct an Effective Prompts
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 构建有效的提示
- en: LLM is a model with a particular case, as the number of tokens affects the cost
    we would need to pay. That’s why we need to build a prompt effectively that uses
    the minimum token either for the input or the output while still maintaining the
    output quality.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 是一种具有特定情况的模型，因为令牌数量会影响我们需要支付的费用。这就是为什么我们需要有效构建提示，以使用最少的令牌（无论是输入还是输出）同时保持输出质量。
- en: Try to build a prompt that specifies a certain amount of paragraph output or
    use a concluding paragraph such as “summarize,” “concise,” and any others. Also,
    precisely construct the input prompt to generate the output you need. Don’t let
    the LLM model generate more than you need.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试构建一个指定特定段落输出的提示，或者使用诸如“总结”、“简洁”等结尾段落。还要精确构造输入提示，以生成你所需的输出。不要让 LLM 模型生成超出你需求的内容。
- en: 5\. Caching Responses
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 缓存响应
- en: There would be information that would be repeatedly asked and have the same
    responses every time. To reduce the number of queries, we can cache all the typical
    information in the database and call them when it’s required.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 会有信息被反复询问，并且每次的响应都是一样的。为了减少查询次数，我们可以将所有典型信息缓存到数据库中，并在需要时调用它们。
- en: Typically, the data is stored in a vector database such as Pinecone or Weaviate,
    but cloud platform should have their vector database as well. The response that
    we want to cache would converted into vector forms and stored for future queries.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，数据存储在如 Pinecone 或 Weaviate 这样的向量数据库中，但云平台也应有它们自己的向量数据库。我们想要缓存的响应会转换为向量形式，并为未来的查询存储。
- en: There are a few challenges when we want to cache the responses effectively,
    as we need to manage policies where the cache response is inadequate to answer
    the input query. Also, some caches are similar to each other, which could result
    in a wrong response. Manage the response well and have an adequate database that
    could help reduce costs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在有效缓存响应时面临一些挑战，因为我们需要管理缓存响应不足以回答输入查询的策略。此外，一些缓存可能类似，这可能导致错误的响应。良好管理响应并拥有足够的数据库可以帮助降低成本。
- en: Conclusion
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'LLM that we deploy might end up costing us too much and have inaccurate performance
    if we don’t treat them right. That’s why here are some strategies you could employ
    to optimize the performance and cost of your LLM in the cloud:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有正确处理 LLM，部署的 LLM 可能会花费过多并表现不准确。这就是为什么这里有一些策略可以用来优化你在云中的 LLM 的性能和成本：
- en: Have a clear budget plan,
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 制定明确的预算计划，
- en: Decide the right model size and hardware,
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决定合适的模型规模和硬件，
- en: Choose the suitable inference options,
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择合适的推理选项，
- en: Construct effective prompts,
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建有效的提示，
- en: Caching responses.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缓存响应。
- en: '**[](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**[Cornellius Yudha
    Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**** is a data science
    assistant manager and data writer. While working full-time at Allianz Indonesia,
    he loves to share Python and data tips via social media and writing media. Cornellius
    writes on a variety of AI and machine learning topics.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**[](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**[Cornellius Yudha
    Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**** 是一位数据科学助理经理和数据撰稿人。在全职工作于
    Allianz Indonesia 的同时，他喜欢通过社交媒体和写作媒体分享 Python 和数据技巧。Cornellius 涉及各种 AI 和机器学习主题的写作。'
- en: More On This Topic
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[The Best Strategies for Fine-Tuning Large Language Models](https://www.kdnuggets.com/the-best-strategies-for-fine-tuning-large-language-models)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化大型语言模型的最佳策略](https://www.kdnuggets.com/the-best-strategies-for-fine-tuning-large-language-models)'
- en: '[How to Make Large Language Models Play Nice with Your Software…](https://www.kdnuggets.com/how-to-make-large-language-models-play-nice-with-your-software-using-langchain)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何让大型语言模型与你的软件和谐相处…](https://www.kdnuggets.com/how-to-make-large-language-models-play-nice-with-your-software-using-langchain)'
- en: '[How a Level System can Help Forecast AI Costs](https://www.kdnuggets.com/2022/03/level-system-help-forecast-ai-costs.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[等级系统如何帮助预测 AI 成本](https://www.kdnuggets.com/2022/03/level-system-help-forecast-ai-costs.html)'
- en: '[Optimizing Your LLM for Performance and Scalability](https://www.kdnuggets.com/optimizing-your-llm-for-performance-and-scalability)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化你的 LLM 性能和可扩展性](https://www.kdnuggets.com/optimizing-your-llm-for-performance-and-scalability)'
- en: '[What are Large Language Models and How Do They Work?](https://www.kdnuggets.com/2023/05/large-language-models-work.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大型语言模型是什么，如何工作？](https://www.kdnuggets.com/2023/05/large-language-models-work.html)'
- en: '[Optimizing Python Code Performance: A Deep Dive into Python Profilers](https://www.kdnuggets.com/2023/02/optimizing-python-code-performance-deep-dive-python-profilers.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化 Python 代码性能：深入探讨 Python 分析器](https://www.kdnuggets.com/2023/02/optimizing-python-code-performance-deep-dive-python-profilers.html)'
