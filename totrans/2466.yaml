- en: Decision Tree Algorithm, Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/3a3912c533fa3be38427762c11a17e10.png)'
  prefs: []
  type: TYPE_IMG
- en: Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Classification is a two-step process, learning step and prediction step, in
    machine learning. In the learning step, the model is developed based on given
    training data. In the prediction step, the model is used to predict the response
    for given data. Decision Tree is one of the easiest and popular classification
    algorithms to understand and interpret.
  prefs: []
  type: TYPE_NORMAL
- en: Decision Tree Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decision Tree algorithm belongs to the family of supervised learning algorithms.
    Unlike other supervised learning algorithms, the decision tree algorithm can be
    used for solving **regression and classification problems** too.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of using a Decision Tree is to create a training model that can use
    to predict the class or value of the target variable by **learning simple decision
    rules** inferred from prior data(training data).
  prefs: []
  type: TYPE_NORMAL
- en: In Decision Trees, for predicting a class label for a record we start from the **root** of
    the tree. We compare the values of the root attribute with the record’s attribute.
    On the basis of comparison, we follow the branch corresponding to that value and
    jump to the next node.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Decision Trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Types of decision trees are based on the type of target variable we have. It
    can be of two types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Categorical Variable Decision Tree: **Decision Tree which has a categorical
    target variable then it called a **Categorical variable decision tree.**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Continuous Variable Decision Tree: **Decision Tree has a continuous target
    variable then it is called **Continuous Variable Decision Tree.**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Example:-** Let’s say we have a problem to predict whether a customer will
    pay his renewal premium with an insurance company (yes/ no). Here we know that
    the income of customers is a significant variable but the insurance company does
    not have income details for all customers. Now, as we know this is an important
    variable, then we can build a decision tree to predict customer income based on
    occupation, product, and various other variables. In this case, we are predicting
    values for the continuous variables.'
  prefs: []
  type: TYPE_NORMAL
- en: Important Terminology related to Decision Trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Root Node: **It represents the entire population or sample and this further
    gets divided into two or more homogeneous sets.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Splitting: **It is a process of dividing a node into two or more sub-nodes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Decision Node: **When a sub-node splits into further sub-nodes, then it is
    called the decision node.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Leaf / Terminal Node: **Nodes do not split is called Leaf or Terminal node.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pruning: **When we remove sub-nodes of a decision node, this process is called
    pruning. You can say the opposite process of splitting.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Branch / Sub-Tree: **A subsection of the entire tree is called branch or
    sub-tree.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Parent and Child Node: **A node, which is divided into sub-nodes is called
    a parent node of sub-nodes whereas sub-nodes are the child of a parent node.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/b8fac40aba9c2297b93f56d82a72a308.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision trees classify the examples by sorting them down the tree from the
    root to some leaf/terminal node, with the leaf/terminal node providing the classification
    of the example.
  prefs: []
  type: TYPE_NORMAL
- en: Each node in the tree acts as a test case for some attribute, and each edge
    descending from the node corresponds to the possible answers to the test case.
    This process is recursive in nature and is repeated for every subtree rooted at
    the new node.
  prefs: []
  type: TYPE_NORMAL
- en: Assumptions while creating Decision Tree
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Below are some of the assumptions we make while using Decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: In the beginning, the whole training set is considered as the **root.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature values are preferred to be categorical. If the values are continuous
    then they are discretized prior to building the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Records are **distributed recursively** on the basis of attribute values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Order to placing attributes as root or internal node of the tree is done by
    using some statistical approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision Trees follow **Sum of Product (SOP) r**epresentation. The Sum of product
    (SOP) is also known as **Disjunctive Normal Form**. For a class, every branch
    from the root of the tree to a leaf node having the same class is conjunction
    (product) of values, different branches ending in that class form a disjunction
    (sum).
  prefs: []
  type: TYPE_NORMAL
- en: The primary challenge in the decision tree implementation is to identify which
    attributes do we need to consider as the root node and each level. Handling this
    is to know as the attributes selection. We have different attributes selection
    measures to identify the attribute which can be considered as the root note at
    each level.
  prefs: []
  type: TYPE_NORMAL
- en: How do Decision Trees work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The decision of making strategic splits heavily affects a tree’s accuracy. The
    decision criteria are different for classification and regression trees.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees use multiple algorithms to decide to split a node into two or
    more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant
    sub-nodes. In other words, we can say that the purity of the node increases with
    respect to the target variable. The decision tree splits the nodes on all available
    variables and then selects the split which results in most homogeneous sub-nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm selection is also based on the type of target variables. Let
    us look at some algorithms used in Decision Trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ID3** → (extension of D3)'
  prefs: []
  type: TYPE_NORMAL
- en: '**C4.5** → (successor of ID3)'
  prefs: []
  type: TYPE_NORMAL
- en: '**CART** → (Classification And Regression Tree)'
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAID** → (Chi-square automatic interaction detection Performs multi-level
    splits when computing classification trees)'
  prefs: []
  type: TYPE_NORMAL
- en: '**MARS** → (multivariate adaptive regression splines)'
  prefs: []
  type: TYPE_NORMAL
- en: The ID3 algorithm builds decision trees using a top-down [greedy search ](https://www.hackerearth.com/practice/algorithms/greedy/basics-of-greedy-algorithms/tutorial/)approach
    through the space of possible branches with no backtracking. A greedy algorithm,
    as the name suggests, always makes the choice that seems to be the best at that
    moment.
  prefs: []
  type: TYPE_NORMAL
- en: '**Steps in ID3 algorithm:**'
  prefs: []
  type: TYPE_NORMAL
- en: It begins with the original set S as the root node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On each iteration of the algorithm, it iterates through the very unused attribute
    of the set S and calculates **Entropy(H)** and **Information gain(IG) **of this
    attribute.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It then selects the attribute which has the smallest Entropy or Largest Information
    gain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The set S is then split by the selected attribute to produce a subset of the
    data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The algorithm continues to recur on each subset, considering only attributes
    never selected before.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attribute Selection Measures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the dataset consists of **N** attributes then deciding which attribute to
    place at the root or at different levels of the tree as internal nodes is a complicated
    step. By just randomly selecting any node to be the root can’t solve the issue.
    If we follow a random approach, it may give us bad results with low accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'For solving this attribute selection problem, researchers worked and devised
    some solutions. They suggested using some *criteria* like :'
  prefs: []
  type: TYPE_NORMAL
- en: '**Entropy**,'
  prefs: []
  type: TYPE_NORMAL
- en: '**Information gain,**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gini index,**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gain Ratio,**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reduction in Variance**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Chi-Square**'
  prefs: []
  type: TYPE_NORMAL
- en: These criteria will calculate values for every attribute. The values are sorted,
    and attributes are placed in the tree by following the order i.e, the attribute
    with a high value(in case of information gain) is placed at the root.
  prefs: []
  type: TYPE_NORMAL
- en: While using Information Gain as a criterion, we assume attributes to be categorical,
    and for the Gini index, attributes are assumed to be continuous.
  prefs: []
  type: TYPE_NORMAL
- en: '**Entropy**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Entropy is a measure of the randomness in the information being processed. The
    higher the entropy, the harder it is to draw any conclusions from that information.
    Flipping a coin is an example of an action that provides information that is random.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e47ddfca8b8db435c48ef441aa56a83f.png)'
  prefs: []
  type: TYPE_IMG
- en: From the above graph, it is quite evident that the entropy H(X) is zero when
    the probability is either 0 or 1\. The Entropy is maximum when the probability
    is 0.5 because it projects perfect randomness in the data and there is no chance
    if perfectly determining the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: '**ID3 follows the rule — A branch with an entropy of zero is a leaf node and
    A brach with entropy more than zero needs further splitting.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Mathematically Entropy for 1 attribute is represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57204386d55659ed9b46775794a49db7.png)'
  prefs: []
  type: TYPE_IMG
- en: Where **S → Current state, and Pi → Probability of an event *i *of state S or
    Percentage of class *i* in a node of state S.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically Entropy for multiple attributes is represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a1944b5e9919dde88ddd097cdd6b44d6.png)'
  prefs: []
  type: TYPE_IMG
- en: where** T→ Current state and X → Selected attribute**
  prefs: []
  type: TYPE_NORMAL
- en: '**Information Gain**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Information gain or **IG **is a statistical property that measures how well
    a given attribute separates the training examples according to their target classification.
    Constructing a decision tree is all about finding an attribute that returns the
    highest information gain and the smallest entropy.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/d88b86d888a94376d44650a42cbea64e.png)[Information Gain](https://becominghuman.ai/decision-trees-in-machine-learning-f362b296594a?gi=a8ffb5170258)'
  prefs: []
  type: TYPE_IMG
- en: Information gain is a decrease in entropy. It computes the difference between
    entropy before split and average entropy after split of the dataset based on given
    attribute values. ID3 (Iterative Dichotomiser) decision tree algorithm uses information
    gain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, IG is represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4c38c19ad3a40772df5891ecee13348e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In a much simpler way, we can conclude that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4be9c5fa0c7275f79eee4c7096462fb3.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Information Gain](https://towardsdatascience.com/from-a-single-decision-tree-to-a-random-forest-b9523be65147)'
  prefs: []
  type: TYPE_NORMAL
- en: Where “before” is the dataset before the split, K is the number of subsets generated
    by the split, and (j, after) is subset j after the split.
  prefs: []
  type: TYPE_NORMAL
- en: Gini Index
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can understand the Gini index as a cost function used to evaluate splits
    in the dataset. It is calculated by subtracting the sum of the squared probabilities
    of each class from one. It favors larger partitions and easy to implement whereas
    information gain favors smaller partitions with distinct values.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/6669e4c680091234497fb509d5101542.png)Gini Index'
  prefs: []
  type: TYPE_NORMAL
- en: Gini Index works with the categorical target variable “Success” or “Failure”.
    It performs only Binary splits.
  prefs: []
  type: TYPE_NORMAL
- en: Higher value of Gini index implies higher inequality, higher heterogeneity.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Steps to Calculate Gini index for a split**'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate Gini for sub-nodes, using the above formula for success(p) and failure(q)
    (p²+q²).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the Gini index for split using the weighted Gini score of each node
    of that split.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CART (Classification and Regression Tree) uses the Gini index method to create
    split points.
  prefs: []
  type: TYPE_NORMAL
- en: Gain ratio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Information gain is biased towards choosing attributes with a large number of
    values as root nodes. It means it prefers the attribute with a large number of
    distinct values.
  prefs: []
  type: TYPE_NORMAL
- en: C4.5, an improvement of ID3, uses Gain ratio which is a modification of Information
    gain that reduces its bias and is usually the best option. Gain ratio overcomes
    the problem with information gain by taking into account the number of branches
    that would result before making the split. It corrects information gain by taking
    the intrinsic information of a split into account.
  prefs: []
  type: TYPE_NORMAL
- en: Let us consider if we have a dataset that has users and their movie genre preferences
    based on variables like gender, group of age, rating, blah, blah. With the help
    of information gain, you split at ‘Gender’ (assuming it has the highest information
    gain) and now the variables ‘Group of Age’ and ‘Rating’ could be equally important
    and with the help of gain ratio, it will penalize a variable with more distinct
    values which will help us decide the split at the next level.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Figure](../Images/a732854b47d32f5f3cdf5fed4b9fa605.png)[Gain Ratio](https://towardsdatascience.com/from-a-single-decision-tree-to-a-random-forest-b9523be65147)'
  prefs: []
  type: TYPE_IMG
- en: Where “before” is the dataset before the split, K is the number of subsets generated
    by the split, and (j, after) is subset j after the split.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reduction in Variance**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Reduction in variance** is an algorithm used for continuous target variables
    (regression problems). This algorithm uses the standard formula of variance to
    choose the best split. The split with lower variance is selected as the criteria
    to split the population:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb8247ef89486499b2dbcc281977b898.png)'
  prefs: []
  type: TYPE_IMG
- en: Above X-bar is the mean of the values, X is actual and n is the number of values.
  prefs: []
  type: TYPE_NORMAL
- en: '**Steps to calculate Variance:**'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate variance for each node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate variance for each split as the weighted average of each node variance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Chi-Square**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The acronym CHAID stands for *Chi*-squared Automatic Interaction Detector. It
    is one of the oldest tree classification methods. It finds out the statistical
    significance between the differences between sub-nodes and parent node. We measure
    it by the sum of squares of standardized differences between observed and expected
    frequencies of the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: It works with the categorical target variable “Success” or “Failure”. It can
    perform two or more splits. Higher the value of Chi-Square higher the statistical
    significance of differences between sub-node and Parent node.
  prefs: []
  type: TYPE_NORMAL
- en: It generates a tree called CHAID (Chi-square Automatic Interaction Detector).
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, Chi-squared is represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c6e29fe0d58d993aae89fa9762398c2f.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Steps to Calculate Chi-square for a split:**'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate Chi-square for an individual node by calculating the deviation for
    Success and Failure both
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculated Chi-square of Split using Sum of all Chi-square of success and Failure
    of each node of the split
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**How to avoid/counter Overfitting in Decision Trees?**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The common problem with Decision trees, especially having a table full of columns,
    they fit a lot. Sometimes it looks like the tree memorized the training data set.
    If there is no limit set on a decision tree, it will give you 100% accuracy on
    the training data set because in the worse case it will end up making 1 leaf for
    each observation. Thus this affects the accuracy when predicting samples that
    are not part of the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are two ways to remove overfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: Pruning Decision Trees.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Random Forest
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pruning Decision Trees**'
  prefs: []
  type: TYPE_NORMAL
- en: The splitting process results in fully grown trees until the stopping criteria
    are reached. But, the fully grown tree is likely to overfit the data, leading
    to poor accuracy on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/1c59703d8f644408230aa4c6ea5e4684.png)[Pruning in action](https://gfycat.com/enchantedyellowishbarasinga)'
  prefs: []
  type: TYPE_IMG
- en: 'In **pruning**, you trim off the branches of the tree, i.e., remove the decision
    nodes starting from the leaf node such that the overall accuracy is not disturbed.
    This is done by segregating the actual training set into two sets: training data
    set, D and validation data set, V. Prepare the decision tree using the segregated
    training data set, D. Then continue trimming the tree accordingly to optimize
    the accuracy of the validation data set, V.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/7bb957cdb54435ba9af5e032a34af199.png)[Pruning](https://www.cs.cmu.edu/~bhiksha/courses/10-601/decisiontrees/)'
  prefs: []
  type: TYPE_IMG
- en: In the above diagram, the ‘Age’ attribute in the left-hand side of the tree
    has been pruned as it has more importance on the right-hand side of the tree,
    hence removing overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '**Random Forest**'
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest is an example of ensemble learning, in which we combine multiple
    machine learning algorithms to obtain better predictive performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why the name “Random”?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two key concepts that give it the name random:'
  prefs: []
  type: TYPE_NORMAL
- en: A random sampling of training data set when building trees.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Random subsets of features considered when splitting nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A technique known as bagging is used to create an ensemble of trees where multiple
    training sets are generated with replacement.
  prefs: []
  type: TYPE_NORMAL
- en: In the bagging technique, a data set is divided into **N** samples using randomized
    sampling. Then, using a single learning algorithm a model is built on all samples.
    Later, the resultant predictions are combined using voting or averaging in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/9a79dc5b0e44ede04284ddcb44888a0a.png)[Random Forest in
    action](https://towardsdatascience.com/why-random-forests-outperform-decision-trees-1b0f175a0b5)'
  prefs: []
  type: TYPE_IMG
- en: Which is better Linear or tree-based models?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Well, it depends on the kind of problem you are solving.
  prefs: []
  type: TYPE_NORMAL
- en: If the relationship between dependent & independent variables is well approximated
    by a linear model, linear regression will outperform the tree-based model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If there is a high non-linearity & complex relationship between dependent &
    independent variables, a tree model will outperform a classical regression method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you need to build a model that is easy to explain to people, a decision tree
    model will always do better than a linear model. Decision tree models are even
    simpler to interpret than linear regression!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decision Tree Classifier Building in Scikit-learn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The dataset that we have is a supermarket data which can be downloaded from [here](https://drive.google.com/open?id=1x1KglkvJxNn8C8kzeV96YePFnCUzXhBS).
  prefs: []
  type: TYPE_NORMAL
- en: Load all the basic libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Load the dataset. It consists of 5 features, `UserID`, `Gender`, `Age`, `EstimatedSalary`
    and `Purchased`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure](../Images/3b51f398bfcc049794918ba1ef132e40.png)Dataset'
  prefs: []
  type: TYPE_NORMAL
- en: We will take only `Age` and `EstimatedSalary` as our independent variables `X` because
    of other features like `Gender` and `User ID` are irrelevant and have no effect
    on the purchasing capacity of a person. Purchased is our dependent variable `y`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to split the dataset into training and test.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Perform feature scaling
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Fit the model in the Decision Tree classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Make predictions and check accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The decision tree classifier gave an accuracy of 91%.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion Matrix
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It means 6 observations have been classified as false.
  prefs: []
  type: TYPE_NORMAL
- en: '**Let us first visualize the model prediction results.**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ddd705134cdcc0f99c1814fe4b72041a.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Let us also visualize the tree:**'
  prefs: []
  type: TYPE_NORMAL
- en: You can use Scikit-learn’s *export_graphviz* function to display the tree within
    a Jupyter notebook. For plotting trees, you also need to install Graphviz and
    pydotplus.
  prefs: []
  type: TYPE_NORMAL
- en: '`conda install python-graphviz`'
  prefs: []
  type: TYPE_NORMAL
- en: '`pip install pydotplus`'
  prefs: []
  type: TYPE_NORMAL
- en: '*export_graphviz* function converts decision tree classifier into dot file
    and pydotplus convert this dot file to png or displayable form on Jupyter.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure](../Images/7f1f7d8aa6d9c4a09aa5e79ec82e76a1.png)Decision Tree.'
  prefs: []
  type: TYPE_NORMAL
- en: In the decision tree chart, each internal node has a decision rule that splits
    the data. Gini referred to as the Gini ratio, which measures the impurity of the
    node. You can say a node is pure when all of its records belong to the same class,
    such nodes known as the leaf node.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the resultant tree is unpruned. This unpruned tree is unexplainable and
    not easy to understand. In the next section, let’s optimize it by pruning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimizing the Decision Tree Classifier**'
  prefs: []
  type: TYPE_NORMAL
- en: '**criterion**: optional (default=”gini”) or Choose attribute selection measure:
    This parameter allows us to use the different-different attribute selection measure.
    Supported criteria are “gini” for the Gini index and “entropy” for the information
    gain.'
  prefs: []
  type: TYPE_NORMAL
- en: '**splitter**: string, optional (default=”best”) or Split Strategy: This parameter
    allows us to choose the split strategy. Supported strategies are “best” to choose
    the best split and “random” to choose the best random split.'
  prefs: []
  type: TYPE_NORMAL
- en: '**max_depth**: int or None, optional (default=None) or Maximum Depth of a Tree:
    The maximum depth of the tree. If None, then nodes are expanded until all the
    leaves contain less than min_samples_split samples. The higher value of maximum
    depth causes overfitting, and a lower value causes underfitting (Source).'
  prefs: []
  type: TYPE_NORMAL
- en: In Scikit-learn, optimization of decision tree classifier performed by only
    pre-pruning. The maximum depth of the tree can be used as a control variable for
    pre-pruning.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Well, the classification rate increased to 94%, which is better accuracy than
    the previous model.
  prefs: []
  type: TYPE_NORMAL
- en: Now let us again visualize the pruned Decision tree after optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure](../Images/c76148e38059f19cd1326993c049ed16.png)Decision Tree after
    pruning'
  prefs: []
  type: TYPE_NORMAL
- en: This pruned model is less complex, explainable, and easy to understand than
    the previous decision tree model plot.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this article, we have covered a lot of details about Decision Tree; It’s
    working, attribute selection measures such as Information Gain, Gain Ratio, and
    Gini Index, decision tree model building, visualization and evaluation on supermarket
    dataset using Python Scikit-learn package and optimizing Decision Tree performance
    using parameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Well, that’s all for this article hope you guys have enjoyed reading it, feel
    free to share your comments/thoughts/feedback in the comment section.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/fe29fca4d34cf017cf70bdb1fde78042.png)[Source](https://www.ilikesticker.com/LineStickerAnimation/W433402-Movie-End-Credits-Style-[English-Ver]/en)'
  prefs: []
  type: TYPE_IMG
- en: Thanks for reading !!!
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Nagesh Singh Chauhan](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)**
    is a Data Science enthusiast. Interested in Big Data, Python, Machine Learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/decision-tree-algorithm-explained-83beb6e78ef4).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Simplifying Decision Tree Interpretability with Python & Scikit-learn](https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding by Implementing: Decision Tree](https://www.kdnuggets.com/2023/02/understanding-implementing-decision-tree.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Telling a Great Data Story: A Visualization Decision Tree](https://www.kdnuggets.com/2021/02/telling-great-data-story-visualization-decision-tree.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Random Forest vs Decision Tree: Key Differences](https://www.kdnuggets.com/2022/02/random-forest-decision-tree-key-differences.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets™ News 22:n09, Mar 2: Telling a Great Data Story: A…](https://www.kdnuggets.com/2022/n09.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Complete Guide To Decision Tree Software](https://www.kdnuggets.com/2022/08/complete-guide-decision-tree-software.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
