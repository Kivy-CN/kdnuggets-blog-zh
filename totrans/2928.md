# 如何自动化超参数优化

> 原文：[https://www.kdnuggets.com/2019/06/automate-hyperparameter-optimization.html](https://www.kdnuggets.com/2019/06/automate-hyperparameter-optimization.html)

![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)

**由 [Suleka Helmini](https://www.linkedin.com/in/suleka-helmini-09803b132/detail/recent-activity/)，WSO2**

### 使用Scikit-Optimize进行贝叶斯优化的初学者指南

在机器学习和深度学习范式中，“参数”和“超参数”是两个经常使用的术语，其中“参数”定义了模型内部的配置变量，其值可以从训练数据中估计，而“超参数”定义了模型外部的配置变量，其值无法从训练数据中估计（[参数和超参数之间有什么区别？](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/)）。因此，超参数值需要由实践者手动分配。

我们制作的每个机器学习和深度学习模型都有一组不同的超参数值，需要通过微调才能获得令人满意的结果。与机器学习模型相比，深度学习模型通常具有更多的超参数需要优化，因为其架构复杂度高于典型的机器学习模型。

反复手动尝试不同的值组合以得出这些超参数的最佳值可能是一个非常耗时且繁琐的任务，这需要良好的直觉、丰富的经验以及对模型的深刻理解。此外，一些超参数值可能需要连续值，这将有无限的可能性，即使超参数需要离散值，可能性也非常庞大，因此手动执行这个任务相当困难。尽管如此，超参数优化可能看起来很具挑战性，但由于网络上有几个现成的库，这个任务变得更加简单。这些库有助于以更少的精力实现不同的超参数优化算法。其中一些库包括[Scikit-Optimize](https://scikit-optimize.github.io/)、[Scikit-Learn](https://scikit-learn.org/stable/)和[Hyperopt](https://github.com/hyperopt/hyperopt)。

有几种超参数优化算法在多年中被频繁使用，包括 Grid Search、Random Search 和自动化超参数优化方法。Grid Search 和 Random Search 都设置了超参数的网格，但在 Grid Search 中，每一个值组合都会被彻底探索，以寻找最佳的超参数值组合，使得该方法非常低效。另一方面，Random Search 会不断从网格中选择随机组合，直到达到指定的迭代次数，并且已被证明比 Grid Search 更有效。然而，即使它能够提供一个良好的超参数组合，我们也不能确定这是否是**最佳**组合。自动化超参数优化使用不同的技术，如贝叶斯优化，它进行引导式搜索以找到最佳超参数 ([超参数调整使用 Grid 和 Random 搜索](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search))。研究表明，贝叶斯优化可以提供比 Random Search 更好的超参数组合 ([贝叶斯优化用于超参数调整](https://arimo.com/data-science/2016/bayesian-optimization-hyperparameter-tuning/))。

在本文中，我们将提供一个逐步指南，介绍如何通过使用高斯过程的贝叶斯优化对深度学习模型进行超参数优化。我们使用了 [gp_minimize](https://scikit-optimize.github.io/#skopt.gp_minimize) 包，该包由 [Scikit-Optimize (skopt)](https://scikit-optimize.github.io/) 库提供，来完成此任务。我们将在一个使用 [TensorFlow](https://www.tensorflow.org/) 开发的简单股票收盘价预测模型上进行超参数优化。

### Scikit-Optimize (skopt)

Scikit-Optimize 是一个相对容易使用的库，比其他超参数优化库更具社区支持和文档说明。该库通过减少昂贵且嘈杂的黑箱函数，实现了几种序列模型优化的方法。

使用高斯过程的贝叶斯优化

贝叶斯优化是 skopt 提供的众多功能之一。贝叶斯优化在参数优化过程中找到后验分布作为待优化的函数，然后使用获取函数（例如，期望改进-EI、另一个函数等）从该后验分布中采样，以找到下一组待探索的参数。由于贝叶斯优化基于考虑了可用数据的更系统方法来决定下一个点，因此预计比网格搜索和随机搜索等耗尽的参数优化技术更快地实现更好的配置。你可以从 [这里](https://orbi.uliege.be/bitstream/2268/226433/1/PyData%202017_%20Bayesian%20optimization%20with%20Scikit-Optimize.pdf) 了解更多关于贝叶斯优化器的信息。

### 代码提醒！

所以，理论讲解完了，我们开始实际操作吧！

这个示例代码是使用 Python 和 TensorFlow 完成的。此外，这次超参数优化任务的目标是获得一组能使我们的深度学习模型达到最低可能均方根误差（RMSE）的超参数值。我们希望这对任何首次尝试的人来说都非常简单。

首先，让我们安装 Scikit-Optimize。你可以通过执行此命令使用 pip 安装它。

[PRE0]

请注意，你需要对现有的深度学习模型代码进行一些调整，以便使其能够与优化功能兼容。

首先，让我们进行一些必要的导入。

[PRE1]

现在我们将设置 TensorFlow 和 Numpy 的种子，以便获得可重复的结果。

[PRE2]

下方展示了一些我们声明的关键 Python 全局变量。在这些变量中，我们还声明了我们希望优化的超参数（第二组变量）。

[PRE3]

“input_size” 描述了预测的一部分形状。“features” 描述了数据集中的特征数量，“columns” 列表包含了这两个特征的标题名称。“column_min_max” 变量包含了这两个特征的上下缩放范围（这是通过检查验证和训练拆分得出的）。

在声明了所有这些变量之后，终于可以声明我们希望优化的每个超参数的搜索空间。

[PRE4]

如果你仔细观察，你会发现我们在 log-uniform 之前声明了 ‘lstm_init_learning_rate’，而不是直接使用 uniform。这是因为，如果你使用 uniform，优化器将需要在均匀分布下从 1e-4（0.0001）搜索到 1e-1（0.1）。但当声明为 log-uniform 时，优化器将在 -4 和 -1 之间搜索，从而使过程更加高效。这是 skopt 库在为学习率分配搜索空间时的 [建议](https://scikit-optimize.github.io/notebooks/hyperparameter-optimization.html)。

你可以使用几种数据类型来定义搜索空间。这些数据类型包括Categorical、Real和Integer。当定义涉及浮点值的搜索空间时，你应该选择“Real”；如果涉及整数，则选择“Integer”。如果你的搜索空间涉及分类值，例如不同的激活函数，那么你应该选择“Categorical”类型。

我们现在将要在“dimensions”列表中列出我们要优化的参数。这个列表稍后将传递给“gp_minimize”函数。你可以看到我们也声明了“default_parameters”。这些是我们为每个超参数提供的默认参数值。****记得按照你在“dimensions”列表中列出的超参数的顺序输入默认值。****

[PRE5]

最重要的一点是，“default_parameters”列表中的超参数将成为你优化任务的起点。贝叶斯优化器将使用你在第一次迭代中声明的默认参数，并根据结果，获取函数将决定接下来要探索的点。

可以说，如果你之前多次运行模型并找到了一个不错的超参数值集，你可以将它们作为默认超参数值，并从那里开始探索。这可能会帮助算法更快地找到最低的RMSE值（迭代次数更少）。但是，请记住，这可能并不总是正确的。此外，记得在分配默认值时，分配一个在你定义的搜索空间内的值。

到目前为止，我们已经完成了超参数优化任务的所有初步工作。我们现在将重点关注深度学习模型的实现。由于本文仅关注超参数优化任务，因此我们将不讨论模型开发过程中的数据预处理。我们将在本文末尾提供完整实现的GitHub链接。

然而，为了给你更多背景，我们将数据集分为训练、验证和测试三个部分。训练集用于训练模型，验证集用于进行超参数优化任务。如前所述，我们使用均方根误差（RMSE）来评估模型并执行优化（最小化RMSE）。

使用验证分割评估的准确率不能用于评估模型，因为在超参数优化过程中，选择的最小化RMSE的超参数可能会对验证集过拟合。因此，标准程序是使用在管道中的任何点都未使用的测试分割来衡量最终模型的准确性。

下面展示的是我们深度学习模型的实现：

[PRE6]

“setupRNN”函数包含了我们的深度学习模型。不过，你可能不需要理解那些细节，因为贝叶斯优化将这个函数视为一个黑箱，它接受某些超参数作为输入，然后输出预测结果。所以，如果你不关心这个函数的内部实现，你可以跳过下一段。

我们的深度学习模型包含一个 LSTM 层、一个 dropout 层和一个输出层。模型运作所需的必要信息需要传递给这个函数（在我们的例子中，是输入和 dropout 率）。然后，你可以在这个函数内部实现你的深度学习模型。在我们的例子中，我们使用了 LSTM 层来识别股票数据集的时间依赖性。

然后，我们将 LSTM 的最后输出传递给 dropout 层进行正则化，并通过输出层获得预测结果。最后，记得将这个预测结果（在分类任务中，这可以是你的 logit）返回给将传递给贝叶斯优化的函数（“setupRNN”将由这个函数调用）。

如果你正在为机器学习算法进行超参数优化（使用像 Scikit-Learn 这样的库），你不需要单独编写函数来实现模型，因为模型本身已经由库提供，你只需编写代码来训练和获得预测。因此，这段代码可以放在将返回给贝叶斯优化的函数内部。

现在我们进入超参数优化任务中最重要的部分——“适应度”函数。

[PRE7]

如上所示，我们将超参数值传递给一个名为“fitness”的函数。这个“fitness”函数将被传递给贝叶斯超参数优化过程（*gp_minimize*）。请注意，在第一次迭代中，传递给这个函数的值将是你定义的默认值，从此以后贝叶斯优化将自行选择超参数值。然后，我们将选定的值赋给在开始时声明的 Python 全局变量，以便能够在适应度函数外部使用最新选定的超参数值。

我们接着来到了优化任务中的一个关键点。如果你在阅读本文之前使用过 TensorFlow，你会知道 TensorFlow 通过为你创建的任何深度学习模型创建计算图来进行操作。

在超参数优化过程中，在每次迭代中，我们将重置现有的计算图并构建一个新的图。这一过程是为了最小化图所占用的内存，防止图堆叠在一起。在重置图之后，你必须设置 TensorFlow 随机种子，以获得可重复的结果。在上述过程完成后，我们可以最终声明 TensorFlow 会话。

在这一点之后，你可以开始添加负责训练和验证深度学习模型的代码，这与你通常做的方式一致。这一部分与优化过程并不直接相关，但此后代码将开始利用贝叶斯优化选择的超参数值。

这里要记住的要点是****返回****为验证拆分获得的最终度量值（在此案例中为RMSE值）。这个值将返回给贝叶斯优化过程，并在决定下一组超参数时使用。

****注意****：如果你处理的是分类问题，你需要将准确度作为负值（例如-96），因为尽管准确度越高模型越好，但贝叶斯函数将持续尝试减少这个值，因为它设计用于找到返回的****最低****值的超参数值。

现在让我们写下整个过程的执行点，即“main”函数。在主函数中，我们声明了“gp_minimize”函数。然后，我们将几个重要的参数传递给这个函数。

[PRE8]

“func”参数是你最终希望使用贝叶斯优化器建模的函数。“dimensions”参数是你希望优化的超参数集，而“acq_func”代表采集函数，是决定下一组应该使用的超参数值的函数。*gp_minimize*支持4种采集函数。它们是：

+   LCB: 下置信界限

+   EI: 期望改进

+   PI: 改进的概率

+   gp_hedge: 在每次迭代时概率性地选择上述三种采集函数中的一种

上述信息摘自文档。每种方法都有其自身的优点，但如果你是贝叶斯优化的新手，建议尝试使用“EI”或“gp_hedge”，因为“EI”是最广泛使用的采集函数，而“gp_hedge”会根据概率选择上述采集函数中的一种，因此你无需过于担心。

请记住，当使用不同的采集函数时，可能还会有其他你希望调整的参数，这会影响你选择的采集函数。请参阅[文档](https://scikit-optimize.github.io/#skopt.gp_minimize)中的参数列表。

回到解释其余参数，“n_calls”参数是你希望运行适应度函数的次数。优化任务将首先使用由“x0”定义的超参数值，即默认超参数值。最后，我们设置超参数优化器的随机状态，以确保结果的可重复性。

现在，当你运行*gp_optimize*函数时，事件的流程将是：

适应度函数将使用传递给 x0 的参数。LSTM 将按照指定的 epoch 进行训练，并运行验证输入以获取其预测的 RMSE 值。然后，根据该值，贝叶斯优化器将决定通过获取函数探索下一个超参数值集。

在第2次迭代中，适应度函数将使用贝叶斯优化得出的超参数值进行运行，并且相同的过程将重复，直到迭代了“n_call”次。当整个过程结束时，Scikit-Optimize 对象将分配给“search_result”变量。

我们可以使用这个对象来检索文档中声明的有用信息。

+   x [list]: 最小值的位置。

+   fun [float]: 最小值处的函数值。

+   models: 每次迭代使用的代理模型。

+   x_iters [list of lists]: 每次迭代的函数评估位置。

+   func_vals [array]: 每次迭代的函数值。

+   space [Space]: 优化空间。

+   specs [dict]`: 调用规格。

+   rng [RandomState instance]: 最小化结束时的随机状态。

“search_result.x” 给我们提供了最佳超参数值，使用“search_result.fun”我们可以获得与获得的超参数值相对应的验证集 RMSE 值（即验证集获得的最低 RMSE 值）。

下面显示的是我们为模型获得的最佳超参数值和验证集的最低 RMSE 值。如果你发现难以确定使用“search_result.x”时超参数值的列出顺序，它与您在“dimensions”列表中指定的超参数顺序相同。

****超参数值:****

+   lstm_num_steps: 6

+   lstm_size: 171

+   lstm_init_epoch: 3

+   lstm_max_epoch: 58

+   lstm_learning_rate_decay: 0.7518394019565194

+   lstm_batch_size: 24

+   lstm_dropout_rate: 0.21830825193089087

+   lstm_init_learning_rate: 0.0006401363567813549

****最低 RMSE:**** 2.73755355221523

### 收敛图

在这个图表中产生最低点的超参数值就是我们获得的最佳超参数值集。

![figure-name](../Images/bdd75378596d881565195f7e17f34e3b.png)

图表展示了贝叶斯优化和随机搜索中每次迭代（50 次迭代）记录的最低 RMSE 值的比较。我们可以看到，贝叶斯优化比随机搜索收敛得更好。然而，在开始时，我们可以看到随机搜索比贝叶斯优化器更快地找到了更好的最小值。这可能是由于随机搜索的随机采样特性。

我们终于结束了这篇文章，总之，我们希望这篇文章通过向你展示一种更好的寻找超参数最优集合的方法，让你在深度学习模型构建任务中更轻松。不再为超参数优化感到压力。祝编程愉快，各位极客！

### 实用材料：

+   完整的代码可以通过这个 [链接](https://github.com/suleka96/Bayesian_Hyperparameter_optimization/tree/master)找到。

+   有关贝叶斯优化的更多信息，可以参考这篇 [论文](https://arxiv.org/pdf/1807.02811.pdf)。

+   有关获取函数的更多信息，请参阅这篇 [论文](https://www.cse.wustl.edu/~garnett/cse515t/spring_2015/files/lecture_notes/12.pdf)。

**简介：[Suleka Helmini](https://www.linkedin.com/in/suleka-helmini-09803b132/)** 是 WS02 的软件工程师。

[原文](https://dzone.com/articles/how-to-automate-hyperparameter-optimization)。经许可转载。

**相关：**

+   [贝叶斯优化的直觉与高斯过程](/2018/10/intuitions-behind-bayesian-optimization-gaussian-processes.html)

+   [在 Google Colab 中使用 Hyperas 调整 Keras 超参数](/2018/12/keras-hyperparameter-tuning-google-colab-hyperas.html)

+   [使用 AutoML 生成 TPOT 机器学习管道](/2018/01/managing-machine-learning-workflows-scikit-learn-pipelines-part-4.html)

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 为你的组织提供 IT 支持

* * *

### 了解更多主题

+   [超参数优化：10 个顶级 Python 库](https://www.kdnuggets.com/2023/01/hyperparameter-optimization-10-top-python-libraries.html)

+   [使用 Python 自动化的 5 项任务](https://www.kdnuggets.com/2021/06/5-tasks-automate-python.html)

+   [使用 Python 自动化 Microsoft Excel 和 Word](https://www.kdnuggets.com/2021/08/automate-microsoft-excel-word-python.html)

+   [使用 GPT-4 和 Python 自动化无聊任务](https://www.kdnuggets.com/2023/03/automate-boring-stuff-chatgpt-python.html)

+   [使用 Promptr 和 GPT 自动化代码库](https://www.kdnuggets.com/2023/04/automate-codebase-promptr-gpt.html)

+   [使用 ChatGPT Canva 插件自动化图形设计活动](https://www.kdnuggets.com/automate-graphic-design-activity-with-chatgpt-canva-plugin)
