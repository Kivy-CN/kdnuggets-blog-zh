["```py\nfrom skmultilearn.model_selection import iterative_train_test_split\n\nmlb = MultiLabelBinarizer()\n\ndef balanced_split(df, mlb, test_size=0.5):\n    ind = np.expand_dims(np.arange(len(df)), axis=1)\n    mlb.fit_transform(df[\"tag\"])\n    labels = mlb.transform(df[\"tag\"])\n    ind_train, _, ind_test, _ = iterative_train_test_split(\n        ind, labels, test_size\n    )\n    return df.iloc[ind_train[:, 0]], df.iloc[ind_test[:, 0]]\n\ndf_train, df_tmp = balanced_split(df, test_size=0.4)\ndf_val, df_test = balanced_split(df_tmp, test_size=0.5)\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nclass ResampleLoss(nn.Module):\n    def __init__(\n        self,\n        use_sigmoid=True,\n        partial=False,\n        loss_weight=1.0,\n        reduction=\"mean\",\n        reweight_func=None,\n        weight_norm=None,\n        focal=dict(focal=True, alpha=0.5, gamma=2),\n        map_param=dict(alpha=10.0, beta=0.2, gamma=0.1),\n        CB_loss=dict(CB_beta=0.9, CB_mode=\"average_w\"),\n        logit_reg=dict(neg_scale=5.0, init_bias=0.1),\n        class_freq=None,\n        train_num=None,\n    ):\n        super(ResampleLoss, self).__init__()\n        assert (use_sigmoid is True) or (partial is False)\n        self.use_sigmoid = use_sigmoid\n        self.partial = partial\n        self.loss_weight = loss_weight\n        self.reduction = reduction\n        if self.use_sigmoid:\n            if self.partial:\n                self.cls_criterion = partial_cross_entropy\n            else:\n                self.cls_criterion = binary_cross_entropy\n        else:\n            self.cls_criterion = cross_entropy\n        # reweighting function\n        self.reweight_func = reweight_func\n        # normalization (optional)\n        self.weight_norm = weight_norm\n        # focal loss params\n        self.focal = focal[\"focal\"]\n        self.gamma = focal[\"gamma\"]\n        self.alpha = focal[\"alpha\"]\n        # mapping function params\n        self.map_alpha = map_param[\"alpha\"]\n        self.map_beta = map_param[\"beta\"]\n        self.map_gamma = map_param[\"gamma\"]\n        # CB loss params (optional)\n        self.CB_beta = CB_loss[\"CB_beta\"]\n        self.CB_mode = CB_loss[\"CB_mode\"]\n        self.class_freq = (\n            torch.from_numpy(np.asarray(class_freq)).float().cuda()\n        )\n        self.num_classes = self.class_freq.shape[0]\n        self.train_num = train_num  # only used to be divided by class_freq\n        # regularization params\n        self.logit_reg = logit_reg\n        self.neg_scale = (\n            logit_reg[\"neg_scale\"] if \"neg_scale\" in logit_reg else 1.0\n        )\n        init_bias = (\n            logit_reg[\"init_bias\"] if \"init_bias\" in logit_reg else 0.0\n        )\n        self.init_bias = (\n            -torch.log(self.train_num / self.class_freq - 1) * init_bias\n        )\n        self.freq_inv = (\n            torch.ones(self.class_freq.shape).cuda() / self.class_freq\n        )\n        self.propotion_inv = self.train_num / self.class_freq\n\n    def forward(\n        self,\n        cls_score,\n        label,\n        weight=None,\n        avg_factor=None,\n        reduction_override=None,\n        **kwargs\n    ):\n        assert reduction_override in (None, \"none\", \"mean\", \"sum\")\n        reduction = (\n            reduction_override if reduction_override else self.reduction\n        )\n        weight = self.reweight_functions(label)\n        cls_score, weight = self.logit_reg_functions(\n            label.float(), cls_score, weight\n        )\n        if self.focal:\n            logpt = self.cls_criterion(\n                cls_score.clone(),\n                label,\n                weight=None,\n                reduction=\"none\",\n                avg_factor=avg_factor,\n            )\n            # pt is sigmoid(logit) for pos or sigmoid(-logit) for neg\n            pt = torch.exp(-logpt)\n            wtloss = self.cls_criterion(\n                cls_score, label.float(), weight=weight, reduction=\"none\"\n            )\n            alpha_t = torch.where(label == 1, self.alpha, 1 - self.alpha)\n            loss = alpha_t * ((1 - pt) ** self.gamma) * wtloss\n            loss = reduce_loss(loss, reduction)\n        else:\n            loss = self.cls_criterion(\n                cls_score, label.float(), weight, reduction=reduction\n            )\n        loss = self.loss_weight * loss\n        return loss\n\n    def reweight_functions(self, label):\n        if self.reweight_func is None:\n            return None\n        elif self.reweight_func in [\"inv\", \"sqrt_inv\"]:\n            weight = self.RW_weight(label.float())\n        elif self.reweight_func in \"rebalance\":\n            weight = self.rebalance_weight(label.float())\n        elif self.reweight_func in \"CB\":\n            weight = self.CB_weight(label.float())\n        else:\n            return None\n        if self.weight_norm is not None:\n            if \"by_instance\" in self.weight_norm:\n                max_by_instance, _ = torch.max(weight, dim=-1, keepdim=True)\n                weight = weight / max_by_instance\n            elif \"by_batch\" in self.weight_norm:\n                weight = weight / torch.max(weight)\n        return weight\n\n    def logit_reg_functions(self, labels, logits, weight=None):\n        if not self.logit_reg:\n            return logits, weight\n        if \"init_bias\" in self.logit_reg:\n            logits += self.init_bias\n        if \"neg_scale\" in self.logit_reg:\n            logits = logits * (1 - labels) * self.neg_scale + logits * labels\n            if weight is not None:\n                weight = (\n                    weight / self.neg_scale * (1 - labels) + weight * labels\n                )\n        return logits, weight\n\n    def rebalance_weight(self, gt_labels):\n        repeat_rate = torch.sum(\n            gt_labels.float() * self.freq_inv, dim=1, keepdim=True\n        )\n        pos_weight = (\n            self.freq_inv.clone().detach().unsqueeze(0) / repeat_rate\n        )\n        # pos and neg are equally treated\n        weight = (\n            torch.sigmoid(self.map_beta * (pos_weight - self.map_gamma))\n            + self.map_alpha\n        )\n        return weight\n\n    def CB_weight(self, gt_labels):\n        if \"by_class\" in self.CB_mode:\n            weight = (\n                torch.tensor((1 - self.CB_beta)).cuda()\n                / (1 - torch.pow(self.CB_beta, self.class_freq)).cuda()\n            )\n        elif \"average_n\" in self.CB_mode:\n            avg_n = torch.sum(\n                gt_labels * self.class_freq, dim=1, keepdim=True\n            ) / torch.sum(gt_labels, dim=1, keepdim=True)\n            weight = (\n                torch.tensor((1 - self.CB_beta)).cuda()\n                / (1 - torch.pow(self.CB_beta, avg_n)).cuda()\n            )\n        elif \"average_w\" in self.CB_mode:\n            weight_ = (\n                torch.tensor((1 - self.CB_beta)).cuda()\n                / (1 - torch.pow(self.CB_beta, self.class_freq)).cuda()\n            )\n            weight = torch.sum(\n                gt_labels * weight_, dim=1, keepdim=True\n            ) / torch.sum(gt_labels, dim=1, keepdim=True)\n        elif \"min_n\" in self.CB_mode:\n            min_n, _ = torch.min(\n                gt_labels * self.class_freq + (1 - gt_labels) * 100000,\n                dim=1,\n                keepdim=True,\n            )\n            weight = (\n                torch.tensor((1 - self.CB_beta)).cuda()\n                / (1 - torch.pow(self.CB_beta, min_n)).cuda()\n            )\n        else:\n            raise NameError\n        return weight\n\n    def RW_weight(self, gt_labels, by_class=True):\n        if \"sqrt\" in self.reweight_func:\n            weight = torch.sqrt(self.propotion_inv)\n        else:\n            weight = self.propotion_inv\n        if not by_class:\n            sum_ = torch.sum(weight * gt_labels, dim=1, keepdim=True)\n            weight = sum_ / torch.sum(gt_labels, dim=1, keepdim=True)\n        return weight\n\ndef reduce_loss(loss, reduction):\n    \"\"\"Reduce loss as specified.\n    Args:\n        loss (Tensor): Elementwise loss tensor.\n        reduction (str): Options are \"none\", \"mean\" and \"sum\".\n    Return:\n        Tensor: Reduced loss tensor.\n    \"\"\"\n    reduction_enum = F._Reduction.get_enum(reduction)\n    # none: 0, elementwise_mean:1, sum: 2\n    if reduction_enum == 0:\n        return loss\n    elif reduction_enum == 1:\n        return loss.mean()\n    elif reduction_enum == 2:\n        return loss.sum()\n\ndef weight_reduce_loss(loss, weight=None, reduction=\"mean\", avg_factor=None):\n    \"\"\"Apply element-wise weight and reduce loss.\n    Args:\n        loss (Tensor): Element-wise loss.\n        weight (Tensor): Element-wise weights.\n        reduction (str): Same as built-in losses of PyTorch.\n        avg_factor (float): Avarage factor when computing the mean of losses.\n    Returns:\n        Tensor: Processed loss values.\n    \"\"\"\n    # if weight is specified, apply element-wise weight\n    if weight is not None:\n        loss = loss * weight\n    # if avg_factor is not specified, just reduce the loss\n    if avg_factor is None:\n        loss = reduce_loss(loss, reduction)\n    else:\n        # if reduction is mean, then average the loss by avg_factor\n        if reduction == \"mean\":\n            loss = loss.sum() / avg_factor\n        # if reduction is 'none', then do nothing, otherwise raise an error\n        elif reduction != \"none\":\n            raise ValueError(\n                'avg_factor can not be used with reduction=\"sum\"'\n            )\n    return loss\n\ndef binary_cross_entropy(\n    pred, label, weight=None, reduction=\"mean\", avg_factor=None\n):\n    # weighted element-wise losses\n    if weight is not None:\n        weight = weight.float()\n    loss = F.binary_cross_entropy_with_logits(\n        pred, label.float(), weight, reduction=\"none\"\n    )\n    loss = weight_reduce_loss(\n        loss, reduction=reduction, avg_factor=avg_factor\n    )\n\n    return loss \n```", "```py\nloss_func = ResampleLoss(\n    reweight_func=\"rebalance\",\n    loss_weight=1.0,\n    focal=dict(focal=True, alpha=0.5, gamma=2),\n    logit_reg=dict(init_bias=0.05, neg_scale=2.0),\n    map_param=dict(alpha=0.1, beta=10.0, gamma=0.405),\n    class_freq=class_freq,\n    train_num=train_num,\n)\n\"\"\"\nclass_freq - list of frequencies for each class,\ntrain_num - size of train dataset\n\"\"\" \n```", "```py\ndef optimise_f1_score(true_labels: np.ndarray, pred_labels: np.ndarray):\n    best_med_th = 0.5\n    true_bools = [tl == 1 for tl in true_labels]\n    micro_thresholds = (np.array(range(-45, 15)) / 100) + best_med_th\n    f1_results, pre_results, recall_results = [], [], []\n    for th in micro_thresholds:\n        pred_bools = [pl > th for pl in pred_labels]\n        test_f1 = f1_score(true_bools, pred_bools, average=\"micro\", zero_division=0)\n        test_precision = precision_score(\n            true_bools, pred_bools, average=\"micro\", zero_division=0\n        )\n        test_recall = recall_score(\n            true_bools, pred_bools, average=\"micro\", zero_division=0\n        )\n        f1_results.append(test_f1)\n        prec_results.append(test_precision)\n        recall_results.append(test_recall)\n        best_f1_idx = np.argmax(f1_results)\n    return micro_thresholds[best_f1_idx]\n```"]