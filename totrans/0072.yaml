- en: 'Greening AI: 7 Strategies to Make Applications More Sustainable'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/greening-ai-7-strategies-to-make-applications-more-sustainable](https://www.kdnuggets.com/greening-ai-7-strategies-to-make-applications-more-sustainable)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Greening AI: 7 Strategies to Make Applications More Sustainable](../Images/313096bb3be8117d714e163bf4bbafb3.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Editor
  prefs: []
  type: TYPE_NORMAL
- en: AI applications possess unparalleled computational capabilities that can propel
    progress at an unprecedented pace. Nevertheless, these tools rely heavily on energy-intensive
    data centers for their operations, resulting in a concerning lack of energy sensitivity
    that contributes significantly to their carbon footprint. Surprisingly, these
    AI applications already account for a substantial [2.5 to 3.7](https://8billiontrees.com/carbon-offsets-credits/carbon-ecological-footprint-calculators/carbon-footprint-of-data-centers/#:~:text=Data%20centers%20account%20for%202.5,that%20fuel%20the%20global%20economy)
    percent of global greenhouse gas emissions, surpassing the emissions from the
    aviation industry.
  prefs: []
  type: TYPE_NORMAL
- en: And unfortunately, this carbon footprint is increasing at a fast pace.
  prefs: []
  type: TYPE_NORMAL
- en: Presently, the pressing need is to measure the carbon footprint of machine learning
    applications, as emphasized by Peter Drucker's wisdom that "You can't manage what
    you can't measure." Currently, there exists a significant lack of clarity in quantifying
    the environmental impact of AI, with precise figures eluding us.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to measuring the carbon footprint, the AI industry's leaders must
    actively focus on optimizing it. This dual approach is vital to addressing the
    environmental concerns surrounding AI applications and ensuring a more sustainable
    path forward.
  prefs: []
  type: TYPE_NORMAL
- en: What factors contribute to the carbon footprint of AI applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The increased use of machine learning requires increased data centers, many
    of which are power hungry and thus have a significant carbon footprint. The global
    electricity usage by data centers amounted to [0.9 to 1.3 percent](https://www.iea.org/reports/data-centres-and-data-transmission-networks)
    in 2021.
  prefs: []
  type: TYPE_NORMAL
- en: A [2021 study](https://ris.utwente.nl/ws/portalfiles/portal/252516283/1_s2.0_S0306261921003019_main.pdf)
    estimated that this usage can increase to 1.86 percent by 2030\. This [figure](https://www.akcp.com/blog/the-real-amount-of-energy-a-data-center-use/)
    represents the increasing trend of energy demand due to data centers
  prefs: []
  type: TYPE_NORMAL
- en: '![Greening AI: 7 Strategies to Make Applications More Sustainable](../Images/569ee8c6690d85fe7a8b9432a1cdf8b1.png)'
  prefs: []
  type: TYPE_IMG
- en: © Energy consumption trend and share of use for data centers
  prefs: []
  type: TYPE_NORMAL
- en: Notably, the higher the energy consumption is, the higher the carbon footprint
    will be. Data centers heat up during processing and can become faulty and even
    stop functioning due to overheating. Hence, they need cooling, which requires
    additional energy. Around [40 percent](https://www.sciencedirect.com/science/article/pii/S1876610215009467)
    of the electricity consumed by data centers is for air conditioning.
  prefs: []
  type: TYPE_NORMAL
- en: Computing Carbon Intensity for AI Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the increasing footprint of AI usage, these tools’ carbon intensity needs
    to be accounted for. Currently, the research on this subject is limited to analyses
    of a few models and does not adequately address the diversity of the said models.
  prefs: []
  type: TYPE_NORMAL
- en: Here is an evolved methodology and a few effective tools to compute carbon intensity
    of AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: Methodology for estimating carbon intensity of AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Software Carbon Intensity (SCI) [standard](https://github.com/Green-Software-Foundation/sci)
    is an effective approach for estimating carbon intensity of AI systems. Unlike
    the conventional methodologies that employ attributional carbon accounting approach,
    it uses a consequential computing approach.
  prefs: []
  type: TYPE_NORMAL
- en: Consequential approach attempts to calculate the marginal change in emissions
    arising from an intervention or decision, such as the decision to generate an
    extra unit. Whereas, attribution refers to accounting average intensity data or
    static inventories of emissions.
  prefs: []
  type: TYPE_NORMAL
- en: 'A [paper](https://dl.acm.org/doi/fullHtml/10.1145/3531146.3533234) on “Measuring
    the Carbon Intensity of AI in Cloud Instances” by Jesse Doge et al. has employed
    this methodology to bring in more informed research. Since a significant amount
    of AI model training is conducted on cloud computing instances, it can be a valid
    framework to compute the carbon footprint of AI models. The paper refines SCI
    formula for such estimations as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Greening AI: 7 Strategies to Make Applications More Sustainable](../Images/0d85e81153b9458cddbb4b35ae8ce2c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'which is refined from:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Greening AI: 7 Strategies to Make Applications More Sustainable](../Images/391d8019836a589269228939d8330048.png)
    that derives from ![Greening AI: 7 Strategies to Make Applications More Sustainable](../Images/9d091f70ee7b67ae92bb65af3100b033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '**E:** Energy consumed by a software system, primarily of graphical processing
    units-GPUs which is specialized ML hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: '**I:** Location-based marginal carbon emissions by the grid powering the datacenter.'
  prefs: []
  type: TYPE_NORMAL
- en: '**M:** Embedded or embodied carbon, which is the carbon emitted during usage,
    creation, and disposal of hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: '**R:** Functional unit, which in this case is one machine learning training
    task.'
  prefs: []
  type: TYPE_NORMAL
- en: '**C= O+M, where O equals E*I **'
  prefs: []
  type: TYPE_NORMAL
- en: 'The paper uses the formula to estimate electricity usage of a single cloud
    instance. In ML systems based on deep learning, major electricity consumption
    owes it to the GPU, which is included in this formula. They trained a BERT-base
    model using a single NVIDIA TITAN X GPU (12 GB) in a commodity server with two
    Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs) to experiment
    the application of this formula. The following figure shows the results of this
    experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Greening AI: 7 Strategies to Make Applications More Sustainable](../Images/6af4c8d1704f436f450fd32b55e47464.png)'
  prefs: []
  type: TYPE_IMG
- en: © Energy consumption and split between components of a server
  prefs: []
  type: TYPE_NORMAL
- en: The GPU claims 74 percent of the energy consumption. Although it is still claimed
    as an underestimation by the paper’s authors, inclusion of GPU is the step in
    the right direction. It is not the focus of the conventional estimation techniques,
    which means that a major contributor of carbon footprint is being overlooked in
    the estimations. Evidently, SCI offers a more wholesome and reliable computation
    of carbon intensity.
  prefs: []
  type: TYPE_NORMAL
- en: Approaches to measure real-time carbon footprint of cloud computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI model training is often conducted on cloud compute instances, as cloud makes
    it flexible, accessible, and cost-efficient. Cloud computing provides the infrastructure
    and resources to deploy and train AI models at scale. That’s why model training
    on cloud computing is increasing gradually.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to measure the real-time carbon intensity of cloud compute instances
    to identify areas suitable for mitigation efforts. Accounting time-based and location-specific
    marginal emissions per unit of energy can help calculate operational carbon emissions,
    as done by a [2022 paper](https://dl.acm.org/doi/fullHtml/10.1145/3531146.3533234).
  prefs: []
  type: TYPE_NORMAL
- en: An [opensource](https://www.cloudcarbonfootprint.org/) tool, Cloud Carbon Footprint
    (CCF) software is also available to compute the impact of cloud instances.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the carbon efficiency of AI applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here are 7 ways to optimize the carbon intensity of AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Write better, more efficient code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Optimized codes can reduce energy consumption by [30 percent](https://www.zuehlke.com/en/insights/green-coding-innovation-for-more-sustainable-it)
    through decreased memory and processor usage. Writing a carbon-efficient code
    involves optimizing algorithms for faster execution, reducing unnecessary computations,
    and selecting energy-efficient hardware to perform tasks with less power.
  prefs: []
  type: TYPE_NORMAL
- en: Developers can use profiling tools to identify performance bottlenecks and areas
    for optimization in their code. This process can lead to more energy-efficient
    software. Also, consider implementing energy-aware programming techniques, where
    code is designed to adapt to the available resources and prioritize energy-efficient
    execution paths.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Select more efficient model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Choosing the right algorithms and data structures is crucial. Developers should
    opt for algorithms that minimize computational complexity and consequently, energy
    consumption. If the more complex model only yields 3-5% improvement but takes
    2-3x more time to train; then pick the simpler and faster model.
  prefs: []
  type: TYPE_NORMAL
- en: Model distillation is another technique for condensing large models into smaller
    versions to make them more efficient while retaining essential knowledge. It can
    be achieved by training a small model to mimic the large one or removing unnecessary
    connections from a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Tune model parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tune hyperparameters for the model using dual-objective optimization that balance
    model performance (e.g., accuracy) and energy consumption. This dual-objective
    approach ensures that you are not sacrificing one for the other, making your models
    more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Leverage techniques like [Parameter-Efficient Fine-Tuning](https://huggingface.co/blog/peft)
    (PEFT) whose goal is to attain performance similar to traditional fine-tuning
    but with a reduced number of trainable parameters. This approach involves fine-tuning
    a small subset of model parameters while keeping the majority of the pre-trained
    Large Language Models (LLMs) frozen, resulting in significant reductions in computational
    resources and energy consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Compress data and use low-energy storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Implement data compression techniques to reduce the amount of data transmitted.
    Compressed data requires less energy to transfer and occupies lower space on disk.
    During the model serving phase, using a cache can help reduce the calls made to
    the online storage layer thereby reducing
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, picking the right storage technology can result in significant
    gains. For eg. AWS Glacier is an efficient data archiving solution and can be
    a more sustainable approach than using S3 if the data does not need to be accessed
    frequently.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Train models on cleaner energy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are using a cloud service for model training, you can choose the region
    to operate computations. Choose a region that employs renewable energy sources
    for this purpose, and you can reduce the emissions by up to [30 times](https://arxiv.org/pdf/2002.05651.pdf).
    AWS [blog post](https://aws.amazon.com/blogs/architecture/how-to-select-a-region-for-your-workload-based-on-sustainability-goals/)
    outlines the balance between optimizing for business and sustainability goals.
  prefs: []
  type: TYPE_NORMAL
- en: Another option is to select the opportune time to run the model. At certain
    times of the day; the energy is cleaner and such data can be acquired through
    a paid service such as [Electricity Map](https://www.electricitymaps.com/), which
    offers access to real-time data and future predictions regarding the carbon intensity
    of electricity in different regions.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Use specialized data centers and hardware for model training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Choosing more efficient data centers and hardware can make a huge difference
    on carbon intensity. ML-specific data centers and hardware can be [1.4-2](https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf)
    and 2-5 times more energy efficient than the general ones.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Use serverless deployments like AWS Lambda, Azure Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional deployments require the server to be always on, which means 24x7
    energy consumption. Serverless deployments like AWS Lambda and Azure Functions
    work just fine with minimal carbon intensity.
  prefs: []
  type: TYPE_NORMAL
- en: Final Notes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The AI sector is experiencing exponential growth, permeating every facet of
    business and daily existence. However, this expansion comes at a cost—a burgeoning
    carbon footprint that threatens to steer us further away from the goal of limiting
    global temperature increases to just 1°C.
  prefs: []
  type: TYPE_NORMAL
- en: This carbon footprint is not just a present concern; its repercussions may extend
    across generations, affecting those who bear no responsibility for its creation.
    Therefore, it becomes imperative to take decisive actions to mitigate AI-related
    carbon emissions and explore sustainable avenues for harnessing its potential.
    It is crucial to ensure that AI's benefits do not come at the expense of the environment
    and the well-being of future generations.
  prefs: []
  type: TYPE_NORMAL
- en: '**[](https://www.linkedin.com/in/ankurgupta101)**[Ankur Gupta](https://www.linkedin.com/in/ankurgupta101)****
    is an engineering leader with a decade of experience spanning sustainability,
    transportation, telecommunication and infrastructure domains; currently holds
    the position of Engineering Manager at Uber. In this role, he plays a pivotal
    role in driving the advancement of Uber''s Vehicles Platform, leading the charge
    towards a zero-emissions future through the integration of cutting-edge electric
    and connected vehicles.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Using Data Science to Make Clean Energy More Equitable](https://www.kdnuggets.com/2022/03/data-science-make-clean-energy-equitable.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Masking: The Core of Ensuring GDPR and other Regulatory…](https://www.kdnuggets.com/2023/05/data-masking-core-ensuring-gdpr-regulatory-compliance-strategies.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Feature Store Summit 2023: Practical Strategies for Deploying ML…](https://www.kdnuggets.com/2023/09/hopsworks-feature-store-summit-2023-practical-strategies-deploying-ml-models-production-environments)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mastering Python: 7 Strategies for Writing Clear, Organized, and…](https://www.kdnuggets.com/mastering-python-7-strategies-for-writing-clear-organized-and-efficient-code)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Strategies for Optimizing Performance and Costs When Using Large…](https://www.kdnuggets.com/strategies-for-optimizing-performance-and-costs-when-using-large-language-models-in-the-cloud)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best Strategies for Fine-Tuning Large Language Models](https://www.kdnuggets.com/the-best-strategies-for-fine-tuning-large-language-models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
