- en: How Bayes’ Theorem is Applied in Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/10/bayes-theorem-applied-machine-learning.html](https://www.kdnuggets.com/2019/10/bayes-theorem-applied-machine-learning.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Jaime Zornoza](https://www.linkedin.com/in/jaime-zornoza/), Universidad
    Politecnica de Madrid**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Header image](../Images/417c5ca8ecb0d2fe52777d09b2a5dc99.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous post we saw **what Bayes’ Theorem is**, and went through an
    easy, intuitive example of how it works. You can find this post [**here**](https://towardsdatascience.com/probability-learning-i-bayes-theorem-708a4c02909a?source=friends_link&sk=29a5c9c9301a1204f16460781eaba113)**. **If
    you don’t know what Bayes’ Theorem is, and you have not had the pleasure to read
    it yet, I recommend you do, as it will make understanding this present article
    a lot easier.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we will see the **uses of this theorem in *Machine Learning.***
  prefs: []
  type: TYPE_NORMAL
- en: '***Ready? Lets go then!***'
  prefs: []
  type: TYPE_NORMAL
- en: Bayes’ Theorem in Machine Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned in the previous post, Bayes’ theorem tells use how to **gradually
    update our knowledge** on *something* as we get more evidence or that about that *something*.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, in **Supervised Machine Learning**, when we want to train a model
    the **main building blocks** are a set of data points that contain **features** (the
    attributes that define such data points),**the labels** of such data point (the
    numeric or categorical tag which we later want to predict on new data points),
    and a **hypothesis function** or model that links such features with their corresponding
    labels. We also have a **loss function**, which is the difference between the
    predictions of the model and the real labels which we want to reduce to achieve
    the best possible results.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/fbcde049a8bc3e5d2627e6f4686e7a2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Main elements of a supervised Learning Problem
  prefs: []
  type: TYPE_NORMAL
- en: 'These supervised Machine Learning problems can be divided into **two main categories:
    regression**, where we want to calculate a number or **numeric** **value** associated
    with some data (like for example the price of a house), and **classification**,
    where we want to assign the data point to a **certain category** (for example
    saying if an image shows a dog or a cat).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayes’ theorem can be used in both regression, and classification.**'
  prefs: []
  type: TYPE_NORMAL
- en: Lets see how!
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayes’ Theorem in Regression**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine we have a very **simple set of data**, which represents the **temperature
    of each day** of the year in a certain area of a town (the** feature** of the
    data points), and the **number of water bottles **sold by a local shop in that
    area every single day (the **label** of the data points).
  prefs: []
  type: TYPE_NORMAL
- en: By making a **very simple model**, we could s**ee if these two are related**,
    and if they are, then use this model to **make predictions** in order to stock
    up on water bottles depending on the temperature and never run out of stock, or
    avoid having too much inventory.
  prefs: []
  type: TYPE_NORMAL
- en: We could try a very simple** linear regression model** to see how these variables
    are related. In the following formula, that describes this linear model, y is
    the target label (the number of water bottles in our example), **each of the θs
    is a parameter of the model **(the slope and the cut with the y-axis) and x would
    be our feature (the temperature in our example).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/dcd3d2aa38efa1fdbd9b71557db7fa51.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation describing a linear model
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this training would be to **reduce the mentioned loss function**,
    so that the predictions that the model makes for the known data points, are close
    to the actual values of the labels of such data points.
  prefs: []
  type: TYPE_NORMAL
- en: After having trained the model with the available data we would get a value
    for both of the **θs**. This training can be performed by using an **iterative
    process **like gradient descent or another **probabilistic method** like Maximum
    Likelihood. In any way, we would just **have ONE single value** for each one of
    the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In this manner, when we get **new data without a label** (new temperature forecasts)
    as we know the value of the **θs**, we could just use this simple equation to
    obtain the wanted ***Ys*** (number of water bottles needed for each day).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/19d7c2937dca6f54c2fba3af72f67475.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure of an uni-variate linear regression. Using the initial blue data points,
    we calculate the line that best fits these points, and then when we get a new
    temperature we can easily calculate the Nº sold bottles for that day.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we use Bayes’ theorem for regression, instead of **thinking of the parameters **(the
    θs) of the model as having a single, unique value, we represent them **as** parameters **having
    a certain distribution**: the prior distribution of the parameters. The following
    figures show the generic Bayes formula, and under it how it can be applied to
    a machine learning model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/80bad4a0f02bf45b7c24c44afee57cab.png)'
  prefs: []
  type: TYPE_IMG
- en: Bayes formula![Figure](../Images/2eec6c3f30b4ba354530210cb67ff621.png)
  prefs: []
  type: TYPE_NORMAL
- en: Bayes formula applied to a machine learning model
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind this is that **we have some previous knowledge of the parameters
    of the model **before we have any actual data: ***P(model)* **is this prior probability.
    Then, **when we get some new data, we update the distribution of the parameters** of
    the model, making it the posterior probability ***P(model|data)***.
  prefs: []
  type: TYPE_NORMAL
- en: What this means is that** our parameter set **(the θs of our model) is not constant,
    but instead **has its own distribution**. Based on previous knowledge (from experts
    for example, or from other works) **we make a first hypothesis** about the distribution
    of the parameters of our model. Then as we train our models with **more data**, **this
    distribution gets updated** and grows more exact (in practice the variance gets
    smaller).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/dfd80faad7b3be9607c854a6b18befb8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure of the a priori and posteriori parameter distributions. θMap is the maximum
    posteior estimation, which we would then use in our models.
  prefs: []
  type: TYPE_NORMAL
- en: This figure shows the **initial distribution of the parameters of the model *p(θ)***,
    and how as we add more data this distribution **gets updated, **making it grow
    more exact to*** p(θ|x)***, where x denotes this new data. The θ here is equivalent
    to the *model* in the formula shown above, and the ***x*** here is equivalent
    to the *data* in such formula.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes’ formula, as always, tells us **how to go from the prior to the posterior
    probabilities**. We do this in an iterative process as we get more and more data,
    having the **posterior probabilities become the prior probabilities for the next
    iteration**. Once we have trained the model with enough data, to choose the set
    of final parameters we would search for the **Maximum posterior (MAP) estimation
    to use a concrete set of values for the parameters of the model.**
  prefs: []
  type: TYPE_NORMAL
- en: 'This kind of analysis **gets its strength from the initial prior distribution**:
    if we do not have any previous information, and can’t make any assumption about
    it, other probabilistic approaches like Maximum Likelihood are better suited.'
  prefs: []
  type: TYPE_NORMAL
- en: However, **if we have some prior information about the distribution of the parameters
    the Bayes’ approach proves to be very powerful**, specially in the case of having **unreliable
    training data**. In this case, as we are not building the model and calculating
    its parameters from scratch using this data, but rather using some kind of previous
    knowledge to infer an initial distribution for these parameters, **this previous
    distribution makes the parameters more robust and less affected by inaccurate
    data.**
  prefs: []
  type: TYPE_NORMAL
- en: I don’t want to get very technical in this part, but the maths behind all this
    reasoning is beautiful; if you want to know about it don’t hesitate and email
    me to jaimezorno@gmail.com or **contact me** on [LinkdIn](https://www.linkedin.com/in/jaime-zornoza/).
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayes’ Theorem in Classification**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have seen how Bayes’ theorem can be used for regression, by estimating the
    parameters of a linear model. The same reasoning could be applied to other kind
    of regression algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Now we will see how to use Bayes’ theorem for classification. This is known
    as **Bayes’ optimal classifier**. The reasoning now is very similar to the previous
    one.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine we have a classification problem with** *i* different classes**. The
    thing we are after here is **the class probability** for each class** w*i***.
    Like in the previous regression case, we also differentiate between prior and
    posterior probabilities, but now we have **prior class probabilities** ***p(wi)*** and **posterior
    class probabilities**, after using data or observations ***p(wi|x)***.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/04e9559d24235af022e7804cf0039701.png)'
  prefs: []
  type: TYPE_IMG
- en: Bayes formula used for Bayes’ optimal classifier
  prefs: []
  type: TYPE_NORMAL
- en: Here ***P(x)*** is the **density function** common to** all the data points**,*** P(x|wi)*** is
    the **density function of the data points belonging to class *wi***, and*** P(wi)*** is
    the prior distribution of class ***wi***. ***P(x|wi)*** is calculated from the
    training data, assuming a certain distribution and calculating a **mean vector
    for each class** and the** covariance of the features** of the data points belonging
    to such class. The prior class distributions ***P(wi)* **are estimated based on **domain
    knowledge**, expert advice or previous works, like in the regression example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lets see an example of how this works: Image we have measured the height of
    34 individuals: **25 males (blue)** and **9 females (red)**, and we get a **new** height **observation **of
    172 cm which we want to classify as male or female. The following figure represents
    the predictions obtained using a **Maximum likelihood classifier and a Bayes optimal
    classifier.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/ade69484436a30253dcaf9ba792fc0b7.png)'
  prefs: []
  type: TYPE_IMG
- en: On the left, the training data for both classes with their estimated normal
    distributions. On the right, Bayes optimal classifier, with prior class probabilities
    p(wA) of male being 25/34 and p(wB) of female being 9/34.
  prefs: []
  type: TYPE_NORMAL
- en: In this case we have used the **number of samples** in the training data **as** the **prior
    knowledge** for our class distributions, but if for example we were doing this
    same differentiation between height and gender for a specific country, and we
    knew the woman there are specially tall, and also knew the mean height of the
    men, we could have used this** information to build our prior class distributions.**
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the example, using these** prior knowledge leads to different
    results **than not using them. Assuming this previous knowledge is of high quality
    (or otherwise we wouldn’t use it), these predictions should be **more accurate** than
    similar trials that don’t incorporate this information.
  prefs: []
  type: TYPE_NORMAL
- en: After this, as always, as we get **more data** these** distributions would get
    updated** to reflect the knowledge obtained from this data.
  prefs: []
  type: TYPE_NORMAL
- en: As in the previous case, I don’t want to get too technical, or extend the article
    too much, so I won’t go into the mathematical details, but **feel free to contact
    me if you are curious about them**.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have seen **how Bayes’ theorem is used in Machine learning**; both in **regression **and **classification**,
    to incorporate previous knowledge into our models and improve them.
  prefs: []
  type: TYPE_NORMAL
- en: In the following post we will see how **simplifications of Bayes’ theorem **are
    one of the most used techniques for **Natural Language Processing** and how they
    are applied to many real world use cases like spam filters or sentiment analysis
    tools. To check it out [**follow me on Medium**](https://medium.com/@jaimezornoza),
    and stay tuned!
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/5dab590ad63e916f60390c57978bf78a.png)'
  prefs: []
  type: TYPE_IMG
- en: Another example of Bayesian classification
  prefs: []
  type: TYPE_NORMAL
- en: That is all, I hope you liked the post. Feel Free to connect with me on [LinkedIn](https://www.linkedin.com/in/jaime-zornoza/) or
    follow me on Twitter at **@jaimezorno**. Also, you can take a look at my other
    posts on Data Science and Machine Learning[**here**](https://medium.com/@jaimezornoza).
    Have a good read!
  prefs: []
  type: TYPE_NORMAL
- en: Additional Resources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In case you want to go more in depth into Bayes and Machine Learning, check
    out these other resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[How Bayesian Inference works](https://brohrer.github.io/how_bayesian_inference_works.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Bayesian statistics Youtube Series](https://www.youtube.com/watch?v=YsJ4W1k0hUg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning Bayesian Learning slides](https://slideplayer.com/slide/4940573/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Bayesian Inference](https://www.umass.edu/landeco/teaching/ecodata/schedule/bayesian.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and as always, contact me with any questions. Have a fantastic day and keep
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Jaime Zornoza](https://www.linkedin.com/in/jaime-zornoza/)** is an
    Industrial Engineer with a bachelor specialized in Electronics and a Masters degree
    specialized in Computer Science.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/probability-learning-ii-how-bayes-theorem-is-applied-in-machine-learning-bd747a960962).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Probability Learning I: Bayes’ Theorem](/2019/10/probability-learning-bayes-theorem.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How Bayesian Inference Works](/2016/11/how-bayesian-inference-works.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Learning for NLP: ANNs, RNNs and LSTMs explained!](/2019/08/deep-learning-nlp-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[3 Ways Understanding Bayes Theorem Will Improve Your Data Science](https://www.kdnuggets.com/2022/06/3-ways-understanding-bayes-theorem-improve-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What is Chebychev''s Theorem and How Does it Apply to Data Science?](https://www.kdnuggets.com/2022/11/chebychev-theorem-apply-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, November 30: What is Chebychev''s Theorem and How…](https://www.kdnuggets.com/2022/n46.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Gaussian Naive Bayes, Explained](https://www.kdnuggets.com/2023/03/gaussian-naive-bayes-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Naïve Bayes Algorithm: Everything You Need to Know](https://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, April 13: Python Libraries Data Scientists Should…](https://www.kdnuggets.com/2022/n15.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
