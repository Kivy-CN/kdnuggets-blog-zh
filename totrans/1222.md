# 数据科学家必知的前 10 种机器学习算法 – 第 1 部分

> 原文：[https://www.kdnuggets.com/2021/04/top-10-must-know-machine-learning-algorithms-data-scientists-1.html](https://www.kdnuggets.com/2021/04/top-10-must-know-machine-learning-algorithms-data-scientists-1.html)

[评论](#comments) ![简单机器学习](../Images/a5441b69289f9fd2607606e638f57b99.png)

图片来源：[图神经网络的全面调查](https://arxiv.org/abs/1901.00596v4)

对于数据科学新手来说，梳理大量关于[机器学习算法](https://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html)的信息可能是一个困难且耗时的过程。弄清楚哪些算法被广泛使用，哪些算法只是新颖或有趣的，并不仅仅是一个学术性练习；在学习初期确定集中时间和精力的地方，可以决定你的职业是快速起步还是经历较长的延迟。

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业的快车道。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你所在组织的 IT 工作

* * *

如何准确区分值得关注的立即有用的算法与那些不那么有用的算法？确定如何制定一个权威的机器学习算法列表本质上是困难的，但直接向从业者征求反馈可能是最佳的方法。这样的过程本身带来了各种困难，正如可以想象的那样，类似的调查结果也少之又少。

然而，KDnuggets [在最近几年进行了这样的调查](https://www.kdnuggets.com/2019/04/top-data-science-machine-learning-methods-2018-2019.html)，询问受访者“你在 2018/2019 年间在实际应用中使用了哪些数据科学/机器学习方法和算法？” 当然，如上所述，这些调查受到自我选择、参与者的可验证性、对实际响应的信任等的限制，但这项调查代表了我们目前拥有的最新、最广泛且最优质的来源。

因此，这就是我们将用于识别当前使用的前10个机器学习算法的来源，作为数据科学家必须了解的前10个算法。这些前10个必须了解的算法中的前5个将在下面介绍，并简要概述这些算法及其工作原理。接下来的几周，我们将发布第2部分。

请注意以下几点：

+   我们跳过了与机器学习算法不相关的条目（例如“可视化”，“描述性统计”，“文本分析”）

+   我们将“回归”条目单独处理为“线性回归”和“逻辑回归”

+   我们将“集成方法”条目替换为“bagging”，一种特定的集成方法，因为此列表中还有其他一些集成方法也单独表示

+   我们跳过了任何神经网络条目，因为这些技术结合了架构和各种不同的算法来实现其目标，这些方面超出了本讨论的范围

### 1\. 线性回归

回归是一种经过时间考验的近似给定数据集合之间关系的方法，并因[不幸的情况](https://en.wikipedia.org/wiki/Regression_analysis#History)而得到了不太合适的命名。

线性回归是一种简单的代数工具，试图找到适合2个或多个属性的“最佳”（为本讨论目的所用的直线）直线，其中一个属性（**简单**线性回归）或多个属性的组合（**多元**线性回归）用于预测另一个，即类属性。利用一组训练实例来计算线性模型，其中一个属性或一组属性与另一个属性进行绘制。然后，该模型尝试确定在给定特定类属性的情况下，新实例将位于回归线上何处。

预测变量和响应变量（分别为*x*和*y*）之间的关系可以通过以下方程式表达，所有阅读此文的人无疑都很熟悉：![方程式](../Images/ef82f76aa240383ae202e5777644a284.png)。

*m*和*b*是回归系数，分别表示直线的斜率和y截距。如果你难以回忆，建议你查看小学代数笔记 :)

对于具有*n*属性的多元线性回归，方程式为：

![方程式](../Images/2228a2c2a643f05fb98f4fdefcbe888c.png)

### 2\. 决策树

在机器学习中，决策树作为有效且易于理解的数据分类器已经使用了几十年（与现存的众多黑箱分类器相比）。多年来，研究产生了许多决策树算法，其中最重要、最有影响力和使用最广泛的3种是：

迭代二分法3（ID3） - Ross Quinlan的C4.5的前身

+   迭代二分法3（ID3） - Ross Quinlan的C4.5的前身

+   C4.5 - 所有时间中最受欢迎的分类器之一，也是Quinlan的作品

+   CART - 与C4.5几乎同时独立发明的，至今仍然非常受欢迎

ID3、C4.5和CART都采用自上而下、递归、分治的方法进行决策树归纳。

多年来，C4.5已成为衡量新分类算法性能的基准。昆兰的原始实现包含专有代码；然而，随着时间的推移，已经编写了各种开源版本，包括（曾经非常受欢迎的）Weka机器学习工具包中的J48决策树算法。

决策树分类算法的模型（或树）构建方面包括两个主要任务：树归纳和树剪枝。**树归纳**是将一组预分类实例作为输入，决定最适合分裂的属性，分裂数据集，并在结果分裂的数据集上递归，直到所有训练实例都被分类。**树剪枝**涉及通过消除冗余或对分类过程非必要的分支来减少决策树的大小。

在构建我们的树时，目标是对那些能够创建最纯净子节点的属性进行分裂，这将使得为分类数据集中所有实例所需的分裂次数最小化。这种纯度通常通过多种不同的属性选择度量方法来衡量。

有3种显著的属性选择度量方法用于决策树归纳，每种都与3种显著的决策树分类器之一配对。

+   信息增益 - 用于ID3算法

+   增益率 - 用于C4.5算法

+   基尼指数 - 用于CART算法

在ID3中，纯度是通过信息增益的概念来测量的，这基于克劳德·香农的工作，涉及到对一个之前未见的实例进行适当分类所需知道多少信息。实际上，这是通过比较当前数据集分区中单个实例的熵，或分类所需的信息量，与在给定属性上进一步划分当前数据集分区后对单个实例进行分类所需的信息量来进行测量的。

从这次讨论中最重要的收获之一应该是，决策树是一种分类策略，而不是某种单一、明确的分类算法。虽然我们简要了解了3种不同的决策树算法实现，但每种算法的不同方面有多种配置方式。确实，任何旨在对数据进行分类，并采取自上而下、递归、分治方法来构建一个树状图以供后续实例分类的算法，不论其他细节（包括属性分裂选择方法和可选的树剪枝方法），都被视为决策树。

### 3\. *k*-均值聚类

*k*-均值是一种简单但常常有效的聚类方法。传统上，从给定数据集中随机选择*k*个数据点作为集群中心或质心，并将所有训练实例绘制并添加到最接近的集群中。在所有实例都被添加到集群后，代表每个集群实例均值的质心将重新计算，这些重新计算的质心将成为各自集群的新中心。

此时，所有集群成员资格被重置，训练集中的所有实例被重新绘制并重新添加到其最近的、可能重新中心化的集群中。这个迭代过程持续进行，直到质心或其成员资格没有变化，集群被认为是稳定的。

一旦重新计算的质心与上一轮迭代的质心匹配，或在某个预设的范围内，就达到了收敛。在*k*-均值中，距离的度量通常是欧几里得距离，对于形式为（*x, y*）的两个点，可以表示为：

![图片](../Images/6caecd241af5ba356579714b862cb7ae.png)

技术上值得注意的是，尤其是在并行计算时代，*k*-均值中的迭代聚类是串行的；然而，迭代中的距离计算不一定是串行的。因此，对于大规模的数据集，距离计算是*k*-均值聚类算法中值得并行化的目标。

此外，虽然我们刚刚描述了使用集群均值和欧几里得距离的特定聚类方法，但可以想象，还有许多其他可能的方法，例如，使用集群中位数值或任何其他距离度量（余弦、曼哈顿、切比雪夫等）。

### 4\. 袋装法

在特定场景中，将分类器串联或分组在一起，使用投票、加权或组合技术以追求最准确的分类器，可能比不这样做更有用。集成学习器是提供这种功能的分类器。袋装法就是一个集成学习器的例子。

袋装法的基本概念很简单：构建多个模型，观察这些模型的结果，并选择多数结果。我最近遇到了汽车后轴组件的问题：我对经销商的诊断不满意，因此我将其带到另外2个车库，两者都认为问题与经销商建议的不同。瞧，这就是袋装法的实际应用。

我在示例中只访问了3个车库，但你可以想象，如果我访问了数十个或数百个车库，准确性可能会提高，特别是当我的汽车问题较为复杂时。这对于袋装法也适用，袋装分类器通常比单一的组成分类器准确得多。同时，请注意，所使用的组成分类器类型无关紧要；最终的模型可以由任何单一分类器类型组成。

Bagging是*自助聚合*的缩写，因为它从数据集中提取多个样本，每个样本集被视为自助样本。然后将这些自助样本的结果进行聚合。

### 5\. 支持向量机

如前所述，支持向量机（SVM）是一种特殊的分类策略。SVM通过将训练数据集转换到更高维度来工作，然后检查该高维度数据中类之间的最佳分隔边界或边界。在SVM中，这些边界被称为超平面，它们通过定位支持向量（即最本质地定义类的实例）及其边距（即平行于超平面、由超平面与其支持向量之间的最短距离定义的线）来确定。因此，SVM能够对线性和非线性数据进行分类。

SVM的宏伟思想是，通过足够高的维度，可以始终找到一个超平面，将特定类别与所有其他类别分开，从而划分数据集成员类别。当重复足够多次时，可以生成足够的超平面来分隔*n*维空间中的所有类别。重要的是，SVM不仅寻找任何分隔超平面，而是寻找最大间隔超平面，即与各自类别的支持向量等距的超平面。

SVM的核心是核技巧，它可以比较原始数据和可能的高维特征空间转换，以确定这种转换是否有助于数据类的分隔，从而使超平面能够分隔类。核技巧至关重要，因为它可以使潜在的难以处理的转换计算变得可行，这些比较通过成对的相似性比较来实现。

当数据是线性可分时，可以选择许多分隔线。这样的超平面可以表示为

![图像](../Images/e8afe37e7dc375b373dea975c6278eba.png)

其中 *W* 是一个权重向量，b 是一个标量偏置，X 是训练数据（形式为 (*x[1], x[2]*, ...）。如果将我们的偏置 *b* 看作是一个额外的权重，那么该方程可以表示为

![图像](../Images/a94de6f6481000d3f547e2a04a772acb.png)

这可以进一步被重写为一对线性不等式，解为大于或小于零，其中任何一个满足条件的情况表示一个特定点位于超平面上方或下方。找到最大间隔超平面，或者说与支持向量等距的超平面，通过将线性不等式合并为一个方程，并将其转换为约束二次优化问题，使用拉格朗日公式并通过Karush-Kuhn-Tucker条件解决，这是我们在这里不讨论的内容。

因此，这里有5种数据科学家必须了解的算法，其中*必须了解*的定义为在实践中最常用的算法，而最常用的算法则基于最近KDnuggets调查的结果。我们将在接下来的几周内跟进此列表的第二部分。在此之前，我们希望你能发现这个列表和简单的解释对你有帮助，并且你能够从中扩展，进一步了解每一种算法。

**相关**：

+   [如何在面试中解释关键机器学习算法](/2020/10/explain-machine-learning-algorithms-interview.html)

+   [机器学习工程师需要知道的10种算法](/2016/08/10-algorithms-machine-learning-engineers.html)

+   [初学者的十大机器学习算法](/2017/10/top-10-machine-learning-algorithms-beginners.html)

### 更多相关主题

+   [KDnuggets新闻，6月22日：主要的监督学习算法…](https://www.kdnuggets.com/2022/n25.html)

+   [检测虚假数据科学家的20个问题（附答案）：ChatGPT…](https://www.kdnuggets.com/2023/01/20-questions-detect-fake-data-scientists-chatgpt-1.html)

+   [检测虚假数据科学家的20个问题（附答案）：ChatGPT…](https://www.kdnuggets.com/2023/02/20-questions-detect-fake-data-scientists-chatgpt-2.html)

+   [初学者指南：十大机器学习算法](https://www.kdnuggets.com/a-beginner-guide-to-the-top-10-machine-learning-algorithms)

+   [机器学习中使用的主要监督学习算法](https://www.kdnuggets.com/2022/06/primary-supervised-learning-algorithms-used-machine-learning.html)

+   [基础机器学习算法：初学者指南](https://www.kdnuggets.com/2021/05/essential-machine-learning-algorithms-beginners.html)
