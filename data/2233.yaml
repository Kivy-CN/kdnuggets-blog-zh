- en: Linear Regression from Scratch with NumPy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 NumPy 从头开始线性回归
- en: 原文：[https://www.kdnuggets.com/linear-regression-from-scratch-with-numpy](https://www.kdnuggets.com/linear-regression-from-scratch-with-numpy)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/linear-regression-from-scratch-with-numpy](https://www.kdnuggets.com/linear-regression-from-scratch-with-numpy)
- en: '![Linear Regression from Scratch with NumPy](../Images/1951ea4b9d9ce604ed154efff3629c4f.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![使用 NumPy 从头开始线性回归](../Images/1951ea4b9d9ce604ed154efff3629c4f.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: Motivation
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全领域。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能。'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的信息技术。'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Linear Regression is one of the most fundamental tools in machine learning.
    It is used to find a straight line that fits our data well. Even though it only
    works with simple straight-line patterns, understanding the math behind it helps
    us understand Gradient Descent and Loss Minimization methods. These are important
    for more complicated models used in all machine learning and deep learning tasks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是机器学习中最基础的工具之一。它用于找到一条适合我们数据的直线。尽管它仅适用于简单的直线模式，但理解其背后的数学帮助我们理解梯度下降和损失最小化方法。这些对于所有机器学习和深度学习任务中的更复杂模型都是重要的。
- en: In this article, we'll roll up our sleeves and build Linear Regression from
    scratch using NumPy. Instead of using abstract implementations such as those provided
    by Scikit-Learn, we will start from the basics.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将动手使用 NumPy 从头开始构建线性回归。我们将从基础开始，而不是使用 Scikit-Learn 提供的抽象实现。
- en: Dataset
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: We generate a dummy dataset using Scikit-Learn methods. We only use a single
    variable for now, but the implementation will be general that can train on any
    number of features.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Scikit-Learn 方法生成一个虚拟数据集。目前我们只使用一个变量，但该实现将是通用的，可以在任意数量的特征上进行训练。
- en: The make_regression method provided by Scikit-Learn generates random linear
    regression datasets, with added Gaussian noise to add some randomness.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 提供的 make_regression 方法生成随机的线性回归数据集，并加入了高斯噪声以增加一些随机性。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We generate 500 random values, each with 1 single feature. Therefore, X has
    shape (500, 1) and each of the 500 independent X values, has a corresponding y
    value. So, y also has shape (500, ).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成 500 个随机值，每个值具有 1 个特征。因此，X 的形状是 (500, 1)，每个 500 个独立的 X 值都有一个对应的 y 值。因此，y
    的形状也是 (500, )。
- en: 'Visualized, the dataset looks as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化的数据集如下所示：
- en: '![Linear Regression from Scratch with NumPy](../Images/a1f435f09880ff61db42c639fa6d2b15.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![使用 NumPy 从头开始线性回归](../Images/a1f435f09880ff61db42c639fa6d2b15.png)'
- en: Image by Author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: '**We aim to find a best-fit line that passes through the center of this data,
    minimizing the average difference between the predicted and original y values.**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们的目标是找到一条最适合的直线，使其通过数据中心，最小化预测值与原始 y 值之间的平均差异。**'
- en: Intuition
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直观
- en: 'The general equation for a linear line is:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 线性直线的一般方程是：
- en: y = m*X + b
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: y = m*X + b
- en: X is numeric, single-valued. Here m and b represent the gradient and y-intercept
    (or bias). These are unknowns, and varying values of these can generate different
    lines. In machine learning, X is dependent on the data, and so are the y values.
    **We only have control over m and b, that act as our model parameters.** We aim
    to find optimal values of these two parameters, that generate a line that minimizes
    the difference between predicted and actual y values.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: X 是数值型的，单一值。这里 m 和 b 代表梯度和 y 截距（或偏置）。这些是未知的，改变这些值可以生成不同的直线。在机器学习中，X 依赖于数据，y
    值也是如此。**我们只控制 m 和 b，它们作为我们的模型参数。** 我们的目标是找到这两个参数的最佳值，以生成一条最小化预测值与实际 y 值之间差异的直线。
- en: This extends to the scenario where X is multi-dimensional. In that case, the
    number of m values will equal the number of dimensions in our data. For example,
    if our data has three different features, we will have three different m values,
    called **weights**.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这扩展到 X 是多维的情况。在这种情况下，m 值的数量将等于数据中的维度数量。例如，如果我们的数据有三个不同的特征，我们将有三个不同的 m 值，称为**权重**。
- en: 'The equation will now become:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 方程现在变为：
- en: y = w1*X1 + w2*X2 + w3*X3 + b
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: y = w1*X1 + w2*X2 + w3*X3 + b
- en: This can then extend to any number of features.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以扩展到任意数量的特征。
- en: But how do we know the optimal values of our bias and weight values? Well, we
    don’t. But we can iteratively find it out using Gradient Descent. We start with
    random values and change them slightly for multiple steps until we get close to
    the optimal values.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何知道偏置和权重的最优值？嗯，我们不知道。但是我们可以使用梯度下降方法逐步找出。我们从随机值开始，并在多个步骤中稍微调整它们，直到接近最优值。
- en: First, let us initialize Linear Regression, and we will go over the optimization
    process in greater detail later.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们初始化线性回归，稍后我们会更详细地讲解优化过程。
- en: Initialize Linear Regression Class
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化线性回归类
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We use a learning rate and number of iterations hyperparameters, that will be
    explained later. The weights and biases are set to None because the number of
    weight parameters depends on the input features within the data. We do not have
    access to the data yet, so we initialize them to None for now.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用学习率和迭代次数这两个超参数，稍后会详细解释。权重和偏置被设置为 None，因为权重参数的数量取决于数据中的输入特征。由于我们还没有数据，因此目前将它们初始化为
    None。
- en: The Fit Method
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合方法
- en: In the fit method, we are provided with data and their associated values. We
    can now use these, to initialize our weights, and then train the model to find
    optimal weights.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在 fit 方法中，我们提供了数据及其相关值。现在我们可以利用这些数据来初始化权重，然后训练模型以找到最优权重。
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The independent feature X will be a NumPy array of shape (num_samples, num_features).
    In our case, the shape of X is (500, 1). Each row in our data will have an associated
    target value, so y is also of shape (500,) or (num_samples).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 自变量 X 将是一个形状为 (num_samples, num_features) 的 NumPy 数组。在我们的例子中，X 的形状是 (500, 1)。我们数据中的每一行都有一个相关的目标值，因此
    y 的形状也是 (500,) 或 (num_samples)。
- en: We extract this and randomly initialize the weights given the number of input
    features. So now our weights are also a NumPy array of size (num_features, ).
    Bias is a single value initialized to zero.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提取这些，并根据输入特征的数量随机初始化权重。因此，现在我们的权重也是一个大小为 (num_features, ) 的 NumPy 数组。偏置是初始化为零的单个值。
- en: Predicting Y Values
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测 Y 值
- en: We use the line equation discussed above to calculate predicted y values. However,
    instead of an iterative approach to sum all values, we can follow a vectorized
    approach for faster computation. Given that the weights and X values are NumPy
    arrays, we can use matrix multiplication to get predictions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用上述讨论的直线方程来计算预测的 y 值。然而，我们可以采用向量化的方法进行更快的计算，而不是通过迭代方法来求和所有值。由于权重和 X 值都是 NumPy
    数组，我们可以使用矩阵乘法来获得预测值。
- en: X has shape (num_samples, num_features) and weights have shape (num_features,
    ). We want the predictions to be of shape (num_samples, ) matching the original
    y values. Therefore we can multiply X with weights, or (num_samples, num_features)
    x (num_features, ) to obtain predictions of shape (num_samples, ).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: X 的形状是 (num_samples, num_features)，而权重的形状是 (num_features, )。我们希望预测值的形状是 (num_samples,
    )，以匹配原始 y 值。因此，我们可以将 X 与权重相乘，即 (num_samples, num_features) x (num_features, )
    来获得形状为 (num_samples, ) 的预测值。
- en: The bias value is added at the end of each prediction. This can simply be implemented
    in a single line.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置值在每个预测的末尾添加。这可以简单地在一行中实现。
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: However, are these predictions correct? Obviously not. We are using randomly
    initialized values for the weights and bias, so the predictions will also be random.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些预测正确吗？显然不是。我们使用随机初始化的权重和偏置值，因此预测也会是随机的。
- en: How do we get the optimal values? **Gradient Descent.**
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何获得最优值？**梯度下降**。
- en: Loss Function and Gradient Descent
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数和梯度下降
- en: 'Now that we have both predicted and target y values, we can find the difference
    between both values. Mean Square Error (MSE) is used to compare real-valued numbers.
    The equation is as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了预测的 y 值和目标 y 值，我们可以找到这两者之间的差异。均方误差（MSE）用于比较实际值。方程如下：
- en: '![Linear Regression from Scratch with NumPy](../Images/d5f03ab0e7f212765a705a6189468d4d.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![从零开始的线性回归与 NumPy](../Images/d5f03ab0e7f212765a705a6189468d4d.png)'
- en: We only care about the absolute difference between our values. A prediction
    higher than the original value is as bad as a lower prediction. So we square the
    difference between our target value and predictions, to convert negative differences
    to positive. Moreover, this penalizes a larger difference between targets and
    predictions, as higher differences squared will contribute more to the final loss.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只关心值之间的绝对差异。预测值高于原始值和低于原始值同样糟糕。因此，我们对目标值和预测值之间的差异进行平方处理，以将负差异转换为正值。此外，这也惩罚了目标和预测之间的较大差异，因为较大的差异平方后会对最终损失贡献更多。
- en: For our predictions to be as close to original targets as possible, we now try
    to minimize this function. The loss function will be minimum, where the gradient
    is zero. As we can only optimize our weights and bias values, we take the partial
    derivates of the MSE function with respect to weights and bias values.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的预测尽可能接近原始目标，我们现在尝试最小化这个函数。损失函数在梯度为零时达到最小值。由于我们只能优化权重和偏置值，我们对 MSE 函数关于权重和偏置值的偏导数进行计算。
- en: '![Linear Regression from Scratch with NumPy](../Images/83e89da79a360a5709bf54902e4a76cb.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![使用 NumPy 从头开始进行线性回归](../Images/83e89da79a360a5709bf54902e4a76cb.png)'
- en: We then optimize our weights given the gradient values, using Gradient Descent.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用梯度下降法根据梯度值优化我们的权重。
- en: '![Linear Regression from Scratch with NumPy](../Images/19a05045d5c5faa8874379aad3328eda.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![使用 NumPy 从头开始进行线性回归](../Images/19a05045d5c5faa8874379aad3328eda.png)'
- en: Image from [Sebasitan Raschka](https://sebastianraschka.com/faq/docs/gradient-optimization.html)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Sebasitan Raschka](https://sebastianraschka.com/faq/docs/gradient-optimization.html)
- en: 'We take the gradient with respect to each weight value and then move them to
    the opposite of the gradient. This pushes the the loss towards minimum. As per
    the image, the gradient is positive, so we decrease the weight. This pushes the
    J(W) or loss towards the minimum value. Therefore, the optimization equations
    look as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对每个权重值取梯度，然后将其移动到梯度的相反方向。这将使损失值趋向最小值。根据图片，梯度为正，因此我们减少权重。这将使 J(W) 或损失值趋向最小值。因此，优化方程如下：
- en: '![Linear Regression from Scratch with NumPy](../Images/87a11aefee51baeef82929e742c74a05.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![使用 NumPy 从头开始进行线性回归](../Images/87a11aefee51baeef82929e742c74a05.png)'
- en: The learning rate (or alpha) controls the incremental steps shown in the image.
    We only make a small change in the value, for stable movement towards the minimum.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率（或 alpha）控制图片中显示的增量步长。我们只对值进行小幅度的调整，以稳定地向最小值移动。
- en: Implementation
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: If we simplify the derivate equation using basic algebraic manipulation, this
    becomes very simple to implement.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用基本的代数运算简化导数方程，这将变得非常简单。
- en: '![Linear Regression from Scratch with NumPy](../Images/d05edcda87c786e8fe178a475a1d8375.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![使用 NumPy 从头开始进行线性回归](../Images/d05edcda87c786e8fe178a475a1d8375.png)'
- en: 'For the derivate, we implement this using two lines of code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于导数，我们使用两行代码来实现：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: dw is again of shape (num_features, ) So we have a separate derivate value for
    each weight. We optimize them separately. db has a single value.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: dw 的形状仍为 (num_features, )，因此我们为每个权重有一个单独的导数值。我们分别优化这些权重。db 只有一个值。
- en: To optimize the values now, we move the values in the opposite direction of
    the gradient using basic subtraction.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了优化这些值，我们使用基本的减法将值移动到梯度的相反方向。
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Again, this is only a single step. We only make a small change to the randomly
    initialized values. We now repeatedly perform the same steps, to converge towards
    a minimum.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这只是一个单一的步骤。我们仅对随机初始化的值进行小幅度调整。现在我们重复相同的步骤，以趋近于最小值。
- en: 'The complete loop is as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的循环如下：
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Prediction
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测
- en: We predict the same way as we did during training. However, now we have the
    optimal set of weights and biases. The predicted values should now be close to
    the original values.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以与训练时相同的方式进行预测。然而，现在我们拥有了最佳的权重和偏置集合。预测值现在应该接近原始值。
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Results
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: 'With randomly initialized weights and bias, our predictions were as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机初始化的权重和偏置，我们的预测结果如下：
- en: '![Linear Regression from Scratch with NumPy](../Images/30983147e20a744358c6845a763a3492.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![使用 NumPy 从头开始进行线性回归](../Images/30983147e20a744358c6845a763a3492.png)'
- en: Image by Author
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Weight and bias were initialized very close to 0, so we obtain a horizontal
    line. After training the model for 1000 iterations, we get this:![Linear Regression
    from Scratch with NumPy](../Images/ff5eda8a6ccf432c7b79a8a6d371f38d.png)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 权重和偏差初始化非常接近 0，因此我们得到了一条水平线。经过 1000 次迭代训练后，我们得到了这个：![从零开始的线性回归与 NumPy](../Images/ff5eda8a6ccf432c7b79a8a6d371f38d.png)
- en: Image by Author
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: The predicted line passes right through the center of our data and seems to
    be the best-fit line possible.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的线正好穿过我们数据的中心，似乎是可能的最佳拟合线。
- en: Conclusion
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: You have now implemented Linear Regression from scratch. The complete code is
    also available on [GitHub](https://github.com/MuhammadArham-43/ML_Algos).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经从零开始实现了线性回归。完整代码也可以在[GitHub](https://github.com/MuhammadArham-43/ML_Algos)上找到。
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**[Muhammad Arham](https://www.linkedin.com/in/muhammad-arham-a5b1b1237/)**
    is a Deep Learning Engineer working in Computer Vision and Natural Language Processing.
    He has worked on the deployment and optimizations of several generative AI applications
    that reached the global top charts at Vyro.AI. He is interested in building and
    optimizing machine learning models for intelligent systems and believes in continual
    improvement.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**[穆罕默德·阿赫曼](https://www.linkedin.com/in/muhammad-arham-a5b1b1237/)** 是一名深度学习工程师，专注于计算机视觉和自然语言处理。他曾在
    Vyro.AI 上工作，负责多个生成型 AI 应用的部署和优化，这些应用在全球排行榜上名列前茅。他对构建和优化智能系统的机器学习模型充满兴趣，并相信持续改进。'
- en: More On This Topic
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Comparing Linear and Logistic Regression](https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性回归与逻辑回归比较](https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html)'
- en: '[3 Reasons Why You Should Use Linear Regression Models Instead of…](https://www.kdnuggets.com/2021/08/3-reasons-linear-regression-instead-neural-networks.html)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用线性回归模型而非…的 3 个理由](https://www.kdnuggets.com/2021/08/3-reasons-linear-regression-instead-neural-networks.html)'
- en: '[Linear vs Logistic Regression: A Succinct Explanation](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性回归与逻辑回归：简明解释](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
- en: '[KDnuggets News 22:n12, March 23: Best Data Science Books for…](https://www.kdnuggets.com/2022/n12.html)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻 22:n12, 3 月 23 日：最佳数据科学书籍…](https://www.kdnuggets.com/2022/n12.html)'
- en: '[Linear Regression for Data Science](https://www.kdnuggets.com/2022/07/linear-regression-data-science.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中的线性回归](https://www.kdnuggets.com/2022/07/linear-regression-data-science.html)'
- en: '[Making Predictions: A Beginner''s Guide to Linear Regression in Python](https://www.kdnuggets.com/2023/06/making-predictions-beginner-guide-linear-regression-python.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[预测指南：Python 中线性回归的初学者指南](https://www.kdnuggets.com/2023/06/making-predictions-beginner-guide-linear-regression-python.html)'
