- en: The ABCs of NLP, From A to Z
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/10/abcs-nlp-a-to-z.html](https://www.kdnuggets.com/2022/10/abcs-nlp-a-to-z.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![The ABCs of NLP](../Images/a95abd83df49823b923f73ff8556bcff.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Towfiqu barbhuiya](https://unsplash.com/@towfiqu999999?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: There is no shortage of text data available today. Vast amounts of text are
    created each and every day, with this data ranging from fully structured to semi-structured
    to fully unstructured.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: What can we do with this text? Well, quite a bit, actually; depending on exactly
    what your objectives are, there are 2 intricately related yet differentiated umbrellas
    of tasks which can be exploited in order to leverage the availability of all of
    this data.
  prefs: []
  type: TYPE_NORMAL
- en: Natural Languguage Processing vs. Text Mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start with some definitions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Natural language processing (NLP)** concerns itself with the interaction
    between natural human languages and computing devices. NLP is a major aspect of
    computational linguistics, and also falls within the realms of computer science
    and artificial intelligence.**Text mining** exists in a similar realm as NLP,
    in that it is concerned with identifying interesting, non-trivial patterns in
    textual data.'
  prefs: []
  type: TYPE_NORMAL
- en: As you may be able to tell from the above, the exact boundaries of these 2 concepts
    are not well-defined and agreed-upon, and bleed into one another to varying degrees,
    depending on the practitioners and researchers with whom one would discuss such
    matters. I find it easiest to differentiate by degree of insight. If raw text
    is data, then text mining can extract **information**, while NLP extracts *knowledge*
    (see the Pyramid of Understanding below). Syntax versus semantics, if you will.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Pyramid of Understanding](../Images/fcba6856f9d93514b15b4a585e1fb548.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Pyramid of Understanding: data, information, knowledge'
  prefs: []
  type: TYPE_NORMAL
- en: Various other explanations as to the precise relationship between text mining
    and NLP exist, and you are free to find something that works better for you. We
    aren't really as concerned with the **exact** definitions — absolute or relative
    — as much as we are with the intuitive recognition that the concepts are related
    with some overlap, yet still distinct.
  prefs: []
  type: TYPE_NORMAL
- en: 'The point we will move forward with: although NLP and text mining are not the
    same thing, they are closely related, deal with the same raw data type, and have
    some crossover in their uses. For the remainder of our discussion, we will conveniently
    refer to these 2 concepts as NLP. Importantly, much of the preprocessing of data
    for the tasks falling under these 2 umbrellas is identical.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of the exact NLP task you are looking to perform, maintaining text
    meaning and avoiding ambiguity is paramount. As such, attempts to avoid ambiguity
    is a large part of the text preprocessing process. We want to preserve intended
    meaning while eliminating noise. In order to do so, the following is required:'
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge about language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge about the world
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A way to combine knowledge sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If this were easy, we likely wouldn't be talking about it. What are some reasons
    why processing text is difficult?
  prefs: []
  type: TYPE_NORMAL
- en: Why is Processing Text Difficult?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![Why is processing text difficult?](../Images/5a4c88a2115dd4c217aabde148806d05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: CS124 Stanford (https://web.stanford.edu/class/cs124/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may have noticed something curious about the above: "non-standard English."
    While certainly not the intention of the figure — we are taking it fully out of
    context here — for a variety of reasons, much of NLP research has historically
    taken place within the confines of the English language. So we can add to the
    question of "Why is processing text difficult?" the additional layers of trouble
    which non-English languages — and especially languages with smaller numbers of
    speakers, or even [endangered languages](https://en.wikipedia.org/wiki/Endangered_language)
    — must go through in order to be processed and have insights drawn from.'
  prefs: []
  type: TYPE_NORMAL
- en: '![The ABCs of NLP](../Images/4e246faf25596cb69da1df0fd63fba91.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image by rawpixel.com](https://www.freepik.com/free-vector/illustration-language-concept_2825850.htm#query=language&position=0&from_view=search&track=sph)
    on Freepik'
  prefs: []
  type: TYPE_NORMAL
- en: Just being cognizant of the fact that NLP ≠ English when thinking and speaking
    about NLP is one small way in which this bias can be stemmed.
  prefs: []
  type: TYPE_NORMAL
- en: Textual Data Science Task Framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can we craft a sufficiently general framework for approaching textual data science
    tasks? It turns out that processing text is very similar to other non-text processing
    tasks, and so we can look to the [KDD Process](https://www.kdnuggets.com/2016/03/data-science-process-rediscovered.html/2)
    for inspiration.
  prefs: []
  type: TYPE_NORMAL
- en: We can say that these are the main steps of a generic text-based task, one which
    falls under text mining or NLP.
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Data Collection or Assembly**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Obtain or build corpus, which could be anything from emails, to the entire set
    of English Wikipedia articles, to our company's financial reports, to the complete
    works of Shakespeare, to something different altogether
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2\. Data Preprocessing**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Perform the preparation tasks on the raw text corpus in anticipation of text
    mining or NLP task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preprocessing consists of a number of steps, any number of which may or
    not apply to a given task, but generally fall under the broad categories of tokenization,
    normalization, and substitution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3\. Data Exploration & Visualization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regardless of what our data is -- text or not -- exploring and visualizing it
    is an essential step in gaining insight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common tasks may include visualizing word counts and distributions, generating
    wordclouds, and performing distance measures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**4\. Model Building**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is where our bread and butter text mining or NLP task takes place (includes
    training and testing)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also includes feature selection & engineering when applicable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Language models: Finite state machines, Markov models, vector space modeling
    of word meanings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Machine learning classifiers: Naive bayes, logistic regression, decision trees,
    Support Vector Machines, neural networks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sequence models: Hidden Markov models, recursive neural networks (RNNs), Long
    short term memory neural networks (LSTMs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**5\. Model Evaluation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Did the model perform as expected?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics will vary dependent on what type of text mining or NLP task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Even thinking outside of the box with chatbots (an NLP task) or generative
    models: some form of evaluation is necessary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![The ABCs of NLP](../Images/15b55f9e2420ef63c99915f836f88aa9.png)'
  prefs: []
  type: TYPE_IMG
- en: A simple textual data task framework
  prefs: []
  type: TYPE_NORMAL
- en: 'Clearly, any framework focused on the preprocessing of textual data would have
    to be synonymous with step number 2\. Expanding upon this step, specifically,
    we had the following to say about what this step would likely entail:'
  prefs: []
  type: TYPE_NORMAL
- en: Perform the preparation tasks on the raw text corpus in anticipation of text
    mining or NLP task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preprocessing consists of a number of steps, any number of which may or
    not apply to a given task, but generally fall under the broad categories of tokenization,
    normalization, and substitution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More generally, we are interested in taking some predetermined body of text
    and performing upon it some basic analysis and transformations, in order to be
    left with artefacts which will be much more useful for performing some further,
    more meaningful analytic task afterward. This further task would be our core text
    mining or natural language processing work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, as mentioned above, it seems as though there are 3 main components of text
    preprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: tokenization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: subsitution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we lay out a framework for approaching preprocessing, we should keep these
    high-level concepts in mind.
  prefs: []
  type: TYPE_NORMAL
- en: Text Preprocessing Framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will introduce this framework conceptually, independent of tools. We will
    then followup with a practical implementation of these steps next time, in order
    to see how they would be carried out in the Python ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: The sequence of these tasks is not necessarily as follows, and there can be
    some iteration upon them as well.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Noise Removal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Noise removal performs some of the substitution tasks of the framework. Noise
    removal is a much more task-specific section of the framework than are the following
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind again that we are not dealing with a linear process, the steps
    of which must exclusively be applied in a specified order. Noise removal, therefore,
    can occur before or after the previously-outlined sections, or at some point between).
  prefs: []
  type: TYPE_NORMAL
- en: '**Corpus** (literally Latin for body) refers to a collection of texts. Such
    collections may be formed of a single language of texts, or can span multiple
    languages; there are numerous reasons for which multilingual corpora (the plural
    of corpus) may be useful. Corpora may also consist of themed texts (historical,
    Biblical, etc.). Corpora are generally solely used for statistical linguistic
    analysis and hypothesis testing.'
  prefs: []
  type: TYPE_NORMAL
- en: How about something more concrete. Let's assume we obtained a corpus from the
    world wide web, and that it is housed in a raw web format. We can, then, assume
    that there is a high chance our text could be wrapped in HTML or XML tags. While
    this accounting for metadata can take place as part of the text collection or
    assembly process (step 1 of our textual data task framework), it depends on how
    the data was acquired and assembled. Sometimes we have control of this data collection
    and assembly process, and so our corpus may already have been de-noised during
    collection.
  prefs: []
  type: TYPE_NORMAL
- en: But this is not always the case. If the corpus you happen to be using is noisy,
    you have to deal with it. Recall that analytics tasks are often talked about as
    being 80% data preparation!
  prefs: []
  type: TYPE_NORMAL
- en: The good thing is that pattern matching can be your friend here, as can existing
    software tools built to deal with just such pattern matching tasks.
  prefs: []
  type: TYPE_NORMAL
- en: remove text file headers, footers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: remove HTML, XML, etc. markup and metadata
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: extract valuable data from other formats, such as JSON, or from within databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if you fear regular expressions, this could potentially be the part of text
    preprocessing in which your worst fears are realized
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regular expressions**, often abbreviated *regexp* or *regexp*, are a tried
    and true method of concisely describing patterns of text. A regular expression
    is represented as a special text string itself, and is meant for developing search
    patterns on selections of text. Regular expressions can be thought of as an expanded
    set of rules beyond the wildcard characters of **?** and *****. Though often cited
    as frustrating to learn, regular expressions are incredibly powerful text searching
    tools.'
  prefs: []
  type: TYPE_NORMAL
- en: As you can imagine, the boundary between noise removal and data collection and
    assembly is a fuzzy one, and as such some noise removal must absolutely take place
    before other preprocessing steps. For example, any text required from a JSON structure
    would obviously need to be removed prior to tokenization.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before further processing, text needs to be normalized.
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalization** generally refers to a series of related tasks meant to put
    all text on a level playing field: converting all text to the same case (upper
    or lower), removing punctuation, expanding contractions, converting numbers to
    their word equivalents, and so on. Normalization puts all words on equal footing,
    and allows processing to proceed uniformly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Normalizing text can mean performing a number of tasks, but for our framework
    we will approach normalization in 3 distinct steps: (1) stemming, (2) lemmatization,
    and (3) everything else.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stemming** is the process of eliminating affixes (suffixed, prefixes, infixes,
    circumfixes) from a word in order to obtain a word stem.'
  prefs: []
  type: TYPE_NORMAL
- en: '`running → run` **Lemmatization** is related to stemming, differing in that
    lemmatization is able to capture canonical forms based on a word''s lemma.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, stemming the word "better" would fail to return its citation form
    (another word for lemma); however, lemmatization would result in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`better → good`'
  prefs: []
  type: TYPE_NORMAL
- en: It should be easy to see why the implementation of a stemmer would be the less
    difficult feat of the two.
  prefs: []
  type: TYPE_NORMAL
- en: Everything else
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A clever catch-all, right? Stemming and lemmatization are major parts of a text
    preprocessing endeavor, and as such they need to be treated with the respect they
    deserve. These aren't simple text manipulation; they rely on detailed and nuanced
    understanding of grammatical rules and norms.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are, however, numerous other steps that can be taken to help put all
    text on equal footing, many of which involve the comparatively simple ideas of
    substitution or removal. They are, however, no less important to the overall process.
    These include:'
  prefs: []
  type: TYPE_NORMAL
- en: set all characters to lowercase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: remove numbers (or convert numbers to textual representations)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: remove punctuation (generally part of tokenization, but still worth keeping
    in mind at this stage, even as confirmation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: strip white space (also generally part of tokenization)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: remove default stop words (general English stop words)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stop words are those words which are filtered out before further processing
    of text, since these words contribute little to overall meaning, given that they
    are generally the most common words in a language. For instance, "the," "and,"
    and "a," while all required words in a particular passage, don''t generally contribute
    greatly to one''s understanding of content. As a simple example, the following
    panagram is just as legible if the stop words are removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`~~The~~ quick brown fox jumps over ~~the~~ lazy dog.`'
  prefs: []
  type: TYPE_NORMAL
- en: remove given (task-specific) stop words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: remove sparse terms (not always necessary or helpful, though!)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A this point, it should be clear that text preprocessing relies heavily on pre-built
    dictionaries, databases, and rules. You will be relieved to find that when we
    undertake a practical text preprocessing task in the Python ecosystem in our next
    article that these pre-built support tools are readily available for our use;
    there is no need to be inventing our own wheels.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Tokenization** is, generally, an early step in the NLP process, a step which
    splits longer strings of text into smaller pieces, or tokens. Larger chunks of
    text can be tokenized into sentences, sentences can be tokenized into words, etc.
    Further processing is generally performed after a piece of text has been appropriately
    tokenized.'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization is also referred to as text segmentation or lexical analysis. Sometimes
    **segmentation** is used to refer to the breakdown of a large chunk of text into
    pieces larger than words (e.g. paragraphs or sentences), while tokenization is
    reserved for the breakdown process which results exclusively in words.
  prefs: []
  type: TYPE_NORMAL
- en: This may sound like a straightforward process, but it is anything but. How are
    sentences identified within larger bodies of text? Off the top of your head you
    probably say "sentence-ending punctuation," and may even, just for a second, think
    that such a statement is unambiguous.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sure, this sentence is easily identified with some basic segmentation rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '`The quick brown fox jumps over the lazy dog.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'But what about this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Dr. Ford did not ask Col. Mustard the name of Mr. Smith''s dog.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Or this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"What is all the fuss about?" asked Mr. Peters.`'
  prefs: []
  type: TYPE_NORMAL
- en: And that's just sentences. What about words? Easy, right? Right?
  prefs: []
  type: TYPE_NORMAL
- en: '`This full-time student isn''t living in on-campus housing, and she''s not
    wanting to visit Hawai''i.`'
  prefs: []
  type: TYPE_NORMAL
- en: It should be intuitive that there are varying strategies not only for identifying
    segment boundaries, but also what to do when boundaries are reached. For example,
    we might employ a segmentation strategy which (correctly) identifies a particular
    boundary between word tokens as the apostrophe in the word *she's* (a strategy
    tokenizing on whitespace alone would not be sufficient to recognize this). But
    we could then choose between competing strategies such as keeping the punctuation
    with one part of the word, or discarding it altogether. One of these approaches
    just seems correct, and does not seem to pose a real problem. But just think of
    all the other special cases in just the English language we would have to take
    into account.
  prefs: []
  type: TYPE_NORMAL
- en: '**Consideration**: when we segment text chunks into sentences, should we preserve
    sentence-ending delimiters? Are we interested in remembering where sentences ended?'
  prefs: []
  type: TYPE_NORMAL
- en: What NLP Tasks Exist?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered some text (pre)processing steps useful for NLP tasks, but what
    about the tasks themselves?
  prefs: []
  type: TYPE_NORMAL
- en: There are no hard lines between these task types; however, many are fairly well-defined
    at this point. A given macro NLP task may include a variety of sub-tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We first outlined the main approaches, since the technologies are often focused
    on for beginners, but it's good to have a concrete idea of what types of NLP tasks
    there are. Below are the main categories of NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Text Classification Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Representation: bag of words, n-grams, one-hot encoding (sparse matrix) - these
    methods do not preserve word order'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goal: predict tags, categories, sentiment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Application: filtering spam emails, classifying documents based on dominant
    content'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bag of words** is a particular representation model used to simplify the
    contents of a selection of text. The bag of words model omits grammar and word
    order, but is interested in the number of occurrences of words within the text.
    The ultimate representation of the text selection is that of a bag of words (**bag**
    referring to the set theory concept of [multisets](https://en.wikipedia.org/wiki/Multiset),
    which differ from simple sets).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Actual storage mechanisms for the bag of words representation can vary, but
    the following is a simple example using a dictionary for intuitiveness. Sample
    text:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"Well, well, well," said John.`'
  prefs: []
  type: TYPE_NORMAL
- en: '"There, there," said James. "There, there."'
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting bag of words representation as a dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0] **n-grams** is another representation model for simplifying text selection
    contents. As opposed to the orderless representation of bag of words, n-grams
    modeling is interested in preserving contiguous sequences of *N* items from the
    text selection.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of trigram (3-gram) model of the second sentence of the above example
    ("There, there," said James. "There, there.") appears as a list representation
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1] Prior to the wide-spread use of neural networks in NLP — in what we
    will refer to as "traditional" NLP — vectorization of text often occurred via
    **one-hot encoding** (note that this persists as a useful encoding practice for
    a number of exercises, and has not fallen out of fashion due to the use of neural
    networks). For one-hot encoding, each word, or token, in a text corresponds to
    a vector element.![one-hot encoding](../Images/5ef1df6b072c42628ebebb1bed002594.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: [Adrian Colyer](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)'
  prefs: []
  type: TYPE_NORMAL
- en: We could consider the image above, for example, as a small excerpt of a vector
    representing the sentence "The queen entered the room." Note that only the element
    for "queen" has been activated, while those for "king," "man," etc. have not.
    You can imagine how differently the one-hot vector representation of the sentence
    "The king was once a man, but is now a child" would appear in the same vector
    element section pictured above.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Word Sequence Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Representation: sequences (preserves word order)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goal: language modeling - predict next/previous word(s), text generation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Application: translation, chatbots, sequence tagging (predict POS tags for
    each word in sequence), named entity recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language modeling** is the process of building a statistical language model
    which is meant to provide an estimate of a natural language. For a sequence of
    input words, the model would assign a probability to the entire sequence, which
    contributes to the estimated likelihood of various possible sequences. This can
    be especially useful for NLP applications which generate or predict text.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Text Meaning Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Representation: word vectors, the mapping of words to vectors (*n*-dimensional
    numeric vectors) aka embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goal: how do we represent meaning?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Application: finding similar words (similar vectors), sentence embeddings (as
    opposed to word embeddings), topic modeling, search, question answering'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dense embedding vectors** aka word embeddings result in the representation
    of core features embedded into an embedding space of size *d* dimensions. We can
    compress, if you will, the number of dimensions used to represent 20,000 unique
    words down to, perhaps, 50 or 100 dimensions. In this approach, each feature no
    longer has its own dimension, and is instead mapped to a vector.![Dense embedding
    vectors](../Images/8913c8af571d99ba5f4f14898a5f3a8e.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: [Adrian Colyer](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)'
  prefs: []
  type: TYPE_NORMAL
- en: So, what exactly are these features? We leave it to a neural network to determine
    the important aspects of relationships between words. Though human interpretation
    of these features would not be precisely possible, the image above provides an
    insight into what the underlying process may look like, relating to the famous
    `King - Man + Woman = Queen` [example](https://www.technologyreview.com/s/541356/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/).
  prefs: []
  type: TYPE_NORMAL
- en: '**Named entity recognition** is the attempt to identify [named entities](https://en.wikipedia.org/wiki/Named_entity)
    in text data, and categorize them appropriately. Named entities categories include,
    but are not limited to, person names, locations, organizations, monetary values,
    dates and time, quantities, etc., and can include custom categories dependent
    on the application (medical, scientific, business, etc.). You can think of named
    entities as **proper noun++**.![NER](../Images/5b9fbdd9038ed540762215991c040a03.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Named entity recognition (NER) using spaCy (text excerpt taken from [here](https://www.nomadicmatt.com/travel-blogs/three-days-in-new-york-city/))
  prefs: []
  type: TYPE_NORMAL
- en: '**Part-of-speech tagging** consists of assigning a category tag to the tokenized
    parts of a sentence. The most popular POS tagging would be identifying words as
    nouns, verbs, adjectives, etc.![POS Tagging](../Images/a8df09740c1f85d2d5d57166eca3d9ae.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Part-of-speech tagging
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Sequence to Sequence Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many tasks in NLP can be framed as such
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples are machine translation, summarization, simplification, Q&A systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Such systems are characterized by encoders and decoders, which work in complement
    to find a hidden representation of text, and to use that hidden representation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5\. Dialog Systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2 main categories of dialog systems, categorized by their scope of use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goal-oriented dialog systems focus on being useful in a particular, restricted
    domain; more precision, less generalizable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversational dialog systems are concerned with being helpful or entertaining
    in a much more general context; less precision, more generalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether it be in dialog systems or the practical difference between rule-based
    and more complex approaches to solving NLP tasks, note the trade-off between precision
    and generalizability; you generally sacrifice in one area for an increase in the
    other.
  prefs: []
  type: TYPE_NORMAL
- en: Approaches to NLP Tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While not cut and dry, there are 3 main groups of approaches to solving NLP
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Dependency parse tree using spaCy](../Images/06df8524acd26c1d5673e7fcf1602ad5.png)'
  prefs: []
  type: TYPE_IMG
- en: Dependency parse tree using spaCy
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Rule-based
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Rule-based approaches are the oldest approaches to NLP. Why are they still
    used, you might ask? It''s because they are tried and true, and have been proven
    to work well. Rules applied to text can offer a lot of insight: think of what
    you can learn about arbitrary text by finding what words are nouns, or what verbs
    end in -ing, or whether a pattern recognizable as Python code can be identified.
    [Regular expressions](https://en.wikipedia.org/wiki/Regular_expression) and [context
    free grammars](https://en.wikipedia.org/wiki/Context-free_grammar) are textbook
    examples of rule-based approaches to NLP.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Rule-based approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: tend to focus on pattern-matching or parsing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: can often be thought of as "fill in the blanks" methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: are low precision, high recall, meaning they can have high performance in specific
    use cases, but often suffer performance degradation when generalized
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2\. "Traditional" Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '"Traditional" machine learning approaches include probabilistic modeling, likelihood
    maximization, and linear classifiers. Notably, these are not neural network models
    (see those below).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional machine learning approaches are characterized by:'
  prefs: []
  type: TYPE_NORMAL
- en: training data - in this case, a corpus with markup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: feature engineering - word type, surrounding words, capitalized, plural, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: training a model on parameters, followed by fitting on test data (typical of
    machine learning systems in general)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: inference (applying model to test data) characterized by finding most probable
    words, next word, best category, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"semantic slot filling"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3\. Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is similar to "traditional" machine learning, but with a few differences:'
  prefs: []
  type: TYPE_NORMAL
- en: feature engineering is generally skipped, as networks will "learn" important
    features (this is generally one of the claimed big benefits of using neural networks
    for NLP)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: instead, streams of raw parameters ("words" -- actually vector representations
    of words) without engineered features, are fed into neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: very large training corpus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specific neural networks of use in NLP have "historically" included recurrent
    neural networks (RNNs) and convolutional neural networks (CNNs). Today, the one
    architecture that rules them all is the transformer.
  prefs: []
  type: TYPE_NORMAL
- en: Why use "traditional" machine learning (or rule-based) approaches for NLP?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: still good for sequence labeling (using probabilistic modeling)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: some ideas in neural networks are very similar to earlier methods (word2vec
    similar in concept to distributional semantic methods)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use methods from traditional approaches to improve neural network approaches
    (for example, word alignments and attention mechanisms are similar)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why deep learning over "traditional" machine learning?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SOTA in many applications (for example, machine translation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a lot of research (majority?) in NLP happening here now
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Importantly**, both neural network and non-neural network approaches can
    be useful for contemporary NLP in their own right; they can also can be used or
    studied in tandem for maximum potential benefit'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[From Languages to Information](https://web.stanford.edu/class/cs124/), Stanford'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Natural Language Processing](https://www.coursera.org/learn/language-processing),
    National Research University Higher School of Economics (Coursera)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Neural Network Methods for Natural Language Processing](https://www.morganclaypool.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037),
    Yoav Goldberg'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Natural Language Processing](https://github.com/yandexdataschool/nlp_course/),
    Yandex Data School'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[The amazing power of word vectors](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/),
    Adrian Colyer'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**[Matthew Mayo](https://www.linkedin.com/in/mattmayo13/)** ([**@mattmayo13**](https://twitter.com/mattmayo13))
    is a Data Scientist and the Editor-in-Chief of KDnuggets, the seminal online Data
    Science and Machine Learning resource. His interests lie in natural language processing,
    algorithm design and optimization, unsupervised learning, neural networks, and
    automated approaches to machine learning. Matthew holds a Master''s degree in
    computer science and a graduate diploma in data mining. He can be reached at editor1
    at kdnuggets[dot]com.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Explain NLP Models with LIME](https://www.kdnuggets.com/2022/01/explain-nlp-models-lime.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Must Read NLP Papers from the Last 12 Months](https://www.kdnuggets.com/2023/03/must-read-nlp-papers-last-12-months.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Ultimate Guide To Different Word Embedding Techniques In NLP](https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Range of NLP Applications in the Real World: A Different…](https://www.kdnuggets.com/2022/03/different-solution-problem-range-nlp-applications-real-world.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning’s Sweet Spot: Pure Approaches in NLP and Document Analysis](https://www.kdnuggets.com/2022/05/machine-learning-sweet-spot-pure-approaches-nlp-document-analysis.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[oBERT: Compound Sparsification Delivers Faster Accurate Models for NLP](https://www.kdnuggets.com/2022/05/obert-compound-sparsification-delivers-faster-accurate-models-nlp.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
