- en: 'Comparing Natural Language Processing Techniques: RNNs, Transformers, BERT'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较自然语言处理技术：RNN、变换器、BERT
- en: 原文：[https://www.kdnuggets.com/comparing-natural-language-processing-techniques-rnns-transformers-bert](https://www.kdnuggets.com/comparing-natural-language-processing-techniques-rnns-transformers-bert)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/comparing-natural-language-processing-techniques-rnns-transformers-bert](https://www.kdnuggets.com/comparing-natural-language-processing-techniques-rnns-transformers-bert)
- en: '![Comparing Natural Language Processing Techniques: RNNs, Transformers, BERT](../Images/0bd14f1879f44952bb0f2a35143da063.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![比较自然语言处理技术：RNN、变换器、BERT](../Images/0bd14f1879f44952bb0f2a35143da063.png)'
- en: Image by [Freepik](https://www.freepik.com/free-vector/hand-drawn-flat-design-npl-illustration_22379566.htm#query=natural%20language%20processing&position=9&from_view=search&track=ais)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Freepik](https://www.freepik.com/free-vector/hand-drawn-flat-design-npl-illustration_22379566.htm#query=natural%20language%20processing&position=9&from_view=search&track=ais)
    提供
- en: Natural Language Processing, or NLP, is a field within artificial intelligence
    for machines to have the ability to understand textual data. NLP research has
    existed for a long time, but only recently has it become more prominent with the
    introduction of big data and higher computational processing power.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是人工智能领域中的一个领域，旨在让机器具备理解文本数据的能力。NLP 研究已经存在了很长时间，但随着大数据和更高计算处理能力的引入，它最近才变得更加突出。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您组织的 IT 工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: With the NLP field becoming bigger, many researchers would try to improve the
    machine's capability to understand the textual data better. Through much progress,
    many techniques are proposed and applied in the NLP field.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 NLP 领域的不断扩大，许多研究人员会尝试提高机器理解文本数据的能力。经过多次进展，许多技术在 NLP 领域中被提出和应用。
- en: This article will compare various techniques for processing text data in the
    NLP field. This article will focus on discussing RNN, Transformers, and BERT because
    it’s the one that is often used in research. Let’s get into it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将比较 NLP 领域中处理文本数据的各种技术。本文将重点讨论 RNN、变换器和 BERT，因为它们是研究中经常使用的技术。让我们开始吧。
- en: Recurrent Neural Network
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归神经网络
- en: '[Recurrent Neural Network](https://en.wikipedia.org/wiki/Recurrent_neural_network)
    or RNN was developed in 1980 but only recently gained attraction in the NLP field.
    RNN is a particular type within the neural network family used for sequential
    data or data that can’t be independent of each other. Sequential data examples
    are time series, audio, or text sentence data, basically any kind of data with
    meaningful order.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[递归神经网络](https://en.wikipedia.org/wiki/Recurrent_neural_network)（RNN）于1980年开发，但最近才在
    NLP 领域获得关注。RNN 是神经网络家族中的一种特殊类型，用于处理序列数据或彼此之间不能独立的数据。序列数据的例子有时间序列、音频或文本句子数据，基本上是任何具有有意义顺序的数据。'
- en: RNNs are different from regular feed-forward neural networks as they process
    information differently. In the normal feed-forward, the information is processed
    following the layers. However,  RNN is using a loop cycle on the information input
    as consideration. To understand the differences, let’s see the image below.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: RNN（递归神经网络）与普通的前馈神经网络不同，因为它们处理信息的方式不同。在普通的前馈神经网络中，信息是按层次进行处理的。然而，RNN 是通过对信息输入进行循环处理来考虑的。要理解这些差异，我们可以看看下面的图像。
- en: '![Comparing Natural Language Processing Techniques: RNNs, Transformers, BERT](../Images/1c48bad07f4ce12a89b0e314bbe3fd82.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![比较自然语言处理技术：RNN、变换器、BERT](../Images/1c48bad07f4ce12a89b0e314bbe3fd82.png)'
- en: Image by Author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: As you can see, the RNNs model implements a loop cycle during the information
    processing. RNNs would consider the current and previous data input when processing
    this information. That’s why the model is suitable for any type of sequential
    data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，RNN模型在信息处理过程中实现了循环。RNN在处理这些信息时会考虑当前和先前的数据输入。这就是为什么该模型适用于任何类型的序列数据。
- en: If we take an example in the text data, imagine we have the sentence “I wake
    up at 7 AM”, and we have the word as input. In the feed-forward neural network,
    when we reach the word “up,” the model would already forget the words “I,” “wake,”
    and “up.” However, RNNs would use every output for each word and loop them back
    so the model would not forget.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们以文本数据为例，假设我们有句子“I wake up at 7 AM”，并且将每个单词作为输入。在前馈神经网络中，当我们到达单词“up”时，模型可能已经忘记了单词“I”、“wake”和“up”。然而，RNN会利用每个单词的输出并将其循环回来，以便模型不会遗忘。
- en: In the NLP field, RNNs are often used in many textual applications, such as
    text classification and generation. It’s often used in word-level applications
    such as Part of Speech tagging, next-word generation, etc.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理领域，RNN常用于许多文本应用，例如文本分类和生成。它常用于词级应用，如词性标注、下一个词生成等。
- en: Looking at the RNNs more in-depth on the textual data, there are many types
    of RNNs. For example, the below image is the many-to-many types.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 深入分析文本数据中的RNN，有许多不同类型的RNN。例如，下面的图像展示了多对多类型。
- en: '![Comparing Natural Language Processing Techniques: RNNs, Transformers, BERT](../Images/c8aa7900dbf0cdac49becd02f5d6eb4c.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![比较自然语言处理技术：RNN、变换器、BERT](../Images/c8aa7900dbf0cdac49becd02f5d6eb4c.png)'
- en: Image by Author
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Looking at the image above, we can see that the output for each step (time-step
    in RNN) is processed one step at a time, and every iteration always considers
    the previous information.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的图像可以看到，每一步的输出（RNN中的时间步）是逐步处理的，每次迭代始终考虑先前的信息。
- en: Another RNN type used in many NLP applications is the encoder-decoder type (Sequence-to-Sequence).
    The structure is shown in the image below.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种在许多NLP应用中使用的RNN类型是编码器-解码器类型（序列到序列）。其结构如下面的图像所示。
- en: '![Comparing Natural Language Processing Techniques: RNNs, Transformers, BERT](../Images/a458219e621d57011c85d559fe3fe587.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![比较自然语言处理技术：RNN、变换器、BERT](../Images/a458219e621d57011c85d559fe3fe587.png)'
- en: Image by Author
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: This structure introduces two parts that are used in the model. The first part
    is called Encoder, which is a part that receives data sequence and creates a new
    representation based on it. The representation would be used in the second part
    of the model, which is the decoder. With this structure, the input and output
    lengths don’t necessarily need to be equal. The example use case is a language
    translation, which often does not have the same length between the input and output.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构引入了模型中使用的两个部分。第一个部分称为编码器，它接收数据序列并基于其创建一个新的表示。该表示将用于模型的第二部分，即解码器。通过这种结构，输入和输出的长度不一定需要相等。一个示例用例是语言翻译，输入和输出之间的长度通常不相同。
- en: 'There are various benefits of using RNNs to process natural language data,
    including:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RNN处理自然语言数据有多种好处，包括：
- en: RNN can be used to process text input without length limitations.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN可以处理没有长度限制的文本输入。
- en: The model shares the same weights across all the time steps, which allows the
    neural network to use the same parameter in each step.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型在所有时间步中共享相同的权重，这使得神经网络在每一步使用相同的参数。
- en: Having the memory of past input makes RNN suitable for any sequential data.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于记忆过去的输入，使得RNN适用于任何序列数据。
- en: 'But, there are several disadvantages as well:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，也存在一些缺点：
- en: RNN is susceptible to both vanishing and exploding gradients. This is where
    the gradient result is the near-zero value (vanishing), causing network weight
    to only be updated for a tiny amount, or the gradient result is so significant
    (exploding) that it assigns an unrealistic enormous importance to the network.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN容易受到梯度消失和梯度爆炸的影响。这是指梯度结果接近零值（消失），导致网络权重只更新很少，或者梯度结果非常显著（爆炸），使网络赋予不现实的巨大重要性。
- en: Long time of training because of the sequential nature of the model.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于模型的序列性质，训练时间较长。
- en: Short-term memory means that the model starts to forget the longer the model
    is trained. There is an extension of RNN called [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory)
    to alleviate this problem.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 短期记忆意味着模型在训练时间较长时开始遗忘。有一种名为[LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory)的
    RNN 扩展用于缓解此问题。
- en: Transformers
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变换器
- en: Transformers is an NLP model architecture that tries to solve the sequence-to-sequence
    tasks previously encountered in the RNNs. As mentioned above, RNNs have problems
    with short-term memory. The longer the input, the more prominent the model was
    in forgetting the information. This is where the attention mechanism could help
    solve the problem.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器是一个自然语言处理模型架构，旨在解决之前在 RNN 中遇到的序列到序列任务。如前所述，RNN 存在短期记忆问题。输入越长，模型遗忘信息的现象越明显。这是注意力机制可以帮助解决的问题所在。
- en: The attention mechanism is introduced in the paper by [Bahdanau *et al*. (2014)](https://arxiv.org/abs/1409.0473)
    to solve the long input problem, especially with encoder-decoder type of RNNs.
    I would not explain the attention mechanism in detail. Basically, it is a layer
    that allows the model to focus on the critical part of the model input while having
    the output prediction. For example, the word input “Clock” would correlate highly
    with “Jam” in Indonesian if the task is for translation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制在[Bahdanau *et al*. (2014)](https://arxiv.org/abs/1409.0473)的论文中引入，用于解决长输入问题，特别是对于编码器-解码器类型的
    RNN。我不会详细解释注意力机制。基本上，它是一个允许模型在进行输出预测时关注模型输入关键部分的层。例如，如果任务是翻译，那么“Clock”这个词在印尼语中与“Jam”会有很高的相关性。
- en: The transformers model is introduced by [Vaswani *et al.* (2017)](https://arxiv.org/pdf/1706.03762v5.pdf).
    The architecture is inspired by the encoder-decoder RNN and built with the attention
    mechanism in mind and does not process data in sequential order. The overall transformers
    model is structured like the image below.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器模型由[Vaswani *et al.* (2017)](https://arxiv.org/pdf/1706.03762v5.pdf)引入。该架构受到编码器-解码器
    RNN 的启发，并在设计时考虑了注意力机制，且不按顺序处理数据。整体变换器模型结构如下图所示。
- en: '![Comparing Natural Language Processing Techniques: RNNs, Transformers, BERT](../Images/90d580b1b12ee73411994c316ec1d188.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![比较自然语言处理技术：RNNs、变换器、BERT](../Images/90d580b1b12ee73411994c316ec1d188.png)'
- en: Transformers Architecture ([Vaswani *et al*. 2017](https://arxiv.org/pdf/1706.03762v5.pdf))
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器架构 ([Vaswani *et al*. 2017](https://arxiv.org/pdf/1706.03762v5.pdf))
- en: In the structure above, the transformers encode the data vector sequence into
    the word embedding with positional encoding in place while using the decoding
    to transform data into the original form. With the attention mechanism in place,
    the encoding can given importance according to the input.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述结构中，变换器将数据向量序列编码为带有位置编码的词嵌入，同时使用解码将数据转换回原始形式。借助注意力机制，编码可以根据输入赋予不同的重要性。
- en: 'Transformers provide few advantages compared to the other model, including:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他模型相比，变换器提供了一些优势，包括：
- en: The parallelization process increases the training and inference speed.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 并行化过程提高了训练和推理速度。
- en: Capable of processing longer input, which offers a better understanding of the
    context
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 能够处理更长的输入，这提供了对上下文的更好理解。
- en: 'There are still some disadvantages to the transformers model:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器模型仍然存在一些缺点：
- en: High computational processing and demand.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高计算处理和需求。
- en: The attention mechanism might require the text to be split because of the length
    limit it can handle.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意力机制可能要求对文本进行拆分，因为它能处理的长度有限制。
- en: Context might be lost if the split were done wrong.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果拆分方式不正确，可能会丢失上下文。
- en: '**BERT**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERT**'
- en: BERT, or Bidirectional Encoder Representations from Transformers, is a model
    developed by [Devlin *et al.* (2019)](https://arxiv.org/pdf/1810.04805.pdf) that
    involves two steps (pre-training and fine-tuning) to create the model. If we compare,
    BERT is a stack of transformers encoder (BERT Base has 12 Layers while BERT Large
    has 24 layers).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: BERT，即双向编码器表示变换器，是由[Devlin *et al.* (2019)](https://arxiv.org/pdf/1810.04805.pdf)开发的模型，包含两个步骤（预训练和微调）来构建模型。比较而言，BERT
    是一个变换器编码器堆叠（BERT Base 有 12 层，而 BERT Large 有 24 层）。
- en: BERT's overall model development can be shown in the image below.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 的整体模型开发可以在下图中看到。
- en: '![Comparing Natural Language Processing Techniques: RNNs, Transformers, BERT](../Images/8ea1ee96e72c9847b3af563700d17031.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![比较自然语言处理技术：RNNs、变换器、BERT](../Images/8ea1ee96e72c9847b3af563700d17031.png)'
- en: BERT overall procedures ([Devlin *et al.* (2019)](https://arxiv.org/pdf/1810.04805.pdf)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: BERT整体流程（[Devlin *et al.* (2019)](https://arxiv.org/pdf/1810.04805.pdf)）
- en: Pre-training tasks initiate the model's training at the same time, and once
    it is done, the model can be fine-tuned for various downstream tasks (question-answering,
    classification, etc.).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练任务同时启动模型的训练，完成后，模型可以针对各种下游任务（问答、分类等）进行微调。
- en: What makes BERT special is that it is the first unsupervised bidirectional language
    model that is pre-trained on text data. BERT was previously pre-trained on the
    entire Wikipedia and book corpus, consisting of over 3000 million words.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的独特之处在于它是第一个在文本数据上进行预训练的无监督双向语言模型。BERT之前在整个维基百科和书籍语料库上进行了预训练，包含了超过30亿个词。
- en: BERT is considered bidirectional because it didn’t read data input sequentially
    (from left to right or vice versa), but the transformer encoder read the whole
    sequence simultaneously.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: BERT被认为是双向的，因为它没有顺序读取数据输入（从左到右或反之），而是变换器编码器同时读取整个序列。
- en: Unlike directional models, which read the text input sequentially (left-to-right
    or right-to-left), the Transformer encoder reads the entire sequence of words
    simultaneously. That’s why the model is considered bidirectional and allows the
    model to understand the whole context of the input data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与顺序读取文本输入（从左到右或从右到左）的方向模型不同，变换器编码器同时读取整个词序列。这就是为什么该模型被认为是双向的，并且使模型能够理解输入数据的整体上下文。
- en: 'To achieve bidirectional, BERT uses two techniques:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现双向，BERT使用了两种技术：
- en: '**Mask Language Model (MLM)** — Word masking technique. The technique would
    mask 15% of the input words and try to predict this masked word based on the non-masked
    word.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**掩码语言模型（MLM）**——词掩码技术。该技术会掩盖输入词的15%，并尝试根据未掩盖的词来预测被掩盖的词。'
- en: '**Next Sentence Prediction (NSP)** —  BERT tries to learn the relationship
    between sentences. The model has pairs of sentences as the data input and tries
    to predict if the subsequent sentence exists in the original document.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**下一句预测（NSP）**——BERT尝试学习句子之间的关系。该模型将句子对作为数据输入，并尝试预测后续句子是否存在于原始文档中。'
- en: 'There are a few advantages to using BERT in the NLP field, including:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP领域使用BERT有几个优点，包括：
- en: BERT is easy to use for pre-trained various NLP downstream tasks.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERT易于用于预训练的各种NLP下游任务。
- en: Bidirectional makes BERT understand the text context better.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双向使BERT能够更好地理解文本上下文。
- en: It’s a popular model that has much support from the community
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是一个受到社区广泛支持的热门模型。
- en: 'Although, there are still a few disadvantages, including:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，仍然存在一些缺点，包括：
- en: Requires high computational power and long training time for some downstream
    task fine-tuning.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于一些下游任务微调，需要较高的计算能力和较长的训练时间。
- en: The BERT model might result in a big model requiring much bigger storage.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERT模型可能会导致一个需要更大存储空间的大型模型。
- en: It’s better to use for complex tasks as the performance for simple tasks is
    not much different than using simpler models.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于复杂任务，BERT的表现更好，因为在简单任务上的表现与使用更简单的模型并没有太大区别。
- en: Conclusion
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'NLP has become more prominent recently, and much research has focused on improving
    the applications. In this article, we discuss three NLP techniques that are often
    used:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）最近变得更加突出，许多研究集中在改进应用上。在本文中，我们讨论了三种常用的NLP技术：
- en: RNN
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN
- en: Transformers
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变换器
- en: BERT
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERT
- en: Each of the techniques has its advantages and disadvantages, but overall, we
    can see the model evolving in a better way.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 每种技术都有其优缺点，但总体而言，我们可以看到模型在不断进化。
- en: '**[](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**[Cornellius Yudha
    Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**** is a data science
    assistant manager and data writer. While working full-time at Allianz Indonesia,
    he loves to share Python and data tips via social media and writing media. Cornellius
    writes on a variety of AI and machine learning topics.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[Cornellius Yudha Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)
    是数据科学助理经理和数据撰稿人。在全职工作于安联印尼期间，他喜欢通过社交媒体和写作媒体分享Python和数据技巧。Cornellius撰写了各种人工智能和机器学习主题的文章。'
- en: More On This Topic
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[N-gram Language Modeling in Natural Language Processing](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理中的N-gram语言建模](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
- en: '[How to Fine-Tune BERT for Sentiment Analysis with Hugging Face Transformers](https://www.kdnuggets.com/how-to-fine-tune-bert-sentiment-analysis-hugging-face-transformers)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 Hugging Face Transformers 微调 BERT 以进行情感分析](https://www.kdnuggets.com/how-to-fine-tune-bert-sentiment-analysis-hugging-face-transformers)'
- en: '[Natural Language Processing Key Terms, Explained](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理关键术语解析](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
- en: '[Data Representation for Natural Language Processing Tasks](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理任务的数据表示](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
- en: '[Transfer Learning for Image Recognition and Natural Language Processing](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[图像识别和自然语言处理的迁移学习](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
- en: '[How to Start Using Natural Language Processing With PyTorch](https://www.kdnuggets.com/2022/04/start-natural-language-processing-pytorch.html)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何开始使用 PyTorch 进行自然语言处理](https://www.kdnuggets.com/2022/04/start-natural-language-processing-pytorch.html)'
