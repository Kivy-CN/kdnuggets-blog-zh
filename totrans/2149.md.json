["```py\npip install -U autotrain-advanced\n```", "```py\npip install datasets transformers\n```", "```py\nfrom datasets import load_dataset\nimport pandas as pd\n\n# Load the dataset\ntrain= load_dataset(\"tatsu-lab/alpaca\",split='train[:10%]')\ntrain = pd.DataFrame(train)\n```", "```py\ndef text_formatting(data):\n\n    # If the input column is not empty\n    if data['input']:\n\n        text = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{data[\"instruction\"]} \\n\\n### Input:\\n{data[\"input\"]}\\n\\n### Response:\\n{data[\"output\"]}\"\"\"\n\n    else:\n\n        text = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{data[\"instruction\"]}\\n\\n### Response:\\n{data[\"output\"]}\"\"\" \n\n    return text\n\ntrain['text'] = train.apply(text_formatting, axis =1)\n```", "```py\ntrain.to_csv('train.csv', index = False)\n```", "```py\n<s>[INST] Instruction [/INST] Model answer</s>[INST] Follow-up instruction [/INST]\n```", "```py\ntrain_chat = train[train['input'] == ''].reset_index(drop = True).copy()\n```", "```py\ndef chat_formatting(data):\n\n  text = f\"<s>[INST] {data['instruction']} [/INST] {data['output']} </s>\"\n\n  return text\n\ntrain_chat['text'] = train_chat.apply(chat_formatting, axis =1)\ntrain_chat.to_csv('train_chat.csv', index =False)\n```", "```py\n!autotrain setup\n```", "```py\nproject_name = 'my_autotrain_llm'\nmodel_name = 'mistralai/Mistral-7B-Instruct-v0.1'\n```", "```py\npush_to_hub = False\nhf_token = \"YOUR HF TOKEN\"\nrepo_id = \"username/repo_name\"\n```", "```py\nlearning_rate = 2e-4\nnum_epochs = 4\nbatch_size = 1\nblock_size = 1024\ntrainer = \"sft\"\nwarmup_ratio = 0.1\nweight_decay = 0.01\ngradient_accumulation = 4\nuse_fp16 = True\nuse_peft = True\nuse_int4 = True\nlora_r = 16\nlora_alpha = 32\nlora_dropout = 0.045\n```", "```py\nimport os\nos.environ[\"PROJECT_NAME\"] = project_name\nos.environ[\"MODEL_NAME\"] = model_name\nos.environ[\"PUSH_TO_HUB\"] = str(push_to_hub)\nos.environ[\"HF_TOKEN\"] = hf_token\nos.environ[\"REPO_ID\"] = repo_id\nos.environ[\"LEARNING_RATE\"] = str(learning_rate)\nos.environ[\"NUM_EPOCHS\"] = str(num_epochs)\nos.environ[\"BATCH_SIZE\"] = str(batch_size)\nos.environ[\"BLOCK_SIZE\"] = str(block_size)\nos.environ[\"WARMUP_RATIO\"] = str(warmup_ratio)\nos.environ[\"WEIGHT_DECAY\"] = str(weight_decay)\nos.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\nos.environ[\"USE_FP16\"] = str(use_fp16)\nos.environ[\"USE_PEFT\"] = str(use_peft)\nos.environ[\"USE_INT4\"] = str(use_int4)\nos.environ[\"LORA_R\"] = str(lora_r)\nos.environ[\"LORA_ALPHA\"] = str(lora_alpha)\nos.environ[\"LORA_DROPOUT\"] = str(lora_dropout)\n```", "```py\n!autotrain llm \\\n--train \\\n--model ${MODEL_NAME} \\\n--project-name ${PROJECT_NAME} \\\n--data-path data/ \\\n--text-column text \\\n--lr ${LEARNING_RATE} \\\n--batch-size ${BATCH_SIZE} \\\n--epochs ${NUM_EPOCHS} \\\n--block-size ${BLOCK_SIZE} \\\n--warmup-ratio ${WARMUP_RATIO} \\\n--lora-r ${LORA_R} \\\n--lora-alpha ${LORA_ALPHA} \\\n--lora-dropout ${LORA_DROPOUT} \\\n--weight-decay ${WEIGHT_DECAY} \\\n--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n$( [[ \"$USE_FP16\" == \"True\" ]] && echo \"--fp16\" ) \\\n$( [[ \"$USE_PEFT\" == \"True\" ]] && echo \"--use-peft\" ) \\\n$( [[ \"$USE_INT4\" == \"True\" ]] && echo \"--use-int4\" ) \\\n$( [[ \"$PUSH_TO_HUB\" == \"True\" ]] && echo \"--push-to-hub --token ${HF_TOKEN} --repo-id ${REPO_ID}\" )\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = \"my_autotrain_llm\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\n```", "```py\ninput_text = \"Give three tips for staying healthy.\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\noutput = model.generate(input_ids, max_new_tokens = 200)\npredicted_text = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(predicted_text)\n```"]