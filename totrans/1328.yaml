- en: Understanding Transformers, the Data Science Way
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/10/understanding-transformers-data-science-way.html](https://www.kdnuggets.com/2020/10/understanding-transformers-data-science-way.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: Transformers have become the defacto standard for any NLP tasks nowadays. Not
    only that, but they are now also being used in Computer Vision and to generate
    music. I am sure you would all have heard about the GPT3 Transformer and its applications
    thereof. ***But all these things aside, they are still hard to understand as ever.***
  prefs: []
  type: TYPE_NORMAL
- en: It has taken me multiple readings through the Google research [paper](https://arxiv.org/pdf/1706.03762.pdf) that
    first introduced transformers along with just so many blog posts to really understand
    how a transformer works.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: So, I thought of putting the whole idea down in as simple words as possible
    along with some very basic Math and some puns as I am a proponent of having some
    fun while learning. I will try to keep both the jargon and the technicality to
    a minimum, yet it is such a topic that I could only do so much. And my goal is
    to make the reader understand even the goriest details of Transformer by the end
    of this post.
  prefs: []
  type: TYPE_NORMAL
- en: '***Also, this is officially my longest post both in terms of time taken to
    write it as well as the length of the post. Hence, I will advise you to Grab A
    Coffee. *☕️**'
  prefs: []
  type: TYPE_NORMAL
- en: So, here goes — This post will be a highly conversational one and it is about
    “***Decoding The Transformer”.***
  prefs: []
  type: TYPE_NORMAL
- en: '***Q: So, Why should I even understand Transformer?***'
  prefs: []
  type: TYPE_NORMAL
- en: In the past, the LSTM and GRU architecture(as explained here in my past [post](https://towardsdatascience.com/nlp-learning-series-part-3-attention-cnn-and-what-not-for-text-classification-4313930ed566) on
    NLP) along with attention mechanism used to be the State of the Art Approach for
    Language modeling problems (put very simply, predict the next word) and Translation
    systems. But, the main problem with these architectures is that they are recurrent
    in nature, and the runtime increases as the sequence length increases. That is,
    these architectures take a sentence and process each word in a ***sequential*** way,
    and hence with the increase in sentence length the whole runtime increases.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer, a model architecture first explained in the paper Attention is
    all you need, lets go of this recurrence and instead relies entirely on an attention
    mechanism to draw global dependencies between input and output. And that makes
    it FAST.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/db28f3c80be7e025d0e8b78e557c3e29.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Source](https://arxiv.org/pdf/1706.03762.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: This is the picture of the full transformer as taken from the paper. And, it
    surely is intimidating. So, I will aim to demystify it in this post by going through
    each individual piece. So read ahead.
  prefs: []
  type: TYPE_NORMAL
- en: The Big Picture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '***Q: That sounds interesting. So, what does a transformer do exactly?***'
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, a transformer can perform almost any NLP task. It can be used for
    language modeling, Translation, or Classification as required, and it does it
    fast by removing the sequential nature of the problem. So, the transformer in
    a machine translation application would convert one language to another, or for
    a classification problem will provide the class probability using an appropriate
    output layer.
  prefs: []
  type: TYPE_NORMAL
- en: It all will depend on the final outputs layer for the network but, the Transformer
    basic structure will remain quite the same for any task. For this particular post,
    I will be continuing with the machine translation example.
  prefs: []
  type: TYPE_NORMAL
- en: So from a very high place, this is how the transformer looks for a translation
    task. It takes as input an English sentence and returns a German sentence.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/602af3cd3df7d3c26732da85179370a0.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformer for Translation (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: The Building Blocks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '***Q: That was too basic. *????* Can you expand on it?***'
  prefs: []
  type: TYPE_NORMAL
- en: Okay, just remember in the end, you asked for it. Let’s go a little deeper and
    try to understand what a transformer is composed of.
  prefs: []
  type: TYPE_NORMAL
- en: So, a transformer is essentially composed of a stack of encoder and decoder
    layers. The role of an encoder layer is to encode the English sentence into a
    numerical form using the attention mechanism, while the decoder aims to use the
    encoded information from the encoder layers to give the German translation for
    the particular English sentence.
  prefs: []
  type: TYPE_NORMAL
- en: In the figure below, the transformer is given as input an English sentence,
    which gets encoded using 6 encoder layers. The output from the final encoder layer
    then goes to each decoder layer to translate English to German.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/da94e3e12969ca0c086329ad99947d0c.png)'
  prefs: []
  type: TYPE_IMG
- en: Data Flow in a Transformer (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Encoder Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '***Q: That’s alright but, how does an encoder stack encode an English sentence
    exactly?***'
  prefs: []
  type: TYPE_NORMAL
- en: 'Patience, I am getting to it. So, as I said the encoder stack contains six
    encoder layers on top of each other(As given in the paper, but the future versions
    of transformers use even more layers). And each encoder in the stack has essentially
    two main layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**a multi-head self-attention Layer, and**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**a position-wise fully connected feed-forward network**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure](../Images/aa577079b526eb4d3bd8af2104b52238.png)'
  prefs: []
  type: TYPE_IMG
- en: Very basic encoder Layer (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: They are a mouthful. Right? Don’t lose me yet as I will explain both of them
    in the coming sections. Right now, just remember that the encoder layer incorporates
    attention and a position-wise feed-forward network.
  prefs: []
  type: TYPE_NORMAL
- en: '***Q: But, how does this layer expect its inputs to be?***'
  prefs: []
  type: TYPE_NORMAL
- en: This layer expects its inputs to be of the shape `SxD` (as shown in the figure
    below) where `S` is the source sentence(English Sentence) length, and `D` is the
    dimension of the embedding whose weights can be trained with the network. In this
    post, we will be using `D` as 512 by default throughout. While S will be the maximum
    length of sentence in a batch. So it normally changes with batches.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/67d84024cccc6a5321f4f1a998bf653d.png)'
  prefs: []
  type: TYPE_IMG
- en: Encoder — Input and Output shapes are the same (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: And what about the outputs of this layer? Remember that the encoder layers are
    stacked on top of each other. So, we want to be able to have an output of the
    same dimension as the input so that the output can flow easily into the next encoder.
    So the output is also of the shape, `SxD`.
  prefs: []
  type: TYPE_NORMAL
- en: '***Q: Enough about the sizes talk, I understand what goes in and what goes
    out but what actually happens in the Encoder layer?***'
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, let’s go through the attention layer and the feedforward layer one by
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: A) Self-attention layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Figure](../Images/050821f689542c5fc7f16ef3fff7df31.png)'
  prefs: []
  type: TYPE_IMG
- en: How Self-Attention Works (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: The above figure must look daunting but it is easy to understand. So just stay
    with me here.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning is essentially nothing but a lot of matrix calculations and what
    we are essentially doing in this layer is a lot of matrix calculations intelligently.
    The self-attention layer initializes with 3 weight matrices — Query(W_q), Key(W_k),
    and Value(W_v). Each of these matrices has a size of (`Dxd`) where d is taken
    as 64 in the paper. The weights for these matrices will be trained when we train
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: In the first calculation(Calc 1 in the figure), we create matrices Q, K, and
    V by multiplying the input with the respective Query, Key, and Value matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Till now it is trivial and shouldn’t make any sense, but it is at the second
    calculation where it gets interesting. Let’s try to understand the output of the
    softmax function. We start by multiplying the Q and Kᵀ matrix to get a matrix
    of size (`SxS`) and divide it by the scalar √d. We then take a softmax to make
    the rows sum to one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Intuitively, we can think of the resultant `SxS` matrix as the contribution
    of each word in another word. For example, it might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/e436dcb4757a035ca23b1cec9b93480f.png)'
  prefs: []
  type: TYPE_IMG
- en: Softmax(QxKt/sqrt(d)) (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: As you can see the diagonal entries are big. This is because the word contribution
    to itself is high. That is reasonable. But we can see here that the word “quick”
    devolves into “quick” and “fox” and the word “brown” also devolves into “brown”
    and “fox”. That intuitively helps us to say that both the words — “quick” and
    “brown” each refers to the “fox”.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have this SxS matrix with contributions we multiply this matrix by the
    Value matrix(Sxd) of the sentence and it gives us back a matrix of shape Sxd(4x64).
    So, what the operation actually does is that it replaces the embedding vector
    of a word like “quick” with say .75 x (quick embedding) and .2x(fox embedding)
    and thus now the resultant output for the word “quick” has attention embedded
    in itself.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the output of this layer has the dimension (Sxd) and before we get
    done with the whole encoder we need to change it back to D=512 as we need the
    output of this encoder as the input of another encoder.
  prefs: []
  type: TYPE_NORMAL
- en: '***Q: But, you called this layer Multi-head self-attention Layer. What is the
    multi-head?***'
  prefs: []
  type: TYPE_NORMAL
- en: Okay, my bad but in my defense, I was just getting to that.
  prefs: []
  type: TYPE_NORMAL
- en: It’s called a multi-head because we use many such self-attention layers in parallel.
    That is, we have many self-attention layers stacked on top of each other. The
    number of attention layers,h, is kept as 8 in the paper. So the input X goes through
    many self-attention layers parallelly, each of which gives a z matrix of shape
    (Sxd) = 4x64\. We concatenate these 8(h) matrices and again apply a final output
    linear layer, Wo, of size DxD.
  prefs: []
  type: TYPE_NORMAL
- en: What size do we get? For the concatenate operation we get a size of SxD(4x(64x8)
    = 4x512). And multiplying this output by Wo, we get the final output Z with the
    shape of SxD(4x512) as desired.
  prefs: []
  type: TYPE_NORMAL
- en: Also, note the relation between h,d, and D i.e. h x d = D
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/672d4f0db3576685d7bb30479d6be1c1.png)'
  prefs: []
  type: TYPE_IMG
- en: The Full multi-headed self-attention Layer (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we finally get the output Z of shape 4x512 as intended. But before it
    goes into another encoder we pass it through a Feed-Forward Network.
  prefs: []
  type: TYPE_NORMAL
- en: B) Position-wise feed-forward network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we understand the multi-headed attention layer, the Feed-forward network
    is actually pretty easy to understand. It is just a combination of various linear
    and dropout layers on the output Z. Consequentially, it is again just a lot of
    Matrix multiplication here.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/e44a29a2a8c8f571e7e60e56dbe388f9.png)'
  prefs: []
  type: TYPE_IMG
- en: Each word goes into the feed-forward network. (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: The feed-forward network applies itself to each position in the output Z parallelly(Each
    position can be thought of as a word) and hence the name Position-wise feed-forward
    network. The feed-forward network also shares weight, so that the length of the
    source sentence doesn’t matter(Also, if it didn’t share weights, we would have
    to initialize a lot of such networks based on max source sentence length and that
    is not feasible)
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/fa0d92e074d6fa580e16e1adeaf4efd3.png)'
  prefs: []
  type: TYPE_IMG
- en: It is actually just a linear layer that gets applied to each position(or word)
    (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: With this, we near an okayish understanding of the encoder part of the Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: '***Q: Hey, I was just going through the picture in the paper, and the encoder
    stack has something called “positional encoding” and “Add & Norm” also. What are
    these?***'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/4d5a56f76e2039776d53e46aaf96f144.png)'
  prefs: []
  type: TYPE_IMG
- en: I am back again here so you don’t have to scroll [Source](https://arxiv.org/pdf/1706.03762.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: Okay, These two concepts are pretty essential to this particular architecture.
    And I am glad you asked this one. So, we will discuss these steps before moving
    further to the decoder stack.
  prefs: []
  type: TYPE_NORMAL
- en: C. Positional Encodings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since, our model contains no recurrence and no convolution, in order for the
    model to make use of the order of the sequence, we must inject some information
    about the relative or absolute position of the tokens in the sequence. To this
    end, we add “positional encodings” to the input embeddings at the bottoms of both
    the encoder and decoder stacks(as we will see later). The positional encodings
    need to have the same dimension, D as the embeddings have so that the two can
    be summed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/670092d703424a38695f25763b982df5.png)'
  prefs: []
  type: TYPE_IMG
- en: Add a static positional pattern to X (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: In the paper, the authors used sine and cosine functions to create positional
    embeddings for different positions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/99d8631c9f4f9db9651e38562108c935.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Source](https://arxiv.org/pdf/1706.03762.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: This particular mathematical thing actually generates a 2d matrix which is added
    to the embedding vector that goes into the first encoder step.
  prefs: []
  type: TYPE_NORMAL
- en: Put simply, it’s just a constant matrix that we add to the sentence so that
    the network could get the position of the word.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/fd7c6d99a74914d576445b02b17d66c4.png)  ![Figure](../Images/09a8aaa978d699dcbfb0b8a8cee7c653.png)'
  prefs: []
  type: TYPE_IMG
- en: Positional encoding matrix for the first 300 and 3000 positions (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: Above is the heatmap of the position encoding matrix that we will add to the
    input that is to be given to the first encoder. I am showing the heatmap for the
    first 300 positions and the first 3000 positions. We can see that there is a distinct
    pattern that we provide to our Transformer to understand the position of each
    word. And since we are using a function comprised of sin and cos, we are able
    to embed positional embeddings for very high positions also pretty well as we
    can see in the second picture.
  prefs: []
  type: TYPE_NORMAL
- en: '**Interesting Fact:** The authors also let the Transformer learn these encodings
    too and didn’t see any difference in performance as such. So, they went with the
    above idea as it doesn’t depend on sentence length and so even if the test sentence
    is bigger than train samples, we would be fine.'
  prefs: []
  type: TYPE_NORMAL
- en: D. Add and Normalize
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another thing, that I didn’t mention for the sake of simplicity while explaining
    the encoder is that the encoder(the decoder architecture too) architecture has
    skip level residual connections(something akin to resnet50) also. So, the exact
    encoder architecture in the paper looks like below. Simply put, it helps traverse
    information for a much greater length in a Deep Neural Network. This can be thought
    of as akin(intuitively) to information passing in an organization where you have
    access to your manager as well as to your manager’s manager.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/ea9ebb9891e4733fed5eea39f4f7f409.png)'
  prefs: []
  type: TYPE_IMG
- en: The Skip level connections help information flow in the network (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Decoder Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '***Q: Okay, so till now we have learned that an encoder takes an input sentence
    and encodes its information in a matrix of size SxD(4x512). That’s all great but
    how does it help the decoder decode it to German?***'
  prefs: []
  type: TYPE_NORMAL
- en: Good things come to those who wait. So, before understanding how the decoder
    does that, let us understand the decoder stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'The decoder stack contains 6 decoder layers in a stack (As given in the paper
    again) and each decoder in the stack is comprised of these main three layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Masked multi-head self-attention Layer**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**multi-head self-attention Layer, and**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**a position-wise fully connected feed-forward network**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It also has the same positional encoding as well as the skip level connection
    as well. We already know how the multi-head attention and feed-forward network
    layers work, so we will get straight into what is different in the decoder as
    compared to the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/4708687e5efa6ec096a3a00419857a16.png)'
  prefs: []
  type: TYPE_IMG
- en: Decoder Architecture (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: '***Q: Wait, but do I see the output we need flowing into the decoder as input?
    What? Why? *????**'
  prefs: []
  type: TYPE_NORMAL
- en: I am noticing that you are getting pretty good at asking questions. And that
    is a great question, something I even though myself a lot of times, and something
    that I hope will get much clearer by the time you reach the end of this post.
  prefs: []
  type: TYPE_NORMAL
- en: But to give an intuition, we can think of a transformer as a conditional language
    model in this case. A model that predicts the next word given an input word and
    an English sentence on which to condition upon or base its prediction on.
  prefs: []
  type: TYPE_NORMAL
- en: Such models are inherently sequential as in how would you train such a model?
    You start by giving the start token(`<s>`) and the model predicts the first word
    conditioned on the English sentence. You change the weights based on if the prediction
    is right or wrong. Then you give the start token and the first word (`<s> der`)
    and the model predicts the second word. You change weights again. And so on.
  prefs: []
  type: TYPE_NORMAL
- en: The transformer decoder learns just like that but the beauty is that it doesn’t
    do that in a sequential manner. It uses masking to do this calculation and thus
    takes the whole output sentence (although shifted right by adding a `<s>` token
    to the front) while training. Also, please note that at prediction time we won’t
    give the output to the network
  prefs: []
  type: TYPE_NORMAL
- en: '***Q: But, how does this masking exactly work?***'
  prefs: []
  type: TYPE_NORMAL
- en: A) Masked Multi-Head Self Attention Layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It works, as usual, you wear it I mean **????**. Kidding aside, as you can see
    that this time we have a **Masked** Multi-Head attention Layer in our decoder.
    This means that we will mask our shifted output (that is the input to the decoder)
    in a way that the network is never able to see the subsequent words since otherwise,
    it can easily copy that word while training.
  prefs: []
  type: TYPE_NORMAL
- en: So, how does the mask exactly work in the masked attention layer? If you remember,
    in the attention layer we multiplied the query(Q) and keys(K) and divided them
    by sqrt(d) before taking the softmax.
  prefs: []
  type: TYPE_NORMAL
- en: In a masked attention layer, though, we add the resultant matrix before the
    softmax(which will be of shape (TxT)) to a masking matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, In a masked layer, the function changes from:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/2c0c26d49103a53b6aa4ace6762916a6.png)'
  prefs: []
  type: TYPE_IMG
- en: (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: '***Q: I still don’t get it, what happens if we do that?***'
  prefs: []
  type: TYPE_NORMAL
- en: That’s understandable actually. Let me break it in steps. So, our resultant
    matrix(QxK/sqrt(d)) of shape (TxT) might look something like below:(The numbers
    can be big as softmax not applied yet)
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/31cc52fb62bd798ed2455178208d4d5b.png)'
  prefs: []
  type: TYPE_IMG
- en: Schnelle currently attends to both Braune and Fuchs (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: 'The word Schnelle will now be composed of both Braune and Fuchs if we take
    the above matrix’s softmax and multiply it with the value matrix V. But we don’t
    want that, so we add the mask matrix to it to give:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/91883c8093be6333785af3424524d184.png)'
  prefs: []
  type: TYPE_IMG
- en: The mask operation applied to the matrix. (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: And, now what will happen after we do the softmax step?
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/94e612fc5e9c04025c415c5d6f8ed70b.png)'
  prefs: []
  type: TYPE_IMG
- en: Schnelle never attends to any word after Schnelle. (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: Since e^{-inf} = 0, all positions subsequent to Schnelle have been converted
    to 0\. Now, if we multiply this matrix with the value matrix V, the vector corresponding
    to Schnelle’s position in the Z vector passing through the decoder would not contain
    any information of the subsequent words Braune and Fuchs just like we wanted.
  prefs: []
  type: TYPE_NORMAL
- en: And that is how the transformer takes the whole shifted output sentence at once
    and doesn’t learn in a sequential manner. Pretty neat I must say.
  prefs: []
  type: TYPE_NORMAL
- en: '***Q: Are you kidding me? That’s actually awesome.***'
  prefs: []
  type: TYPE_NORMAL
- en: 'So glad that you are still with me and you appreciate it. Now, coming back
    to the decoder. The next layer in the decoder is:'
  prefs: []
  type: TYPE_NORMAL
- en: B) Multi-Headed Attention Layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you can see in the decoder architecture, a Z vector(Output of encoder) flows
    from the encoder to the multi-head attention layer in the Decoder. This Z output
    from the last encoder has a special name and is often called as memory. The attention
    layer takes as input both the encoder output and data flowing from below(shifted
    outputs) and uses attention. The Query vector Q is created from the data flowing
    in the decoder, while the Key(K) and value(V) vectors come from the encoder output.
  prefs: []
  type: TYPE_NORMAL
- en: '***Q: Isn’t there any mask here?***'
  prefs: []
  type: TYPE_NORMAL
- en: No, there is no mask here. The output coming from below is already masked and
    this allows every position in the decoder to attend over all the positions in
    the Value vector. So for every word position to be generated the decoder has access
    to the whole English sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a single attention layer(which will be part of a multi-head just like
    before):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/837c4d11841186b66d667a66cf1a207c.png)'
  prefs: []
  type: TYPE_IMG
- en: (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: '***Q: But won’t the shapes of Q, K, and V be different this time?***'
  prefs: []
  type: TYPE_NORMAL
- en: You can look at the figure where I have done all the weights calculation. I
    would also ask you to see the shapes of the resultant Z vector and how our weight
    matrices until now never used the target or source sentence length in any of their
    dimensions. Normally, the shape cancels away in all our matrix calculations. For
    example, see how the S dimension cancels away in calculation 2 above. That is
    why while selecting the batches during training the authors talk about tight batches.
    That is in a batch all source sentences have similar lengths. And different batches
    could have different source lengths.
  prefs: []
  type: TYPE_NORMAL
- en: I will now talk about the skip level connections and the feed-forward layer.
    They are actually the same as in ….
  prefs: []
  type: TYPE_NORMAL
- en: '***Q: Ok, I get it. We have the skip level connections and the FF layer and
    get a matrix of shape TxD after this whole decode operation.* *But where is the
    German translation?***'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Output Head
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are actually very much there now friend. Once, we are done with the transformer,
    the next thing is to add a task-specific output head on the top of the decoder
    output. This can be done by adding some linear layers and softmax on top to get
    the probability *across all the words in the german vocab*. We can do something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/358e58c4af26262b19003c68d0e5eeb7.png)'
  prefs: []
  type: TYPE_IMG
- en: (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: As you can see we are able to generate probabilities. So far we know how to
    do a forward pass through this Transformer architecture. Let us see how we do
    the training of such a Neural Net Architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Till now, if we take a bird-eye view of the structure we have something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/c3b005a70e775af0c889dc8574617c98.png)'
  prefs: []
  type: TYPE_IMG
- en: (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: We can give an English sentence and shifted output sentence and do a forward
    pass and get the probabilities over the German vocabulary. And thus we should
    be able to use a loss function like cross-entropy where the target could be the
    German word we want, and train the neural network using the Adam Optimizer. Just
    like any classification example. So, there is your German.
  prefs: []
  type: TYPE_NORMAL
- en: In the paper though, the authors use slight variations of optimizers and loss.
    You can choose to skip the below 2 sections on KL Divergence Loss and Learning
    rate schedule with Adam if you want as it is done only to churn out more performance
    out of the model and not an inherent part of the Transformer architecture as such.
  prefs: []
  type: TYPE_NORMAL
- en: '***Q: I have been here for such a long time and have I complained? *????**'
  prefs: []
  type: TYPE_NORMAL
- en: Okay. Okay. I get you. Let’s do it then.
  prefs: []
  type: TYPE_NORMAL
- en: 'A) KL Divergence with Label Smoothing:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: KL Divergence is the information loss that happens when the distribution P is
    approximated by the distribution Q. When we use the KL Divergence loss, we try
    to estimate the target distribution(P) using the probabilities(Q) we generate
    from the model. And we try to minimize this information loss in the training.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/e5da050d39e1b35cbc1eb71c3867eca8.png)'
  prefs: []
  type: TYPE_IMG
- en: (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: If you notice, in this form(without label smoothing which we will discuss) this
    is exactly the same as cross-entropy. Given two distributions like below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/961e1bb98f0b8d448f803a9170f452e8.png)  ![Figure](../Images/0e17ece021206f2109d722f8297c5eb8.png)'
  prefs: []
  type: TYPE_IMG
- en: Target distribution and probability distribution for a word(token) (*Image by
    author*)
  prefs: []
  type: TYPE_NORMAL
- en: The KL Divergence formula just plain gives `-logq(oder)` and that is the cross-entropy
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: In the paper, though the authors used label smoothing with α = 0.1 and so the
    KL Divergence loss is not cross-entropy. What that means is that in the target
    distribution the output value is substituted by (1-α) and the remaining 0.1 is
    distributed across all the words. The authors say that this is so that the model
    is not too confident.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/7a6ac740fd128a326672e21bc2ec0a02.png)  ![Figure](../Images/0e17ece021206f2109d722f8297c5eb8.png)'
  prefs: []
  type: TYPE_IMG
- en: (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: '***Q: But, why do we make our models not confident? It seems absurd.***'
  prefs: []
  type: TYPE_NORMAL
- en: Yes, it does but intuitively, you can think of it as when we give the target
    as 1 to our loss function, we have no doubts that the true label is True and others
    are not. But vocabulary is inherently a non-standardized target. For example,
    who is to say that you cannot use good in place of great? So we add some confusion
    in our labels so our model is not too rigid.
  prefs: []
  type: TYPE_NORMAL
- en: B) A particular Learning Rate schedule with Adam
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The authors use a learning rate scheduler to increase the learning rate until
    warmup steps and then decrease it using the below function. And they used the
    Adam optimizer with β¹ = 0.9, β² = 0.98\. Nothing too interesting here just some
    learning choices.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/286b59360c1cc1432ba1ac54ceb659ab.png)'
  prefs: []
  type: TYPE_IMG
- en: Source: [Paper](https://arxiv.org/pdf/1706.03762.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: '***Q: But wait I just remembered that we won’t have the shifted output at the
    prediction time, would we? How do we do predictions then?***'
  prefs: []
  type: TYPE_NORMAL
- en: If you realize what we have at this point is a generative model and we will
    have to do the predictions in a generative way as we won’t know the output target
    vector when doing prediction. So predictions are still sequential.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction Time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Figure](../Images/c4feb01de609231103f61961d18ef7d4.png)'
  prefs: []
  type: TYPE_IMG
- en: Predicting with a greedy search using the Transformer (*Image by author*)
  prefs: []
  type: TYPE_NORMAL
- en: 'This model does piece-wise predictions. In the original paper, they use the
    Beam Search to do prediction. But a greedy search would work fine as well for
    the purpose of explaining it. In the above example, I have shown how a greedy
    search would work exactly. The greedy search would start with:'
  prefs: []
  type: TYPE_NORMAL
- en: Passing the whole English sentence as encoder input and just the start token `<st> `as
    shifted output(input to the decoder) to the model and doing the forward pass.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model will predict the next word — `der`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we pass the whole English sentence as encoder input and add the last predicted
    word to the shifted output(input to the decoder = `<st> der`) and do the forward
    pass.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model will predict the next word — `schnelle`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Passing the whole English sentence as encoder input and `<st> der schnelle` as
    shifted output(input to the decoder) to the model and doing the forward pass.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and so on, until the model predicts the end token `</s>` or we generate some
    maximum number of tokens(something we can define) so the translation doesn’t run
    for an infinite duration in any case it breaks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Beam Search:**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '***Q: Now I am greedy, Tell me about beam search as well.***'
  prefs: []
  type: TYPE_NORMAL
- en: Okay, the beam search idea is inherently very similar to the above idea. In
    beam search, we don’t just look at the highest probability word generated but
    the top two words.
  prefs: []
  type: TYPE_NORMAL
- en: So, For example, when we gave the whole English sentence as encoder input and
    just the start token as shifted output, we get two best words as `i`(p=0.6) and `der`(p=0.3).
    We will now generate the output model for both output sequences,`<s> i` and `<s>
    der` and look at the probability of the next top word generated. For example,
    if `<s> i` gave a probability of (p=0.05) for the next word and `<s> der>` gave
    (p=0.5) for the next predicted word, we discard the sequence `<s> i`and go with `<s>
    der` instead, as the sum of probability of sentence is maximized(`<s> der next_word_to_der` p
    = 0.3+0.5 compared to `<s> i next_word_to_i` p = 0.6+0.05). We then repeat this
    process to get the sentence with the highest probability.
  prefs: []
  type: TYPE_NORMAL
- en: Since we used the top 2 words, the beam size is 2 for this Beam Search. In the
    paper, they used beam search of size 4.
  prefs: []
  type: TYPE_NORMAL
- en: '**PS**: I showed that the English sentence is passed at every step for brevity,
    but in practice, the output of the encoder is saved and only the shifted output
    passes through the decoder at each time step.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Q: Anything else you forgot to tell me? I will let you have your moment.***'
  prefs: []
  type: TYPE_NORMAL
- en: 'Yes. Since you asked. Here it is:'
  prefs: []
  type: TYPE_NORMAL
- en: BPE, Weight Sharing and Checkpointing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the paper, the authors used Byte pair encoding to create a common English
    German vocabulary. They then used shared weights across both the English and german
    embedding and pre-softmax linear transformation as the embedding weight matrix
    shape would work (Vocab Length X D).
  prefs: []
  type: TYPE_NORMAL
- en: Also, the authors average the last k checkpoints to create an ensembling effect
    to reach the performance*.* This is a pretty known technique where we average
    the weights in the last few epochs of the model to create a new model which is
    sort of an ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: '***Q: Can you show me some code?***'
  prefs: []
  type: TYPE_NORMAL
- en: This post has already been so long, so I will do that in the next post. Stay
    tuned.
  prefs: []
  type: TYPE_NORMAL
- en: '***Now, finally, my turn to ask the question: Did you get how a transformer
    works? Yes, or No, you can answer in the comments. :)***'
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Attention Is All You Need](https://arxiv.org/abs/1706.03762): The Paper which
    started it all.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html):
    This one has all the code. Although I will write a simple transformer in the next
    post too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/):
    This is one of the best posts on transformers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this post, I covered how the Transformer architecture works from a detail-oriented,
    intuitive perspective.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about NLP, I would like to call out an excellent course
    on [**Natural Language Processing**](https://click.linksynergy.com/link?id=lVarvwc5BD0&offerid=467035.11503135394&type=2&murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing) from
    the Advanced Machine Learning Specialization. Do check it out.
  prefs: []
  type: TYPE_NORMAL
- en: I am going to be writing more of such posts in the future too. Let me know what
    you think about them. Should I write on heavily technical topics or more beginner
    level articles? The comment section is your friend. Use it. Also, follow me up
    at [**Medium**](https://medium.com/@rahul_agarwal) or Subscribe to my [**blog**](https://mlwhiz.ck.page/a9b8bda70c).
  prefs: []
  type: TYPE_NORMAL
- en: And, finally a small disclaimer — There might be some affiliate links in this
    post to relevant resources, as sharing knowledge is never a bad idea.
  prefs: []
  type: TYPE_NORMAL
- en: This story was first published [**here**](https://lionbridge.ai/articles/what-are-transformer-models-in-machine-learning/).
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Rahul Agarwal](https://www.linkedin.com/in/rahulagwl/)** is Senior
    Statistical Analyst at WalmartLabs. Follow him on Twitter [@mlwhiz](https://twitter.com/MLWhiz).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/understanding-transformers-the-data-science-way-e4670a4ee076).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[A Deep Dive Into the Transformer Architecture – The Development of Transformer
    Models](/2020/08/transformer-architecture-development-transformer-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Hitchhiker’s Guide to Feature Extraction](/2019/06/hitchhikers-guide-feature-extraction.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Learning’s Most Important Ideas](/2020/09/deep-learnings-most-important-ideas.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[A Faster Way to Prepare Time-Series Data with the AI & Analytics Engine](https://www.kdnuggets.com/2021/12/piexchange-faster-way-prepare-timeseries-data-ai-analytics-engine.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Graphs: The natural way to understand data](https://www.kdnuggets.com/2022/10/manning-graphs-natural-way-understand-data.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A new book that will revolutionize the way your organization…](https://www.kdnuggets.com/2022/02/manning-new-book-revolutionize-way-organization-approaches-data.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Optimal Way to Input Missing Data with Pandas fillna()](https://www.kdnuggets.com/2023/02/optimal-way-input-missing-data-pandas-fillna.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Easiest Way to Make Beautiful Interactive Visualizations With Pandas](https://www.kdnuggets.com/2021/12/easiest-way-make-beautiful-interactive-visualizations-pandas.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A New Way of Managing Deep Learning Datasets](https://www.kdnuggets.com/2022/03/new-way-managing-deep-learning-datasets.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
