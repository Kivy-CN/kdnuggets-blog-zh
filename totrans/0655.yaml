- en: Research Guide for Video Frame Interpolation with Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/10/research-guide-video-frame-interpolation-deep-learning.html](https://www.kdnuggets.com/2019/10/research-guide-video-frame-interpolation-deep-learning.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: In this research guide, we’ll look at deep learning papers aimed at synthesizing
    video frames within an existing video. This could be in between video frames,
    known as *interpolation,* or after them, known as *extrapolation*.
  prefs: []
  type: TYPE_NORMAL
- en: The better part of this guide will cover *interpolation. Interpolation *is useful
    in software editing tools as well as in generating video animations. It can also
    be used to generate clear video frames in sections where a video is blurred.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Video frame interpolation is a very common task, especially in film and video
    production. [Optical flow](https://en.wikipedia.org/wiki/Optical_flow) is one
    of the common tactics used in solving this problem. Optical Flow Estimation is
    the process of estimating the motion of each pixel in a sequence of frames. In
    this paper, we’ll look at advanced methods of video frame interpolation using
    deep learning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Video Frame Interpolation via Adaptive Separable Convolution (ICCV, 2017)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this paper, the authors propose a deep fully convolutional neural network
    that’s fed with two input frames and estimates pairs of 1D kernels for all pixels.
    The method is capable of estimating kernels and synthesizing the entire video
    frame at once. This makes it possible to incorporate perceptual loss to train
    the neural network, in order to produce visually appealing frames.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Video Frame Interpolation via Adaptive Separable Convolution**](https://arxiv.org/abs/1708.01692v1?source=post_page-----519ab2eb3dda----------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Standard video frame interpolation methods first estimate optical flow between
    input frames and then synthesize an…
  prefs: []
  type: TYPE_NORMAL
- en: The paper introduces a spatially-adaptive separable convolution technique, which
    aims to interpolate a new frame in the middle of two video frames. The convolution-based
    interpolation method then estimates a pair of 2D convolution kernels. This is
    then used to convolve the two video frames in order to compute the color of the
    output pixel.
  prefs: []
  type: TYPE_NORMAL
- en: The pixel-dependent kernels capture both motion and re-sampling information
    that’s required for interpolation. Four sets of 1D kernels are estimated by directing
    the information flow into four sub-networks. Each of the subnetworks estimates
    one kernel. The Rectified Linear Unit is used with the 3x3 convolutional layers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/295c861179ce968d140212eef910baae.png)'
  prefs: []
  type: TYPE_IMG
- en: '![Figure](../Images/8d5f28e5e6fbe6460428e07315727a06.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1708.01692v1)'
  prefs: []
  type: TYPE_NORMAL
- en: The network was trained using the [AdaMax](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adamax) optimizer
    with a learning rate of 0.001 and a mini-batch size of 16 samples. Training videos
    were obtained from various YouTube channels such as “Tom Scott”, “Casey Neistat”,
    “Linus Tech Tips”, and “Austin Evans”.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation was performed by random cropping to ensure that the network
    isn’t biased. Implementation of the convolutional neural network was done using
    Torch. Here’s how this model performs in comparison to other models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c5358a2c0978c33ee007d811933bc3a.png)'
  prefs: []
  type: TYPE_IMG
- en: '![Figure](../Images/b22c44bbc772ca60d09fbda679fec8ef.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1708.01692v1)'
  prefs: []
  type: TYPE_NORMAL
- en: Video Frame Interpolation via Adaptive Convolution (CVPR 2017)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This paper presents a method that combines motion estimation and pixel synthesis
    into a single process for video frame interpolation. A deep fully convolutional
    neural network is implemented to estimate a spatially-adaptive convolution kernel
    for each pixel.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Video Frame Interpolation via Adaptive Convolution**](https://arxiv.org/abs/1703.07514?source=post_page-----519ab2eb3dda----------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Video frame interpolation typically involves two steps: motion estimation and
    pixel synthesis. Such a two-step approach…'
  prefs: []
  type: TYPE_NORMAL
- en: For a pixel in the interpolated frame, the deep neural network takes two receptive
    field patches centered at that pixel as input and estimates the convolution kernel.
    The convolution kernel is used to convolve with the input patches to synthesize
    the output pixel. Given two video frames, this model aims at creating a temporarily
    frame in between them.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/d6d09265a926ee228a03cc59ff08bb29.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1703.07514)'
  prefs: []
  type: TYPE_NORMAL
- en: This method estimates a convolution kernel directly and uses that to convolve
    the two frames to interpolate the pixel color. Pixel synthesis is accomplished
    by the convolution kernel capturing motion and re-sampling coefficients. Pixel
    interpolation as convolution enables pixel synthesis to be done in a single step,
    which makes this approach more robust.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91d8525c12b2988ec531a569df401c4a.png)'
  prefs: []
  type: TYPE_IMG
- en: The convolutional neural network is made up of several convolutional layers
    and down convolutions as alternatives to max-pooling layers. For regularization,
    the authors use ReLUs as activations and batch normalization. The table below
    is an illustration of the architecture of this network.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/0b569373cf0ef890b74b03606d048b1e.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1703.07514)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The model is implemented using Torch. Here’s the performance of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/640639e4d8c9fee3c8f70459a0d13edd.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1703.07514)'
  prefs: []
  type: TYPE_NORMAL
- en: Video processing techniques like interpolation make many computer vision applications
    possible — not just on servers and in the cloud, but on mobile devices, too. [Learn
    more about how Fritz can teach your mobile devices to see.](https://www.fritz.ai/features/?utm_campaign=vidinterp&utm_source=heartbeat)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Video Frame Synthesis using Deep Voxel Flow (ICCV 2017)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The authors of this paper propose a deep neural network that learns to synthesize
    video frames by flowing pixel values from existing ones. This paper combines the
    strengths of generative convolutional neural networks and optical flow to solve
    this problem.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Video Frame Synthesis using Deep Voxel Flow**](https://arxiv.org/abs/1702.02463?source=post_page-----519ab2eb3dda----------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: We address the problem of synthesizing new video frames in an existing video,
    either in-between existing frames…
  prefs: []
  type: TYPE_NORMAL
- en: The network used in this model is trained in an unsupervised fashion. Pixels
    are generated by interpolating pixel values from frames that are close by. This
    network includes a voxel flow layer across space and time in the input video.
    Trilinear interpolation across the input video volume generates the final pixel.
    The network is trained on the [UCF-101 dataset](https://www.crcv.ucf.edu/data/UCF101.php) and
    tested on various videos.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/0117bd62895e4943c1c6707632e19b4e.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1702.02463.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Their proposed model, Deep Voxel Flow (DVF), is an end-to-end, fully differentiable
    network for video frame synthesis. DVF adopts a fully-convolutional encoder-decoder
    architecture, containing three convolution layers, three deconvolution layers,
    and one bottleneck layer. In the training process of this model, two frames are
    provided as input and the remaining frame is used as a reconstruction target.
    The method is self-supervised and learns to reconstruct a frame by borrowing voxels
    from frames that are nearby. This leads to results that are sharper and more realistic.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/727ec9aeb8875e6d17bcfc7c3e5dade6.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1702.02463.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The authors use Peak Signal to Noise Ratio (PSN) and Structural Similarity Index
    (SSIM) for analyzing the quality of the interpolated image. Below are the results
    they achieved.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/21c8da83a9178796d4a0e14a32da8a2e.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1702.02463.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Long-Term Video Interpolation with Bidirectional Predictive Network (2017)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This paper addresses the challenge of generating multiple frames between two
    non-consecutive frames in videos. The authors present a deep bidirectional predictive
    network (BiPN) that predicts intermediate frames from two opposite directions.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Long-Term Video Interpolation with Bidirectional Predictive Network**](https://arxiv.org/abs/1706.03947?source=post_page-----519ab2eb3dda----------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: This paper considers the challenging task of long-term video interpolation.
    Unlike most existing methods that only…
  prefs: []
  type: TYPE_NORMAL
- en: The authors train a convolutional encoder-decoder network given two nonconsecutive
    frames. The network is trained to regress the missing intermediate frames from
    two opposite directions. The network consists of a bi-directional encoder-decoder
    that predicts the *future-forward* from the start frame and predicts the *past-backward* from
    the end frame all at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: The model is evaluated on a synthetic dataset Moving 2D Shapes and a natural
    video dataset UCF101.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/5549fd68b4f69e76c54c16733f4e9965.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1706.03947)'
  prefs: []
  type: TYPE_NORMAL
- en: The BiPN architecture is an encoder-decoder pipeline with a bidirectional encoder
    and a single decoder. A latent frame representation is produced by the bidirectional
    encoder through encoding information from the start frame and end frame.
  prefs: []
  type: TYPE_NORMAL
- en: The multiple missing frames are predicted by the decoder after taking the feature
    representations as input. The forward and reverse encoders consist of several
    convolutional layers, each with a rectified linear unit (ReLU).
  prefs: []
  type: TYPE_NORMAL
- en: The decoder is composed of a series of up-convolutional layers and ReLUs. The
    decoder outputs a feature map with the size of *l ×h×w ×c *as the prediction of
    the target in-between frames, where l is the length of frames to be predicted,
    h, w and c are the height, width and the number of channels for each frame, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c3b195c4fb4f59987af4bc2208756ad3.png)'
  prefs: []
  type: TYPE_IMG
- en: '![Figure](../Images/6b636709e32911d866dae097e5855c09.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1706.03947)'
  prefs: []
  type: TYPE_NORMAL
- en: The model is implemented using [TensorFlow](https://www.tensorflow.org/) and
    deployed on the [Tesla K80 GPU](https://www.nvidia.com/en-gb/data-center/tesla-k80/).
    The model has been tested using the UCF101 dataset for natural high-resolution
    videos.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/1cb5696326a800d9fcf2f87501d04a30.png)'
  prefs: []
  type: TYPE_IMG
- en: '[close](https://arxiv.org/abs/1706.03947)'
  prefs: []
  type: TYPE_NORMAL
- en: The authors use Peak Signal to Noise Ratio (PSN) and Structural Similarity Index
    (SSIM) for analyzing the quality of the interpolated frames. Below are the results
    they achieved.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/20ca7445006889891bf741486816952f.png)'
  prefs: []
  type: TYPE_IMG
- en: PhaseNet for Video Frame Interpolation (CVPR 2018)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PhaseNet consists of a neural network decoder that estimates the phase decomposition
    of the intermediate frame. The architecture is a neural network that combines
    the phase-based approach with a learning framework. The network proposed in this
    paper aims to synthesize an intermediate image given two neighboring images as
    input.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/73984ebb070ee13bfa9eb459fd7b5d9a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1804.00884)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**PhaseNet for Video Frame Interpolation**](https://arxiv.org/abs/1804.00884?source=post_page-----519ab2eb3dda----------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Most approaches for video frame interpolation require accurate dense correspondences
    to synthesize an in-between frame…
  prefs: []
  type: TYPE_NORMAL
- en: PhaseNet is designed as a decoder-only network, hence increasing its resolution
    level by level. At each level, the corresponding decomposition information is
    incorporated. Apart from the lowest level, all other levels are structurally identical.
    Information from the previous level is also included at each level.
  prefs: []
  type: TYPE_NORMAL
- en: The input to the network is the response from the steerable pyramid decomposition
    of the two input frames, consisting of the phase and amplitude values for each
    pixel at each level. These values are normalized before being passed through the
    network.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/73d32597b732bd598002964f1d815427.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1804.00884)'
  prefs: []
  type: TYPE_NORMAL
- en: Each resolution level has a PhaseNet block that takes the decomposition values
    from the input images as its input. It also takes in the resized feature maps
    and the resized predicted values from the previous level. This information is
    then passed through two convolution layers, each followed by batch normalization
    and ReLU nonlinearity.
  prefs: []
  type: TYPE_NORMAL
- en: Each convolution produces 64 feature maps. After each PhaseNet block, values
    of the in-between frame decomposition are predicted. This is done by passing the
    output feature maps of the PhaseNet block through one convolution layer with size
    1 x 1.
  prefs: []
  type: TYPE_NORMAL
- en: This is followed by the hyperbolic tangent function to predict output values.
    The decomposition values of the intermediate image are then computed from these
    values. Now the intermediate image can be reconstructed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/d08094f9bf507f42069ee9342c515fc1.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1804.00884)'
  prefs: []
  type: TYPE_NORMAL
- en: Training of this network is done using triplets of frames from the [DAVIS video
    dataset.](https://davischallenge.org/)
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/19d6fa55751bca761cb46bc05e707992.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1804.00884)'
  prefs: []
  type: TYPE_NORMAL
- en: Here are some of the error measurements obtained for this model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/f2d7978e4f1cd016cb676eb2ae0594d3.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1804.00884)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video
    Interpolation (CVPR 2018)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The authors of this paper propose an end-end-end convolution neural network
    for variable-length multi-frame video interpolation. In this model, motion interception
    and occlusion reasoning are jointly modeled.
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional optical flows between input images are computed using a [U-Net
    architecture](https://en.wikipedia.org/wiki/U-Net). The flows are then linearly
    combined at each time step to approximate the intermediate bi-directional optical
    flows. The approximated optical flows are refined using another U-Net.
  prefs: []
  type: TYPE_NORMAL
- en: This U-Net also predicts soft visibility maps. The two input images are then
    warped and linearly joined to form each intermediate frame. This approach is able
    to produce many intermediate frames as needed because the learned network parameters
    are time-independent.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Super SloMo: High Quality Estimation of Multiple Intermediate Frames for
    Video Interpolation**](https://arxiv.org/abs/1712.00080v2?source=post_page-----519ab2eb3dda----------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Given two consecutive frames, video interpolation aims at generating intermediate
    frame(s) to form both spatially and…
  prefs: []
  type: TYPE_NORMAL
- en: In this network, a flow computation CNN is used to first estimate the bi-directional
    optical flow between the two input images. This is then linearly joined to approximate
    the required intermediate optical flow in order to warp input images.
  prefs: []
  type: TYPE_NORMAL
- en: The network is trained by collecting 240-FPS videos from YouTube and hand-held
    cameras. The trained model is evaluated on several datasets including the [Middlebury](http://vision.middlebury.edu/flow/data/), [UCF101](https://www.crcv.ucf.edu/data/UCF101.php/), [slow
    flow dataset](http://www.cvlibs.net/projects/slow_flow/), and [high-frame-rate
    MPI Sintel.](http://sintel.is.tue.mpg.de/) The unsupervised optical flow results
    were also evaluated on the [KITTI 2012 optical flow benchmark](http://www.cvlibs.net/).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/dffa1e76f54a3ac7c62cc9026840e4e8.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1712.00080v2.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The U-Net used in this architecture is fully convolutional and consists of an
    encoder and a decoder. There are skip connections between the encoder and the
    decoder features at the same spatial resolution. There are six hierarchies in
    the encoder made up of two convolutional and Leaky ReLU layers.
  prefs: []
  type: TYPE_NORMAL
- en: An average pooling layer with a stride of 2 is used to decrease the spatial
    dimension at each hierarchy, except the last one. The decoder section has five
    hierarchies. The beginning of each hierarchy is a bilinear upsampling layer that’s
    used to increase the spatial dimension by a factor of 2\. This is followed by
    two convolutional and Leaky ReLU layers. 7 x 7 kernels are used in the first two
    convolutional layers and 5 x 5 layers in the second hierarchy. The remaining part
    of the network uses 3 x 3 convolution kernels.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/256dc68a5cb874d52cf5cc0eb622cab5.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1712.00080v2.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the performance of the said model on the UCF101 and Adobe datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4934c92495e6e0c565efeab107fb0125.png)'
  prefs: []
  type: TYPE_IMG
- en: '![Figure](../Images/87a8f2430bfc3305f41e928ae5c506b8.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1712.00080v2.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Depth-Aware Video Frame Interpolation (CVPR 2019)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This paper proposes a video frame interpolation method that detects occlusion
    by exploring depth information. The authors develop a depth-aware flow projection
    layer that synthesizes immediate flows that sample closer objects than ones that
    are far away.
  prefs: []
  type: TYPE_NORMAL
- en: Learning of hierarchical features is done by gathering contextual information
    from neighboring pixels. The output frame is then generated by warping the input
    frames, depth maps, and contextual features based on the optical flow and local
    interpolation kernels.
  prefs: []
  type: TYPE_NORMAL
- en: The authors propose a Depth-Aware video frame INterpolation (DAIN) model that
    effectively exploits the optical flow, local interpolation kernels, depth maps,
    and contextual features to generate high-quality video frames.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Depth-Aware Video Frame Interpolation**](https://arxiv.org/abs/1904.00830v1?source=post_page-----519ab2eb3dda----------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Video frame interpolation aims to synthesize nonexistent frames in-between the
    original frames. While significant…
  prefs: []
  type: TYPE_NORMAL
- en: The model uses [PWC-Net](https://github.com/NVlabs/PWC-Net/tree/master/PyTorch) as
    its flow estimation network. The flow estimation network is initialized from the
    pre-trained PWC-Net. For the depth estimation network, the authors use the hourglass
    architecture. The depth estimation network is also initialized from a pre-trained
    version. Contextual information is obtained by using a pre-trained [ResNet](https://arxiv.org/abs/1512.03385).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/e99fa95eea1f4c0e9219fb12559d1109.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1904.00830v1.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The authors build a context extraction network with one 7x 7 convolutional layer
    and two residual blocks without any normalization layer. A hierarchical feature
    is then obtained by concatenating the features from the first convolution layer
    and the two residual blocks. Training the context extraction network from scratch
    ensures that it learns effective contextual features for video frame interpolation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/ff82f12a037f14222c94a49c61a03949.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1904.00830v1.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: For the kernel estimation and adaptive warping layers, the authors use a U-Net
    architecture to estimate 4 x 4 local kernels for each pixel. The depth-aware flow
    projection layer generates the interpolation kernels and intermediate flows. The
    adaptive warping layer is adopted to warp the input frames, depth maps, and contextual
    features.
  prefs: []
  type: TYPE_NORMAL
- en: The final frame output is generated from a frame synthesis network. The network
    takes the warped input frames, warped depth maps, contextual features, projected
    flows, and interpolation kernels as its input. In order to ensure that the network
    predicts residuals between the ground-truth frame and the blended frame, the two
    warped frames are linearly blended.
  prefs: []
  type: TYPE_NORMAL
- en: The model is trained on the [Vimeo90K dataset](http://toflow.csail.mit.edu/) with
    AdaMax as the optimization strategy. The results obtained are shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/8442686e881cfad008c066de4f6715fb.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1904.00830v1.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Frame Interpolation with Multi-Scale Deep Loss Functions and Generative Adversarial
    Networks (2019)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this paper, the authors propose a multi-scale generative adversarial network
    for frame interpolation (FIGAN). The efficiency of this network is maximized by
    a multiscale residual estimation module, where the predicted flow and synthesized
    frame are constructed in a corse-to-fine fashion.
  prefs: []
  type: TYPE_NORMAL
- en: The quality of the synthesized intermediate video frames is improved by the
    fact that the network is jointly supervised at different levels with a perceptual
    loss that’s made up of an adversarial and two content losses. The network is evaluated
    on 60fps videos from YouTube.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/11f8fae9eb978c37a03b5182d808f688.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1711.06045.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Frame Interpolation with Multi-Scale Deep Loss Functions and Generative
    Adversarial Networks**](https://arxiv.org/abs/1711.06045?source=post_page-----519ab2eb3dda----------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Frame interpolation attempts to synthesise frames given one or more consecutive
    video frames. In recent years, deep…
  prefs: []
  type: TYPE_NORMAL
- en: The proposed model is made up of a trainable CNN architecture that directly
    estimates an interpolated frame from two input frames. Synthesis features are
    obtained by building a pyramidal structure and estimating optical flow between
    two frames at different scales. The synthesis refinement module is made up of
    a CNN that enables the joint processing of the synthesized image with the original
    input frames that produced it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/10329dde81ce4c02ba745b0b53476f8a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1711.06045.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Some of the results obtained from this network are shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/690f8df9e936d1ef74636d28034af166.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1711.06045.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We should now be up to speed on some of the most common — and a couple of very
    recent — techniques for performing video frame interpolation in a variety of contexts.
  prefs: []
  type: TYPE_NORMAL
- en: The papers/abstracts mentioned and linked to above also contain links to their
    code implementations. We’d be happy to see the results you obtain after testing
    them.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Derrick Mwiti](https://derrickmwiti.com/)** is a data analyst, a writer,
    and a mentor. He is driven by delivering great results in every task, and is a
    mentor at Lapid Leaders Africa.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://heartbeat.fritz.ai/research-guide-for-video-frame-interpolation-with-deep-learning-519ab2eb3dda).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Research Guide for Neural Architecture Search](/2019/10/research-guide-neural-architecture-search.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A 2019 Guide to Speech Synthesis with Deep Learning](/2019/09/2019-guide-speech-synthesis-deep-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A 2019 Guide for Automatic Speech Recognition](/2019/09/2019-guide-automatic-speech-recognition.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Customize Your Data Frame Column Names in Python](https://www.kdnuggets.com/2022/08/customize-data-frame-column-names-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Deal with Missing Data Using Interpolation Techniques in Pandas](https://www.kdnuggets.com/how-to-deal-with-missing-data-using-interpolation-techniques-in-pandas)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Bounding Box Deep Learning: The Future of Video Annotation](https://www.kdnuggets.com/2022/07/bounding-box-deep-learning-future-video-annotation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Text-2-Video Generation: Step-by-Step Guide](https://www.kdnuggets.com/2023/08/text2video-generation-stepbystep-guide.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AI, Analytics, Machine Learning, Data Science, Deep Learning…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Solid Plan for Learning Data Science, Machine Learning, and Deep Learning](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
