- en: First Open Source Implementation of DeepMind’s AlphaTensor
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DeepMind的AlphaTensor首次开源实现
- en: 原文：[https://www.kdnuggets.com/2023/03/first-open-source-implementation-deepmind-alphatensor.html](https://www.kdnuggets.com/2023/03/first-open-source-implementation-deepmind-alphatensor.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/03/first-open-source-implementation-deepmind-alphatensor.html](https://www.kdnuggets.com/2023/03/first-open-source-implementation-deepmind-alphatensor.html)
- en: '![First Open Source Implementation of DeepMind’s AlphaTensor](../Images/def9077780ef328cfbfa3725f2d93ac4.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![DeepMind的AlphaTensor首次开源实现](../Images/def9077780ef328cfbfa3725f2d93ac4.png)'
- en: Photo by [DeepMind](https://unsplash.com/@deepmind?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/4QVqSh4VvP4)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[DeepMind](https://unsplash.com/@deepmind?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)提供，来源于[Unsplash](https://unsplash.com/photos/4QVqSh4VvP4)
- en: Matrix multiplication is a fundamental operation used in many systems, from
    neural networks to scientific computing routines. Finding efficient and provably
    correct algorithms for matrix multiplication can have a huge impact on making
    computation faster and more efficient, but is a very challenging task. The space
    of possible algorithms is enormous, and traditional methods for discovering algorithms,
    such as human-designed heuristics or combinatorial search, are often suboptimal.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法是一种在许多系统中使用的基础操作，从神经网络到科学计算例程。寻找高效且可证明正确的矩阵乘法算法可以对加快计算速度和提高效率产生巨大影响，但这是一项非常具有挑战性的任务。可能的算法空间非常庞大，而传统的算法发现方法，如人工设计的启发式方法或组合搜索，往往不够理想。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的IT需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[DeepMind](https://www.deepmind.com/)''s recently proposed AI-based solution
    for automated search goes far beyond human intuition. The solution consists of
    a deep reinforcement learning agent called AlphaTensor, built on top of [AlphaZero](https://www.deepmind.com/blog/alphazero-shedding-new-light-on-chess-shogi-and-go).
    This agent is trained to play a single-player game, TensorGame, where the goal
    is to discover computationally efficient algorithms for matrix multiplication.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[DeepMind](https://www.deepmind.com/)最近提出的基于AI的自动化搜索解决方案远超人类直觉。该解决方案由一个称为AlphaTensor的深度强化学习代理组成，建立在[AlphaZero](https://www.deepmind.com/blog/alphazero-shedding-new-light-on-chess-shogi-and-go)之上。这个代理被训练来玩一个单人游戏TensorGame，目标是发现计算高效的矩阵乘法算法。'
- en: AlphaTensor is particularly good at handling large matrices by decomposing large
    matrix multiplications into smaller multiplications. Moreover, AlphaTensor can
    be used to achieve state-of-the-art performance for matrix multiplication once
    fine-tuned on a specific hardware device.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaTensor特别擅长处理大矩阵，通过将大型矩阵乘法分解为较小的乘法。此外，一旦针对特定硬件设备进行微调，AlphaTensor可以实现矩阵乘法的最先进性能。
- en: AlphaTensor has great potential for accelerating deep learning computing. In
    deep learning, many time-consuming operations can be mapped to matrix multiplications.
    By using AlphaTensor to optimize these operations, the overall performance of
    deep learning models can be significantly improved.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaTensor在加速深度学习计算方面具有巨大的潜力。在深度学习中，许多耗时的操作可以映射为矩阵乘法。通过使用AlphaTensor优化这些操作，可以显著提高深度学习模型的整体性能。
- en: Recently, OpenAlphaTensor, the [first open source implementation of AlphaTensor](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/open_alpha_tensor),
    was released, which could revolutionize the computational power of deep learning
    models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，OpenAlphaTensor，[AlphaTensor的首个开源实现](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/open_alpha_tensor)发布了，这可能会彻底改变深度学习模型的计算能力。
- en: Matrix Multiplication Tensor
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵乘法张量
- en: For non-experts in matrix multiplication optimization, it may not be straightforward
    to understand how an operation such as matrix multiplication can be mapped in
    a three-dimensional tensor. I will try to explain it in simple words and with
    examples.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于矩阵乘法优化的非专家来说，理解如何将诸如矩阵乘法这样的操作映射到三维张量中可能并不直观。我将尝试用简单的语言和示例来解释。
- en: Let’s consider the product C = A*B, where for simplicity both A and B are square
    matrices of size N. The multiplication operation can be mapped in a 3D tensor
    of shape (N^2, N^2, N^2). The first tensor dimension represents the flattened
    matrix A, the second dimension the flattened matrix B and the third dimension
    the flattened matrix C.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑乘积C = A*B，为了简单起见，假设A和B都是大小为N的方阵。乘法操作可以映射到形状为(N^2, N^2, N^2)的3D张量中。第一个张量维度表示展平的矩阵A，第二个维度表示展平的矩阵B，第三个维度表示展平的矩阵C。
- en: The tensor has only binary values (either 1 or 0) for each entry. Note that
    the tensor represents the multiplication operation, so it is independent of the
    values of the matrices A and B.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 张量的每个条目只有二进制值（要么是1，要么是0）。请注意，张量表示乘法操作，因此它与矩阵A和B的值无关。
- en: Every entry of the tensor corresponds to the coefficient of the operation. For
    example, to compute C[1,1], it is necessary to multiply both A[1,1] and B[1,1].
    Therefore, the tensor entry [0,0,0], which corresponds to A[1,1], B[1,1] and C[1,1],
    will have value 1\. In contrast, to compute C[1,1], A[2,1] is not needed. Thus,
    the tensor row T[N+1, :, 0] will contain only zeros.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 张量的每个条目对应于操作的系数。例如，要计算C[1,1]，需要同时乘以A[1,1]和B[1,1]。因此，张量条目[0,0,0]，即A[1,1]、B[1,1]和C[1,1]，的值将为1。相反，要计算C[1,1]，A[2,1]是不需要的。因此，张量行T[N+1,
    :, 0]将仅包含零。
- en: The image below shows an example of a tensor for N=2.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了N=2时张量的一个例子。
- en: '![XXXXX](../Images/52899e232c8a0ef3f9c334c3d8cbf684.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/52899e232c8a0ef3f9c334c3d8cbf684.png)'
- en: Image from DeepMind's [paper](https://www.nature.com/articles/s41586-022-05172-4)
    published in [Nature](https://www.nature.com/)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于DeepMind的[论文](https://www.nature.com/articles/s41586-022-05172-4)，该论文发表在[Nature](https://www.nature.com/)上
- en: As shown in (b) and (c) in the figure above, it is possible to implement an
    algorithm for computing the product using a decomposition of the 3D tensor. More
    specifically, the algorithm below can be used for converting a tensor decomposition
    (the matrices U, V, W) into a matrix multiplication algorithm.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图（b）和（c）所示，可以使用3D张量的分解实现计算乘积的算法。更具体地说，下面的算法可以用于将张量分解（矩阵U、V、W）转换为矩阵乘法算法。
- en: '![XXXXX](../Images/b1ee92f0b18f2bb0b65a7b5dffb577ad.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/b1ee92f0b18f2bb0b65a7b5dffb577ad.png)'
- en: Meta-algorithm parameterized for computing the matrix product C=AB introduced
    in DeepMind's [paper](https://www.nature.com/articles/s41586-022-05172-4)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 用于计算矩阵乘积C=AB的Meta算法参数在DeepMind的[论文](https://www.nature.com/articles/s41586-022-05172-4)中介绍
- en: The TensorGame
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorGame
- en: The problem of finding efficient algorithms for matrix multiplication is extremely
    challenging because the number of possible algorithms to consider is much larger
    than the number of atoms in the universe, even for small instances of matrix multiplication.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 找到高效的矩阵乘法算法是极具挑战性的，因为即使对于小规模的矩阵乘法，考虑的算法数量也远远大于宇宙中的原子数。
- en: DeepMind converted this problem into a single-player game, and called it the
    TensorGame. In this game, the player chooses how to combine different entries
    of matrices to multiply them. A score is assigned based on the number of operations
    required to achieve the correct multiplication result. The game ends when the
    zero tensor is reached or when the maximum number of moves has been made. The
    final factorization is evaluated based on an estimation of the residual rank and
    certain optimization criteria, such as asymptotic time complexity or practical
    runtime.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind将这个问题转化为单人游戏，并称之为TensorGame。在这个游戏中，玩家选择如何组合矩阵的不同条目以进行乘法运算。根据实现正确乘法结果所需的操作次数来分配分数。当达到零张量或已进行最大移动次数时，游戏结束。最终的分解结果基于残差秩的估计以及某些优化标准，如渐近时间复杂度或实际运行时间。
- en: The initial position in the TensorGame corresponds to the Matrix Multiplication
    Tensor expressed on some random basis.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: TensorGame中的初始位置对应于在某个随机基底上表示的矩阵乘法张量。
- en: 'In each step t of the game, the player writes down three vectors ![Equation](../Images/d568d531f9a3dde56d490c5d1966be4e.png)
    , which specifies the rank-1 tensors ![Equation](../Images/f9bd3adb3c72387355dda12e508c4b53.png)
    . The state of the game is updated by subtracting the vectors selected by the
    player:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏的每一步 t，玩家写下三个向量 ![Equation](../Images/d568d531f9a3dde56d490c5d1966be4e.png)，这些向量指定了秩-1
    张量 ![Equation](../Images/f9bd3adb3c72387355dda12e508c4b53.png)。通过减去玩家选择的向量来更新游戏状态：
- en: '![XXXXX](../Images/720a9cfb94d12e3fbcdd4ea89472c67d.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/720a9cfb94d12e3fbcdd4ea89472c67d.png)'
- en: where ![Equation](../Images/1a98fb9d63d4304c60d03670488b05f0.png) is the Matrix
    Multiplication Tensor.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![Equation](../Images/1a98fb9d63d4304c60d03670488b05f0.png) 是矩阵乘法张量。
- en: If the game ends in p steps, this means that the Matrix Multiplication Tensor
    ![Equation](../Images/6470cf9ff893747aeaab2ab01ca80c07.png) can be decomposed
    into p rank-1 tensors ![Equation](../Images/f9bd3adb3c72387355dda12e508c4b53.png),
    i.e. it has at least rank p.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果游戏在 p 步中结束，这意味着矩阵乘法张量 ![Equation](../Images/6470cf9ff893747aeaab2ab01ca80c07.png)
    可以分解为 p 个秩-1 张量 ![Equation](../Images/f9bd3adb3c72387355dda12e508c4b53.png)，即它至少具有秩
    p。
- en: The TensorGame can then be interpreted as a rank-decomposition algorithm and
    AlphaTensor can be seen as an algorithm for estimating the rank of the tensor.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: TensorGame 可以被解释为一种秩分解算法，AlphaTensor 可以被视为一种估计张量秩的算法。
- en: AlphaTensor Architecture
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AlphaTensor 架构
- en: So far we have learned about the TensorGame and clarified how its solution can
    be seen as a matrix multiplication algorithm. Let’s now explore the main concepts
    of AlphaTensor, the algorithm used for the game.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们了解了 TensorGame，并澄清了其解决方案如何被视为矩阵乘法算法。现在让我们探讨 AlphaTensor 的主要概念，这是用于游戏的算法。
- en: 'AlphaTensor architecture is basically an encoder-decoder Transformer architecture
    where:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaTensor 架构基本上是一个编码器-解码器 Transformer 架构，其中：
- en: the encoder takes as input the game state ![Equation](../Images/0cd1b24c6bde995465c5ed7520d3324e.png),
    the n previous actions taken by the model (usually n=7), and the time index t
    of the current action. Information is stacked together in a tensor with shape
    (n+1, N^2, N^2, N^2). This tensor is then reshaped and transformed (using three
    linear layers) in a tensor of shape (N^2, N^2, c) where c is the inner dimension
    of the model.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器以游戏状态 ![Equation](../Images/0cd1b24c6bde995465c5ed7520d3324e.png) 为输入，以及模型采取的
    n 个之前的动作（通常 n=7），和当前动作的时间索引 t。信息被堆叠在一个形状为 (n+1, N^2, N^2, N^2) 的张量中。这个张量随后被重塑并通过三个线性层转换成形状为
    (N^2, N^2, c) 的张量，其中 c 是模型的内部维度。
- en: the decoder generates the n_steps actions from the embedded vector given by
    the encoder in an auto-regressive way. Each action corresponds to a token of the
    triplets ![Equation](../Images/3321496b29a6e770b29e3b538191fcb1.png) representing
    one of the triplets decomposing the game tensor (i.e. reducing its rank)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器以自回归方式从编码器给出的嵌入向量生成 n_steps 动作。每个动作对应于三元组的一个标记 ![Equation](../Images/3321496b29a6e770b29e3b538191fcb1.png)，这些三元组表示分解游戏张量（即降低其秩）的其中一个三元组。
- en: The model is trained by alternating back-propagation and model acting. Model
    acting is used to generate data that is then used to train the model. In practice,
    the model is trained with a mixture of synthetically generated data and data generated
    by the model during acting. The acting step is done by taking a 3D tensor corresponding
    to a matrix operation and playing n_actors games on it. Each actor plays a game
    either on the standard basis or on an alternative basis (the change of basis is
    applied with a given probability). The results are then collected and can be used
    in the training step with the synthetic data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 模型通过交替进行反向传播和模型行动进行训练。模型行动用于生成数据，然后用于训练模型。在实践中，模型是通过合成生成的数据和模型在行动过程中生成的数据的混合进行训练的。行动步骤是通过对矩阵操作对应的
    3D 张量进行 n_actors 游戏来完成的。每个演员要么在标准基底上进行游戏，要么在替代基底上进行游戏（基底变化以给定的概率应用）。然后收集结果，并可以与合成数据一起用于训练步骤。
- en: The acting step is based on AlphaZero's Monte Carlo Tree Search (MCTS), modified
    to support large action spaces. In short, before choosing the action, n_sims paths
    are explored from the model output with a maximum future exploration of 5 steps.
    The probabilities generated by the model are then adjusted taking into account
    the generated paths. Then the action with the most promising future path(s) is
    chosen to continue the game.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 动作步骤基于AlphaZero的蒙特卡洛树搜索（MCTS），经过修改以支持大型动作空间。简而言之，在选择动作之前，从模型输出中探索n_sims条路径，最大未来探索步数为5步。然后，模型生成的概率会根据生成的路径进行调整。接着选择具有最有前景的未来路径的动作以继续游戏。
- en: While training the model, the reward is actually a negative reward (penalty).
    Its absolute value increases with each additional step required to solve the game.
    If the model takes m steps to solve a TensorGame, the reward associated with the
    game is r=-m. If the model is not able to solve the TensorGame in max_rank steps,
    the reward is computed by estimating the rank of the remaining tensor. The rank
    is estimated as the sum of the ranks of the matrices that compose the tensor.
    The estimate is an upper bound on the true rank of the tensor.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，奖励实际上是负奖励（惩罚）。其绝对值随着解决游戏所需的每一步而增加。如果模型需要m步才能解决一个TensorGame，那么与该游戏相关的奖励是r=-m。如果模型在max_rank步数内无法解决TensorGame，则通过估计剩余张量的秩来计算奖励。秩估计为组成张量的矩阵的秩之和。这个估计是张量真实秩的上界。
- en: When fine-tuning the model, the penalty reward at the terminal state should
    also take into account the latency of the algorithm produced by the model. The
    reward formula becomes rt'=rt+λbt, where rt is the reward scheme described earlier,
    bt is the benchmark reward (non-zero only at the terminal state), and *λ* is a
    user-specified coefficient.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调模型时，终态的惩罚奖励还应考虑模型所产生的算法的延迟。奖励公式变为rt'=rt+λbt，其中rt是之前描述的奖励方案，bt是基准奖励（仅在终态为非零），*λ*是用户指定的系数。
- en: '![XXXXX](../Images/8972270b37ef828419347a12b5415f00.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/8972270b37ef828419347a12b5415f00.png)'
- en: 'Speed-ups (%) of AlphaTensor-discovered algorithms tailored for a GPU and a
    TPU, extracted from DeepMind’s paper. Speed-ups are measured relative to standard
    (e.g. cuBLAS for the GPU) matrix multiplication on the same hardware and compared
    to the [Strassen-square algorithm](https://en.wikipedia.org/wiki/Strassen_algorithm).
    Source: [DeepMind](https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从DeepMind的论文中提取的AlphaTensor发现的针对GPU和TPU优化的算法的加速比（%）。加速比是相对于相同硬件上的标准（例如GPU上的cuBLAS）矩阵乘法进行测量的，并与[Strassen平方算法](https://en.wikipedia.org/wiki/Strassen_algorithm)进行比较。来源：[DeepMind](https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor)。
- en: The Open Source Implementation of DeepMind’s AlphaTensor
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深Mind的AlphaTensor开源实现
- en: I recently released [OpenAlphaTensor](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/open_alpha_tensor),
    the first open source implementation of AlphaTensor. In this section I will walk
    through the implementation. As we discussed earlier, the AlphaTensor architecture
    is fairly straightforward, based on a standard transformer with an encoder-decoder
    architecture. The most interesting components of AlphaTensor are the first layer
    in the encoder part and the way the actions are sampled.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我最近发布了[OpenAlphaTensor](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/open_alpha_tensor)，这是AlphaTensor的第一个开源实现。在这一部分，我将详细介绍实现过程。正如我们之前讨论的，AlphaTensor的架构非常简单，基于一个标准的带有编码器-解码器架构的transformer。AlphaTensor中最有趣的组件是编码器部分的第一层和动作采样的方式。
- en: Let’s start with the first encoding layer.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一个编码层开始。
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the snippet above, we show how the input tensor is decomposed into three
    tensors, which are then used as query, key, and value inputs of the transformer-layer.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码片段中，我们展示了输入张量如何分解为三个张量，然后将这些张量用作transformer层的查询、键和值输入。
- en: Across the three tensor dimensions representing the flattened matrices (A, B,
    C), the input tensor is flattened along each dimension together with the dimension
    representing the previous actions. In this way, in each flattened-copy of the
    input tensor, the selected dimension is an aggregation of the last T-1 values
    and the actual value, for all the S values of the selected dimension, where S=N^2\.
    Philosophically, it is as if, for each dimension, we focus on what happened in
    the previous actions in that dimension.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在表示展平矩阵 (A, B, C) 的三个张量维度中，输入张量沿每个维度展平，同时与表示先前动作的维度一起。在这种方式下，在每个展平的输入张量副本中，所选维度是最后
    T-1 个值和实际值的聚合，对于所选维度的所有 S 个值，其中 S=N^2。从哲学上讲，就好像对于每个维度，我们专注于在该维度中先前动作发生的情况。
- en: The scalars are mapped in three different spaces of dimension S^2, and then
    reshaped to be concatenated with the tensors obtained at the previous point. Conceptually,
    the scalars are mapped to an embedding space of dimension S^2, and then the embedded
    information is chunked into S vectors and stacked together, similar to what happens
    to text when tokenized.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标量被映射到三个不同的维度为 S^2 的空间中，然后重新形状以与先前点获得的张量连接。概念上，标量被映射到维度为 S^2 的嵌入空间，然后将嵌入的信息分块成
    S 个向量并堆叠在一起，类似于文本被标记化时的情况。
- en: Scalar tokens are concatenated with the restructured input tensor and then given
    as input to a linear layer for mapping the scalars+channel-history focus information
    in the internal dimension of the model.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标量标记与重新结构化的输入张量连接在一起，然后作为输入提供给线性层，以将标量+通道历史关注信息映射到模型的内部维度中。
- en: These three steps can be interpreted as a way of giving to the model both information
    about the scalars (as in the TensorGame time step) and the focus on the previous
    actions for each channel.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这三步可以解释为一种方法，既向模型提供关于标量的信息（如在 TensorGame 时间步中），又关注每个通道的先前动作。
- en: Regarding the way the actions are produced, it is interesting to note that AlphaTensor
    generates as output the triplet u, v, w, which aims to reduce the tensor rank.
    The three vectors have size S and since they are concatenated the model has to
    produce a vector of size 3*S. AlphaTensor is trained with a RL algorithm, so all
    possible actions must be expressed in terms of probabilities in an enumerated
    space, i.e. the model produces a probability over the different actions. This
    means that each vector in the 3S space should be mapped to a different action.
    This results in an action space of size |F|^(3S), where |F| is the number of different
    values that the element of u, v, w can take. Usually, the values are restricted
    to (-2, -1, 0, 1, 2), resulting in a cardinality of 5 elements.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 关于动作产生的方式，值得注意的是 AlphaTensor 输出的三元组 u, v, w 旨在降低张量秩。这三个向量的大小为 S，由于它们被连接在一起，模型必须产生一个大小为
    3*S 的向量。AlphaTensor 使用 RL 算法进行训练，因此所有可能的动作必须以概率形式表达在一个枚举空间中，即模型对不同的动作生成一个概率。这意味着
    3S 空间中的每个向量都应该映射到一个不同的动作。这导致了一个大小为 |F|^(3S) 的动作空间，其中 |F| 是 u、v、w 元素可以取的不同值的数量。通常，这些值限制在
    (-2, -1, 0, 1, 2)，结果是 5 个元素的基数。
- en: 'Here comes a major challenge: to generate the action probabilities for a matrix
    product of matrices of size 5 we would need a memory of 5^75 * 4 bytes, which
    would mean ~10^44 GB of memory. Clearly, we cannot manage such a large action
    space.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里出现了一个重大挑战：为了生成大小为 5 的矩阵乘法的动作概率，我们需要 5^75 * 4 字节的内存，这意味着大约 10^44 GB 的内存。显然，我们无法管理如此大的动作空间。
- en: How do we solve the problem? To reduce the memory footprint of the action probabilities
    we can split the triplets into smaller chunks, “tokenize” them, and treat the
    chunks as generated tokens in the transformer architecture, i.e. the tokens are
    given as input to the decoder in an auto-regressive way. In the example above
    we can split the triplets into 15 chunks, reducing the memory consumption to 15
    * 5^(75/15) * 4, i.e. 187.5 KB.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解决这个问题？为了减少动作概率的内存占用，我们可以将三元组拆分成更小的块，“标记化”它们，并将这些块作为生成的标记输入到变换器架构中，即这些标记以自回归方式作为输入提供给解码器。在上面的例子中，我们可以将三元组拆分成
    15 个块，将内存消耗减少到 15 * 5^(75/15) * 4，即 187.5 KB。
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Above we show the code snippet for generating the full action. In the code,
    self.core contains the decoder layer and the tensor e represents the output of
    the encoder layer. Zero can be considered as the <eos> token in NLP models and
    the n_steps actions representing the n_steps chunks are generated in a progressive
    way.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 上面展示了生成完整动作的代码片段。在代码中，self.core包含解码层，而张量e表示编码层的输出。Zero可以被视为NLP模型中的<eos>标记，而表示n_steps块的n_steps动作是逐步生成的。
- en: 'The model returns three quantities:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 模型返回三个量：
- en: The generated actions
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成的动作
- en: The probability associated to the full action
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与完整动作相关的概率
- en: The logits produced for generating the first action (the first chunk) that will
    be used for computing the model value.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成第一个动作（第一个块）的logits，这将用于计算模型值。
- en: It is worth spending a few words on the n_samples parameter. The parameter is
    used for the acting step and it allows the model to generate different versions
    of the triplets which will then be used for exploring the action space in the
    Monte Carlo Tree Search algorithm used in the Acting process. The n_samples different
    actions are sampled according to the policy generated by the model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 值得花几句话说明n_samples参数。该参数用于行动步骤，允许模型生成不同版本的三元组，这些三元组将用于探索蒙特卡罗树搜索算法中的动作空间。n_samples个不同的动作是根据模型生成的策略进行抽样的。
- en: Acting Step
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 行动步骤
- en: The most tricky part of the whole algorithm is probably the Acting step used
    for solving the TensorGame. The algorithm is not deeply explained in the AlphaTensor
    paper, since it is based on several DeepMind’s previous papers which are just
    cited and given as known. Here, I’ll reconstruct all the missing pieces and explain
    step by step our implementation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 整个算法中最棘手的部分可能是用于解决TensorGame的行动步骤。该算法在AlphaTensor论文中没有深入解释，因为它基于几篇DeepMind的先前论文，这些论文只是被引用并作为已知。在这里，我将重建所有缺失的部分，并逐步解释我们的实现。
- en: 'We can organize the acting steps in three different components:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将行动步骤组织为三个不同的组件：
- en: The Monte-Carlo Tree Search
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒙特卡罗树搜索
- en: The game simulation
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游戏模拟
- en: The Improved policy computation
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进的策略计算
- en: Let us analyze them one by one.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一分析它们。
- en: Monte-Carlo Tree Search (MCTS)
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蒙特卡罗树搜索（MCTS）
- en: Monte Carlo Tree Search (MCTS) is a widely used artificial intelligence technique
    for game playing, particularly in board games and video games. The algorithm creates
    a game tree that simulates potential moves and outcomes and uses random sampling
    to evaluate the expected reward for each move. The algorithm then iteratively
    selects the move with the highest expected reward and simulates outcomes until
    it reaches a terminal state or a specified stopping condition. The simulations
    are used to estimate the probability of winning for each move and guide the decision-making
    process. MCTS has been shown to be effective in complex games where the number
    of possible moves and outcomes is large, and it has been used in successful game-playing
    AI systems, such as AlphaGo.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡罗树搜索（MCTS）是一种广泛使用的人工智能技术，主要用于游戏，特别是棋盘游戏和视频游戏。该算法创建一个游戏树，模拟潜在的移动和结果，并使用随机抽样来评估每个动作的预期奖励。然后，算法反复选择预期奖励最高的动作，并模拟结果直到达到终止状态或指定的停止条件。这些模拟用于估算每个动作的获胜概率，并指导决策过程。MCTS已被证明在可能的动作和结果数量庞大的复杂游戏中有效，并且已被成功应用于游戏AI系统，如AlphaGo。
- en: In AlphaTensor a modified version of the original MCTS is used. In particular,
    instead of randomly selecting the action from the whole action space, the action
    is selected among a subset generated directly by the model (through the n_samples
    presented before). The correction to the policy upgrade is then applied in the
    Improved Policy computation step.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在AlphaTensor中，使用了原始MCTS的修改版本。特别是，与从整个动作空间随机选择动作不同，该动作是从模型直接生成的子集（通过前面提到的n_samples）中选择的。然后在改进的策略计算步骤中应用对策略升级的修正。
- en: In our implementation, we decided to keep all the information about the Monte-Carlo
    tree in a dictionary having as key the hash-version of the TensorGame state and
    as values the information associated with the state itself. Each Monte-Carlo step
    starts from a node and simulates n_sim mini-games, exploring the future with a
    horizon of 5 moves. If the node has already been explored in previous simulations,
    n_sim is adjusted considering the number of previous explorations. For each node
    the number of visits is stored in the N_s_a tensor, since this tensor contains
    the number of visits per node child action (among the ones sampled by the model).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实现中，我们决定将关于蒙特卡洛树的所有信息保存在一个字典中，该字典的键是TensorGame状态的哈希版本，值是与状态本身相关的信息。每个蒙特卡洛步骤从一个节点开始，并模拟`n_sim`个迷你游戏，探索未来5步的范围。如果节点在先前的模拟中已经被探索过，则`n_sim`会根据之前的探索次数进行调整。每个节点的访问次数存储在`N_s_a`张量中，因为该张量包含每个节点子动作的访问次数（在模型抽样的动作中）。
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The code above shows our implementation of the algorithm. For a matter of code
    simplicity, the policy correction is performed in the simulate_game function.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码展示了我们对算法的实现。为了代码的简洁性，策略修正是在`simulate_game`函数中完成的。
- en: Game Simulation
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 游戏模拟
- en: 'The simulate_game function is responsible for exploring the tree composed of
    nodes representing a particular state of the TensorGame. It also runs the model
    whenever a leaf node is encountered and it stores all node information in the
    state_dict dictionary. Let’s give a deep look at its implementation:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`simulate_game`函数负责探索由代表TensorGame某一特定状态的节点组成的树。当遇到叶子节点时，它还会运行模型，并将所有节点信息存储在`state_dict`字典中。让我们深入了解它的实现：'
- en: '[PRE3]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Each simulation is divided in three parts:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 每次模拟被分成三部分：
- en: Selection
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择
- en: Expansion
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展
- en: Backup
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 备份
- en: 'In the selection part the simulation is run on the already generated tree-nodes,
    and the following node is selected using the following function:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择部分，模拟在已生成的树节点上运行，使用以下函数选择下一个节点：
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In practice, the action maximizing the ucb function:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，最大化ucb函数的动作：
- en: '![XXXXX](../Images/54f61ca39bee17e4a0e1dddd3d279be5.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/54f61ca39bee17e4a0e1dddd3d279be5.png)'
- en: for the given state is selected. Here Q represents the Q values generated by
    the model and π represents the random distribution over the actions sampled using
    the model policy. N(s, a) represents the number of visits of the node to action
    a from node s.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定状态选择的。这里Q表示模型生成的Q值，π表示基于模型策略抽样的动作上的随机分布。N(s, a)表示从节点s到动作a的节点访问次数。
- en: 'Once the selection phase reaches a leaf node, if the simulation has not reached
    a terminal condition (in terms of either maximum exploration, i.e. future horizon,
    or game ending), the model is then used for selecting n_samples alternative nodes
    (they will be leaf nodes in the successive iteration). This is called the expansion
    phase, since new nodes are added to the tree. Then, no further node is explored
    in the current simulation, but the leaf q_value is sent to the following simulation
    step: the backup.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选择阶段达到叶子节点，如果模拟没有达到终止条件（无论是最大探索，即未来范围，还是游戏结束），则使用模型选择`n_samples`个替代节点（它们将在后续迭代中成为叶子节点）。这称为扩展阶段，因为新节点被添加到树中。然后，在当前模拟中不再探索更多节点，但叶子q_value会被传递到下一模拟步骤：备份。
- en: Backup is the final stage of each simulation. During backup, if the leaf node
    was a terminal state the final reward is computed; otherwise the leaf q value
    is used as an estimated reward. Then the reward is back-propagated on the simulation
    trajectory updating both the states q_values and updating the visit counter N(s,
    a). In the snippet below we show the code for the reward back-propagation.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 备份是每次模拟的最终阶段。在备份过程中，如果叶子节点是终止状态，则计算最终奖励；否则使用叶子q值作为估计奖励。然后将奖励在模拟轨迹上反向传播，更新状态的q_values并更新访问计数器N(s,
    a)。在下面的代码片段中，我们展示了奖励反向传播的代码。
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Improved Policy Computation
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改进的策略计算
- en: Once all the simulations have been run and the MCTS offers an interesting snapshot
    of the near future it is time to update the policy associated with the predicted
    nodes and return them, so that they can be used during training. The improved
    policy, following the method described in [Hubert et al](https://arxiv.org/pdf/2104.06303.pdf),
    is used for managing large action spaces. In fact, for small search space, it
    is possible during MCTS to sample an action randomly from the action space and
    evaluate its impact. A similar approach in a much larger action space would lead
    to all trajectories diverging in different paths and it would need an infinite
    amount of trajectories for getting meaningful statistics and then updating the
    policy. Since here we are using sample-MCTS for avoiding the dispersion, i.e.
    n_samples actions are sampled accordingly to the model policy and then MCTS just
    selects one of the sampled actions while exploring the tree, we need to take into
    account the sample-correction when computing the final updated policy that will
    be used while training the model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有模拟运行完毕，并且MCTS提供了对近期未来的有趣快照，就该更新与预测节点相关联的策略并返回这些节点，以便在训练过程中使用。改进的策略，遵循[Hubert
    et al](https://arxiv.org/pdf/2104.06303.pdf)中描述的方法，用于管理大型动作空间。实际上，对于小型搜索空间，在MCTS过程中，可以从动作空间中随机采样一个动作并评估其影响。在更大动作空间中采用类似的方法会导致所有轨迹在不同路径上分歧，并且需要无限量的轨迹来获取有意义的统计数据，然后更新策略。由于我们这里使用的是采样-MCTS来避免分散，即n_samples动作根据模型策略进行采样，然后MCTS在探索树时只选择采样的一个动作，因此在计算最终更新策略时需要考虑样本修正，以便在训练模型时使用。
- en: In practice, the improved policy is computed as
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，改进的策略计算如下
- en: '![XXXXX](../Images/c6070e2ec5a1f4ddb1a2b2b1a6af6921.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/c6070e2ec5a1f4ddb1a2b2b1a6af6921.png)'
- en: where
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![XXXXX](../Images/54f61ca39bee17e4a0e1dddd3d279be5.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/54f61ca39bee17e4a0e1dddd3d279be5.png)'
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that in our implementation after having computed the policy from the N_s_a
    tensor we have to map it back to the original action tensor. In fact, N_s_a just
    considers the actions sampled by the model, while the final policy must contain
    probabilities also for the not-explored actions.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在我们的实现中，计算完来自N_s_a张量的策略后，我们必须将其映射回原始的动作张量。事实上，N_s_a只考虑了模型采样的动作，而最终的策略必须包含未探索动作的概率。
- en: Differences with respect to ChatGPT training algorithm
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与ChatGPT训练算法的区别
- en: AlphaTensor is the latest member of the AlphaGo/AlphaZero family of artificial
    intelligence methods by DeepMind. These methods are based on the Monte Carlo Tree
    Search (MCTS) algorithm, which has been refined and enhanced by DeepMind to tackle
    increasingly complex tasks. Another AI system, OpenAI's ChatGPT, which has caused
    a lot of buzz for its remarkable performance, was trained with a different approach,
    called Reinforcement Learning with Human Feedback (RLHF).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaTensor是DeepMind最新的AlphaGo/AlphaZero系列人工智能方法中的成员。这些方法基于蒙特卡罗树搜索（MCTS）算法，DeepMind已经对其进行了改进和增强，以应对越来越复杂的任务。另一种引起广泛关注的AI系统，OpenAI的ChatGPT，其表现非常出色，采用了不同的方法，即带有人类反馈的强化学习（RLHF）。
- en: RLHF is a fine-tuning technique used to tune language models to follow a set
    of written instructions. It uses human preferences as a reward signal to fine-tune
    the model, thereby aligning the behavior of the language model with the stated
    preferences of a specific group of people, rather than some broader notion of
    ‘human values’.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF是一种微调技术，用于调整语言模型以遵循一组书面指令。它使用人类偏好作为奖励信号来微调模型，从而使语言模型的行为与特定人群的声明偏好对齐，而不是某种更广泛的“人类价值观”。
- en: In contrast, MCTS is a tree-based search algorithm used to determine the optimal
    moves in games. It simulates potential moves and updates the values of each move
    based on their outcomes, guiding the selection of the best move.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，MCTS是一种基于树的搜索算法，用于确定游戏中的最佳动作。它模拟潜在的动作，并根据这些动作的结果更新每个动作的值，从而指导最佳动作的选择。
- en: RLHF collects data from human-written demonstrations and human-labeled comparisons
    between AI models, and trains a reward model to predict the preferences of a given
    group of people. The reward model is then used to fine-tune the AI models. MCTS,
    on the other hand, uses simulations and evaluations to determine the best decision.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF收集来自人工编写的演示和人工标记的AI模型之间的比较的数据，并训练一个奖励模型来预测特定人群的偏好。然后，奖励模型用于微调AI模型。另一方面，MCTS则使用模拟和评估来确定最佳决策。
- en: Although they are different approaches, RLHF and MCTS also have similarities.
    Both artificial intelligence techniques use decision-making and problem solving
    methods, and both use a trial-and-error approach to explore different options
    and make decisions based on available information. Both are also iterative processes
    that improve over time as more information and experience are gathered.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们是不同的方法，RLHF和MCTS也有相似之处。两者都是人工智能技术，采用决策制定和问题解决的方法，并且都使用试错法来探索不同的选项，并基于现有信息做出决策。它们也是迭代过程，随着更多信息和经验的积累而不断改进。
- en: The choice between RLHF and MCTS depends on the task at hand. RLHF is ideal
    when there is no clear metric for evaluating the model performance, while MCTS
    has proven effective in game-like tasks where knowledge and exploration of the
    future give the model a significant advantage.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在RLHF和MCTS之间的选择取决于手头的任务。当没有明确的度量标准来评估模型性能时，RLHF是理想的选择，而MCTS在游戏类任务中表现有效，其中知识和对未来的探索为模型提供了显著的优势。
- en: Code Optimization for AlphaTensor training
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AlphaTensor训练的代码优化
- en: Implementing the AlphaTensor training algorithm requires finding the perfect
    compromise between training speed and memory consumption. As seen in the Model
    section, simply considering the action tokenization can save a lot of memory,
    but an overly aggressive action space reduction can lead to both drop in accuracy
    and slower performance. The latter happens because all tokens are generated sequentially
    in an autoregressive way by the model decoder. Therefore, the inference time grows
    linearly with the number of tokens per action once the softmax on the action space
    is not the bottleneck anymore.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 实现AlphaTensor训练算法需要在训练速度和内存消耗之间找到完美的平衡。如模型部分所示，仅考虑行动标记化就能节省大量内存，但过于激进的行动空间减少可能会导致准确性下降和性能变慢。后者发生的原因是所有标记都由模型解码器以自回归方式顺序生成。因此，一旦行动空间上的softmax不再成为瓶颈，推理时间就会随着每个行动的标记数量线性增长。
- en: When setting up AlphaTensor training, the main difficulties were found in dealing
    with the acting process. If the tensors are not stored in the correct format,
    the MCTS can easily cause uncontrolled memory usage growth. On the other hand,
    if the number of tensors stored during each simulation is reduced too much, the
    MCTS can spend an infinite amount of time re-computing the required states.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置AlphaTensor训练时，主要的困难在于处理行动过程。如果张量没有以正确的格式存储，MCTS可能会导致无法控制的内存使用增长。另一方面，如果每次模拟中存储的张量数量减少得过多，MCTS可能会花费无限时间重新计算所需的状态。
- en: Let's take an example of the game simulation step, where the game is explored
    by looking at possible future scenarios. For each state, if we don't save the
    actions generated by the model and we decide to save only the random seed used
    to sample the actions from the policy, then each time we explore a tree node we
    would have to recompute the policy and then sample the actions. Clearly, we decided
    to store the sampled actions to save time and to avoid having to manage model
    sharing between different processes in the case of MCTS exploration parallelization.
    However, just saving the actions was not enough to get a sufficiently efficient
    acting step. In fact, the time for converting the n_steps actions into the (u,
    v, w) triplet, reducing the game tensor state and creating the new3D tensors from
    the n_samples actions would easily be a bottleneck for the whole training. Secondly,
    we didn't want to store all possible future states for each sampled action, as
    this would have a huge impact on the memory used by the algorithm. Suppose we
    set n_samples=32, n=7 and N=5, and let's remember that N is the size of the square
    matrix product we want to reduce and n is the number of previous actions remembered
    by the model. In this situation, each state tensor would have the form (8, 25,
    25, 25), which multiplied by 32 would result in 32*8*25*25*25*4 bytes for each
    node in the graph. Now, considering that each simulation in the expansion phase
    generates a new node (and n_sim=200), we would have a final memory consumption
    of 200*32*8*25*25*25*4 = 3.2GB for the first MCTS node alone. In the worst-case
    scenario, while exploring acting max_rank nodes (where max_rank=150), this would
    result in a total memory consumption of 150 * 3.2GB = 480GB in RAM memory (or
    GPU memory if all tensors were stored on the GPU). We ran the training on our
    workstation with 128 GB of RAM and 48 GB of GPU memory, so we had to reduce the
    memory consumption.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以游戏模拟步骤为例，其中游戏通过观察可能的未来场景来进行探索。对于每个状态，如果我们不保存模型生成的动作，而只决定保存用于从策略中采样动作的随机种子，那么每次我们探索一个树节点时，都必须重新计算策略，然后再采样动作。显然，我们决定存储采样的动作以节省时间，并避免在MCTS探索并行化的情况下管理不同进程之间的模型共享。然而，仅仅保存动作还不足以获得足够高效的执行步骤。实际上，将n_steps动作转换为(u,
    v, w)三元组、缩减游戏张量状态并从n_samples动作中创建新的3D张量的时间很容易成为整个训练的瓶颈。其次，我们不想为每个采样动作存储所有可能的未来状态，因为这会对算法使用的内存产生巨大影响。假设我们设置n_samples=32，n=7，N=5，并且记住N是我们想要缩减的方阵积的大小，而n是模型记住的前一个动作的数量。在这种情况下，每个状态张量的形式为(8,
    25, 25, 25)，乘以32会导致每个图节点需要32*8*25*25*25*4字节。现在，考虑到每次扩展阶段的模拟都会生成一个新节点（且n_sim=200），我们会得到仅第一个MCTS节点的最终内存消耗为200*32*8*25*25*25*4
    = 3.2GB。在最坏的情况下，在探索acting max_rank节点（其中max_rank=150）时，这将导致总内存消耗为150 * 3.2GB =
    480GB的RAM内存（或如果所有张量都存储在GPU上，则为GPU内存）。我们在拥有128GB RAM和48GB GPU内存的工作站上运行训练，因此我们必须减少内存消耗。
- en: Since we didn't want to increase the execution time, we adopted an optimization
    that exploits the redundancy in the state tensors produced. In fact, the tensors
    have n-1 previous actions in common, which can then be stored once and not repeated
    for each stored tensor. This results in a memory reduction of 2/7~28%, meaning
    that in the worst-case 137GB can be stored. At this point, by simply pruning the
    unused part of the tree (such as the unselected trajectories) and storing the
    tensors in CPU memory, we were able to avoid any memory error during training.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不想增加执行时间，我们采用了一种优化方法，利用了生成的状态张量中的冗余。实际上，这些张量具有n-1个共同的前动作，这些动作可以被存储一次，而不是为每个存储的张量重复存储。这导致内存减少了2/7~28%，意味着在最坏情况下可以存储137GB。此时，通过简单地修剪树的未使用部分（例如未选择的轨迹）并将张量存储在CPU内存中，我们能够在训练过程中避免任何内存错误。
- en: Next Steps
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一步
- en: With OpenAlphaTensor now being open source, several exciting avenues for further
    development open up.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 随着OpenAlphaTensor现在成为开源，许多令人兴奋的进一步发展途径正在打开。
- en: A natural progression is the fine-tuning of OpenAlphaTensor on target hardware
    devices. This is expected to lead to very competitive computational performance.
    I will publish more about the performance of OpenAlphaTensor on various hardware
    on [GitHub](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/open_alpha_tensor).
    At the time of writing this article, OpenAlphaTensor was undergoing training.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自然的进展是对 OpenAlphaTensor 在目标硬件设备上的微调。这预计将带来非常有竞争力的计算性能。我将在 [GitHub](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/open_alpha_tensor)
    上发布更多关于 OpenAlphaTensor 在各种硬件上表现的内容。在撰写本文时，OpenAlphaTensor 正在进行训练。
- en: Another important advance would be the support for remote compilation, allowing
    users to build algorithms optimized for edge devices. This can be achieved by
    storing the OpenAlphaTensor model on a server, while the matrix multiplication
    algorithm is evaluated on different hardware.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的进展是支持远程编译，使用户能够为边缘设备构建优化的算法。这可以通过将 OpenAlphaTensor 模型存储在服务器上来实现，而矩阵乘法算法则在不同的硬件上进行评估。
- en: It could also be important to extend support for different compilers to compute
    the latency-based reward correction. Different compilers can lead to different
    optimized algorithms on a given hardware. For example, the DeepMind paper showed
    promising results using JAX and the XLA compiler on TPU and Nvidia GPUs. It would
    be interesting to evaluate this using NCCL on Nvidia or LLVM on CPUs.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展对不同编译器的支持以计算基于延迟的奖励修正可能也很重要。不同的编译器可能会在给定的硬件上导致不同的优化算法。例如，DeepMind 的论文显示，使用
    JAX 和 XLA 编译器在 TPU 和 Nvidia GPU 上取得了很有前景的结果。使用 Nvidia 上的 NCCL 或 CPUs 上的 LLVM 来评估这种情况将会很有趣。
- en: Finally, extending the model and training algorithm to support larger matrix
    sizes remains a major open challenge. Currently, OpenAlphaTensor supports a maximum
    matrix size of 5, but it can be applied by splitting larger matrix multiplications
    into groups of tiny MMs with a size smaller than 5\. This approach is suboptimal,
    and performing the reduction directly on the large tensor corresponding to the
    full MM could theoretically lead to better results.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将模型和训练算法扩展以支持更大的矩阵尺寸仍然是一个主要的开放挑战。目前，OpenAlphaTensor 支持的最大矩阵尺寸为 5，但可以通过将较大的矩阵乘法拆分为多个尺寸小于
    5 的小矩阵来应用。这种方法是次优的，直接对对应于完整矩阵乘法的大张量进行归约理论上可能会取得更好的结果。
- en: '**[Diego Fiori](https://www.linkedin.com/in/diego-fiori-/)** is the CTO of
    Nebuly AI, a company committed to making AI optimization part of every developer''s
    toolkit.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Diego Fiori](https://www.linkedin.com/in/diego-fiori-/)** 是 Nebuly AI 的首席技术官，该公司致力于将
    AI 优化纳入每个开发者的工具包中。'
- en: More On This Topic
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Closed Source VS Open Source Image Annotation](https://www.kdnuggets.com/closed-source-vs-open-source-image-annotation)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[闭源 VS 开源图像注释](https://www.kdnuggets.com/closed-source-vs-open-source-image-annotation)'
- en: '[Inside DeepMind’s New Efforts to Use Deep Learning to Advance Mathematics](https://www.kdnuggets.com/2021/12/inside-deepmind-new-efforts-deep-learning-advance-mathematics.html)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深入了解 DeepMind 在利用深度学习推动数学发展的新努力](https://www.kdnuggets.com/2021/12/inside-deepmind-new-efforts-deep-learning-advance-mathematics.html)'
- en: '[Will DeepMind’s AlphaCode Replace Programmers?](https://www.kdnuggets.com/2022/04/deepmind-alphacode-replace-programmers.html)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DeepMind 的 AlphaCode 会取代程序员吗？](https://www.kdnuggets.com/2022/04/deepmind-alphacode-replace-programmers.html)'
- en: '[Cutting Down Implementation Time by Integrating Jupyter and KNIME](https://www.kdnuggets.com/2021/12/cutting-implementation-time-integrating-jupyter-knime.html)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过集成 Jupyter 和 KNIME 缩短实现时间](https://www.kdnuggets.com/2021/12/cutting-implementation-time-integrating-jupyter-knime.html)'
- en: '[Announcing PyCaret 3.0: Open-source, Low-code Machine Learning in Python](https://www.kdnuggets.com/2023/03/announcing-pycaret-30-opensource-lowcode-machine-learning-python.html)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[宣布 PyCaret 3.0：开源、低代码的 Python 机器学习](https://www.kdnuggets.com/2023/03/announcing-pycaret-30-opensource-lowcode-machine-learning-python.html)'
- en: '[Generate Synthetic Time-series Data with Open-source Tools](https://www.kdnuggets.com/2022/06/generate-synthetic-timeseries-data-opensource-tools.html)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用开源工具生成合成时间序列数据](https://www.kdnuggets.com/2022/06/generate-synthetic-timeseries-data-opensource-tools.html)'
