# 使用 Python 实现你自己的 k-最近邻算法

> 原文：[https://www.kdnuggets.com/2016/01/implementing-your-own-knn-using-python.html/3](https://www.kdnuggets.com/2016/01/implementing-your-own-knn-using-python.html/3)

**获取邻居对测试点类别的投票**

最后，使用你刚刚识别的最近邻居，你可以通过多数投票来获得测试实例的类别预测——只需统计在最近邻居中出现最频繁的类别。

```py
from collections import Counter

# 3) given an array of nearest neighbours for a test case, tally up their classes to vote on test case class

def get_majority_vote(neighbours):
    # index 1 is the class
    classes = [neighbour[1] for neighbour in neighbours]
    count = Counter(classes)
    return count.most_common()[0][0]

```

[neighbour[1] for neighbour in neighbours] 只是获取最近邻居的类别（这就是为什么在 _get_tuple_distance 中保持训练实例信息也是好的，而不仅仅是跟踪距离）。

接下来，Counter，这是一个字典子类，用于计数对象的出现次数。试试：

```py
>>> Counter([7,7,7,6,6,9])
Counter({7: 3, 6: 2, 9: 1})

>>> Counter('bananas')
Counter({'a': 3, 'n': 2, 's': 1, 'b': 1})

```

然后，可以使用 Counter 的 most_common 方法返回包含最常见元素及其频率的元组：

```py
>>> Counter('bananas').most_common()
[('a', 3), ('n', 2), ('s', 1), ('b', 1)]

```

以类似的方式，你可以获取最近邻居的类别，统计不同类别标签出现的频率，然后找到最常见的标签。这个最常见的标签将是测试实例的类别预测。

**运行我们的算法**

就是这样。现在，只需将数据加载和这 3 个主要函数与一个主方法串在一起并运行即可。我们将 k 设置为 5，即查看 5 个最近邻居来对新测试实例进行分类。一旦对测试用例的类别做出了预测，获取我们预测准确性报告将是有用的。你可以通过 scikit 的 [accuracy_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) 和 [classification_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) 函数获得这些汇总统计数据。

```py
from sklearn.metrics import classification_report, accuracy_score

# setting up main executable method
def main():

    # load the data and create the training and test sets
    # random_state = 1 is just a seed to permit reproducibility of the train/test split
    iris = load_iris()
    X_train, X_test, y_train, y_test = cross_validation.train_test_split(iris.data, iris.target, test_size=0.4, random_state=1)

    # reformat train/test datasets for convenience
    train = np.array(zip(X_train,y_train))
    test = np.array(zip(X_test, y_test))

    # generate predictions
    predictions = []

    # let's arbitrarily set k equal to 5, meaning that to predict the class of new instances,
    k = 5

    # for each instance in the test set, get nearest neighbours and majority vote on predicted class
    for x in range(len(X_test)):

            print 'Classifying test instance number ' + str(x) + ":",
            neighbours = get_neighbours(training_set=train, test_instance=test[x][0], k=5)
            majority_vote = get_majority_vote(neighbours)
            predictions.append(majority_vote)
            print 'Predicted label=' + str(majority_vote) + ', Actual label=' + str(test[x][1])

    # summarize performance of the classification
    print '\nThe overall accuracy of the model is: ' + str(accuracy_score(y_test, predictions)) + "\n"
    report = classification_report(y_test, predictions, target_names = iris.target_names)
    print 'A detailed classification report: \n\n' + report

if __name__ == "__main__":
    main()

```

这个方法将之前的函数汇总在一起，应该相对易于理解。一个潜在的混淆点可能是脚本的最后部分——与其直接调用 main() 来运行脚本，更有用的是首先检查 `__name__ == "__main__"`。如果你只想从命令行或交互式 shell 中按原样运行此脚本，这样做没有任何区别——当读取源代码时，Python 解释器会将特殊的 `__name__` 变量设置为 `"__main__"` 并运行所有内容。然而，假设你想将这些函数仅仅导入到另一个模块（另一个 .py 文件）中而不运行所有代码。那时，__name__ 将被设置为其他模块的名称。这将允许我们使用 kNN 代码而不会每次都执行它。所以，这个检查允许脚本根据脚本是直接运行还是从其他地方导入而表现不同。

**改进**

在这个实现中，当尝试对新的数据点进行分类时，我们计算了每个测试实例与训练集中每个数据点之间的距离。这种方法效率低下，存在对kNN的改进方法，它们通过细分搜索空间来减少成对计算的数量（例如，参见 [k-d 树](https://www.youtube.com/watch?v=TLxWtXEbtFE)）。对kNN算法的另一个改进是通过根据测试实例与特定邻居的距离加权邻居的重要性。这将使得更近的邻居对类别投票过程产生更大的影响，这在直观上是合理的。

另一个需要记住的点是，尽管我们在这里任意选择了k=5，但我们可以选择其他值（这会影响准确性、噪声敏感性等）。理想情况下，可以通过查看哪个值产生最准确的预测来优化k（参见 [交叉验证](http://scikit-learn.org/stable/modules/cross_validation.html)）。

对kNN的一个很好的概述可以在 [这里](https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/) 阅读。一个更深入的实现，包括加权和搜索树，见 [这里](http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_ml/py_knn/py_knn_understanding/py_knn_understanding.html)。

**完整脚本**

完整的脚本如下：

```py
from sklearn.datasets import load_iris
from sklearn import cross_validation
from sklearn.metrics import classification_report, accuracy_score
from operator import itemgetter
import numpy as np
import math
from collections import Counter

# 1) given two data points, calculate the euclidean distance between them
def get_distance(data1, data2):
    points = zip(data1, data2)
    diffs_squared_distance = [pow(a - b, 2) for (a, b) in points]
    return math.sqrt(sum(diffs_squared_distance))

# 2) given a training set and a test instance, use getDistance to calculate all pairwise distances
def get_neighbours(training_set, test_instance, k):
    distances = [_get_tuple_distance(training_instance, test_instance) for training_instance in training_set]
    # index 1 is the calculated distance between training_instance and test_instance
    sorted_distances = sorted(distances, key=itemgetter(1))
    # extract only training instances
    sorted_training_instances = [tuple[0] for tuple in sorted_distances]
    # select first k elements
    return sorted_training_instances[:k]

def _get_tuple_distance(training_instance, test_instance):
    return (training_instance, get_distance(test_instance, training_instance[0]))

# 3) given an array of nearest neighbours for a test case, tally up their classes to vote on test case class
def get_majority_vote(neighbours):
    # index 1 is the class
    classes = [neighbour[1] for neighbour in neighbours]
    count = Counter(classes)
    return count.most_common()[0][0] 

# setting up main executable method
def main():

    # load the data and create the training and test sets
    # random_state = 1 is just a seed to permit reproducibility of the train/test split
    iris = load_iris()
    X_train, X_test, y_train, y_test = cross_validation.train_test_split(iris.data, iris.target, test_size=0.4, random_state=1)

    # reformat train/test datasets for convenience
    train = np.array(zip(X_train,y_train))
    test = np.array(zip(X_test, y_test))

    # generate predictions
    predictions = []

    # let's arbitrarily set k equal to 5, meaning that to predict the class of new instances,
    k = 5

    # for each instance in the test set, get nearest neighbours and majority vote on predicted class
    for x in range(len(X_test)):

            print 'Classifying test instance number ' + str(x) + ":",
            neighbours = get_neighbours(training_set=train, test_instance=test[x][0], k=5)
            majority_vote = get_majority_vote(neighbours)
            predictions.append(majority_vote)
            print 'Predicted label=' + str(majority_vote) + ', Actual label=' + str(test[x][1])

    # summarize performance of the classification
    print '\nThe overall accuracy of the model is: ' + str(accuracy_score(y_test, predictions)) + "\n"
    report = classification_report(y_test, predictions, target_names = iris.target_names)
    print 'A detailed classification report: \n\n' + report

if __name__ == "__main__":
    main()

```

**想了解更多？查看我们的两天数据科学训练营**：

[https://cambridgecoding.com/datascience-bootcamp](https://cambridgecoding.com/datascience-bootcamp)

**简介：[Natasha Latysheva](http://blog.cambridgecoding.com/author/natlat/)** 是MRC分子生物学实验室的计算生物学博士生。她的研究集中于癌症基因组学、统计网络分析和蛋白质结构。更广泛地说，她的研究兴趣包括数据密集型分子生物学、机器学习（特别是深度学习）和数据科学。

[原文](http://blog.cambridgecoding.com/2016/01/16/machine-learning-under-the-hood-writing-your-own-k-nearest-neighbour-algorithm/)。经允许转载。

**相关：**

+   [Scikit-learn 和 Python 堆栈教程：简介，分类器实现](/2016/01/scikit-learn-tutorials-introduction-classifiers.html)

+   [7 Steps to Mastering Machine Learning With Python](/2015/11/seven-steps-machine-learning-python.html)

+   [7 Steps to Understanding Deep Learning](/2016/01/seven-steps-deep-learning.html)

* * *

## 我们的三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速入门网络安全职业。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你所在组织的 IT

* * *

### 更多相关话题

+   [从理论到实践：构建 k-最近邻分类器](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)

+   [用于分类的最近邻](https://www.kdnuggets.com/2022/04/nearest-neighbors-classification.html)

+   [Scikit-learn 中的 K 最近邻](https://www.kdnuggets.com/2022/07/knearest-neighbors-scikitlearn.html)

+   [LangChain 101：构建你自己的 GPT 驱动应用](https://www.kdnuggets.com/2023/04/langchain-101-build-gptpowered-applications.html)

+   [使用 LlamaIndex 构建你自己的 PandasAI](https://www.kdnuggets.com/build-your-own-pandasai-with-llamaindex)

+   [使用 ChatGPT 的 GPTs 创建你自己的 GPT](https://www.kdnuggets.com/make-your-own-gpts-with-chatgpts-gpts)
