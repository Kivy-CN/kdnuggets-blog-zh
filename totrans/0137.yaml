- en: Unraveling the Power of Chain-of-Thought Prompting in Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/07/power-chain-thought-prompting-large-language-models.html](https://www.kdnuggets.com/2023/07/power-chain-thought-prompting-large-language-models.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Unraveling the Power of Chain-of-Thought Prompting in Large Language Models](../Images/786c67d6492286a8cfa0ded2b2775d7e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by author with Midjourney
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Chain-of-Thought Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have revolutionized the field of artificial intelligence,
    offering unprecedented capabilities in natural language understanding and generation.
    However, their ability to perform complex reasoning tasks has been a subject of
    intense research. One technique that has shown promise in this regard is Chain-of-Thought
    (CoT) prompting. This article explores the intricacies of CoT prompting and its
    implications for the future of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: CoT prompting, as introduced in a [recent paper](https://arxiv.org/pdf/2201.11903.pdf),
    is a method that encourages LLMs to explain their reasoning process. This is achieved
    by providing the model with a few-shot exemplars where the reasoning process is
    explicitly outlined. The LLM is then expected to follow a similar reasoning process
    when answering the prompt. This technique has been found to significantly improve
    the model's performance on tasks that require complex reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key advantages of CoT prompting is its ability to enhance the performance
    of LLMs on tasks that require arithmetic, commonsense, and symbolic reasoning.
    The technique has been shown to yield performance gains, particularly with models
    of approximately 100 billion parameters. Smaller models, however, have been found
    to produce illogical chains of thought, leading to lower accuracy than standard
    prompting.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Chain-of-Thought Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At its core, CoT prompting is about guiding the LLM to think step by step. This
    is achieved by providing the model with a few-shot exemplar that outlines the
    reasoning process. The model is then expected to follow a similar chain of thought
    when answering the prompt. This approach is particularly effective for complex
    tasks that require a series of reasoning steps before a response can be generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a sample CoT prompt (taken from [this article](https://machinelearningmastery.com/prompt-engineering-for-effective-interaction-with-chatgpt/)
    on Machine Learning Mastery), using a few-shot strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q: Joe has 20 eggs. He buys 2 more cartons of eggs. Each carton contains 12
    eggs. How many eggs does Joe have now?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A: Joe started with 20 eggs. 2 cartons of 12 eggs is 24 eggs. 20 + 24 = 44\.
    Therefore, Joe has 44 eggs, and the answer is 44.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Q: Farmer John had 93 chickens. If he sold 20 to Farmer Bill and bought twice
    that number more, how many chickens does Farmer John have now?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'ChatGPT’s response:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Farmer John sold 20 chickens to Farmer Bill, so he was left with 93 – 20
    = 73 chickens. He then bought twice the number of chickens he sold, so he bought
    2 * 20 = 40 chickens. Adding these newly bought chickens to his remaining stick,
    Farmer John now has 73 + 40 = 113 chickens.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: CoT prompting can also be used in a zero-shot setting. This involves adding
    a phrase like "Let's think step by step" to the original prompt, which can also
    be made use of alongside few-shot prompting. This simple addition has been found
    to be effective at improving the model's performance on tasks where there are
    not many examples to use in the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Automating Chain-of-Thought Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While CoT prompting can be effective, it often involves hand-crafting examples,
    which can be time-consuming and may lead to suboptimal solutions. To address this,
    [researchers have proposed](https://arxiv.org/abs/2210.03493) an approach known
    as Automatic Chain-of-Thought (Auto-CoT). This method leverages LLMs to generate
    reasoning chains for demonstrations automatically, thereby eliminating the need
    for manual effort.
  prefs: []
  type: TYPE_NORMAL
- en: 'Auto-CoT consists of two main stages: question clustering and demonstration
    sampling. In the question clustering stage, questions from a given dataset are
    partitioned into a few clusters. In the demonstration sampling stage, a representative
    question is selected from each cluster, and its reasoning chain is generated using
    Zero-Shot-CoT with simple heuristics. This process encourages the model to use
    simple and accurate demonstrations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Auto-CoT process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question clustering: Partition questions of a given dataset into a few clusters'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Demonstration sampling: Select a representative question from each cluster
    and generate its reasoning chain using Zero-Shot-CoT with simple heuristics'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Limitations and Future Directions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While CoT prompting has shown promise, it is not without its limitations. For
    one, it only yields performance gains when used with models of approximately 100
    billion parameters. Smaller models tend to produce illogical chains of thought,
    leading to lower accuracy than standard prompting. Furthermore, the performance
    boosts from CoT prompting are generally proportional to the size of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these limitations, CoT prompting represents a significant step forward
    in the quest to enhance the reasoning capabilities of LLMs. Future research will
    likely focus on refining this technique and exploring ways to make it more effective
    across a wider range of tasks and model sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chain-of-Thought prompting represents a significant advancement in the field
    of artificial intelligence, particularly in enhancing the reasoning capabilities
    of Large Language Models. By encouraging these models to explain their reasoning
    process, CoT prompting has shown promise in improving performance on complex tasks
    that require arithmetic, commonsense, and symbolic reasoning. While the technique
    has its limitations, it opens up exciting possibilities for the future of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: As we continue to push the boundaries of what LLMs can achieve, techniques like
    CoT prompting will play a crucial role. By enabling these models to think step
    by step and explain their reasoning, we can not only improve their performance
    on complex tasks but also gain valuable insights into their inner workings. The
    journey towards fully reasoning LLMs is still long, but with techniques like CoT
    prompting, we are certainly on the right path.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Matthew Mayo](https://www.linkedin.com/in/mattmayo13/)** ([**@mattmayo13**](https://twitter.com/mattmayo13))
    is a Data Scientist and the Editor-in-Chief of KDnuggets, the seminal online Data
    Science and Machine Learning resource. His interests lie in natural language processing,
    algorithm design and optimization, unsupervised learning, neural networks, and
    automated approaches to machine learning. Matthew holds a Master''s degree in
    computer science and a graduate diploma in data mining. He can be reached at editor1
    at kdnuggets[dot]com.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Thought Propagation: An Analogical Approach to Complex Reasoning…](https://www.kdnuggets.com/thought-propagation-an-analogical-approach-to-complex-reasoning-with-large-language-models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Exploring Tree of Thought Prompting: How AI Can Learn to Reason…](https://www.kdnuggets.com/2023/07/exploring-tree-of-thought-prompting-ai-learn-reason-through-search.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Enhancing LLM Reasoning: Unveiling Chain of Code Prompting](https://www.kdnuggets.com/enhancing-llm-reasoning-unveiling-chain-of-code-prompting)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unlocking GPT-4 Summarization with Chain of Density Prompting](https://www.kdnuggets.com/unlocking-gpt-4-summarization-with-chain-of-density-prompting)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Automating the Chain of Thought: How AI Can Prompt Itself to Reason](https://www.kdnuggets.com/2023/07/automating-chain-of-thought-ai-prompt-itself-reason.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Diving into the Pool: Unraveling the Magic of CNN Pooling Layers](https://www.kdnuggets.com/diving-into-the-pool-unraveling-the-magic-of-cnn-pooling-layers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
