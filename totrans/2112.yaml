- en: Vector Databases in AI and LLM Use Cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/vector-databases-in-ai-and-llm-use-cases](https://www.kdnuggets.com/vector-databases-in-ai-and-llm-use-cases)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Vector Databases in AI and LLM Use Cases](../Images/819ba66755bbc22a8231bc910f0f7852.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated with [Ideogram.ai](http://ideogram.ai)
  prefs: []
  type: TYPE_NORMAL
- en: So, you might hear all these Vector Database terms. Some might understand about
    it, and some might not. No worries if you don’t know about them, as Vector Databases
    have only become a more prominent topic in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases have risen in popularity thanks to the introduction of Generative
    AI to the public, especially the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Many LLM products, such as GPT-4 and Gemini, help our work by providing text
    generation capability for our input. Well, vector databases actually play a part
    in these LLM products.
  prefs: []
  type: TYPE_NORMAL
- en: But How did Vector Database work? And what are their relevances in the LLM?
  prefs: []
  type: TYPE_NORMAL
- en: The question above is what we would answer in this article. Well, Let’s explore
    them together.
  prefs: []
  type: TYPE_NORMAL
- en: Vector Databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A vector database is a specialized database storage designed to store, index,
    and query vector data. It’s often optimized for high-dimensional vector data as
    usually it is the output for the machine learning model, especially LLM.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of a Vector Database, the vector is a mathematical representation
    of the data. Each vector consists of an array of numerical points representing
    the data position. Vector is often used in the LLM to represent the text data
    as a vector is easier to process than the text data.
  prefs: []
  type: TYPE_NORMAL
- en: In the LLM space, the model might have a text input and could transform the
    text into a high-dimensional vector representing the semantic and syntactic characteristics
    of the text. This process is what we call Embedding. In simpler terms, embedding
    is a process that transforms text into vectors with numerical data.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding generally uses a Neural Network model called the Embedding Model to
    represent the text in the Embedding Space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use an example text: “I Love Data Science”. Representing them with the
    OpenAI model text-embedding-3-small would result in a vector with 1536 dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The number within the vector is the coordinate within the model’s embedding
    space. Together, they would form a unique representation of the sentence meaning
    coming from the model.
  prefs: []
  type: TYPE_NORMAL
- en: Vector Database would then be responsible for storing these embedding model
    outputs. The user then could query, index, and retrieve the vector as they need.
  prefs: []
  type: TYPE_NORMAL
- en: Maybe that’s enough introduction, and let’s get into a more technical hands-on.
    We would try to establish and store vectors with an open-source vector database
    called [Weaviate](https://weaviate.io/).
  prefs: []
  type: TYPE_NORMAL
- en: Weaviate is a scalable open-source Vector Database that serves as a framework
    to store our vector. We can run Weaviate in instances like Docker or use Weaviate
    Cloud Services (WCS).
  prefs: []
  type: TYPE_NORMAL
- en: 'To start using Weaviate, we need to install the packages using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To make things easier, we would use a sandbox cluster from WCS to act as our
    Vector Database. Weaviate provides a 14-day free cluster that we can use to store
    our vectors without registering any payment method. To do that, you need to register
    on their [WCS console](https://console.weaviate.cloud/signin) initially.
  prefs: []
  type: TYPE_NORMAL
- en: Once within the WCS platform, select Create a Cluster and input your Sandbox
    name. The UI should look like the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Vector Databases in AI and LLM Use Cases](../Images/4f9a0cb53ba4630631309397f523f2d8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Don’t forget to enable authentication, as we also want to access this cluster
    via the WCS API Key. After the cluster is ready, find the API key and Cluster
    URL, which we will use to access the Vector Database.
  prefs: []
  type: TYPE_NORMAL
- en: Once things are ready, we would simulate storing our first vector in the Vector
    Database.
  prefs: []
  type: TYPE_NORMAL
- en: For the Vector Database storing example, I would use the [Book Collection](https://www.kaggle.com/datasets/kononenko/commonlit-texts)
    example dataset from Kaggle. I would only use the top 100 rows and 3 columns (title,
    description, intro).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let’s set aside our data and connect to our Vector Database. First, we need
    to set up a remote connection using the API key and your Cluster URL.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Once you set up your client variable, we will connect to the Weaviate Cloud
    Service and create a class to store the vector. Class in Weaviate is the data
    collection or analogs to the table name in a relational database.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the code above, we connect to the Weaviate Cluster and create a BookCollection
    class. The class object also uses the OpenAI text2vec embedding model to vectorize
    the text data and OpenAI generative module.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to store the text data in a vector database. To do that, you can use
    the following code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Vector Databases in AI and LLM Use Cases](../Images/405ed56bdc827843c29d85199652e61d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: We just successfully stored our dataset in the Vector Database! How easy is
    that?
  prefs: []
  type: TYPE_NORMAL
- en: Now, you might be curious about the use cases for using Vector Databases with
    LLM. That’s what we are going to discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Vector Database and LLM Use Cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A few use cases in which LLM can be applied with Vector Database. Let’s explore
    them together.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Semantic Search is a process of searching for data by using the meaning of the
    query to retrieve relevant results rather than relying solely on the traditional
    keyword-based search.
  prefs: []
  type: TYPE_NORMAL
- en: The process involves the utilization of the LLM Model Embedding of the query
    and performing embedding similarity search into our stored embedded in the vector
    database.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to use Weaviate to perform a semantic search based on a specific query.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the code above, we try to perform a semantic search with Weaviate to find
    the top two books closely related to the query childhood story. The semantic search
    uses the OpenAI embedding model we previously set up. The result is what you can
    see in below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, no direct words about childhood stories are in the result above.
    However, the result is still closely related to a story that aims for children.
  prefs: []
  type: TYPE_NORMAL
- en: Generative Search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Generative Search could be defined as an extension application for the Semantic
    Search. The Generative Search, or Retrieval Augmented Generation (RAG), utilizes
    LLM prompting with the Semantic search that retrieved data from the vector database.
  prefs: []
  type: TYPE_NORMAL
- en: With RAG, the result from the query search is processed to LLM, so we get them
    in the form we want instead of the raw data. Let’s try a simple implementation
    of the RAG with Vector Database.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The result can be seen in the text below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the data content is the same as before but has now been processed
    with OpenAI LLM to provide a short LinkedIn post. In this way, RAG is useful when
    we want specific form output from the data.
  prefs: []
  type: TYPE_NORMAL
- en: Question Answering with RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our previous example, we used a query to get the data we wanted, and RAG
    processed that data into the intended output.
  prefs: []
  type: TYPE_NORMAL
- en: However, we can turn the RAG capability into a question-answering tool. We can
    achieve this by combining them with the LangChain framework.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s install the necessary packages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Then, let’s try to import the packages and initiate the variables we require
    to make QA with RAG work.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the code above, we set up the LLM for the text generation, embedding model,
    and the Weaviate client connection.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we set the Weaviate connection to the Vector Database.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the code above, make the Weaviate Database BookCollection the RAG tool that
    would search the ‘intro’ feature when prompted.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we would create Question Answering Chain from the LangChain with the code
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Everything is now ready. Let’s try out the QA with RAG using the following code
    example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The result is shown in the text below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: With the Vector Database as the place to store all the text data, we can implement
    RAG to perform QA with LangChain. How neat is that?
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A vector database is a specialized storage solution designed to store, index,
    and query vector data. It is often used to store text data and implemented in
    conjunction with Large Language Models (LLMs). This article will try a hands-on
    setup of the Vector Database Weaviate, including example use cases such as Semantic
    Search, Retrieval-Augmented Generation (RAG), and Question Answering with RAG.
  prefs: []
  type: TYPE_NORMAL
- en: '**[](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**[Cornellius Yudha
    Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**** is a data science
    assistant manager and data writer. While working full-time at Allianz Indonesia,
    he loves to share Python and data tips via social media and writing media. Cornellius
    writes on a variety of AI and machine learning topics.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Python Vector Databases and Vector Indexes: Architecting LLM Apps](https://www.kdnuggets.com/2023/08/python-vector-databases-vector-indexes-architecting-llm-apps.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[NoSQL Databases and Their Use Cases](https://www.kdnuggets.com/2023/03/nosql-databases-cases.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are Vector Databases and Why Are They Important for LLMs?](https://www.kdnuggets.com/2023/06/vector-databases-important-llms.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[An Honest Comparison of Open Source Vector Databases](https://www.kdnuggets.com/an-honest-comparison-of-open-source-vector-databases)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Semantic Search with Vector Databases](https://www.kdnuggets.com/semantic-search-with-vector-databases)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Comprehensive Guide to Pinecone Vector Databases](https://www.kdnuggets.com/a-comprehensive-guide-to-pinecone-vector-databases)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
