- en: 'Vanishing Gradient Problem: Causes, Consequences, and Solutions'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度消失问题：原因、后果及解决方案
- en: 原文：[https://www.kdnuggets.com/2022/02/vanishing-gradient-problem.html](https://www.kdnuggets.com/2022/02/vanishing-gradient-problem.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/02/vanishing-gradient-problem.html](https://www.kdnuggets.com/2022/02/vanishing-gradient-problem.html)
- en: '![Vanishing Gradient Problem: Causes, Consequences, and Solutions](../Images/0a76a6fdac48e94ed85b1ee128af5a45.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![梯度消失问题：原因、后果及解决方案](../Images/0a76a6fdac48e94ed85b1ee128af5a45.png)'
- en: Image by Editor
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由编辑提供
- en: The sigmoid function is one of the most popular activations functions used for
    developing deep neural networks. The use of sigmoid function restricted the training
    of deep neural networks because it caused the vanishing gradient problem. This
    caused the neural network to learn at a slower pace or in some cases no learning
    at all. This blog post aims to describe the vanishing gradient problem and explain
    how use of the sigmoid function resulted in it.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数是用于开发深度神经网络的最流行的激活函数之一。Sigmoid 函数的使用限制了深度神经网络的训练，因为它导致了梯度消失问题。这使得神经网络的学习速度变慢，甚至在某些情况下完全无法学习。这篇博客文章旨在描述梯度消失问题，并解释使用
    Sigmoid 函数如何导致这个问题。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Sigmoid function
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sigmoid 函数
- en: 'Sigmoid functions are used frequently in neural networks to activate neurons.
    It is a logarithmic function with a characteristic S shape. The output value of
    the function is between 0 and 1\. The sigmoid function is used for activating
    the output layers in binary classification problems. It is calculated as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数在神经网络中经常用于激活神经元。它是一个具有特征 S 形状的对数函数。函数的输出值介于 0 和 1 之间。Sigmoid 函数用于在二分类问题中激活输出层。它的计算公式如下：
- en: '![Vanishing Gradient Problem, Explained](../Images/d8d2fa8be9674346f77dba068732404e.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![梯度消失问题解析](../Images/d8d2fa8be9674346f77dba068732404e.png)'
- en: On the graph below you can see a comparison between the sigmoid function itself
    and its derivative. First derivatives of sigmoid functions are bell curves with
    values ranging from 0 to 0.25.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图表中，你可以看到 Sigmoid 函数本身及其导数的比较。Sigmoid 函数的一阶导数是钟形曲线，其值范围从 0 到 0.25。
- en: '![Vanishing Gradient Problem, Explained](../Images/c5aeb35ef714f979fd9fdbfb64261d9e.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![梯度消失问题解析](../Images/c5aeb35ef714f979fd9fdbfb64261d9e.png)'
- en: Our knowledge of how neural networks perform forward and backpropagation is
    essential to understanding the vanishing gradient problem.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对神经网络如何执行前向传播和反向传播的了解对于理解梯度消失问题至关重要。
- en: '![Vanishing Gradient Problem, Explained](../Images/862e4f3d9e4cf38da6981eeb8a049e45.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![梯度消失问题解析](../Images/862e4f3d9e4cf38da6981eeb8a049e45.png)'
- en: Forward Propagation
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向传播
- en: The basic structure of a neural network is an input layer, one or more hidden
    layers, and a single output layer. The weights of the network are randomly initialized
    during forward propagation. The input features are multiplied by the corresponding
    weights at each node of the hidden layer, and a bias is added to the net sum at
    each node. This value is then transformed into the output of the node using an
    activation function. To generate the output of the neural network, the hidden
    layer output is multiplied by the weights plus bias values, and the total is transformed
    using another activation function. This will be the predicted value of the neural
    network for a given input value.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的基本结构包括输入层、一个或多个隐藏层和一个输出层。网络的权重在前向传播过程中被随机初始化。输入特征在每个隐藏层节点处与相应的权重相乘，并在每个节点处加上一个偏置。然后，这个值通过激活函数转换为节点的输出。为了生成神经网络的输出，隐藏层输出乘以权重加上偏置值，然后使用另一个激活函数进行转换。这将是神经网络对给定输入值的预测值。
- en: '![Vanishing Gradient Problem, Explained](../Images/51d627e5d0acde8bababdde965dc010c.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![消失梯度问题解释](../Images/51d627e5d0acde8bababdde965dc010c.png)'
- en: Back Propagation
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: As the network generates an output, the loss function(C) indicates how well
    it predicted the output. The network performs back propagation to minimize the
    loss. A back propagation method minimizes the loss function by adjusting the weights
    and biases of the neural network. In this method, the gradient of the loss function
    is calculated with respect to each weight in the network.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当网络生成输出时，损失函数(C)表示它预测输出的准确性。网络执行反向传播以最小化损失。反向传播方法通过调整神经网络的权重和偏置来最小化损失函数。在此方法中，损失函数的梯度相对于网络中的每个权重进行计算。
- en: In back propagation, the new weight(w[new]) of a node is calculated using the
    old weight(w[old]) and product of the learning rate(ƞ) and gradient of the loss
    function ![Vanishing Gradient Problem, Explained](../Images/26acc3e4c6a799aac5744ac88dcdb036.png).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播中，节点的新权重(w[new])是通过旧权重(w[old])和学习率(ƞ)与损失函数梯度的乘积来计算的。![消失梯度问题解释](../Images/26acc3e4c6a799aac5744ac88dcdb036.png)
- en: '![Vanishing Gradient Problem, Explained](../Images/be7ea7065f53e1ede2b5b33d9f3e3cf4.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![消失梯度问题解释](../Images/be7ea7065f53e1ede2b5b33d9f3e3cf4.png)'
- en: With the chain rule of partial derivatives, we can represent gradient of the
    loss function as a product of gradients of all the activation functions of the
    nodes with respect to their weights.Therefore, the updated weights of nodes in
    the network depend on the gradients of the activation functions of each node.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 利用偏导数链式法则，我们可以将损失函数的梯度表示为所有节点的激活函数相对于其权重的梯度的乘积。因此，网络中节点的更新权重依赖于每个节点激活函数的梯度。
- en: For the nodes with sigmoid activation functions, we know that the partial derivative
    of the sigmoid function reaches a maximum value of 0.25\. When there are more
    layers in the network, the value of the product of derivative decreases until
    at some point the partial derivative of the loss function approaches a value close
    to zero, and the partial derivative vanishes. We call this the vanishing gradient
    problem.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有sigmoid激活函数的节点，我们知道sigmoid函数的偏导数最大值为0.25。当网络中层数增加时，导数的乘积值会减少，直到某个点，损失函数的偏导数接近零，偏导数消失。我们称之为消失梯度问题。
- en: With shallow networks, sigmoid function can be used as the small value of gradient
    does not become an issue. When it comes to deep networks, the vanishing gradient
    could have a significant impact on performance. The weights of the network remain
    unchanged as the derivative vanishes. During back propagation, a neural network
    learns by updating its weights and biases to reduce the loss function. In a network
    with vanishing gradient, the weights cannot be updated, so the network cannot
    learn. The performance of the network will decrease as a result.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在浅层网络中，sigmoid函数可以使用，因为梯度的小值不会造成问题。然而，对于深层网络，消失梯度可能对性能产生重大影响。由于导数消失，网络的权重保持不变。在反向传播过程中，神经网络通过更新其权重和偏置来减少损失函数。在具有消失梯度的网络中，权重无法更新，因此网络无法学习。结果是网络性能下降。
- en: Method to overcome the problem
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 克服问题的方法
- en: The vanishing gradient problem is caused by the derivative of the activation
    function used to create the neural network. The simplest solution to the problem
    is to replace the activation function of the network. Instead of sigmoid, use
    an activation function such as ReLU.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度消失问题是由用于创建神经网络的激活函数的导数引起的。解决这个问题的最简单方法是替换网络的激活函数。可以用 ReLU 等激活函数代替 sigmoid。
- en: Rectified Linear Units (ReLU) are activation functions that generate a positive
    linear output when they are applied to positive input values. If the input is
    negative, the function will return zero.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 修正线性单元（ReLU）是激活函数，当它们应用于正输入值时会生成正线性输出。如果输入为负值，该函数将返回零。
- en: '![Vanishing Gradient Problem, Explained](../Images/3955a28d80709d4d297fc54a5546ba89.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![梯度消失问题解释](../Images/3955a28d80709d4d297fc54a5546ba89.png)'
- en: The derivative of a ReLU function is defined as 1 for inputs that are greater
    than zero and 0 for inputs that are negative. The graph shared below indicates
    the derivative of a ReLU function
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 函数的导数定义为对于大于零的输入为 1，对于负输入为 0。下面共享的图表显示了 ReLU 函数的导数。
- en: '![Vanishing Gradient Problem, Explained](../Images/47848f7a08be7ca8124605fd7199f9c0.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![梯度消失问题解释](../Images/47848f7a08be7ca8124605fd7199f9c0.png)'
- en: If the ReLU function is used for activation in a neural network in place of
    a sigmoid function, the value of the partial derivative of the loss function will
    be having values of 0 or 1 which prevents the gradient from vanishing. The use
    of ReLU function thus prevents the gradient from vanishing. The problem with the
    use of ReLU is when the gradient has a value of 0\. In such cases, the node is
    considered as a dead node since the old and new values of the weights remain the
    same. This situation can be avoided by the use of a leaky ReLU function which
    prevents the gradient from falling to the zero value.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在神经网络中用 ReLU 函数代替 sigmoid 函数进行激活，损失函数的偏导数值将为 0 或 1，这防止了梯度消失。因此，ReLU 函数可以防止梯度消失。使用
    ReLU 的问题是，当梯度值为 0 时，节点被视为死节点，因为权重的旧值和新值保持不变。这种情况可以通过使用 leaky ReLU 函数来避免，该函数防止梯度降到零值。
- en: Another technique to avoid the vanishing gradient problem is weight initialization.
    This is the process of assigning initial values to the weights in the neural network
    so that during back propagation, the weights never vanish.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 避免梯度消失问题的另一种技术是权重初始化。这是将初始值分配给神经网络中的权重的过程，以便在反向传播过程中，权重不会消失。
- en: In conclusion, the vanishing gradient problem arises from the nature of the
    partial derivative of the activation function used to create the neural network.
    The problem can bevworse in deep neural networks using Sigmoid activation function.
    It can be significantly reduced by using activation functions like ReLU and leaky
    ReLU.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，梯度消失问题源于用于创建神经网络的激活函数的偏导数的特性。使用 Sigmoid 激活函数的深度神经网络中，这个问题可能会更加严重。通过使用像 ReLU
    和 leaky ReLU 这样的激活函数，可以显著减少这个问题。
- en: '**[Tina Jacob](https://www.linkedin.com/in/tina-jacob-77068253/)** is passionate
    about data science and believes life is about learning and growing no matter what
    it brings.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**[蒂娜·雅各布](https://www.linkedin.com/in/tina-jacob-77068253/)** 对数据科学充满热情，认为生活就是不断学习和成长，无论遇到什么。'
- en: More On This Topic
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解更多相关话题
- en: '[5 Concepts You Should Know About Gradient Descent and Cost Function](https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你应该了解的5个梯度下降和成本函数的概念](https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html)'
- en: '[90% of Today''s Code is Written to Prevent Failure, and That''s a Problem](https://www.kdnuggets.com/2022/07/90-today-code-written-prevent-failure-problem.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90%的今日代码是为了防止失败，这也是一个问题](https://www.kdnuggets.com/2022/07/90-today-code-written-prevent-failure-problem.html)'
- en: '[Back To Basics, Part Dos: Gradient Descent](https://www.kdnuggets.com/2023/03/back-basics-part-dos-gradient-descent.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[回到基础，第二部分：梯度下降](https://www.kdnuggets.com/2023/03/back-basics-part-dos-gradient-descent.html)'
- en: '[Gradient Descent: The Mountain Trekker''s Guide to Optimization with…](https://www.kdnuggets.com/gradient-descent-the-mountain-trekker-guide-to-optimization-with-mathematics)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[梯度下降：优化的登山者指南…](https://www.kdnuggets.com/gradient-descent-the-mountain-trekker-guide-to-optimization-with-mathematics)'
- en: '[The Range of NLP Applications in the Real World: A Different…](https://www.kdnuggets.com/2022/03/different-solution-problem-range-nlp-applications-real-world.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NLP应用的范围：一个不同的…](https://www.kdnuggets.com/2022/03/different-solution-problem-range-nlp-applications-real-world.html)'
- en: '[Genetic Programming in Python: The Knapsack Problem](https://www.kdnuggets.com/2023/01/knapsack-problem-genetic-programming-python.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 Python 解决背包问题的遗传编程](https://www.kdnuggets.com/2023/01/knapsack-problem-genetic-programming-python.html)'
