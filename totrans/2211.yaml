- en: A Brief History of the Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的简史
- en: 原文：[https://www.kdnuggets.com/a-brief-history-of-the-neural-networks](https://www.kdnuggets.com/a-brief-history-of-the-neural-networks)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/a-brief-history-of-the-neural-networks](https://www.kdnuggets.com/a-brief-history-of-the-neural-networks)
- en: '![A Brief History of the Neural Networks](../Images/508823c0bcef0c6e6bade57563bb964d.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络的简史](../Images/508823c0bcef0c6e6bade57563bb964d.png)'
- en: Walkthrough of neural network evolution (image by author).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络演变的全景图（图片作者）。
- en: Neural networks, the fundamental building blocks of artificial intelligence,
    have revolutionized the way we process information, offering a glimpse into the
    future of technology. These complex computational systems, inspired by the intricacies
    of the human brain, have become pivotal in tasks ranging from image recognition
    and natural language understanding to autonomous driving and medical diagnosis.
    As we explore neural networks’ historical evolution, we will uncover their remarkable
    journey of how they have evolved to shape the modern landscape of AI.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络，人工智能的基本构建块，已经彻底改变了我们处理信息的方式，提供了对未来技术的展望。这些复杂的计算系统受人脑复杂性的启发，已成为从图像识别和自然语言理解到自动驾驶和医疗诊断等任务中的关键。随着我们探索神经网络的历史演变，我们将揭示它们如何演变并塑造现代AI的进程。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速通道进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织在IT领域'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: How It All Began?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一切是如何开始的？
- en: Neural networks, the foundational components of deep learning, owe their conceptual
    roots to the intricate biological networks of neurons within the human brain.
    This remarkable concept began with a fundamental analogy, drawing parallels between
    biological neurons and computational networks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络，深度学习的基础组件，其概念根源于人脑中复杂的生物神经网络。这个卓越的概念起初是通过一个基本的类比展开的，将生物神经元与计算网络进行对比。
- en: This analogy centers around the brain, which is composed of roughly 100 billion
    neurons. Each neuron maintains about 7,000 synaptic connections with other neurons,
    creating a complex neural network that underlies human cognitive processes and
    decision making.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类比围绕大脑展开，大脑由大约1000亿个神经元组成。每个神经元与其他神经元保持大约7000个突触连接，形成一个复杂的神经网络，这一网络是人类认知过程和决策制定的基础。
- en: 'Individually, a biological neuron operates through a series of simple electrochemical
    processes. It receives signals from other neurons through its dendrites. When
    these incoming signals add up to a certain level (a predetermined threshold),
    the neuron switches on and sends an electrochemical signal along its axon. This,
    in turn, affects the neurons connected to its axon terminals. The key thing to
    note here is that a neuron''s response is like a binary switch: it either fires
    (activates) or stays quiet, without any in-between states.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 单独来看，生物神经元通过一系列简单的电化学过程来运作。它通过树突接收来自其他神经元的信号。当这些输入信号累计到一定水平（一个预定的阈值）时，神经元会激活并沿其轴突发送电化学信号。这会影响连接到其轴突末端的神经元。需要注意的是，神经元的反应就像一个二进制开关：要么发射（激活），要么保持安静，没有任何中间状态。
- en: '![A Brief History of the Neural Networks](../Images/e0e38b28d799db65f6df851a7b6bd7d6.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络的简史](../Images/e0e38b28d799db65f6df851a7b6bd7d6.png)'
- en: 'Biological neurons were the inspiration for artificial neural networks (image:
    Wikipedia).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经元是人工神经网络的灵感来源（图片来源：维基百科）。
- en: Artificial neural networks, as impressive as they are, remain a far cry from
    even remotely approaching the astonishing intricacies and profound complexities
    of the human brain. Nonetheless, they have demonstrated significant prowess in
    addressing problems that are challenging for conventional computers but appear
    intuitive to human cognition. Some examples are image recognition and predictive
    analytics based on historical data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管人工神经网络非常令人印象深刻，但它们仍然远未接近人脑的惊人复杂性和深刻复杂性。然而，它们在解决那些对传统计算机具有挑战性但对人类认知而言直观的问题上表现出了显著的能力。一些例子包括图像识别和基于历史数据的预测分析。
- en: Now that we've explored the foundational principles of how biological neurons
    function and their inspiration for artificial neural networks, let's journey through
    the evolution of neural network frameworks that have shaped the landscape of artificial
    intelligence.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了生物神经元的基本原理及其对人工神经网络的启发，让我们一起回顾那些塑造了人工智能领域的神经网络框架的演变历程。
- en: FFNN - Feed Forward Neural Network
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FFNN - 前馈神经网络
- en: Feed forward neural networks, often referred to as a multilayer perceptron,
    are a fundamental type of neural networks, whose operation is deeply rooted in
    the principles of information flow, interconnected layers, and parameter optimization.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络，通常被称为多层感知器，是一种基本的神经网络类型，其操作深深根植于信息流动、互连层和参数优化的原则中。
- en: At their core, FFNNs orchestrate a unidirectional journey of information. It
    all begins with the input layer containing *n* neurons, where data is initially
    ingested. This layer serves as the entry point for the network, acting as a receptor
    for the input features that need to be processed. From there, the data embarks
    on a transformative voyage through the network's hidden layers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在核心上，FFNNs 协调了信息的单向流动。一切从包含*n* 个神经元的输入层开始，数据在此被初步接收。该层作为网络的入口点，充当需要处理的输入特征的接收器。从这里开始，数据经过网络的隐藏层进行变换。
- en: One important aspect of FFNNs is their connected structure, which means that
    each neuron in a layer is intricately connected to every neuron in that layer.
    This interconnectedness allows the network to perform computations and capture
    relationships within the data. It's like a communication network where every node
    plays a role in processing information.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: FFNNs 的一个重要方面是其连接结构，这意味着每一层中的每个神经元都与该层中的每个神经元紧密相连。这种互连性使得网络能够进行计算并捕捉数据中的关系。这就像一个通信网络，其中每个节点在处理信息中扮演着角色。
- en: As the data passes through the hidden layers, it undergoes a series of calculations.
    Each neuron in a hidden layer receives inputs from all neurons in the previous
    layer, applies a weighted sum to these inputs, adds a bias term, and then passes
    the result through an activation function (commonly ReLU, Sigmoid, or tanH). These
    mathematical operations enable the network to extract relevant patterns from the
    input, and capture complex, nonlinear relationships within data. This is where
    FFNNs truly excel compared to more shallow ML models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据通过隐藏层时，它会经过一系列计算。每个隐藏层的神经元从前一层的所有神经元接收输入，对这些输入应用加权和，添加一个偏置项，然后通过一个激活函数（通常是
    ReLU、Sigmoid 或 tanH）传递结果。这些数学运算使得网络能够从输入中提取相关模式，并捕捉数据中的复杂非线性关系。正是在这一点上，前馈神经网络（FFNNs）相比于更浅的机器学习模型真正表现出色。
- en: '![A Brief History of the Neural Networks](../Images/1d4e429c0425fe40ea9ada3670e04d51.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络简史](../Images/1d4e429c0425fe40ea9ada3670e04d51.png)'
- en: Architecture of fully-connected feed-forward neural networks (image by author).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 完全连接的前馈神经网络的架构（作者提供的图像）。
- en: However, that's not where it ends. The real power of FFNNs lies in their ability
    to adapt. During training the network adjusts its weights to minimize the difference
    between its predictions and the actual target values. This iterative process,
    often based on optimization algorithms like gradient descent, is called *backpropagation.*
    Backpropagation empowers FFNNs to actually learn from data and improve their accuracy
    in making predictions or classifications.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并不是终点。FFNNs 的真正力量在于它们的适应能力。在训练过程中，网络调整其权重以最小化其预测值与实际目标值之间的差异。这个迭代过程通常基于如梯度下降等优化算法，称为*反向传播*。反向传播使
    FFNNs 能够真正从数据中学习，并提高预测或分类的准确性。
- en: '![A Brief History of the Neural Networks](../Images/68318f05d8cc842cd587ad827a582c7d.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络简史](../Images/68318f05d8cc842cd587ad827a582c7d.png)'
- en: Example [KNIME workflow](https://hub.knime.com/-/spaces/-/latest/~MTi8mL9fZePO9A-S/)
    of FFNN used for the binary classification of certification exams (pass vs fail).
    In the upper branch, we can see the network architecture, which is made of an
    input layer, a fully connected hidden layer with a tanH activation function, and
    an output layer that uses a Sigmoid activation function (image by author).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 [KNIME 工作流](https://hub.knime.com/-/spaces/-/latest/~MTi8mL9fZePO9A-S/) 用于二分类认证考试（通过与失败）。在上部分支中，我们可以看到网络架构，包括一个输入层，一个具有
    tanH 激活函数的全连接隐藏层，以及一个使用 Sigmoid 激活函数的输出层（图像由作者提供）。
- en: While powerful and versatile, FFNNs display some relevant limitations. For example,
    they fail to capture sequentiality and temporal/syntactic dependencies in the
    data –two crucial aspects for tasks in language processing and time series analysis.
    The need to overcome these limitations prompted the evolution of a new type of
    neural network architecture. This transition paved the way for Recurrent Neural
    Networks (RNNs), which introduced the concept of feedback loops to better handle
    sequential data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管强大而多才多艺，FFNN 显示出一些相关的局限性。例如，它们未能捕捉数据中的顺序性和时间/句法依赖性——这两个方面对于语言处理和时间序列分析任务至关重要。克服这些局限性的需求促成了新型神经网络架构的发展。这一过渡为递归神经网络（RNN）的出现铺平了道路，RNN
    引入了反馈回路的概念，以更好地处理顺序数据。
- en: RNN and LSTM - Recurrent Neural Network and Long Short-Term Memory
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 和 LSTM - 递归神经网络和长短期记忆
- en: At their core, RNNs share some similarities with FFNNs. They too are composed
    of layers of interconnected nodes, processing data to make predictions or classifications.
    However, their key differentiator lies in their ability to handle sequential data
    and capture temporal dependencies.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，RNN 与 FFNN 有一些相似之处。它们也由互联的节点层组成，处理数据以进行预测或分类。然而，它们的关键区别在于能够处理顺序数据并捕捉时间依赖性。
- en: In a FFNN, information flows in a single, unidirectional path from the input
    layer to the output layer. This is suitable for tasks where the order of data
    doesn't matter much. However, when dealing with sequences like time series data,
    language, or speech, maintaining context and understanding the order of data is
    crucial. This is where RNNs shine.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在 FFNN 中，信息沿着从输入层到输出层的单一单向路径流动。这适用于数据顺序不太重要的任务。然而，当处理如时间序列数据、语言或语音这样的序列时，保持上下文和理解数据的顺序至关重要。这正是
    RNN 发挥作用的地方。
- en: RNNs introduce the concept of feedback loops. These act as a sort of “memory”
    and allow the network to maintain a hidden state that captures information about
    previous inputs and to influence the current input and output. While traditional
    neural networks assume that inputs and outputs are independent of each other,
    the output of recurrent neural networks depend on the prior elements within the
    sequence. This recurrent connection mechanism makes RNNs particularly fit to handle
    sequences by “remembering” past information.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 引入了反馈回路的概念。这些回路作为一种“记忆”，使得网络能够保持一个隐藏状态，以捕捉关于先前输入的信息，并影响当前的输入和输出。虽然传统神经网络假设输入和输出彼此独立，但递归神经网络的输出依赖于序列中的先前元素。这种递归连接机制使
    RNN 特别适合通过“记住”过去的信息来处理序列。
- en: Another distinguishing characteristic of recurrent networks is that they share
    the same weight parameter within each layer of the network, and those weights
    are adjusted leveraging the backpropagation through time (BPTT) algorithm, which
    is slightly different from traditional backpropagation as it is specific to sequence
    data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 递归网络的另一个区别特征是，它们在网络的每一层中共享相同的权重参数，这些权重通过时间反向传播（BPTT）算法进行调整，该算法与传统的反向传播略有不同，因为它专门针对序列数据。
- en: '![A Brief History of the Neural Networks](../Images/4df008317e01785e412d438e3815e153.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络的简史](../Images/4df008317e01785e412d438e3815e153.png)'
- en: Unrolled representation of RNNs, where each input is enriched with context information
    coming from previous inputs. The color represents the propagation of context information
    (image by author).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 展开的 RNN 表示，其中每个输入都带有来自先前输入的上下文信息。颜色表示上下文信息的传播（图像由作者提供）。
- en: However, traditional RNNs have their limitations. While in theory they should
    be able to capture long-range dependencies, in reality they struggle to do so
    effectively, and can even suffer from the vanishing gradient problem, which hinders
    their ability to learn and remember information over many time steps.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，传统的RNN也有其局限性。虽然理论上它们应该能够捕捉长期依赖关系，但实际上它们往往难以有效做到这一点，甚至可能遭遇梯度消失问题，这阻碍了它们在多个时间步长上学习和记忆信息的能力。
- en: 'This is where Long Short-Term Memory (LSTM) units come into play. They are
    specifically designed to handle these issues by incorporating three gates into
    their structure: the Forget gate, Input gate, and Output gate.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是长短期记忆（LSTM）单元发挥作用的地方。它们通过在结构中引入三个门控：遗忘门、输入门和输出门，专门设计来处理这些问题。
- en: '**Forget gate**: This gate decides which information from the time step should
    be discarded or forgotten. By examining the cell state and the current input,
    it determines which information is irrelevant for making predictions in the present.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗忘门**：该门控决定应丢弃或忘记时间步长中的哪些信息。通过检查单元状态和当前输入，它决定哪些信息在当前预测中是无关的。'
- en: '**Input gate**: This gate is responsible for incorporating information into
    the cell state. It takes into account both the input and the previous cell state
    to decide what new information should be added to enhance its state.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入门**：该门控负责将信息引入单元状态。它考虑输入和先前的单元状态，以决定应添加哪些新信息来增强其状态。'
- en: '**Output gate**: This gate concludes what output will be generated by the LSTM
    unit. It considers both the current input and the updated cell state to produce
    an output that can be utilized for predictions or passed on to time steps.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出门**：该门控决定LSTM单元将生成什么输出。它考虑当前输入和更新后的单元状态，以产生可以用于预测或传递给时间步长的输出。'
- en: '![A Brief History of the Neural Networks](../Images/8ddc6cb11b21d6b3bc8e97d76059f9cc.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络简史](../Images/8ddc6cb11b21d6b3bc8e97d76059f9cc.png)'
- en: Visual representation of Long-Short Term Memory units ([image](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
    by Christopher Olah).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 长短期记忆单元的可视化表示（[图片](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
    由 Christopher Olah 提供）。
- en: '![A Brief History of the Neural Networks](../Images/8c59c20f314a5a80e69b6f5e7e2383d6.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络简史](../Images/8c59c20f314a5a80e69b6f5e7e2383d6.png)'
- en: Example [KNIME workflow](https://hub.knime.com/-/spaces/-/latest/~uyVWG2YbtsOhdp5v/)
    of RNNs with LSTM units used for a multi-class sentiment prediction (positive,
    negative, neutral). The upper branch defines the network architecture using an
    input layer to handle strings of different lengths, an embedding layer, an LSTM
    layer with several units, and a fully connected output layer with a Softmax activation
    function to return predictions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 [KNIME 工作流](https://hub.knime.com/-/spaces/-/latest/~uyVWG2YbtsOhdp5v/) 使用LSTM单元的RNN用于多类别情感预测（积极、消极、中立）。上支定义了网络架构，包括一个输入层来处理不同长度的字符串，一个嵌入层，一个包含多个单元的LSTM层，以及一个全连接输出层，配有Softmax激活函数以返回预测结果。
- en: In summary, RNNs, and especially LSTM units, are tailored for sequential data,
    allowing them to maintain memory and capture temporal dependencies, which is a
    critical capability for tasks like natural language processing, speech recognition,
    and time series prediction.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，RNN（递归神经网络），特别是LSTM（长短期记忆）单元，专为序列数据设计，能够保持记忆并捕捉时间依赖关系，这对于自然语言处理、语音识别和时间序列预测等任务至关重要。
- en: As we shift from RNNs capturing sequential dependencies, the evolution continues
    with Convolutional Neural Networks (CNNs). Unlike RNNs, CNNs excel at spatial
    feature extraction from structured grid-like data, making them ideal for image
    and pattern recognition tasks. This transition reflects the diverse applications
    of neural networks across different data types and structures.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从RNN捕捉序列依赖关系转变时，卷积神经网络（CNN）不断发展。与RNN不同，CNN在从结构化网格状数据中提取空间特征方面表现优异，使其非常适合图像和模式识别任务。这种过渡反映了神经网络在不同数据类型和结构中的多样化应用。
- en: CNN - Convolutional Neural Network
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN - 卷积神经网络
- en: CNNs are a special breed of neural networks, particularly well-suited for processing
    image data, such as 2D images or even 3D video data. Their architecture relies
    on a multilayered feed-forward neural network with at least one convolutional
    layer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: CNN（卷积神经网络）是一种特殊类型的神经网络，特别适合处理图像数据，如二维图像甚至三维视频数据。它们的架构依赖于一个多层前馈神经网络，至少包含一个卷积层。
- en: What makes CNNs stand out is their network connectivity and approach to feature
    extraction, which allows them to automatically identify relevant patterns in the
    data. Unlike traditional FFNNs, which connect every neuron in one layer to every
    neuron in the next, CNNs employ a sliding window known as a *kernel* or *filter*.
    This sliding window scans across the input data and is especially powerful for
    tasks where spatial relationships matter, like identifying objects in images or
    tracking motion in videos. As the kernel is moved across the image, a convolution
    operation is performed between the kernel and the pixel values (from a strictly
    mathematical standpoint, this operation is a cross correlation), and a nonlinear
    activation function, usually ReLU, is applied. This produces a high value if the
    feature is in the image patch and a small value if it is not.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs的独特之处在于它们的网络连接性和特征提取方法，这使得它们能够自动识别数据中的相关模式。与传统的FFNNs不同，CNNs不是将一层中的每个神经元与下一层中的每个神经元连接，而是采用称为*卷积核*或*滤波器*的滑动窗口。这个滑动窗口扫描输入数据，特别适用于空间关系重要的任务，如图像中的对象识别或视频中的运动跟踪。当卷积核在图像上移动时，会在卷积核与像素值之间执行卷积操作（从严格的数学角度看，这个操作是交叉相关），并应用非线性激活函数，通常是ReLU。如果特征存在于图像补丁中，则产生较高的值，如果不存在，则产生较小的值。
- en: Together with the kernel, the addition and fine-tuning of hyperparameters, such
    as *stride* (i.e., the number of pixels by which we slide the kernel) and *dilation
    rate* (i.e., the spaces between each kernel cell), allows the network to focus
    on specific features, recognizing patterns and details in specific regions without
    considering the entire input at once.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与卷积核一起，超参数的添加和微调，如*步幅*（即我们滑动卷积核的像素数）和*扩张率*（即每个卷积核单元之间的间隔），使网络能够关注特定特征，在特定区域识别模式和细节，而无需一次考虑整个输入。
- en: '![A Brief History of the Neural Networks](../Images/f9987172a7a399a01e28c8d6b8589a8f.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络简史](../Images/f9987172a7a399a01e28c8d6b8589a8f.png)'
- en: Convolution operation with stride length = 2 ([GIF](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)
    by Sumit Saha).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 带有步幅长度 = 2 的卷积操作（[GIF](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)由Sumit
    Saha提供）。
- en: Some kernels may specialize in detecting edges or corners, while others might
    be tuned to recognize more complex objects like cats, dogs, or street signs within
    an image. By stacking together several convolutional and pooling layers, CNNs
    build a hierarchical representation of the input, gradually abstracting features
    from low-level to high-level, just as our brains process visual information.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一些卷积核可能专门用于检测边缘或角落，而其他的可能会被调整以识别图像中的更复杂对象，如猫、狗或街道标志。通过堆叠多个卷积层和池化层，CNNs构建了输入的层次化表示，逐渐将特征从低级别抽象到高级别，就像我们的大脑处理视觉信息一样。
- en: '![A Brief History of the Neural Networks](../Images/0a7aa9d80f125b97f8cf3d9278899708.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络简史](../Images/0a7aa9d80f125b97f8cf3d9278899708.png)'
- en: Example [KNIME workflow](https://hub.knime.com/-/spaces/-/latest/~RZjun0ed5Uu11SaE/)
    of CNN for binary image classification (cats vs dogs). The upper branch defines
    the network architecture using a series of convolutional layers and max pooling
    layers for automatic feature extraction from images. A flatten layer is then used
    to prepare the extracted features as a unidimensional input for the FFNN to perform
    a binary classification.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 示例[KNIME工作流程](https://hub.knime.com/-/spaces/-/latest/~RZjun0ed5Uu11SaE/)用于二进制图像分类（猫与狗）。上层分支通过一系列卷积层和最大池化层定义网络架构，以自动从图像中提取特征。然后，使用展平层将提取的特征准备为一维输入，供FFNN进行二进制分类。
- en: 'While CNNs excel at feature extraction and have revolutionized computer vision
    tasks, they act as passive observers, for they are not designed to generate new
    data or content. This is not an inherent limitation of the network per se but
    having a powerful engine and no fuel makes a fast car useless. Indeed, real and
    meaningful image and video data tend to be hard and expensive to collect and tend
    to face copyright and data privacy restrictions. This constraint led to the development
    of a novel paradigm that builds on CNNs but marks a leap from image classification
    to creative synthesis: Generative Adversarial Networks (GANs).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 CNNs 在特征提取方面表现出色，并且已经彻底改变了计算机视觉任务，但它们只是被动观察者，因为它们并不设计用来生成新的数据或内容。这并不是网络本身的固有限制，而是拥有强大引擎却没有燃料使得快速汽车变得无用。实际上，真实和有意义的图像和视频数据往往难以收集且成本高昂，并且往往面临版权和数据隐私的限制。这一限制促使了一个新颖范式的开发，它建立在
    CNNs 的基础上，但从图像分类跃迁到创意合成：生成对抗网络（GANs）。
- en: GAN - Generative Adversarial Networks
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAN - 生成对抗网络
- en: 'GANs are a particular family of neural networks whose primary, but not the
    only, purpose is to produce synthetic data that closely mimics a given dataset
    of real data. Unlike most neural networks, GANs’ ingenious architectural design
    consisting of two core models:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: GANs 是神经网络中的一个特定家族，其主要但不是唯一的目的，是生成与给定真实数据集非常相似的合成数据。与大多数神经网络不同，GANs 的独特架构设计由两个核心模型组成：
- en: '**Generator** model: The first player in this neural network duet is the generator
    model. This component is tasked with a fascinating mission: given random noise
    or input vectors, it strives to create artificial samples that are as close to
    resembling real samples as possible. Imagine it as an art forger, attempting to
    craft paintings that are indistinguishable from masterpieces.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成器**模型：这个神经网络二重奏的第一位成员是生成器模型。这个组件肩负着一个令人着迷的任务：给定随机噪声或输入向量，它努力创建与真实样本尽可能相似的人工样本。可以把它想象成一个艺术伪造者，试图创作出与名画无法区分的伪作。'
- en: '**Discriminator** model: Playing the adversary role is the discriminator model.
    Its job is to differentiate between the generated samples produced by the generator
    and the authentic samples from the original dataset. Think of it as an art connoisseur,
    trying to spot the forgeries among the genuine artworks.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**鉴别器**模型：扮演对立角色的是鉴别器模型。它的工作是区分生成器生成的样本和原始数据集中真实的样本。可以把它想象成一个艺术鉴赏家，试图在真正的艺术作品中识别伪作。'
- en: 'Now, here''s where the magic happens: GANs engage in a continuous, adversarial
    dance. The generator aims to improve its artistry, continually fine-tuning its
    creations to become more convincing. Meanwhile, the discriminator becomes a sharper
    detective, honing its ability to tell the real from the fake.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，魔法发生的地方是：GANs 进行持续的对抗性舞蹈。生成器旨在提升其艺术水平，不断微调其创作，使其更具说服力。同时，鉴别器则成为更敏锐的侦探，磨练其区分真实与虚假的能力。
- en: '![A Brief History of the Neural Networks](../Images/964899440aa26432d166399fe4e76282.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络简史](../Images/964899440aa26432d166399fe4e76282.png)'
- en: GAN architecture (image by author).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 架构（图像作者提供）。
- en: As training progresses, this dynamic interplay between the generator and discriminator
    leads to a fascinating outcome. The generator strives to generate samples that
    are so realistic that even the discriminator can't tell them apart from the genuine
    ones. This competition drives both components to refine their abilities continuously.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练的进行，生成器和鉴别器之间的动态互动会产生令人着迷的结果。生成器努力生成足够逼真的样本，以至于连鉴别器都无法将它们与真实样本区分开来。这种竞争驱使两个组件不断提高各自的能力。
- en: The result? A generator that becomes astonishingly adept at producing data that
    appears authentic, be it images, music, or text. This capability has led to remarkable
    applications in various fields, including image synthesis, data augmentation,
    image-to-image translation, and image editing.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 结果？生成器变得异常擅长生成看起来真实的数据，无论是图像、音乐还是文本。这种能力在各种领域中产生了显著的应用，包括图像合成、数据增强、图像到图像的翻译和图像编辑。
- en: '![A Brief History of the Neural Networks](../Images/b3c73eab4c946677d04b855f07a9a4da.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络简史](../Images/b3c73eab4c946677d04b855f07a9a4da.png)'
- en: Example [KNIME workflow](https://hub.knime.com/-/spaces/-/latest/~L4rc__X_CMzL1ubO/)
    of GANs for the generation of synthetic images (i.e., animals, human faces and
    Simpson characters).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 [KNIME 工作流](https://hub.knime.com/-/spaces/-/latest/~L4rc__X_CMzL1ubO/) 用于生成合成图像（例如动物、人脸和辛普森角色）。
- en: GANs pioneered realistic image and video content creation by pitting a generator
    against a discriminator. Extending the need for creativity and advanced operations
    from image to sequential data, models for more sophisticated natural language
    understanding, machine translation, and text generation were introduced. This
    initiated the development of Transformers,  a remarkable deep neural network architecture
    that not only outperformed previous architectures by effectively capturing long-range
    language dependencies and semantic context, but also became the undisputed foundation
    of the most recent AI-driven applications.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: GANs开创了现实图像和视频内容创作的先河，通过对抗生成器和判别器来实现。在图像到序列数据的创意和高级操作需求不断扩展的背景下，推出了更为复杂的自然语言理解、机器翻译和文本生成模型。这促成了Transformers的开发，这是一种显著的深度神经网络架构，它不仅通过有效捕捉长期语言依赖关系和语义上下文超越了以前的架构，还成为了最新AI驱动应用的无可争议的基础。
- en: Transformers
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformers
- en: 'Developed in 2017, Transformers boast a unique feature that allows them to
    replace traditional recurrent layers: a self-attention mechanism that allows them
    to model intricate relationships between all words in a document, regardless of
    their position. This makes Transformers excellent at tackling the challenge of
    long-range dependencies in natural language. Transformer architectures consist
    of two main building blocks:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年开发的Transformers拥有一个独特的特点，使其能够取代传统的递归层：一种自注意力机制，使它们能够建模文档中所有单词之间的复杂关系，而不管它们的位置。这使得Transformers在处理自然语言中的长期依赖问题时表现出色。Transformer架构由两个主要构建块组成：
- en: '***   **Encoder.** Here the input sequence is embedded into vectors and then
    is exposed to the self-attention mechanism. The latter computes attention scores
    for each token, determining its importance in relation to others. These scores
    are used to create weighted sums, which are fed into a FFNN to generate context-aware
    representations for each token. Multiple encoder layers repeat this process, enhancing
    the model''s ability to capture hierarchical and contextual information.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '***   **编码器。** 在这里，输入序列被嵌入到向量中，然后暴露于自注意力机制中。后者计算每个令牌的注意力分数，确定其相对于其他令牌的重要性。这些分数用于创建加权和，输入到FFNN中生成每个令牌的上下文感知表示。多个编码器层重复这一过程，增强模型捕捉层次结构和上下文信息的能力。'
- en: '**Decoder.** This block is responsible for generating output sequences and
    follows a similar process to that of the encoder. It is able to place the proper
    focus on and understand the encoder''s output and its own past tokens during each
    step, ensuring accurate generation by considering both input context and previously
    generated output.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器。** 这个模块负责生成输出序列，其过程类似于编码器。它能够在每一步上正确关注和理解编码器的输出及其自身的过去令牌，通过考虑输入上下文和之前生成的输出，确保准确生成。'
- en: '![A Brief History of the Neural Networks](../Images/11a9c79661ed63fe13d905a0ec847112.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络的简要历史](../Images/11a9c79661ed63fe13d905a0ec847112.png)'
- en: 'Transformer model architecture ([image](https://arxiv.org/abs/1706.03762) by:
    Vaswani et al., 2017).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer模型架构（[图片](https://arxiv.org/abs/1706.03762)作者：Vaswani等，2017年）。
- en: 'Consider this sentence: "I arrived at the bank after crossing the river”. The
    word "bank" can have two meanings –either a financial institution or the edge
    of a river. Here''s where transformers shine. They can swiftly focus on the word
    "river" to disambiguate "bank" by comparing "bank" to every other word in the
    sentence and assigning attention scores. These scores determine the influence
    of each word on the next representation of "bank". In this case, "river" gets
    a higher score, effectively clarifying the intended meaning.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 例如这句话：“我在过河后到达了银行”。单词“bank”可以有两种含义——金融机构或河岸。这里是transformers的优势所在。它们能够迅速关注单词“river”来消除“bank”的歧义，通过将“bank”与句子中的其他每个单词进行比较，并分配注意力分数。这些分数决定了每个单词对“bank”下一个表示的影响。在这种情况下，“river”获得了更高的分数，有效地澄清了意图。
- en: To work that well, Transformers rely on millions of trainable parameters, require
    large corpora of texts and sophisticated training strategies. One notable training
    approach employed with Transformers is *masked language modeling (MLM)*. During
    training, specific tokens within the input sequence are randomly masked, and the
    model's objective is to predict these masked tokens accurately. This strategy
    encourages the model to grasp contextual relationships between words because it
    must rely on the surrounding words to make accurate predictions. This approach,
    popularized by the BERT model, has been instrumental in achieving state-of-the-art
    results in various NLP tasks.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现如此高效，Transformers 依赖于数百万个可训练参数，需要大量的文本语料库和复杂的训练策略。Transformers 使用的一个显著训练方法是
    *掩码语言建模（MLM）*。在训练过程中，输入序列中的特定词元会被随机掩盖，模型的目标是准确预测这些掩盖的词元。这种策略鼓励模型掌握单词之间的上下文关系，因为它必须依赖周围的单词来做出准确的预测。这种方法由
    BERT 模型推广，对于在各种 NLP 任务中取得最先进的结果起到了关键作用。
- en: An alternative to MLM for Transformers is *autoregressive modeling*. In this
    method, the model is trained to generate one word at a time while conditioning
    on previously generated words. Autoregressive models like GPT (Generative Pre-trained
    Transformer) follow this methodology and excel in tasks where the goal is to predict
    unidirectionally the next most suitable word, such as free text generation, question
    answering and text completion.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 的 MLM 替代方法是 *自回归建模*。在这种方法中，模型被训练为一次生成一个单词，同时考虑先前生成的单词。像 GPT（Generative
    Pre-trained Transformer）这样的自回归模型遵循这种方法，并在目标是单向预测下一个最合适的单词的任务中表现出色，例如自由文本生成、问答和文本补全。
- en: Furthermore, to compensate for the need for extensive text resources, Transformers
    excel in parallelization, meaning they can process data during training faster
    than traditional sequential approaches like RNNs or LSTM units. This efficient
    computation reduces training time and has led to groundbreaking applications in
    natural language processing, machine translation, and more.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了弥补对大量文本资源的需求，Transformers 在并行化方面表现出色，这意味着它们可以比传统的序列方法（如 RNN 或 LSTM 单元）更快地处理训练数据。这种高效的计算减少了训练时间，并导致了在自然语言处理、机器翻译等领域的突破性应用。
- en: A pivotal Transformer model developed by Google in 2018 that made a substantial
    impact is BERT (Bidirectional Encoder Representations from Transformers). BERT
    relied on MLM training and introduced the concept of bidirectional context, meaning
    it considers both the left and right context of a word when predicting the masked
    token. This bidirectional approach significantly enhanced the model's understanding
    of word meanings and contextual nuances, establishing new benchmarks for natural
    language understanding and a wide array of downstream NLP tasks.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Google 于 2018 年开发的一个关键 Transformer 模型是 BERT（Bidirectional Encoder Representations
    from Transformers）。BERT 依赖于 MLM 训练，并引入了双向上下文的概念，意味着它在预测掩盖的词元时会考虑单词的左侧和右侧上下文。这种双向方法显著增强了模型对单词含义和上下文细微差别的理解，为自然语言理解和各种下游
    NLP 任务建立了新的基准。
- en: '![A Brief History of the Neural Networks](../Images/3dd394890b99cfdb7cc740cbea6e9cdc.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络的简要历史](../Images/3dd394890b99cfdb7cc740cbea6e9cdc.png)'
- en: Example [KNIME workflow](https://hub.knime.com/-/spaces/-/latest/~3VGz0gdnmIy-DfT4/)
    of BERT for multi-class sentiment prediction (positive, negative, neutral). Minimal
    preprocessing is performed and the pretrained BERT model with fine-tuning is leveraged.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 [KNIME 工作流](https://hub.knime.com/-/spaces/-/latest/~3VGz0gdnmIy-DfT4/) 用于
    BERT 的多类情感预测（积极、消极、中立）。进行了最小预处理，并利用了经过微调的预训练 BERT 模型。
- en: On the heels of Transformers that introduced powerful self-attention mechanisms,
    the growing demand for versatility in applications and performing complex natural
    language tasks, such as document summarization, text editing, or code generation,
    necessitated the development of *large language models*. These models employ deep
    neural networks with billions of parameters to excel in such tasks and meet the
    evolving requirements of the data analytics industry.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在引入强大自注意力机制的 Transformers 之后，对应用的多样性和执行复杂自然语言任务（如文档摘要、文本编辑或代码生成）的日益增长的需求，促使了
    *大型语言模型* 的发展。这些模型使用具有数十亿参数的深度神经网络，以在这些任务中表现出色，并满足数据分析行业不断变化的需求。
- en: LLM - Large Language Model
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM - 大型语言模型
- en: Large language models (LLMs) are a revolutionary category of multi-purpose and
    multi-modal (accepting image, audio and text inputs) deep neural networks that
    have garnered significant attention in recent years. The adjective *large* stems
    from their vast size, as they encompass billions of trainable parameters. Some
    of the most well-known examples include OpenAI’s ChatGTP, Google’s Bard or Meta’s
    LLaMa.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）是一类革命性的多用途和多模态（接受图像、音频和文本输入）深度神经网络，近年来获得了大量关注。形容词*大型*源于它们的巨大规模，因为它们包含了数十亿的可训练参数。一些最知名的例子包括
    OpenAI 的 ChatGPT、谷歌的 Bard 或 Meta 的 LLaMa。
- en: What sets LLMs apart is their unparalleled ability and flexibility to process
    and generate human-like text. They excel in natural language understanding and
    generation tasks, ranging from text completion and translation to question answering
    and content summarization. The key to their success lies in their extensive training
    on massive text corpora, allowing them to capture a rich understanding of language
    nuances, context, and semantics.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 的特点在于它们处理和生成类似人类文本的能力和灵活性。它们在自然语言理解和生成任务中表现出色，从文本完成和翻译到问答和内容总结。它们成功的关键在于对海量文本语料库的广泛训练，使它们能够捕捉语言的细微差别、上下文和语义。
- en: These models employ a deep neural architecture with multiple layers of self-attention
    mechanisms, enabling them to weigh the importance of different words and phrases
    in a given context. This dynamic adaptability makes them exceptionally proficient
    in processing inputs of various types, comprehending complex language structures,
    and generating outputs based on human-defined prompts.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型采用了深度神经网络架构，拥有多个自注意力机制的层，使它们能够在给定的上下文中权衡不同词语和短语的重要性。这种动态适应性使它们在处理各种类型的输入、理解复杂语言结构以及生成基于人类定义的提示的输出方面表现得非常出色。
- en: '![A Brief History of the Neural Networks](../Images/bd116db787a48603b76968008432a50a.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络的简史](../Images/bd116db787a48603b76968008432a50a.png)'
- en: Example [KNIME workflow](https://hub.knime.com/-/spaces/-/latest/~s9uceqYIt_tUKJQc/)
    of creating an AI assistant that relies on OpenAI’s ChatGPT and a vector store
    with custom documents to answer domain-specific questions.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 [KNIME 工作流](https://hub.knime.com/-/spaces/-/latest/~s9uceqYIt_tUKJQc/) 用于创建一个依赖于
    OpenAI 的 ChatGPT 和一个包含自定义文档的向量存储的 AI 助手，以回答特定领域的问题。
- en: LLMs have paved the way for a multitude of applications across various industries,
    from healthcare and finance to entertainment and customer service. They've even
    sparked new frontiers in creative writing and storytelling.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 为多个行业的应用铺平了道路，从医疗保健和金融到娱乐和客户服务。它们甚至激发了创意写作和讲故事的新领域。
- en: However, their enormous size, resource-intensive training processes and potential
    copyright infringements for generated content have also raised concerns about
    ethical usage, environmental impact, and accessibility. Lastly, while increasingly
    enhanced, LLMs may contain some serious flaws, such as “hallucinating” incorrect
    facts, being biased, gullible, or persuaded into creating toxic content.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它们巨大的规模、资源密集型的训练过程以及生成内容可能存在的版权问题也引发了对伦理使用、环境影响和可访问性的担忧。最后，尽管不断增强，LLMs 可能仍存在一些严重缺陷，如“幻想”错误事实、存在偏见、易受影响或生成有害内容。
- en: Is there an End?
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这是否会有终点？
- en: 'The evolution of neural networks, from their humble beginnings to the uprising
    of large language models, raises a profound philosophical question: Will this
    journey ever come to an end?'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的演变，从其卑微的起点到大型语言模型的兴起，引发了一个深刻的哲学问题：这段旅程会永远结束吗？
- en: The trajectory of technology has always been marked by relentless advancement.
    Each milestone only serves as a stepping stone to the next innovation. As we strive
    to create machines that can replicate human cognition and understanding, it's
    tempting to ponder whether there's an ultimate destination, a point where we say,
    "This is it; we've reached the pinnacle."
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 技术的发展轨迹始终以不断的进步为特征。每一个里程碑只是通向下一次创新的垫脚石。当我们努力创建能够复制人类认知和理解的机器时，我们不禁要思考是否存在一个*终极*目标，一个我们可以说“就是这样；我们已达巅峰”的点。
- en: However, the essence of human curiosity and the boundless complexities of the
    natural world suggest otherwise. Just as our understanding of the universe continually
    deepens, the quest to develop more intelligent, capable, and ethical neural networks
    may be an endless journey.Walkthrough of neural network evolution (image by author).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，人类的好奇心和自然世界的无限复杂性却另有启示。正如我们对宇宙的理解不断深化，开发更智能、更有能力、更具伦理的神经网络的追求可能是一个无尽的旅程。（神经网络进化过程概述，作者提供的图像）
- en: Neural networks, the fundamental building blocks of artificial intelligence,
    have revolutionized the way we process information, offering a glimpse into the
    future of technology. These complex computational systems, inspired by the intricacies
    of the human brain, have become pivotal in tasks ranging from image recognition
    and natural language understanding to autonomous driving and medical diagnosis.
    As we explore neural networks’ historical evolution, we will uncover their remarkable
    journey of how they have evolved to shape the modern landscape of AI.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络，作为人工智能的基本构建块，已经彻底改变了我们处理信息的方式，为未来技术提供了一瞥。这些复杂的计算系统，受到人脑复杂性的启发，在从图像识别和自然语言理解到自动驾驶和医疗诊断等任务中发挥了关键作用。在探索神经网络的历史进化过程中，我们将揭示它们如何演变以塑造现代人工智能的景观。
- en: How It All Began?
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一切是如何开始的？
- en: Neural networks, the foundational components of deep learning, owe their conceptual
    roots to the intricate biological networks of neurons within the human brain.
    This remarkable concept began with a fundamental analogy, drawing parallels between
    biological neurons and computational networks.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络，作为深度学习的基础组件，其概念根源于人脑中复杂的生物神经网络。这一了不起的概念始于一个基本的类比，将生物神经元与计算网络进行了类比。
- en: This analogy centers around the brain, which is composed of roughly 100 billion
    neurons. Each neuron maintains about 7,000 synaptic connections with other neurons,
    creating a complex neural network that underlies human cognitive processes and
    decision making.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类比围绕着大脑展开，大脑由大约1000亿个神经元组成。每个神经元与其他神经元保持约7000个突触连接，形成了一个复杂的神经网络，支撑着人类的认知过程和决策制定。
- en: 'Individually, a biological neuron operates through a series of simple electrochemical
    processes. It receives signals from other neurons through its dendrites. When
    these incoming signals add up to a certain level (a predetermined threshold),
    the neuron switches on and sends an electrochemical signal along its axon. This,
    in turn, affects the neurons connected to its axon terminals. The key thing to
    note here is that a neuron''s response is like a binary switch: it either fires
    (activates) or stays quiet, without any in-between states.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 单个生物神经元通过一系列简单的电化学过程来运作。它通过树突接收来自其他神经元的信号。当这些传入信号累加到一定水平（预定阈值）时，神经元就会被激活，并沿着轴突发送电化学信号。这会影响连接到其轴突末梢的神经元。需要注意的是，神经元的反应像一个二进制开关：它要么激活，要么保持安静，没有中间状态。
- en: 'Biological neurons were the inspiration for artificial neural networks (image:
    Wikipedia).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经元是人工神经网络的灵感来源（图像：维基百科）。
- en: Artificial neural networks, as impressive as they are, remain a far cry from
    even remotely approaching the astonishing intricacies and profound complexities
    of the human brain. Nonetheless, they have demonstrated significant prowess in
    addressing problems that are challenging for conventional computers but appear
    intuitive to human cognition. Some examples are image recognition and predictive
    analytics based on historical data.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管人工神经网络令人印象深刻，但它们仍远未接近人脑的惊人复杂性和深奥复杂性。然而，它们在解决对传统计算机具有挑战性但对人类认知直观的问题上表现出了显著的能力。比如图像识别和基于历史数据的预测分析。
- en: Now that we've explored the foundational principles of how biological neurons
    function and their inspiration for artificial neural networks, let's journey through
    the evolution of neural network frameworks that have shaped the landscape of artificial
    intelligence.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了生物神经元功能的基础原理及其对人工神经网络的启发，接下来让我们回顾神经网络框架的进化历程，这些框架塑造了人工智能的领域。
- en: FFNN - Feed Forward Neural Network
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FFNN - 前馈神经网络
- en: Feed forward neural networks, often referred to as a multilayer perceptron,
    are a fundamental type of neural networks, whose operation is deeply rooted in
    the principles of information flow, interconnected layers, and parameter optimization.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络，通常被称为多层感知机，是一种基础的神经网络类型，其操作深深植根于信息流动、互连层和参数优化的原则中。
- en: At their core, FFNNs orchestrate a unidirectional journey of information. It
    all begins with the input layer containing *n* neurons, where data is initially
    ingested. This layer serves as the entry point for the network, acting as a receptor
    for the input features that need to be processed. From there, the data embarks
    on a transformative voyage through the network's hidden layers.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，FFNNs orchestrate 信息的单向流动。所有一切都始于包含 *n* 个神经元的输入层，数据在此被初步接收。该层作为网络的入口点，充当需要处理的输入特征的接收器。从那里，数据开始经过网络的隐藏层进行转化的旅程。
- en: One important aspect of FFNNs is their connected structure, which means that
    each neuron in a layer is intricately connected to every neuron in that layer.
    This interconnectedness allows the network to perform computations and capture
    relationships within the data. It's like a communication network where every node
    plays a role in processing information.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: FFNNs 的一个重要方面是它们的连接结构，这意味着每个层中的神经元都与该层中的每个神经元紧密相连。这种互连性使网络能够进行计算并捕捉数据中的关系。这就像一个通信网络，每个节点在信息处理过程中都扮演着角色。
- en: As the data passes through the hidden layers, it undergoes a series of calculations.
    Each neuron in a hidden layer receives inputs from all neurons in the previous
    layer, applies a weighted sum to these inputs, adds a bias term, and then passes
    the result through an activation function (commonly ReLU, Sigmoid, or tanH). These
    mathematical operations enable the network to extract relevant patterns from the
    input, and capture complex, nonlinear relationships within data. This is where
    FFNNs truly excel compared to more shallow ML models.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据通过隐藏层时，它经历了一系列计算。隐藏层中的每个神经元接收来自前一层所有神经元的输入，对这些输入应用加权和，添加偏置项，然后通过激活函数（通常是
    ReLU、Sigmoid 或 tanH）传递结果。这些数学操作使网络能够从输入中提取相关模式，并捕捉数据中的复杂、非线性关系。这是 FFNNs 相比于更浅层的
    ML 模型真正出色的地方。
- en: Architecture of fully-connected feed-forward neural networks (image by author).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 完全连接的前馈神经网络的架构（图片由作者提供）。
- en: However, that's not where it ends. The real power of FFNNs lies in their ability
    to adapt. During training the network adjusts its weights to minimize the difference
    between its predictions and the actual target values. This iterative process,
    often based on optimization algorithms like gradient descent, is called *backpropagation.*
    Backpropagation empowers FFNNs to actually learn from data and improve their accuracy
    in making predictions or classifications.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这还不是全部。FFNNs 的真正力量在于它们的适应能力。在训练过程中，网络调整其权重，以最小化预测值与实际目标值之间的差异。这个迭代过程，通常基于像梯度下降这样的优化算法，被称为
    *反向传播*。反向传播使 FFNNs 能够真正从数据中学习，并提高其预测或分类的准确性。
- en: Example [KNIME workflow](https://hub.knime.com/-/spaces/-/latest/~MTi8mL9fZePO9A-S/)
    of FFNN used for the binary classification of certification exams (pass vs fail).
    In the upper branch, we can see the network architecture, which is made of an
    input layer, a fully connected hidden layer with a tanH activation function, and
    an output layer that uses a Sigmoid activation function (image by author).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[KNIME 工作流示例](https://hub.knime.com/-/spaces/-/latest/~MTi8mL9fZePO9A-S/)展示了用于认证考试（二元分类：通过与失败）的
    FFNN。在上部分支中，我们可以看到网络架构，它由一个输入层、一个带有 tanH 激活函数的全连接隐藏层和一个使用 Sigmoid 激活函数的输出层组成（图片由作者提供）。'
- en: While powerful and versatile, FFNNs display some relevant limitations. For example,
    they fail to capture sequentiality and temporal/syntactic dependencies in the
    data –two crucial aspects for tasks in language processing and time series analysis.
    The need to overcome these limitations prompted the evolution of a new type of
    neural network architecture. This transition paved the way for Recurrent Neural
    Networks (RNNs), which introduced the concept of feedback loops to better handle
    sequential data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管强大且多功能，FFNNs 显示出一些相关的限制。例如，它们无法捕捉数据中的顺序性和时间/句法依赖关系——这对语言处理和时间序列分析任务至关重要。克服这些限制的需求促使了一种新型神经网络架构的演变。这一过渡为递归神经网络（RNNs）铺平了道路，后者引入了反馈回路的概念，以更好地处理序列数据。
- en: RNN and LSTM - Recurrent Neural Network and Long Short-Term Memory
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 和 LSTM - 循环神经网络和长短期记忆
- en: At their core, RNNs share some similarities with FFNNs. They too are composed
    of layers of interconnected nodes, processing data to make predictions or classifications.
    However, their key differentiator lies in their ability to handle sequential data
    and capture temporal dependencies.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从核心来看，RNN 与 FFNN 存在一些相似之处。它们也由互联节点的层组成，处理数据以进行预测或分类。然而，它们的关键区别在于其处理序列数据和捕获时间依赖性的能力。
- en: In a FFNN, information flows in a single, unidirectional path from the input
    layer to the output layer. This is suitable for tasks where the order of data
    doesn't matter much. However, when dealing with sequences like time series data,
    language, or speech, maintaining context and understanding the order of data is
    crucial. This is where RNNs shine.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在 FFNN 中，信息沿着从输入层到输出层的单一路径流动。这适用于数据顺序不太重要的任务。然而，在处理时间序列数据、语言或语音等序列时，保持上下文和理解数据的顺序至关重要。这是
    RNN 发挥作用的地方。
- en: RNNs introduce the concept of feedback loops. These act as a sort of “memory”
    and allow the network to maintain a hidden state that captures information about
    previous inputs and to influence the current input and output. While traditional
    neural networks assume that inputs and outputs are independent of each other,
    the output of recurrent neural networks depend on the prior elements within the
    sequence. This recurrent connection mechanism makes RNNs particularly fit to handle
    sequences by “remembering” past information.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 引入了反馈循环的概念。这些循环作为一种“记忆”，允许网络保持一个隐藏状态，该状态捕获有关先前输入的信息，并影响当前输入和输出。虽然传统神经网络假设输入和输出彼此独立，但递归神经网络的输出依赖于序列中的前一个元素。这种递归连接机制使
    RNN 特别适合处理序列，通过“记住”过去的信息。
- en: Another distinguishing characteristic of recurrent networks is that they share
    the same weight parameter within each layer of the network, and those weights
    are adjusted leveraging the backpropagation through time (BPTT) algorithm, which
    is slightly different from traditional backpropagation as it is specific to sequence
    data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 循环网络的另一个区别特征是它们在每一层网络中共享相同的权重参数，并通过时间反向传播（BPTT）算法调整这些权重，这与传统的反向传播略有不同，因为它专门针对序列数据。
- en: Unrolled representation of RNNs, where each input is enriched with context information
    coming from previous inputs. The color represents the propagation of context information
    (image by author).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的展开表示，其中每个输入都结合了来自先前输入的上下文信息。颜色表示上下文信息的传播（图像作者提供）。
- en: However, traditional RNNs have their limitations. While in theory they should
    be able to capture long-range dependencies, in reality they struggle to do so
    effectively, and can even suffer from the vanishing gradient problem, which hinders
    their ability to learn and remember information over many time steps.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，传统的 RNN 存在其局限性。虽然理论上它们应该能够捕捉长期依赖关系，但在实际中，它们难以有效做到这一点，并且可能会受到梯度消失问题的影响，这妨碍了它们在许多时间步上学习和记忆信息的能力。
- en: 'This is where Long Short-Term Memory (LSTM) units come into play. They are
    specifically designed to handle these issues by incorporating three gates into
    their structure: the Forget gate, Input gate, and Output gate.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）单元在这里发挥作用。它们专门设计用于处理这些问题，通过在其结构中引入三个门：遗忘门、输入门和输出门。
- en: '**Forget gate**: This gate decides which information from the time step should
    be discarded or forgotten. By examining the cell state and the current input,
    it determines which information is irrelevant for making predictions in the present.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗忘门**：这个门决定了应该丢弃或遗忘时间步中的哪些信息。通过检查单元状态和当前输入，它确定哪些信息对当前的预测是无关的。'
- en: '**Input gate**: This gate is responsible for incorporating information into
    the cell state. It takes into account both the input and the previous cell state
    to decide what new information should be added to enhance its state.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入门**：这个门负责将信息纳入单元状态。它考虑了输入和先前的单元状态，以决定应该添加哪些新信息以增强其状态。'
- en: '**Output gate**: This gate concludes what output will be generated by the LSTM
    unit. It considers both the current input and the updated cell state to produce
    an output that can be utilized for predictions or passed on to time steps.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出门**：这个门决定了 LSTM 单元将生成什么输出。它考虑了当前输入和更新后的单元状态，以生成可以用于预测或传递到时间步的输出。'
- en: Visual representation of Long-Short Term Memory units ([image](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
    by Christopher Olah).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 长短期记忆单元的可视化表示（[图片](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)由Christopher
    Olah提供）。
- en: Example [KNIME workflow](https://hub.knime.com/-/spaces/-/latest/~uyVWG2YbtsOhdp5v/)
    of RNNs with LSTM units used for a multi-class sentiment prediction (positive,
    negative, neutral). The upper branch defines the network architecture using an
    input layer to handle strings of different lengths, an embedding layer, an LSTM
    layer with several units, and a fully connected output layer with a Softmax activation
    function to return predictions.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[KNIME工作流](https://hub.knime.com/-/spaces/-/latest/~uyVWG2YbtsOhdp5v/)的示例，使用带有LSTM单元的RNNs进行多类情感预测（正面、负面、中立）。上支定义了网络架构，使用输入层处理不同长度的字符串，嵌入层，具有多个单元的LSTM层，以及具有Softmax激活函数的全连接输出层以返回预测结果。'
- en: In summary, RNNs, and especially LSTM units, are tailored for sequential data,
    allowing them to maintain memory and capture temporal dependencies, which is a
    critical capability for tasks like natural language processing, speech recognition,
    and time series prediction.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，RNNs，尤其是LSTM单元，专门用于序列数据，使它们能够保持记忆并捕捉时间依赖关系，这对于自然语言处理、语音识别和时间序列预测等任务至关重要。
- en: As we shift from RNNs capturing sequential dependencies, the evolution continues
    with Convolutional Neural Networks (CNNs). Unlike RNNs, CNNs excel at spatial
    feature extraction from structured grid-like data, making them ideal for image
    and pattern recognition tasks. This transition reflects the diverse applications
    of neural networks across different data types and structures.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从捕捉序列依赖关系的RNNs转变时，演变继续向卷积神经网络（CNNs）发展。与RNNs不同，CNNs在从结构化网格数据中提取空间特征方面表现出色，使其成为图像和模式识别任务的理想选择。这种转变反映了神经网络在不同数据类型和结构中的多样应用。
- en: CNN - Convolutional Neural Network
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN - 卷积神经网络
- en: CNNs are a special breed of neural networks, particularly well-suited for processing
    image data, such as 2D images or even 3D video data. Their architecture relies
    on a multilayered feed-forward neural network with at least one convolutional
    layer.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs是一种特殊的神经网络，特别适合处理图像数据，如2D图像或甚至3D视频数据。它们的架构依赖于具有至少一个卷积层的多层前馈神经网络。
- en: What makes CNNs stand out is their network connectivity and approach to feature
    extraction, which allows them to automatically identify relevant patterns in the
    data. Unlike traditional FFNNs, which connect every neuron in one layer to every
    neuron in the next, CNNs employ a sliding window known as a *kernel* or *filter*.
    This sliding window scans across the input data and is especially powerful for
    tasks where spatial relationships matter, like identifying objects in images or
    tracking motion in videos. As the kernel is moved across the image, a convolution
    operation is performed between the kernel and the pixel values (from a strictly
    mathematical standpoint, this operation is a cross correlation), and a nonlinear
    activation function, usually ReLU, is applied. This produces a high value if the
    feature is in the image patch and a small value if it is not.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使CNNs脱颖而出的是其网络连接性和特征提取方法，这使得它们能够自动识别数据中的相关模式。与传统的FFNNs不同，后者将一层中的每个神经元与下一层中的每个神经元连接，CNNs使用一种称为*内核*或*滤波器*的滑动窗口。这个滑动窗口扫描输入数据，对于空间关系重要的任务（如图像中的物体识别或视频中的运动跟踪）特别有效。当内核在图像上移动时，会在内核和像素值之间执行卷积操作（从严格的数学角度看，这个操作是交叉相关），并应用通常是ReLU的非线性激活函数。如果特征在图像块中，则会产生高值，否则产生小值。
- en: Together with the kernel, the addition and fine-tuning of hyperparameters, such
    as *stride* (i.e., the number of pixels by which we slide the kernel) and *dilation
    rate* (i.e., the spaces between each kernel cell), allows the network to focus
    on specific features, recognizing patterns and details in specific regions without
    considering the entire input at once.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 与内核一起，*步幅*（即滑动内核的像素数）和*扩张率*（即每个内核单元之间的间隔）等超参数的添加和微调，使网络能够关注特定特征，识别特定区域的模式和细节，而不需要一次性考虑整个输入。
- en: Convolution operation with stride length = 2 ([GIF](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)
    by Sumit Saha).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 具有步幅长度=2的卷积操作（[GIF](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)由Sumit
    Saha提供）。
- en: Some kernels may specialize in detecting edges or corners, while others might
    be tuned to recognize more complex objects like cats, dogs, or street signs within
    an image. By stacking together several convolutional and pooling layers, CNNs
    build a hierarchical representation of the input, gradually abstracting features
    from low-level to high-level, just as our brains process visual information.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 有些卷积核可能专门用于检测边缘或角落，而其他卷积核则可能被调整用于识别图像中的更复杂对象，比如猫、狗或街道标志。通过将几个卷积层和池化层堆叠在一起，卷积神经网络（CNN）构建了输入数据的层次表示，逐渐从低层次特征抽象到高层次特征，就像我们的脑袋处理视觉信息一样。
- en: Example [KNIME workflow](https://hub.knime.com/-/spaces/-/latest/~RZjun0ed5Uu11SaE/)
    of CNN for binary image classification (cats vs dogs). The upper branch defines
    the network architecture using a series of convolutional layers and max pooling
    layers for automatic feature extraction from images. A flatten layer is then used
    to prepare the extracted features as a unidimensional input for the FFNN to perform
    a binary classification.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 [KNIME工作流](https://hub.knime.com/-/spaces/-/latest/~RZjun0ed5Uu11SaE/) 用于二分类图像分类（猫与狗）。上部分支通过一系列卷积层和最大池化层定义网络架构，用于从图像中自动提取特征。然后，使用一个展平层将提取的特征准备为FFNN的单维输入，以执行二分类任务。
- en: 'While CNNs excel at feature extraction and have revolutionized computer vision
    tasks, they act as passive observers, for they are not designed to generate new
    data or content. This is not an inherent limitation of the network per se but
    having a powerful engine and no fuel makes a fast car useless. Indeed, real and
    meaningful image and video data tend to be hard and expensive to collect and tend
    to face copyright and data privacy restrictions. This constraint led to the development
    of a novel paradigm that builds on CNNs but marks a leap from image classification
    to creative synthesis: Generative Adversarial Networks (GANs).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管卷积神经网络（CNN）在特征提取方面表现出色，并且在计算机视觉任务中取得了突破，但它们作为被动观察者，因为它们并不被设计用来生成新的数据或内容。这并不是网络本身的固有限制，但拥有强大的引擎却没有燃料会使一辆快车变得无用。确实，真实而有意义的图像和视频数据通常很难且昂贵地收集，并且往往面临版权和数据隐私限制。这种限制促使了一种新范式的出现，它建立在CNN的基础上，但标志着从图像分类到创造性合成的飞跃：生成对抗网络（GANs）。
- en: GAN - Generative Adversarial Networks
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAN - 生成对抗网络
- en: 'GANs are a particular family of neural networks whose primary, but not the
    only, purpose is to produce synthetic data that closely mimics a given dataset
    of real data. Unlike most neural networks, GANs’ ingenious architectural design
    consisting of two core models:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: GANs是一类特殊的神经网络，其主要但不是唯一的目的是生成与给定真实数据集非常相似的合成数据。与大多数神经网络不同，GANs的巧妙架构设计包括两个核心模型：
- en: '**Generator** model: The first player in this neural network duet is the generator
    model. This component is tasked with a fascinating mission: given random noise
    or input vectors, it strives to create artificial samples that are as close to
    resembling real samples as possible. Imagine it as an art forger, attempting to
    craft paintings that are indistinguishable from masterpieces.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成器**模型：在这个神经网络二重奏中，第一个角色是生成器模型。这个组件的任务是执行一个迷人的使命：给定随机噪声或输入向量，它努力创建尽可能接近真实样本的人工样本。可以把它想象成一个艺术伪造者，试图制作出与杰作无法区分的画作。'
- en: '**Discriminator** model: Playing the adversary role is the discriminator model.
    Its job is to differentiate between the generated samples produced by the generator
    and the authentic samples from the original dataset. Think of it as an art connoisseur,
    trying to spot the forgeries among the genuine artworks.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**判别器**模型：扮演对抗角色的是判别器模型。它的工作是区分由生成器生成的样本和来自原始数据集的真实样本。可以把它想象成一个艺术鉴赏家，试图从真实的艺术品中识别伪造品。'
- en: 'Now, here''s where the magic happens: GANs engage in a continuous, adversarial
    dance. The generator aims to improve its artistry, continually fine-tuning its
    creations to become more convincing. Meanwhile, the discriminator becomes a sharper
    detective, honing its ability to tell the real from the fake.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，魔法发生的地方在于：生成对抗网络（GAN）进行着持续的对抗性舞蹈。生成器旨在提升其艺术性，不断微调其创作，以变得更加可信。与此同时，判别器则变得更加敏锐，磨练其分辨真假样本的能力。
- en: GAN architecture (image by author).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: GAN架构（作者提供的图像）。
- en: As training progresses, this dynamic interplay between the generator and discriminator
    leads to a fascinating outcome. The generator strives to generate samples that
    are so realistic that even the discriminator can't tell them apart from the genuine
    ones. This competition drives both components to refine their abilities continuously.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练的进行，生成器和判别器之间的这种动态互动会产生一个迷人的结果。生成器力求生成如此真实的样本，以至于连判别器也无法将其与真实样本区分开。这种竞争推动了两个组件不断完善自己的能力。
- en: The result? A generator that becomes astonishingly adept at producing data that
    appears authentic, be it images, music, or text. This capability has led to remarkable
    applications in various fields, including image synthesis, data augmentation,
    image-to-image translation, and image editing.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 结果呢？生成器变得非常擅长生成看起来真实的数据，无论是图像、音乐还是文本。这一能力在图像合成、数据增强、图像到图像翻译和图像编辑等各个领域产生了显著的应用。
- en: Example [KNIME workflow](https://hub.knime.com/-/spaces/-/latest/~L4rc__X_CMzL1ubO/)
    of GANs for the generation of synthetic images (i.e., animals, human faces and
    Simpson characters).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 示例[KNIME 工作流](https://hub.knime.com/-/spaces/-/latest/~L4rc__X_CMzL1ubO/)用于生成合成图像（即动物、人脸和辛普森角色）的
    GANs。
- en: GANs pioneered realistic image and video content creation by pitting a generator
    against a discriminator. Extending the need for creativity and advanced operations
    from image to sequential data, models for more sophisticated natural language
    understanding, machine translation, and text generation were introduced. This
    initiated the development of Transformers,  a remarkable deep neural network architecture
    that not only outperformed previous architectures by effectively capturing long-range
    language dependencies and semantic context, but also became the undisputed foundation
    of the most recent AI-driven applications.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: GANs通过将生成器与判别器对抗，开创了逼真的图像和视频内容创作。随着对创造力和高级操作的需求从图像扩展到序列数据，出现了更复杂的自然语言理解、机器翻译和文本生成模型。这催生了
    Transformers 的发展，这是一种卓越的深度神经网络架构，不仅通过有效捕捉长程语言依赖关系和语义上下文超越了之前的架构，而且成为了最新 AI 驱动应用的无可争议的基础。
- en: Transformers
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformers
- en: 'Developed in 2017, Transformers boast a unique feature that allows them to
    replace traditional recurrent layers: a self-attention mechanism that allows them
    to model intricate relationships between all words in a document, regardless of
    their position. This makes Transformers excellent at tackling the challenge of
    long-range dependencies in natural language. Transformer architectures consist
    of two main building blocks:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 于 2017 年开发，拥有一个独特的特点，使其可以替代传统的递归层：自注意力机制，这使它们能够建模文档中所有单词之间的复杂关系，而不受它们位置的影响。这使得
    Transformers 在处理自然语言中的长程依赖关系方面表现出色。Transformer 架构由两个主要构建模块组成：
- en: '***   **Encoder.** Here the input sequence is embedded into vectors and then
    is exposed to the self-attention mechanism. The latter computes attention scores
    for each token, determining its importance in relation to others. These scores
    are used to create weighted sums, which are fed into a FFNN to generate context-aware
    representations for each token. Multiple encoder layers repeat this process, enhancing
    the model''s ability to capture hierarchical and contextual information.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '***   **编码器。** 在这里，输入序列被嵌入到向量中，然后暴露给自注意力机制。后者为每个标记计算注意力分数，以确定其相对于其他标记的重要性。这些分数用于创建加权和，随后输入到前馈神经网络（FFNN）中，以生成每个标记的上下文感知表示。多个编码器层重复这一过程，从而增强模型捕捉层次和上下文信息的能力。'
- en: '**Decoder.** This block is responsible for generating output sequences and
    follows a similar process to that of the encoder. It is able to place the proper
    focus on and understand the encoder''s output and its own past tokens during each
    step, ensuring accurate generation by considering both input context and previously
    generated output.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器。** 这个模块负责生成输出序列，过程类似于编码器。它能够在每一步中适当关注并理解编码器的输出及其自身的历史标记，通过同时考虑输入上下文和先前生成的输出来确保准确生成。'
- en: 'Transformer model architecture ([image](https://arxiv.org/abs/1706.03762) by:
    Vaswani et al., 2017).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型架构（[图片](https://arxiv.org/abs/1706.03762) 作者：Vaswani 等，2017年）。
- en: 'Consider this sentence: "I arrived at the bank after crossing the river”. The
    word "bank" can have two meanings –either a financial institution or the edge
    of a river. Here''s where transformers shine. They can swiftly focus on the word
    "river" to disambiguate "bank" by comparing "bank" to every other word in the
    sentence and assigning attention scores. These scores determine the influence
    of each word on the next representation of "bank". In this case, "river" gets
    a higher score, effectively clarifying the intended meaning.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 例如考虑这个句子：“我在过河后到达了银行”。单词“bank”可以有两个意思——金融机构或河岸。变换器在这里发挥了作用。它们可以迅速关注单词“river”，通过将“bank”与句子中的每个其他单词进行比较，并分配注意力得分，从而消除歧义。这些得分决定了每个单词对“bank”下一个表示的影响。在这种情况下，“river”获得了更高的得分，有效地澄清了意图的意义。
- en: To work that well, Transformers rely on millions of trainable parameters, require
    large corpora of texts and sophisticated training strategies. One notable training
    approach employed with Transformers is *masked language modeling (MLM)*. During
    training, specific tokens within the input sequence are randomly masked, and the
    model's objective is to predict these masked tokens accurately. This strategy
    encourages the model to grasp contextual relationships between words because it
    must rely on the surrounding words to make accurate predictions. This approach,
    popularized by the BERT model, has been instrumental in achieving state-of-the-art
    results in various NLP tasks.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，Transformers 依赖于数百万的可训练参数，需要大量的文本语料库和复杂的训练策略。与 Transformers 一起使用的一个显著训练方法是
    *掩码语言建模 (MLM)*。在训练期间，输入序列中的特定标记会被随机掩盖，模型的目标是准确预测这些掩盖的标记。这种策略鼓励模型掌握单词之间的上下文关系，因为它必须依赖周围的单词来进行准确的预测。这种方法由
    BERT 模型推广，在各种 NLP 任务中取得了最先进的结果。
- en: An alternative to MLM for Transformers is *autoregressive modeling*. In this
    method, the model is trained to generate one word at a time while conditioning
    on previously generated words. Autoregressive models like GPT (Generative Pre-trained
    Transformer) follow this methodology and excel in tasks where the goal is to predict
    unidirectionally the next most suitable word, such as free text generation, question
    answering and text completion.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Transformers 的替代方法是 *自回归建模*。在这种方法中，模型被训练为一次生成一个单词，同时依据先前生成的单词进行条件化。像 GPT（生成式预训练变换器）这样的自回归模型遵循这种方法，在预测单向的下一个最合适的单词时表现出色，例如自由文本生成、问答和文本补全。
- en: Furthermore, to compensate for the need for extensive text resources, Transformers
    excel in parallelization, meaning they can process data during training faster
    than traditional sequential approaches like RNNs or LSTM units. This efficient
    computation reduces training time and has led to groundbreaking applications in
    natural language processing, machine translation, and more.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了弥补对大量文本资源的需求，Transformers 在并行化方面表现出色，这意味着它们可以比传统的序列化方法（如 RNNs 或 LSTM 单元）更快地处理数据，从而减少训练时间，并在自然语言处理、机器翻译等领域取得了突破性的应用。
- en: A pivotal Transformer model developed by Google in 2018 that made a substantial
    impact is BERT (Bidirectional Encoder Representations from Transformers). BERT
    relied on MLM training and introduced the concept of bidirectional context, meaning
    it considers both the left and right context of a word when predicting the masked
    token. This bidirectional approach significantly enhanced the model's understanding
    of word meanings and contextual nuances, establishing new benchmarks for natural
    language understanding and a wide array of downstream NLP tasks.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 由谷歌在 2018 年开发的一个具有重大影响的关键 Transformer 模型是 BERT（双向编码器表示来自 Transformers）。BERT
    依赖于 MLM 训练，并引入了双向上下文的概念，这意味着在预测掩盖的标记时，它考虑单词的左右上下文。这种双向方法显著增强了模型对单词含义和上下文细微差别的理解，为自然语言理解和广泛的下游
    NLP 任务建立了新的基准。
- en: Example [KNIME workflow](https://hub.knime.com/-/spaces/-/latest/~3VGz0gdnmIy-DfT4/)
    of BERT for multi-class sentiment prediction (positive, negative, neutral). Minimal
    preprocessing is performed and the pretrained BERT model with fine-tuning is leveraged.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 [KNIME 工作流](https://hub.knime.com/-/spaces/-/latest/~3VGz0gdnmIy-DfT4/) 用于多类别情感预测（正面、负面、中性）。执行了最小的预处理，并利用了经过微调的预训练
    BERT 模型。
- en: On the heels of Transformers that introduced powerful self-attention mechanisms,
    the growing demand for versatility in applications and performing complex natural
    language tasks, such as document summarization, text editing, or code generation,
    necessitated the development of *large language models*. These models employ deep
    neural networks with billions of parameters to excel in such tasks and meet the
    evolving requirements of the data analytics industry.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在引入强大自注意力机制的 Transformers 之后，对应用的多样性和执行复杂自然语言任务（如文档总结、文本编辑或代码生成）的需求不断增长，这促使了*大型语言模型*的开发。这些模型使用具有数十亿参数的深度神经网络，在这些任务中表现出色，并满足数据分析行业不断变化的需求。
- en: LLM - Large Language Model
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM - 大型语言模型
- en: Large language models (LLMs) are a revolutionary category of multi-purpose and
    multi-modal (accepting image, audio and text inputs) deep neural networks that
    have garnered significant attention in recent years. The adjective *large* stems
    from their vast size, as they encompass billions of trainable parameters. Some
    of the most well-known examples include OpenAI’s ChatGTP, Google’s Bard or Meta’s
    LLaMa.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）是一类革命性的多用途和多模态（接受图像、音频和文本输入）深度神经网络，近年来获得了广泛关注。形容词*large*源于它们的庞大规模，因为它们包含数十亿个可训练参数。一些最著名的例子包括
    OpenAI 的 ChatGTP、Google 的 Bard 或 Meta 的 LLaMa。
- en: What sets LLMs apart is their unparalleled ability and flexibility to process
    and generate human-like text. They excel in natural language understanding and
    generation tasks, ranging from text completion and translation to question answering
    and content summarization. The key to their success lies in their extensive training
    on massive text corpora, allowing them to capture a rich understanding of language
    nuances, context, and semantics.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 的独特之处在于它们无与伦比的处理和生成类人文本的能力。它们在自然语言理解和生成任务中表现出色，范围涵盖从文本补全和翻译到问答和内容总结。它们成功的关键在于对大量文本语料的广泛训练，使它们能够捕捉语言细微差别、上下文和语义的丰富理解。
- en: These models employ a deep neural architecture with multiple layers of self-attention
    mechanisms, enabling them to weigh the importance of different words and phrases
    in a given context. This dynamic adaptability makes them exceptionally proficient
    in processing inputs of various types, comprehending complex language structures,
    and generating outputs based on human-defined prompts.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型采用深度神经架构，具有多层自注意力机制，使它们能够权衡给定上下文中不同单词和短语的重要性。这种动态适应性使它们在处理各种类型的输入、理解复杂语言结构以及根据人类定义的提示生成输出方面表现得特别高效。
- en: Example [KNIME workflow](https://hub.knime.com/-/spaces/-/latest/~s9uceqYIt_tUKJQc/)
    of creating an AI assistant that relies on OpenAI’s ChatGPT and a vector store
    with custom documents to answer domain-specific questions.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 [KNIME 工作流](https://hub.knime.com/-/spaces/-/latest/~s9uceqYIt_tUKJQc/) 通过依赖
    OpenAI 的 ChatGPT 和包含自定义文档的向量存储来创建 AI 助手，以回答特定领域的问题。
- en: LLMs have paved the way for a multitude of applications across various industries,
    from healthcare and finance to entertainment and customer service. They've even
    sparked new frontiers in creative writing and storytelling.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 为各种行业带来了众多应用，从医疗保健和金融到娱乐和客户服务。它们甚至在创意写作和讲故事方面开辟了新的领域。
- en: However, their enormous size, resource-intensive training processes and potential
    copyright infringements for generated content have also raised concerns about
    ethical usage, environmental impact, and accessibility. Lastly, while increasingly
    enhanced, LLMs may contain some serious flaws, such as “hallucinating” incorrect
    facts, being biased, gullible, or persuaded into creating toxic content.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它们巨大的规模、资源密集型的训练过程以及生成内容可能涉及的版权问题也引发了对伦理使用、环境影响和可访问性的担忧。最后，尽管不断增强，LLMs 可能仍存在一些严重缺陷，例如“幻觉”错误事实、偏见、易受影响或被迫生成有害内容。
- en: Is there an End?
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 是否有终点？
- en: 'The evolution of neural networks, from their humble beginnings to the uprising
    of large language models, raises a profound philosophical question: Will this
    journey ever come to an end?'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 从神经网络的谦逊起点到大型语言模型的兴起，这一演变提出了一个深刻的哲学问题：这一旅程是否会有尽头？
- en: The trajectory of technology has always been marked by relentless advancement.
    Each milestone only serves as a stepping stone to the next innovation. As we strive
    to create machines that can replicate human cognition and understanding, it's
    tempting to ponder whether there's an ultimate destination, a point where we say,
    "This is it; we've reached the pinnacle."
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 技术的发展历程始终以不懈的进步为特征。每一个里程碑只是通向下一个创新的垫脚石。当我们努力创造能够复制人类认知和理解的机器时，很容易产生这样的疑问：是否存在一个*终极目标*，一个我们可以说“这就是它；我们已经达到了巅峰”的点。
- en: However, the essence of human curiosity and the boundless complexities of the
    natural world suggest otherwise. Just as our understanding of the universe continually
    deepens, the quest to develop more intelligent, capable, and ethical neural networks
    may be an endless journey.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，人类好奇心的本质和自然界的无限复杂性表明情况并非如此。正如我们对宇宙的理解不断加深，开发更智能、更有能力、更具伦理的神经网络的探索也许是一次无尽的旅程。
- en: '**[Anil](https://www.linkedin.com/in/anil-oezer/?locale=en_US)** is a Data
    Science Evangelist at KNIME.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Anil](https://www.linkedin.com/in/anil-oezer/?locale=en_US)** 是KNIME的数据科学传播者。'
- en: More On This Topic
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[History and Future of LLMs](https://www.kdnuggets.com/history-and-future-of-llms)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LLMs的历史与未来](https://www.kdnuggets.com/history-and-future-of-llms)'
- en: '[10 Simple Things to Try Before Neural Networks](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在使用神经网络之前尝试的10个简单方法](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用PyTorch解释神经网络](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
- en: '[Deep Neural Networks Don''t Lead Us Towards AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度神经网络不会引导我们走向AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
- en: '[Explainable Forecasting and Nowcasting with State-of-the-art Deep…](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用最先进的深度学习进行可解释的预测和现在预测](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
- en: '[Image Classification with Convolutional Neural Networks (CNNs)](https://www.kdnuggets.com/2022/05/image-classification-convolutional-neural-networks-cnns.html)****'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用卷积神经网络（CNNs）进行图像分类](https://www.kdnuggets.com/2022/05/image-classification-convolutional-neural-networks-cnns.html)****'
