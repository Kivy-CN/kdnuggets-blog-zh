# 理解集成学习技术

> 原文：[https://www.kdnuggets.com/2020/03/making-sense-ensemble-learning-techniques.html](https://www.kdnuggets.com/2020/03/making-sense-ensemble-learning-techniques.html)

[评论](#comments)

**作者 [Ido Zehori](https://www.linkedin.com/in/ido-zehori/)，[Bigabid](https://www.bigabid.com/) 数据科学团队负责人**

![图示](../Images/733e44c46833a8497ef2f1341a1d187c.png)

图片来源: [Packt](https://hub.packtpub.com/what-is-ensemble-learning/)

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速通道进入网络安全职业。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持组织的 IT 工作

* * *

对于许多专注于或从事机器学习（ML）的公司/数据科学家来说，集成学习方法已经成为首选武器。由于集成学习方法结合了多个基础模型，因此它们能够更准确地生成机器学习模型。例如，在 Bigabid，我们一直在使用集成学习成功解决各种问题，从优化 LTV（[客户终身价值](https://www.qualtrics.com/experience-management/customer/customer-lifetime-value/)）到 [欺诈检测](https://searchsecurity.techtarget.com/definition/fraud-detection)。

难以夸大集成学习在整体机器学习过程中的重要性，包括 [偏差-方差权衡](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) 以及三种主要的集成技术：袋装、提升和堆叠。这些强大的技术应成为任何数据科学家工具包的一部分，因为它们是无处不在的概念。此外，理解它们的基本机制是机器学习领域的核心。

### 集成学习方法概述

集成学习是一种机器学习范式，其中将多个基础模型（通常称为“弱学习器”）进行组合并训练以解决相同的问题。这种方法基于这样的理论：通过正确地组合几个基础模型，这些弱学习器可以作为设计更复杂机器学习模型的构建块。通过这种方式，这些集成模型（恰当地称为“强学习器”）能够实现更好、更准确的结果。

换句话说，弱学习器仅仅是一个单独表现相当差的基础模型。事实上，它的准确度仅略高于随机猜测，这意味着它预测结果的效果仅比随机猜测好一点。这些弱学习器通常也计算简单。通常，基础模型表现不佳的原因在于它们要么有高偏差，要么有过多的方差，从而使其变得较弱。

这就是集成学习发挥作用的地方。该方法试图通过将多个弱学习器组合在一起来减少总体误差。将集成模型视为数据科学版本的“两个脑袋总比一个好”这一说法。如果一个模型效果良好，那么多个模型一起工作可以做得更好。

### 关于偏差-方差权衡的说明

理解弱学习器的概念及其名字的由来非常重要，因为原因归结于偏差或方差。更具体地说，机器学习模型的预测误差，即训练模型与真实值之间的差异，可以分解为以下几个部分的总和：偏差和方差。例如：

+   **偏差引起的误差：** 这是模型的期望预测与我们希望预测的精确值之间的差异。

+   **方差引起的误差：** 这是模型在指定数据点上的预测方差。

如果一个模型过于简单且参数较少，那么它可能有高偏差和低方差。相比之下，如果一个模型有许多参数，它可能有高方差和低偏差。因此，必须找到正确的平衡点，避免欠拟合或过拟合数据，因为复杂性的这种权衡正是方差与偏差之间存在权衡的原因。简单来说，一个算法不能同时变得更复杂和更简单。

### 集成学习技术 - 组合弱学习器

有三种主要的集成技术：袋装法、提升法和堆叠法。它们定义如下：

**袋装法（Bagging）：**

袋装法试图在小样本群体中整合相似的学习器，并计算所有预测的平均值。通常，袋装法允许在不同的群体中使用不同的学习器。通过这样做，这种方法有助于减少方差误差。

**提升法（Boosting）**

提升是一种迭代方法，根据最近的分类来微调观察值的权重。如果一个观察值被错误分类了，这种方法将在下一轮（即下一个模型将被训练的轮次）增加该观察值的权重，从而减少误分类的可能性。同样，如果观察值被正确分类，则会减少它在下一个分类器中的权重。权重表示特定数据点正确分类的重要程度，因为这使得顺序分类器能够关注先前被错误分类的示例。一般来说，提升减少了偏差误差并形成了强大的预测模型，但有时它们可能会在训练数据上过拟合。

**堆叠**

堆叠是一种巧妙的将不同模型提供的信息结合起来的方法。使用这种方法，可以将任何类型的学习者用于组合不同学习者的输出。结果可能是由于使用了哪些组合模型而导致偏差或方差的减少。

### **集成学习的前景**

集成学习是将多个基础模型结合起来，以实现一个更有效、更准确的集成模型，该模型具有更强大的属性，因此表现更好。集成学习方法在具有挑战性的数据集上成功地创下了记录，并且不断成为[Kaggle竞赛](https://www.kaggle.com/competitions)获胜提交的一部分。

值得注意的是，即使是三种主要的集成技术——装袋、提升和堆叠——也可能存在变体，并且可以更好地设计以更有效地适应特定问题，如分类、回归、时间序列分析等。这首先需要理解手头的问题，并在解决问题时发挥创造力！

使用集成学习方法是开始解决任何问题的一个很好的、有前途的方式。

**简介: [Ido Zehori](https://www.linkedin.com/in/ido-zehori/)** 是[Bigabid](https://www.bigabid.com/)的数据科学团队负责人，该公司开发了一种针对应用内广告用户获取和再参与优化的第二代DSP，提供了一级DSP的规模和最前沿DMP的精确度。

**相关内容：**

+   [机器学习的集成方法：AdaBoost](/2019/09/ensemble-methods-machine-learning-adaboost.html)

+   [随机森林® — 强大的集成学习算法](/2020/01/random-forest-powerful-ensemble-learning-algorithm.html)

+   [解释黑箱模型：使用LIME和SHAP进行集成和深度学习](/2020/01/explaining-black-box-models-ensemble-deep-learning-lime-shap.html)

### 更多相关主题

+   [集成学习技术：Python中的随机森林详细讲解](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)

+   [何时使用集成技术是一个好选择？](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)

+   [带实例的集成学习](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)

+   [成功的机器学习模型中数据质量的重要性](https://www.kdnuggets.com/2022/03/significance-data-quality-making-successful-machine-learning-model.html)

+   [Ploomber 与 Kubeflow：简化 MLOps](https://www.kdnuggets.com/2022/02/ploomber-kubeflow-mlops-easier.html)

+   [使智能文档处理更智能：第 1 部分](https://www.kdnuggets.com/2023/02/making-intelligent-document-processing-smarter-part-1.html)
