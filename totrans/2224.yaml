- en: 'Machine Learning Evaluation Metrics: Theory and Overview'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/machine-learning-evaluation-metrics-theory-and-overview](https://www.kdnuggets.com/machine-learning-evaluation-metrics-theory-and-overview)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Machine Learning Evaluation Metrics: Theory and Overview](../Images/6b32ae2ad62dc68177dc6ed7abf1bdb8.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration by Author
  prefs: []
  type: TYPE_NORMAL
- en: Building a machine learning model that generalizes well on new data is very
    challenging. It needs to be evaluated to understand if the model is enough good
    or needs some modifications to improve the performance.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: If the model doesn’t learn enough of the patterns from the training set, it
    will perform badly on both training and test sets. This is the so-called underfitting
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Learning too much about the patterns of training data, even the noise, will
    lead the model to perform very well on the training set, but it will work poorly
    on the test set. This situation is overfitting. The generalization of the model
    can be obtained if the performances measured both in training and test sets are
    similar.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we are going to see the most important evaluation metrics for
    classification and regression problems that will help to verify if the model is
    capturing well the patterns from the training sample and performing well on unknown
    data. Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When our target is categorical, we are dealing with a classification problem.
    The choice of the most appropriate metrics depends on different aspects, such
    as the characteristics of the dataset, whether it’s imbalanced or not, and the
    goals of the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Before showing the evaluation metrics, there is an important table that needs
    to be explained, called Confusion Matrix, that summarizes well the performance
    of a classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say that we want to train a model to detect breast cancer from an ultrasound
    image. We have only two classes, malignant and benign.
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positives**: The number of terminally ill people that are predicted
    to have a malignant cancer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negatives**: The number of healthy people that are predicted to have
    a benign cancer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positives**: The number of healthy people that are predicted to have
    malignant cancer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negatives**: The number of terminally ill people that predicted to
    have benign cancer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Machine Learning Evaluation Metrics: Theory and Overview](../Images/e982401a7464adcca3b006f32d08f3ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of Confusion Matrix. Illustration by Author.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Machine Learning Evaluation Metrics: Theory and Overview](../Images/0c6542986e34a4d884a85747645bccd4.png)'
  prefs: []
  type: TYPE_IMG
- en: Accuracy is one of the most known and popular metrics to evaluate a classification
    model. It is the fraction of the corrected predictions divided by the number of
    Samples.
  prefs: []
  type: TYPE_NORMAL
- en: The Accuracy is employed when we are aware that the dataset is balanced. So,
    each class of the output variable has the same number of observations.
  prefs: []
  type: TYPE_NORMAL
- en: Using Accuracy, we can answer the question “Is the model predicting correctly
    all the classes?”. For this reason, we have the correct predictions of both the
    positive class (malignant cancer) and the negative class (benign cancer).
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Machine Learning Evaluation Metrics: Theory and Overview](../Images/0d60f4f6d383b0177b3a163445c1779f.png)'
  prefs: []
  type: TYPE_IMG
- en: Differently from Accuracy, Precision is an evaluation metric for classification
    used when the classes are imbalanced.
  prefs: []
  type: TYPE_NORMAL
- en: 'Precision answer to the following question: “What proportion of malignant cancer
    identifications was actually correct?”. It’s calculated as the ratio between True
    Positives and Positive Predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: We are interested in using Precision if we are worried about False Positives
    and we want to minimize it. It would be better to avoid running the lives of healthy
    people with fake news of a malignant cancer.
  prefs: []
  type: TYPE_NORMAL
- en: The lower the number of False Positives, the higher the Precision will be.
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Machine Learning Evaluation Metrics: Theory and Overview](../Images/b572a5b0cb721a853cf7df971becd869.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Together with Precision, Recall is another metric applied when the classes
    of the output variable have a different number of observations. Recall answers
    to the following question: “What proportion of patients with malignant cancer
    I was able to recognize?”.'
  prefs: []
  type: TYPE_NORMAL
- en: We care about Recall if our attention is focused on the False Negatives. A false
    negative means that that patient has a malignant cancer, but we weren’t able to
    identify it. Then, both Recall and Precision should be monitored to obtain the
    desirable good performance on unknown data.
  prefs: []
  type: TYPE_NORMAL
- en: F1-Score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Machine Learning Evaluation Metrics: Theory and Overview](../Images/cd586e43655c8d4568bfc136de1fd12b.png)'
  prefs: []
  type: TYPE_IMG
- en: Monitoring both Precision and Recall can be messy and it would be preferable
    to have a measure that summarizes both these measures. This is possible with the
    F1-score, which is defined as the harmonic mean of precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: A high f1-score is justified by the fact that both Precision and Recall have
    high values. If recall or precision has low values, the f1-score will be penalized
    and, then, will have a low value too.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![Machine Learning Evaluation Metrics: Theory and Overview](../Images/017f3c95d6a235b4570cdc4796c89607.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration by Author
  prefs: []
  type: TYPE_NORMAL
- en: When the output variable is numerical, we are dealing with a regression problem.
    As in the classification problem, it’s crucial to choose the metric for evaluating
    the regression model, depending on the purposes of the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The most popular example of a regression problem is the prediction of house
    prices. Are we interested in predicting accurately the house prices? Or do we
    just care about minimizing the overall error?
  prefs: []
  type: TYPE_NORMAL
- en: In all these metrics, the building block is the residual, which is the difference
    between predicted values and actual values.
  prefs: []
  type: TYPE_NORMAL
- en: MAE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Machine Learning Evaluation Metrics: Theory and Overview](../Images/f69f5cce003d2c7a2f455e1d4a6901c5.png)'
  prefs: []
  type: TYPE_IMG
- en: '*The Mean Absolute Error calculates the average absolute residuals. *'
  prefs: []
  type: TYPE_NORMAL
- en: It doesn’t penalize high errors as much as other evaluation metrics. Every error
    is treated equally, even the errors of outliers, so this metric is robust to outliers.
    Moreover, the absolute value of the differences ignores the direction of error.
  prefs: []
  type: TYPE_NORMAL
- en: MSE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Machine Learning Evaluation Metrics: Theory and Overview](../Images/5d94f5a69a01dae253fa99d79641dbdc.png)'
  prefs: []
  type: TYPE_IMG
- en: '*The Mean Squared Error calculates the average squared residuals.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Since the differences between predicted and actual values are squared, It
    gives more weight to higher errors, *'
  prefs: []
  type: TYPE_NORMAL
- en: '*so it can be useful when big errors are not desirable, rather than minimizing
    the overall error.  *'
  prefs: []
  type: TYPE_NORMAL
- en: RMSE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Machine Learning Evaluation Metrics: Theory and Overview](../Images/61fec1d4635556155c0557d49d167afa.png)'
  prefs: []
  type: TYPE_IMG
- en: '*The Root Mean Squared Error calculates the* ***square root*** *of the average
    squared residuals.*'
  prefs: []
  type: TYPE_NORMAL
- en: When you understand MSE, you keep a second to grasp the Root Mean Squared Error,
    which is just the square root of MSE.
  prefs: []
  type: TYPE_NORMAL
- en: 'The good point of RMSE is that it is easier to interpret since the metric is
    in the scale of the target variable. Except for the shape, it’s very similar to
    MSE: it always gives more weight to higher differences.'
  prefs: []
  type: TYPE_NORMAL
- en: MAPE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Machine Learning Evaluation Metrics: Theory and Overview](../Images/e0ed29472adda7f3fd1b05fb6c2c1349.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Mean Absolute Percentage Error calculates the average absolute percentage
    difference between predicted values and actual values.*'
  prefs: []
  type: TYPE_NORMAL
- en: Like MAE, it disregards the direction of the error and the best possible value
    is ideally 0.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we obtain a MAPE with a value of 0.3 for predicting house prices,
    it means that, on average, the predictions are below of 30%.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I hope that you have enjoyed this overview of the evaluation metrics. I just
    covered the most important measures for evaluating the performance of classification
    and regression models. If you have discovered other life-saving metrics, that
    helped you on solving a problem, but they are not nominated here, drop them in
    the comments.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Eugenia Anello](https://www.linkedin.com/in/eugenia-anello/)** is currently
    a research fellow at the Department of Information Engineering of the University
    of Padova, Italy. Her research project is focused on Continual Learning combined
    with Anomaly Detection.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[More Performance Evaluation Metrics for Classification Problems You…](https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding Supervised Learning: Theory and Overview](https://www.kdnuggets.com/understanding-supervised-learning-theory-and-overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Statistics in Data Science: Theory and Overview](https://www.kdnuggets.com/statistics-in-data-science-theory-and-overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Visualization: Theory and Techniques](https://www.kdnuggets.com/data-visualization-theory-and-techniques)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[From Theory to Practice: Building a k-Nearest Neighbors Classifier](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Use Graph Theory to Scout Soccer](https://www.kdnuggets.com/2022/11/graph-theory-scout-soccer.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
