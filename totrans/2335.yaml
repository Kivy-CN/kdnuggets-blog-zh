- en: 'Machine Learning from Scratch: Decision Trees'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始的机器学习：决策树
- en: 原文：[https://www.kdnuggets.com/2022/11/machine-learning-scratch-decision-trees.html](https://www.kdnuggets.com/2022/11/machine-learning-scratch-decision-trees.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/11/machine-learning-scratch-decision-trees.html](https://www.kdnuggets.com/2022/11/machine-learning-scratch-decision-trees.html)
- en: '![v](../Images/36ac427f5a02fcb2b5f248c0eecb8ace.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![v](../Images/36ac427f5a02fcb2b5f248c0eecb8ace.png)'
- en: Image from [Pexel](https://www.pexels.com/photo/leafless-tree-on-grass-field-1102909/)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [Pexel](https://www.pexels.com/photo/leafless-tree-on-grass-field-1102909/)
- en: Decision trees are one of the simplest non-linear supervised algorithms in the
    machine learning world. As the name suggests they are used for making decisions
    in ML terms we call it classification (although they can be used for regression
    as well).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是机器学习世界中最简单的非线性监督算法之一。顾名思义，它们用于在ML术语中进行决策，我们称之为分类（尽管它们也可以用于回归）。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的IT需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The decision trees have a unidirectional tree structure i.e. at every node the
    algorithm makes a decision to split into child nodes based on certain stopping
    criteria. Most commonly DTs use entropy, information gain, Gini index, etc.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树具有单向树结构，即在每个节点上，算法根据某些停止标准做出决策以分裂成子节点。决策树通常使用熵、信息增益、基尼指数等。
- en: 'There are a few known algorithms in DTs such as ID3, C4.5, CART, C5.0, CHAID,
    QUEST, CRUISE. In this article, we would discuss the simplest and most ancient
    one: ID3.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树中有一些已知的算法，如ID3、C4.5、CART、C5.0、CHAID、QUEST、CRUISE。在这篇文章中，我们将讨论最简单且最古老的一个：ID3。
- en: '**ID3**, or Iternative Dichotomizer, was the first of three Decision Tree implementations
    developed by Ross Quinlan.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**ID3**，即Iterative Dichotomiser，是Ross Quinlan开发的三种决策树实现中的第一个。'
- en: The algorithm builds a tree in a top-down fashion, starting from a set of rows/objects
    and a specification of features. At each node of the tree, one feature is tested
    based on minimizing entropy or maximizing information gain to split the object
    set. This process is continued until the set in a given node is homogeneous (i.e.
    node contains objects of the same category). The algorithm uses a greedy search.
    It selects a test using the information gain criterion, and then never explores
    the possibility of alternate choices.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 算法以自上而下的方式构建树，从一组行/对象和特征规范开始。在树的每个节点上，根据最小化熵或最大化信息增益来测试一个特征以分裂对象集。这个过程会持续进行，直到某个节点中的集合是同质的（即节点包含相同类别的对象）。该算法使用贪婪搜索。它使用信息增益标准选择测试，然后从不探索替代选择的可能性。
- en: 'Cons:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: The model may be over-fitted.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型可能存在过拟合的情况。
- en: Only works on categorical features
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅适用于分类特征
- en: Does not handle missing values
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不处理缺失值
- en: Low speed
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低速
- en: Doesn’t support pruning
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不支持剪枝
- en: Doesn’t support boosting
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不支持提升
- en: So many disadvantages of one algorithm, why are we even discussing this?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 既然一个算法有这么多缺点，我们为什么还要讨论它呢？
- en: 'Answer: it’s simple and is great to develop the intuition for tree algorithms.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 答：这很简单，非常适合培养对树算法的直觉。
- en: Let’s Understand with an Example
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解
- en: One of the most popular nursery rhyme where rain decides whether Johny/Arthur
    would play outside is our sample today. The only change is that not just rain
    but any bad weather impacts the child’s play and we would use a DT to predict
    his presence outside.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的示例是最受欢迎的童谣之一，其中雨决定Johny/Arthur是否会在外面玩。唯一的变化是不仅仅是雨，任何恶劣天气都会影响孩子的玩耍，我们将使用决策树来预测他是否会在外面。
- en: '![Machine Learning from scratch: Decision Trees](../Images/372d0781819ea8688ab43a50782ecf20.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![从零开始的机器学习：决策树](../Images/372d0781819ea8688ab43a50782ecf20.png)'
- en: 'Source: Wikipedia'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：维基百科
- en: 'The data looks like this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 数据如下所示：
- en: '![Machine Learning from scratch: Decision Trees](../Images/2c58f39b5313bb05eaab4d9000b15892.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![从零开始的机器学习：决策树](../Images/2c58f39b5313bb05eaab4d9000b15892.png)'
- en: ‘Temperature’, ‘WindSpeed’, ‘Outlook’, and ‘Humidity’ are the features, and
    ‘Play’ is the target variable. Only categorical data and no missing values mean
    we can use ID3.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ‘Temperature’，‘WindSpeed’，‘Outlook’和‘Humidity’是特征，而‘Play’是目标变量。只有分类数据且没有缺失值意味着我们可以使用ID3。
- en: Let’s go through the splitting criteria before jumping onto the algorithm itself.
    For simplicity, we will discuss each criterion only for the binary classification
    case.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入算法之前，我们先了解一下划分标准。为了简化起见，我们将仅讨论二分类情况的每个标准。
- en: '**Entropy:** This is used to calculate the heterogeneity of a sample and its
    value lies between 0 and 1\. If the sample is homogeneous the entropy is 0 and
    if the sample has the same proportion of all the classes it has an entropy of
    1.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**熵：** 这用于计算样本的异质性，其值在0和1之间。如果样本是同质的，则熵为0，如果样本中所有类别的比例相同，则熵为1。'
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Where `p` and `q` are respective proportion of the 2 classes in the sample.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`p`和`q`是样本中两个类别的各自比例。'
- en: This can also be written as: `S = -(p * log₂p + (1-p)* log₂(1-p))`
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以写作：`S = -(p * log₂p + (1-p)* log₂(1-p))`
- en: '![Machine Learning from scratch: Decision Trees](../Images/a7189f79ab54740aa38045d1954de6fd.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![从零开始的机器学习：决策树](../Images/a7189f79ab54740aa38045d1954de6fd.png)'
- en: '**Information Gain:** It is the difference in entropy of a node and the average
    entropy for all the values of a child node. The feature which gives maximum Information
    Gain is chosen for the split.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**信息增益：** 它是节点的熵与子节点所有值的平均熵之间的差异。选择信息增益最大的特征进行拆分。'
- en: Let’s Understand this with our Example
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们通过示例来理解这一点
- en: 'The entropy of root node: (9 — Yes and 5 — No)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 根节点的熵：（9 — Yes 和 5 — No）
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'There are 4 possible ways to split the root node. (‘Temperature’, ‘WindSpeed’,
    ‘Outlook’, and ‘Humidity’). Thus we compute the weighted average entropy of the
    child node if we choose any one of the above:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 根节点有4种可能的拆分方式。（‘Temperature’，‘WindSpeed’，‘Outlook’，和‘Humidity’）。因此，如果选择上述任意一个，我们将计算子节点的加权平均熵。
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Where Hot, Mild, and Cool represent the proportion of the 3 values in the data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，Hot，Mild和Cool代表数据中三个值的比例。
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here Entropy for each value is computed by filtering the sample using the value
    of the feature and then using the formula for entropy. For example, E(Temperature=Hot)
    is computed by filtering the original sample where the Temperature is Hot (in
    this case we have an equal number of Yes and No which means Entropy equals 1).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，每个值的熵通过使用特征的值过滤样本，然后使用熵的公式来计算。例如，E(Temperature=Hot) 是通过过滤原始样本中温度为Hot的样本来计算的（在这种情况下，我们有相等数量的Yes和No，这意味着熵等于1）。
- en: '![Machine Learning from scratch: Decision Trees](../Images/2faaecf786f9881367c444e38303a93b.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![从零开始的机器学习：决策树](../Images/2faaecf786f9881367c444e38303a93b.png)'
- en: We compute the Information Gain of splitting on Temperature by subtracting the
    Average Entropy for Temperature from the root nodes Entropy.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过从根节点的熵中减去温度的平均熵来计算温度划分的信息增益。
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Likewise, we compute the Gain from all the four features and choose the 1 with
    maximum Gain.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们计算所有四个特征的增益，并选择增益最大的一项。
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Outlook has the maximum Information Gain, thus we would split the root node
    on Outlook and each for the child nodes represents the sample filtered by one
    of the values of Outlook i.e. Sunny, Overcast, and Rainy.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Outlook具有最大的 信息增益，因此我们将在Outlook上拆分根节点，每个子节点代表按Outlook的某个值（即Sunny，Overcast和Rainy）过滤的样本。
- en: Now we would repeat the same process by treating the new nodes formed as root
    nodes with filtered samples and compute entropies for each and testing the splits
    by computing Average Entropy for each further split and subtracting that from
    the current node Entropy to get the Information Gain. Please note that ID3 doesn’t
    allow using already used feature for splitting child nodes. Thus each feature
    can only be used once in a tree for the split.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将重复相同的过程，将新形成的节点视为根节点，使用过滤样本计算每个节点的熵，并通过计算每个进一步拆分的平均熵并从当前节点熵中减去来获得 信息增益。请注意，ID3不允许使用已使用的特征来拆分子节点。因此，每个特征在树中只能用于一次拆分。
- en: 'Here is the final tree formed by all the splits:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过所有拆分形成的最终树：
- en: '![Machine Learning from scratch: Decision Trees](../Images/40ba6d18ea1dde3426503c175f884f32.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![从零开始的机器学习：决策树](../Images/40ba6d18ea1dde3426503c175f884f32.png)'
- en: Simple implementation with Python code can be found [here](https://www.kaggle.com/ankitmalik/decision-trees-from-scratch-id3)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python 代码的简单实现可以在[这里](https://www.kaggle.com/ankitmalik/decision-trees-from-scratch-id3)找到
- en: Conclusion
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: I tried my best to explain the ID3 working but I know you might have questions.
    Please let me know in the comments and I would be happy to take them all.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我尽力解释了 ID3 的工作原理，但我知道你可能还有问题。请在评论中告诉我，我会很高兴回答。
- en: Thanks for reading!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 谢谢阅读！
- en: '**[Ankit Malik](https://www.linkedin.com/in/ankitmalikds/)** is building scalable
    AI/ML solutions across multiple domains like Marketing, Supply Chain, Retail,
    Advertising, and Process Automation. He has worked on both ends of the spectrum
    from leading data science projects in Fortune 500 companies to being the founding
    member of a data science incubator in multiple start-ups. He has pioneered various
    innovative products and services and is a believer in servant leadership.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**[安基特·马利克](https://www.linkedin.com/in/ankitmalikds/)** 正在多个领域（如市场营销、供应链、零售、广告和流程自动化）构建可扩展的
    AI/ML 解决方案。他曾在财富 500 强公司领导数据科学项目，也曾是多个初创公司数据科学孵化器的创始成员。他开创了各种创新产品和服务，并且相信服务型领导。'
- en: '[Original](https://ai.plainenglish.io/ml-from-scratch-decision-trees-da68cdaa13bc).
    Reposted with permission.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://ai.plainenglish.io/ml-from-scratch-decision-trees-da68cdaa13bc)。经授权转载。'
- en: More On This Topic
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Decision Trees vs Random Forests, Explained](https://www.kdnuggets.com/2022/08/decision-trees-random-forests-explained.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[决策树与随机森林的解释](https://www.kdnuggets.com/2022/08/decision-trees-random-forests-explained.html)'
- en: '[Generalized and Scalable Optimal Sparse Decision Trees(GOSDT)](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通用且可扩展的最优稀疏决策树（GOSDT）](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
- en: '[Demystifying Decision Trees for the Real World](https://www.kdnuggets.com/demystifying-decision-trees-for-the-real-world)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[揭开决策树在现实世界中的神秘面纱](https://www.kdnuggets.com/demystifying-decision-trees-for-the-real-world)'
- en: '[Linear Regression from Scratch with NumPy](https://www.kdnuggets.com/linear-regression-from-scratch-with-numpy)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 NumPy 从头实现线性回归](https://www.kdnuggets.com/linear-regression-from-scratch-with-numpy)'
- en: '[How to Build and Train a Transformer Model from Scratch with…](https://www.kdnuggets.com/how-to-build-and-train-a-transformer-model-from-scratch-with-hugging-face-transformers)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何从头构建和训练一个 Transformer 模型……](https://www.kdnuggets.com/how-to-build-and-train-a-transformer-model-from-scratch-with-hugging-face-transformers)'
- en: '[Decision Tree Algorithm, Explained](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[决策树算法的解释](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)'
