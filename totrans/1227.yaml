- en: What’s ETL?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/04/whats-etl.html](https://www.kdnuggets.com/2021/04/whats-etl.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Omer Mahmood](https://www.linkedin.com/in/omahmood/), Head of Cloud Customer
    Engineering, CPG & Travel at Google**'
  prefs: []
  type: TYPE_NORMAL
- en: In my [last post](https://towardsdatascience.com/whats-mlops-5bf60dd693dd),
    I talked about what it means to move machine learning (ML) models into production
    by introducing the concept of MLOps. This time we’re going to look at the opposite
    end of the [data science steps for ML](https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning#data_science_steps_for_ml) —
    data extraction and integration.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The TL;DR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ETL stands for *Extract-Transform-Load*, it usually involves moving data from
    one or more sources, making some changes, and then loading it into a new single
    destination.
  prefs: []
  type: TYPE_NORMAL
- en: In most companies **data tends to be in silos**, stored in various formats and
    is often inaccurate or inconsistent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This situation is far from ideal if we want to be able to easily analyse and
    getinsights from that data or **use it** **for data science**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ???????? How we got here
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most ML algorithms require large amounts of training data in order to produce
    models that can make accurate predictions. They also require good quality training
    data, representative of the problem we are trying to solve.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reinforce this point there is a great example I came across, analogous to
    ‘[Maslow’s hierarchy of needs](https://en.wikipedia.org/wiki/Maslow%27s_hierarchy_of_needs)’
    that highlights the importance of data collection and storage as it relates to
    data science:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data science hierachy of needs](../Images/122661a46d4ddd1a9208b838242c391b.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1: The Data Science Hierarchy of Needs Pyramid, SOURCE: “THE AI HIERARCHY
    OF NEEDS” MONICA ROGATI[1]*'
  prefs: []
  type: TYPE_NORMAL
- en: At the bottom of the pyramid is the basic need to gather the right data, in
    the right formats and systems, and in the right quantity.
  prefs: []
  type: TYPE_NORMAL
- en: '**Any application of AI and ML will only be as good as the quality of data
    collected.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So, let’s say you’ve [framed your problem and determined that it’s a good fit
    for ML](https://developers.google.com/machine-learning/problem-framing). You know
    what data you need, at least to start experimenting. But unfortunately it’s sitting
    in different systems and scattered across your organisation.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to figure out how to bring that data together, transform it
    as needed, and then land it somewhere as a single integrated dataset. You can
    only begin to explore the data, carry out feature engineering, and model training
    once it is accessible — this is where our friendly acronym ETL comes into play!
  prefs: []
  type: TYPE_NORMAL
- en: ???? How does it work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To make it a bit more concrete, let’s use a modern real world ETL example.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you are an online retailer that uses a Customer Relationship Management
    (CRM) system such as SalesForce to keep track of your registered customers.
  prefs: []
  type: TYPE_NORMAL
- en: You also use a payment processor such as Stripe to handle and store details
    of sales transactions made via your e-commerce website.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose your goal is to improve your conversion rate by using data about what
    your customers purchased historically, to make better product recommendations
    when they are browsing your website.
  prefs: []
  type: TYPE_NORMAL
- en: 'You could certainly use an ML model to power a recommendation engine to achieve
    this goal. But the challenge is that the data you need is sitting in two different
    systems. The solution in our case is to use an ETL process to extract, transform
    and combine them into a data warehouse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating the ETL process](../Images/f5f3cdce7d52dd69603d6bf8334d1f00.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2: The process of moving data from different sources to a warehouse
    using ETL. Illustration by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break down what’s happening in the diagram above:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Extract** — this part of the process involves retrieving data from our
    two sources, SalesForce and Stripe. Once the data has been retrieved, the ETL
    tool will load it into a staging area in preparation for the next step.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Transform** — this is a critical step, because it handles the specifics
    of how our data will be integrated. Any cleansing, reformatting, deduplication,
    and blending of data happens here before it can move further down the pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: In our case, let’s say in one system a customer record is stored with the name
    “K. Reeves”, in another system that same customer record is stored against the
    name “Keanu Reeves”.
  prefs: []
  type: TYPE_NORMAL
- en: Assume we know it’s the same customer (based on their shipping address), but
    the system still needs to reconcile the two, so we don’t end up with duplicate
    records.
  prefs: []
  type: TYPE_NORMAL
- en: ➡️ ETL frameworks and tools provide us with the logic needed to automate this
    sort of transformation, and can cater for many other scenarios too.
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Load** — involves successfully inserting the incoming data into the target
    database, data store, or in our case a data warehouse.'
  prefs: []
  type: TYPE_NORMAL
- en: '**So there you have it, we have collected our data, integrated it using an
    ETL pipeline and loaded it somewhere that is accessible for data science.**'
  prefs: []
  type: TYPE_NORMAL
- en: ???? ***Side note* **????
  prefs: []
  type: TYPE_NORMAL
- en: '**ETL vs. ELT**'
  prefs: []
  type: TYPE_NORMAL
- en: You might have also come across the term ‘ELT’. Extract, load, and transform
    (ELT) differs from ETL solely in where the transformation takes place. In the
    ELT process, the data transformation occurs in the destination data store.
  prefs: []
  type: TYPE_NORMAL
- en: This can simplify the architecture by removing what is sometimes a separate
    or intermediate staging system that hosts the data transformation. The other advantage
    is that you can benefit from the additional scale and compute performance usually
    present in destinations such as cloud data warehouses.
  prefs: []
  type: TYPE_NORMAL
- en: ???? ***Side note* **????
  prefs: []
  type: TYPE_NORMAL
- en: ???? Common challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'OK, all this ETL stuff sounds pretty simple, right? Here are some ‘gotchas’
    to look out for:'
  prefs: []
  type: TYPE_NORMAL
- en: ☄️ Scaling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The amount of data businesses produce is only expected to grow — 175 Zettabytes
    by 2025 according to a report by IDC[2]. So you should ensure that the ETL tool
    you choose has the ability to scale to not just your current but also future needs.
    You may move data in batches now, but will that always be the case? How many jobs
    can you run in parallel?
  prefs: []
  type: TYPE_NORMAL
- en: '**Moving to the cloud is a pretty safe bet if you want to future-proof your
    ETL processes** — by having access to theoretically limitless scalability of storage
    and compute while also reducing your IT capital expenditure.'
  prefs: []
  type: TYPE_NORMAL
- en: ???? Data Accuracy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another big ETL challenge is ensuring that the data you transform is accurate
    and complete. Manual coding and changes or failure to plan and test before running
    an ETL job can sometimes introduce errors, including loading duplicates, missing
    data, and other issues.
  prefs: []
  type: TYPE_NORMAL
- en: An ETL tool will definitely reduce the need for hand-coding and help cut down
    on errors. Data accuracy testing can help spot inconsistencies and duplicates,
    and monitoring features can help identify instances where you are dealing with
    incompatible data types and other data management issues.
  prefs: []
  type: TYPE_NORMAL
- en: ???? Diversity of Data Sources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data is growing in volume. But more importantly, it’s growing in complexity.
    One enterprise could be handling diverse data from hundreds — or even thousands
    — of data sources. These can include structured and semi-structured sources, real-time
    sources, flat files, CSVs, object buckets, streaming sources, and whatever new
    comes along.
  prefs: []
  type: TYPE_NORMAL
- en: Some of this data is best transformed in batches, while for others, streaming,
    continuous data transformation works better.
  prefs: []
  type: TYPE_NORMAL
- en: Having a strategy for how you intend to cope with different data sources is
    key. Some modern ETL tools can offer support for a wide variety, including batch
    and streaming in one place.
  prefs: []
  type: TYPE_NORMAL
- en: ????????‍♀️ So how do I get started?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this point you should have a good idea why and when you might need to use
    ETL in your data science workflow. We also covered common challenges to look out
    for as you begin thinking about your ETL processes.
  prefs: []
  type: TYPE_NORMAL
- en: I’ll close with a simple methodology for choosing an ETL tool, and some other
    useful resources.
  prefs: []
  type: TYPE_NORMAL
- en: ????????‍♀️ Which ETL tool should I use, and when?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So we understand what happens during ETL, but what does it mean in more practical
    terms?
  prefs: []
  type: TYPE_NORMAL
- en: '**You will need to design an ETL pipeline that explicitly describes:**'
  prefs: []
  type: TYPE_NORMAL
- en: What data sources to extract from and how to connect to them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What transformations to carry out on the data once you have it, and finally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where to load the data once the pipeline is complete
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ETL pipelines can be expressed using a code based framework, or a more popular
    choice these days is to use ETL tools that provide a ‘drag and drop’ user interface
    that lets you define the steps in your pipeline in a visual way.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve implemented your ETL pipeline, it typically needs to run somewhere
    i.e. using an ETL tool that will execute your pipeline, and an environment that
    will provide the resources required to temporarily store and transform your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have tried to simplify the decision-making steps for you in the diagram below
    (click to zoom in):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision tree for deciding which ETL tool to use and when](../Images/406a4f7f5ff78624116a03ded80efbe0.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3: Which ETL tool to use and when. Illustration by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**NB. This decision tree is by no means an exhaustive list of either; the decisions
    you will need to make, frameworks or products available.**'
  prefs: []
  type: TYPE_NORMAL
- en: Indeed for every intermediate ETL step, there are dozens of open source and
    proprietary offerings. Ranging from orchestration to scheduling — we’re not going
    to be able to cover everything here.
  prefs: []
  type: TYPE_NORMAL
- en: The aim of this post was to serve as a springboard into the world of ETL! Good
    luck on your data integration journey! ????
  prefs: []
  type: TYPE_NORMAL
- en: ???? Useful resources and further reading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Links**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Data Preparation and Feature Engineering for Machine Learning](https://developers.google.com/machine-learning/data-prep)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Gartner — Data Integration Tools Reviews and Ratings](https://www.gartner.com/reviews/market/data-integration-tools)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '****Books****'
  prefs: []
  type: TYPE_NORMAL
- en: '[The Data Warehouse ETL Toolkit](https://amzn.to/3qNtT85): Practical Techniques
    for Extracting, Cleaning, Conforming, and Delivering Data, Wiley, Authors: Ralph
    Kimball, Joe Caserta'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Streaming Systems](https://amzn.to/3qNHrQT): The What, Where, When, and How
    of Large-Scale Data Processing, O’Reilly, Authors: Tyler Akidau, Slava Chernyak,
    Reuven Lax'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '****Data Agnostic ETL tools****'
  prefs: []
  type: TYPE_NORMAL
- en: '[Fivetran](https://fivetran.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stitch](https://www.stitchdata.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**???? References**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[1] The AI Hierarchy of Needs, Monica Rogati'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://hackernoon.com/the-ai-hierarchy-of-needs-18f111fcc007](https://hackernoon.com/the-ai-hierarchy-of-needs-18f111fcc007)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] 175 Zettabytes By 2025, Forbes, Tom Coughlin'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.forbes.com/sites/tomcoughlin/2018/11/27/175-zettabytes-by-2025/?sh=6a5d2e7a5459](https://www.forbes.com/sites/tomcoughlin/2018/11/27/175-zettabytes-by-2025/?sh=6a5d2e7a5459)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Omer Mahmood](https://www.linkedin.com/in/omahmood/)** is Head of Cloud
    Customer Engineering, CPG & Travel at Google.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/whats-etl-b4903a57f8ce). Reposted
    with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Introducing dbt, the ETL and ELT Disrupter](/2021/03/dbt-etl-elt-disrupter.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Role of the Data Engineer is Changing](/2019/01/role-data-engineer-changing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Why the Future of ETL Is Not ELT, But EL(T)](/2020/12/future-etl-is-elt.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
