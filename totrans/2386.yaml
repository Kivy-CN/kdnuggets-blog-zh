- en: 'Data Transformation: Standardization vs Normalization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/04/data-transformation-standardization-normalization.html](https://www.kdnuggets.com/2020/04/data-transformation-standardization-normalization.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Data Transformation: Standardization vs Normalization](../Images/fb62856a6313e463c04d9e9d31b5d822.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [365datascience](https://365datascience.com/standardization/)
  prefs: []
  type: TYPE_NORMAL
- en: 'Data transformation is one of the fundamental steps in the part of data processing.
    When I first learnt the technique of feature scaling, the terms *scale*, *standardise*,
    and *normalis*e are often being used. However, it was pretty hard to find information
    about which of them I should use and also when to use. Therefore, I’m going to
    explain the following key aspects in this article:'
  prefs: []
  type: TYPE_NORMAL
- en: the difference between Standardisation and Normalisation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: when to use Standardisation and when to use Normalisation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: how to apply feature scaling in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: What Does Feature Scaling Mean?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In practice, we often encounter different types of variables in the same dataset.
    A significant issue is that the range of the variables may differ a lot. Using
    the original scale may put more weights on the variables with a large range. In
    order to deal with this problem, we need to apply the technique of features rescaling
    to independent variables or features of data in the step of data pre-processing.
    The terms *normalisation* and *standardisation* are sometimes used interchangeably,
    but they usually refer to different things.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of applying Feature Scaling is to make sure features are on almost
    the same scale so that each feature is equally important and make it easier to
    process by most ML algorithms
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a dataset that contains an independent variable (Purchased) and 3 dependent
    variables (Country, Age, and Salary). We can easily notice that the variables
    are not on the same scale because the range of *Age* is from 27 to 50, while the
    range of *Salary* going from 48 K to 83 K. The range of *Salary* is much wider
    than the range of *Age*. This will cause some issues in our models since a lot
    of machine learning models such as k-means clustering and nearest neighbour classification
    are based on the Euclidean Distance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Data Transformation: Standardization vs Normalization](../Images/441edcee4f7afe2b81f1a5432fe4734c.png)'
  prefs: []
  type: TYPE_IMG
- en: Focusing on age and salary
  prefs: []
  type: TYPE_NORMAL
- en: When we calculate the equation of Euclidean distance, the number of (x2-x1)²
    is much bigger than the number of (y2-y1)² which means the Euclidean distance
    will be dominated by the salary if we do not apply feature scaling. The difference
    in Age contributes less to the overall difference. Therefore, we should use Feature
    Scaling to bring all values to the same magnitudes and, thus, solve this issue.
    To do this, there are primarily two methods called Standardisation and Normalisation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Data Transformation: Standardization vs Normalization](../Images/6d2fa25d289bd786ae3c1249e8b60b2e.png)'
  prefs: []
  type: TYPE_IMG
- en: Euclidean distance application.
  prefs: []
  type: TYPE_NORMAL
- en: Standardisation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The result of **standardization** (or **Z-score normalization**) is that the
    features will be rescaled to ensure the mean and the standard deviation to be
    0 and 1, respectively. The equation is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data Transformation: Standardization vs Normalization](../Images/77643cc990cf40eb68b3a06c4a5e7904.png)'
  prefs: []
  type: TYPE_IMG
- en: This technique is to re-scale features value with the distribution value between
    0 and 1 is useful for the optimization algorithms, such as gradient descent, that
    are used within machine learning algorithms that weight inputs (e.g., regression
    and neural networks). Rescaling is also used for algorithms that use distance
    measurements, for example, K-Nearest-Neighbours (KNN).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Max-Min Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another common approach is the so-called **Max-Min Normalization (**Min-Max
    scaling). This technique is to re-scales features with a distribution value between
    0 and 1\. For every feature, the minimum value of that feature gets transformed
    into 0, and the maximum value gets transformed into 1\. The general equation is
    shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data Transformation: Standardization vs Normalization](../Images/6a867a912bf5ecce4b1e7d97356cc4a8.png)'
  prefs: []
  type: TYPE_IMG
- en: The equation of Max-Min Normalization
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Standardisation vs Max-Min Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In contrast to standardisation, we will obtain smaller standard deviations through
    the process of Max-Min Normalisation. Let me illustrate more in this area using
    the above dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Data Transformation: Standardization vs Normalization](../Images/dd75ff7d0d85b469fc746efb091a7fdf.png)'
  prefs: []
  type: TYPE_IMG
- en: After Feature scaling
  prefs: []
  type: TYPE_NORMAL
- en: '![Data Transformation: Standardization vs Normalization](../Images/84fd2bbb301990feda0d8d0fc6ce3756.png)'
  prefs: []
  type: TYPE_IMG
- en: Normal distribution and Standard Deviation of Salary
  prefs: []
  type: TYPE_NORMAL
- en: '![Data Transformation: Standardization vs Normalization](../Images/1a1081d694f82f1780da9f2038e87e6b.png)'
  prefs: []
  type: TYPE_IMG
- en: Normal distribution and Standard Deviation of Age
  prefs: []
  type: TYPE_NORMAL
- en: From the above graphs, we can clearly notice that applying Max-Min Nomaralisation
    in our dataset has generated smaller standard deviations (Salary and Age) than
    using Standardisation method. It implies the data are more concentrated around
    the mean if we scale data using Max-Min Nomaralisation.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, if you have outliers in your feature (column), normalizing your
    data will scale most of the data to a small interval, which means all features
    will have the same scale but does not handle outliers well. Standardisation is
    more robust to outliers, and in many cases, it is preferable over Max-Min Normalisation.
  prefs: []
  type: TYPE_NORMAL
- en: When Feature Scaling Matters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![Data Transformation: Standardization vs Normalization](../Images/d54c8f6ac39f36b1ce13ee5fdf41bf8d.png)'
  prefs: []
  type: TYPE_IMG
- en: Some machine learning models are fundamentally based on distance matrix, also
    known as the distance-based classifier, for example, K-Nearest-Neighbours, SVM,
    and Neural Network. Feature scaling is extremely essential to those models, especially
    when the range of the features is very different. Otherwise, features with a large
    range will have a large influence in computing the distance.
  prefs: []
  type: TYPE_NORMAL
- en: Max-Min Normalisation typically allows us to transform the data with varying
    scales so that no specific dimension will dominate the statistics, and it does
    not require making a very strong assumption about the distribution of the data,
    such as k-nearest neighbours and artificial neural networks. However, [Normalisation](https://www.codecademy.com/articles/normalization) does
    not treat outliners very well. On the contrary, standardisation allows users to
    better handle the outliers and facilitate convergence for some computational algorithms
    like gradient descent. Therefore, we usually prefer standardisation over Min-Max
    Normalisation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example: What algorithms need feature scaling**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data Transformation: Standardization vs Normalization](../Images/5112548c35615961899d8e16f7a1b734.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Note**: If an algorithm is not distance-based, feature scaling is unimportant,
    including Naive Bayes, Linear Discriminant Analysis, and Tree-Based models (gradient
    boosting, random forest, etc.).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Summary: Now You Should Know'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: the objective of using Feature Scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the difference between Standardisation and Normalisation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the algorithms that need to apply Standardisation or Normalisation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: applying feature scaling in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Please find the code and dataset **[**here**](https://github.com/clareyan/feasturescaling)**.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/data-transformation-standardisation-vs-normalisation-a47b2f38cec2).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Clare Liu](https://www.linkedin.com/in/clareliuchungyan/)** is a Data Scientist
    at fintech (bank) industry, based in HK. Passionate in resolving mystery about
    data science and machine learning. Join me on the self-learning journey.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Optimizing Data Storage: Exploring Data Types and Normalization in SQL](https://www.kdnuggets.com/optimizing-data-storage-exploring-data-types-and-normalization-in-sql)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Does the Random Forest Algorithm Need Normalization?](https://www.kdnuggets.com/2022/07/random-forest-algorithm-need-normalization.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Evolution in ETL: How Skipping Transformation Enhances Data Management](https://www.kdnuggets.com/evolution-in-etl-how-skipping-transformation-enhances-data-management)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Chatbot Transformation: From Failure to the Future](https://www.kdnuggets.com/2021/12/chatbot-transformation-failure-future.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The AI Transformation Strategy in the GenAI Era](https://www.kdnuggets.com/the-ai-transformation-strategy-in-the-genai-era)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Digital Transformation Playbook for Modern Businesses](https://www.kdnuggets.com/digital-transformation-playbook-for-modern-businesses)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
