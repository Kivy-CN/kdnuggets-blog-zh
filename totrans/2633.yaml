- en: Machine learning adversarial attacks are a ticking time bomb
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/01/machine-learning-adversarial-attacks.html](https://www.kdnuggets.com/2021/01/machine-learning-adversarial-attacks.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf462880f32504c8511171ef0dd53996.png)'
  prefs: []
  type: TYPE_IMG
- en: If you’ve been following news about artificial intelligence, you’ve probably
    heard of or seen modified images of pandas and turtles and stop signs that look
    ordinary to the human eye but cause AI systems to behave erratically. Known as [adversarial
    examples or adversarial attacks](https://bdtechtalks.com/2018/12/27/deep-learning-adversarial-attacks-ai-malware/),
    these images—and their [audio](https://bdtechtalks.com/2019/04/29/ai-audio-adversarial-examples/) and
    textual [counterparts](https://bdtechtalks.com/2019/04/02/ai-nlp-paraphrasing-adversarial-attacks/)—have
    become a source of growing interest and concern for the machine learning community.
  prefs: []
  type: TYPE_NORMAL
- en: But despite the growing body of research on [adversarial machine learning](https://bdtechtalks.com/2020/07/15/machine-learning-adversarial-examples/),
    the numbers show that there has been little progress in tackling adversarial attacks
    in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: The fast-expanding adoption of machine learning makes it paramount that the
    tech community traces a roadmap to secure the AI systems against adversarial attacks.
    Otherwise, adversarial machine learning can be a disaster in the making.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/28f6c254c67fed2d1f983de3e635d807.png)'
  prefs: []
  type: TYPE_IMG
- en: '*AI researchers discovered that by adding small black and white stickers to
    stop signs, they could make them invisible to computer vision algorithms (Source:
    arxiv.org).*'
  prefs: []
  type: TYPE_NORMAL
- en: What makes adversarial attacks different?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Every type of software has its own unique security vulnerabilities, and with
    new trends in software, new threats emerge. For instance, as web applications
    with database backends started replacing static websites, SQL injection attacks
    became prevalent. The widespread adoption of browser-side scripting languages
    gave rise to cross-site scripting attacks. Buffer overflow attacks overwrite critical
    variables and execute malicious code on target computers by taking advantage of
    the way programming languages such as C handle memory allocation. Deserialization
    attacks exploit flaws in the way programming languages such as Java and Python
    transfer information between applications and processes. And more recently, we’ve
    seen a surge in [prototype pollution attacks](https://portswigger.net/daily-swig/prototype-pollution-the-dangerous-and-underrated-vulnerability-impacting-javascript-applications),
    which use peculiarities in the JavaScript language to cause erratic behavior on
    NodeJS servers.
  prefs: []
  type: TYPE_NORMAL
- en: In this regard, adversarial attacks are no different than other cyber threats.
    As machine learning becomes [an important component of many applications](https://bdtechtalks.com/2019/12/30/computer-vision-applications-deep-learning/),
    bad actors will look for ways to plant and trigger malicious behavior in AI models.
  prefs: []
  type: TYPE_NORMAL
- en: What makes adversarial attacks different, however, is their nature and the possible
    countermeasures. For most security vulnerabilities, the boundaries are very clear.
    Once a bug is found, security analysts can precisely document the conditions under
    which it occurs and find the part of the source code that is causing it. The response
    is also straightforward. For instance, SQL injection vulnerabilities are the result
    of not sanitizing user input. Buffer overflow bugs happen when you copy string
    arrays without setting limits on the number of bytes copied from the source to
    the destination.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, adversarial attacks exploit peculiarities in the learned parameters
    of machine learning models. An attacker probes a target model by meticulously
    making changes to its input until it produces the desired behavior. For instance,
    by making gradual changes to the pixel values of an image, an attacker can cause
    the [convolutional neural network](https://bdtechtalks.com/2020/01/06/convolutional-neural-networks-cnn-convnets/) to
    change its prediction from, say, “turtle” to “rifle.” The adversarial perturbation
    is usually a layer of noise that is imperceptible to the human eye.
  prefs: []
  type: TYPE_NORMAL
- en: '(Note: in some cases, such as [data poisoning](https://bdtechtalks.com/2020/10/07/machine-learning-data-poisoning/),
    adversarial attacks are made possible through vulnerabilities in other components
    of the machine learning pipeline, such as a tampered training data set.)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de7ad72408df5d0445d289e0be91c91b.png)'
  prefs: []
  type: TYPE_IMG
- en: '*A neural network thinks this is a picture of a rifle. The human vision system
    would never make this mistake (source: [LabSix](http://www.labsix.org/physical-objects-that-fool-neural-nets/)).*'
  prefs: []
  type: TYPE_NORMAL
- en: The statistical nature of machine learning makes it difficult to find and patch
    adversarial attacks. An adversarial attack that works under some conditions might
    fail in others, such as a change of angle or lighting conditions. Also, you can’t
    point to a line of code that is causing the vulnerability because it spread across
    the thousands and millions of parameters that constitute the model.
  prefs: []
  type: TYPE_NORMAL
- en: Defenses against adversarial attacks are also a bit fuzzy. Just as you can’t
    pinpoint a location in an AI model that is causing an adversarial vulnerability,
    you also can’t find a precise patch for the bug. Adversarial defenses usually
    involve statistical adjustments or general changes to the architecture of the
    machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, one popular method is adversarial training, where researchers
    probe a model to produce adversarial examples and then retrain the model on those
    examples and their correct labels. Adversarial training readjusts all the parameters
    of the model to make it robust against the types of examples it has been trained
    on. But with enough rigor, an attacker can find other noise patterns to create
    adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: The plain truth is, we are still learning how to cope with adversarial machine
    learning. Security researchers are used to perusing code for vulnerabilities.
    Now they must learn to find security holes in machine learning that are composed
    of millions of numerical parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Growing interest in adversarial machine learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recent years have seen a surge in the number of papers on adversarial attacks.
    To track the trend, I searched the arXiv preprint server for papers that mention
    “adversarial attacks” or “adversarial examples” in the abstract section. In [2014](https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=%22adversarial+attack%22&terms-0-field=abstract&terms-1-operator=OR&terms-1-term=%22adversarial+example%22&terms-1-field=abstract&classification-physics_archives=all&classification-include_cross_list=include&date-filter_by=specific_year&date-year=2014&date-from_date=&date-to_date=&date-date_type=submitted_date&abstracts=show&size=50&order=-announced_date_first),
    there were zero papers on adversarial machine learning. [In 2020](https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=%22adversarial+attack%22&terms-0-field=abstract&terms-1-operator=OR&terms-1-term=%22adversarial+example%22&terms-1-field=abstract&classification-physics_archives=all&classification-include_cross_list=include&date-filter_by=specific_year&date-year=2020&date-from_date=&date-to_date=&date-date_type=submitted_date&abstracts=show&size=50&order=-announced_date_first),
    around 1,100 papers on adversarial examples and attacks were submitted to arxiv.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ca23e3db2f231abbe93c59246d42cda.png)'
  prefs: []
  type: TYPE_IMG
- en: '*From 2014 to 2020, arXiv.org has gone from zero papers on adversarial machine
    learning to 1,100 papers in one year.*'
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial attacks and defense methods have also become a key highlight of
    prominent AI conferences such as NeurIPS and ICLR. Even cybersecurity conferences
    such as DEF CON, Black Hat, and Usenix have started featuring workshops and presentations
    on adversarial attacks.
  prefs: []
  type: TYPE_NORMAL
- en: The research presented at these conferences shows tremendous progress in detecting
    adversarial vulnerabilities and developing defense methods that can make machine
    learning models more robust. For instance, researchers have found new ways to
    protect machine learning models against adversarial attacks using [random switching
    mechanisms](https://bdtechtalks.com/2019/08/20/ai-adversarial-examples-hierarchical-random-switching/) and [insights
    from neuroscience](https://bdtechtalks.com/2020/12/07/vonenet-neurscience-inspired-deep-learning/).
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting, however, that AI and security conferences focus on cutting
    edge research. And there’s a sizeable gap between the work presented at AI conferences
    and the practical work done at organizations every day.
  prefs: []
  type: TYPE_NORMAL
- en: The lackluster response to adversarial attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Alarmingly, despite growing interest in and louder warnings on the threat of
    adversarial attacks, there’s very little activity around tracking adversarial
    vulnerabilities in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: I referred to several sources that track bugs, vulnerabilities, and bug bounties.
    For instance, out of more than 145,000 records in the NIST National Vulnerability
    Database, there are no entries on adversarial attacks or adversarial examples.
    A search for “machine learning” returns five results. Most of them are cross-site
    scripting (XSS) and XML external entity (XXE) vulnerabilities in systems that
    contain machine learning components. One of them regards a vulnerability that
    allows an attacker to create a copy-cat version of a machine learning model and
    gain insights, which could be a window to adversarial attacks. But there are no
    direct reports on adversarial vulnerabilities. A search for “deep learning” shows
    a single [critical flaw](https://nvd.nist.gov/vuln/detail/CVE-2017-5719) filed
    in November 2017\. But again, it’s not an adversarial vulnerability but rather
    a flaw in another component of a deep learning system.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/362cef7d3d7e04b914489427224ac384.png)'
  prefs: []
  type: TYPE_IMG
- en: '*The National Vulnerability Database contains very little information on adversarial
    attacks.*'
  prefs: []
  type: TYPE_NORMAL
- en: I also checked GitHub’s Advisory database, which tracks security and bug fixes
    on projects hosted on GitHub. Search for “adversarial attacks,” “adversarial examples,”
    “machine learning,” and “deep learning” yielded no results. A search for “TensorFlow”
    yields 41 records, but they’re mostly bug reports on the codebase of TensorFlow.
    There’s nothing about adversarial attacks or hidden vulnerabilities in the parameters
    of TensorFlow models.
  prefs: []
  type: TYPE_NORMAL
- en: This is noteworthy because GitHub already hosts many [deep learning](https://bdtechtalks.com/2019/02/15/what-is-deep-learning-neural-networks/) models
    and pre-trained [neural networks](https://bdtechtalks.com/2019/08/05/what-is-artificial-neural-network-ann/).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1de45823b690ce5f50dfda94c7905cbe.png)'
  prefs: []
  type: TYPE_IMG
- en: '*GitHub Advisory contains no records on adversarial attacks.*'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, I checked HackerOne, the platform many companies use to run bug bounty
    programs. Here too, none of the reports contained any mention of adversarial attacks.
  prefs: []
  type: TYPE_NORMAL
- en: While this might not be a very precise assessment, the fact that none of these
    sources have anything on adversarial attacks is very telling.
  prefs: []
  type: TYPE_NORMAL
- en: The growing threat of adversarial attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adversarial vulnerabilities are deeply embedded in the many parameters of machine
    learning models, which makes it hard to detect them with traditional security
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3e9cd8cc27c1f7233a8340b6dce235b.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Automated defense is another area that is worth discussing. When it comes
    to code-based vulnerabilities, developers have a large set of defensive tools
    at their disposal.*'
  prefs: []
  type: TYPE_NORMAL
- en: Static analysis tools can help developers find vulnerabilities in their code.
    Dynamic testing tools examine an application at runtime for vulnerable patterns
    of behavior. Compilers already use many of these techniques to track and patch
    vulnerabilities. Today, even your browser is equipped with tools to find and block
    possibly malicious code in client-side script.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, organizations have learned to combine these tools with the
    right policies to enforce secure coding practices. Many companies have adopted
    procedures and practices to rigorously test applications for known and potential
    vulnerabilities before making them available to the public. For instance, GitHub,
    Google, and Apple make use of these and other tools to vet the millions of applications
    and projects uploaded on their platforms.
  prefs: []
  type: TYPE_NORMAL
- en: But the tools and procedures for defending machine learning systems against
    adversarial attacks are still in the preliminary stages. This is partly why we
    see very few reports and advisories on adversarial attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, another worrying trend is the growing use of deep learning models
    by developers of all levels. Ten years ago, only people who had a full understanding
    of machine learning and deep learning algorithms could use them in their applications.
    You had to know how to set up a neural network, tune the hyperparameters through
    intuition and experimentation, and you also needed access to the compute resources
    that could train the model.
  prefs: []
  type: TYPE_NORMAL
- en: But today, integrating a pre-trained neural network into an application is very
    easy.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, PyTorch, which is one of the leading Python deep learning platforms, [has
    a tool](https://pytorch.org/docs/stable/hub.html) that enables machine learning
    engineers to publish pre-trained neural networks on GitHub and make them accessible
    to developers. If you want to integrate an image classifier deep learning model
    into your application, you only need a rudimentary knowledge of deep learning
    and PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Since GitHub has no procedure to detect and block adversarial vulnerabilities,
    a malicious actor could easily use these kinds of tools to publish deep learning
    models that have hidden backdoors and exploit them after thousands of developers
    integrate them in their applications.
  prefs: []
  type: TYPE_NORMAL
- en: How to address the threat of adversarial attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/4283a29402dfb17dd75300b4deb3981a.png)'
  prefs: []
  type: TYPE_IMG
- en: Understandably, given the statistical nature of adversarial attacks, it’s difficult
    to address them with the same methods used against code-based vulnerabilities.
    But fortunately, there have been some positive developments that can guide future
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: The [Adversarial ML Threat Matrix](https://bdtechtalks.com/2020/10/26/adversarial-machine-learning-threat-matrix/),
    published last month by researchers at Microsoft, IBM, Nvidia, MITRE, and other
    security and AI companies, provides security researchers with a framework to find
    weak spots and potential adversarial vulnerabilities in software ecosystems that
    include machine learning components. The Adversarial ML Threat Matrix follows
    the ATT&CK framework, a known and trusted format among security researchers.
  prefs: []
  type: TYPE_NORMAL
- en: Another useful project is IBM’s Adversarial Robustness Toolbox, an open-source
    Python library that provides tools to evaluate machine learning models for adversarial
    vulnerabilities and help developers harden their AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: These and other adversarial defense tools that will be developed in the future
    need to be backed by the right policies to make sure machine learning models are
    safe. Software platforms such as GitHub and Google Play must establish procedures
    and integrate some of these tools into the vetting process of applications that
    include machine learning models. Bug bounties for adversarial vulnerabilities
    can also be a good measure to make sure the machine learning systems used by millions
    of users are robust.
  prefs: []
  type: TYPE_NORMAL
- en: New regulations for the security of machine learning systems might also be necessary.
    Just as the software that handles sensitive operations and information is expected
    to conform to a set of standards, machine learning algorithms used in critical
    applications such as biometric authentication and medical imaging must be audited
    for robustness against adversarial attacks.
  prefs: []
  type: TYPE_NORMAL
- en: As the adoption of machine learning continues to expand, the threat of adversarial
    attacks is becoming more imminent. Adversarial vulnerabilities are a ticking timebomb.
    Only a systematic response can defuse it.
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://bdtechtalks.com/2020/12/16/machine-learning-adversarial-attacks-against-machine-learning-time-bomb/).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Adversarial Examples in Deep Learning – A Primer](https://www.kdnuggets.com/2020/11/adversarial-examples-deep-learning-primer.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Intro to Adversarial Machine Learning and Generative Adversarial Networks](https://www.kdnuggets.com/2019/10/adversarial-machine-learning-generative-adversarial-networks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Why Machine Learning is vulnerable to adversarial attacks and how to fix it](https://www.kdnuggets.com/2019/06/machine-learning-adversarial-attacks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[GPT-4 is Vulnerable to Prompt Injection Attacks on Causing Misinformation](https://www.kdnuggets.com/2023/05/gpt4-vulnerable-prompt-injection-attacks-causing-misinformation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What is Adversarial Machine Learning?](https://www.kdnuggets.com/2022/03/adversarial-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Feature Stores for Real-time AI & Machine Learning](https://www.kdnuggets.com/2022/03/feature-stores-realtime-ai-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cutting Down Implementation Time by Integrating Jupyter and KNIME](https://www.kdnuggets.com/2021/12/cutting-implementation-time-integrating-jupyter-knime.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Market Data and News: A Time Series Analysis](https://www.kdnuggets.com/2022/06/market-data-news-time-series-analysis.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Faster Way to Prepare Time-Series Data with the AI & Analytics Engine](https://www.kdnuggets.com/2021/12/piexchange-faster-way-prepare-timeseries-data-ai-analytics-engine.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
