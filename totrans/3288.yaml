- en: Understanding Deep Learning Requires Re-thinking Generalization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/06/understanding-deep-learning-rethinking-generalization.html](https://www.kdnuggets.com/2017/06/understanding-deep-learning-rethinking-generalization.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Understanding deep learning requires re-thinking generalization](https://openreview.net/forum?id=Sy8gdB9xx&noteId=Sy8gdB9xx)
    Zhang et al., *ICLR’17*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'This paper has a wonderful combination of properties: the results are easy
    to understand, somewhat surprising, and then leave you pondering over what it
    all might mean for a long while afterwards!'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'The question the authors set out to answer was this:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: What is it that distinguishes neural networks that generalize well from those
    that don’t? A satisfying answer to this question would not only help to make neural
    networks more interpretable, but it might also lead to more principled and reliable
    model architecture design.
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: By “generalize well,” the authors simply mean “what causes a network that performs
    well on training data to also perform well on the (held out) test data?” (As opposed
    to transfer learning, which involves applying the trained network to a related
    but different problem). If you think about that for a moment, the question pretty
    much boils down to “why do neural networks work as well as they do?” Generalisation
    is the difference between just memorising portions of the training data and parroting
    it back, and actually developing some meaningful intuition about the dataset that
    can be used to make predictions. So it would be somewhat troubling, would it not,
    if the answer to the question “why do neural networks work (generalize) as well
    as they do?” turned out to be “we don’t really know!”
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: The curious case of the random labels
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our story begins in a familiar place – the CIFAR 10 (50,000 training images
    split across 10 classes, 10,000 validation images) and the ILSVRC (ImageNet) 2012
    (1,281,167 training, 50,000 validation images, 1000 classes) datasets and variations
    of the [Inception](https://blog.acolyer.org/2017/03/21/convolution-neural-nets-part-2/)
    network architecture.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Train the networks using the training data, and you won’t be surprised to hear
    that they can reach zero errors on the *training set*. This is highly indicative
    of *overfitting* – memorising training examples rather than learning true predictive
    features. We can use techniques such as regularisation to combat overfitting,
    leading to networks that generalise better. More on that later.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Take the same training data, but this time randomly jumble the labels (i.e.,
    such that there is no longer any genuine correspondence between the label and
    what’s in the image). Train the networks using these random labels and what do
    you get? *Zero training error!*
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: In [this] case, there is no longer any relationship between the instances and
    the class labels. As a result, learning is impossible. Intuition suggests that
    this impossibility should manifest itself clearly during training, e.g., by training
    not converging or slowing down substantially. To our suprise, several properties
    of the training process for multiple standard architectures is largely unaffected
    by this transformation of the labels.
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'As the authors succinctly put it, “*Deep neural networks easily fit random
    labels*.” Here are three key observations from this first experiment:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: The effective capacity of neural networks is sufficient for memorising the entire
    data set.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Even optimisation on *random labels* remains easy. In fact, training time increases
    by only a small constant factor compared with training on the true labels.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomising labels is solely a data transformation, leaving all other properties
    of the learning problem unchanged.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you take the network trained on random labels, and then see how well it
    performs on the test data, it of course doesn’t do very well at all because it
    hasn’t truly learned anything about the dataset. A fancy way of saying this is
    that it has a high generalisation error. Put all this together and you realise
    that:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: … by randomizing labels alone we can force the generalization error of a model
    to jump up considerably *without changing the model, its size, hyperparameters,
    or the optimizer.* We establish this fact for several different standard architectures
    trained on the CIFAR 10 and ImageNet classification benchmarks. (Emphasis mine).
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Or in other words: the model, its size, hyperparameters, and the optimiser
    cannot explain the generalisation performance of state-of-the-art neural networks.
    This must be the case because the generalisation performance can vary significantly
    while they all remain unchanged.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: The even more curious case of the random images
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What happens if we don’t just mess with the labels, but we also mess with the
    images themselves. In fact, what if just replace the true images with random noise??
    In the figures this is labeled as the ‘Gaussian’ experiment because a Gaussian
    distribution with matching mean and variance to the original image dataset is
    used to generate random pixels for each image.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: In turns out that what happens is the networks train to zero training error
    still, but they get there even faster than the random labels case! A hypothesis
    for why this happens is that the random pixel images are more separated from each
    other than the random label case of images that originally all belonged to the
    same class, but now must be learned as differing classes due to label swaps.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'The team experiment with a spectrum of changes introducing different degrees
    and kinds of randomisation into the dataset:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: true labels (original dataset without modification)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: partially corrupted labels (mess with some of the labels)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: random labels (mess with all of the labels)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: shuffled pixels (choose a pixel permutation, and then apply it uniformly to
    all images)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: random pixels (apply a different random permutation to each image independently)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guassian (just make stuff up for each image, as described previously)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/e95aab14ed016d23369c11732e68ee35.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: All the way along the spectrum, the networks are still able to perfectly fit
    the training data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: We furthermore vary the amount of randomization, interpolating smoothly between
    the case of no noise and complete noise. This leads to a range of intermediate
    learning problems where there remains some level of signal in the labels. We observe
    a steady deterioration of the generalization error as we increase the noise level.
    This shows that neural networks are able to capture the remaining signal in the
    data, while at the same time fit the noisy part using brute-force.
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For me that last sentence is key. Certain choices we make in model architecture
    clearly do make a difference in the ability of a model to generalise (otherwise
    all architectures would generalise the same). The best generalising network in
    the world is still going to have to fallback on memorisation when there is no
    other true signal in the data though. So maybe we need a way to tease apart the
    true potential for generalisation that exists in the dataset, and how efficient
    a given model architecture is at capturing this latent potential. A simple way
    of doing that is to train different architectures on the same dataset! (Which
    we do all the time of course). That still doesn’t help us with the original quest
    though – understanding *why* some models generalise better than others.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Regularization to the rescue?
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The model architecture itself is clearly not a sufficient regulariser (can’t
    prevent overfitting / memorising). But what about commonly used regularisation
    techniques?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'We show that explicit forms of regularization, such as weight decay, dropout,
    and data augmentation, do not adequately explain the generalization error of neural
    networks: *Explicit regularization may improve generalization performance, but
    is neither necessary nor by itself sufficient for controlling generalization error.*'
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/d58e95f0f54f415be1f649119ec37010.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: 'Explicit regularisation seems to be more of a tuning parameter that helps improve
    generalisation, but its absence does not necessarily imply poor generalisation
    error. It is certainly not the case that not all models that fit the training
    data generalise well though. An interesting piece of analysis in the paper shows
    that we pick up a certain amount of regularisation just through the process of
    using gradient descent:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: We analyze how SGD acts as an implicit regularizer. For linear models, SGD always
    converges to a solution with small norm. Hence, the algorithm itself is implicitly
    regularizing the solution… Though this doesn’t explain why certain architectures
    generalize better than other architectures, it does suggest that more investigation
    is needed to understand exactly what the properties are that are inherited by
    models trained using SGD.
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The effective capacity of machine learning models
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider the case of neural networks working with a finite sample size of *n*.
    If a network has *p* parameters, where *p* is greater than *n*, then *even a simple
    two-layer neural network can represent any function of the input sample.* The
    authors prove (in an appendix), the following theorem:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: There exists a two-layer neural network with ReLU activations and *2n + d* weights
    that can represent any function on a sample of size *n* in *d* dimensions.
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Even depth-2 networks of linear size can already represent any labeling of the
    training data!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: So where does this all leave us?
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This situation poses a conceptual challenge to statistical learning theory as
    traditional measures of model complexity struggle to explain the generalization
    ability of large artificial neural networks. We argue that we have yet to discover
    a precise formal measure under which these enormous models are simple. Another
    insight resulting from our experiments is that optimization continues to be empirically
    easy even if the resulting model does not generalize. This shows that the reasons
    for why optimization is empirically easy must be different from the true cause
    of generalization.
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Original](https://blog.acolyer.org/2017/05/11/understanding-deep-learning-requires-re-thinking-generalization/).
    Reposted with permission.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[Deep Learning Papers Reading Roadmap](/2017/06/deep-learning-papers-reading-roadmap.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Learning 101: Demystifying Tensors](/2017/06/deep-learning-demystifying-tensors.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Why Does Deep Learning Not Have a Local Minimum?](/2017/06/deep-learning-local-minimum.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[建立一个高效的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用管道编写清晰的 Python 代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功数据科学家的五个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
