# 使用机器学习寻找系外行星

> 原文：[https://www.kdnuggets.com/2020/01/exoplanet-hunting-machine-learning.html](https://www.kdnuggets.com/2020/01/exoplanet-hunting-machine-learning.html)

[评论](#comments)![图像](../Images/efa1286d320f4636938db561140826e3.png)

来源：[https://weheartit.com/entry/298477443](https://weheartit.com/entry/298477443)

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全领域。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析能力

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你所在组织的IT工作

* * *

我们的太阳系形成于大约46亿年前。我们通过对陨石和放射性的研究知道这一点。所有的一切始于一团气体和尘埃云。附近的一次超新星爆炸可能扰动了这团平静的云，随后它开始由于引力收缩，形成一个平坦的旋转盘，大部分物质集中在中心：[**原恒星**](https://en.wikipedia.org/wiki/Protostar)。后来，引力将剩余的物质拉成团块，并使一些团块变圆，形成了行星和矮行星。剩下的物质则形成了彗星、小行星和流星体。

**但是系外行星是什么？**

系外行星是我们太阳系之外的行星。在过去二十年里，已经发现了数千颗系外行星，大多数是通过[**NASA的开普勒太空望远镜**](https://en.wikipedia.org/wiki/Kepler_space_telescope)发现的。

这些系外行星的大小和轨道种类繁多。有些是紧贴母星的巨大行星；有些则是冰冷的，有些是岩石的。NASA和其他机构正在寻找一种特殊的行星：一种与地球大小相同，围绕类似太阳的恒星在可居住区内运转的行星。

**可居住区**是恒星周围的区域，在这个区域内，液态水可以在周围行星的表面存在，而不会过热或过冷。想象一下，如果地球在冥王星的位置，太阳几乎看不见（大约像一颗豌豆），地球的海洋和大部分大气将会冻结。

![图像](../Images/2b34204387d24ac71528333b718cbf90.png)

可居住区。来源：[https://www.e-education.psu.edu/astro801/content/l12_p4.html](https://www.e-education.psu.edu/astro801/content/l12_p4.html)

[**系外行星：我们太阳系之外的世界**](https://www.space.com/17738-exoplanets.html?jwsource=cl)

系外行星是我们太阳系之外的行星。在过去二十年里，已经发现了数千颗系外行星，大多数是通过…

**为什么要寻找系外行星？**

我们银河系中大约有1000亿颗恒星。我们预期有多少颗系外行星——即太阳系外的行星——存在呢？为什么有些恒星被行星包围？行星系统的多样性有多大？这种多样性是否能告诉我们关于行星形成过程的一些信息？这些都是推动系外行星研究的一些问题。某些系外行星可能具有复杂有机化学存在所需的物理条件（来自恒星的光的量和质量、温度、气氛组成），也许还有可能支持生命的存在（这种生命可能与地球上的生命大相径庭）。

然而，探测系外行星并非易事。我们可能在书籍和电影中幻想过其他星球上的生命已有几个世纪，但实际探测系外行星却是一个近期现象。行星本身几乎不发光。我们之所以能在夜空中看到木星或金星，是因为它们反射了太阳的光线。如果我们要观测一颗系外行星（离我们最近的一颗也在4光年以外），它将非常接近一颗明亮的恒星，使得行星几乎不可见。

![图示](../Images/023e68ed4cf23b132b8e5bbf10e04ba4.png)

来源: [https://media.giphy.com/media/YA2bZh31eFXi0/giphy.gif](https://media.giphy.com/media/YA2bZh31eFXi0/giphy.gif)

科学家们发现了一种非常有效的研究这些现象的方法；行星本身不发光，但它们绕行的恒星会发光。考虑到这一点，NASA的科学家们开发了一种被称为凌日方法的方法，其中使用类似数字相机的技术来检测和测量恒星亮度的微小下降，这些下降发生在行星穿过恒星前面时。通过观察凌日行星，天文学家可以计算行星半径与其恒星半径的比率——即行星阴影的大小——并通过这个比率来计算行星的大小。

开普勒太空望远镜寻找行星的主要方法是“[**凌日**](https://en.wikipedia.org/wiki/Methods_of_detecting_exoplanets)”方法。

> **凌日方法：在下图中，一颗恒星被一颗行星环绕。从图表中可以看出，由于行星在我们的位置上部分遮挡了恒星的光线，恒星的光强度会下降。当行星从恒星前面经过时，光强度会恢复到原来的水平。**

![图示](../Images/0ce1b5ca2765281c4977fdb507f28552.png)

来源: [https://gfycat.com/viciousthaticelandichorse](https://gfycat.com/viciousthaticelandichorse)

直到几年前，天文学家仅确认了不到一千颗系外行星的存在。然后是开普勒任务，系外行星的数量激增。虽然开普勒任务在2018年结束，但[**TESS**](https://exoplanets.nasa.gov/tess/)**任务** **或过境系外行星探测卫星**接替了它，并定期在夜空中发现新的系外行星。TESS监测星星的亮度，以检测因行星过境而引起的周期性下降。TESS任务发现的行星从小型岩石世界到巨大行星不等，展示了银河系中行星的多样性。

我想看看是否可以查看可用的系外行星数据，并预测哪些行星可能适合生命。NASA公开的数据非常有用，因为它包含许多有用的特征。目标是创建一个模型，利用3198颗不同星星的光通量（光强度）读取数据来预测系外行星的存在。

数据集可以从[这里](https://www.kaggle.com/keplersmachines/kepler-labelled-time-series-data)下载。

让我们从导入所有库开始：

```py
import os
import warnings
import math
warnings.filterwarnings('ignore')
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
from pylab import rcParams
rcParams['figure.figsize'] = 10, 6
from sklearn.metrics import mean_squared_error, mean_absolute_error
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split 
from sklearn import linear_model
from sklearn.model_selection import cross_val_score
from sklearn.metrics import precision_score, recall_score,roc_curve,auc, f1_score, roc_auc_score,confusion_matrix, accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler, normalize
from scipy import ndimage
import seaborn as sns
```

加载训练和测试数据。

```py
test_data = pd.read_csv('/Users/nageshsinghchauhan/Downloads/ML/kaggle/exoplanet/exoTest.csv').fillna(0)
train_data = pd.read_csv('/Users/nageshsinghchauhan/Downloads/ML/kaggle/exoplanet/exoTrain.csv').fillna(0)train_data.head()
```

![图像](../Images/794ba68f4eff5ae97394fe5106c5a227.png)

数据集

现在目标列`LABEL`包含两个类别**1**（不表示系外行星）和**2**（表示系外行星的存在）。因此，将其转换为二进制值以便于数据处理。

```py
categ = {2: 1,1: 0}
train_data.LABEL = [categ[item] for item in train_data.LABEL]
test_data.LABEL = [categ[item] for item in test_data.LABEL]
```

在继续之前，我们还需要减少测试和训练数据框使用的内存量。

```py
#Reduce memory
def reduce_memory(df):
    """ iterate through all the columns of a dataframe and modify the data type
        to reduce memory usage.        
    """
    start_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))

    for col in df.columns:
        col_type = df[col].dtype

        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        else:
            df[col] = df[col].astype('category')end_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))
    return dftest_data = reduce_memory(test_data)#Output
Memory usage of dataframe is 13.91 MB
Memory usage after optimization is: 6.25 MB
Decreased by 55.1%
```

这一步是为了内存优化，已将`test_data`数据框的内存使用量减少了55.1%，你也可以对`train_data`数据框进行相同处理。

现在可视化`train_dataset`中的目标列，并了解类别分布情况。

```py
plt.figure(figsize=(6,4))
colors = ["0", "1"]
sns.countplot('LABEL', data=train_data, palette=colors)
plt.title('Class Distributions \n (0: Not Exoplanet || 1: Exoplanet)', fontsize=14)
```

![图像](../Images/9eb693a2604cc7677b5508e8ab2e1b2c.png)

目标变量的类别分布。

结果发现数据高度不平衡。因此，我们首先从数据预处理技术开始。

让我们绘制训练数据的前4行，并观察光通量值的强度。

```py
from pylab import rcParams
rcParams['figure.figsize'] = 13, 8
plt.title('Distribution of flux values', fontsize=10)
plt.xlabel('Flux values')
plt.ylabel('Flux intensity')
plt.plot(train_data.iloc[0,])
plt.plot(train_data.iloc[1,])
plt.plot(train_data.iloc[2,])
plt.plot(train_data.iloc[3,])
plt.show()
```

![](../Images/4cc9ec153f85ab700bfde46eaa1bbe41.png)

好吧，我们的数据是干净的，但还没有标准化。让我们绘制非系外行星数据的高斯直方图。

```py
labels_1=[100,200,300]
for i in labels_1:
    plt.hist(train_data.iloc[i,:], bins=200)
    plt.title("Gaussian Histogram")
    plt.xlabel("Flux values")
    plt.show()
```

![图像](../Images/a0822a9cc24b40226c8a61946cac1b92.png)  ![图像](../Images/9001ca8ea5774715a00fed55e45e9145.png)  ![图像](../Images/93d1fc1e345f143f9d5248bf2542bd92.png)

系外行星的缺失

现在绘制存在系外行星时的数据高斯直方图。

```py
labels_1=[16,21,25]
for i in labels_1:
    plt.hist(train_data.iloc[i,:], bins=200)
    plt.title("Gaussian Histogram")
    plt.xlabel("Flux values")
    plt.show()
```

![图像](../Images/c16b2907c1494661c2154a011cf89981.png)  ![图像](../Images/0315edf6f2973d1af16a430b293f2490.png)  ![图像](../Images/b45f6147421e72db54c26e172d68427a.png)

系外行星的存在

所以让我们首先拆分数据集并进行标准化。

```py
x_train = train_data.drop(["LABEL"],axis=1)
y_train = train_data["LABEL"]   
x_test = test_data.drop(["LABEL"],axis=1)
y_test = test_data["LABEL"]
```

**数据归一化**是机器学习数据准备中常用的一种技术。归一化的目标是将数据集中数值列的值转换为一个共同的尺度，而不扭曲值范围之间的差异。

```py
x_train = normalized = normalize(x_train)
x_test = normalize(x_test)
```

下一步是对测试和训练数据应用高斯滤波器。

在[**概率论**](https://en.m.wikipedia.org/wiki/Probability_theory)中，**正态**（或**高斯**或**高斯分布**或**拉普拉斯-高斯**）**分布**是一种非常常见的[连续概率分布](https://en.m.wikipedia.org/wiki/Continuous_probability_distribution)。正态分布在统计学中很重要，常用于自然科学和社会科学中表示那些分布未知的实值随机变量。

> 正态分布之所以有用是因为[**中心极限定理**](https://en.m.wikipedia.org/wiki/Central_limit_theorem)。在其最一般的形式下，在一些条件下（包括有限的[方差](https://en.m.wikipedia.org/wiki/Variance)），它指出来自同一分布的随机变量的观察样本的平均值在分布上收敛于正态分布，即，当观察数量足够大时，它们变成正态分布。预期是许多独立过程的总和的物理量通常具有近似正态的分布。

```py
x_train = filtered = ndimage.filters.gaussian_filter(x_train, sigma=10)
x_test = ndimage.filters.gaussian_filter(x_test, sigma=10)
```

我们使用特征缩放，使所有值保持在可比较的范围内。

```py
#Feature scaling
std_scaler = StandardScaler()
x_train = scaled = std_scaler.fit_transform(x_train)
x_test = std_scaler.fit_transform(x_test)
```

我们处理的列/特征数量非常庞大。我们的训练数据集中有5087行和3198列。基本上，我们需要减少特征数量（降维），以消除[**维度诅咒**](https://en.wikipedia.org/wiki/Curse_of_dimensionality)的可能性。

为了减少维度/特征的数量，我们将使用最流行的降维算法，即**PCA（主成分分析）**。

要执行PCA，我们必须选择我们数据中想要的特征/维度的数量。

```py
#Dimentioanlity reduction
from sklearn.decomposition import PCA
pca = PCA() 
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)
total=sum(pca.explained_variance_)
k=0
current_variance=0
while current_variance/total < 0.90:
    current_variance += pca.explained_variance_[k]
    k=k+1
```

上述代码给出了**k=37**。

现在我们选择k=37并在我们的独立变量上应用PCA。

```py
#Apply PCA with n_componenets
pca = PCA(n_components=37)
x_train = pca.fit_transform(x_train)
x_test = pca.transform(x_test)
plt.figure()
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Variance (%)') #for each component
plt.title('Exoplanet Dataset Explained Variance')
plt.show()
```

![](../Images/d045912d373721889541b5d5f661e9b0.png)

上述图表告诉我们，选择37个主成分可以保留数据总方差的约98.8%或99%。这是合理的，我们不会使用100%的方差，因为它表示所有主成分，而我们只需要主要的几个。

![](../Images/9f56c846369d3faab37ef982fc8da626.png)

列的数量在测试和训练数据集中都减少到了37。

现在进入下一步，正如我们所知，目标类的分布是不均衡的，一个类主导了另一个。因此，我们需要重新采样数据，以使目标类均匀分布。

有4种方法来解决这些类不平衡问题：

+   合成新的少数类实例

+   对少数类进行过采样

+   对多数类进行欠采样

+   调整成本函数，使得少数类实例的误分类比多数类实例的误分类更重要。

```py
#Resampling
print("Before OverSampling, counts of label '1': {}".format(sum(y_train==1)))
print("Before OverSampling, counts of label '0': {} \n".format(sum(y_train==0)))sm = SMOTE(random_state=27, ratio = 1.0)
x_train_res, y_train_res = sm.fit_sample(x_train, y_train.ravel())print("After OverSampling, counts of label '1': {}".format(sum(y_train_res==1)))
print("After OverSampling, counts of label '0': {}".format(sum(y_train_res==0)))
```

我们使用了**SMOTE**（合成少数类过采样技术）重采样方法。这是一种过采样方法。它的作用是创建少数类的合成（而非重复）样本，从而使少数类等于多数类。SMOTE通过选择相似记录并一次调整一列，以随机量改变该记录与邻近记录的差异。

```py
Before OverSampling, counts of label '1': 37
Before OverSampling, counts of label '0': 5050 

After OverSampling, counts of label '1': 5050
After OverSampling, counts of label '0': 5050
```

现在我们要构建一个能够对测试数据中的系外行星进行分类的模型。

我将创建一个函数`model`，它将：

1.  拟合模型

1.  执行交叉验证

1.  检查我们模型的准确性

1.  生成分类报告

1.  生成混淆矩阵

```py
def model(classifier,dtrain_x,dtrain_y,dtest_x,dtest_y):
    #fit the model
    classifier.fit(dtrain_x,dtrain_y)
    predictions = classifier.predict(dtest_x)

    #Cross validation
    accuracies = cross_val_score(estimator = classifier, X = x_train_res, y = y_train_res, cv = 5, n_jobs = -1)
    mean = accuracies.mean()
    variance = accuracies.std()
    print("Accuracy mean: "+ str(mean))
    print("Accuracy variance: "+ str(variance))

    #Accuracy
    print ("\naccuracy_score :",accuracy_score(dtest_y,predictions))

    #Classification report
    print ("\nclassification report :\n",(classification_report(dtest_y,predictions)))

    #Confusion matrix
    plt.figure(figsize=(13,10))
    plt.subplot(221)
    sns.heatmap(confusion_matrix(dtest_y,predictions),annot=True,cmap="viridis",fmt = "d",linecolor="k",linewidths=3)
    plt.title("CONFUSION MATRIX",fontsize=20)
```

始终需要验证机器学习模型的稳定性。我的意思是，你不能仅仅将模型拟合到你的训练数据上，并希望它在从未见过的真实数据上也能准确工作。***你需要一些保证，确保你的模型已经从数据中正确地捕捉到大部分模式，并且它没有过多地受到噪音的影响，换句话说，它的偏差和方差都很低。***

现在将支持向量机（SVM）算法拟合到训练集上并进行预测。

```py
from sklearn.svm import SVC
SVM_model=SVC()
model(SVM_model,x_train_res,y_train_res,x_test,y_test)
```

![](../Images/ed7228718a7447f4bc1764ff0daef518.png)

![](../Images/87ac1716a42a21783bc2c67450de0ae6.png)

另外，尝试随机森林模型并获取特征重要性，但在此之前请将以下代码包含在函数模型中。

```py
#Display feature importance   
    df1 = pd.DataFrame.from_records(dtrain_x)     
    tmp = pd.DataFrame({'Feature': df1.columns, 'Feature importance': classifier.feature_importances_})
    tmp = tmp.sort_values(by='Feature importance',ascending=False)
    plt.figure(figsize = (7,4))
    plt.title('Features importance',fontsize=14)
    s = sns.barplot(x='Feature',y='Feature importance',data=tmp)
    s.set_xticklabels(s.get_xticklabels(),rotation=90)
    plt.show()
```

并调用随机森林分类算法。

```py
from sklearn.ensemble import RandomForestClassifier
rf_classifier = RandomForestClassifier()
model(rf_classifier,x_train_res,y_train_res,x_test,y_test)
```

![](../Images/70ed6182436ec7fff1766fca6f656a93.png)

![](../Images/f132e7c156545845582b5159caabdefc.png)

通常，**特征重要性**提供一个分数，表示每个**特征**在构建模型中的有用性或价值。特征在决策树中用于做关键决策的频率越高，其相对**重要性**越高。

![](../Images/e01f93b8f1b8d6bacd52bd61aeef2d37.png)

我们可以看到，从SVM和随机森林算法中得到了相当不错的结果。然而，你可以继续调整参数，也可以使用其他算法，查看准确率的差异。

现在，让我们尝试使用Keras Python库来解决相同的问题。

```py
from tensorflow import set_random_seed
set_random_seed(101)
from sklearn.model_selection import cross_val_score
from keras.wrappers.scikit_learn import KerasClassifier
from keras.models import Sequential # initialize neural network library
from keras.layers import Dense # build our layers library
def build_classifier():
    classifier = Sequential() # initialize neural network
    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train_res.shape[1]))
    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))
    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))
    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
    return classifierclassifier = KerasClassifier(build_fn = build_classifier, epochs = 40)
accuracies = cross_val_score(estimator = classifier, X = x_train_res, y = y_train_res, cv = 5, n_jobs = -1)
mean = accuracies.mean()
variance = accuracies.std()
print("Accuracy mean: "+ str(mean))
print("Accuracy variance: "+ str(variance))#Accuracy mean: 0.9186138613861387
#Accuracy variance: 0.07308084375906461
```

神经网络模型的准确率均值为91.86%

在交叉验证后，准确率的变化为7.30%，这是一个相当不错的结果。

### 结论：未来

我们能够从遥远的星星中收集光线，研究这些已经旅行了数千年的光线，并对这些星星可能存在的潜在世界得出结论，这真是令人惊叹。

![图](../Images/da105b1d806ff1a605968073cff727e2.png)

来源：[https://astrobiology.nasa.gov/news/the-just-approved-european-ariel-mission-will-be-first-dedicated-to-probing-exoplanet-atmospheres/](https://astrobiology.nasa.gov/news/the-just-approved-european-ariel-mission-will-be-first-dedicated-to-probing-exoplanet-atmospheres/)

在接下来的 10 年内，直径 30 到 40 米的望远镜将在地球上运作，通过成像和恒星的速度变化来探测系外行星。包括 Cheops、[JWST](https://www.jwst.nasa.gov/)、Plato 和 Ariel 在内的卫星望远镜将被发射以通过凌日法探测行星。JWST 还将进行直接成像。NASA 正在设计直径 8 到 18 米的大型空间望远镜（LUVOIR、Habex），以期在 2050 年之前探测系外行星上的生命迹象。

在更遥远的未来，巨大的空间 [干涉仪](https://en.wikipedia.org/wiki/Interferometry) 将绘制行星的详细地图。可能还会发射星际探测器前往最近的系外行星以拍摄特写照片。工程师们已经在研发推进技术，以到达这些遥远的目标。

在这篇文章中，我们使用机器学习模型和神经网络预测了系外行星的存在。

好了，这篇文章就到这里，希望你们喜欢阅读。如果这篇文章对你有所帮助，我会很高兴。欢迎在评论区分享你的意见/想法/反馈。

![图像](../Images/354a692d250030ec20654f39dd6202a5.png)

来源：[https://imgur.com/gallery/qcU0h](https://imgur.com/gallery/qcU0h)

你可以在我的 [Github 仓库](https://github.com/nageshsinghc4/Exoplanet-exploration) 中找到代码：

感谢阅读！！！

**简介：[纳格什·辛格·乔汉](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)** 是一名数据科学爱好者，对大数据、Python 和机器学习感兴趣。

[原文](https://towardsdatascience.com/exoplanet-hunting-using-machine-learning-d615958e1787)。经授权转载。

**相关内容：**

+   [通过时间序列分析进行股票市场预测](/2020/01/stock-market-forecasting-time-series-analysis.html)

+   [使用 5 种机器学习算法对稀有事件进行分类](/2020/01/classify-rare-event-machine-learning-algorithms.html)

+   [用开放数据进行地理可视化](/2020/01/open-data-germany-maps-viz.html)

### 更多相关内容

+   [为什么越来越多的开发者在他们的机器学习项目中使用 Python？](https://www.kdnuggets.com/2022/01/developers-python-machine-learning-projects.html)

+   [使用 SHAP 值进行机器学习模型的可解释性分析](https://www.kdnuggets.com/2023/08/shap-values-model-interpretability-machine-learning.html)

+   [使用基本和现代算法解决计算机科学问题……](https://www.kdnuggets.com/2023/11/packt-tackle-computer-science-problems-fundamental-modern-algorithms-machine-learning)

+   [构建 GPU 机器 vs. 使用 GPU 云](https://www.kdnuggets.com/building-a-gpu-machine-vs-using-the-gpu-cloud)

+   [每个机器学习工程师应该掌握的 5 项机器学习技能](https://www.kdnuggets.com/2023/03/5-machine-learning-skills-every-machine-learning-engineer-know-2023.html)

+   [KDnuggets 新闻，12 月 14 日：3 门免费机器学习课程](https://www.kdnuggets.com/2022/n48.html)
