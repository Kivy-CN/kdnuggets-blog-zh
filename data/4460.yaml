- en: The Most Important Fundamentals of PyTorch you Should Know
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你应该知道的 PyTorch 最重要的基础知识
- en: 原文：[https://www.kdnuggets.com/2020/06/fundamentals-pytorch.html](https://www.kdnuggets.com/2020/06/fundamentals-pytorch.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/06/fundamentals-pytorch.html](https://www.kdnuggets.com/2020/06/fundamentals-pytorch.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '![](../Images/d2969ec5e746ad4540d7f8cf3b322008.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2969ec5e746ad4540d7f8cf3b322008.png)'
- en: Fundamentals of PyTorch – Introduction
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch 基础知识 – 介绍
- en: Since it was introduced by the Facebook AI Research (FAIR) team, back in early
    2017, [PyTorch](https://www.exxactcorp.com/PyTorch) has become a highly popular
    and widely used Deep Learning (DL) framework. Since the humble beginning, it has
    caught the attention of serious AI researchers and practitioners around the world,
    both in industry and academia, and has matured significantly over the years.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 2017 年初由 Facebook AI 研究（FAIR）团队推出以来，[PyTorch](https://www.exxactcorp.com/PyTorch)
    已成为一个非常流行且广泛使用的深度学习（DL）框架。从 humble 开始，它吸引了全球各地的严肃 AI 研究人员和从业者，无论是工业界还是学术界，并且在这些年中显著成熟。
- en: Scores of DL enthusiasts and professionals started their journey with the Google
    TensorFlow (TF), but the learning curve with base TensorFlow has always been steep.
    On the other hand, PyTorch has approached DL programming in an intuitive fashion
    since the beginning, focusing on fundamental linear algebra and data flow operations
    in a manner that is easily understood and amenable to step-by-step learning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 众多深度学习爱好者和专业人士从 Google TensorFlow（TF）开始了他们的旅程，但基础 TensorFlow 的学习曲线一直很陡峭。另一方面，PyTorch
    自开始以来便以直观的方式进行深度学习编程，专注于基本的线性代数和数据流操作，易于理解，适合逐步学习。
- en: Due to this modular approach, building and experimenting with complex DL architectures
    has been much easier with PyTorch than following the somewhat rigid framework
    of TF and TF-based tools. Moreover, PyTorch was built to integrate seamlessly
    with the numerical computing infrastructure of the Python ecosystem and Python
    being the lingua franca of data science and machine learning, it has ridden over
    that wave of increasing popularity.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种模块化的方法，使用 PyTorch 构建和实验复杂的深度学习架构比遵循 TF 及其相关工具的相对僵化的框架要容易得多。此外，PyTorch 是为了与
    Python 生态系统的数值计算基础设施无缝集成而构建的，而 Python 作为数据科学和机器学习的通用语言，它也顺应了这一日益增长的普及浪潮。
- en: Tensor Operations with PyTorch
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch 的张量操作
- en: '[Tensors](https://www.kdnuggets.com/2018/05/wtf-tensor.html) are at the heart
    of any DL framework. PyTorch provides tremendous flexibility to a programmer about
    how to create, combine, and process tensors as they flow through a network (called
    computational graph) paired with a relatively high-level, object-oriented API.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[张量](https://www.kdnuggets.com/2018/05/wtf-tensor.html) 是任何深度学习框架的核心。PyTorch
    为程序员提供了巨大的灵活性，关于如何创建、组合和处理张量，它们在网络（称为计算图）中流动，同时配有相对高级的面向对象 API。'
- en: '**What are Tensors?**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是张量？**'
- en: Representing data (e.g., about the physical world or some business process)
    for Machine Learning (ML), in particular for DNN, is accomplished via a data/mathematical
    structure known as the *tensor*. A tensor is a container that can house data in *N* dimensions.
    A tensor is often used interchangeably with another more familiar mathematical
    object *matrix* (which is specifically a 2-dimensional tensor). In fact, tensors
    are generalizations of 2-dimensional matrices to *N*-dimensional space.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习（ML），特别是在深度神经网络（DNN）中，表示数据（例如关于物理世界或某些业务过程的数据）是通过一种称为 *张量* 的数据/数学结构完成的。张量是一个可以容纳*N*维数据的容器。张量通常与另一个更为熟悉的数学对象
    *矩阵*（具体是 2 维张量）交替使用。实际上，张量是对 2 维矩阵在 *N* 维空间的推广。
- en: In simplistic terms, one can think of scalar-vectors-matrices- tensors as a
    flow.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，可以将标量-向量-矩阵-张量视为一个流。
- en: Scalar are 0-dimensional tensors.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标量是 0 维张量。
- en: Vectors are 1-dimensional tensors.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量是 1 维张量。
- en: Matrices are 2-dimensional tensors
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵是 2 维张量
- en: Tensors are generalized N-dimensional *tensors*. N can be anything from 3 to
    infinity…
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量是广义的 N 维 *张量*。N 可以是 3 到无限大…
- en: Often, these dimensions are also called *ranks*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些维度通常也被称为 *秩*。
- en: '![](../Images/230f7ae97d3d12688a50fdee8f7e4a3d.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/230f7ae97d3d12688a50fdee8f7e4a3d.png)'
- en: '*Fig 1: Tensors of various dimensions (ranks) ([Image source](https://medium.com/mlait/tensors-representation-of-data-in-neural-networks-bbe8a711b93b)).*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1：各种维度（秩）的张量 ([图片来源](https://medium.com/mlait/tensors-representation-of-data-in-neural-networks-bbe8a711b93b)).*'
- en: '**Why Are Tensors Important for ML and DL?**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**张量对 ML 和 DL 重要的原因是什么？**'
- en: Think of a supervised ML problem. You are given a table of data with some labels
    (could be numerical entities or binary classification such as Yes/No answers).
    For ML algorithms to process it, the data must be fed as a mathematical object.
    A table is naturally equivalent to a 2-D matrix where an individual row (or instance)
    or individual column (or feature) can be treated as a 1-D vector.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 设想一个监督学习问题。你获得了一张包含一些标签的数据表（可能是数值实体或二元分类，如是/否回答）。为了让ML算法处理这些数据，数据必须以数学对象的形式输入。表自然等同于二维矩阵，其中每一行（或实例）或每一列（或特征）可以视作一维向量。
- en: Similarly, a black-and-white image can be treated as a 2-D matrix containing
    numbers 0 or 1\. This can be fed into a neural network for image classification
    or segmentation tasks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，黑白图像可以视作一个包含数字0或1的二维矩阵。这可以输入到神经网络中进行图像分类或分割任务。
- en: A time-series or sequence data (e.g., ECG data from a monitoring machine or
    a stock market price tracking data stream) is another example of 2-D data where
    one dimension (time) is fixed.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列或序列数据（例如，来自监测机器的ECG数据或股票市场价格跟踪数据流）是二维数据的另一个例子，其中一个维度（时间）是固定的。
- en: These are examples of using 2-D tensors in classical ML (e.g., linear regression,
    support vector machines, decision trees, etc.) and DL algorithms.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是使用二维张量的经典机器学习（例如线性回归、支持向量机、决策树等）和深度学习算法的示例。
- en: Going beyond 2-D, a color or grayscale image can be treated as a 3-D tensor
    where each pixel is associated with a so-called ‘color-channel’ – a vector of
    3 numbers representing intensities in the Red-Green-Blue (RGB) spectrum. This
    is an example of a 3-D tensor.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 超越二维，彩色或灰度图像可以视作三维张量，其中每个像素与所谓的‘颜色通道’相关联——一个由3个数字组成的向量，表示红绿蓝（RGB）光谱中的强度。这是一个三维张量的例子。
- en: Similarly, videos can be thought of as sequences of color images (or frames)
    in time and can be thought of as 4-D tensors.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，视频可以看作是时间上的颜色图像（或帧）序列，可以视为四维张量。
- en: In short, all kinds of data from the physical world, sensors and instruments,
    business and finance, scientific or social experiments, can be easily represented
    by multi-dimensional tensors to make them amenable for processing by ML/DL algorithms
    inside a computing machine.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，来自物理世界、传感器和仪器、商业和金融、科学或社会实验的各种数据，都可以通过多维张量轻松表示，以便让计算机中的ML/DL算法进行处理。
- en: Let’s see how PyTorch defines and handles tensors.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看PyTorch如何定义和处理张量。
- en: Creating and Converting Tensors in PyTorch
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在PyTorch中创建和转换张量
- en: Tensors can be defined from a Python list as follows,
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 张量可以从Python列表定义如下，
- en: '![](../Images/1a645f3d7902d5ce23a854604d8b7a32.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a645f3d7902d5ce23a854604d8b7a32.png)'
- en: Actual elements can be accessed and indexed as follows,
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 实际元素可以按如下方式访问和索引，
- en: '![](../Images/176f1784554eb13d6d89b77dea3bd98c.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/176f1784554eb13d6d89b77dea3bd98c.png)'
- en: Tensors with specific data types can be created easily (e.g., floating points),
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 可以轻松创建具有特定数据类型的张量（例如，浮点数），
- en: '![](../Images/fa5c22461ae055d6eeba75130f44bded.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa5c22461ae055d6eeba75130f44bded.png)'
- en: Size and dimensions can be read easily,
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 大小和维度可以很容易地读取，
- en: '![](../Images/ed711fb8f4d40821ee2a5059feb39b7b.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed711fb8f4d40821ee2a5059feb39b7b.png)'
- en: We can change the view of a tensor. Let us start with a 1-dimensional tensor
    as follows,
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以更改张量的视图。让我们从以下一维张量开始，
- en: '![](../Images/69d4901515ecbf307f803ebda11f0e4e.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69d4901515ecbf307f803ebda11f0e4e.png)'
- en: Then change the view to a 2-D tensor,
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将视图更改为二维张量，
- en: '![](../Images/6edac16b9d2c56f0f1901236417dc92d.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6edac16b9d2c56f0f1901236417dc92d.png)'
- en: Changing back and forth between a PyTorch tensor and a NumPy array is easy and
    efficient.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch张量和NumPy数组之间来回转换是简单高效的。
- en: '![](../Images/3a1ea52f3b7034fde69e0e3edbfd6d2f.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a1ea52f3b7034fde69e0e3edbfd6d2f.png)'
- en: Converting from a Pandas series object is also easy,
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从Pandas系列对象转换也很简单，
- en: '![](../Images/b56ef4ea2a7282ecb74362422c9d3a3a.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b56ef4ea2a7282ecb74362422c9d3a3a.png)'
- en: Finally, converting back to a Python list can be accomplished,
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，可以完成转换回Python列表的操作，
- en: '![](../Images/4dd65f6c4260e45877800ed96e338c72.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4dd65f6c4260e45877800ed96e338c72.png)'
- en: '**Vector and matrix mathematics with PyTorch tensors**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**PyTorch 张量的向量和矩阵数学**'
- en: PyTorch provides an easy-to-understand API and programmatic toolbox to manipulate
    tensors mathematically. We show basic operations with 1-D and 2-D tensors here.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch提供了一个易于理解的API和程序化工具箱，用于在数学上操作张量。我们在这里展示了1维和2维张量的基本操作。
- en: Simple vector addition,
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的向量加法，
- en: '![](../Images/792db69dd3d8147151c0489df387b266.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/792db69dd3d8147151c0489df387b266.png)'
- en: Vector multiplication with a scalar,
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 向量与标量的乘法，
- en: '![](../Images/2585875d695b37dae47730cf28dae5bb.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2585875d695b37dae47730cf28dae5bb.png)'
- en: Linear combination,
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 线性组合，
- en: '![](../Images/5a452cd3fe0cc1313c43d1d34154fa4f.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a452cd3fe0cc1313c43d1d34154fa4f.png)'
- en: Element-wise product,
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 元素逐位乘积，
- en: '![](../Images/650b8f02bbdc14f58b39dc29346e57b9.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/650b8f02bbdc14f58b39dc29346e57b9.png)'
- en: Dot product,
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 点积，
- en: '![](../Images/b7ddef771720eb6ea283cff17f1f34dc.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7ddef771720eb6ea283cff17f1f34dc.png)'
- en: Adding a scalar to every element of a tensor, i.e., broadcasting,
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个标量添加到张量的每个元素中，即广播，
- en: '![](../Images/7fd9c5cab8f1dd8f93a69d0f1311a6cd.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7fd9c5cab8f1dd8f93a69d0f1311a6cd.png)'
- en: Creating 2-D tensor from list of lists,
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从列表列表中创建 2-D 张量，
- en: '![](../Images/1423ffffc615874e9c2f1bccf51152a1.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1423ffffc615874e9c2f1bccf51152a1.png)'
- en: Slicing and indexing of matrix elements,
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵元素的切片和索引，
- en: '![](../Images/4b5cfe3f44046093ae52160707c246ec.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b5cfe3f44046093ae52160707c246ec.png)'
- en: Matrix multiplication,
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法，
- en: '![](../Images/c9a5efb99bbea4ad4012f1630eee0e75.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9a5efb99bbea4ad4012f1630eee0e75.png)'
- en: Matrix transpose,
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵转置，
- en: '![](../Images/7b7c3c5960a6476daafb186a9008df0d.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b7c3c5960a6476daafb186a9008df0d.png)'
- en: Matrix inverse and determinant,
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的逆和行列式，
- en: '![](../Images/84422c9a7221bfd93314a8f2c68dbb53.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84422c9a7221bfd93314a8f2c68dbb53.png)'
- en: 'Autograd: Automatic differentiation'
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动求导：自动微分
- en: 'Neural network training and prediction involves taking derivatives of various
    functions (tensor-valued) over and over. The Tensor object supports the magical
    Autograd feature, i.e., automatic differentiation, which is achieved by tracking
    and storing all the operations performed on the tensor while it flows through
    a network. You can watch this wonderful tutorial video for a visual explanation:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络训练和预测涉及对各种函数（张量值）进行反复求导。Tensor 对象支持神奇的 Autograd 特性，即自动微分，这通过跟踪和存储张量在网络中流动时执行的所有操作来实现。你可以观看这个精彩的教程视频以获得可视化的解释：
- en: The Pytorch autograd [**official documentation is here**](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 的自动求导 [**官方文档在这里**](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)。
- en: We show simple examples to illustrate the autograd feature of PyTorch.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示简单的示例来说明 PyTorch 的自动求导特性。
- en: '![](../Images/0f3e5c1f6916685796339bd0e5c3f05f.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f3e5c1f6916685796339bd0e5c3f05f.png)'
- en: We define a generic function and a tensor variable ***x***, then define another
    variable ***y*** assigning it to the function of ***x***.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个通用函数和一个张量变量 ***x***，然后定义另一个变量 ***y***，将其赋值为 ***x*** 的函数。
- en: '![](../Images/17fb29b0b9c5c79077442c3b983a802f.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17fb29b0b9c5c79077442c3b983a802f.png)'
- en: Then, we use a special **backward()** method on ***y*** to take the derivative
    and calculate the derivative value at the given value of ***x***.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在 ***y*** 上使用特殊的 **backward()** 方法来进行求导，并计算在给定 ***x*** 值下的导数值。
- en: '![](../Images/5e7047c8e0a61295c48b8e7c569618f9.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e7047c8e0a61295c48b8e7c569618f9.png)'
- en: We can also deal with partial derivatives!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以处理偏导数！
- en: '![](../Images/7ad48aa7236ee4fe387a62d85e7ab384.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ad48aa7236ee4fe387a62d85e7ab384.png)'
- en: We can define ***u*** and ***v*** as tensor variables, define a function combining
    them, apply the **backward()** method, and calculate the partial derivatives.
    See below,
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 ***u*** 和 ***v*** 定义为张量变量，定义一个结合它们的函数，应用 **backward()** 方法，并计算偏导数。见下图，
- en: '![](../Images/0bf12a5f2ee5ce5649bd279fb6244ab3.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0bf12a5f2ee5ce5649bd279fb6244ab3.png)'
- en: PyTorch computes derivatives of scalar functions only, but if we pass a vector,
    then essentially it computes derivatives element-wise and stores them in an array
    of the same dimension.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 仅计算标量函数的导数，但如果我们传递一个向量，它实际上会逐元素计算导数并将其存储在相同维度的数组中。
- en: '![](../Images/96c8ad30783fa05147414bf1b920dabb.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96c8ad30783fa05147414bf1b920dabb.png)'
- en: The following code will calculate the derivative with respect to the three constituent
    vectors.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将计算相对于三个组成向量的导数。
- en: '![](../Images/b4908eab9b5f412f72ff6e338d39de71.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4908eab9b5f412f72ff6e338d39de71.png)'
- en: We can show the plot of the derivative. Note, a derivative of a quadratic function
    is a straight line, tangent to the parabolic curve.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以展示导数的图像。注意，二次函数的导数是一条直线，与抛物线曲线相切。
- en: '![](../Images/9a3bdfd29fc4271ac66efab2f21cfb77.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a3bdfd29fc4271ac66efab2f21cfb77.png)'
- en: Building a Full-Fledged Neural Network
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建一个功能齐全的神经网络
- en: Apart from the tensors and automatic differentiation ability, there are few
    more core components/features of PyTorch that come together for a deep neural
    network definition.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 除了张量和自动微分功能外，PyTorch 还有一些核心组件/特性，这些组件/特性共同作用于深度神经网络的定义。
- en: The core components of PyTorch that will be used for building the neural classifier
    are,
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 构建神经分类器所需的 PyTorch 核心组件包括，
- en: The **Tensor** (the central data structure in PyTorch)
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**张量**（PyTorch 中的核心数据结构）'
- en: The **Autograd** feature of the Tensor (automatic differentiation formula baked
    into the
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量的**自动求导**特性（自动微分公式内置于
- en: The **nn.Module** class that is used to build any other neural classifier class
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**nn.Module** 类用于构建任何其他神经分类器类'
- en: The **Optimizer** (of course, there are many of them to choose from)
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器**（当然，你可以选择很多）'
- en: The **Loss** function (a big selection is available for your choice)
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失**函数（有很多选择供你挑选）'
- en: We have already described in detail the Tensor and the Autograd. Let us quickly
    discuss the other components,
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经详细描述了张量和自动求导。接下来，我们快速讨论其他组件，
- en: '**The nn.Module Class**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**nn.Module 类**'
- en: In PyTorch, we construct a neural network by defining it as a custom class.
    However, instead of deriving from the native Python object, this class inherits
    from the [nn.Module class](https://pytorch.org/docs/stable/nn.html). This imbues
    the neural net class with useful properties and powerful methods. This way, the
    full power of Object-Oriented-Programming (OOP) can be maintained while working
    with neural net models. We will see a full example of such a class definition
    in our article.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，我们通过将神经网络定义为自定义类来构建神经网络。然而，这个类不是从原生 Python 的 object 继承，而是继承自 [nn.Module
    类](https://pytorch.org/docs/stable/nn.html)。这使得神经网络类具有有用的属性和强大的方法。这样，在处理神经网络模型时可以保持面向对象编程（OOP）的全部力量。我们将在文章中看到这样的类定义的完整示例。
- en: '**The Loss Function**'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**损失函数**'
- en: In a neural network architecture and operation, the loss functions define how
    far the final prediction of the neural net is from the ground truth (given labels/classes
    or data for supervised training). The quantitative measure of loss helps drive
    the network to move closer to the configuration (the optimal settings of the weights
    of the neurons), which classifies the given dataset best or predicts the numerical
    output with the least total error.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络架构和操作中，损失函数定义了神经网络的最终预测与真实值（给定标签/类别或用于监督训练的数据）之间的距离。损失的定量度量有助于驱动网络更接近于最佳配置（神经元权重的最佳设置），以最佳方式分类给定数据集或以最小的总误差预测数值输出。
- en: PyTorch offers all the usual loss functions for classification and regression
    tasks —
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 提供了所有常见的分类和回归任务的损失函数 —
- en: binary and multi-class cross-entropy,
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二元和多类交叉熵，
- en: mean squared and mean absolute errors,
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均方误差和均绝对误差，
- en: smooth L1 loss,
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平滑 L1 损失，
- en: neg log-likelihood loss, and even
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负对数似然损失，甚至
- en: Kullback-Leibler divergence.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kullback-Leibler 散度。
- en: '[**A detailed discussion of these can be found in this article.**](https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[**这些的详细讨论可以在这篇文章中找到。**](https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7)'
- en: '**The Optimizer**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**优化器**'
- en: Optimization of the weights to achieve the lowest loss is at the heart of the
    backpropagation algorithm for training a neural network. PyTorch offers a plethora
    of optimizers to do the job, exposed through the torch.optim module —
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 优化权重以实现最低损失是训练神经网络的反向传播算法的核心。PyTorch 提供了大量的优化器来完成这个任务，通过 torch.optim 模块进行暴露
    —
- en: Stochastic gradient descent (SGD),
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机梯度下降（SGD），
- en: Adam, Adadelta, Adagrad, SpareAdam,
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adam、Adadelta、Adagrad、SparseAdam，
- en: L-BFGS,
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L-BFGS，
- en: RMSprop, etc.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMSprop 等。
- en: '[**Look at this article**](https://blog.exxactcorp.com/activation-functions-and-optimizers-for-deep-learning-models/) to
    know more about activation functions and optimizers, used in modern deep neural
    networks.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[**查看这篇文章**](https://blog.exxactcorp.com/activation-functions-and-optimizers-for-deep-learning-models/)
    以了解更多关于在现代深度神经网络中使用的激活函数和优化器的信息。'
- en: The Five-Step-Process
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 五步过程
- en: Using these components, we will build the classifier in five simple steps,
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这些组件，我们将通过五个简单步骤构建分类器，
- en: Construct our neural network as our custom class (inherited from the **nn.Module** class),
    complete with hidden layer tensors and forward method for propagating the input
    tensor through various layers and activation function
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将我们的神经网络构建为自定义类（继承自**nn.Module**类），包含隐藏层张量和前向方法，用于通过各种层和激活函数传播输入张量
- en: Propagate the feature (from a dataset) tensor through the network using this **forward()** method
    — say we get an output tensor as a result
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这个**forward()**方法将特征（来自数据集）张量传播通过网络 —— 假设我们得到一个输出张量作为结果
- en: Calculate the loss by comparing the output to the ground truth and using built-in
    loss functions
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将输出与真实值进行比较并使用内置损失函数来计算损失
- en: Propagate the gradient of the loss using the automatic differentiation ability
    (**Autograd**) with the backward method
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自动微分功能（**Autograd**）和反向传播方法传播损失的梯度
- en: Update the weights of the network using the gradient of the loss — this is accomplished
    by executing one step of the so-called optimizer — **optimizer.step()**.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用损失的梯度更新网络的权重 —— 这是通过执行所谓的优化器的一步 —— **optimizer.step()**来完成的。
- en: And that’s it. This five-step process constitutes **one complete epoch of training**.
    We just repeat it a bunch of times to drive down the loss and obtain high classification
    accuracy.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。这个五步过程构成了**一个完整的训练周期**。我们只需重复这个过程多次，以降低损失并获得高分类准确率。
- en: The idea looks like following,
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 思路如下，
- en: '![](../Images/ddaf1cc441b4dd42f218c0b85151a2f7.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddaf1cc441b4dd42f218c0b85151a2f7.png)'
- en: Hands-on example
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实践示例
- en: Let’s suppose we want to build and train the following 2-layer neural network.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想构建并训练以下的 2 层神经网络。
- en: '![](../Images/e1e16c7faacd7079046dc18db4961113.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1e16c7faacd7079046dc18db4961113.png)'
- en: We start with the class definition,
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从类定义开始，
- en: '![](../Images/b41c6384b2a1c9ab8c9cc90980137ce3.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b41c6384b2a1c9ab8c9cc90980137ce3.png)'
- en: We can define a variable as an object belonging to this class and print the
    summary.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义一个属于这个类的变量并打印其摘要。
- en: '![](../Images/bc4381932a3ae01a5f0d67e5f74501f8.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc4381932a3ae01a5f0d67e5f74501f8.png)'
- en: We choose the Binary cross-entropy loss,
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择二元交叉熵损失，
- en: '![](../Images/b82c0484793c825febc6aea49f12c68c.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b82c0484793c825febc6aea49f12c68c.png)'
- en: Let us run the input dataset through the neural net model we have defined, i.e., **forward
    pass once and compute the output probabilities**. As the weights have been initialized
    as random, we will see random output probabilities (mostly close to 0.5). **This
    network has not been trained yet**.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将输入数据集通过我们定义的神经网络模型，即**进行一次前向传播并计算输出概率**。由于权重已被初始化为随机值，我们会看到随机的输出概率（大多数接近
    0.5）。**这个网络尚未经过训练**。
- en: '![](../Images/c93b460e863ae9dc8d82fb7f41a3fe2e.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c93b460e863ae9dc8d82fb7f41a3fe2e.png)'
- en: '![](../Images/d1ad6dbac80d3fe8267d06985d9a7a50.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1ad6dbac80d3fe8267d06985d9a7a50.png)'
- en: We define the optimizer,
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义优化器，
- en: '![](../Images/55175acd3a200fc0dbb21da83e19cf9e.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55175acd3a200fc0dbb21da83e19cf9e.png)'
- en: Next, we show how to do forward and backward passes with one step of optimizer. **This
    set of code can be found at the heart of any PyTorch neural net model**. We follow
    another five-step process,
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展示如何使用一个优化器的步骤进行前向和反向传播。**这段代码可以在任何 PyTorch 神经网络模型的核心中找到**。我们按照另外一个五步过程进行，
- en: reset the gradients to zero (to prevent the accumulation of grads)
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将梯度重置为零（以防止梯度的累积）
- en: forward pass the tensors through the layers
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将张量前向传播通过各层
- en: calculate the loss tensor
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算损失张量
- en: calculate the gradients of the loss
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算损失的梯度
- en: update the weights by incrementing the optimizer by one step (in the direction
    of the negative gradient)
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将优化器更新一步（沿负梯度方向）来更新权重
- en: The five steps above** are exactly what you can observe and read about in all
    the theoretical discussion (and in the textbooks) on neural nets and deep learning**.
    And, with PyTorch, you are able to implement this process with deceptively simple
    code, step-by-step.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 上述五个步骤**正是你在所有关于神经网络和深度学习的理论讨论（以及教科书）中观察到和阅读到的内容**。而且，使用 PyTorch，你可以通过看似简单的代码一步步实现这个过程。
- en: The code is shown below,
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 代码如下所示，
- en: '![](../Images/c331585b9ea3bbc0bd384a69e1a8bfb6.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c331585b9ea3bbc0bd384a69e1a8bfb6.png)'
- en: When **we run the same type of code over a loop (for multiple epochs)**, we
    can observe the familiar loss-curve going down, i.e., the neural network getting
    trained gradually.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当**我们在一个循环中运行相同类型的代码（多次周期）**时，我们可以观察到熟悉的损失曲线下降，即神经网络逐渐被训练。
- en: '![](../Images/ab330f2f41dab4503f83d3df06a60b91.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab330f2f41dab4503f83d3df06a60b91.png)'
- en: After training for 200 epochs, we can look at the probability distributions
    again directly to see how the neural network output probabilities are now different
    (trying to match with the true data distributions).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 经过 200 轮训练后，我们可以再次直接查看概率分布，以查看神经网络输出的概率现在如何不同（试图与真实数据分布匹配）。
- en: '![](../Images/268637fd77fe58320394ddec8b27666c.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/268637fd77fe58320394ddec8b27666c.png)'
- en: Summary of PyTorch Fundamentals
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch 基础总结
- en: PyTorch is a great package for reaching out to the heart of a neural net and
    customizing it for your application or trying out bold new ideas with the architecture,
    optimization, and mechanics of the network.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 是一个很棒的工具包，可以深入了解神经网络的核心，并根据你的应用进行定制，或尝试对网络的架构、优化和机制进行大胆的新想法。
- en: You can easily build complex interconnected networks, try out novel activation
    functions, mix and match custom loss functions, etc. The core ideas of computation
    graphs, easy auto-differentiation, and forward and backward flow of tensors will
    come in handy for any of your neural network definitions and optimization.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以轻松构建复杂的互联网络，尝试新颖的激活函数，混合和匹配自定义损失函数等。计算图的核心思想、简便的自动微分以及张量的前向和后向流动将对你的任何神经网络定义和优化大有帮助。
- en: In this article, we summarized a few key steps which can be followed to quickly
    build a neural network for classification or regression tasks. We also showed
    how neat ideas could be easily tried out with this framework.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们总结了一些关键步骤，这些步骤可以快速构建用于分类或回归任务的神经网络。我们还展示了如何使用这个框架轻松尝试新颖的想法。
- en: All the code for this article [**can be found here in this Github repo**](https://github.com/tirthajyoti/PyTorch_Machine_Learning).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的所有代码 [**可以在这个 GitHub 仓库找到**](https://github.com/tirthajyoti/PyTorch_Machine_Learning)。
- en: '[Original](https://blog.exxactcorp.com/the-most-important-fundamentals-of-pytorch-you-should-know/).
    Reposted with permission.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://blog.exxactcorp.com/the-most-important-fundamentals-of-pytorch-you-should-know/)。已获得许可转载。'
- en: '**Related:**'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Build PyTorch Models Easily Using torchlayers](https://www.kdnuggets.com/2020/04/pytorch-models-torchlayers.html)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 torchlayers 轻松构建 PyTorch 模型](https://www.kdnuggets.com/2020/04/pytorch-models-torchlayers.html)'
- en: '[OpenAI is Adopting PyTorch… They Aren’t Alone](https://www.kdnuggets.com/2020/01/openai-pytorch-adoption.html)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAI 正在采用 PyTorch……他们并不孤单](https://www.kdnuggets.com/2020/01/openai-pytorch-adoption.html)'
- en: '[A Gentle Introduction to PyTorch 1.2](https://www.kdnuggets.com/2019/09/gentle-introduction-pytorch-12.html)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch 1.2 简明介绍](https://www.kdnuggets.com/2019/09/gentle-introduction-pytorch-12.html)'
- en: '* * *'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT 工作'
- en: '* * *'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个数据科学家都应该知道的三个 R 库（即使你使用 Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学来寻找目标，并找到目标来……](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一个 90 亿美元的 AI 失败，深度剖析](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学的统计学顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功数据科学家的 5 个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么 Python 是初创公司理想的编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
