# Tensorflow中的多任务学习：第1部分

> 原文：[https://www.kdnuggets.com/2016/07/multi-task-learning-tensorflow-part-1.html](https://www.kdnuggets.com/2016/07/multi-task-learning-tensorflow-part-1.html)

**由乔纳森·戈德温，伦敦大学学院**。

> 本博客文章附带一个Jupyter笔记本。请下载[这里](https://github.com/jg8610/multi-task-part-1-notebook/tree/master)。

### 介绍

**为什么选择多任务学习**

当你思考人们如何学习新事物时，他们通常会利用自己的经验和世界知识来加速学习过程。当我学习一门新语言，尤其是相关的语言时，我会利用我已经掌握的语言知识来快速学习。这个过程也反过来有效——学习一门新语言可以帮助你更好地理解和使用自己已知的语言。

我们的大脑能够同时学习多种不同的任务——无论是将英语翻译成德语还是法语，我们的大脑结构都是一样的。如果我们使用机器学习算法同时处理这两个任务，我们可能称之为“多任务”学习。

这是未来几年机器学习中最有趣和令人兴奋的研究领域之一，它可以大幅减少学习新概念所需的数据量。深度学习的一个伟大承诺是，利用模型的力量和在任务之间共享参数的简单方法，我们应该能够在多任务学习方面取得显著进展。

当我开始在这个领域进行实验时，我遇到了一些困难——虽然理解实现多任务学习所需的架构变化很容易，但在Tensorflow中实现它则更为困难。在Tensorflow中进行非标准网络的操作需要对其工作原理有很好的理解，但大多数现成的示例并没有提供有用的指导。我希望下面的教程能够简单地解释一些关键概念，并帮助那些遇到困难的人。

### 我们将要做的事情

**第1部分**

1.  **通过示例理解Tensorflow计算图**。进行多任务学习需要了解计算图的工作原理——如果你已经了解这些，可以跳过。

1.  **了解如何使用图进行多任务学习**。我们将通过一个示例展示如何调整一个简单的图来进行多任务学习。

**第2部分**

1.  **为词性标注和浅层解析构建图**。我们将填写一个模板，以训练一个用于两个相关语言任务的网络。别担心，你不需要知道它们是什么！

1.  **联合和单独训练网络**。我们将实际以两种不同的方式训练模型。你应该能够在你的笔记本电脑上完成这项任务。

### 使用玩具示例理解计算图

计算图是使Tensorflow（以及其他类似软件包）快速的关键。它是深度学习机制的核心部分，但可能会令人困惑。

图具有一些巧妙的特性，使得进行多任务学习非常简单，但首先我们将保持简单，解释关键概念。

**定义：计算图**

**计算图**是你将要运行的计算（即算法）的**模板**。它**不执行任何计算**，但这意味着你的计算机可以更快地进行反向传播。

如果你询问 Tensorflow **计算的结果**，它会**仅进行完成工作的计算**，**而不是整个图**。

### 玩具示例 - 线性变换：设置图

我们将查看一个简单计算的图——对输入进行线性变换，并计算平方损失：

![玩具示例](../Images/52c75dcdf2f4ae57c1cc42de0b09640c.png)

```py
# Import Tensorflow and numpy
import Tensorflow as tf
import numpy as np

# ======================
# Define the Graph
# ======================

# Create Placeholders For X And Y (for feeding in data)
X = tf.placeholder("float",[10, 10],name="X") # Our input is 10x10
Y = tf.placeholder("float", [10, 1],name="Y") # Our output is 10x1

# Create a Trainable Variable, "W", our weights for the linear transformation
initial_W = np.zeros((10,1))
W = tf.Variable(initial_W, name="W", dtype="float32")

# Define Your Loss Function
Loss = tf.pow(tf.add(Y,-tf.matmul(X,W)),2,name="Loss")

```

有几点需要强调关于这个图：

+   **如果我们现在运行这段代码，我们不会得到任何输出**。记住，计算图只是一个模板——它不会做任何事情。如果我们想要一个答案，我们必须告诉 Tensorflow 使用**会话**来运行计算。

+   **我们还没有明确创建图对象**。你可能会期望我们需要在某个地方创建一个图对象，以便 Tensorflow 知道我们想要创建一个图。事实上，通过使用 Tensorflow 操作，我们在告诉 Tensorflow 我们代码中的哪些部分在图中。

**提示：保持图的独立**。你通常会在图之外进行大量的数据操作和计算，这意味着在 python 内部跟踪什么是可用的可能会有些混乱。我喜欢把图放在一个单独的文件中，通常在一个单独的类中，以保持关注点的分离，但这不是必需的。

### 玩具示例 - 线性变换：获取结果

图上的计算是在 Tensorflow **会话**内进行的。要从会话中获取结果，你需要提供两样东西：目标结果和输入。

1.  **目标结果或操作**。你告诉 Tensorflow 你希望图的哪些部分返回值，它会**自动找出需要运行的计算**。你还可以调用操作，例如初始化你的变量。

1.  **按需输入（‘Feed Dict’）**。在大多数计算中，你会临时提供输入数据。在这种情况下，你会为这些数据构建一个带有**占位符**的图，并在计算时输入这些数据。并不是所有计算或操作都需要输入——对于许多计算，所有信息已包含在图中。

```py
# Import Tensorflow and Numpy
import Tensorflow as tf
import numpy as np

# ======================
# Define the Graph
# ======================

# Create Placeholders For X And Y (for feeding in data)
X = tf.placeholder("float",[10, 10],name="X") # Our input is 10x10
Y = tf.placeholder("float", [10, 1],name="Y") # Our output is 10x1

# Create a Trainable Variable, "W", our weights for the linear transformation
initial_W = np.zeros((10,1))
W = tf.Variable(initial_W, name="W", dtype="float32")

# Define Your Loss Function
Loss = tf.pow(tf.add(Y,-tf.matmul(X,W)),2,name="Loss")

with tf.Session() as sess: # set up the session
    sess.run(tf.initialize_all_variables())
    Model_Loss = sess.run(
                Loss, # the first argument is the name of the Tensorflow variabl you want to return
                { # the second argument is the data for the placeholders
                  X: np.random.rand(10,10),
                  Y: np.random.rand(10).reshape(-1,1)
                })
    print(Model_Loss)

```

### 如何使用图进行多任务学习

当我们创建一个执行多个任务的神经网络时，我们希望有些网络部分是共享的，而其他部分则特定于每个任务。在训练时，我们希望将每个任务的信息转移到网络的共享部分。

所以，首先，让我们绘制一个简单的两任务网络图，该网络具有一个共享层和每个任务的特定层。我们将把这些输出输入到我们的损失函数中与目标一起计算。我已经标记出我们需要在图中创建占位符的位置。

![基本共享图](../Images/8ae5537858dcd61fd6448f72993af879.png)

```py
#  GRAPH CODE
# ============

# Import Tensorflow
import Tensorflow as tf

# ======================
# Define the Graph
# ======================

# Define the Placeholders
X = tf.placeholder("float", [10, 10], name="X")
Y1 = tf.placeholder("float", [10, 1], name="Y1")
Y2 = tf.placeholder("float", [10, 1], name="Y2")

# Define the weights for the layers
shared_layer_weights = tf.Variable([10,20], name="share_W")
Y1_layer_weights = tf.Variable([20,1], name="share_Y1")
Y2_layer_weights = tf.Variable([20,1], name="share_Y2")

# Construct the Layers with RELU Activations
shared_layer = tf.nn.relu(tf.matmul(X,shared_layer_weights))
Y1_layer = tf.nn.relu(tf.matmul(shared_layer,Y1_layer_weights))
Y2_layer_weights = tf.nn.relu(tf.matmul(shared_layer,Y2_layer_weights))

# Calculate Loss
Y1_Loss = tf.nn.l2_loss(Y1,Y1_layer)
Y2_Loss = tf.nn.l2_loss(Y2,Y2_layer)

```

当我们训练这个网络时，**我们希望任务 1 的层参数不变，无论任务 2 的结果有多么错误**，但**共享层的参数要随着两个任务而变化**。这可能看起来有点困难——通常你在一个图中只有一个优化器，因为你只优化一个损失函数。幸运的是，利用图的属性，以两种方式训练这种模型是非常简单的。

* * *

## 我们的三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的 IT

* * *

### 更多相关话题

+   [TensorFlow 在计算机视觉中的应用 - 转移学习变得简单](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)

+   [PyTorch 还是 TensorFlow？比较流行的机器学习框架](https://www.kdnuggets.com/2022/02/packt-pytorch-tensorflow-comparing-popular-machine-learning-frameworks.html)

+   [Tensorflow 的“Hello World”](https://www.kdnuggets.com/2022/05/hello-world-tensorflow.html)

+   [使用 Tensorflow 训练图像分类模型指南](https://www.kdnuggets.com/2022/12/guide-train-image-classification-model-tensorflow.html)

+   [使用 TensorFlow 和 Keras 构建和训练你的第一个神经网络](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)

+   [免费 TensorFlow 2.0 完整课程](https://www.kdnuggets.com/2023/02/free-tensorflow-20-complete-course.html)
