- en: 'Building Predictive Models: Logistic Regression in Python'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/building-predictive-models-logistic-regression-in-python](https://www.kdnuggets.com/building-predictive-models-logistic-regression-in-python)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Building Predictive Models: Logistic Regression in Python](../Images/9b57e1ab133578eb46103b199d894906.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: When you are getting started with machine learning, logistic regression is one
    of the first algorithms you’ll add to your toolbox. It's a simple and robust algorithm,
    commonly used for binary classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a binary classification problem with classes 0 and 1\. Logistic regression
    fits a logistic or sigmoid function to the input data and predicts the probability
    of a query data point belonging to class 1\. Interesting, yes?
  prefs: []
  type: TYPE_NORMAL
- en: 'In this tutorial, we’ll learn about logistic regression from the ground up
    covering:'
  prefs: []
  type: TYPE_NORMAL
- en: The logistic (or sigmoid) function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How we move from linear to logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How logistic regression works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we’ll build a simple logistic regression model to [classify RADAR returns
    from the ionosphere](https://www.google.com/url?q=https://archive.ics.uci.edu/dataset/52/ionosphere&sa=D&source=editors&ust=1700937825039807&usg=AOvVaw3pi2xfFVRgGw7d3kaG9jKd).
  prefs: []
  type: TYPE_NORMAL
- en: Logistic Function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we learn more about logistic regression, let''s review how the logistic
    function works. The logistic (or sigmoid function) is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Predictive Models: Logistic Regression in Python](../Images/5470699828e45c6e4c6bbbd0ab92f232.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When you plot the sigmoid function, it’ll look like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Predictive Models: Logistic Regression in Python](../Images/015467f002599f01250517c7d221363c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the plot, we see that:'
  prefs: []
  type: TYPE_NORMAL
- en: When x = 0, σ(x) takes a value of 0.5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When x approaches +∞, σ(x) approaches 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When x approaches -∞, σ(x) approaches 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So for all real inputs, the sigmoid function squishes them to take on values
    in the range [0, 1].
  prefs: []
  type: TYPE_NORMAL
- en: From Linear Regression to Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's first discuss why we cannot use linear regression for a binary classification
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: In a binary classification problem, the output is categorical label (0 or 1).
    Because linear regression predicts continuous-valued outputs which can be less
    than 0 or greater than 1, it does not make sense for the problem at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Also, a straight line may not be the best fit when the output labels belong
    to one of the two categories.
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Predictive Models: Logistic Regression in Python](../Images/08e7b90149d1ea9b98cd273de503be32.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'So how do we go from linear to logistic regression? In linear regression the
    predicted output is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Predictive Models: Logistic Regression in Python](../Images/5e8d3aac667acaf223b559c5ab0d78b1.png)'
  prefs: []
  type: TYPE_IMG
- en: Where the βs are the coefficients and X_is are the predictors (or features).
  prefs: []
  type: TYPE_NORMAL
- en: 'Without loss of generality, let’s assume X_0 = 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Predictive Models: Logistic Regression in Python](../Images/146c38b7a9bf443e3bc8084f0ff8f0fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So we can have a more concise expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Predictive Models: Logistic Regression in Python](../Images/b13fb0c2ca244c0a3b0abe86c0fe657d.png)'
  prefs: []
  type: TYPE_IMG
- en: In logistic regression, we need the predicted probability p_i in the [0,1] interval.
    We know that the logistic function squishes inputs so that they take on values
    in the [0,1] interval.
  prefs: []
  type: TYPE_NORMAL
- en: 'So plugging in this expression into the logistic function, we have the predicted
    probability as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Predictive Models: Logistic Regression in Python](../Images/61a5d98c740dfeffd22729546d40cb18.png)'
  prefs: []
  type: TYPE_IMG
- en: Under the Hood of Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So how do we find the best fit logistic curve for the given data set? To answer
    this, let’s understand maximum likelihood estimation.
  prefs: []
  type: TYPE_NORMAL
- en: '[Maximum Likelihood Estimation (MLE)](https://www.google.com/url?q=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&sa=D&source=editors&ust=1700937825042844&usg=AOvVaw21aHhVf72aBaA4ufZShBzV) is
    used to estimate the parameters of the logistic regression model by maximizing
    the likelihood function. Let''s break down the process of MLE in logistic regression
    and how the cost function is formulated for optimization using gradient descent.'
  prefs: []
  type: TYPE_NORMAL
- en: Breaking Down Maximum Likelihood Estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As discussed, we model the probability that a binary outcome occurs as a function
    of one or more predictor variables (or features):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Predictive Models: Logistic Regression in Python](../Images/6d61301bfda248966710f8d9c80f2b0b.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the βs  are the model parameters or coefficients. X_1, X_2,..., X_n are
    the predictor variables.
  prefs: []
  type: TYPE_NORMAL
- en: MLE aims to find the values of β that maximize the likelihood of the observed
    data. The likelihood function, denoted as L(β), represents the probability of
    observing the given outcomes for the given predictor values under the logistic
    regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Formulating the Log-Likelihood Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To simplify the optimization process, it's common to work with the log-likelihood
    function. Because it transforms products of probabilities into sums of log probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The log-likelihood function for logistic regression is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Predictive Models: Logistic Regression in Python](../Images/e1a36859b3caf1255cfd6dd6ff2fe87e.png)'
  prefs: []
  type: TYPE_IMG
- en: Cost Function and Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know the essence of log-likelihood, let's proceed to formulate the
    cost function for logistic regression and subsequently gradient descent for finding
    the best model parameters
  prefs: []
  type: TYPE_NORMAL
- en: Cost Function for Logistic Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To optimize the logistic regression model, we need to maximize the log-likelihood.
    So we can use the negative log-likelihood as the cost function to minimize during
    training. The negative log-likelihood, often referred to as the logistic loss,
    is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Predictive Models: Logistic Regression in Python](../Images/f5958e87ce2efc03e51e0fc69676c3ec.png)'
  prefs: []
  type: TYPE_IMG
- en: The goal of the learning algorithm, therefore, is to find the values of ? that
    minimize this cost function. Gradient descent is a commonly used optimization
    algorithm for finding the minimum of this cost function.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent in Logistic Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Gradient descent](https://www.google.com/url?q=https://en.wikipedia.org/wiki/Gradient_descent&sa=D&source=editors&ust=1700937825044751&usg=AOvVaw1huVLDtM5ldfX2Ae4YfvwJ) is
    an iterative optimization algorithm that updates the model parameters β in the
    opposite direction of the gradient of the cost function with respect to β. The
    update rule at step t+1 for logistic regression using gradient descent is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Predictive Models: Logistic Regression in Python](../Images/6d1026850a60eb36a19268dabe15beec.png)'
  prefs: []
  type: TYPE_IMG
- en: Where α is the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: The partial derivatives can be computed using the chain rule. Gradient descent
    iteratively updates the parameters—until convergence—aiming to minimize the logistic
    loss. As it converges, it finds the optimal values of β that maximize the likelihood
    of the observed data.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic Regression in Python with Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you know how logistic regression works, let’s build a predictive model
    using the scikit-learn library.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use the [ionosphere dataset from the UCI machine learning repository](https://www.google.com/url?q=https://archive.ics.uci.edu/dataset/52/ionosphere&sa=D&source=editors&ust=1700937825045668&usg=AOvVaw2GHhc6h9v8DkLk9xMlILvJ) for
    this tutorial. The dataset comprises 34 numerical features. The output is binary,
    one of ‘good’ or ‘bad’ (denoted by ‘g’ or ‘b’). The output label ‘good’ refers
    to RADAR returns that have detected some structure in the ionosphere.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – Loading the Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, download the dataset and read it into a pandas dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Step 2 – Exploring the Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s take a look at the first few rows of the dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![Building Predictive Models: Logistic Regression in Python](../Images/0abc9ceccca83b2defec4acdc1d9f703.png)'
  prefs: []
  type: TYPE_IMG
- en: Truncated Output of df.head()
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get some information about the dataset: the number of non-null values
    and the data types of each of the columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Building Predictive Models: Logistic Regression in Python](../Images/a5aaae99d5278e347e5418a880cdd2b6.png)
    Truncated Output of df.info()'
  prefs: []
  type: TYPE_IMG
- en: 'Because we have all numeric features, we can also get some descriptive statistics
    using the `describe()` method on the dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![Building Predictive Models: Logistic Regression in Python](../Images/85955e4396dd100955cfde03dbbf3410.png)'
  prefs: []
  type: TYPE_IMG
- en: Truncated Output of df.describe()
  prefs: []
  type: TYPE_NORMAL
- en: 'The column names are currently 0 through 34—including the label. Because the
    dataset does not provide descriptive names for the columns, it just refers to
    them as attribute_1 to attribute_34 if you would like you can rename the columns
    of the data frame as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Note: This step is purely optional. You can proceed with the default column
    names if you prefer.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Building Predictive Models: Logistic Regression in Python](../Images/df9f8a99c2b3225b7a0e5061a3ff9e43.png)'
  prefs: []
  type: TYPE_IMG
- en: Truncated Output of df.head() [After Renaming Columns]
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – Renaming Class Labels and Visualizing Class Distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because the output class labels are ‘g’ and ‘b’, we need to map them to 1 and
    0 , respectively. You can do it using `map()` or `replace()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also visualize the distribution of the class labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![Building Predictive Models: Logistic Regression in Python](../Images/279fd1c0009397d6c1defb62946ad994.png)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of Class Labels
  prefs: []
  type: TYPE_NORMAL
- en: We see that there is an imbalance in the distribution. There are more records
    belonging to class 1 than to class 0\. We’ll handle this class imbalance when
    building the logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – Preprocessing the Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s collect the features and output labels like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: After splitting the dataset into the train and test sets, we need to preprocess
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: When there are many numeric features—each on a potentially different scale—we
    need to preprocess the numeric features. A common method is to transform them
    such that they follow a distribution with zero mean and unit variance.
  prefs: []
  type: TYPE_NORMAL
- en: The `StandardScaler` from scikit-learn’s preprocessing module helps us achieve
    this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Step 6 – Building a Logistic Regression Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we can instantiate a logistic regression classifier. The `LogisticRegression`
    class is part of scikit-learn’s linear_model module.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we have set the `class_weight` parameter to ‘balanced’. This will
    help us account for the class imbalance. By assigning weights to each class—inversely
    proportional to the number of records in the classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'After instantiating the class, we can fit the model to the training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Step 7 – Evaluating the Logistic Regression Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can call the `predict()` method to get the model’s predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the accuracy score, we can also get a classification report with
    metrics like precision, recall, and F1-score.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Building Predictive Models: Logistic Regression in Python](../Images/1a6416799f97aad89cb5c8f9a9468cf5.png)'
  prefs: []
  type: TYPE_IMG
- en: Congratulations, you have coded your first logistic regression model!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this tutorial, we learned about logistic regression in detail: from theory
    and math to coding a logistic regression classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: As a next step, try building a logistic regression model for a suitable dataset
    of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Credits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Ionosphere dataset is licensed under a [Creative Commons Attribution 4.0
    International](https://www.google.com/url?q=https://creativecommons.org/licenses/by/4.0/legalcode&sa=D&source=editors&ust=1700937825057781&usg=AOvVaw2hz8NeH5w9ltvexC6Yg-wA) (CC
    BY 4.0) license:'
  prefs: []
  type: TYPE_NORMAL
- en: Sigillito,V., Wing,S., Hutton,L., and Baker,K.. (1989). Ionosphere. UCI Machine
    Learning Repository. https://doi.org/10.24432/C5W01B.
  prefs: []
  type: TYPE_NORMAL
- en: '**[](https://twitter.com/balawc27)**[Bala Priya C](https://www.kdnuggets.com/wp-content/uploads/bala-priya-author-image-update-230821.jpg)****
    is a developer and technical writer from India. She likes working at the intersection
    of math, programming, data science, and content creation. Her areas of interest
    and expertise include DevOps, data science, and natural language processing. She
    enjoys reading, writing, coding, and coffee! Currently, she''s working on learning
    and sharing her knowledge with the developer community by authoring tutorials,
    how-to guides, opinion pieces, and more. Bala also creates engaging resource overviews
    and coding tutorials.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Comparing Linear and Logistic Regression](https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[An Overview of Logistic Regression](https://www.kdnuggets.com/2022/02/overview-logistic-regression.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear vs Logistic Regression: A Succinct Explanation](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News 22:n12, March 23: Best Data Science Books for…](https://www.kdnuggets.com/2022/n12.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Logistic Regression for Classification](https://www.kdnuggets.com/2022/04/logistic-regression-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How Does Logistic Regression Work?](https://www.kdnuggets.com/2022/07/logistic-regression-work.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
