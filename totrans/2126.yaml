- en: 'Exploring the Zephyr 7B: A Comprehensive Guide to the Latest Large Language
    Model'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 Zephyr 7B：最新大型语言模型的全面指南
- en: 原文：[https://www.kdnuggets.com/exploring-the-zephyr-7b-a-comprehensive-guide-to-the-latest-large-language-model](https://www.kdnuggets.com/exploring-the-zephyr-7b-a-comprehensive-guide-to-the-latest-large-language-model)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/exploring-the-zephyr-7b-a-comprehensive-guide-to-the-latest-large-language-model](https://www.kdnuggets.com/exploring-the-zephyr-7b-a-comprehensive-guide-to-the-latest-large-language-model)
- en: '![Exploring the Zephyr 7B: A Comprehensive Guide to the Latest Large Language
    Model](../Images/10b39d79a6e686d9cc4e3145e18c937d.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![探索 Zephyr 7B：最新大型语言模型的全面指南](../Images/10b39d79a6e686d9cc4e3145e18c937d.png)'
- en: Photo by [Google DeepMind](https://www.pexels.com/photo/an-artist-s-illustration-of-artificial-intelligence-ai-this-image-represents-how-ai-powered-tools-can-support-us-and-save-time-it-was-created-by-martina-stiftinger-as-part-of-the-visua-18069239/)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Google DeepMind](https://www.pexels.com/photo/an-artist-s-illustration-of-artificial-intelligence-ai-this-image-represents-how-ai-powered-tools-can-support-us-and-save-time-it-was-created-by-martina-stiftinger-as-part-of-the-visua-18069239/)
    提供
- en: '2023 was the year of Large Language Models and Open Source. Many startups and
    companies open-sourced their models and weights to combat proprietary LLMs such
    as ChatGPT and Claude. Some of the important companies and models (open source)
    for 2023 were:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 2023 年是大型语言模型和开源的年份。许多初创公司和公司开源了他们的模型和权重，以对抗像 ChatGPT 和 Claude 这样的专有 LLM。2023
    年的一些重要公司和模型（开源）包括：
- en: Meta (LLama, LLamav2)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta (LLama, LLamav2)
- en: TII (Falcon 7B, 40B, 180B)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TII（Falcon 7B, 40B, 180B）
- en: Mistral (Mistral 7B, Mixtral8x7B)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mistral（Mistral 7B, Mixtral8x7B）
- en: '* * *'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - 支持您的组织的
    IT'
- en: '* * *'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: However, a 7B model which is relatively easy and cheaper to deploy is not up
    to par with bigger models such as 70B. The strongest open-source contender was
    Mistral 7B which would outperform many bigger models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，7B 模型相对容易且便宜部署，但无法与 70B 等更大模型媲美。最强的开源竞争者是 Mistral 7B，它会超越许多更大的模型。
- en: '![Exploring the Zephyr 7B: A Comprehensive Guide to the Latest Large Language
    Model](../Images/885bea73025d52991e66d7eda74d2446.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![探索 Zephyr 7B：最新大型语言模型的全面指南](../Images/885bea73025d52991e66d7eda74d2446.png)'
- en: Comparison of Mistral-7B from [Mistral.ai](https://mistral.ai/news/announcing-mistral-7b/)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [Mistral.ai](https://mistral.ai/news/announcing-mistral-7b/) 的 Mistral-7B
    比较
- en: These small models however still do not respond well to natural prompts and
    require good prompt engineering.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些小模型对自然提示的响应仍然不佳，需要良好的提示工程。
- en: Introduction
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Zephyr 7B is a model created by the HuggingFace H4 (Helpful, Honest, Harmless,
    Huggy) team whose main goal was to create a smaller language model that is aligned
    with user intent and outperforms even bigger models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Zephyr 7B 是由 HuggingFace H4（Helpful, Honest, Harmless, Huggy）团队创建的模型，其主要目标是创建一个对齐用户意图的小型语言模型，并超越甚至更大规模的模型。
- en: Zephyr is an aligned version of Mistral-7B mainly created with the power of
    Distillation, and is comparable to 70B models in academic and conversational benchmarks.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Zephyr 是 Mistral-7B 的对齐版本，主要通过蒸馏技术创建，并在学术和对话基准测试中与 70B 模型相当。
- en: '![Exploring the Zephyr 7B: A Comprehensive Guide to the Latest Large Language
    Model](../Images/e16f6ba0bbd29b5cb68165720bd5cb79.png)Performance comparison of
    Zephyr-7B | Source: [Zephyr paper](https://arxiv.org/abs/2310.16944)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![探索 Zephyr 7B：最新大型语言模型的全面指南](../Images/e16f6ba0bbd29b5cb68165720bd5cb79.png)Zephyr-7B
    性能比较 | 来源：[Zephyr 论文](https://arxiv.org/abs/2310.16944)'
- en: Key Features
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主要特性
- en: The reason behind the outstanding performance of Zephyr is these 4 key techniques
    that the H4 Team has used.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Zephyr 卓越表现的原因在于 H4 团队使用的这四项关键技术。
- en: Self-Instruct data creation & DSFT (Distilled Supervised Fine-Tuning)
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Self-Instruct 数据创建与 DSFT（蒸馏监督微调）
- en: Feedback collection
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反馈收集
- en: DDPO (Distilled Direct Preference Optimization) of the DSFT model
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DSFT 模型的 DDPO（精炼直接偏好优化）
- en: '****Self-Instruct Data Creation & DSFT****'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '****自我指导数据创建与 DSFT****'
- en: Traditionally **Supervised Fine-Tuning (SFT)** is performed on a Large Language
    Model via a high-quality instruction completion pair. Construction of this data
    is costly and requires human supervision (Chung et al., 2022; Sanh et al., 2021).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，**监督微调（SFT）**是在大型语言模型上通过高质量的指令完成对进行的。这些数据的构建成本高昂，并且需要人工监督（Chung et al.,
    2022；Sanh et al., 2021）。
- en: One of the interesting approaches here is to use a Teacher model (already trained
    LLM) to generate the instructions and responses. This distillation technique was
    first used on Alpaca (Taori et al., 2023) which proved that a small model can
    outperform larger models with **Distilled Supervised Fine-Tuning**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这里一个有趣的方法是使用教师模型（已训练的大型语言模型）来生成指令和响应。这种精炼技术首次用于 Alpaca（Taori et al., 2023），证明了小模型可以通过**精炼监督微调**超越更大的模型。
- en: '![Exploring the Zephyr 7B: A Comprehensive Guide to the Latest Large Language
    Model](../Images/e71c20282740d59d9e9fa0a01422208d.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![探索 Zephyr 7B：最新大型语言模型的全面指南](../Images/e71c20282740d59d9e9fa0a01422208d.png)'
- en: 'Self-Instruct pipeline | Source: [Self-Instruct paper](https://arxiv.org/abs/2212.10560)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 自我指导管道 | 来源：[自我指导论文](https://arxiv.org/abs/2212.10560)
- en: 'The H4 Team used Zephyr for constructing high-quality supervised (instruction,
    completion) datasets that were used for doing DSFT. (Training a model on instructions/completions
    generated is a form of distillation known as DSFT: Distilled Supervised Fine-Tuning).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: H4 团队使用 Zephyr 构建了高质量的监督（指令，完成）数据集，这些数据集用于进行 DSFT。（在生成的指令/完成上训练模型是一种精炼形式，称为
    DSFT：精炼监督微调。）
- en: '****Feedback Collection****'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '****反馈收集****'
- en: Large Language Models are aligned typically with the help of **Reinforcement
    learning from human feedback (RLHF)**. Zephyr instead uses Feedback from a better
    teacher model (such as GPT-4) to align the interests of the model, following the
    approach of Ultra Feedback.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型通常通过**来自人类反馈的强化学习（RLHF）**进行对齐。Zephyr 则使用来自更优教师模型（如 GPT-4）的反馈来对齐模型的兴趣，采用了
    Ultra Feedback 的方法。
- en: '![Exploring the Zephyr 7B: A Comprehensive Guide to the Latest Large Language
    Model](../Images/6fd933bf69fbba697ba316aaa0ec5579.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![探索 Zephyr 7B：最新大型语言模型的全面指南](../Images/6fd933bf69fbba697ba316aaa0ec5579.png)'
- en: 'UltraFeedback construction process | Source: [UltraFeedback paper](https://arxiv.org/abs/2310.01377)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: UltraFeedback 构建过程 | 来源：[UltraFeedback 论文](https://arxiv.org/abs/2310.01377)
- en: The way it works is that each prompt supervised prompt from SFT is passed to
    4 models (Claude, LLama, Falcon, etc.) and each of the 4 responses against the
    single prompt is scored with the help of GPT-4\. Now we have a dataset of an Input
    (x), highest scoring completion (yw), and a random prompt denoted as low scoring
    completion (yl), i.e we have a triplet of (x, yw, yl).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 它的工作原理是，每个来自 SFT 的监督提示会传递给 4 个模型（Claude、LLama、Falcon 等），每个模型对单一提示的响应都通过 GPT-4
    进行评分。现在我们有一个数据集，其中包括输入（x）、最高评分的完成（yw）以及一个随机提示，表示为低评分的完成（yl），即我们有一个三元组（x, yw, yl）。
- en: '****Preference Optimization****'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '****偏好优化****'
- en: The goal of this last step is to maximize the preference of the model from yw(highest-scoring
    completion) over yl (low-scoring completion). This is done using **DPO** (**Direct
    Preference Optimization**). Using DPO is simpler than using plain RLHF and intuitively
    it performs better than RLHF. The approach in this case is known as **dDPO** because
    it uses a distilled dataset generated with the help of a teacher model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一阶段的目标是最大化模型对 yw（最高评分的完成）相对于 yl（低评分的完成）的偏好。这是通过**DPO**（**直接偏好优化**）来完成的。使用
    DPO 比使用纯 RLHF 更简单，直观上它比 RLHF 表现更好。在这种情况下，该方法被称为**dDPO**，因为它使用了由教师模型生成的精炼数据集。
- en: '![Exploring the Zephyr 7B: A Comprehensive Guide to the Latest Large Language
    Model](../Images/d5c62eeb7e9fb475d39ffd35cff700e8.png)DPO vs RLHF | Source: [Zephyr
    paper](https://arxiv.org/abs/2310.16944)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![DPO 与 RLHF 对比 | 来源:](../Images/d5c62eeb7e9fb475d39ffd35cff700e8.png) [Zephyr
    论文](https://arxiv.org/abs/2310.16944)'
- en: 'The overall algorithm looks somewhat like this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 整体算法看起来大致如下：
- en: '![Exploring the Zephyr 7B: A Comprehensive Guide to the Latest Large Language
    Model](../Images/94a6f3973c2f1e21a14e30e049876142.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![探索 Zephyr 7B：最新大型语言模型的全面指南](../Images/94a6f3973c2f1e21a14e30e049876142.png)'
- en: 'And can be translated into the following steps:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤可以转化为以下几个步骤：
- en: Compute the probability for (x, yw) and (x, yl) from the dSFT model (forward-only).
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 dSFT 模型 (仅前向) 中 (x, yw) 和 (x, yl) 的概率。
- en: Compute the probability for (x, yw) and (x, yl) from the dDPO model.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 dDPO 模型中 (x, yw) 和 (x, yl) 的概率。
- en: Compute Eq 1 and backpropagate to update. Repeat
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算公式 1 并反向传播以进行更新。重复
- en: Training Details
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练细节
- en: The base model that Zephyr used is Mistral-7B which was the state-of-the-art
    open source at the time of release. They used the [TRL](https://github.com/huggingface/trl)
    library for fine-tuning and alignment. Deep-Speed Zero 3 and Flash-Attention 2
    were used to optimize and speed up the training and to fully utilize the GPU.
    The models were trained using AdamW optimizer and no weight decay was used. All
    experiments were run on 16 A100s using bfloat16 precision and typically took 2–4
    hours to complete. You can refer to the [original paper](https://arxiv.org/pdf/2310.16944.pdf)
    for in-depth details on the Training Procedure of Zephyr.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Zephyr 使用的基础模型是 Mistral-7B，该模型在发布时是最先进的开源模型。他们使用了 [TRL](https://github.com/huggingface/trl)
    库进行微调和对齐。为了优化和加速训练并充分利用 GPU，使用了 Deep-Speed Zero 3 和 Flash-Attention 2。模型使用 AdamW
    优化器进行训练，并未使用权重衰减。所有实验都在 16 个 A100 上运行，使用 bfloat16 精度，通常需要 2-4 小时完成。有关 Zephyr 训练过程的详细信息，请参阅
    [原始论文](https://arxiv.org/pdf/2310.16944.pdf)。
- en: Results
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: Zephyr team combines the best techniques to train the Large Language Models
    and it matched the performance of 40B models with just 7B parameters and matched
    70B for chat models.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Zephyr 团队结合了训练大型语言模型的最佳技术，并且凭借仅 7B 参数的模型达到了 40B 模型的性能，并且在聊天模型中达到了 70B 的水平。
- en: '![Exploring the Zephyr 7B: A Comprehensive Guide to the Latest Large Language
    Model](../Images/89bd26616561e68efc609e6014d45108.png)Comparison of Zephyr vs
    other LLMs | Source: [Zephyr paper](https://arxiv.org/abs/2310.16944)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![探索 Zephyr 7B：最新大型语言模型的全面指南](../Images/89bd26616561e68efc609e6014d45108.png)Zephyr
    与其他 LLM 的比较 | 来源：[Zephyr 论文](https://arxiv.org/abs/2310.16944)'
- en: '![Exploring the Zephyr 7B: A Comprehensive Guide to the Latest Large Language
    Model](../Images/e16f6ba0bbd29b5cb68165720bd5cb79.png)Comparison of Zephyr vs
    other LLMs | Source: [Zephyr paper](https://arxiv.org/abs/2310.16944)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![探索 Zephyr 7B：最新大型语言模型的全面指南](../Images/e16f6ba0bbd29b5cb68165720bd5cb79.png)Zephyr
    与其他 LLM 的比较 | 来源：[Zephyr 论文](https://arxiv.org/abs/2310.16944)'
- en: Usage
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用
- en: Zephyr models are publically available on Hugging Face and can be used similarly
    to any other Language Model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Zephyr 模型在 Hugging Face 上公开提供，可以像使用其他语言模型一样使用。
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Output:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Conclusion
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Zephyr-7B is a small model that showed the power of distillation from a LLM
    to a smaller model. The resulting model ZEPHYR-7B, based on MISTRAL-7B, sets a
    new state-of-the-art for 7B parameter chat models and even outperforms LLAMA2-CHAT-70B
    on MT-Bench.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Zephyr-7B 是一个小型模型，展示了从大型语言模型蒸馏到小型模型的力量。结果模型 ZEPHYR-7B 基于 MISTRAL-7B，设定了 7B 参数聊天模型的新最先进水平，甚至在
    MT-Bench 上超越了 LLAMA2-CHAT-70B。
- en: References
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Zephyr: Direct Distillation of LM Alignment ([https://arxiv.org/abs/2310.16944](https://arxiv.org/abs/2310.16944))'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Zephyr: 直接蒸馏的 LM 对齐 ([https://arxiv.org/abs/2310.16944](https://arxiv.org/abs/2310.16944))'
- en: HuggingFace Zephyr blog ([https://huggingface.co/blog/Isamu136/understanding-zephyr](https://huggingface.co/blog/Isamu136/understanding-zephyr))
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: HuggingFace Zephyr 博客 ([https://huggingface.co/blog/Isamu136/understanding-zephyr](https://huggingface.co/blog/Isamu136/understanding-zephyr))
- en: 'Self Instruct: [https://arxiv.org/abs/2212.10560](https://arxiv.org/abs/2212.10560)'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Self Instruct: [https://arxiv.org/abs/2212.10560](https://arxiv.org/abs/2212.10560)'
- en: 'UltraFeedback: [https://arxiv.org/abs/2310.01377](https://arxiv.org/abs/2310.01377)'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'UltraFeedback: [https://arxiv.org/abs/2310.01377](https://arxiv.org/abs/2310.01377)'
- en: '**[](https://twitter.com/AhmadMustafaAn1)**[Ahmad Anis](https://twitter.com/AhmadMustafaAn1)****
    is a passionate machine learning engineer and researcher currently working at
    [redbuffer.ai](https://redbuffer.ai/). Beyond his day job, Ahmad actively engages
    with the machine learning community. He serves as a regional lead for Cohere for
    AI, a nonprofit dedicated to open science, and is an AWS community builder. Ahmad
    is an active contributor at Stackoverflow, where he has 2300+ points. He has contributed
    to many famous open-source projects, including Shap-E by OpenAI.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**[](https://twitter.com/AhmadMustafaAn1)**[Ahmad Anis](https://twitter.com/AhmadMustafaAn1)****
    是一位充满热情的机器学习工程师和研究员，目前在 [redbuffer.ai](https://redbuffer.ai/) 工作。除了日常工作外，Ahmad
    积极参与机器学习社区。他担任 Cohere for AI 的地区负责人，该组织致力于开放科学，并且是 AWS 社区建设者。Ahmad 还活跃于 Stackoverflow，拥有
    2300+ 点。他对许多著名的开源项目做出了贡献，包括 OpenAI 的 Shap-E。'
- en: More On This Topic
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Exploring Google''s Latest AI Tools: A Beginner''s Guide](https://www.kdnuggets.com/exploring-googles-latest-ai-tools-a-beginners-guide)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[探索谷歌最新的 AI 工具：初学者指南](https://www.kdnuggets.com/exploring-googles-latest-ai-tools-a-beginners-guide)'
- en: '[A Comprehensive List of Resources to Master Large Language Models](https://www.kdnuggets.com/a-comprehensive-list-of-resources-to-master-large-language-models)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握大型语言模型的全面资源列表](https://www.kdnuggets.com/a-comprehensive-list-of-resources-to-master-large-language-models)'
- en: '[Exploring the Latest Trends in AI/DL: From Metaverse to Quantum Computing](https://www.kdnuggets.com/2023/07/exploring-latest-trends-aidl-metaverse-quantum-computing.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[探索 AI/DL 的最新趋势：从元宇宙到量子计算](https://www.kdnuggets.com/2023/07/exploring-latest-trends-aidl-metaverse-quantum-computing.html)'
- en: '[The Ultimate Open-Source Large Language Model Ecosystem](https://www.kdnuggets.com/2023/05/ultimate-opensource-large-language-model-ecosystem.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[终极开源大型语言模型生态系统](https://www.kdnuggets.com/2023/05/ultimate-opensource-large-language-model-ecosystem.html)'
- en: '[7 Steps to Mastering Large Language Model Fine-tuning](https://www.kdnuggets.com/7-steps-to-mastering-large-language-model-fine-tuning)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握大型语言模型微调的 7 个步骤](https://www.kdnuggets.com/7-steps-to-mastering-large-language-model-fine-tuning)'
- en: '[Introduction to NExT-GPT: Any-to-Any Multimodal Large Language Model](https://www.kdnuggets.com/introduction-to-nextgpt-anytoany-multimodal-large-language-model)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NExT-GPT 介绍：任意到任意的多模态大型语言模型](https://www.kdnuggets.com/introduction-to-nextgpt-anytoany-multimodal-large-language-model)'
