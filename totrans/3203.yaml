- en: 'Interpreting Machine Learning Models: An Overview'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/11/interpreting-machine-learning-models-overview.html](https://www.kdnuggets.com/2017/11/interpreting-machine-learning-models-overview.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[An article](https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning)
    on machine learning interpretation appeared on O''Reilly''s blog back in March,
    written by Patrick Hall, Wen Phan, and SriSatish Ambati, which outlined a number
    of methods beyond the usual go-to measures. By chance I happened back upon the
    article again over the weekend, and with a fresh read decided to share some of
    the ideas contained within. The article is a great (if lengthy) read, and recommend
    it to anyone who has the time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The article is organized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the differing complexities of (machine learning) functions to be
    explained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of the scope of interpretability, local (small regions of conditional
    distributions) vs. global (entire conditional distributions)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A discussion of understanding and trust, and how traditional measures of understanding
    -- such as cross-validation and assessment plots -- often don't do enough to inspire
    *trust* in a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A breakdown into 3 sections of interpretation techniques (the real meat of the
    article)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 1 includes approaches for seeing and understanding your data in the context
    of training and interpreting machine learning algorithms, Part 2 introduces techniques
    for combining linear models and machine learning algorithms for situations where
    interpretability is of paramount importance, and Part 3 describes approaches for
    understanding and validating the most complex types of predictive models.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The deconstruction of the interpretability of each technique and group of techniques
    is the focus of the article, while this post is a summary of the techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 1: Seeing your data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section starts the article off slowly, and points out some methods to perform
    visual data exploration beyond the more traditional.
  prefs: []
  type: TYPE_NORMAL
- en: '[T]here are many, many ways to visualize data sets. Most of the techniques
    highlighted below help illustrate all of a data set in just two dimensions, not
    just univariate or bivariate slices of a data set (meaning one or two variables
    at a time). This is important in machine learning because most machine learning
    algorithms automatically model high-degree interactions between variables (meaning
    the effect of combining many i.e., more than two or three variables together).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Visualization techniques presented in this section include:'
  prefs: []
  type: TYPE_NORMAL
- en: Glyphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correlation graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2D projections, such as PCA, MDS, and t-SNE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partial dependence plots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Residual analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Visualization](../Images/ae76e2a1bda4bdeb5a8c435c9a59cf9e.png)'
  prefs: []
  type: TYPE_IMG
- en: A correlation graph representing loans made by a large financial firm. Figure
    courtesy of Patrick Hall and the H2O.ai team.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recommended questions to be asked to help determine the value of these visualization
    techniques (which are similarly asked of techniques in subsequent parts) include:'
  prefs: []
  type: TYPE_NORMAL
- en: What complexity of functions can visualizations help interpret?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do visualizations enhance understanding?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do visualizations enhance trust?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 2: Using machine learning in regulated industry'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Things get a bit more interesting here.
  prefs: []
  type: TYPE_NORMAL
- en: The techniques presented in this section are newer types of linear models or
    models that use machine learning to augment traditional, linear modeling methods.
    These techniques are meant for practitioners who just can't use machine learning
    algorithms to build predictive models because of interpretability concerns.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Techniques outlined in this section include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generalized Additive Models (GAMs)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quantile regression**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Building toward machine learning model benchmarks** -- that is, employ a
    deliberate process when moving from traditional linear models toward machine learning
    algorithms, taking baby steps, and comparing performance and outcomes along the
    way, as opposed to jumping from a simple regression model into the deep end with
    black boxes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine learning in traditional analytics processes** -- this is the suggestion
    to use ML algorithms to augment analytical lifecycle processes for more accurate
    predictions, such as predicting linear model degredation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Small, interpretable ensembles** -- it''s a foregone conclusion at this point
    that there is massive value in ensemble methods, in general, but employing *simple*
    such methods can potentially help boost both accuracy as well as interpretability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[Monotonicity](https://en.wikipedia.org/wiki/Monotonic_function)** constraints
    -- such constraints can possibly transform complex models into interpretable,
    nonlinear, montonic models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Monotonicity is very important for at least two reasons:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Monotonicity is often expected by regulators
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: Monotonicity enables consistent reason code generation
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Ensemble methods](../Images/cd51696c84f76ff168f5f9fd852f0f22.png)'
  prefs: []
  type: TYPE_IMG
- en: A diagram of a small, stacked ensemble. Figure courtesy of Vinod Iyengar and
    the H2O.ai team.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: Understanding complex machine learning models'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In my opinion, this is where things get especially interesting. I approach
    complex machine learning model interpretability as an advocate of automated machine
    learning, since I feel the two techniques are flipsides of the same coin: if we
    are going to be using automated techniques to generate models on the front-end,
    then devising and employing appropriate ways to simplify and understand these
    models on the back-end becomes of utmost importance.'
  prefs: []
  type: TYPE_NORMAL
- en: Here are the approaches outlined for helping understand complex ML models within
    this article.
  prefs: []
  type: TYPE_NORMAL
- en: '**Surrogate models** -- simply, a surrogate is a simple model which can be
    used to explain a more complex model. If the surrogate model is created by training,
    say, a simple linear regression or a decision tree with original input data and
    predictions from the more complex model, the characteristics of the simple model
    can then be assumed to be an accurately descriptive stand-in of the more complex
    model. And it may not not be accurate at all.'
  prefs: []
  type: TYPE_NORMAL
- en: Why, then, employ a surrogate model?
  prefs: []
  type: TYPE_NORMAL
- en: Surrogate models enhance trust when their coefficients, variable importance,
    trends, and interactions are in line with human domain knowledge and reasonable
    expectations of modeled phenomena. Surrogate models can increase trust when used
    in conjunction with sensitivity analysis to test that explanations remain stable
    and in line with human domain knowledge, and reasonable expectations when data
    is lightly and purposefully perturbed, when interesting scenarios are simulated,
    or as data changes over time.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Local Interpretable Model-agnostic Explanations (LIME)** -- LIME is for building
    surrogate models based on a single observation'
  prefs: []
  type: TYPE_NORMAL
- en: An implementation of LIME may proceed as follows. First, the set of explainable
    records are scored using the complex model. Then, to interpret a decision about
    another record, the explanatory records are weighted by their closeness to that
    record, and an L1 regularized linear model is trained on this weighted explanatory
    set. The parameters of the linear model then help explain the prediction for the
    selected record.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Maximum activation analysis** -- a technique which looks to isolate particular
    instances which elicit a maximum response of some model hyperparameter'
  prefs: []
  type: TYPE_NORMAL
- en: In maximum activation analysis, examples are found or simulated that maximally
    activate certain neurons, layers, or filters in a neural network or certain trees
    in decision tree ensembles. For the purposes of maximum activation analysis, low
    residuals for a certain tree are analogous to high-magnitude neuron output in
    a neural network.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So... LIME, or maximum activation analysis, or both?
  prefs: []
  type: TYPE_NORMAL
- en: LIME, discussed above, helps explain the predictions of a model in local regions
    of the conditional distribution. Maximum activation analysis helps enhance trust
    in localized, internal mechanisms of a model. The two make a great pair for creating
    detailed, local explanations of complex response functions.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Sensitivity analysis** -- this technique helps to determine whether intentionally
    perturbed data, or similar data changes, modify model behavior and destabilizes
    the outputs; it is also useful for investigating model behavior for particular
    scenarios of interest or corner cases'
  prefs: []
  type: TYPE_NORMAL
- en: '**Global variable importance measures** -- typically the domain of tree-based
    models; a heuristic for determining varibale importance relates to the depth and
    frequency of a given variable''s split point; higher and more frequent variables
    are decidedly more important'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: For a single decision tree, a variable's importance is quantitatively determined
    by the cumulative change in the splitting criterion for every node in which that
    variable was chosen as the best splitting candidate.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Global variable importance measures](../Images/ac13d1b89b00496705c203dcfb57fa33.png)'
  prefs: []
  type: TYPE_IMG
- en: An illustration of variable importance in a decision tree ensemble model. Figure
    courtesy of Patrick Hall and the H2O.ai team.
  prefs: []
  type: TYPE_NORMAL
- en: '**Leave-One-Covariate-Out (LOCO)** -- "model-agnostic generalization of mean
    accuracy decrease variable importance measures"; originally developed for regression
    models, but applicable more generally; the technique''s goal is to determine the
    variable with the largest absolute impact on a given row -- by iteratively zeroing
    out variable values -- which is determined to be the most important for that row''s
    prediction'
  prefs: []
  type: TYPE_NORMAL
- en: '**How do variable importance measures enhance understanding?**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Variable importance measures increase understanding because they tell us the
    most influential variables in a model and their relative rank.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Treeinterpreter**](https://github.com/andosa/treeinterpreter) -- strictly
    a tree-based model (decision trees, random forest, etc.) interpretation approach'
  prefs: []
  type: TYPE_NORMAL
- en: Treeinterpreter simply outputs a list of the bias and individual contributions
    for a variable in a given model or the contributions the input variables in a
    single record make to a single prediction.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I am currently experimenting with Treeinterpreter and hope to soon share my
    experience.
  prefs: []
  type: TYPE_NORMAL
- en: To reiterate, [this O'Reilly article](https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning)
    which I have superficially summarized merits a full reading if you have time,
    wherein it fleshes out the techniques of which I have only scratched the surface,
    and does a much better job than did I of weaving together how useful each technique
    is and what one should expect as a resultof its utilization.
  prefs: []
  type: TYPE_NORMAL
- en: A thanks to Patrick Hall, Wen Phan, and SriSatish Ambati, the authors of this
    informative article.
  prefs: []
  type: TYPE_NORMAL
- en: '**Related**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Simplifying Decision Tree Interpretability with Python & Scikit-learn](/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Myth of Model Interpretability](/2015/04/model-interpretability-neural-networks-deep-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introduction to Local Interpretable Model-Agnostic Explanations (LIME)](/2016/08/introduction-local-interpretable-model-agnostic-explanations-lime.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
