- en: How to Process a DataFrame with Millions of Rows in Seconds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/01/process-dataframe-millions-rows-seconds.html](https://www.kdnuggets.com/2022/01/process-dataframe-millions-rows-seconds.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![How to process a DataFrame with millions of rows in seconds?](../Images/2de067561faae5b8eb4d567583434cd1.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jason Blackeye](https://unsplash.com/@jeisblack?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Data Science is having its renaissance moment. It's hard to keep track of all
    new Data Science tools that have the potential to change the way Data Science
    gets done.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: I learned about this new Data Processing Engine only recently in a conversation
    with a colleague, also a Data Scientist. We had a discussion about Big Data processing,
    which is at the forefront of innovation in the field, and this new tool popped
    up.
  prefs: []
  type: TYPE_NORMAL
- en: While [pandas](https://towardsdatascience.com/11-pandas-built-in-functions-you-should-know-1cf1783c2b9) is
    the defacto tool for data processing in Python, it doesn’t handle big data well.
    With bigger datasets, you’ll get an out-of-memory exception sooner or later.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers were confronted with this issue a long time ago, which prompted
    the development of tools like [Dask and Spark](https://towardsdatascience.com/are-you-still-using-pandas-for-big-data-12788018ba1a),
    which try to overcome “the single machine” constrain by distributing processing
    to multiple machines.
  prefs: []
  type: TYPE_NORMAL
- en: This active area of innovation also brought us tools like [Vaex](https://towardsdatascience.com/are-you-still-using-pandas-to-process-big-data-in-2021-850ab26ad919),
    which try to solve this issue by making processing on a single machine more memory
    efficient.
  prefs: []
  type: TYPE_NORMAL
- en: And it doesn’t end there. There is another tool for big data processing you
    should know about …
  prefs: []
  type: TYPE_NORMAL
- en: Meet Terality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![How to process a DataFrame with millions of rows in seconds?](../Images/b5eb8dba8279c6a93aa0d3ff1bffb3b3.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [frank mckenna](https://unsplash.com/@frankiefoto?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[Terality](https://www.terality.com/) is a Serverless Data Processing Engine
    that processes the data in the Cloud. There is no need to manage infrastructure
    as Terality takes care of scaling compute resources. Its target audiences are
    Engineers and Data Scientists.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I exchanged a few emails with the Terality’s team as I was interested in the
    tool they’ve developed. They answered swiftly. These were my questions to the
    team:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to process a DataFrame with millions of rows in seconds?](../Images/e2d665667a01d44cb4b0515081c0cb5c.png)'
  prefs: []
  type: TYPE_IMG
- en: My n-th email to the Terality’s team (screenshot by author)
  prefs: []
  type: TYPE_NORMAL
- en: What are the main steps of data processing with Terality?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Terality comes with a Python client that you import into a Jupyter Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then you write the code in **“a pandas way”** and Terality securely uploads
    your data and takes care of distributed processing (and scaling) to calculate
    your analysis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After processing is completed, you can convert the data back to a regular pandas
    DataFrame and continue with analysis locally.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What’s happening behind the scenes?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Terality team developed a proprietary data processing engine — it’s not a fork
    of Spark or Dask.
  prefs: []
  type: TYPE_NORMAL
- en: The goal was to avoid the imperfections of Dask, which doesn’t have the same
    syntax as pandas, it’s asynchronous, doesn’t have all pandas functions and it
    doesn’t support auto-scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Terality’s Data Processing Engine solves these issues.
  prefs: []
  type: TYPE_NORMAL
- en: Is Terality FREE to use?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Terality](https://www.terality.com/pricing) has a free plan with which you
    can process up to 500 GB of data per month. It also offers a paid plan for companies
    and individuals with greater requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we’ll focus on the free plan as it’s applicable to many Data
    Scientists.
  prefs: []
  type: TYPE_NORMAL
- en: How does Terality calculate data usage? ([From Terality’s documentation](https://www.terality.com/pricing))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Consider a dataset with a total size of 15GB in memory, as would be returned
    by the operation *df.memory_usage(deep=True).sum()*.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Running one (1) operation on this dataset, such as a *.sum* or a *.sort_values*,
    would consume 15GB of processed data in Terality.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The billable usage is only recorded when task runs enter a Success state.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What about Data Privacy?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a user performs a read operation, the Terality client copies the dataset
    on Terality’s secured cloud storage on Amazon S3.
  prefs: []
  type: TYPE_NORMAL
- en: Terality has a strict policy around data privacy and protection. They guarantee
    that they’ll not use the data and process it securely.
  prefs: []
  type: TYPE_NORMAL
- en: Terality is not a storage solution. They will delete your data maximum within
    3 days after Terality’s client session is closed.
  prefs: []
  type: TYPE_NORMAL
- en: Terality processing currently occurs on AWS in the Frankfurt region.
  prefs: []
  type: TYPE_NORMAL
- en: '[See the security section for more information.](https://docs.terality.com/faq/security)'
  prefs: []
  type: TYPE_NORMAL
- en: Does the data need to be public?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: No!
  prefs: []
  type: TYPE_NORMAL
- en: The user needs to have access to the dataset on his local machine and Terality
    will handle the uploading process behind the scene.
  prefs: []
  type: TYPE_NORMAL
- en: The upload operation is also parallelized so that is faster.
  prefs: []
  type: TYPE_NORMAL
- en: Can Terality process Big Data?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the moment, **in November 2021, **Terality is still in beta. It’s optimized
    for datasets up to 100–200 GB.
  prefs: []
  type: TYPE_NORMAL
- en: I asked the team if they plan to increase this and they plan to soon start to
    optimize for Terabytes.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take it for a test drive
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![How to process a DataFrame with millions of rows in seconds?](../Images/5049cb89fc1e3da7e44cdff7875f87a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Eugene Chystiakov](https://unsplash.com/@eugenechystiakov?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: I was surprised that you can simply drop in replace pandas import statement
    with Terality’s package and rerun your analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Note, once you import Terality’s Python client, the data processing is not any
    longer performed on your local machine but with Terality’s Data Processing Engine
    in the Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s install Terality and try it in practice…
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can install Terality by simply running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you create a free account on [Terality](https://app.terality.com/) and
    generate an API key:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to process a DataFrame with millions of rows in seconds?](../Images/0e12eac837f2e08e697cc1a50885c409.png)'
  prefs: []
  type: TYPE_IMG
- en: Generate new API key on [Terality](https://app.terality.com/) (screenshot by
    author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is to enter your API key (also replace the email with your email):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Let’s start small…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, that we have Terality installed, we can run a small example to get familiar
    with it.
  prefs: []
  type: TYPE_NORMAL
- en: The practice shows that you get the best of both worlds while using both Terality
    and pandas — one to aggregate the data and the other to analyze the aggregate
    locally
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The command below creates a terality.DataFrame by importing a pandas.DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, that the data is in Terality’s Cloud, we can proceed with analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Running filtering operations and other familiar pandas operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we finish with the analysis, we can convert it back to a pandas DataFrame
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can validate that the DataFrames are equal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Let’s go Big…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I would suggest you check [Terality’s Quick Start Jupyter Notebook,](https://docs.terality.com/getting-terality/quick-start/tutorial) which
    takes you through an analysis of 40 GB of Reddit comments dataset. They also have
    a tutorial with a smaller 5 GB dataset.
  prefs: []
  type: TYPE_NORMAL
- en: I clicked through Terality’s Jupyter Notebook and processed the 40 GB dataset.
    It read the data in 45 seconds and needed 35 seconds to sort it. The merge with
    another table took 1 minute and 17 seconds. It felt like I’m processing a much
    smaller dataset on my laptop.
  prefs: []
  type: TYPE_NORMAL
- en: Then I tried loading the same 40GB dataset with pandas on my laptop with 16
    GB of main memory — it returned an out-of-memory exception.
  prefs: []
  type: TYPE_NORMAL
- en: '[Official Terality tutorial takes you through the analysis of a 5GB file with
    Reddit comments.](https://docs.terality.com/getting-terality/quick-start/tutorial)'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![How to process a DataFrame with millions of rows in seconds?](../Images/008b2b1520e546c7af167ea70812c0aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Sven Scheuermeier](https://unsplash.com/@sveninho?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: I played with Terality quite a bit and my experience was without major issues.
    This surprised me as they are officially still in beta. A great sign is also that
    their support team is really responsive.
  prefs: []
  type: TYPE_NORMAL
- en: I see a great use case for Terality when you have a big data set that you can’t
    process on your local machine — may that be because of memory constraints or the
    speed of processing.
  prefs: []
  type: TYPE_NORMAL
- en: Using Dask (or Spark) would require spinning up a cluster which would cost much
    more than using Terality to complete your analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Also, configuring such a cluster is a cumbersome process, while with Terality
    you only need to change the import statement.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing that I like is that I can use it in my local JupyterLab, because
    I have many extensions, configurations, dark mode, etc.
  prefs: []
  type: TYPE_NORMAL
- en: I’m looking forward to the progress that the team makes with Terality in the
    coming months.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Roman Orac](https://www.linkedin.com/in/romanorac/?originalSubdomain=si)**
    is a Machine Learning engineer with notable successes in improving systems for
    document classification and item recommendation. Roman has experience with managing
    teams, mentoring beginners and explaining complex concepts to non-engineers.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/how-to-process-a-dataframe-with-millions-of-rows-in-seconds-fe7065b8f986).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Create Stunning Data Viz in Seconds with ChatGPT](https://www.kdnuggets.com/create-stunning-data-viz-in-seconds-with-chatgpt)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Select Rows and Columns in Pandas Using [ ], .loc, iloc, .at…](https://www.kdnuggets.com/2019/06/select-rows-columns-pandas.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3 Ways to Append Rows to Pandas DataFrames](https://www.kdnuggets.com/2022/08/3-ways-append-rows-pandas-dataframes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Convert JSON Data into a DataFrame with Pandas](https://www.kdnuggets.com/how-to-convert-json-data-into-a-dataframe-with-pandas)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pandas vs. Polars: A Comparative Analysis of Python''s Dataframe Libraries](https://www.kdnuggets.com/pandas-vs-polars-a-comparative-analysis-of-python-dataframe-libraries)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Frameworks for Approaching the Machine Learning Process](https://www.kdnuggets.com/2018/05/general-approaches-machine-learning-process.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
