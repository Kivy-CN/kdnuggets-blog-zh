- en: What is Softmax Regression and How is it Related to Logistic Regression?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 Softmax 回归以及它与逻辑回归的关系？
- en: 原文：[https://www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html](https://www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html](https://www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html)
- en: 'Softmax Regression (synonyms: Multinomial Logistic, Maximum Entropy Classifier,
    or just Multi-class Logistic Regression) is a generalization of logistic regression
    that we can use for multi-class classification (under the assumption that the
    classes are mutually exclusive). In contrast, we use the (standard) Logistic Regression
    model in binary classification tasks.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 回归（同义词：多项式逻辑回归、最大熵分类器或仅多类逻辑回归）是逻辑回归的一种推广，适用于多类分类（假设各类别是相互排斥的）。相比之下，在二分类任务中，我们使用（标准）逻辑回归模型。
- en: 'Now, let me briefly explain how that works and how softmax regression differs
    from logistic regression. I have a more detailed explanation on logistic regression
    here: [LogisticRegression - mlxtend](http://rasbt.github.io/mlxtend/user_guide/classifier/LogisticRegression/) ,
    but let me re-use one of the figures to make things more clear:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我简要解释一下这如何工作以及 softmax 回归与逻辑回归的区别。我在这里有一个关于逻辑回归的更详细的解释：[LogisticRegression
    - mlxtend](http://rasbt.github.io/mlxtend/user_guide/classifier/LogisticRegression/)，但让我重用其中一个图来使事情更清楚：
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速通道进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[![](../Images/f9ab2178b82863a5f535d5fc68c71052.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/logistic_regression_schematic.png)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/f9ab2178b82863a5f535d5fc68c71052.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/logistic_regression_schematic.png)'
- en: 'As the name suggests, in softmax regression (SMR), we replace the sigmoid logistic
    function by the so-called *softmax function* φ:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 正如名字所示，在 softmax 回归（SMR）中，我们用所谓的 *softmax 函数* φ 替代了 sigmoid 逻辑函数：
- en: '[![](../Images/563c1d77e93e9c16ce2d36ea50e28dc5.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/1.png)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/563c1d77e93e9c16ce2d36ea50e28dc5.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/1.png)'
- en: where we define the net input z as
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将净输入 z 定义为
- en: '[![](../Images/d6d875c14ceed22ddb6fd7edd3b8a778.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/2.png)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/d6d875c14ceed22ddb6fd7edd3b8a778.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/2.png)'
- en: (*w* is the weight vector, *x* is the feature vector of 1 training sample, and *w0* is
    the bias unit.)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: (*w* 是权重向量，*x* 是 1 个训练样本的特征向量，*w0* 是偏置项。)
- en: Now, this softmax function computes the probability that this training sample
    x(i) belongs to class *j* given the weight and net input z(i). So, we compute
    the probability *p(y = j | x(i); wj)* for each class label in *j = 1, ..., k*.
    Note the normalization term in the denominator which causes these class probabilities
    to sum up to one.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个 softmax 函数计算这个训练样本 x(i) 属于类别 *j* 的概率，给定权重和净输入 z(i)。因此，我们计算每个类别标签 *j =
    1, ..., k* 的概率 *p(y = j | x(i); wj)*。注意分母中的归一化项，这使得这些类别概率的总和为 1。
- en: To illustrate the concept of softmax, let us walk through a concrete example.
    Let's assume we have a training set consisting of 4 samples from 3 different classes
    (0, 1, and 2).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明 softmax 的概念，让我们通过一个具体的例子来说明。假设我们有一个包含来自 3 个不同类别（0、1 和 2）的 4 个样本的训练集。
- en: '[![](../Images/23f0485b8fb6cbdfdd4bbbee24d2aa8d.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/3.png)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/23f0485b8fb6cbdfdd4bbbee24d2aa8d.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/3.png)'
- en: 'First, we want to encode the class labels into a format that we can more easily
    work with; we apply one-hot encoding:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们希望将类别标签编码成更容易处理的格式；我们应用独热编码：
- en: '[![](../Images/70781052d5025bf09b5fb487e26961f2.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/4.png)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/70781052d5025bf09b5fb487e26961f2.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/4.png)'
- en: A sample that belongs to class 0 (the first row) has a 1 in the first cell,
    a sample that belongs to class 2 has a 1 in the second cell of its row, and so
    forth. Next, let us define the feature matrix of our 4 training samples. Here,
    we assume that our dataset consists of 2 features; thus, we create a 4×(2+1) dimensional
    matrix (+1 one for the bias term).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 属于类别0（第一行）的样本在第一个单元格中有一个1，属于类别2的样本在其行的第二个单元格中有一个1，依此类推。接下来，让我们定义我们4个训练样本的特征矩阵。在这里，我们假设我们的数据集包含2个特征；因此，我们创建一个4×(2+1)维的矩阵（+1是为了偏置项）。
- en: '[![](../Images/f6c45705f182375ba0180fef08793dfe.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/5.png)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/f6c45705f182375ba0180fef08793dfe.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/5.png)'
- en: Similarly, we created a (2+1)×3 dimensional weight matrix (one row per feature
    and one column for each class).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们创建了一个(2+1)×3维的权重矩阵（每个特征一行，每个类别一列）。
- en: To compute the net input, we multiply the 4×(2+1) feature matrix X with the
    (2+1)×3 (n_features × n_classes) weight matrixW.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算净输入，我们将4×(2+1)的特征矩阵X与(2+1)×3的权重矩阵W相乘。
- en: Z = WX
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Z = WX
- en: which yields a 4×3 output matrix (n_samples × n_classes).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生一个4×3的输出矩阵（n_samples × n_classes）。
- en: '[![](../Images/9166d75da035d87587891f4f4d7585d5.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/6.png)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/9166d75da035d87587891f4f4d7585d5.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/6.png)'
- en: 'Now, it''s time to compute the softmax activation that we discussed earlier:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，到了计算我们之前讨论的softmax激活的时候了：
- en: '[![](../Images/9ff62d7a0369f7ce71ba5fa06066e8d8.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/7.png)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/9ff62d7a0369f7ce71ba5fa06066e8d8.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/7.png)'
- en: '[![](../Images/81f5eadda13299d3f2f86cb2bf829b1c.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/8.png)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/81f5eadda13299d3f2f86cb2bf829b1c.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/8.png)'
- en: As we can see, the values for each sample (row) nicely sum up to 1 now. E.g.,
    we can say that the first sample
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，每个样本（行）的值现在很好地加起来等于1。例如，我们可以说第一个样本
- en: '`[ 0.29450637 0.34216758 0.36332605]` has a 29.45% probability to belong to
    class 0\. Now, in order to turn these probabilities back into class labels, we
    could simply take the argmax-index position of each row:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`[ 0.29450637 0.34216758 0.36332605]`有29.45%的概率属于类别0。现在，为了将这些概率转换回类别标签，我们可以简单地取每行的argmax索引位置：'
- en: '[![](../Images/bc0600f0e12a2a1cafce9e75e6f3b0ef.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/9.png)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/bc0600f0e12a2a1cafce9e75e6f3b0ef.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/9.png)'
- en: 'As we can see, our predictions are terribly wrong, since the correct class
    labels are `[0, 1, 2, 2]`. Now, in order to train our logistic model (e.g., via
    an optimization algorithm such as gradient descent), we need to define a cost
    function *J* that we want to minimize:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们的预测完全错误，因为正确的类别标签是`[0, 1, 2, 2]`。现在，为了训练我们的逻辑回归模型（例如，通过优化算法如梯度下降），我们需要定义一个我们想要最小化的成本函数*J*：
- en: '[![](../Images/e3bfeb7a326f23f9a395151da4ab1591.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/10.png)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/e3bfeb7a326f23f9a395151da4ab1591.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/10.png)'
- en: which is the average of all cross-entropies over our n training samples. The
    cross-entropy function is defined as
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们所有n个训练样本上的交叉熵的平均值。交叉熵函数定义为
- en: '[![](../Images/6944aec9fb70dff09389cbe416878974.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/11.png)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/6944aec9fb70dff09389cbe416878974.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/11.png)'
- en: Here the T stands for "target" (the true class labels) and the O stands for
    output (the computed probability via softmax; *not*the predicted class label).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的T代表“目标”（真实类别标签），O代表输出（通过softmax计算的概率；*不是*预测的类别标签）。
- en: '[![](../Images/2c2a95ee44b4571b0d77e5820be7bb51.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/12.png)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/2c2a95ee44b4571b0d77e5820be7bb51.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/12.png)'
- en: In order to learn our softmax model via gradient descent, we need to compute
    the derivative
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过梯度下降学习我们的 softmax 模型，我们需要计算导数
- en: '[![](../Images/06232355e2240b05d66bd77d89e4aec9.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/13.png)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/06232355e2240b05d66bd77d89e4aec9.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/13.png)'
- en: 'which we then use to update the weights in opposite direction of the gradient:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们用它来在梯度的反方向上更新权重：
- en: '[![](../Images/256ab5a0ad5a3b30c58c7801f6c7e185.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/14.png) for
    each class j.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/256ab5a0ad5a3b30c58c7801f6c7e185.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/14.png)
    每个类别 j。'
- en: '(Note that w_j is the weight vector for the class *y=j*.) I don''t want to
    walk through more tedious details here, but this cost derivative turns out to
    be simply:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: （注意 w_j 是类别 *y=j* 的权重向量。）我不想在这里详细讨论更多乏味的细节，但这个成本导数实际上是：
- en: '[![](../Images/2c45b481ea49ca43da98e7b2ec62ed1b.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/15.png)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/2c45b481ea49ca43da98e7b2ec62ed1b.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression/15.png)'
- en: Using this cost gradient, we iteratively update the weight matrix until we reach
    a specified number of epochs (passes over the training set) or reach the desired
    cost threshold.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个成本梯度，我们迭代地更新权重矩阵，直到达到指定的训练周期（对训练集的遍历次数）或达到期望的成本阈值。
- en: '**Bio: [Sebastian Raschka](https://twitter.com/rasbt)** is a ''Data Scientist''
    and Machine Learning enthusiast with a big passion for Python & open source. Author
    of ''[Python Machine Learning](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning)''.
    Michigan State University.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介： [Sebastian Raschka](https://twitter.com/rasbt)** 是一位“数据科学家”和机器学习爱好者，对
    Python 和开源充满热情。著有《[Python 机器学习](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning)》。密歇根州立大学。'
- en: '[Original](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression.md).
    Reposted with permission.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression.md)。经许可转载。'
- en: '**Related:**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[When Does Deep Learning Work Better Than SVMs or Random Forests?](/2016/04/deep-learning-vs-svm-random-forest.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习何时比 SVM 或随机森林效果更好？](/2016/04/deep-learning-vs-svm-random-forest.html)'
- en: '[The Development of Classification as a Learning Machine](/2016/04/development-classification-learning-machine.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分类作为学习机器的发展](/2016/04/development-classification-learning-machine.html)'
- en: '[Why Implement Machine Learning Algorithms From Scratch?](/2016/05/implement-machine-learning-algorithms-scratch.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么从零开始实现机器学习算法？](/2016/05/implement-machine-learning-algorithms-scratch.html)'
- en: More On This Topic
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Data Science Definition Humor: A Collection of Quirky Quotes…](https://www.kdnuggets.com/2022/02/data-science-definition-humor.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学定义幽默：一系列古怪的名言…](https://www.kdnuggets.com/2022/02/data-science-definition-humor.html)'
- en: '[Comparing Linear and Logistic Regression](https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[比较线性回归与逻辑回归](https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html)'
- en: '[Classification Metrics Walkthrough: Logistic Regression with…](https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分类指标演示：逻辑回归与…](https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html)'
- en: '[An Overview of Logistic Regression](https://www.kdnuggets.com/2022/02/overview-logistic-regression.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[逻辑回归概述](https://www.kdnuggets.com/2022/02/overview-logistic-regression.html)'
- en: '[Linear vs Logistic Regression: A Succinct Explanation](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性回归与逻辑回归：简明解释](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
- en: '[KDnuggets News 22:n12, March 23: Best Data Science Books for…](https://www.kdnuggets.com/2022/n12.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻 22:n12，3月23日：最佳数据科学书籍…](https://www.kdnuggets.com/2022/n12.html)'
