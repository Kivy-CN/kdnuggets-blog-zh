- en: A Concise Overview of Standard Model-fitting Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2016/05/concise-overview-model-fitting-methods.html](https://www.kdnuggets.com/2016/05/concise-overview-model-fitting-methods.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In order to explain the differences between alternative approaches to estimating
    the parameters of a model, let''s take a look at a concrete example: Ordinary
    Least Squares (OLS) Linear Regression. The illustration below shall serve as a
    quick reminder to recall the different components of a simple linear regression
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple regresion](../Images/021416dd372001b2ebcefd88bc666f0c.png)'
  prefs: []
  type: TYPE_IMG
- en: In Ordinary Least Squares (OLS) Linear Regression, our goal is to find the line
    (or hyperplane) that minimizes the vertical offsets. Or, in other words, we define
    the best-fitting line as the line that minimizes the sum of squared errors (SSE)
    or mean squared error (MSE) between our target variable (y) and our predicted
    output over all samples *i* in our dataset of size *n*.
  prefs: []
  type: TYPE_NORMAL
- en: '![SSE](../Images/fd700aecb123fddccbe1855cb4f39dc2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can implement a linear regression model for performing ordinary least
    squares regression using one of the following approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Solving the model parameters analytically (closed-form equations)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using an optimization algorithm (Gradient Descent, Stochastic Gradient Descent,
    Newton's Method, Simplex Method, etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1) Normal Equations (closed-form solution)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The closed-form solution may (should) be preferred for "smaller" datasets --
    if computing (a "costly") matrix inverse is not a concern. For very large datasets,
    or datasets where the inverse of XTX may not exist (the matrix is non-invertible
    or singular, e.g., in case of perfect multicollinearity), the GD or SGD approaches
    are to be preferred. The linear function (linear regression model) is defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear model](../Images/18f72e806df0ea656621fbaa5fcd6263.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *y* is the response variable, *x* is an *m*-dimensional sample vector,
    and *w* is the weight vector (vector of coefficients). Note that *w0* represents
    the y-axis intercept of the model and therefore *x0=1*. Using the closed-form
    solution (normal equation), we compute the weights of the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Closed form](../Images/390bdb586f5ea1904be897034215580a.png)'
  prefs: []
  type: TYPE_IMG
- en: 2) Gradient Descent (GD)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using the Gradient Decent (GD) optimization algorithm, the weights are updated
    incrementally after each epoch (= pass over the training dataset).
  prefs: []
  type: TYPE_NORMAL
- en: 'The cost function *J(⋅)*, the sum of squared errors (SSE), can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![J](../Images/9971ba71f72a930fafe670ba77d3c459.png)'
  prefs: []
  type: TYPE_IMG
- en: The magnitude and direction of the weight update is computed by taking a step
    in the opposite direction of the cost gradient
  prefs: []
  type: TYPE_NORMAL
- en: '![dw](../Images/2a8a0c5f09633d23aa090c7a43d83736.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *η* is the learning rate. The weights are then updated after each epoch
    via the following update rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![w_upd](../Images/97ab06d282b695cae3a47b50bd05bbdd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where Δw is a vector that contains the weight updates of each weight coefficient *w*,
    which are computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![w_upd explained](../Images/b17273c5b1d049e3c2a73f800eb790f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Essentially, we can picture GD optimization as a hiker (the weight coefficient)
    who wants to climb down a mountain (cost function) into a valley (cost minimum),
    and each step is determined by the steepness of the slope (gradient) and the leg
    length of the hiker (learning rate). Considering a cost function with only a single
    weight coefficient, we can illustrate this concept as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![GD optimization](../Images/9ad85c8821ba0a5eef7027278d573d00.png)'
  prefs: []
  type: TYPE_IMG
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Removing Outliers Using Standard Deviation in Python](https://www.kdnuggets.com/2017/02/removing-outliers-standard-deviation-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Developing an Open Standard for Analytics Tracking](https://www.kdnuggets.com/2022/07/developing-open-standard-analytics-tracking.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Centroid Initialization Methods for k-means Clustering](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Alternative Feature Selection Methods in Machine Learning](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python String Methods](https://www.kdnuggets.com/2022/12/python-string-methods.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11 Python Magic Methods Every Programmer Should Know](https://www.kdnuggets.com/11-python-magic-methods-every-programmer-should-know)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
