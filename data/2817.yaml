- en: Using AI to Identify Wildlife in Camera Trap Images from the Serengeti
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用人工智能识别塞伦盖蒂摄像陷阱图像中的野生动物
- en: 原文：[https://www.kdnuggets.com/2020/02/using-ai-identify-wildlife-images-serengeti.html](https://www.kdnuggets.com/2020/02/using-ai-identify-wildlife-images-serengeti.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/02/using-ai-identify-wildlife-images-serengeti.html](https://www.kdnuggets.com/2020/02/using-ai-identify-wildlife-images-serengeti.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Marek Rogala](https://www.linkedin.com/in/marrogala/) and [Jędrzej Świeżewski,
    PhD,](https://www.linkedin.com/in/swiezew/) [Appsilon](https://appsilon.com/)**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者： [Marek Rogala](https://www.linkedin.com/in/marrogala/) 和 [Jędrzej Świeżewski,
    PhD,](https://www.linkedin.com/in/swiezew/) [Appsilon](https://appsilon.com/)**'
- en: An opportunity for biodiversity research...
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 生物多样性研究的机会...
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT 需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Camera trap imaging (automatic photography of animal species in the wild) is
    becoming the gold standard in biodiversity conservation efforts. It allows for
    accurately monitoring large swaths of land at an unprecedented scale. However,
    the sheer volume of data generated using these devices makes it very difficult
    for humans to analyse. With recent developments in machine learning and computer
    vision, we acquired the tools to resolve this issue and provide the biodiversity
    community with an ability to tap the potential of the knowledge generated automatically
    with systems triggered by a combination of heat and motion.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 摄像陷阱成像（自动拍摄野生动物物种）正成为生物多样性保护工作的黄金标准。它能够以空前的规模准确监测大片土地。然而，这些设备生成的数据量庞大，使得人工分析变得非常困难。随着机器学习和计算机视觉的最新进展，我们获得了解决这一问题的工具，并为生物多样性社区提供了利用由热量和运动触发的系统自动生成的知识潜力的能力。
- en: '![Figure](../Images/03a373af4a8cc9ecf8e46e92aa13c817.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/03a373af4a8cc9ecf8e46e92aa13c817.png)'
- en: Camera traps, arguably less obstructive than human surveyors, still attract
    attention
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于人类调查员，摄像陷阱虽然干扰较小，但仍然会引起注意
- en: …and a challenge for the Machine Learning community
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: …对机器学习社区的挑战
- en: We recently took part in **Hakuna-ma Data**, a competition organised by [DrivenData](https://www.drivendata.org/competitions/59/camera-trap-serengeti/page/145/) in
    partnership with [Microsoft’s AI for Earth](https://www.microsoft.com/en-us/ai/ai-for-earth),
    which asked participants to build an algorithm for wildlife detection that would
    generalise well across time and locations. This competition differed from previous
    iterations in the sense that researchers, data scientists and developers did not
    have direct access to the test image set. Rather, they were asked to submit their
    models and have them executed by the event organizers in Microsoft Azure. 811
    participants from around the world trained their models on the publicly available
    data from 10 seasons to finally submit their solution to be tested on the 11th
    season’s private data set.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最近参加了**Hakuna-ma Data**，这是由 [DrivenData](https://www.drivendata.org/competitions/59/camera-trap-serengeti/page/145/)
    与 [微软地球人工智能](https://www.microsoft.com/en-us/ai/ai-for-earth) 合作组织的比赛，要求参与者构建一个能够跨时间和地点良好泛化的野生动物检测算法。这次比赛与之前的版本不同，研究人员、数据科学家和开发者无法直接访问测试图像集，而是需要提交他们的模型，由活动组织者在
    Microsoft Azure 上运行。来自全球的811名参与者使用来自10个季节的公开数据训练他们的模型，并最终将解决方案提交以在第11季的私人数据集上进行测试。
- en: We congratulate all our fellow competitors and the *ValAn_picekl* team for taking
    the grand prize of $12,000\. We are also proud to announce that we took 5th place
    in the final leaderboard and we would like to share how we managed to achieve
    this.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们祝贺所有的竞争者和*ValAn_picekl*团队赢得了$12,000的大奖。我们也很自豪地宣布，我们在最终排名中获得了第五名，并且我们希望分享我们如何实现这一成就。
- en: You can play with our final model in a ready-to-run Google Colab notebook [here](https://colab.research.google.com/github/Appsilon/serengeti_try_it_yourself/blob/master/classify_images_on_colab.ipynb).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在准备就绪的Google Colab笔记本中体验我们的最终模型，点击[这里](https://colab.research.google.com/github/Appsilon/serengeti_try_it_yourself/blob/master/classify_images_on_colab.ipynb)。
- en: Our approach
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们的方法
- en: The competition involved processing large volumes of images so using a fast
    neural network framework with strong GPUs in the backend was a must. We were provisioning
    virtual machines from Google Cloud Platform equipped with sufficiently powerful
    GPUs (ranging from Tesla K80 to Tesla V100 for the most heavy lifting). Typically
    we were running a few of them in parallel to speed up experimentation. We worked
    with Python code, coordinating crucial parts of code in a GitHub repository and
    using notebooks for fast experimenting and visual inspection of the performance
    of models. We built the models in Fast.ai (with PyTorch backend), and used the
    integration with [Weights & Biases](https://www.wandb.com/) for keeping track
    of our experiments.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 比赛涉及处理大量图像，因此使用一个快速的神经网络框架和强大的GPU是必须的。我们从Google Cloud Platform中提供了配备足够强大GPU的虚拟机（从Tesla
    K80到Tesla V100，用于最繁重的计算）。通常，我们会并行运行几台机器以加快实验进程。我们使用Python代码，协调GitHub存储库中的关键代码部分，并使用笔记本快速实验和视觉检查模型的性能。我们在Fast.ai中构建了模型（以PyTorch为后端），并使用了与[Weights
    & Biases](https://www.wandb.com/)的集成来跟踪我们的实验。
- en: Based on previous [studies](https://www.pnas.org/content/115/25/E5716) of the
    dataset, we decided to work with ResNet 50, an architecture of a rather moderate
    depth. Since we joined the competition very close to the end, we opted not to
    experiment with other (or simply deeper) architectures, and after seeing promising
    initial results of this one, decided to focus on improving them.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 基于之前对数据集的[研究](https://www.pnas.org/content/115/25/E5716)，我们决定使用ResNet 50，这是一个相对中等深度的架构。由于我们在比赛结束前不久才加入，因此选择不尝试其他（或更深）的架构，并在看到这个架构的初步结果令人满意后，决定专注于改进它。
- en: 'Given the lack of time we had to optimise our strategy. We used an agile approach:
    we quickly created our baseline “MVP” (Minimum Viable Product) of a solution and
    then iterated a lot based on its results. Initially, submissions were allowed
    only every five days, which gave us a natural sprint-like rhythm.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于时间有限，我们不得不优化我们的策略。我们采用了敏捷方法：迅速创建了我们的基线“ MVP”（最小可行产品）解决方案，然后根据结果进行大量迭代。最初，提交每五天仅允许一次，这给了我们一种自然的冲刺节奏。
- en: 'The dataset was challenging in itself: 6.6 million photos with relatively large
    resolution – several terabytes of data. We reused a version of the dataset provided
    by one of the participants in which the photos’ resolution had been significantly
    scaled down (by a factor 4 on each side).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集本身非常具有挑战性：660万张照片，分辨率相对较高——数据量达到几TB。我们使用了一个参与者提供的数据集版本，其中照片的分辨率已经显著缩小（每边缩小了4倍）。
- en: '![class of image and percentage of occurrence ](../Images/317f4a3731b22e9118ed502993f0a611.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图像类别及出现百分比](../Images/317f4a3731b22e9118ed502993f0a611.png)'
- en: The dataset was very imbalanced – around 75% of the images were empty. The most
    popular animals are wildebeest, zebra and Thomson’s gazelle. On the other side
    of the spectrum you have super rare animals like steenbok or bats, visible only
    in a handful of photos.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集非常不平衡——大约75%的图像为空。最常见的动物是角马、斑马和汤姆森的瞪羚。在光谱的另一端，有像steenbok或蝙蝠这样的超稀有动物，仅在少数几张照片中可见。
- en: '![Figure](../Images/74b46d7bdee3dcddb6f66523f5aa0f2e.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/74b46d7bdee3dcddb6f66523f5aa0f2e.png)'
- en: As rare as in nature – rhinos were one of the least common species in the dataset
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然界中也极为稀有——犀牛是数据集中最不常见的物种之一
- en: Top 5 things that worked
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有效的前5个因素
- en: 'We believe the following 5 key factors contributed the most to our models’
    success:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为以下5个关键因素对我们模型的成功贡献最大：
- en: '**1\. Large validation set**'
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**1\. 大规模验证集**'
- en: This was crucial to ensure that the model generalises well, because we were
    assessed on the last season – a private, unseen dataset. Each season comes from
    a different year. We decided to hold out an entire season (season 8) for validation
    – we wanted it to contain many images (almost a million!), and yet be relatively
    recent – since the test set consisted of yet unpublished season 11\. This choice
    was also supported by a study of the distributions of individual species across
    seasons. We noticed that only the last three seasons (8, 9 and 10) contained photos
    of a few of the relatively rare species.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这对确保模型良好泛化至关重要，因为我们在最后一个赛季——一个私有的、未见过的数据集上进行了评估。每个赛季都来自不同的年份。我们决定将整个赛季（第8赛季）留作验证集——我们希望它包含许多图像（近百万张！），且相对较新——因为测试集由尚未公布的第11赛季组成。这个选择也得到了对不同物种在赛季间分布的研究支持。我们注意到只有最后三个赛季（第8、第9和第10赛季）包含了一些相对稀有物种的照片。
- en: '**2\. Training with growing resolution**'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**2. 使用逐渐增长的分辨率进行训练**'
- en: We divided the training process into three stages. Each stage had a part in
    which we trained only the final layers of the network followed by training of
    all the layers. At each stage we trained the model on images of an increasingly
    higher resolution, letting us train longer without overfitting. The maximal resolution
    we used was 512×384, which was still a quarter of the provided images in each
    dimension.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练过程分为三个阶段。每个阶段都有一部分是仅训练网络的最终层，随后训练所有层。在每个阶段，我们在分辨率逐渐提高的图像上训练模型，使我们可以更长时间地训练而不会过拟合。我们使用的最大分辨率是512×384，这仍然是提供的每个维度的四分之一。
- en: 'The final model was trained in the following stages:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最终模型在以下阶段进行训练：
- en: 5 epochs on network’s final layers on 128×96 px images
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在128×96像素图像上对网络最终层进行5个周期的训练
- en: 5 epochs on all layers on 128×96 px images
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在128×96像素图像上对所有层进行5个周期的训练
- en: 5 epochs on network’s final layers on 256×192 px images
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在256×192像素图像上对网络最终层进行5个周期的训练
- en: 5 epochs on all layers on 256×192 px images
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在256×192像素图像上对所有层进行5个周期的训练
- en: 5 epochs on network’s final layers on 512×384 px images
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在512×384像素图像上对网络最终层进行5个周期的训练
- en: 5 epochs on all layers on 512×384 px images
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在512×384像素图像上对所有层进行5个周期的训练
- en: This approach significantly sped up our training – even a model operating on
    128×96 images achieved a good accuracy, and we could train it much faster than
    the 512×384 model. At this point we would like to give kudos to Pavel Pleskov,
    a participant of the competition who, in the spirit of healthy rivalry, shared
    a scaled down version of the dataset with the community saving us download time
    and allowing many more participants to join.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法显著加快了我们的训练速度——即使是操作在128×96图像上的模型也达到了良好的准确性，而且我们可以比512×384模型更快地训练它。此时我们要特别感谢Pavel
    Pleskov，这位竞赛参与者以健康竞争的精神，与社区分享了一个缩小版的数据集，节省了我们的下载时间，并允许更多参与者加入。
- en: '**3\. Data augmentation / one cycle fitting**'
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**3. 数据增强 / 一周期拟合**'
- en: We used standard image augmentations during training (horizontal flips, small
    rotations, small zooms, small warps and tweaks of lighting) to keep the model
    from overfitting on a pixel level and help it generalize. We used [Leslie Smith’s
    one cycle policy](https://arxiv.org/pdf/1803.09820.pdf) to speed up the training.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练过程中使用了标准的图像增强（水平翻转、小旋转、小缩放、小变形和光照调整），以防止模型在像素级别上过拟合并帮助其泛化。我们使用了[Leslie
    Smith的一个周期策略](https://arxiv.org/pdf/1803.09820.pdf)来加速训练。
- en: '**4\. Aggressive undersampling**'
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**4. 激进的欠采样**'
- en: We eliminated large portions of the most frequent classes (e.g., 95% of empty
    photos) from our training to put more stress during training on examples from
    the less frequent classes. This allowed us to train significantly faster, helping
    the model to focus on the challenging part of the input space. A caveat here was
    to remove them in a smart way – keeping examples with more than one animal in
    the training set as they are harder for models to get right.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从训练集中去除了大部分最常见的类别（例如，95%的空照片），以在训练中更多地关注来自较少见类别的示例。这使得我们可以显著加快训练速度，帮助模型集中于输入空间的挑战性部分。这里的一个警告是需要以智能的方式去除这些示例——保留包含多个动物的示例，因为这些对模型来说更难处理。
- en: '![Figure](../Images/b843ba700b8e735ff3b98deeceb8be35.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/b843ba700b8e735ff3b98deeceb8be35.png)'
- en: Multiple animals in one picture – a challenging case for an ML model
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一张图片中的多只动物——对ML模型的挑战
- en: '**5\. Inspection of losses / post-processing**'
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**5. 损失检查 / 后处理**'
- en: What helped us to push the accuracy into top places was working deeply with
    the data and results given by our initial models. We wanted to see which images
    we got wrong and what happened in each range of our predicted certainty. To minimize
    our loss and optimize our submission we studied individual classes and inspected
    where we lose the most.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 帮助我们将准确率提升到前列的是深入分析数据和初始模型的结果。我们想要查看哪些图像被误判了，以及在我们预测的置信度范围内发生了什么。为了最小化损失并优化我们的提交结果，我们研究了各个类别并检查了我们最容易出错的地方。
- en: At this stage we chose to incorporate a simple inference mechanism from all
    the images in a sequence and not just single images. We took the predictions from
    the first image of each sequence (again guided by results on the validation set),
    unless the maximum prediction for a given species in a photo later in the sequence
    was higher and exceeded a certain threshold (this way we allowed, e.g., birds
    from “otherbird” flying by in frames later in the sequence to affect the prediction).
    Analogously for the “empty” category, we took the prediction from the first photo
    in a sequence unless in a later photo, the prediction for this class was below
    a certain threshold (this way, if an animal appeared only later, the prediction
    for “empty” was toned down suitably).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们选择从序列中的所有图像中整合简单的推断机制，而不仅仅是单张图像。我们取每个序列中第一张图像的预测（同样根据在验证集上的结果），除非在序列中后来的某张照片中，某个物种的最大预测值更高且超过了某个阈值（这样，我们允许例如“otherbird”类别的鸟在序列后续帧中出现时影响预测）。对于“空”的类别也是类似的，我们取序列中第一张照片的预测值，除非在后来的照片中，该类别的预测值低于某个阈值（这样，如果动物只在之后出现，"空"的预测值就会适当地降低）。
- en: We next processed ranges of predicted values for each of the species with a
    high occurrence frequency and grid-searched for basic linear transformations minimizing
    our loss on the validation set. This helped the score a lot – most likely by getting
    the model’s returned probabilities to reflect species distribution in the dataset.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对每个高频出现的物种的预测值范围进行了处理，并进行网格搜索以找到最小化验证集上损失的基本线性变换。这大大提高了分数——很可能是通过使模型返回的概率更好地反映数据集中物种的分布。
- en: In retrospect – what we would like to have done
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回顾过去——我们希望做的事情
- en: 'There were a few more ideas we were eager to try, but because we joined the
    competition only 3 weeks before its conclusion, we did not. Most notably:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一些其他的想法很想尝试，但由于我们在比赛结束前仅有3周时间才加入，所以没有尝试。最值得注意的是：
- en: '**Focal loss**'
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**焦点损失**'
- en: The dataset at hand was highly imbalanced. An alternative to the aggressive
    downsampling we implemented (see above), would have been to use a modified loss
    function, namely the focal loss. While [first proposed](https://arxiv.org/pdf/1708.02002.pdf) in
    the context of object detection, it has been recently used also in the context
    of imbalanced classification problems.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的数据集高度不平衡。相比于我们实现的激进下采样方法（见上文），一种替代方案是使用修改后的损失函数，即焦点损失。虽然**[焦点损失](https://arxiv.org/pdf/1708.02002.pdf)**最初是在物体检测的背景下提出的，但最近也在不平衡分类问题中得到了应用。
- en: '**More training time, more epochs**'
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**更多的训练时间，更多的训练轮次**'
- en: Various signs indicated that our models might have been simply trained longer
    to achieve even better scores. Wary of overfitting, we have approached the training
    schedules conservatively, however given more time, we would have liked to explore
    longer training at the various stages (see above).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 各种迹象表明，我们的模型可能只是需要更长时间的训练来获得更好的分数。虽然我们对过拟合保持谨慎，但如果有更多的时间，我们希望探索在各个阶段更长时间的训练（见上文）。
- en: '**Use a 2-stage approach: animal location detection and animal classification**'
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**使用两阶段的方法：动物位置检测和动物分类**'
- en: We considered using a two stage approach, with the first model focusing on animal
    presence regardless of its species, and the second model focusing on species classification.
    Such an approach could have boosted accuracy and was suggested by previous research.
    Due to limited time we chose not to try this approach.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑过使用两阶段的方法，第一阶段的模型关注动物的存在而不考虑其种类，第二阶段的模型则专注于物种分类。这种方法可能会提高准确性，之前的研究也提出过这种建议。由于时间有限，我们选择不尝试这种方法。
- en: After the competition we learned about great work in this area by Siyu Yang
    and Dan Morris from Microsoft AI for Earth. They built [a model for animal detection
    in pictures](https://medium.com/microsoftazure/accelerating-biodiversity-surveys-with-azure-machine-learning-9be53f41e674).
    It identifies the presence of animals in photos, providing bounding boxes for
    positive predictions. The strength of the model comes from it being trained on
    data from multiple geographical locations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 比赛后，我们了解到 Microsoft AI for Earth 的 Siyu Yang 和 Dan Morris 在这一领域的伟大工作。他们建立了[一种动物检测模型](https://medium.com/microsoftazure/accelerating-biodiversity-surveys-with-azure-machine-learning-9be53f41e674)。它识别照片中的动物存在，为正面预测提供边界框。该模型的优势在于它基于来自多个地理位置的数据进行训练。
- en: They suggest a very interesting approach of using a generic model for detecting
    animal location, and a second project- and location-specific model for species
    classification. One clear advantage is that the second model can focus precisely
    on the animal, which clearly has a potential to give better accuracy. This method
    can also help a lot when it comes to recognizing multiple animals in a picture.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 他们建议了一种非常有趣的方法：使用一个通用模型来检测动物的位置，以及一个项目和位置特定的模型来进行物种分类。一个明显的优点是，第二个模型可以精确关注动物，这显然有可能提高准确性。这种方法在识别图像中的多只动物时也能大有帮助。
- en: '**Data cleaning**'
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**数据清洗**'
- en: 'Investigating our models’ performance, we noticed many examples of mislabels
    (e.g., only a leg of a zebra visible in a single image of a sequence – labelled
    as empty, was classified as a zebra by our model with high certainty). There were
    also many examples of images taken at night in which human labellers spotted no
    animals, while models were very sure about seeing them. After manually tweaking
    the color levels of such images, the darkness indeed revealed the detected animals.
    This fact touches on a very important topic: whether we should build models to
    replicate the labelers’ work (an approach promoted by the rules of the competition)
    or rather should we focus on models, which are as good as possible in detecting
    animals in the captured images.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在调查我们模型的性能时，我们注意到许多标签错误的例子（例如，图像序列中只看到一条斑马的腿——被标记为空白，但我们的模型以很高的确定性将其分类为斑马）。还有许多在夜间拍摄的图像，人类标注者未发现任何动物，而模型却非常确信看到它们。手动调整这些图像的颜色水平后，黑暗确实揭示了被检测到的动物。这一事实涉及一个非常重要的话题：我们是否应该构建模型来复制标注者的工作（这是比赛规则提倡的方法），还是应关注尽可能好地检测图像中的动物。
- en: Why we joined and what it meant for us
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们为何加入以及这对我们的意义
- en: The competition gave us a deep dive into the world of wildlife image classification.
    We are very satisfied with the result and had a lot of fun whilst working on these
    models. We were able to demonstrate and polish our computer vision skills, which
    will be critical to our [AI for Good](https://appsilon.com/ai-for-good/) initiative.
    We are currently in the process of building a pilot version of a wildlife image
    classifier for the National Parks Agency of Gabon and we will apply our new knowledge
    to this end. Stay tuned for more information about our upcoming projects. We are
    determined to show that AI can have a tangible, positive impact on life and the
    world we all share.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 比赛让我们深入了解了野生动物图像分类的世界。我们对结果非常满意，并且在这些模型的工作中获得了很多乐趣。我们能够展示和完善我们的计算机视觉技能，这对于我们的[AI
    for Good](https://appsilon.com/ai-for-good/)计划至关重要。我们目前正在为加蓬国家公园管理局构建野生动物图像分类器的试点版本，并将应用我们的新知识。请关注有关我们即将开展的项目的更多信息。我们决心展示人工智能如何对我们共同的生活和世界产生切实的积极影响。
- en: '**Follow Appsilon Data Science on Social Media**'
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**关注 Appsilon 数据科学的社交媒体**'
- en: Follow[ @Appsilon](https://twitter.com/appsilon) on Twitter
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Twitter 上关注[ @Appsilon](https://twitter.com/appsilon)
- en: Follow Appsilon on[ LinkedIn](https://www.linkedin.com/company/appsilon)
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关注 Appsilon 的[LinkedIn](https://www.linkedin.com/company/appsilon)
- en: Sign up for our company[ newsletter](https://appsilon.com/blog/)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注册我们的公司[通讯](https://appsilon.com/blog/)
- en: Try out our R Shiny[ open source](https://appsilon.com/opensource/) packages
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试我们的 R Shiny[开源](https://appsilon.com/opensource/)包
- en: Sign up for the AI for Good[ newsletter](https://appsilon.com/ai-for-good/)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注册 AI for Good[通讯](https://appsilon.com/ai-for-good/)
- en: We are hiring [software engineers](https://appsilon.com/careers/)
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在招聘[软件工程师](https://appsilon.com/careers/)
- en: '[Original](https://appsilon.com/using-ai-identify-wildlife-camera-trap-images-serengeti/).
    Reposted with permission.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://appsilon.com/using-ai-identify-wildlife-camera-trap-images-serengeti/)。经许可转载。'
- en: '**Related:**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Create Your Own Computer Vision Sandbox](/2020/02/computer-vision-sandbox.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[创建你自己的计算机视觉沙盒](/2020/02/computer-vision-sandbox.html)'
- en: '[A bird’s-eye view of modern AI from NeurIPS 2019](/2020/01/modern-ai-from-neurips-2019.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从NeurIPS 2019中窥探现代AI的全貌](/2020/01/modern-ai-from-neurips-2019.html)'
- en: '[AI and Machine Learning In Our Every Day Life](/2020/02/ai-machine-learning-everyday-life.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我们日常生活中的AI和机器学习](/2020/02/ai-machine-learning-everyday-life.html)'
- en: More On This Topic
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[4 Factors to Identify Machine Learning Solvable Problems](https://www.kdnuggets.com/2022/04/4-factors-identify-machine-learning-solvable-problems.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[识别机器学习可解决问题的4个因素](https://www.kdnuggets.com/2022/04/4-factors-identify-machine-learning-solvable-problems.html)'
- en: '[How to Identify Missing Data in Time-Series Datasets](https://www.kdnuggets.com/how-to-identify-missing-data-in-timeseries-datasets)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何识别时间序列数据集中缺失的数据](https://www.kdnuggets.com/how-to-identify-missing-data-in-timeseries-datasets)'
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用管道编写干净的Python代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目的，找到目的再…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成为出色数据科学家所需的5项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个初学者数据科学家应该掌握的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
