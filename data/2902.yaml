- en: 'Introducing AI Explainability 360: A New Toolkit to Help You Understand what
    Machine Learning Models are Doing'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍AI Explainability 360：帮助您理解机器学习模型正在做什么的新工具包
- en: 原文：[https://www.kdnuggets.com/2019/08/introducing-ai-explainability-360-toolkit-understand-machine-learning-models.html](https://www.kdnuggets.com/2019/08/introducing-ai-explainability-360-toolkit-understand-machine-learning-models.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/08/introducing-ai-explainability-360-toolkit-understand-machine-learning-models.html](https://www.kdnuggets.com/2019/08/introducing-ai-explainability-360-toolkit-understand-machine-learning-models.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)![Figure](../Images/066fab538f9b0815ffb6cac2f09d73ec.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)![图](../Images/066fab538f9b0815ffb6cac2f09d73ec.png)'
- en: Interpretability is one of the most difficult challenges in modern machine learning
    solutions. While building sophisticated machine learning models is getting easier,
    understanding how models develop knowledge and arrive to conclusions remains a
    very difficult challenge. Typically, the more accurate the models the harder they
    are to interpret. Recently, AI researchers from IBM open sourced [AI Explainability
    360](http://aix360.mybluemix.net/), a new toolkit of state-of-the-art algorithms
    that support the interpretability and explainability of machine learning models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性是现代机器学习解决方案中最困难的挑战之一。虽然构建复杂的机器学习模型变得越来越容易，但理解模型如何发展知识并得出结论仍然是一个非常困难的挑战。通常，模型越准确，解释起来就越困难。最近，IBM的AI研究人员开源了[AI
    Explainability 360](http://aix360.mybluemix.net/)，这是一个支持机器学习模型可解释性和说明性的最先进算法工具包。
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的IT'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The release of AI Explainability 360 is the first practical implementation of
    ideas outlined in dozens of research papers in the last few years. In the same
    way traditional software applications incorporate instrumentation code to help
    its runtime monitoring, machine learning models need to add interpretability techniques
    to facilitate debugging, troubleshooting and versioning. However, interpreting
    machine learning models is a multiple of complexity higher than traditional software
    applications. For starters, we need have very little understanding of what makes
    a machine learning model interpretable.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: AI Explainability 360的发布是近年来在几十篇研究论文中概述的想法的首次实际实施。传统的软件应用程序通过引入监控代码来帮助运行时监控，机器学习模型也需要添加可解释性技术以便于调试、故障排除和版本控制。然而，解释机器学习模型的复杂性远高于传统软件应用程序。首先，我们对什么使机器学习模型具有可解释性了解非常有限。
- en: The Building Blocks of Interpretability
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可解释性的构建模块
- en: 'A few months ago, researchers from Google published [a very comprehensive paper
    in which they outlined the core components of interpretability](https://distill.pub/2018/building-blocks/).
    The paper presents four fundamental challenges to make a model interpretable:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 几个月前，谷歌的研究人员发布了[一篇非常全面的论文，概述了可解释性的核心组成部分](https://distill.pub/2018/building-blocks/)。该论文提出了使模型可解释的四个基本挑战：
- en: '**Understanding what Hidden Layers Do:** The bulk of the knowledge in a deep
    learning model is formed in the hidden layers. Understanding the functionality
    of the different hidden layers at a macro level is essential to be able to interpret
    a deep learning model.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解隐层的功能：** 深度学习模型中的大部分知识是在隐层中形成的。了解不同隐层在宏观层面的功能对于解释深度学习模型至关重要。'
- en: '**Understanding what Hidden Layers Do:** The bulk of the knowledge in a deep
    learning model is formed in the hidden layers. Understanding the functionality
    of the different hidden layers at a macro level is essential to be able to interpret
    a deep learning model.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解隐藏层的作用：** 深度学习模型中的大部分知识是在隐藏层中形成的。宏观上理解不同隐藏层的功能对于解释深度学习模型至关重要。'
- en: '**Understanding How Nodes are Activated:** The key to interpretability is not
    to understand the functionality of individual neurons in a network but rather
    groups of interconnected neurons that fire together in the same spatial location.
    Segmenting a network by groups of interconnected neurons will provide a simpler
    level of abstraction to understand its functionality.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解节点如何被激活：** 解释能力的关键在于不是理解网络中单个神经元的功能，而是理解在相同空间位置上共同激活的相互连接的神经元组。通过按互联神经元组对网络进行分段，将提供一个更简单的抽象层次来理解其功能。'
- en: '**Understanding How Concepts are Formed:** Understanding how deep neural network
    forms individual concepts that can then be assembled into the final output is
    another key building block of interpretability.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解概念是如何形成的：** 理解深度神经网络如何形成单个概念，然后将这些概念组装成最终输出，是解释性另一个关键组成部分。'
- en: The other aspect to realize is that the interpretability of AI modes decreases
    as their sophistication increases. Do you care about obtaining the best results
    or do you care about understanding how those results were produced? That’s a essence
    of the interpretability vs. accuracy friction that is at the center of every machine
    learning scenario. Many deep learning techniques are complex in nature and, although
    they result very accurate in many scenarios, they can become incredibly difficult
    to interpret.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要认识到的方面是，AI 模型的可解释性随着其复杂性的增加而减少。你关心的是获得最佳结果，还是关心如何获得这些结果？这就是解释性与准确性之间的矛盾的本质，它是每个机器学习场景的核心。许多深度学习技术本质上是复杂的，尽管它们在许多场景中结果非常准确，但它们可能变得难以解释。
- en: To address the challenges of explainability, machine learning models need to
    start incorporating interpretable building blocks a first class citizen. That’s
    the goal of IBM’s new toolkit.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对解释性的挑战，机器学习模型需要将可解释的构建块作为一个重要组成部分。这就是 IBM 新工具包的目标。
- en: AI Explainability 360
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AI Explainability 360
- en: AI Explainability 360 is an open source toolkit for streamlining the implementation
    of interpretable machine learning models. The toolkit includes a series of interpretability
    algorithms that reflect state-of-the-art research in this topic as well as an
    intuitive user interface that helps understand machine learning models from different
    perspectives. One of the main contributions of the AI Explainability 360 is that
    it doesn’t rely on a single form of interpretation for a machine learning model.
    In the same way that humans rely on rich, expressive explanations to interpret
    a specific outcome, the explainability of machine learning models varies depending
    on personas and context involved. AI Explainability 360, produces different explanations
    for different roles such as data scientists or business stake holders.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: AI Explainability 360 是一个开源工具包，用于简化可解释机器学习模型的实现。该工具包包括一系列可解释性算法，反映了该领域的最新研究成果，并具有直观的用户界面，有助于从不同角度理解机器学习模型。AI
    Explainability 360 的主要贡献之一是它不依赖于机器学习模型的单一解释形式。正如人类依赖丰富且富有表现力的解释来解读特定结果一样，机器学习模型的解释性也会因涉及的角色和上下文而有所不同。AI
    Explainability 360 为数据科学家或业务相关人员等不同角色生成不同的解释。
- en: '![](../Images/9b9aee3e41db3607a28a591d11eb0f1c.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b9aee3e41db3607a28a591d11eb0f1c.png)'
- en: The explanations generated by AI Explainability 360 can be divided in two main
    groups depending on whether they are based on the data or the models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: AI Explainability 360 生成的解释可以分为两大类，取决于它们是否基于数据或模型。
- en: '**Data:** Understanding the characteristics of a dataset is very often the
    shortest path to explainability. This is specially true in supervised learning
    algorithms that rely heavily on datasets in order to build relevant knowledge.
    Sometimes the features in a given dataset are meaningful to consumers, but other
    times they are entangled, i.e. multiple meaningful attributes are combined together
    in a single feature. AI Explainability 360 includes several algorithms that focus
    on data interpretability.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据：** 理解数据集的特征往往是可解释性的最短路径。这在依赖数据集构建相关知识的监督学习算法中尤其如此。有时，给定数据集中的特征对消费者有意义，但其他时候它们是纠缠在一起的，即多个有意义的属性组合在一个特征中。AI可解释性360包含了几个专注于数据可解释性的算法。'
- en: '**Model:** The interpretation of models is the key building block of any machine
    learning explainability. There are several ways to make a machine learning model
    comprehensible to consumers. The first distinction is direct interpretability
    vs. post hoc explanation. Directly interpretable models are model formats such
    as decision trees, Boolean rule sets, and generalized additive models, that are
    fairly easily understood by people and learned straight from the training data.
    Post hoc explanation methods first train a black box model and then build another
    explanation model on top of the black box model. The second distinction is global
    vs. local explanation. Global explanations are for entire models whereas local
    explanations are for single sample points.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型：** 模型的解释是任何机器学习可解释性的关键构建块。有几种方法可以使机器学习模型对消费者易于理解。第一种区分是直接可解释性与事后解释。直接可解释的模型是诸如决策树、布尔规则集和广义加性模型等模型格式，这些模型比较容易被人理解，并且可以直接从训练数据中学习。事后解释方法首先训练一个黑箱模型，然后在黑箱模型上构建另一个解释模型。第二种区分是全局解释与局部解释。全局解释针对整个模型，而局部解释则针对单个样本点。'
- en: '![](../Images/144c477bc71123ff1fa2b7a32d4969ea.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/144c477bc71123ff1fa2b7a32d4969ea.png)'
- en: The distinction between global interpretable models, global and local post hoc
    explanations is one of the key contributions of AI Explainability 360\. Typically,
    global interpretable models are better suited for scenarios that need a complete
    and discrete interpretation paths of the models. Many of those scenarios include
    areas such as safety, reliability or compliance. Global post hoc explanations
    are useful for decision maker personas that are being supported by the machine
    learning model. Physicians, judges, and loan officers develop an overall understanding
    of how the model works, but there is necessarily a gap between the black box model
    and the explanation. Local post hoc explanations are relevant to induvial personas
    such as patients, defendants of applicants that are affected by the outcome of
    a model and need to understand its interpretation from a their very specific viewpoint.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 全局可解释模型、全局和局部事后解释的区分是AI可解释性360的关键贡献之一。通常，全局可解释模型更适合需要完整且离散的模型解释路径的场景。这些场景包括安全性、可靠性或合规性等领域。全局事后解释对由机器学习模型支持的决策者角色非常有用。医生、法官和贷款官员可以对模型的整体工作方式有一个全面的了解，但黑箱模型和解释之间必然存在差距。局部事后解释与受模型结果影响的个人角色相关，例如患者、被告或申请人，他们需要从特定的视角理解解释。
- en: Developers can start using AI Explainability 360 by incorporating the interpretable
    components using the APIs included in the toolkit. Additionally, AI Explainability
    360 includes [a series of demos and tutorials](https://github.com/IBM/AIX360/blob/master/examples/README.md)
    that can help developers get started relatively quickly. Finally, the toolkit
    provides a very simple user interface that can be used to get started with the
    concepts of machine learning interpretability.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员可以通过使用工具包中包含的API来开始使用AI可解释性360。此外，AI可解释性360包含了[一系列演示和教程](https://github.com/IBM/AIX360/blob/master/examples/README.md)，可以帮助开发人员相对快速地入门。最后，该工具包提供了一个非常简单的用户界面，可用于入门机器学习可解释性的概念。
- en: Interpretability is one of the most important building blocks of modern machine
    learning applications. AI Explainability 360 provides one of the most complete
    stacks to simplify the interpretability of machine learning programs without having
    to become an expert in this particular area of research. It is likely that we
    will see some of the ideas behind AI Explainability 360 being incorporated into
    mainstream deep learning frameworks and platforms.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 解释性是现代机器学习应用中最重要的构建块之一。AI 解释性 360 提供了最完整的堆栈之一，以简化机器学习程序的解释性，而无需成为该领域的专家。我们很可能会看到
    AI 解释性 360 背后的某些理念被纳入主流深度学习框架和平台中。
- en: '[Original](https://towardsdatascience.com/introducing-ai-explainability-360-a-new-toolkit-to-help-you-understand-what-machine-learning-93438d734d04).
    Reposted with permission.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/introducing-ai-explainability-360-a-new-toolkit-to-help-you-understand-what-machine-learning-93438d734d04)。已获许可转载。'
- en: '**Related:**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Two Major Difficulties in AI and One Applied Solution](/2019/02/ai-chess-difficulties-solution.html)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能中的两个主要难点及一种应用解决方案](/2019/02/ai-chess-difficulties-solution.html)'
- en: '[6 Key Concepts in Andrew Ng’s “Machine Learning Yearning”](/2019/08/key-concepts-andrew-ng-machine-learning-yearning.html)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[安德鲁·吴《机器学习的渴望》中 6 个关键概念](/2019/08/key-concepts-andrew-ng-machine-learning-yearning.html)'
- en: '[How Creating an AI Study Group Boosted My Skills and Got Me a Job](/2019/08/ai-study-group-boosted-skills-got-job.html)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何通过创建 AI 学习小组提升我的技能并获得工作](/2019/08/ai-study-group-boosted-skills-got-job.html)'
- en: More On This Topic
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[What certification are you adding to your toolkit in 2022?](https://www.kdnuggets.com/2022/03/sas-certification-adding-toolkit-2022.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你在 2022 年计划增加哪些认证？](https://www.kdnuggets.com/2022/03/sas-certification-adding-toolkit-2022.html)'
- en: '[A Comprehensive Survey on Trustworthy Graph Neural Networks:…](https://www.kdnuggets.com/2022/05/comprehensive-survey-trustworthy-graph-neural-networks-privacy-robustness-fairness-explainability.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于可信图神经网络的全面调查：…](https://www.kdnuggets.com/2022/05/comprehensive-survey-trustworthy-graph-neural-networks-privacy-robustness-fairness-explainability.html)'
- en: '[How ML Model Explainability Accelerates the AI Adoption Journey for…](https://www.kdnuggets.com/2022/07/ml-model-explainability-accelerates-ai-adoption-journey-financial-services.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何通过机器学习模型解释性加速人工智能的应用旅程](https://www.kdnuggets.com/2022/07/ml-model-explainability-accelerates-ai-adoption-journey-financial-services.html)'
- en: '[Introducing Healthcare-Specific Large Language Models from John Snow Labs](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍 John Snow Labs 的医疗保健专用大型语言模型](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
- en: '[Introducing TPU v4: Googles Cutting Edge Supercomputer for Large…](https://www.kdnuggets.com/2023/04/introducing-tpu-v4-googles-cutting-edge-supercomputer-large-language-models.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍 TPU v4：谷歌的尖端超级计算机，用于大型…](https://www.kdnuggets.com/2023/04/introducing-tpu-v4-googles-cutting-edge-supercomputer-large-language-models.html)'
- en: '[Introducing MPT-7B: A New Open-Source LLM](https://www.kdnuggets.com/2023/05/introducing-mpt7b-new-opensource-llm.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍 MPT-7B：一个新的开源大语言模型](https://www.kdnuggets.com/2023/05/introducing-mpt7b-new-opensource-llm.html)'
