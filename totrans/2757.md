# 机器学习中的模型评估指标

> 原文：[https://www.kdnuggets.com/2020/05/model-evaluation-metrics-machine-learning.html](https://www.kdnuggets.com/2020/05/model-evaluation-metrics-machine-learning.html)

[评论](#comments)

预测模型已成为许多企业值得信赖的顾问，并且有充分的理由。这些模型可以“预见未来”，并且有许多不同的方法可用，这意味着任何行业都可以找到适合其特定挑战的方法。

当我们谈论预测模型时，我们在谈论**回归模型**（连续输出）或**分类模型**（名义或二元输出）。在分类问题中，我们使用两种类型的算法（依赖于它生成的输出类型）：

1.  **类输出**：像 SVM 和 KNN 这样的算法生成类输出。例如，在二元分类问题中，输出将是 0 或 1。然而，今天我们有算法可以将这些类输出转换为概率。

1.  **概率输出**：像逻辑回归、随机森林、梯度提升、Adaboost 等算法给出概率输出。将概率输出转换为类输出只是创建一个阈值概率的问题。

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升您的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持您的组织的 IT

* * *

### 介绍

虽然数据准备和训练机器学习模型是机器学习流程中的关键步骤，但同样重要的是衡量这个训练好的模型的性能。模型在未见数据上的泛化能力定义了自适应与非自适应机器学习模型。

通过使用不同的性能评估指标，我们应能在将模型投入生产之前提升模型的整体预测能力。

如果不使用不同的指标对机器学习模型进行适当评估，而仅依赖准确率，可能会导致在将模型部署到未见数据上时出现问题，并且可能导致预测效果不佳。

之所以会这样，是因为在这种情况下，我们的模型并没有学习，而是记住了；因此，它们无法对未见数据进行良好的泛化。

### 模型评估指标

现在让我们定义用于评估机器学习模型性能的评估指标，这是任何数据科学项目的一个重要组成部分。其目的是估计模型在未来（未见/样本外）数据上的泛化准确性。

### 混淆矩阵

混淆矩阵是对任何二元测试预测结果的矩阵表示，常用于**描述分类模型**（或“分类器”）在一组已知真实值的测试数据上的性能。

混淆矩阵本身相对简单易懂，但相关术语可能会令人困惑。

![图示](../Images/238722baa8343d76a6f5ae4014af4dbe.png)

带有两个类别标签的混淆矩阵。

每个预测可以是四种结果之一，基于它与实际值的匹配程度：

+   **真阳性（TP）：** 预测为真实，实际上也是真的。

+   **真阴性（TN）：** 预测为假，实际上也是假的。

+   **假阳性（FP）：** 预测为真实，但实际上是假的。

+   **假阴性（FN）：** 预测为假，实际上是真的。

现在让我们通过假设检验来理解这个概念。

一个**假设**是基于不足证据的猜测或理论，可以通过进一步测试和实验来验证。通过进一步测试，假设通常可以被证明为真或假。

一个**零假设**是一个假设，表明假设中的两个变量之间没有统计学上的显著性。它是研究者试图驳斥的假设。

> 当零假设错误时，我们总是会拒绝零假设，而当零假设确实正确时，我们会接受零假设。

尽管假设检验旨在可靠，但**可能会发生两种类型的错误**。

这些错误被称为**一类错误和二类错误**。

例如，在检验药物的有效性时，零假设是药物不影响疾病。

**一类错误：** 等同于假阳性（FP）。

第一种可能的错误涉及拒绝一个真实的零假设。

让我们回到药物用于治疗疾病的例子。如果我们在这种情况下拒绝零假设，那么我们声称药物对疾病有一定的效果。但如果零假设是正确的，那么实际上药物对疾病没有任何作用。药物被错误地宣称对疾病有正面效果。

**二类错误：** 等同于假阴性（FN）。

另一种错误是当我们接受一个虚假的零假设时发生的。这种错误称为二类错误，也被称为第二类错误。

如果我们回顾一下测试药物的场景，那么二类错误会是什么样的呢？如果我们接受药物对疾病没有影响的假设，但实际上药物有影响，这就是二类错误。

混淆矩阵的一个示例Python实现。

![图示](../Images/32ef98a849a9bf9dadcf60b17ff8df47.png)

带有三个类别标签的混淆矩阵。

对角线元素表示预测标签与真实标签相同的点的数量，而对角线外的元素则是分类器标记错误的点。因此，混淆矩阵中对角线的值越高越好，表示正确预测的数量多。

在我们的例子中，分类器在测试数据中完美地预测了所有 13 个山鸢尾和 18 个维吉尼亚鸢尾植物。然而，它错误地将 4 朵变色鸢尾植物分类为维吉尼亚鸢尾。

还有一系列常从混淆矩阵中计算出的二分类器指标：

### 1\. 准确率

总体上，分类器的正确频率是多少？

> 准确率 = (TP+TN)/total

当我们的类别大小大致相等时，我们可以使用准确率**，**这将给出正确分类的值。

准确率是分类问题中常用的评估指标。它是正确预测的数量与所有预测数量的比率。

**错误分类率（误差率）：** 总体上，它错误的频率是多少。由于准确率是我们正确分类的百分比（成功率），因此我们的错误率（错误的百分比）可以按如下方式计算：

> 错误分类率 = (FP+FN)/total

我们使用 sklearn 模块来计算分类任务的准确率，如下所示。

分类准确率在验证集上为**88%**。

### 2\. 精确度

当它预测“是”时，它的准确性有多高？

> 精确度 = TP/预测的“是”

当我们有类别不平衡时，准确率可能会成为测量我们性能的不可靠指标。例如，如果我们在两个类别 A 和 B 之间有 99/1 的划分，其中稀有事件 B 是我们的正类，我们可以建立一个 99% 准确的模型，通过简单地将所有内容归为类别 A。显然，如果模型不能识别类别 B，我们就不应该费心去建立它；因此，我们需要不同的指标来抑制这种行为。为此，我们使用精确度和召回率代替准确率。

### 3\. 召回率或灵敏度

当实际为“是”时，它预测“是”的频率是多少？

> 真正率 = TP/实际的“是”

召回率给出了**真正率** (**TPR**)，即真正例与所有正例的比率。

在类 A 和 B 之间有 99/1 的划分情况下，模型如果将所有内容都归为 A，则对正类 B 的召回率为 0%（精确度将不定义——0/0）。精确度和召回率提供了在类别不平衡情况下评估模型性能的更好方法。它们会正确地告诉我们模型对我们的用例几乎没有价值。

和准确率一样，精确度和召回率都容易计算和理解，但需要阈值。此外，精确度和召回率仅考虑混淆矩阵的一半：

![](../Images/91ec868e492267fb92b1daed648438bf.png)

### 4\. F1 分数

F1分数是[调和平均数](https://en.wikipedia.org/wiki/Harmonic_mean) 和[精确率与召回率](https://en.wikipedia.org/wiki/Precision_and_recall)的调和平均数，其中F1分数在1（完美的精确率和召回率）时达到最佳值，在0时达到最差值。

![](../Images/77f68629d1a37e25bca6efa9b7204007.png)

***为什么用调和平均数？*** 因为列表中调和平均数会强烈偏向列表中最小的元素，它（相比于算术平均数）通常会减轻大离群值的影响，并加重小离群值的影响。

F1分数对极端值的惩罚更多。理想情况下，F1分数可以在以下分类场景中作为有效的评估指标：

+   *当假阳性（FP）和假阴性（FN）的代价相等时——即它们漏掉了真正的正例或找到了虚假的正例——两者对模型的影响几乎相同，就像我们在癌症检测分类的例子中一样*

+   *添加更多的数据不会有效地改变结果*

+   *TN很高（例如在洪水预测、癌症预测等中）*

一个F1分数的Python示例实现。

![](../Images/0386367949b03779167d8b46213b15b8.png)

### 5\. 特异性

当它的答案是否时，它预测“否”的频率如何？

> 真负率=TN/实际负数

这是**真正负率** 或真正负例占所有应被分类为负例的比例。

注意，特异性和灵敏度一起考虑了整个混淆矩阵：

![](../Images/c1fa5a1519c294b00d3fe48bfcbf3574.png)

### 6\. 接收者操作特征（ROC）曲线

测量ROC曲线下的面积也是评估模型非常有用的方法。通过绘制真正阳性率（灵敏度）与假阳性率（1 — 特异性）的关系，我们得到**接收者操作特征** (**ROC**) **曲线**。这个曲线让我们可以可视化真正阳性率与假阳性率之间的权衡。

以下是良好ROC曲线的示例。虚线表示随机猜测（没有预测价值），作为基线；任何低于此的情况都被认为比猜测更差。我们希望靠近左上角：

![](../Images/d04f5b222c9909d1cb36c50e47631161.png)

一个ROC曲线的Python示例实现。

![](../Images/beff968a8224e5e7c16acd4fe6153e37.png)

在上面的例子中，AUC相对接近1且大于0.5。一个完美的分类器的ROC曲线将沿Y轴走，然后沿X轴走。

### 对数损失

对数损失是基于概率的最重要的分类指标。

![图](../Images/4525d39c13e7dc150aed104370ae1e9e.png)

随着**预测概率** 的**真实类别** 越来越**接近零**，**损失呈指数增加**

它衡量分类模型的表现，其中预测输入是介于 0 和 1 之间的概率值。Log Loss 随着预测概率与实际标签的偏离而增加。任何机器学习模型的目标都是最小化这个值。因此，较小的 log loss 更好，完美模型的 log loss 为 0。

Log Loss 的 Python 示例实现。

```py
Logloss: 8.02
```

### Jaccard 指数

Jaccard 指数是计算和找出分类 ML 模型准确性最简单的方法之一。让我们通过一个例子来理解它。假设我们有一个标记的测试集，标签如下 –

```py
y = [0,0,0,0,0,1,1,1,1,1]
```

我们的模型预测的标签如下 –

```py
y1 = [1,1,0,0,0,1,1,1,1,1]
```

![](../Images/667302f21a8d59e942ab7771b3436eb0.png)

上述 Venn 图展示了测试集的标签、预测标签及其交集和并集。

Jaccard 指数或 Jaccard 相似系数是一种用于理解样本集合之间相似性的统计量。该测量强调有限样本集合之间的相似性，正式定义为交集大小与两个标记集合的并集大小之比，公式如下 –

![](../Images/ed169e327c593c70f7820fcb39920d2b.png)

![图](../Images/c2d7428abbdb789e23a8de8841b4db80.png)

[Jaccard 指数或交集与并集（IoU）](https://commons.wikimedia.org/wiki/File:Intersection_over_Union_-_visual_equation.png)

因此，对于我们的例子，我们可以看到两个集合的交集为 8（因为正确预测了八个值），并且并集为**10 + 10–8 = 12**。因此，Jaccard 指数给出的准确性为 –

![](../Images/948566c9337719113a09c9a1547bce4b.png)

因此，按照**Jaccard 指数**，我们的模型准确性为 0.66，即 66%。

Jaccard 指数越高，分类器的准确性越高。

Jaccard 指数的 Python 示例实现。

```py
Jaccard Similarity Score : 0.375
```

### Kolmogorov-Smirnov 图表

K-S 或 Kolmogorov-Smirnov 图表衡量分类模型的性能。更准确地说，K-S 是衡量正例和负例分布之间分离程度的指标。

![图](../Images/4f71119b16403ed7cdcec55d961cede5.png)

[观察到的和假设分布的累积频率与有序频率绘制图。垂直双箭头表示最大垂直差异](https://www.sciencedirect.com/topics/medicine-and-dentistry/kolmogorov-smirnov-test)。

如果分数将总体划分为两个独立的组，其中一个组包含所有正例，另一个组包含所有负例，则 `K-S is 100`。另一方面，如果模型无法区分正例和负例，那么模型就像是从总体中随机选择案例一样。此时，`K-S would be 0`。

在大多数分类模型中，K-S 值会在 0 和 100 之间变化，值越高，模型在分离正例和负例方面越好。

K-S检验也可以用来测试两个一维概率分布是否有差异。这是一种非常有效的方式来判断两个样本是否显著不同。

Kolmogorov-Smirnov的一个Python实现示例。

![](../Images/d6746fe34f97779b5ade9c549dd64480.png)

此处使用的零假设假设数据服从正态分布。它返回统计数据和p值。如果**p值 < alpha**，我们将拒绝零假设。

*Alpha被定义为在零假设（H*0*）为真的情况下拒绝零假设的概率。在大多数实际应用中，alpha选择为0.05。*

### 增益和提升图

增益或提升是分类模型效果的度量，计算为有模型和无模型的结果之比。增益和提升图是评估分类模型性能的视觉工具。然而，与评估全体数据的混淆矩阵不同，增益或提升图评估的是数据的一部分。

提升值越高（即离基线越远），模型越好。

以下增益图表在验证集上运行，显示使用50%的数据时，模型包含了90%的目标，增加更多的数据对模型中包含的目标百分比的增加几乎没有影响。

![图](../Images/dddcc35be3fae9094d555ba1d1e2cf8a.png)

[增益/提升图](https://www.datasciencecentral.com/profiles/blogs/comparing-model-evaluation-techniques-part-2)

提升图通常显示为**累计提升图**，也称为**增益图**。因此，增益图有时（可能令人困惑地）被称为“提升图”，但它们更准确地是*累计提升图*。

其最常见的用途之一是在营销中，以决定一个潜在客户是否值得联系。

### 基尼系数

基尼系数或基尼指数是用于不平衡类值的流行度量。该系数范围从0到1，其中0表示完全平等，1表示完全不平等。在这里，如果指数值更高，则数据会更分散。

基尼系数可以通过以下公式从ROC曲线下面积计算得出：

> 基尼系数 = (2 * ROC_curve) — 1

### 结论

理解机器学习模型在未见数据上表现如何是使用这些评估指标的终极目的。像准确率、精确率、召回率这样的指标适用于平衡数据集的分类模型评估，但如果数据不平衡且存在类别差异，则ROC/AUC、基尼系数等方法在评估模型性能方面表现更佳。

好的，这篇文章到此结束**。**希望你们阅读愉快，欢迎在评论区分享你们的评论/想法/反馈。

**感谢阅读 !!!**

**简历：[纳戈什·辛格·乔汉](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)** 是CirrusLabs的一个大数据开发人员。他在电信、分析、销售、数据科学等多个领域拥有超过4年的工作经验，专注于各种大数据组件。

[原文](https://levelup.gitconnected.com/model-evaluation-metrics-in-machine-learning-8988739236fc)。经许可转载。

**相关：**

+   [主成分分析（PCA）降维](/2020/05/dimensionality-reduction-principal-component-analysis.html)

+   [机器学习模型的超参数优化](/2020/05/hyperparameter-optimization-machine-learning-models.html)

+   [机器学习中的DBSCAN聚类算法](/2020/04/dbscan-clustering-algorithm-machine-learning.html)

### 主题拓展

+   [机器学习评估指标：理论与概述](https://www.kdnuggets.com/machine-learning-evaluation-metrics-theory-and-overview)

+   [停止学习数据科学以寻找目标，并通过寻找目标来……](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)

+   [学习数据科学统计学的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)

+   [一个90亿美元的AI失败案例，分析](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)

+   [成功数据科学家的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)

+   [是什么让Python成为初创企业的理想编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)
