- en: 17 More Must-Know Data Science Interview Questions and Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/02/17-data-science-interview-questions-answers.html](https://www.kdnuggets.com/2017/02/17-data-science-interview-questions-answers.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](/2017/02/17-data-science-interview-questions-answers.html/2#comments)'
  prefs: []
  type: TYPE_IMG
- en: The post [21 Must-Know Data Science Interview Questions and Answers](/2016/02/21-data-science-interview-questions-answers.html)
    was the [most viewed post of 2016](/2016/12/top-2016-kdnuggets-stories.html),
    with over 250,000 page views. For 2017, KDnuggets Editors bring you 17 more new
    and important Data Science Interview Questions and Answers. Because some of the
    answers are quite lengthy, we will publish them in 3 parts over 3 weeks. This
    is part 1, which answers the 6 questions below. Here is [part 2](/2017/02/17-data-science-interview-questions-answers-part-2.html)
    and [part 3](/2017/03/17-data-science-interview-questions-answers-part-3.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'This post answers questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Q1\. What are Data Science lessons from failure to predict 2016 US Presidential
    election (and from Super Bowl LI comeback)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q2\. What problems arise if the distribution of the new (unseen) test data is
    significantly different than the distribution of the training data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q3\. What are bias and variance, and what are their relation to modeling data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q4\. Why might it be preferable to include fewer predictors over many?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q5\. What error metric would you use to evaluate how good a binary classifier
    is? What if the classes are imbalanced? What if there are more than 2 groups?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q6\. What are some ways I can make my model more robust to outliers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Q1\. What are Data Science lessons from failure to predict 2016 US Presidential
    election (and from Super Bowl LI comeback)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**[Gregory Piatetsky](/author/gregory-piatetsky) answers:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![nytimes-upshot-forecast-trump-15](../Images/2a793244923ce511c11097d33450cdfc.png)'
  prefs: []
  type: TYPE_IMG
- en: Just before the Nov 8, 2016 election, most pollsters gave Hillary Clinton an
    edge of ~3% in popular vote and 70-95% chance of victory in electoral college.
    Nate Silver's FiveThirtyEight had the highest chances of Trump Victory at ~30%,
    while New York Times Upshot and Princeton Election Consortium estimated only ~15%,
    and other pollsters like Huffington Post gave Trump only 2% chance of victory.
    Still, Trump won. So what are the lessons for Data Scientists?
  prefs: []
  type: TYPE_NORMAL
- en: To make a statistically valid prediction we need
  prefs: []
  type: TYPE_NORMAL
- en: 1) enough historical data and
  prefs: []
  type: TYPE_NORMAL
- en: 2) assumption that past events are sufficiently similar to current event we
    are trying to predict.
  prefs: []
  type: TYPE_NORMAL
- en: Events can placed on the scale from deterministic (2+2 will always equal to
    4) to strongly predictable (e.g. orbits of planets and moons, avg. number of heads
    when tossing a fair coin) to weakly predictable (e.g. elections and sporting events)
    to random (e.g. honest lottery).
  prefs: []
  type: TYPE_NORMAL
- en: If we toss a fair coin 100 million times, we have the expected number of heads
    (mean) as 50 million, the standard deviation =10,000 (using formula 0.5 * SQRT(N)),
    and we can predict that 99.7% of the time the expected number of heads will be
    within [3 standard deviations](https://en.wikipedia.org/wiki/68–95–99.7_rule)
    of the mean.
  prefs: []
  type: TYPE_NORMAL
- en: But using polling to predict the votes of 100 million people is much more difficult.
    Pollsters need to get a representative sample, estimate the likelihood of a person
    actually voting, make many justified and unjustified assumptions, and avoid following
    their conscious and unconscious biases.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of US Presidential election, correct prediction is even more difficult
    because of the antiquated Electoral college system when each state (except for
    Maine and Nebraska) awards the winner all its votes in the electoral college,
    and the need to poll and predict results for each state separately.
  prefs: []
  type: TYPE_NORMAL
- en: The chart below shows that in 2016 US presidential elections pollsters were
    off the mark in many states. They mostly underestimated the Trump vote, especially
    in 3 critical states of Michigan, Wisconsin, and Pennsylvania which all flipped
    to Trump.
  prefs: []
  type: TYPE_NORMAL
- en: '![Us Elections 2016 Poll Shift, according to 538](../Images/9374b47f7145fa507df38dba4004931f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [**@NateSilver538**](https://twitter.com/NateSilver538) tweet, Nov
    9, 2016.'
  prefs: []
  type: TYPE_NORMAL
- en: A few statisticians like Salil Mehta [@salilstatistics](https://twitter.com/salilstatistics)
    were warning about [unreliability of polls](https://twitter.com/salilstatistics/status/796248050851139584),
    and David Wasserman of 538 actually described this scenario in Sep 2016 [How Trump
    Could Win The White House While Losing The Popular Vote](https://fivethirtyeight.com/features/how-trump-could-win-the-white-house-while-losing-the-popular-vote/?ex_cid=story-twitter),
    but most pollsters were way off.
  prefs: []
  type: TYPE_NORMAL
- en: So a good lesson for Data Scientists is to **question their assumptions** and
    to be very skeptical when predicting a weakly predictable event, especially when
    based on human behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Other important lessons are
  prefs: []
  type: TYPE_NORMAL
- en: Examine data quality - in this election polls were not reaching all likely voters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beware of your own biases: many pollsters were likely Clinton supporters and
    did not want to question the results that favored their candidate. For example,
    Huffington Post had forecast over 95% chance of Clinton Victory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See also other analyses of 2016 polling failures:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Wired: [Trump’s Win Isn’t the Death of Data—It Was Flawed All Along](https://www.wired.com/2016/11/trumps-win-isnt-death-data-flawed-along).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NYTimes [How Data Failed Us in Calling an Election](http://www.nytimes.com/2016/11/10/technology/the-data-said-clinton-would-win-why-you-shouldnt-have-believed-it.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datanami [Six Data Science Lessons from the Epic Polling Failure](https://www.datanami.com/2016/11/11/data-science-lessons-epic-polling-failure/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'InformationWeek [Trump''s Election: Poll Failures Hold Data Lessons For IT](http://www.informationweek.com/big-data/big-data-analytics/trumps-election-poll-failures-hold-data-lessons-for-it/d/d-id/1327455)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Why I Had to Eat a Bug on CNN](http://www.nytimes.com/2016/11/19/opinion/why-i-had-to-eat-a-bug-on-cnn.html?emc=eta1&_r=0),
    by Sam Wang, Princeton, whose Princeton Election Consortium gave Trump 15% to
    win.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(*Note: this answer is based on a previous KDnuggets post: [/2016/11/trump-shows-limits-prediction.html](/2016/11/trump-shows-limits-prediction.html))*'
  prefs: []
  type: TYPE_NORMAL
- en: We had another example of statistically very unlikely event happen in Super
    Bowl LI on Feb 5, 2017.  After the half time, Atlanta Falcons were leading 21:3
    after halftime and 28:9 after 3rd quarter. ESPN estimated Falcons win probability
    at that time at almost 100%.
  prefs: []
  type: TYPE_NORMAL
- en: '![Super Bowl 2017 win probability](../Images/d7a31a019989e946ed5aa26be3d6403d.png)'
  prefs: []
  type: TYPE_IMG
- en: '(reference: Salil Mehta tweet [Salil Mehta tweet, Feb 6, 2017](https://twitter.com/salilstatistics/status/828581183558713344))'
  prefs: []
  type: TYPE_NORMAL
- en: Never before has a team lost a Super Bowl after holding such advantage.  However,
    each Super Bowl is different, and this one was turned out to be very different. 
    Combination of superior skill (Patriots, after all, were favorites before the
    game) and luck (e.g. a very lucky catch by Julian Edelman in 4th quarter, Patriots
    winning coin toss in overtime) gave victory to Pats.
  prefs: []
  type: TYPE_NORMAL
- en: This Super Bowl was another good lesson for Data Scientists of danger of having
    **too much confidence** when predicting weakly predictable events. You need to
    understand the risk factors when dealing with such events, and try to avoid using
    probabilities, or if you have to use numbers, have a wide confidence range.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if the odds seem to be against you but the event is only weakly predictable,
    go ahead and do your best - sometimes you will be able to beat the odds.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Q2\. What problems arise if the distribution of the new (unseen) test data is
    significantly different than the distribution of the training data?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**[Gregory Piatetsky](/author/gregory-piatetsky) and [Thuy Pham](/author/thuy-pham)
    answer:**'
  prefs: []
  type: TYPE_NORMAL
- en: The main problem is that the predictions will be wrong !
  prefs: []
  type: TYPE_NORMAL
- en: If the new test data is sufficiently different in key parameters of the prediction
    model from the training data, then predictive model is no longer valid.
  prefs: []
  type: TYPE_NORMAL
- en: The main reasons this can happen are sample selection bias, population drift,
    or non-stationary environment.
  prefs: []
  type: TYPE_NORMAL
- en: '**a) Sample selection bias**'
  prefs: []
  type: TYPE_NORMAL
- en: Here the data is static, but the training examples have been obtained through
    a biased method, such as non-uniform selection or non-random split of data into
    train and test.
  prefs: []
  type: TYPE_NORMAL
- en: If you have a large static dataset, then you should randomly split it into train/test
    data, and the distribution of test data should be similar to training data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Covariate shift](../Images/44bd3aaf781eae2e665248d2bcd34bc8.png)'
  prefs: []
  type: TYPE_IMG
- en: '**b) Covariate shift aka population drift**'
  prefs: []
  type: TYPE_NORMAL
- en: Here the data is not static, with one population used as a training data, and
    another population used for testing.
  prefs: []
  type: TYPE_NORMAL
- en: '*(Figure from http://iwann.ugr.es/2011/pdf/InvitedTalk-FHerrera-IWANN11.pdf).*'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes the training data and test data are derived via different processes
    - eg a drug tested on one population is given to a new population that may have
    significant differences. As a result, a classifier based on training data will
    perform poorly.
  prefs: []
  type: TYPE_NORMAL
- en: One proposed solution is to apply a statistical test to decide if the probabilities
    of target classes and key variables used by the classifier are significantly different,
    and if they are, to retrain the model using new data.
  prefs: []
  type: TYPE_NORMAL
- en: '**c) Non-stationary environments**'
  prefs: []
  type: TYPE_NORMAL
- en: Training environment is different from the test one, whether it's due to a temporal
    or a spatial change.
  prefs: []
  type: TYPE_NORMAL
- en: This is similar to case b, but applies to situation when data is not static
    -  we have a stream of data and we periodically sample it to develop predictive
    models of future behavior.  This happens in adversarial classification problems,
    such as spam filtering and network intrusion detection, where spammers and hackers
    constantly change their behavior in response. Another typical case is customer
    analytics where customer behavior changes over time.  A telephone company develops
    a model for predicting customer churn or a credit card company develops a model
    to predict transaction fraud.  Training data is historical data, while (new) test
    data is the current data.
  prefs: []
  type: TYPE_NORMAL
- en: Such models periodically need to be retrained and to determine when you can
    compare the distribution of key variables in the predictive model in the old data
    (training set) and the new data, and if there is a sufficiently significant difference,
    the model needs to be retrained.
  prefs: []
  type: TYPE_NORMAL
- en: For a more detailed and technical discussion, see references below.
  prefs: []
  type: TYPE_NORMAL
- en: '**References:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Marco Saerens, Patrice Latinne, Christine Decaestecker: Adjusting the Outputs
    of a Classifier to New a Priori Probabilities: A Simple Procedure. Neural Computation
    14(1): 21-41 (2002)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Machine Learning in Non-stationary Environments: Introduction to Covariate
    Shift Adaptation, Masashi Sugiyama, Motoaki Kawanabe, MIT Press, 2012, ISBN 0262017091,
    9780262017091'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Quora answer to [What could be some issues if the distribution of the test
    data is significantly different than the distribution of the training data?](https://www.quora.com/What-could-be-some-issues-if-the-distribution-of-the-test-data-is-significantly-different-than-the-distribution-of-the-training-data)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [Dataset Shift in Classification: Approaches and Problems](http://iwann.ugr.es/2011/pdf/InvitedTalk-FHerrera-IWANN11.pdf),
    Francisco Herrera invited talk, 2011.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [When Training and Test Sets are Different: Characterising Learning Transfer](http://homepages.inf.ed.ac.uk/amos/publications/Storkey2009TrainingTestDifferent.pdf),
    Amos Storkey, 2013.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Q3\. What are bias and variance, and what are their relation to modeling data?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**[Matthew Mayo](/author/matt-mayo) answers:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bias** is how far removed a model''s predictions are from correctness, while
    **variance** is the degree to which these predictions vary between model iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bias vs Variance](../Images/b96b54746b97c6cc4d80a70ba9f99dab.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Bias vs Variance**, [Image source](http://scott.fortmann-roe.com/docs/BiasVariance.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[As an example](/2016/08/bias-variance-tradeoff-overview.html), using a simple
    flawed Presidential election survey as an example, errors in the survey are then
    explained through the twin lenses of bias and variance: selecting survey participants
    from a phonebook is a source of bias; a small sample size is a source of variance.'
  prefs: []
  type: TYPE_NORMAL
- en: Minimizing total model error relies on the balancing of bias and variance errors.
    Ideally, models are the result of a collection of **unbiased data of low variance**.
    Unfortunately, however, the more complex a model becomes, its tendency is toward
    less bias but greater variance; therefore an optimal model would need to consider
    a balance between these 2 properties.
  prefs: []
  type: TYPE_NORMAL
- en: The statistical evaluation method of cross-validation is useful in both demonstrating
    the **importance** of this balance, as well as actually **searching** it out.
    The number of data folds to use -- the value of *k* in *k*-fold cross-validation
    -- is an important decision; the lower the value, the higher the bias in the error
    estimates and the less variance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Bias variance total error](../Images/110b09d1df30869536aad9bf0fdda2a9.png)**Bias
    and variance contributing to total error**, [Image source](http://scott.fortmann-roe.com/docs/BiasVariance.html)Conversely,
    when *k* is set equal to the number of instances, the error estimate is then very
    low in bias but has the possibility of high variance.'
  prefs: []
  type: TYPE_NORMAL
- en: The most important takeaways are that bias and variance are two sides of an
    important trade-off when building models, and that even the most routine of statistical
    evaluation methods are directly reliant upon such a trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: On next page, we answer
  prefs: []
  type: TYPE_NORMAL
- en: Why might it be preferable to include fewer predictors over many?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What error metric would you use to evaluate how good a binary classifier is?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are some ways I can make my model more robust to outliers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Data Analytics Interview Questions & Answers](https://www.kdnuggets.com/2022/09/7-data-analytics-interview-questions-answers.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Python Interview Questions & Answers](https://www.kdnuggets.com/2022/09/5-python-interview-questions-answers.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20 Questions (with Answers) to Detect Fake Data Scientists: ChatGPT…](https://www.kdnuggets.com/2023/01/20-questions-detect-fake-data-scientists-chatgpt-1.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20 Questions (with Answers) to Detect Fake Data Scientists: ChatGPT…](https://www.kdnuggets.com/2023/02/20-questions-detect-fake-data-scientists-chatgpt-2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
