- en: Building Microservice for Multi-Chat Backends Using Llama and ChatGPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/building-microservice-for-multichat-backends-using-llama-and-chatgpt](https://www.kdnuggets.com/building-microservice-for-multichat-backends-using-llama-and-chatgpt)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Building Microservice for Multi-Chat Backends Using Llama and ChatGPT](../Images/96539f9eef41d5c2b771fbf02a8cb2c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Microservices architecture promotes the creation of flexible, independent services
    with well-defined boundaries. This scalable approach enables developers to maintain
    and evolve services individually without affecting the entire application. However,
    realizing the full potential of microservices architecture, particularly for AI-powered
    chat applications, requires robust integration with the latest Large Language
    Models (LLMs) like Meta Llama V2 and OpenAI’s ChatGPT and other fine-tuned released
    based on each application use case to provide a multi-model approach for a diversified
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are large-scale models that generate human-like text based on their training
    on diverse data. By learning from billions of words on the internet, LLMs understand
    the context and generate tuned content in various domains. However, the integration
    of various LLMs into a single application often poses challenges due to the requirement
    of unique interfaces, access endpoints, and specific payloads for each model.
    So, having a single integration service that can handle a variety of models improves
    the architecture design and empowers the scale of independent services.
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial will introduce you to IntelliNode integrations for ChatGPT and
    LLaMA V2 in a microservice architecture using Node.js and Express.
  prefs: []
  type: TYPE_NORMAL
- en: Chatbot Integration Options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are a few chat integration options provided by IntelliNode:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LLaMA V2**: You can integrate the LLaMA V2 model either via Replicate’s API
    for a straightforward process or via your AWS SageMaker host for an additional
    control.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LLaMA V2 is a powerful open source Large Language Model (LLM) that has been
    pre-trained and fine-tuned with up to 70B parameters. It excels in complex reasoning
    tasks across various domains, including specialized fields like programming and
    creative writing. Its training methodology involves self-supervised data and alignment
    with human preferences through Reinforcement Learning with Human Feedback (RLHF).
    LLaMA V2 surpasses existing open-source models and is comparable to closed-source
    models like ChatGPT and BARD in usability and safety.
  prefs: []
  type: TYPE_NORMAL
- en: '**ChatGPT**: By simply providing your OpenAI API key, IntelliNode module allows
    integration with the model in a simple chat interface. You can access ChatGPT
    through GPT 3.5 or GPT 4 models. These models have been trained on vast amounts
    of data and fine-tuned to provide highly contextual and accurate responses.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Step-by-Step Integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s start by initializing a new Node.js project. Open up your terminal, navigate
    to your project’s directory, and run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This command will create a new `package.json` file for your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, install Express.js, which will be used to handle HTTP requests and responses
    and intellinode for LLM models connection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Once the installation concludes, create a new file named `*app.js*` in your
    project’s root directory. then, add the express initializing code in `app.js`.
  prefs: []
  type: TYPE_NORMAL
- en: Code by Author
  prefs: []
  type: TYPE_NORMAL
- en: Llama V2 Integration Using Replicate’s API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Replicate provides a fast integration path with Llama V2 through API key, and
    IntelliNode provides the chatbot interface to decouple your business logic from
    the Replicate backend allowing you to switch between different chat models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by integrating with Llama hosted in Replica’s backend:'
  prefs: []
  type: TYPE_NORMAL
- en: Code by Author
  prefs: []
  type: TYPE_NORMAL
- en: Get your trial key from [replicate.com](https://replicate.com/) to activate
    the integration.
  prefs: []
  type: TYPE_NORMAL
- en: Llama V2 Integration Using AWS SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let’s cover Llama V2 integration via AWS SageMaker, providing privacy and
    extra layer of control.
  prefs: []
  type: TYPE_NORMAL
- en: 'The integration requires to generate an API endpoint from your AWS account,
    first we will setup the integration code in our micro service app:'
  prefs: []
  type: TYPE_NORMAL
- en: Code by Author
  prefs: []
  type: TYPE_NORMAL
- en: The following steps are to create a Llama endpoint in your account, once you
    set up the API gateway copy the URL to use for running the ‘*/llama/aws*’ service.
  prefs: []
  type: TYPE_NORMAL
- en: '**To setup a Llama V2 endpoint in your AWS account:**'
  prefs: []
  type: TYPE_NORMAL
- en: 1- **SageMaker Service:** select the SageMaker service from your AWS account
    and click on domains.
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Microservice for Multi-Chat Backends Using Llama and ChatGPT](../Images/fa908c1810c1c06b2a8da3ed6615f130.png)'
  prefs: []
  type: TYPE_IMG
- en: aws account-select sagemaker
  prefs: []
  type: TYPE_NORMAL
- en: '**2- Create a SageMaker Domain**: Begin by creating a new domain on your AWS
    SageMaker. This step establishes a controlled space for your SageMaker operations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Microservice for Multi-Chat Backends Using Llama and ChatGPT](../Images/d3e4eb2c2fe35c765526b482edbe6478.png)'
  prefs: []
  type: TYPE_IMG
- en: aws account-sagemaker domain
  prefs: []
  type: TYPE_NORMAL
- en: '**3- Deploy the Llama Model**: Utilize SageMaker JumpStart to deploy the Llama
    model you plan to integrate. It is recommended to start with the 2B model due
    to the higher monthly cost for running the 70B model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Microservice for Multi-Chat Backends Using Llama and ChatGPT](../Images/3e897946137642e54a6d7f3d94056685.png)'
  prefs: []
  type: TYPE_IMG
- en: aws account-sagemaker jump start
  prefs: []
  type: TYPE_NORMAL
- en: '**4- Copy the Endpoint Name**: Once you have a model deployed, make sure to
    note the endpoint name, which is crucial for future steps.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Microservice for Multi-Chat Backends Using Llama and ChatGPT](../Images/ad609dc12332f5d4c631826d65e4fcfe.png)'
  prefs: []
  type: TYPE_IMG
- en: aws account-sagemaker endpoint
  prefs: []
  type: TYPE_NORMAL
- en: '**5- Create Lambda Function**: AWS Lambda allows running the back-end code
    without managing servers. Create a Node.js lambda function to use for integrating
    the deployed model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**6- Set Up Environment Variable**: Create an environment variable inside your
    lambda named llama_endpoint with the value of the SageMaker endpoint.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Microservice for Multi-Chat Backends Using Llama and ChatGPT](../Images/37275ac694fe83dc35855734bbe1b8d5.png)'
  prefs: []
  type: TYPE_IMG
- en: aws account-lmabda settings
  prefs: []
  type: TYPE_NORMAL
- en: '**7- Intellinode Lambda Import**: You need to import the prepared Lambda zip
    file that establishes a connection to your SageMaker Llama deployment. This export
    is a zip file, and it can be found in the [lambda_llama_sagemaker](https://github.com/Barqawiz/IntelliNode/tree/main/samples/lambda_llama_sagemaker)
    directory.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Microservice for Multi-Chat Backends Using Llama and ChatGPT](../Images/1c7df03875aad3f7548ed5ebabc00313.png)'
  prefs: []
  type: TYPE_IMG
- en: aws account-lambda upload from zip file
  prefs: []
  type: TYPE_NORMAL
- en: '**8- API Gateway Configuration**: Click on the “Add trigger” option on the
    Lambda function page, and select “API Gateway” from the list of available triggers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Microservice for Multi-Chat Backends Using Llama and ChatGPT](../Images/427a4ab9401c1456a2da220ed802a700.png)'
  prefs: []
  type: TYPE_IMG
- en: aws account-lambda trigger![Building Microservice for Multi-Chat Backends Using
    Llama and ChatGPT](../Images/5fd367351cb4bf63c16a130be4171536.png)
  prefs: []
  type: TYPE_NORMAL
- en: aws account-api gateway trigger
  prefs: []
  type: TYPE_NORMAL
- en: '**9- Lambda Function Settings**: Update the lambda role to grant necessary
    permissions to access SageMaker endpoints. Additionally, the function’s timeout
    period should be extended to accommodate the processing time. Make these adjustments
    in the “Configuration” tab of your Lambda function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the role name to update the permissions and povides the permission
    to access sagemaker:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building Microservice for Multi-Chat Backends Using Llama and ChatGPT](../Images/b6a23770eebd8848a4ea0fd4ec22e592.png)'
  prefs: []
  type: TYPE_IMG
- en: aws account-lambda role
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT Integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, we’ll illustrate the steps to integrate Openai ChatGPT as another
    option in the micro service architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Code by Author
  prefs: []
  type: TYPE_NORMAL
- en: Get your trial key from [platform.openai.com](https://platform.openai.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Execution Experiment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First export the API key in your terminal as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: Code by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Then run the node app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Type the following url in the browser to test chatGPT service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We built a microservice empowered by the capabilities of Large Language Models
    such as Llama V2 and OpenAI’s ChatGPT. This integration opens the door for leveraging
    endless business scenarios powered by advanced AI.
  prefs: []
  type: TYPE_NORMAL
- en: By translating your machine learning requirements into decoupled microservices,
    your application can gain the benefits of flexibility, and scalability. Instead
    of configuring your operations to suit the constraints of a monolithic model,
    the language models function can now be individually managed and developed; this
    promises better efficiency and easier troubleshooting and upgrade management.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ChatGPT API: [link](https://platform.openai.com/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Replica API: [link](https://docs.replicastudios.com/#replica-api).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SageMaker Llama Jump Start: [link](https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'IntelliNode Get Started: [link](https://www.intellinode.ai/getting-started)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Full code GitHub repo: [link](https://github.com/intelligentnode/IntelliServer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[Ahmad Albarqawi](https://www.linkedin.com/in/barqawi/)** is a Engineer and
    data science master at Illinois Urbana-Champaign.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Llama, Llama, Llama: 3 Simple Steps to Local RAG with Your Content](https://www.kdnuggets.com/3-simple-steps-to-local-rag-with-your-content)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using Groq Llama 3 70B Locally: Step by Step Guide](https://www.kdnuggets.com/using-groq-llama-3-70b-locally-step-by-step-guide)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10 Hurdles of Building a Deep Tech Startup in the Age of ChatGPT](https://www.kdnuggets.com/2023/04/10-hurdles-building-deep-tech-startup-age-chatgpt.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Visual ChatGPT: Microsoft Combine ChatGPT and VFMs](https://www.kdnuggets.com/2023/03/visual-chatgpt-microsoft-combine-chatgpt-vfms.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Generative AI Playground: LLMs with Camel-5b and Open LLaMA 3B on…](https://www.kdnuggets.com/2024/02/intel-generative-ai-playground-llms-with-camel-5b-and-open-llama-3b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ChatGPT CLI: Transform Your Command-Line Interface Into ChatGPT](https://www.kdnuggets.com/2023/07/chatgpt-cli-transform-commandline-interface-chatgpt.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
