- en: 'Key Machine Learning Technique: Nested Cross-Validation, Why and How, with
    Python code'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键机器学习技术：嵌套交叉验证，为什么以及如何使用 Python 代码
- en: 原文：[https://www.kdnuggets.com/2020/10/nested-cross-validation-python.html](https://www.kdnuggets.com/2020/10/nested-cross-validation-python.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/10/nested-cross-validation-python.html](https://www.kdnuggets.com/2020/10/nested-cross-validation-python.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Omar Martinez](https://www.linkedin.com/in/omarmartinez182/), Arcalea**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [奥马尔·马丁内斯](https://www.linkedin.com/in/omarmartinez182/)，Arcalea**。'
- en: In this article, we’ll briefly discuss and implement a technique that, in the
    grand scheme of things, might be getting less attention than it deserves. The
    previous statement comes from the observation that it’s a well-known issue that
    some models have a tendency to underperform in production compared to the performance
    in the model building stage. While there is an abundance of potential culprits
    for this issue, a common cause could lie in the model selection process.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将简要讨论并实施一种技术，从宏观角度来看，这种技术可能没有得到应有的关注。前述陈述源于观察到某些模型在生产中的表现往往低于模型构建阶段的表现。尽管这种问题有许多潜在的罪魁祸首，但一个常见的原因可能在于模型选择过程。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大推荐课程
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT。'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'A standard model selection process will usually include a hyperparameter optimization
    phase, in which, through the use of a validation technique, such as k-fold cross-validation
    (CV), an “optimal” model will be selected based on the results of a validation
    test. However, this process is vulnerable to a form of selection bias, which makes
    it unreliable in many applications. This is discussed in detail on a paper from
    Gavin Cawley & Nicola Talbot, in which we can find the following nugget of information:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 标准模型选择过程通常包括一个超参数优化阶段，在此阶段，通过使用验证技术，如 k 折交叉验证（CV），将根据验证测试结果选择一个“最佳”模型。然而，这一过程容易受到一种选择偏差的影响，使其在许多应用中不可靠。Gavin
    Cawley 和 Nicola Talbot 的论文中详细讨论了这一点，其中包含以下重要信息：
- en: '*“In a biased evaluation protocol, occasionally observed in machine learning
    studies, an initial model selection step is performed using all of the available
    data, often interactively as part of a “preliminary study.” The data are then
    repeatedly re-partitioned to form one or more pairs of random, disjoint design
    and test sets. These are then used for performance evaluation using the same fixed
    set of hyper-parameter values. This practice may seem at first glance to be fairly
    innocuous, however the test data are no longer statistically pure, as they have
    been “seen” by the models in tuning the hyperparameters.”*'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“在机器学习研究中，偶尔会观察到一种偏差的评估协议，即使用所有可用数据进行初步模型选择步骤，这通常作为“初步研究”的一部分进行互动操作。然后，这些数据被反复重新划分，形成一个或多个随机、互不重叠的设计和测试集。这些集随后用于使用相同的固定超参数值进行性能评估。这个做法乍看似乎无害，但测试数据不再是统计上的纯净数据，因为它们已经被模型在调整超参数时“看到过”。”*'
- en: ''
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- [On Over-fitting in Model Selection and Subsequent Selection Bias in Performance
    Evaluation](https://jmlr.csail.mit.edu/papers/volume11/cawley10a/cawley10a.pdf),
    2010.'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- [关于模型选择中的过拟合及其在性能评估中的后续选择偏差](https://jmlr.csail.mit.edu/papers/volume11/cawley10a/cawley10a.pdf)，2010年。'
- en: To illustrate why this is happening, let’s use an example. Suppose that we are
    working on a machine learning task, in which we are selecting a model based on
    *n* rounds of hyperparameter optimization, and we do this by using a grid search
    and cross-validation. Now, if we are using the same train and test data at each
    one of the *nth* iterations, this means that information of performance on the
    test set is being incorporated into the training data via the choice of hyperparameters.
    At each iteration, this information can be capitalized by the process to find
    the best performing hyperparameters, and that leads to a test dataset that is
    no longer pure for performance evaluation. If *n* is large at some point, then
    we might consider the test set as a secondary training set (loosely speaking).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明为什么会发生这种情况，让我们用一个例子。假设我们正在进行一个机器学习任务，在其中我们基于*n*轮超参数优化选择一个模型，我们通过使用网格搜索和交叉验证来完成这项工作。现在，如果我们在每次*n*次迭代中使用相同的训练和测试数据，这意味着测试集上的性能信息通过超参数的选择被纳入训练数据。在每次迭代中，这些信息可以被过程利用以找到性能最佳的超参数，这会导致测试数据集不再纯净，无法用于性能评估。如果*n*很大，那么我们可能会将测试集视为一个次级训练集（宽泛地说）。
- en: On the bright side, there are some techniques that can help us tackle this problem.
    One consists of having a train set, a test set, and also a validation set, and
    then tuning hyperparameters based on performance on the validation set.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，有一些技巧可以帮助我们解决这个问题。其中一种方法是拥有一个训练集、一个测试集，以及一个验证集，然后根据验证集上的表现调整超参数。
- en: The other strategy, and the focus of this article, is **nested cross-validation**,
    which coincides with one of the proposed solutions from the paper above, by treating
    the hyperparameter optimization as a part of the model fitting itself and evaluating
    it with a different validation set that is part of an outer layer of cross-validation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种策略，也是本文的重点，是**嵌套交叉验证**，它与上文所提论文中的一种解决方案相吻合，通过将超参数优化视为模型拟合的一部分，并用不同的验证集来评估它，这个验证集是外层交叉验证的一部分。
- en: Put simply, fitting, including the fitting of the hyperparameters, that in itself
    includes an inner CV process, is just like any other part of the model process
    and not a tool to evaluate model performance for that particular fitting approach.
    To evaluate performance, you use the outer cross-validation process. In practice,
    you do this by letting grid-search (or any other object you use for optimization)
    handle the inner cross-validation and then use cross_val_score to estimate generalization
    error in the outer loop. Thus, the final score will be obtained by averaging test
    set scores over several splits, as a regular CV process.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，拟合，包括超参数的拟合，这本身包括一个内部交叉验证过程，就像模型过程中的其他部分一样，并不是用于评估特定拟合方法的模型性能的工具。要评估性能，你需要使用外部交叉验证过程。实际上，你可以让网格搜索（或任何其他优化工具）处理内部交叉验证，然后使用cross_val_score来估计外部循环中的泛化误差。因此，最终分数将通过对多个拆分的测试集分数进行平均，作为常规的交叉验证过程。
- en: Here’s a simplified overview of the approach. This illustration is by no means
    technically rigorous but is to provide some intuition behind the whole process.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这是该方法的简化概述。这个示例在技术上并不严格，但旨在提供整个过程的直观理解。
- en: '![](../Images/de4aa608e4442ce1f06da7ffe8109d41.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de4aa608e4442ce1f06da7ffe8109d41.png)'
- en: '*Simplified Illustration of the Nested Cross-Validation Process.*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*嵌套交叉验证过程的简化示意图。*'
- en: Nested Cross-validation in Python
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python 中的嵌套交叉验证
- en: Implementing nested CV in python, thanks to [scikit-learn](https://scikit-learn.org/),
    is relatively straightforward.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在 python 中实现嵌套交叉验证，多亏了[scikit-learn](https://scikit-learn.org/)，相对来说比较简单。
- en: Let’s look at an example. We’ll start by loading the [wine dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine)
    from sklearn.datasets and all of the necessary modules.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。我们将从加载[葡萄酒数据集](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine)开始，使用来自sklearn.datasets的所有必要模块。
- en: Now, we proceed to instantiate the classifier and then specify the number of
    rounds/trials we would like to run, in other words, how many times we’ll perform
    the whole process of going through both the inner and outer loops. Because this
    is computationally expensive and we are doing this only for demonstration purposes,
    let's choose 20\. Remember, the number of rounds represents how many times you’ll
    split your dataset differently on each of the CV processes.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们开始实例化分类器，然后指定我们希望运行的轮数/试验次数，换句话说，就是我们将进行多少次完整的内外循环过程。由于这是计算上昂贵的，我们仅仅为了演示目的，所以选择20轮。请记住，轮数表示你将如何在每个交叉验证过程中以不同的方式拆分数据集。
- en: The next step involves creating a dictionary that will establish the hyperparameter
    space that we are going to explore in each round, and we also create two empty
    arrays to store the results from the nested and the non-nested processes.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建一个字典，以建立我们在每轮中将要探索的超参数空间，同时创建两个空数组以存储来自嵌套和非嵌套过程的结果。
- en: The next step is crucial, we are going to create a for loop that will iterate
    for the number of rounds we've specified, and that will contain two different
    cross-validation objects.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步非常重要，我们将创建一个循环，该循环将迭代我们指定的轮数，并且包含两个不同的交叉验证对象。
- en: For this example, we'll use 5-fold cross-validation for both the outer and inner
    loops, and we use the value of each round (i) as the random_state for both CV
    objects.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将对外部和内部循环都使用5折交叉验证，并且使用每轮（i）的值作为两个交叉验证对象的`random_state`。
- en: Then, we proceed to create and configure the object to perform the hyperparameter
    optimization. In this case, we'll use grid-search. Notice that we pass to grid-search
    the 'inner_cv' object as the cross-validation method.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们开始创建和配置对象以进行超参数优化。在这个例子中，我们将使用网格搜索。请注意，我们将`'inner_cv'`对象作为交叉验证方法传递给网格搜索。
- en: Subsequently, notice that for the final scores of the "nested" cross-validation
    process, we use the cross_val_score function and feed it the classifier object
    'clf' (includes its own CV process), which is the object we used to perform the
    hyperparameter optimization, and also the 'outer_cv' cross-validation object.
    In this process, we also fit the data and then store the results of each process
    inside the empty arrays we created before.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，请注意，对于“嵌套”交叉验证过程的最终得分，我们使用`cross_val_score`函数，并将分类器对象`'clf'`（包括其自己的交叉验证过程）传递给它，这个对象就是我们用于进行超参数优化的对象，同时也传递`'outer_cv'`交叉验证对象。在此过程中，我们还会拟合数据，然后将每个过程的结果存储在我们之前创建的空数组中。
- en: We can now calculate the difference in the accuracy scores of both the "simple"
    cross-validation and the nested cross-validation processes to see how much they
    disagree with each other on average.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以计算“简单”交叉验证和嵌套交叉验证过程中的准确率得分差异，以查看它们之间的平均不一致程度。
- en: In this case, by nested cross-validation scores, we mean the scores of the nested
    process (not to be confused with the inner cross-validation process), and we compare
    them with the scores of the regular process (non-nested).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，嵌套交叉验证得分指的是嵌套过程的得分（不要与内部交叉验证过程混淆），我们将其与常规过程（非嵌套）的得分进行比较。
- en: '**Output:**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As we can see, the non-nested scores, on average, are more optimistic. Thus,
    relying solely on the information of that process might result in a biased model
    selection.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，非嵌套得分的平均值更为乐观。因此，单纯依赖该过程的信息可能会导致模型选择的偏差。
- en: We can also plot the scores of each iteration and create a plot to get a visual
    comparison of how both processes behaved.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以绘制每次迭代的得分，并创建一个图表以获取两种过程表现的可视化比较。
- en: '**Output:**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '![](../Images/4fd278f5460b4e7fb02cff5c757ca4a7.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fd278f5460b4e7fb02cff5c757ca4a7.png)'
- en: Finally, we can also plot the difference in each round for both CV processes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还可以绘制每一轮两个交叉验证过程的差异。
- en: '**Output:**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '![](../Images/2866efccbead0cc67410ff0a538a951f.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2866efccbead0cc67410ff0a538a951f.png)'
- en: Final thoughts
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最后的思考
- en: While cross-validation is an industry-standard to assess generalization, it’s
    important to consider the problem at hand and potentially implement a more rigorous
    process to avoid a selection bias while choosing the final model. From personal
    experience, this is significatively more important when you’re dealing with small
    datasets and when the system is built to support non-trivial decisions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然交叉验证是评估泛化能力的行业标准，但重要的是要考虑手头的问题，并可能实施更严格的过程，以避免在选择最终模型时出现选择偏差。从个人经验来看，当处理小数据集且系统用于支持非平凡决策时，这一点尤为重要。
- en: Nested cross-validation is not a perfect process, it is **computationally expensive**,
    and it’s definitely not a panacea for poor model performance in production. However,
    in most scenarios, the process will allow you to get a more realistic view of
    the generalization capacity of each model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌套交叉验证并不是一个完美的过程，它是**计算上昂贵**的，并且绝对不是生产中模型表现差的灵丹妙药。然而，在大多数情况下，这个过程将使你对每个模型的泛化能力有一个更现实的了解。
- en: You can find the notebook with all of the code from the article on [GitHub](https://github.com/omartinez182/data-science-notebooks/blob/master/Nested_Cross_Validation_in_Python.ipynb).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[GitHub](https://github.com/omartinez182/data-science-notebooks/blob/master/Nested_Cross_Validation_in_Python.ipynb)上找到包含本文所有代码的笔记本。
- en: '**Bio:** [Eduardo Martinez](https://www.linkedin.com/in/omarmartinez182/) is
    a marketer with a postgraduate degree in business analytics. Currently at [Arcalea](https://arcalea.com/),
    Eduardo consolidates data science frameworks with marketing processes.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：** [爱德华多·马丁内斯](https://www.linkedin.com/in/omarmartinez182/)是一个拥有商业分析学研究生学位的市场营销人员。目前在[Arcalea](https://arcalea.com/)工作，爱德华多将数据科学框架与营销流程整合在一起。'
- en: '**Related:**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[How (and Why) to Create a Good Validation Set](https://www.kdnuggets.com/2017/11/create-good-validation-set.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何（以及为什么）创建一个好的验证集](https://www.kdnuggets.com/2017/11/create-good-validation-set.html)'
- en: '[5 Reasons Why You Should Use Cross-Validation in Your Data Science Projects](https://www.kdnuggets.com/2018/10/5-reasons-cross-validation-data-science-projects.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你应该在数据科学项目中使用交叉验证的5个理由](https://www.kdnuggets.com/2018/10/5-reasons-cross-validation-data-science-projects.html)'
- en: '[Building Reliable Machine Learning Models with Cross-validation](https://www.kdnuggets.com/2018/08/building-reliable-machine-learning-models-cross-validation.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用交叉验证构建可靠的机器学习模型](https://www.kdnuggets.com/2018/08/building-reliable-machine-learning-models-cross-validation.html)'
- en: More On This Topic
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关阅读
- en: '[Parallel Processing in Prompt Engineering: The Skeleton-of-Thought…](https://www.kdnuggets.com/parallel-processing-in-prompt-engineering-the-skeleton-of-thought-technique)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在Prompt工程中的并行处理：思维骨架...](https://www.kdnuggets.com/parallel-processing-in-prompt-engineering-the-skeleton-of-thought-technique)'
- en: '[AI, Analytics, Machine Learning, Data Science, Deep Learning…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AI、分析、机器学习、数据科学、深度学习等](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
- en: '[Announcing PyCaret 3.0: Open-source, Low-code Machine Learning in Python](https://www.kdnuggets.com/2023/03/announcing-pycaret-30-opensource-lowcode-machine-learning-python.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[宣布PyCaret 3.0：开源、低代码Python机器学习](https://www.kdnuggets.com/2023/03/announcing-pycaret-30-opensource-lowcode-machine-learning-python.html)'
- en: '[Optimizing Python Code Performance: A Deep Dive into Python Profilers](https://www.kdnuggets.com/2023/02/optimizing-python-code-performance-deep-dive-python-profilers.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化Python代码性能：深入了解Python分析工具](https://www.kdnuggets.com/2023/02/optimizing-python-code-performance-deep-dive-python-profilers.html)'
- en: '[Main 2021 Developments and Key 2022 Trends in AI, Data Science,…](https://www.kdnuggets.com/2021/12/trends-ai-data-science-ml-technology.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2021年主要进展和2022年AI、数据科学等关键趋势](https://www.kdnuggets.com/2021/12/trends-ai-data-science-ml-technology.html)'
- en: '[Key Data Science, Machine Learning, AI and Analytics Developments of 2022](https://www.kdnuggets.com/2022/12/key-data-science-machine-learning-ai-analytics-developments-2022.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2022年数据科学、机器学习、AI和分析的关键进展](https://www.kdnuggets.com/2022/12/key-data-science-machine-learning-ai-analytics-developments-2022.html)'
