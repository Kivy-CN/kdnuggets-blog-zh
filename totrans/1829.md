# 如何在你的第一次 Kaggle 比赛中排名前 10%

> 原文：[https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2)

**异常值**

[![异常值示例](../Images/70eadff4905dbdee3a23ff64ef3219a8.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example_outlier.png)

* * *

## 我们的前 3 个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的 IT 部门

* * *

图表显示了一些缩放后的坐标数据。我们可以看到在右上角有一些异常值。排除这些异常值后，分布看起来很好。

**虚拟变量**

对于分类变量，一个常见的做法是**[独热编码](https://en.wikipedia.org/wiki/One-hot)**。对于具有`n`个可能值的分类变量，我们创建一组`n`个虚拟变量。假设数据中的一条记录对于这个变量取一个值，那么对应的虚拟变量被设置为`1`，而同一组中的其他虚拟变量都被设置为`0`。

[![虚拟变量示例](../Images/f7891877228b4993c0d084ec1c553387.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example-dummies.png)

在这个例子中，我们将`DayOfWeek`转换为 7 个虚拟变量。

注意，当分类变量可以取许多值（数百个或更多）时，这可能效果不好。虽然很难找到通用的解决方案，但我会在下一节讨论一种情况。

**特征工程**

有人将 Kaggle 比赛的本质描述为**特征工程辅以模型调优和集成学习**。是的，这确实很有道理。**特征工程可以使你走得很远。** 然而，你对给定数据领域的了解程度决定了你能走多远。例如，在一个数据主要由文本组成的比赛中，自然语言处理技术是必不可少的。构造有用特征的方法是我们必须不断学习的，以便做得更好。

基本上，**当你觉得一个变量在任务中直观上有用时，你可以将其作为特征包含进去**。但你怎么知道它确实有效呢？最简单的方法是将其与目标变量绘制在一起，如下所示：

[![检查特征有效性](../Images/d9b85e951f68e16d26e5899549ae2247.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-vis_check_feat.png)

**特征选择**

一般来说，**我们应该尽量创建尽可能多的特征，并相信模型能挑选出最重要的特征**。然而，事先进行特征选择仍然有其益处：

+   特征较少意味着训练更快

+   一些特征与其他特征线性相关。这可能对模型造成压力。

+   通过挑选最重要的特征，我们可以将它们之间的交互用作新特征。有时这会带来意想不到的改善。

检查特征重要性的最简单方法是拟合随机森林模型。虽然有更稳健的特征选择算法（例如[这个](http://jmlr.org/papers/volume10/tuv09a/tuv09a.pdf)），理论上更优，但由于缺乏高效的实现，实际操作中不可行。你可以通过增加随机森林中使用的树的数量来对抗噪声数据（在一定程度上）。

这对于数据**[匿名化](https://en.wikipedia.org/wiki/Data_anonymization)**的比赛非常重要，因为你不会浪费时间去弄清楚一个无关变量的意义。

**特征编码**

有时需要将原始特征转换为其他格式以使其正常工作。

例如，假设我们有一个可以取超过10K个不同值的分类变量。那么，天真地创建虚拟变量并不是一个可行的选择。一个可接受的解决方案是仅为一部分值（例如，构成95%特征重要性的值）创建虚拟变量，并将其他所有值分配到“其他”类别。

**更新于2016年10月28日：**对于上述情况，另一个可能的解决方案是使用**因子分解机**。有关详情，请参见Kaggle用户“idle_speculation”发布的[这篇帖子](https://www.kaggle.com/c/expedia-hotel-recommendations/forums/t/21607/1st-place-solution-summary)。

**模型选择**

当特征确定后，我们可以开始训练模型。Kaggle比赛通常偏爱**基于树的模型**：

+   **梯度提升树**

+   随机森林

+   额外随机树

以下模型在总体性能方面略逊一筹，但适合作为集成学习中的基础模型（将在后面讨论）：

+   支持向量机

+   线性回归

+   逻辑回归

+   神经网络

请注意，这不适用于计算机视觉比赛，这些比赛基本上被神经网络模型主导。

所有这些模型都在**[Sklearn](http://scikit-learn.org/)**中实现。

在这里，我想强调**[Xgboost](https://github.com/dmlc/xgboost)**的伟大。梯度提升树的出色表现和Xgboost的高效实现使其在Kaggle比赛中非常受欢迎。如今，几乎每个获胜者都以某种方式使用Xgboost。

**更新于2016年10月28日：**最近微软开源了**[LightGBM](https://github.com/Microsoft/LightGBM)**，它可能是比Xgboost更好的梯度提升库。

顺便提一下，对于 Windows 用户来说，安装 Xgboost 可能是一个艰难的过程。如果你遇到问题，可以参考 [这篇文章](https://dnc1994.com/2016/03/installing-xgboost-on-windows/) 。

### 相关话题

+   [LinkedIn 如何利用机器学习来排序你的信息流](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)

+   [如何在没有工作经验的情况下获得数据科学领域的第一份工作](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)

+   [使用 TensorFlow 和 Keras 构建并训练你的第一个神经网络](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)

+   [它活了！用 Python 和一些便宜的组件构建你的第一个机器人](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)

+   [从零到英雄：用 PyTorch 创建你的第一个机器学习模型](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)

+   [部署你的第一个机器学习模型](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)
