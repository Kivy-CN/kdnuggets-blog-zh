- en: K-Means 8x faster, 27x lower error than Scikit-learn in 25 lines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-Means 比 Scikit-learn 快 8 倍，误差低 27 倍，代码仅需 25 行
- en: 原文：[https://www.kdnuggets.com/2021/01/k-means-faster-lower-error-scikit-learn.html](https://www.kdnuggets.com/2021/01/k-means-faster-lower-error-scikit-learn.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/01/k-means-faster-lower-error-scikit-learn.html](https://www.kdnuggets.com/2021/01/k-means-faster-lower-error-scikit-learn.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Jakub Adamczyk](https://github.com/j-adamczyk), Computer science student,
    Python and Machine Learning enthusiast.**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由[Jakub Adamczyk](https://github.com/j-adamczyk)，计算机科学学生，Python 和机器学习爱好者**。'
- en: '![](../Images/a2f968618e10547fcf2c6c6a6bbac309.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2f968618e10547fcf2c6c6a6bbac309.png)'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速通道进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In [my last article on the faiss library](https://towardsdatascience.com/make-knn-300-times-faster-than-scikit-learns-in-20-lines-5e29d74e76bb), I
    showed how to make kNN up to 300 times faster than Scikit-learn’s in 20 lines
    using [Facebook’s faiss library](https://github.com/facebookresearch/faiss). But
    we can do much more with it, including both faster and more accurate K-Means clustering,
    in just 25 lines!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在[我上一篇关于 faiss 库的文章](https://towardsdatascience.com/make-knn-300-times-faster-than-scikit-learns-in-20-lines-5e29d74e76bb)中，我展示了如何通过使用[Facebook
    的 faiss 库](https://github.com/facebookresearch/faiss)将 kNN 加速至比 Scikit-learn 快
    300 倍，仅需 20 行代码。但我们可以用它做更多的事情，包括更快且更准确的 K-Means 聚类，代码仅需 25 行！
- en: 'K-Means is an iterative algorithm, which clusters the data points into *k*
    clusters, each represented with a mean/center point (a centroid). Training starts
    with some initial guesses and then alternates between two steps: assignment and
    update.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: K-Means 是一种迭代算法，它将数据点聚类到 *k* 个簇中，每个簇由一个均值/中心点（质心）表示。训练从一些初始猜测开始，然后在两个步骤之间交替进行：分配和更新。
- en: In the assignment phase, we assign each point to the nearest cluster (using
    Euclidean distance between point and centroids). In the update step, we recalculate
    each centroid by calculating a mean point from all points assigned to that cluster
    in the current step.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在分配阶段，我们将每个点分配到最近的簇（使用点与质心之间的欧几里得距离）。在更新步骤中，我们通过计算当前步骤中分配给该簇的所有点的均值来重新计算每个质心。
- en: The final quality of clustering is calculated as a sum of in-cluster distances,
    where for each cluster, we calculate a sum of Euclidean distances between points
    in that cluster and its centroid. This is also called inertia.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的最终质量计算为簇内距离的总和，对于每个簇，我们计算该簇内点与其质心之间的欧几里得距离的总和。这也称为惯性。
- en: For prediction, we perform a 1-nearest neighbor search (kNN with *k* = 1) between
    new points and centroids.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预测，我们在新点和质心之间执行 1-最近邻搜索（kNN，*k* = 1）。
- en: Scikit-learn vs faiss
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Scikit-learn 与 faiss
- en: 'In both libraries, we have to specify algorithm hyperparameters: number of
    clusters, number of restarts (each starting with other initial guesses), and maximal
    number of iterations.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个库中，我们需要指定算法的超参数：簇的数量、重启的次数（每次从不同的初始猜测开始）以及最大迭代次数。
- en: As we can see from the example, the core of the algorithm is searching for the
    nearest neighbors, specifically the nearest centroids, both for training and prediction.
    And that’s where faiss is orders of magnitude faster than Scikit-learn! It leverages
    great C++ implementation, concurrency wherever possible, and even the GPU, if
    you want.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从例子中可以看出，该算法的核心是搜索最近邻，特别是最近的质心，适用于训练和预测。这也是 faiss 比 Scikit-learn 快几个数量级的地方！它利用了出色的
    C++ 实现、尽可能的并发，甚至可以使用 GPU（如果你需要的话）。
- en: Implementing K-Means clustering with faiss
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 faiss 实现 K-Means 聚类
- en: A great feature of faiss is that it has both installation and build instructions
    and excellent documentation with examples. After the installation, we can write
    the actual clustering. The code is quite simple because we just mimic the Scikit-learn
    API.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: faiss的一个伟大特点是它提供了安装和构建说明以及带有示例的优秀文档。安装后，我们可以编写实际的聚类代码。代码相当简单，因为我们只是模仿Scikit-learn的API。
- en: 'Important elements:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 重要元素：
- en: faiss has a built-in *Kmeans* class specifically for this task, but its arguments
    have different names than in Scikit-learn (see [the docs](https://github.com/facebookresearch/faiss/wiki/Faiss-building-blocks:-clustering,-PCA,-quantization))
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: faiss有一个内置的*Kmeans*类专门用于此任务，但其参数的名称与Scikit-learn不同（参见[文档](https://github.com/facebookresearch/faiss/wiki/Faiss-building-blocks:-clustering,-PCA,-quantization)）。
- en: we have to make sure we use *np.float32* type, as faiss only works with this
    type
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们必须确保使用*np.float32*类型，因为faiss仅支持这种类型。
- en: '*kmeans.obj* returns a list of errors through the training, so to get only
    the final one, like in Scikit-learn, we use the [-1] index'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*kmeans.obj*通过训练返回一个误差列表，因此为了得到最终的误差，就像在Scikit-learn中一样，我们使用[-1]索引。'
- en: prediction is done with the *Index* data structure, which is the basic building
    block of faiss, and is used in all nearest neighbor searches
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测是通过*Index*数据结构完成的，这是faiss的基本构建块，所有的最近邻搜索都使用它。
- en: in prediction, we perform the kNN search with *k* = 1, returning indices of
    nearest centroids from *self.cluster_centers_* (index [1], because *index.search()* returns
    distances and indices)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预测中，我们进行kNN搜索，*k* = 1，返回*自.cluster_centers_*中的最近质心的索引（索引[1]，因为*index.search()*返回距离和索引）。
- en: Time and accuracy comparison
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 时间和准确性比较
- en: I’ve chosen a few popular datasets available in Scikit-learn for comparison.
    The train and predict times are compared. For easier reading, I’ve explicitly
    written how many times faster is the faiss-based clustering than Scikit-learn’s.
    For error comparison, I’ve just written how many times lower error the faiss-based
    clustering achieves (because numbers are large and not very informative).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择了一些在Scikit-learn中可用的流行数据集进行比较。比较了训练和预测时间。为了更容易阅读，我明确写出基于faiss的聚类比Scikit-learn快多少倍。为了比较误差，我只写了基于faiss的聚类实现了多少倍的更低误差（因为数字较大且不太具参考性）。
- en: All of these times have been measured with the *time.process_time()* function
    that measures process time instead of wall clock time, for more accurate results.
    Results are averages of 100 runs, except for MNIST, where it took too long for
    Scikit-learn, and I had to do 5 runs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些时间都是通过*time.process_time()*函数测量的，该函数测量进程时间而非挂钟时间，以获得更准确的结果。结果是100次运行的平均值，除了MNIST，因为Scikit-learn花费的时间太长，我只做了5次运行。
- en: '![](../Images/c5ed1c5e3c332b11b91dc3e0e19051d7.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5ed1c5e3c332b11b91dc3e0e19051d7.png)'
- en: '*Train times (image by author).*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*训练时间（图像由作者提供）。*'
- en: '![](../Images/325ecf334311ed067d590720ead00ade.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/325ecf334311ed067d590720ead00ade.png)'
- en: '*Predict times (image by author).*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*预测时间（图像由作者提供）。*'
- en: '![](../Images/c00dfc80bbbc4290db07ab33415e23d9.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c00dfc80bbbc4290db07ab33415e23d9.png)'
- en: '*Training errors (image by author).*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*训练误差（图像由作者提供）。*'
- en: As we can see, for K-Means clustering for small datasets (first 4 datasets)
    faiss-based version is slower for training and has a larger error. For prediction,
    it works universally faster.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，对于小数据集的K-Means聚类（前4个数据集），基于faiss的版本训练速度较慢且误差较大。对于预测，它的表现普遍更快。
- en: For the larger MNIST dataset, faiss is a clear winner. Training 20.5 times faster
    is huge, especially because it reduces time from almost 3 minutes to less than
    8 seconds! A 1.5 times faster prediction is also nice. The true achievement, however,
    is a spectacular 27.5 times lower error. This means that for a larger, real-world
    dataset, the faiss-based version is vastly more accurate. And this takes only
    25 lines of code!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较大的MNIST数据集，faiss明显胜出。训练速度快20.5倍是巨大的，特别是因为它将时间从将近3分钟减少到不到8秒！预测速度快1.5倍也是不错的。然而，真正的成就的是误差降低了27.5倍。这意味着对于较大的现实世界数据集，基于faiss的版本准确性高得多。而且这只需要25行代码！
- en: 'So based on this: if you have a large (at least a few thousand samples) real-world
    dataset, the faiss-based version is just plain better. For a small toy dataset,
    Scikit-learn is a better choice; however, if you have a GPU, the GPU-accelerated
    faiss version may turn out faster (I haven’t checked it for fair CPU comparison).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此：如果你有一个大（至少几千个样本）的现实世界数据集，基于faiss的版本就更好。对于小的玩具数据集，Scikit-learn是更好的选择；然而，如果你有GPU，GPU加速的faiss版本可能会更快（我还未检查过以确保公平的CPU比较）。
- en: Summary
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: With 25 lines of code, we can get a huge speed and accuracy boost for K-Means
    clustering for reasonably sized datasets with the faiss library. If you need,
    you can get even better with a GPU, multiple GPUs, and more, which is nicely explained
    in the faiss docs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过25行代码，我们可以利用faiss库为K-Means聚类提供显著的速度和准确性提升，适用于合理大小的数据集。如果需要，你可以使用GPU、多个GPU等进一步提高，faiss文档中对此做了很好的解释。
- en: '[Original](https://towardsdatascience.com/k-means-8x-faster-27x-lower-error-than-scikit-learns-in-25-lines-eaedc7a3a0c8).
    Reposted with permission.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/k-means-8x-faster-27x-lower-error-than-scikit-learns-in-25-lines-eaedc7a3a0c8)。已获许可转载。'
- en: '**Related:**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Key Data Science Algorithms Explained: From k-means to k-medoids clustering](https://www.kdnuggets.com/2020/12/algorithms-explained-k-means-k-medoids-clustering.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学关键算法解释：从k-means到k-medoids聚类](https://www.kdnuggets.com/2020/12/algorithms-explained-k-means-k-medoids-clustering.html)'
- en: '[A complete guide to K-means clustering algorithm](https://www.kdnuggets.com/2019/05/guide-k-means-clustering-algorithm.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[K-means聚类算法完整指南](https://www.kdnuggets.com/2019/05/guide-k-means-clustering-algorithm.html)'
- en: '[Most Popular Distance Metrics Used in KNN and When to Use Them](https://www.kdnuggets.com/2020/11/most-popular-distance-metrics-knn.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KNN中最受欢迎的距离度量及其使用时机](https://www.kdnuggets.com/2020/11/most-popular-distance-metrics-knn.html)'
- en: More On This Topic
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成为优秀数据科学家需要的5项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个初学者数据科学家应该掌握的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2021年最佳ETL工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
- en: '[Multi-modal deep learning in less than 15 lines of code](https://www.kdnuggets.com/2023/01/predibase-multi-modal-deep-learning-less-15-lines-code.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[少于15行代码实现多模态深度学习](https://www.kdnuggets.com/2023/01/predibase-multi-modal-deep-learning-less-15-lines-code.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以找到目标，然后找到目标去…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一个90亿美元的人工智能失败案例分析](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
