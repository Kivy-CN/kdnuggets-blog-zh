- en: 'Neural Networks with Numpy for Absolute Beginners: Introduction'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/03/neural-networks-numpy-absolute-beginners-introduction.html](https://www.kdnuggets.com/2019/03/neural-networks-numpy-absolute-beginners-introduction.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By [Suraj Donthi](https://www.surajdonthi.com/), Computer Vision Consultant
    & Course Instructor at DataCamp**'
  prefs: []
  type: TYPE_NORMAL
- en: Artificial Intelligence has become one of the hottest fields in the current
    day and most of us willing to dive into this field start off with Neural Networks!!
  prefs: []
  type: TYPE_NORMAL
- en: But on confronting the math intensive concepts of Neural Networks we just end
    up learning a few frameworks like Tensorflow, Pytorch etc., for implementing Deep
    Learning Models.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, just learning these frameworks and not understanding the underlying
    concepts is like playing with a black box.
  prefs: []
  type: TYPE_NORMAL
- en: Whether you want to work in the industry or academia, you will be working, tweaking
    and playing with the models for which you need to have a clear understanding.
    Both the industry and the academia expect you to have full clarity of these concepts
    including the math.
  prefs: []
  type: TYPE_NORMAL
- en: In this series of tutorials, I’ll make it extremely simple to understand Neural
    Networks by providing step by step explanation. Also, the math you’ll need will
    be the level of high school.
  prefs: []
  type: TYPE_NORMAL
- en: Let us start with the inception of artificial neural networks and gain some
    inspiration as to how it evolved.
  prefs: []
  type: TYPE_NORMAL
- en: A little bit into the history of how Neural Networks evolved
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It must be noted that most of the Algorithms for Neural Networks that were developed
    during the period 1950–2000 and now existing, are highly inspired by the working
    of our brain, the neurons, their structure and how they learn and transfer data.
    The most popular works include the Perceptron(1958) and the Neocognitron(1980).
    These papers were extremely instrumental in unwiring the brain code. They try
    to mathematically formulate a model of the neural networks in our brain.
  prefs: []
  type: TYPE_NORMAL
- en: And everything changed after the God Father of AI Geoffrey Hinton formulated
    the back-propagation algorithm in 1986(That’s right! what you are learning is
    more than 30 years old!).
  prefs: []
  type: TYPE_NORMAL
- en: A biological Neuron
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![figure-name](../Images/e3a2dbe5d4d4c8051fca72a379720e58.png)The figure above
    shows a biological neuron. It has *dendrites* that receive information from neurons.
    The received information is passed on to the *cell body or the nucleus* of the
    neuron. The *nucleus* is where the information is processed and passed on to the
    next layer of neurons through *axons*.'
  prefs: []
  type: TYPE_NORMAL
- en: Our brain consists of about 100 billion such neurons which communicate through
    electrochemical signals. Each neuron is connected to 100s and 1000s of other neurons
    which constantly transmit and receive signals.
  prefs: []
  type: TYPE_NORMAL
- en: But how can our brain process so much information just by sending electrochemical
    signals? How can the neurons understand which signal is important and which isn't?
    How do the neurons know what information to pass forward?
  prefs: []
  type: TYPE_NORMAL
- en: The electrochemical signals consist of strong and weak signals. The strong signals
    are the ones to dominate which information is important. So only the strong signal
    or a combination of them pass through the nuclues (the CPU of neurons) and are
    transmitted to the next set of neurons through the axons.
  prefs: []
  type: TYPE_NORMAL
- en: But how are some signals strong and some signals week?
  prefs: []
  type: TYPE_NORMAL
- en: Well, through millions of years of evolution, the neurons have become sensitive
    to certain kinds of signals. When the neuron encounters a specific pattern, they
    get triggered(activated) and as a consequence send strong signals to other neurons
    and hence the information is transmitted.
  prefs: []
  type: TYPE_NORMAL
- en: Most of us also know that different regions of our brain are activated (or receptive)
    for different actions like seeing, hearing, creative thinking and so on. This
    is because the neurons belonging to a specific region in the brain are trained
    to process a certain kind of information better and hence get activated only when
    certain kinds of information is being sent. The figure below gives us a better
    understanding of the different receptive regions of the brain.
  prefs: []
  type: TYPE_NORMAL
- en: '![alternate text](../Images/35b55fb1a389baa89cde9023eb63fc32.png)If that is
    so… can the neurons be made sensitive to a different pattern(i.e., if they have
    truly become sensitive based on some patterns)?'
  prefs: []
  type: TYPE_NORMAL
- en: It has been shown through Neuroplasticity that the different regions of the
    brain can be rewired to perform totally different tasks. Such as the neurons responsible
    for touch sensing can be rewired to become sensitive to smell. Check out this
    great TEDx video below to know more about neuroplasticity.
  prefs: []
  type: TYPE_NORMAL
- en: TEDx video on Neuroplasticity [[Source]](https://www.youtube.com/watch?v=xzbHtIrb14s)
  prefs: []
  type: TYPE_NORMAL
- en: But what is the mechanism by which the neurons become sensitive?
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, neuroscientists are still trying to figure that out!!
  prefs: []
  type: TYPE_NORMAL
- en: But fortunately enough, god father of AI Geff has saved the day by inventing
    back propagation which accomplishes the same task for our Artificial Neurons,
    i.e., sensitizing them to certain patterns.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll explore the working of a perceptron and also gain
    a mathematical intuition.
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron/Artificial Neuron
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/bbd835297009a0c24b5acc73e39a6d00.png)'
  prefs: []
  type: TYPE_IMG
- en: From the figure, you can observe that the perceptron is a reflection of the
    biological neuron. The inputs combined with the weights(*wᵢ*) are analogous to
    dendrites. These values are summed and passed through an activation function (like
    the thresholding function as shown in fig.). This is analogous to the nucleus.
    Finally, the activated value is transmitted to the next neuron/perceptron which
    is analogous to the axons.
  prefs: []
  type: TYPE_NORMAL
- en: The latent weights(*wᵢ*) multiplied with each input(*xᵢ*) depicts the significance(strength)
    of the respective input signal. Hence, larger the value of a weight, more important
    is the feature.
  prefs: []
  type: TYPE_NORMAL
- en: You can infer from this architecture that the weights are what is learned in
    a perceptron so as to arrive at the required result. An additional bias(*b*, here
    *w₀*) is also learned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, when there are multiple inputs (say *n*), the equation can be generalized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/379910f15ba571c347905c8b1b3342ec.png)![Equation](../Images/68bac74c2701bd3d5270a73e2d19398f.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, the output of summation (assume as *z*) is fed to the *thresholding
    activation function*, where the function outputs
  prefs: []
  type: TYPE_NORMAL
- en: An Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![logic-gates](../Images/72bff0e49b84cadc2e36e8077e41cc42.png)'
  prefs: []
  type: TYPE_IMG
- en: Let us consider our perceptron to perform as *logic gates* to gain more intuition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s choose an *AND Gate*. The Truth Table for the gate is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: The perceptron for the *AND Gate* can be formed as shown in the figure. It is
    clear that the perceptron has two inputs (here *x₁ = A* and *x₂ = B*).
  prefs: []
  type: TYPE_NORMAL
- en: '![threshold-function](../Images/ba979f442dfea6e1392af3988d3a6e18.png)![threshold-function-eq](../Images/5caacf2f8f14fbcac3a7ab4b807dc330.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that for inputs *x₁, x₂* and *x₀* = 1, setting their weights as
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/87dbdc6f996cdfd46e54f905db58c6a1.png)![Equation](../Images/1cf563cb01c06d75daf731d1c46db107.png)![Equation](../Images/05dee933fdf09eee757a5d693178c757.png)'
  prefs: []
  type: TYPE_IMG
- en: respectively and keeping the *Threshold function* as the activation function
    we can arrive at the *AND Gate*.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s get our hands dirty and codify this and test it out!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Similarly for *NOR Gate* the Truth Table is,
  prefs: []
  type: TYPE_NORMAL
- en: '![nor-gate-truth-table](../Images/7a93c890f845f018f767a3c1036c0433.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The perceptron for *NOR Gate* will be as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![nor-gate=perceptron](../Images/f00b0279cec3976ab8f92b7359aee1e4.png)'
  prefs: []
  type: TYPE_IMG
- en: You can set the weights as
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/009579f99c16b1c77b7d43fafd76c5ce.png)![Equation](../Images/c14c9c9e6dab4255694305da0f1d7dce.png)![Equation](../Images/7665e03d35078c41cf61daae80a25b50.png)'
  prefs: []
  type: TYPE_IMG
- en: so that you obtain a *NOR Gate.*
  prefs: []
  type: TYPE_NORMAL
- en: 'You can go ahead and implement this in code as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: What you are actually calculating…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you analyse what you were trying to do in the above examples, you will realize
    that you were actually trying to adjust the values of the weights to obtain the
    required output.
  prefs: []
  type: TYPE_NORMAL
- en: Lets consider the *NOR Gate* example and break it down to very miniscule steps
    to gain more understanding.
  prefs: []
  type: TYPE_NORMAL
- en: What you would usually do first is to simply set some values to the weights
    and observe the result, say
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/1664f5c911dd3834b15ae94a0f4adc01.png)![Equation](../Images/fdad0fc3d15c9f516e3eaecd32f765c5.png)![Equation](../Images/e7f0d60adce140c228e6f813e0c73a84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then the output will be as shown in below table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/6c9985311aad0f3841fd23651f371576.png)'
  prefs: []
  type: TYPE_IMG
- en: So, how can you fix the values of weights so that you get the right output?
  prefs: []
  type: TYPE_NORMAL
- en: By intuition, you can easily observe that *w₀* must be increased and *w₁* and
    *w₀* must be reduced or rather made negative so that you obtain the actual output.
    But if you breakdown this intuition, you will observe that you are actually finding
    the difference between the actual output and the predicted output and finally
    reflecting that on the weights…
  prefs: []
  type: TYPE_NORMAL
- en: This is a very important concept that you will be digging deeper and will be
    the core to formulate the ideas behind *gradient descent* and also *backward propagation*.
  prefs: []
  type: TYPE_NORMAL
- en: What did you learn?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Neurons must be made sensitive to a pattern in order to recognize it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, similarly, in our perceptron/artificial neuron, **the weights are what is
    to be learnt**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the later articles you’ll fully understand how the weights are trained to
    recognize patterns and also the different techniques that exist.
  prefs: []
  type: TYPE_NORMAL
- en: As you’ll see later, the neural networks are very similar to the structure of
    biological neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: While it is true that we learnt only a few small concepts (although very crucial)
    in this first part of the article, they will serve as the strong foundation for
    implementing Neural Networks. Moreover, I’m keeping this article short and sweet
    so that too much is information is not dumped at once and will help absorb more!
  prefs: []
  type: TYPE_NORMAL
- en: In the next tutorial, you will learn about **Linear Regression** (which can
    otherwise be called a perceptron with linear activation function) in detail and
    also implement them. The **Gradient Descent algorithm which helps learn the weights**
    are described and implemented in detail. Lastly, you’ll be able to **predict the
    outcome of an event** with the help of Linear Regression. So, head on to the next
    article to implement it!
  prefs: []
  type: TYPE_NORMAL
- en: 'You can checkout the next part of the article here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Neural Networks with Numpy for Absolute Beginners — Part 2: Linear Regression**](https://medium.com/@surajdonthi95/neural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio**: [Suraj Donthi](https://www.surajdonthi.com/) is a Computer Vision
    Consultant, Author, Machine Learning and Deep Learning Trainer.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/neural-networks-with-numpy-for-absolute-beginners-introduction-c1394639edb2).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Neural Networks – an Intuition](/2019/02/neural-networks-intuition.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Backpropagation Algorithm Demystified](/2019/01/backpropagation-algorithm-demystified.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mastering the Learning Rate to Speed Up Deep Learning](/2018/11/mastering-learning-rate-speed-up-deep-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[The Absolute Basics of MLOps](https://www.kdnuggets.com/2022/09/absolute-basics-mlops.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10 Simple Things to Try Before Neural Networks](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Neural Networks Don''t Lead Us Towards AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Explainable Forecasting and Nowcasting with State-of-the-art Deep…](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Image Classification with Convolutional Neural Networks (CNNs)](https://www.kdnuggets.com/2022/05/image-classification-convolutional-neural-networks-cnns.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
