- en: 'Regularization in Logistic Regression: Better Fit and Better Generalization?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2016/06/regularization-logistic-regression.html](https://www.kdnuggets.com/2016/06/regularization-logistic-regression.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Regularization does NOT improve the performance on the data set that the algorithm
    used to learn the model parameters (feature weights). However, it can improve
    the generalization performance, i.e., the performance on new, unseen data, which
    is exactly what we want.
  prefs: []
  type: TYPE_NORMAL
- en: In intuitive terms, we can think of regularization as a penalty against complexity.
    Increasing the regularization strength penalizes "large" weight coefficients --
    our goal is to prevent that our model picks up "peculiarities," "noise," or "imagines
    a pattern where there is none."
  prefs: []
  type: TYPE_NORMAL
- en: Again, we don't want the model to memorize the training dataset, we want a model
    that generalizes well to new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: In more specific terms, we can think of regularization as adding (or increasing
    the) bias if our model suffers from (high) variance (i.e., it overfits the training
    data). On the other hand, too much bias will result in underfitting (a characteristic
    indicator of high bias is that the model shows a "bad" performance for both the
    training and test dataset). We know that our goal in an unregularized model is
    to minimize the cost function, i.e., we want to find the feature weights that
    correspond to the global cost minimum (remember that the logistic cost function
    is convex).
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/246f23a1b7db46b1291182f90099e8e3.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/regularized-logistic-regression-performance/unregularized.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Now, if we regularize the cost function (e.g., via L2 regularization), we add
    an additional to our cost function (J) that increases as the value of your parameter
    weights (w) increase; keep in mind that the regularization we add a new hyperparameter,
    lambda, to control the regularization strength.
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/00455a306f38c40c05ede9be3aa95de4.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/regularized-logistic-regression-performance/l2-term.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, our new problem is to minimize the cost function given this added
    constraint.
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/c60871e2ad31b45d65061c2e7dbb97cb.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/regularized-logistic-regression-performance/regularized.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Intuitively, we can think of the "sphere" at the coordinate center in the figure
    above as our "budget." Now, our objective is still the same: we want to minimize
    the cost function. However, we are now constraint by the regularization term;
    we want to get as close as possible to the global minimum while staying within
    our "budget" (i.e., the sphere).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Sebastian Raschka](https://twitter.com/rasbt)** is a ''Data Scientist''
    and Machine Learning enthusiast with a big passion for Python & open source. Author
    of ''[Python Machine Learning](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning)''.
    Michigan State University.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/regularized-logistic-regression-performance.md).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[When Does Deep Learning Work Better Than SVMs or Random Forests?](/2016/04/deep-learning-vs-svm-random-forest.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Development of Classification as a Learning Machine](/2016/04/development-classification-learning-machine.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Why Implement Machine Learning Algorithms From Scratch?](/2016/05/implement-machine-learning-algorithms-scratch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Comparing Linear and Logistic Regression](https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Classification Metrics Walkthrough: Logistic Regression with…](https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Tailor ChatGPT to Fit Your Needs with Custom Instructions](https://www.kdnuggets.com/2023/08/tailor-chatgpt-fit-needs-custom-instructions.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear vs Logistic Regression: A Succinct Explanation](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News 22:n12, March 23: Best Data Science Books for…](https://www.kdnuggets.com/2022/n12.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Logistic Regression for Classification](https://www.kdnuggets.com/2022/04/logistic-regression-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
