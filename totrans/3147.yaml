- en: 5 Things to Know About Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 件你需要知道的关于机器学习的事
- en: 原文：[https://www.kdnuggets.com/2018/03/5-things-know-about-machine-learning.html](https://www.kdnuggets.com/2018/03/5-things-know-about-machine-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/03/5-things-know-about-machine-learning.html](https://www.kdnuggets.com/2018/03/5-things-know-about-machine-learning.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: There is always something new to learn on any fast-evolving topic, and machine
    learning is no exception. This post will point out 5 things to know about machine
    learning, 5 things which you may not know, may not have been aware of, or may
    have once known and now forgotten.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何快速发展的领域总有新的东西可以学习，机器学习也不例外。这篇文章将指出5件你可能不知道的、可能不曾意识到的，或曾经知道但现在已忘记的关于机器学习的事。
- en: Note that the title of this post is not "The 5 Most Important Things..." or
    "Top 5 Things..." to know about machine learning; it's *just* "5 Things." It's
    not authoritative or exhaustive, but rather a collection of 5 things that may
    be of use.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这篇文章的标题不是“最重要的5件事...”或“关于机器学习的前5件事...”；它*只是*“5件事”。这不是权威或详尽的，而是5件可能有用的事情的集合。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前3个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的IT工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 1\. Data preparation is 80% of machine learning, so...
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1\. 数据准备占机器学习的80%，所以...
- en: It's fairly well-discussed that data preparation takes a disproportionate amount
    of time in a machine learning task. Or, at least, a **seemingly** disproportionate
    amount of time.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备在机器学习任务中确实占用了不成比例的时间。或者说，**看起来**占用了不成比例的时间。
- en: What is commonly lacking in these discussions, beyond the specifics of performing
    data preparation and the reasons for its importance, is *why you should care about
    performing data preparation*. And I don't mean just to have conforming data, but
    more like a philosophical diatribe as to why you should embrace the data preparation.
    Live the data preparation. Be one with the data preparation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些讨论中，除了执行数据准备的具体细节和其重要性原因外，常常缺乏的是*为什么你应该关心数据准备*。我指的不仅仅是为了获得合规的数据，而更像是哲学上的讨论，为什么你应该接受数据准备。活在数据准备中，与数据准备融为一体。
- en: '![Data prep in CRISP-DM](../Images/46d01de23b57f7116bef588b1995aa70.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![CRISP-DM中的数据准备](../Images/46d01de23b57f7116bef588b1995aa70.png)'
- en: Data preparation in the [CRISP-DM model](https://en.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在[CRISP-DM模型](https://en.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining)中的数据准备。
- en: Some of the best machine learning advice that I can think of is that since you
    are ultimately destined to spend so much of your time on preparing data for The
    Big Show, being determined to be the very best data preparation professional around
    is a pretty good goal. Since it's not only time-consuming but of great importance
    to the steps which follow (garbage in, garbage out), having a reputation as a
    bad-ass data preparer wouldn't be the worst thing in the world.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我能想到的一些最佳机器学习建议是，因为你最终注定会花费大量时间准备数据以迎接大场面，所以决心成为最优秀的数据准备专业人士是一个相当不错的目标。由于这不仅耗时且对后续步骤（垃圾进，垃圾出）极为重要，拥有一个出色的数据准备者的声誉也不会是世界上最糟糕的事情。
- en: So yeah, while data preparation might take a while to perform and master, that's
    really not a bad thing. There is opportunity in this necessity, both to stand
    out in your role, as well as the intrinsic value of knowing you're good at your
    job.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，是的，虽然数据准备可能需要花费一些时间来执行和掌握，但这其实并不是一件坏事。这一必要性带来的机会，不仅可以在你的角色中脱颖而出，还有认识到你在工作中很优秀的内在价值。
- en: 'For some more practical insight into data preparation, here are a couple of
    places to start out:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 若要获取更多关于数据准备的实用见解，可以从以下几个地方开始：
- en: '[7 Steps to Mastering Data Preparation with Python](/2017/06/7-steps-mastering-data-preparation-python.html)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握 Python 数据准备的 7 个步骤](/2017/06/7-steps-mastering-data-preparation-python.html)'
- en: '[Machine Learning Workflows in Python from Scratch Part 1: Data Preparation](/2017/05/machine-learning-workflows-python-scratch-part-1.html)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从零开始的 Python 机器学习工作流 第 1 部分：数据准备](/2017/05/machine-learning-workflows-python-scratch-part-1.html)'
- en: 2\. The value of a performance baseline
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2\. 性能基准的价值
- en: So you have modeled some data with a particular algorithm, spent time tuning
    your hyperparameters, performed some feature engineering and/or selection, and
    you're happy that you have squeezed out a training accuracy of, say, 75%. You
    pat yourself on the back for all of your hard work.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你使用特定算法对一些数据进行了建模，花时间调整了超参数，进行了特征工程和/或选择，并且你很高兴地发现训练准确率达到了 75%。你为自己的辛勤工作感到自豪。
- en: But what are you comparing your results to? If you don't have a baseline --
    [a simple sanity check consists of comparing one’s estimator against simple rules
    of thumb](http://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators)
    -- then you are literally comparing that hard work to **nothing**. It's reasonable
    to assume that almost any accuracy could be considered back pat-worthy without
    something with which to compare it.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 但你在将结果与什么进行比较呢？如果没有基准 -- [一个简单的合理检查包括将估计器与简单的经验法则进行比较](http://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators)
    -- 那么你实际上是在将那辛苦的工作与 **空无** 进行比较。合理的假设是，没有可以比较的东西，几乎任何准确度都可以被认为值得拍背。
- en: 'Random guessing isn''t the best strategy for a baseline; instead, accepted
    methods exist for determining a baseline accuracy for comparison. Scikit-learn,
    for example, provides a series of baseline classifiers in its `[DummyClassifier](http://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators)`
    class:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 随机猜测不是基准的最佳策略；相反，存在用于确定基准准确度的公认方法。例如，Scikit-learn 提供了一系列基准分类器，位于其 `[DummyClassifier](http://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators)`
    类中：
- en: '`stratified` generates random predictions by respecting the training set class
    distribution.'
  id: totrans-25
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stratified` 通过尊重训练集类别分布来生成随机预测。'
- en: '`most_frequent` always predicts the most frequent label in the training set.'
  id: totrans-26
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`most_frequent` 总是预测训练集中最频繁的标签。'
- en: '`prior` always predicts the class that maximizes the class prior (like [PRE0]
    returns the class prior.'
  id: totrans-27
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior` 总是预测最大化类别先验的类别（例如，[PRE0] 返回类别先验。'
- en: '`uniform` generates predictions uniformly at random.'
  id: totrans-28
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`uniform` 生成均匀随机的预测。'
- en: '`constant` always predicts a constant label that is provided by the user.'
  id: totrans-29
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`constant` 总是预测由用户提供的常量标签。'
- en: Baselines aren't just for classifiers, either; statistical methods exist for
    baselining regression tasks, for example.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 基准不仅仅适用于分类器；例如，存在用于基准回归任务的统计方法。
- en: After exploratory data analysis and data preparation and preprocessing, establishing
    a baseline is a logical next step in your machine learning workflow.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行探索性数据分析和数据准备及预处理之后，建立基准是机器学习工作流中的逻辑下一步。
- en: '3\. Validation: Beyond training and testing'
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3\. 验证：超越训练和测试
- en: When we build machine learning models, we train them using *training data*.
    When we test the resulting models, we use *testing data*. So where does validation
    come in?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们构建机器学习模型时，我们使用 *训练数据* 进行训练。当我们测试结果模型时，我们使用 *测试数据*。那么验证在哪里呢？
- en: '[Rachel Thomas](https://twitter.com/math_rachel) of [fast.ai](http://www.fast.ai/)
    recently wrote a solid treatment of [how and why to create good validation sets](/2017/11/create-good-validation-set.html).
    In it, she covered these 3 types of data as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[Rachel Thomas](https://twitter.com/math_rachel) 在 [fast.ai](http://www.fast.ai/)
    最近写了一篇关于 [如何及为何创建良好验证集]( /2017/11/create-good-validation-set.html) 的深入文章。在其中，她介绍了这三种数据类型：'
- en: the training set is used to train a given model
  id: totrans-35
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集用于训练给定模型
- en: the validation set is used to choose between models (for instance, does a random
    forest or a neural net work better for your problem? do you want a random forest
    with 40 trees or 50 trees?)
  id: totrans-36
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证集用于在模型之间进行选择（例如，随机森林还是神经网络更适合你的问题？你想要一个有 40 棵树还是 50 棵树的随机森林？）
- en: the test set tells you how you’ve done. If you’ve tried out a lot of different
    models, you may get one that does well on your validation set just by chance,
    and having a test set helps make sure that is not the case.
  id: totrans-37
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试集告诉你你的表现如何。如果你尝试了很多不同的模型，你可能会偶然间得到一个在验证集上表现良好的模型，而有一个测试集有助于确保这不是偶然的。
- en: '![under-fitting and over-fitting](../Images/628c051fa81b348e055c44294678c8d2.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![欠拟合和过拟合](../Images/628c051fa81b348e055c44294678c8d2.png)'
- en: Source: Andrew Ng's Machine Learning class at Stanford
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：斯坦福大学 Andrew Ng 的机器学习课程
- en: 'So, is randomly splitting your data into test, train, and validation sets always
    a good idea? As it turns out, no. Rachel addresses this in the context of time
    series data:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，将数据随机拆分为测试集、训练集和验证集总是一个好主意吗？事实证明，不一定。Rachel 在时间序列数据的背景下讨论了这一点：
- en: Kaggle currently has a competition to [predict the sales in a chain of Ecuadorian
    grocery stores](https://www.kaggle.com/c/favorita-grocery-sales-forecasting).
    Kaggle’s “training data” runs from Jan 1 2013 to Aug 15 2017 and the test data
    spans Aug 16 2017 to Aug 31 2017\. A good approach would be to use Aug 1 to Aug
    15 2017 as your validation set, and all the earlier data as your training set.
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Kaggle 目前有一个竞赛，旨在[预测厄瓜多尔连锁超市的销售情况](https://www.kaggle.com/c/favorita-grocery-sales-forecasting)。Kaggle
    的“训练数据”涵盖了 2013 年 1 月 1 日至 2017 年 8 月 15 日，而测试数据的时间跨度为 2017 年 8 月 16 日至 2017 年
    8 月 31 日。一个好的方法是将 2017 年 8 月 1 日至 8 月 15 日作为你的验证集，将所有早期的数据作为训练集。
- en: Much of the rest of the post relates dataset splitting to Kaggle competition
    data, which is practical information, as well as roping cross-validation into
    the discussion, which I will leave for you to seek out yourself.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其余部分的内容将数据集拆分与 Kaggle 竞赛数据相关联，这些是实用的信息，同时也涉及交叉验证的讨论，这部分我留给你自己去探索。
- en: Other times, random splits of data will be useful; it depends on further factors
    such as the state of the data when you get it (is it split into train/test already?),
    as well as what type of data it is (see the time series excerpt above).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其他时候，数据的随机拆分也会有用；这取决于你获取数据时的状态（数据是否已经被拆分为训练集/测试集？），以及数据的类型（见上面的时间序列摘录）。
- en: For when random splits **are** feasible, Scikit-learn may not have a `train_validate_test_split`
    method, but you can leverage standard Python libraries to create your own, [such
    as that which is found here](https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于随机拆分**可行**的情况，Scikit-learn 可能没有 `train_validate_test_split` 方法，但你可以利用标准 Python
    库来创建你自己的方法，[例如这里提到的](https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test)。
- en: 4\. There is more to ensembles than a bunch of trees
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4\. 集成方法不仅仅是树的集合
- en: Algorithm selection can be challenging for machine learning newcomers. Often
    when building classifiers, especially for beginners, an approach is adopted to
    problem solving which considers single instances of single algorithms.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习新手来说，算法选择可能是一个挑战。通常在构建分类器时，尤其是对于初学者，采用的方法会考虑单个算法的单个实例。
- en: However, in a given scenario, it may prove more useful to chain or group classifiers
    together, using the techniques of voting, weighting, and combination to pursue
    the most accurate classifier possible. Ensemble learners are classifiers which
    provide this functionality in a variety of ways.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在特定场景中，将分类器串联或分组在一起，利用投票、加权和组合的技术，以追求最准确的分类器可能会更有用。集成学习器就是以各种方式提供这种功能的分类器。
- en: Random Forests is a very prominent example of an ensemble learner, which uses
    numerous decision trees in a single predictive model. Random Forests have been
    applied to problems with great success, and are celebrated accordingly. But they
    are not the only ensemble method which exists, and numerous others are also worthy
    of a look.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一个非常突出的集成学习器的例子，它在单一预测模型中使用了大量的决策树。随机森林已经成功地应用于许多问题，并因此受到赞誉。但它们并不是唯一存在的集成方法，还有许多其他方法也值得一看。
- en: '**Bagging** operates by simple concept: build a number of models, observe the
    results of these models, and settle on the majority result. I recently had an
    issue with the rear axle assembly in my car: I wasn''t sold on the diagnosis of
    the dealership, and so I took it to 2 other garages, both of which agreed the
    issue was something different than the dealership suggested. *Voila*. Bagging
    in action. Random Forests are based on modified bagging techniques.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**Bagging** 的操作原理很简单：构建多个模型，观察这些模型的结果，然后选择多数结果。我最近在车的后轴组件上遇到了问题：我对经销商的诊断不太信服，于是把车送到了另外
    2 家车库，这两家车库都认为问题与经销商所说的不同。*瞧*。Bagging 在实际应用中。随机森林基于修改过的 bagging 技术。'
- en: '![Bagging](../Images/da14f46784242ccf6a49c3d7ea114504.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![Bagging](../Images/da14f46784242ccf6a49c3d7ea114504.png)'
- en: Bagging, or *bootstrap aggregation*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging，或*自助聚合*。
- en: '**Boosting** is similar to bagging, but with one conceptual modification. Instead
    of assigning equal weighting to models, boosting assigns varying weights to classifiers,
    and derives its ultimate result based on weighted voting.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**Boosting** 与 bagging 类似，但有一个概念上的修改。与给模型分配相等的权重不同，boosting 为分类器分配不同的权重，并根据加权投票得出最终结果。'
- en: Thinking again of my car problem, perhaps I had been to one particular garage
    numerous times in the past, and trusted their diagnosis slightly more than others.
    Also suppose that I was not a fan of previous interactions with the dealership,
    and that I trusted their insight less. The weights I assigned would be reflective.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑我的汽车问题，或许我曾经去过某个特定的车库多次，并且稍微更信任他们的诊断。此外，假设我对与经销商的先前互动不太满意，并且对他们的见解不太信任。我分配的权重将会反映这种情况。
- en: '**Stacking** is a bit different from the previous 2 techniques as it trains
    multiple single classifiers, as opposed to various incarnations of the same learner.
    While bagging and boosting would use numerous models built using various instances
    of the same classification algorithm (eg. decision tree), stacking builds its
    models using different classification algorithms (perhaps decision trees, logistic
    regression, an ANNs, or some other combination).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**Stacking** 与之前的两种技术略有不同，因为它训练多个单一分类器，而不是相同学习器的不同版本。虽然 bagging 和 boosting
    使用使用同一种分类算法（例如决策树）的多个模型，stacking 则使用不同的分类算法（可能是决策树、逻辑回归、人工神经网络或其他组合）来构建模型。'
- en: A combiner algorithm is then trained to make ultimate predictions using the
    predictions of other algorithms. This combiner can be any ensemble technique,
    but logistic regression is often found to be an adequate and simple algorithm
    to perform this combining. Along with classification, stacking can also be employed
    in unsupervised learning tasks such as density estimation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，训练一个组合算法来利用其他算法的预测做出最终预测。这个组合器可以是任何集成技术，但逻辑回归通常被发现是一种有效且简单的组合算法。除了分类，堆叠也可以应用于无监督学习任务，如密度估计。
- en: For some additional detail, [read this introduction to ensemble learners](/2016/11/data-science-basics-intro-ensemble-learners.html).
    You can read more on implementing ensembles in Python in [this very thorough tutorial](/2018/02/introduction-python-ensembles.html).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多细节，[请阅读这篇关于集成学习者的介绍](/2016/11/data-science-basics-intro-ensemble-learners.html)。你可以在[这篇非常详尽的教程](/2018/02/introduction-python-ensembles.html)中阅读更多关于在
    Python 中实现集成学习的内容。
- en: 5\. Google Colab?
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5\. Google Colab？
- en: Finally, let's look at something more practical. Jupyter Notebooks have become
    a *de facto* data science development tool, with most people running notebooks
    locally or via some other configuration-heavy method such as in Docker containers,
    or in a virtual machine. Google's Colaboratory is an initiative which allows for
    Jupyter-style and -compatible notebooks to be run directly in your Google Drive,
    free of configuration.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们来看一些更实用的内容。Jupyter Notebook 已经成为一个*事实上的*数据科学开发工具，大多数人都在本地或通过其他需要大量配置的方法（如
    Docker 容器或虚拟机）运行 Notebook。Google 的 Colaboratory 是一个可以直接在你的 Google Drive 中运行 Jupyter
    风格和兼容的 Notebook 的项目，无需配置。
- en: Colaboratory is pre-configured with a number of the most popular Python libraries,
    and more can be installed within the notebooks themselves thanks to supported
    package management. For instance, TensorFlow is included, but Keras is not, yet
    installing Keras via `pip` takes a matter of seconds.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Colaboratory 预配置了许多流行的 Python 库，并且可以在 Notebook 中安装更多库，这得益于支持的包管理。例如，TensorFlow
    已包含在内，但 Keras 尚未包含，然而通过 `pip` 安装 Keras 只需几秒钟。
- en: In what is likely the best news, if you are working with neural networks you
    can use GPU hardware acceleration in your training for free for up to 12 hours
    at a time. This isn't the panacea it may first seem to be, but it's an added bonus,
    and a good start to democratizing GPU access.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能是最好的消息中，如果你正在使用神经网络，你可以免费使用 GPU 硬件加速进行训练，每次最长可达 12 小时。这并不是一剂灵丹妙药，但它是一个额外的好处，也是实现
    GPU 访问民主化的良好开端。
- en: Read [3 Essential Google Colaboratory Tips & Tricks](/2018/02/essential-google-colaboratory-tips-tricks.html)
    for more information on how to take advantage of Colaboratory's notebooks in the
    cloud.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读 [3 个必备的 Google Colaboratory 提示与技巧](/2018/02/essential-google-colaboratory-tips-tricks.html)
    了解如何充分利用 Colaboratory 的云端笔记本。
- en: '**Related**:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关**：'
- en: '[5 Things You Need To Know About Data Science](/2018/02/5-things-about-data-science.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学你需要知道的 5 件事](/2018/02/5-things-about-data-science.html)'
- en: '[7 Steps to Mastering Machine Learning With Python](/2015/11/seven-steps-machine-learning-python.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握 Python 机器学习的 7 个步骤](/2015/11/seven-steps-machine-learning-python.html)'
- en: '[4 Things You Probably Didn’t Know Machine Learning and AI was used for](/2018/02/4-things-machine-learning-ai.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你可能不知道的 4 个机器学习和人工智能的应用](/2018/02/4-things-machine-learning-ai.html)'
- en: More On This Topic
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个数据科学家都应该了解的三个 R 语言库（即使你使用 Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学去寻找目标，然后去寻找目标……](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学统计学的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9 亿美元的人工智能失败案例分析](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功数据科学家的 5 个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[是什么让 Python 成为初创公司的理想编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
