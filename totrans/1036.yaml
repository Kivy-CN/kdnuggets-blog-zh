- en: Should The Data Warehouse Be Immutable?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/05/data-warehouse-immutable.html](https://www.kdnuggets.com/2022/05/data-warehouse-immutable.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*By [Barr Moses](https://www.linkedin.com/in/barrmoses/) & [Chad Sanderson](https://www.linkedin.com/in/chad-sanderson/)*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The data warehouse is the foundation of the modern data stack, so it caught
    my attention when I saw Convoy head of data [Chad Sanderson](https://www.linkedin.com/in/chad-sanderson/)
    declare, “[the data warehouse is broken](https://www.linkedin.com/posts/chad-sanderson_there-are-two-main-types-of-data-quality-activity-6909891078686953472-keb3?utm_source=linkedin_share&utm_medium=member_desktop_web)”
    on LinkedIn.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Of course, Chad isn’t referring to the technology, but how it’s being used.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: As he sees it, data quality and usability issues arise from the conventional
    best practice of “dumping” data in the warehouse to be manipulated and transformed
    afterward to fit the needs of the business.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: This is not out of line with the general efforts of providers like Snowflake
    and Databricks to ensure their customers are being efficient (in other words,
    saving money and resources) in their storage and consumption.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Whether or not you agree with Chad’s approach detailed below, what can’t be
    disputed is how his opinions have generated a tremendous amount of debate.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: “One camp is mad at me because they think this is nothing new and it requires
    long manual processes and data architects with 30 years of experience. The other
    camp is mad at me because their modern data stack is fundamentally not set up
    this way and it isn’t how they have been building out their data products,” said
    Chad.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: I’ll let you decide for yourself if the “immutable data warehouse” (or active
    vs passive ETL) is the right path for your data team.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Either way, I’m a strong proponent that moving our industry forward will require
    more than overviews of technologies, but frank discussions and unique perspectives
    on how to deploy them.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: How an Immutable Data Warehouse Combines Scale and Usability
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**A perspective From Chad Sanderson**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'The modern data stack has many permutations, but the data warehouse is a foundational
    component. To oversimplify:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Data is extracted via passive pipelines (really just the “E” in ETL) and dumped
    into…
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data warehouse where it is processed and stored before it is then…
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformed into the format needed by data consumers for…
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A specific use such as analytic dashboards, machine learning models, or activation
    in systems of records such as Salesforce or Google Analytics…
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With technologies or processes such as data observability, governance, discovery,
    and cataloging running across the stack.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before diving into the challenges of this approach, and a suggested alternative,
    it’s worth exploring how we arrived at what we define as “the modern data stack".
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: How Did we Get Here?
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the early days of data, with pioneers such as [Bill Inmon](https://www.linkedin.com/in/billinmon),
    the original ETL (extract, transform, load) process involved extracting from the
    source and transforming it before landing in the data warehouse.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Many businesses still operate this way today. For large companies where data
    quality is paramount, this process involves a manual, intensive governance framework
    with a tight coupling between data engineers and the data architects embedded
    across different domains in order to leverage data quickly for operational insights.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Tech giants like Google, Facebook, and others ditched this process and started
    dumping virtually everything in the data warehouse. The ROI of logically organizing
    the data just wasn’t as high for rapidly growing startups as this much faster,
    more scalable process. Not to mention, loading (the “L” in ELT) had become much
    easier to integrate in the cloud.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Along the way, popular transformation tools made transforming data in the warehouse
    easier than ever. Modular code and dramatically reduced runtimes made the ETL
    model radically less painful…so much so the use of popular transformation tools
    expanded downstream from data engineers to data consumers such as data scientists
    and analysts.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: It seemed like we had found a new best practice and we were on our way to a
    de facto standardization. So much so, that suggesting an alternative would generate
    swift and strong reactions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: The Challenge With Passive ETL or Transformations in the Warehouse
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![Should The Data Warehouse Be Immutable?](../Images/c2620b35aae8a950205844750a462b85.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: The modern data warehouse architecture creates problems across many layers.
    Image courtesy of Chad Sanderson.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: There are several problems with an architecture and process that heavily relies
    on transforming data once it has entered the data warehouse.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The first problem is the disconnect, really chasm, it creates between the data
    consumer (analysts/data scientists) and the data engineer.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: A project manager and a data engineer will build pipelines upstream from the
    analyst, who will be tasked with answering certain business questions from internal
    stakeholders. Inevitably, the analyst will discover that data will not answer
    all of their questions and that the program manager and data engineer have moved
    on.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: The second challenge arises when the analyst’s response is to go directly into
    the warehouse and write a brittle 600 line SQL query to get their answer. Or,
    a data scientist might find the only way they can build their model is to extract
    data from production tables which operate as the implementation details of services.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: The data in production tables are not intended for analytics or machine learning.
    In fact, service engineers often explicitly state NOT to take critical dependencies
    on this data considering it could change at any time. However, our data scientist
    needs to do their job so they do it anyway and when the table is modified everything
    breaks downstream.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: The third challenge is when your data warehouse is a dumping ground, it becomes
    a data junkyard.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: An [older Forrester study](https://www.forrester.com/blogs/hadoop-is-datas-darling-for-a-reason/)
    from the Hadoop era found between 60% and 73% of all data within an enterprise
    goes unused for analytics. A more [recent Seagate study](https://www.seagate.com/news/news-archive/seagates-rethink-data-report-reveals-that-68-percent-of-data-available-to-businesses-goes-unleveraged-pr-master/)
    found 68% of data available to the enterprise goes unused.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: As a result, data scientists and analysts spend too much of their time searching
    for context in an overly processed production code haystack. As data engineers,
    we need to emphasize data *usability* in addition to data quality.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: If your users can’t find and leverage what they need reliably in your current
    data warehouse, what’s the point?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'Another Approach: Introducing the Immutable Data Warehouse'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The immutable data warehouse concept (also referred to as active ETL) holds
    that the warehouse should be a representation of the real world through the data
    instead of a tangled mess of random queries, broken pipelines, and duplicated
    information.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'There are five core pillars:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '**The business is mapped and owners are assigned**. For businesses to truly
    gain value from the massive amounts of data they possess, teams need to take a
    step back and model their business semantically before defining entities and events
    through code for the express purpose of analytics. This can be an iterative process
    starting with the most crucial elements of the business.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An entity relationship diagram (ERD) is a map of the business based on the REAL
    world, not what exists in the data warehouse or production databases today. It
    defines the critical entities, their relationships (cardinality, etc), and the
    real world actions that indicate they have interacted. An engineering owner is
    established for each entity and event. End-to-end automated lineage can help establish
    the ERD and make it actionable.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Data consumers define their needs upfront and contracts are created**. Perhaps
    the most controversial tenant is that data should bubble up from business needs
    instead of trickle down from unstructured pipelines. Instead of data analysts
    and scientists combing through the dusty shelves of your warehouse to see if there
    is a data set *close enough* to what they need, no data enters the warehouse unless
    it is directly requested and defined by the data consumer first.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No data enters the warehouse without a business question, process, or problem
    driving it. Everything is purpose built for the task to be done.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It’s essential this process is designed to be simple as data needs are always
    changing and increased friction will threaten adoption. At Convoy, implementing
    a new contract takes minutes to hours not days to weeks.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, it’s time to draw up the data contract, which is an agreement between
    the business and engineering leads about what the schema of an event/entity should
    be and the data that is needed most for that asset to be most effective. For example,
    perhaps an existing inboundCall event is missing an OrderID which makes it difficult
    to tie phone calls to completed orders.
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: SLAs, SLIs, and SLOs are one type of [data contract you can apply](https://www.montecarlodata.com/blog-one-sla-at-a-time-our-data-quality-journey-at-red-digital/)
    to this model of change management and stakeholder alignment.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Peer reviewed documentation within an active environment**. In the same way
    we need a peer review process for code (GitHub) or UX (Figma) that hits production,
    there should be an equivalent for data assets. However, the right level of abstraction
    for this review is not code - but semantics.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That review process should have the same outcome as a GitHub pull request -
    version control, sign-off of relevant parties, etc – all handled through the cloud.
    By applying modern, cloud based technologies we can speed up old processes making
    them far more viable forevent the fastest growing internet businesses.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There is a place for data catalogs as a pre-data warehouse definition surface,
    but the challenge is there is no carrot and no stick for data consumers to keep
    metadata current. What is the incentive for a data scientist that uses an ELT
    process and finishes their model to go back and document their work?
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Data is piped into the warehouse pre-modeled as defined in the contract.
    Transformations take place upstream from the consumption layer (ideally in the
    service)**. Engineers then implement the data contracts within their service.
    The data is piped into the data warehouse, and with the modeling metadata can
    ideally be automatically JOINed and categorized.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**An emphasis is placed on preventing data loss as well as ensuring data observability,
    integrity, usability, and lifecycle management**. A transactional outbox pattern
    is used to ensure the events in the production system match what’s in the data
    warehouse while a log and offset processing pattern ([which we use extensively
    at Convoy](https://medium.com/convoy-tech/logs-offsets-near-real-time-elt-with-apache-kafka-snowflake-473da1e4d776))
    protects against data loss. Together, these two systems ensure data is preserved
    with complete integrity so the immutable data warehouse is a direct representation
    and source of truth for what is occurring across the business.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data quality and usability require two different mindsets. Data quality is a
    technical challenge for the most part. Think “back-end” engineering. Data usability
    on the other hand, is a “front-end” engineering challenge that requires the same
    skill set used to create great customer experiences.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, an immutable data warehouse does not lend itself to petabyte measuring
    contests and whipping out your big data stats. Deprecation and maintenance is
    just as important as provisioning.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: This approach leverages the advantages of technology to achieve the best of
    both worlds. The governance and business driven approach of traditional approaches
    with the speed and scalability associated with the modern data stack.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: How an Immutable Data Warehouse Works, Treating Data Like an API
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![The layers of an immutable data warehouse. Image courtesy of Chad Sanderson](../Images/ae0492666de9c033ea5fb8ebd72ee630.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: The layers of an immutable data warehouse. Image courtesy of Chad Sanderson
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by reviewing the full stack surrounding the immutable data warehouse.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '**Descriptive Layer:** Unlike traditional warehouses, a descriptive layer moves
    the business logic above the services layer and puts the data consumer in the
    driver''s seat. The consumer is able to provide their requirements without the
    need for technical skills as the data engineer serves as a crucial requirement
    to code translator. These contracts can be held in a data catalog or even a general
    document repository.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data Warehouse:** The warehouses function primarily as a ''data showcase''
    and the underlying compute layer.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Semantic Layer:** Data consumers build data products that are validated and
    shared with the business. Assets in the semantic layer should be defined, versioned,
    reviewed, and then made available through an API for consumption in the application
    layer.'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Application Layer:** This is where data is used to accomplish some business
    function, such as experimentation, machine learning, or analytics.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**End-To-End Support:** Solutions that support data operations across the data
    stack such as data observability, catalogs, testing, governance, and more. The
    ideal is to have perfect, pre-modeled, highly reliable data once it hits the warehouse,
    but you still need to cover all the permutations the real world may throw at you
    (and have enforcement mechanisms when processes move out of bounds).'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The immutable data warehouse itself is designed for streaming – it’s easier
    to go from streaming to batch data than vice versa–and therefore fed by three
    different types of APIs.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![The immutable data warehouse itself is designed for streaming](../Images/cf41cedec61927c4766c0816aeb2f410.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: Image courtesy of Monte Carlo.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '**Semantic Events API**: This API is for semantic real world service level
    events that are the core building blocks of the company, not events from front
    end applications. For example, in Convoy’s case this could be when a shipment
    is created or goes off hold. Events from the real world are built in the service
    code, not SQL queries.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CRUD Abstraction API:** Data consumers do not need to see all production
    tables, particularly when they’re simply implementation details of the data service
    they’re using to generate insights or power decision making. Instead, when properties
    of a data asset in a production table are updated, , an API wrapper or abstraction
    layer (for instance, dbt)  will expose the CRUD concepts that are meaningful to
    data consumers in the warehouse - for instance, whether or not the data is fresh
    or row volume is within expected thresholds.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Frontend API**: There are many tools that already handle front-end event
    definition and emission like Snowplow, Mixpanel, and Amplitude. That being said,
    some front-end events are important enough that teams need to be able to ensure
    their delivery and integrity using the long offset pipeline. In some cases, front-end
    events are critical for machine learning workflows and “close enough” systems
    just won’t cut it.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There also needs to be a mapping layer that sits outside of the warehouse as
    things change (perhaps one service needs to become many) or if a schema a data
    scientist has in mind does not fit with what is happening in the real world. Mapping
    should be handled either upstream of the warehouse through a streaming database
    or in the warehouse itself. This layer is where a BI engineer matches what is
    coming up from engineering to what a data consumer needs, which can be automated
    to produce [Kimball data marts](https://www.astera.com/type/blog/data-warehouse-concepts/).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Immutable Data Warehouses Have Challenges Too, Here Are Some Possible Solutions
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I am under no delusion that an immutable data warehouse is a silver bullet.
    Like any approach it has its pros and cons, and is certainly not for every organization.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Like the data mesh and other lofty data architectural initiatives, the immutable
    data warehouse is an ideal state and rarely the reality. Achieving one - or attempting
    to achieve one - is a journey and not a destination.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'Challenges that should be considered and mitigated are:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: The upfront costs of defining the descriptive layer
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling entities without clear ownership in place
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing new methods to enable rapid experimentation
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While there is a cost to defining the descriptive layer, it can be greatly accelerated
    through software and done iteratively by prioritizing the most important business
    components.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: This needs to be a collaborative design effort that includes data engineers
    to prevent the diffusion of data quality responsibility across distributed data
    consumers. It’s OK if you don’t get it right the first time, this is an iterative
    process.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Handling entities without clear ownership can be a tricky governance problem
    (and one that is frequently bedeviling data mesh proponents). It’s not typically
    in the data team’s purview to sort these issues on the business side.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: If there is a core business concept that crosses multiple teams and is generated
    by a monolith rather than microservice, the best way forward is to have a strong
    review system in place and a dedicated team standing by to make changes.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Data engineers can still be allowed to experiment and given flexibility without
    limiting workflow. One way to do this would be through a separate staging layer.
    However, API data from these staging areas should not be allowed to be consumed
    downstream or across external teams.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: The key is that when you move from experiment to production or make it accessible
    to the border team, it must go through the same review process. Just like in software
    engineering, you can’t make a code change without a review process just because
    you want to move faster.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Wishing You Luck on Your Data Quality Journey
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many permutations of the modern data stack, and as an industry, we’re
    still going through an experimentation phase to understand how to best lay our
    data infrastructure.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: What’s clear is that we are rapidly moving toward a future where more mission
    critical, external facing, and sophisticated products are” powered by” the data
    warehouse.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the chosen approach, this will require us as data professionals
    to raise our standards and redouble our efforts toward reliable, scalable, usable
    data. Data quality must be at the heart of all data warehouses, no matter the
    type.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'The bottom line from my perspective: when you build on a large, amorphous foundation,
    stuff breaks and it’s hard to find. And when you do find it, it can be hard to
    figure out exactly what that “thing” is.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Immutable or not, maybe it’s time we try something new.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[KDnuggets™ News 22:n03, Jan 19: A Deep Look Into 13 Data…](https://www.kdnuggets.com/2022/n03.html)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, May 25: The 6 Python Machine Learning Tools Every…](https://www.kdnuggets.com/2022/n21.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，5月25日：每个…都需要的6个Python机器学习工具](https://www.kdnuggets.com/2022/n21.html)'
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个初学者数据科学家都应该掌握的6个预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
- en: '[3 Reasons Why Data Scientists Should Use LightGBM](https://www.kdnuggets.com/2022/01/data-scientists-reasons-lightgbm.html)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学家应使用LightGBM的3个理由](https://www.kdnuggets.com/2022/01/data-scientists-reasons-lightgbm.html)'
- en: '[Top 13 Skills That Every Data Scientist Should Have](https://www.kdnuggets.com/2022/03/top-13-skills-every-data-scientist.html)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个数据科学家都应该具备的13项技能](https://www.kdnuggets.com/2022/03/top-13-skills-every-data-scientist.html)'
- en: '[Python Libraries Data Scientists Should Know in 2022](https://www.kdnuggets.com/2022/04/python-libraries-data-scientists-know-2022.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2022年数据科学家应该了解的Python库](https://www.kdnuggets.com/2022/04/python-libraries-data-scientists-know-2022.html)'
