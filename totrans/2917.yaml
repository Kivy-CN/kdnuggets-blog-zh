- en: 'Bayesian deep learning and near-term quantum computers: A cautionary tale in
    quantum machine learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯深度学习与近期量子计算机：量子机器学习中的警示故事
- en: 原文：[https://www.kdnuggets.com/2019/07/bayesian-deep-learning-near-term-quantum-computers.html](https://www.kdnuggets.com/2019/07/bayesian-deep-learning-near-term-quantum-computers.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/07/bayesian-deep-learning-near-term-quantum-computers.html](https://www.kdnuggets.com/2019/07/bayesian-deep-learning-near-term-quantum-computers.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Peter Wittek](https://peterwittek.com/) (Edited by Amir Feizpour & Nick
    Morrison)**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由[彼得·维特克](https://peterwittek.com/)（阿米尔·费兹普尔和尼克·莫里森编辑）**'
- en: '*This blog post is an overview of quantum machine learning written by the author
    of the paper [Bayesian deep learning on a quantum computer](https://link.springer.com/article/10.1007%2Fs42484-019-00004-7).
    In it, we explore the application of machine learning in the quantum computing
    space. The authors of this paper hope that the results of the experiment help
    influence the future development of quantum machine learning.*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*这篇博客文章是由[《量子计算机上的贝叶斯深度学习》](https://link.springer.com/article/10.1007%2Fs42484-019-00004-7)一文的作者撰写的量子机器学习概述。文章探讨了量子计算领域中的机器学习应用。本文的作者希望实验结果能够影响量子机器学习的未来发展。*'
- en: With no shortage of research problems, education programs, and demand for talent,
    machine learning is one of the hottest topics in technology today. Parallel to
    the success of learning algorithms, the development of quantum computing hardware
    has accelerated over the last few years. In fact, we are at the threshold of achieving
    a quantum advantage, that is, a speed or other performance boost over classical
    algorithms, in certain specific application areas – [machine learning](https://en.wikipedia.org/wiki/Quantum_machine_learning) being
    one of them. This sounds exciting, but don’t ditch your GPU cluster just yet;
    quantum computers and your parallel processing units solve different classes of
    problems.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 由于研究问题、教育项目和人才需求不断增加，机器学习成为当今技术领域最热门的话题之一。与学习算法的成功相并行的是，量子计算硬件的发展在过去几年中加速了。实际上，我们正处于实现量子优势的门槛上，也就是在某些特定应用领域（例如[机器学习](https://en.wikipedia.org/wiki/Quantum_machine_learning)）相较于经典算法获得速度或其他性能提升。这听起来很令人兴奋，但不要急于丢弃你的GPU集群；量子计算机和你的并行处理单元解决的是不同类别的问题。
- en: 'If you look at the figure below, you’ll get a quick idea of how to think about
    the role of quantum computing in machine learning. As the complexities of the
    problems to be solved have grown, AI stacks have begun to include more and more
    different hardware back-ends, including anything from off-the-shelf CPUs to Tensor
    Processing Units (TPUs) and neuromorphic chips. Yet, there are still problems
    that remain too difficult to solve: some of these are amenable to quantum computing.
    In other words, quantum computers can enable the creation of more complex machine
    learning and deep learning models in the same way that GPUs have done in the past.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看下图，你将快速了解如何思考量子计算在机器学习中的角色。随着待解决问题复杂性的增长，人工智能堆栈已经开始包含越来越多不同的硬件后端，包括从现成的CPU到张量处理单元（TPUs）和神经形态芯片。然而，仍然存在一些问题过于复杂而难以解决，其中一些问题适合用量子计算解决。换句话说，量子计算机可以像过去的GPU一样，使创建更复杂的机器学习和深度学习模型成为可能。
- en: '![](../Images/d054a7663750a7b89adfcca616e64068.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d054a7663750a7b89adfcca616e64068.png)'
- en: '***Figure 1.** AI already uses a heterogeneous mix of hardware back-ends. In
    the future, this blend will incorporate quantum technologies that enable certain
    learning algorithms. Based on a [talk](http://www.marketforintelligence.com/talks/why-will-machine-intelligence-be-so-transformational/) by
    Steve Jurvetson at the Machine Learning and the Market for Intelligence Conference.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '***图1。** 人工智能已经使用了异构的硬件后端组合。在未来，这种组合将包括量子技术，这些技术能够支持某些学习算法。基于斯蒂夫·朱尔维特森在机器学习与智能市场会议上的[讲座](http://www.marketforintelligence.com/talks/why-will-machine-intelligence-be-so-transformational/)。*'
- en: 'A quantum computer uses “quantum bits” (qubits): a qubit is a special probability
    distribution between 0 and 1\. The number of qubits and the quality (the fidelity)
    of the operations that we can perform are the most important indicators of how
    good a quantum computer is. Quantum machine learning uses quantum computers to
    run certain parts of a learning algorithm. The near-term feasibility of the various
    quantum machine learning proposals varies. Some quantum algorithms can already
    be run on current quantum hardware (e.g., algorithms that need few qubits and
    gates, so-called variational algorithms), others need better quantum computers,
    with a lot more and higher quality qubits (such as the most famous algorithms
    like factorization by Shor’s algorithm or unstructured search by Grover’s algorithm).
    So far, only a handful of benchmark results are available for running actual quantum
    machine learning algorithms on real hardware.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 量子计算机使用“量子比特”（qubits）：量子比特是0和1之间的特殊概率分布。量子比特的数量和我们可以执行的操作的质量（保真度）是衡量量子计算机优劣的最重要指标。量子机器学习使用量子计算机来运行学习算法的某些部分。各种量子机器学习提案的近期可行性各不相同。一些量子算法已经可以在当前量子硬件上运行（例如，需要少量量子比特和量子门的算法，称为变分算法），其他算法则需要更好的量子计算机，具有更多且质量更高的量子比特（如最著名的算法如Shor算法的因式分解或Grover算法的无结构搜索）。到目前为止，仅有少数基准结果可用于在真实硬件上运行实际的量子机器学习算法。
- en: 'All of these benchmarks focus on the more obvious classes of algorithms that
    were designed for these noisy, early quantum devices. We were interested in the
    other extreme: let''s take one of the hardest algorithmic primitives (algorithms
    where all the steps are basic operations and do not make calls to other algorithms)
    and see what kind of results we can get. Why? A well-known algorithm is the [quantum
    matrix inversion algorithm](https://en.wikipedia.org/wiki/Quantum_algorithm_for_linear_systems_of_equations) (HHL
    for short after the authors of the [original article](https://arxiv.org/abs/0811.3171)),
    which offers an exponential speedup in matrix inversion compared to classical
    algorithms, is one of the most common algorithmic primitives in quantum machine
    learning. We believed that there were advantages in using quantum computers, but
    we also believed that we had to factor in what the current quantum hardware is
    capable of. So we set out on a quest to show how bad things can get.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些基准测试都专注于那些为这些嘈杂、早期量子设备设计的更明显的算法类别。我们则关注另一个极端：让我们选择一个最困难的算法原语（所有步骤都是基本操作，并且不调用其他算法），看看我们能得到什么结果。为什么？一个著名的算法是[量子矩阵求逆算法](https://en.wikipedia.org/wiki/Quantum_algorithm_for_linear_systems_of_equations)（简称HHL，取自[原始文章](https://arxiv.org/abs/0811.3171)的作者），它在矩阵求逆方面相比经典算法提供了指数级的加速，是量子机器学习中最常见的算法原语之一。我们相信使用量子计算机有其优势，但我们也认为必须考虑当前量子硬件的能力。因此，我们开始了一项展示情况可能变得多么糟糕的任务。
- en: 'Countless caveats apply with the quantum matrix inversion algorithm: it requires
    quantum circuits that have thousands of [error-corrected qubits](https://en.wikipedia.org/wiki/Quantum_error_correction) and
    an even larger number of gates. In other words, it needs a perfect, large-scale,
    universal quantum computer. This is a stark contrast to the handful of noisy qubits
    we have today and the shallow circuits we can build given the short times the
    qubits can maintain superpositions (also known as coherence time). The superposition
    is the special probability distribution between 0 and 1 that we mentioned earlier.
    If the coherence time is bad, the probability distribution is not under our control:
    we cannot perform transformations on it to express a calculation.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 量子矩阵求逆算法存在无数的警告：它需要数千个[错误校正量子比特](https://en.wikipedia.org/wiki/Quantum_error_correction)和更多的量子门。换句话说，它需要一个完美的大规模通用量子计算机。这与我们今天拥有的少量嘈杂量子比特和由于量子比特只能维持短时间的叠加状态（也称为相干时间）而能够构建的浅层电路形成鲜明对比。叠加状态是我们前面提到的0和1之间的特殊概率分布。如果相干时间不好，概率分布不在我们的控制之下：我们无法对其进行变换以表达计算。
- en: 'Given that quantum matrix inversion is a common subroutine in quantum machine
    learning and it requires very high-quality quantum computers, we wanted to tell
    a cautionary tale. We found an opportunity: [two](https://arxiv.org/abs/1711.00165) [papers](https://openreview.net/forum?id=H1-nGgWC-) appeared
    in late 2017 that made a connection between deep learning and Gaussian processes.
    This is an important feat since combining deep learning with Bayesian techniques
    (of which Gaussian processes is an example) allows for getting much more information
    from the network. In particular, it provides easy ways of knowing the uncertainty
    associated with a prediction. Unfortunately, Bayesian learning is hard, even with
    the best classical hardware. That is why, exploiting this new-found connection,
    we came up with a new quantum Bayesian deep learning protocol that internally
    used quantum matrix inversion for achieving a speedup. On the surface, the new
    protocol looks good: it is a problem that is hard to solve classically, we applied
    some mathematical tricks, and we could give it a significant lift in speed by
    using a quantum algorithm! Unfortunately, this speedup requires very, very high-quality
    quantum computers.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于量子矩阵反演是量子机器学习中的常见子程序，并且它需要非常高质量的量子计算机，我们想要讲述一个警示故事。我们发现了一个机会：在2017年末出现了[two](https://arxiv.org/abs/1711.00165)
    [papers](https://openreview.net/forum?id=H1-nGgWC-) ，它们将深度学习与高斯过程联系了起来。这是一个重要的成就，因为将深度学习与贝叶斯技术（其中高斯过程是一个例子）结合起来，可以从网络中获取更多信息。特别是，它提供了了解预测不确定性的简便方法。不幸的是，贝叶斯学习是困难的，即使在最好的经典硬件上也是如此。这就是为什么利用这一新发现的联系，我们提出了一种新的量子贝叶斯深度学习协议，该协议内部使用量子矩阵反演来实现加速。从表面上看，新协议看起来不错：这是一个在经典计算中很难解决的问题，我们应用了一些数学技巧，通过使用量子算法使其速度显著提升！不幸的是，这种加速需要非常非常高质量的量子计算机。
- en: We used this opportunity to implement the quantum matrix inversion algorithm
    and benchmark it on contemporary quantum computers. The corresponding [paper](https://doi.org/10.1007/s42484-019-00004-7) has
    just been published (the open version can be found on [arXiv](https://arxiv.org/abs/1806.11463)),
    with a matching [git repository](https://gitlab.com/apozas/bayesian-dl-quantum/).
    The figure below gives an overview of what we proposed.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用这个机会实现了量子矩阵反演算法，并在现代量子计算机上进行了基准测试。相关的[论文](https://doi.org/10.1007/s42484-019-00004-7)刚刚发表（开放版可以在[arXiv](https://arxiv.org/abs/1806.11463)上找到），还有一个匹配的[git
    仓库](https://gitlab.com/apozas/bayesian-dl-quantum/)。下图概述了我们提出的内容。
- en: '![](../Images/94f7e58948478db9b352e773c1572538.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94f7e58948478db9b352e773c1572538.png)'
- en: '***Figure 2.** Schematic overview of quantum Bayesian deep learning. A layer
    of neurons in a neural network can be considered equivalent to a Gaussian process
    under certain conditions. Recent work showed that the equivalence still holds
    if we consider many layers. To train the Gaussian process fast, we use the quantum-assisted
    protocol and run it on a quantum processing unit. Image credit: Alejandro Pozas-Kerstjens.*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '***图 2.** 量子贝叶斯深度学习的示意图。在某些条件下，神经网络中的一层神经元可以被视为等同于高斯过程。最近的研究表明，如果我们考虑许多层，这种等价关系仍然成立。为了快速训练高斯过程，我们使用了量子辅助协议，并在量子处理单元上运行它。图像来源：Alejandro
    Pozas-Kerstjens。*'
- en: The results confirmed that despite its widespread usage, quantum matrix inversion
    is not the first target for achieving a real-world quantum advantage. Any gate
    noise immediately destroys the calculations, which is intuitive, since we need
    dozens of gates for even a small matrix inversion. Reading out the result of a
    quantum computation is done through measurements on the device. Faults in the
    devices measuring the qubits also affect the performance of the algorithm, however,
    as there are few measurements to be done, this source of error has a smaller impact.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 结果证实了尽管量子矩阵反演被广泛使用，但它并不是实现实际量子优势的首选目标。任何门噪声都会立即破坏计算，这很直观，因为即使是一个小的矩阵反演也需要几十个门。量子计算结果的读取是通过对设备的测量来完成的。测量量子比特的设备中的故障也会影响算法的性能，不过，由于需要进行的测量较少，这种误差源的影响较小。
- en: '![](../Images/9a245ce50eaf60da17d2115aea824607.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a245ce50eaf60da17d2115aea824607.png)'
- en: '***Figure 3.** Fidelity of final state after inverting a 4x4 matrix. Simulations
    using different noise levels and noise models.*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '***图 3.** 反转 4x4 矩阵后的最终状态的保真度。使用不同噪声水平和噪声模型进行的模拟。*'
- en: 'To run on real hardware, we needed a simplification, because the full-blown
    algorithm was too deep even for a 2x2 matrix. Fortunately, [we found just what
    we needed](https://arxiv.org/abs/1110.2232): a shallow circuit specifically designed
    for inverting 2x2 matrices. While this is not relevant for any practical application,
    this circuit has slightly over a dozen gates (including state preparation) and
    interesting results. We used both the [16-qubit IBM chip](https://www.research.ibm.com/ibm-q/technology/devices/#ibmq_16_melbourne) and
    the [8-qubit Agave chip](https://medium.com/rigetti/how-to-write-a-quantum-program-in-10-lines-of-code-for-beginners-540224ac6b45) by
    Rigetti. The Agave chip has a specific linear topology, that introduces a handful
    of extra gates for operating the quantum state as desired. The IBM QPU ran with
    an 89% success rate after measurement, which translates to about 0.78 fidelity
    with the correct target state. The Agave QPU achieved a 50% success rate, which
    translates to zero fidelity, but we believe this is mainly because of the linear
    topology and the extra gates that need to be inserted. These are not many, but
    enough to make the execution of the algorithm take longer than the coherence time
    of the qubits.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在真实硬件上运行，我们需要简化，因为完整算法即使对2x2矩阵来说也过于深奥。幸运的是，[我们找到了所需的解决方案](https://arxiv.org/abs/1110.2232)：一个专门为求逆2x2矩阵设计的浅层电路。虽然这对任何实际应用无关，但这个电路有略多于十二个门（包括状态准备）和有趣的结果。我们使用了[16量子比特的IBM芯片](https://www.research.ibm.com/ibm-q/technology/devices/#ibmq_16_melbourne)和[8量子比特的Agave芯片](https://medium.com/rigetti/how-to-write-a-quantum-program-in-10-lines-of-code-for-beginners-540224ac6b45)。Agave芯片具有特定的线性拓扑，增加了一些额外的门来按需操作量子状态。IBM
    QPU在测量后成功率为89%，相当于0.78的保真度。Agave QPU的成功率为50%，相当于零保真度，但我们认为这主要是由于线性拓扑和需要插入的额外门。这些并不多，但足以使算法的执行时间超过量子比特的相干时间。
- en: There is no point in creating a QML algorithm for something that can be efficiently
    done on a GPU cluster. As Bayesian deep learning is difficult to execute when
    using classical resources, we attacked it with quantum protocols. With this theory,
    we have made non-trivial advances towards allowing quantum computers to train
    deep neural networks.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量子机器学习（QML）算法而创建的内容，如果在GPU集群上可以有效完成，则没有意义。由于贝叶斯深度学习在使用经典资源时很难执行，我们通过量子协议进行了攻击。凭借这一理论，我们在允许量子计算机训练深度神经网络方面取得了非平凡的进展。
- en: On the implementation side, we have established a complete implementation of
    the quantum matrix inversion algorithm, which can serve as a hard benchmark for
    future quantum computers. We concluded that it is better to stay away from these
    abstract algorithms and pay more attention to the limits of the quantum hardware
    of today. We hope that the classical and the quantum machine learning communities
    will find the results intriguing and that the next generation of quantum coders
    can learn from this experiment.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现方面，我们建立了量子矩阵求逆算法的完整实现，这可以作为未来量子计算机的一个严格基准。我们得出结论，最好远离这些抽象算法，更多关注今天量子硬件的局限性。我们希望经典和量子机器学习社区能发现这些结果具有趣味性，并且希望下一代量子编码者能够从这次实验中学习。
- en: '[Original](https://aisc.ai.science/blog/2019/bayesian-deep-learning-and-near-term-quantim-computers/).
    Reposted with permission.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://aisc.ai.science/blog/2019/bayesian-deep-learning-and-near-term-quantim-computers/)。经许可转载。'
- en: '**Related:**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Pre-training, Transformers, and Bi-directionality](/2019/07/pre-training-transformers-bi-directionality.html)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[预训练、变压器和双向性](/2019/07/pre-training-transformers-bi-directionality.html)'
- en: '[GANs Need Some Attention, Too](/2019/03/gans-need-some-attention-too.html)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[生成对抗网络也需要关注](/2019/03/gans-need-some-attention-too.html)'
- en: '[Large-Scale Evolution of Image Classifiers](/2019/05/large-scale-evolution-image-classifiers.html)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大规模图像分类器的演变](/2019/05/large-scale-evolution-image-classifiers.html)'
- en: '* * *'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前3课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全领域的职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析水平'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的 IT 需求'
- en: '* * *'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成为优秀数据科学家所需的 5 个关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个初学者数据科学家应该掌握的 6 种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2021 年最佳 ETL 工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
- en: '[There and Back Again… a RAPIDS Tale](https://www.kdnuggets.com/2023/06/back-again-rapids-tale.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[前往又回来的…一个 RAPIDS 故事](https://www.kdnuggets.com/2023/06/back-again-rapids-tale.html)'
- en: '[Reinforcement Learning: Teaching Computers to Make Optimal Decisions](https://www.kdnuggets.com/2023/07/reinforcement-learning-teaching-computers-make-optimal-decisions.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[强化学习：教计算机做出最佳决策](https://www.kdnuggets.com/2023/07/reinforcement-learning-teaching-computers-make-optimal-decisions.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，而是通过寻找目标…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
