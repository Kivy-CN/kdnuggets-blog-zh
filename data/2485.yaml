- en: How to Speed Up XGBoost Model Training
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何加速 XGBoost 模型训练
- en: 原文：[https://www.kdnuggets.com/2021/12/speed-xgboost-model-training.html](https://www.kdnuggets.com/2021/12/speed-xgboost-model-training.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/12/speed-xgboost-model-training.html](https://www.kdnuggets.com/2021/12/speed-xgboost-model-training.html)
- en: '**By [Michael Galarnyk](https://www.linkedin.com/in/michaelgalarnyk/), Data
    Science Professional**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：[Michael Galarnyk](https://www.linkedin.com/in/michaelgalarnyk/)，数据科学专业人士**'
- en: Gradient boosting algorithms are widely used in supervised learning. While they
    are powerful, they can take a long time to train. Extreme gradient boosting, or [XGBoost](https://xgboost.readthedocs.io/en/latest/),
    is an open-source implementation of gradient boosting designed for speed and performance.
    However, even XGBoost training can sometimes be slow.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升算法广泛应用于监督学习。虽然它们功能强大，但训练时间可能很长。极端梯度提升，或称 [XGBoost](https://xgboost.readthedocs.io/en/latest/)，是一个开源的梯度提升实现，旨在提升速度和性能。然而，即使是
    XGBoost 的训练有时也会很慢。
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业领域。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的 IT'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'There are quite a few approaches to accelerating this process like:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多方法可以加速这个过程，例如：
- en: Changing tree construction method
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改树构建方法
- en: Leveraging cloud computing
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用云计算
- en: '[Distributed XGBoost on Ray](https://docs.ray.io/en/latest/xgboost-ray.html#:~:text=XGBoost%2DRay%20integrates%20with%20Ray,training%20run%20parallelized%20by%20itself.)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Ray 上的分布式 XGBoost](https://docs.ray.io/en/latest/xgboost-ray.html#:~:text=XGBoost%2DRay%20integrates%20with%20Ray,training%20run%20parallelized%20by%20itself.)'
- en: This article will review the advantages and disadvantages of each approach as
    well as go over how to get started.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将回顾每种方法的优缺点，并介绍如何入门。
- en: Changing your tree construction algorithm
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更改你的树构建算法
- en: XGBoost’s `tree_method` parameter allows you to specify which tree construction
    algorithm you want to use. Choosing an appropriate tree construction algorithm
    (`exact`, `approx`, `hist`, `gpu_hist`, `auto`) for your problem can help you
    produce an optimal model faster. Let’s now review the algorithms.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 的 `tree_method` 参数允许你指定要使用的树构建算法。为你的问题选择合适的树构建算法（`exact`、`approx`、`hist`、`gpu_hist`、`auto`）可以帮助你更快地生成优化模型。现在让我们来回顾这些算法。
- en: '[**exact**](https://xgboost.readthedocs.io/en/latest/treemethod.html#exact-solution)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[**exact**](https://xgboost.readthedocs.io/en/latest/treemethod.html#exact-solution)'
- en: It is an accurate algorithm, but it is not very scalable as during each split
    find procedure it iterates over all entries of input data. In practice, this means
    long training times. It also doesn’t support distributed training. You can learn
    more about this algorithm in the original [XGBoost paper](https://arxiv.org/abs/1603.02754).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个准确的算法，但在每次分裂查找过程中，它会遍历所有输入数据条目，因此不具有很好的扩展性。在实际应用中，这意味着较长的训练时间。它也不支持分布式训练。你可以在原始的
    [XGBoost 论文](https://arxiv.org/abs/1603.02754) 中了解更多关于该算法的信息。
- en: '[**approx**](https://xgboost.readthedocs.io/en/stable/treemethod.html#approximated-solutions)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[**approx**](https://xgboost.readthedocs.io/en/stable/treemethod.html#approximated-solutions)'
- en: While the exact algorithm is accurate, it is inefficient when the data does
    not completely fit into memory. The approximate tree method from the original [XGBoost
    paper](https://arxiv.org/abs/1603.02754) uses quantile sketch and gradient histograms.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管准确算法是准确的，但当数据不完全适合内存时，它的效率较低。原始 [XGBoost 论文](https://arxiv.org/abs/1603.02754)
    中的近似树方法使用分位数草图和梯度直方图。
- en: '[**hist**](https://xgboost.readthedocs.io/en/stable/treemethod.html#approximated-solutions)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[**hist**](https://xgboost.readthedocs.io/en/stable/treemethod.html#approximated-solutions)'
- en: An approximation tree method used in [LightGBM](https://lightgbm.readthedocs.io/en/latest/) with
    slight differences in implementation (uses some performance improvements such
    as bins caching) from `approx`. This is typically faster than `approx`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [LightGBM](https://lightgbm.readthedocs.io/en/latest/) 中使用的近似树方法，与 `approx`
    的实现有些许不同（例如使用一些性能改进，如bins缓存）。这通常比 `approx` 更快。
- en: '[**gpu_hist**](https://xgboost.readthedocs.io/en/stable/treemethod.html#approximated-solutions)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[**gpu_hist**](https://xgboost.readthedocs.io/en/stable/treemethod.html#approximated-solutions)'
- en: As GPUs are critical for many machine learning applications, XGBoost has a GPU
    implementation of the hist algorithm `gpu_hist`) that has support for external
    memory. [It is much faster and uses considerably less memory than hist](https://xgboost.readthedocs.io/en/latest/gpu/index.html).
    Note that XGBoost doesn’t have **native support** for GPUs on some operating systems.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GPU在许多机器学习应用中至关重要，XGBoost提供了一个支持外部内存的GPU实现的hist算法`gpu_hist`。 [它比hist快得多，并且使用的内存明显更少](https://xgboost.readthedocs.io/en/latest/gpu/index.html)。请注意，XGBoost在某些操作系统上没有**原生支持**GPU。
- en: '![Figure](../Images/95469c31b40f2c1610179db6491bf5f8.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/95469c31b40f2c1610179db6491bf5f8.png)'
- en: '[XGBoost documentation](https://xgboost.readthedocs.io/en/latest/install.html#python)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[XGBoost文档](https://xgboost.readthedocs.io/en/latest/install.html#python)'
- en: '[**auto**](https://xgboost.readthedocs.io/en/stable/treemethod.html#approximated-solutions)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[**auto**](https://xgboost.readthedocs.io/en/stable/treemethod.html#approximated-solutions)'
- en: This is the default value for the parameter. Based on the dataset size, XGBoost
    will choose the “fastest method”. For small datasets, exact will be used. For
    larger datasets, approx will be used. Note that hist and gpu_hist aren’t considered
    in this heuristic based approach even though they are often faster.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是参数的默认值。根据数据集的大小，XGBoost将选择“最快的方法”。对于小数据集，将使用exact。对于大数据集，将使用approx。请注意，尽管hist和gpu_hist通常更快，但在这种启发式方法中，它们并未被考虑。
- en: If you run this [code](https://gist.github.com/mGalarnyk/16d15183f691594bc2c256505a4c42b1),
    you will see how running models using gpu_hist can save a lot of time. On a relatively
    small dataset (100,000 rows, 1000 features) on my computer, changing from hist
    to gpu_hist decreased training time by about a factor of 2.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这个 [代码](https://gist.github.com/mGalarnyk/16d15183f691594bc2c256505a4c42b1)，你将看到使用gpu_hist运行模型可以节省大量时间。在我的计算机上，处理一个相对较小的数据集（100,000行，1000个特征），将hist更改为gpu_hist将训练时间减少了约一半。
- en: Leveraging cloud computing
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用云计算
- en: '![Figure](../Images/63ba4b59cda9c8aad2caed8c9f1a7a36.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/63ba4b59cda9c8aad2caed8c9f1a7a36.png)'
- en: Cloud computing allows you to not only utilize more cores and memory than your
    local machine, but can also give you access to specialized resources like GPUs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算不仅允许你使用比本地计算机更多的核心和内存，还可以让你访问像GPU这样的专业资源。
- en: The last section was mostly about choosing more efficient algorithms in order
    to better use of available computational resources. However, sometimes available
    computational resources are not enough and you simply need more. For example,
    the MacBook shown in the image below only has 4 cores and 16GB memory. Furthermore,
    it runs on MacOS which at the time of this writing, XGBoost doesn’t have GPU support
    for.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一部分主要讨论了选择更高效的算法，以更好地利用可用的计算资源。然而，有时可用的计算资源不够，你可能需要更多。例如，下面图片中的MacBook只有4个核心和16GB内存。此外，它运行在MacOS上，而在写作时，XGBoost尚未对其提供GPU支持。
- en: '![Figure](../Images/3433f8727939e797b3baa086e57ff2a6.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/3433f8727939e797b3baa086e57ff2a6.png)'
- en: For the purpose of this post, you can think of the MacBook above as a single
    node with 4 cores.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本文的目的，你可以将上面的MacBook视为一个具有4个核心的单节点。
- en: A way around this problem is to utilize more resources on the cloud. Utilizing
    cloud providers aren’t free, but they often allow you to utilize more cores and
    memory than your local machine. Additionally, if XGBoost doesn’t have support
    for your local machine, it is easy to choose an instance type that XGBoost has
    GPU support for.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是利用更多的云资源。使用云服务提供商不是免费的，但它们通常允许你使用比本地计算机更多的核心和内存。此外，如果XGBoost不支持你的本地计算机，选择一个XGBoost支持GPU的实例类型也是很简单的。
- en: 'If you would like to try speeding up your training on the cloud, below is an
    overview of the steps from [Jason Brownlee’s article](https://machinelearningmastery.com/train-xgboost-models-cloud-amazon-web-services/) on
    how to train an XGBoost model on an AWS EC2 instance:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想尝试加速云端训练，以下是来自 [Jason Brownlee的文章](https://machinelearningmastery.com/train-xgboost-models-cloud-amazon-web-services/)
    的步骤概览，介绍如何在AWS EC2实例上训练XGBoost模型：
- en: 1\. Setup an AWS account (if needed)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 设置 AWS 账户（如有需要）
- en: 2\. Launch an AWS Instance
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 启动 AWS 实例
- en: 3\. Login and run the code
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 登录并运行代码
- en: 4\. Train an XGBoost model
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 训练 XGBoost 模型
- en: 5\. Close the AWS Instance (only pay for the instance when you are using it)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 关闭 AWS 实例（仅在使用时支付实例费用）
- en: If you select a more powerful instance than what you have locally, you’ll likely
    see that training on the cloud is faster. **Note that multi-GPU training with
    XGBoost actually requires distributed training which means you need more than
    a single node/instance to accomplish this**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您选择比本地更强大的实例，您会发现云端训练速度更快。**请注意，使用 XGBoost 进行多 GPU 训练实际上需要分布式训练，这意味着您需要多个节点/实例来完成此任务**。
- en: Distributed XGBoost Training with Ray
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Ray 进行分布式 XGBoost 训练
- en: So far, this tutorial has covered speeding up training by changing the tree
    construction algorithm and by increasing computing resources through cloud computing.
    Another solution is to distribute XGBoost model training with [XGBoost-Ray](https://docs.ray.io/en/latest/xgboost-ray.html) which
    leverages Ray.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本教程已介绍通过更改树构建算法和通过云计算增加计算资源来加快训练速度。另一种解决方案是通过[XGBoost-Ray](https://docs.ray.io/en/latest/xgboost-ray.html)分布式训练
    XGBoost 模型，它利用了 Ray。
- en: What is Ray?
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是 Ray？
- en: '[Ray](https://www.ray.io/) is a fast, simple distributed execution framework
    that makes it easy to scale your applications and to leverage state of the art
    machine learning libraries. Using Ray, you can take Python code that runs sequentially
    and with minimal code changes transform it into a distributed application. If
    you would like to learn about Ray and the [actor model](https://en.wikipedia.org/wiki/Actor_model),
    you can learn about it [here](https://www.anyscale.com/blog/writing-your-first-distributed-python-application-with-ray).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ray](https://www.ray.io/) 是一个快速、简单的分布式执行框架，使得扩展应用程序和利用先进的机器学习库变得容易。使用 Ray，您可以将顺序运行的
    Python 代码通过最小的代码更改转换为分布式应用程序。如果您想了解 Ray 和 [actor 模型](https://en.wikipedia.org/wiki/Actor_model)，可以在[这里](https://www.anyscale.com/blog/writing-your-first-distributed-python-application-with-ray)了解更多。'
- en: '![Figure](../Images/083311ce0dee8896c9f5d6e7e590e60a.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/083311ce0dee8896c9f5d6e7e590e60a.png)'
- en: While this tutorial explores how Ray makes it easy to parallelize and distribute
    XGBoost code, it is important to note that Ray and its ecosystem also make it
    easy to distribute plain Python code as well as existing libraries like [scikit-learn](https://www.anyscale.com/blog/how-to-speed-up-scikit-learn-model-training), [LightGBM](https://www.anyscale.com/blog/introducing-distributed-lightgbm-training-with-ray), [PyTorch](https://medium.com/pytorch/getting-started-with-distributed-machine-learning-with-pytorch-and-ray-fd83c98fdead),
    and much more.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本教程探讨了 Ray 如何简化 XGBoost 代码的并行化和分布式处理，但需要注意的是，Ray 及其生态系统还简化了普通 Python 代码以及现有库的分布式处理，例如[scikit-learn](https://www.anyscale.com/blog/how-to-speed-up-scikit-learn-model-training)、[LightGBM](https://www.anyscale.com/blog/introducing-distributed-lightgbm-training-with-ray)、[PyTorch](https://medium.com/pytorch/getting-started-with-distributed-machine-learning-with-pytorch-and-ray-fd83c98fdead)等。
- en: How to get started with XGBoost-Ray
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何开始使用 XGBoost-Ray
- en: To get started with XGBoost-Ray, [you first need to install it](https://docs.ray.io/en/latest/xgboost-ray.html#installation).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用 XGBoost-Ray，[您首先需要安装它](https://docs.ray.io/en/latest/xgboost-ray.html#installation)。
- en: '`pip install "xgboost_ray"`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`pip install "xgboost_ray"`'
- en: Since it is fully compatible with the core XGBoost API, all you need is a few
    code changes to scale XGBoost training from a single machine to a cluster with
    hundreds of nodes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于与核心 XGBoost API 完全兼容，您只需进行少量代码更改即可将 XGBoost 训练从单台机器扩展到拥有数百个节点的集群。
- en: '![Figure](../Images/c23814fc9e976d61881209a2f55c06f7.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/c23814fc9e976d61881209a2f55c06f7.png)'
- en: XGBoost-Ray supports multi-node/multi-GPU training. On a machine, GPUs communicate
    gradients via NCCL2\. Between nodes, they use Rabit instead ​​([learn more](https://www.anyscale.com/blog/distributed-xgboost-training-with-ray)).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost-Ray 支持多节点/多 GPU 训练。在单台机器上，GPU 通过 NCCL 进行梯度通信。在节点之间，它们使用 Rabit 代替 ([了解更多](https://www.anyscale.com/blog/distributed-xgboost-training-with-ray))。
- en: As you can see in the code below, the API is very similar to XGBoost. The highlighted
    portions are where the code is different than the normal XGBoost API.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如下代码所示，API 与 XGBoost 非常相似。高亮部分显示了与普通 XGBoost API 不同的代码部分。
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The code above shows how little you need to change your code to use XGBoost-Ray.
    While you don’t need XGboost-Ray to train the breast cancer dataset, [a previous
    post](https://www.anyscale.com/blog/distributed-xgboost-training-with-ray) ran
    benchmarks on several dataset sizes (~1.5M to ~12M rows) across different amounts
    of workers (1 to 8) to show how it performs on bigger datasets on a single node.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码展示了使用 XGBoost-Ray 你需要对代码进行多小的修改。虽然你不需要 XGBoost-Ray 来训练乳腺癌数据集， [之前的一篇文章](https://www.anyscale.com/blog/distributed-xgboost-training-with-ray) 在不同数量的工作节点（1
    到 8）上对几个数据集大小（约 1.5M 到 12M 行）进行了基准测试，以展示它在单节点上处理更大数据集的表现。
- en: '![Figure](../Images/265932e9cf9560e6c44b7ac24d546857.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/265932e9cf9560e6c44b7ac24d546857.png)'
- en: Training times for single node benchmarks (lower is better). XGBoost-Ray and
    XGBoost-Dask achieve similar performance on a single AWS m5.4xlarge instance with
    16 cores and 64 GB memory.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 单节点基准测试的训练时间（时间越少越好）。XGBoost-Ray 和 XGBoost-Dask 在单个 AWS m5.4xlarge 实例（具有 16
    核心和 64 GB 内存）上表现相似。
- en: XGBoost-Ray is also performant in multi-node (distributed) settings as the image
    below shows.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost-Ray 在多节点（分布式）设置下也表现出色，如下图所示。
- en: '![Figure](../Images/fc94371aa91631c47093678af8cd51e5.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/fc94371aa91631c47093678af8cd51e5.png)'
- en: Multi-node training times on several synthetic dataset sizes ranging from ~400k
    to ~2B rows (where lower is better). XGBoost-Ray and XGBoost-Spark achieve similar
    performance.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在几个合成数据集大小范围（约 400k 到 2B 行）上的多节点训练时间（时间越少越好）。XGBoost-Ray 和 XGBoost-Spark 表现相似。
- en: If you would like to learn more about XGBoost Ray, [check out this post on XGBoost-Ray](https://www.anyscale.com/blog/distributed-xgboost-training-with-ray).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于 XGBoost-Ray 的内容， [可以查看这篇关于 XGBoost-Ray 的文章](https://www.anyscale.com/blog/distributed-xgboost-training-with-ray)。
- en: Conclusion
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: This post went over a couple approaches you can use to speed up XGBoost model
    training like changing tree construction methods, leveraging cloud computing,
    and distributed XGBoost on Ray. If you have any questions or thoughts about XGBoost
    on Ray, please feel free to join our community through [Discourse](https://discuss.ray.io/) or [Slack](https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform).
    If you would like to keep up to date with all things Ray, [consider following
    @raydistributed on twitter](https://twitter.com/raydistributed) and [sign up for
    the Ray newsletter](https://anyscale.us5.list-manage.com/subscribe?u=524b25758d03ad7ec4f64105f&id=d94e960a03).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了几种加速 XGBoost 模型训练的方法，比如更改树构建方法、利用云计算以及在 Ray 上分布式 XGBoost。如果你对 Ray 上的 XGBoost
    有任何问题或想法，请随时通过 [Discourse](https://discuss.ray.io/) 或 [Slack](https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform) 加入我们的社区。如果你想了解
    Ray 的最新动态， [可以关注 @raydistributed 在 Twitter 上](https://twitter.com/raydistributed) 并 [订阅
    Ray 通讯](https://anyscale.us5.list-manage.com/subscribe?u=524b25758d03ad7ec4f64105f&id=d94e960a03)。
- en: '**Bio: [Michael Galarnyk](https://www.linkedin.com/in/michaelgalarnyk/)** is
    a Data Science Professional, and works in Developer Relations at Anyscale.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介: [Michael Galarnyk](https://www.linkedin.com/in/michaelgalarnyk/)** 是一名数据科学专家，并在
    Anyscale 从事开发者关系工作。'
- en: '[Original](https://www.anyscale.com/blog/how-to-speed-up-xgboost-model-training).
    Reposted with permission.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://www.anyscale.com/blog/how-to-speed-up-xgboost-model-training).
    经许可转载。'
- en: More On This Topic
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[What are the Assumptions of XGBoost?](https://www.kdnuggets.com/2022/08/assumptions-xgboost.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost 的假设是什么？](https://www.kdnuggets.com/2022/08/assumptions-xgboost.html)'
- en: '[Tuning XGBoost Hyperparameters](https://www.kdnuggets.com/2022/08/tuning-xgboost-hyperparameters.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[调优 XGBoost 超参数](https://www.kdnuggets.com/2022/08/tuning-xgboost-hyperparameters.html)'
- en: '[Leveraging XGBoost for Time-Series Forecasting](https://www.kdnuggets.com/2023/08/leveraging-xgboost-timeseries-forecasting.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用 XGBoost 进行时间序列预测](https://www.kdnuggets.com/2023/08/leveraging-xgboost-timeseries-forecasting.html)'
- en: '[WTF is the Difference Between GBM and XGBoost?](https://www.kdnuggets.com/wtf-is-the-difference-between-gbm-and-xgboost)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GBM 和 XGBoost 有什么区别？](https://www.kdnuggets.com/wtf-is-the-difference-between-gbm-and-xgboost)'
- en: '[How To Use Synthetic Data To Overcome Data Shortages For Machine…](https://www.kdnuggets.com/2022/03/synthetic-data-overcome-data-shortages-machine-learning-model-training.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用合成数据克服机器学习模型训练中的数据短缺](https://www.kdnuggets.com/2022/03/synthetic-data-overcome-data-shortages-machine-learning-model-training.html)'
- en: '[3 Simple Ways to Speed Up Your Python Code](https://www.kdnuggets.com/2022/10/3-simple-ways-speed-python-code.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[加速你的 Python 代码的 3 种简单方法](https://www.kdnuggets.com/2022/10/3-simple-ways-speed-python-code.html)'
