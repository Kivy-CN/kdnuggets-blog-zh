- en: 'Only Numpy: Implementing GANs and Adam Optimizer using Numpy'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2018/08/only-numpy-implementing-gans-adam-optimizer.html](https://www.kdnuggets.com/2018/08/only-numpy-implementing-gans-adam-optimizer.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By [Jae Duk Seo](https://jaedukseo.me/), Ryerson University**'
  prefs: []
  type: TYPE_NORMAL
- en: So today I was inspired by this blog post, “[Generative Adversarial Nets in
    TensorFlow](https://wiseodd.github.io/techblog/2016/09/17/gan-tensorflow/)” and
    I wanted to implement GAN myself using Numpy. Here is the [original GAN paper](https://arxiv.org/abs/1406.2661) by [**@**goodfellow_ian](https://twitter.com/goodfellow_ian).
    Below is a gif of all generated images from Simple GAN.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/53ba5fda91dd8892b617e9c6df1f6ef4.png)'
  prefs: []
  type: TYPE_IMG
- en: Before reading along, please note that I won’t be covering too much of math.
    Rather the implementation of the code and results, I will cover the math maybe
    later. And I am using Adam Optimizer, however, I won’t go into explaining the
    implementation of Adam at this post.
  prefs: []
  type: TYPE_NORMAL
- en: '**Forward Feed / Partial Back Propagation of Discriminator in GAN**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/92e29f62d3b24fb4a204a0ef4016facb.png)'
  prefs: []
  type: TYPE_IMG
- en: Again, I won’t go into too much details, but please note the Red Boxed Region
    called Data. For the Discriminator Network in GAN, that Data either can be Real
    Image or Fake Image Generated by the Generator Network. Our images are (1,784)
    vector of MNIST data set.
  prefs: []
  type: TYPE_NORMAL
- en: One more thing to note is **Red (L2A) and Blue (L2A)**. Red (L2A) is the final
    output of our Discriminator Network with Real Image as input. And Blue (L2A) is
    the final output of our Discriminator Network with Fake Image as input.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/621c48ea875a48847e0c6708185cdf6e.png)'
  prefs: []
  type: TYPE_IMG
- en: The way we implement this is by getting the real image, and fake data before
    putting them both into the network.
  prefs: []
  type: TYPE_NORMAL
- en: Line 128 — Getting the Real Image Data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Line 147 — Getting the Fake Image Data (Generated By Generator Network)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Line 162 — Cost Function of our Discriminator Network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, please take note of the Blue Box Region, that is our cost function. Lets
    compare the cost function from the original paper, shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/357d14a736bed4c408f787a3a28772b0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image[ from original Paper](https://arxiv.org/pdf/1406.2661.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: The difference is the fact that we are putting a (-) negative sign in front
    of the first term log(L2A).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e31bc8abaeb8cf6f9311a0630fd5b4d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image [from Agustinus Kristiadi](https://wiseodd.github.io/techblog/2016/12/24/conditional-gan-tensorflow/)
  prefs: []
  type: TYPE_NORMAL
- en: As seen above, in TensorFlow implementation we flip the signs if we want to
    Maximize some value, since TF auto differentiation only can Minimize.
  prefs: []
  type: TYPE_NORMAL
- en: I thought about this and I decided to implement in a similar way. Cuz I wanted
    to Maximize the chance of our discriminator guessing right for real image while
    Minimize the chance of our discriminator guessing wrong for fake images, and I
    wanted the sum of those values to balance out. However, I am not 100 % sure of
    this part as of yet, and will revisit this matter soon.
  prefs: []
  type: TYPE_NORMAL
- en: '**Forward Feed / Partial Back Propagation of Generator in GAN**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/271b5c892299581325f4b6e3aeaf0aea.png)'
  prefs: []
  type: TYPE_IMG
- en: The Back Propagation process for generator network in GAN is bit complicated.
  prefs: []
  type: TYPE_NORMAL
- en: Blue Box — Generated Fake Data from the Generator Network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Green Box (Left Corner) — Discriminator Takes the Generated (Blue Box) Input
    and perform Forward Feed process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orange Box — Cost Function for Generator Network (Again we want to Maximize
    the chance of producing a Realistic Data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Green Box (Right Corner) — Back Propagation Process for Generator Network, but
    we have to pass Gradient all the way Discriminator Network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Below is the screen shot of implemented code.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96b04dff34b3a7adb53d5c755e74d033.png)'
  prefs: []
  type: TYPE_IMG
- en: Standard Back propagation, nothing too special.
  prefs: []
  type: TYPE_NORMAL
- en: '**Training Results: Failed Attempts**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I quickly realized that training GAN is extremely hard, even with Adam Optimizer,
    the network just didn’t seem to converge well. So, I will first present you all
    of the failed attempts and it’s network architectures.
  prefs: []
  type: TYPE_NORMAL
- en: '*1\. Generator, 2 Layers: 200, 560 Hidden Neurons, Input Vector Size 100*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db544005fa036d235b647301bdccc234.png)'
  prefs: []
  type: TYPE_IMG
- en: '*2\. Generator, tanh() Activation, 2 Layers: 245, 960 Hidden Neurons, IVS 100*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05c0c34086bb1bc162a0b272c34575e8.png)'
  prefs: []
  type: TYPE_IMG
- en: '*3\. Generator, 3 Layers: 326, 356,412 Hidden Neurons, Input Vector Size 326*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fca293dfd8d27c1253a9645494278ece.png)'
  prefs: []
  type: TYPE_IMG
- en: '*4\. Generator, 2 Layers: 420, 640 Hidden Neurons, Input Vector Size 350*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ed74e5863193c43fbbbbf3183f3a7a3.png)'
  prefs: []
  type: TYPE_IMG
- en: '*5\. Generator, 2 Layers: 660, 780 Hidden Neurons, Input Vector Size 600*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/116ef5a027168b5c5a72867a841bc824.png)'
  prefs: []
  type: TYPE_IMG
- en: '*6\. Generator, 2 Layers: 320, 480 Hidden Neurons, Input Vector Size 200*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7db05c1d1e38126e17de30b621ba359b.png)'
  prefs: []
  type: TYPE_IMG
- en: So as seen above, all of them seems to learn something, but not really LOL.
    However, I was able to use one neat trick to generate an image that kinda look
    like numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Extreme Step Wise Gradient Decay**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/1e679f74769b9f026b2939639a473771.png)'
  prefs: []
  type: TYPE_IMG
- en: Above is a gif, I know the difference is subtle but trust me [I ain’t Rick Rollying](https://www.youtube.com/watch?v=dQw4w9WgXcQ)
    you. The trick is extremely simple and easy to implement. We first set the learning
    rate high rate for the first training, and after the first training we set the
    decay the learning rate by factor of 0.01\. And for unknown reason (I want to
    investigate further more into this), this seems to work.
  prefs: []
  type: TYPE_NORMAL
- en: But with a huge cost, I think we are converging to a ‘place’ where the network
    is only able to generate only certain kind of data. Meaning, from the uniform
    distribution of numbers between -1 and 1\. The Generator will only generate image
    that ONLY looks like a 3 or a 2 etc... But the key point here is that the network
    is not able to generate different set of numbers. This is evidence by the fact
    that, well, all of the numbers represented in the image look like 3.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is some what reasonable image that looks like a number. So lets
    see some more results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41b171e99a5b6f1692fa0017c4de70fe.png)'
  prefs: []
  type: TYPE_IMG
- en: As seen above, as time goes on, the numbers become sharper. A good example is
    the generated image of 3 or 9.
  prefs: []
  type: TYPE_NORMAL
- en: '**Interactive Code**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/871598a423ec4616a213c205f4edb3a3.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Update: I moved to Google Colab for Interactive codes! So you would need a
    google account to view the codes, also you can’t run read only scripts in Google
    Colab so make a copy on your play ground. Finally, I will never ask for permission
    to access your files on Google Drive, just FYI. Happy Coding!*'
  prefs: []
  type: TYPE_NORMAL
- en: Please click[ here to access the interactive code, online.](https://colab.research.google.com/notebook#fileId=1D2kF1uBbJlVuglpuBSrw7A_3ws-Nvxk-)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8e73637d1e0084468998e3abe489d41.png)'
  prefs: []
  type: TYPE_IMG
- en: When running the code, make sure you are on the ‘main.py’ tap, as seen above
    in the Green Box. The program will ask you a random number for seeding, as seen
    in the Blue Box. After it will generate one image, to view that image please click
    on the click me tab above, Red Box.
  prefs: []
  type: TYPE_NORMAL
- en: '**Final Words**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training GAN to even partially work is a huge chunk of work, I want to investigate
    on more effective way of training of GAN’s. One last thing, shout out to [**@**replit](https://twitter.com/replit),
    these guys are amazing!
  prefs: []
  type: TYPE_NORMAL
- en: If any errors are found, please email me at jae.duk.seo@gmail.com.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile follow me on my twitter [here](https://twitter.com/JaeDukSeo), and
    visit [my website](https://jaedukseo.me/), or my [Youtube channel](https://www.youtube.com/c/JaeDukSeo) for
    more content. I also did comparison of Decoupled Neural Network [here if you](https://becominghuman.ai/only-numpy-implementing-and-comparing-combination-of-google-brains-decoupled-neural-interfaces-6712e758c1af) are
    interested.
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs: []
  type: TYPE_NORMAL
- en: Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., … & Bengio, Y. (2014). Generative adversarial nets. In *Advances in neural
    information processing systems* (pp. 2672–2680).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Free Online Animated GIF Maker — Make GIF Images Easily. (n.d.). Retrieved January
    31, 2018, from [http://gifmaker.me/](http://gifmaker.me/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generative Adversarial Nets in TensorFlow. (n.d.). Retrieved January 31, 2018,
    from [https://wiseodd.github.io/techblog/2016/09/17/gan-tensorflow/](https://wiseodd.github.io/techblog/2016/09/17/gan-tensorflow/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: J. (n.d.). Jrios6/Adam-vs-SGD-Numpy. Retrieved January 31, 2018, from [https://github.com/jrios6/Adam-vs-SGD-Numpy/blob/master/Adam%20vs%20SGD%20-%20On%20Kaggles%20Titanic%20Dataset.ipynb](https://github.com/jrios6/Adam-vs-SGD-Numpy/blob/master/Adam%20vs%20SGD%20-%20On%20Kaggles%20Titanic%20Dataset.ipynb)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ruder, S. (2018, January 19). An overview of gradient descent optimization algorithms.
    Retrieved January 31, 2018, from [http://ruder.io/optimizing-gradient-descent/index.html#adam](http://ruder.io/optimizing-gradient-descent/index.html#adam)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: E. (1970, January 01). Eric Jang. Retrieved January 31, 2018, from [https://blog.evjang.com/2016/06/generative-adversarial-nets-in.html](https://blog.evjang.com/2016/06/generative-adversarial-nets-in.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Bio: [Jae Duk Seo](https://jaedukseo.me/)** is a fourth year computer scientist
    at Ryerson University.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/only-numpy-implementing-gan-general-adversarial-networks-and-adam-optimizer-using-numpy-with-2a7e4e032021).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Inside the Mind of a Neural Network with Interactive Code in Tensorflow](/2018/06/inside-mind-neural-network-interactive-code-tensorflow.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building Convolutional Neural Network using NumPy from Scratch](/2018/04/building-convolutional-neural-network-numpy-scratch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How I Used CNNs and Tensorflow and Lost a Silver Medal in Kaggle Challenge](/2018/05/lost-silver-medal-kaggle-mercari-price-suggestion-challenge-cnns-tensorflow.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Tuning Adam Optimizer Parameters in PyTorch](https://www.kdnuggets.com/2022/12/tuning-adam-optimizer-parameters-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
