# 学习如何在5分钟内使用PySpark（安装 + 教程）

> 原文：[https://www.kdnuggets.com/2019/08/learn-pyspark-installation-tutorial.html](https://www.kdnuggets.com/2019/08/learn-pyspark-installation-tutorial.html)

![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)

**由 [Georgios Drakos](https://gdcoder.com/author/)，TUI的数据科学家**

我发现对于大多数人来说，开始使用Apache Spark（本教程将重点介绍PySpark）并在本地机器上安装它有点困难。通过这个简单的教程，你将能非常快速地掌握！

* * *

## 我们的前3个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析能力

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 在IT领域支持你的组织

* * *

[**Apache Spark**](http://spark.apache.org/)** 对于大数据爱好者来说是必备的**，因为它是一个快速、易用的大数据处理通用引擎，具有流处理、SQL、机器学习和图形处理等内置模块。这项技术是数据工程师的热门技能，但数据科学家在进行探索性数据分析（EDA）、特征提取和机器学习时也能从学习Spark中受益。但请记住，Spark只有在大规模节点集群上运行时才能真正发挥作用。

### 目录

+   介绍

+   Spark定义

+   Spark应用程序

+   在Mac上安装PySpark

+   使用PySpark打开Jupyter Notebook

+   启动SparkSession

+   结论

+   参考文献

![figure-name](../Images/35cf5da02d3e664136e0ff5dee351c8a.png)

### 介绍

Apache Spark 是数据处理框架中最炙手可热和规模最大的开源项目之一，具有丰富的高级API，支持Scala、Python、Java和R等编程语言。它实现了将大数据和机器学习结合起来的潜力。这是因为：

+   由于内存操作，Spark运行速度非常快（比传统的 [Hadoop MapReduce](https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm) 快多达100倍）。

+   它提供了强大、分布式、容错的数据对象（称为 [RDDs](https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.htm)）

+   它通过附加包如 [MLlib](https://spark.apache.org/mllib/) 和 [GraphX](https://spark.apache.org/graphx/) 与机器学习和图分析领域无缝集成。

Spark 基于 [Hadoop/HDFS](https://www.bernardmarr.com/default.asp?contentID=1080) 实现，主要使用 [Scala](https://www.scala-lang.org/) 语言编写，这是一种函数式编程语言。然而，对于大多数初学者来说，Scala 并不是进入数据科学领域时的最佳首选语言。

幸运的是，Spark 提供了一个很棒的 Python API，叫做 [PySpark](https://spark.apache.org/docs/latest/api/python/index.html)。这允许 Python 程序员与 Spark 框架接口——让你在大规模数据上进行操作，并在分布式文件系统上处理对象。因此，Spark 并不是你必须学习的新编程语言，而是一个在 HDFS 之上的框架。

这引入了节点、惰性求值以及变换-行动（或“映射和归约”）编程范式等新概念。事实上，Spark 足够灵活，可以与 Hadoop 以外的其他文件系统配合使用——如 Amazon S3 或 Databricks (DBFS)。

互联网巨头如 Netflix、Yahoo 和 eBay 已大规模部署 Spark，集体处理多个 PB 的数据，集群节点超过 8,000 个。

### Spark 定义

通常，当你想到计算机时，会想到家中或工作中的一台机器。这台机器在处理小数据集的机器学习应用时表现良好。然而，当你拥有巨大的数据集（以 TB 或 GB 计）时，有些事情是你的计算机无法完成的。一个特别具有挑战性的领域是数据处理。单台机器没有足够的能力和资源来处理大量信息（否则你可能要等到计算完成）。

![figure-name](../Images/c36231c95b1b58e1c053a9d632f4efad.png)

集群，即机器组，将多台机器的资源汇集在一起，使我们可以像使用单台机器一样使用所有累积的资源。单独的机器并不强大，你需要一个框架来协调它们之间的工作。**Spark 就是这样一个工具，管理和协调在计算机集群上执行任务的执行。**

### Spark 应用程序

一个 Spark 应用程序包括：

+   驱动程序

+   执行器（分布式工作进程的集合）

### 驱动程序

驱动程序运行应用程序的 main() 方法，负责以下职责：

+   在我们的集群节点上运行，或在客户端上运行，并通过集群管理器调度作业执行

+   响应用户的程序或输入

+   分析、调度并分发工作到执行器

### 执行器

执行器是一个负责执行任务的分布式进程。每个 Spark 应用程序都有自己的执行器集合，这些执行器在单个 Spark 应用程序的生命周期内保持活跃。

+   执行器负责处理 Spark 作业的所有数据处理。

+   将结果存储在内存中，仅在驱动程序程序明确指示时才持久化到磁盘

+   一旦完成任务，将结果返回给驱动程序

+   每个节点可以从每个节点一个执行器到每个核心一个执行器

** 节点是单个实体机器或服务器。

![figure-name](../Images/ce110256af4c8d0e3e7b93dde3d91146.png)

### Spark 的应用程序工作流

当你提交作业给 Spark 进行处理时，后台有很多事情在发生。

1.  我们的独立应用程序启动并初始化其 SparkContext。只有在拥有 SparkContext 后，应用程序才能称为驱动程序。

1.  我们的驱动程序向集群管理器请求资源以启动其执行器

1.  集群管理器启动执行器

1.  我们的驱动程序运行实际的 Spark 代码

1.  执行器运行任务并将其结果发送回驱动程序

1.  SparkContext 停止，所有执行器关闭，将资源返回到集群

### 在 Mac 上安装 Spark（本地）

***第一步：安装 Brew***

如果你还没有安装 brew，请安装它；如果已经安装，可以跳过此步骤：

1\. 打开 Mac 上的终端。你可以通过 Spotlight 搜索终端（或者可以在 /Applications/Utilities/ 中找到它）。

2\. 输入以下命令。

```py
$ /usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"
```

3\. 按下 Return 键，脚本将运行。它会在终端输出将要安装的日志。按 Return 键继续，或按其他任意键中止。

4\. 可能会要求**sudo**权限。如果发生这种情况，你需要输入管理员密码并再次按下 Return 键。

**注意：** 命令行工具（Apple 的 XCode）将在本指南之后安装。

安装将如下图所示。

![figure-name](../Images/0561cfe8388405c80924de741a51e719.png)通过命令行安装 Homebrew

安装成功完成后，将如下图所示。

![figure-name](../Images/e72a72372711566e135617971f063ce3.png)说明

默认情况下，Homebrew 会发送匿名数据和分析。你可以在[这里](https://docs.brew.sh/Analytics)找到更多信息。你可以通过运行命令选择退出。

```py
$ brew analytics off
```

***第二步：安装 Anaconda***

在同一个终端中简单地输入：`$ brew cask install anaconda`。如果在此步骤中遇到任何问题，请查看资源部分。

***第三步：安装 PySpark***

1\. 在终端中输入`$ brew install apache-spark`

2\. 如果你看到此错误信息，输入`$ brew cask install caskroom/versions/java8`来安装 Java8，如果你已经安装了它，则不会看到此错误。

![figure-name](../Images/c41b2ed8a71ba60c5e55ad5b6b5f89fd.png)

3\. 通过在终端中输入`$ pyspark`检查 PySpark 是否正确安装。如果你看到以下内容，说明它已正确安装：

![figure-name](../Images/48936f60f3da3b12e50bc0a205db863c.png)

### 打开 Jupyter Notebook，PySpark 已准备就绪

本节假设 PySpark 已正确安装，并且在终端输入`$ pyspark`时没有出现错误。在此步骤中，我将展示创建自动初始化 SparkContext 的 Jupyter Notebooks 的步骤。

为了创建终端会话的全局配置文件，你需要创建或修改.bash_profile或.bashrc文件。在这里，我将以.bash_profile作为示例。

检查系统中是否有.bash_profile，通过`$ ls -a`，如果没有，使用`$ touch ~/.bash_profile`创建一个。

查找Spark路径，通过`$ brew info apache-spark`

![figure-name](../Images/0ed7b859ff7ba04f678b5c67416b5240.png)

如果你已经有一个.bash_profile，打开它通过`$ vim ~/.bash_profile`，按`I`进入插入模式，将以下代码粘贴到任何位置（不要删除文件中的任何内容）：

```py
export SPARK_PATH=(path found above by running brew info apache-spark)
export PYSPARK_DRIVER_PYTHON="jupyter"
export PYSPARK_DRIVER_PYTHON_OPTS="notebook"
#For python 3, You have to add the line below or you will get an error#
export PYSPARK_PYTHON=python3
alias snotebook='$SPARK_PATH/bin/pyspark --master local[2]'

```

按`ESC`退出插入模式，输入`:wq`退出VIM。*你可以在这里找到更多VIM命令。*

刷新终端配置文件，通过`$ source ~/.bash_profile`

我在Jupyter Notebook中使用PySpark的最喜欢的方法是安装[findSpark](https://github.com/minrk/findspark)包，它允许我在代码中使用Spark Context。

*findSpark包并不特定于Jupyter Notebook，你也可以在你喜欢的IDE中使用这个技巧。*

通过在终端上运行以下命令安装findspark

```py
$ pip install findspark
```

启动一个普通的Jupyter Notebook，并运行以下命令：

```py
# useful to have this code snippet to avoid getting an error in case forgeting
# to close spark

try:
    spark.stop()
except:
    pass

# Using findspark to find automatically the spark folder
import findspark
findspark.init()

# import python libraries
import random

# initialize
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()
num_samples = 100000000

def inside(p):
    x, y = random.random(), random.random()
    return x*x + y*y < 1

count = spark.sparkContext.parallelize(range(0, num_samples)).filter(inside).count()
pi = 4 * count / num_samples
print(pi)
```

输出应为：

![figure-name](../Images/272c85c537c5ad986fc8099a97520f23.png)

请注意，使用Spark 2.2时，许多人建议只需执行`pip install pyspark`。我尝试使用`pip`安装`pyspark`但无法正确启动`pyspark`集群。阅读了多个Stack Overflow的答案和[官方文档](https://pypi.org/project/pyspark/2.2.0/)，我发现了这个：

> *Spark的Python打包并不是为了替代所有其他使用情况。这个Python打包版本的Spark适合与现有集群（无论是Spark独立集群、YARN还是Mesos）进行交互，但不包含设置自己独立Spark集群所需的工具。你可以从[Apache Spark下载页面](http://spark.apache.org/downloads.html)下载Spark的完整版。*

因此，我建议遵循我上述描述的步骤。

### 启动SparkSession

这是Spark功能的主要入口点：它代表与Spark集群的连接，你可以使用它来创建RDDs和广播变量。当你使用Spark时，一切都从这个SparkSession开始并结束。**注意**，SparkSession是Spark 2.0的新特性，它最小化了需要记住或构建的概念数量。（在Spark 2.0.0之前，主要的连接对象是SparkContext、SqlContext和HiveContext）。

在交互环境中，会为你创建一个名为spark的SparkSession。为了保持一致性，你在自己应用程序中创建SparkSession时应该使用这个名称。

你可以通过 Builder 模式创建一个新的 SparkSession，该模式使用“流畅接口”风格的编码，通过方法链来构建一个新的对象。可以传入 Spark 属性，如下例所示：

```py
from pyspark.sql import SparkSession
spark = SparkSession\
        .builder
        .master("local[*]")
        .config("spark.driver.cores", 1)
        .appName("understanding_sparksession")
        .getOrCreate()
```

在应用程序结束时，请记得调用 `spark.stop()` 以结束 SparkSession。让我们了解一下我们上面定义的各种设置：

+   `**master**`**: **设置要连接的 Spark master URL，例如“local”用于本地运行，“local[4]”用于本地运行并使用 4 个核心，或“spark://master:7077”用于在 Spark 独立集群上运行。

+   `**config**`**:**通过指定（键，值）对来设置配置选项。

+   `**appName**`**: **为应用程序设置一个名称，如果未设置名称，将使用随机生成的名称。

+   `**getOrCreate**`**:**获取现有的 [`**SparkSession**`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?#pyspark.sql.SparkSession) ，如果没有现有的，则根据此构建器中设置的选项创建一个新的。如果返回现有的 SparkSession，则此构建器中影响 `SQLContext` 配置的配置选项将会应用。由于 `SparkContext` 配置无法在运行时修改（必须先停止现有上下文），而 `SQLContext` 配置可以在运行时修改。

### 结论

Spark 在过去几年里经历了巨大的增长。数百名贡献者共同努力，使 Spark 成为一项出色的技术，为各个行业的大数据处理和数据科学提供了事实上的标准。但是请记住，当面临性能问题时，**大数据集**的操作更适合使用它，否则可能会产生相反的效果。对于小数据集（几GB），建议使用 [Pandas](https://pandas.pydata.org/)。

感谢阅读，期待你的问题 :)

*敬请关注，祝机器学习愉快。*

### 参考文献

+   [https://medium.com/ayuth/install-anaconda-on-macos-with-homebrew-c94437d63a37](https://medium.com/ayuth/install-anaconda-on-macos-with-homebrew-c94437d63a37)

+   [https://picocoder.io/how-to-install-homebrew-on-mac/](https://picocoder.io/how-to-install-homebrew-on-mac/)

+   [https://stackoverflow.com/](https://stackoverflow.com/questions/46387574/installing-pyspark-on-macbook)

+   [https://www.oreilly.com/library/view/learning-spark/9781449359034/](https://www.oreilly.com/library/view/learning-spark/9781449359034/)

**简介：[Georgios Drakos](https://gdcoder.com/author/)** 是一名数据科学家，拥有雅典国家技术大学电气与计算机工程的学士和硕士学位，以及伦敦帝国学院的硕士学位，目前在 TUI 从事数据科学工作。他专注于决策支持的计算智能、数据工程、复杂分析和技术创新管理。对基于云的技术、分布式计算、大数据以及数据科学和机器学习的商业应用非常感兴趣。

[原文](https://gdcoder.com/learn-how-to-use-pyspark-in-under-5-minutes/)。经授权转载。

**相关：**

+   [Spark NLP：企业中最广泛使用的 NLP 库入门](/2019/06/spark-nlp-getting-started-with-worlds-most-widely-used-nlp-library-enterprise.html)

+   [使用 Pandas UDFs 的可扩展 Python 代码：数据科学应用](/2019/06/scalable-python-code-pandas-udfs.html)

+   [用 Spark、Optimus 和 Twint 在几分钟内分析推文](https://www.kdnuggets.com/2019/05/analyzing-tweets-nlp-spark-optimus-twint.html)

### 了解更多相关话题

+   [FastAPI 教程：用 Python 在几分钟内构建 API](https://www.kdnuggets.com/fastapi-tutorial-build-apis-with-python-in-minutes)

+   [PySpark 数据科学](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)

+   [使用 Pandera 对 PySpark 应用进行数据验证](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)

+   [3 分钟了解偏差-方差权衡](https://www.kdnuggets.com/2020/09/understanding-bias-variance-trade-off-3-minutes.html)

+   [在 5 分钟内构建机器学习 Web 应用](https://www.kdnuggets.com/2022/03/build-machine-learning-web-app-5-minutes.html)

+   [用 Python 在 5 分钟内构建网络爬虫](https://www.kdnuggets.com/2022/02/build-web-scraper-python-5-minutes.html)
