- en: How to Create Unbiased Machine Learning Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何创建无偏见的机器学习模型
- en: 原文：[https://www.kdnuggets.com/2021/07/create-unbiased-machine-learning-models.html](https://www.kdnuggets.com/2021/07/create-unbiased-machine-learning-models.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/07/create-unbiased-machine-learning-models.html](https://www.kdnuggets.com/2021/07/create-unbiased-machine-learning-models.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By Philip Tannor, Co-Founder & CEO of [Deepchecks](https://deepchecks.com/)**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由[Deepchecks](https://deepchecks.com/)的联合创始人兼首席执行官Philip Tannor**。'
- en: '![Figure](../Images/f109257a81bf17a819ae1929ce210afe.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/f109257a81bf17a819ae1929ce210afe.png)'
- en: '*Image by* [*Clker-Free-Vector-Images*](https://pixabay.com/users/clker-free-vector-images-3736/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=307248)
    *from* [*Pixabay*](https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=307248)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片由* [*Clker-Free-Vector-Images*](https://pixabay.com/users/clker-free-vector-images-3736/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=307248)
    *提供* [*Pixabay*](https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=307248)'
- en: '* * *'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的IT工作'
- en: '* * *'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: AI systems are becoming increasingly popular and central in many industries.
    They decide who might get a loan from the bank, whether an individual should be
    convicted, and we may even entrust them with our lives when using systems such
    as autonomous vehicles in the near future. Thus, there is a growing need for mechanisms
    to harness and control these systems so that we may ensure that they behave as
    desired.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: AI系统在许多行业变得越来越受欢迎并且核心。它们决定谁可能从银行获得贷款，一个人是否应该被定罪，我们甚至可能在不久的将来将我们的生命托付给诸如自动驾驶车辆等系统。因此，越来越需要机制来控制这些系统，以确保它们按预期行为。
- en: One important issue that has been gaining popularity in the last few years is
    *fairness*. While usually ML models are evaluated based on metrics such as accuracy,
    the idea of fairness is that we must ensure that our models are unbiased with
    regard to attributes such as gender, race and other selected attributes.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来一个重要的问题是*公平性*。虽然通常机器学习模型是根据准确度等指标来评估的，但公平性的理念是我们必须确保我们的模型在性别、种族和其他选定属性方面不带有偏见。
- en: A classic example of an episode regarding racial bias in AI systems, is the
    COMPAS software system, developed by Northpointe, which aims to assist US courts
    with assessing the likelihood of a defendant becoming a recidivist. Propublica
    published an [article](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)
    which claims that this system is biased against blacks, giving them higher risk
    ratings.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经典的种族偏见AI系统案例是由Northpointe开发的COMPAS软件系统，它旨在协助美国法院评估被告成为惯犯的可能性。Propublica发布了一篇[文章](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)，声称该系统对黑人存在偏见，给他们更高的风险评级。
- en: '![Figure](../Images/286f789c76493394720b3c8cbe64ca84.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/286f789c76493394720b3c8cbe64ca84.png)'
- en: '*ML system bias against African Americans? (*[*source*](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)*)*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*机器学习系统对非裔美国人的偏见？ (*[*来源*](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)*)*'
- en: In this post we will try to understand where biases in ML models originate,
    and explore methods for creating unbiased models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将尝试了解机器学习模型中的偏见来源，并探索创建无偏见模型的方法。
- en: Where Does Bias Come From?
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 偏见来源于哪里？
- en: '"Humans are the weakest link"'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “人类是最薄弱的环节”
- en: —Bruce Schneier
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —布鲁斯·施奈尔
- en: In the field of Cybersecurity it is often said that “humans are the weakest
    link” (Schneier). This idea applies in our case as well. Biases are in fact introduced
    into ML models by humans unintentionally.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络安全领域，常有人说“人类是最薄弱的环节”（Schneier）。这个观点在我们的情况中也适用。偏见实际上是由人类无意中引入到机器学习模型中的。
- en: Remember, an ML model can only be as good as the data it’s trained on, and thus
    if the training data contains biases, we can expect our model to mimic those same
    biases. Some representative examples for this can be found in the field of word
    embeddings in NLP. Word embeddings are learned dense vector representations of
    words, that are meant to capture semantic information of a word, which can then
    be fed to ML models for different downstream tasks. Thus, embeddings of words
    with similar meanings are expected to be “close” to each other.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，机器学习模型的好坏仅能取决于其训练数据，因此如果训练数据包含偏见，我们可以预期我们的模型也会模仿这些偏见。一些代表性的例子可以在自然语言处理中的词嵌入领域找到。词嵌入是学习到的词语密集向量表示，旨在捕捉词语的语义信息，然后可以将其输入到机器学习模型中用于不同的下游任务。因此，具有相似含义的词语的嵌入预计应该是“接近”的。
- en: '![Figure](../Images/226ab20e7f0444a5cf42625469fff5d4.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/226ab20e7f0444a5cf42625469fff5d4.png)'
- en: '*Word embeddings can capture the semantic meaning of words. (*[*source*](https://www.ibm.com/blogs/research/2018/11/word-movers-embedding/)*)*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*词嵌入可以捕捉词语的语义含义。 (*[*source*](https://www.ibm.com/blogs/research/2018/11/word-movers-embedding/)*)*'
- en: It turns out that the embedded space can be used to extract relations between
    words, and to find analogies as well. A classic example for this is the [well
    known](https://www.technologyreview.com/2015/09/17/166211/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/)
    king-man+woman=queen equation. However, if we substitute the word “doctor” for
    the word “king” we get “nurse” as the female equivalent of the “doctor”. This
    undesired result simply reflects existing gender biases in our society and history.
    If in most available texts doctors are generally male and nurses are generally
    female, that’s what our model will understand.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，嵌入空间可以用来提取词语之间的关系，并找到类比。一个经典的例子是[著名的](https://www.technologyreview.com/2015/09/17/166211/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/)
    king-man+woman=queen 等式。然而，如果我们将“doctor”替换成“king”，我们会得到“nurse”作为“doctor”的女性对应词。这一不理想的结果简单地反映了我们社会和历史中存在的性别偏见。如果在大多数可用的文本中，医生通常是男性，而护士通常是女性，那么我们的模型也会这样理解。
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Code example:* *man* *is to* *doctor* *as* *woman* *is to* *nurse* *according
    to gensim word2vec* *(*[*source*](https://colab.research.google.com/drive/165qN7RfKByFDlWB6m-E5gRvcSEEodGV0?usp=sharing)*)*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*代码示例：* *man* *对* *doctor* *的类比就像* *woman* *对* *nurse* *的类比，依据 gensim word2vec*
    *(*[*source*](https://colab.research.google.com/drive/165qN7RfKByFDlWB6m-E5gRvcSEEodGV0?usp=sharing)*)*'
- en: Culture Specific Tendencies
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文化特定的倾向
- en: Currently, the most used language on the internet is [English](https://www.statista.com/statistics/262946/share-of-the-most-common-languages-on-the-internet/#:~:text=As%20of%20January%202020%2C%20English,percent%20of%20global%20internet%20users.).
    Much of the research and products in the field of Data Science and ML is done
    in English as well. Thus, many of the “natural” datasets that are used to create
    huge language models tend to match American thought and culture, and may be biased
    towards other nationalities and cultures.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，互联网使用最多的语言是[英语](https://www.statista.com/statistics/262946/share-of-the-most-common-languages-on-the-internet/#:~:text=As%20of%20January%202020%2C%20English,percent%20of%20global%20internet%20users.)。数据科学和机器学习领域的大部分研究和产品也使用英语。因此，许多用于创建巨大语言模型的“自然”数据集往往反映了美国的思维和文化，并可能对其他国籍和文化存在偏见。
- en: '![Figure](../Images/3a6bd2ee7625df70901f1fe33257b7ca.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/3a6bd2ee7625df70901f1fe33257b7ca.png)'
- en: '*Cultural bias: GPT-2 needs active steering in order to produce a positive
    paragraph with the given prompt. (*[*source*](https://blog.einstein.ai/gedi/)*)*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*文化偏见：GPT-2需要主动引导才能生成符合给定提示的积极段落。 (*[*source*](https://blog.einstein.ai/gedi/)*)*'
- en: Synthetic Datasets
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 合成数据集
- en: Some biases in the data may be created unintentionally in the process of the
    dataset’s construction. During construction and evaluation people are more likely
    to notice and pay attention to details they are familiar with. A well known example
    for an image classification mistake, is when Google Photos [misclassified black
    people as gorillas](https://www.wsj.com/articles/BL-DGB-42522). While a single
    misclassification of this sort may not have a strong impact on the overall evaluation
    metrics, it is a sensitive issue, and could have a large impact on the product
    and the way customers relate to it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中的一些偏见可能是在数据集构建过程中无意中产生的。在构建和评估过程中，人们更容易注意到和关注他们熟悉的细节。一个著名的图像分类错误的例子是，当谷歌照片[错误地将黑人分类为猩猩](https://www.wsj.com/articles/BL-DGB-42522)。尽管这种单一的错误分类可能不会对整体评估指标产生强烈影响，但这是一个敏感问题，可能对产品及其客户关系产生重大影响。
- en: '![Figure](../Images/05cedf7c97a4e086de2b0fa7ba670e26.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/05cedf7c97a4e086de2b0fa7ba670e26.png)'
- en: '*Racist AI algorithm? Misclassification of black people as gorillas. (*[*source*](https://www.wsj.com/articles/BL-DGB-42522)*)*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*种族主义AI算法？将黑人误分类为猩猩。（*[*来源*](https://www.wsj.com/articles/BL-DGB-42522)*)*'
- en: In conclusion, no dataset is perfect. Whether a dataset is handcrafted or “natural”,
    it is likely to reflect the biases of it’s creators, and thus the resulting model
    will contain the same biases as well.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，没有数据集是完美的。无论数据集是手工制作的还是“自然”的，它都可能反映出创建者的偏见，因此最终模型也会包含相同的偏见。
- en: Creating fair ML models
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建公平的机器学习模型
- en: There are multiple proposed methods for creating fair ML models, which generally
    fall into one of the following stages.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种提出的方法来创建公平的机器学习模型，这些方法通常分为以下几个阶段之一。
- en: Preprocessing
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理
- en: A naive approach to creating ML models that are unbiased with respect to sensitive
    attributes is to simply remove these attributes from the data, so that the model
    cannot use them for its prediction. However, it is not always straightforward
    to divide attributes into clear cut categories. For example, a person’s name may
    be correlated with their gender or ethnicity, nevertheless we would not necessarily
    want to regard this attribute as sensitive. More sophisticated approaches attempt
    to use dimensionality reduction techniques in order to eliminate sensitive attributes.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的创建无敏感属性偏见的机器学习模型的方法是简单地从数据中删除这些属性，使模型无法使用这些属性进行预测。然而，将属性划分为明确的类别并不总是简单的。例如，一个人的名字可能与他们的性别或民族相关，但我们不一定希望将这个属性视为敏感属性。更复杂的方法尝试使用降维技术来消除敏感属性。
- en: At Training Time
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在训练时
- en: An elegant method for [creating unbiased ML models](https://deepchecks.com/how-to-create-unbiased-ml-models/)
    is using adversarial debiasing. In this method we simultaneously train two models.
    The adversary model is trained to predict the protected attributes given the predictors
    prediction or hidden representation. The predictor is trained to succeed on the
    original task while making the adversary fail, thus minimizing the bias.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 创建[无偏见机器学习模型](https://deepchecks.com/how-to-create-unbiased-ml-models/)的一种优雅方法是使用对抗去偏见技术。在这种方法中，我们同时训练两个模型。对抗模型被训练以预测给定预测者预测或隐藏表示的受保护属性。预测者则被训练以在原始任务中成功，同时使对抗者失败，从而最小化偏见。
- en: '![Figure](../Images/ac04940d304eced71c619b269d1c2468.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/ac04940d304eced71c619b269d1c2468.png)'
- en: '*Adversarial debiasing illustration: The predictor loss function consists of
    two terms, the predictor loss, and the adversarial loss. (*[*source*](https://arxiv.org/pdf/1801.07593.pdf)*)*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*对抗去偏见示意图：预测者损失函数由两个部分组成，预测者损失和对抗损失。（*[*来源*](https://arxiv.org/pdf/1801.07593.pdf)*)*'
- en: This method can achieve great results for debiasing models without having to
    “throw away” the input data, however, it may suffer from difficulties that arise
    in general when training adversarial networks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以在不“丢弃”输入数据的情况下实现很好的去偏见效果，然而，它可能会遇到在训练对抗网络时通常出现的困难。
- en: Post Processing
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 后处理
- en: In the post processing stage we get the model’s predictions as probabilities,
    but we can still choose how to act based on these outputs, for example we can
    move the decision threshold for different groups in order to meet our fairness
    requirements.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在后处理阶段，我们获得模型的预测概率，但我们仍然可以根据这些输出选择如何行动，例如，我们可以为不同的群体调整决策阈值，以满足我们的公平性要求。
- en: One way to ensure model fairness in the post processing stage is to look at
    the intersection of the area under the ROC curve for all groups. The intersection
    represents TPRs and FPRs that can be achieved for all classes simultaneously.
    Note that in order to satisfy the desired result of  equal TPRs and FPRs for all
    classes one might need to purposefully choose to get less good results on some
    of the classes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 确保模型公平性的一种方法是在所有组的 ROC 曲线下面积的交集。交集代表了可以同时为所有类别实现的 TPR 和 FPR。请注意，为了满足所有类别 TPR
    和 FPR 的平等结果，可能需要故意选择在某些类别上取得较差的结果。
- en: '![Figure](../Images/1bd7e256c87e70aaceaf57dc23972269.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/1bd7e256c87e70aaceaf57dc23972269.png)'
- en: '*The colored region is what’s achievable while fulfilling the separability
    criterion for fairness. (*[*source*](https://fairmlbook.org/classification.html)*)*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*彩色区域是实现公平性分离性标准时所能达到的范围。(*[*来源*](https://fairmlbook.org/classification.html)*)*'
- en: 'Another method for debiasing a model in the post processing stage involves
    *calibrating* the predictions for each class independently. *Calibration* is a
    method for ensuring that the probability outputs of a classification model indeed
    reflect the matching ratio of positive labels. Formally, a classification model
    is calibrated if for each value of r:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种在后处理阶段去偏见的方法涉及对每个类别的预测进行 *校准*。*校准* 是确保分类模型的概率输出确实反映正标签匹配比例的方法。形式上，分类模型被称为已校准，如果对于每个
    r 值：
- en: '![Equation](../Images/4261c701bf4517a4b0357b5fd7d1157d.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![方程](../Images/4261c701bf4517a4b0357b5fd7d1157d.png)'
- en: When a model is properly calibrated error rates will be similar across the different
    values of protected attributes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型得到适当校准时，错误率将在不同保护属性值之间相似。
- en: Conclusion
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: To sum it up, we have discussed the concepts of *bias* and *fairness* in the
    ML world, we have seen that model biases often reflect existing biases in society.
    There are various ways in which we could enforce and test for fairness in our
    models, and hopefully, using these methods will lead to more just decision making
    in AI assisted systems around the world.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们讨论了 ML 世界中的 *偏见* 和 *公平性* 概念，看到模型偏见通常反映了社会中现有的偏见。我们可以通过各种方式来强制执行和测试模型的公平性，希望使用这些方法能带来全球
    AI 辅助系统中更公正的决策。
- en: Further Reading
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[Gender bias in word embeddings](https://towardsdatascience.com/gender-bias-word-embeddings-76d9806a0e17)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[词嵌入中的性别偏见](https://towardsdatascience.com/gender-bias-word-embeddings-76d9806a0e17)'
- en: '[Propublica article](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[Propublica 文章](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)'
- en: Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Langford, & Hanna Wallach.
    (2018). A Reductions Approach to Fair Classification.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Langford, & Hanna Wallach.
    (2018). 一种公平分类的减少方法。
- en: Brian Hu Zhang, Blake Lemoine, & Margaret Mitchell. (2018). Mitigating Unwanted
    Biases with Adversarial Learning.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Brian Hu Zhang, Blake Lemoine, & Margaret Mitchell. (2018). 通过对抗学习减轻不必要的偏见。
- en: Solon Barocas, Moritz Hardt, & Arvind Narayanan (2019). *Fairness and Machine
    Learning*. fairmlbook.org.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Solon Barocas, Moritz Hardt, & Arvind Narayanan (2019). *公平性与机器学习*。fairmlbook.org。
- en: Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, & Aram Galstyan.
    (2019). A Survey on Bias and Fairness in Machine Learning.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, & Aram Galstyan.
    (2019). 关于机器学习中的偏见和公平性的调查。
- en: '**Bio: Philip Tannor** is Co-Founder & CEO of [Deepchecks](https://deepchecks.com/).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：Philip Tannor** 是 [Deepchecks](https://deepchecks.com/) 的联合创始人兼首席执行官。'
- en: '**Related:**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Data Protection Techniques Needed to Guarantee Privacy](/2020/10/data-protection-techniques-guarantee-privacy.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[保证隐私所需的数据保护技术](/2020/10/data-protection-techniques-guarantee-privacy.html)'
- en: '[What Makes AI Trustworthy?](/2021/05/what-makes-ai-trustworthy.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么让人工智能值得信赖？](/2021/05/what-makes-ai-trustworthy.html)'
- en: '[Ethics, Fairness, and Bias in AI](/2021/06/ethics-fairness-ai.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能中的伦理、公平性和偏见](/2021/06/ethics-fairness-ai.html)'
- en: More On This Topic
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Building Data Pipelines to Create Apps with Large Language Models](https://www.kdnuggets.com/building-data-pipelines-to-create-apps-with-large-language-models)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[构建数据管道以创建大型语言模型应用程序](https://www.kdnuggets.com/building-data-pipelines-to-create-apps-with-large-language-models)'
- en: '[How to Create a Dataset for Machine Learning](https://www.kdnuggets.com/2022/02/create-dataset-machine-learning.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何为机器学习创建数据集](https://www.kdnuggets.com/2022/02/create-dataset-machine-learning.html)'
- en: '[How to Create a Sampling Plan for Your Data Project](https://www.kdnuggets.com/2022/11/create-sampling-plan-data-project.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何为你的数据项目制定采样计划](https://www.kdnuggets.com/2022/11/create-sampling-plan-data-project.html)'
- en: '[Create Efficient Combined Data Sources with Tableau](https://www.kdnuggets.com/2022/05/create-efficient-combined-data-sources-tableau.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Tableau 创建高效的组合数据源](https://www.kdnuggets.com/2022/05/create-efficient-combined-data-sources-tableau.html)'
- en: '[Use your Data Science Skills to Create 5 Streams of Income](https://www.kdnuggets.com/2023/03/data-science-skills-create-5-streams-income.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用你的数据科学技能创造 5 种收入来源](https://www.kdnuggets.com/2023/03/data-science-skills-create-5-streams-income.html)'
- en: '[Create a Time Series Ratio Analysis Dashboard](https://www.kdnuggets.com/2023/06/wolfer-create-time-series-ratio-analysis-dashboard.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[创建时间序列比率分析仪表盘](https://www.kdnuggets.com/2023/06/wolfer-create-time-series-ratio-analysis-dashboard.html)'
