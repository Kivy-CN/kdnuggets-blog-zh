- en: 'Effective Small Language Models: Microsoft’s 1.3 Billion Parameter phi-1.5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效的小型语言模型：微软的 13 亿参数 phi-1.5
- en: 原文：[https://www.kdnuggets.com/effective-small-language-models-microsoft-phi-15](https://www.kdnuggets.com/effective-small-language-models-microsoft-phi-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/effective-small-language-models-microsoft-phi-15](https://www.kdnuggets.com/effective-small-language-models-microsoft-phi-15)
- en: '![Effective Small Language Models: Microsoft’s 1.3 Billion Parameter phi-1.5](../Images/b849647a0bb3e8b48392d4773f0f727e.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![高效的小型语言模型：微软的 13 亿参数 phi-1.5](../Images/b849647a0bb3e8b48392d4773f0f727e.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: When you thought you had heard enough news about Large Language Models (LLMs),
    Microsoft Research has come out to disturb the market again. In June 2023, Microsoft
    Research released a paper called “[Textbooks is All You Need](https://arxiv.org/abs/2306.11644),”
    where they introduced [phi-1](https://huggingface.co/microsoft/phi-1), a new large
    language model for code. phi-1 is a transformer-based model with 1.3B parameters,
    which was trained for 4 days on 8 A100s GPUs, which used a selection of “textbook
    quality” data from the web.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 当你以为你已经听够了有关大型语言模型（LLMs）的新闻时，微软研究院再次搅动了市场。2023 年 6 月，微软研究院发布了一篇名为 “[教材就是你所需的一切](https://arxiv.org/abs/2306.11644)”
    的论文，在其中他们介绍了 [phi-1](https://huggingface.co/microsoft/phi-1)，一个新的大型代码语言模型。phi-1
    是一个基于 Transformer 的模型，具有 13 亿参数，在 8 个 A100 GPU 上训练了 4 天，使用了来自网络的“教科书质量”数据。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析水平'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你在 IT 领域的组织'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: It seems like LLMs are getting smaller and smaller.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 看来 LLM 正变得越来越小。
- en: What is phi-1.5?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 phi-1.5？
- en: Now Microsoft Research introduces to you [phi-1.5](https://huggingface.co/microsoft/phi-1_5),
    a Transformer with 1.3B parameters, which was trained using the same data sources
    as phi-1\. As stated above, phi-1 was trained on high-quality textbook data, whereas
    phi-1.5 was trained on synthetic data only.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，微软研究院向你介绍 [phi-1.5](https://huggingface.co/microsoft/phi-1_5)，这是一个具有 13 亿参数的
    Transformer，它使用了与 phi-1 相同的数据来源进行训练。如上所述，phi-1 在高质量的教科书数据上进行训练，而 phi-1.5 仅在合成数据上进行训练。
- en: phi-1.5 used 32xA100-40G GPUs and was successfully trained in 8 days. The aim
    behind phi-1.5 was to craft an open-source model that can play a role in the research
    community using a non-restricted small model which allows you to explore the different
    safety challenges with LLMs, such as reducing toxicity, enhancing controllability,
    and more.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: phi-1.5 使用了 32xA100-40G GPU，并在 8 天内成功训练完成。phi-1.5 的目标是打造一个开源模型，它可以在研究社区中发挥作用，使用一个不受限制的小型模型，这样可以探索与
    LLM 相关的不同安全挑战，例如减少有害内容、增强可控性等。
- en: By using the ‘Synthetic Data Generation’ approach, phi-1.5 performance is equivalent
    to models that are 5x larger on natural language tests and has been shown to outperform
    most LLMs on more difficult reasoning tasks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用‘合成数据生成’方法，phi-1.5 在自然语言测试中的表现相当于规模大 5 倍的模型，并且在更困难的推理任务中表现优于大多数 LLM。
- en: Pretty impressive right?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 相当令人印象深刻，对吧？
- en: The model’s learning journey is very interesting. It draws data from a variety
    of sources, including Python code snippets from StackOverflow, synthetic Python
    textbooks as well exercises that were generated by GPT-3.5-turbo-0301.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的学习过程非常有趣。它从多种来源获取数据，包括 StackOverflow 上的 Python 代码片段、合成的 Python 教科书以及由 GPT-3.5-turbo-0301
    生成的练习。
- en: Addressing Toxicity and Biases
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理有害内容和偏见
- en: One of the major challenges with LLMs is toxicity and biased content. Microsoft
    Research aimed to overcome this ongoing challenge of harmful/offensive content
    and content that promotes a specific ideology.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的一个主要挑战是有害内容和偏见内容。微软研究院旨在克服这一持续挑战，即有害/冒犯性内容和推广特定意识形态的内容。
- en: 'The synthetic data used to train the model generated responses with a lower
    propensity for generating toxic content in comparison to other LLMs such as Falcon-7B
    and Llama 2–7B, as shown in the image below:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练模型的合成数据生成的响应，相较于其他LLMs如Falcon-7B和Llama 2–7B，生成有害内容的倾向较低，如下图所示：
- en: '![Effective Small Language Models: Microsoft’s 1.3 Billion Parameter phi-1.5](../Images/d214eb2af724ebdc4c9347a1b547e1e8.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![有效的小型语言模型：微软的13亿参数phi-1.5](../Images/d214eb2af724ebdc4c9347a1b547e1e8.png)'
- en: 'Image via [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/pdf/2309.05463.pdf)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于[教科书就是你所需的II：phi-1.5技术报告](https://arxiv.org/pdf/2309.05463.pdf)
- en: Benchmarks
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准测试
- en: 'The image below shows how phi-1.5 performed slightly better than state-of-the-art
    models, such as Llama 2–7B, Llama-7B, and Falcon-RW-1.3B) on 3 benchmarks: common
    sense reasoning, language skills, and multi-step reasoning.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了phi-1.5在3个基准测试中表现略优于最先进的模型，如Llama 2–7B、Llama-7B和Falcon-RW-1.3B，测试包括常识推理、语言技能和多步骤推理。
- en: '![Effective Small Language Models: Microsoft’s 1.3 Billion Parameter phi-1.5](../Images/44c24b924f8ef1c49cdb6710bc245a03.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![有效的小型语言模型：微软的13亿参数phi-1.5](../Images/44c24b924f8ef1c49cdb6710bc245a03.png)'
- en: 'Image via [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/pdf/2309.05463.pdf)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于[教科书就是你所需的II：phi-1.5技术报告](https://arxiv.org/pdf/2309.05463.pdf)
- en: How was this done?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是怎么做的？
- en: The use of textbook-like data differentiated the use of such data in LLMs in
    comparison to data extracted from the internet. To further assess how the model
    deals with toxic content, ToxiGen was used as well 86 prompts were designed and
    manually labeled ‘pass’, ‘fail’ or ‘did not understand’ to get a better understanding
    of the model's limitations.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 教科书式的数据使用方式使得LLMs中对这种数据的使用与从互联网提取的数据有所不同。为了进一步评估模型如何处理有害内容，使用了ToxiGen，并设计了86个提示，手动标记为“通过”、“失败”或“未理解”，以更好地了解模型的局限性。
- en: With this being said, phi-1.5 passed 47 prompts, failed 34 prompts and did not
    understand 4 prompts. The HumanEval approach to assess the models generates responses
    showing that phi-1.5 scored higher in comparison to other well-known models.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，phi-1.5通过了47个提示，失败了34个提示，并且没有理解4个提示。使用HumanEval方法评估模型的结果显示，phi-1.5的评分高于其他知名模型。
- en: 'Key Takeaways:'
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要收获：
- en: 'Here are the major talking points you should take away from here regarding
    phi-1.5:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是你应该了解的关于phi-1.5的主要要点：
- en: Is a transformer-based model
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是一个基于transformer的模型
- en: Is a LLM that focuses on next-word prediction objectives
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是一个专注于下一个词预测目标的LLM
- en: Was trained on 30 billion tokens
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经过了300亿个token的训练
- en: Used 32xA100-40G GPUs
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用了32xA100-40G GPUs
- en: Was successfully trained in 8 days
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成功在8天内完成训练
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist,
    Freelance Technical Writer and Community Manager at KDnuggets. She is particularly
    interested in providing Data Science career advice or tutorials and theory based
    knowledge around Data Science. She also wishes to explore the different ways Artificial
    Intelligence is/can benefit the longevity of human life. A keen learner, seeking
    to broaden her tech knowledge and writing skills, whilst helping guide others.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** 是一位数据科学家、自由技术撰稿人以及KDnuggets的社区经理。她特别感兴趣于提供数据科学职业建议或教程和理论知识。她还希望探索人工智能如何能够或已经在延长人类寿命方面发挥作用。她是一个热衷学习的人，寻求拓宽技术知识和写作技能，同时帮助指导他人。'
- en: More On This Topic
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Overview of PEFT: State-of-the-art Parameter-Efficient Fine-Tuning](https://www.kdnuggets.com/overview-of-peft-stateoftheart-parameterefficient-finetuning)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PEFT概述：最先进的参数高效微调](https://www.kdnuggets.com/overview-of-peft-stateoftheart-parameterefficient-finetuning)'
- en: '[How to Digest 15 Billion Logs Per Day and Keep Big Queries Within 1 Second](https://www.kdnuggets.com/how-to-digest-15-billion-logs-per-day-and-keep-big-queries-within-1-second)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何每天处理150亿条日志并将大查询保持在1秒以内](https://www.kdnuggets.com/how-to-digest-15-billion-logs-per-day-and-keep-big-queries-within-1-second)'
- en: '[7 Steps to Running a Small Language Model on a Local CPU](https://www.kdnuggets.com/7-steps-to-running-a-small-language-model-on-a-local-cpu)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在本地CPU上运行小型语言模型的7个步骤](https://www.kdnuggets.com/7-steps-to-running-a-small-language-model-on-a-local-cpu)'
- en: '[N-gram Language Modeling in Natural Language Processing](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理中的N-gram语言建模](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
- en: '[Top Open Source Large Language Models](https://www.kdnuggets.com/2022/09/john-snow-top-open-source-large-language-models.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[顶级开源大型语言模型](https://www.kdnuggets.com/2022/09/john-snow-top-open-source-large-language-models.html)'
- en: '[AI: Large Language & Visual Models](https://www.kdnuggets.com/2023/06/ai-large-language-visual-models.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能：大型语言与视觉模型](https://www.kdnuggets.com/2023/06/ai-large-language-visual-models.html)'
