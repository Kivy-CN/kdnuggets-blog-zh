- en: Hyperparameter Optimization for Machine Learning Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型的超参数优化
- en: 原文：[https://www.kdnuggets.com/2020/05/hyperparameter-optimization-machine-learning-models.html](https://www.kdnuggets.com/2020/05/hyperparameter-optimization-machine-learning-models.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/05/hyperparameter-optimization-machine-learning-models.html](https://www.kdnuggets.com/2020/05/hyperparameter-optimization-machine-learning-models.html)
- en: '[comments](#comments)![Figure](../Images/e60360ab2907502e8bfd8adc536d14bf.png)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)![图](../Images/e60360ab2907502e8bfd8adc536d14bf.png)'
- en: '[Credits](https://swisscognitive.ch/2020/01/13/machine-learning-in-adversity/)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[致谢](https://swisscognitive.ch/2020/01/13/machine-learning-in-adversity/)'
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引言
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Model optimization is one of the toughest challenges in the implementation of
    machine learning solutions. Entire branches of machine learning and deep learning
    theory have been dedicated to the optimization of models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 模型优化是实现机器学习解决方案的最艰巨挑战之一。整个机器学习和深度学习理论的分支都致力于模型的优化。
- en: Hyperparameter optimization in machine learning intends to find the hyperparameters
    of a given machine learning algorithm that deliver the best performance as measured
    on a validation set. Hyperparameters, in contrast to model parameters, are set
    by the machine learning engineer before training. The number of trees in a random
    forest is a hyperparameter while the weights in a neural network are model parameters
    learned during training. I like to think of hyperparameters as the model settings
    to be tuned so that the model can optimally solve the machine learning problem.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的超参数优化旨在找到一组超参数，这些参数在验证集上能提供最佳性能。与模型参数不同，超参数由机器学习工程师在训练之前设置。随机森林中的树木数量是一个超参数，而神经网络中的权重是训练期间学习到的模型参数。我喜欢将超参数视为需要调整的模型设置，以便模型能够最佳地解决机器学习问题。
- en: 'Some examples of model hyperparameters include:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型超参数的例子包括：
- en: The `learning rate` for training a neural network.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练神经网络的 `learning rate`。
- en: The `**C**` and `**????**` hyperparameters for support vector machines.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**C**` 和 `**????**` 支持向量机的超参数。'
- en: The `**k**` in k-nearest neighbors.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**k**` 在 k-最近邻算法中。'
- en: Hyperparameter optimization finds a combination of hyperparameters that returns
    an optimal model which reduces a predefined loss function and in turn increases
    the accuracy on given independent data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数优化寻找一组超参数组合，返回一个最佳模型，从而减少预定义的损失函数，并提高在给定独立数据上的准确性。
- en: '![Figure](../Images/1a76857e56c4740636dc8f28879c8971.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/1a76857e56c4740636dc8f28879c8971.png)'
- en: '[Classification models with their respective hyperparameters.](https://www.computer.org/csdl/journal/ts/2018/06/07990590/13rRUx0gerA)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[分类模型及其相应的超参数。](https://www.computer.org/csdl/journal/ts/2018/06/07990590/13rRUx0gerA)'
- en: Hyperparameter Optimization methods
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超参数优化方法
- en: 'Hyperparameters can have a direct impact on the training of machine learning
    algorithms. Thus, to achieve maximal performance, it is important to understand
    how to optimize them. Here are some common strategies for optimizing hyperparameters:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数可以直接影响机器学习算法的训练。因此，为了实现最大性能，了解如何优化它们非常重要。以下是一些优化超参数的常见策略：
- en: '**1\. Manual Hyperparameter Tuning**'
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**1\. 手动超参数调整**'
- en: Traditionally, hyperparameters were tuned manually by trial and error. This **is** still
    commonly done, and experienced engineers can “guess” parameter values that will
    deliver very high accuracy for ML models. However, there isa continual search
    for better, faster, and more automatic methods to optimize hyperparameters.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，超参数通过试错手动调整。这**仍然**很常见，并且经验丰富的工程师可以“猜测”出能够为ML模型提供非常高准确率的参数值。然而，仍在不断寻找更好、更快和更自动化的超参数优化方法。
- en: 2\. Grid Search
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2\. 网格搜索
- en: Grid search is arguably the most basic hyperparameter tuning method. With this
    technique, we simply build a model for each possible combination of all of the
    hyperparameter values provided, evaluating each model, and selecting the architecture
    which produces the best results.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索可以说是最基础的超参数调优方法。使用这种技术，我们简单地为所有提供的超参数值的每一个可能组合构建一个模型，评估每个模型，然后选择产生最佳结果的架构。
- en: '![](../Images/0d65f74594b1f825f12beb47db397245.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d65f74594b1f825f12beb47db397245.png)'
- en: Grid-search does NOT only apply to one model type but can be applied across
    machine learning to calculate the best parameters to use for any given model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索不仅仅适用于一种模型类型，而是可以跨机器学习应用，以计算给定模型的最佳参数。
- en: 'For example, a typical soft-margin SVM classifier equipped with an RBF kernel
    has at least two hyperparameters that need to be optimized for good performance
    on unseen data: a regularization constant *C* and a kernel hyperparameter γ. Both
    parameters are continuous, so to perform grid search, one selects a finite set
    of “reasonable” values for each, let’s say'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，配备RBF核的典型软边界SVM分类器有至少两个需要优化的超参数，以在未见数据上获得良好性能：正则化常数*C*和一个核超参数γ。这两个参数都是连续的，因此为了执行网格搜索，需要为每个参数选择一个有限的“合理”值，假设
- en: '![](../Images/e78be57e455f5dfa0eac00be4c135791.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e78be57e455f5dfa0eac00be4c135791.png)'
- en: Grid search then trains an SVM with each pair (*C*, γ) in the c[artesian product](https://www.wikiwand.com/en/Cartesian_product) of
    these two sets and evaluates their performance on a held-out validation set (or
    by internal cross-validation on the training set, in which case multiple SVMs
    are trained per pair). Finally, the grid search algorithm outputs the settings
    that achieved the highest score in the validation procedure.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索然后在这两个集合的c[笛卡尔积](https://www.wikiwand.com/en/Cartesian_product)中对每一对(*C*,
    γ)训练一个SVM，并在保留的验证集上评估它们的表现（或者在训练集上进行内部交叉验证，此情况下每对参数会训练多个SVM）。最后，网格搜索算法输出在验证过程中获得最高分的设置。
- en: '**How does it work in python?**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**它在Python中如何工作？**'
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here’s a python implementation of grid search using `[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)` from
    the `sklearn` library.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用来自`sklearn`库的`[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)`的网格搜索的Python实现。
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Fitting the Grid Search:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 适应网格搜索：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Methods to Run on Grid-Search:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在网格搜索中运行的方法：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We then use the best set of hyperparameter values chosen in the grid search,
    in the actual model as shown above.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用在网格搜索中选择的最佳超参数值集，如上所示，在实际模型中应用。
- en: One of the **drawbacks** of grid search is that when it comes to dimensionality,
    it suffers when evaluating the number of hyperparameters grows exponentially.
    However, there is no guarantee that the search will produce the perfect solution,
    as it usually finds one by aliasing around the right set.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索的一个**缺点**是当涉及到维度时，当超参数的数量指数级增长时会遭遇困难。然而，并没有保证搜索会产生完美的解决方案，因为它通常通过在正确的集合周围进行别名来找到一个解。
- en: 3\. Random Search
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3\. 随机搜索
- en: Often some of the hyperparameters matter much more than others. Performing random
    search rather than grid search allows a much more precise discovery of good values
    for the important ones.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通常一些超参数比其他的更为重要。进行随机搜索而不是网格搜索可以更精确地发现重要参数的良好值。
- en: '![](../Images/8292a0cb03eeaa9f5cf4f473a2581516.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8292a0cb03eeaa9f5cf4f473a2581516.png)'
- en: Random Search sets up a grid of hyperparameter values and selects random combinations
    to train the model and score. This allows you to explicitly control the number
    of parameter combinations that are attempted. The number of search iterations
    is set based on time or resources. Scikit Learn offers the `RandomizedSearchCV` function
    for this process.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索设置一个超参数值的网格，并选择随机组合来训练模型和评分。这允许你明确控制尝试的参数组合数量。搜索迭代的次数是基于时间或资源设置的。Scikit
    Learn提供了`RandomizedSearchCV`函数来实现这一过程。
- en: While it’s possible that `RandomizedSearchCV` will not find as accurate a result
    as `GridSearchCV`, it surprisingly picks the best result more often than not and
    in a *fraction* of the time it takes `GridSearchCV` would have taken. Given the
    same resources, Randomized Search can even outperform Grid Search. This can be
    visualized in the graphic below when continuous parameters are used.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`RandomizedSearchCV`可能不会找到像`GridSearchCV`那样准确的结果，但它惊人地更常选择最佳结果，并且用`GridSearchCV`所需时间的*一小部分*。在相同资源下，随机搜索甚至可以超越网格搜索。这可以在下图中通过使用连续参数来可视化。
- en: The chances of finding the optimal parameter are comparatively higher in random
    search because of the random search pattern where the model might end up being
    trained on the optimized parameters without any aliasing. Random search works
    best for lower dimensional data since the time taken to find the right set is
    less with less number of iterations. Random search is the best parameter search
    technique when there is less number of dimensions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索找到最佳参数的机会相对较高，因为随机搜索模式下，模型可能会被训练在优化的参数上而没有任何混叠。随机搜索最适合低维数据，因为找到正确集合所需的时间较少，迭代次数也较少。对于维度较少的情况，随机搜索是最好的参数搜索技术。
- en: In the case of deep learning algorithms, it outperforms the grid search.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习算法的情况下，它超越了网格搜索。
- en: '![Figure](../Images/b1b1600a34c9a27779b36bbb9d9a02cc.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/b1b1600a34c9a27779b36bbb9d9a02cc.png)'
- en: '[Credits](https://community.alteryx.com/t5/Data-Science-Blog/Hyperparameter-Tuning-Black-Magic/ba-p/449289)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[致谢](https://community.alteryx.com/t5/Data-Science-Blog/Hyperparameter-Tuning-Black-Magic/ba-p/449289)'
- en: In the above figure, yay that you have two parameters, with 5x6 grid search
    you check only 5 different parameter values from each of the parameters (six rows
    and five columns on the plot on the left), while with the random search you check
    14 different parameter values of each of the parameters.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，如果你有两个参数，5x6 网格搜索只检查每个参数的5个不同值（左侧图中的六行五列），而随机搜索则检查每个参数的14个不同值。
- en: '**How does it work in python?**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**它在 Python 中是如何工作的？**'
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here’s a python implementation of grid search using `RandomizedSearchCV` of
    the `sklearn` library.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用`sklearn`库的`RandomizedSearchCV`进行网格搜索的 Python 实现。
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Fitting the Random Search:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索拟合：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Methods to Run on Grid-Search:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索运行方法：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 4\. Bayesian Optimization
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4\. 贝叶斯优化
- en: The previous two methods performed individual experiments building models with
    various hyperparameter values and recording the model performance for each. Because
    each experiment was performed in isolation, it’s very easy to parallelize this
    process. However, because each experiment was performed in isolation, we’re not
    able to use the information from one experiment to improve the next experiment.
    Bayesian optimization belongs to a class of *sequential model-based optimization* (SMBO)
    algorithms that allow for one to use the results of our previous iteration to
    improve our sampling method of the next experiment.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 前两种方法执行了各自的实验，构建了具有不同超参数值的模型，并记录了每个模型的性能。由于每个实验都是独立进行的，因此很容易并行化这个过程。然而，由于每个实验都是独立的，我们无法利用一个实验的信息来改进下一个实验。贝叶斯优化属于*基于模型的序列优化*（SMBO）算法的一类，它允许我们利用前一次迭代的结果来改进下一个实验的采样方法。
- en: This, in turn, limits the number of times a model needs to be trained for validation
    as solely those settings that are expected to generate a higher validation score
    are passed through for evaluation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这反过来限制了模型需要为验证而训练的次数，因为只有那些预计会产生更高验证得分的设置才会被传递进行评估。
- en: Bayesian optimization works by constructing a posterior distribution of functions
    (Gaussian process) that best describes the function you want to optimize. As the
    number of observations grows, the posterior distribution improves, and the algorithm
    becomes more certain of which regions in parameter space are worth exploring and
    which are not.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化通过构建一个后验分布（高斯过程），来最佳描述你想要优化的函数。随着观察数量的增加，后验分布会得到改善，算法对哪些参数空间的区域值得探索以及哪些不值得探索变得更加确定。
- en: 'We can see this in the image below:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下面的图像中看到这一点：
- en: '![Figure](../Images/23d29016155406c3b70ff0644e409dd3.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/23d29016155406c3b70ff0644e409dd3.png)'
- en: Source: [bayesian-optimization](https://github.com/fmfn/BayesianOptimization)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[bayesian-optimization](https://github.com/fmfn/BayesianOptimization)
- en: As you iterate over and over, the algorithm balances its needs of exploration
    and exploitation taking into account what it knows about the target function.
    At each step, a Gaussian Process is fitted to the known samples (points previously
    explored), and the posterior distribution, combined with an exploration strategy
    (such as UCB (Upper Confidence Bound), or EI (Expected Improvement)), is used
    to determine the next point that should be explored.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当你不断迭代时，算法在探索和利用的需求之间取得平衡，考虑到它对目标函数的了解。在每一步，都会将高斯过程拟合到已知样本（之前探索过的点），然后结合探索策略（如
    UCB（上置信界）或 EI（期望改进）），来确定下一个应该探索的点。
- en: Using Bayesian Optimization, we can explore the parameter space more smartly,
    and thus reduce the time required to do this process.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝叶斯优化，我们可以更智能地探索参数空间，从而减少完成这一过程所需的时间。
- en: 'You can check the python implementation of Bayesian optimization below:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看下面的贝叶斯优化的 Python 实现：
- en: '[**thuijskens/bayesian-optimization**](https://github.com/thuijskens/bayesian-optimization/blob/master/ipython-notebooks/svm-optimization.ipynb)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[**thuijskens/bayesian-optimization**](https://github.com/thuijskens/bayesian-optimization/blob/master/ipython-notebooks/svm-optimization.ipynb)'
- en: 5\. Gradient-based Optimization
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5\. 基于梯度的优化
- en: It is specially used in the case of Neural Networks. It computes the gradient
    with respect to hyperparameters and optimizes them using the gradient descent
    algorithm.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 它特别用于神经网络的情况。它计算超参数的梯度，并使用梯度下降算法优化它们。
- en: The calculation of the gradient is the least of problems. At least in times
    of advanced [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) software.
    (**Implementing this in a general way for all sklearn-classifiers, of course,
    is not easy**).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 计算梯度是问题中最小的部分。至少在先进的 [自动微分](https://en.wikipedia.org/wiki/Automatic_differentiation)
    软件时代。（**以通用方式为所有 sklearn 分类器实现这一点，当然是不容易的**）。
- en: '![Figure](../Images/43a7177eaea30f7a4c2b619f758eb304.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/43a7177eaea30f7a4c2b619f758eb304.png)'
- en: '[Credits](https://www.mathworks.com/matlabcentral/fileexchange/27631-derivative-based-optimization)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[致谢](https://www.mathworks.com/matlabcentral/fileexchange/27631-derivative-based-optimization)'
- en: 'And while there are works of people who used this kind of idea, they only did
    this for some specific and well-formulated problem (e.g. SVM-tuning). Furthermore,
    there probably were a lot of assumptions because:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然已经有一些人使用过这种理念的研究，但他们仅限于一些特定且明确的问题（例如 SVM 调优）。此外，可能有许多假设，因为：
- en: Why is this not a good idea?
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么这不是一个好主意？
- en: '**1\. Hyperparameter optimization is in general non-smooth**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 超参数优化通常是不平滑的**'
- en: GD really likes smooth functions as a gradient of zero is not helpful
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GD 非常喜欢平滑的函数，因为零梯度没有帮助。
- en: Each hyper-parameter which is defined by some discrete-set (e.g. choice of l1
    vs. l2 penalization) introduces non-smooth surfaces.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一个由离散集合定义的超参数（例如选择 l1 或 l2 惩罚）都会引入不平滑的表面。
- en: '**2\. Hyperparameter optimization is in general non-convex**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 超参数优化通常是非凸的**'
- en: The whole convergence-theory of gradient descent assumes, that the underlying
    problem is convex.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降的整个收敛理论假设底层问题是凸的。
- en: 'Good-case: you obtain some local-minimum (can be arbitrarily bad).'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳情况：你得到一些局部最小值（可能非常差）。
- en: 'Worst-case: gradient descent is not even converging to some local-minimum.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最坏的情况：梯度下降甚至无法收敛到局部最小值。
- en: To get python implementation and more about the Gradient Descent Optimization
    algorithm [click here.](https://hackernoon.com/implementing-different-variants-of-gradient-descent-optimization-algorithm-in-python-using-numpy-809e7ab3bab4)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取 Python 实现和更多关于梯度下降优化算法的信息 [点击这里。](https://hackernoon.com/implementing-different-variants-of-gradient-descent-optimization-algorithm-in-python-using-numpy-809e7ab3bab4)
- en: '**6\. Evolutionary Optimization**'
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**6\. 进化优化**'
- en: Evolutionary optimization follows a process inspired by the biological concept
    of evolution and since natural evolution is a dynamic process in a changing environment,
    they are also well suited to dynamic optimization problems.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 进化优化遵循一种受生物进化概念启发的过程，由于自然进化是一个在不断变化的环境中进行的动态过程，因此它们也非常适合动态优化问题。
- en: '![](../Images/801484956ac7dd0a3414aecce35df824.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/801484956ac7dd0a3414aecce35df824.png)'
- en: Evolutionary algorithms are often used to find good approximate solutions that
    cannot be easily solved by other techniques. Optimization problems often don’t
    have an exact solution as it may be too time-consuming and computationally intensive
    to find an optimal solution. However, evolutionary algorithms are ideal in such
    situations as they can be used to find a near-optimal solution which is often
    sufficient.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 进化算法通常用于寻找无法通过其他技术轻松解决的良好近似解。优化问题通常没有精确的解决方案，因为找到最优解决方案可能过于耗时和计算密集。然而，在这种情况下，进化算法是理想的，因为它们可以用来找到一个接近最优的解决方案，这通常是足够的。
- en: One advantage of evolutionary algorithms is that they develop solutions free
    of any human misconceptions or biases, which means they can produce surprising
    ideas which we might never generate ourselves.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 进化算法的一个优点是它们能够开发出没有任何人为误解或偏见的解决方案，这意味着它们可以产生一些我们自己可能永远无法想到的惊人想法。
- en: You can learn more about evolutionary algorithms [here](https://en.wikipedia.org/wiki/Evolutionary_algorithm).
    You can also check python implementation [here](https://github.com/MorvanZhou/Evolutionary-Algorithm).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[这里](https://en.wikipedia.org/wiki/Evolutionary_algorithm)了解更多关于进化算法的内容。你也可以在[这里](https://github.com/MorvanZhou/Evolutionary-Algorithm)查看
    Python 实现。
- en: As a general rule of thumb, any time you want to optimize tuning hyperparameters,
    think Grid Search and Randomized Search!
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一般来说，每当你想优化超参数调优时，就想想网格搜索和随机搜索！
- en: Conclusion
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we’ve learned that finding the right values for hyperparameters
    can be a frustrating task and can lead to underfitting or overfitting machine
    learning models. We saw how this hurdle can be overcome by using Grid Search &
    Randomized Search and other algorithms — which optimize tuning of hyperparameters
    to save time and eliminate the chance of overfitting or underfitting by random
    guessing.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们了解到找到超参数的正确值可能是一个令人沮丧的任务，并可能导致机器学习模型的欠拟合或过拟合。我们看到如何通过使用网格搜索、随机搜索和其他算法来克服这一障碍——这些算法优化超参数调优，以节省时间并消除随机猜测导致的过拟合或欠拟合的可能性。
- en: Well, this concludes this article**. **I hope you guys have enjoyed reading
    it, feel free to share your comments/thoughts/feedback in the comment section.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，这篇文章就到此为止**。**希望你们喜欢阅读这篇文章，欢迎在评论区分享你们的意见/想法/反馈。
- en: Thanks for reading !!!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！！！
- en: '**Bio: [Nagesh Singh Chauhan](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)**
    is a Data Science enthusiast. Interested in Big Data, Python, Machine Learning.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[纳戈什·辛格·乔汉](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)**
    是一位数据科学爱好者。对大数据、Python 和机器学习感兴趣。'
- en: '[Original](https://medium.com/swlh/hyperparameter-optimization-for-machine-learning-models-12582f00ae52).
    Reposted with permission.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/swlh/hyperparameter-optimization-for-machine-learning-models-12582f00ae52)。转载已获许可。'
- en: '**Related:**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[How to Do Hyperparameter Tuning on Any Python Script in 3 Easy Steps](/2020/04/hyperparameter-tuning-python.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在 3 个简单步骤中对任何 Python 脚本进行超参数调优](/2020/04/hyperparameter-tuning-python.html)'
- en: '[Coronavirus COVID-19 Genome Analysis using Biopython](/2020/04/coronavirus-covid-19-genome-analysis-biopython.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Biopython 进行冠状病毒 COVID-19 基因组分析](/2020/04/coronavirus-covid-19-genome-analysis-biopython.html)'
- en: '[Practical Hyperparameter Optimization](/2020/02/practical-hyperparameter-optimization.html)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实用的超参数优化](/2020/02/practical-hyperparameter-optimization.html)'
- en: More On This Topic
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Hyperparameter Optimization: 10 Top Python Libraries](https://www.kdnuggets.com/2023/01/hyperparameter-optimization-10-top-python-libraries.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[超参数优化：10 个顶级 Python 库](https://www.kdnuggets.com/2023/01/hyperparameter-optimization-10-top-python-libraries.html)'
- en: '[Machine Learning Pipeline Optimization with TPOT](https://www.kdnuggets.com/2021/05/machine-learning-pipeline-optimization-tpot.html)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 TPOT 进行机器学习管道优化](https://www.kdnuggets.com/2021/05/machine-learning-pipeline-optimization-tpot.html)'
- en: '[Hyperparameter Tuning Using Grid Search and Random Search in Python](https://www.kdnuggets.com/2022/10/hyperparameter-tuning-grid-search-random-search-python.html)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用网格搜索和随机搜索进行超参数调优](https://www.kdnuggets.com/2022/10/hyperparameter-tuning-grid-search-random-search-python.html)'
- en: '[Hyperparameter Tuning: GridSearchCV and RandomizedSearchCV, Explained](https://www.kdnuggets.com/hyperparameter-tuning-gridsearchcv-and-randomizedsearchcv-explained)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[超参数调优：GridSearchCV 和 RandomizedSearchCV 解释](https://www.kdnuggets.com/hyperparameter-tuning-gridsearchcv-and-randomizedsearchcv-explained)'
- en: '[SQL Query Optimization Techniques](https://www.kdnuggets.com/2023/03/sql-query-optimization-techniques.html)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQL 查询优化技巧](https://www.kdnuggets.com/2023/03/sql-query-optimization-techniques.html)'
- en: '[Database Optimization: Exploring Indexes in SQL](https://www.kdnuggets.com/2023/07/database-optimization-exploring-indexes-sql.html)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据库优化：探索 SQL 中的索引](https://www.kdnuggets.com/2023/07/database-optimization-exploring-indexes-sql.html)'
