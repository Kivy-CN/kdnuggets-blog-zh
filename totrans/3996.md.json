["```py\npip install -U nltk \n```", "```py\nimport nltk from nltk.tokenize\nimport word_tokenize\n\n# Download the necessary resources\nnltk.download('punkt')\n\ntext = \"The fruit in the table is a banana\"\ntokens = word_tokenize(text)\n\nprint(tokens) \n```", "```py\nOutput>> \n['The', 'fruit', 'in', 'the', 'table', 'is', 'a', 'banana'] \n```", "```py\nfrom nltk.tag import pos_tag\n\nnltk.download('averaged_perceptron_tagger')\n\ntext = \"The fruit in the table is a banana\"\npos_tags = pos_tag(tokens)\n\nprint(pos_tags) \n```", "```py\nOutput>>\n[('The', 'DT'), ('fruit', 'NN'), ('in', 'IN'), ('the', 'DT'), ('table', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('banana', 'NN')] \n```", "```py\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nnltk.download('wordnet')\nnltk.download('punkt')\n\ntext = \"The striped bats are hanging on their feet for best\"\ntokens = word_tokenize(text)\n\n# Stemming\nstemmer = PorterStemmer()\nstems = [stemmer.stem(token) for token in tokens]\nprint(\"Stems:\", stems)\n\n# Lemmatization\nlemmatizer = WordNetLemmatizer()\nlemmas = [lemmatizer.lemmatize(token) for token in tokens]\nprint(\"Lemmas:\", lemmas) \n```", "```py\nOutput>> \nStems: ['the', 'stripe', 'bat', 'are', 'hang', 'on', 'their', 'feet', 'for', 'best']\nLemmas: ['The', 'striped', 'bat', 'are', 'hanging', 'on', 'their', 'foot', 'for', 'best'] \n```", "```py\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ntext = \"Brad is working in the U.K. Startup called AIForLife for 7 Months.\"\ndoc = nlp(text)\n#Perform the NER\nfor ent in doc.ents:\n    print(ent.text, ent.label_) \n```", "```py\nOutput>>\nBrad PERSON\nthe U.K. Startup ORG\n7 Months DATE \n```", "```py\nimport spacy\nfrom spacy import displacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ntext = \"SpaCy excels at dependency parsing.\"\ndoc = nlp(text)\nfor token in doc:\n    print(f\"{token.text}: {token.dep_}, {token.head.text}\")\n\ndisplacy.render(doc, jupyter=True) \n```", "```py\nOutput>> \nBrad: nsubj, working\nis: aux, working\nworking: ROOT, working\nin: prep, working\nthe: det, Startup\nU.K.: compound, Startup\nStartup: pobj, in\ncalled: advcl, working\nAIForLife: oprd, called\nfor: prep, called\n7: nummod, Months\nMonths: pobj, for\n.: punct, working \n```", "```py\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndoc1 = nlp(\"I like pizza\")\ndoc2 = nlp(\"I love hamburger\")\n\n# Calculate similarity\nsimilarity = doc1.similarity(doc2)\nprint(\"Similarity:\", similarity) \n```", "```py\nOutput>>\nSimilarity: 0.6159097609586724 \n```", "```py\npip install -U textblob\npython -m textblob.download_corpora \n```", "```py\nfrom textblob import TextBlob\n\ntext = \"I am in the top of the world\"\nblob = TextBlob(text)\nsentiment = blob.sentiment\n\nprint(sentiment) \n```", "```py\nOutput>>\nSentiment(polarity=0.5, subjectivity=0.5) \n```", "```py\nfrom textblob import TextBlob\n\ntext = \"I havv goood speling.\"\nblob = TextBlob(text)\n\n# Spelling Correction\ncorrected_blob = blob.correct()\nprint(\"Corrected Text:\", corrected_blob) \n```", "```py\nOutput>>\nCorrected Text: I have good spelling. \n```", "```py\npip install gensim \n```", "```py\nimport gensim\nfrom gensim import corpora\nfrom gensim.models import LdaModel\n\n# Sample documents\ndocuments = [\n    \"Tennis is my favorite sport to play.\",\n    \"Football is a popular competition in certain country.\",\n    \"There are many athletes currently training for the olympic.\"\n]\n\n# Preprocess documents\ntexts = [[word for word in document.lower().split()] for document in documents]\n\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\n\n#The LDA model\nlda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=15)\n\ntopics = lda_model.print_topics()\nfor topic in topics:\n    print(topic) \n```", "```py\nOutput>>\n(0, '0.073*\"there\" + 0.073*\"currently\" + 0.073*\"olympic.\" + 0.073*\"the\" + 0.073*\"athletes\" + 0.073*\"for\" + 0.073*\"training\" + 0.073*\"many\" + 0.073*\"are\" + 0.025*\"is\"')\n(1, '0.094*\"is\" + 0.057*\"football\" + 0.057*\"certain\" + 0.057*\"popular\" + 0.057*\"a\" + 0.057*\"competition\" + 0.057*\"country.\" + 0.057*\"in\" + 0.057*\"favorite\" + 0.057*\"tennis\"') \n```", "```py\nimport gensim\nfrom gensim.models import Word2Vec\n\n# Sample sentences\nsentences = [\n    ['machine', 'learning'],\n    ['deep', 'learning', 'models'],\n    ['natural', 'language', 'processing']\n]\n\n# Train Word2Vec model\nmodel = Word2Vec(sentences, vector_size=20, window=5, min_count=1, workers=4)\n\nvector = model.wv['machine']\nprint(vector) \n```", "```py\n Output>>\n[ 0.01174188 -0.02259516  0.04194366 -0.04929082  0.0338232   0.01457208\n -0.02466416  0.02199094 -0.00869787  0.03355692  0.04982425 -0.02181222\n -0.00299669 -0.02847819  0.01925411  0.01393313  0.03445538  0.03050548\n  0.04769249  0.04636709] \n```"]