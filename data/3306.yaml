- en: 'Machine Learning Crash Course: Part 1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习速成课程：第一部分
- en: 原文：[https://www.kdnuggets.com/2017/05/machine-learning-crash-course-part-1.html](https://www.kdnuggets.com/2017/05/machine-learning-crash-course-part-1.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/05/machine-learning-crash-course-part-1.html](https://www.kdnuggets.com/2017/05/machine-learning-crash-course-part-1.html)
- en: '**By Daniel Geng and Shannon Shih, UC Berkeley.**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**由丹尼尔·耿和香农·史密斯，UC 伯克利。**'
- en: '**Introduction, Regression/Classification, Cost Functions, and Gradient Descent**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**介绍、回归/分类、成本函数和梯度下降**'
- en: 'Machine learning (ML) has received a lot of attention recently, and not without
    good reason. It has already revolutionized fields from image recognition to healthcare
    to transportation. Yet a typical explanation for machine learning sounds like
    this:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）最近受到了很多关注，这绝非没有理由。它已经彻底改变了从图像识别到医疗保健再到交通运输等多个领域。然而，典型的机器学习解释通常是这样的：
- en: “A computer program is said to learn from experience *E* with respect to some
    class of tasks *T* and performance measure *P* if its performance at tasks in
    *T*, as measured by *P*, improves with experience *E*.”
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “一个计算机程序被称为从经验*E*中学习，涉及某些任务类别*T*和性能测量*P*，如果它在*T*中的任务表现，如*P*所测量的那样，随着经验*E*的增加而改善。”
- en: Not very clear, is it? This post, the first in a series of ML tutorials, aims
    to make machine learning accessible to anyone willing to learn. We’ve designed
    it to give you a solid understanding of how ML algorithms work as well as provide
    you the knowledge to harness it in your projects.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 不太清楚，对吧？这篇文章是机器学习教程系列中的第一篇，旨在使机器学习对任何愿意学习的人都变得可及。我们设计它是为了让你对机器学习算法如何工作有一个扎实的理解，并提供你在项目中利用它的知识。
- en: So what is Machine Learning?
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 那么，什么是机器学习？
- en: 'At its core, machine learning is not a difficult concept to grasp. In fact,
    the vast majority of machine learning algorithms are concerned with just one simple
    task: drawing lines. In particular, machine learning is all about drawing lines
    through *data*. What does that mean? Let’s look at a simple example.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，机器学习并不是一个难以理解的概念。事实上，绝大多数的机器学习算法都关注一个简单的任务：绘制线条。特别是，机器学习的核心就是在*数据*中绘制线条。这是什么意思呢？我们来看看一个简单的例子。
- en: Classification
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类
- en: 'Let’s say you’re a computer with a collection of apple and orange images. From
    each image you can infer the color and size of a fruit, and you want to classify
    the images as either an image of an apple or an orange. The first step in many
    machine learning algorithms is to obtain **labeled training data**. In our example,
    this means getting a large number of images of fruit each labeled as either being
    an apple or an orange. From these images, we can extract the color and size information
    and then see how they correlate with being an apple or an orange. For example,
    graphing our labeled training data might look like something this:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你是一台计算机，拥有一组苹果和橙子的图像。从每张图像中，你可以推断出水果的颜色和大小，并且你想将这些图像分类为苹果或橙子。许多机器学习算法的第一步是获取**标记的训练数据**。在我们的例子中，这意味着获得大量的水果图像，每张图像都标记为苹果或橙子。从这些图像中，我们可以提取颜色和大小信息，然后观察它们与苹果或橙子的关系。例如，我们标记的训练数据可能会绘制成如下图样：
- en: '![](../Images/a387d319ffeb0c4fa59ad5e29d4074d2.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a387d319ffeb0c4fa59ad5e29d4074d2.png)'
- en: The red x’s are labeled apples and the orange x’s are labeled oranges. As you’ll
    probably notice there’s a pattern in the data. Apples seem to congregate on the
    left side of the graph because they’re mostly red, and oranges seem to congregate
    on the right side because they’re mostly orange. We want our algorithm to learn
    these types of patterns.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 红色的 x 标记的是苹果，橙色的 x 标记的是橙子。你可能会注意到数据中存在一个模式。苹果似乎集中在图表的左侧，因为它们大多是红色的，而橙子则集中在右侧，因为它们大多是橙色的。我们希望我们的算法能够学习这些类型的模式。
- en: 'For this particular problem, our goal is to create an algorithm that draws
    a line between the two labeled groups, called a **decision boundary**. The simplest
    decision boundary for our data might look something like this:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个特定问题，我们的目标是创建一个算法，在两个标记的组之间绘制一条线，这条线称为**决策边界**。我们数据的最简单决策边界可能看起来像这样：
- en: '![](../Images/38cece74c80ce71dd532286c795ae5a0.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38cece74c80ce71dd532286c795ae5a0.png)'
- en: 'Just a straight line between the apples and the oranges. However, much more
    complicated machine learning algorithms may end up drawing much more complicated
    decision boundaries such as this:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 只是一条在苹果和橙子之间的直线。然而，更复杂的机器学习算法可能最终会绘制出更复杂的决策边界，如下所示：
- en: '![](../Images/250f46298856796d339c0b6fd9429d8d.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/250f46298856796d339c0b6fd9429d8d.png)'
- en: 'Our assumption is that the line we’ve drawn to distinguish an apple image from
    an orange image in our *labeled training data* above will be able to distinguish
    an apple from an orange in any image. **In other words, by giving our algorithm
    examples of apples and oranges to learn from, it can generalize its experience
    to images of apples and oranges that it has never encountered before.** For instance,
    if we were given an image of a fruit, represented by the blue X below, we could
    classify it as an orange based on the decision boundary we drew:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的假设是，我们在上面的*标记训练数据*中绘制的用于区分苹果图像和橙子图像的线条，将能够在任何图像中区分苹果和橙子。**换句话说，通过给我们的算法提供苹果和橙子的示例，它可以将经验推广到它以前从未遇到过的苹果和橙子的图像上。**
    例如，如果我们给出一个水果的图像，如下面的蓝色 X，我们可以根据我们绘制的决策边界将其分类为橙子：
- en: '![](../Images/1666829e250366f626ee75ef803eb249.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1666829e250366f626ee75ef803eb249.png)'
- en: This is the power of machine learning. We take some training data, run a machine
    learning algorithm which draws a decision boundary on the data, and then extrapolate
    what we’ve learned to completely new pieces of data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是机器学习的威力。我们获取一些训练数据，运行一个机器学习算法，绘制出数据上的决策边界，然后将我们学到的知识推广到全新的数据上。
- en: Of course, distinguishing between apples and oranges is quite a mundane task.
    However, we can apply this strategy to much more exciting problems, such as classifying
    tumors as malignant or benign, marking emails as spam or not spam, or analyzing
    fingerprints for security systems. This type of machine learning—drawing lines
    to *separate* data—is just one subfield of machine learning, called **classification**.
    Another subfield, called **regression,** is all about drawing lines that *describe*
    data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，区分苹果和橙子是一个相当平凡的任务。然而，我们可以将这种策略应用于更令人兴奋的问题，例如将肿瘤分类为恶性或良性，将电子邮件标记为垃圾邮件或非垃圾邮件，或分析指纹以用于安全系统。这种类型的机器学习——绘制线条以*分隔*数据——只是机器学习的一个子领域，称为**分类**。另一个子领域，称为**回归**，则专注于绘制能*描述*数据的线条。
- en: Regression
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回归
- en: 'Say we have some labeled training data. In particular, let’s say we have the
    price of various houses versus their square footage. If we visualize the information
    as a graph, it looks like this:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一些标记好的训练数据。特别是，假设我们有各种房子的价格与其平方面积的关系。如果我们将这些信息可视化成图表，它看起来是这样的：
- en: '![](../Images/c55ec432a2a39d3b9429298b38c25a82.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c55ec432a2a39d3b9429298b38c25a82.png)'
- en: 'Each of the X’s represents a different house with some price and some square
    footage. Notice that although there is some variation in the data (in other words,
    each data point is a bit scattered), there is also a pattern: as houses get bigger,
    they also become more expensive. We want our algorithm to find and use this pattern
    to predict house prices based on house size.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 每一个 X 代表了一个不同的房子，每个房子都有不同的价格和平方面积。注意到虽然数据中有一些变化（换句话说，每个数据点有些散乱），但也存在一个模式：房子越大，价格也越高。我们希望我们的算法能找到并利用这一模式，根据房屋大小预测房价。
- en: Just by looking at the training data intuitively we can see that there is a
    diagonal strip in the graph that most houses seem to land on. We can generalize
    this idea and say that *all* houses will have a high probability of being on the
    diagonal cluster of data points. For example, there is a pretty high chance of
    a house being at the green X in the graph below and a pretty low chance that a
    house would be at the red X in the graph below.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 仅通过直观地观察训练数据，我们可以看到图表中大多数房子似乎落在一个对角线区域上。我们可以概括这个想法，认为*所有*的房子都有很高的概率出现在对角线的数据点簇上。例如，在下面的图表中，房子处于绿色
    X 位置的可能性很高，而处于红色 X 位置的可能性则很低。
- en: '![](../Images/626b435612aeebffcd8ebf0ac190ee6d.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/626b435612aeebffcd8ebf0ac190ee6d.png)'
- en: Now we can generalize even more and ask, for any given square footage, how much
    will a house be worth? Of course, it would be very hard to get an exact answer.
    However, an approximate answer is much easier to get. To do this, we draw a line
    through the cluster of data, as close as possible to each data point. This line,
    called a **predictor**, predicts the price of a house from its square footage.
    For any point on the predictor, there is a high chance that a house of that square
    footage has that price. In a sense, we can say that the predictor represents an
    “average” of house prices for a given footage.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以进一步概括，问一下，对于任意给定的平方英尺，房子的价值是多少？当然，得到一个精确的答案是非常困难的。然而，得到一个近似答案要容易得多。为此，我们绘制一条线穿过数据簇，尽可能靠近每个数据点。这条线称为**预测器**，根据房子的平方英尺预测房价。对于预测器上的任何点，高概率的情况下，该平方英尺的房子会有那个价格。从某种意义上说，我们可以说预测器代表了给定平方英尺房子的“平均”价格。
- en: '![](../Images/70f95bdcda2ba2120efef4b3fefa9823.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70f95bdcda2ba2120efef4b3fefa9823.png)'
- en: The predictor doesn’t necessarily have to be linear. It can be any type of function,
    or model, you can imagine—quadratic, sinusoidal, and even arbitrary functions
    will work. However, using the most complex model for a predictor won’t always
    work; different functions work better for different problems, and it’s up to the
    programmer to figure out what kind of model to use.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 预测器不一定非得是线性的。它可以是你能想象到的任何类型的函数或模型——二次函数、正弦函数，甚至是任意函数都可以使用。然而，使用最复杂的模型作为预测器并不总是有效；不同的函数对不同的问题更为有效，程序员需要找出合适的模型。
- en: 'Looking back at our model for house price we could ask: why limit ourselves
    to just one input variable? Turns out we can consider as many types of information
    as we want, such as the cost of living in the city, condition, building material,
    and so on. For example, we can plot the price against the cost of living in the
    house’s location and its square footage on a single graph like this, where the
    vertical axis plots price, and the two horizontal axes plot square footage and
    cost of living:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们关于房价的模型，我们可以问：为什么要限制自己只使用一个输入变量？实际上，我们可以考虑任意多种信息，例如城市的生活成本、房屋的状况、建筑材料等等。例如，我们可以将价格与房屋所在位置的生活成本和平方英尺在一个图表上绘制，其中纵轴表示价格，两个横轴分别表示平方英尺和生活成本：
- en: <https://i.imgur.com/9C5ZeYa.mp4?_=1>
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: <https://i.imgur.com/9C5ZeYa.mp4?_=1>
- en: '[https://i.imgur.com/9C5ZeYa.mp4](https://i.imgur.com/9C5ZeYa.mp4)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://i.imgur.com/9C5ZeYa.mp4](https://i.imgur.com/9C5ZeYa.mp4)'
- en: In this case we can again fit a predictor to the data. But instead of drawing
    a line through the data we have to draw a plane through the data because the function
    that best predicts the housing price is a function of two variables.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以再次将预测器拟合到数据中。但不是绘制一条线通过数据，我们需要绘制一个平面，因为最能预测房价的函数是一个二变量函数。
- en: <https://i.imgur.com/m2cTv6d.mp4?_=2>
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: <https://i.imgur.com/m2cTv6d.mp4?_=2>
- en: '[https://i.imgur.com/m2cTv6d.mp4](https://i.imgur.com/m2cTv6d.mp4)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://i.imgur.com/m2cTv6d.mp4](https://i.imgur.com/m2cTv6d.mp4)'
- en: So we’ve seen examples of one and two input variables, but many machine learning
    applications take into account hundreds and even thousands of variables. Although
    humans are regrettably unable to visualize anything higher than three dimensions,
    the same principles we just learned will apply to those systems.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了一些关于一个和两个输入变量的示例，但许多机器学习应用需要考虑数百甚至数千个变量。虽然人类遗憾地无法可视化超过三维的情况，但我们刚刚学到的相同原则也适用于这些系统。
- en: The Predictor
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测器
- en: 'As we mentioned earlier, there are many different types of predictors. In our
    example with house prices, we used a linear model to approximate our data. The
    mathematical form of a linear predictor looks something like this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，有许多不同类型的预测器。在我们关于房价的例子中，我们使用了线性模型来逼近我们的数据。线性预测器的数学形式大致如下：
- en: '![Equation](../Images/e02cc4770e26d5e9ec32892fd8ec10db.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![方程式](../Images/e02cc4770e26d5e9ec32892fd8ec10db.png)'
- en: Each *x* represents a different input feature, such as square footage or cost
    of living, and each *c* is called a parameter or a weight. The greater a particular
    weight is, the more the model considers its corresponding feature. For example,
    square footage is a good predictor of house prices, so our algorithm should give
    square footage a lot of consideration by increasing the coefficient associated
    with square footage. In contrast, if our data included the number of power outlets
    in the house, our algorithm will probably give it a relatively low weight because
    the number of outlets doesn’t have much to do with the price of a house.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每个*x*代表一个不同的输入特征，如平方英尺或生活成本，每个*c*被称为参数或权重。某个特定权重越大，模型对其对应特征的考虑就越多。例如，平方英尺是预测房价的一个很好的指标，因此我们的算法应通过增加与平方英尺相关的系数来给予平方英尺更多考虑。相比之下，如果我们的数据包括房间的电源插座数量，我们的算法可能会给予它相对较低的权重，因为插座数量与房价的关系不大。
- en: 'In our example of predicting house prices based on square footage, since we’re
    only considering one variable our model only needs one input feature, or just
    one x:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们基于平方英尺预测房价的例子中，由于我们只考虑一个变量，我们的模型只需要一个输入特征，即一个x：
- en: '![Equation](../Images/f59fb89f5a42f90a7918d3e48820f192.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![方程](../Images/f59fb89f5a42f90a7918d3e48820f192.png)'
- en: 'This equation is probably more recognizable in this form:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程式可能在这个形式下更容易识别：
- en: '![Equation](../Images/b1aea269c89c8b7e89ab82f8e7faffd6.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![方程](../Images/b1aea269c89c8b7e89ab82f8e7faffd6.png)'
- en: '*y(x)* is our output, or in this case the price of a house, and *x* is our
    feature, or in this case the size of the house. *c[0]* is the y intercept, to
    account for the base price of the house.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*y(x)*是我们的输出，在这种情况下是房价，而*x*是我们的特征，在这种情况下是房子的大小。*c[0]*是y截距，用于考虑房子的基础价格。'
- en: 'Now the question becomes: How does a machine learning algorithm choose *c[2]*
    and *c[1]* so that the line best predicts house prices?'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是：机器学习算法如何选择*c[2]*和*c[1]*以使直线能够最好地预测房价？
- en: '* It’s worth noting here that the coefficients can actually be found directly
    and very efficiently through a matrix relation called the [normal equation](http://mathworld.wolfram.com/NormalEquation.html).
    However, since this method becomes impractical when working with hundreds or thousands
    of variables, we’ll be using the method machine learning algorithms often use.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '* 值得注意的是，系数实际上可以通过一种叫做[正规方程](http://mathworld.wolfram.com/NormalEquation.html)的矩阵关系直接且非常高效地找到。然而，由于这种方法在处理数百或数千个变量时变得不切实际，我们将使用机器学习算法常用的方法。'
- en: Cost Functions
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 成本函数
- en: The key to determining what parameters to choose to best approximate the data
    is to find a way to characterize how “wrong” our predictor is. We do this by using
    a **cost function** (or a **loss function**). A cost function takes a line and
    a set of data, and returns a value called the **cost**. If the line approximates
    the data well the cost will be low, and if the line approximates the data poorly
    the cost will be high.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 确定选择哪些参数以最佳近似数据的关键在于找到一种方法来描述我们预测器的“错误”程度。我们通过使用**成本函数**（或**损失函数**）来实现这一点。成本函数接受一条直线和一组数据，并返回一个称为**成本**的值。如果直线对数据的近似较好，成本会很低；如果直线对数据的近似较差，成本会很高。
- en: 'The best predictor will minimize the output of the cost function, or in other
    words, it will minimize the cost. To visualize this, let’s look at the three predictor
    functions below:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳预测器将最小化成本函数的输出，换句话说，它将最小化成本。为了可视化这一点，让我们看看下面的三个预测函数：
- en: '![](../Images/c7f433ebd9f99c6f345babd7473fc5e1.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7f433ebd9f99c6f345babd7473fc5e1.png)'
- en: Predictors *a* and *c* don’t really fit the data very well, and our cost function
    should give the two lines a high cost. On the other hand, predictor *b* seems
    to fit the data very well, and as a result our cost function should give it a
    very low cost.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 预测器*a*和*c*与数据的拟合程度并不好，我们的成本函数应该给这两条直线一个高成本。另一方面，预测器*b*似乎与数据拟合得非常好，因此我们的成本函数应该给它一个非常低的成本。
- en: To play around with an interactive visualization of linear regression, [please
    see the original post](https://ml.berkeley.edu/blog/2016/11/06/tutorial-1/).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看线性回归的交互式可视化，[请查看原始帖子](https://ml.berkeley.edu/blog/2016/11/06/tutorial-1/)。
- en: So just what is the cost function? There are actually many different types of
    cost functions we can use, but for this example we’ll stick to a very commonly
    used one called the **mean squared error**.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，成本函数到底是什么？实际上，我们可以使用许多不同类型的成本函数，但在这个例子中，我们将使用一个非常常用的叫做**均方误差**的函数。
- en: Let’s break down the name “mean squared error.” Error in this case means the
    vertical distance between a data point and a predictor, or just the difference
    *( x[i] - y[i] )*. We can visualize the error by using the graph below, where
    each of the bars is a different *( x[i] - y[i] )*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们拆解一下“均方误差”这个名字。在这里，误差指的是数据点与预测值之间的垂直距离，或者只是差值 *( x[i] - y[i] )*。我们可以使用下面的图来可视化误差，其中每一个条形图代表不同的
    *( x[i] - y[i] )*。
- en: '![](../Images/9153ea088522de8a3022c36a6bed045b.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9153ea088522de8a3022c36a6bed045b.png)'
- en: 'So for a single data point *( x[i] , y[i] )*, where *x[i]* is the square footage
    of the house and *y[i]* is the price of the house, and a predictor *y(x)* the
    squared error is:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个数据点 *( x[i] , y[i] )*，其中 *x[i]* 是房屋的面积，而 *y[i]* 是房屋的价格，预测器 *y(x)* 的平方误差是：
- en: '![Equation](../Images/a000239d4905831c739605fcee3a6e0c.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![公式](../Images/a000239d4905831c739605fcee3a6e0c.png)'
- en: 'The nice thing about the square of the error is that everything is positive.
    This way we can actually minimize the squared error. Now we take the mean, or
    the average, over all the data points to get the mean squared error:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 误差的平方值的好处在于一切都是正值。这样我们可以实际最小化平方误差。现在我们对所有数据点取均值，即得到均方误差：
- en: '![Equation](../Images/318c18158dabf8c819f6e02f6f76abf6.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![公式](../Images/318c18158dabf8c819f6e02f6f76abf6.png)'
- en: Here, we’ve summed up all of the squared errors, and divided by N, which is
    the number of data points we have, which is just the average of the squared errors.
    Hence, the mean squared error.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们总结了所有的平方误差，并除以N，即我们拥有的数据点的数量，这只是平方误差的平均值。因此，就是均方误差。
- en: Gradient Descent
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度下降
- en: 'When we graph the cost function (with only two variables) it will look something
    like this:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们绘制成本函数（只有两个变量）时，它会看起来像这样：
- en: '![](../Images/320b648014287fa2b888287aa66d5bef.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/320b648014287fa2b888287aa66d5bef.png)'
- en: Now, it’s pretty evident where the minimum of this cost function is. We can
    just eyeball it. However, remember we only have one feature—square footage. In
    reality, almost all applications for modern machine learning algorithms take in
    much more than just one feature. In some cases, up to [tens of millions](https://www.nvidia.cn/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf)
    of parameters are used—have fun trying to picture a ten million dimensional space!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，很明显这个成本函数的最小值在哪里。我们可以用肉眼估算。然而，请记住我们只有一个特征——面积。在现实中，几乎所有现代机器学习算法的应用都涉及多个特征。在某些情况下，使用的参数多达[tens
    of millions](https://www.nvidia.cn/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf)——试图想象一个千万维的空间真是太有趣了！
- en: So, in order to find the minimum of very high dimensional cost functions, we’ll
    use an algorithm called **gradient descent**. As we’ll see soon gradient descent
    has a very intuitive explanation in two dimensions, but the algorithm also generalizes
    readily to any number of dimensions.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，为了找到高维成本函数的最小值，我们将使用一种叫做**梯度下降**的算法。正如我们很快将看到的那样，梯度下降在二维空间中有一个非常直观的解释，但该算法也可以很容易地推广到任意维数。
- en: To begin, imagine rolling a ball along the cost function graph. As the ball
    rolls, it will always follow the steepest route, eventually coming to rest at
    the bottom. In a nutshell, that’s what gradient descent is. We pick any point
    on the graph, find the direction that has the steepest slope, move a tiny bit
    in that direction, and repeat. Eventually, we necessarily have to reach a minimum
    of the cost function. And because that point is the minimum of the cost function,
    it’s also the parameters that we’ll use for drawing our line.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，想象一下在成本函数图上滚动一个球。当球滚动时，它总是会沿着最陡的路线前进，最终停在底部。简而言之，这就是梯度下降。我们选择图上的任何一点，找出最陡的方向，朝着那个方向移动一点，然后重复这个过程。最终，我们必然会达到成本函数的最小值。因为那个点是成本函数的最小值，所以它也是我们用来绘制直线的参数。
- en: So, what did I just learn?
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 那么，我刚刚学到了什么？
- en: So after reading all of that, hopefully machine learning is starting to make
    more sense to you right now. And hopefully, it doesn’t seem as complicated as
    you once thought it was. Just remember machine learning is *literally just drawing
    lines through training data*. We decide what purpose the line services, such as
    a **decision boundary** in a **classification** algorithm, or a **predictor**
    that models real-world behavior. And these lines in turn just come from finding
    the minimum of a **cost function** using **gradient descent**.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在阅读了这些内容之后，希望机器学习现在开始对你变得更有意义。也希望，它不像你曾经认为的那样复杂。只要记住，机器学习*实际上只是通过训练数据绘制线条*。我们决定线条服务的目的，比如在**分类**算法中的**决策边界**，或者模拟现实世界行为的**预测器**。这些线条则来源于使用**梯度下降**找到**成本函数**的最小值。
- en: Put another way, really machine learning is just pattern recognition. ML algorithms
    learn patterns by drawing lines through training data, and then generalizes the
    patterns it sees to new data. But that begs the question, is machine learning
    actually “learning”? Well, who’s to say learning isn’t just pattern recognition?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，实际上机器学习只是模式识别。机器学习算法通过在训练数据中绘制线条来学习模式，然后将其看到的模式推广到新数据。但这引出了一个问题，机器学习是否真的在“学习”？谁能说学习不是模式识别呢？
- en: '**Editor''s note:** If you enjoyed this post, you may want to check out parts
    2 and 3 on the [Machine Learning @ Berkeley blog](https://ml.berkeley.edu/blog):'
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**编辑注:** 如果你喜欢这篇文章，可能会想查看[Machine Learning @ Berkeley blog](https://ml.berkeley.edu/blog)的第2部分和第3部分：'
- en: ''
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Machine Learning Crash Course: Part 2](https://ml.berkeley.edu/blog/2016/12/24/tutorial-2/)'
  id: totrans-73
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习速成课程：第2部分](https://ml.berkeley.edu/blog/2016/12/24/tutorial-2/)'
- en: '[Machine Learning Crash Course: Part 3](https://ml.berkeley.edu/blog/2017/02/04/tutorial-3/)'
  id: totrans-74
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习速成课程：第3部分](https://ml.berkeley.edu/blog/2017/02/04/tutorial-3/)'
- en: ''
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**[Daniel Geng](https://dangeng.github.io/)** is a freshman at UC Berkeley
    who hails from Ann Arbor, Michigan. You can catch him coding, math-ing, physics-ing
    or meme-ing somewhere with a view.'
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**[Daniel Geng](https://dangeng.github.io/)** 是来自密歇根州安娜堡的UC Berkeley新生。你可以在某个有风景的地方看到他编程、做数学、物理或玩梗。'
- en: ''
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**[Shannon Shih](http://shannonsh.github.io/)** is an artist and a programmer,
    and a Network Engineering Assistant at UC Berkeley.'
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**[Shannon Shih](http://shannonsh.github.io/)** 是一位艺术家和程序员，也是UC Berkeley的网络工程助理。'
- en: ''
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Original](https://ml.berkeley.edu/blog/2016/11/06/tutorial-1/). Reposted with
    permission.'
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[原文](https://ml.berkeley.edu/blog/2016/11/06/tutorial-1/)。经许可转载。'
- en: '**Related:**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关:**'
- en: '[Keep it simple! How to understand Gradient Descent algorithm](/2017/04/simple-understand-gradient-descent-algorithm.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[保持简单！如何理解梯度下降算法](/2017/04/simple-understand-gradient-descent-algorithm.html)'
- en: '[Machine Learning Key Terms, Explained](/2016/05/machine-learning-key-terms-explained.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习关键术语解释](/2016/05/machine-learning-key-terms-explained.html)'
- en: '[7 More Steps to Mastering Machine Learning With Python](/2017/03/seven-more-steps-machine-learning-python.html)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握Python机器学习的7个步骤](/2017/03/seven-more-steps-machine-learning-python.html)'
- en: '* * *'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行IT管理'
- en: '* * *'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Free Artificial Intelligence And Deep Learning Crash Course](https://www.kdnuggets.com/2022/07/free-artificial-intelligence-deep-learning-crash-course.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[免费人工智能和深度学习速成课程](https://www.kdnuggets.com/2022/07/free-artificial-intelligence-deep-learning-crash-course.html)'
- en: '[Free Python Crash Course](https://www.kdnuggets.com/2022/07/free-python-crash-course.html)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[免费Python速成课程](https://www.kdnuggets.com/2022/07/free-python-crash-course.html)'
- en: '[Free MLOps Crash Course for Beginners](https://www.kdnuggets.com/2022/08/free-mlops-crash-course.html)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[免费MLOps速成课程](https://www.kdnuggets.com/2022/08/free-mlops-crash-course.html)'
- en: '[Free Intermediate Python Programming Crash Course](https://www.kdnuggets.com/2022/12/free-intermediate-python-programming-crash-course.html)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[免费的中级 Python 编程速成课程](https://www.kdnuggets.com/2022/12/free-intermediate-python-programming-crash-course.html)'
- en: '[Unlock Your Potential with This FREE DevOps Crash Course](https://www.kdnuggets.com/2023/03/corise-unlock-potential-with-this-free-devops-crash-course.html)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过这个免费的 DevOps 速成课程释放你的潜力](https://www.kdnuggets.com/2023/03/corise-unlock-potential-with-this-free-devops-crash-course.html)'
- en: '[Hands-on Reinforcement Learning Course Part 3: SARSA](https://www.kdnuggets.com/2022/01/handson-reinforcement-learning-course-part-3-sarsa.html)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实用强化学习课程第 3 部分：SARSA](https://www.kdnuggets.com/2022/01/handson-reinforcement-learning-course-part-3-sarsa.html)'
