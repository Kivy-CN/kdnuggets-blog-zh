- en: There is No Free Lunch in Data Science
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学中没有免费的午餐
- en: 原文：[https://www.kdnuggets.com/2019/09/no-free-lunch-data-science.html](https://www.kdnuggets.com/2019/09/no-free-lunch-data-science.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/09/no-free-lunch-data-science.html](https://www.kdnuggets.com/2019/09/no-free-lunch-data-science.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Sydney Firmin](https://www.linkedin.com/in/sydney-firmin-4369a65b/),
    Alteryx**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由[悉尼·费尔敏](https://www.linkedin.com/in/sydney-firmin-4369a65b/)，Alteryx**。'
- en: During your adventures in machine learning, you may have already come across
    the “No Free Lunch” Theorem. Borrowing its name from the adage "[there ain’t no
    such thing as a free lunch](https://en.wikipedia.org/wiki/There_ain%27t_no_such_thing_as_a_free_lunch)," the [mathematical
    folklore theorem](https://en.wikipedia.org/wiki/Mathematical_folklore) describes
    the phenomena that there is no single algorithm that is best suited for all possible
    scenarios and data sets.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在你机器学习的历程中，你可能已经遇到过“没有免费午餐”定理。借用这一定理的名字来源于谚语“[没有免费的午餐](https://en.wikipedia.org/wiki/There_ain%27t_no_such_thing_as_a_free_lunch)”，[数学民间定理](https://en.wikipedia.org/wiki/Mathematical_folklore)描述了一个现象：没有一种算法能在所有可能的场景和数据集上表现最佳。
- en: The No Free Lunch Theorem(s)
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 没有免费午餐定理
- en: '* * *'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的快车道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织IT'
- en: '* * *'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'There are, generally speaking, two No Free Lunch (NFL) theorems: one for machine
    learning and one for search and optimization. These two theorems are related and
    tend to be bundled into one general axiom (the folklore theorem).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，有两个“没有免费午餐”（NFL）定理：一个是机器学习的，另一个是搜索和优化的。这两个定理是相关的，通常被合并为一个通用的公理（民间定理）。
- en: Although many different researchers have contributed to the collective publications
    on the No Free Lunch theorems, the most prevalent name associated with these works
    is David Wolpert.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多不同的研究人员为“没有免费午餐”定理的集体出版物做出了贡献，但与这些工作最相关的名字是David Wolpert。
- en: '**No Free Lunch for Supervised Machine Learning**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**没有监督学习的免费午餐**'
- en: In his 1996 paper [The Lack of A Priori Distinctions Between Learning Algorithms](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.390.9412&rep=rep1&type=pdf),
    David Wolpert examines if it is possible to get useful theoretical results with
    a training data set and a learning algorithm without making any assumptions about
    the target variable.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在他1996年的论文[《学习算法之间的先验区分缺失》](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.390.9412&rep=rep1&type=pdf)中，David
    Wolpert研究了在不对目标变量做任何假设的情况下，是否可以通过训练数据集和学习算法获得有用的理论结果。
- en: What Wolpert proves in his 1996 paper (through a series of mathematical proofs)
    is that given a noise-free data set (i.e., no random variation, only trend) and
    a machine learning algorithm where the cost function is the error rate, all machine
    learning algorithms are equivalent when assessed with a [generalization error
    rate](https://en.wikipedia.org/wiki/Generalization_error) (the model’s error rate
    on a validation data set).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Wolpert在他1996年的论文中（通过一系列数学证明）证明了：在给定一个无噪声数据集（即，没有随机变化，只有趋势）和一个以错误率为成本函数的机器学习算法时，当用[泛化误差率](https://en.wikipedia.org/wiki/Generalization_error)（模型在验证数据集上的误差率）进行评估时，所有机器学习算法都是等效的。
- en: Extending this logic (again, Wolpert does it with math equations), he demonstrates
    that for any two algorithms, A and B, there are as many scenarios where A will
    perform worse than B as there are where A will outperform B. This even holds true
    when one of the given algorithms is random guessing. Wolpert proved that for all
    possible domains (all possible problem instances drawn from a uniform probability
    distribution), the average performance for algorithms A and B is the same.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 延伸这一逻辑（同样，沃尔珀特用数学方程来说明），他证明了对于任意两个算法A和B，有同样多的场景中A的表现会比B差，也有同样多的场景中A的表现会优于B。这一点即使在其中一个算法是随机猜测时也成立。沃尔珀特证明了对于所有可能的领域（从均匀概率分布中提取的所有可能问题实例），算法A和B的平均性能是相同的。
- en: '![](../Images/8c6033a7e2b0fc288269ff8f6338bda7.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c6033a7e2b0fc288269ff8f6338bda7.png)'
- en: This is because nearly all ([non-rote](https://en.wikipedia.org/wiki/Rote_learning#In_computer_science))
    machine learning algorithms make some assumptions (known as [inductive or learning
    bias](https://en.wikipedia.org/wiki/Inductive_bias)) about the relationships between
    the predictor and target variables, introducing [bias](https://en.wikipedia.org/wiki/Bias) into
    the model. The assumptions made by machine learning algorithms mean that some
    algorithms will fit certain data sets better than others. It also (by definition)
    means that there will be as many data sets that a given algorithm will not be
    able to model effectively. How effective a model will be is directly dependent
    on how well the assumptions made by the model fit the true nature of the data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为几乎所有的（[非机械记忆](https://en.wikipedia.org/wiki/Rote_learning#In_computer_science)）机器学习算法对预测变量和目标变量之间的关系做出了一些假设（称为 [归纳或学习偏差](https://en.wikipedia.org/wiki/Inductive_bias)），从而将 [偏差](https://en.wikipedia.org/wiki/Bias) 引入模型中。机器学习算法所做的假设意味着某些算法在某些数据集上的拟合效果优于其他算法。这也（按定义）意味着会有许多数据集是某个给定算法无法有效建模的。模型的有效性直接依赖于模型所做的假设与数据的真实特性之间的契合程度。
- en: Coming back to the lunch of it all, you can’t get good machine learning “for
    free.” You must use knowledge about your data and the context of the world we
    live in (or the world your data lives in) to select an appropriate machine learning
    model. There is no such thing as a single, universally-best machine learning algorithm,
    and there are no context or usage-independent ([a priori](https://en.wikipedia.org/wiki/A_priori_and_a_posteriori))
    reasons to favor one algorithm over all others.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 回到午餐的话题，你不能“免费”获得良好的机器学习。你必须利用你对数据及我们所生活的世界（或数据所在的世界）的知识来选择一个合适的机器学习模型。没有所谓的单一、普遍最佳的机器学习算法，也没有任何与背景或用途无关的（[先验](https://en.wikipedia.org/wiki/A_priori_and_a_posteriori)）理由去偏向某一个算法。
- en: Fun fact, this 1996 paper is a mathematical formalization of the work of the
    philosopher [David Hume](https://en.wikipedia.org/wiki/David_Hume). I promise
    I will circle back to this a little later.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这篇1996年的论文是对哲学家 [大卫·休谟](https://en.wikipedia.org/wiki/David_Hume) 工作的数学化形式。我保证稍后会回到这个话题。
- en: '**No Free Lunch for Search and Optimization Either**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**没有免费的午餐**用于搜索和优化'
- en: Wolpert and Macready’s 1997 paper [No Free Lunch Theorems for Optimization](https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf) demonstrates
    the application of similar “no free lunch” logic to the mathematical fields of
    search and optimization.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 沃尔珀特和麦克雷迪在1997年的论文 [优化的无免费午餐定理](https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf) 展示了类似的“无免费午餐”逻辑在数学领域中的应用，即搜索和优化。
- en: The No Free Lunch theorems for search and optimization demonstrate that for
    search/optimization problems in a limited search space, where the points being
    searched through are *not* resampled, the performance of any two given search
    algorithms over all possible problems is the same.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 无免费午餐定理对于搜索和优化表明，对于在有限搜索空间中的搜索/优化问题，当被搜索的点 *不* 被重新采样时，任何两个给定的搜索算法在所有可能问题上的性能是相同的。
- en: As an example, consider that you are on foot, looking for the highest point
    (extremum) on the earth. To find it, you would likely always make a choice to
    go in the direction that elevation increased, acting as a “hill-climber.”
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你在徒步寻找地球上的最高点（极值）。为了找到它，你可能总是选择朝着海拔升高的方向前进，表现得像是一个“爬山者”。
- en: This would work fine on Earth, where the highest point is a part of a very tall
    mountain range.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这在地球上会很好用，因为地球上最高的点是非常高的山脉的一部分。
- en: '![](../Images/e1de396b0d18eb1387393ab0e1dbfdf1.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1de396b0d18eb1387393ab0e1dbfdf1.png)'
- en: Now, imagine a world where the highest point on the planet is surrounded by
    the lowest points on the planet.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下一个世界，其中地球上最高的点被地球上最低的点所包围。
- en: '![](../Images/3da553d6b50555fa7eb0ae68a5a9a464.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3da553d6b50555fa7eb0ae68a5a9a464.png)'
- en: '*This guy is headed for disappointment.*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*这个家伙注定会失望。*'
- en: In this scenario, a search where you always go down in elevation would perform
    better.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，始终下降海拔的搜索会表现得更好。
- en: Neither hill climbing nor hill descending performs better on both planets, meaning
    that a random search would be just as good (on average). Without being able to
    assume that the highest point on a planet will occur next to other high points
    (a smooth surface), we can’t know that hill climbing will perform better than
    descending.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是爬山算法还是下坡算法在两个星球上都没有表现更好，这意味着随机搜索效果也一样（平均而言）。如果不能假设一个星球上的最高点会出现在其他高点附近（一个平滑的表面），我们无法知道爬山算法是否会比下坡算法表现更好。
- en: 'What this all boils down to: if no prior assumptions about the optimization
    program can be made, no optimization strategy can be expected to perform better
    than any other strategy (including random searching). A general purpose universal
    optimization strategy is theoretically impossible, and the only way one strategy
    can outperform another is for it to be specialized for the specific problem at
    hand. Sound familiar? There is no free efficient search optimization either.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 归结起来：如果对优化程序没有先验假设，则没有优化策略可以预期比其他策略（包括随机搜索）表现更好。理论上，通用的优化策略是不可能实现的，唯一能使一种策略优于另一种策略的方法是让它专门针对当前特定问题。听起来熟悉吗？也没有免费的高效搜索优化。
- en: In his 2012 paper [What the No Free Lunch Theorems Really mean; How to Improve
    Search Algorithms](https://sfi-edu.s3.amazonaws.com/sfi-edu/production/uploads/sfi-com/dev/uploads/filer/33/44/33440e97-fe46-4827-a1eb-a27196e1c49a/12-10-017.pdf),
    Wolpert discusses the close relationship between search and supervised learning
    and its implications to the No Free Lunch theorems. He demonstrates that, in the
    context of the No Free Lunch theorem, supervised learning is closely analogous
    to search/optimization. For these two theorems, “search algorithm” is seemingly
    interchangeable with “learning algorithm” and “objective function” with “target
    function.”
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的2012年论文 [无免费午餐定理的真正含义；如何改进搜索算法](https://sfi-edu.s3.amazonaws.com/sfi-edu/production/uploads/sfi-com/dev/uploads/filer/33/44/33440e97-fe46-4827-a1eb-a27196e1c49a/12-10-017.pdf) 中，Wolpert
    讨论了搜索与监督学习之间的紧密关系及其对无免费午餐定理的影响。他证明，在无免费午餐定理的背景下，监督学习与搜索/优化非常相似。对于这两个定理，“搜索算法”似乎可以与“学习算法”互换，而“目标函数”与“目标函数”也是如此。
- en: 'If you are interested in knowing more about each of the No Free Lunch theorems,
    there is a great summary resource on [http://www.no-free-lunch.org/](http://www.no-free-lunch.org/),
    including a [powerpoint deck](http://www.no-free-lunch.org/coev.pdf) from David
    Wolpert on an overview of the No Free Lunch Theorems. For a more philosophical
    explanation of the No Free Lunch Theorem(s) give the note [Notice: No Free Lunches
    for Anyone, Bayesians Included](http://www.no-free-lunch.org/Fors99.pdf) a read.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对无免费午餐定理的每个方面感兴趣，可以参考 [http://www.no-free-lunch.org/](http://www.no-free-lunch.org/)
    上的一个很好的总结资源，包括 David Wolpert 关于无免费午餐定理概述的 [幻灯片](http://www.no-free-lunch.org/coev.pdf)。如果你想要更哲学性的解释，可以阅读 [注意：没有免费的午餐，贝叶斯人也不例外](http://www.no-free-lunch.org/Fors99.pdf)。
- en: Are We Sure About No Free Lunch?
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们对无免费午餐的认识是否准确？
- en: The No Free Lunch theorems have sparked a lot of research and academic publications.
    These publications are not always in favor of No Free Lunch (because who doesn’t
    want a free lunch)?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 无免费午餐定理 引发了大量的研究和学术出版物。这些出版物并不总是支持无免费午餐（因为谁不想要一个免费午餐呢？）。
- en: The “all possible problems” component of the no free lunch theorems is the first
    sticking point. All possible problems don’t reflect the conditions of the real
    world. In fact, many would argue that problems taken on by machine learning and
    optimization, “real world” problems, are nothing like many of the domains included
    in all problems. The real world tends to exhibit patterns and structures, where
    the “all possible problems” space includes scenarios that are entirely random
    or chaotic.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: “所有可能的问题”这一部分是无免费午餐定理的第一个难点。所有可能的问题并不反映现实世界的条件。实际上，许多人会认为，机器学习和优化所面对的“现实世界”问题，与所有问题中包含的许多领域完全不同。现实世界往往展示出某种模式和结构，而“所有可能的问题”空间则包含了完全随机或混乱的情境。
- en: Researchers also argue [against taking a uniform average](https://pdfs.semanticscholar.org/83cd/86c2c7e507e8ebba9563a9efaba7c966a1b3.pdf) of
    performance on philosophical grounds. If you have one hand in boiling water, and
    one hand froze in an ice block, on average the temperature of your hands is fine.
    Averaging performance of algorithms might cause trouble where one algorithm performs
    well on most problems but epically fails on a few edge cases.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员还从哲学的角度辩称 [反对取统一平均值](https://pdfs.semanticscholar.org/83cd/86c2c7e507e8ebba9563a9efaba7c966a1b3.pdf)来评估表现。如果你一只手放在热水中，另一只手冻在冰块里，平均而言你两只手的温度看起来正常。对算法表现进行平均可能会导致问题，因为某些算法在大多数问题上表现良好，但在一些边缘情况中却会失败得非常惨烈。
- en: In empirical research, it has been demonstrated that some algorithms consistently
    perform better than others.  In the 2014 paper [Do we Need Hundreds of Classifiers
    to Solve Real-World Classification Problems](http://jmlr.org/papers/volume15/delgado14a/delgado14a.pdf),
    researchers found that for all of the data sets in the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.html?format=&task=&att=&area=life&numAtt=&numIns=&type=&sort=nameUp&view=table) (121
    data sets), the family of classifiers that perform best includes random forest,
    support vector machines (SVM), neural networks, and boosting ensembles.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在实证研究中，已经证明一些算法的表现 consistently 优于其他算法。在2014年的论文 [我们是否需要数百个分类器来解决现实世界的分类问题](http://jmlr.org/papers/volume15/delgado14a/delgado14a.pdf)中，研究人员发现，在所有的 [UCI机器学习库](https://archive.ics.uci.edu/ml/datasets.html?format=&task=&att=&area=life&numAtt=&numIns=&type=&sort=nameUp&view=table) (121个数据集)中，表现最佳的分类器家族包括随机森林、支持向量机(SVM)、神经网络和提升集成方法。
- en: This is an important consideration, as it does seem that some algorithms seem
    to consistently top the Kaggle leaderboards or get implemented into production
    environments more than others. However, the No Free Lunch theorem is an important
    reminder about the assumptions each model makes, and the (often subconscious)
    assumptions we make about data and the environment it was gathered in.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个重要的考虑，因为似乎一些算法 consistently 排名前列或更频繁地被实施到生产环境中。然而，“没有免费的午餐”定理提醒我们每个模型所做的假设，以及我们（通常是无意识的）对数据及其采集环境所做的假设。
- en: '**No Free Lunch and David Hume**'
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**没有免费的午餐与大卫·休谟**'
- en: Remember when (back about 800 words) I promised I would circle back to the old
    dead philosopher? This is that section.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得我在大约800字前承诺会回到那个已故哲学家吗？这就是那个部分。
- en: '![](../Images/8e5ea3f3b34dde006bceb3c1d54f818f.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e5ea3f3b34dde006bceb3c1d54f818f.png)'
- en: '*This guy…*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*这个家伙…*'
- en: '[David Hume](https://en.wikipedia.org/wiki/David_Hume) was an 18^(th)-century philosopher
    from Edinburgh, particularly interested in skepticism and empiricism. He did a
    lot of thinking about the [problem of induction](https://en.wikipedia.org/wiki/Problem_of_induction),
    which falls under the field of epistemology (the study of knowledge).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[大卫·休谟](https://en.wikipedia.org/wiki/David_Hume)是18^(th)世纪的爱丁堡哲学家，特别关注怀疑主义和经验主义。他对[归纳问题](https://en.wikipedia.org/wiki/Problem_of_induction)进行了大量思考，这属于认识论（知识研究）领域。'
- en: 'To get into the problem of induction, let’s first split the world into two
    different types of knowledge: a priori and a posteriori (this division is going
    back to the work of [Immanuel Kant](https://en.wikipedia.org/wiki/Immanuel_Kant)).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了深入探讨归纳问题，让我们首先将世界分为两种不同类型的知识：先验和后验（这一划分可以追溯到[伊曼努尔·康德](https://en.wikipedia.org/wiki/Immanuel_Kant)的研究）。
- en: A priori knowledge is knowledge that exists independently of experience. This
    is the type of knowledge that is based on the relation of ideas, for example,
    2 + 2 = 4, or an octagon has eight sides. This type of knowledge will be true
    regardless of what the world is like outside.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 先验知识是指独立于经验而存在的知识。这种知识基于思想的关系，例如，2 + 2 = 4，或者一个八边形有八条边。这种知识无论外部世界如何都是真实的。
- en: A posteriori knowledge or “matters of fact” are types of knowledge that require
    experience or empirical evidence. This is the type of knowledge that makes up
    most personal and scientific knowledge. A posteriori knowledge requires observation.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 后验知识或“事实问题”是指需要经验或实证证据的知识。这种知识构成了大多数个人和科学知识。后验知识需要观察。
- en: '[Inductive reasoning](https://en.wikipedia.org/wiki/Inductive_reasoning) is
    a method of reasoning for a posteriori knowledge in which we apply observed regular
    patterns to instances we haven’t directly observed; e.g., all rain I have observed
    is wet, therefore, all rain I haven’t observed must be wet, too.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[归纳推理](https://en.wikipedia.org/wiki/Inductive_reasoning)是一种后验知识的推理方法，我们将观察到的规律模式应用于我们未直接观察过的实例；例如，我观察到的所有雨水都是湿的，因此，我未观察过的所有雨水也必须是湿的。'
- en: With induction, we are implicitly assuming the [uniformity of nature](https://en.wikipedia.org/wiki/Uniformitarianism) (sometimes
    called the Uniformity Principle or Resemblance Principle) and that the future
    will always resemble the past.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通过归纳，我们隐含地假设了[自然的统一性](https://en.wikipedia.org/wiki/Uniformitarianism)（有时称为统一性原则或相似性原则），即未来将始终类似于过去。
- en: Science is entirely based on inductive reasoning. Without assumptions like the
    uniformity of nature, no algorithm (including high-level algorithms like the [scientific
    method](https://en.wikipedia.org/wiki/Scientific_method) or [cross-validation
    routines](https://en.wikipedia.org/wiki/Cross-validation_(statistics))) can be *proven* to
    perform better than random guessing.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 科学完全基于归纳推理。没有像自然统一性这样的假设，没有任何算法（包括像[科学方法](https://en.wikipedia.org/wiki/Scientific_method)或[交叉验证程序](https://en.wikipedia.org/wiki/Cross-validation_(statistics))这样的高级算法）可以*证明*比随机猜测表现更好。
- en: Hume argued that inductive reasoning and the belief in causality *cannot* be
    rationally justified. There is no reasoning behind assuming uniformity. According
    to Hume, induction is instinctive for humans, like how dogs seem to instinctively
    chase rabbits and squirrels. Hume doesn’t really suggest a solution or justification
    for induction -- he kind of just shrugs and says, we can’t really help the way
    we are, so let’s not worry about it. (I am imagining the shrugging.)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 休谟认为归纳推理和因果关系的信念*不能*被理性地证明。假设统一性没有理由。根据休谟的观点，归纳对人类来说是本能的，就像狗似乎本能地追逐兔子和松鼠一样。休谟并没有真正提出归纳的解决方案或证明——他只是有些耸肩地说，我们无法改变自己的本性，所以不必担心这个问题。（我在想象那种耸肩的样子。）
- en: 'David Wolpert’s work on the No Free Lunch theorem for supervised machine learning
    is a formalization of Hume. Wolpert even opens his 1996 paper with this quote
    from David Hume:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 大卫·沃尔珀特对有监督机器学习的无免费午餐定理的研究是对休谟的形式化。沃尔珀特甚至在他的1996年论文开头引用了这句来自大卫·休谟的名言：
- en: '*Even after the observation of the frequent conjunction of objects, we have
    no reason to draw any inference concerning any object beyond those of which we
    have had experience.*'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*即使在观察到物体的频繁联结之后，我们也没有理由对我们没有直接观察过的物体做出任何推断。*'
- en: ''
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- David Hume, in A Treatise of Human Nature, Book I, part 3, Section 12.'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 大卫·休谟在《人性论》第一卷第三部分第十二节中。'
- en: Science (and machine learning models) inherently believe that under the same
    conditions everything always happens the same way. There is no reason to assume
    that because a model worked well on one data set, it will work well on other data
    sets. Additionally, science, statistics, and machine learning models cannot give
    guarantees about future experiments based on the results of previous experiments.
    No system can give guarantees about prediction, control, or observation in any
    environment.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 科学（以及机器学习模型）本质上相信在相同条件下，一切总是以相同的方式发生。没有理由假设因为一个模型在一个数据集上表现良好，它就会在其他数据集上表现良好。此外，科学、统计学和机器学习模型不能根据之前实验的结果对未来实验作出保证。没有系统可以在任何环境下对预测、控制或观察做出保证。
- en: '![](../Images/445f511b5797a6bc7b1279c8f02b7d0d.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/445f511b5797a6bc7b1279c8f02b7d0d.png)'
- en: 'If you’ve enjoyed the philosophy detour, and want more information on Hume
    and the problem of induction, there is a great video on Khan Academy called [Hume:
    Skepticism and Induction, Part 2](https://www.khanacademy.org/partner-content/wi-phi/wiphi-history/wiphi-hume/v/humes-skepticism-and-induction-part-2) ([Part
    1](https://www.khanacademy.org/partner-content/wi-phi/wiphi-history/wiphi-hume/v/humes-skepticism-part-1) is
    about a priori vs. a posteriori knowledge).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢哲学的绕道，想要了解更多关于休谟和归纳问题的信息，Khan Academy上有一个很棒的视频叫做[休谟：怀疑主义与归纳，第2部分](https://www.khanacademy.org/partner-content/wi-phi/wiphi-history/wiphi-hume/v/humes-skepticism-and-induction-part-2)（[第1部分](https://www.khanacademy.org/partner-content/wi-phi/wiphi-history/wiphi-hume/v/humes-skepticism-part-1)讨论的是先验知识与后验知识）。
- en: The No Free Lunch Theorem and You
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无免费午餐定理与你
- en: 'If you’ve stuck with me this long, you’re probably looking for an explanation
    of what this all means to you. The two most important things to take away from
    the No Free Lunch theorems are:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一直跟到这里，你可能在寻找这对你来说意味着什么的解释。从“无免费午餐”定理中可以得出的两个最重要的结论是：
- en: '**Always check your assumptions before relying on a model or search algorithm.**'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在依赖模型或搜索算法之前，始终检查你的假设。**'
- en: '**There is no “super algorithm” that will work perfectly for all datasets.**'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**没有“超级算法”可以完美适用于所有数据集。**'
- en: 'The No Free Lunch theorems were not written to tell you what to do in different
    scenarios. The No Free Lunch theorems were specifically written to counter claims
    along the lines of:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: “无免费午餐”定理并不是为了告诉你在不同情境下应该怎么做。这些定理 是 特别写来反驳如下主张的：
- en: My machine learning algorithm/optimization strategy is *the best, always and
    forever*, for all the scenarios.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我的机器学习算法/优化策略是 *最好的，永远都是*，适用于所有情境。
- en: Models are simplifications of a specific component of reality (observed with
    data). To simplify reality, a machine learning algorithm or statistical model
    needs to make assumptions and introduce bias (known specifically as [inductive
    or learning bias](https://en.wikipedia.org/wiki/Inductive_bias)). Bias-free learning
    is futile because a learner that makes no a priori assumptions will have no rational
    basis for creating estimates when provided new, unseen input data. The assumptions
    of an algorithm will work for some data sets but fail for others. This phenomenon
    is important to understand the concepts of [underfitting](https://en.wikipedia.org/wiki/Overfitting#Underfitting) and [the
    bias/variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是对现实特定组件的简化（通过数据观察得出）。为了简化现实，机器学习算法或统计模型需要做出假设并引入偏差（具体称为 [归纳偏差](https://en.wikipedia.org/wiki/Inductive_bias)）。没有偏差的学习是徒劳的，因为一个没有先验假设的学习者在面对新的、未见过的输入数据时，将没有合理的基础来创建估计。一个算法的假设可能对某些数据集有效，但对其他数据集无效。理解这一现象对于理解 [欠拟合](https://en.wikipedia.org/wiki/Overfitting#Underfitting) 和 [偏差/方差权衡](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)的概念非常重要。
- en: The combination of your data and a randomly selected machine learning model
    are not enough to make accurate or meaningful predictions about the future or
    unknown outcomes. You, the human, will need to make assumptions about the nature
    of your data and the world we live in. Playing an active role in making assumptions
    will only strengthen your models and make them more useful, [even if they are
    wrong](https://community.alteryx.com/t5/Data-Science-Blog/All-Models-Are-Wrong/ba-p/348080).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你的数据和随机选择的机器学习模型的组合不足以对未来或未知结果做出准确或有意义的预测。你，作为人类，需要对你的数据和我们生活的世界做出假设。积极参与假设的制定只会增强你的模型，使其更有用， [即使它们是错误的](https://community.alteryx.com/t5/Data-Science-Blog/All-Models-Are-Wrong/ba-p/348080)。
- en: '[Original](https://community.alteryx.com/t5/Data-Science-Blog/There-is-No-Free-Lunch-in-Data-Science/ba-p/347402).
    Reposted with permission.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://community.alteryx.com/t5/Data-Science-Blog/There-is-No-Free-Lunch-in-Data-Science/ba-p/347402)。经许可转载。'
- en: '**Bio: **A geographer by training and a data geek at heart, [Sydney Firmin](https://www.linkedin.com/in/sydney-firmin-4369a65b/)
    strongly believes that data and knowledge are most valuable when they can be clearly
    communicated and understood. In her current role as a Sr. Data Science Content
    Engineer, she gets to spend her days doing what she loves best; transforming technical
    knowledge and research into engaging, creative, and fun content for the Alteryx
    Community.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介：** Sydney Firmin是一名受过地理学培训的数据迷，[Sydney Firmin](https://www.linkedin.com/in/sydney-firmin-4369a65b/)坚信，数据和知识在能够清晰传达和理解时最具价值。在她目前担任高级数据科学内容工程师的角色中，她可以全天从事她最喜欢的工作：将技术知识和研究转化为引人入胜、富有创意和趣味的内容，为Alteryx社区提供服务。'
- en: '**Related:**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[10 Gradient Descent Optimisation Algorithms + Cheat Sheet](https://www.kdnuggets.com/2019/06/gradient-descent-algorithms-cheat-sheet.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10种梯度下降优化算法 + 备忘单](https://www.kdnuggets.com/2019/06/gradient-descent-algorithms-cheat-sheet.html)'
- en: '[How Optimization Works](https://www.kdnuggets.com/2019/04/how-optimization-works.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化如何工作](https://www.kdnuggets.com/2019/04/how-optimization-works.html)'
- en: '[Supervised vs. Unsupervised Learning](https://www.kdnuggets.com/2018/04/supervised-vs-unsupervised-learning.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[监督学习与非监督学习](https://www.kdnuggets.com/2018/04/supervised-vs-unsupervised-learning.html)'
- en: More On This Topic
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，并通过寻找目标来……](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功数据科学家的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每位数据科学家都应该知道的三个R库（即使你使用Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9亿美元AI失败案例分析](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[是什么让Python成为初创公司的理想编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
