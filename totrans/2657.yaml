- en: Implementing the AdaBoost Algorithm From Scratch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/12/implementing-adaboost-algorithm-from-scratch.html](https://www.kdnuggets.com/2020/12/implementing-adaboost-algorithm-from-scratch.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '**By [James Ajeeth J](https://www.linkedin.com/in/jamesajeeth/), Praxis Business
    School**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of this article, you will be able to:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Understand the working and math behind Adaptive Boosting (AdaBoost) Algorithm.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Able to write the AdaBoost python code from scratch.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction to Boosting:**'
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Boosting is an ensemble technique that attempts to create strong classifiers
    from a number of weak classifiers. Unlike many machine learning models which focus
    on high quality prediction done using single model, boosting algorithms seek to
    improve the prediction power by training a sequence of weak models, each compensating
    the weaknesses of its predecessors. Boosting grants power to machine learning
    models to improve their accuracy of prediction.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '**AdaBoost:**'
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AdaBoost, short for Adaptive [Boosting](https://en.wikipedia.org/wiki/Boosting_(meta-algorithm)),
    is a [machine learning](https://en.wikipedia.org/wiki/Machine_learning) [algorithm](https://en.wikipedia.org/wiki/Meta-algorithm) formulated
    by Yoav Freund and [Robert Schapire](https://en.wikipedia.org/wiki/Robert_Schapire).
    AdaBoost technique follows a decision tree model with a depth equal to one. AdaBoost
    is nothing but the forest of stumps rather than trees. AdaBoost works by putting
    more weight on difficult to classify instances and less on those already handled
    well. AdaBoost algorithm is developed to solve both classification and regression
    problem.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Idea behind AdaBoost:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Stumps (one node and two leaves) are not great in making accurate classification
    so it is nothing but a week classifier/ weak learner. Combination of many weak
    classifier makes a strong classifier and this is the principle behind the AdaBoost
    algorithm.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some stumps get more performance or classify better than others.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consecutive stump is made by taking the previous stumps mistakes into account.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AdaBoost Algorithm from scratch:**'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here I have used Iris dataset as an example in building the algorithm from scratch
    and also considered only two classes (Versicolor and Virginica) for better understanding.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/684d4d2f6b75e9cc52851b944bbcc8bb.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: '**Step 1: Assign Equal Weights to all the observations**'
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Initially assign same weights to each record in the dataset.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '**Sample weight = 1/N**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Where N = Number of records
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/523c83e9b569eaab7c22e1ac785e6e5f.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: '**Step 2: Classify random samples using stumps**'
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Draw random samples with replacement from original data with the probabilities
    equal to the sample weights and fit the model. Here **the model (base learners)
    used in AdaBoost is decision tree.** Decision trees are created with one depth
    which has one node and two leaves also referred to as stumps. Fit the model to
    the random samples and predict the classes for the original data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/75827366c2da5626c461e8a3b4899753.png)![Image](../Images/b7dfa69c1c768a0195e9d0676398754a.png)![Image](../Images/909572be292032a4501fb9286cf58f17.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: ‘pred1’ is the newly predicted class.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3: Calculate Total Error**'
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Total error is nothing but the sum of weights of misclassified record.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '**Total Error** = **Weights of misclassified records**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Total error will be always between 0 and 1.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 0 represents perfect stump (correct classification)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 1 represents weak stump (misclassification)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/f9fce9002733b6143b12c503b3b4449d.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: '**Step 4: Calculate Performance of the Stump**'
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using the Total Error, determine the performance of the base learner. The calculated
    performance of stump(α) value is used to update the weights in consecutive iteration
    and also used for final prediction calculation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance of the stump(**α**)** **= ½ ln (1 – Total error/Total error)**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/71e36ca2c4146de740943dd37d0dea5c.png)![Image](../Images/820b41fa912784fd293e372d08a276e0.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: 'Cases:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: If the total error is 0.5, then the performance of the stump will be zero.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the total error is 0 or 1, then the performance will become infinity or -infinity
    respectively.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the total errors are equal to 1 or 0, the above equation will behave in
    a weird manner. So, in practice a small error term is added to prevent this from
    happening.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: When the performance(α) is relatively large, the stump did a good job in classifying
    the records. When the performance(α) is relatively low, the stump did not do a
    good job in classifying the records. **Using the performance parameter(α), we
    can increase the weights of the wrongly classified records and decrease the weights
    of the correctly classified records.**
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5: Update Weights**'
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on the performance of the stump(α) update the weights. We need the next
    stump to correctly classify the misclassified record by increasing the corresponding
    sample weight and decreasing the sample weights of the correctly classified records.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '**New weight = Weight * e^((performance))** **→** **misclassified records**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '**New weight = Weight * e^(-(performance))** **→** **correctly classified records**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/26e66e6fc54b7e933e7bcad7f23eb31e.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: If the ‘Label’ and ‘pred1’ are same (i.e. 1 or -1) then substituting the values
    in the above equation will give the equation corresponding to correctly classified
    record. Similarly, if the values are different then substituting the values in
    the above equation will give the equation corresponding to misclassified record.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ‘Label’ 和 ‘pred1’ 相同（即 1 或 -1），则将这些值代入上述方程会得到正确分类记录对应的方程。类似地，如果这些值不同，则代入上述方程会得到误分类记录对应的方程。
- en: Short note on e^(performance) i.e. for misclassification
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 e^(performance)，即分类错误的情况的简短说明
- en: When the performance is relatively large the last stump did a good job in classifying
    the records now the new sample weight will be much larger than the old one. When
    the performance is relatively low the last stump did not do a good job in classifying
    the records now the new sample weight will only be little larger than the old
    one.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当性能较好时，最后一个基分类器在分类记录方面表现良好，此时新的样本权重会比旧权重大得多。当性能较低时，最后一个基分类器在分类记录方面表现不佳，此时新的样本权重仅比旧权重大一点。
- en: '![Image](../Images/ed4a5934f9d10e91107f157793b6267b.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/ed4a5934f9d10e91107f157793b6267b.png)'
- en: Short note on e^-(performance) i.e. for no misclassification
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 e^-(performance)，即没有分类错误的情况的简短说明
- en: When the performance is relatively large the last stump did a good job in classifying
    the records now the new sample weight will be very small than the old one. When
    the performance is relatively small the last stump did not do a good job in classifying
    the records now the new sample weight will only be little smaller than the old
    one.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当性能较好时，最后一个基分类器在分类记录方面表现良好，此时新的样本权重会比旧权重小得多。当性能较差时，最后一个基分类器在分类记录方面表现不佳，此时新的样本权重仅比旧权重小一点。
- en: '![Image](../Images/531899bac550c7b6aa666076e92b70ee.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/531899bac550c7b6aa666076e92b70ee.png)'
- en: Here the sum of the updated weights is not equal to 1\. whereas in case of initial
    sample weight the sum of total weights is equal to 1\. So, to achieve this we
    will be dividing it by a number which is nothing but the sum of the updated weights
    (normalizing constant).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这里更新权重的总和不等于 1，而在初始样本权重的情况下，总权重的和等于 1。因此，为了实现这一点，我们将其除以一个数字，即更新权重的总和（归一化常数）。
- en: '**Normalizing constant =** ∑ **New weight**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**归一化常数 =** ∑ **新权重**'
- en: '**Normalized weight = New weight / Normalizing constant**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**归一化权重 = 新权重 / 归一化常数**'
- en: Now the sum of normalized weight is equal to 1.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在归一化权重的总和等于 1。
- en: '![Image](../Images/180a15354bbb57dbbe664b95b8e28bae.png)![Image](../Images/33557a613af3aae1bdd1b56638846780.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/180a15354bbb57dbbe664b95b8e28bae.png)![图像](../Images/33557a613af3aae1bdd1b56638846780.png)'
- en: ‘prob2’ is the newly updated weights.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ‘prob2’ 是新更新的权重。
- en: '**Step 6: Update weights in iteration**'
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**步骤 6：迭代中更新权重**'
- en: Use the normalized weight and make the second stump in the forest. Create a
    new dataset of same size of the original dataset with repetition based on the
    newly updated sample weight. So that the misclassified records get higher probability
    of getting selected. Repeat step 2 to 5 again by updating the weights for a particular
    number of iterations.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用归一化权重，创建森林中的第二个基分类器。基于新更新的样本权重，创建一个与原始数据集大小相同的新数据集，其中包含重复的样本。这样，分类错误的记录将有更高的选择概率。重复步骤
    2 到 5，再次更新权重，进行指定次数的迭代。
- en: '![Image](../Images/56cef35eaa73e308fdda007cbf2266a5.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/56cef35eaa73e308fdda007cbf2266a5.png)'
- en: ‘prob4’ is the final weights of each observation.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ‘prob4’ 是每个观察值的最终权重。
- en: '**Step 7: Final Predictions**'
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**步骤 7：最终预测**'
- en: Final prediction is done by obtaining the sign of the weighted sum of final
    predicted value.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最终预测通过获得最终预测值的加权和的符号来完成。
- en: '**Final prediction/sign (weighted sum) = ∑ (α[i]* (predicted value at each
    iteration))**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**最终预测/符号（加权和） = ∑ (α[i]*（每次迭代中的预测值）)**'
- en: 'For example: 5 weak classifiers may predict the values 1.0, 1.0, -1.0, 1.0,
    -1.0\. From a majority vote, it looks like the model will predict a value of 1.0
    or the first class. These same 5 weak classifiers may have the performance (α)
    values as 0.2, 0.5, 0.8, 0.2 and 0.9 respectively. Calculating the weighted sum
    of these predictions results in an output of -0.8, which would be an ensemble
    prediction of -1.0 or the second class.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：5 个弱分类器可能预测的值为 1.0、1.0、-1.0、1.0、-1.0。通过多数投票，模型似乎会预测值为 1.0 或第一类。这 5 个弱分类器可能具有的性能（α）值分别为
    0.2、0.5、0.8、0.2 和 0.9。计算这些预测值的加权和会得到 -0.8，这将是 -1.0 或第二类的集成预测。
- en: 'Calculation:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 计算：
- en: t = 1.0*0.2 + 1.0*0.5 - 1.0*0.8 + 1.0*0.2 - 1.0*0.9 = -0.8
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Taking the sign alone into consideration, the final prediction will be -1.0
    or the second class.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/c77356b05f968155144d230c1b761691.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: '**Advantages of AdaBoost Algorithm:**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: One of the many advantages of the AdaBoost Algorithm is it is fast, simple and
    easy to program.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting has been shown to be robust to overfitting.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has been extended to learning problems beyond binary classification (i.e.)
    it can be used with text or numeric data.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Drawbacks:**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost can be sensitive to noisy data and outliers.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weak classifiers being too weak can lead to low margins and overfitting.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conclusion:**'
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AdaBoost helps in choosing the training set for each new classifier that is
    trained based on the results of the previous classifier. Also, while combining
    the results; it determines how much weight should be given to each classifier’s
    proposed answer. It combines the weak learners to create a strong one to correct
    classification errors which is also the first successful boosting algorithm for
    binary classification problems.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'GitHub Link for the code file and data:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jamesajeeth/Data-Science/tree/master/Adaboost%20from%20scratch](https://github.com/jamesajeeth/Data-Science/tree/master/Adaboost%20from%20scratch)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '**References:**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'Book:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Hastie, Trevor, Tibshirani, Robert, Friedman, Jerome, *The Elements of Statistical
    Learning, Data Mining, Inference, and Prediction, Second Edition.*
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sites:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[Brief about Adaboost Algorithm](https://www.educba.com/adaboost-algorithm/)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Statquest](https://statquest.org/adaboost-clearly-explained/)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Wikipedia](https://en.wikipedia.org/wiki/AdaBoost)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bio:** [**James Ajeeth J**](https://www.linkedin.com/in/jamesajeeth/) is
    a Postgraduate Student at Praxis Business School in Bangalore, India.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[Ensemble Methods for Machine Learning: AdaBoost](/2019/09/ensemble-methods-machine-learning-adaboost.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Exploring The Brute Force K-Nearest Neighbors Algorithm](/2020/10/exploring-brute-force-nearest-neighbors-algorithm.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Most Popular Distance Metrics Used in KNN and When to Use Them](/2020/11/most-popular-distance-metrics-knn.html)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Implementing Adaboost in Scikit-learn](https://www.kdnuggets.com/2022/10/implementing-adaboost-scikitlearn.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning from Scratch: Decision Trees](https://www.kdnuggets.com/2022/11/machine-learning-scratch-decision-trees.html)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear Regression from Scratch with NumPy](https://www.kdnuggets.com/linear-regression-from-scratch-with-numpy)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Build and Train a Transformer Model from Scratch with…](https://www.kdnuggets.com/how-to-build-and-train-a-transformer-model-from-scratch-with-hugging-face-transformers)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding by Implementing: Decision Tree](https://www.kdnuggets.com/2023/02/understanding-implementing-decision-tree.html)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ten Key Lessons of Implementing Recommendation Systems in Business](https://www.kdnuggets.com/2022/07/ten-key-lessons-implementing-recommendation-systems-business.html)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在商业中实施推荐系统的十大关键经验](https://www.kdnuggets.com/2022/07/ten-key-lessons-implementing-recommendation-systems-business.html)'
