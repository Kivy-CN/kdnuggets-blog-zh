- en: A Beginner’s Guide to Data Engineering – Part II
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《数据工程初学者指南》– 第二部分
- en: 原文：[https://www.kdnuggets.com/2018/03/beginners-guide-data-engineering-part-2.html](https://www.kdnuggets.com/2018/03/beginners-guide-data-engineering-part-2.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/03/beginners-guide-data-engineering-part-2.html](https://www.kdnuggets.com/2018/03/beginners-guide-data-engineering-part-2.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](/2018/03/beginners-guide-data-engineering-part-2.html?page=2#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](/2018/03/beginners-guide-data-engineering-part-2.html?page=2#comments)'
- en: '**By [Robert Chang](https://www.linkedin.com/in/robert-chang-877b1720/), Airbnb**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：[Robert Chang](https://www.linkedin.com/in/robert-chang-877b1720/)，Airbnb**。'
- en: '![Header image](../Images/8eaf2321867f07225ee55fa3de019591.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![Header image](../Images/8eaf2321867f07225ee55fa3de019591.png)'
- en: '[Image Credit](https://www.archdaily.com/295502/hangar-16-inaqui-carnicero-architecture/50aa9e31b3fc4b0b54000045-hangar-16-inaqui-carnicero-architecture-image):
    A transformed modern warehouse at Hangar 16, Madrid (Cortesía de Iñaqui Carnicero
    Arquitectura)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[图片来源](https://www.archdaily.com/295502/hangar-16-inaqui-carnicero-architecture/50aa9e31b3fc4b0b54000045-hangar-16-inaqui-carnicero-architecture-image):
    马德里Hangar 16的改造现代仓库（由Iñaqui Carnicero Arquitectura提供）'
- en: '* * *'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业领域。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织在IT方面'
- en: '* * *'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Recapitulation
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: In [**A Beginner’s Guide to Data Engineering** — **Part I**](https://medium.com/@rchang/a-beginners-guide-to-data-engineering-part-i-4227c5c457d7)**, **I
    explained that an organization’s analytics capability is built layers upon layers.
    From collecting raw data and building data warehouses to applying Machine Learning,
    we saw why data engineering plays a critical role in all of these areas.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在[**《数据工程初学者指南》—**](https://medium.com/@rchang/a-beginners-guide-to-data-engineering-part-i-4227c5c457d7)**第一部分**中，我解释了组织的分析能力是层层构建的。从收集原始数据和构建数据仓库，到应用机器学习，我们看到数据工程在这些领域中发挥着关键作用。
- en: One of any data engineer’s most highly sought-after skills is the ability to
    design, build, and maintain data warehouses. I defined what data warehousing is
    and discussed its three common building blocks — **E**xtract, **T**ransform, and **L**oad,
    where the name ETL comes from.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师最受欢迎的技能之一是设计、构建和维护数据仓库的能力。我定义了数据仓库的概念，并讨论了其三个常见构建块——**提取**、**转换**和**加载**，这也是ETL名称的由来。
- en: For those who are new to ETL processes, I introduced a few popular open source
    frameworks built by companies like LinkedIn, Pinterest, Spotify, and highlight
    Airbnb’s own open-sourced tool Airflow. Finally, I argued that data scientist
    can learn data engineering much more effectively with the SQL-based ETL paradigm.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于新手来说，我介绍了一些由LinkedIn、Pinterest、Spotify等公司开发的流行开源框架，并重点介绍了Airbnb自己开源的工具Airflow。最后，我认为数据科学家可以通过基于SQL的ETL范式更有效地学习数据工程。
- en: Part II Overview
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第二部分概览
- en: The discussion in part I was somewhat high level. In Part II (this post), I
    will share more technical details on how to build good data pipelines and highlight
    ETL best practices. Primarily, I will use Python, Airflow, and SQL for our discussion.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分的讨论有些高层次。在第二部分（本篇文章）中，我将分享更多关于如何构建良好的数据管道的技术细节，并重点介绍ETL的最佳实践。主要，我将使用Python、Airflow和SQL进行讨论。
- en: First, I will introduce the concept of **Data Modeling**, a design process where
    one carefully defines table schemas and data relations to capture business metrics
    and dimensions. We will learn **Data Partitioning**, a practice that enables more
    efficient querying and data backfilling. After this section, readers will understand
    the basics of data warehouse and pipeline design.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我将介绍**数据建模**的概念，这是一种设计过程，其中仔细定义表结构和数据关系，以捕捉业务指标和维度。我们将学习**数据分区**，这是一种使查询和数据填充更加高效的实践。在这一部分之后，读者将理解数据仓库和数据管道设计的基础知识。
- en: In later sections, I will dissect the anatomyof an **Airflow job. **Readers
    will learn how to use sensors, operators, and transfers to operationalize the
    concepts of extraction, transformation, and loading. We will highlight **ETL best
    practices**, drawing from real life examples such as Airbnb, Stitch Fix, Zymergen,
    and more.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续部分，我将剖析**Airflow 作业**的结构。读者将学习如何使用传感器、操作符和传输来实现提取、转换和加载的概念。我们将突出**ETL 最佳实践**，借鉴实际案例，例如
    Airbnb、Stitch Fix、Zymergen 等。
- en: By the end of this post, readers will appreciate the versatility of Airflow
    and the concept of [*configuration as code*](https://airflow.apache.org/#principles).
    We will see, in fact, that Airflow has many of these best practices already built
    in.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 到了本文末尾，读者将欣赏到 Airflow 的多功能性以及[*配置即代码*](https://airflow.apache.org/#principles)的概念。我们将实际看到，Airflow
    已经内置了许多这些最佳实践。
- en: Data Modeling
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据建模
- en: '![](../Images/c04d876021524a21b2bb3e3cb0414615.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c04d876021524a21b2bb3e3cb0414615.png)'
- en: '[Image credit](https://digital-photography-school.com/lake-tekapo-stars/):
    Star Schema, when used correctly, can be as beautiful as the actual sky'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[图片来源](https://digital-photography-school.com/lake-tekapo-stars/)：星型模式如果使用得当，可以如实际的天空般美丽'
- en: When a user interacts with a product like Medium, her information, such as her
    avatar, saved posts, and number of views are all captured by the system. In order
    to serve them accurately and on time to users, it is critical to optimize the
    production databases for *online transaction processing* (OLTP for short).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户与像 Medium 这样的产品互动时，她的信息，例如头像、保存的帖子和浏览次数，都会被系统捕捉。为了准确及时地服务用户，优化生产数据库以进行*在线事务处理*（简称
    OLTP）是至关重要的。
- en: When it comes to building an *online analytical processing* system (OLAP for
    short), the objective is rather different. The designer need to focus on insight
    generation, meaning analytical reasoning can be translated into queries easily
    and statistics can be computed efficiently. This analytics-first approach often
    involves a design process called **data modeling.**
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及构建*在线分析处理*系统（简称 OLAP）时，目标则有所不同。设计者需要专注于洞察生成，这意味着分析推理可以轻松转换为查询，统计数据可以高效计算。这种以分析为先的方法通常涉及称为**数据建模**的设计过程。
- en: '**Data Modeling, Normalization, and Star Schema**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据建模、规范化和星型模式**'
- en: To give an example of the design decisions involved, we often need to decide
    the extent to which tables should be [**normalized**](https://en.wikipedia.org/wiki/Database_normalization).
    Generally speaking, normalized tables have simpler schemas, more standardized
    data, and carry less redundancy. However, a proliferation of smaller tables also
    means that tracking data relations requires more diligence, querying patterns
    become more complex (more `JOINs`), and there are more ETL pipelines to maintain.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，我们通常需要决定表的规范化程度[**规范化**](https://en.wikipedia.org/wiki/Database_normalization)。一般而言，规范化表具有更简单的结构、更标准化的数据，并且冗余较少。然而，较多的小表也意味着跟踪数据关系需要更多的细致，查询模式变得更复杂（更多的`JOINs`），以及需要维护更多的
    ETL 管道。
- en: On the other hand, it is often much easier to query from a denormalized table
    (aka a wide table), because all of the metrics and dimensions are already pre-joined.
    Given their larger sizes, however, data processing for wide tables is slower and
    involves more upstream dependencies. This makes maintenance of ETL pipelines more
    difficult because the unit of work is not as modular.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，从去规范化表（即宽表）查询通常要容易得多，因为所有的指标和维度已经预先连接。然而，由于其较大的尺寸，宽表的数据处理较慢，并涉及更多的上游依赖。这使得
    ETL 管道的维护变得更加困难，因为工作单元不够模块化。
- en: Among the many design patterns that try to balance this trade-off, one of the
    most commonly-used patterns, and the one we use at [Airbnb](https://ieondemand.com/presentations/building-airbnb-s-data-culture-insights-from-5-years-of-hypergrowth?_ga=2.230925083.5245429.1516779379-1586560381.1516779379),
    is called [**star schema**](https://en.wikipedia.org/wiki/Star_schema). The name
    arose because tables organized in star schema can be visualized with a star-like
    pattern. This design focuses on building normalized tables, specifically fact
    and dimension tables. When needed, denormalized tables can be built from these
    smaller normalized tables. This design strives for a balance between ETL maintainability
    and ease of analytics.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在众多尝试平衡这种权衡的设计模式中，一种最常用的模式，也是我们在[Airbnb](https://ieondemand.com/presentations/building-airbnb-s-data-culture-insights-from-5-years-of-hypergrowth?_ga=2.230925083.5245429.1516779379-1586560381.1516779379)使用的模式，被称为[**星型模式**](https://en.wikipedia.org/wiki/Star_schema)。这个名字源于表按照星型模式组织的方式，可以用星形模式来可视化。这种设计专注于构建规范化表，特别是事实表和维度表。必要时，可以从这些较小的规范化表中构建非规范化表。这种设计旨在平衡ETL的可维护性和分析的简便性。
- en: '![](../Images/6c703ed92b4505c125c49a049f0965c1.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c703ed92b4505c125c49a049f0965c1.png)'
- en: The star schema organized table in a star-like pattern, with a fact table at
    the center, surrounded by dim tables
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 星型模式组织的表呈星型图案，中心是事实表，周围是维度表
- en: '****Fact & Dimension Tables****'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '****事实表与维度表****'
- en: 'To understand how to build denormalized tables from fact tables and dimension
    tables, we need to discuss their respective roles in more detail:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解如何从事实表和维度表构建非规范化表，我们需要详细讨论它们各自的作用：
- en: '**Fact tables **typically contain point-in-time transactional data. Each row
    in the table can be extremely simple and is often represented as a unit of transaction.
    Because of their simplicity, they are often the source of truth tables from which
    business metrics are derived. For example, at Airbnb, we have various fact tables
    that track transaction-like events such as bookings, reservations, alterations,
    cancellations, and more.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事实表**通常包含特定时间点的事务数据。表中的每一行可以非常简单，通常表示为一个事务单位。由于其简单性，事实表常常是生成业务指标的真实来源。例如，在
    Airbnb，我们有各种事实表，用于跟踪类似于预订、预定、修改、取消等事务事件。'
- en: '**Dimension tables **typically contain slowly changing attributes of specific
    entities, and attributes sometimes can be organized in a hierarchical structure.
    These attributes are often called “dimensions”, and can be joined with the fact
    tables, as long as there is a foreign key available in the fact table. At Airbnb,
    we built various dimension tables such as users, listings, and markets that help
    us to slice and dice our data.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**维度表**通常包含特定实体的缓慢变化的属性，并且这些属性有时可以组织成层次结构。这些属性通常称为“维度”，可以与事实表连接，只要事实表中有外键。在
    Airbnb，我们建立了各种维度表，如用户、房源和市场，这些表帮助我们对数据进行切片和切块。'
- en: Below is a simple example of how fact tables and dimension tables (both are
    normalized tables) can be joined together to answer basic analytics question such
    as how many bookings occurred in the past week in each market. Shrewd users can
    also imagine that if additional metrics `m_a, m_b, m_c` and dimensions `dim_x,
    dim_y, dim_z` are projected in the final `SELECT` clause, a denormalized table
    can be easily built from these normalized tables.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个简单的示例，展示了事实表和维度表（两者都是规范化表）如何结合在一起，以回答基本的分析问题，例如每个市场在过去一周发生了多少预订。聪明的用户还可以想象，如果在最终的`SELECT`子句中投影了额外的度量`m_a,
    m_b, m_c`和维度`dim_x, dim_y, dim_z`，那么可以很容易地从这些规范化表中构建一个非规范化表。
- en: Normalized tables can be used to answer ad-hoc questions or to build denormalized
    tables
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 规范化表可以用来回答临时问题或构建非规范化表
- en: Data Partitioning & Backfilling Historical Data
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据分区与历史数据回填
- en: '![](../Images/d99d4c9905f720e1427ce2fa60f2b199.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d99d4c9905f720e1427ce2fa60f2b199.png)'
- en: In an era where data storage cost is low and computation is cheap, companies
    now can afford to store all of their historical data in their warehouses rather
    than throwing it away. The advantage of such an approach is that companies can
    re-process historical data in response to new changes as they see fit.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据存储成本低且计算便宜的时代，公司现在可以负担得起将所有历史数据存储在数据仓库中，而不是将其丢弃。这种方法的优势在于，公司可以根据需要重新处理历史数据，以应对新变化。
- en: '**Data Partitioning by Datestamp**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据按日期分区**'
- en: With so much data readily available, running queries and performing analytics
    can become inefficient over time. In addition to following SQL best practices
    such as “filter early and often”, “project only the fields that are needed”, one
    of the most effective techniques to improve query performance is to partition
    data.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据的不断增加，运行查询和进行分析可能会随着时间的推移变得低效。除了遵循 SQL 最佳实践，例如“尽早且频繁地过滤”和“仅投影所需的字段”，提高查询性能的一个最有效的技术是对数据进行分区。
- en: The basic idea behind [**data partitioning**](https://en.wikipedia.org/wiki/Partition_%28database%29) is
    rather simple — instead of storing all the data in one chunk, we break it up into
    independent, self-contained chunks. Data from the same chunk will be assigned
    with the same partition key, which means that any subset of the data can be looked
    up extremely quickly. This technique can greatly improve query performance.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[**数据分区**](https://en.wikipedia.org/wiki/Partition_%28database%29)的基本理念非常简单——与其将所有数据存储在一个块中，不如将其拆分为独立的、自包含的块。来自同一块的数据将被分配相同的分区键，这意味着任何数据子集都可以极快地查找。这种技术可以大大提高查询性能。'
- en: In particular, one common partition key to use is **datestamp **(`ds` for short),
    and for good reason. First, in data storage system like [S3](https://aws.amazon.com/s3/),
    raw data is often organized by datestamp and stored in time-labeled directories.
    Furthermore, the unit of work for a batch ETL job is typically one day, which
    means new date partitions are created for each daily run. Finally, many analytical
    questions involve counting events that occurred in a specified time range, so
    querying by datestamp is a very common pattern. It is no wonder that datestamp
    is a popular choice for data partitioning!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，常用的分区键是**日期戳**（`ds` 的缩写），这有很好的理由。首先，在像[S3](https://aws.amazon.com/s3/)这样的数据存储系统中，原始数据通常按日期戳组织并存储在以时间标记的目录中。此外，批量
    ETL 作业的工作单元通常为一天，这意味着每次每日运行都会创建新的日期分区。最后，许多分析问题涉及统计在指定时间范围内发生的事件，因此按日期戳查询是一种非常常见的模式。难怪日期戳是数据分区的热门选择！
- en: A table that is partitioned by ds
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 按 `ds` 分区的表
- en: '**Backfilling Historical Data**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**回填历史数据**'
- en: Another important advantage of using datestamp as the partition key is the ease
    of data backfilling. When a ETL pipeline is built, it computes metrics and dimensions
    forward, not backward. Often, we might desire to revisit the historical trends
    and movements. In such cases, we would need to compute metric and dimensions in
    the past — We called this process **data backfilling**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用日期戳作为分区键的另一个重要优点是数据回填的简便性。当构建 ETL 管道时，它是向前计算指标和维度，而不是向后计算。通常，我们可能希望重新审视历史趋势和变化。在这种情况下，我们需要计算过去的指标和维度——我们称这个过程为**数据回填**。
- en: 'Backfilling is so common that Hive built in the functionality of [**dynamic
    partitions**](https://cwiki.apache.org/confluence/display/Hive/DynamicPartitions)**, **a
    construct that perform the same SQL operations over many partitions and perform
    multiple insertions at once. To illustrate how useful dynamic partitions can be,
    consider a task where we need to backfill the number of bookings in each market
    for a dashboard, starting from `earliest_ds` to `latest_ds` . We might do something
    like this:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 回填（Backfilling）如此常见，以至于 Hive 内置了[**动态分区**](https://cwiki.apache.org/confluence/display/Hive/DynamicPartitions)**功能**，这种构造可以在多个分区上执行相同的
    SQL 操作，并一次性进行多次插入。为了说明动态分区的有用性，考虑一个任务，我们需要回填每个市场的预订数量以供仪表板使用，从`earliest_ds`到`latest_ds`。我们可能会这样做：
- en: 'The operation above is rather tedious, since we are running the same query
    many times but on different partitions. If the time range is large, this work
    can become quickly repetitive. When dynamic partitions are used, however, we can
    greatly simplify this work into just one query:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 上述操作相当繁琐，因为我们在不同的分区上多次运行相同的查询。如果时间范围很大，这项工作可能会变得很重复。然而，当使用动态分区时，我们可以将这项工作大大简化为一个查询：
- en: Notice the extra `ds` in the `SELECT` and `GROUP BY` clause, the expanded range
    in the `WHERE` clause, and how we changed the syntax from `PARTITION (ds= '{{ds}}')` to `PARTITION
    (ds)` . The beauty of dynamic partitions is that we wrap all the same work that
    is needed with a `GROUP BY ds` and insert the results into the relevant ds partitions
    all at once. This query pattern is very powerful and is used by many of Airbnb’s
    data pipelines. In a later section, I will demonstrate how one can write an Airflow
    job that incorporates backfilling logic using [Jinja](http://jinja.pocoo.org/) control
    flow.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`SELECT`和`GROUP BY`子句中的额外`ds`，`WHERE`子句中的扩展范围，以及如何将语法从`PARTITION (ds= '{{ds}}')`更改为`PARTITION
    (ds)`。动态分区的美在于我们将所有相同的工作包装在`GROUP BY ds`中，并将结果一次性插入到相关的ds分区。这种查询模式非常强大，被许多 Airbnb
    的数据管道使用。在后面的部分，我将演示如何编写一个包含回填逻辑的 Airflow 作业，使用[Jinja](http://jinja.pocoo.org/)控制流。
- en: More On This Topic
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[A Beginner’s Guide to Data Engineering](https://www.kdnuggets.com/2023/07/beginner-guide-data-engineering.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[初学者数据工程指南](https://www.kdnuggets.com/2023/07/beginner-guide-data-engineering.html)'
- en: '[A Beginner''s Guide to Anomaly Detection Techniques in Data Science](https://www.kdnuggets.com/2023/05/beginner-guide-anomaly-detection-techniques-data-science.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[初学者异常检测技术指南](https://www.kdnuggets.com/2023/05/beginner-guide-anomaly-detection-techniques-data-science.html)'
- en: '[Introduction to Data Science: A Beginner''s Guide](https://www.kdnuggets.com/2023/07/introduction-data-science-beginner-guide.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学入门：初学者指南](https://www.kdnuggets.com/2023/07/introduction-data-science-beginner-guide.html)'
- en: '[Beginner’s Guide to Data Cleaning with Pyjanitor](https://www.kdnuggets.com/beginners-guide-to-data-cleaning-with-pyjanitor)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[初学者数据清理指南：使用 Pyjanitor](https://www.kdnuggets.com/beginners-guide-to-data-cleaning-with-pyjanitor)'
- en: '[A Beginner''s Guide to End to End Machine Learning](https://www.kdnuggets.com/2021/12/beginner-guide-end-end-machine-learning.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[初学者端到端机器学习指南](https://www.kdnuggets.com/2021/12/beginner-guide-end-end-machine-learning.html)'
- en: '[Essential Machine Learning Algorithms: A Beginner''s Guide](https://www.kdnuggets.com/2021/05/essential-machine-learning-algorithms-beginners.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[必要的机器学习算法：初学者指南](https://www.kdnuggets.com/2021/05/essential-machine-learning-algorithms-beginners.html)'
