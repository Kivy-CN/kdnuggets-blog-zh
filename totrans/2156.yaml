- en: How to Use Hugging Face AutoTrain to Fine-tune LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/how-to-use-hugging-face-autotrain-to-finetune-llms](https://www.kdnuggets.com/how-to-use-hugging-face-autotrain-to-finetune-llms)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![How to Use Hugging Face AutoTrain to Fine-tune LLMs](../Images/9535954ddf526bd0261f1d923485c8ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Editor
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, the Large Language Model (LLM) has changed how people work
    and has been used in many fields, such as education, marketing, research, etc.
    Given the potential, LLM can be enhanced to solve our business problems better.
    This is why we could perform LLM fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: We want to fine-tune our LLM for several reasons, including adopting specific
    domain use cases, improving the accuracy, data privacy and security, controlling
    the model bias, and many others. With all these benefits, it’s essential to learn
    how to fine-tune our LLM to have one in production.
  prefs: []
  type: TYPE_NORMAL
- en: One way to perform LLM fine-tuning automatically is by using [Hugging Face’s
    AutoTrain](https://huggingface.co/docs/autotrain/v0.6.10/index). The HF AutoTrain
    is a no-code platform with Python API  to train state-of-the-art models for various
    tasks such as Computer Vision, Tabular, and NLP tasks. We can use the AutoTrain
    capability even if we don’t understand much about the LLM fine-tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: So, how does it work? Let’s explore further.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started with AutoTrain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even if HF AutoTrain is a no-code solution, we can develop it on top of the
    AutoTrain using Python API. We would explore the code routes as the no-code platform
    isn’t stable for training. However, if you want to use the no-code platform, We
    can create the AutoTrain space using the following [page](https://huggingface.co/new-space?template=autotrain-projects/autotrain-advanced).
    The overall platform will be shown in the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![How to Use Hugging Face AutoTrain to Fine-tune LLMs](../Images/9a19c37473b3ada3812e03be6c43a855.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: To fine-tune the LLM with Python API, we need to install the Python package,
    which you can run using the following code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Also, we would use the Alpaca sample dataset from [HuggingFace](https://huggingface.co/datasets/tatsu-lab/alpaca),
    which required datasets package to acquire.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Then, use the following code to acquire the data we need.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, we would save the data in the CSV format as we would need them
    for our fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: With the environment and the dataset ready, let’s try to use HuggingFace AutoTrain
    to fine-tune our LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning Procedure and Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I would adapt the fine-tuning process from the AutoTrain example, which we can
    find [here](https://colab.research.google.com/github/huggingface/autotrain-advanced/blob/main/colabs/AutoTrain_LLM.ipynb).
    To start the process, we put the data we would use to fine-tune in the folder
    called data.
  prefs: []
  type: TYPE_NORMAL
- en: '![How to Use Hugging Face AutoTrain to Fine-tune LLMs](../Images/d43661e233bf6f5c8ac7bc62b0def2fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: For this tutorial, I try to sample only 100 row data so our training process
    can be much more swifter. After we have our data ready, we could use our Jupyter
    Notebook to fine-tune our model. Make sure the data contain ‘text’ column as the
    AutoTrain would read from that column only.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s run the AutoTrain setup using the following command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Next, we would provide an information required for AutoTrain to run. For the
    following one is the information about the project name and the pre-trained model
    you want. You can only choose the model that was available in the HuggingFace.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Then we would add HF information, if you want push your model to teh repository
    or using a private model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, we would initiate the model parameter information in the variables below.
    You can change them as you like to see if the result is good or not.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: With all the information is ready, we would set up the environment to accept
    all the information we have set up previously.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: To run the AutoTrain in our notebook, we would use the following command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: If you run the AutoTrain successfully, you should find the following folder
    in your directory with all the model and tokenizer producer by AutoTrain.
  prefs: []
  type: TYPE_NORMAL
- en: '![How to Use Hugging Face AutoTrain to Fine-tune LLMs](../Images/48f5dfbec3ac40fe83b88dfa81ee3637.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: To test the model, we would use the HuggingFace transformers package with the
    following code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Then, we can try to evaluate our model based on the training input we have given.
    For example, we use the "Health benefits of regular exercise" as the input.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![How to Use Hugging Face AutoTrain to Fine-tune LLMs](../Images/4ba169bff91d6f375bd16de96a9d151f.png)'
  prefs: []
  type: TYPE_IMG
- en: The result is certainly still could be better, but at least it’s closer to the
    sample data we have provided. We can try to playing around with the pre-trained
    model and the parameter to improve the fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Tips for Successful Fine-tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are few best practices that you might want to know to improve the fine-tuning
    process, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare our dataset with the quality matching the representative task,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Study the pre-trained model that we used,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use an appropriate regularization techniques to avoid overfitting,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Trying out the learning rate from smaller and gradually become bigger,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use fewer epoch as the training as LLM usually learn the new data quite fast,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Don’t ignore the computational cost, as it would become higher with bigger data,
    parameter, and model,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure you follow the ethical consideration regarding the data you use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fine-tuning our Large Language Model is beneficial to our business process,
    especially if there are certain requirements that we required. With the HuggingFace
    AutoTrain, we can boost up our training process and easily using the available
    pre-trained model to fine-tune the model.
  prefs: []
  type: TYPE_NORMAL
- en: '**[](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**[Cornellius Yudha
    Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**** is a data science
    assistant manager and data writer. While working full-time at Allianz Indonesia,
    he loves to share Python and data tips via social media and writing media. Cornellius
    writes on a variety of AI and machine learning topics.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How to Finetune Mistral AI 7B LLM with Hugging Face AutoTrain](https://www.kdnuggets.com/how-to-finetune-mistral-ai-7b-llm-with-hugging-face-autotrain)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Use GPT for Generating Creative Content with Hugging Face…](https://www.kdnuggets.com/how-to-use-gpt-for-generating-creative-content-with-hugging-face-transformers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Use the Hugging Face Tokenizers Library to Preprocess Text Data](https://www.kdnuggets.com/how-to-use-the-hugging-face-tokenizers-library-to-preprocess-text-data)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Use Hugging Face’s Datasets Library for Efficient Data Loading](https://www.kdnuggets.com/how-to-use-hugging-faces-datasets-library-for-efficient-data-loading)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top 10 Machine Learning Demos: Hugging Face Spaces Edition](https://www.kdnuggets.com/2022/05/top-10-machine-learning-demos-hugging-face-spaces-edition.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A community developing a Hugging Face for customer data modeling](https://www.kdnuggets.com/2022/08/objectiv-community-developing-hugging-face-customer-data-modeling.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
