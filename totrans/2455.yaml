- en: What is Adversarial Machine Learning?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是对抗性机器学习？
- en: 原文：[https://www.kdnuggets.com/2022/03/adversarial-machine-learning.html](https://www.kdnuggets.com/2022/03/adversarial-machine-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/03/adversarial-machine-learning.html](https://www.kdnuggets.com/2022/03/adversarial-machine-learning.html)
- en: '![What is Adversarial Machine Learning?](../Images/f538ec918d541053baf0f220da9a42e2.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![什么是对抗性机器学习？](../Images/f538ec918d541053baf0f220da9a42e2.png)'
- en: '[Clint Patterson](https://unsplash.com/@cbpsc1) via Unsplash'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Clint Patterson](https://unsplash.com/@cbpsc1) via Unsplash'
- en: With the continuous rise in Machine Learning (ML), our society becomes heavily
    reliant on its applications in the real world. However the more dependent we become
    on Machine Learning models, the more vulnerabilities on how to defeat these models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习（ML）的不断发展，我们的社会在现实世界中越来越依赖其应用。然而，我们对机器学习模型的依赖程度越高，对抗这些模型的漏洞也就越多。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行 IT 工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'The dictionary definition of an "adversary" is:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: “对手”的字典定义是：
- en: '"one that contends with, opposes, or resists"'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “一个与之争斗、对抗或抵抗的存在”
- en: In the Cybersecurity sector, **adversarial machine learning** attempts to deceive
    and trick models by creating unique deceptive inputs, to confuse the model resulting
    in a malfunction in the model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络安全领域，**对抗性机器学习**试图通过创建独特的欺骗性输入来欺骗和扰乱模型，从而导致模型功能失常。
- en: Adversaries may input data that have an intention to compromise or alter the
    output and exploit its vulnerabilities. We are unable to identify these inputs
    through the human eye, however, it causes the model to fail.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对手可能会输入意图破坏或更改输出的数据，并利用其漏洞。我们无法通过肉眼识别这些输入，但它会导致模型失败。
- en: In Artificial Intelligence systems, there are different forms of vulnerabilities
    such as text, audio files, images. It is much easier to perform digital attacks,
    such as manipulating only one pixel in input data which can lead to misclassification.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能系统中，存在不同形式的漏洞，如文本、音频文件和图像。进行数字攻击要容易得多，例如仅通过操控输入数据中的一个像素，这可能会导致错误分类。
- en: To train machine learning models efficiently and produce accurate outputs, you
    will need large sets of labeled data. If you are not collecting the data from
    a reliable source, some developers use datasets published from Kaggle or GitHub,
    which come with potential vulnerabilities that can lead to data poisoning attacks.
    For example, somebody may have tampered with the training data, affecting the
    model’s ability to produce correct and accurate outputs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要高效地训练机器学习模型并生成准确的输出，你需要大量标记的数据。如果你没有从可靠来源收集数据，一些开发者会使用来自 Kaggle 或 GitHub 的数据集，这些数据集可能存在潜在的漏洞，从而导致数据中毒攻击。例如，有人可能篡改了训练数据，影响了模型生成正确和准确输出的能力。
- en: 'There are two different types of adversary attacks: whitebox and blackbox.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性攻击有两种不同的类型：白盒攻击和黑盒攻击。
- en: Whitebox vs Blackbox Attacks
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 白盒攻击与黑盒攻击
- en: A whitebox attack refers to when an attacker has full access to the target model.
    This includes the architecture and parameters, which allows them to craft adversarial
    samples on the target model. White box attackers will only have this access if
    they are testing the model, as a developer. The developers have detailed knowledge
    of the network architecture. They know the ins and outs of the model and create
    an attack strategy based on loss function.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 白盒攻击是指攻击者对目标模型拥有完全访问权限，包括模型的架构和参数，这使他们能够在目标模型上创建对抗样本。白盒攻击者只有在测试模型时才会拥有这种访问权限，作为开发者，他们对网络架构有详细了解。他们了解模型的内外，并基于损失函数制定攻击策略。
- en: A blackbox attack refers to when an attacker has no access to the target model
    and can only examine the model’s outputs. They do this by using the query access
    to generate adversarial samples.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 黑盒攻击指的是当攻击者无法访问目标模型，只能检查模型的输出。他们通过查询访问生成对抗样本。
- en: Adversarial Attacks on AI/ML
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对抗性攻击在人工智能/机器学习中的应用
- en: There are different types of adversarial attacks that can occur.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 存在不同类型的对抗性攻击。
- en: Poisoning
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 投毒
- en: Attacks on machine learning models during the training phase are referred to
    as ‘poisoning or ‘contaminating’. This requires the adversary to have access or
    control over the training data, from what we understand is known as a white-box
    attacker.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段对机器学习模型的攻击被称为“投毒”或“污染”。这需要攻击者能够访问或控制训练数据，根据我们的理解，这被称为白盒攻击者。
- en: An adversary inputs incorrectly labeled data to a classifier which they class
    as harmless, however, it has a negative impact. This will cause misclassification,
    producing incorrect outputs and decisions in the future.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者向分类器输入错误标记的数据，这些数据被他们视为无害，但却会产生负面影响。这将导致错误分类，未来产生不正确的输出和决策。
- en: A way in which an adversary can manipulate this is using their understanding
    of the model's outputs, allowing them to attempt to slowly introduce data that
    decrease the accuracy of the model, which is known as model skewing.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者可以利用他们对模型输出的理解来操控模型，尝试逐步引入减少模型准确性的数据显示，这被称为模型偏斜。
- en: For example, search engine platforms and social media platforms have a recommendation
    system built-in using machine learning models. Attackers manipulate the recommendation
    systems by using fake accounts to share or prompt certain products or content,
    altering the recommendation system.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，搜索引擎平台和社交媒体平台内置了使用机器学习模型的推荐系统。攻击者通过使用虚假账户分享或推动某些产品或内容来操控推荐系统，从而改变推荐系统的效果。
- en: Evasion attacks
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规避攻击
- en: Evasion attacks typically occur once a machine learning model has already been
    trained and new data has been inputted. These are also known as a white-box attack,
    as the adversary has access to the model and uses a trial and error process to
    understand and manipulate the model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 规避攻击通常发生在机器学习模型已经被训练完毕且新数据已被输入的情况下。这也被称为白盒攻击，因为攻击者可以访问模型并使用试错过程来理解和操控模型。
- en: The adversary has a lack of knowledge of the model and what will cause it to
    break, therefore the trial and error process is used.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者对模型的知识有限，不知道哪些因素会导致模型失效，因此使用试错过程。
- en: For example, an adversary may tune the boundaries of a machine learning model
    that filters out spam emails. Their approach may be to experiment with emails
    that the model has already trained to screen and recognise as spam.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，攻击者可能会调整一个过滤垃圾邮件的机器学习模型的边界。他们的方法可能是实验那些模型已经被训练用来筛选并识别为垃圾邮件的邮件。
- en: If the model has been trained to filter out emails containing words such as
    ‘make fast money’; an adversary may create new emails that include words linked
    to this or are very similar, which will pass through the algorithm. This causes
    an email that would normally be classified as spam to not spam, diminishing the
    model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型已经被训练以过滤包含“快速赚钱”等词语的邮件；攻击者可能会创建包含这些词语相关或非常相似的新邮件，从而绕过算法。这会导致本应被分类为垃圾邮件的邮件不再被识别为垃圾邮件，从而削弱了模型的效果。
- en: However, there are more malicious causes such as adversaries using models such
    as Natural Language Processing (NLP) to obtain and extract personal information
    such as identification numbers, leading to more personal attacks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有更恶意的原因，例如攻击者使用自然语言处理（NLP）模型获取和提取个人信息，如身份证号码，导致更多的个人攻击。
- en: Model extraction
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型提取
- en: A form of blackbox attack is Model extraction. The adversary does not have access
    to the model, therefore their process is to try and rebuild the model or extract
    the output data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 黑盒攻击的一种形式是模型提取。攻击者无法访问模型，因此他们的过程是尝试重建模型或提取输出数据。
- en: This type of attack is prominent in models that are very confidential and can
    be monetised, such as extracting a stock market prediction model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的攻击在那些高度保密且可以变现的模型中尤为突出，例如提取股票市场预测模型。
- en: An example of a blackbox attack is the use of Graph neural networks (GNN) which
    are widely used to analyse graph-structured data in application domains such as
    social networks and anomaly detection. GNN models are valuable property, becoming
    attractive targets to adversaries.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一个黑箱攻击的例子是使用图神经网络（GNN），它们被广泛用于分析图结构数据，例如社交网络和异常检测。GNN模型是有价值的资产，成为对手的吸引目标。
- en: The owner of the data trains an original model, where the adversary receives
    predictions of another model which imitates the original model. The adversary
    may charge others for access on a pay-per-query basis for these outputs to duplicate
    the functionality of the model. This essentially allows the adversary to recreate
    the model by using the process of continuous tuning to duplicate the model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的所有者训练了一个原始模型，对手则接收一个模仿原始模型的另一个模型的预测。对手可能会对这些输出按查询付费的方式向他人收费，从而复制模型的功能。这本质上允许对手通过使用持续调优的过程来重建模型。
- en: How to Avoid Adversarial Attacks
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何避免对抗攻击
- en: Below are two simple ways in which companies should implement to avoid adversarial
    attacks.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是公司应实施的两种简单方法，以避免对抗攻击。
- en: Attack and learn before getting attacked
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在被攻击之前进行攻击和学习
- en: Adversarial training is one approach to improve the efficiency and defense of
    machine learning and that is to generate attacks on it. We simply generate a lot
    of adversarial examples and allow the system to learn what potential adversarial
    attacks may look like, helping it to build its own immune system to adversarial
    attacks. This way the model can either notify or not be fooled by each of them.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗训练是一种提高机器学习效率和防御的方法，即对其生成攻击。我们简单地生成大量对抗样本，并允许系统学习潜在的对抗攻击是什么样的，帮助它建立自己的对抗攻击免疫系统。通过这种方式，模型可以通知或不被每一个对抗样本欺骗。
- en: Changing your model frequently
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 频繁更改你的模型
- en: Continuously altering the algorithms used in the machine learning model will
    create regular blockages for the adversary, making it more difficult for them
    to hack and learn the model. A way to do this is by breaking your own model, through
    trial and error to distinguish its weaknesses and understand the changes that
    are required to improve it and reduce adversarial attacks.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 持续更改机器学习模型中使用的算法将为对手创造定期的阻碍，使他们更难破解和学习模型。做到这一点的一种方法是通过试错法破坏自己的模型，以区分其弱点并了解所需的改进，以减少对抗攻击。
- en: Conclusion
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: A lot of companies are pursuing the investment of AI-enabled technology to modernize
    their ability to solve problems and make decisions. The Department of Defense
    (DoD) particularly benefits AI, where data-driven situations can increase awareness
    and speed up decision-making. DoD has committed to increasing the use of AI therefore
    it must test the capabilities and limitations to ensure that the AI-enabled technology
    adheres to the correct performance and safety standards. However, this is a big
    challenge as AI systems can be unpredictable and adapt to behavior.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 很多公司正在投资人工智能技术，以现代化其解决问题和决策的能力。国防部（DoD）尤其受益于人工智能，在数据驱动的情况下可以提高意识并加快决策速度。DoD承诺增加对人工智能的使用，因此必须测试其能力和局限性，以确保人工智能技术符合正确的性能和安全标准。然而，这是一项重大挑战，因为人工智能系统可能不可预测并适应行为。
- en: We have to be more vigilant and understanding of the risks associated with machine
    learning and the high possibilities of data exploitations. Organizations that
    are investing and adopting machine learning models and artificial intelligence
    must incorporate the correct protocols to reduce the risk of data corruption,
    theft, and adversarial samples.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须更加警惕和理解与机器学习相关的风险以及数据被利用的高概率。投资和采用机器学习模型及人工智能的组织必须纳入正确的协议，以减少数据损坏、盗窃和对抗样本的风险。
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and freelance Technical writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**[尼莎·阿娅](https://www.linkedin.com/in/nisha-arya-ahmed/)**是一名数据科学家和自由技术作家。她特别关注提供数据科学职业建议或教程以及围绕数据科学的理论知识。她还希望探索人工智能如何/可以有利于人类寿命的不同方式。作为一个热衷的学习者，她寻求拓宽自己的技术知识和写作技能，同时帮助指导他人。'
- en: More On This Topic
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[KDnuggets News, December 14: 3 Free Machine Learning Courses for…](https://www.kdnuggets.com/2022/n48.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，12月14日：3个免费的机器学习课程…](https://www.kdnuggets.com/2022/n48.html)'
- en: '[5 Machine Learning Skills Every Machine Learning Engineer Should…](https://www.kdnuggets.com/2023/03/5-machine-learning-skills-every-machine-learning-engineer-know-2023.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个机器学习工程师应掌握的5项机器学习技能…](https://www.kdnuggets.com/2023/03/5-machine-learning-skills-every-machine-learning-engineer-know-2023.html)'
- en: '[A Solid Plan for Learning Data Science, Machine Learning, and Deep Learning](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学、机器学习和深度学习的全面计划](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
- en: '[AI, Analytics, Machine Learning, Data Science, Deep Learning…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能、分析、机器学习、数据科学、深度学习…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
- en: '[Breaking the Data Barrier: How Zero-Shot, One-Shot, and Few-Shot…](https://www.kdnuggets.com/2023/08/breaking-data-barrier-zeroshot-oneshot-fewshot-learning-transforming-machine-learning.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[突破数据障碍：零样本、单样本和少样本学习如何…](https://www.kdnuggets.com/2023/08/breaking-data-barrier-zeroshot-oneshot-fewshot-learning-transforming-machine-learning.html)'
- en: '[Federated Learning: Collaborative Machine Learning with a Tutorial…](https://www.kdnuggets.com/2021/12/federated-learning-collaborative-machine-learning-tutorial-get-started.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[联邦学习：协作机器学习教程…](https://www.kdnuggets.com/2021/12/federated-learning-collaborative-machine-learning-tutorial-get-started.html)'
