["```py\nimport pandas as pd\n\ndf = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf.head() \n```", "```py\n# Get the basic information about the dataset\ndf.info() \n```", "```py\nOutput>>\n\nRangeIndex: 7043 entries, 0 to 7042\nData columns (total 21 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   customerID        7043 non-null   object \n 1   gender            7043 non-null   object \n 2   SeniorCitizen     7043 non-null   int64  \n 3   Partner           7043 non-null   object \n 4   Dependents        7043 non-null   object \n 5   tenure            7043 non-null   int64  \n 6   PhoneService      7043 non-null   object \n 7   MultipleLines     7043 non-null   object \n 8   InternetService   7043 non-null   object \n 9   OnlineSecurity    7043 non-null   object \n 10  OnlineBackup      7043 non-null   object \n 11  DeviceProtection  7043 non-null   object \n 12  TechSupport       7043 non-null   object \n 13  StreamingTV       7043 non-null   object \n 14  StreamingMovies   7043 non-null   object \n 15  Contract          7043 non-null   object \n 16  PaperlessBilling  7043 non-null   object \n 17  PaymentMethod     7043 non-null   object \n 18  MonthlyCharges    7043 non-null   float64\n 19  TotalCharges      7043 non-null   object \n 20  Churn             7043 non-null   object \ndtypes: float64(1), int64(2), object(18)\nmemory usage: 1.1+ MB \n```", "```py\n# Get the numerical summary statistics of the dataset\ndf.describe()\n\n# Get the categorical summary statistics of the dataset\ndf.describe(exclude = 'number') \n```", "```py\n# Check for missing values\nprint(df.isnull().sum()) \n```", "```py\nOutput>>\nMissing Values:\ncustomerID          0\ngender              0\nSeniorCitizen       0\nPartner             0\nDependents          0\ntenure              0\nPhoneService        0\nMultipleLines       0\nInternetService     0\nOnlineSecurity      0\nOnlineBackup        0\nDeviceProtection    0\nTechSupport         0\nStreamingTV         0\nStreamingMovies     0\nContract            0\nPaperlessBilling    0\nPaymentMethod       0\nMonthlyCharges      0\nTotalCharges        0\nChurn               0 \n```", "```py\nprint(df['Churn'].value_counts()) \n```", "```py\nOutput>>\nDistribution of Target Variable:\nNo     5174\nYes    1869 \n```", "```py\nimport numpy as np\ndf['TotalCharges'] = df['TotalCharges'].replace('', np.nan)\ndf['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce').fillna(0)\n\ndf['SeniorCitizen'] = df['SeniorCitizen'].astype('str')\n\ndf['ChurnTarget'] = df['Churn'].apply(lambda x: 1 if x=='Yes' else 0)\n\ndf['ChurnTarget'] = df['Churn'].apply(lambda x: 1 if x=='Yes' else 0)\n\nnum_features = df.select_dtypes('number').columns\ndf[num_features].hist(bins=15, figsize=(15, 6), layout=(2, 5)) \n```", "```py\nimport matplotlib.pyplot as plt\n# Plot distribution of categorical features\ncat_features = df.drop('customerID', axis =1).select_dtypes(include='object').columns\n\nplt.figure(figsize=(20, 20))\nfor i, col in enumerate(cat_features, 1):\n    plt.subplot(5, 4, i)\n    df[col].value_counts().plot(kind='bar')\n    plt.title(col) \n```", "```py\nimport seaborn as sns\n\n# Plot correlations between numerical features\nplt.figure(figsize=(10, 8))\nsns.heatmap(df[num_features].corr())\nplt.title('Correlation Heatmap') \n```", "```py\npip install dython \n```", "```py\nfrom dython.nominal import associations\n\n# Calculate the Cramerâ€™s V and correlation matrix\nassoc = associations(df[cat_features], nominal_columns='all', plot=False)\ncorr_matrix = assoc['corr']\n\n# Plot the heatmap\nplt.figure(figsize=(14, 12))\nsns.heatmap(corr_matrix) \n```", "```py\n# Plot box plots to identify outliers\nplt.figure(figsize=(20, 15))\nfor i, col in enumerate(num_features, 1):\n    plt.subplot(4, 4, i)\n    sns.boxplot(y=df[col])\n    plt.title(col) \n```", "```py\ntarget = 'ChurnTarget'\nnum_features = df.select_dtypes(include=[np.number]).columns.drop(target)\n\n# Calculate correlations\ncorrelations = df[num_features].corrwith(df[target])\n\n# Set a threshold for feature selection\nthreshold = 0.3\nselected_num_features = correlations[abs(correlations) > threshold].index.tolist() \n```", "```py\ncategorical_target = 'Churn'\n\nassoc = associations(df[cat_features], nominal_columns='all', plot=False)\ncorr_matrix = assoc['corr']\n\nthreshold = 0.3\nselected_cat_features = corr_matrix[corr_matrix.loc[categorical_target] > threshold ].index.tolist()\n\ndel selected_cat_features[-1] \n```", "```py\nselected_features = []\nselected_features.extend(selected_num_features)\nselected_features.extend(selected_cat_features)\n\nprint(selected_features) \n```", "```py\nOutput>>\n['tenure',\n 'InternetService',\n 'OnlineSecurity',\n 'TechSupport',\n 'Contract',\n 'PaymentMethod'] \n```", "```py\nfrom sklearn.model_selection import train_test_split\n\ntarget = 'ChurnTarget' \n\nX = df[selected_features]\ny = df[target]\n\ncat_features = X.select_dtypes(include=['object']).columns.tolist()\nnum_features = X.select_dtypes(include=['number']).columns.tolist()\n\n#Splitting data into Train, Validation, and Test Set\nX_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val) \n```", "```py\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\n# Prepare the preprocessing step\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', 'passthrough', num_features),\n        ('cat', OneHotEncoder(), cat_features)\n    ])\n\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression(max_iter=1000))\n])\n\n# Train the logistic regression model\npipeline.fit(X_train, y_train) \n```", "```py\nfrom sklearn.metrics import classification_report\n\n# Evaluate on the validation set\ny_val_pred = pipeline.predict(X_val)\nprint(\"Validation Classification Report:\\n\", classification_report(y_val, y_val_pred))\n\n# Evaluate on the test set\ny_test_pred = pipeline.predict(X_test)\nprint(\"Test Classification Report:\\n\", classification_report(y_test, y_test_pred)) \n```", "```py\nfrom sklearn.model_selection import GridSearchCV\n# Define the logistic regression model within a pipeline\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression(max_iter=1000))\n])\n\n# Define the hyperparameters for GridSearchCV\nparam_grid = {\n    'classifier__C': [0.1, 1, 10, 100],\n    'classifier__solver': ['lbfgs', 'liblinear']\n}\n\n# Perform Grid Search with cross-validation\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='recall')\ngrid_search.fit(X_train, y_train)\n\n# Best hyperparameters\nprint(\"Best Hyperparameters:\", grid_search.best_params_)\n\n# Evaluate on the validation set\ny_val_pred = grid_search.predict(X_val)\nprint(\"Validation Classification Report:\\n\", classification_report(y_val, y_val_pred))\n\n# Evaluate on the test set\ny_test_pred = grid_search.predict(X_test)\nprint(\"Test Classification Report:\\n\", classification_report(y_test, y_test_pred)) \n```", "```py\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import recall_score\n\n# Define the models and their parameter grids\nmodels = {\n    'Logistic Regression': {\n        'model': LogisticRegression(max_iter=1000),\n        'params': {\n            'classifier__C': [0.1, 1, 10, 100],\n            'classifier__solver': ['lbfgs', 'liblinear']\n        }\n    },\n    'Decision Tree': {\n        'model': DecisionTreeClassifier(),\n        'params': {\n            'classifier__max_depth': [None, 10, 20, 30],\n            'classifier__min_samples_split': [2, 10, 20]\n        }\n    },\n    'Random Forest': {\n        'model': RandomForestClassifier(),\n        'params': {\n            'classifier__n_estimators': [100, 200],\n            'classifier__max_depth': [None, 10, 20]\n        }\n    },\n    'SVM': {\n        'model': SVC(),\n        'params': {\n            'classifier__C': [0.1, 1, 10, 100],\n            'classifier__kernel': ['linear', 'rbf']\n        }\n    },\n    'Gradient Boosting': {\n        'model': GradientBoostingClassifier(),\n        'params': {\n            'classifier__n_estimators': [100, 200],\n            'classifier__learning_rate': [0.01, 0.1, 0.2]\n        }\n    },\n    'XGBoost': {\n        'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n        'params': {\n            'classifier__n_estimators': [100, 200],\n            'classifier__learning_rate': [0.01, 0.1, 0.2],\n            'classifier__max_depth': [3, 6, 9]\n        }\n    },\n    'LightGBM': {\n        'model': LGBMClassifier(),\n        'params': {\n            'classifier__n_estimators': [100, 200],\n            'classifier__learning_rate': [0.01, 0.1, 0.2],\n            'classifier__num_leaves': [31, 50, 100]\n        }\n    }\n}\n\nresults = []\n\n# Train and evaluate each model\nfor model_name, model_info in models.items():\n    pipeline = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('classifier', model_info['model'])\n    ])\n\n    grid_search = GridSearchCV(pipeline, model_info['params'], cv=5, scoring='recall')\n    grid_search.fit(X_train, y_train)\n\n    # Best model from Grid Search\n    best_model = grid_search.best_estimator_\n\n    # Evaluate on the validation set\n    y_val_pred = best_model.predict(X_val)\n    val_recall = recall_score(y_val, y_val_pred, pos_label=1)\n\n    # Evaluate on the test set\n    y_test_pred = best_model.predict(X_test)\n    test_recall = recall_score(y_test, y_test_pred, pos_label=1)\n\n    # Save results\n    results.append({\n        'model': model_name,\n        'best_params': grid_search.best_params_,\n        'val_recall': val_recall,\n        'test_recall': test_recall,\n        'classification_report_val': classification_report(y_val, y_val_pred),\n        'classification_report_test': classification_report(y_test, y_test_pred)\n    })\n\n# Plot the test recall scores\nplt.figure(figsize=(10, 6))\nmodel_names = [result['model'] for result in results]\ntest_recalls = [result['test_recall'] for result in results]\nplt.barh(model_names, test_recalls, color='skyblue')\nplt.xlabel('Test Recall')\nplt.title('Comparison of Test Recall for Different Models')\nplt.show() \n```", "```py\nimport joblib\n\nbest_params = {'classifier__C': 1, 'classifier__solver': 'lbfgs'}\nlogreg_model = LogisticRegression(C=best_params['classifier__C'], solver=best_params['classifier__solver'], max_iter=1000)\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', 'passthrough', num_features),\n        ('cat', OneHotEncoder(), cat_features)\n\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', logreg_model)\n])\n\npipeline.fit(X_train, y_train)\n\n# Save the model\njoblib.dump(pipeline, 'logreg_model.joblib') \n```", "```py\npip install fastapi uvicorn\n```", "```py\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nimport joblib\nimport numpy as np\n\n# Load the logistic regression model pipeline\nmodel = joblib.load('logreg_model.joblib')\n\n# Define the input data for model\nclass CustomerData(BaseModel):\n    tenure: int\n    InternetService: str\n    OnlineSecurity: str\n    TechSupport: str\n    Contract: str\n    PaymentMethod: str\n\n# Create FastAPI app\napp = FastAPI()\n\n# Define prediction endpoint\n@app.post(\"/predict\")\ndef predict(data: CustomerData):\n    # Convert input data to a dictionary and then to a DataFrame\n    input_data = {\n        'tenure': [data.tenure],\n        'InternetService': [data.InternetService],\n        'OnlineSecurity': [data.OnlineSecurity],\n        'TechSupport': [data.TechSupport],\n        'Contract': [data.Contract],\n        'PaymentMethod': [data.PaymentMethod]\n    }\n\n    import pandas as pd\n    input_df = pd.DataFrame(input_data)\n\n    # Make a prediction\n    prediction = model.predict(input_df)\n\n    # Return the prediction\n    return {\"prediction\": int(prediction[0])}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000) \n```", "```py\nuvicorn app:app --reload \n```", "```py\ncurl -X POST \"http://127.0.0.1:8000/predict\" -H \"Content-Type: application/json\" -d \"{\\\"tenure\\\": 72, \\\"InternetService\\\": \\\"Fiber optic\\\", \\\"OnlineSecurity\\\": \\\"Yes\\\", \\\"TechSupport\\\": \\\"Yes\\\", \\\"Contract\\\": \\\"Two year\\\", \\\"PaymentMethod\\\": \\\"Credit card (automatic)\\\"}\" \n```", "```py\nOutput>>\n{\"prediction\":0}\n```"]