- en: 'Introduction to Streaming-LLM: LLMs for Infinite-Length Inputs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Streaming-LLM：无限长度输入的LLMs
- en: 原文：[https://www.kdnuggets.com/introduction-to-streaming-llm-llms-for-infinite-length-inputs](https://www.kdnuggets.com/introduction-to-streaming-llm-llms-for-infinite-length-inputs)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/introduction-to-streaming-llm-llms-for-infinite-length-inputs](https://www.kdnuggets.com/introduction-to-streaming-llm-llms-for-infinite-length-inputs)
- en: The large Language Model (LLM) has changed the way people work. With a model
    such as the GPT family that is used widely, everyone has gotten used to these
    models. Leveraging the LLM power, we can quickly get our questions answered, debugging
    code, and others. This makes the model useful in many applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）改变了人们的工作方式。像GPT家族这样广泛使用的模型，每个人都习惯于这些模型。利用LLM的力量，我们可以快速解答问题，调试代码等。这使得模型在许多应用中非常有用。
- en: One of the LLM challenges is that the model is unsuitable for streaming applications
    because of the model's inability to handle long-conversation chat exceeding the
    predefined training sequence length. Additionally, there is a problem with the
    higher memory consumption.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的挑战之一是模型不适用于流应用，因为其无法处理超过预定义训练序列长度的长对话。此外，内存消耗也是一个问题。
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 走上网络安全职业快车道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的IT组织'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: That is why these problems above spawn research to solve them. What is this
    research? Let’s get into it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，以上问题促使研究来解决它们。这项研究是什么？让我们深入了解。
- en: StreamingLLM
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: StreamingLLM
- en: StreamingLLM is a framework established by [Xiao *et al*. (2023)](https://arxiv.org/pdf/2309.17453.pdf)
    research to tackle the streaming application issues. The existing methods are
    challenged because the attention window constrains the LLMs during pre-training.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: StreamingLLM 是由 [Xiao 等人 (2023)](https://arxiv.org/pdf/2309.17453.pdf) 研究创建的框架，用于解决流应用问题。现有方法在预训练期间由于关注窗口的限制而受到挑战。
- en: The attention [window](https://paperswithcode.com/method/sliding-window-attention)
    technique might be efficient but suffers when handling texts longer than its cache
    size. That’s why the researcher tried to use the Key and Value states of several
    initial tokens (attention sink) with the recent tokens. The comparison of StreamingLLM
    and the other techniques can be seen in the image below.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 注意窗口技术可能是有效的，但在处理超过其缓存大小的文本时存在问题。这就是为什么研究人员尝试使用几个初始标记的键和值状态（关注沉降）与最近标记。StreamingLLM
    与其他技术的比较可以在下图中看到。
- en: '![Introduction to Streaming-LLM: LLMs for Infinite-Length Inputs](../Images/81538ada1aa8d1aa3f4a6c4af0152c6b.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![介绍 Streaming-LLM：无限长度输入的LLMs](../Images/81538ada1aa8d1aa3f4a6c4af0152c6b.png)'
- en: StreamingLLM vs Existing Method ([Xiao *et al*. (2023)](https://arxiv.org/pdf/2309.17453.pdf))
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: StreamingLLM vs 现有方法（[Xiao 等人 (2023)](https://arxiv.org/pdf/2309.17453.pdf))
- en: We can see how StreamingLLM tackles the challenge using the attention sink method.
    This attention sink (initial tokens) is used for stable attention computation
    and combines it with recent tokens for efficiency and maintains stable performance
    on longer texts.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 StreamingLLM 如何使用关注沉降方法应对挑战。这种关注沉降（初始标记）用于稳定的关注计算，并将其与最近的标记结合起来，以提高效率，并在较长文本上保持稳定性能。
- en: Additionally, the existing methods suffer from memory optimization. However,
    LLM  avoids these issues by maintaining a fixed-size window on the Key and Value
    states of the most recent tokens. The author also mentions the benefit of StreamingLLM
    as the sliding window recomputation baseline by up to 22.2× speedup.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，现有方法存在内存优化问题。然而，LLM 通过在最近标记的键和值状态上保持固定大小窗口来避免这些问题。作者还提到了 StreamingLLM 作为滑动窗口重新计算基准，速度提升高达
    22.2×。
- en: Performance-wise, StreamingLLM provides excellent accuracy compared to the existing
    method, as seen in the table below.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从性能方面来看，StreamingLLM 提供了优于现有方法的准确度，如下表所示。
- en: '![Introduction to Streaming-LLM: LLMs for Infinite-Length Inputs](../Images/ce77ac4fa07427e249d2be0758ba66d5.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![引入 Streaming-LLM：适用于无限长度输入的 LLM](../Images/ce77ac4fa07427e249d2be0758ba66d5.png)'
- en: StreamingLLM accuracy ([Xiao *et al*. (2023)](https://arxiv.org/pdf/2309.17453.pdf))
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: StreamingLLM 准确度 ([Xiao *et al*. (2023)](https://arxiv.org/pdf/2309.17453.pdf))
- en: The table above shows that StreamingLLM accuracy can outperform the other methods
    in the benchmark datasets. That’s why StreamingLLM could have potential for many
    streaming applications.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 上表显示，StreamingLLM 的准确度可以超越基准数据集中的其他方法。这就是为什么 StreamingLLM 可能在许多流媒体应用中具有潜力。
- en: To try out the StreamingLLM, you could visit their [GitHub page](https://github.com/mit-han-lab/streaming-llm).
    Clone the repository on your intended directory and use the following code in
    your CLI to set the environment.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要尝试 StreamingLLM，你可以访问他们的 [GitHub 页面](https://github.com/mit-han-lab/streaming-llm)。将代码库克隆到你想要的目录中，然后在
    CLI 中使用以下代码来设置环境。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then, you can use the following code to run the Llama chatbot with LLMstreaming.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以使用以下代码运行带有 LLMstreaming 的 Llama 聊天机器人。
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The overall sample comparison with StreamingLLM can be shown in the image below.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 总体的样本比较可以在下面的图片中看到。
- en: '![Introduction to Streaming-LLM: LLMs for Infinite-Length Inputs](../Images/76e7b64ada489af40bed5d29e4af83dc.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![引入 Streaming-LLM：适用于无限长度输入的 LLM](../Images/76e7b64ada489af40bed5d29e4af83dc.png)'
- en: StreamingLLM showed outstanding performance in more extended conversations ([Streaming-llm](https://github.com/mit-han-lab/streaming-llm))
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: StreamingLLM 在更长对话中表现出色 ([Streaming-llm](https://github.com/mit-han-lab/streaming-llm))
- en: That’s all for the introduction of StreamingLLM. Overall, I believe StreamingLLM
    can have a place in streaming applications and help change how the application
    works in the future.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是对 StreamingLLM 的介绍。总体来看，我相信 StreamingLLM 可以在流媒体应用中占有一席之地，并有助于改变未来应用的工作方式。
- en: Conclusion
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Having an LLM in streaming applications would help the business in the long
    run; however, there are challenges to implement. Most LLMs can’t exceed the predefined
    training sequence length and have higher memory consumption. [Xiao *et al*. (2023)](https://arxiv.org/pdf/2309.17453.pdf)
    developed a new framework called StreamingLLM to handle these issues. Using the
    StreamingLLM, it is now possible to have working LLM in the streaming application.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在流媒体应用中使用 LLM 会从长远来看对业务有所帮助；然而，实施过程中存在挑战。大多数 LLM 无法超越预定义的训练序列长度，并且内存消耗较高。 [Xiao
    *et al*. (2023)](https://arxiv.org/pdf/2309.17453.pdf) 开发了一种新的框架，称为 StreamingLLM，以处理这些问题。通过使用
    StreamingLLM，现在可以在流媒体应用中实现有效的 LLM。
- en: '**[](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**[Cornellius Yudha
    Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**** is a data science
    assistant manager and data writer. While working full-time at Allianz Indonesia,
    he loves to share Python and data tips via social media and writing media. Cornellius
    writes on a variety of AI and machine learning topics.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**[](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**[Cornellius Yudha
    Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**** 是一名数据科学助理经理和数据撰稿人。在全职工作于
    Allianz Indonesia 的同时，他喜欢通过社交媒体和写作媒体分享 Python 和数据技巧。Cornellius 涉及各种 AI 和机器学习话题的写作。'
- en: More On This Topic
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Machine Learning Is Not Like Your Brain Part 5: Biological Neurons…](https://www.kdnuggets.com/2022/07/machine-learning-like-brain-part-5-biological-neurons-cant-summation-inputs.html)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习不像你的大脑 第 5 部分：生物神经元……](https://www.kdnuggets.com/2022/07/machine-learning-like-brain-part-5-biological-neurons-cant-summation-inputs.html)'
- en: '[RedPajama Project: An Open-Source Initiative to Democratizing LLMs](https://www.kdnuggets.com/2023/06/redpajama-project-opensource-initiative-democratizing-llms.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RedPajama 项目：一个开源倡议，旨在民主化 LLM](https://www.kdnuggets.com/2023/06/redpajama-project-opensource-initiative-democratizing-llms.html)'
- en: '[Falcon LLM: The New King of Open-Source LLMs](https://www.kdnuggets.com/2023/06/falcon-llm-new-king-llms.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Falcon LLM：开源 LLM 的新王者](https://www.kdnuggets.com/2023/06/falcon-llm-new-king-llms.html)'
- en: '[Ensuring Reliable Few-Shot Prompt Selection for LLMs](https://www.kdnuggets.com/2023/07/ensuring-reliable-fewshot-prompt-selection-llms.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[确保 LLM 的少量示例提示选择可靠](https://www.kdnuggets.com/2023/07/ensuring-reliable-fewshot-prompt-selection-llms.html)'
- en: '[How Watermarking Can Help Mitigate The Potential Risks Of LLMs?](https://www.kdnuggets.com/2023/03/watermarking-help-mitigate-potential-risks-llms.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[水印如何帮助缓解 LLM 的潜在风险？](https://www.kdnuggets.com/2023/03/watermarking-help-mitigate-potential-risks-llms.html)'
- en: '[Explore LLMs Easily on Your Laptop with openplayground](https://www.kdnuggets.com/2023/04/explore-llms-easily-laptop-openplayground.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在您的笔记本上轻松探索LLMs，使用openplayground](https://www.kdnuggets.com/2023/04/explore-llms-easily-laptop-openplayground.html)'
