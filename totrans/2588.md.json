["```py\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nbase_estimator=DecisionTreeClassifier(max_depth=1,criterion='gini', splitter='best', min_samples_split=2)\nmodel = AdaBoostClassifier(base_estimator=base_estimator,n_estimators=100)\nmodel.fit(X_train, y_train)\n```", "```py\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nbase_estimator = DecisionTreeRegressor(max_depth=1, splitter='best', min_samples_split=2)\nmodel = AdaBoostRegressor(base_estimator=base_estimator,n_estimators=100)\nmodel.fit(X_train, y_train)\n```", "```py\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1)\ngbc.fit(X_train,y_train)\n```", "```py\ngbc.feature_importances_\n```", "```py\nfrom sklearn.ensemble import GradientBoostingRegressor\nparams = {'n_estimators': 500,\n          'max_depth': 4,\n          'min_samples_split': 5,\n          'learning_rate': 0.01,\n          'loss': 'ls'}\ngbc = GradientBoostingRegressor(**params)\ngbc.fit(X_train,y_train)\n```", "```py\ngbc.feature_importances_\n```", "```py\nimport xgboost as xgb\nparams = {\"objective\":\"binary:logistic\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n                'max_depth': 5, 'alpha': 10}\nclassification = xgb.XGBClassifier(**params)\nclassification.fit(X_train, y_train)\n```", "```py\nimport xgboost as xgb\nparams = {\"objective\":\"reg:squarederror\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n                'max_depth': 5, 'alpha': 10}\nregressor = xgb.XGBRegressor(**params)\nregressor.fit(X_train, y_train)\n```", "```py\nregressor.feature_importances_\n```", "```py\nimport matplotlib.pyplot as plt\nxgb.plot_importance(regressor)\nplt.rcParams['figure.figsize'] = [5, 5]\nplt.show()\n```", "```py\nregressor.save_model(\"model.pkl\")\n```", "```py\nimport lightgbm as lgb\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\nparams = {'boosting_type': 'gbdt',\n              'objective': 'binary',\n              'num_leaves': 40,\n              'learning_rate': 0.1,\n              'feature_fraction': 0.9\n              }\ngbm = lgb.train(params,\n    lgb_train,\n    num_boost_round=200,\n    valid_sets=[lgb_train, lgb_eval],\n    valid_names=['train','valid'],\n   )\n\n```", "```py\nimport lightgbm as lgb\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\nparams = {'boosting_type': 'gbdt',\n              'objective': 'regression',\n              'num_leaves': 40,\n              'learning_rate': 0.1,\n              'feature_fraction': 0.9\n              }\ngbm = lgb.train(params,\n    lgb_train,\n    num_boost_round=200,\n    valid_sets=[lgb_train, lgb_eval],\n    valid_names=['train','valid'],\n   )\n```", "```py\nlgb.plot_importance(gbm)\n```", "```py\ngbm.save_model('mode.pkl')\n```", "```py\nfrom catboost import CatBoostClassifier\nmodel = CatBoostClassifier()\nmodel.fit(X_train,y_train,verbose=False, plot=True)\n```", "```py\nfrom catboost import CatBoostRegressor\nmodel = CatBoostRegressor()\nmodel.fit(X_train,y_train,verbose=False, plot=True)\n```", "```py\nmodel.feature_importances_\n```", "```py\nfrom catboost import Pool, cv\nparams = {\"iterations\": 100,\n          \"depth\": 2,\n          \"loss_function\": \"RMSE\",\n          \"verbose\": False}\ncv_dataset = Pool(data=X_train,\n                  label=y_train)\nscores = cv(cv_dataset,\n            params,\n            fold_count=2, \n            plot=True)\n```", "```py\ngrid = {'learning_rate': [0.03, 0.1],\n        'depth': [4, 6, 10],\n        'l2_leaf_reg': [1, 3, 5, 7, 9]}\n\ngrid_search_result = model.grid_search(grid, X=X_train, y=y_train, plot=True)\n```", "```py\nmodel.plot_tree(tree_idx=0)\n```"]