- en: Deploy your PyTorch model to Production
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将你的 PyTorch 模型部署到生产环境
- en: 原文：[https://www.kdnuggets.com/2019/03/deploy-pytorch-model-production.html](https://www.kdnuggets.com/2019/03/deploy-pytorch-model-production.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/03/deploy-pytorch-model-production.html](https://www.kdnuggets.com/2019/03/deploy-pytorch-model-production.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Nicolás Metallo](https://www.linkedin.com/in/nicolas-metallo/?originalSubdomain=uk),
    Audatex**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Nicolás Metallo](https://www.linkedin.com/in/nicolas-metallo/?originalSubdomain=uk)，Audatex**'
- en: Following the last article about [Training a Choripan Classifier](https://medium.com/@nicolas.metallo/train-a-choripan-classifier-with-fast-ai-v1-in-google-colab-6e438817656a)
    with PyTorch and Google Colab, we will now talk about what are some steps that
    you can do if you want to deploy your recently trained model as an API. The discussion
    on how to do this with Fast.ai is [currently ongoing](https://forums.fast.ai/t/using-a-fast-ai-model-in-production/12033)
    ([more](https://forums.fast.ai/t/productionizing-models-thread/28353/54)) and
    will most likely continue until PyTorch releases their official 1.0 version. You
    can find more information in the Fast.ai Forums, PyTorch Documentation/Forums,
    and their respective GitHub repositories.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一篇关于[训练 Choripan 分类器](https://medium.com/@nicolas.metallo/train-a-choripan-classifier-with-fast-ai-v1-in-google-colab-6e438817656a)的文章中，我们讨论了如何使用
    PyTorch 和 Google Colab 进行训练。接下来，我们将探讨如果你想将最近训练的模型作为 API 部署，可以采取哪些步骤。关于如何使用 Fast.ai
    进行此操作的讨论[正在进行中](https://forums.fast.ai/t/using-a-fast-ai-model-in-production/12033)（[更多](https://forums.fast.ai/t/productionizing-models-thread/28353/54)），可能会持续到
    PyTorch 发布其官方 1.0 版本。你可以在 Fast.ai 论坛、PyTorch 文档/论坛及其各自的 GitHub 仓库中找到更多信息。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Saving and Loading Models
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保存和加载模型
- en: It’s recommended that you take a look at the [PyTorch Documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html)
    as it’s a great place to start, but in short, there are two ways to serialize
    and restore a model. One is loading only the weights and the other loading the
    entire model (and weights). You will need to first create a model to define its
    architecture otherwise you will end up with an `OrderedDict` with just the weight
    values. Both options would work for inference and/or for resuming a model's training
    from a previous checkpoint.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐你查看[PyTorch 文档](https://pytorch.org/tutorials/beginner/saving_loading_models.html)，它是一个很好的起点，简而言之，有两种序列化和恢复模型的方法。一种是仅加载权重，另一种是加载整个模型（包括权重）。你需要首先创建一个模型来定义其架构，否则你将得到一个只包含权重值的
    `OrderedDict`。这两种选项都适用于推断和/或从以前的检查点恢复模型训练。
- en: 1\. Using `torch.save()` and `torch.load()`
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1\. 使用 `torch.save()` 和 `torch.load()`
- en: This save/load process uses the most intuitive syntax and involves the least
    amount of code. Saving a model in this way will save the entire module using Python’s
    [pickle](https://docs.python.org/3/library/pickle.html) module. The disadvantage
    of this approach is that the serialized data is bound to the specific classes
    and the exact directory structure used when the model is saved. The reason for
    this is because pickle does not save the model class itself. Rather, it saves
    a path to the file containing the class, which is used during load time. Because
    of this, your code can break in various ways when used in other projects or after
    refactors.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种保存/加载过程使用了最直观的语法，涉及的代码最少。以这种方式保存模型会使用 Python 的 [pickle](https://docs.python.org/3/library/pickle.html)
    模块保存整个模块。这种方法的缺点是序列化数据绑定到模型保存时使用的特定类和确切目录结构。这是因为 pickle 不保存模型类本身，而是保存指向包含类的文件的路径，该路径在加载时使用。因此，当在其他项目或重构后使用时，代码可能会出现各种问题。
- en: '**Save model**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**保存模型**'
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Sometimes `pickle` is not able to serialize some model creations functions (e.g.
    `resnext_50_32x4d` which is found in previous versions of Fastai) so you need
    to use `dill` instead. Here's the fix.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有时 `pickle` 无法序列化一些模型创建函数（例如在旧版本的 Fastai 中发现的 `resnext_50_32x4d`），因此你需要使用 `dill`。这是解决方法。
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can read more about the limitations of `pickle` in this [article](invalid#zSoyz).
    A common PyTorch convention is to save models using either a `.pt` or `.pth` file
    extension.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这篇 [文章](invalid#zSoyz) 中阅读有关 `pickle` 限制的更多信息。一个常见的 PyTorch 约定是使用 `.pt`
    或 `.pth` 文件扩展名保存模型。
- en: '**Load model**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**加载模型**'
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**2\. Using `state_dict`**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 使用 `state_dict`**'
- en: In PyTorch, the learnable parameters (e.g. weights and biases) of an `torch.nn.Module`
    model are contained in the model’s *parameters* (accessed with `model.parameters()`).
    A *state_dict* is simply a Python dictionary object that maps each layer to its
    parameter tensor. Note that only layers with learnable parameters (convolutional
    layers, linear layers, etc.) have entries in the model’s *state_dict*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，`torch.nn.Module` 模型的可学习参数（例如权重和偏差）包含在模型的 *parameters* 中（通过 `model.parameters()`
    访问）。*state_dict* 只是一个 Python 字典对象，将每一层映射到其参数张量。注意，只有具有可学习参数的层（卷积层、线性层等）才会在模型的
    *state_dict* 中有条目。
- en: We will need to re-initialize our model in the same way it was originally defined
    and created when we saved the weights, making sure the variables, classes, functions
    that go into creating the model are available, whether through module imports
    or directly within the same script/file. One potential advantage of using this
    method is that you can use updated scripts to load the old model if the parameters
    are the same, and it’s also the [recommended](https://pytorch.org/docs/master/notes/serialization.html)
    approach by the official documentation. One thing you should also remember is
    that `state_dict` takes a dictionary object, not a path to a saved object, so
    you can't load using `model.load_state_dict(PATH)`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要以与最初定义和创建模型时相同的方式重新初始化模型，并确保创建模型所需的变量、类、函数可用，无论是通过模块导入还是直接在同一个脚本/文件中。使用这种方法的一个潜在优势是，如果参数相同，你可以使用更新的脚本加载旧模型，它也是官方文档推荐的
    [方法](https://pytorch.org/docs/master/notes/serialization.html)。另一个需要记住的事情是 `state_dict`
    接受的是字典对象，而不是保存对象的路径，因此你不能使用 `model.load_state_dict(PATH)` 来加载。
- en: '**Save model**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**保存模型**'
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Load model**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**加载模型**'
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You run `model.eval()` after loading because you usually have `BatchNorm` and
    `Dropout` layers that by default are in train mode on construction. You don't
    need to call `model.eval()` if you want to resume your model training.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 加载后运行 `model.eval()`，因为你通常会有 `BatchNorm` 和 `Dropout` 层，它们在构建时默认是训练模式。如果你想恢复模型训练，则不需要调用
    `model.eval()`。
- en: As we have done our training with Fastai, we can call `[Learner.save](https://docs.fast.ai/basic_train.html#Learner.save)`
    and `[Learner.load](https://docs.fast.ai/basic_train.html#Learner.load)` to save
    and load models (more info in the [documentation](https://docs.fast.ai/basic_train.html#Saving-and-loading-models))
    . This is running `state_dict()` in the back so only the model parameters will
    be saved and not the architecture. This means that you will need to run the create_cnn
    method to get a pre-trained model from a given architecture (the same that you
    used before to train your model, e.g. *models.resnet34*) with a custom head that
    is suitable for your data. Models are saved and loaded from the `path`/`model_dir`
    directory, and the `.pth` extension is automatically added for both operations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经使用 Fastai 进行了训练，我们可以调用 `[Learner.save](https://docs.fast.ai/basic_train.html#Learner.save)`
    和 `[Learner.load](https://docs.fast.ai/basic_train.html#Learner.load)` 来保存和加载模型（更多信息请参见
    [文档](https://docs.fast.ai/basic_train.html#Saving-and-loading-models)）。这会在后台运行
    `state_dict()`，因此只会保存模型参数，而不是模型结构。这意味着你需要运行 `create_cnn` 方法来从给定的结构中获取一个预训练模型（与之前用于训练模型的结构相同，例如
    *models.resnet34*），并且要为你的数据设置一个合适的自定义头。模型会保存在 `path`/`model_dir` 目录中，并且 `.pth`
    扩展名会在这两个操作中自动添加。
- en: Simple Deployment With Flask
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Flask 进行简单部署
- en: After we have trained our classifier using the free GPU available in Google
    Colab, we are ready to do inference on our end. We can do it locally or in the
    cloud and there are many different options (AWS, Paperspace, Google Cloud, etc)
    that we can choose from. As I have some free Amazon AWS credits remaining I’ll
    be using an Amazon AMI that comes with several ML libraries already installed
    hosted on a t2.medium instance. Here are some simple instructions to run a Docker
    image on your end (should be around the same when we are not doing GPU training).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们使用 Google Colab 提供的免费 GPU 训练完分类器后，我们准备在本地进行推理。我们可以在本地或云端进行推理，并且有许多不同的选项（AWS、Paperspace、Google
    Cloud 等）可供选择。由于我还有一些免费的 Amazon AWS 额度，我将使用一个预装了多个 ML 库的 Amazon AMI，并在 t2.medium
    实例上托管。以下是一些在你的端运行 Docker 镜像的简单说明（在不进行 GPU 训练时应该差不多）。
- en: '**Ready-to-run Docker images**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**即开即用的 Docker 镜像**'
- en: Jupyter Docker Stacks are a great way to get a notebook up and going in no time
    with the latest libraries. These are ready-to-run Docker images that contain Jupyter
    applications and interactive computing tools. Learn more about them in the [Official
    Documentation](https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html#jupyter-datascience-notebook).
    We will use the **Jupyter Notebook Data Science Stack** from [this repository](https://github.com/jupyter/docker-stacks).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter Docker Stacks 是一种快速启动 notebook 并使用最新库的绝佳方式。这些是即开即用的 Docker 镜像，包含 Jupyter
    应用程序和交互式计算工具。在 [官方文档](https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html#jupyter-datascience-notebook)
    中了解更多关于它们的信息。我们将使用来自 [此仓库](https://github.com/jupyter/docker-stacks) 的 **Jupyter
    Notebook 数据科学栈**。
- en: Once you have [installed](https://docs.docker.com/install/) Docker, open the
    terminal, `cd` into your working directory, and run
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你 [安装](https://docs.docker.com/install/) 了 Docker，打开终端，`cd` 进入你的工作目录，然后运行
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will create a server that you can log into where you will connect to a
    Jupyter notebook. You can run some commands directly from there or you can also
    run bash or any command in a Docker container by getting the `container id`, typing
    `docker ps` in the terminal and then running
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个你可以登录的服务器，你可以在那里连接到 Jupyter notebook。你可以直接从那里运行一些命令，也可以通过获取 `container
    id`、在终端中输入 `docker ps`，然后运行 bash 或任何命令在 Docker 容器中执行。
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Install PyTorch & [Fastai](https://github.com/fastai/fastai)**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装 PyTorch 和 [Fastai](https://github.com/fastai/fastai)**'
- en: Depending on your machine configuration you will want to run inference on either
    GPU or CPU. In our example, we are going to run everything on the CPU, so you
    need to run the following to install the latest [PyTorch](https://pytorch.org/).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的机器配置，你可能希望在 GPU 或 CPU 上运行推理。在我们的例子中，我们将所有操作都在 CPU 上进行，因此你需要运行以下命令来安装最新的
    [PyTorch](https://pytorch.org/)。
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now you can install Fastai with `pip install fastai`
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以通过 `pip install fastai` 安装 Fastai。
- en: '**Create a Flask Application**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**创建 Flask 应用程序**'
- en: Install the Flask library by running
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行安装 Flask 库
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We will create a folder called `flask_app` and two new python files `server.py`
    with our code to load the model weights and run the inference server and `settings.py`
    that sets some basic params to give us more flexibility in the future. The following
    is an example of what `flask_app/settings.py` might look like. We will then import
    this with `from settings import *` into `server.py` .
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个名为 `flask_app` 的文件夹和两个新的 Python 文件 `server.py`（包含加载模型权重和运行推理服务器的代码）以及
    `settings.py`（设置一些基本参数，为未来提供更多灵活性）。以下是 `flask_app/settings.py` 可能的示例。然后我们将使用 `from
    settings import *` 导入到 `server.py` 中。
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We can now go through the `flask_app/server.py`. This first part will import
    the libraries and settings.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以查看 `flask_app/server.py`。这第一部分将导入库和设置。
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In order to run our single image inference prediction, we first need to create
    a new model that follows the same folder structure that we used when we trained
    it. That’s why we are going to create a new empty dir based on the labels that
    we have set before in `settings.py`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行我们的单图像推理预测，我们首先需要创建一个新的模型，遵循我们训练时使用的相同文件夹结构。这就是为什么我们将基于之前在 `settings.py`
    中设置的标签创建一个新的空目录。
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Once that `path` is defined, we are going to create a new `learn` model and
    download the pre-trained weights for the [Choripan Classifier](https://medium.com/@nicolas.metallo/train-a-choripan-classifier-with-fast-ai-v1-in-google-colab-6e438817656a).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 `path` 被定义，我们将创建一个新的 `learn` 模型并下载 [Choripan 分类器](https://medium.com/@nicolas.metallo/train-a-choripan-classifier-with-fast-ai-v1-in-google-colab-6e438817656a)
    的预训练权重。
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: There are already many great tutorials [online](http://flask.pocoo.org/) that
    are amazingly detailed so I won’t explain much of how Flask works. I created a
    `predict` function that takes the URL that you receive as INPUT, runs it through
    `learn.predict(img)` to get the predicted class, and then returns a `json`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 网上已经有很多很棒的 [教程](http://flask.pocoo.org/) 详细介绍了 Flask 的使用，因此我不会多讲 Flask 是如何工作的。我创建了一个
    `predict` 函数，该函数接受作为输入的 URL，通过 `learn.predict(img)` 获得预测类别，然后返回一个 `json`。
- en: '[PRE13]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Once we have that done, we can head over to the terminal, cd into the `flask_app`
    dir, and run `python server.py`. We should see something like this.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成这些，我们可以进入终端，切换到 `flask_app` 目录，并运行 `python server.py`。我们应该会看到类似的内容。
- en: '[PRE14]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**That’s it!** Now we can run commands like these from the terminal (I’m running
    an AWS instance). Let’s take this image for example.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**就这些！** 现在我们可以从终端运行类似这样的命令（我正在运行一个 AWS 实例）。以这张图片为例。'
- en: '![](../Images/1bc93c133e36d2aa8f5cfd14033110ae.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bc93c133e36d2aa8f5cfd14033110ae.png)'
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: And this is how it looks from the server side
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从服务器端来看，它的样子是这样的
- en: '[PRE16]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Not bad!** You now have your very own “Choripan/Not Choripan” API. If you
    want to move to the next level, please check out [this tutorial](http://flask.pocoo.org/docs/1.0/tutorial/deploy/)
    from the Flask Documentation to deploy to Production and/or [this other tutorial](http://containertutorials.com/docker-compose/flask-simple-app.html)
    if you want to Dockerize your Flask application (you can also use [docker-compose](http://containertutorials.com/docker-compose/flask-compose.html)).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**不错！** 你现在拥有了自己的“Choripan/Not Choripan” API。如果你想提升到下一个层次，请查看 [Flask 文档中的这个教程](http://flask.pocoo.org/docs/1.0/tutorial/deploy/)
    以部署到生产环境和/或 [另一个教程](http://containertutorials.com/docker-compose/flask-simple-app.html)，如果你想将
    Flask 应用程序 Docker 化（你也可以使用 [docker-compose](http://containertutorials.com/docker-compose/flask-compose.html)）。'
- en: Other Ways to Deploy to Production
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署到生产环境的其他方法
- en: 1\. Image Classification Example Using [Clipper](http://clipper.ai)
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1\. 使用 [Clipper](http://clipper.ai) 的图像分类示例
- en: There’s a great `ipynb` that you can follow in the [ClipperTutorials](invalid#zSoyz)
    GitHub with the basics of how everything works. They provide a Docker image or
    you can just run their Amazon AMI. Sadly, this is only working with PyTorch 0.4.0
    which makes it a real pain to convert to when your models have been trained with
    the latest preview versions of PyTorch and Fastai. Works great with the example
    pre-trained model though.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [ClipperTutorials](invalid#zSoyz) GitHub 上有一个很棒的 `ipynb` 文件，你可以跟随它了解一切的基本工作原理。他们提供了一个
    Docker 镜像，或者你可以直接运行他们的 Amazon AMI。遗憾的是，这仅适用于 PyTorch 0.4.0，这使得将其转换为最新预览版本的 PyTorch
    和 Fastai 训练的模型变得非常麻烦。不过，它与示例预训练模型配合得很好。
- en: '**Creating a ClipperConnection**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**创建 ClipperConnection**'
- en: To start Clipper, you must first create a `[ClipperConnection](http://docs.clipper.ai/en/develop/#clipper-connection)`
    object with the type of `ContainerManager` you want to use. In this case, you
    will be using the `DockerContainerManager`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动 Clipper，你必须首先创建一个 `[ClipperConnection](http://docs.clipper.ai/en/develop/#clipper-connection)`
    对象，并指定你想要使用的 `ContainerManager` 类型。在这种情况下，你将使用 `DockerContainerManager`。
- en: '[PRE17]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Starting Clipper**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**启动 Clipper**'
- en: Now that you have a `ClipperConnection` object, you can start a Clipper cluster.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经拥有了一个 `ClipperConnection` 对象，你可以启动一个 Clipper 集群。
- en: 'The following command will start 3 Docker containers:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令将启动 3 个 Docker 容器：
- en: 'The Query Frontend: The Query Frontend container listens for incoming prediction
    requests and schedules and routes them to the deployed models.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查询前端：查询前端容器监听传入的预测请求，并将其调度和路由到已部署的模型。
- en: 'The Management Frontend: The Management Frontend container manages and updates
    the cluster’s internal configuration state, such as tracking which models are
    deployed and which application endpoints have been registered.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 管理前端：管理前端容器管理和更新集群的内部配置状态，例如跟踪已部署的模型和已注册的应用程序端点。
- en: 'A Redis instance: Redis is used to persistently store Clipper’s internal configuration
    state. By default, Redis is started on port 6380 instead of the standard Redis
    default port 6379 to avoid collisions with any Redis instances that are already
    running.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个 Redis 实例：Redis 用于持久存储 Clipper 的内部配置状态。默认情况下，Redis 在端口 6380 上启动，而不是标准 Redis
    默认端口 6379，以避免与已经运行的 Redis 实例发生冲突。
- en: '[PRE18]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Take a look at the containers Clipper has started.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 Clipper 启动的容器。
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**Create an Application**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**创建应用程序**'
- en: '[PRE20]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: When you list the applications registered with Clipper, you should see the newly
    registered `squeezenet-classifier` application show up.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当你列出已注册的应用程序时，你应该会看到新注册的 `squeezenet-classifier` 应用程序。
- en: '[PRE21]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Load an example pre-trained PyTorch model**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**加载一个示例预训练的 PyTorch 模型**'
- en: '[PRE22]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: PyTorch models cannot just be pickled and loaded. Instead, they must be saved
    using PyTorch’s native serialization API. Because of this, you cannot use the
    generic Python model deployer to deploy the model to Clipper. Instead, you will
    use the Clipper PyTorch deployer to deploy it. The Docker container will load
    and reconstruct the model from the serialized model checkpoint when the container
    is started.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 模型不能仅通过 pickle 序列化并加载。相反，它们必须使用 PyTorch 的原生序列化 API 保存。因此，你不能使用通用的 Python
    模型部署工具来将模型部署到 Clipper。相反，你将使用 Clipper 的 PyTorch 部署工具进行部署。Docker 容器在启动时将从序列化的模型检查点中加载并重建模型。
- en: '**Preprocessing**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**预处理**'
- en: '[PRE23]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Define a predict function and add metrics**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义预测函数并添加指标**'
- en: '[PRE24]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Clipper must download this Docker image from the internet, so this may take
    a minute
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Clipper 必须从互联网下载这个 Docker 镜像，所以可能需要一点时间。
- en: '[PRE25]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now link the generated `pytorch-model` to the application `squeezenet-classsifier`
    we created before.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将生成的 `pytorch-model` 链接到之前创建的应用 `squeezenet-classsifier`。
- en: '[PRE26]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '**That’s it!**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**就这样！**'
- en: '**How to query the API with Requests**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**如何使用 Requests 查询 API**'
- en: '[PRE27]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '**Stopping Clipper**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**停止 Clipper**'
- en: If you run into issues and want to completely stop Clipper, you can do this
    by calling `[ClipperConnection.stop_all()](http://docs.clipper.ai/en/latest/#clipper_admin.ClipperConnection.stop_all)`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果遇到问题并希望完全停止 Clipper，你可以通过调用 `[ClipperConnection.stop_all()](http://docs.clipper.ai/en/latest/#clipper_admin.ClipperConnection.stop_all)`
    来实现。
- en: '[PRE28]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: When you list all the Docker containers a final time, you should see that all
    of the Clipper containers have been stopped.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当你最后列出所有 Docker 容器时，你应该会看到所有 Clipper 容器都已停止。
- en: '[PRE29]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 2\. Using Now from Zeit
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2\. 使用来自 Zeit 的 Now
- en: Another option under discussion in the Forums is to use the [Now](https://zeit.co/now)
    service from [Zeit](https://zeit.co/). You can follow [this guide](https://course-v3.fast.ai/deployment_zeit.html)
    in the Fast.ai Documentation. I have tried this but have not gotten accurate results
    (probably because of normalization). Seems very promising.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 论坛讨论中的另一个选项是使用 [Now](https://zeit.co/now) 服务，来自 [Zeit](https://zeit.co/)。你可以参考
    Fast.ai 文档中的 [这个指南](https://course-v3.fast.ai/deployment_zeit.html)。我尝试过这个方法，但没有得到准确的结果（可能是由于归一化问题）。看起来很有前景。
- en: You will need to run these commands only one time. The first installs Now’s
    CLI (Command Line Interface).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需运行这些命令一次。第一次安装 Now 的 CLI（命令行界面）。
- en: '[PRE30]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: And the next downloads a **starter pack** for model deployment based on Fast.ai
    Lesson 2.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步下载基于 Fast.ai 课程 2 的模型部署 **入门包**。
- en: '[PRE31]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '**Upload your trained model file**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**上传你的训练模型文件**'
- en: Upload your trained model file (for example `stage-2.pth`) to a cloud service
    like Google Drive or Dropbox. Copy the download link for the file. **Note:** the
    download link is the one which starts the file download directly—and is normally
    different than the share link which presents you with a view to download the file
    (use [https://rawdownload.now.sh/](https://rawdownload.now.sh/) if needed)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 将你的训练模型文件（例如 `stage-2.pth`）上传到 Google Drive 或 Dropbox 等云服务。复制该文件的下载链接。**注意：**下载链接是直接启动文件下载的链接，通常与提供下载视图的分享链接不同（如有需要，使用
    [https://rawdownload.now.sh/](https://rawdownload.now.sh/)）。
- en: '**Customize the app for your model**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**根据你的模型自定义应用**'
- en: Open up the file `server.py` inside the `app` directory and update the `model_file_url`
    variable with the url copied above
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 `app` 目录中的 `server.py` 文件，并用上面复制的 URL 更新 `model_file_url` 变量。
- en: In the same file, update the line `classes = ['black', 'grizzly', 'teddys']`
    with the classes you are expecting from your model
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一文件中，用你期望的模型类别更新 `classes = ['black', 'grizzly', 'teddys']` 行。
- en: '**Deploy**'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**部署**'
- en: 'On the terminal, make sure you are in the `zeit` directory, then type:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端中，确保你在 `zeit` 目录下，然后输入：
- en: '[PRE32]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The first time you run this, it will prompt for your email address and create
    your Now account for you. After your account is created, run it again to deploy
    your project.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次运行时，它会提示输入你的电子邮件地址，并为你创建一个 Now 帐户。账户创建后，再次运行以部署你的项目。
- en: Every time you deploy with `now` it’ll create a unique **Deployment URL** for
    the app. It has a format of `xxx.now.sh`, and is shown while you are deploying
    the app.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 每次使用 `now` 部署时，它都会为应用创建一个唯一的 **部署 URL**。其格式为 `xxx.now.sh`，在部署应用时会显示。
- en: 3\. Using `Torch Script` and `PyTorch C++ API`
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3\. 使用 `Torch Script` 和 `PyTorch C++ API`
- en: These are some of the most recent changes will come with the official 1.0 version
    of PyTorch. You can follow the instructions in [this](https://pytorch.org/tutorials/advanced/cpp_export.html)
    article from the Documentation or check out the last chapter of the [Intro to
    Deep Learning with PyTorch](https://classroom.udacity.com/courses/ud188) class
    in Udacity where they go through this step by step. This is just an overview of
    the main steps.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是 PyTorch 官方 1.0 版本即将推出的一些最新变化。你可以按照[这篇](https://pytorch.org/tutorials/advanced/cpp_export.html)文档中的说明操作，或查看
    Udacity 的 [Intro to Deep Learning with PyTorch](https://classroom.udacity.com/courses/ud188)
    课程的最后一章，其中详细讲解了这些步骤。这只是主要步骤的概述。
- en: '[PRE33]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '**Build a minimal C++ application**'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**构建一个最小化的 C++ 应用程序**'
- en: Follow [these steps](https://pytorch.org/tutorials/advanced/cpp_export.html)
    and build `example-app.cpp` and `CMakeLists.txt`.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照 [这些步骤](https://pytorch.org/tutorials/advanced/cpp_export.html) 操作，构建 `example-app.cpp`
    和 `CMakeLists.txt`。
- en: Install [Anaconda](https://www.anaconda.com/download/) and get CMAKE to run
    on your machine. You can install it through their [binaries](https://cmake.org/download/)
    or, if you are running MacOS, type `brew install cmake` (install `homebrew` through
    [these](https://brew.sh/) instructions). If you have some problems with CMAKE,
    remember to download the X-Code command line tools as a `.dmg` directly from [here](https://developer.apple.com/download/more/)
    (MacOS 10.14 in my case).
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 [Anaconda](https://www.anaconda.com/download/) 并在你的机器上运行 CMAKE。你可以通过他们的 [binaries](https://cmake.org/download/)
    安装它，或者如果你使用的是 MacOS，输入 `brew install cmake`（通过 [这些](https://brew.sh/) 指令安装 `homebrew`）。如果你遇到
    CMAKE 问题，记得直接从 [这里](https://developer.apple.com/download/more/) 下载 X-Code 命令行工具的
    `.dmg`（在我的情况下是 MacOS 10.14）。
- en: Install Caffe2 from [here](https://caffe2.ai/docs/getting-started.html?platform=mac)
    and run `conda install pytorch-nightly-cpu -c pytorch`
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 [这里](https://caffe2.ai/docs/getting-started.html?platform=mac) 安装 Caffe2 并运行
    `conda install pytorch-nightly-cpu -c pytorch`
- en: '[Build the application](https://pytorch.org/tutorials/advanced/cpp_export.html)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[构建应用程序](https://pytorch.org/tutorials/advanced/cpp_export.html)'
- en: '**PyTorch, Libtorch, C++, and NodeJS**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**PyTorch, Libtorch, C++ 和 NodeJS**'
- en: Look at [http://blog.christianperone.com/2018/10/pytorch-1-0-tracing-jit-and-libtorch-c-api-to-integrate-pytorch-into-nodejs/](http://blog.christianperone.com/2018/10/pytorch-1-0-tracing-jit-and-libtorch-c-api-to-integrate-pytorch-into-nodejs/)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 [http://blog.christianperone.com/2018/10/pytorch-1-0-tracing-jit-and-libtorch-c-api-to-integrate-pytorch-into-nodejs/](http://blog.christianperone.com/2018/10/pytorch-1-0-tracing-jit-and-libtorch-c-api-to-integrate-pytorch-into-nodejs/)
- en: Conclusion
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: I tried my best to summarize some of the options available to deploy your recently
    trained PyTorch models. Hope this is useful and looking forward to reading your
    comments.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我尽力总结了一些部署你最近训练的 PyTorch 模型的选项。希望这对你有帮助，期待阅读你的评论。
- en: '**Bio: [Nicolás Metallo](https://www.linkedin.com/in/nicolas-metallo/?originalSubdomain=uk)**
    is an award-winning entrepreneur with nearly 10 years of professional experience.
    He graduated from New York University with an MSc in Technology Management & Innovation
    and he works as a management consultant and freelance deep learning engineer.
    Nicolas is also the co-founder of INVIP Labs Inc., a social enterprise that helps
    blind and low vision individuals understand their environment better through computer
    vision. His particular interest in data science is focused on providing cities
    with data to make them more connected, efficient, resilient, vibrant, and prosperous.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介： [Nicolás Metallo](https://www.linkedin.com/in/nicolas-metallo/?originalSubdomain=uk)**
    是一位获奖企业家，拥有近 10 年的专业经验。他毕业于纽约大学，获得技术管理与创新硕士学位，并担任管理顾问和自由深度学习工程师。Nicolas 还是 INVIP
    Labs Inc. 的共同创始人，这是一个通过计算机视觉帮助盲人和低视力者更好地理解环境的社会企业。他对数据科学的特别兴趣在于为城市提供数据，使其更加互联、高效、韧性强、充满活力和繁荣。'
- en: '[Original](https://medium.com/datadriveninvestor/deploy-your-pytorch-model-to-production-f69460192217).
    Reposted with permission.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/datadriveninvestor/deploy-your-pytorch-model-to-production-f69460192217)。已获得许可转载。'
- en: '**Related:**'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[How to build an API for a machine learning model in 5 minutes using Flask](https://www.kdnuggets.com/2019/01/build-api-machine-learning-model-using-flask.html)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在 5 分钟内使用 Flask 为机器学习模型构建 API](https://www.kdnuggets.com/2019/01/build-api-machine-learning-model-using-flask.html)'
- en: '[Introduction to PyTorch for Deep Learning](https://www.kdnuggets.com/2018/11/introduction-pytorch-deep-learning.html)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch 深度学习简介](https://www.kdnuggets.com/2018/11/introduction-pytorch-deep-learning.html)'
- en: '[Deep Learning Cheat Sheets](https://www.kdnuggets.com/2018/11/deep-learning-cheat-sheets.html)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习备忘单](https://www.kdnuggets.com/2018/11/deep-learning-cheat-sheets.html)'
- en: More On This Topic
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Deploying Your Machine Learning Model to Production in the Cloud](https://www.kdnuggets.com/deploying-your-ml-model-to-production-in-the-cloud)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[将你的机器学习模型部署到云端生产环境](https://www.kdnuggets.com/deploying-your-ml-model-to-production-in-the-cloud)'
- en: '[Detecting Data Drift for Ensuring Production ML Model Quality Using Eurybia](https://www.kdnuggets.com/2022/07/detecting-data-drift-ensuring-production-ml-model-quality-eurybia.html)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Eurybia 检测数据漂移以确保生产 ML 模型质量](https://www.kdnuggets.com/2022/07/detecting-data-drift-ensuring-production-ml-model-quality-eurybia.html)'
- en: '[Managing Model Drift in Production with MLOps](https://www.kdnuggets.com/2023/05/managing-model-drift-production-mlops.html)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 MLOps 管理生产中的模型漂移](https://www.kdnuggets.com/2023/05/managing-model-drift-production-mlops.html)'
- en: '[From Zero to Hero: Create Your First ML Model with PyTorch](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从零到英雄：使用 PyTorch 创建你的第一个 ML 模型](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)'
- en: '[How to Successfully Deploy Data Science Projects](https://www.kdnuggets.com/2022/01/successfully-deploy-data-science-projects.html)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何成功部署数据科学项目](https://www.kdnuggets.com/2022/01/successfully-deploy-data-science-projects.html)'
- en: '[Learn How to Design & Deploy Responsible AI Systems](https://www.kdnuggets.com/2023/10/teradata-design-deploy-responsible-ai-systems-whitepaper)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习如何设计和部署负责任的人工智能系统](https://www.kdnuggets.com/2023/10/teradata-design-deploy-responsible-ai-systems-whitepaper)'
