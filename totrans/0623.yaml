- en: A Beginner’s Guide to the CLIP Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html](https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Matthew Brems](https://www.linkedin.com/in/matthewbrems/), Growth Manager
    @ Roboflow**'
  prefs: []
  type: TYPE_NORMAL
- en: You may have heard about [OpenAI's CLIP model](https://openai.com/blog/clip/).
    If you looked it up, you read that CLIP stands for "Contrastive Language-Image
    Pre-training." That doesn't immediately make much sense to me, so [I read the
    paper](https://arxiv.org/pdf/2103.00020.pdf) where they develop the CLIP model
    – [and the corresponding blog post](https://openai.com/blog/clip/).
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'I''m here to break CLIP down for you in a – hopefully – accessible and fun
    read! In this post, I''ll cover:'
  prefs: []
  type: TYPE_NORMAL
- en: what CLIP is,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: how CLIP works, and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: why CLIP is cool.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is CLIP?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CLIP is the first multimodal (in this case, vision and text) model tackling
    computer vision and was recently released [by OpenAI](https://openai.com/blog/clip/) on
    January 5, 2021\. From the [OpenAI CLIP repository](https://github.com/openai/CLIP),
    "CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on
    a variety of (image, text) pairs. It can be instructed in natural language to
    predict the most relevant text snippet, given an image, without directly optimizing
    for the task, similarly to the zero-shot capabilities of GPT-2 and 3."
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your background, this *may* make sense -- but there's a lot in
    here that may be unfamiliar to you. Let's unpack it.
  prefs: []
  type: TYPE_NORMAL
- en: CLIP is a neural network model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is trained on 400,000,000 (image, text) pairs. An (image, text) pair might
    be a picture and its caption. So this means that there are 400,000,000 pictures
    and their captions that are matched up, and this is the data that is used in training
    the CLIP model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*"It can predict the most relevant text snippet, given an image."* You can
    input an image into the CLIP model, and it will return for you the likeliest caption
    or summary of that image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*"without directly optimizing for the task, similarly to the zero-shot capabilities
    of GPT-2 and 3."* Most machine learning models learn a specific task. For example,
    an image classifier trained on classifying dogs and cats is expected to do well
    on the task we''ve given it: classifying dogs and cats. We generally would not
    expect a machine learning model trained on dogs and cats to be very good at detecting
    raccoons. However, some models -- including CLIP, GPT-2, and GPT-3 -- tend to
    perform well on tasks they aren''t directly trained to do, which is called "zero-shot
    learning."'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"Zero-shot learning" is when a model attempts to predict a class it saw zero
    times in the training data. So, using a model trained on exclusively cats and
    dogs to then detect raccoons. A model like CLIP, because of how it uses the text
    information in the (image, text) pairs, tends to do really well with zero-shot
    learning -- even if the image you''re looking at is really different from the
    training images, your CLIP model will likely be able to give a good guess for
    the caption for that image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To put this all together, the CLIP model is:'
  prefs: []
  type: TYPE_NORMAL
- en: a neural network model built on hundreds of millions of images and captions,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: can return the best caption given an image, and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: has impressive "zero-shot" capabilities, making it able to accurately predict
    entire classes it's never seen before!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When I wrote my [Introduction to Computer Vision](https://blog.roboflow.com/intro-to-computer-vision/) post,
    I described computer vision as "the ability for a computer to see and understand
    what it sees in manner similar to humans."
  prefs: []
  type: TYPE_NORMAL
- en: '[When I''ve taught natural language processing](https://github.com/matthewbrems/nlp-fundamentals-python),
    I described NLP in a similar way: "the ability for a computer to understand language
    in manner similar to humans."'
  prefs: []
  type: TYPE_NORMAL
- en: '**CLIP is a bridge between computer vision and natural language processing.**'
  prefs: []
  type: TYPE_NORMAL
- en: It's not *just* a bridge between computer vision and natural language processing
    -- it's a **very powerful bridge** between the two that has a lot of flexibility
    and a lot of applications.
  prefs: []
  type: TYPE_NORMAL
- en: How does CLIP work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order for images and text to be connected to one another, they must both
    be [embedded](https://en.wikipedia.org/wiki/Embedding). You''ve worked with embeddings
    before, even if you haven''t thought of it that way. Let''s go through an example.
    Suppose you have one cat and two dogs. You could represent that as a dot on a
    graph, like below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da688b366c7e92a9420424b780e0ca7a.png)'
  prefs: []
  type: TYPE_IMG
- en: Embedding of "1 cat, 2 dogs." ([Source](https://www.wolframalpha.com/input/?i=%281%2C2%29).)
  prefs: []
  type: TYPE_NORMAL
- en: It may not seem very exciting, but we just embedded that information on the
    X-Y grid that you probably learned about in middle school (formally called [Euclidean
    space](https://en.wikipedia.org/wiki/Euclidean_space)). You could also embed your
    friends' pet information on the same graph and/or you could have chosen plenty
    of different ways to represent that information (e.g. put dogs before cats, or
    add a third dimension for raccoons).
  prefs: []
  type: TYPE_NORMAL
- en: I like to think of embedding as a way to smash information into mathematical
    space. We just took information about dogs and cats and smashed it into mathematical
    space. ***We can do the same thing with text and with images!***
  prefs: []
  type: TYPE_NORMAL
- en: '**The CLIP model consists of two sub-models called encoders:**'
  prefs: []
  type: TYPE_NORMAL
- en: a text encoder that will embed (smash) text into mathematical space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an image encoder that will embed (smash) images into mathematical space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whenever you fit a [supervised learning model](https://towardsdatascience.com/a-brief-introduction-to-supervised-learning-54a3e3932590),
    you have to find some way to measure the "goodness" or the "badness" of that model
    – the goal is to fit a model that is as "most good" and "least bad" as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CLIP model is no different: the text encoder and image encoder are fit
    to maximize goodness and minimize badness.'
  prefs: []
  type: TYPE_NORMAL
- en: '**So, how do we measure "goodness" and "badness?"**'
  prefs: []
  type: TYPE_NORMAL
- en: In the image below, you'll see a set of purple text cards going into the text
    encoder. The output for each card would be a series of numbers. For example, the
    top card, `pepper the aussie pup` would enter the text encoder – the thing smashing
    it into mathematical space – and come out as a series of numbers like (0, 0.2,
    0.8).
  prefs: []
  type: TYPE_NORMAL
- en: 'The exact same thing will happen for the images: each image will go into the
    image encoder and the output for each image will also be a series of numbers.
    The picture of, presumably Pepper the Aussie pup, will come out like (0.05, 0.25,
    0.7).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e6be4d3d47d9e227f3975927e4c436de.png)'
  prefs: []
  type: TYPE_IMG
- en: The pre-training phase. ([Source](https://openai.com/blog/clip/).)
  prefs: []
  type: TYPE_NORMAL
- en: '**"Goodness" of our model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In an ideal world, the series of numbers for the text "pepper the aussie pup"
    will be very close (identical) to the series of numbers for the corresponding
    image. *In fact, this should be the case everywhere*: the series of numbers for
    the text should be very close to the series of numbers for the corresponding image.
    One way for us to measure "goodness" of our model is how close the embedded representation
    (series of numbers) for each text is to the embedded representation for each image.'
  prefs: []
  type: TYPE_NORMAL
- en: There is a convenient way to calculate the similarity between two series of
    numbers: [the cosine similarity](https://www.machinelearningplus.com/nlp/cosine-similarity/).
    We won't get into the inner workings of that formula here, but rest assured that
    it's a tried and true method of seeing how similar two vectors, or series of numbers,
    are. (Though it isn't the only way!)
  prefs: []
  type: TYPE_NORMAL
- en: In the image above, the light blue squares represent where the text and image
    coincide. For example, T1 is the embedded representation of the first text; I1
    is the embedded representation of the first image. We want the cosine similarity
    for I1 and T1 to be as high as possible. We want the same for I2 and T2, and so
    on for all of the light blue squares. **The higher these cosine similarities are,
    the more "goodness" our model has!**
  prefs: []
  type: TYPE_NORMAL
- en: '**"Badness" of our model**'
  prefs: []
  type: TYPE_NORMAL
- en: At the same time as wanting to maximize the cosine similarity for each of those
    blue squares, there are a lot of grey squares that indicate where the text and
    image don't align. For example, T1 is the text "pepper the aussie pup" but perhaps
    I2 is [an image of a raccoon](https://public.roboflow.com/object-detection/raccoon).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/733d91cf5addc3335394fbde3eff9d2e.png)'
  prefs: []
  type: TYPE_IMG
- en: Picture of a raccoon with bounding box annotation. ([Source](https://public.roboflow.com/object-detection/raccoon).)
  prefs: []
  type: TYPE_NORMAL
- en: Cute though this raccoon is, we want the cosine similarity between this image
    (I2) and the text `pepper the aussie pup` to be pretty small, because this isn't
    Pepper the Aussie pup!
  prefs: []
  type: TYPE_NORMAL
- en: While we wanted the blue squares to all have high cosine similarities (as that
    measured "goodness"), we want all of the grey squares to have low cosine similarities,
    because that measures "badness."
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5daa99f9af00dfc14023717b1561fad6.png)'
  prefs: []
  type: TYPE_IMG
- en: Maximize cosine similarity of the blue squares; minimize cosine similarity of
    the grey squares. ([Source](https://openai.com/blog/clip/).)
  prefs: []
  type: TYPE_NORMAL
- en: '**How do the text and image encoders get fit?**'
  prefs: []
  type: TYPE_NORMAL
- en: The text encoder and image encoder get fit at the same time by simultaneously
    maximizing the cosine similarity of those blue squares and minimizing the cosine
    similarity of the grey squares, across all of our text+image pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: this can take a very long time depending on the size of your data. The
    CLIP model trained on 400,000,000 labeled images. The training process took 30
    days across [592 V100 GPUs](https://www.nvidia.com/en-us/data-center/v100/). This
    would have cost $1,000,000 to train on AWS on-demand instances!'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Once the model is fit, you can pass an image into the image encoder to retrieve
    the text description that best fits the image – or, vice versa, you can pass a
    text description into the model to retrieve an image, as you'll see in some of
    the applications below!
  prefs: []
  type: TYPE_NORMAL
- en: '**CLIP is a bridge between computer vision and natural language processing.**'
  prefs: []
  type: TYPE_NORMAL
- en: Why is CLIP cool?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With this bridge between computer vision and natural language processing, CLIP
    has a ton of advantages and cool applications. We''ll focus on the applications,
    but a few advantages to call out:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generalizability: Models are usually super brittle, capable of knowing only
    the very specific thing you trained them to do. CLIP expands knowledge of classification
    models to a wider array of things by leveraging semantic information in text.
    Standard classification models completely discard the semantic meaning of the
    class labels and simply  enumerated numeric classes behind the scenes; CLIP works
    by understanding the meaning of the classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Connecting text / images better than ever before: CLIP may quite literally
    be the "world''s best caption writer" when considering speed and accuracy together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Already-labeled data: CLIP is built on images and captions that were already
    created; other state-of-the-art computer vision algorithms required significant
    additional human time spent labeling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why does [@OpenAI](https://twitter.com/OpenAI?ref_src=twsrc%5Etfw)'s CLIP model
    matter?[https://t.co/X7bnSgZ0or](https://t.co/X7bnSgZ0or)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Joseph Nelson (@josephofiowa) [January 6, 2021](https://twitter.com/josephofiowa/status/1346639942571712521?ref_src=twsrc%5Etfw)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Some of the uses of CLIP so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[CLIP has been used to index photos on sites like Unsplash](https://twitter.com/metasemantic/status/1349446585952989186?s=20).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One Twitter user took celebrities including Elvis Presley, Beyoncé, and Billie
    Eilish, [and used CLIP and StyleGAN to generate portraits in the style of "My
    Little Pony."](https://twitter.com/metasemantic/status/1368713208429764616)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have you played Pictionary? Now you can play online at [paint.wtf, where you'll
    be judged by CLIP](https://paint.wtf/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CLIP could be used to easily improve NSFW filters!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Find photos matching a mood – for example, via a poetry passage](https://twitter.com/metasemantic/status/1349446585952989186).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI has also created [DALL-E, which creates images from text](https://openai.com/blog/dall-e/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We hope you'll check out some of the above – or create your own! We've got a [CLIP
    tutorial](https://blog.roboflow.com/how-to-use-openai-clip/) for you to follow.
    If you do something with it, [let us know so we can add it to the above list](https://roboflow.com/contact)!
  prefs: []
  type: TYPE_NORMAL
- en: It's important to note that CLIP is *a* bridge between computer vision and natural
    language processing. CLIP is not the only bridge between them. You could build
    those text and image encoders very differently or find other ways of connecting
    the two. **However, CLIP has so far been an exceptionally innovative technique
    that has promoted significant additional innovation.**
  prefs: []
  type: TYPE_NORMAL
- en: We're eager to see what you build with CLIP and to see the advancements that
    are built on top of it!
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Matthew Brems](https://www.linkedin.com/in/matthewbrems/)** is Growth
    Manager @ Roboflow.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://blog.roboflow.com/clip-model-eli5-beginner-guide/). Reposted
    with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[OpenAI Releases Two Transformer Models that Magically Link Language and Computer
    Vision](/2021/01/openai-transformer-models-link-language-computer-vision.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Evaluating Object Detection Models Using Mean Average Precision](/2021/03/evaluating-object-detection-models-using-mean-average-precision.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Reducing the High Cost of Training NLP Models With SRU++](/2021/03/reducing-high-cost-training-nlp-models-sru.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[A Beginner''s Guide to End to End Machine Learning](https://www.kdnuggets.com/2021/12/beginner-guide-end-end-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Essential Machine Learning Algorithms: A Beginner''s Guide](https://www.kdnuggets.com/2021/05/essential-machine-learning-algorithms-beginners.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Beginner''s Guide to Q Learning](https://www.kdnuggets.com/2022/06/beginner-guide-q-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Beginner’s Guide to Web Scraping Using Python](https://www.kdnuggets.com/2022/10/beginner-guide-web-scraping-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Beginner''s Guide to Cloud Computing](https://www.kdnuggets.com/2023/01/beginner-guide-cloud-computing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Beginner''s Guide to Anomaly Detection Techniques in Data Science](https://www.kdnuggets.com/2023/05/beginner-guide-anomaly-detection-techniques-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
