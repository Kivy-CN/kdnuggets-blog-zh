- en: Training and Visualising Word Vectors
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和可视化词向量
- en: 原文：[https://www.kdnuggets.com/2018/01/training-visualising-word-vectors.html](https://www.kdnuggets.com/2018/01/training-visualising-word-vectors.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/01/training-visualising-word-vectors.html](https://www.kdnuggets.com/2018/01/training-visualising-word-vectors.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Priyanka Kochhar](https://github.com/priya-dwivedi), Deep Learning Consultant**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Priyanka Kochhar](https://github.com/priya-dwivedi) 编写，深度学习顾问**'
- en: In this tutorial I want to show how you can implement a skip gram model in tensorflow
    to generate word vectors for any text you are working with and then use tensorboard
    to visualize them. I found this exercise super useful to i) understand how skip
    gram model works and ii) get a feel for the kind of relationship these vectors
    are capturing about your text before you use them downstream in CNNs or RNNs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我想展示如何在 TensorFlow 中实现 Skip Gram 模型，以生成你正在处理的任何文本的词向量，然后使用 TensorBoard
    进行可视化。我发现这个练习非常有用：一是理解 Skip Gram 模型如何工作，二是感受这些向量在你将它们用于 CNN 或 RNN 之前捕捉的文本关系。
- en: I trained a skip gram model on text8 dataset which is collection of English
    Wikipedia articles. I used Tensorboard to visualize the embeddings. Tensorboard
    allows you to see the whole word cloud by using PCA to select 3 main axis to project
    the data. Super cool! You can type in any word and it will show its neighbours.
    You can also isolate the 101 points closest to it.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 text8 数据集上训练了一个 Skip Gram 模型，该数据集是英语维基百科文章的集合。我使用 TensorBoard 可视化了这些嵌入。TensorBoard
    允许你通过使用 PCA 选择三个主轴来投影数据，从而查看整个词云。非常酷！你可以输入任何单词，它会显示其邻近词。你也可以隔离离它最近的 101 个点。
- en: See clip below.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见下面的剪辑。
- en: '![](../Images/95e7808cb6d9ccfb709d13388288fc97.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95e7808cb6d9ccfb709d13388288fc97.png)'
- en: You can find the full code on my [Github](https://github.com/priya-dwivedi/Deep-Learning/blob/master/word2vec_skipgram/Skip-Grams-Solution.ipynb) repo.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我的 [Github](https://github.com/priya-dwivedi/Deep-Learning/blob/master/word2vec_skipgram/Skip-Grams-Solution.ipynb)
    仓库中找到完整代码。
- en: To visualize training, I also looked at the closest predicted word to a random
    set of words. In the first iteration the closest predicted words seem very arbitrary
    which makes sense since all word vectors were randomly initialized
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化训练，我还查看了与一组随机单词最接近的预测单词。在第一次迭代中，最接近的预测单词似乎非常随意，这很合理，因为所有词向量都是随机初始化的。
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: By the end of training, the model had become much better at finding relationship
    between words.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到训练结束时，模型在发现单词之间的关系方面变得更为出色。
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Word2Vec and Skip Gram model
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Word2Vec 和 Skip Gram 模型
- en: Creating word vectors is the process of taking a large corpus of text and creating
    a vector for each word such that words that share common contexts in the corpus
    are located in close proximity to one another in the vector space.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 创建词向量是将大量文本语料库中的每个单词创建一个向量的过程，使得在语料库中共享相同上下文的单词在向量空间中彼此接近。
- en: These word vectors can get amazingly good at capturing contextual relationship
    between words (example vectors for black, white and red would be close together)
    and we get far better performance with using these vectors instead of raw words
    for NLP tasks like text classification or new text generation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些词向量在捕捉单词之间的上下文关系方面表现得非常好（例如，黑色、白色和红色的向量会非常接近），我们在使用这些向量而不是原始单词进行自然语言处理任务（如文本分类或新文本生成）时，性能会更好。
- en: 'There are two main models for generating these word vectors — Continuous Bag
    of Words (CBOW) and Skip Gram Model. The CBOW model tries to predict the center
    word given context word while skip gram model tries to predict context words given
    center word. A simplified example would be:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 生成这些词向量主要有两种模型——连续词袋模型（CBOW）和 Skip Gram 模型。CBOW 模型试图根据上下文单词预测中心单词，而 Skip Gram
    模型试图根据中心单词预测上下文单词。一个简化的例子是：
- en: 'CBOW: The cat ate _____. Fill in the blank, in this case, it’s “food”.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 'CBOW: 猫吃了____。填补空白，在这种情况下，是“食物”。'
- en: 'Skip-gram: ___ ___ ___ food. Complete the word’s context. In this case, it’s
    “The cat ate”'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 'Skip-gram: ___ ___ ___ 食物。填补词的上下文。在这种情况下，是“猫吃了”'
- en: If you are interested in a more detailed comparison of these two methods, then
    please see this [link](https://iksinc.wordpress.com/tag/continuous-bag-of-words-cbow/).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这两种方法的详细比较感兴趣，请参见这个 [链接](https://iksinc.wordpress.com/tag/continuous-bag-of-words-cbow/)。
- en: Various papers have found that Skip gram model results in better word vectors
    and so I have focused on implementing that.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 各种论文发现 Skip Gram 模型生成的词向量更好，因此我专注于实现这一点。
- en: Implementing Skip Gram model in Tensorflow
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Tensorflow 中实现 Skip Gram 模型
- en: Here I will list the main steps to build the model. Please see the detailed
    implementation on my [Github](https://github.com/priya-dwivedi/Deep-Learning/blob/master/word2vec_skipgram/Skip-Grams-Solution.ipynb)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我将列出构建模型的主要步骤。详细的实现请参见我的[Github](https://github.com/priya-dwivedi/Deep-Learning/blob/master/word2vec_skipgram/Skip-Grams-Solution.ipynb)。
- en: 1\. Preprocessing the data
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 数据预处理
- en: We first clean our data. Remove any punctuation, digits and split the text into
    individual words. Since programs deal much better with integers than words we
    map every word to an int by creating a vocab to int dictionary. Code below.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先清理数据。去除任何标点符号、数字，并将文本拆分为单独的词。由于程序对整数的处理比对词的处理更好，我们将每个词映射到一个整数，通过创建一个词到整数的字典。下面是代码。
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 2\. Subsampling
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 子采样
- en: Words that show up often such as “the”, “of”, and “for” don’t provide much context
    to the nearby words. If we discard some of them, we can remove some of the noise
    from our data and in return get faster training and better representations. This
    process is called subsampling by [Mikolov](https://arxiv.org/pdf/1301.3781.pdf).
    For each word in the training set, we’ll discard it with probability given by
    inverse of its frequency.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 像“the”、“of”和“for”这样的常见词对附近的词没有提供太多上下文。如果我们丢弃一些这样的词，我们可以去除数据中的一些噪声，从而获得更快的训练速度和更好的表示。这个过程被称为由[Mikolov](https://arxiv.org/pdf/1301.3781.pdf)提出的子采样。对于训练集中每个词，我们将以其频率的倒数给出的概率丢弃它。
- en: 3\. Creating inputs and targets
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 创建输入和目标
- en: The input for skip gram is each word (coded as int) and the target is words
    around that window. Mikolov et al found that performance was better if this window
    was variable in size and words closer to to the center word were sampled more
    frequently.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Skip Gram的输入是每个词（编码为整数），目标是该窗口周围的词。Mikolov等人发现，如果这个窗口的大小是可变的，并且离中心词较近的词被更频繁地采样，性能会更好。
- en: “Since the more distant words are usually less related to the current word than
    those close to it, we give less weight to the distant words by sampling less from
    those words in our training examples… If we choose window size=5, for each training
    word we will select randomly a number R in range between 1 and window size, and
    then use R words from history and R words from the future of the current word
    as correct labels.”
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: “由于距离较远的词通常与当前词的相关性不如靠近的词，因此我们通过在训练示例中减少从这些词中采样的频率来给予远离词较少的权重……如果我们选择窗口大小=5，对于每个训练词，我们将随机选择一个在1到窗口大小之间的数字R，然后使用当前词的历史中R个词和未来中R个词作为正确标签。”
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 4\. Building the model
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 构建模型
- en: From [Chris McCormick’s blog](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/),
    we can see the general structure of the network that we will build.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从[Chris McCormick 的博客](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)中，我们可以看到我们将要构建的网络的一般结构。
- en: '![](../Images/968c128d8ca16ad61a212372dd92165f.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/968c128d8ca16ad61a212372dd92165f.png)'
- en: We’re going to represent an input word like “ants” as a one-hot vector. This
    vector will have 10,000 components (one for every word in our vocabulary) and
    we’ll place a “1” in the position corresponding to the word “ants”, and 0s in
    all of the other positions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把输入词“ants”表示为一个独热向量。这个向量将有10,000个分量（每个词汇表中的一个），我们将在对应“ants”词的位置上放置一个“1”，其他位置上放置“0”。
- en: The output of the network is a single vector (also with 10,000 components) containing,
    for every word in our vocabulary, the probability that a randomly selected nearby
    word is that vocabulary word.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的输出是一个包含10,000个分量的单一向量，其中包含我们词汇表中每个词的概率，即随机选择的附近词是该词汇表词的概率。
- en: At the end of training the hidden layer will have the trained word vectors.
    The size of the hidden layer corresponds to the num of dimensions in our vector.
    In the example above each word will have a vector of length 300.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练结束时，隐藏层将拥有训练好的词向量。隐藏层的大小对应于我们向量的维度数量。在上述示例中，每个词将具有长度为300的向量。
- en: You may have noticed that the skip-gram neural network contains a huge number
    of weights… For our example with 300 features and a vocab of 10,000 words, that’s
    3M weights in the hidden layer and output layer each! Training this on a large
    dataset would be prohibitive, so the word2vec authors introduced a number of tweaks
    to make training feasible. You can read more about them in the [link](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/).
    The code on [Github](https://github.com/priya-dwivedi/Deep-Learning/blob/master/word2vec_skipgram/Skip-Grams-Solution.ipynb) implements
    these to speed up training.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，skip-gram神经网络包含大量权重……以我们300个特征和10,000个单词的词汇表为例，每个隐藏层和输出层都有300万权重！在大型数据集上训练将非常困难，因此word2vec的作者引入了一些调整以使训练成为可能。你可以在
    [链接](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)
    中阅读更多内容。 [Github](https://github.com/priya-dwivedi/Deep-Learning/blob/master/word2vec_skipgram/Skip-Grams-Solution.ipynb)
    上的代码实现了这些调整以加速训练。
- en: 5\. Visualizing using Tensorboard
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 使用Tensorboard进行可视化
- en: 'You can using the embeddings projector in Tensorboard to visualize the embeddings.
    To do this you need to do a few things:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用Tensorboard中的嵌入投影器来可视化嵌入。为此，你需要做几件事：
- en: Save your model at the end of training in a checkpoints directory
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练结束时将模型保存在检查点目录中
- en: Create a metadata.tsv file that has the mapping for each int back to word so
    that Tensorboard displays words instead of ints. Save this tsv file in the same
    checkpoints directory
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个metadata.tsv文件，其中包含每个整数到单词的映射，以便Tensorboard显示单词而不是整数。将此tsv文件保存在相同的检查点目录中
- en: 'Run this code:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行这段代码：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Open tensorboard by pointing it to the checkpoints directory
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将Tensorboard指向检查点目录来打开Tensorboard
- en: That’s it!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了！
- en: Give me a ❤️ if you liked this post:) Hope you pull the code and try it yourself.
    If you have other ideas on this topic please comment on this post or mail me at
    [priya.toronto3@gmail.com](mailto:priya.toronto3@gmail.com)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章，请给我一个❤️:) 希望你能拉取代码并自己尝试。如果你对此话题有其他想法，请在这篇文章下评论或通过 [priya.toronto3@gmail.com](mailto:priya.toronto3@gmail.com)
    联系我。
- en: '**Other writings**:[ https://medium.com/@priya.dwivedi/](https://medium.com/@priya.dwivedi/)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**其他文章**：[ https://medium.com/@priya.dwivedi/](https://medium.com/@priya.dwivedi/)'
- en: 'PS: I have a deep learning consultancy and love to work on interesting problems.
    If you have a project that we can work collaborate on then please contact me at
    [priya.toronto3@gmail.com](mailto:priya.toronto3@gmail.com)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: PS：我有一家深度学习咨询公司，喜欢处理有趣的问题。如果你有一个我们可以合作的项目，请通过 [priya.toronto3@gmail.com](mailto:priya.toronto3@gmail.com)
    联系我。
- en: '**References:**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献：**'
- en: '[Udacity](https://www.udacity.com/) Deep Learning Nanodegree'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Udacity](https://www.udacity.com/) 深度学习纳米学位'
- en: '**Bio: [Priyanka Kochhar](https://github.com/priya-dwivedi)** has been a data
    scientist for 10+ years. She now has her own deep learning consultancy and loves
    to work on interesting problems. She has helped several startups deploy innovative
    AI based solutions. If you have a project that she can collaborate on then please
    contact her at [priya.toronto3@gmail.com](mailto:priya.toronto3@gmail.com)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[Priyanka Kochhar](https://github.com/priya-dwivedi)** 已从事数据科学工作超过10年。她现在有自己的深度学习咨询公司，喜欢处理有趣的问题。她帮助多个初创公司部署创新的AI解决方案。如果你有一个她可以合作的项目，请通过
    [priya.toronto3@gmail.com](mailto:priya.toronto3@gmail.com) 联系她。'
- en: '[Original](https://towardsdatascience.com/training-and-visualising-word-vectors-2f946c6430f8).
    Reposted with permission.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/training-and-visualising-word-vectors-2f946c6430f8)。经许可转载。'
- en: '**Related:**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[A Framework for Approaching Textual Data Science Tasks](/2017/11/framework-approaching-textual-data-tasks.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[接近文本数据科学任务的框架](/2017/11/framework-approaching-textual-data-tasks.html)'
- en: '[A General Approach to Preprocessing Text Data](/2017/12/general-approach-preprocessing-text-data.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本数据预处理的一般方法](/2017/12/general-approach-preprocessing-text-data.html)'
- en: '[Beyond Word2Vec Usage For Only Words](/2018/01/beyond-word2vec-for-words.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[超越Word2Vec仅用于词语](/2018/01/beyond-word2vec-for-words.html)'
- en: '* * *'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的 IT 工作'
- en: '* * *'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Automate Microsoft Excel and Word Using Python](https://www.kdnuggets.com/2021/08/automate-microsoft-excel-word-python.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 自动化 Microsoft Excel 和 Word](https://www.kdnuggets.com/2021/08/automate-microsoft-excel-word-python.html)'
- en: '[The Ultimate Guide To Different Word Embedding Techniques In NLP](https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NLP 中不同词嵌入技术的终极指南](https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html)'
- en: '[Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 TensorFlow 和 Keras 构建并训练你的第一个神经网络](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
- en: '[Online Training and Workshops with Nvidia](https://www.kdnuggets.com/2022/07/online-training-workshops-nvidia.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[与 Nvidia 的在线培训和研讨会](https://www.kdnuggets.com/2022/07/online-training-workshops-nvidia.html)'
- en: '[The Difference Between Training and Testing Data in Machine Learning](https://www.kdnuggets.com/2022/08/difference-training-testing-data-machine-learning.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中训练数据和测试数据的区别](https://www.kdnuggets.com/2022/08/difference-training-testing-data-machine-learning.html)'
- en: '[New From Anaconda! Data Science Training and Cloud Hosted Notebooks](https://www.kdnuggets.com/2022/11/anaconda-new-anaconda-data-science-training-cloud-hosted-notebooks.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[来自 Anaconda 的新动态！数据科学培训和云托管笔记本](https://www.kdnuggets.com/2022/11/anaconda-new-anaconda-data-science-training-cloud-hosted-notebooks.html)'
