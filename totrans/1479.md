# 机器学习中的偏差是否都不好？

> 原文：[https://www.kdnuggets.com/2019/07/bias-machine-learning-bad.html](https://www.kdnuggets.com/2019/07/bias-machine-learning-bad.html)

![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)![figure-name](../Images/1efd83fa84124475a2da048279623546.png)

Tom M. Mitchell 在1980年发表了一篇论文：[学习概括中的偏差需求](http://dml.cs.byu.edu/~cgc/docs/mldm_tools/Reading/Need%20for%20Bias.pdf?source=post_page---------------------------)指出：

> 学习涉及从过去的经验中概括的能力，以处理与这些经验“相关”的新情况。应对新情况所需的归纳跳跃似乎只有在某些偏差下才能实现，这些偏差在选择一种概括而非另一种时起作用。这篇论文精确定义了概括问题中的偏差概念，然后表明偏差对于归纳跳跃是必要的。

* * *

## 我们的三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业轨道

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织进行 IT 支持

* * *

我们在多年的预测模型构建中被教导，偏差会对模型造成伤害。偏差控制需要由能够区分正确和错误偏差的人来掌控。

他的论文指出，有某些偏差可以帮助我们为手头的问题创建合适的模型：

1.  **领域的事实知识**

+   在为特定目的学习概括时，可能会有

    通过求助于关于任务领域的知识，限制考虑的概括范围。在一个学习棒球规则的程序（Soloway & Riseman 1978）中，关于竞争游戏的普遍知识限制了考虑的概括数量。这种前置知识可以为考虑的概括提供强有力且合理的约束。在这种情况下，概括系统的目标变为“确定与训练实例一致的概括，以及与任务领域的其他已知事实一致的概括”。

1.  **学习概括的预期用途**

+   了解学习到的概括的预期用途可以为学习提供强有力的偏见。举个简单的例子，如果学习到的概括的预期用途涉及对错误的积极分类的成本远高于对错误的消极分类的成本，那么学习程序应该更倾向于更具体的概括，而不是与相同训练数据一致的更一般的替代方案。

1.  **有关训练数据来源的知识**

+   了解训练实例的来源也可以为学习提供有用的约束。例如，在从人类教师学习时，我们似乎利用了关于存在组织化课程的许多假设来约束我们对适当概括的搜索。在组织化课程中，我们的注意力被精确地集中在实例的特定特征上。

    消除了关于哪种可能的概括最合适的模糊性。

1.  **对简单性和通用性的偏见**

    +   人类似乎使用的一个偏见是对简单、通用解释的偏见。可解释性使我们能够将模型传达给外行，而黑箱模型可能会让模型构建者陷入困境。

1.  **与之前学习的概括的类比**

+   如果系统正在学习一组相关概念或概括，那么对其中任何一个概括的可能约束是考虑其他概括的成功。例如，考虑学习块世界对象的结构描述任务，如“拱形”、“塔”等。在学习了几个概念后，学习到的描述可能会揭示出某些特征在描述这个类别的概念时比其他特征更重要。例如，如果概括语言包含“形状”、“颜色”和“年龄”等特征，系统可能会注意到“年龄”很少是相关特征。

    用于表征结构类别，并可能对其他特征产生偏见。对这种学习到的偏见的 justification 必须基于对所学习概念预期使用的一些假定相似性。

尽管这篇论文是1980年的，但其中的逻辑确实让我们思考如何区分模型中必要和不必要的偏见。我们当然必须消除会成为障碍的偏见，但随着我们的实践和深入挖掘数据，我们可能会意识到一些偏见实际上可以改善我们的模型，以符合我们的实际需求。

在后续文章中，我将讨论在机器学习中应该避免的偏见类型。

**相关**：

+   [大数据的3个主要问题及解决方案](/2019/04/3-big-problems-big-data.html)

+   [设计伦理算法](/2019/03/designing-ethical-algorithms.html)

+   [算法并不偏见，我们才偏见](/2019/01/algorithms-arent-biased-we-are.html)

### 更多相关话题

+   [数据质量：好、坏与丑](https://www.kdnuggets.com/2022/01/data-quality-good-bad-ugly.html)

+   [揭示糟糕科学](https://www.kdnuggets.com/2022/01/demystifying-bad-science.html)

+   [3分钟了解偏差-方差权衡](https://www.kdnuggets.com/2020/09/understanding-bias-variance-trade-off-3-minutes.html)

+   [应对推荐系统和搜索中的位置偏差](https://www.kdnuggets.com/2023/03/dealing-position-bias-recommendations-search.html)

+   [偏差-方差权衡](https://www.kdnuggets.com/2022/08/biasvariance-tradeoff.html)

+   [免费数据科学学习路线图：适用于所有级别，IBM提供](https://www.kdnuggets.com/a-free-data-science-learning-roadmap-for-all-levels-with-ibm)
