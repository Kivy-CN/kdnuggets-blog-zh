- en: Four Basic Steps in Data Preparation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备中的四个基本步骤
- en: 原文：[https://www.kdnuggets.com/2021/10/four-basic-steps-data-preparation.html](https://www.kdnuggets.com/2021/10/four-basic-steps-data-preparation.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/10/four-basic-steps-data-preparation.html](https://www.kdnuggets.com/2021/10/four-basic-steps-data-preparation.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Rosaria Silipo](https://www.linkedin.com/in/rosaria/?originalSubdomain=ch),
    Head of Data Science Evangelism at KNIME**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由[Rosaria Silipo](https://www.linkedin.com/in/rosaria/?originalSubdomain=ch)撰写，KNIME数据科学布道主管**'
- en: What are the steps in data preparation? Are there specific steps we need to
    take for specific problems? The answer is not that straightforward: Practice and
    knowledge will design the best recipe for each case.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备的步骤是什么？我们是否需要针对特定问题采取具体步骤？答案并不那么简单：实践和知识将为每种情况设计最佳方案。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'First, there are two types of data preparation: KPI calculation to extract
    the information from the raw data and data preparation for the data science algorithm.
    While the first one is domain and business dependent, the second one is more standardized.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，数据准备有两种类型：KPI计算用于从原始数据中提取信息，以及数据科学算法的数据准备。前者依赖于领域和业务，而后者则更为标准化。
- en: In this blog post, we focus on operations to prepare data to feed a machine
    learning algorithm. There are many of these data operations, some more general
    and some more dedicated to specific situations. When you consider that there are
    entire university courses that focus on data preparation operations, the topic
    is too broad to cover them all in a short article.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我们专注于准备数据以供机器学习算法使用的操作。这些数据操作有很多，其中一些更为通用，而另一些则更针对特定情况。当你考虑到有整门大学课程专注于数据准备操作时，这个话题太广泛，无法在一篇简短的文章中涵盖所有内容。
- en: What we would like to do here is introduce four very basic and very general
    steps in data preparation for machine learning algorithms. We will describe how
    and why to apply such transformations within a specific example.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望在这里介绍四个非常基础且非常通用的机器学习算法数据准备步骤。我们将描述在特定示例中如何及为什么应用这些转换。
- en: Normalization
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 归一化
- en: Conversion
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换
- en: Missing value imputation
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缺失值插补
- en: Resampling
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重采样
- en: 'Our Example: Churn Prediction'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的例子：流失预测
- en: 'Let’s take a simple example in data science: churn prediction. Briefly explained,
    churn prediction aims at distinguishing between those customers at risk of churning
    and the other customers. Usually, data from the company’s CRM system are used,
    including features for customers behavior, demographics, and revenue. Some customers
    churn, some do not. These data are then used to train a predictive model to distinguish
    between the two classes of customers.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以数据科学中的一个简单例子来说明：流失预测。简单来说，流失预测旨在区分那些有流失风险的客户和其他客户。通常，使用公司CRM系统中的数据，包括客户行为、人口统计信息和收入特征。有些客户流失，有些则没有。这些数据然后用于训练一个预测模型，以区分这两类客户。
- en: 'The problem is a binary classification problem. The output class (churn) has
    only two possible values: churn “yes” and churn “no”. Any classification algorithm
    would work here: decision tree, random forest, logistic regression, and so on.
    Logistic regression is somewhat the historical algorithm, fast to run and easy
    to interpret. We will adopt it to solve this problem.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题是一个二分类问题。输出类别（流失）只有两个可能的值：流失“是”和流失“否”。任何分类算法都适用：决策树、随机森林、逻辑回归等。逻辑回归有点像历史算法，运行快速且易于解释。我们将采用它来解决这个问题。
- en: Let’s start to build our workflow. First, we read the data from two separate
    files, a CSV file and an Excel file, then we apply the logistic regression, and
    finally we write the model to a file.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建我们的工作流程。首先，我们从两个独立的文件中读取数据，一个CSV文件和一个Excel文件，然后应用逻辑回归，最后将模型写入文件。
- en: But hold your horses, Nelly! Let’s have a look at the data first.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 但别急，Nelly！让我们先查看数据。
- en: '[![The Four Basic Steps in Data Preparation](../Images/0772a826459e2f6793bb00e922d3b7fb.png)](https://www.knime.com/sites/default/files/four-steps-data-preparation.png)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[![数据准备的四个基本步骤](../Images/0772a826459e2f6793bb00e922d3b7fb.png)](https://www.knime.com/sites/default/files/four-steps-data-preparation.png)'
- en: Fig. 1\. Workflow to train and evaluate a logistic regression model, including
    four common data preparation steps (**click to enlarge**).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. 训练和评估逻辑回归模型的工作流程，包括四个常见的数据准备步骤（**点击放大**）。
- en: Partition the Data to Produce Two Subsets of the Data
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据分区以生成两个数据子集
- en: Training a model is not enough to claim that we have a good model. We also need
    to evaluate it by means of an accuracy or an error metric on a separate subset
    of data. Translated, we need to create a pair of non-overlapping subsets - training
    set and test set - randomly extracted from the original dataset. For that, we
    use the [Partitioning ](https://kni.me/n/T0AhxSe0Yi42rQK8)node. The Partitioning
    node randomly extracts data from the input data table, in the proportion specified
    in its configuration window, and produces the two data subsets at the two output
    ports.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个模型还不足以声称我们有一个好的模型。我们还需要通过在单独的数据子集上使用准确性或误差度量来评估它。换句话说，我们需要创建一对不重叠的子集——训练集和测试集——从原始数据集中随机抽取。为此，我们使用[分区](https://kni.me/n/T0AhxSe0Yi42rQK8)节点。分区节点从输入数据表中随机提取数据，按其配置窗口中指定的比例，生成两个数据子集并在两个输出端口输出。
- en: The training set will be used to train the model by the [Logistic Regression
    Learner](https://kni.me/n/czTL7CtWUSI10JzJ) node and the test set to score the
    model by the Logistic Regression Predictor node followed by a Scorer node. The
    Logistic Regression Predictor node generates the churn predictions, and the Scorer
    node evaluates how correct those predictions are.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集将由[逻辑回归学习器](https://kni.me/n/czTL7CtWUSI10JzJ)节点用于训练模型，测试集将由逻辑回归预测器节点和后续的评分节点用于评分模型。逻辑回归预测器节点生成客户流失预测，而评分节点评估这些预测的准确性。
- en: Here, we don't include the partitioning operation among the data preparation
    operations, because it doesn't really change the data. However, this is only our
    opinion. If you want to include partitioning among the data preparation operations,
    just change the title from “Four” to “Five basic steps in data preparation” :-)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们没有将分区操作包含在数据准备操作中，因为它并不会真正改变数据。然而，这只是我们的看法。如果你想将分区包含在数据准备操作中，只需将标题从“四”改为“数据准备的五个基本步骤”
    :-)
- en: 1\. Normalization
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 标准化
- en: Next, logistic regression needs the input data to be normalized into the interval
    [0, 1], even better if it is Gaussian normalized. Any algorithm including distances
    or variances will work on normalized data. This is because features with larger
    ranges affect the calculation of variances and distances and might end up dominating
    the whole algorithm. Since we want all input features to be considered equally,
    the normalization of the data is required.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，逻辑回归需要将输入数据标准化到区间[0, 1]，如果能够高斯标准化则更好。任何包括距离或方差的算法都将在标准化的数据上有效。这是因为具有较大范围的特征会影响方差和距离的计算，可能会主导整个算法。由于我们希望所有输入特征被平等考虑，因此数据的标准化是必要的。
- en: For example, the decision tree relies on probabilities and does not need normalized
    data, but logistic regression relies on variances and therefore requires previous
    normalization; many clustering algorithms, like k-Means, rely on distances and
    therefore require normalization; neural networks use activation functions where
    the argument falls in [0,1] and therefore also require normalization; and so on.
    You will learn to recognize which algorithms require normalization and why.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，决策树依赖于概率，不需要标准化数据，但逻辑回归依赖于方差，因此需要先前的标准化；许多聚类算法，如k-Means，依赖于距离，因此需要标准化；神经网络使用激活函数，其参数落在[0,1]之间，因此也需要标准化；等等。你将学会识别哪些算法需要标准化及其原因。
- en: Logistic regression does require Gaussian-normalized data. So, a [Normalizer ](https://kni.me/n/7isNz6mji9l5c7Iv)node
    must be introduced to normalize the training data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归确实需要高斯标准化的数据。因此，必须引入一个[Normalizer](https://kni.me/n/7isNz6mji9l5c7Iv)节点来标准化训练数据。
- en: 2\. Conversion
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 转换
- en: Logistic Regression works on numerical attributes. This is true for the original
    algorithm. Sometimes, in some packages, you can see that logistic regression also
    accepts categorical, i.e. nominal, input features. In this case, a preparation
    step has been implemented within the logistic regression learning function, to
    convert the categorical features into numbers. In general, logistic regression
    works only on numerical features.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归在数值属性上工作。这对于原始算法来说是正确的。有时，在一些包中，你会看到逻辑回归也接受分类的，即名义输入特征。在这种情况下，逻辑回归学习函数内部实现了一个准备步骤，将分类特征转换为数字。通常，逻辑回归只在数值特征上工作。
- en: 'Since we want to be in control and not blindly delegate the conversion operation
    to the logistic regression extended package, we can implement this conversion
    step on our own. Practically, we want to convert a nominal column into one or
    more numerical columns. There are two options here:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望掌控操作，而不是盲目地将转换操作委托给逻辑回归扩展包，因此我们可以自行实现这个转换步骤。实际上，我们希望将一个名义列转换为一个或多个数值列。这里有两个选项：
- en: Index Encoding. Each nominal value is mapped to a number.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 索引编码。每个名义值被映射为一个数字。
- en: One hot encoding. Each nominal value creates a new column, whose cells are filled
    with 1 or 0 depending on the presence or absence of that value in the original
    column.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独热编码。每个名义值创建一个新列，其单元格根据原始列中该值的存在与否填充1或0。
- en: The index encoding solution generates one numerical column from one nominal
    column. However, it introduces an artificial numerical distance between two values
    due to the mapping function.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 索引编码解决方案从一个名义列生成一个数值列。然而，由于映射函数，它在两个值之间引入了人为的数值距离。
- en: The one hot encoding solution does not introduce any artificial distance. However,
    it generates many columns from the one original column, therefore increasing the
    dimensionality of the dataset and artificially weighting the original column more.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码解决方案不会引入任何人为距离。然而，它从一个原始列生成许多列，因此增加了数据集的维度，并人为地加重了原始列的权重。
- en: There is no perfect solution. In the end it is a compromise between accuracy
    and speed.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 没有完美的解决方案。最终这是在准确性和速度之间的妥协。
- en: In our case, there are two categorical columns in the original dataset: *State* and *Phone*. *Phone* is
    a unique ID used to identify each customer. We will not use it in our analysis
    since it does not contain any general information about the customer behavior
    or contract. *State*, on the opposite, might contain relevant information. It
    is plausible that customers from a certain state might be more propense to churn
    due for example to a local competitor.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，原始数据集中有两个分类列：*State*和*Phone*。*Phone*是一个唯一的ID，用于识别每个客户。由于它不包含关于客户行为或合同的任何一般信息，我们在分析中不会使用它。*State*则相反，可能包含相关信息。某个州的客户可能由于当地竞争者的存在，更倾向于流失。
- en: To convert the input feature *State*, we implemented an index-based encoding
    using the [Category to Number](https://kni.me/n/WFiVV3UvOmAjUNSr) node.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了转换输入特征*State*，我们使用了基于索引的编码，通过[Category to Number](https://kni.me/n/WFiVV3UvOmAjUNSr)节点实现。
- en: On the opposite side of the spectrum, we have the target column *Churn*, containing
    the information on whether the customer has churned (1) or not (0). Many classifier
    training algorithms require a categorical target column for the class labels.
    In our case, column Churn was read initially as numerical (0/1) and must be converted
    to the categorical type with a [Number To String](https://kni.me/n/s--Phzu0nUXeg0E9) node.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在光谱的另一端，我们有目标列*Churn*，其中包含有关客户是否流失（1）或未流失（0）的信息。许多分类器训练算法要求类别标签的目标列为分类的。在我们的案例中，Churn列最初被读取为数值（0/1），必须转换为分类类型，通过[Number
    To String](https://kni.me/n/s--Phzu0nUXeg0E9)节点。
- en: 3\. Missing Value Imputation
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 缺失值填充
- en: Another drawback of logistic regression is that it cannot deal with missing
    values in the data. Some logistic regression learning functions include a missing
    value strategy. As we said earlier, however, we want to be in control of the whole
    process. Let’s also decide the strategy for missing value imputation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的另一个缺点是它不能处理数据中的缺失值。一些逻辑回归学习函数包括缺失值策略。然而，正如我们之前所说的，我们希望控制整个过程。让我们也决定缺失值填补的策略。
- en: 'There are many strategies in the literature for missing value imputation. You
    can check the details in the article “[Missing Value Imputation: A Review](/2020/09/missing-value-imputation-review.html)”.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中有很多缺失值填补的策略。你可以在文章“[缺失值填补：综述](/2020/09/missing-value-imputation-review.html)”中查看详细信息。
- en: The crudest strategy just ignores the data rows with missing values. It is crude,
    but if we have data to spare, is not wrong. It becomes problematic when we have
    little data. In this case, to throw away some data just because some features
    are missing, might not be the smartest decision.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的策略就是忽略有缺失值的数据行。这是粗糙的，但如果我们有多余的数据，并不算错。当数据很少时，这就变得有问题了。在这种情况下，仅仅因为某些特征缺失而丢弃一些数据，可能不是最聪明的决定。
- en: If we had some knowledge of the domain, we would know what the missing values
    mean and we would be able to provide a reasonable substitute value.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对领域有一些了解，我们就会知道缺失值的含义，并且能够提供一个合理的替代值。
- en: Since we are clueless and our dataset is really not that big, we need to find
    some alternative. If we do not want to think, then the adoption of the median,
    the mean, or the most frequent value is a reasonable choice. If we know nothing,
    we go with the majority or the middle value. This is the strategy that we have
    adopted here.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们一无所知，数据集也确实不大，我们需要找到一些替代方案。如果我们不想思考，那么采用中位数、均值或最频繁值是一个合理的选择。如果我们一无所知，就选择多数值或中间值。这是我们在这里采用的策略。
- en: However, if we want to think just a little, we might want to run a little statistic
    on the dataset, via the [Data Explorer](https://kni.me/n/F0ZMkNAGudnmoAyE) node
    for example, we could estimate how serious the missing value problem is, if at
    all. As a second step, we could isolate those rows with missing values in the
    most affected columns. From such observations, an idea might come for a reasonable
    replacement value. We did that, of course after implementing the missing value
    strategy in the Missing Value node. And indeed, the view of Data Explorer node
    showed that our dataset has no missing values. None whatsoever. Oh well! Let’s
    leave the Missing Value node in there for completion.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们想稍微思考一下，我们可能想通过 [数据探索器](https://kni.me/n/F0ZMkNAGudnmoAyE) 节点对数据集进行一些统计分析，以估计缺失值问题的严重性。如果缺失值问题严重，我们可以在受影响最严重的列中隔离那些有缺失值的行。从这些观察中，可能会得到一个合理的替代值的想法。当然，在实施缺失值策略后，我们这样做了。事实上，数据探索器节点的视图显示我们的数据集没有缺失值。完全没有。哦，好吧！让我们把缺失值节点留在那里以完成工作。
- en: 4\. Resampling
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 重采样
- en: 'Now the last of the basic questions: are the target classes equally distributed
    across the dataset?'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在最后一个基本问题：目标类别在数据集中是否均匀分布？
- en: Well in our case, they are not. There is an imbalance between the two classes
    of 85% (not churning) vs. 15% (churning). When one class is much less numerous
    than the other, there is the risk that is going to be overlooked by the training
    algorithm. If the imbalance is not that strong, then using a stratified sampling
    strategy for the random extraction of data in the Partitioning node should be
    enough. However, if the imbalance is stronger, like in this case, it might be
    useful to perform some resampling before feeding the training algorithm.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，它们并不是这样。两个类别之间存在85%（未流失）对15%（流失）的不平衡。当一个类别比另一个类别少得多时，训练算法可能会忽视这个类别。如果不平衡不是很严重，那么在分区节点中使用分层抽样策略进行随机数据抽取应该足够了。然而，如果不平衡较强，如本例所示，则可能需要在训练算法之前进行一些重采样。
- en: To resample, you can either undersample the majority class or oversample the
    minority class, depending on how much data you are dealing with and if you can
    afford the luxury of throwing away data from the majority class. This dataset
    is not that big, so we decided to go for an oversampling of the minority class
    via the SMOTE algorithm.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行重采样，你可以选择对多数类别进行欠采样，或者对少数类别进行过采样，这取决于你处理的数据量以及是否可以承担丢弃多数类别数据的奢侈。这种数据集不大，因此我们决定使用SMOTE算法对少数类别进行过采样。
- en: Prevent Data Leakage
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 防止数据泄漏
- en: Now, we have to repeat all these transformations for the test set as well, the
    same exact transformations as defined in the training branch of the workflow.
    In order to prevent data leakage, we cannot involve test data in the making of
    any of those transformations. Therefore, in the test branch of the workflow, we
    used (Apply) nodes to purely apply the transformations to the test data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须对测试集重复所有这些变换，使用与工作流的训练分支中定义的完全相同的变换。为了防止数据泄漏，我们不能将测试数据用于任何这些变换的制作。因此，在工作流的测试分支中，我们使用了（应用）节点，仅仅将变换应用于测试数据。
- en: Notice that there is no SMOTE (Apply) node. This is because we either reapply
    the SMOTE algorithm to oversample the minority class in the test set or we adopt
    an evaluation metric that takes into account the class imbalance, like the Cohen’s
    kappa. We chose to evaluate the model quality with the Cohen’s kappa metric.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，没有 SMOTE（应用）节点。这是因为我们要么重新应用 SMOTE 算法来对测试集中的少数类进行过采样，要么采用考虑类别不平衡的评估指标，如 Cohen’s
    Kappa。我们选择用 Cohen’s Kappa 指标来评估模型质量。
- en: Packaging and Conclusions
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 打包和结论
- en: We have reached the end of our workflow (you can download the workflow [Four
    Basic Steps in Data Preparation before Training a Churn Predictor](https://kni.me/w/vCqYslFYL15AILPF) from
    the KNIME Hub). We have prepared the training data, trained a predictive algorithm
    on them, prepared the test data using the exact same transformations, applied
    the trained model on them, and scored the predictions against the real classes.
    The [Scorer ](https://kni.me/n/5tsiYT6vDKM7YChN)node produces a number of accuracy
    metrics to evaluate the quality of the model. We chose the Cohen’s Kappa, since
    it measures the algorithm performances on both classes, even if they are highly
    imbalanced.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了工作流的最后一步（你可以从 KNIME Hub 下载工作流 [数据准备的四个基本步骤](https://kni.me/w/vCqYslFYL15AILPF)）。我们已经准备了训练数据，对其进行了预测算法训练，使用完全相同的变换准备了测试数据，将训练好的模型应用于测试数据，并将预测结果与实际类别进行评分。[评分器](https://kni.me/n/5tsiYT6vDKM7YChN)
    节点生成了多个准确度指标来评估模型的质量。我们选择了 Cohen’s Kappa，因为它可以衡量算法在两个类别上的表现，即使它们高度不平衡。
- en: We got a Cohen’s Kappa of almost 0.4\. Is that the best we can do? We should
    try to go back, tweak some parameters, and see if we can get a better model. Instead
    of tweaking the parameters manually, we might even introduce an optimization cycle.
    Check out the free ebook, [From Modeling to Model Evaluation](https://www.knime.com/knimepress/from-modeling-to-model-evaluation),
    for more scoring techniques that show a model's performance in a broader context.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了接近 0.4 的 Cohen’s Kappa。这是我们能做的最好结果吗？我们应该尝试回到之前的步骤，调整一些参数，看看是否能得到更好的模型。与其手动调整参数，我们甚至可以引入优化周期。查看免费的电子书
    [从建模到模型评估](https://www.knime.com/knimepress/from-modeling-to-model-evaluation)，了解更多展示模型性能的评分技术。
- en: We conclude this article now, having introduced four very basic data transformations.
    We invite you to deepen your knowledge on these four and to investigate other
    data transformations, such as dimensionality reduction, feature selection, feature
    engineering, outlier detection, PCA, to name just a few.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在结束这篇文章，介绍了四种非常基础的数据变换。我们邀请你深入了解这四种变换，并探索其他数据变换，例如降维、特征选择、特征工程、异常值检测、PCA
    等。
- en: '**Bio: [Rosaria Silipo](https://www.linkedin.com/in/rosaria/?originalSubdomain=ch)**
    is not only an expert in data mining, machine learning, reporting, and data warehousing,
    she has become a recognized expert on the KNIME data mining engine, about which
    she has published three books: KNIME Beginner’s Luck, The KNIME Cookbook, and
    The KNIME Booklet for SAS Users. Previously Rosaria worked as a freelance data
    analyst for many companies throughout Europe. She has also led the SAS development
    group at Viseca (Zürich), implemented the speech-to-text and text-to-speech interfaces
    in C# at Spoken Translation (Berkeley, California), and developed a number of
    speech recognition engines in different languages at Nuance Communications (Menlo
    Park, California). Rosaria gained her doctorate in biomedical engineering in 1996
    from the University of Florence, Italy.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介： [Rosaria Silipo](https://www.linkedin.com/in/rosaria/?originalSubdomain=ch)**
    不仅是数据挖掘、机器学习、报告和数据仓库领域的专家，她还成为了 KNIME 数据挖掘引擎的公认专家，关于这一点她已出版了三本书：《KNIME Beginner’s
    Luck》、《The KNIME Cookbook》和《The KNIME Booklet for SAS Users》。之前，Rosaria 曾为欧洲多家公司担任自由数据分析师。她还曾在
    Viseca（苏黎世）领导 SAS 开发组，在 Spoken Translation（加州伯克利）使用 C# 实现语音转文本和文本转语音接口，并在 Nuance
    Communications（加州门罗公园）开发了多种语言的语音识别引擎。Rosaria 于 1996 年获得意大利佛罗伦萨大学的生物医学工程博士学位。'
- en: As first published on the [KNIME Blog](https://www.knime.com/blog).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首次发布于 [KNIME 博客](https://www.knime.com/blog)
- en: '[Original](https://www.knime.com/blog/four-basic-steps-in-data-preparation).
    Reposted with permission.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://www.knime.com/blog/four-basic-steps-in-data-preparation). 经许可转载。'
- en: '**Related:**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Data Preparation in R using dplyr, with Cheat Sheet!](/2021/10/data-preparation-r-dplyr-cheat-sheet.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 dplyr 在 R 中进行数据准备，附带备忘单！](/2021/10/data-preparation-r-dplyr-cheat-sheet.html)'
- en: '[Messy Data is Beautiful](/2021/09/sparkbeyond-messy-data-is-beautiful.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[混乱的数据是美丽的](/2021/09/sparkbeyond-messy-data-is-beautiful.html)'
- en: '[Automated Data Labeling with Machine Learning](/2021/08/watchful-automated-data-labeling-machine-learning.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基于机器学习的自动数据标注](/2021/08/watchful-automated-data-labeling-machine-learning.html)'
- en: More On This Topic
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目的，并通过寻找目的来…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学的顶级统计资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功数据科学家的 5 个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每位数据科学家都应了解的三个 R 语言库（即使你使用 Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一个 90 亿美元的 AI 失败案例，解析](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[是什么让 Python 成为初创公司的理想编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
