# 数据科学家使用 PyTorch 的最完整指南

> 原文：[https://www.kdnuggets.com/2020/09/most-complete-guide-pytorch-data-scientists.html](https://www.kdnuggets.com/2020/09/most-complete-guide-pytorch-data-scientists.html)

[comments](#comments)![Header](../Images/43c438dc46265ffdd4bc06ff3d0f68a3.png)

***PyTorch*** 现在已成为创建神经网络的事实标准之一，我喜欢它的接口。然而，对初学者来说，它还是有些难以掌握。

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业的快车道。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织 IT

* * *

我记得在几年前经过一些广泛的实验后才开始使用 PyTorch。说实话，我花了很多时间才掌握它，但我很高兴我从 [Keras 迁移到 PyTorch](https://towardsdatascience.com/moving-from-keras-to-pytorch-f0d4fff4ce79)。*凭借其高度自定义和 Python 风格的语法，*PyTorch 的确是一种愉悦的工作工具，我会推荐给任何想要进行深度学习重载的人。

因此，在本 PyTorch 指南中，***我会尝试减轻初学者在使用 PyTorch 时的痛苦***，并讲解创建任何神经网络所需的一些最重要的类和模块。

不过，这并不意味着本教程仅面向初学者，因为***我还会讨论*** ***PyTorch 提供的高度自定义功能，并讲解自定义层、数据集、数据加载器和损失函数***。

所以，让我们喝杯咖啡 ☕️ 开始吧。

### Tensors

[Tensors](https://www.kdnuggets.com/2018/05/wtf-tensor.html) 是 PyTorch 中的基本构建块，简单来说，它们是 GPU 上的 NumPy 数组。在这一部分，我将列出一些在处理 Tensors 时最常用的操作。这绝不是你可以对 Tensors 执行操作的详尽列表，但在深入更有趣的部分之前，了解 tensors 是很有帮助的。

### 1\. 创建 Tensor

我们可以通过多种方式创建 PyTorch tensor，其中包括从 NumPy 数组转换。以下只是一些入门示例，但你可以做很多其他事情，[更多内容](https://pytorch.org/docs/stable/tensors.html) 就像你可以用 NumPy 数组做的那样。

![Image for post](../Images/be8c3218e6ad5ab93ddbc41fa18b206d.png)

### 2\. Tensor 操作

同样，你可以对这些张量进行很多操作。所有函数的完整列表可以在 [这里](https://pytorch.org/docs/stable/torch.html?highlight=mm#math-operations)找到。

![Image for post](../Images/ba8d46e45d52acf8c3009404823db6fd.png)

**注意：** PyTorch 变量是什么？在 PyTorch 的早期版本中，Tensor 和 Variables 是不同的并且提供了不同的功能，但现在 Variable API 已经 [弃用](https://pytorch.org/docs/stable/autograd.html#variable-deprecated)，所有变量的方法都适用于 Tensors。所以，如果你不了解它们，也没关系，因为它们已经不需要了，如果你知道它们，你可以忘记它们。

### `nn.Module`

![Figure](../Images/cbff37d276c1915bc74c150f60bef3d3.png)照片由 [Fernand De Canne](https://unsplash.com/@fernanddecanne?utm_source=medium&utm_medium=referral) 提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

现在到了有趣的部分，我们将讨论在创建深度学习项目时 PyTorch 中一些最常用的构造。`nn.Module` 允许你将深度学习模型作为类创建。你可以从 `nn.Module` 继承来将任何模型定义为类。每个模型类必然包含一个 `__init__` 过程块和一个 `forward` 过程块。

+   在 `__init__` 部分，用户可以定义网络将拥有的所有层，但尚未定义这些层将如何相互连接。

+   在 `forward` 过程块中，用户定义数据如何在网络的各层之间流动。

简而言之，我们定义的任何网络都将如下所示：

在这里，我们定义了一个非常简单的网络，它接收大小为 784 的输入，并将其依次通过两个线性层。但需要注意的是，在定义前向传递时，我们可以定义任何计算，这使得 PyTorch 在研究中高度可定制。例如，在我们的疯狂实验模式中，我们可能使用了下面的网络，其中我们任意地附加了我们的层。在这里，我们将第二个线性层的输出再次发送回第一个层，并将输入加到它上面（跳跃连接）（老实说，我不知道这会有什么效果）。

我们还可以检查神经网络前向传递是否有效。我通常通过首先创建一些随机输入，然后将其通过我创建的网络来检查。

[PRE0]

### 关于层的说明

PyTorch 功能强大，你实际上可以使用 `nn.Module` 自行创建任何新的实验层。例如，我们本可以创建我们自己的 **自定义线性层**，而不是使用上述的 PyTorch 预定义线性层 `nn.Linear`。

你可以看到我们是如何将权重张量包装在`nn.Parameter.`中的。这是为了让张量被视为模型参数。来自 PyTorch 的 [文档](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter):

> 参数是 `[*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)` 子类，这些子类在与 `*Module*` 一起使用时具有一个非常特殊的属性——当它们被分配为模块属性时，它们会自动添加到参数列表中，并会出现在 `*parameters()*` 迭代器中。

如你稍后将看到的，`model.parameters()` 迭代器将作为优化器的输入。但更多细节稍后再谈。

现在，我们可以像使用其他层一样在任何 PyTorch 网络中使用这个自定义层。

不过，如果 PyTorch 没有提供大量在各种神经网络架构中经常使用的现成层，它就不会被如此广泛使用。一些示例包括：[nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)、[nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)、[nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d)、[nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)、`[nn.BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d)`、[nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout)、[nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding)、`[nn.GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU)/[nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM)`、[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax)、`[nn.LogSoftmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax)`、[nn.MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention)、[nn.TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder)、[nn.TransformerDecoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder)。

我已将所有层链接到其来源，你可以在这些来源中阅读所有相关信息，但为了展示我通常如何理解一层并阅读文档，我将尝试查看一个非常简单的卷积层。

![Image for post](../Images/c6dbe2d59cfe06a6d60c85ce6574c285.png)

因此，`Conv2d` 层需要一个高度为 H 和宽度为 W 的图像作为输入，并具有`Cin`通道。对于 convnet 的第一层，`in_channels` 的数量通常是 3（RGB），`out_channels` 的数量可以由用户定义。最常用的 `kernel_size` 是 3x3，而通常使用的 `stride` 是 1。

要检查我不太了解的新层，我通常会尝试查看该层的输入和输出，如下所示，我会首先初始化该层：

[PRE1]

然后通过它传递一些随机输入。在这里，100是批量大小。

[PRE2]

所以，我们根据需要从卷积操作中获得输出，并且我有足够的信息来了解如何在我设计的任何神经网络中使用这个层。

### 数据集和DataLoader

我们在训练或测试时如何将数据传递给我们的神经网络？我们可以像上面那样传递张量，但Pytorch还提供了预构建的数据集，方便我们将数据传递给神经网络。你可以查看[torchvision.datasets](https://pytorch.org/docs/stable/torchvision/datasets.html)和[torchtext.datasets](https://pytorch.org/text/datasets.html)提供的完整数据集列表。但为了给出一个具体的数据集例子，假设我们需要通过一个包含以下结构的文件夹将图像传递给图像神经网络：

[PRE3]

我们可以使用`torchvision.datasets.ImageFolder`数据集来获取如下例图片：

![文章图片](../Images/8f36e3e7f1066b9c820066cbac26ddbf.png)

这个数据集有847张图片，我们可以使用索引来获取图片及其标签。现在我们可以使用for循环将图片逐一传递给任何图像神经网络：

[PRE4]

***但这不是最优的。我们想要进行批量处理。***我们实际上可以编写更多代码，将图片和标签附加到一个批次中，然后将其传递给神经网络。但Pytorch为我们提供了一个实用的迭代器`torch.utils.data.DataLoader`来准确完成这个任务。现在我们可以简单地将我们的`train_dataset`包装在Dataloader中，这样我们将获得批次，而不是单独的示例。

[PRE5]

我们可以使用批量迭代：

[PRE6]

所以实际上，使用数据集和DataLoader的整个过程变成了：

你可以在我之前的博客文章中查看这个特定的例子，关于使用深度学习的图像分类[在这里](https://towardsdatascience.com/end-to-end-pipeline-for-setting-up-multiclass-image-classification-for-data-scientists-2e051081d41c)。

这非常好，Pytorch确实提供了许多开箱即用的功能。但Pytorch的主要优势在于其巨大的自定义能力。如果Pytorch提供的数据集不适合我们的用例，我们还可以创建自己的自定义数据集。

### 理解自定义数据集

要编写自定义数据集，我们可以利用Pytorch提供的抽象类`torch.utils.data.Dataset`。我们需要继承这个`Dataset`类，并定义两个方法来创建自定义数据集。

+   `__len__`：一个函数，返回数据集的大小。在大多数情况下，这个函数编写起来相当简单。

+   `__getitem__`：一个函数，接收一个索引`i`作为输入，返回索引`i`处的样本。

例如，我们可以创建一个简单的自定义数据集，从文件夹中返回图像和标签。请注意，大多数任务都发生在`__init__`部分，在这里我们使用`glob.glob`来获取图像名称并进行一些通用预处理。

另外，请注意，我们在`__getitem__`方法中逐个打开图像，而不是在初始化时。这么做是因为我们不想将所有图像加载到内存中，只需加载需要的图像即可。

我们现在可以像之前一样使用这个数据集与`Dataloader`。它的工作方式和PyTorch之前提供的数据集一样，只是没有一些实用函数。

### 理解自定义 DataLoaders

**这一部分稍微复杂一些，通常情况下可以跳过，因为在很多情况下并不需要。** 但我在这里添加是为了完整性。

比如说你希望为处理文本输入的网络提供批次，该网络可以处理任何序列长度，只要批次中的大小保持一致。例如，我们可以有一个BiLSTM网络，它可以处理任何长度的序列。如果你现在不理解其中使用的层也没关系；只需知道它可以处理变长序列即可。

这个网络期望输入形状为(`batch_size`, `seq_length`)，并且可以处理任何`seq_length`。我们可以通过将模型传入两个不同序列长度（10和25）的随机批次来检查这一点。

[PRE7]

现在，我们希望为模型提供紧凑的批次，使得每个批次的序列长度相同，基于批次中的最大序列长度来最小化填充。这还有一个附加好处，就是让神经网络运行得更快。实际上，这是Kaggle Quora Insincere挑战赛获胜提交中使用的方法之一，当时运行时间至关重要。

那么，我们如何做到这一点呢？我们先编写一个非常简单的自定义数据集类。

另外，我们来生成一些随机数据，用于这个自定义数据集。

![Figure](../Images/09c6e3ad648b1c216c3dffeec8a2eb70.png)一个随机序列和标签的示例。序列中的每个整数对应句子中的一个词。

我们现在可以使用自定义数据集：

[PRE8]

如果我们尝试在这个数据集上使用`batch_size`>1的Dataloader，我们将得到一个错误。为什么会这样？

[PRE9]

![Image for post](../Images/f16ed32a2e9e8eb50c11b791e59b59ff.png)

这是因为序列的长度不同，而我们的数据加载器期望序列长度相同。记住，在前面的图像示例中，我们使用变换将所有图像调整为224的大小，因此我们没有遇到这个错误。

***那么，我们如何遍历这个数据集，使得每个批次的序列长度相同，但不同批次可能有不同的序列长度？***

我们可以在DataLoader中使用`collate_fn`参数，这让我们可以定义如何在特定批次中堆叠序列。要使用它，我们需要定义一个函数，该函数以批次作为输入，并根据批次中的`max_sequence_length`返回(`x_batch`，`y_batch`)及其填充的序列长度。我在下面的函数中使用的函数是简单的NumPy操作。此外，函数已适当注释，以便你理解发生了什么。

我们现在可以将这个`collate_fn`与我们的Dataloader一起使用，如下所示：

[PRE10]

![图](../Images/071eb46c3a18b3d3e7c289a3cb5d1471.png)现在可以看到批次具有不同的序列长度

这次会有效，因为我们提供了自定义的`collate_fn`。现在可以看到批次具有不同的序列长度。因此，我们将能够使用可变输入大小来训练我们的BiLSTM，就像我们想要的那样。

### 训练一个神经网络

我们知道如何使用`nn.Module`创建神经网络。但是如何训练它呢？任何需要训练的神经网络都会有一个训练循环，其形式大致如下：

在上述代码中，我们运行了五个训练周期，并且在每个周期中：

1.  我们使用数据加载器遍历数据集。

1.  在每次迭代中，我们使用`model(x_batch)`进行前向传播。

1.  我们使用`loss_criterion`计算损失。

1.  我们使用`loss.backward()`调用反向传播损失。我们无需担心梯度计算，因为这个简单的调用为我们完成了所有工作。

1.  通过`optimizer.step()`进行优化器步进，以更改整个网络中的权重。这是网络权重根据`loss.backward()`调用计算的梯度进行修改的地方。

1.  我们通过验证数据加载器检查验证得分/指标。在进行验证之前，我们使用`model.eval()`将模型设置为评估模式。请注意，在评估模式下，我们不会进行损失反向传播。

到目前为止，我们讨论了如何使用`nn.Module`创建网络以及如何在Pytorch中使用自定义数据集和数据加载器。接下来我们来谈谈损失函数和优化器的各种选择。

### 损失函数

Pytorch为我们提供了各种[损失函数](https://pytorch.org/docs/stable/nn.html#loss-functions)，适用于最常见的任务，比如分类和回归。一些常用的例子有`[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)`、`[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)`、`[nn.KLDivLoss](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss)`和`[nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)`。你可以阅读每个损失函数的文档，但为了解释如何使用这些损失函数，我将以`[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)`为例。

![Image for post](../Images/cfb45e3ebca92aab225660e6ddccde92.png)

NLLLoss 的文档非常简洁。即，这个损失函数用于多类别分类，文档中说明：

+   期望的输入大小应为 (`batch_size` x `Num_Classes`)——这些是我们创建的神经网络的预测结果。

+   我们需要在输入中获得每个类别的对数概率——为了从神经网络中获取对数概率，我们可以在网络的最后一层添加一个 `LogSoftmax` 层。

+   目标需要是一个包含类编号的张量，类编号范围在 (0, C-1) 之间，其中 C 是类别数。

所以，我们可以尝试将这个损失函数用于一个简单的分类网络。请注意，在最后一个线性层后面的 `LogSoftmax` 层。如果你不想使用这个 `LogSoftmax` 层，你可以直接使用 `[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)`。

让我们定义一个随机输入以传递给网络进行测试：

[PRE11]

并通过模型进行预测：

[PRE12]

我们现在可以得到损失值：

[PRE13]

### 自定义损失函数

自定义损失函数的定义也很简单，只要在损失函数中使用张量操作就可以了。例如，这里是 `customMseLoss`。

[PRE14]

你可以像以前一样使用这个自定义损失函数。但请注意，这次我们没有使用 criterion 实例化损失函数，因为我们已将其定义为函数。

[PRE15]

如果我们愿意，也可以将其写成一个类，使用 `nn.Module`，然后可以将其作为对象使用。这里是一个 NLLLoss 自定义示例：

### 优化器

一旦我们使用 `loss.backward()` 调用获得梯度，我们需要进行优化器步骤来改变整个网络中的权重。Pytorch 提供了多种现成的优化器，使用 `torch.optim` 模块。例如: `[torch.optim.Adadelta](https://pytorch.org/docs/stable/optim.html#torch.optim.Adadelta)`、`[torch.optim.Adagrad](https://pytorch.org/docs/stable/optim.html#torch.optim.Adagrad)`、`[torch.optim.RMSprop](https://pytorch.org/docs/stable/optim.html#torch.optim.RMSprop)` 和最广泛使用的 `[torch.optim.Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam)`。要使用 Pytorch 中最常用的 Adam 优化器，我们可以简单地实例化它：

[PRE16]

然后在训练模型时使用 `optimizer**.**zero_grad()` 和 `optimizer.step()`。

我不会讨论如何编写自定义优化器，因为这是一个不常见的用例，但如果你想要更多的优化器，请查看 [pytorch-optimizer](https://pytorch-optimizer.readthedocs.io/en/latest/) 库，它提供了许多在研究论文中使用的其他优化器。此外，如果你想要创建自己的优化器，可以参考 [PyTorch](https://github.com/pytorch/pytorch/tree/master/torch/optim) 或 [pytorch-optimizers](https://github.com/jettify/pytorch-optimizer/tree/master/torch_optimizer) 中已实现优化器的源代码来获取灵感。

![图](../Images/8fa38ecb11772198ed87d804bc7c312e.png)来自 [pytorch-optimizer](https://github.com/jettify/pytorch-optimizer) 库的其他优化器

### 使用 GPU/多个 GPU

到目前为止，我们所做的都是在 CPU 上进行的。如果你想使用 GPU，你可以通过 `model.to('cuda')` 将模型转移到 GPU 上。或者，如果你想使用多个 GPU，你可以使用 `nn.DataParallel`。这里有一个实用函数，可以检查机器中的 GPU 数量，并在需要时使用 `DataParallel` 自动设置并行训练。

我们唯一需要改变的就是在训练时如果有 GPU，就将数据加载到 GPU 上。这只需在训练循环中添加几行代码即可完成。

### 结论

Pytorch 提供了大量的自定义选项，且代码量最小。虽然一开始，理解整个生态系统如何通过类来构建可能比较困难，但最终，它还是简单的 Python。在这篇文章中，我尝试分解你在使用 Pytorch 时可能需要的大部分内容，希望阅读后你能更好地理解这些内容。

你可以在我的 [GitHub](https://github.com/MLWhiz/data_science_blogs/tree/master/pytorch_guide) 仓库中找到这篇文章的代码，我在那里保存了所有博客的代码。

如果你想通过课程结构学习更多关于 Pytorch 的内容，可以查看 IBM 在 Coursera 上提供的 [深度神经网络与 PyTorch](https://www.coursera.org/learn/deep-neural-networks-with-pytorch?ranMID=40328&ranEAID=lVarvwc5BD0&ranSiteID=lVarvwc5BD0-Mh_whR0Q06RCh47zsaMVBQ&siteID=lVarvwc5BD0-Mh_whR0Q06RCh47zsaMVBQ&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0) 课程。另外，如果你想了解更多关于深度学习的内容，我推荐这个 [计算机视觉中的深度学习](https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0) 课程，它属于 [高级机器学习专修](https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0)。

感谢阅读。我将来也会写更多适合初学者的文章。关注我在 [**Medium**](https://medium.com/@rahul_agarwal?source=post_page---------------------------) 或订阅我的 [**博客**](http://eepurl.com/dbQnuX?source=post_page---------------------------) 以获取最新信息。正如往常一样，我欢迎反馈和建设性的批评，可以通过Twitter联系我 [**@mlwhiz**](https://twitter.com/MLWhiz?source=post_page---------------------------)。

另外，简单的免责声明——这篇文章中可能包含一些关联链接，分享知识从来没有坏处。

**个人简介：[Rahul Agarwal](https://www.linkedin.com/in/rahulagwl/)** 是WalmartLabs的高级统计分析师。关注他的Twitter [@mlwhiz](https://twitter.com/MLWhiz)。

[原文](https://towardsdatascience.com/minimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b)。已获授权转载。

**相关：**

+   [给数据科学家的6条建议](/2019/09/advice-data-scientists.html)

+   [特征提取的指南](/2019/06/hitchhikers-guide-feature-extraction.html)

+   [数据科学家必须知道的5种分类评估指标](/2019/10/5-classification-evaluation-metrics-every-data-scientist-must-know.html)

### 更多相关话题

+   [完全免费的PyTorch深度学习课程](https://www.kdnuggets.com/2022/10/complete-free-pytorch-course-deep-learning.html)

+   [如何建立数据科学赋能团队：完整指南](https://www.kdnuggets.com/2022/10/build-data-science-enablement-team-complete-guide.html)

+   [Python中的地理编码：完整指南](https://www.kdnuggets.com/2022/11/geocoding-python-complete-guide.html)

+   [决策树软件的完整指南](https://www.kdnuggets.com/2022/08/complete-guide-decision-tree-software.html)

+   [使用PyTorch进行迁移学习的实用指南](https://www.kdnuggets.com/2023/06/practical-guide-transfer-learning-pytorch.html)

+   [初学者的PyTorch指南](https://www.kdnuggets.com/a-beginners-guide-to-pytorch)
