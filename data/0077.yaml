- en: 'Fasten Your Seatbelt: Falcon 180B is Here!'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系好安全带：Falcon 180B 到来了！
- en: 原文：[https://www.kdnuggets.com/fasten-your-seatbelt-falcon-180b-is-here](https://www.kdnuggets.com/fasten-your-seatbelt-falcon-180b-is-here)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/fasten-your-seatbelt-falcon-180b-is-here](https://www.kdnuggets.com/fasten-your-seatbelt-falcon-180b-is-here)
- en: '![Fasten Your Seatbelt: Falcon 180B is Here!](../Images/e65a7e76e2f4c174d5c28fe9e2c34569.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![系好安全带：Falcon 180B 到来了！](../Images/e65a7e76e2f4c174d5c28fe9e2c34569.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者
- en: A few months ago, we learnt about [Falcon LLM](/2023/06/falcon-llm-new-king-llms.html),
    which was founded by the [Technology Innovation Institute](https://www.tii.ae/)
    (TII), a company part of the Abu Dhabi Government’s Advanced Technology Research
    Council. Fast forward a few months, they’ve just got even bigger and better -
    literally, so much bigger.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 几个月前，我们了解到 [Falcon LLM](/2023/06/falcon-llm-new-king-llms.html)，这是由 [技术创新研究所](https://www.tii.ae/)（TII）创立的，该公司是阿布扎比政府先进技术研究委员会的一部分。几个月后，他们变得更大、更强——字面上的意义，更大。
- en: 'Falcon 180B: All You Need to Know'
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Falcon 180B: 你需要了解的一切'
- en: Falcon 180B is the largest openly available language model, with 180 billion
    parameters. Yes, that’s right, you read correctly - 180 billion. It was trained
    on 3.5 trillion tokens using TII's RefinedWeb dataset. This represents the longest
    single-epoch pre-training for an open model.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon 180B 是最大且公开可用的语言模型，拥有 1800 亿个参数。没错，你没看错——1800 亿。它在 TII 的 RefinedWeb 数据集上进行了
    3.5 万亿标记的训练。这代表了对一个开放模型进行的最长单周期预训练。
- en: But it’s not just about the size of the model that we’re going to focus on here,
    it’s also about the power and potential behind it. Falcon 180B is creating new
    standards with Large language models (LLMs) when it comes to capabilities.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们这里关注的不仅仅是模型的规模，还包括其背后的力量和潜力。在大语言模型（LLMs）领域，Falcon 180B 正在创造新的标准。
- en: 'The models that are available:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 目前可用的模型：
- en: '[Falcon 180B](https://huggingface.co/tiiuae/falcon-180B)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Falcon 180B](https://huggingface.co/tiiuae/falcon-180B)'
- en: '[Falcon 180B Chat](https://huggingface.co/tiiuae/falcon-180B-chat)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Falcon 180B Chat](https://huggingface.co/tiiuae/falcon-180B-chat)'
- en: The Falcon-180B Base model is a causal decoder-only model. I would recommend
    using this model for further fine-tuning your own data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon-180B 基础模型是一个因果解码器模型。我建议使用该模型对自己的数据进行进一步的细化训练。
- en: The Falcon-180B-Chat model has similarities to the base version but goes in
    a bit deeper by fine-tuning using a mix of Ultrachat, Platypus, and Airoboros
    instruction (chat) datasets.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon-180B-Chat 模型与基础版本有相似之处，但通过使用 Ultrachat、Platypus 和 Airoboros 指令（对话）数据集进行细化调整，进一步提升了能力。
- en: Training
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: Falcon 180B scaled up for its predecessor Falcon 40B, with new capabilities
    such as multiquery attention for enhanced scalability. The model used 4096 GPUs
    on Amazon SageMaker and was trained on 3.5 trillion tokens. This is roughly around
    7,000,000 GPU hours. This means that Falcon 180B is 2.5x faster than LLMs such
    as Llama 2 and was trained on 4x more computing.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon 180B 在其前身 Falcon 40B 的基础上进行了扩展，新增了多查询注意力机制以提高可扩展性。该模型在 Amazon SageMaker
    上使用了 4096 个 GPU，并在 3.5 万亿个标记上进行了训练。这大约需要 7,000,000 GPU 小时。这意味着 Falcon 180B 比如
    Llama 2 等大语言模型快 2.5 倍，并且训练所用计算资源是其 4 倍。
- en: Wow, that’s a lot.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，真是太多了。
- en: Data
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据
- en: The dataset used for Falcon 180B was predominantly sourced (85%) from RefinedWeb,
    as well as being trained on a mix of curated data such as technical papers, conversations,
    and some elements of code.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon 180B 使用的数据集主要来自 RefinedWeb，占比达 85%，此外还在一系列精心策划的数据（如技术论文、对话和一些代码元素）上进行了训练。
- en: Benchmark
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准测试
- en: The part you all want to know - how is Falcon 180B doing amongst its competitors?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你们都想知道的部分——Falcon 180B 在竞争对手中表现如何？
- en: Falcon 180B is currently the best openly released LLM to date (September 2023).
    It has been shown to outperform Llama 2 70B and OpenAI’s GPT-3.5 on [MMLU](https://paperswithcode.com/dataset/mmlu).
    It typically sits somewhere between GPT 3.5 and GPT 4.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止（2023年9月），Falcon 180B 是公开发布的最佳大语言模型。它在 [MMLU](https://paperswithcode.com/dataset/mmlu)
    上的表现超过了 Llama 2 70B 和 OpenAI 的 GPT-3.5。它通常介于 GPT 3.5 和 GPT 4 之间。
- en: '![Fasten Your Seatbelt: Falcon 180B is Here!](../Images/2b578ab8e51a55f1336154eed1d9b9a5.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![系好安全带：Falcon 180B 到来了！](../Images/2b578ab8e51a55f1336154eed1d9b9a5.png)'
- en: Image by [HuggingFace Falcon 180B](https://huggingface.co/blog/falcon-180b)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [HuggingFace Falcon 180B](https://huggingface.co/blog/falcon-180b)
- en: Falcon 180B ranked 68.74 on the Hugging Face Leaderboard, making it the highest-scoring
    openly released pre-trained LLM where it surpassed Meta’s LLaMA 2 which was at
    67.35.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon 180B 在 Hugging Face 排行榜上排名 68.74，使其成为最高分的公开发布的预训练大语言模型，超过了 Meta 的 LLaMA
    2，后者的得分为 67.35。
- en: How to use Falcon 180B?
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用 Falcon 180B？
- en: For the developer and natural language processing (NLP) enthusiasts out there,
    Falcon 180B is available on the Hugging Face ecosystem, starting with Transformers
    version 4.33.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于开发者和自然语言处理（NLP）爱好者，Falcon 180B 在 Hugging Face 生态系统中可用，开始于 Transformers 版本
    4.33。
- en: 'However, as you can imagine due to the model’s size, you will need to take
    into consideration hardware requirements. To get a better understanding of the
    hardware requirements, HuggingFace ran tests needed to run the model for different
    use cases, as shown in the image below:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如你可以想象的，由于模型的体积，你需要考虑硬件要求。为了更好地了解硬件要求，HuggingFace 进行了不同使用案例所需的模型运行测试，如下图所示：
- en: '![Fasten Your Seatbelt: Falcon 180B is Here!](../Images/424f925c66da18a9be9a9153c7800098.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![系好安全带：Falcon 180B 来了！](../Images/424f925c66da18a9be9a9153c7800098.png)'
- en: Image by [HuggingFace Falcon 180B](https://huggingface.co/blog/falcon-180b)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [HuggingFace Falcon 180B](https://huggingface.co/blog/falcon-180b) 提供
- en: 'If you would like to give it a test and play around with it, you can try out
    Falcon 180B through the demo by clicking on this link: [Falcon 180B Demo](https://huggingface.co/spaces/tiiuae/falcon-180b-demo).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想测试一下并尝试一下，可以通过点击这个链接尝试 Falcon 180B 的演示：[Falcon 180B 演示](https://huggingface.co/spaces/tiiuae/falcon-180b-demo)。
- en: Falcon 180B vs ChatGPT
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Falcon 180B 与 ChatGPT
- en: The model has some serious hardware requirements which are not easily accessible
    to everybody. However, based on other people's findings on testing both Falcon
    180B against ChatGPT by asking them the same questions, ChatGPT took the win.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型有一些严苛的硬件要求，并不是所有人都能轻易获得。然而，根据其他人对 Falcon 180B 和 ChatGPT 的测试结果，ChatGPT 在回答相同问题时胜出。
- en: It performed well on code generation, however, it needs a boost on text extraction
    and summarization.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码生成方面表现良好，但在文本提取和总结方面仍需提升。
- en: Wrapping it up
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结一下
- en: If you’ve had a chance to play around with it, let us know what your findings
    were against other LLMs. Is Falcon 180B worth all the hype that’s around it as
    it is currently the largest publicly available model on the Hugging Face model
    hub?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有机会尝试过它，请告诉我们你与其他 LLMs 的比较结果。Falcon 180B 作为目前 Hugging Face 模型中心最大的公开模型，是否值得所有的宣传？
- en: Well, it seems to be as it has shown to be at the top of the charts for open-access
    models, and models like PaLM-2, a run for their money. We’ll find out sooner or
    later.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，看来它确实如此，因为它已经在开放访问模型的排行榜上名列前茅，并且对像 PaLM-2 这样的模型构成了威胁。我们迟早会知道结果。
- en: '[](https://www.linkedin.com/in/nisha-arya-ahmed/)****[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)****
    is a data scientist, freelance technical writer, and an editor and community manager
    for KDnuggets. She is particularly interested in providing data science career
    advice or tutorials and theory-based knowledge around data science. Nisha covers
    a wide range of topics and wishes to explore the different ways artificial intelligence
    can benefit the longevity of human life. A keen learner, Nisha seeks to broaden
    her tech knowledge and writing skills, while helping guide others.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.linkedin.com/in/nisha-arya-ahmed/)****[尼莎·阿雅](https://www.linkedin.com/in/nisha-arya-ahmed/)****
    是一位数据科学家、自由技术作家，以及 KDnuggets 的编辑和社区经理。她特别关注提供数据科学职业建议或教程及理论知识。尼莎涵盖了广泛的主题，并希望探索人工智能如何有助于人类寿命的延续。作为一个热衷学习者，尼莎寻求拓宽她的技术知识和写作技能，同时帮助指导他人。'
- en: More On This Topic
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Falcon LLM: The New King of Open-Source LLMs](https://www.kdnuggets.com/2023/06/falcon-llm-new-king-llms.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Falcon LLM：开源 LLM 的新王者](https://www.kdnuggets.com/2023/06/falcon-llm-new-king-llms.html)'
- en: '[Want to Use Your Data Skills to Solve Global Problems? Here’s What…](https://www.kdnuggets.com/2022/04/jhu-want-data-skills-solve-global-problems.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[想用你的数据技能解决全球问题？这里是…](https://www.kdnuggets.com/2022/04/jhu-want-data-skills-solve-global-problems.html)'
- en: '[I Used ChatGPT (Every Day) for 5 Months. Here Are Some Hidden Gems…](https://www.kdnuggets.com/2023/07/used-chatgpt-every-day-5-months-hidden-gems-change-life.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我每天使用 ChatGPT 5 个月。这里是一些隐藏的宝石…](https://www.kdnuggets.com/2023/07/used-chatgpt-every-day-5-months-hidden-gems-change-life.html)'
- en: '[Here Are the AI Tools I Use Along With My Skills to Make $10,000…](https://www.kdnuggets.com/2023/07/ai-tools-along-skills-make-10000-monthly-bs.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[这里是我使用的 AI 工具及我的技能来赚取 $10,000 的方法…](https://www.kdnuggets.com/2023/07/ai-tools-along-skills-make-10000-monthly-bs.html)'
- en: '[Unable to Land a Data Science Job? Here’s Why](https://www.kdnuggets.com/2022/01/unable-land-data-science-job.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[无法找到数据科学职位？这里是原因](https://www.kdnuggets.com/2022/01/unable-land-data-science-job.html)'
- en: '[Data Science is Overrated, Here’s Why](https://www.kdnuggets.com/2022/06/data-science-overrated.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学被高估了，原因如下](https://www.kdnuggets.com/2022/06/data-science-overrated.html)'
