- en: 'Understanding Supervised Learning: Theory and Overview'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解监督学习：理论与概述
- en: 原文：[https://www.kdnuggets.com/understanding-supervised-learning-theory-and-overview](https://www.kdnuggets.com/understanding-supervised-learning-theory-and-overview)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/understanding-supervised-learning-theory-and-overview](https://www.kdnuggets.com/understanding-supervised-learning-theory-and-overview)
- en: '![Understanding Supervised Learning: Theory and Overview](../Images/e8dc88ea3399cedf36133942f5d82b33.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![理解监督学习：理论与概述](../Images/e8dc88ea3399cedf36133942f5d82b33.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Supervised is a subcategory of machine learning in which the computer learns
    from the labeled dataset containing both the input as well as the correct output.
    It tries to find the mapping function that relates the input (x) to the output
    (y). You can think of it as teaching your younger brother or sister how to recognize
    different animals. You will show them some pictures (x) and tell them what each
    animal is called (y). After a certain time, they will learn the differences and
    will be able to recognize the new picture correctly. This is the basic intuition
    behind supervised learning. Before moving forward, let's take a deeper look at
    its workings.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习是机器学习的一个子类别，其中计算机从包含输入和正确输出的标记数据集中学习。它试图找出将输入（x）与输出（y）相关联的映射函数。你可以把它想象成教你的弟弟或妹妹如何识别不同的动物。你会给他们看一些图片（x），并告诉他们每种动物的名称（y）。经过一段时间，他们将学习到这些差异，并能够正确识别新的图片。这就是监督学习的基本直觉。在继续之前，让我们深入了解一下它的工作原理。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的 top 3 课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织在 IT 方面'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: How Does Supervised Learning Work?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习是如何工作的？
- en: '![Understanding Supervised Learning: Theory and Overview](../Images/2a4574ae6e1142a8697461a291df47ee.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![理解监督学习：理论与概述](../Images/2a4574ae6e1142a8697461a291df47ee.png)'
- en: Image by Author
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: 'Suppose that you want to build a model that can differentiate between apples
    and oranges based on some characteristics. We can break down the process into
    the following tasks:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想建立一个模型，根据一些特征区分苹果和橙子。我们可以将这个过程分解为以下任务：
- en: '**Data Collection:**  Gather a dataset with pictures of apples and oranges,
    and each image is labeled as either "apple" or "orange."'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据收集：** 收集一个包含苹果和橙子的图片的数据集，并将每张图片标记为“苹果”或“橙子”。'
- en: '**Model Selection**: We have to pick the right classifier here often known
    as the right supervised machine learning algorithm for your task. It is just like
    picking the right glasses that will help you see better'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型选择**：我们必须选择合适的分类器，通常被称为适合您任务的监督机器学习算法。这就像选择合适的眼镜来帮助你看得更清楚。'
- en: '**Training the Model:** Now, you feed the algorithm with the labeled images
    of apples and oranges. The algorithm looks at these pictures and learns to recognize
    the differences, such as the color, shape, and size of apples and oranges.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练模型：** 现在，你将带标签的苹果和橙子的图片输入算法。算法查看这些图片，学习识别它们之间的差异，如颜色、形状和大小。'
- en: '**Evaluating & Testing:** To check if your model is working correctly, we will
    feed some unseen pictures to it and compare the predictions with the actual one.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估与测试：** 为了检查你的模型是否工作正常，我们将输入一些未见过的图片，并将预测结果与实际结果进行比较。'
- en: Types of Supervised Learning
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习的类型
- en: 'Supervised learning can be divided into two main types:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习可以分为两种主要类型：
- en: Classification
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类
- en: In classification tasks, the primary objective is to assign data points to specific
    categories from a set of discrete classes. When there are only two possible outcomes,
    such as "yes" or "no," "spam" or "not spam," "accepted" or "rejected," it is referred
    to as binary classification. However, when there are more than two categories
    or classes involved, like grading students based on their marks (e.g., A, B, C,
    D, F), it becomes an example of a multi-classification problem.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类任务中，主要目标是将数据点分配到从离散类别集合中的特定类别。当只有两个可能的结果，例如“是”或“否”、“垃圾邮件”或“非垃圾邮件”、“接受”或“拒绝”时，这被称为二分类。然而，当涉及到多个类别或类时，例如根据成绩对学生进行评分（例如，A、B、C、D、F），它就成为多分类问题的一个例子。
- en: Regression
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归
- en: For regression problems, you are trying to predict a continuous numerical value.
    For example, you might be interested in predicting your final exam scores based
    on your past performance in the class. The predicted scores can span any value
    within a specific range, typically from 0 to 100 in our case.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归问题，你试图预测一个连续的数值。例如，你可能希望根据你在课堂上的过去表现预测你的期末考试成绩。预测的成绩可以在特定范围内的任何值，通常在我们的例子中是从0到100。
- en: Overview of Popular Supervised Learning Algorithms
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流行监督学习算法概述
- en: 'Now, we have a basic understanding of the overall process. We will explore
    the popular supervised machine learning algorithms, their usage, and how they
    work:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对整体过程有了基本了解。我们将探索流行的监督学习算法，它们的使用以及它们是如何工作的：
- en: 1\. Linear Regression
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 线性回归
- en: 'As the name suggests, it is used for regression tasks like predicting stock
    prices, forecasting the temperature, estimating the likelihood of disease progression,
    etc. We try to predict the target (dependent variable) using the set of labels
    (independent variables). It assumes that we have a linear relationship between
    our input features and the label. The central idea revolves around predicting
    the best-fit line for our data points by minimizing the error between our actual
    and predicted values. This line is represented by the equation:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名字所示，它用于回归任务，如预测股票价格、预测温度、估计疾病进展的可能性等。我们尝试使用一组标签（自变量）来预测目标（因变量）。它假设输入特征与标签之间存在线性关系。核心思想是通过最小化实际值与预测值之间的误差来预测数据点的最佳拟合直线。该直线由以下方程表示：
- en: '![Equation](../Images/5fafd0dd11eff7815530e8bc855d776c.png "Y=b_{0}+b_{1}X")'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![方程](../Images/5fafd0dd11eff7815530e8bc855d776c.png "Y=b_{0}+b_{1}X")'
- en: Where,
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，
- en: '**Y**   Predicted output.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Y**   预测输出。'
- en: '**X** =  Input feature or feature matrix in multiple linear regression'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**X** = 多元线性回归中的输入特征或特征矩阵'
- en: '**b0** = Intercept (where the line crosses the Y-axis).'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**b0** = 截距（直线与Y轴交点）。'
- en: '**b1** =  Slope or coefficient that determines the line''s steepness.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**b1** = 确定直线陡峭度的斜率或系数。'
- en: It estimates the slope of the line (weight) and its intercept(bias). This line
    can be used further to make predictions. Although it is the simplest and useful
    model for developing the baselines it is highly sensitive to outliers that may
    influence the position of the line.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 它估计了直线的斜率（权重）和截距（偏差）。这条直线可以进一步用于进行预测。尽管它是开发基准的最简单且有用的模型，但它对可能影响直线位置的离群值非常敏感。
- en: '![Understanding Supervised Learning: Theory and Overview](../Images/0816f56ee0c1a92a0784679dac4b2d97.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![理解监督学习：理论与概述](../Images/0816f56ee0c1a92a0784679dac4b2d97.png)'
- en: Gif on [Primo.ai](https://cdn-images-1.medium.com/max/640/1*eeIvlwkMNG1wSmj3FR6M2g.gif)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Gif 在 [Primo.ai](https://cdn-images-1.medium.com/max/640/1*eeIvlwkMNG1wSmj3FR6M2g.gif)
- en: 2\. Logistic Regression
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 逻辑回归
- en: 'Although it has regression in its name, but is fundamentally used for binary
    classification problems. It predicts the probability of a positive outcome (dependent
    variable) which lies in the range of 0 to 1\. By setting a threshold (usually
    0.5), we classify data points: those with a probability greater than the threshold
    belongs to the positive class, and vice versa. Logistic regression calculates
    this probability using the sigmoid function applied to the linear combination
    of the input features which is specified as:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它的名字中包含回归，但本质上用于二分类问题。它预测一个正向结果的概率（因变量），该概率范围在0到1之间。通过设置阈值（通常为0.5），我们对数据点进行分类：那些概率大于阈值的数据点属于正类，反之亦然。逻辑回归使用应用于输入特征的线性组合的Sigmoid函数来计算这个概率，具体如下：
- en: '![Equation](../Images/d72c23a2ab99505557f2743b27dd08dd.png "P(Y=1)=\frac{1}{1+e^{-(b_0+b_1X_1+b_2X_2+\ldots+b_nX_n)}}")'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![方程](../Images/d72c23a2ab99505557f2743b27dd08dd.png "P(Y=1)=\frac{1}{1+e^{-(b_0+b_1X_1+b_2X_2+\ldots+b_nX_n)}}")'
- en: Where,
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，
- en: P(Y=1) = Probability of the data point belonging to the positive class
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(Y=1) = 数据点属于正类的概率
- en: X1 ,... ,Xn = Input Features
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X1 ,... ,Xn = 输入特征
- en: b0,....,bn = Input weights that the algorithm learns during training
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: b0,....,bn = 算法在训练过程中学习的输入权重
- en: This sigmoid function is in the form of S like curve that transforms any data
    point to a probability score within the range of 0-1\. You can see the below graph
    for a better understanding.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Sigmoid函数呈S形曲线，将任何数据点转换为0-1范围内的概率分数。你可以查看下面的图表以获得更好的理解。
- en: '![Understanding Supervised Learning: Theory and Overview](../Images/320a811842b87f33e0d8398ec5313ae5.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![理解监督学习：理论与概述](../Images/320a811842b87f33e0d8398ec5313ae5.png)'
- en: Image on [Wikipedia](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自于[维基百科](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png)
- en: A closer value to 1 indicates a higher confidence in the model in its prediction.
    Just like linear regression, it is known for its simplicity but we cannot perform
    the multi-class classification without modification to the original algorithm.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 值越接近1，表示模型对其预测的信心越高。与线性回归一样，它以其简单性而闻名，但我们不能在不对原算法进行修改的情况下执行多类别分类。
- en: 3\. Decision Trees
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 决策树
- en: 'Unlike the above two algorithms, decision trees can be used for both classification
    and regression tasks. It has a hierarchical structure just like the flowcharts.
    At each node, a decision about the path is made based on some feature values.
    The process continues unless we reach the last node that depicts the final decision.
    Here is some basic terminology that you must be aware of:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述两种算法不同，决策树可以用于分类和回归任务。它具有层次结构，类似于流程图。在每个节点，根据某些特征值做出路径决策。该过程继续，直到我们到达表示最终决策的最后一个节点。以下是一些你必须了解的基本术语：
- en: '**Root Node:** The top node containing the entire dataset is called the root
    node. We then select the best feature using some algorithm to split the dataset
    into 2 or more sub-trees.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**根节点：** 包含整个数据集的顶部节点称为根节点。然后，我们使用某些算法选择最佳特征，将数据集分成两个或更多子树。'
- en: '**Internal Nodes:** Each Internal node represents a specific feature and a
    decision rule to decide the next possible direction for a data point.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内部节点：** 每个内部节点表示一个特定的特征和一个决策规则，以决定数据点的下一步方向。'
- en: '**Leaf Nodes:** The ending nodes that represent a class label are referred
    to as leaf nodes.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**叶节点：** 代表类别标签的结束节点称为叶节点。'
- en: It predicts the continuous numerical values for the regression tasks. As the
    size of the dataset grows, it captures the noise leading to overfitting. This
    can be handled by pruning the decision tree. We remove branches that don't significantly
    improve the accuracy of our decisions. This helps keep our tree focused on the
    most important factors and prevents it from getting lost in the details.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 它预测回归任务中的连续数值。随着数据集的大小增加，它会捕捉到噪声，从而导致过拟合。这可以通过修剪决策树来处理。我们去除那些不会显著提高决策准确性的分支。这有助于保持树的关注点在最重要的因素上，防止它在细节中迷失。
- en: '![Understanding Supervised Learning: Theory and Overview](../Images/349f500f34b03161b6de7c918b240f37.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![理解监督学习：理论与概述](../Images/349f500f34b03161b6de7c918b240f37.png)'
- en: Image by [Jake Hoare](https://cdn-dfnaj.nitrocdn.com/xxeFXDnBIOflfPsgwjDLywIQwPChAOzV/assets/images/optimized/rev-4a6533c/www.displayr.com/wp-content/uploads/2018/07/what-is-a-decision-tree.png)
    on Displayr
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自于[Jake Hoare](https://cdn-dfnaj.nitrocdn.com/xxeFXDnBIOflfPsgwjDLywIQwPChAOzV/assets/images/optimized/rev-4a6533c/www.displayr.com/wp-content/uploads/2018/07/what-is-a-decision-tree.png)
    在Displayr上
- en: 4\. Random Forest
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 随机森林
- en: 'Random forest can also be used for both the classification and the regression
    tasks. It is a group of decision trees working together to make the final prediction.
    You can think of it as the committee of experts making a collective decision.
    Here is how it works:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林也可以用于分类和回归任务。它是多个决策树共同工作以做出最终预测的集合。你可以把它看作是专家委员会做出的集体决策。它的工作原理如下：
- en: '**Data Sampling:** Instead of taking the entire dataset at once, it takes the
    random samples via a process called bootstrapping or bagging.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据采样：** 不是一次性处理整个数据集，而是通过称为自助法或袋装法的过程获取随机样本。'
- en: '**Feature Selection:** For each decision tree in a random forest, only the
    random subset of features is considered for the decision-making instead of the
    complete feature set.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征选择：** 在随机森林中的每个决策树中，仅考虑随机特征子集进行决策，而不是完整的特征集。'
- en: '**Voting:** For classification, each decision tree in the random forest casts
    its vote and the class with the highest votes is selected. For regression, we
    average the values obtained from all trees.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**投票：** 对于分类问题，随机森林中的每个决策树都会投票，选票最多的类别被选择。对于回归问题，我们对所有树得到的值进行平均。'
- en: Although it reduces the effect of overfitting caused by individual decision
    trees, but is computationally expensive. One word that you will read frequently
    in the literature is that the random forest is an ensemble learning method, which
    means it combines multiple models to improve overall performance.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它减少了个别决策树引起的过拟合效应，但计算成本较高。文献中常出现的一句话是，随机森林是一种集成学习方法，意味着它结合了多个模型以提高整体性能。
- en: 5\. Support Vector Machines (SVM)
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 支持向量机 (SVM)
- en: 'It is primarily used for classification problems but can handle regression
    tasks as well. It tries to find the best hyperplane that separates the distinct
    classes using the statistical approach, unlike the probabilistic approach of logistic
    regression. We can use the linear SVM for the linearly separable data. However,
    most of the real-world data is non-linear and we use the kernel tricks to separate
    the classes. Let''s dive deep into how it works:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 它主要用于分类问题，但也可以处理回归任务。它尝试通过统计方法找到最佳的超平面，以分隔不同的类别，这与逻辑回归的概率方法不同。我们可以使用线性 SVM 来处理线性可分的数据。然而，大多数实际数据是非线性的，我们使用核技巧来分隔类别。让我们深入探讨它的工作原理：
- en: '**Hyperplane Selection:** In binary classification, SVM finds the best hyperplane
    (2-D line) to separate the classes while maximizing the margin. Margin is the
    distance between the hyperplane and the closest data points to the hyperplane.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超平面选择：** 在二分类中，SVM 寻找最佳的超平面（2D 线）来分隔类别，同时最大化边际。边际是超平面与离超平面最近的数据点之间的距离。'
- en: '**Kernel Trick:** For linearly inseparable data, we employ a kernel trick that
    maps the original data space into a high-dimensional space where they can be separated
    linearly. Common kernels include linear, polynomial, radial basis function (RBF),
    and sigmoid kernels.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**核技巧：** 对于线性不可分的数据，我们采用核技巧，将原始数据空间映射到高维空间，在那里可以线性分隔。常见的核包括线性核、多项式核、径向基函数（RBF）核和
    sigmoid 核。'
- en: '**Margin Maximization:** SVM also tries to improve the generalization of the
    model by increasing the maximizing margin.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边际最大化：** SVM 还通过增加最大化边际来提高模型的泛化能力。'
- en: '**Classification:** Once the model is trained, the predictions can be made
    based on their position relative to the hyperplane.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类：** 一旦模型训练完成，可以根据数据相对于超平面的位置进行预测。'
- en: SVM also has a parameter called C that controls the trade-off between maximizing
    the margin and keeping the classification error to a minimum. Although they can
    handle high-dimensional and non-linear data well, choosing the right kernel and
    hyperparameter is not as easy as it seems.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 还有一个参数叫做 C，用于控制最大化边际和保持分类误差最小之间的权衡。尽管它们可以很好地处理高维和非线性数据，但选择正确的核和超参数并不像看起来那么简单。
- en: '![Understanding Supervised Learning: Theory and Overview](../Images/de4cdeab60c3720657daf47783c423d2.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![理解监督学习：理论与概述](../Images/de4cdeab60c3720657daf47783c423d2.png)'
- en: Image on [Javatpoint](https://static.javatpoint.com/tutorial/machine-learning/images/support-vector-machine-algorithm.png)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Javatpoint](https://static.javatpoint.com/tutorial/machine-learning/images/support-vector-machine-algorithm.png)
- en: 6\. k-Nearest Neighbors (k-NN)
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. k-最近邻 (k-NN)
- en: K-NN is the simplest supervised learning algorithm mostly used for classification
    tasks. It doesn’t make any assumptions about the data and assigns the new data
    point a category based on its similarity with the existing ones. During the training
    phase, it keeps the entire dataset as a reference point. It then calculates the
    distance between the new data point and all the existing points using a distance
    metric (Eucilinedain distance e.g.). Based on these distances, it identifies the
    K nearest neighbors to these data points. We then count the occurrence of each
    class in the K nearest neighbors and assign the most frequently appearing class
    as the final prediction.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: K-NN是最简单的监督学习算法，主要用于分类任务。它对数据没有任何假设，并根据与现有数据点的相似性为新数据点分配一个类别。在训练阶段，它将整个数据集作为参考点。然后，它使用距离度量（例如欧几里得距离）计算新数据点与所有现有点之间的距离。根据这些距离，它识别出与这些数据点最接近的K个邻居。我们统计K个邻居中每个类别的出现次数，并将出现频率最高的类别作为最终预测。
- en: '![Understanding Supervised Learning: Theory and Overview](../Images/f378070bfb874decbeca262401c1d6ee.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![理解监督学习：理论与概述](../Images/f378070bfb874decbeca262401c1d6ee.png)'
- en: Image on [GeeksforGeeks](https://media.geeksforgeeks.org/wp-content/uploads/20200616145419/Untitled2781.png)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [GeeksforGeeks](https://media.geeksforgeeks.org/wp-content/uploads/20200616145419/Untitled2781.png)
- en: Choosing the right value of K requires experimentation. Although it is robust
    to noisy data it is not suitable for high dimensional datasets and has a high
    cost associated due to the calculation of the distance from all data points.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的K值需要实验。虽然它对噪声数据具有鲁棒性，但不适用于高维数据集，并且由于需要计算所有数据点之间的距离，成本较高。
- en: Wrapping Up
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'As I conclude this article, I would encourage the readers to explore more algorithms
    and try to implement them from scratch. This will strengthen your understanding
    of how things are working under the hood. Here are some additional resources to
    help you get started:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在总结这篇文章时，我鼓励读者探索更多的算法，并尝试从零开始实现它们。这将增强你对底层工作原理的理解。以下是一些额外资源，帮助你入门：
- en: '[Mastering Machine Learning Algorithms - Second Edition](https://www.oreilly.com/library/view/mastering-machine-learning/9781838820299/)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握机器学习算法 - 第二版](https://www.oreilly.com/library/view/mastering-machine-learning/9781838820299/)'
- en: Machine Learning Course - [Javatpoint](https://www.javatpoint.com/machine-learning)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习课程 - [Javatpoint](https://www.javatpoint.com/machine-learning)
- en: Machine Learning Specialization - [Coursera](https://www.coursera.org/specializations/machine-learning-introduction)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习专业化 - [Coursera](https://www.coursera.org/specializations/machine-learning-introduction)
- en: '**[Kanwal Mehreen](https://www.linkedin.com/in/kanwal-mehreen1)** is an aspiring
    software developer with a keen interest in data science and applications of AI
    in medicine. Kanwal was selected as the Google Generation Scholar 2022 for the
    APAC region. Kanwal loves to share technical knowledge by writing articles on
    trending topics, and is passionate about improving the representation of women
    in tech industry.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Kanwal Mehreen](https://www.linkedin.com/in/kanwal-mehreen1)** 是一位有志的软件开发人员，对数据科学和AI在医学中的应用充满兴趣。Kanwal被选为2022年APAC地区的Google
    Generation Scholar。Kanwal喜欢通过撰写关于热门话题的文章分享技术知识，并且热衷于提升女性在技术行业中的代表性。'
- en: More On This Topic
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Machine Learning Evaluation Metrics: Theory and Overview](https://www.kdnuggets.com/machine-learning-evaluation-metrics-theory-and-overview)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习评估指标：理论与概述](https://www.kdnuggets.com/machine-learning-evaluation-metrics-theory-and-overview)'
- en: '[Statistics in Data Science: Theory and Overview](https://www.kdnuggets.com/statistics-in-data-science-theory-and-overview)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中的统计学：理论与概述](https://www.kdnuggets.com/statistics-in-data-science-theory-and-overview)'
- en: '[Primary Supervised Learning Algorithms Used in Machine Learning](https://www.kdnuggets.com/2022/06/primary-supervised-learning-algorithms-used-machine-learning.html)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中使用的主要监督学习算法](https://www.kdnuggets.com/2022/06/primary-supervised-learning-algorithms-used-machine-learning.html)'
- en: '[KDnuggets News, June 22: Primary Supervised Learning Algorithms…](https://www.kdnuggets.com/2022/n25.html)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets新闻，6月22日：主要监督学习算法…](https://www.kdnuggets.com/2022/n25.html)'
- en: '[Understanding Machine Learning Algorithms: An In-Depth Overview](https://www.kdnuggets.com/understanding-machine-learning-algorithms-an-indepth-overview)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解机器学习算法：深入概述](https://www.kdnuggets.com/understanding-machine-learning-algorithms-an-indepth-overview)'
- en: '[Hands-On with Supervised Learning: Linear Regression](https://www.kdnuggets.com/handson-with-supervised-learning-linear-regression)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[动手实践监督学习：线性回归](https://www.kdnuggets.com/handson-with-supervised-learning-linear-regression)'
