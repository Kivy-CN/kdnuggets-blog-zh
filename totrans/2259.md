# 缩小人类理解与机器学习之间的差距：可解释人工智能作为解决方案

> 原文：[https://www.kdnuggets.com/2023/06/closing-gap-human-understanding-machine-learning-explainable-ai-solution.html](https://www.kdnuggets.com/2023/06/closing-gap-human-understanding-machine-learning-explainable-ai-solution.html)

![缩小人类理解与机器学习之间的差距：可解释人工智能作为解决方案](../Images/f61a1b91d9d13f774674910b3eac2305.png)

图片由 Bing 图像创作者提供

# 介绍

你是否曾打开你最喜欢的购物应用程序，看到的第一件事是推荐一个你甚至不知道需要的产品，但由于及时的推荐你最终购买了它？或者你是否打开你常用的音乐应用程序，看到一个被遗忘的宝藏由你最喜欢的艺术家推荐在最上面，作为“你可能喜欢”的内容感到高兴？无论是知道还是不知道，我们今天都遇到由人工智能（AI）生成的决策、行动或体验。虽然这些体验中的一些是相当无害的（比如精准的音乐推荐），但有些可能会引起一些不安（“*这个应用怎么知道我一直在考虑进行减肥计划？*”）。当涉及到关于自己和亲人的隐私问题时，这种不安会升级为担忧和不信任。然而，了解*如何*或*为什么*会推荐某些内容，可以帮助缓解一些这种不安。

这就是可解释人工智能（Explainable AI，简称 XAI）的作用所在。随着人工智能系统变得越来越普及，理解这些系统如何做出决策的需求也在增长。本文将探讨 XAI，讨论可解释 AI 模型中的挑战，介绍使这些模型更具可解释性的进展，并为公司和个人提供在产品中实施 XAI 的指导，以促进用户对人工智能的信任。

# 什么是可解释人工智能？

可解释人工智能（XAI）是指 AI 系统能够提供其决策或行动的解释的能力。XAI 填补了 AI 系统决策与最终用户理解*为什么*做出该决策之间的重要差距。在人工智能出现之前，系统通常是基于规则的（例如，如果客户购买裤子，则推荐皮带；或者如果某人打开“智能电视”，则在固定的三个选项之间循环推荐#1）。这些经历提供了一种可预测的感觉。然而，随着人工智能的普及，倒推为什么会显示某些内容或某些决策是如何由产品做出的并不简单。可解释人工智能可以在这些情况下提供帮助。

可解释的 AI（XAI）允许用户理解 AI 系统做出某个决策的*原因*以及*决策所依据的因素*。例如，当你打开音乐应用时，你可能会看到一个名为“*因为你喜欢泰勒·斯威夫特*”的部件，接着是类似泰勒·斯威夫特歌曲的流行音乐推荐。或者你可能打开一个购物应用，看到“*基于你最近的购物历史的推荐*”，接着是婴儿产品推荐，因为你在最近几天购买了一些婴儿玩具和衣物。

XAI 在 AI 做出高风险决策的领域尤为重要。例如，算法交易和其他金融建议、医疗保健、自动驾驶汽车等。能够提供决策解释有助于用户理解决策理由，识别因训练数据引入的模型偏见，纠正决策错误，并帮助建立人类与 AI 之间的信任。此外，随着日益增加的监管指南和法律要求，XAI 的重要性只会不断增长。

# XAI 的挑战

如果 XAI 能够向用户提供透明度，那为什么不使所有 AI 模型都具备可解释性呢？有几个挑战阻碍了这一目标的实现。

高级 AI 模型如深度神经网络在输入和输出之间有多个隐藏层。每一层接受来自前一层的输入，对其进行计算，然后将结果传递给下一层。层与层之间的复杂交互使得跟踪决策过程以使其可解释变得困难。这就是为什么这些模型通常被称为黑箱的原因。

这些模型还处理高维数据，如图像、音频、文本等。解释每一个特征的影响以确定哪个特征对决策的贡献最大是具有挑战性的。简化这些模型以提高可解释性会导致性能下降。例如，更简单且“易于理解”的模型如决策树可能会牺牲预测性能。因此，为了可预测性而在性能和准确性之间进行权衡也是不可接受的。

# XAI 的进展

随着对 XAI 的需求日益增长以继续建立人类对 AI 的信任，近年来在这一领域取得了一些进展。例如，一些模型如决策树或线性模型使可解释性相当明显。还有一些符号或基于规则的 AI 模型专注于信息和知识的明确表示。这些模型通常需要人为定义规则并向模型提供领域信息。随着这一领域的积极发展，也出现了结合深度学习与可解释性的混合模型，最大限度地减少了性能上的牺牲。

# 实施 XAI 的产品指南

赋予用户更多理解AI模型决策原因的能力，有助于促进对模型的信任和透明度。这可以导致人机之间改进的、互利的合作，其中AI模型帮助人类做出透明的决策，而人类则帮助调整AI模型以消除偏差、不准确和错误。

以下是公司和个人可以在其产品中实施XAI的一些方式：

1.  **选择可解释的模型** – 在可解释的AI模型能满足需求且表现良好的情况下，应优先选择这些模型。例如，在医疗保健领域，像决策树这样的简单模型可以帮助医生理解AI模型推荐某种诊断的原因，这有助于建立医生与AI模型之间的信任。应使用提高可解释性的特征工程技术，如独热编码或特征缩放。

1.  **使用事后解释** – 使用像特征重要性和注意力机制这样的技术生成事后解释。例如，LIME（局部可解释模型无关解释）是一种解释模型预测的技术。它生成特征重要性分数，以突出每个特征对模型决策的贡献。例如，如果你最终“喜欢”某个播放列表推荐，LIME方法会尝试添加和移除播放列表中的某些歌曲，并预测你喜欢该播放列表的可能性，从而得出播放列表中歌曲的艺术家在你喜欢或不喜欢该播放列表中扮演了重要角色。

1.  **与用户的沟通** – 可以使用像LIME或SHAP（SHapley Additive exPlanations）这样的技术，提供关于具体局部决策或预测的有用解释，而不必解释模型的所有复杂性。还可以利用激活图或注意力图等视觉提示，突出输入与模型生成的输出之间的相关性。像Chat GPT这样的新技术可以用来简化复杂的解释，以用户可以理解的简单语言表达。最后，给予用户一定的控制权，以便他们与模型互动，这有助于建立信任。例如，用户可以尝试以不同的方式调整输入，以查看输出的变化。

1.  **持续监测** – 公司应实施机制来监控模型的性能，并在检测到偏差或漂移时自动发出警报。应定期更新和微调模型，并进行审计和评估，以确保模型符合监管法律并满足伦理标准。最后，即使很少，也应有人参与提供反馈和必要的修正。

# 结论

总结来说，随着人工智能的不断发展，构建可解释的人工智能（XAI）变得至关重要，以维持用户对人工智能的信任。通过采用上述指导方针，公司和个人可以构建更透明、易于理解和简单的人工智能。公司越多地采用XAI，用户与人工智能系统之间的沟通就会越好，用户也会越有信心让人工智能改善他们的生活。

**[Ashlesha Kadam](https://www.linkedin.com/in/ashleshakadam/)** 领导着亚马逊音乐的全球产品团队，为来自45多个国家的数百万客户在Alexa和亚马逊音乐应用（网页、iOS、Android）上打造音乐体验。她还是女性科技领域的热情倡导者，担任Grace Hopper Celebration（全球最大的女性科技会议，参会者超过3万人，来自115个国家）的计算机与人类交互（HCI）分会的联合主席。在空闲时间，Ashlesha喜欢阅读小说、听商业科技播客（当前最爱 - Acquired），在美丽的太平洋西北地区徒步旅行，以及和丈夫、儿子及5岁的金毛猎犬共度时光。

* * *

## 我们的前三推荐课程

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你所在组织的IT工作

* * *

### 更多相关主题

+   [深度学习与人类认知能力之间的差距](https://www.kdnuggets.com/2022/10/gap-deep-learning-human-cognitive-abilities.html)

+   [人工智能教育差距及如何弥补](https://www.kdnuggets.com/2022/11/ai-education-gap-close.html)

+   [有没有办法弥补MLOps工具的差距？](https://www.kdnuggets.com/2022/08/way-bridge-mlops-tools-gap.html)

+   [现实世界中NLP应用的范围：不同的解决方案](https://www.kdnuggets.com/2022/03/different-solution-problem-range-nlp-applications-real-world.html)

+   [机器学习中训练数据与测试数据的区别](https://www.kdnuggets.com/2022/08/difference-training-testing-data-machine-learning.html)

+   [梦境与现实之间：生成文本与幻觉](https://www.kdnuggets.com/between-dreams-and-reality-generative-text-and-hallucinations)
