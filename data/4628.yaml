- en: Classifying Heart Disease Using K-Nearest Neighbors
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 K-最近邻算法分类心脏病
- en: 原文：[https://www.kdnuggets.com/2019/07/classifying-heart-disease-using-k-nearest-neighbors.html](https://www.kdnuggets.com/2019/07/classifying-heart-disease-using-k-nearest-neighbors.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/07/classifying-heart-disease-using-k-nearest-neighbors.html](https://www.kdnuggets.com/2019/07/classifying-heart-disease-using-k-nearest-neighbors.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](/2019/07/classifying-heart-disease-using-k-nearest-neighbors.html?page=2#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](/2019/07/classifying-heart-disease-using-k-nearest-neighbors.html?page=2#comments)'
- en: Classification of objects is an important area of research and application in
    a variety of fields. In the presence of full knowledge of the underlying probabilities,
    Bayes decision theory gives optimal error rates. In those cases where this information
    is not present, many algorithms make use of distance or similarity among samples
    as a means of classification.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对象分类是各种领域中的一个重要研究和应用领域。在完全了解潜在概率的情况下，贝叶斯决策理论提供了最佳的错误率。在这些信息缺失的情况下，许多算法利用样本之间的距离或相似性进行分类。
- en: The article has been divided into 2 parts. In the first part, we’ll talk all
    about the K-NN machine learning algorithm and in the second part, we will implement
    K-NN in real life and classify Heart disease patients.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章分为两部分。第一部分，我们将讨论 K-NN 机器学习算法，第二部分，我们将实际应用 K-NN 并分类心脏病患者。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前 3 个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT 工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Table of content**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**内容表**'
- en: What is a K-NN algorithm?
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是 K-NN 算法？
- en: How does the K-NN algorithm work?
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K-NN 算法是如何工作的？
- en: When to choose K-NN?
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 何时选择 K-NN？
- en: How to choose the optimal value of K?
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何选择 K 的最佳值？
- en: What is Curse of dimensionality?
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是维度诅咒？
- en: Building K-NN classifier using python sci-kit learn.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Python 的 sci-kit learn 构建 K-NN 分类器。
- en: How to improve the performance of your classifier?
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何提高分类器的性能？
- en: What is a K-NN Algorithm?
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是 K-NN 算法？
- en: '![](../Images/fe225aa90f1f3ec39506d32db01afc17.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe225aa90f1f3ec39506d32db01afc17.png)'
- en: '[K-NN Algorithm representation](https://acadgild.com/blog/k-nearest-neighbor-algorithm)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[K-NN 算法表示](https://acadgild.com/blog/k-nearest-neighbor-algorithm)'
- en: K-NN or K-Nearest Neighbors is one of the most famous classification algorithms
    as of now in the industry simply because of its simplicity and accuracy.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: K-NN 或 K-最近邻算法是当前业界最著名的分类算法之一，主要因其简单性和准确性。
- en: K-NN is a simple algorithm that stores all available cases and classifies new
    cases based on a similarity measure (e.g., distance functions). KNN has been used
    in statistical estimation and pattern recognition already at the beginning of
    the 1970s as a non-parametric technique.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: K-NN 是一种简单的算法，它存储所有可用的案例，并根据相似度度量（例如，距离函数）对新案例进行分类。KNN 已经在 1970 年代初期作为一种非参数技术被用于统计估计和模式识别。
- en: The algorithm assumes that similar things exist in close proximity. In other
    words, entities which are similar exist together.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法假设相似的事物存在于近距离内。换句话说，相似的实体会聚集在一起。
- en: How the K-NN algorithm works?
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K-NN 算法如何工作？
- en: In K-NN, K is the number of nearest neighbors. The number of neighbors is the
    core deciding factor. **K is generally an odd number if the number of classes
    is 2**. When K=1, then the algorithm is known as the nearest neighbor algorithm.
    This is the simplest case.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在 K-NN 中，K 是最近邻居的数量。邻居的数量是核心决定因素。**如果类别数为 2，则 K 通常是奇数**。当 K=1 时，算法被称为最近邻算法。这是最简单的情况。
- en: In the below figure, suppose yellow colored “**?**” let's say P is the point,
    for which label needs to predict. First, you find the one closest point to P and
    then the label of the nearest point assigned to P.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，假设黄色的“**?**”假设 P 是需要预测标签的点。首先，找到离 P 最近的一个点，然后将最近点的标签分配给 P。
- en: '![](../Images/861a2f7b786e09051bbd0d37b8db1857.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/861a2f7b786e09051bbd0d37b8db1857.png)'
- en: 'Second, you find the k closest point to P and then classify points by majority
    vote of its K neighbors. Each object votes for their class and the class with
    the most votes is taken as the prediction. For finding closest similar points,
    we find the distance between points using distance measures such as Euclidean
    distance, Hamming distance, Manhattan distance, and Minkowski distance. The algorithm
    has the following basic steps:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，找到离 P 最近的 k 个点，然后通过 K 个邻居的多数投票来分类点。每个对象为其类别投票，获得最多票数的类别被视为预测。为了找到最接近的相似点，我们使用距离度量（如欧几里得距离、汉明距离、曼哈顿距离和闵可夫斯基距离）来计算点之间的距离。算法的基本步骤如下：
- en: Calculate distance
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算距离
- en: Find closest neighbors
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到最近的邻居
- en: Vote for labels
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为标签投票
- en: '![](../Images/be2fc1f9343745ff2ec1e5a9747b2f7a.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be2fc1f9343745ff2ec1e5a9747b2f7a.png)'
- en: 'Three most commonly used distance measures used to calculate the distance between
    point P and its nearest neighbors are represented as :'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 计算点 P 与其最近邻点之间距离的三种最常用的度量方式为：
- en: '![](../Images/82d48a4486aae9019ed734c5115b1878.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82d48a4486aae9019ed734c5115b1878.png)'
- en: In this article we will go ahead with Euclidean distance, so let's understand
    it first.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将首先使用欧几里得距离，所以先了解它。
- en: '**Euclidean distance:** It is the most commonly used distance measure also
    called simply distance. The usage of a Euclidean distance measure is highly recommended
    when the data is dense or continuous. Euclidean distance is the best proximity
    measure. The Euclidean distance between two points is the length of the path connecting
    them. The Pythagorean theorem gives this distance between two points.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**欧几里得距离：** 这是最常用的距离度量，也称为简单距离。当数据是密集或连续时，强烈建议使用欧几里得距离度量。欧几里得距离是最佳的接近度量。两点之间的欧几里得距离是连接它们的路径长度。毕达哥拉斯定理给出了两点之间的距离。'
- en: Below figure shows how to calculate Euclidean distance between two points in
    a 2-dimensional plane.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了如何计算二维平面上两点之间的欧几里得距离。
- en: '![Figure](../Images/181a263e28b55514150e7ae38d495336.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/181a263e28b55514150e7ae38d495336.png)'
- en: '[Euclidean distance between two points in 2-D](https://mccormickml.com/2013/08/15/the-gaussian-kernel/)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[二维空间中两点的欧几里得距离](https://mccormickml.com/2013/08/15/the-gaussian-kernel/)'
- en: When to use K-NN algorithm?
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 何时使用 K-NN 算法？
- en: 'KNN can be used for both classification and regression predictive problems.
    However, it is more widely used in classification problems in the industry. To
    evaluate any technique we generally look at 3 important aspects:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 可用于分类和回归预测问题。然而，在行业中，它更多地用于分类问题。要评估任何技术，我们通常关注三个重要方面：
- en: Ease to interpret the output
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出易于解释
- en: Calculation time of the algorithm
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法的计算时间
- en: Predictive Power
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测能力
- en: 'Let us compare KNN with different models:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 KNN 与不同模型进行比较：
- en: '![Figure](../Images/d52f21040e5ef16e8e24e34609aa2528.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/d52f21040e5ef16e8e24e34609aa2528.png)'
- en: 'source: Analytics Vidhya'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：Analytics Vidhya
- en: As you can see K-NN surpasses Logistic Regression, CART and Random Forest in
    terms of the aspects which we are considering.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，K-NN 在我们考虑的方面上超越了逻辑回归、CART 和随机森林。
- en: How to choose the optimal value of K?
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何选择 K 的最佳值？
- en: The number of neighbors(K) in K-NN is a hyperparameter that you need to choose
    at the time of building your model. You can think of K as a controlling variable
    for the prediction model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: K-NN 中的邻居数量（K）是一个超参数，你需要在构建模型时选择。你可以将 K 看作是预测模型的控制变量。
- en: Now, choosing the optimal value for K is best done by first inspecting the data.
    In general, a large K value is more precise as it reduces the overall noise but
    there is no guarantee. Cross-validation is another way to retrospectively determine
    a good K value by using an independent dataset to validate the K value. Historically,
    the optimal K for most datasets has been between 3–10\. That produces much better
    results than 1NN(when K=1).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，选择 K 的最佳值最好是通过首先检查数据来完成。通常，较大的 K 值更精确，因为它减少了总体噪声，但不能保证。交叉验证是另一种回顾性地确定良好 K
    值的方法，通过使用独立的数据集来验证 K 值。从历史上看，大多数数据集的最佳 K 值在 3 到 10 之间。它比 1NN（当 K=1 时）产生更好的结果。
- en: Generally, an odd number is chosen if the number of classes is even. You can
    also check by generating the model on different values of K and check their performance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如果类别数量是偶数，则选择奇数。你也可以通过生成不同 K 值的模型并检查它们的性能来验证。
- en: Curse of Dimensionality
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 维度诅咒
- en: K-NN performs better with a lower number of features than a large number of
    features. You can say that when the number of features increases than it requires
    more data. Increase in dimension also leads to the problem of overfitting. To
    avoid overfitting, the needed data will need to grow exponentially as you increase
    the number of dimensions. This problem of higher dimension is known as the Curse
    of Dimensionality.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: K-NN 在特征较少时表现更好，而不是特征较多时。你可以说，当特征数量增加时，它需要更多的数据。维度增加还会导致过拟合问题。为了避免过拟合，随着维度数量的增加，需要的数据量也需要呈指数增长。这种高维度的问题称为维度诅咒。
- en: '![](../Images/40e1bba71a6b963a2370165dc5a23d79.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40e1bba71a6b963a2370165dc5a23d79.png)'
- en: From the above graphical representation, it is clearly visible that the performance
    of your model decreases with an increase in the number of features(dimensions).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述图形表示可以明显看出，随着特征（维度）数量的增加，你的模型性能会下降。
- en: To deal with the problem of the curse of dimensionality, you need to perform
    principal component analysis(PCA) before applying any machine learning algorithm,
    or you can also use feature selection approach. Research has shown that in large
    dimension Euclidean distance is not useful anymore. Therefore, you can prefer
    other measures such as cosine similarity, which get decidedly less affected by
    high dimension.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决维度诅咒的问题，你需要在应用任何机器学习算法之前进行主成分分析（PCA），或者你也可以使用特征选择方法。研究表明，在高维情况下，欧几里得距离不再有用。因此，你可以选择其他度量方法，例如余弦相似度，它受到高维度的影响较小。
- en: '**The KNN algorithm can compete with the most accurate models because it makes
    highly accurate predictions. Therefore, you can use the KNN algorithm for applications
    that require high accuracy but that do not require a human-readable model. — **source:[IBM](https://www.ibm.com/support/knowledgecenter/en/SS6NHC/com.ibm.swg.im.dashdb.analytics.doc/doc/r_knn_usage.html)'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**KNN 算法可以与最精确的模型竞争，因为它提供了非常准确的预测。因此，你可以使用 KNN 算法用于需要高精度但不需要人类可读模型的应用。—**来源：[IBM](https://www.ibm.com/support/knowledgecenter/en/SS6NHC/com.ibm.swg.im.dashdb.analytics.doc/doc/r_knn_usage.html)'
- en: 'Steps to compute K-NN algorithm:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 K-NN 算法的步骤：
- en: Determine parameter K = number of nearest neighbors.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定参数 K = 最近邻的数量。
- en: Calculate the distance between the query-instance and all the training samples.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算查询实例与所有训练样本之间的距离。
- en: Sort the distance and determine nearest neighbors based on the K-th minimum
    distance.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对距离进行排序，并根据第 K 个最小距离确定最近邻。
- en: Gather the category of the nearest neighbors
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集最近邻的类别
- en: Use a simple majority of the category of nearest neighbors as the prediction
    value of the query.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最近邻类别的简单多数作为查询的预测值。
- en: In the next section, we are going to solve a real world scenario using K-NN
    algorithm.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用 K-NN 算法解决一个实际问题。
- en: More On This Topic
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[From Theory to Practice: Building a k-Nearest Neighbors Classifier](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从理论到实践：构建 k-最近邻分类器](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)'
- en: '[Nearest Neighbors for Classification](https://www.kdnuggets.com/2022/04/nearest-neighbors-classification.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分类的最近邻](https://www.kdnuggets.com/2022/04/nearest-neighbors-classification.html)'
- en: '[K-nearest Neighbors in Scikit-learn](https://www.kdnuggets.com/2022/07/knearest-neighbors-scikitlearn.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Scikit-learn 中的 K-最近邻](https://www.kdnuggets.com/2022/07/knearest-neighbors-scikitlearn.html)'
- en: '[Classifying Long Text Documents Using BERT](https://www.kdnuggets.com/2022/02/classifying-long-text-documents-bert.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 BERT 对长文本进行分类](https://www.kdnuggets.com/2022/02/classifying-long-text-documents-bert.html)'
- en: '[Automate Microsoft Excel and Word Using Python](https://www.kdnuggets.com/2021/08/automate-microsoft-excel-word-python.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 自动化 Microsoft Excel 和 Word](https://www.kdnuggets.com/2021/08/automate-microsoft-excel-word-python.html)'
- en: '[How to Determine the Best Fitting Data Distribution Using Python](https://www.kdnuggets.com/2021/09/determine-best-fitting-data-distribution-python.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 Python 确定最佳拟合数据分布](https://www.kdnuggets.com/2021/09/determine-best-fitting-data-distribution-python.html)'
