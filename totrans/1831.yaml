- en: How to Rank 10% in Your First Kaggle Competition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在第一次Kaggle比赛中排名前10%
- en: 原文：[https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/4](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/4)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/4](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/4)
- en: Home Depot Search Relevance
  id: totrans-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Home Depot 搜索相关性
- en: In this section I will share my solution in [Home Depot Search Relevance Competition](https://www.kaggle.com/c/home-depot-product-search-relevance) and
    what I learned from top teams after the competition.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我将分享我在[Home Depot搜索相关性比赛](https://www.kaggle.com/c/home-depot-product-search-relevance)中的解决方案以及我从顶尖团队中学到的东西。
- en: The task in this competition is to predict how relevant a result is for a search
    term on Home Depot website. The relevance is an average score from three human
    evaluators and ranges between 1 ~ 3\. Therefore it’s a regression task. The datasets
    contains search terms, product titles / descriptions and some attributes like
    brand, size and color. The metric is[RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本次比赛的任务是预测在Home Depot网站上搜索词的结果相关性。相关性是由三位人工评估者给出的平均分数，范围为1~3。因此，这是一个回归任务。数据集包含搜索词、产品标题/描述以及一些属性，如品牌、尺寸和颜色。评价指标是[RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation)。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的IT工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: This is much like [Crowdflower Search Results Relevance](https://www.kaggle.com/c/crowdflower-search-relevance).
    The difference is that [Quadratic Weighted Kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa#Weighted_kappa) is
    used in Crowdflower competition and therefore complicated the final cutoff of
    regression scores. Also there were no attributes provided in Crowdflower.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这很像 [Crowdflower搜索结果相关性](https://www.kaggle.com/c/crowdflower-search-relevance)。区别在于 [Quadratic
    Weighted Kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa#Weighted_kappa) 在Crowdflower比赛中被使用，因此使最终回归分数的截止值更加复杂。而且Crowdflower中没有提供属性。
- en: '**EDA**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**EDA**'
- en: 'There were several quite good EDAs by the time I joined the competition, especially [this
    one](https://www.kaggle.com/briantc/home-depot-product-search-relevance/homedepot-first-dataexploreation-k).
    I learned that:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我加入比赛时，已经有几个相当不错的EDA，特别是 [这个](https://www.kaggle.com/briantc/home-depot-product-search-relevance/homedepot-first-dataexploreation-k)。我了解到：
- en: Many search terms / products appeared several times.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多搜索词/产品出现了多次。
- en: Text similarities are great features.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本相似度是很好的特征。
- en: Many products don’t have attributes features. Would this be a problem?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多产品没有属性特征。这会成为问题吗？
- en: Product ID seems to have strong predictive power. However the overlap of product
    ID between the training set and the testing set is not very high. Would this contribute
    to overfitting?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品ID似乎具有很强的预测能力。然而，训练集和测试集之间的产品ID重叠度不是很高。这是否会导致过拟合？
- en: '**Preprocessing**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据预处理**'
- en: 'You can find how I did preprocessing and feature engineering [on GitHub](https://github.com/dnc1994/Kaggle-Playground/blob/master/home-depot/Preprocess.ipynb).
    I’ll only give a brief summary here:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[GitHub](https://github.com/dnc1994/Kaggle-Playground/blob/master/home-depot/Preprocess.ipynb)上找到我进行数据预处理和特征工程的方式。我这里只做简要总结：
- en: Use [typo dictionary](https://www.kaggle.com/steubk/home-depot-product-search-relevance/fixing-typos) posted
    in the forum to correct typos in search terms.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 [错别字词典](https://www.kaggle.com/steubk/home-depot-product-search-relevance/fixing-typos) 来纠正搜索词中的错别字。
- en: Count attributes. Find those frequent and easily exploited ones.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算属性特征。找出那些频繁出现且容易利用的特征。
- en: Join the training set with the testing set. This is important because otherwise
    you’ll have to do feature transformation twice.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练集与测试集合并。这很重要，否则你需要进行两次特征变换。
- en: Do **[stemming](https://en.wikipedia.org/wiki/Stemming)** and **[tokenizing](https://en.wikipedia.org/wiki/Text_segmentation#Word_segmentation)** for
    all the text fields. Some **normalization** (with digits and units) and **synonym
    substitutions** are performed manually.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对所有文本字段进行**[词干提取](https://en.wikipedia.org/wiki/Stemming)**和**[分词](https://en.wikipedia.org/wiki/Text_segmentation#Word_segmentation)**。一些**标准化**（涉及数字和单位）和**同义词替换**是手动进行的。
- en: '**Feature**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征**'
- en: '*Attribute Features'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*属性特征*'
- en: Whether the product contains a certain attribute (brand, size, color, weight,
    indoor/outdoor, energy star certified …)
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品是否包含某个特定属性（品牌、尺寸、颜色、重量、室内/室外、能源之星认证等）
- en: Whether a certain attribute matches with the search term
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 某个属性是否与搜索词匹配
- en: Meta Features
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元特征
- en: Length of each text field
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个文本字段的长度
- en: Whether the product contains attribute fields
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品是否包含属性字段
- en: Brand (encoded as integers)
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 品牌（以整数编码）
- en: Product ID
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品 ID
- en: Matching
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 匹配
- en: Whether search term appears in product title / description / attributes
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索词是否出现在产品标题/描述/属性中
- en: Count and ratio of search term’s appearance in product title / description /
    attributes
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索词在产品标题/描述/属性中的出现次数和比例
- en: '*Whether the i-th word of search term appears in product title / description
    / attributes'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*搜索词的第 i 个词是否出现在产品标题/描述/属性中*'
- en: Text similarities between search term and product title/description/attributes
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索词与产品标题/描述/属性之间的文本相似度
- en: '[BOW](https://en.wikipedia.org/wiki/Bag-of-words_model) [Cosine Similairty](https://en.wikipedia.org/wiki/Cosine_similarity)'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BOW](https://en.wikipedia.org/wiki/Bag-of-words_model) [余弦相似度](https://en.wikipedia.org/wiki/Cosine_similarity)'
- en: '[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) Cosine Similarity'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) 余弦相似度'
- en: '[Jaccard Similarity](https://en.wikipedia.org/wiki/Jaccard_index)'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Jaccard 相似度](https://en.wikipedia.org/wiki/Jaccard_index)'
- en: '*[Edit Distance](https://en.wikipedia.org/wiki/Edit_distance)'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*[编辑距离](https://en.wikipedia.org/wiki/Edit_distance)*'
- en: '[Word2Vec](https://en.wikipedia.org/wiki/Word2vec) Distance (I didn’t include
    this because of its poor performance and slow calculation. Yet it seems that I
    was using it wrong.)'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Word2Vec](https://en.wikipedia.org/wiki/Word2vec) 距离（我没有包含这个，因为它的表现不佳且计算速度慢。然而，似乎是我使用不当。）'
- en: '**[Latent Semantic Indexing](https://en.wikipedia.org/wiki/Latent_semantic_indexing):
    By performing [SVD decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) to
    the matrix obtained from BOW/TF-IDF Vectorization, we get a latent representation
    of different search term / product groups. This enables our model to distinguish
    between groups and assign different weights to features, therefore solving the
    issue of dependent data and products lacking some features (to an extent).**'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[潜在语义索引](https://en.wikipedia.org/wiki/Latent_semantic_indexing)：通过对从 BOW/TF-IDF
    向量化获得的矩阵进行[SVD 分解](https://en.wikipedia.org/wiki/Singular_value_decomposition)，我们得到不同搜索词/产品组的潜在表示。这使我们的模型能够区分不同组，并对特征分配不同的权重，从而解决了数据依赖和产品缺少某些特征的问题（在一定程度上）。**'
- en: Note that features listed above with `*` are the last batch of features I added.
    The problem is that the model trained on data that included these features performed
    worse than the previous ones. At first I thought that the increase in number of
    features would require re-tuning of model parameters. However, after wasting much
    CPU time on grid search, I still could not beat the old model. I think it might
    be the issue of **feature correlation**mentioned above. I actually knew a solution
    that might work, which is to **combine models trained on different version of
    features by stacking**. Unfortunately I didn’t have enough time to try it. **As
    a matter of fact, most of top teams regard the ensemble of models trained with
    different preprocessing and feature engineering pipelines as a key to success**.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述以`*`标记的特征是我最后添加的一批特征。问题是，包含这些特征的数据训练的模型表现比之前的模型更差。起初我认为特征数量的增加需要重新调整模型参数。然而，在花费了大量
    CPU 时间进行网格搜索后，我仍然无法超越旧模型。我认为这可能是上述**特征相关性**的问题。实际上，我知道一个可能有效的解决方案，即通过堆叠**结合不同特征版本训练的模型**。不幸的是，我没有足够的时间去尝试。**实际上，大多数顶级团队认为，结合不同预处理和特征工程管道训练的模型是成功的关键**。
- en: '**Model**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型**'
- en: At first I was using `RandomForestRegressor` to build my model. Then I tried**Xgboost** and
    it turned out to be more than twice as fast as Sklearn. From that on what I do
    everyday is basically running grid search on my work station while working on
    features on my laptop.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，我使用`RandomForestRegressor`来构建模型。然后我尝试了**Xgboost**，结果发现它比Sklearn快了两倍多。从那时起，我每天的工作基本上是进行网格搜索，同时在笔记本电脑上进行特征工程。
- en: Dataset in this competition is not trivial to validate. It’s not i.i.d. and
    many records are dependent. Many times I used better features / parameters only
    to end with worse LB scores. As repeatedly stated by many accomplished Kagglers,
    you have to trust your own CV score under such situation. Therefore I decided
    to use 10-fold instead of 5-fold in cross validation and ignore the LB score in
    the following attempts.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本次竞赛中的数据集并不容易验证。数据不是独立同分布的，许多记录是相关的。我多次使用更好的特征/参数，结果却得到了更差的LB分数。正如许多成功的Kaggler反复提到的那样，在这种情况下，你必须相信自己的交叉验证分数。因此，我决定在交叉验证中使用10折而不是5折，并在后续尝试中忽略LB分数。
- en: '**Ensemble**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**集成**'
- en: 'My final model is an ensemble consisting of 4 base models:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我的最终模型是由4个基础模型组成的集成模型：
- en: '`RandomForestRegressor`'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomForestRegressor`'
- en: '`ExtraTreesRegressor`'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ExtraTreesRegressor`'
- en: '`GradientBoostingRegressor`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GradientBoostingRegressor`'
- en: '`XGBRegressor`'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`XGBRegressor`'
- en: The stacker is also a `XGBRegressor`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠器也是一个`XGBRegressor`。
- en: The problem is that all my base models are highly correlated (with a lowest
    correlation of 0.9). I thought of including linear regression, SVM regression
    and `XGBRegressor`with linear booster into the ensemble, but these models had
    RMSE scores that are 0.02 higher (this accounts for a gap of hundreds of places
    on the leaderboard) than the 4 models I finally used. Therefore I decided not
    to use more models although they would have brought much more diversity.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是，我所有的基础模型都高度相关（最低相关性为0.9）。我考虑将线性回归、SVM回归和带有线性增强器的`XGBRegressor`包含在集成中，但这些模型的RMSE分数比我最终使用的4个模型高出0.02（这在排行榜上相当于数百名的差距）。因此，我决定不使用更多模型，尽管它们会带来更多的多样性。
- en: The good news is that, despite base models being highly correlated, stacking
    still bumps up my score a lot. **What’s more, my CV score and LB score are in
    complete sync after I started stacking.**
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，尽管基础模型高度相关，堆叠仍然大幅提高了我的分数。**更重要的是，自从我开始堆叠后，我的交叉验证分数和LB分数完全同步。**
- en: During the last two days of the competition, I did one more thing: **use 20
    or so different random seeds to generate the ensemble and take a weighted average
    of them as the final submission**. This is actually a kind of **bagging**. It
    makes sense in theory because in stacking I used 80% of the data to train base
    models in each iteration, whereas 100% of the data is used to train the stacker.
    Therefore it’s less clean. Making multiple runs with different seeds makes sure
    that **different 80% of the data are used each time**, thus reducing the risk
    of information leak. Yet by doing this I only achieved an increase of `0.0004`,
    which might be just due to randomness.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在竞赛的最后两天，我做了一件事：**使用20个不同的随机种子生成集成模型，并将它们的加权平均作为最终提交**。这实际上是一种**袋装法**。理论上是有意义的，因为在堆叠中，我使用80%的数据训练每次迭代的基础模型，而100%的数据用于训练堆叠器。因此，它不够干净。使用不同种子的多次运行确保了**每次使用不同的80%数据**，从而降低了信息泄露的风险。然而，通过这种方式，我只实现了`0.0004`的提高，这可能只是随机性所致。
- en: After the competition, I found out that my best single model scores `0.46378` on
    the private leaderboard, whereas my best stacking ensemble scores `0.45849`. That
    was the difference between the 174th place and the 98th place. In other words,
    feature engineering and model tuning got me into 10%, whereas stacking got me
    into 5%.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 竞赛结束后，我发现我的最佳单模型在私人排行榜上的分数为`0.46378`，而我的最佳堆叠集成模型的分数为`0.45849`。这差距使我从第174名提升到了第98名。换句话说，特征工程和模型调优让我进入了前10%，而堆叠让我进入了前5%。
- en: '**Lessons Learned**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**经验教训**'
- en: 'There’s much to learn from the solutions shared by top teams:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从顶级团队分享的解决方案中可以学到很多：
- en: There’s a pattern in the product title. For example, whether a product is accompanied
    by a certain accessory will be indicated by `With/Without XXX` at the end of the
    title.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品标题中有一个模式。例如，标题末尾的`With/Without XXX`会指示产品是否附带某个配件。
- en: Use external data. For example use [WordNet](https://wordnet.princeton.edu/) or [Reddit
    Comments Dataset](https://www.kaggle.com/reddit/reddit-comments-may-2015) to train
    synonyms and [hypernyms](https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy).
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用外部数据。例如，使用[WordNet](https://wordnet.princeton.edu/)或[Reddit Comments Dataset](https://www.kaggle.com/reddit/reddit-comments-may-2015)来训练同义词和[上位词](https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy)。
- en: Some features based on **letters** instead of **words**. At first I was rather
    confused by this. But it makes perfect sense if you consider it. For example,
    the team that won the 3rd place took the number of letters matched into consideration
    when computing text similarity. They argued that **longer words are more specific
    and thus more likely to be assigned high relevance scores by human**. They also
    used char-by-char sequence comparison (`difflib.SequenceMatcher`) to measure **visual
    similarity**, which they claimed to be important for human.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些特征基于**字母**而不是**词语**。一开始我对此感到困惑，但仔细考虑后，这完全有意义。例如，获得第3名的团队在计算文本相似性时考虑了匹配字母的数量。他们认为**较长的词语更具体，因此更可能被人类分配较高的相关性评分**。他们还使用逐字符序列比较（`difflib.SequenceMatcher`）来测量**视觉相似性**，他们认为这对人类来说很重要。
- en: POS-tag words and find **[head](https://en.wikipedia.org/wiki/Head_(linguistics))** in
    phrases and use them when computing various distance metrics.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对词汇进行词性标注并找出**[主语](https://en.wikipedia.org/wiki/Head_(linguistics))**在短语中，并在计算各种距离度量时使用它们。
- en: Extract top-ranking trigrams from the TF-IDF of product title / description
    field and compute the ratio of word from search terms that appear in these trigrams.
    Vice versa. This is like computing latent indexes from another point of view.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从产品标题/描述字段的TF-IDF中提取排名靠前的三元组，并计算这些三元组中出现的搜索词的比例。反之亦然。这就像从另一个角度计算潜在索引。
- en: Some novel distance metrics like [Word Movers Distance](http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些新颖的距离度量，如[Word Movers Distance](http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf)。
- en: Apart from SVD, some used [NMF](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization).
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了SVD，一些使用了[NMF](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization)。
- en: Generate **pairwise polynomial interactions** between top-ranking features.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成**成对的多项式交互**在排名前列的特征之间。
- en: '**For CV, construct splits in which product IDs do not overlap between training
    set and testing set, and splits in which IDs do. Then we can use these with corresponding
    ratio to approximate the impact of public/private LB split in our local CV.**'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对于交叉验证，构建产品ID在训练集和测试集之间不重叠的拆分，以及ID重叠的拆分。然后我们可以使用这些拆分及其对应的比例来近似公共/私有LB拆分在本地交叉验证中的影响。**'
- en: Summary
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: '**Takeaways**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**收获**'
- en: It was a good call to **start doing ensembles early in the competition**. As
    it turned out, I was still playing with features during the very last days.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 早早开始进行集成学习是个不错的决定。事实证明，我在最后几天仍在玩弄特征。
- en: It’s of high priority that I build a pipeline capable of automatic model training
    and recording best parameters.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高优先级是构建一个能够自动训练模型和记录最佳参数的流程。
- en: '**Features matter the most!** I didn’t spend enough time on features in this
    competition.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征最为重要！** 我在这次竞赛中没有花足够的时间在特征上。'
- en: If possible, spend some time to manually inspect raw data for patterns.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果可能，花些时间手动检查原始数据中的模式。
- en: '**Issues Raised**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**提出的问题**'
- en: Several issues I encountered in this competitions are of high research values.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些竞赛中遇到的几个问题具有很高的研究价值。
- en: How to do reliable CV with dependent data.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何使用依赖数据进行可靠的交叉验证。
- en: How to quantify **the trade-off between diversity and accuracy** in ensemble
    learning.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何量化**多样性与准确性之间的权衡**在集成学习中的影响。
- en: How to deal with feature interaction which harms the model’s performance. And**how
    to determine whether new features are effective in such situations**.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何处理特征交互对模型性能的损害，以及**如何确定在这种情况下新特征是否有效**。
- en: '**Beginner Tips**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**初学者提示**'
- en: Choose a competition you’re interested in. **It would be better if you’ve already
    have some insights about the problem domain.**
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个你感兴趣的竞赛。**如果你对问题领域已有一些见解，那就更好了。**
- en: Following my approach or somebody else’s, start exploring, understanding and
    modeling data.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遵循我的方法或其他人的方法，开始探索、理解和建模数据。
- en: Learn from forum and scripts. See how others interpret data and construct features.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从论坛和脚本中学习。看看其他人如何解释数据和构建特征。
- en: '**Find winner interviews / blog posts of previous competitions. They’re extremely
    helpful, especially if from competitions that share some similarities with that
    one you’re working on.**'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**找到之前比赛的获胜者访谈/博客文章。它们非常有帮助，特别是那些与你正在做的比赛有一些相似之处的比赛。**'
- en: Start doing ensemble after you have reached a pretty good score (e.g. 10% ~
    20%) or you feel that there isn’t much room for new features (which, sadly, always
    turns out to be false).
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你取得了相当不错的分数（例如 10% ~ 20%）或你觉得新特征的空间不大时（这通常是错误的），开始做集成方法。
- en: If you think you may have a chance to win the prize, try teaming up!
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你认为你可能有机会赢得奖品，尝试组队！
- en: '**Don’t give up until the end of the competition. At least try something new
    every day.**'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**不要在比赛结束前放弃。至少每天尝试一些新的东西。**'
- en: Learn from the sharings of top teams after the competition. Reflect on your
    approaches. **If possible, spend some time verifying what you learn.**
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从比赛结束后的顶级团队分享中学习。反思你的方法。**如果可能，花些时间验证你所学到的东西。**
- en: Get some rest!
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 休息一下吧！
- en: Reference
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考
- en: '[Beating Kaggle the Easy Way - Dong Ying](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiPxZHewLbMAhVKv5QKHb3PCGwQFggcMAA&url=http%3A%2F%2Fwww.ke.tu-darmstadt.de%2Flehre%2Farbeiten%2Fstudien%2F2015%2FDong_Ying.pdf&usg=AFQjCNE9o2BcEkqdnu_-lQ3EFD3eRAFWiw&sig2=oiU8TCEH57EYF9v9l6Scrw&bvm=bv.121070826,d.dGo)'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[以简单的方式击败 Kaggle - 董颖](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiPxZHewLbMAhVKv5QKHb3PCGwQFggcMAA&url=http%3A%2F%2Fwww.ke.tu-darmstadt.de%2Flehre%2Farbeiten%2Fstudien%2F2015%2FDong_Ying.pdf&usg=AFQjCNE9o2BcEkqdnu_-lQ3EFD3eRAFWiw&sig2=oiU8TCEH57EYF9v9l6Scrw&bvm=bv.121070826,d.dGo)'
- en: '[Search Results Relevance Winner’s Interview: 1st place, Chenglong Chen](https://github.com/ChenglongChen/Kaggle_CrowdFlower/blob/master/BlogPost/BlogPost.md)'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[搜索结果相关性获胜者访谈：第一名，陈成龙](https://github.com/ChenglongChen/Kaggle_CrowdFlower/blob/master/BlogPost/BlogPost.md)'
- en: '[(Chinese) Solution for Prudential Life Insurance Assessment - Nutastray](http://rstudio-pubs-static.s3.amazonaws.com/158725_5d2f977f4004490e9b095c0ef9357c6b.html)'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[(中文) 预测人寿保险评估的解决方案 - Nutastray](http://rstudio-pubs-static.s3.amazonaws.com/158725_5d2f977f4004490e9b095c0ef9357c6b.html)'
- en: '![Linghao Zhang](../Images/47effce75b02d75ca99360d2793970ea.png)**Bio: [Linghao
    Zhang](https://www.linkedin.com/in/linghaozh)** is a senior year Computer Science
    student at Fudan University and Data Mining Engineer at Strikingly. His interests
    include machine learning, data mining, natural language processing, knowledge
    graphs, and big data analytics.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '![Linghao Zhang](../Images/47effce75b02d75ca99360d2793970ea.png)**简介： [Linghao
    Zhang](https://www.linkedin.com/in/linghaozh)** 是复旦大学计算机科学的高年级学生和 Strikingly 的数据挖掘工程师。他的兴趣包括机器学习、数据挖掘、自然语言处理、知识图谱和大数据分析。'
- en: '[Original](https://dnc1994.com/2016/05/rank-10-percent-in-first-kaggle-competition-en/).
    Reposted with permission.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://dnc1994.com/2016/05/rank-10-percent-in-first-kaggle-competition-en/)。经授权转载。'
- en: '**Related:**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Approaching (Almost) Any Machine Learning Problem](/2016/08/approaching-almost-any-machine-learning-problem.html)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[解决（几乎）任何机器学习问题](/2016/08/approaching-almost-any-machine-learning-problem.html)'
- en: '[Automated Data Science & Machine Learning: An Interview with the Auto-sklearn
    Team](/2016/10/interview-auto-sklearn-automated-data-science-machine-learning-team.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动化数据科学与机器学习：与 Auto-sklearn 团队的采访](/2016/10/interview-auto-sklearn-automated-data-science-machine-learning-team.html)'
- en: '[Data Science Basics: An Introduction to Ensemble Learners](/2016/11/data-science-basics-intro-ensemble-learners.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学基础：集成学习者简介](/2016/11/data-science-basics-intro-ensemble-learners.html)'
- en: More On This Topic
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[How LinkedIn Uses Machine Learning To Rank Your Feed](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LinkedIn 如何使用机器学习来排名你的动态](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)'
- en: '[How to Get Your First Job in Data Science without Any Work Experience](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在没有工作经验的情况下获得你的第一份数据科学工作](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)'
- en: '[Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 TensorFlow 和 Keras 构建并训练你的第一个神经网络](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
- en: '[It''s alive! Build your first robots with Python and some cheap,…](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[它活了！用 Python 和一些便宜的基础组件构建你的第一个机器人](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
- en: '[From Zero to Hero: Create Your First ML Model with PyTorch](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从零到英雄：使用 PyTorch 创建你的第一个 ML 模型](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)'
- en: '[Deploying Your First Machine Learning Model](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[部署你的第一个机器学习模型](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)'
