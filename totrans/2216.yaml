- en: 'Unveiling Hidden Patterns: An Introduction to Hierarchical Clustering'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/unveiling-hidden-patterns-an-introduction-to-hierarchical-clustering](https://www.kdnuggets.com/unveiling-hidden-patterns-an-introduction-to-hierarchical-clustering)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Unveiling Hidden Patterns: An Introduction to Hierarchical Clustering](../Images/80acf3364dfb9a8aa29ab8b5d8f2be6d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: When you’re familiarizing yourself with the unsupervised learning paradigm,
    you'll learn about clustering algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of clustering is often to understand patterns in the given unlabeled
    dataset. Or it can be to *find* groups in the dataset—and label them—so that we
    can perform supervised learning on the now-labeled dataset. This article will
    cover the basics of hierarchical clustering.
  prefs: []
  type: TYPE_NORMAL
- en: What Is Hierarchical Clustering?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Hierarchical clustering** algorithm aims at finding similarity between instances—quantified
    by a distance metric—to group them into segments called clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the algorithm is to find clusters such that data points in a cluster
    are *more similar* to each other than they are to data points in other clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two common hierarchical clustering algorithms, each with its own
    approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Agglomerative Clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Divisive Clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agglomerative Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Suppose there are n distinct data points in the dataset. Agglomerative clustering
    works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with n clusters; each data point is a cluster in itself.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Group data points together based on *similarity* between them. Meaning similar
    clusters are merged depending on the distance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step 2 until there is *only one* cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divisive Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As the name suggests, divisive clustering tries to perform the inverse of agglomerative
    clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: All the n data points are in a single cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divide this single large cluster into smaller groups. Note that the grouping
    together of data points in agglomerative clustering is based on similarity. But
    splitting them into different clusters is based on dissimilarity; data points
    in different clusters are dissimilar to each other.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat until each data point is a cluster in itself.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distance Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned, the *similarity* between data points is quantified using *distance*.
    [Commonly used distance metrics](/2023/03/distance-metrics-euclidean-manhattan-minkowski-oh.html)
    include the Euclidean and Manhattan distance.
  prefs: []
  type: TYPE_NORMAL
- en: 'For any two data points in the n-dimensional feature space, the Euclidean distance
    between them given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unveiling Hidden Patterns: An Introduction to Hierarchical Clustering](../Images/b696770b49515a67996ec128a16a0874.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another commonly used distance metric is the Manhattan distance given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unveiling Hidden Patterns: An Introduction to Hierarchical Clustering](../Images/f5f80a94b1350afe1f943d1015542869.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Minkowski distance is a generalization—for a general p >= 1—of these distance
    metrics in an n-dimensional space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unveiling Hidden Patterns: An Introduction to Hierarchical Clustering](../Images/fbadb5f9710a995262107b914de161b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Distance Between Clusters: Understanding Linkage Criteria'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the distance metrics, we can compute the distance between any two data
    points in the dataset. But you also need to define a distance to determine “how”
    to group together clusters at each step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that at each step in agglomerative clustering, we pick the *two closest
    groups* to merge. This is captured by the **linkage** criterion. And the commonly
    used linkage criteria include:'
  prefs: []
  type: TYPE_NORMAL
- en: Single linkage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complete linkage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average linkage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ward’s linkage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single Linkage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In **single linkage** or single-link clustering, the distance between two groups/clusters
    is taken as the *smallest* distance between all pairs of data points in the two
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '![Unveiling Hidden Patterns: An Introduction to Hierarchical Clustering](../Images/d8456e47634a5f2217d4492924dfb390.png)'
  prefs: []
  type: TYPE_IMG
- en: Complete Linkage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In **complete linkage** or **complete-link clustering**, the distance between
    two clusters is chosen as the *largest* distance between all pairs of points in
    the two clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '![Unveiling Hidden Patterns: An Introduction to Hierarchical Clustering](../Images/b9b9d53bb114c70cd07bbe6183700a06.png)'
  prefs: []
  type: TYPE_IMG
- en: Average Linkage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes **average linkage** is used which uses the average of the distances
    between all pairs of data points in the two clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Ward’s Linkage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ward''s linkage aims to *minimize the variance* within the merged clusters:
    merging clusters should minimize the overall increase in variance after merging.
    This leads to more compact and well-separated clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: The distance between two clusters is calculated by considering the *increase*
    in the total sum of squared deviations (variance) from the mean of the merged
    cluster. The idea is to measure *how much* the variance of the merged cluster
    increases compared to the variance of the individual clusters before merging.
  prefs: []
  type: TYPE_NORMAL
- en: When we code hierarchical clustering in Python, we’ll use Ward’s linkage, too.
  prefs: []
  type: TYPE_NORMAL
- en: What Is a Dendrogram?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can visualize the result of clustering as a **dendrogram**. It is a **hierarchical
    tree structure** that helps us understand how the data points—and subsequently
    clusters—are grouped or merged together as the algorithm proceeds.
  prefs: []
  type: TYPE_NORMAL
- en: In the hierarchical tree structure, the **leaves** denote the *instances* or
    the *data points* in the data set. The corresponding distances at which the merging
    or grouping occurs can be inferred from the y-axis.
  prefs: []
  type: TYPE_NORMAL
- en: '![Unveiling Hidden Patterns: An Introduction to Hierarchical Clustering](../Images/2ed5c7075f4f629251985296b0d78120.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample Dendrogram | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Because the type of linkage determines *how* the data points are grouped together,
    different linkage criteria yield different dendrograms.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the distance, we can use the dendrogram—cut or slice it at a specific
    point—to get the required number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike some clustering algorithms like K-Means clustering, hierarchical clustering
    does not require you to specify the number of clusters beforehand. However, agglomerative
    clustering can be computationally very expensive when working with large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical Clustering in Python with SciPy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we’ll perform hierarchical clustering on the built-in [wine dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html)—one
    step at a time. To do so, we’ll leverage the [clustering package](https://docs.scipy.org/doc/scipy/reference/cluster.html)—**scipy.cluster**—from
    SciPy.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – Import Necessary Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let''s import the libraries and the necessary modules from the libraries
    scikit-learn and SciPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Step 2 – Load and Preprocess the Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we load the *wine dataset* into a pandas dataframe. It is a simple dataset
    that is part of scikit-learn’s `datasets` and is helpful in exploring hierarchical
    clustering.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check the first few rows of the dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Unveiling Hidden Patterns: An Introduction to Hierarchical Clustering](../Images/c0c4611ee5de3f18675a278a6db72ec9.png)'
  prefs: []
  type: TYPE_IMG
- en: Truncated output of wine_df.head()
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we’ve loaded only the features—and not the output label—so that
    we can peform clustering to discover groups in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check the shape of the dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'There are 178 records and 14 features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Because the data set contains numeric values that are spread across different
    ranges, let's preprocess the dataset. We’ll use `MinMaxScaler` to transform each
    of the features to take on values in the range [0, 1].
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Step 3 – Perform Hierarchical Clustering and Plot the Dendrogram
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s compute the linkage matrix, perform clustering, and plot the dendrogram.
    We can use `linkage` from the [hierarchy](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html#module-scipy.cluster.hierarchy)
    module to calculate the linkage matrix based on Ward’s linkage (set `method` to
    'ward').
  prefs: []
  type: TYPE_NORMAL
- en: As discussed, Ward’s linkage minimizes the variance within each cluster. We
    then plot the dendrogram to visualize the hierarchical clustering process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Because we haven't (yet) truncated the dendrogram, we get to visualize how each
    of the 178 data points are grouped together into a single cluster. Though this
    is seemingly difficult to interpret, we can still see that there are *three* different
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '![Unveiling Hidden Patterns: An Introduction to Hierarchical Clustering](../Images/d9893a60d0db9f9489b5f1c5458b5c89.png)'
  prefs: []
  type: TYPE_IMG
- en: Truncating the Dendrogram for Easier Visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In practice, instead of the entire dendrogram, we can visualize a truncated
    version that's easier to interpret and understand.
  prefs: []
  type: TYPE_NORMAL
- en: To truncate the dendrogram, we can set `truncate_mode` to 'level' and `p = 3`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Doing so will truncate the dendrogram to include only those clusters which are
    *within 3 levels* from the final merge.
  prefs: []
  type: TYPE_NORMAL
- en: '![Unveiling Hidden Patterns: An Introduction to Hierarchical Clustering](../Images/2ed5c7075f4f629251985296b0d78120.png)'
  prefs: []
  type: TYPE_IMG
- en: In the above dendrogram, you can see that some data points such as 158 and 159
    are represented individually. Whereas some others are mentioned within parentheses;
    these are *not* individual data points but the *number of data points* in a cluster.
    (k) denotes a cluster with k samples.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – Identify the Optimal Number of Clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dendrogram helps us choose the optimal number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: We can observe where the distance along the y-axis *increases drastically*,
    choose to truncate the dendrogram at that point—and use the distance as the threshold
    to form clusters.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, the optimal number of clusters is 3.
  prefs: []
  type: TYPE_NORMAL
- en: '![Unveiling Hidden Patterns: An Introduction to Hierarchical Clustering](../Images/5e536cf4ad699bd40121b7c2e8853f0e.png)'
  prefs: []
  type: TYPE_IMG
- en: Step 5 – Form the Clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have decided on the optimal number of clusters, we can use the corresponding
    distance along the y-axis—a threshold distance. This ensures that above the threshold
    distance, the clusters are no longer merged. We choose a `threshold_distance`
    of 3.5 (as inferred from the dendrogram).
  prefs: []
  type: TYPE_NORMAL
- en: 'We then use `fcluster` with `criterion` set to ''distance'' to get the cluster
    assignment for all the data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You should now be able to see the cluster labels (one of {1, 2, 3}) for all
    the data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Step 6 – Visualize the Clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that each data point has been assigned to a cluster, you can visualize
    a subset of features and their cluster assignments. Here''s the scatter plot of
    two such features along with their cluster mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Unveiling Hidden Patterns: An Introduction to Hierarchical Clustering](../Images/6fc4e9e449c353fb847d93e281783f9c.png)'
  prefs: []
  type: TYPE_IMG
- en: Wrapping Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: And that's a wrap! In this tutorial, we used SciPy to perform hierarchical clustering
    just so we can cover the steps involved in greater detail. Alternatively, you
    can also use the [AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)
    class from scikit-learn’s *cluster* module. Happy coding clustering!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] [Introduction to Machine Learning](https://mitpress.mit.edu/9780262043793/introduction-to-machine-learning/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [An Introduction to Statistical Learning (ISLR)](https://www.statlearning.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Bala Priya C](https://www.linkedin.com/in/bala-priya/)** is a developer
    and technical writer from India. She likes working at the intersection of math,
    programming, data science, and content creation. Her areas of interest and expertise
    include DevOps, data science, and natural language processing. She enjoys reading,
    writing, coding, and coffee! Currently, she''s working on learning and sharing
    her knowledge with the developer community by authoring tutorials, how-to guides,
    opinion pieces, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Clustering Unleashed: Understanding K-Means Clustering](https://www.kdnuggets.com/2023/07/clustering-unleashed-understanding-kmeans-clustering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Use MultiIndex for Hierarchical Data Organization in Pandas](https://www.kdnuggets.com/how-to-use-multiindex-for-hierarchical-data-organization-in-pandas)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introduction to Clustering in Python with PyCaret](https://www.kdnuggets.com/2021/12/introduction-clustering-python-pycaret.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unveiling Midjourney 5.2: A Leap Forward in AI Image Generation](https://www.kdnuggets.com/2023/06/unveiling-midjourney-52-leap-forward.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unveiling the Potential of CTGAN: Harnessing Generative AI for…](https://www.kdnuggets.com/2023/04/unveiling-potential-ctgan-harnessing-generative-ai-synthetic-data.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unveiling Unsupervised Learning](https://www.kdnuggets.com/unveiling-unsupervised-learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
