["```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as  optim\n\nfrom torch.utils.data import DataLoader\n\n# Using MNIST dataset provided by PyTorch\nfrom torchvision.datasets.mnist import MNIST\nimport torchvision.transforms as transforms\n\n# Import Model implemented in a different file\nfrom model import Classifier\n\nimport matplotlib.pyplot as plt\n```", "```py\nINPUT_SIZE = 784\t# Flattened 28x28 images\nNUM_CLASSES = 10\t# 0-9 hand-written digits.\nBATCH_SIZE = 128\t# Using Mini-Batches for Training\nLEARNING_RATE = 0.01\t# Opitimizer Step\nNUM_EPOCHS = 5  \t# Total Training Epochs\n```", "```py\ndata_transforms = transforms.Compose([\n    \ttransforms.ToTensor(),\n    \ttransforms.Lambda(lambda x: torch.flatten(x))\n])\n\ntrain_dataset = MNIST(root=\".data/\", train=True, download=True, transform=data_transforms)\n\ntest_dataset = MNIST(root=\".data/\", train=False, download=True, transform=data_transforms)\n```", "```py\ndata_transforms = transforms.Compose([\n    \ttransforms.ToTensor(),\n    \ttransforms.Lambda(lambda x: torch.flatten(x))\n])\n```", "```py\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n```", "```py\nimport torch\nimport torch.nn as nn\n\nclass Classifier(nn.Module):\n\tdef __init__(\n        \tself,\n        \tinput_size:int,\n        \tnum_classes:int\n    \t) -> None:\n    \tsuper().__init__()\n    \t  self.input_layer = nn.Linear(input_size, 512)\n    \t  self.hidden_1 = nn.Linear(512, 256)\n    \t  self.hidden_2 = nn.Linear(256, 128)\n    \t  self.output_layer = nn.Linear(128, num_classes)\n\n    \tself.activation = nn.ReLU()\n\n\tdef forward(self, x):\n    \t  # Pass Input Sequentially through each dense layer and activation\n    \t  x = self.activation(self.input_layer(x))\n    \t  x = self.activation(self.hidden_1(x))\n    \t  x = self.activation(self.hidden_2(x))\n    \t  return self.output_layer(x)\n```", "```py\nmodel = Classifier(input_size=784, num_classes=10)\nmodel.to(DEVICE)\n```", "```py\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n```", "```py\nfor epoch in range(NUM_EPOCHS):\n    \tfor batch in iter(train_dataloader):\n          # Train the Model for each batch. \n```", "```py\nfor epoch in range(NUM_EPOCHS):\n    \tfor batch in iter(train_dataloader):\n        \timages, labels = batch # Separate inputs and labels\n        \t# Convert Tensor Hardware Devices to either GPU or CPU\n        \timages = images.to(DEVICE)\n        \tlabels = labels.to(DEVICE)\n\n        \t# Calls the model.forward() function to generate predictions \n        \tpredictions = model(images)\n```", "```py\n# Calculate Cross Entropy Loss\nloss = criterion(predictions, labels)\n# Clears gradient values from previous batch\noptimizer.zero_grad()\n# Computes backprop gradient based on the loss\nloss.backward()\n# Optimizes the model weights\noptimizer.step()\n```", "```py\nfor epoch in range(NUM_EPOCHS):\n    \ttotal_epoch_loss = 0\n    \tsteps = 0\n    \tfor batch in iter(train_dataloader):\n        \timages, labels = batch # Separate inputs and labels\n        \t# Convert Tensor Hardware Devices to either GPU or CPU\n        \timages = images.to(DEVICE)\n        \tlabels = labels.to(DEVICE)\n\n        \t# Calls the model.forward() function to generate predictions       \t \n        \tpredictions = model(images)\n\n        \t# Calculate Cross Entropy Loss\n        \tloss = criterion(predictions, labels)\n        \t# Clears gradient values from previous batch\n        \toptimizer.zero_grad()\n        \t# Computes backprop gradient based on the loss\n        \tloss.backward()\n        \t# Optimizes the model weights\n        \toptimizer.step()\n\n        \tsteps += 1\n        \ttotal_epoch_loss += loss.item()\n\n    \tprint(f'Epoch: {epoch + 1} / {NUM_EPOCHS}: Average Loss: {total_epoch_loss / steps}')\n```", "```py\nfor batch in iter(test_dataloader):\n    \timages, labels = batch\n    \timages = images.to(DEVICE)\n    \tlabels = labels.to(DEVICE)\n\n    \tpredictions = model(images)\n\n    \t# Taking the predicted label with highest probability\n    \tpredictions = torch.argmax(predictions, dim=1)\n\n    \tcorrect_predictions += (predictions == labels).sum().item()\n    \ttotal_predictions += labels.shape[0]\n\nprint(f\"\\nTEST ACCURACY: {((correct_predictions / total_predictions) * 100):.2f}\")\n```", "```py\nimport torch\nimport torch.nn as nn\n\nclass Classifier(nn.Module):\n\tdef __init__(\n        \tself,\n        \tinput_size:int,\n        \tnum_classes:int\n    \t) -> None:\n    \tsuper().__init__()\n    \t  self.input_layer = nn.Linear(input_size, 512)\n    \t  self.hidden_1 = nn.Linear(512, 256)\n    \t  self.hidden_2 = nn.Linear(256, 128)\n    \t  self.output_layer = nn.Linear(128, num_classes)\n\n    \t  self.activation = nn.ReLU()\n\n\tdef forward(self, x):\n    \t  # Pass Input Sequentially through each dense layer and activation\n    \t  x = self.activation(self.input_layer(x))\n    \t  x = self.activation(self.hidden_1(x))\n    \t  x = self.activation(self.hidden_2(x))\n    \t  return self.output_layer(x)\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as  optim\n\nfrom torch.utils.data import DataLoader\n\n# Using MNIST dataset provided by PyTorch\nfrom torchvision.datasets.mnist import MNIST\nimport torchvision.transforms as transforms\n\n# Import Model implemented in a different file\nfrom model import Classifier\n\nimport matplotlib.pyplot as plt\n\nif __name__ == \"__main__\":\n\n\tINPUT_SIZE = 784\t# Flattened 28x28 images\n\tNUM_CLASSES = 10\t# 0-9 hand-written digits.\n\tBATCH_SIZE = 128\t# Using Mini-Batches for Training\n\tLEARNING_RATE = 0.01\t# Opitimizer Step\n\tNUM_EPOCHS = 5  \t# Total Training Epochs\n\n\tDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n\t# Will be used to convert Images to PyTorch Tensors\n\tdata_transforms = transforms.Compose([\n    \ttransforms.ToTensor(),\n    \ttransforms.Lambda(lambda x: torch.flatten(x))\n\t])\n\n\ttrain_dataset = MNIST(root=\".data/\", train=True, download=True, transform=data_transforms)\n\ttest_dataset = MNIST(root=\".data/\", train=False, download=True, transform=data_transforms)    \n\n\ttrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\ttest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n\tmodel = Classifier(input_size=784, num_classes=10)\n\tmodel.to(DEVICE)\n\n\tcriterion = nn.CrossEntropyLoss()\n\toptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n\tfor epoch in range(NUM_EPOCHS):\n    \ttotal_epoch_loss = 0\n    \tsteps = 0\n    \tfor batch in iter(train_dataloader):\n        \timages, labels = batch # Separate inputs and labels\n        \t# Convert Tensor Hardware Devices to either GPU or CPU\n        \timages = images.to(DEVICE)\n        \tlabels = labels.to(DEVICE)\n\n        \t# Calls the model.forward() function to generate predictions       \t \n        \tpredictions = model(images)\n\n        \t# Calculate Cross Entropy Loss\n        \tloss = criterion(predictions, labels)\n        \t# Clears gradient values from previous batch\n        \toptimizer.zero_grad()\n        \t# Computes backprop gradient based on the loss\n        \tloss.backward()\n        \t# Optimizes the model weights\n        \toptimizer.step()\n\n        \tsteps += 1\n        \ttotal_epoch_loss += loss.item()\n\n    \tprint(f'Epoch: {epoch + 1} / {NUM_EPOCHS}: Average Loss: {total_epoch_loss / steps}')\n\t# Save Trained Model\n\ttorch.save(model.state_dict(), 'trained_model.pth')\n\n\tmodel.eval()\n\tcorrect_predictions = 0\n\ttotal_predictions = 0\n\tfor batch in iter(test_dataloader):\n    \timages, labels = batch\n    \timages = images.to(DEVICE)\n    \tlabels = labels.to(DEVICE)\n\n    \tpredictions = model(images)\n\n    \t# Taking the predicted label with highest probability\n    \tpredictions = torch.argmax(predictions, dim=1)\n\n    \tcorrect_predictions += (predictions == labels).sum().item()\n    \ttotal_predictions += labels.shape[0]\n\n\tprint(f\"\\nTEST ACCURACY: {((correct_predictions / total_predictions) * 100):.2f}\")\n\n\t# --  Code For Plotting Results  -- #\n\n\tbatch = next(iter(test_dataloader))\n\timages, labels = batch\n\n\tfig, ax = plt.subplots(nrows=1, ncols=4, figsize=(16,8))\n\tfor i in range(4):\n    \timage = images[i]\n    \tprediction = torch.softmax(model(image), dim=0)\n    \tprediction = torch.argmax(prediction, dim=0)\n    \t# print(type(prediction), type(prediction.item()))\n    \tax[i].imshow(image.view(28,28))\n    \tax[i].set_title(f'Prediction: {prediction.item()}')\n\tplt.show()\n```"]