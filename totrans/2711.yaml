- en: How to Evaluate the Performance of Your Machine Learning Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/09/performance-machine-learning-model.html](https://www.kdnuggets.com/2020/09/performance-machine-learning-model.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Saurabh Raj](https://www.linkedin.com/in/saurabh-raj-445821153/), IIT
    Jammu**.'
  prefs: []
  type: TYPE_NORMAL
- en: Why evaluation is necessary?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Let me start with a very simple example.
  prefs: []
  type: TYPE_NORMAL
- en: '*Robin and Sam both started preparing for an entrance exam for engineering
    college. They both shared a room and put an equal amount of hard work while solving
    numerical problems. They both studied almost the same hours for the entire year
    and appeared in the final exam. Surprisingly, Robin cleared, but Sam did not.
    When asked, we got to know that there was one difference in their strategy of
    preparation, “test series.” Robin had joined a test series, and he used to test
    his knowledge and understanding by giving those exams and then further evaluating
    where is he lagging. But Sam was confident, and he just kept training himself.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the same fashion, as discussed above, a machine learning model can be trained
    extensively with many parameters and new techniques, but as long as you are skipping
    its evaluation, you cannot trust it.
  prefs: []
  type: TYPE_NORMAL
- en: '**How to read the Confusion Matrix?**'
  prefs: []
  type: TYPE_NORMAL
- en: A **confusion matrix** is a correlation between the predictions of a model and
    the actual class labels of the data points.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/30f2fc2e52391c8308b3603b4a398421.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Confusion Matrix for a Binary Classification.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say you are building a model that detects whether a person has diabetes
    or not. After the train-test split, you got a test set of length 100, out of which
    70 data points are labeled positive (1), and 30 data points are labelled negative
    (0). Now let me draw the matrix for your test prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07a2cf18b521e3d4d6f037e3e3b3c416.png)'
  prefs: []
  type: TYPE_IMG
- en: Out of 70 actual positive data points, your model predicted 64 points as positive
    and 6 as negative. Out of 30 actual negative points, it predicted 3 as positive
    and 27 as negative.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** In the notations, **True Positive, True Negative, False Positive,
    & False Negative**, notice that the second term (Positive or Negative) is denoting
    your prediction, and the first term denotes whether you predicted right or wrong.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the above matrix, we can define some very important ratios:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TPR (True Positive Rate) = ( True Positive / Actual Positive )**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TNR (True Negative Rate) = ( True Negative/ Actual Negative)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FPR (False Positive Rate) = ( False Positive / Actual Negative )**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FNR (False Negative Rate) = ( False Negative / Actual Positive )**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For our case of diabetes detection model, we can calculate these ratios:'
  prefs: []
  type: TYPE_NORMAL
- en: TPR = 91.4%
  prefs: []
  type: TYPE_NORMAL
- en: TNR = 90%
  prefs: []
  type: TYPE_NORMAL
- en: FPR = 10%
  prefs: []
  type: TYPE_NORMAL
- en: FNR = 8.6%
  prefs: []
  type: TYPE_NORMAL
- en: If you want your model to be smart, then your model has to predict correctly.
    This means your **True Positives **and**True Negatives** *should be as high as
    possible*, and at the same time, you need to minimize your mistakes for which
    your **False Positives **and**False Negatives** *should be as low as possible.*Also
    in terms of ratios, your **TPR & TNR** *should be very high*whereas**FPR & FNR**
    *should be very low***,**
  prefs: []
  type: TYPE_NORMAL
- en: '**A smart model:** TPR ↑ , TNR ↑, FPR ↓, FNR ↓'
  prefs: []
  type: TYPE_NORMAL
- en: '**A dumb model:** Any other combination of TPR, TNR, FPR, FNR'
  prefs: []
  type: TYPE_NORMAL
- en: One may argue that it is not possible to take care of all four ratios equally
    because, at the end of the day, no model is perfect. Then what should we do?
  prefs: []
  type: TYPE_NORMAL
- en: Yes, it is true. So that is why we build a model keeping the domain in our mind.
    There are certain domains that demand us to keep a specific ratio as the main
    priority, even at the cost of other ratios being poor. For example, in cancer
    diagnosis, we cannot miss any positive patient at any cost. So we are supposed
    to keep TPR at the maximum and FNR close to 0\. Even if we predict any healthy
    patient as diagnosed, it is still okay as he can go for further check-ups.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Accuracy is what its literal meaning says, a measure of how accurate your model
    is.
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy = Correct Predictions / Total Predictions**'
  prefs: []
  type: TYPE_NORMAL
- en: '**By using confusion matrix, Accuracy = (TP + TN)/(TP+TN+FP+FN)**'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy is one of the simplest performance metrics we can use. But let me warn
    you, accuracy can sometimes lead you to false illusions about your model, and
    hence you should first know your data set and algorithm used then only decide
    whether to use accuracy or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before going to the failure cases of accuracy, let me introduce you with two
    types of data sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Balanced:**A data set that contains almost equal entries for all labels/classes.
    E.g., out of 1000 data points, 600 are positive, and 400 are negative.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Imbalanced:**A data set that contains a biased distribution of entries towards
    a particular label/class. E.g., out of 1000 entries, 990 are positive class, 10
    are negative class.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Very Important: Never use accuracy as a measure when dealing with imbalanced
    test set.**'
  prefs: []
  type: TYPE_NORMAL
- en: Why?
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have an imbalanced test set of 1000 entries with **990 (+ve)** and **10
    (-ve)**. And somehow, you ended up creating a poor model which always predicts
    “+ve” due to the imbalanced train set. Now when you predict your test set labels,
    it will always predict “+ve.” So out of 1000 test set points, you get 1000 “+ve”
    predictions. Then your accuracy would come,
  prefs: []
  type: TYPE_NORMAL
- en: 990/1000 = 99%
  prefs: []
  type: TYPE_NORMAL
- en: Whoa! Amazing! You are happy to see such an awesome accuracy score.
  prefs: []
  type: TYPE_NORMAL
- en: But, you should know that your model is really poor because it always predicts
    “+ve” label.
  prefs: []
  type: TYPE_NORMAL
- en: '**Very Important: Also, we cannot compare two models that return probability
    scores and have the same accuracy.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are certain models that give the probability of each data point for belonging
    to a particular class like that in Logistic Regression. Let us take this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05861a474f677b329bfc620bf8dc6030.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 1.*'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, **If P(Y=1) > 0.5, it predicts class 1.** When we calculate
    accuracy for both M1 and M2, it comes out the same, but it is quite evident that** M1
    is a much better model than M2 by taking a look at the probability scores.**
  prefs: []
  type: TYPE_NORMAL
- en: This issue is beautifully dealt with by **Log Loss**, which I explain later
    in the blog.
  prefs: []
  type: TYPE_NORMAL
- en: Precision & Recall
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Precision:** It is the ratio of True Positives (TP) and the total positive
    predictions. Basically, it tells us how many times your positive prediction was
    actually positive.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf09000fabffe30a8aad38e173e9a92c.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Recall :** It is nothing but TPR (True Positive Rate explained above). It
    tells us about out of all the positive points how many were predicted positive.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b57cd45c6855c191615d9787865b7b00.png)'
  prefs: []
  type: TYPE_IMG
- en: '**F-Measure:** Harmonic mean of precision and recall.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/22ffa6e5026b2bb424756c20378fb9fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To understand this, let’s see this example: When you ask a query in google,
    it returns 40 pages, but only 30 were relevant. But your friend, who is an employee
    at Google, told you that there were 100 total relevant pages for that query. So
    it’s precision is 30/40 = 3/4 = 75% while it’s recall is 30/100 = 30%. So, in
    this case, precision is “how useful the search results are,” and recall is “how
    complete the results are.”'
  prefs: []
  type: TYPE_NORMAL
- en: ROC & AUC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Receiver Operating Characteristic Curve (ROC):**'
  prefs: []
  type: TYPE_NORMAL
- en: It is a **plot between TPR (True Positive Rate) and FPR (False Positive Rate)** calculated
    by taking multiple threshold values from the reverse sorted list of probability
    scores given by a model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a8cecf6aa0a8e5bd17da5c886e1826a6.png)'
  prefs: []
  type: TYPE_IMG
- en: '*A typical ROC curve.*'
  prefs: []
  type: TYPE_NORMAL
- en: Now, how do we plot ROC?
  prefs: []
  type: TYPE_NORMAL
- en: To answer this, let me take you back to Table 1 above. Just consider the M1
    model. You see, for all x values, we have a probability score. In that table,
    we have assigned the data points that have a score of more than 0.5 as class 1\.
    Now sort all the values in descending order of probability scores and one by one
    take threshold values equal to all the probability scores. Then we will have threshold
    values = [0.96,0.94,0.92,0.14,0.11,0.08]. Corresponding to each threshold value,
    predict the classes, and calculate TPR and FPR. You will get 6 pairs of TPR &
    FPR. Just plot them, and you will get the ROC curve.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note: Since the maximum TPR and FPR value is 1, the area under the curve
    (AUC) of ROC lies between 0 and 1.**'
  prefs: []
  type: TYPE_NORMAL
- en: The area under the blue dashed line is 0.5\. AUC = 0 means very poor model,
    AUC = 1 means perfect model. As long as your model’s AUC score is more than 0.5\.
    your model is making sense because even a random model can score 0.5 AUC.
  prefs: []
  type: TYPE_NORMAL
- en: '**Very Important:** You can get very high AUC even in a case of a dumb model
    generated from an imbalanced data set. So always be careful while dealing with
    imbalanced data set.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** AUC had nothing to do with the numerical values probability scores
    as long as the order is maintained. AUC for all the models will be the same as
    long as all the models give the same order of data points after sorting based
    on probability scores.'
  prefs: []
  type: TYPE_NORMAL
- en: Log Loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This performance metric checks the deviation of probability scores of the data
    points from the cut-off score and assigns a penalty proportional to the deviation.
  prefs: []
  type: TYPE_NORMAL
- en: For each data point in a binary classification, we calculate it’s log loss using
    the formula below,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f33815e4c2d63cbb613945cbbc3c354f.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Log Loss formula for a Binary Classification.*'
  prefs: []
  type: TYPE_NORMAL
- en: where p = probability of the data point to belong to class 1 and y is the class
    label (0 or 1).
  prefs: []
  type: TYPE_NORMAL
- en: Suppose if p_1 for some x_1 is 0.95 and p_2 for some x_2 is 0.55 and cut off
    probability for qualifying for class 1 is 0.5\. Then both qualify for class 1,
    but the log loss of p_2 will be much more than the log loss of p_1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c6427275f7d1f6c3954f8c53d9769ba.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from the curve, the range of log loss is [0, infinity).
  prefs: []
  type: TYPE_NORMAL
- en: For each data point in multi-class classification, we calculate it’s log loss
    using the formula below,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ae498217f3f357a2cc9d20c32d61779.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Log Loss formula for multi-class classification.*'
  prefs: []
  type: TYPE_NORMAL
- en: where y(o,c) = 1 if x(o,c) belongs to class 1\. The rest of the concept is the
    same.
  prefs: []
  type: TYPE_NORMAL
- en: Coefficient of Determination
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is denoted by *R².* While predicting target values of the test set, we encounter
    a few errors (e_i), which is the difference between the predicted value and actual
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we have a test set with n entries. As we know, all the data points
    will have a target value, say [y1,y2,y3…….yn]. Let us take the predicted values
    of the test data be [f1,f2,f3,……fn].
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the **Residual Sum of Squares,** which is the sum of all the errors
    (e_i) squared,by using this formula where fi is the predicted target value by
    a model for i’th data point.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e190eeb6f2c5cb8c62d7d67a0f9349ce.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Total Sum of Squares.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the mean of all the actual target values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96866a341b0907ede51ac3a55e06ca01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then calculate the **Total Sum of Squares,** which is proportional to the variance
    of the test set target values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96e43b6be85e7379ea598a204a3d62ae.png)'
  prefs: []
  type: TYPE_IMG
- en: If you observe both the formulas of the sum of squares, you can see that the
    only difference is the 2nd term, i.e., y_bar and fi. The total sum of squares
    somewhat gives us an intuition that it is the same as the residual sum of squares
    only but with predicted values as [ȳ, ȳ, ȳ,…….ȳ ,n times]. Yes, your intuition
    is right. Let’s say there is a very simple mean model that gives the prediction
    of the average of the target values every time irrespective of the input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we formulate R² as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d59ceef42c369d77947ff9d2aa943379.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see now, R² is a metric to compare your model with a very simple
    mean model that returns the average of the target values every time irrespective
    of input data. The comparison has 4 cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**case 1: SS_R = 0**'
  prefs: []
  type: TYPE_NORMAL
- en: (R² = 1) Perfect model with no errors at all.
  prefs: []
  type: TYPE_NORMAL
- en: '**case 2: SS_R > SS_T**'
  prefs: []
  type: TYPE_NORMAL
- en: (R² < 0) Model is even worse than the simple mean model.
  prefs: []
  type: TYPE_NORMAL
- en: '**case 3: SS_R = SS_T**'
  prefs: []
  type: TYPE_NORMAL
- en: (R² = 0) Model is same as the simple mean model.
  prefs: []
  type: TYPE_NORMAL
- en: '**case 4: SS_R < SS_T**'
  prefs: []
  type: TYPE_NORMAL
- en: (0< R² <1) Model is okay.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, in a nutshell, you should know your data set and problem very well, and
    then you can always create a confusion matrix and check for its accuracy, precision,
    recall, and plot the ROC curve and find out AUC as per your needs. But if your
    data set is imbalanced, never use accuracy as a measure. If you want to evaluate
    your model even more deeply so that your probability scores are also given weight,
    then go for Log Loss.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, always evaluate your training!
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/swlh/how-to-evaluate-the-performance-of-your-machine-learning-model-40769784d654).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Idiot’s Guide to Precision, Recall, and Confusion Matrix](https://www.kdnuggets.com/2020/01/guide-precision-recall-confusion-matrix.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using Confusion Matrices to Quantify the Cost of Being Wrong](https://www.kdnuggets.com/2018/10/confusion-matrices-quantify-cost-being-wrong.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Achieving Accuracy with your Training Dataset](https://www.kdnuggets.com/2020/03/supahands-accuracy-training-dataset.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
