# 初学者十大机器学习算法

> 原文：[https://www.kdnuggets.com/2017/10/top-10-machine-learning-algorithms-beginners.html/2](https://www.kdnuggets.com/2017/10/top-10-machine-learning-algorithms-beginners.html/2)

### V. 无监督学习算法：

**6\. Apriori**

Apriori算法用于事务数据库中，以挖掘频繁项集并生成关联规则。它在市场篮子分析中被广泛使用，检查数据库中经常共同出现的产品组合。通常，我们将“如果一个人购买了X商品，那么他也会购买Y商品”的关联规则写作：X -> Y。

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你所在组织的IT需求

* * *

例如：如果一个人购买了牛奶和糖，那么他很可能会购买咖啡粉。这可以写成关联规则的形式：{牛奶,糖} -> 咖啡粉。关联规则在支持度和置信度达到阈值后生成。

![](../Images/644f10b1b1ef53a08eec4a0458b93880.png)

图5：关联规则X->Y的支持度、置信度和提升度公式。[来源](http://chem-eng.utoronto.ca/~datamining/dmc/association_rules.htm)支持度度量有助于减少在频繁项集生成过程中需要考虑的候选项集的数量。这个支持度度量由Apriori原理指导。Apriori原理表明，如果一个项集是频繁的，那么它的所有子集也必须是频繁的。

**7\. K-means**

K-means是一种迭代算法，将相似的数据分组到聚类中。它计算k个聚类的中心点，并将数据点分配给与其中心点距离最小的聚类。

![](../Images/fffb3ddbea2ab8b4c211f642866fbe29.png)

图6：K-means算法的步骤。[来源](https://www.packtpub.com/books/content/clustering-and-other-unsupervised-learning-methods)*步骤1：K-means初始化：*

a) 选择一个k的值。这里，我们取k=3。

b) 随机将每个数据点分配到3个聚类中的任何一个。

c) 计算每个聚类的中心点。红色、蓝色和绿色的星星分别表示3个聚类的中心点。

*步骤2：将每个观察分配到一个聚类中：*

将每个点重新分配到最近的簇质心。这里，上面的5个点被分配到具有蓝色质心的簇。按照相同的程序将点分配到包含红色和绿色质心的簇中。

*步骤 3: 重新计算质心：*

计算新簇的质心。旧的质心由灰色星星表示，而新的质心是红色、绿色和蓝色的星星。

*步骤 4: 迭代，然后如果没有变化则退出。*

重复步骤2-3，直到没有点从一个簇切换到另一个簇。一旦连续2步没有切换，退出k-means算法。

**8\. PCA**

主成分分析（PCA）用于通过减少变量数量来使数据易于探索和可视化。这是通过将数据中的最大方差捕捉到一个新的坐标系统中，该系统的轴称为‘主成分’来完成的。每个成分是原始变量的线性组合，并且彼此正交。成分之间的正交性表示这些成分之间的相关性为零。

第一个主成分捕捉数据中最大变异性的方向。第二个主成分捕捉剩余的方差，但与第一个成分的变量不相关。类似地，所有后续的主成分（PC3、PC4等）捕捉剩余的方差，同时与前一个成分不相关。

![](../Images/d44c1bcdae7f89b678e8c00568b3fdc4.png)

图 7：3个原始变量（基因）被减少为2个新变量，称为主成分（PC）。[来源](http://www.nlpca.org/pca_principal_component_analysis.html)

### VI. 集成学习技术：

集成方法意味着通过投票或平均来结合多个学习者（分类器）的结果，以提高结果。分类时使用投票，回归时使用平均。其思想是，集成学习者的表现优于单一学习者。

有3种集成算法：袋装、提升和堆叠。我们这里不讨论‘堆叠’，但如果你想了解详细解释，请在评论区告诉我，我可以写一篇单独的博客。

**9\. 使用随机森林进行袋装**

随机森林（多个学习者）比袋装决策树（单一学习者）有更好的改进。

Bagging：Bagging 的第一步是使用 Bootstrap Sampling 方法创建多个模型。在 Bootstrap Sampling 中，每个生成的训练集都是由原始数据集中的随机子样本组成的。这些训练集的大小与原始数据集相同，但一些记录会重复出现，而一些记录则根本不出现。然后，整个原始数据集被用作测试集。因此，如果原始数据集的大小是 N，那么每个生成的训练集的大小也是 N，唯一记录的数量约为（2N/3）；测试集的大小也是 N。

Bagging 的第二步是使用相同的算法对不同生成的训练集创建多个模型。在这种情况下，我们讨论随机森林。与决策树不同，在决策树中，每个节点根据最小化错误的最佳特征进行分裂，而在随机森林中，我们选择特征的随机选择来构建最佳分裂。随机性的原因是：即使在 bagging 中，当决策树选择最佳特征进行分裂时，它们最终会具有相似的结构和相关的预测。但在对特征的随机子集进行分裂后进行 bagging，这意味着子树之间的预测相关性较小。

在每个分裂点搜索的特征数量作为参数指定给随机森林算法。

因此，在随机森林的 bagging 中，每棵树都是使用记录的随机样本构建的，每次分裂是使用预测变量的随机样本构建的。

**10\. 使用 AdaBoost 进行提升**

a) Bagging 是并行集成，因为每个模型都是独立构建的。另一方面，提升是顺序集成，其中每个模型是基于纠正前一个模型的误分类来构建的。

b) Bagging 主要涉及“简单投票”，即每个分类器投票以获得最终结果——该结果由平行模型的多数决定；提升涉及“加权投票”，即每个分类器投票以获得最终结果——但顺序模型是通过对前一个模型的误分类实例分配更大权重来构建的。

Adaboost 代表自适应提升。

![](../Images/480921b711d02ec9e399316c1876264d.png)

图 9：决策树的 Adaboost。[来源](https://sebastianraschka.com/faq/docs/bagging-boosting-rf.html)在图 9 中，步骤 1、2、3 涉及一种称为决策树桩的弱学习器（一个基于仅 1 个输入特征的值进行预测的 1 级决策树；根节点直接连接到叶节点的决策树）。构建弱学习器的过程持续进行，直到构建出用户定义数量的弱学习器或在训练过程中没有进一步的改进。步骤 4 结合了之前模型的 3 个决策树桩（因此在决策树中有 3 个分裂规则）。

*步骤 1：从 1 个决策树桩开始，对 1 个输入变量做出决策：*

数据点的大小显示我们已应用相等的权重来分类它们为圆圈或三角形。决策树桩在上半部分生成了一条水平线来分类这些点。我们可以看到有 2 个圆圈被错误预测为三角形。因此，我们将为这 2 个圆圈分配更高的权重，并应用另一个决策树桩。

*步骤 2：转到另一个决策树桩，对另一个输入变量做出决策：*

我们观察到前一步骤中被误分类的 2 个圆圈比剩余点要大。现在，第 2 个决策树桩将尝试正确预测这 2 个圆圈。

由于分配了更高的权重，这 2 个圆圈已被左侧的垂直线正确分类。但这导致了顶部 3 个圆圈的误分类。因此，我们将为这 3 个圆圈分配更高的权重，并应用另一个决策树桩。

*步骤 3：训练另一个决策树桩，对另一个输入变量做出决策。*

前一步骤中被误分类的 3 个圆圈比其他数据点要大。现在，已经生成了一个向右的垂直线来分类圆圈和三角形。

*步骤 4：组合决策树桩：*

我们将之前 3 个模型的分隔符结合起来，观察到这个模型的复杂规则与任何单一的弱学习器相比，可以更准确地分类数据点。

### 第 VII 部分 结论：

总结一下，我们已经学习了：

1.  5 种监督学习技术——线性回归、逻辑回归、CART、朴素贝叶斯、KNN。

1.  3 种无监督学习技术——Apriori、K-means、PCA。

1.  2 种集成技术——随机森林的袋装法，XGBoost 的提升法。

在我的下一篇博客中，我们将学习一种在 Kaggle 比赛中变得非常流行的技术——XGBoost 算法。

**相关：**

+   [数据科学家使用的顶级算法和方法](/2016/09/poll-algorithms-used-data-scientists.html)

+   [机器学习工程师需要了解的 10 种算法](/2016/08/10-algorithms-machine-learning-engineers.html)

+   [前 10 大数据挖掘算法详解](/2015/05/top-10-data-mining-algorithms-explained.html)

### 更多相关内容

+   [建立一个强大的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)

+   [使用管道编写干净的 Python 代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)

+   [学习数据科学统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)

+   [停止学习数据科学以寻找目的，并找到目的以…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)

+   [每个初学者数据科学家应掌握的 6 种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)

+   [一个 $9B 人工智能失败的检视](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)
