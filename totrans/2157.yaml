- en: 'Graph of Thoughts: A New Paradigm for Elaborate Problem-Solving in Large Language
    Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/graph-of-thoughts-a-new-paradigm-for-elaborate-problem-solving-in-large-language-models](https://www.kdnuggets.com/graph-of-thoughts-a-new-paradigm-for-elaborate-problem-solving-in-large-language-models)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Graph of Thoughts: A New Paradigm for Elaborate Problem-Solving in Large
    Language Models](../Images/64427c89afa9e69d0bb7407300ba5d50.png)'
  prefs: []
  type: TYPE_IMG
- en: '# Key Takeaways'
  prefs: []
  type: TYPE_NORMAL
- en: Graph of Thoughts (GoT) is a novel framework designed to enhance the prompting
    capabilities of Large Language Models (LLMs) for complex problem-solving tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GoT surpasses existing paradigms like Chain-of-Thought (CoT) and Tree of Thoughts
    (ToT) by representing the information generated by an LLM as a graph, allowing
    for more flexible and efficient reasoning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The framework has shown significant improvements in task performance, including
    a 62% increase in sorting quality and a cost reduction of over 31% compared to
    Tree of Thoughts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: This work brings the LLM reasoning closer to human thinking or brain mechanisms
    such as recurrence, both of which form complex networks.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The burgeoning landscape of artificial intelligence has given rise to increasingly
    sophisticated Large Language Models (LLMs) capable of a wide range of tasks. Yet,
    one of the ongoing challenges is improving these models' ability to solve elaborate
    problems efficiently. Enter [Graph of Thoughts (GoT)](https://arxiv.org/abs/2308.09687),
    a framework hoping to take a giant leap in this direction. GoT advances the prompting
    capabilities of LLMs by structuring the information they generate into a graph,
    thereby enabling a more intricate and flexible form of reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: While existing paradigms like Chain-of-Thought (CoT) and Tree of Thoughts (ToT)
    have contributed to the structured output and hierarchical reasoning in LLMs,
    they often operate within a linear or tree-like constraint. This limitation can
    sometimes hinder the model's ability to handle complex problem-solving tasks that
    require multi-dimensional reasoning and the ability to combine disparate pieces
    of information. Graph of Thoughts addresses this gap by introducing a graph-based
    structure for managing "LLM thoughts." This allows for an unprecedented level
    of flexibility in how information is stored, accessed, and manipulated within
    the model. With GoT, developers and researchers can fine-tune the prompting strategy
    to navigate this graph effectively, enabling LLMs to solve intricate problems
    in a more human-like manner.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Graph of Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Graph of Thoughts operates on a simple yet powerful concept: it models the
    information produced by an LLM as a graph where each vertex represents a unit
    of information, often referred to as "LLM thoughts." The edges between these vertices
    signify the dependencies or relationships between different units of thought.
    This graph-based approach allows for:'
  prefs: []
  type: TYPE_NORMAL
- en: Combining arbitrary LLM thoughts into harmonious outcomes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refining the essence of complex networks of thoughts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strengthening thoughts with the use of feedback loops
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In comparison to existing paradigms like CoT and ToT, GoT offers a more flexible
    and efficient way to manage and manipulate the information generated by LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Graph of Thoughts process compared](../Images/ac1f50e79f940d0de84691bc31a6464e.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 1**: Comparison of Graph of Thoughts (GoT) to other prompting strategies
    (Image from paper)'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Graph of Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To implement GoT, developers need to represent the problem-solving process as
    a graph, where each node or vertex represents a thought or a piece of information.
    Then, the relationships or dependencies between these thoughts are mapped as edges
    in the graph. This mapping allows for various operations like merging nodes to
    create more complex thoughts, or applying transformations to enhance the existing
    thoughts.
  prefs: []
  type: TYPE_NORMAL
- en: One of the standout features of GoT is its extensibility, allowing it to adapt
    to a variety of tasks and domains. Unlike more rigid structures, the graph-based
    representation in GoT can be dynamically altered during the problem-solving process.
    This means that as an LLM generates new thoughts or gains additional insights,
    these can be seamlessly incorporated into the existing graph without requiring
    a complete overhaul.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, GoT enables the implementation of feedback loops, where the model
    can revisit and refine its earlier thoughts based on newly acquired information.
    This dynamic and iterative process serves to significantly enhance the quality
    of the model's output, making it a particularly powerful tool for complex tasks
    that require ongoing refinement and adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The introduction of GoT may mark a significant advancement in the field of LLMs
    and their application in complex problem-solving tasks. By adopting a graph-based
    approach to represent and manipulate the information generated by LLMs, GoT offers
    a more flexible and efficient form of reasoning. Its success in improving task
    performance and reducing computational costs makes it a promising framework for
    future research and applications. Developers and researchers should explore this
    new paradigm in order to attempt to unlock the full problem-solving potential
    of their LLMs and improve their prompting.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.linkedin.com/in/mattmayo13/)****[Matthew Mayo](https://www.kdnuggets.com/wp-content/uploads/./profile-pic.jpg)****
    ([**@mattmayo13**](https://twitter.com/mattmayo13)) holds a master''s degree in
    computer science and a graduate diploma in data mining. As managing editor of
    [KDnuggets](https://www.kdnuggets.com/) & [Statology](https://www.statology.org/),
    and contributing editor at [Machine Learning Mastery](https://machinelearningmastery.com/),
    Matthew aims to make complex data science concepts accessible. His professional
    interests include natural language processing, language models, machine learning
    algorithms, and exploring emerging AI. He is driven by a mission to democratize
    knowledge in the data science community. Matthew has been coding since he was
    6 years old.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Top Open Source Large Language Models](https://www.kdnuggets.com/2022/09/john-snow-top-open-source-large-language-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[More Free Courses on Large Language Models](https://www.kdnuggets.com/2023/06/free-courses-large-language-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Learn About Large Language Models](https://www.kdnuggets.com/2023/03/learn-large-language-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introducing Healthcare-Specific Large Language Models from John Snow Labs](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are Large Language Models and How Do They Work?](https://www.kdnuggets.com/2023/05/large-language-models-work.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AI: Large Language & Visual Models](https://www.kdnuggets.com/2023/06/ai-large-language-visual-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
