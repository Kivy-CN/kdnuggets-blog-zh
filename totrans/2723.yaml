- en: Facebook Uses Bayesian Optimization to Conduct Better Experiments in Machine
    Learning Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/08/facebook-bayesian-optimization-better-experiments-machine-learning.html](https://www.kdnuggets.com/2020/08/facebook-bayesian-optimization-better-experiments-machine-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)![Figure](../Images/5a41adeaa39bd46f9876c1a54d9e5f5a.png)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Source: [https://www.persado.com/2016/06/machine-learning-trumps-a-b-split-testing/](https://www.persado.com/2016/06/machine-learning-trumps-a-b-split-testing/)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'I recently started a new newsletter focus on AI education. TheSequence is a
    no-BS( meaning no hype, no news etc) AI-focused newsletter that takes 5 minutes
    to read. The goal is to keep you up to date with machine learning projects, research
    papers and concepts. Please give it a try by subscribing below:'
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[![Image](../Images/f2aed90f956dea213be7c9bbf9cd7072.png)](https://thesequence.substack.com/)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter optimization is a key aspect of the lifecycle of machine learning
    applications. While methods such as grid search are incredibly effective for optimizing
    hyperparameters for specific isolated models, they are very difficult to scale
    across large permutations of models and experiments. A company like Facebook operates
    thousands of concurrent machine learning models that need to be constantly tuned.
    To achieve that, Facebook engineering teams need to regularly conduct A/B tests
    in order to determine the right hyperparameter configuration. Data in those tests
    is difficult to collect and they are typically conducted in isolation of each
    other which end up resulting in very computationally expensive exercises. One
    of the most innovative approaches in this area came from a team of AI researchers
    from Facebook who [published a paper proposing a method based on Bayesian optimization](https://projecteuclid.org/euclid.ba/1533866666) to
    adaptively design rounds of A/B tests based on the results of prior tests.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Why Bayesian Optimization?
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bayesian optimization is a powerful method for solving black-box optimization
    problems that involve expensive function evaluations. Recently, Bayesian optimization
    has evolved as an important technique for optimizing hyperparameters in machine
    learning models. Conceptually, Bayesian optimization starts by evaluating a small
    number of randomly selected function values, and fitting a Gaussian process (GP)
    regression model to the results. The GP posterior provides an estimate of the
    function value at each point, as well as the uncertainty in that estimate. The
    GP works well for Bayesian optimization because it provides excellent uncertainty
    estimates and is analytically tractable. It provides an estimate of how an online
    metric varies with the parameters of interest.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Let’s imagine an environment in which we are conducting random and regular experiments
    on machine learning models. In that scenario, Bayesian optimization can be used
    to construct a statistical model of the relationship between the parameters and
    the online outcomes of interest and uses that model to decide which experiments
    to run. The concept is well illustrated in the following figure in which each
    data marker corresponds to the outcome of an A/B test of that parameter value.
    We can use the GP to decide which parameter to test next by balancing exploration
    (high uncertainty) with exploitation (good model estimate). This is done by computing
    an acquisition function that estimates the value of running an experiment with
    any given parameter value.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/764a2ed588a06c28afc8cbad22a9d87c.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: Source: [https://projecteuclid.org/download/pdfview_1/euclid.ba/1533866666](https://projecteuclid.org/download/pdfview_1/euclid.ba/1533866666)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: The fundamental goal of Bayesian optimization when applied to hyperparameter
    optimization is to determine how valuable is an experiment for a specific hyperparameter
    configuration. Conceptually, Bayesian optimization works very efficiently for
    isolated models but its value proposition is challenged when used in scenarios
    running random experiments. The fundamental challenge is related to the noise
    introduced in the observations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Noise and Bayesian Optimization
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Random experiments in machine learning systems introduce high levels of noise
    in the observations. Additionally, many of the constraints for a given experiment
    can be considered noisy data in and out itself which can affect the results of
    an experiment. Suppose that we are trying to evaluate the value of a function *f(x)* for
    a given observation *x*. With observation noise, we now have uncertainty not only
    in the value *f(x)*, but we also have uncertainty in which observation is the
    current best, *x**, and its value, *f(x*).*
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, Bayesian optimization models use heuristics to handle noisy observations
    but those perform very poorly with high levels of noise. To address this challenge,
    the Facebook team came up with a clever answer: why not to factor in noise as
    part of the observations?'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Imagine if, instead of computing the expectation of observing *f(x)* we observe *yi *= *f*(**x***i*)
    + *€i*, where *€i *is the observation noise. Mathematically, GP works similarly
    with noise observation as it does with noiseless data. Without going crazy about
    the math, in their research paper, the Facebook team showed that this type of
    approximation is very well suited for Monte Carlo optimizations which yield incredibly
    accurate results estimating the correct observation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![Image for post](../Images/e8655a6e8c580d48a6fec401f7eb52e5.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Bayesian Optimization with Noisy Data in Action
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Facebook team tested their research in a couple of real world scenarios
    at Facebook scale. The first was to optimize 6 parameters of one of Facebook’s
    ranking systems. The second example was to optimize 7 numeric compiler flags related
    to CPU usage used in their HipHop Virtual Machine(HHVM). For that second experiment,
    the first 30 iterations were randomly created. At that point, the Bayesian optimization
    with noisy data method was able to identity CPU time as the hyperparameter configuration
    that needed to be evaluated and started running different experiments to optimize
    its value. The results are clearly illustrated in the following figure.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/afda12066733a77f0cce424f606da881.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: Source: [https://projecteuclid.org/download/pdfview_1/euclid.ba/1533866666](https://projecteuclid.org/download/pdfview_1/euclid.ba/1533866666)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Techniques such as Bayesian optimization with noisy data are incredibly powerful
    in large scale machine learning algorithms. While we have done a lot of work on
    optimization methods, most of those methods remain highly theoretical. Its nice
    to see Facebook pushing the boundaries of this nascent space.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/dataseries/facebook-uses-bayesian-optimization-to-conduct-better-experiments-in-machine-learning-models-6f834169d005).
    Reposted with permission.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[Essential Resources to Learn Bayesian Statistics](/2020/07/essential-resources-learn-bayesian-statistics.html)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Uber’s Ludwig is an Open Source Framework for Low-Code Machine Learning](/2020/06/uber-ludwig-open-source-framework-machine-learning.html)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Facebook Open Sources Blender, the Largest-Ever Open Domain Chatbot](/2020/05/facebook-open-sources-blender-largest-open-domain-chatbot.html)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How LinkedIn Uses Machine Learning To Rank Your Feed](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, November 16: How LinkedIn Uses Machine Learning •…](https://www.kdnuggets.com/2022/n45.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Versioning Machine Learning Experiments vs Tracking Them](https://www.kdnuggets.com/2021/12/versioning-machine-learning-experiments-tracking.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hydra Configs for Deep Learning Experiments](https://www.kdnuggets.com/2023/03/hydra-configs-deep-learning-experiments.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Programming Languages and Their Uses](https://www.kdnuggets.com/2021/05/top-programming-languages.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets™ News 22:n04, Jan 26: The High Paying Side Hustles…](https://www.kdnuggets.com/2022/n04.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets™ 新闻 22:n04, 1月 26日: 高收入的副业…](https://www.kdnuggets.com/2022/n04.html)'
