- en: Memory Complexity with Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变压器的内存复杂度
- en: 原文：[https://www.kdnuggets.com/2022/12/memory-complexity-transformers.html](https://www.kdnuggets.com/2022/12/memory-complexity-transformers.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/12/memory-complexity-transformers.html](https://www.kdnuggets.com/2022/12/memory-complexity-transformers.html)
- en: The key innovation in Transformers is the introduction of a self-attention mechanism,
    which computes similarity scores for all pairs of positions in an input sequence,
    and can be evaluated in parallel for each token of the input sequence, avoiding
    the sequential dependency of recurrent neural networks, and enabling Transformers
    to vastly outperform previous sequence models like LSTM.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器模型的关键创新是引入了自注意力机制，它为输入序列中的所有位置对计算相似度分数，并且可以并行评估每个标记，避免了递归神经网络的顺序依赖，从而使变压器能够大大超越以前的序列模型如LSTM。
- en: There are a lot of deep explanations elsewhere so here I’d like to share some
    example questions in an **interview setting.**
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 其他地方有很多深度解释，因此在这里我想分享一些**面试设置**中的示例问题。
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析水平'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织IT'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: What’s the problem with running a transformer model on a book with 1 million
    tokens? What can be a solution to this problem?
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在一个有100万标记的书籍上运行变压器模型的问题是什么？解决这个问题的办法是什么？
- en: '![Memory Complexity with Transformers](../Images/80ca231462808dc45630f195ba2547af.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![变压器的内存复杂度](../Images/80ca231462808dc45630f195ba2547af.png)'
- en: '![Memory Complexity with Transformers](../Images/2d61db24c250ff1ddf07881a6494581f.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![变压器的内存复杂度](../Images/2d61db24c250ff1ddf07881a6494581f.png)'
- en: '**Here are some tips for readers’ reference:**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**以下是供读者参考的一些提示：**'
- en: Simply put,
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，
- en: If you try to run a large transformer on the long sequence, you just run out
    of memory.
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你尝试在长序列上运行大型变压器模型，你会发现内存不够用。
- en: 'According to the Google Research Blog (2021):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 根据谷歌研究博客（2021年）：
- en: A limitation of existing Transformer models and their derivatives is that the
    full [self-attention mechanism](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) has
    computational and memory requirements that are quadratic with the input sequence
    length. With commonly available current hardware and model sizes, this typically
    limits the input sequence to roughly 512 tokens, and prevents Transformers from
    being directly applicable to tasks that require larger context, like [question
    answering](https://huggingface.co/tasks/question-answering), [document summarization](https://arxiv.org/pdf/1804.05685) or [genome
    fragment classification](https://www.biorxiv.org/content/10.1101/353474v3.full).
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现有变压器模型及其衍生模型的一个限制是，完整的[self-attention mechanism](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)的计算和内存需求与输入序列长度的平方成正比。使用当前常见的硬件和模型尺寸，这通常将输入序列限制在大约512个标记左右，并且阻止变压器直接应用于需要更大上下文的任务，如[问答](https://huggingface.co/tasks/question-answering)、[文档摘要](https://arxiv.org/pdf/1804.05685)或[基因组片段分类](https://www.biorxiv.org/content/10.1101/353474v3.full)。
- en: '*Check the explanation by***Dr.Younes Bensouda Mourri ***from *[*Deeplearning.ai*](https://www.deeplearning.ai/courses/natural-language-processing-specialization/)*:*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*查看***Dr.Younes Bensouda Mourri***来自*[*Deeplearning.ai*](https://www.deeplearning.ai/courses/natural-language-processing-specialization/)*的解释：*'
- en: '[Check the explanation!](https://www.coursera.org/lecture/attention-models-in-nlp/transformer-complexity-oGXK3)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看解释！](https://www.coursera.org/lecture/attention-models-in-nlp/transformer-complexity-oGXK3)'
- en: To solve the memory complexity problem of the transformer models.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 解决变压器模型的内存复杂度问题。
- en: 'Two ‘reforms’ were made to the Transformer to make it more memory and compute
    efficient: the Reversible Layers reduce memory and the Locality Sensitive Hashing(LSH) reduces
    the cost of the Dot Product attention for large input sizes.'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对变换器进行的两个“改革”使其在内存和计算方面更高效：可逆层减少内存，局部敏感哈希（LSH）减少大输入大小下点积注意力的成本。
- en: Of course, there are other solutions such as [Extended Transformer Construction](https://arxiv.org/abs/2004.08483) (ETC)
    and the like. We will cover more details in a later post!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，还有其他解决方案，如 [扩展变换器构建](https://arxiv.org/abs/2004.08483) （ETC）等。我们将在后续文章中深入探讨更多细节！
- en: '*Happy practicing!*'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*快乐练习！*'
- en: '**Note:** there are different angles to answer an interview question. The author
    of this newsletter does not try to find a reference that answers a question exhaustively.
    Rather, the author would like to share some quick insights and help the readers
    to think, practice and do further research as necessary.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：**回答面试问题有不同的角度。此新闻通讯的作者并未尝试找到一个详尽回答问题的参考，而是希望分享一些快速见解，帮助读者思考、练习并在必要时进行进一步研究。'
- en: Source of images/Good reads: [Paper](https://arxiv.org/pdf/2112.04426.pdf).
    Improving language models by retrieving from trillions of tokens by Deepmind (2022) [Blog](https://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html).
    Constructing Transformers For Longer Sequences with Sparse Attention Methods by
    Google (2021)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源/好读物： [论文](https://arxiv.org/pdf/2112.04426.pdf)。通过 Deepmind 改进语言模型，通过数万亿个标记进行检索（2022） [博客](https://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html)。通过
    Google（2021）使用稀疏注意力方法构建更长序列的变换器
- en: Source of video/answers: [Natural Language Processing with Attention Models ](https://www.coursera.org/learn/attention-models-in-nlp)by **Dr.Younes
    Bensouda Mourri ***from *[*Deeplearning.ai*](https://www.deeplearning.ai/courses/natural-language-processing-specialization/)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 视频/答案来源： [自然语言处理中的注意力模型](https://www.coursera.org/learn/attention-models-in-nlp)由**Dr.Younes
    Bensouda Mourri** *提供，来自*[*Deeplearning.ai*](https://www.deeplearning.ai/courses/natural-language-processing-specialization/)
- en: '**[Angelina Yang](https://www.linkedin.com/in/yangyy/)** is data and machine
    learning senior executive with more than 15 years of experience delivering advanced
    machine learning solutions and capabilities to increase business values in the
    financial service and fintech industry. Expertise includes AI/ML/NLP/DL model
    development and deployment in the areas of customer experience, surveillance,
    conversational AI, risk and compliance, marketing, operations, pricing and data
    services.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Angelina Yang](https://www.linkedin.com/in/yangyy/)** 是一位数据和机器学习高级执行官，拥有超过15年的经验，致力于提供先进的机器学习解决方案和能力，以提高金融服务和金融科技行业的业务价值。专长包括在客户体验、监控、对话式
    AI、风险与合规、营销、运营、定价和数据服务领域的 AI/ML/NLP/DL 模型开发和部署。'
- en: '[Original](https://medium.com/@angelina.yang/memory-complexity-with-transformers-c4e1517670b1).
    Reposted with permission.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/@angelina.yang/memory-complexity-with-transformers-c4e1517670b1)。经授权转载。'
- en: More On This Topic
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Linear Regression Model Selection: Balancing Simplicity and Complexity](https://www.kdnuggets.com/2023/02/linear-regression-model-selection-balancing-simplicity-complexity.html)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性回归模型选择：平衡简洁性与复杂性](https://www.kdnuggets.com/2023/02/linear-regression-model-selection-balancing-simplicity-complexity.html)'
- en: '[Introduction to Memory Profiling in Python](https://www.kdnuggets.com/introduction-to-memory-profiling-in-python)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 中的内存分析简介](https://www.kdnuggets.com/introduction-to-memory-profiling-in-python)'
- en: '[How to Perform Memory-Efficient Operations on Large Datasets with Pandas](https://www.kdnuggets.com/how-to-perform-memory-efficient-operations-on-large-datasets-with-pandas)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 Pandas 对大数据集执行内存高效操作](https://www.kdnuggets.com/how-to-perform-memory-efficient-operations-on-large-datasets-with-pandas)'
- en: '[Simple NLP Pipelines with HuggingFace Transformers](https://www.kdnuggets.com/2023/02/simple-nlp-pipelines-huggingface-transformers.html)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 HuggingFace Transformers 构建简单的 NLP 流水线](https://www.kdnuggets.com/2023/02/simple-nlp-pipelines-huggingface-transformers.html)'
- en: '[Comparing Natural Language Processing Techniques: RNNs, Transformers, BERT](https://www.kdnuggets.com/comparing-natural-language-processing-techniques-rnns-transformers-bert)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[比较自然语言处理技术：RNNs、变换器、BERT](https://www.kdnuggets.com/comparing-natural-language-processing-techniques-rnns-transformers-bert)'
- en: '[How to Fine-Tune BERT for Sentiment Analysis with Hugging Face Transformers](https://www.kdnuggets.com/how-to-fine-tune-bert-sentiment-analysis-hugging-face-transformers)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 Hugging Face Transformers 微调 BERT 进行情感分析](https://www.kdnuggets.com/how-to-fine-tune-bert-sentiment-analysis-hugging-face-transformers)'
