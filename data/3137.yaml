- en: Supervised vs. Unsupervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习与无监督学习
- en: 原文：[https://www.kdnuggets.com/2018/04/supervised-vs-unsupervised-learning.html](https://www.kdnuggets.com/2018/04/supervised-vs-unsupervised-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/04/supervised-vs-unsupervised-learning.html](https://www.kdnuggets.com/2018/04/supervised-vs-unsupervised-learning.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Devin Soni](https://www.linkedin.com/in/devinsoni/), Computer Science
    Student**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Devin Soni](https://www.linkedin.com/in/devinsoni/)，计算机科学学生**'
- en: 'Within the field of machine learning, there are two main types of tasks: supervised,
    and unsupervised. The main difference between the two types is that supervised
    learning is done using a **ground truth**, or in other words, we have prior knowledge
    of what the output values for our samples should be. Therefore, the goal of supervised
    learning is to learn a function that, given a sample of data and desired outputs,
    best approximates the relationship between input and output observable in the
    data. Unsupervised learning, on the other hand, does not have labeled outputs,
    so its goal is to infer the natural structure present within a set of data points.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习领域，有两种主要的任务类型：监督学习和无监督学习。这两种类型的主要区别在于，监督学习是使用**真实值**进行的，换句话说，我们事先知道样本的输出值应该是什么。因此，监督学习的目标是学习一个函数，该函数在给定数据样本和期望输出的情况下，最佳地近似输入和输出之间的关系。而无监督学习则没有标签输出，因此其目标是推断数据点集中的自然结构。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三名课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的快车道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你在IT领域的组织'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**Supervised Learning**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督学习**'
- en: '![](../Images/fdcbabef85fee7ab67ce1d8c1bed14a0.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fdcbabef85fee7ab67ce1d8c1bed14a0.png)'
- en: Supervised learning is typically done in the context of classification, when
    we want to map input to output labels, or regression, when we want to map input
    to a continuous output. Common algorithms in supervised learning include logistic
    regression, naive bayes, support vector machines, artificial neural networks,
    and random forests. In both regression and classification, the goal is to find
    specific relationships or structure in the input data that allow us to effectively
    produce correct output data. Note that “correct” output is determined entirely
    from the training data, so while we do have a ground truth that our model will
    assume is true, it is not to say that data labels are always correct in real-world
    situations. Noisy, or incorrect, data labels will clearly reduce the effectiveness
    of your model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习通常在分类的背景下进行，当我们想将输入映射到输出标签时，或在回归的背景下进行，当我们想将输入映射到连续输出时。监督学习中的常见算法包括逻辑回归、朴素贝叶斯、支持向量机、人工神经网络和随机森林。在回归和分类中，目标是找到输入数据中的特定关系或结构，以便我们能够有效地生成正确的输出数据。请注意，“正确”的输出完全由训练数据决定，因此尽管我们确实有一个模型会假设为真的真实值，但这并不意味着数据标签在现实世界中总是正确的。噪声或错误的数据标签将显著降低模型的有效性。
- en: When conducting supervised learning, the main considerations are model complexity,
    and the bias-variance tradeoff. Note that both of these are interrelated.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行监督学习时，主要考虑因素是模型复杂性和偏差-方差权衡。请注意，这两者是相互关联的。
- en: Model complexity refers to the complexity of the function you are attempting
    to learn — similar to the degree of a polynomial. The proper level of model complexity
    is generally determined by the nature of your training data. If you have a small
    amount of data, or if your data is not uniformly spread throughout different possible
    scenarios, you should opt for a low-complexity model. This is because a high-complexity
    model will **overfit** if used on a small number of data points. Overfitting refers
    to learning a function that fits your training data very well, but does not **generalize**
    to other data points — in other words, you are strictly learning to produce your
    training data without learning the actual trend or structure in the data that
    leads to this output. Imagine trying to fit a curve between 2 points. In theory,
    you can use a function of any degree, but in practice, you would parsimoniously
    add complexity, and go with a linear function.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 模型复杂性指的是你试图学习的函数的复杂程度——类似于多项式的度数。模型复杂性的适当水平通常由你的训练数据的性质决定。如果你拥有的数据量较小，或者数据在不同可能场景中的分布不均匀，你应该选择一个低复杂度的模型。这是因为高复杂度的模型如果用于少量数据点，会**过拟合**。过拟合是指学习一个能够很好地拟合你的训练数据的函数，但不能**泛化**到其他数据点——换句话说，你只是严格地学习如何生成你的训练数据，而没有学习导致这些输出的实际趋势或数据结构。可以想象在两个点之间拟合一条曲线。理论上，你可以使用任何度数的函数，但实际上，你会节俭地增加复杂性，并选择线性函数。
- en: The bias-variance tradeoff also relates to model generalization. In any model,
    there is a balance between bias, which is the constant error term, and variance,
    which is the amount by which the error may vary between different training sets.
    So, high bias and low variance would be a model that is consistently wrong 20%
    of the time, whereas a low bias and high variance model would be a model that
    can be wrong anywhere from 5%-50% of the time, depending on the data used to train
    it. Note that bias and variance typically move in opposite directions of each
    other; increasing bias will usually lead to lower variance, and vice versa. When
    making your model, your specific problem and the nature of your data should allow
    you to make an informed decision on where to fall on the bias-variance spectrum.
    Generally, increasing bias (and decreasing variance) results in models with relatively
    guaranteed baseline levels of performance, which may be critical in certain tasks.
    Additionally, in order to produce models that generalize well, the variance of
    your model should scale with the size and complexity of your training data — small,
    simple data-sets should usually be learned with low-variance models, and large,
    complex data-sets will often require higher-variance models to fully learn the
    structure of the data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差-方差权衡也与模型的泛化能力有关。在任何模型中，存在偏差，即恒定的误差项，以及方差，即误差在不同训练集之间的变化量。因此，高偏差和低方差的模型是指在20%的时间里始终错误的模型，而低偏差和高方差的模型则是指根据用于训练的数据，模型可能在5%-50%的时间里出现错误。注意，偏差和方差通常是相反的；增加偏差通常会导致方差降低，反之亦然。在构建模型时，你的具体问题和数据的性质应该使你能够做出关于偏差-方差谱上位置的明智决策。一般来说，增加偏差（并减少方差）会导致具有相对保证的基准性能的模型，这在某些任务中可能是至关重要的。此外，为了生成具有良好泛化能力的模型，你的模型方差应随训练数据的大小和复杂性而变化——小而简单的数据集通常应使用低方差模型来学习，而大而复杂的数据集通常需要高方差模型才能充分学习数据的结构。
- en: '**Unsupervised Learning**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**无监督学习**'
- en: '![](../Images/d69aa6c7c9c75e9894d819952844cf5e.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d69aa6c7c9c75e9894d819952844cf5e.png)'
- en: The most common tasks within unsupervised learning are clustering, representation
    learning, and density estimation. In all of these cases, we wish to learn the
    inherent structure of our data without using explicitly-provided labels. Some
    common algorithms include k-means clustering, principal component analysis, and
    autoencoders. Since no labels are provided, there is no specific way to compare
    model performance in most unsupervised learning methods.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习中最常见的任务是聚类、表示学习和密度估计。在所有这些情况下，我们希望在不使用显式提供的标签的情况下学习数据的内在结构。一些常见的算法包括k均值聚类、主成分分析和自编码器。由于没有提供标签，因此在大多数无监督学习方法中，没有具体的方法来比较模型性能。
- en: Two common use-cases for unsupervised learning are exploratory analysis and
    dimensionality reduction.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习的两个常见使用案例是探索性分析和降维。
- en: Unsupervised learning is very useful in exploratory analysis because it can
    automatically identify structure in data. For example, if an analyst were trying
    to segment consumers, unsupervised clustering methods would be a great starting
    point for their analysis. In situations where it is either impossible or impractical
    for a human to propose trends in the data, unsupervised learning can provide initial
    insights that can then be used to test individual hypotheses.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习在探索性分析中非常有用，因为它可以自动识别数据中的结构。例如，如果分析师尝试对消费者进行细分，无监督聚类方法将是分析的一个很好的起点。在人类提出数据趋势不可能或不切实际的情况下，无监督学习可以提供初步的见解，然后可以用来测试个别假设。
- en: Dimensionality reduction, which refers to the methods used to represent data
    using less columns or features, can be accomplished through unsupervised methods.
    In representation learning, we wish to learn relationships between individual
    features, allowing us to represent our data using the latent features that interrelate
    our initial features. This sparse latent structure is often represented using
    far fewer features than we started with, so it can make further data processing
    much less intensive, and can eliminate redundant features.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 维度减少，即指通过使用较少的列或特征来表示数据的方法，可以通过无监督方法完成。在表示学习中，我们希望学习个别特征之间的关系，从而使用潜在特征表示数据，这些特征相互关联。这个稀疏的潜在结构通常使用比最初使用的特征要少得多的特征来表示，因此可以使进一步的数据处理变得更加轻便，并且可以消除冗余特征。
- en: '**TLDR:**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**TLDR:**'
- en: '![](../Images/c316b4cd9604e4034c545e55d5fffac1.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c316b4cd9604e4034c545e55d5fffac1.png)'
- en: '**Bio: [Devin Soni](https://www.linkedin.com/in/devinsoni/)** is a computer
    science student interested in machine learning and data science. He will be a
    software engineering intern at Airbnb in 2018\. He can be reached [via LinkedIn](https://www.linkedin.com/in/devinsoni/).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介：[Devin Soni](https://www.linkedin.com/in/devinsoni/)** 是一名计算机科学学生，对机器学习和数据科学感兴趣。他将在2018年成为Airbnb的**软件工程实习生**。可以通过[LinkedIn](https://www.linkedin.com/in/devinsoni/)与他联系。'
- en: '[Original](https://towardsdatascience.com/supervised-vs-unsupervised-learning-14f68e32ea8d).
    Reposted with permission.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[原始](https://towardsdatascience.com/supervised-vs-unsupervised-learning-14f68e32ea8d)。经许可转载。'
- en: '**Related:**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Machine Learning Algorithms: Which One to Choose for Your Problem](/2017/11/machine-learning-algorithms-choose-your-problem.html)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习算法：为你的问题选择哪一个](/2017/11/machine-learning-algorithms-choose-your-problem.html)'
- en: '[Introduction to k-Nearest Neighbors](/2018/03/introduction-k-nearest-neighbors.html)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[k-最近邻介绍](/2018/03/introduction-k-nearest-neighbors.html)'
- en: '[Introduction to Markov Chains](/2018/03/introduction-markov-chains.html)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[马尔可夫链介绍](/2018/03/introduction-markov-chains.html)'
- en: More On This Topic
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，并寻找目标以...](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学学习统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90亿美元的人工智能失败，解析](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功数据科学家的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[是什么使Python成为初创公司的理想编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个数据科学家都应了解的三个R库（即使你使用Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
