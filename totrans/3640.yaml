- en: 7 Steps to Mastering Large Language Model Fine-tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/7-steps-to-mastering-large-language-model-fine-tuning](https://www.kdnuggets.com/7-steps-to-mastering-large-language-model-fine-tuning)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![7 Steps to Mastering Large Language Model Fine-tuning](../Images/13b32ea8eeb1fe6ceb3783aa8acd200e.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Over the recent year and a half, the landscape of natural language processing
    (NLP) has seen a remarkable evolution, mostly thanks to the rise of Large Language
    Models (LLMs) like OpenAI’s GPT family.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: These powerful models have revolutionized our approach to handling natural language
    tasks, offering unprecedented capabilities in translation, sentiment analysis,
    and automated text generation. Their ability to understand and generate human-like
    text has opened up possibilities once thought unattainable.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: However, despite their impressive capabilities, the journey to train these models
    is full of challenges, such as the significant time and financial investments
    required.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to the critical role of fine-tuning LLMs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: By refining these pre-trained models to better suit specific applications or
    domains, we can significantly enhance their performance on particular tasks. This
    step not only elevates their quality but also extends their utility across a wide
    array of sectors.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: This guide aims to break down this process into 7 simple steps to get any LLM
    fine-tuned for a specific task.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Pre-trained Large Language Models
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs are a specialized category of ML algorithms designed to predict the next
    word in a sequence based on the context provided by the preceding words. These
    models are built upon the Transformers architecture, a breakthrough in machine
    learning techniques and first explained in Google’s [All you need is attention](https://arxiv.org/abs/1706.03762)
    article.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Models like GPT (Generative Pre-trained Transformer) are examples of pre-trained
    language models that have been exposed to large volumes of textual data. This
    extensive training allows them to capture the underlying rules of language usage,
    including how words are combined to form coherent sentences.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![7 Steps to Mastering Large Language Model Fine-tuning](../Images/3bb692738fdcc3d7882e4f855e864edd.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: A key strength of these models lies in their ability to not only understand
    natural language but also to produce text that closely mimics human writing based
    on the inputs they are given.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型的一个关键优势在于它们不仅能理解自然语言，还能根据给定的输入生成接近人类写作风格的文本。
- en: So what’s the best of this?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这有什么好处呢？
- en: These models are already open to the masses using APIs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型已经通过 API 向大众开放。
- en: What is Fine-tuning, and Why is it Important?
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是微调，为什么它很重要？
- en: Fine-tuning is the process of picking a pre-trained model and improving it with
    further training on a domain-specific dataset.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是选择一个预训练模型，并通过在特定领域数据集上进一步训练来改进它的过程。
- en: Most LLM models have very good natural language skills and generic knowledge
    performance but fail in specific task-oriented problems. The fine-tuning process
    offers an approach to improve model performance for specific problems while lowering
    computation expenses without the necessity of building them from the ground up.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数大型语言模型具有非常好的自然语言处理能力和通用知识表现，但在特定任务导向的问题上表现较差。微调过程提供了一种方法，以提高模型在特定问题上的性能，同时降低计算开支，无需从头构建模型。
- en: '![7 Steps to Mastering Large Language Model Fine-tuning](../Images/3aca95c4978bdad858e24830287394ed.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![掌握大型语言模型微调的7个步骤](../Images/3aca95c4978bdad858e24830287394ed.png)'
- en: Image by Author
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: To put it simply, Fine-tuning tailors the model to have a better performance
    for specific tasks, making it more effective and versatile in real-world applications.
    This process is essential for improving an existing model for a particular task
    or domain.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，微调使模型在特定任务上的表现更好，从而使其在实际应用中更有效、更具适应性。这个过程对于改进现有模型以适应特定任务或领域至关重要。
- en: A Step-by-Step Guide to Fine-tuning a LLM
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调 LLM 的逐步指南
- en: Let’s exemplify this concept by fine-tuning a real model in only 7 steps.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过在仅仅7个步骤中微调一个实际模型来说明这个概念。
- en: 'Step 1: Having our concrete objective clear'
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第一步：明确我们的具体目标
- en: Imagine we want to infer the sentiment of any text and decide to try GPT-2 for
    such a task.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们想要推断任何文本的情感，并决定尝试使用 GPT-2 来完成这个任务。
- en: 'I’m pretty sure there’s no surprise that we will soon enough detect it is quite
    bad at doing so. Then, one natural question that comes to mind is:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我敢肯定，毫无惊讶，我们很快会发现它在这方面的表现相当糟糕。然后，一个自然的问题是：
- en: Can we do something to improve its performance?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能做些什么来提高它的性能？
- en: And of course, the answer is that we can!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，答案是我们可以！
- en: Taking advantage of fine-tuning by training our pre-trained GPT-2 model from
    the Hugging Face Hub with a dataset containing tweets and their corresponding
    sentiments so the performance improves.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用微调，对 Hugging Face Hub 上的预训练 GPT-2 模型进行训练，使用一个包含推文及其相应情感的数据集，从而提高性能。
- en: So our ultimate goal is **to have a model that is good at inferring the sentiment
    out of text. **
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终目标是**拥有一个能够准确推断文本情感的模型。**
- en: 'Step 2: Choose a pre-trained model and a dataset'
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第二步：选择一个预训练模型和一个数据集
- en: 'The second step is to pick what model to take as a base model. In our case,
    we already picked the model: GPT-2\. So we are going to perform some simple fine-tuning
    to it.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是选择一个作为基础模型的模型。在我们的案例中，我们已经选择了模型：GPT-2。因此，我们将对其进行一些简单的微调。
- en: '![7 Steps to Mastering Large Language Model Fine-tuning](../Images/bb120b72862dd720822dd420560db0f1.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![掌握大型语言模型微调的7个步骤](../Images/bb120b72862dd720822dd420560db0f1.png)'
- en: Screenshot of Hugging Face Datasets Hub. Selecting OpenAI’s GPT2 model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 数据集中心的截图。选择 OpenAI 的 GPT2 模型。
- en: Always keep in mind to select a model that fits your task.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 始终记住选择一个适合你任务的模型。
- en: 'Step 3: Load the data to use'
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第三步：加载要使用的数据
- en: Now that we have both our model and our main task, we need some data to work
    with.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了模型和主要任务，我们需要一些数据来进行操作。
- en: But no worries, Hugging Face has everything arranged!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 但不用担心，Hugging Face 已经安排好了所有内容！
- en: This is where their dataset library kicks in.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是它们的数据集库发挥作用的地方。
- en: In this example, we will take advantage of the Hugging Face dataset library
    to import a dataset with tweets labeled with their corresponding sentiment (Positive,
    Neutral or Negative).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将利用 Hugging Face 数据集库导入一个包含推文及其相应情感（积极、中性或消极）标签的数据集。
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The data looks like follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 数据大致如下：
- en: '![7 Steps to Mastering Large Language Model Fine-tuning](../Images/289efc50c9aa54035a8390b86c86070f.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![掌握大型语言模型微调的7个步骤](../Images/289efc50c9aa54035a8390b86c86070f.png)'
- en: The data set to be used.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的数据集。
- en: 'Step 4: Tokenizer'
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第四步：分词器
- en: Now we have both our model and the dataset to fine-tune it. So the following
    natural step is to load a tokenizer. As LLMs work with tokens (and not with words!!),
    we require a tokenizer to send the data to our model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: We can easily perform this by taking advantage of the map method to tokenize
    the whole dataset.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**BONUS:** To improve our processing performance, two smaller subsets are generated:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '**The training set:** To fine-tune our model.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The testing set:**  To evaluate it.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Step 5: Initialize our base model'
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once we have the dataset to be used, we load our model and specify the number
    of expected labels. From the Tweet’s sentiment dataset, you can know there are
    three possible labels:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 0 or Negative
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 or Neutral
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2  or Positive
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Step 6: Evaluate method'
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Transformers library provides a class called “Trainer” that optimizes both
    the training and the evaluation of our model. Therefore, before the actual training
    is begun, we need to define a function to evaluate the fine-tuned model.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Step 7: Fine-tune using the Trainer Method'
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final step is fine-tuning the model. To do so, we set up the training arguments
    together with the evaluation strategy and execute the Trainer object.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: To execute the Trainer object we just use the train() command.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Once our model has been fine-tuned, we use the test set to evaluate its performance.
    The trainer object already contains an optimized evaluate() method.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This is a basic process to perform a fine-tuning of any LLM.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Also, remember that the process of fine-tuning a LLM is highly computationally
    demanding, so your local computer may not have enough power to perform it.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Main Conclusions
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Today, fine-tuning pre-trained large language models like GPT for specific tasks
    is crucial to enhancing LLMs performance in specific domains. It allows us to
    take advantage of their natural language power while improving their efficiency
    and the potential for customization, making the process accessible and cost-effective.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Following these simple 7 steps —from selecting the right model and dataset to
    training and evaluating the fine-tuned model— we can achieve a superior model
    performance in specific domains.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: For those who want to check the full code, it is available in my l[arge language
    models GitHub repo. ](https://github.com/rfeers/large-language-models/blob/main/7%20Steps%20to%20Fine-Tune%20LLMs.ipynb)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '**[](https://www.linkedin.com/in/josep-ferrer-sanchez/)**[Josep Ferrer](https://www.linkedin.com/in/josep-ferrer-sanchez)****
    is an analytics engineer from Barcelona. He graduated in physics engineering and
    is currently working in the data science field applied to human mobility. He is
    a part-time content creator focused on data science and technology. Josep writes
    on all things AI, covering the application of the ongoing explosion in the field.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[7 Steps to Mastering Large Language Models (LLMs)](https://www.kdnuggets.com/7-steps-to-mastering-large-language-models-llms)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[RAG vs Finetuning: Which Is the Best Tool to Boost Your LLM Application?](https://www.kdnuggets.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RAG 与微调：哪个是提升你的 LLM 应用的最佳工具？](https://www.kdnuggets.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application)'
- en: '[The Ultimate Open-Source Large Language Model Ecosystem](https://www.kdnuggets.com/2023/05/ultimate-opensource-large-language-model-ecosystem.html)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[终极开源大语言模型生态系统](https://www.kdnuggets.com/2023/05/ultimate-opensource-large-language-model-ecosystem.html)'
- en: '[Introduction to NExT-GPT: Any-to-Any Multimodal Large Language Model](https://www.kdnuggets.com/introduction-to-nextgpt-anytoany-multimodal-large-language-model)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NExT-GPT 介绍：任何到任何的多模态大语言模型](https://www.kdnuggets.com/introduction-to-nextgpt-anytoany-multimodal-large-language-model)'
- en: '[Exploring the Zephyr 7B: A Comprehensive Guide to the Latest Large…](https://www.kdnuggets.com/exploring-the-zephyr-7b-a-comprehensive-guide-to-the-latest-large-language-model)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[探索 Zephyr 7B：最新大语言模型的全面指南](https://www.kdnuggets.com/exploring-the-zephyr-7b-a-comprehensive-guide-to-the-latest-large-language-model)'
- en: '[Free Mastery Course: Become a Large Language Model Expert](https://www.kdnuggets.com/ree-mastery-course-become-a-large-language-model-expert)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[免费精通课程：成为大语言模型专家](https://www.kdnuggets.com/ree-mastery-course-become-a-large-language-model-expert)'
