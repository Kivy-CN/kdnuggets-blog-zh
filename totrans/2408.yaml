- en: Deep Learning Key Terms, Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习关键术语解释
- en: 原文：[https://www.kdnuggets.com/2016/10/deep-learning-key-terms-explained.html](https://www.kdnuggets.com/2016/10/deep-learning-key-terms-explained.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/10/deep-learning-key-terms-explained.html](https://www.kdnuggets.com/2016/10/deep-learning-key-terms-explained.html)
- en: Enjoying a surge in research and industry, due mainly to its incredible successes
    in a number of different areas, deep learning is the process of applying deep
    neural network technologies — that is, neural network architectures with multiple
    hidden layers — to solve problems. Deep learning is a process, like data mining,
    which employs deep neural network architectures, which are particular types of
    machine learning algorithms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习因其在多个领域取得了令人瞩目的成功而研究和产业上都在快速发展。深度学习是应用深度神经网络技术——即具有多个隐藏层的神经网络架构——来解决问题的过程。深度学习是一个过程，类似于数据挖掘，它使用深度神经网络架构，这些架构是特定类型的机器学习算法。
- en: '![Deep Learning Key Terms, Explained](../Images/35cca5c97cb4d4033c23fdadcb5820b8.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习关键术语解释](../Images/35cca5c97cb4d4033c23fdadcb5820b8.png)'
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT需求'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Deep learning has racked up an impressive collection of accomplishments in
    the past several years. In light of this, it''s important to keep a few things
    in mind, at least in my opinion:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在过去几年中取得了令人印象深刻的成就。鉴于此，至少在我看来，重要的是要记住以下几点：
- en: '**Deep learning is not a panacea** - it is not an easy one-size-fits-all solution
    to every problem out there'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度学习不是灵丹妙药** - 它不是一个适用于所有问题的简单解决方案。'
- en: '**It is not the fabled master algorithm** - deep learning will not displace
    all other machine learning algorithms and data science techniques, or, at the
    very least, it has not yet proven so'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它不是传说中的万能算法** - 深度学习不会取代所有其他机器学习算法和数据科学技术，或者至少，它尚未证明如此。'
- en: '**Tempered expectations are necessary** - while great strides have recently
    been made in all types of classification problems, notably computer vision and
    natural language processing, as well as reinforcement learning and other areas,
    contemporary deep learning does not scale to working on very complex problems
    such as "solve world peace"'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适度的期望是必要的** - 尽管在所有类型的分类问题中，特别是计算机视觉和自然语言处理以及强化学习和其他领域，最近取得了巨大进展，但当代深度学习并不能解决诸如“解决世界和平”等非常复杂的问题。'
- en: '**Deep learning and artificial intelligence are not synonymous**'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度学习和人工智能不是同义词**'
- en: Deep learning can provide an awful lot to data science in the form of additional
    processes and tools to help solve problems, and when observed in that light, **deep
    learning is a very valuable addition to the data science landscape**
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习可以为数据科学提供大量的额外过程和工具来帮助解决问题，从这个角度看，**深度学习是数据科学领域的一个非常有价值的补充**。
- en: So, with that, let's look at some deep learning-related terminology, focusing
    on concise, no-nonsense definitions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们看看一些与深度学习相关的术语，专注于简明扼要的定义。
- en: 1\. Deep Learning
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 深度学习
- en: As defined above, deep learning is the process of applying deep neural network
    technologies to solve problems. **Deep** neural networks are neural networks with
    one hidden layer minimum (see below). Like data mining, deep learning refers to
    a **process**, which employs deep neural network architectures, which are particular
    types of machine learning algorithms.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所定义，深度学习是应用深度神经网络技术来解决问题的过程。**深度**神经网络是具有至少一个隐藏层的神经网络（见下文）。类似于数据挖掘，深度学习指的是一种**过程**，它采用深度神经网络架构，这些架构是特定类型的机器学习算法。
- en: 2\. Artificial Neural Networks
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 人工神经网络
- en: The machine learning architecture was originally inspired by the biological
    brain (particularly the neuron) by which deep learning is carried out. Actually,
    artificial neural networks (ANNs) alone (the non-deep variety) have been around
    for a very long time, and have been able to solve certain types of problems historically.
    However, comparatively recently, neural network architectures were devised which
    included layers of hidden neurons (beyond simply the input and output layers),
    and this added level of complexity is what enables deep learning, and provides
    a more powerful set of problem-solving tools.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习架构最初是受生物大脑（特别是神经元）的启发，从而实现深度学习。实际上，人工神经网络（ANNs）本身（非深度类型）已经存在很长时间，并且历史上能够解决某些类型的问题。然而，相对较近的时间内，神经网络架构被设计出来，包含了隐藏神经元层（不仅仅是输入层和输出层），这种附加的复杂性使得深度学习成为可能，并提供了一套更强大的问题解决工具。
- en: ANNs actually vary in their architectures quite considerably, and therefore
    there is no definitive neural network definition. The 2 generally-cited characteristics
    of all ANNs are the possession of adaptive weight sets, and the capability of
    approximating non-linear functions of the inputs to neurons.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ANNs 的架构实际上变化很大，因此没有明确的神经网络定义。所有 ANNs 通常引用的两个特征是具有适应性权重集和近似神经元输入的非线性函数的能力。
- en: 3\. Biological Neuron
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 生物神经元
- en: Much is often made of the definitive connection between biological and artificial
    neural networks. Popular publications propagate the idea that ANNs are somehow
    an exact replica of what's going on in the human (or other biological) brain.
    This is clearly inaccurate; at best, early artificial neural networks were inspired
    by biology. The abstract relationship between the 2 are no more definitive than
    the abstract comparison drawn between the make up and functionality of atoms and
    the solar system.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通常会强调生物神经网络与人工神经网络之间的明确联系。流行的出版物传播的观点是，人工神经网络在某种程度上是对人类（或其他生物）大脑发生的情况的精确复制。这显然是不准确的；充其量，早期的人工神经网络受到生物学的启发。这两者之间的抽象关系不比原子组成与太阳系功能之间的抽象比较更为明确。
- en: That said, it **does** do us some good to see how biological neurons work at
    a very high level, if simply to understand the inspiration for ANNs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，了解生物神经元在很高层次上的工作方式对我们还是有益的，至少可以帮助我们理解人工神经网络的灵感来源。
- en: '![Deep Learning Key Terms, Explained](../Images/5158d1fbee8ed1fd959d9b3c99cf8e3e.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习关键术语解释](../Images/5158d1fbee8ed1fd959d9b3c99cf8e3e.png)'
- en: 'Image source: [Wikipedia](https://en.wikipedia.org/wiki/Neuron)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[维基百科](https://en.wikipedia.org/wiki/Neuron)
- en: 'The major components of the biological neuron of interest to us are:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们感兴趣的生物神经元的主要组件有：
- en: The **nucleus** holds genetic information (i.e. DNA)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**细胞核**储存遗传信息（即 DNA）'
- en: The **cell body** processes input activations and converts them to output activations
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**细胞体**处理输入激活并将其转换为输出激活'
- en: '**Dendrites** receive activations from other neurons'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**树突**接收来自其他神经元的激活'
- en: '**Axons** transmit activations to other neurons'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轴突**将激活传输到其他神经元'
- en: The **axon endings**, along with neighboring dendrites, form the synapses between
    neurons
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轴突末端**与相邻的树突一起形成神经元之间的突触'
- en: Chemicals called neurotransmitters then diffuse across the **synaptic cleft**
    between an axon ending and a neighboring dendrite, constituting a neurotransmission.
    The essential operation of the neuron is that an activation flows into a neuron
    via a dendrite, is processed, and is then retransmitted out an axon, through its
    axon endings, where it crosses the synaptic cleft, and reaches a number of receiving
    neurons’ dendrites, where the process is repeated.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 化学物质称为神经递质，然后扩散穿过**突触间隙**，在轴突末端和相邻的树突之间形成神经递质传输。神经元的基本操作是，激活通过树突流入神经元，经过处理后，通过轴突及其轴突末端重新传输，在这里穿过突触间隙，达到许多接收神经元的树突，过程重复进行。
- en: 4\. Perceptron
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. 感知器
- en: A perceptron is a simple linear binary classifier. Perceptrons take inputs and
    associated weights (representing relative input importance), and combine them
    to produce an output, which is then used for classification. Perceptrons have
    been around a long time, with early implementations dating back to the 1950s,
    the first of which were involved in early ANN implementations.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器是一种简单的线性二分类器。感知器接收输入和相关的权重（表示相对输入的重要性），并将它们组合生成输出，然后用于分类。感知器存在了很长时间，早期的实现可以追溯到
    1950 年代，其中的第一个实现涉及早期的 ANN 实现。
- en: 5\. Multilayer Perceptron
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 多层感知机
- en: A multilayer perceptron (MLP) is the implementation of several fully adjacently-connected
    layers of perceptrons, forming a simple feedforward neural network (see below).
    This multilayer perceptron has the additional benefit of nonlinear activation
    functions, which single perceptrons do not possess.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知机（MLP）是几个完全相邻连接的感知机层的实现，形成一个简单的前馈神经网络（见下文）。这个多层感知机具有非线性激活函数的额外优点，这是单层感知机所不具备的。
- en: 6\. Feedforward Neural Network
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. 前馈神经网络
- en: Feedforward neural networks are the simplest form of neural network architecture,
    in which connections are non-cyclical. The original artificial neural network,
    information in a feedforward network advances in a single direction from the input
    nodes, though any hidden layers, to the output nodes; no cycles are present. Feedforward
    networks differ from later, recurrent network architectures (see below), in which
    connections form a directed cycle.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络是最简单的神经网络架构形式，其中连接是非周期性的。原始的人工神经网络中，信息在前馈网络中单向从输入节点流向输出节点，经过任何隐藏层；没有循环。前馈网络与后来的循环网络架构不同（见下文），后者中的连接形成一个有向循环。
- en: 7\. Recurrent Neural Network
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7\. 循环神经网络
- en: In contrast to the above feedforward neural networks, the connections of recurrent
    neural networks form a directed cycle. This bidirectional flow allows for internal
    temporal state representation, which, in turn, allows sequence processing, and,
    of note, provides the necessary capabilities for recognizing speech and handwriting.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述前馈神经网络相对的是，循环神经网络的连接形成一个有向循环。这种双向流动允许内部时间状态的表示，这反过来允许序列处理，并且，值得注意的是，提供了识别语音和手写的必要能力。
- en: 8\. Activation Function
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8\. 激活函数
- en: In neural networks, the activation function produces the output decision boundaries
    by combining the network's weighted inputs. Activation functions range from identity
    (linear) to sigmoid (logistic, or soft step) to hyperbolic (tangent) and beyond.
    In order to employ backpropagation (see below), the network must utilize activation
    functions which are differentiable.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，激活函数通过结合网络的加权输入来生成输出决策边界。激活函数的范围从恒等函数（线性）到 sigmoid 函数（逻辑函数或软阶跃）再到双曲正切函数及其他。为了使用反向传播（见下文），网络必须使用可微的激活函数。
- en: 9\. Backpropagation
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9\. 反向传播
- en: 'The best concise, **elementary** definition of backpropagation I have ever
    come across was by data scientist [Mikio L. Braun](https://www.quora.com/profile/Mikio-L-Braun),
    giving the following answer [on Quora](https://www.quora.com/How-do-you-explain-back-propagation-algorithm-to-a-beginner-in-neural-network/answer/Mikio-L-Braun),
    which I reproduce verbatim so as not soil its simple perfection:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我见过的最简明的**基本**反向传播定义来自数据科学家[Mikio L. Braun](https://www.quora.com/profile/Mikio-L-Braun)，他在[Quora](https://www.quora.com/How-do-you-explain-back-propagation-algorithm-to-a-beginner-in-neural-network/answer/Mikio-L-Braun)上给出了如下回答，我将其逐字转述，以保持其简单的完美：
- en: Back prop is just gradient descent on individual errors. You compare the predictions
    of the neural network with the desired output and then compute the gradient of
    the errors with respect to the weights of the neural network. This gives you a
    direction in the parameter weight space in which the error would become smaller.
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 反向传播只是对个别错误的梯度下降。你将神经网络的预测与期望输出进行比较，然后计算错误相对于神经网络权重的梯度。这为你提供了在参数权重空间中使错误变小的方向。
- en: 10\. Cost Function
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10\. 成本函数
- en: When training a neural network, the correctness of the network's output must
    be assessed. As we know the expected correct output of training data, the output
    of training can be compared. The cost function measures the difference between
    actual and training outputs. A cost of zero between the actual and expected outputs
    would signify that the network has been training as would be possible; this would
    clearly be ideal.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络时，必须评估网络输出的正确性。由于我们知道训练数据的期望正确输出，可以对比训练输出。成本函数衡量实际输出与期望输出之间的差异。如果实际输出与期望输出之间的成本为零，则表示网络已经以可能的最佳方式进行训练；这显然是理想的。
- en: So, by what mechanism is the cost function adjusted, with a goal of minimizing
    it?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，成本函数是通过什么机制进行调整的，以实现最小化的目标呢？
- en: 11\. Gradient Descent
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11\. 梯度下降
- en: Gradient descent is an optimization algorithm used for finding local minima
    of functions. While it does not guarantee a global minimum, gradient descent is
    especially useful for functions which are difficult to solve analytically for
    precise solutions, such as setting derivatives to zero and solving.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种优化算法，用于寻找函数的局部极小值。虽然它不能保证全局最小值，但梯度下降对于那些难以通过解析方法精确求解的函数尤其有用，比如设置导数为零并求解。
- en: '![Gradient descent ](../Images/94eda817597caeb0cdaf77e939e34353.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![梯度下降](../Images/94eda817597caeb0cdaf77e939e34353.png)'
- en: As alluded to above, in the context of neural networks, **stochastic gradient
    descent** is used to make informed adjustments to your network's parameters with
    the goal of minimizing the cost function, thus bringing your network's actual
    outputs closer and closer, iteratively, to the expected outputs during the course
    of training. This iterative minimization employs calculus, namely differentiation.
    After a training step, the network weights receive updates according the gradient
    of the cost function and the network's current weights, so that the next training
    step's results may be a little closer to correct (as measured by a smaller cost
    function). Backpropagation (backward propagation of errors) is the method used
    to dole these updates out to the network.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，在神经网络的背景下，**随机梯度下降**用于对网络的参数进行有信息的调整，目标是最小化成本函数，从而使网络的实际输出逐步接近预期输出。这种迭代的最小化使用了微积分，特别是微分。在每次训练步骤后，网络权重根据成本函数的梯度和网络当前的权重进行更新，以便下一次训练步骤的结果可能更接近正确（通过较小的成本函数来衡量）。反向传播（误差的反向传播）是用于将这些更新分配到网络的方法。
- en: 12\. Vanishing Gradient Problem
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12. 消失梯度问题
- en: Backpropagation uses the [chain rule](https://en.wikipedia.org/wiki/Chain_rule)
    to compute gradients (by differentiation), in that layers toward the "front" (input)
    of an *n*-layer neural network would have their small number updated gradient
    value multiplied *n* times before having this settled value used as an update.
    This means that the gradient would decrease exponentially, a problem with larger
    values of *n*, and front layers would take increasingly more time to train effectively.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播使用[链式法则](https://en.wikipedia.org/wiki/Chain_rule)来计算梯度（通过微分），即在* n* 层神经网络的“前面”（输入）层，其小数量的更新梯度值会被乘以*n*次，然后将这一稳定值用作更新。这意味着梯度会呈指数级减少，这是较大值的*n*的一个问题，前面层需要更长时间才能有效训练。
- en: 13\. Convolutional Neural Network
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13. 卷积神经网络
- en: Typically associated with computer vision and image recogntion, Convolutional
    Neural Networks (CNNs) employ the mathematical concept of convolution to mimic
    the neural connectivity mesh of the biological visual cortex.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通常与计算机视觉和图像识别相关，卷积神经网络（CNNs）运用卷积的数学概念来模拟生物视觉皮层的神经连接网。
- en: First, convolution, as [nicely described by Denny Britz](/2015/11/understanding-convolutional-neural-networks-nlp.html),
    can be thought of as a sliding window over top a matrix representation of an image
    (see below). This allows for the loose mimicking of the overlapping tiling of
    the biological visual field.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，卷积如[Denny Britz 详尽描述的](../2015/11/understanding-convolutional-neural-networks-nlp.html)那样，可以被视为在图像的矩阵表示上滑动的窗口（见下文）。这允许松散地模拟生物视觉场的重叠铺排。
- en: '![Convolution](../Images/2975d711123d889f2cba78afe99c5195.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![卷积](../Images/2975d711123d889f2cba78afe99c5195.png)'
- en: 'Image credit: [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/07/convolution-neural-network-the-base-for-many-deep-learning-algorithms-cnn-illustrated-by-1-d-ecg-signal-physionet/)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/07/convolution-neural-network-the-base-for-many-deep-learning-algorithms-cnn-illustrated-by-1-d-ecg-signal-physionet/)
- en: Implementation of this concept in the architecture of the neural network results
    in collections of neurons dedicated to processing image sections, at least when
    employed in computer vision. When utilized in some other domain, such as natural
    language processing, the same approach can be used, given that input (words, sentences,
    etc.) could be arranged in matrices and processed in similar fashion.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络架构中实施这一概念会导致专门处理图像部分的神经元集合，至少在计算机视觉中是这样。当在其他领域使用时，如自然语言处理，同样的方法可以应用，因为输入（单词、句子等）可以被排列成矩阵，并以类似的方式处理。
- en: 14\. Long Short Term Memory Network
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14. 长短期记忆网络
- en: A Long Short Term Memory Network (LSTM) is a recurrent neural network which
    is optimized for learning from and acting upon time-related data which may have
    undefined or unknown lengths of time between events of relevance. Their particular
    architecture allows for persistence, giving the ANN a "memory." Recent breakthroughs
    in handwriting recognition and automatic speech recognition have benefited from
    LSTM networks.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 长短期记忆网络（LSTM）是一种递归神经网络，优化用于学习和处理时间相关的数据，这些数据可能在相关事件之间有不确定或未知的时间间隔。它们特有的架构允许持久性，为人工神经网络提供了“记忆”。最近在手写识别和自动语音识别领域的突破受益于LSTM网络。
- en: '![LSTM](../Images/93ed8d39f58131aeb44cb0f2e4b38e98.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![LSTM](../Images/93ed8d39f58131aeb44cb0f2e4b38e98.png)'
- en: 'Image credit: [Christopher Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: [克里斯托弗·奥拉赫](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
- en: This is clearly only a small subset of deep learning terminology, and many additional
    concepts, from elementary to advanced, await your exploration as you learn more
    about the current leading field in machine learning research.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然只是深度学习术语的一个小子集，许多额外的概念，从基础到高级，等待着你在深入了解当前领先的机器学习研究领域时去探索。
- en: '**[Matthew Mayo](https://www.linkedin.com/in/mattmayo13/)** ([**@mattmayo13**](https://twitter.com/mattmayo13))
    is a Data Scientist and the Editor-in-Chief of KDnuggets, the seminal online Data
    Science and Machine Learning resource. His interests lie in natural language processing,
    algorithm design and optimization, unsupervised learning, neural networks, and
    automated approaches to machine learning. Matthew holds a Master''s degree in
    computer science and a graduate diploma in data mining. He can be reached at editor1
    at kdnuggets[dot]com.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**[马修·梅约](https://www.linkedin.com/in/mattmayo13/)** （[**@mattmayo13**](https://twitter.com/mattmayo13)）是一名数据科学家，同时也是KDnuggets的总编辑，KDnuggets是一个开创性的在线数据科学和机器学习资源。他的兴趣领域包括自然语言处理、算法设计与优化、无监督学习、神经网络以及机器学习的自动化方法。马修拥有计算机科学硕士学位和数据挖掘研究生文凭。他可以通过editor1
    at kdnuggets[dot]com联系到。'
- en: More On This Topic
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Machine Learning Key Terms, Explained](https://www.kdnuggets.com/2016/05/machine-learning-key-terms-explained.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习关键术语详解](https://www.kdnuggets.com/2016/05/machine-learning-key-terms-explained.html)'
- en: '[Database Key Terms, Explained](https://www.kdnuggets.com/2016/07/database-key-terms-explained.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据库关键术语详解](https://www.kdnuggets.com/2016/07/database-key-terms-explained.html)'
- en: '[Descriptive Statistics Key Terms, Explained](https://www.kdnuggets.com/2017/05/descriptive-statistics-key-terms-explained.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[描述性统计关键术语详解](https://www.kdnuggets.com/2017/05/descriptive-statistics-key-terms-explained.html)'
- en: '[Natural Language Processing Key Terms, Explained](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理关键术语详解](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
- en: '[Generative AI Key Terms Explained](https://www.kdnuggets.com/generative-ai-key-terms-explained)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[生成式AI关键术语详解](https://www.kdnuggets.com/generative-ai-key-terms-explained)'
- en: '[Genetic Algorithm Key Terms, Explained](https://www.kdnuggets.com/2018/04/genetic-algorithm-key-terms-explained.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[遗传算法关键术语详解](https://www.kdnuggets.com/2018/04/genetic-algorithm-key-terms-explained.html)'
