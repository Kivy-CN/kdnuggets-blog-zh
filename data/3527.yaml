- en: 'In Machine Learning, What is Better: More Data or better Algorithms'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中，哪个更好：更多数据还是更好的算法？
- en: 原文：[https://www.kdnuggets.com/2015/06/machine-learning-more-data-better-algorithms.html](https://www.kdnuggets.com/2015/06/machine-learning-more-data-better-algorithms.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2015/06/machine-learning-more-data-better-algorithms.html](https://www.kdnuggets.com/2015/06/machine-learning-more-data-better-algorithms.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By Xavier Amatriain** (VP of Engineering at Quora).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 Xavier Amatriain**（Quora 的工程副总裁）。'
- en: “In machine learning, is more data always better than better algorithms?” No. There
    are times when more data helps, there are times when it doesn’t.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: “在机器学习中，更多数据是否总是比更好的算法更好？” 不一定。更多数据有时会有帮助，有时则没有。
- en: Probably one of the most famous quotes defending the power of data is that of
    Google’s Research Director Peter Norvig claiming that “We don’t have better algorithms.
    We just have more data.”. This quote is usually linked to the article on “The
    Unreasonable Effectiveness of Data”, co-authored by Norvig  himself (you should
    probably be able to find the pdf on the web although [the original](https://googleresearch.blogspot.com/2009/03/unreasonable-effectiveness-of-data.html)
    is behind the IEEE paywall). The last nail on the coffin of better models is when
    Norvig is misquoted as saying that “All models are wrong, and you don’t need them
    anyway” (read [here](http://norvig.com/fact-check.html) for the author’s own clarifications
    on how he was misquoted).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最著名的支持数据力量的名言是 Google 的研究总监 Peter Norvig 宣称的 “我们没有更好的算法。我们只是有更多的数据。”。这句话通常与
    Norvig 本人合著的《数据的非理性有效性》一文有关（你可能可以在网上找到 pdf，虽然 [原文](https://googleresearch.blogspot.com/2009/03/unreasonable-effectiveness-of-data.html)
    被 IEEE 付费墙挡住了）。当 Norvig 被误引为说 “所有模型都是错误的，你根本不需要它们” 时，关于更好模型的最后一根钉子就被钉上了（阅读 [这里](http://norvig.com/fact-check.html)
    了解作者对如何被误引的澄清）。
- en: '![](../Images/19fca3acedbb3ad590829f7049690d06.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19fca3acedbb3ad590829f7049690d06.png)'
- en: The effect that Norvig et. al were referring to in their article, had already
    been captured years before in the famous paper by Microsoft Researchers Banko
    and Brill [2001]” [Scaling to Very Very Large Corpora for Natural Language Disambiguation](http://acl.ldc.upenn.edu/P/P01/P01-1005.pdf)“.
    In that paper, the authors included the plot below.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Norvig 等人在他们的文章中提到的效果，早在微软研究员 Banko 和 Brill 的著名论文《Scaling to Very Very Large
    Corpora for Natural Language Disambiguation》[2001] 中就已经被捕捉到了。在那篇论文中，作者展示了下面的图表。
- en: '![](../Images/d5daaabc8d7789addd6dea6cbfa04418.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5daaabc8d7789addd6dea6cbfa04418.png)'
- en: That figure shows that, for the given problem, very different algorithms perform
    virtually the same. however, adding more examples (words) to the training set
    monotonically increases the accuracy of the model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 该图显示，对于给定的问题，非常不同的算法几乎表现相同。然而，向训练集添加更多示例（词汇）会单调地提高模型的准确性。
- en: So, case closed, you might think. Well… not so fast. The reality is that both
    Norvig’s assertions and Banko and Brill’s paper are right… in a context. But,
    they are now and again misquoted in contexts that are completely different than
    the original ones. But, in order to understand why, we need to get slightly technical. 
    (I don’t plan on giving a full machine learning tutorial in this post. If you
    don’t understand what I explain below, read my answer to [How do I learn machine
    learning?](https://www.quora.com/How-do-I-learn-machine-learning-1/answer/Xavier-Amatriain))
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你可能会认为案子已经结案了。实际上，情况并非如此。现实是，Norvig 的论断和 Banko 与 Brill 的论文在某种背景下都是正确的。但它们有时会被引用在与原始背景完全不同的上下文中。不过，为了理解为什么如此，我们需要稍微深入一些技术细节。（我不打算在这篇文章中提供完整的机器学习教程。如果你不理解我下面解释的内容，请阅读我对
    [如何学习机器学习？](https://www.quora.com/How-do-I-learn-machine-learning-1/answer/Xavier-Amatriain)
    的回答。）
- en: '**Variance or Bias?**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**方差还是偏差？**'
- en: The basic idea is that there are two possible (and almost opposite) reasons
    a model might not perform well.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的想法是，模型表现不佳可能有两个可能（且几乎相反）的原因。
- en: 'In the first case, we might have a model that is too complicated for the amount
    of data we have. This situation, known as*high variance*, leads to model overfitting.
    We know that we are facing a high variance issue when the training error is much
    lower than the test error. High variance problems can be addressed by reducing
    the number of features, and… yes, by increasing the number of data points. So,
    what kind of models were Banko & Brill’s, and Norvig dealing with? Yes, you got
    it right: high variance. In both cases, the authors were  working on language
    models in which roughly every word in the vocabulary makes a feature. These are
    models with many features as compared to the training examples. Therefore, they
    are likely to overfit. And, yes, in this case adding more examples will help.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种情况下，我们可能会有一个模型对我们拥有的数据量过于复杂。这种情况被称为*高方差*，导致模型过拟合。我们知道当训练误差远低于测试误差时，我们面临的是高方差问题。高方差问题可以通过减少特性数量来解决，并且…是的，通过增加数据点的数量来解决。那么，Banko
    & Brill 和 Norvig 处理的是哪种模型？是的，你猜对了：高方差。在这两种情况下，作者们都在处理语言模型，其中词汇表中的每个词大致都会成为一个特性。这些模型与训练示例相比，特性数量非常多。因此，它们很可能会过拟合。是的，在这种情况下，添加更多示例将有帮助。
- en: But, in the opposite case, we might have a model that is too simple to explain
    the data we have. In that case, known as *high bias*, adding more data will not
    help.  See below a plot of a real production system at Netflix and its performance
    as we add more training examples.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，相反的情况是，我们可能会有一个模型过于简单，无法解释我们拥有的数据。在这种情况下，被称为*高偏差*，添加更多的数据是没有帮助的。下面是Netflix的真实生产系统的一个图示，以及随着我们添加更多训练示例时的性能。
- en: '![](../Images/7fc45f89a72a843c9fdae7b364438f4d.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7fc45f89a72a843c9fdae7b364438f4d.png)'
- en: So, no, **more data does not always help**. As we have just seen there can be
    many cases in which adding more examples to our training set will not improve
    the model performance.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，不，**更多数据并不总是有帮助**。正如我们刚刚看到的，有许多情况添加更多示例到我们的训练集中不会改善模型的性能。
- en: '**More features to the rescue**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**更多特性来救援**'
- en: If you are with me so far, and you have done your homework in understanding
    high variance and high bias problems, you might be thinking that I have deliberately
    left something out of the discussion. Yes, high bias models will not benefit from
    more training examples, but they might very well benefit from more features. So,
    in the end, it is all about adding “more” data, right? Well, again, it depends.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你到目前为止跟上了我的思路，并且你已经做了功课理解了高方差和高偏差问题，你可能会认为我故意遗漏了某些讨论内容。是的，高偏差模型不会从更多的训练示例中受益，但它们可能会非常受益于更多的特性。所以，最终，这一切都是关于添加“更多”数据，对吗？嗯，再次，这要视情况而定。
- en: Let’s take the Netflix Prize, for example. Pretty early on in the game, there
    was [a blog post](http://anand.typepad.com/datawocky/2008/03/more-data-usual.html)
    by serial entrepreneur and Stanford professor [Anand Rajaraman](https://en.wikipedia.org/wiki/Anand_Rajaraman)
    commenting on the use of extra features to solve the problem. The post explains
    how a team of students got an improvement on the prediction accuracy by adding
    content features from IMDb.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以Netflix奖为例。在游戏初期，有[一篇博客文章](http://anand.typepad.com/datawocky/2008/03/more-data-usual.html)由连续创业者和斯坦福大学教授[Anand
    Rajaraman](https://en.wikipedia.org/wiki/Anand_Rajaraman)评论使用额外特性来解决问题。文章解释了一个学生团队通过添加来自IMDb的内容特性来提高预测准确率。
- en: '![](../Images/e5868e9ed42194643ec16d1e553f347c.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e5868e9ed42194643ec16d1e553f347c.png)'
- en: In retrospect, it is easy to criticize the post for making a gross over-generalization
    from a single data point. Even more, the [follow-up post](http://anand.typepad.com/datawocky/2008/04/data-versus-alg.html)
    references SVD as one of the “complex” algorithms not worth trying because it
    limits the ability of scaling up to larger number of features. Clearly, Anand’s
    students did not win the Netflix Prize, and they probably now realize that SVD
    did have a major role in the winning entry.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 从 retrospect 来看，很容易批评这篇文章因为从一个数据点做了过度概括。更重要的是，[后续文章](http://anand.typepad.com/datawocky/2008/04/data-versus-alg.html)提到了SVD作为一种“复杂”的算法，不值得尝试，因为它限制了扩展到更多特性的能力。显然，Anand的学生没有赢得Netflix奖，他们现在可能意识到SVD确实在获胜条目中扮演了重要角色。
- en: 'As a matter of fact, many teams showed later that adding content features from
    IMDB or the like to an optimized algorithm had little to no improvement. Some
    of the members of the [Gravity team](http://www.gravityrd.com/references/netflix-prize?lang=en),
    one of the top contenders for the Prize, published a detailed paper in which they
    showed how those content-based features would add no improvement to the highly
    optimized collaborative filtering matrix factorization approach. The paper was
    entitled [Recommending New Movies: Even a Few Ratings Are More Valuable Than Metadata](http://dl.acm.org/citation.cfm?id=1639731&dl=ACM&coll=DL&CFID=122239967&CFTOKEN=16331362).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，许多团队后来展示了，将来自 IMDB 或类似来源的内容特征添加到优化算法中几乎没有任何改善。[重力团队](http://www.gravityrd.com/references/netflix-prize?lang=en)，作为奖项的顶级竞争者之一，发布了一篇详细的论文，展示了这些基于内容的特征如何对高度优化的协同过滤矩阵分解方法没有任何改善。该论文题为[推荐新电影：即使是少量评分也比元数据更有价值](http://dl.acm.org/citation.cfm?id=1639731&dl=ACM&coll=DL&CFID=122239967&CFTOKEN=16331362)。
- en: '![](../Images/55db293f30aba4d9ca8f500e583052e5.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55db293f30aba4d9ca8f500e583052e5.png)'
- en: 'To be fair, the title of the paper is also an over-generalization. Content-based
    features (or different features in general) might be able to improve accuracy
    in many cases. But, you get my point again: **More data does not always help**.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 公平地说，论文的标题也是一种过度概括。基于内容的特征（或一般的不同特征）在许多情况下可能能够提高准确性。但你明白我的观点：**更多的数据并不总是有帮助的**。
- en: '**Better Data != More Data (Added this section** ***in response to a comment)***'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**更好的数据 != 更多的数据（新增此部分** ***以回应一个评论）***'
- en: It is important to point out that, in my opinion, better data is always better.
    There is no arguing against that. So any effort you can direct towards “improving”
    your data is always well invested. The issue is that better data does not mean
    **more** data. As a matter of fact, sometimes it might mean **less**!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 需要指出的是，在我看来，更好的数据总是更好。这一点没有争论。所以你可以投入任何精力去“改善”你的数据，总是值得的。问题是，更好的数据并不意味着**更多**的数据。事实上，有时这可能意味着**更少**！
- en: Think of data cleansing or outlier removal as one trivial illustration of my
    point. But, there are many other examples that are more subtle. For example, I
    have seen people invest a lot of effort in implementing distributed [Matrix Factorization](https://www.quora.com/Matrix-Factorization)
    when the truth is that they could have probably gotten by with sampling their
    data and gotten to very similar results. In fact, doing some form of smart sampling
    on your population the right way (e.g. using stratified sampling) can get you
    to better results than if you used the whole unfiltered data set.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以数据清洗或异常值移除作为我观点的一个简单例子。但，还有许多其他更微妙的例子。例如，我见过有人花费大量精力实施分布式[矩阵分解](https://www.quora.com/Matrix-Factorization)，而实际上他们可能只需对数据进行采样就能得到非常相似的结果。事实上，对你的人群进行某种形式的智能采样（例如，使用分层抽样）可以比使用完整的未过滤数据集得到更好的结果。
- en: '**The End of the Scientific Method?**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**科学方法的终结？**'
- en: 'Of course, whenever there is a heated debate about a possible paradigm change,
    there are people like Malcolm Gladwell or Chris Anderson that make a living out
    of heating it even more (don’t get me wrong, I am a fan of both, and have read
    most of their books). In this case, Anderson picked on some of Norvig’s comments,
    and misquoted them in an article entitled: [The End of Theory: The Data Deluge
    Makes the Scientific Method Obsolete](http://www.wired.com/science/discoveries/magazine/16-07/pb_theory/).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，每当关于可能的范式变化进行激烈辩论时，总会有像马尔科姆·格拉德威尔或克里斯·安德森这样的人通过加剧讨论来谋生（不要误解我的意思，我是他们的粉丝，并且读过他们的大部分书籍）。在这种情况下，安德森挑剔了一些诺维格的评论，并在一篇题为[理论的终结：数据洪流使科学方法过时](http://www.wired.com/science/discoveries/magazine/16-07/pb_theory/)的文章中对其进行了误引。
- en: '![](../Images/b899e2f67ba919e641f8e16f215f3bff.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b899e2f67ba919e641f8e16f215f3bff.png)'
- en: 'The article explains several examples of how the abundance of data helps people
    and companies take decision without even having to understand the meaning of the
    data itself. As Norvig himself points out in [his rebuttal](http://norvig.com/fact-check.html),
    Anderson has a few points right, but goes above and beyond to try to make them.
    And the result is a set of false statements, starting from the title: the data
    deluge does not make the scientific method obsolete. I would argue it is rather
    the other way around.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '-   文章解释了数据丰富如何帮助个人和公司做出决策，即使不理解数据本身的意义。如Norvig在[他的反驳](http://norvig.com/fact-check.html)中指出，Anderson有些观点是正确的，但为了证明这些观点却走得太远。结果是一系列错误的陈述，从标题开始：数据洪流并没有使科学方法过时。我认为恰恰相反。'
- en: '**Data Without a Sound Approach = Noise**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**没有良好方法的数据 = 噪声**'
- en: So, am I trying to make the point that the Big Data revolution is only hype?
    No way. Having more data, both in terms of more examples or more features, is
    a blessing. The availability of data enables more and better insights and applications.
    More data indeed enables better approaches. More than that, it **requires** better
    approaches.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我是否在试图表明大数据革命只是炒作？绝对不是。拥有更多数据，无论是更多的样本还是更多的特征，都是一种福祉。数据的可用性带来了更多更好的洞察和应用。更多的数据确实使方法更好。更重要的是，它**要求**更好的方法。
- en: '![](../Images/a3465a3c529b650114d425d05eb9c1ac.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3465a3c529b650114d425d05eb9c1ac.png)'
- en: In summary, we should dismiss simplistic voices that proclaim the uselessness
    of theory or models, or the triumph of data over these. As much as data is needed,
    so are good models and theory that explains them. But, overall, what we need is
    good approaches that help us understand how to interpret data, models, and the
    limitations of both in order to produce the best possible output.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '-   总之，我们应该摒弃那些简单化的声音，它们宣称理论或模型毫无用处，或者数据战胜了这些。数据固然重要，但同样需要好的模型和解释它们的理论。总体来说，我们需要的是有效的方法，帮助我们理解如何解读数据、模型以及两者的局限性，从而产生最佳的输出。'
- en: In other words, data is important. But, data without a sound approach becomes
    noise.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '-   换句话说，数据很重要。但没有良好方法的数据就成了噪声。'
- en: '**[Original](https://www.quora.com/In-machine-learning-is-more-data-always-better-than-better-algorithms/answer/Xavier-Amatriain)**.
    Reposted with permission.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**[原文](https://www.quora.com/In-machine-learning-is-more-data-always-better-than-better-algorithms/answer/Xavier-Amatriain)**。已获得许可转载。'
- en: '**Bio: ** [Xavier Amatriain](http://xavier.amatriain.net/), is a VP of Engineering
    at Quora, well known for his work on Recommender Systems and Machine Learning.
    He build teams and algorithms to solve hard problems with business impact. Previously,
    he was Research/Engineering Director at Netflix.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：** [Xavier Amatriain](http://xavier.amatriain.net/)，是Quora的工程副总裁，因其在推荐系统和机器学习方面的工作而闻名。他建立了团队和算法，以解决具有商业影响的难题。此前，他曾是Netflix的研究/工程总监。'
- en: '**Related**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关**'
- en: '[Debunking Big Data Myths. Again](/2015/01/debunking-big-data-myths-again.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[揭穿大数据神话，再次](/2015/01/debunking-big-data-myths-again.html)'
- en: '[Interview: Josh Hemann, Activision on Why the Tolerance for Ambiguity is Vital](/2015/03/interview-josh-hemann-activision-data-science.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[采访：Josh Hemann，Activision谈为什么对模糊性的容忍至关重要](/2015/03/interview-josh-hemann-activision-data-science.html)'
- en: '[Will Deep Learning take over Machine Learning, make other algorithms obsolete?](/2014/10/deep-learning-make-machine-learning-algorithms-obsolete.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习会取代机器学习，使其他算法过时吗？](/2014/10/deep-learning-make-machine-learning-algorithms-obsolete.html)'
- en: '* * *'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '-   我们的前三大课程推荐'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的IT工作'
- en: '* * *'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[15 More Free Machine Learning and Deep Learning Books](https://www.kdnuggets.com/2022/11/15-free-machine-learning-deep-learning-books.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15 本更多的免费机器学习和深度学习书籍](https://www.kdnuggets.com/2022/11/15-free-machine-learning-deep-learning-books.html)'
- en: '[KDnuggets News, June 22: Primary Supervised Learning Algorithms…](https://www.kdnuggets.com/2022/n25.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，6 月 22 日：主要监督学习算法…](https://www.kdnuggets.com/2022/n25.html)'
- en: '[Primary Supervised Learning Algorithms Used in Machine Learning](https://www.kdnuggets.com/2022/06/primary-supervised-learning-algorithms-used-machine-learning.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中使用的主要监督学习算法](https://www.kdnuggets.com/2022/06/primary-supervised-learning-algorithms-used-machine-learning.html)'
- en: '[Why are More Developers Using Python for Their Machine Learning Projects?](https://www.kdnuggets.com/2022/01/developers-python-machine-learning-projects.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么越来越多的开发者选择 Python 进行机器学习项目？](https://www.kdnuggets.com/2022/01/developers-python-machine-learning-projects.html)'
- en: '[A (Much) Better Approach to Evaluate Your Machine Learning Model](https://www.kdnuggets.com/2022/01/much-better-approach-evaluate-machine-learning-model.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[评估机器学习模型的（更好）方法](https://www.kdnuggets.com/2022/01/much-better-approach-evaluate-machine-learning-model.html)'
- en: '[Essential Machine Learning Algorithms: A Beginner''s Guide](https://www.kdnuggets.com/2021/05/essential-machine-learning-algorithms-beginners.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基本机器学习算法：初学者指南](https://www.kdnuggets.com/2021/05/essential-machine-learning-algorithms-beginners.html)'
