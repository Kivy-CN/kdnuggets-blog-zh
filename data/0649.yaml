- en: Using AI to Identify Wildlife in Camera Trap Images from the Serengeti
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用人工智能识别塞伦盖蒂相机陷阱图像中的野生动物
- en: 原文：[https://www.kdnuggets.com/2020/02/using-ai-identify-wildlife-images-serengeti.html](https://www.kdnuggets.com/2020/02/using-ai-identify-wildlife-images-serengeti.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/02/using-ai-identify-wildlife-images-serengeti.html](https://www.kdnuggets.com/2020/02/using-ai-identify-wildlife-images-serengeti.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Marek Rogala](https://www.linkedin.com/in/marrogala/) and [Jędrzej Świeżewski,
    PhD,](https://www.linkedin.com/in/swiezew/) [Appsilon](https://appsilon.com/)**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Marek Rogala](https://www.linkedin.com/in/marrogala/) 和 [Jędrzej Świeżewski,
    PhD](https://www.linkedin.com/in/swiezew/) [Appsilon](https://appsilon.com/)**'
- en: An opportunity for biodiversity research...
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于生物多样性研究的机会...
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的快车道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的IT需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Camera trap imaging (automatic photography of animal species in the wild) is
    becoming the gold standard in biodiversity conservation efforts. It allows for
    accurately monitoring large swaths of land at an unprecedented scale. However,
    the sheer volume of data generated using these devices makes it very difficult
    for humans to analyse. With recent developments in machine learning and computer
    vision, we acquired the tools to resolve this issue and provide the biodiversity
    community with an ability to tap the potential of the knowledge generated automatically
    with systems triggered by a combination of heat and motion.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 相机陷阱成像（自动拍摄野生动物物种的摄影）正成为生物多样性保护工作的黄金标准。它能够在前所未有的规模上准确监测大片土地。然而，使用这些设备生成的数据量巨大，使得人类分析非常困难。随着机器学习和计算机视觉的最新发展，我们获得了处理这个问题的工具，为生物多样性社区提供了利用由热量和运动触发的系统自动生成知识的潜力。
- en: '![Figure](../Images/03a373af4a8cc9ecf8e46e92aa13c817.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/03a373af4a8cc9ecf8e46e92aa13c817.png)'
- en: Camera traps, arguably less obstructive than human surveyors, still attract
    attention
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 相机陷阱，或许比人工调查员干扰更小，但仍然引起关注
- en: …and a challenge for the Machine Learning community
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: …以及对机器学习社区的挑战
- en: We recently took part in **Hakuna-ma Data**, a competition organised by [DrivenData](https://www.drivendata.org/competitions/59/camera-trap-serengeti/page/145/) in
    partnership with [Microsoft’s AI for Earth](https://www.microsoft.com/en-us/ai/ai-for-earth),
    which asked participants to build an algorithm for wildlife detection that would
    generalise well across time and locations. This competition differed from previous
    iterations in the sense that researchers, data scientists and developers did not
    have direct access to the test image set. Rather, they were asked to submit their
    models and have them executed by the event organizers in Microsoft Azure. 811
    participants from around the world trained their models on the publicly available
    data from 10 seasons to finally submit their solution to be tested on the 11th
    season’s private data set.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最近参加了**Hakuna-ma Data**，这是由[DrivenData](https://www.drivendata.org/competitions/59/camera-trap-serengeti/page/145/)与[微软AI
    for Earth](https://www.microsoft.com/en-us/ai/ai-for-earth)合作组织的比赛，要求参与者构建一个能够在不同时间和地点良好泛化的野生动物检测算法。与以往的比赛不同，这次比赛中研究人员、数据科学家和开发人员无法直接访问测试图像集，而是需要提交他们的模型，由活动组织者在微软Azure中执行。来自全球的811名参与者使用10个季节的公开数据训练他们的模型，最后将解决方案提交至第11季的私有数据集进行测试。
- en: We congratulate all our fellow competitors and the *ValAn_picekl* team for taking
    the grand prize of $12,000\. We are also proud to announce that we took 5th place
    in the final leaderboard and we would like to share how we managed to achieve
    this.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们祝贺所有参赛者以及*ValAn_picekl*团队获得了12,000美元的大奖。我们也很自豪地宣布，我们在最终排行榜中获得了第五名，并且愿意分享我们是如何实现这一成就的。
- en: You can play with our final model in a ready-to-run Google Colab notebook [here](https://colab.research.google.com/github/Appsilon/serengeti_try_it_yourself/blob/master/classify_images_on_colab.ipynb).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[这里](https://colab.research.google.com/github/Appsilon/serengeti_try_it_yourself/blob/master/classify_images_on_colab.ipynb)的Google
    Colab笔记本中试用我们的最终模型。
- en: Our approach
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们的方法
- en: The competition involved processing large volumes of images so using a fast
    neural network framework with strong GPUs in the backend was a must. We were provisioning
    virtual machines from Google Cloud Platform equipped with sufficiently powerful
    GPUs (ranging from Tesla K80 to Tesla V100 for the most heavy lifting). Typically
    we were running a few of them in parallel to speed up experimentation. We worked
    with Python code, coordinating crucial parts of code in a GitHub repository and
    using notebooks for fast experimenting and visual inspection of the performance
    of models. We built the models in Fast.ai (with PyTorch backend), and used the
    integration with [Weights & Biases](https://www.wandb.com/) for keeping track
    of our experiments.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 比赛涉及处理大量图像，因此必须使用快速的神经网络框架，并配备强大的GPU。我们从Google Cloud Platform配置了虚拟机，配备了足够强大的GPU（从Tesla
    K80到Tesla V100，用于最重的计算任务）。通常，我们会并行运行几个虚拟机以加快实验速度。我们使用Python代码，在GitHub仓库中协调关键代码部分，并利用笔记本进行快速实验和模型性能的可视化检查。我们在Fast.ai（PyTorch后台）中构建模型，并利用[Weights
    & Biases](https://www.wandb.com/)跟踪我们的实验。
- en: Based on previous [studies](https://www.pnas.org/content/115/25/E5716) of the
    dataset, we decided to work with ResNet 50, an architecture of a rather moderate
    depth. Since we joined the competition very close to the end, we opted not to
    experiment with other (or simply deeper) architectures, and after seeing promising
    initial results of this one, decided to focus on improving them.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 根据之前对数据集的[研究](https://www.pnas.org/content/115/25/E5716)，我们决定使用ResNet 50，这是一种相当适中的深度架构。由于我们在比赛接近尾声时才加入，因此选择不实验其他（或更深的）架构。在看到这个架构的初步结果有前景后，我们决定专注于改进这些结果。
- en: 'Given the lack of time we had to optimise our strategy. We used an agile approach:
    we quickly created our baseline “MVP” (Minimum Viable Product) of a solution and
    then iterated a lot based on its results. Initially, submissions were allowed
    only every five days, which gave us a natural sprint-like rhythm.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于时间紧迫，我们必须优化策略。我们使用了敏捷的方法：快速创建了我们的“最小可行产品”（MVP）解决方案，然后根据其结果进行了大量迭代。最初，每五天允许提交一次，这给了我们一种自然的冲刺节奏。
- en: 'The dataset was challenging in itself: 6.6 million photos with relatively large
    resolution – several terabytes of data. We reused a version of the dataset provided
    by one of the participants in which the photos’ resolution had been significantly
    scaled down (by a factor 4 on each side).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集本身具有挑战性：660万张相对较大分辨率的照片——几TB的数据。我们重新使用了其中一个参与者提供的数据集版本，在这个版本中，照片的分辨率显著缩小（每边缩小4倍）。
- en: '![class of image and percentage of occurrence ](../Images/317f4a3731b22e9118ed502993f0a611.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图像类别和出现百分比](../Images/317f4a3731b22e9118ed502993f0a611.png)'
- en: The dataset was very imbalanced – around 75% of the images were empty. The most
    popular animals are wildebeest, zebra and Thomson’s gazelle. On the other side
    of the spectrum you have super rare animals like steenbok or bats, visible only
    in a handful of photos.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集非常不平衡——大约75%的图像是空的。最常见的动物是角马、斑马和汤普森的瞪羚。另一方面，有一些超级稀有的动物，如steenbok或蝙蝠，只能在少数几张照片中看到。
- en: '![Figure](../Images/74b46d7bdee3dcddb6f66523f5aa0f2e.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/74b46d7bdee3dcddb6f66523f5aa0f2e.png)'
- en: As rare as in nature – rhinos were one of the least common species in the dataset
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然界中罕见——犀牛是数据集中最不常见的物种之一
- en: Top 5 things that worked
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 成功的5个关键因素
- en: 'We believe the following 5 key factors contributed the most to our models’
    success:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信以下5个关键因素对我们模型的成功贡献最大：
- en: '**1\. Large validation set**'
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**1\. 大型验证集**'
- en: This was crucial to ensure that the model generalises well, because we were
    assessed on the last season – a private, unseen dataset. Each season comes from
    a different year. We decided to hold out an entire season (season 8) for validation
    – we wanted it to contain many images (almost a million!), and yet be relatively
    recent – since the test set consisted of yet unpublished season 11\. This choice
    was also supported by a study of the distributions of individual species across
    seasons. We noticed that only the last three seasons (8, 9 and 10) contained photos
    of a few of the relatively rare species.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于确保模型的良好泛化至关重要，因为我们是在最后一个赛季——一个私有的、未公开的数据集上进行评估的。每个赛季来自不同的年份。我们决定将整个赛季（第8赛季）保留用于验证——我们希望它包含许多图像（几乎有一百万张！），并且相对较新——因为测试集包含的是尚未发布的第11赛季。这个选择还得到了对各赛季间个体物种分布的研究支持。我们注意到，只有最后三个赛季（第8、9和10赛季）包含了一些相对稀有物种的照片。
- en: '**2\. Training with growing resolution**'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**2\. 使用递增分辨率的训练**'
- en: We divided the training process into three stages. Each stage had a part in
    which we trained only the final layers of the network followed by training of
    all the layers. At each stage we trained the model on images of an increasingly
    higher resolution, letting us train longer without overfitting. The maximal resolution
    we used was 512×384, which was still a quarter of the provided images in each
    dimension.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练过程分为三个阶段。每个阶段包括先训练网络的最终层，然后训练所有层。在每个阶段，我们都在逐渐更高分辨率的图像上训练模型，从而使训练时间更长而不发生过拟合。我们使用的最大分辨率是512×384，这仍然是每个维度提供图像的四分之一。
- en: 'The final model was trained in the following stages:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最终模型在以下阶段进行训练：
- en: 5 epochs on network’s final layers on 128×96 px images
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在128×96 px图像上进行网络最终层的5个周期训练
- en: 5 epochs on all layers on 128×96 px images
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在128×96 px图像上进行5个周期的训练
- en: 5 epochs on network’s final layers on 256×192 px images
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在256×192 px图像上进行网络最终层的5个周期训练
- en: 5 epochs on all layers on 256×192 px images
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在256×192 px图像上进行所有层的5个周期训练
- en: 5 epochs on network’s final layers on 512×384 px images
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在512×384 px图像上进行网络最终层的5个周期训练
- en: 5 epochs on all layers on 512×384 px images
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在512×384 px图像上进行所有层的5个周期训练
- en: This approach significantly sped up our training – even a model operating on
    128×96 images achieved a good accuracy, and we could train it much faster than
    the 512×384 model. At this point we would like to give kudos to Pavel Pleskov,
    a participant of the competition who, in the spirit of healthy rivalry, shared
    a scaled down version of the dataset with the community saving us download time
    and allowing many more participants to join.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法显著加快了我们的训练速度——即便是处理128×96图像的模型也达到了良好的准确性，我们可以比512×384模型训练得更快。在此，我们要特别感谢Pavel
    Pleskov，他是比赛的参与者，以健康竞争的精神，与社区分享了一个缩小版的数据集，节省了我们的下载时间，并使更多的参与者能够加入。
- en: '**3\. Data augmentation / one cycle fitting**'
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**3\. 数据增强 / 单周期拟合**'
- en: We used standard image augmentations during training (horizontal flips, small
    rotations, small zooms, small warps and tweaks of lighting) to keep the model
    from overfitting on a pixel level and help it generalize. We used [Leslie Smith’s
    one cycle policy](https://arxiv.org/pdf/1803.09820.pdf) to speed up the training.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们使用了标准的图像增强方法（水平翻转、小幅旋转、小幅缩放、小幅变形和光照调整），以防止模型在像素级别过拟合，并帮助其泛化。我们使用了[Leslie
    Smith的单周期策略](https://arxiv.org/pdf/1803.09820.pdf)来加快训练速度。
- en: '**4\. Aggressive undersampling**'
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**4\. 激进的欠采样**'
- en: We eliminated large portions of the most frequent classes (e.g., 95% of empty
    photos) from our training to put more stress during training on examples from
    the less frequent classes. This allowed us to train significantly faster, helping
    the model to focus on the challenging part of the input space. A caveat here was
    to remove them in a smart way – keeping examples with more than one animal in
    the training set as they are harder for models to get right.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从训练集中剔除了大量最常见类别的样本（例如，95%的空照片），以便在训练过程中对来自较少见类别的样本施加更多压力。这使得我们能够显著加快训练速度，帮助模型专注于输入空间中的挑战部分。这里的一个警告是需要聪明地去除这些样本——保留训练集中包含多只动物的样本，因为这些样本对模型来说更具挑战性。
- en: '![Figure](../Images/b843ba700b8e735ff3b98deeceb8be35.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/b843ba700b8e735ff3b98deeceb8be35.png)'
- en: Multiple animals in one picture – a challenging case for an ML model
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一张图片中包含多只动物——这是对ML模型的一个挑战性案例
- en: '**5\. Inspection of losses / post-processing**'
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**5\. 损失检查 / 后处理**'
- en: What helped us to push the accuracy into top places was working deeply with
    the data and results given by our initial models. We wanted to see which images
    we got wrong and what happened in each range of our predicted certainty. To minimize
    our loss and optimize our submission we studied individual classes and inspected
    where we lose the most.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使我们能将准确性推向顶级位置的是深入分析数据和初始模型给出的结果。我们想查看哪些图像我们处理错误，以及在我们预测的置信度范围内发生了什么。为了最小化损失并优化我们的提交，我们研究了各个类别，并检查了我们最容易失分的地方。
- en: At this stage we chose to incorporate a simple inference mechanism from all
    the images in a sequence and not just single images. We took the predictions from
    the first image of each sequence (again guided by results on the validation set),
    unless the maximum prediction for a given species in a photo later in the sequence
    was higher and exceeded a certain threshold (this way we allowed, e.g., birds
    from “otherbird” flying by in frames later in the sequence to affect the prediction).
    Analogously for the “empty” category, we took the prediction from the first photo
    in a sequence unless in a later photo, the prediction for this class was below
    a certain threshold (this way, if an animal appeared only later, the prediction
    for “empty” was toned down suitably).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一阶段，我们选择了从所有图像中进行简单推断机制，而不仅仅是单个图像。我们从每个序列的第一张图像中获取预测（再次由验证集上的结果指导），除非序列中后面的照片中某个物种的最大预测值更高并超过某个阈值（这样我们允许，例如，“其他鸟”在序列后面的帧中飞过来影响预测）。对于“空”的类别也是类似，我们从序列中的第一张照片中获取预测，除非在后来的照片中，该类别的预测低于某个阈值（这样，如果动物只出现在后面，"空"的预测会适当地降低）。
- en: We next processed ranges of predicted values for each of the species with a
    high occurrence frequency and grid-searched for basic linear transformations minimizing
    our loss on the validation set. This helped the score a lot – most likely by getting
    the model’s returned probabilities to reflect species distribution in the dataset.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们处理了每种物种的预测值范围，并进行了基础线性变换的网格搜索，以最小化验证集上的损失。这大大提高了得分——很可能是通过使模型返回的概率更好地反映数据集中物种的分布。
- en: In retrospect – what we would like to have done
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回顾一下——我们希望做的事情
- en: 'There were a few more ideas we were eager to try, but because we joined the
    competition only 3 weeks before its conclusion, we did not. Most notably:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一些其他想尝试的想法，但由于我们是在比赛结束前仅三周才加入的，因此没有尝试。特别是：
- en: '**Focal loss**'
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**焦点损失**'
- en: The dataset at hand was highly imbalanced. An alternative to the aggressive
    downsampling we implemented (see above), would have been to use a modified loss
    function, namely the focal loss. While [first proposed](https://arxiv.org/pdf/1708.02002.pdf) in
    the context of object detection, it has been recently used also in the context
    of imbalanced classification problems.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的数据集高度不平衡。除了我们实施的激进下采样方法（见上文），另一种选择是使用修改后的损失函数，即焦点损失。虽然 [首次提出](https://arxiv.org/pdf/1708.02002.pdf) 是在目标检测的背景下，但最近也在不平衡分类问题中得到应用。
- en: '**More training time, more epochs**'
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**更多的训练时间，更多的周期**'
- en: Various signs indicated that our models might have been simply trained longer
    to achieve even better scores. Wary of overfitting, we have approached the training
    schedules conservatively, however given more time, we would have liked to explore
    longer training at the various stages (see above).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 各种迹象表明，我们的模型可能只是需要更长时间的训练才能取得更好的得分。虽然担心过拟合，我们采取了保守的训练安排，不过如果有更多时间，我们希望在各个阶段探索更长时间的训练（见上文）。
- en: '**Use a 2-stage approach: animal location detection and animal classification**'
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**使用两阶段方法：动物位置检测和动物分类**'
- en: We considered using a two stage approach, with the first model focusing on animal
    presence regardless of its species, and the second model focusing on species classification.
    Such an approach could have boosted accuracy and was suggested by previous research.
    Due to limited time we chose not to try this approach.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑使用两阶段的方法，第一阶段的模型专注于动物的存在，而不考虑其物种，第二阶段的模型专注于物种分类。这种方法可以提高准确性，并且在以前的研究中有所建议。由于时间有限，我们选择不尝试这种方法。
- en: After the competition we learned about great work in this area by Siyu Yang
    and Dan Morris from Microsoft AI for Earth. They built [a model for animal detection
    in pictures](https://medium.com/microsoftazure/accelerating-biodiversity-surveys-with-azure-machine-learning-9be53f41e674).
    It identifies the presence of animals in photos, providing bounding boxes for
    positive predictions. The strength of the model comes from it being trained on
    data from multiple geographical locations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 比赛结束后，我们了解到微软AI for Earth的Siyu Yang和Dan Morris在这一领域的出色工作。他们建立了[一种动物检测模型](https://medium.com/microsoftazure/accelerating-biodiversity-surveys-with-azure-machine-learning-9be53f41e674)。该模型识别照片中的动物，并为正向预测提供边界框。该模型的强大之处在于其训练数据来自多个地理位置。
- en: They suggest a very interesting approach of using a generic model for detecting
    animal location, and a second project- and location-specific model for species
    classification. One clear advantage is that the second model can focus precisely
    on the animal, which clearly has a potential to give better accuracy. This method
    can also help a lot when it comes to recognizing multiple animals in a picture.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 他们建议使用一种通用模型来检测动物位置，以及一个针对项目和地点的特定模型来进行物种分类。一大优势是，第二个模型可以准确地专注于动物，这显然有助于提高准确性。这种方法在识别图像中的多种动物时也能大大帮助。
- en: '**Data cleaning**'
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**数据清理**'
- en: 'Investigating our models’ performance, we noticed many examples of mislabels
    (e.g., only a leg of a zebra visible in a single image of a sequence – labelled
    as empty, was classified as a zebra by our model with high certainty). There were
    also many examples of images taken at night in which human labellers spotted no
    animals, while models were very sure about seeing them. After manually tweaking
    the color levels of such images, the darkness indeed revealed the detected animals.
    This fact touches on a very important topic: whether we should build models to
    replicate the labelers’ work (an approach promoted by the rules of the competition)
    or rather should we focus on models, which are as good as possible in detecting
    animals in the captured images.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在调查我们模型的表现时，我们注意到许多标记错误的例子（例如，在一系列图像中仅显示了一只斑马的腿——标记为空的图像被我们的模型以高确定性分类为斑马）。还有许多夜间拍摄的图像，人工标注者未发现任何动物，而模型却非常确定地看到了它们。在手动调整这些图像的色彩水平后，黑暗确实揭示了检测到的动物。这一事实涉及一个非常重要的话题：我们应该建立复制标注者工作的模型（这种方法由比赛规则推荐），还是应专注于在捕获的图像中尽可能准确地检测动物的模型。
- en: Why we joined and what it meant for us
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们为何加入以及这对我们的意义。
- en: The competition gave us a deep dive into the world of wildlife image classification.
    We are very satisfied with the result and had a lot of fun whilst working on these
    models. We were able to demonstrate and polish our computer vision skills, which
    will be critical to our [AI for Good](https://appsilon.com/ai-for-good/) initiative.
    We are currently in the process of building a pilot version of a wildlife image
    classifier for the National Parks Agency of Gabon and we will apply our new knowledge
    to this end. Stay tuned for more information about our upcoming projects. We are
    determined to show that AI can have a tangible, positive impact on life and the
    world we all share.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 比赛让我们深入了解了野生动物图像分类的世界。我们对结果非常满意，并且在处理这些模型时非常愉快。我们能够展示和完善我们的计算机视觉技能，这对我们的[AI
    for Good](https://appsilon.com/ai-for-good/)计划至关重要。我们目前正在为加蓬国家公园管理局构建一个野生动物图像分类器的试点版本，并将应用我们的新知识。敬请关注我们即将发布的项目。我们决心展示AI可以对生活和我们共同分享的世界产生切实的积极影响。
- en: '**Follow Appsilon Data Science on Social Media**'
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**关注Appsilon数据科学的社交媒体**'
- en: Follow[ @Appsilon](https://twitter.com/appsilon) on Twitter
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[Twitter](https://twitter.com/appsilon)上关注[@Appsilon](https://twitter.com/appsilon)。
- en: Follow Appsilon on[ LinkedIn](https://www.linkedin.com/company/appsilon)
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[LinkedIn](https://www.linkedin.com/company/appsilon)上关注Appsilon。
- en: Sign up for our company[ newsletter](https://appsilon.com/blog/)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注册我们的公司[通讯](https://appsilon.com/blog/)。
- en: Try out our R Shiny[ open source](https://appsilon.com/opensource/) packages
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 试用我们的R Shiny[开源](https://appsilon.com/opensource/)包。
- en: Sign up for the AI for Good[ newsletter](https://appsilon.com/ai-for-good/)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注册[AI for Good](https://appsilon.com/ai-for-good/)通讯。
- en: We are hiring [software engineers](https://appsilon.com/careers/)
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在招聘[软件工程师](https://appsilon.com/careers/)。
- en: '[Original](https://appsilon.com/using-ai-identify-wildlife-camera-trap-images-serengeti/).
    Reposted with permission.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://appsilon.com/using-ai-identify-wildlife-camera-trap-images-serengeti/)。经许可转载。'
- en: '**Related:**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Create Your Own Computer Vision Sandbox](/2020/02/computer-vision-sandbox.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[创建你自己的计算机视觉沙盒]( /2020/02/computer-vision-sandbox.html)'
- en: '[A bird’s-eye view of modern AI from NeurIPS 2019](/2020/01/modern-ai-from-neurips-2019.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从 NeurIPS 2019 看现代 AI 的全景]( /2020/01/modern-ai-from-neurips-2019.html)'
- en: '[AI and Machine Learning In Our Every Day Life](/2020/02/ai-machine-learning-everyday-life.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AI 和机器学习在我们日常生活中的应用]( /2020/02/ai-machine-learning-everyday-life.html)'
- en: More On This Topic
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[4 Factors to Identify Machine Learning Solvable Problems](https://www.kdnuggets.com/2022/04/4-factors-identify-machine-learning-solvable-problems.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[识别机器学习可解决问题的 4 个因素](https://www.kdnuggets.com/2022/04/4-factors-identify-machine-learning-solvable-problems.html)'
- en: '[How to Identify Missing Data in Time-Series Datasets](https://www.kdnuggets.com/how-to-identify-missing-data-in-timeseries-datasets)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何识别时间序列数据集中的缺失数据](https://www.kdnuggets.com/how-to-identify-missing-data-in-timeseries-datasets)'
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用管道编写清晰的 Python 代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，并通过寻找目标…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成为优秀数据科学家需要的 5 个关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个初学者数据科学家应掌握的 6 个预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
