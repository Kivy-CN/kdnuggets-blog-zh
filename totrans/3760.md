# 深度学习研究回顾：自然语言处理

> 原文：[https://www.kdnuggets.com/2017/01/deep-learning-review-natural-language-processing.html/2](https://www.kdnuggets.com/2017/01/deep-learning-review-natural-language-processing.html/2)

### 循环神经网络（RNNs）

好了，现在我们有了词向量，让我们看看它们如何融入到循环神经网络中。RNN是现在大多数自然语言处理任务的首选。RNN的最大优势在于它能够有效地利用来自先前时间步的数据。这是RNN的一个小片段。

![](../Images/7bff85999c56243549e81f29b186e61b.png)

所以，在底部我们有词向量（x[t], x[t-1], x[t+1]）。每个向量在同一时间步都有一个隐藏状态向量（h[t], h[t-1], h[t+1]）。我们称之为一个模块。

![](../Images/e15f2e2928b838c1376bdb1ab42de229.png)

RNN中每个模块的隐藏状态是*一个函数*，它结合了词向量和前一个时间步的隐藏状态向量。

![](../Images/6d46e022691a5417e0b090aab3daeeec.png)

如果你仔细看这些上标，你会看到有一个权重矩阵 W^(hx)，我们将用它来与输入相乘，还有一个递归权重矩阵 W^(hh)，它与*前一个*时间步的隐藏状态向量相乘。请记住，这些递归权重矩阵在所有时间步中是*相同的*。**这是RNN的关键点。** 仔细考虑一下，这与传统的2层神经网络非常不同。在后者中，我们通常为每一层有一个不同的W矩阵（W1和W2）。而在这里，递归权重矩阵在整个网络中是相同的。

要获得特定模块的输出（Yhat），这将是h乘以 W^S，这又是一个权重矩阵。

![](../Images/d7a2aed95a7d28dd490a30831cdf3c68.png)

让我们退一步，了解RNN的优势。与传统的神经网络不同，RNN接收的是一个*序列*的输入（在我们的例子中是单词）。你可以将其与典型的CNN对比，后者通常只有一张单独的图像作为输入。然而，RNN的输入可以是从短句到5段文章的任意长度。此外，这个序列中输入的*顺序*可以大大影响训练过程中权重矩阵和隐藏状态向量的变化。经过训练，隐藏状态应该能够捕捉到过去的信息（之前的时间步）。

### 门控循环单元（GRUs）

现在让我们看一下门控循环单元（GRU）。这个单元的目的是提供一种更复杂的方式来计算 RNN 中的隐藏状态向量。这种方法允许我们保留捕捉长距离依赖的信息。让我们想象一下为什么长期依赖在传统的 RNN 设置中会成为问题。在反向传播过程中，误差会通过 RNN 传播，从最新的时间步到最早的时间步。如果初始梯度是一个小数字（比如 < 0.25），那么在第 3 或第 4 个模块中，梯度将基本上消失（链式法则将梯度相乘），因此早期时间步的隐藏状态不会更新。

在传统的 RNN 中，隐藏状态向量是通过这个公式计算的。

![](../Images/ffa90bec3215efe6639d863d03b64317.png)

GRU 提供了一种不同的方式来计算隐藏状态向量 h(t)。计算被分解为三个组件：更新门、重置门和新记忆容器。这两个门都是输入词向量和前一个时间步的隐藏状态的函数。

![](../Images/3f332e74d25d460ede9d08a5fda23bb1.png)

主要区别在于每个门使用了不同的权重。这通过不同的上标来表示。更新门使用 W^z 和 U^z，而重置门使用 W^r 和 U^r。

现在，新记忆容器通过以下公式计算。

![](../Images/142df130628886519aa24dc197373a75.png)

（开放点表示[哈达玛积](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))）

现在，如果你仔细查看这个公式，你会发现如果重置门单元接近 0，那么整个项也会变为 0，从而忽略前一个时间步的 h[t-1] 信息。在这种情况下，单元仅是新词向量 x[t] 的函数。

h(t) 的最终公式写作：

![](../Images/06d61e3056f74c6a2aa70f772eb7881c.png)

h[t] 是所有三个组件的函数：重置门、更新门和记忆容器。最好的理解方式是通过可视化 z[t] 接近 1 和接近 0 时发生的情况。当 z[t] 接近 1 时，新的隐藏状态向量 h[t] 主要依赖于前一个隐藏状态，我们忽略当前的记忆容器，因为 (1 - z[t]) 接近 0。当 z[t] 接近 0 时，新的隐藏状态向量 h[t] 主要依赖于当前的记忆容器，我们忽略前一个隐藏状态。直观地看这三个组件可以通过以下方式总结。

+   更新门：

    +   如果 z[t] ~ 1，则 h[t] 完全忽略当前词向量，只复制前一个隐藏状态（如果这不清楚，查看 h[t] 方程，注意当 z[t] ~ 1 时 1 - z[t] 项的变化）。

    +   如果 z[t] ~ 0，则 h[t] 完全忽略前一个时间步的隐藏状态，依赖于新的记忆容器。

    +   这个门让模型控制之前隐藏状态中的信息应在多大程度上影响当前隐藏状态。

+   重置门：

    +   如果 r[t] ~ 1，则内存容器保留来自之前隐藏状态的信息。

    +   如果 r[t] ~ 0，则内存容器会忽略之前的隐藏状态。

    +   这个门让模型在信息未来不相关时丢弃该信息。

+   内存容器：依赖于重置门。

一个常见的例子来说明 GRUs 的有效性是以下内容。假设你有以下段落。

![](../Images/4990d39799f6ecdf4c881e6ddad2211b.png)

和相关问题“两个数字的和是多少？”相比，由于中间句子对当前问题没有任何影响，重置和更新门将允许网络在某种程度上“忘记”中间句子，并学习只有特定信息（在这种情况下是数字）应该修改隐藏状态。

### 长短期记忆单元（LSTMs）

如果你对 GRUs 感到舒适，那么 LSTMs 也不会是太大的跃进。LSTM 也是由一系列门组成的。

![](../Images/e8890214fc454dfea9501a0e6f873871.png)

绝对有很多信息需要吸收。由于这可以看作是 GRU 背后思想的扩展，我不会深入分析，但如果你想了解每个门和每个计算部分的详细情况，请查看 Chris Olah 精心撰写的 [博客文章](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)。这篇文章迄今为止是关于 LSTMs 的最受欢迎的教程，它将帮助你更好地理解这些单元为何以及如何工作。

### 比较和对比 LSTMs 和 GRUs

让我们先从相似之处开始。这两种单元都有一个特殊功能，即能够保持序列中词语之间的长期依赖关系。长期依赖关系指的是两词或短语可能在非常不同的时间步骤中出现，但它们之间的关系仍对解决最终目标至关重要。LSTMs 和 GRUs 能通过门控机制捕捉这些依赖关系，门控机制可以忽略或保留序列中的某些信息。

这两种单元的区别在于它们拥有的门的数量（GRU – 2，LSTM – 3）。这影响了输入经过的非线性处理的数量，最终影响整体计算。GRU 也没有 LSTM 所具有的记忆单元（c[t]）。

### 在深入阅读论文之前

想快速补充一点。有一些其他深度模型在自然语言处理（NLP）中也很有用。递归神经网络和用于 NLP 的卷积神经网络有时在实际中使用，但不如 RNNs 普及，后者实际上是大多数深度学习 NLP 系统的核心。

好的。既然我们对深度学习与自然语言处理的关系有了充分的理解，让我们来看一些论文。由于自然语言处理领域内存在着众多不同的问题领域（从机器翻译到问答系统），我们可以参考许多论文，但这里有三篇我认为特别有洞察力的。2016 年在自然语言处理方面有一些重大的进展，但我们首先从2015年的一篇论文开始。

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业的捷径。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你组织的 IT

* * *

### 更多相关主题

+   [建立一个稳固的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)

+   [使用管道编写干净的 Python 代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)

+   [是什么让 Python 成为初创企业理想的编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)

+   [N-gram 语言建模在自然语言处理中的应用](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)

+   [停止学习数据科学以寻找目标，并寻找目标来…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)

+   [数据科学统计学习的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)
