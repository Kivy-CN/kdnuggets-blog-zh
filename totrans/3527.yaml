- en: 'In Machine Learning, What is Better: More Data or better Algorithms'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2015/06/machine-learning-more-data-better-algorithms.html](https://www.kdnuggets.com/2015/06/machine-learning-more-data-better-algorithms.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By Xavier Amatriain** (VP of Engineering at Quora).'
  prefs: []
  type: TYPE_NORMAL
- en: “In machine learning, is more data always better than better algorithms?” No. There
    are times when more data helps, there are times when it doesn’t.
  prefs: []
  type: TYPE_NORMAL
- en: Probably one of the most famous quotes defending the power of data is that of
    Google’s Research Director Peter Norvig claiming that “We don’t have better algorithms.
    We just have more data.”. This quote is usually linked to the article on “The
    Unreasonable Effectiveness of Data”, co-authored by Norvig  himself (you should
    probably be able to find the pdf on the web although [the original](https://googleresearch.blogspot.com/2009/03/unreasonable-effectiveness-of-data.html)
    is behind the IEEE paywall). The last nail on the coffin of better models is when
    Norvig is misquoted as saying that “All models are wrong, and you don’t need them
    anyway” (read [here](http://norvig.com/fact-check.html) for the author’s own clarifications
    on how he was misquoted).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/19fca3acedbb3ad590829f7049690d06.png)'
  prefs: []
  type: TYPE_IMG
- en: The effect that Norvig et. al were referring to in their article, had already
    been captured years before in the famous paper by Microsoft Researchers Banko
    and Brill [2001]” [Scaling to Very Very Large Corpora for Natural Language Disambiguation](http://acl.ldc.upenn.edu/P/P01/P01-1005.pdf)“.
    In that paper, the authors included the plot below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5daaabc8d7789addd6dea6cbfa04418.png)'
  prefs: []
  type: TYPE_IMG
- en: That figure shows that, for the given problem, very different algorithms perform
    virtually the same. however, adding more examples (words) to the training set
    monotonically increases the accuracy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: So, case closed, you might think. Well… not so fast. The reality is that both
    Norvig’s assertions and Banko and Brill’s paper are right… in a context. But,
    they are now and again misquoted in contexts that are completely different than
    the original ones. But, in order to understand why, we need to get slightly technical. 
    (I don’t plan on giving a full machine learning tutorial in this post. If you
    don’t understand what I explain below, read my answer to [How do I learn machine
    learning?](https://www.quora.com/How-do-I-learn-machine-learning-1/answer/Xavier-Amatriain))
  prefs: []
  type: TYPE_NORMAL
- en: '**Variance or Bias?**'
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea is that there are two possible (and almost opposite) reasons
    a model might not perform well.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first case, we might have a model that is too complicated for the amount
    of data we have. This situation, known as*high variance*, leads to model overfitting.
    We know that we are facing a high variance issue when the training error is much
    lower than the test error. High variance problems can be addressed by reducing
    the number of features, and… yes, by increasing the number of data points. So,
    what kind of models were Banko & Brill’s, and Norvig dealing with? Yes, you got
    it right: high variance. In both cases, the authors were  working on language
    models in which roughly every word in the vocabulary makes a feature. These are
    models with many features as compared to the training examples. Therefore, they
    are likely to overfit. And, yes, in this case adding more examples will help.'
  prefs: []
  type: TYPE_NORMAL
- en: But, in the opposite case, we might have a model that is too simple to explain
    the data we have. In that case, known as *high bias*, adding more data will not
    help.  See below a plot of a real production system at Netflix and its performance
    as we add more training examples.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7fc45f89a72a843c9fdae7b364438f4d.png)'
  prefs: []
  type: TYPE_IMG
- en: So, no, **more data does not always help**. As we have just seen there can be
    many cases in which adding more examples to our training set will not improve
    the model performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**More features to the rescue**'
  prefs: []
  type: TYPE_NORMAL
- en: If you are with me so far, and you have done your homework in understanding
    high variance and high bias problems, you might be thinking that I have deliberately
    left something out of the discussion. Yes, high bias models will not benefit from
    more training examples, but they might very well benefit from more features. So,
    in the end, it is all about adding “more” data, right? Well, again, it depends.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take the Netflix Prize, for example. Pretty early on in the game, there
    was [a blog post](http://anand.typepad.com/datawocky/2008/03/more-data-usual.html)
    by serial entrepreneur and Stanford professor [Anand Rajaraman](https://en.wikipedia.org/wiki/Anand_Rajaraman)
    commenting on the use of extra features to solve the problem. The post explains
    how a team of students got an improvement on the prediction accuracy by adding
    content features from IMDb.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e5868e9ed42194643ec16d1e553f347c.png)'
  prefs: []
  type: TYPE_IMG
- en: In retrospect, it is easy to criticize the post for making a gross over-generalization
    from a single data point. Even more, the [follow-up post](http://anand.typepad.com/datawocky/2008/04/data-versus-alg.html)
    references SVD as one of the “complex” algorithms not worth trying because it
    limits the ability of scaling up to larger number of features. Clearly, Anand’s
    students did not win the Netflix Prize, and they probably now realize that SVD
    did have a major role in the winning entry.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a matter of fact, many teams showed later that adding content features from
    IMDB or the like to an optimized algorithm had little to no improvement. Some
    of the members of the [Gravity team](http://www.gravityrd.com/references/netflix-prize?lang=en),
    one of the top contenders for the Prize, published a detailed paper in which they
    showed how those content-based features would add no improvement to the highly
    optimized collaborative filtering matrix factorization approach. The paper was
    entitled [Recommending New Movies: Even a Few Ratings Are More Valuable Than Metadata](http://dl.acm.org/citation.cfm?id=1639731&dl=ACM&coll=DL&CFID=122239967&CFTOKEN=16331362).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55db293f30aba4d9ca8f500e583052e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To be fair, the title of the paper is also an over-generalization. Content-based
    features (or different features in general) might be able to improve accuracy
    in many cases. But, you get my point again: **More data does not always help**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Better Data != More Data (Added this section** ***in response to a comment)***'
  prefs: []
  type: TYPE_NORMAL
- en: It is important to point out that, in my opinion, better data is always better.
    There is no arguing against that. So any effort you can direct towards “improving”
    your data is always well invested. The issue is that better data does not mean
    **more** data. As a matter of fact, sometimes it might mean **less**!
  prefs: []
  type: TYPE_NORMAL
- en: Think of data cleansing or outlier removal as one trivial illustration of my
    point. But, there are many other examples that are more subtle. For example, I
    have seen people invest a lot of effort in implementing distributed [Matrix Factorization](https://www.quora.com/Matrix-Factorization)
    when the truth is that they could have probably gotten by with sampling their
    data and gotten to very similar results. In fact, doing some form of smart sampling
    on your population the right way (e.g. using stratified sampling) can get you
    to better results than if you used the whole unfiltered data set.
  prefs: []
  type: TYPE_NORMAL
- en: '**The End of the Scientific Method?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, whenever there is a heated debate about a possible paradigm change,
    there are people like Malcolm Gladwell or Chris Anderson that make a living out
    of heating it even more (don’t get me wrong, I am a fan of both, and have read
    most of their books). In this case, Anderson picked on some of Norvig’s comments,
    and misquoted them in an article entitled: [The End of Theory: The Data Deluge
    Makes the Scientific Method Obsolete](http://www.wired.com/science/discoveries/magazine/16-07/pb_theory/).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b899e2f67ba919e641f8e16f215f3bff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The article explains several examples of how the abundance of data helps people
    and companies take decision without even having to understand the meaning of the
    data itself. As Norvig himself points out in [his rebuttal](http://norvig.com/fact-check.html),
    Anderson has a few points right, but goes above and beyond to try to make them.
    And the result is a set of false statements, starting from the title: the data
    deluge does not make the scientific method obsolete. I would argue it is rather
    the other way around.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Without a Sound Approach = Noise**'
  prefs: []
  type: TYPE_NORMAL
- en: So, am I trying to make the point that the Big Data revolution is only hype?
    No way. Having more data, both in terms of more examples or more features, is
    a blessing. The availability of data enables more and better insights and applications.
    More data indeed enables better approaches. More than that, it **requires** better
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a3465a3c529b650114d425d05eb9c1ac.png)'
  prefs: []
  type: TYPE_IMG
- en: In summary, we should dismiss simplistic voices that proclaim the uselessness
    of theory or models, or the triumph of data over these. As much as data is needed,
    so are good models and theory that explains them. But, overall, what we need is
    good approaches that help us understand how to interpret data, models, and the
    limitations of both in order to produce the best possible output.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, data is important. But, data without a sound approach becomes
    noise.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Original](https://www.quora.com/In-machine-learning-is-more-data-always-better-than-better-algorithms/answer/Xavier-Amatriain)**.
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: ** [Xavier Amatriain](http://xavier.amatriain.net/), is a VP of Engineering
    at Quora, well known for his work on Recommender Systems and Machine Learning.
    He build teams and algorithms to solve hard problems with business impact. Previously,
    he was Research/Engineering Director at Netflix.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Debunking Big Data Myths. Again](/2015/01/debunking-big-data-myths-again.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Interview: Josh Hemann, Activision on Why the Tolerance for Ambiguity is Vital](/2015/03/interview-josh-hemann-activision-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Will Deep Learning take over Machine Learning, make other algorithms obsolete?](/2014/10/deep-learning-make-machine-learning-algorithms-obsolete.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[15 More Free Machine Learning and Deep Learning Books](https://www.kdnuggets.com/2022/11/15-free-machine-learning-deep-learning-books.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, June 22: Primary Supervised Learning Algorithms…](https://www.kdnuggets.com/2022/n25.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Primary Supervised Learning Algorithms Used in Machine Learning](https://www.kdnuggets.com/2022/06/primary-supervised-learning-algorithms-used-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Why are More Developers Using Python for Their Machine Learning Projects?](https://www.kdnuggets.com/2022/01/developers-python-machine-learning-projects.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A (Much) Better Approach to Evaluate Your Machine Learning Model](https://www.kdnuggets.com/2022/01/much-better-approach-evaluate-machine-learning-model.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Essential Machine Learning Algorithms: A Beginner''s Guide](https://www.kdnuggets.com/2021/05/essential-machine-learning-algorithms-beginners.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
