- en: Dimensionality Reduction with Principal Component Analysis (PCA)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用主成分分析（PCA）进行降维
- en: 原文：[https://www.kdnuggets.com/2020/05/dimensionality-reduction-principal-component-analysis.html](https://www.kdnuggets.com/2020/05/dimensionality-reduction-principal-component-analysis.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/05/dimensionality-reduction-principal-component-analysis.html](https://www.kdnuggets.com/2020/05/dimensionality-reduction-principal-component-analysis.html)
- en: '[comments](#comments)![Figure](../Images/794e2a70f5ae4d67b0e90c08f32dd767.png)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)![图](../Images/794e2a70f5ae4d67b0e90c08f32dd767.png)'
- en: '[Credits](https://medium.com/datadriveninvestor/principal-components-analysis-pca-71cc9d43d9fb)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[致谢](https://medium.com/datadriveninvestor/principal-components-analysis-pca-71cc9d43d9fb)'
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍
- en: With the availability of high-performance CPUs and GPUs, it is pretty much possible
    to solve every regression, classification, clustering, and other related problems
    using machine learning and deep learning models. However, there are still various
    portions that cause performance bottlenecks while developing such models. A large
    number of features in the dataset are one of the major factors that affect both
    the training time as well as the accuracy of machine learning models.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 随着高性能 CPU 和 GPU 的普及，几乎可以使用机器学习和深度学习模型解决每一个回归、分类、聚类及其他相关问题。然而，在开发这些模型时，仍然存在各种性能瓶颈。数据集中大量的特征是影响机器学习模型训练时间和准确性的主要因素之一。
- en: The Curse of Dimensionality
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 维度诅咒
- en: In machine learning, “dimensionality” simply refers to the number of features
    (i.e. input variables) in your dataset.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，“维度”简单指数据集中特征（即输入变量）的数量。
- en: While the performance of any machine learning model increases if we add additional
    features/dimensions, at some point a further insertion leads to performance degradation
    that is when the number of features is very large commensurate with the number
    of observations in your dataset, several linear algorithms strive hard to train
    efficient models. This is called the “**Curse of Dimensionality”.**
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然添加额外特征/维度会提高任何机器学习模型的性能，但到某个点时，进一步的添加会导致性能下降，这就是当特征数量与数据集中的观察数非常大时，许多线性算法努力训练有效模型的情况。这被称为“**维度诅咒**”。
- en: '**Dimensionality reduction** is a set of techniques that studies how to shrivel
    the size of data while preserving the most important information and further eliminating
    the curse of dimensionality. It plays an important role in the performance of
    classification and clustering problems.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**降维**是一组技术，研究如何在保留最重要信息的同时缩小数据的大小，并进一步消除维度诅咒。它在分类和聚类问题的性能中发挥了重要作用。'
- en: '![Figure](../Images/7d40c39e40111d5c0194acdbebff0f1a.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/7d40c39e40111d5c0194acdbebff0f1a.png)'
- en: '[The figure illustrates a 3-D feature space is split into two 1-D feature spaces,
    and later, if found to be correlated, the number of features can be reduced even
    further.](https://www.geeksforgeeks.org/dimensionality-reduction/)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[该图示例展示了一个3-D特征空间被分割成两个1-D特征空间，之后如果发现相关性，特征数量可以进一步减少。](https://www.geeksforgeeks.org/dimensionality-reduction/)'
- en: 'The various techniques used for dimensionality reduction include:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 用于降维的各种技术包括：
- en: Principal Component Analysis (PCA)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）
- en: Linear Discriminant Analysis (LDA)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性判别分析（LDA）
- en: Generalized Discriminant Analysis (GDA)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广义判别分析（GDA）
- en: '[Multi-Dimension Scaling (MDS)](https://blog.paperspace.com/dimension-reduction-with-multi-dimension-scaling/)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多维缩放 (MDS)](https://blog.paperspace.com/dimension-reduction-with-multi-dimension-scaling/)'
- en: LLE
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLE
- en: IsoMap
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IsoMap
- en: Autoencoders
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器
- en: This article is focused on the design principals of PCA and its implementation
    in python.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本文集中于 PCA 的设计原则及其在 Python 中的实现。
- en: Principal Component Analysis(PCA)
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）
- en: Principal Component Analysis(PCA) is one of the most popular linear dimension
    reduction algorithms. It is a projection based method that transforms the data
    by projecting it onto a set of orthogonal(perpendicular) axes.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）是最受欢迎的线性降维算法之一。它是一种基于投影的方法，通过将数据投影到一组正交（垂直）轴上来转换数据。
- en: “PCA works on a condition that while the data in a higher-dimensional space
    is mapped to data in a lower dimension space, the variance or spread of the data
    in the lower dimensional space should be maximum.”
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “PCA 的工作条件是：当数据从高维空间映射到低维空间时，低维空间中数据的方差或扩散应尽可能大。”
- en: In the below figure the data has maximum variance along the red line in two-dimensional
    space.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，数据在二维空间中沿红色线的方差最大。
- en: '![](../Images/ee73debf16c2275f7e1c520259589717.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee73debf16c2275f7e1c520259589717.png)'
- en: Intuition
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 直观理解
- en: Let’s develop an intuitive understanding of PCA. Suppose, you wish to distinguish
    between different food items based on their nutritional content. Which variable
    will be a good choice to differentiate food items? If you choose a variable that
    varies a lot from one food item to another, you will be able to detach them properly.
    Your job will be much harder if the chosen variable is almost in the same quantity
    in food items. What if data doesn’t have a variable that segregates food items
    properly? We can create an artificial variable through a linear combination of
    original variables like
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对PCA有一个直观的理解。假设你希望根据食品的营养成分区分不同的食物项目。哪个变量将是区分食物项目的好选择？如果你选择一个在不同食物项目之间变化很大的变量，你将能够很好地区分它们。如果所选变量在食物项目中几乎相同，你的工作将会更加困难。如果数据中没有可以很好区分食物项目的变量怎么办？我们可以通过原始变量的线性组合创建一个人工变量，如
- en: '`New_Var = 4*Var1 - 4*Var2 + 5*Var3`.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`New_Var = 4*Var1 - 4*Var2 + 5*Var3`。'
- en: This is what essentially PCA does, it finds the best linear combinations of
    the original variables so that the variance or spread along the new variable is
    maximum.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是PCA的本质，它找到原始变量的最佳线性组合，使得新变量的方差或扩展最大。
- en: Suppose we have to transform a 2-dimensional representation of data points to
    a one-dimensional representation. So we will try to find a straight line and project
    data points on them. (A straight line is one dimensional). There are many possibilities
    to select a straight line.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们要将二维数据点的表示转换为一维表示。那么我们将尝试找一条直线，并将数据点投影到这条直线上（直线是一维的）。选择直线有许多可能性。
- en: '![Figure](../Images/529937a1420cac8bde037badbfafa5ba.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/529937a1420cac8bde037badbfafa5ba.png)'
- en: '[Dataset](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)![Figure](../Images/f5d5bc755632bf9f9d2ce18dd2988a39.png)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[数据集](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)![图示](../Images/f5d5bc755632bf9f9d2ce18dd2988a39.png)'
- en: '[PCA in action](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[PCA的实际应用](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)'
- en: Say the magenta line will be our new dimension.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 假设品红色的线将是我们的新维度。
- en: If you see the red lines (connecting the projection of blue points on a magenta
    line) i.e. the perpendicular distance of each data point from the straight line
    is the projection error. The sum of the error of all data points will be the total
    projection error.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看到红线（连接蓝点在品红线上的投影），即每个数据点到直线的垂直距离就是投影误差。所有数据点的误差之和就是总投影误差。
- en: Our new data points will be the projections (red points) of those original blue
    data points. As we can see we have transformed 2-dimensional data points to one-dimensional
    data points by projection them on 1-dimensional space i.e. a straight line. That
    magenta straight line is called the** principal axis***. *Since we are projecting
    to a single dimension, we have only one principal axis. We apply the same procedure
    to find the next principal axis from the residual variance. Apart from being the
    direction of maximum variance, the next principal axis must be orthogonal(perpendicular
    or Uncorrelated to each other,) to the other principal axes.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新数据点将是那些原始蓝色数据点的投影（红色点）。正如我们所见，我们通过将数据点投影到一维空间（即一条直线）上，将二维数据点转换为一维数据点。那条品红色的直线被称为**主轴**。*由于我们投影到单一维度，我们只有一个主轴。我们使用相同的过程从残差方差中找出下一个主轴。除了具有最大方差的方向，下一个主轴必须与其他主轴正交（垂直或彼此不相关）。*
- en: Once, we get all the principal axes, the dataset is projected onto these axes.
    The columns in the projected or transformed dataset are called **principal components**.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦得到所有主轴，数据集将投影到这些轴上。投影或转换后的数据集中的列被称为**主成分**。
- en: The principal components are essentially the linear combinations of the original
    variables, the weights vector in this combination is actually the eigenvector
    found which in turn satisfies the principle of least squares.
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 主成分本质上是原始变量的线性组合，这些组合中的权重向量实际上是找到的特征向量，而这些特征向量又满足最小二乘原则。
- en: Luckily, thanks to linear algebra we don’t have to sweat much for PCA. Eigenvalue
    Decomposition and Singular Value Decomposition(SVD) from linear algebra are the
    two main procedures used in PCA to reduce dimensionality.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，得益于线性代数，我们不必为主成分分析（PCA）付出太多汗水。线性代数中的特征值分解和奇异值分解（SVD）是PCA中用于降维的两个主要过程。
- en: Eigenvalue Decomposition
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征值分解
- en: '**Matrix Decomposition **is a process in which a matrix is reduced to its constituent
    parts to simplify a range of more complex operations. **Eigenvalue Decomposition** is
    the most used matrix decomposition method which involves decomposing a square
    matrix(**n*n**) into a set of eigenvectors and eigenvalues.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**矩阵分解** 是将矩阵简化为其组成部分的过程，以简化一系列更复杂的操作。**特征值分解** 是最常用的矩阵分解方法，它涉及将一个方阵（**n*n**）分解为一组特征向量和特征值。'
- en: '**Eigenvectors** are unit vectors, which means that their length or magnitude
    is equal to 1.0\. They are often referred to as right vectors, which simply means
    a column vector (as opposed to a row vector or a left vector).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征向量** 是单位向量，这意味着它们的长度或幅度等于 1.0。它们通常被称为右向量，这仅仅意味着列向量（相对于行向量或左向量）。'
- en: '**Eigenvalues** are coefficients applied to eigenvectors that give the vectors
    their length or magnitude. For example, a negative eigenvalue may reverse the
    direction of the eigenvector as part of scaling it.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征值** 是施加在特征向量上的系数，赋予向量其长度或幅度。例如，负特征值可能会在缩放特征向量时反转其方向。'
- en: 'Mathematically, A vector is an eigenvector of a matrix any n*n square matrix `**A**` if
    it satisfies the following equation:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，一个向量是矩阵的特征向量，如果它满足以下方程：
- en: '`**A . v = **???? **. v**`'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`**A . v = **???? **. v**`'
- en: This is called the eigenvalue equation, where `**A**` is an n*n parent square
    matrix that we are decomposing, `**v**` is the eigenvector of the matrix, and `???? `represents
    the eigenvalue scalar.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为特征值方程，其中 `**A**` 是我们要分解的 n*n 方阵，`**v**` 是矩阵的特征向量，`????` 表示特征值标量。
- en: In simpler words, the linear transformation of a vector `**v**` by `**A**`has
    the same effect of scaling the vector by factor `????`. Note that for `**m*n**` non-square
    matrix **A** with m ≠ n, `**A.v**` an m-D vector but `????.v`is an n-D vector,
    i.e., no eigenvalues and eigenvectors are defined. If you wanna diver deeper into
    mathematics [check this out](http://fourier.eng.hmc.edu/e176/lectures/algebra/node9.html).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，矩阵 `**A**` 对向量 `**v**` 的线性变换具有相同的效果，即按因子 `????` 缩放向量。请注意，对于 `**m*n**` 非方阵
    **A**（m ≠ n），`**A.v**` 是一个 m 维向量，但 `????.v` 是一个 n 维向量，即没有定义特征值和特征向量。如果你想深入了解数学，[请查看这个](http://fourier.eng.hmc.edu/e176/lectures/algebra/node9.html)。
- en: '![Figure](../Images/92eb12576988649be276033fd927146e.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/92eb12576988649be276033fd927146e.png)'
- en: '[Eigenvalue decomposition](https://guzintamath.com/textsavvy/2019/02/02/eigenvalue-decomposition/)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[特征值分解](https://guzintamath.com/textsavvy/2019/02/02/eigenvalue-decomposition/)'
- en: Multiplying these constituent matrices together, or combining the transformations
    represented by the matrices will result in the original matrix.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些组成矩阵相乘，或者组合这些矩阵所代表的变换将会得到原始矩阵。
- en: A decomposition operation does not result in a compression of the matrix; instead,
    it breaks it down into constituent parts to make certain operations on the matrix
    easier to perform. Like other matrix decomposition methods, Eigendecomposition
    is used as an element to simplify the calculation of other more complex matrix
    operations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 分解操作不会导致矩阵的压缩；相反，它将矩阵分解为组成部分，使得在矩阵上执行某些操作变得更容易。像其他矩阵分解方法一样，特征分解作为元素用于简化其他更复杂矩阵操作的计算。
- en: Singular Value Decomposition (SVD)
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 奇异值分解（SVD）
- en: Singular value decomposition is a method of decomposing a matrix into three
    other matrices.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 奇异值分解是一种将矩阵分解为三个其他矩阵的方法。
- en: '![Figure](../Images/d36700e54ef763f98ee3b1cbad9a8c36.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/d36700e54ef763f98ee3b1cbad9a8c36.png)'
- en: '[SVD](https://blog.paperspace.com/dimension-reduction-with-principal-component-analysis/)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[SVD](https://blog.paperspace.com/dimension-reduction-with-principal-component-analysis/)'
- en: 'Mathematically Singular value decomposition is given by:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，奇异值分解表示为：
- en: '![](../Images/7fae8befbd0ec05bf24083e0017af212.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7fae8befbd0ec05bf24083e0017af212.png)'
- en: 'Where:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*A* is an *m* × *n* matrix'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*A* 是一个 *m* × *n* 矩阵'
- en: '*U* is an *m* × *n* *orthogonal* matrix'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*U* 是一个 *m* × *n* *正交* 矩阵'
- en: '*S* is an *n* × *n* *diagonal matrix*'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*S* 是一个 *n* × *n* *对角矩阵*'
- en: '*V* is an *n* × *n* orthogonal matrix'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*V* 是一个 *n* × *n* 正交矩阵'
- en: As shown, SVD produces three matrices U, S & V. U and V orthogonal matrices
    whose columns represent eigenvectors of AAT and ATA respectively. The matrix S
    is a diagonal matrix and diagonal values are called singular values. Each singular
    value is the square-root of the corresponding eigenvalue.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，SVD产生了三个矩阵U、S和V。U和V是正交矩阵，其列分别表示AAT和ATA的特征向量。矩阵S是对角矩阵，对角线上的值称为奇异值。每个奇异值是对应特征值的平方根。
- en: '**How does dimension reduction fit into these mathematical equations?**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**降维如何适应这些数学方程？**'
- en: Well, once you have calculated eigenvalues and eigenvectors choose the important
    eigenvectors to form a set of principal axes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你计算了特征值和特征向量，就选择重要的特征向量来形成一组主轴。
- en: Selection of EigenVectors
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征向量的选择
- en: The importance of an eigenvector is measured by the percentage of total variance
    explained by the corresponding eigenvalue. Suppose `V1` & `V2` are two eigenvectors
    with `40%` & `10%` of total variance along with their directions respectively.
    If asked to pick one from these two eigenvectors, our choice would be `V1` because
    it gives us more information about data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量的重要性由其对应特征值解释的总方差百分比来衡量。假设`V1`和`V2`是两个特征向量，分别对应`40%`和`10%`的总方差。如果要求从这两个特征向量中选择一个，我们会选择`V1`，因为它能提供更多的数据相关信息。
- en: All eigenvectors are arranged according to their eigenvalues in descending order.
    Now, we have to decide how many eigenvectors to retain and for that we need to
    discuss two methods **Total variance explained** and **Scree Plot** for that.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所有特征向量根据其特征值以降序排列。现在，我们必须决定保留多少个特征向量，为此我们需要讨论两种方法**总方差解释**和**碎石图**。
- en: Total Variance Explained
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总解释方差
- en: '**Total Explained variance** is used to measure the discrepancy between a model
    and actual data. It is the part of the model’s total variance that is explained
    by factors that are present.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**总解释方差**用于衡量模型与实际数据之间的差异。它是模型总方差的一部分，由存在的因子解释。'
- en: Suppose, we have a vector of `n` eigenvalues(e0,..., en) sorted in descending
    order. Take the cumulative sum of eigenvalues at every index until the sum is
    greater than `95%` of the total variance. Reject all eigenvalues and eigenvectors
    after that index.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个按降序排列的`n`个特征值（e0,..., en）的向量。在每个索引处取特征值的累计和，直到和大于`95%`的总方差。之后拒绝所有该索引之后的特征值和特征向量。
- en: Scree Plot
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 碎石图
- en: From the scree plot, we can read off the percentage of the variance in the data
    explained as we add principal components.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从碎石图中，我们可以读取添加主成分时数据中方差的解释百分比。
- en: It shows the eigenvalues on the y-axis and the number of factors on the x-axis.
    It always displays a downward curve. The point where the slope of the curve is
    leveling off (the “elbow) indicates the number of factors.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 它在y轴上显示特征值，在x轴上显示因子数量。它总是显示一个向下的曲线。曲线斜率趋于平缓的点（“肘部”）表示因子的数量。
- en: '![Figure](../Images/dead94de468da0f20315d549a33a011b.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/dead94de468da0f20315d549a33a011b.png)'
- en: '[Scree plot](https://www.stata.com/features/overview/principal-components/)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[碎石图](https://www.stata.com/features/overview/principal-components/)'
- en: For example in the scree plot shown above the sharp bend(elbow) is at 4\. So,
    the number of principal axes should be 4.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在上图中的碎石图中，急剧弯曲（肘部）出现在4处。因此，主轴的数量应该是4。
- en: Principal Component Analysis(PCA) in python from scratch
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python中从零开始的主成分分析（PCA）
- en: The example below defines a small 3×2 matrix, centers the data in the matrix,
    calculates the covariance matrix of the centered data, and then the eigenvalue
    decomposition of the covariance matrix. The eigenvectors and eigenvalues are taken
    as the principal components and singular values which are finally used to project
    the original data on the new axes.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的例子定义了一个小的3×2矩阵，对矩阵中的数据进行中心化，计算中心化数据的协方差矩阵，然后对协方差矩阵进行特征值分解。特征向量和特征值被作为主成分和奇异值，最终用来将原始数据投影到新的轴上。
- en: '[PRE0]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Conclusion
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: With all the effectiveness PCA provides, but if the number of variables is large,
    it becomes hard to interpret the principal components. PCA is most suitable when
    variables have a linear relationship among them. Also, PCA is susceptible to big
    outliers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管PCA提供了很高的有效性，但如果变量的数量很大，就很难解释主成分。PCA最适用于变量之间具有线性关系的情况。此外，PCA对大离群值比较敏感。
- en: PCA is an old method and has been well researched. There are many extensions
    of basic PCA which address its shortcomings like robust PCA, kernel PCA, incremental
    PCA. Through this article, we developed a basic and intuitive understanding of
    PCA. We discussed a few important concepts related to its implementation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）是一种古老的方法，并且已经得到了充分的研究。基础PCA有许多扩展版本，例如鲁棒PCA、核PCA、增量PCA等，这些扩展版本解决了其不足之处。通过这篇文章，我们对PCA进行了基本且直观的理解。我们讨论了与PCA实施相关的一些重要概念。
- en: Well, that’s all for this article hope you guys have enjoyed reading it and
    I’ll be glad if it is of any help. Feel free to share your comments/thoughts/feedback
    in the comment section.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，这篇文章就到此为止，希望大家喜欢阅读，如果有所帮助我会很高兴。欢迎在评论区分享你的意见/想法/反馈。
- en: Thanks for reading!!!????
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 感谢阅读！！！????
- en: '**Bio: [Nagesh Singh Chauhan](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)**
    is a Data Science enthusiast. Interested in Big Data, Python, Machine Learning.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[Nagesh Singh Chauhan](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)**
    是一位数据科学爱好者。对大数据、Python、机器学习感兴趣。'
- en: '[Original](https://levelup.gitconnected.com/dimensionality-reduction-principal-component-analysis-pca-d59bc1fed3dd).
    Reposted with permission.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://levelup.gitconnected.com/dimensionality-reduction-principal-component-analysis-pca-d59bc1fed3dd)。转载自许可。'
- en: '**Related:**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Sparse Matrix Representation in Python](/2020/05/sparse-matrix-representation-python.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python中的稀疏矩阵表示](/2020/05/sparse-matrix-representation-python.html)'
- en: '[Hyperparameter Optimization for Machine Learning Models](/2020/05/hyperparameter-optimization-machine-learning-models.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习模型的超参数优化](/2020/05/hyperparameter-optimization-machine-learning-models.html)'
- en: '[7 Steps to Mastering Intermediate Machine Learning with Python — 2019 Edition](/2019/06/7-steps-mastering-intermediate-machine-learning-python.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握中级机器学习的7个步骤：2019版](/2019/06/7-steps-mastering-intermediate-machine-learning-python.html)'
- en: '* * *'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT工作'
- en: '* * *'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Principal Component Analysis (PCA) with Scikit-Learn](https://www.kdnuggets.com/2023/05/principal-component-analysis-pca-scikitlearn.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Scikit-Learn中的主成分分析（PCA）](https://www.kdnuggets.com/2023/05/principal-component-analysis-pca-scikitlearn.html)'
- en: '[Dimensionality Reduction Techniques in Data Science](https://www.kdnuggets.com/2022/09/dimensionality-reduction-techniques-data-science.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中的降维技术](https://www.kdnuggets.com/2022/09/dimensionality-reduction-techniques-data-science.html)'
- en: '[Essential Math for Data Science: Eigenvectors and Application to PCA](https://www.kdnuggets.com/2022/06/essential-math-data-science-eigenvectors-application-pca.html)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中的基础数学：特征值及其在主成分分析中的应用](https://www.kdnuggets.com/2022/06/essential-math-data-science-eigenvectors-application-pca.html)'
- en: '[Market Data and News: A Time Series Analysis](https://www.kdnuggets.com/2022/06/market-data-news-time-series-analysis.html)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[市场数据与新闻：时间序列分析](https://www.kdnuggets.com/2022/06/market-data-news-time-series-analysis.html)'
- en: '[The Best Python Courses: An Analysis Summary](https://www.kdnuggets.com/2022/01/best-python-courses-analysis-summary.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[最佳Python课程：分析总结](https://www.kdnuggets.com/2022/01/best-python-courses-analysis-summary.html)'
- en: '[Machine Learning’s Sweet Spot: Pure Approaches in NLP and Document Analysis](https://www.kdnuggets.com/2022/05/machine-learning-sweet-spot-pure-approaches-nlp-document-analysis.html)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习的甜蜜点：NLP和文档分析中的纯粹方法](https://www.kdnuggets.com/2022/05/machine-learning-sweet-spot-pure-approaches-nlp-document-analysis.html)'
