- en: 'Transforming AI with LangChain: A Text Data Game Changer'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/08/transforming-ai-langchain-text-data-game-changer.html](https://www.kdnuggets.com/2023/08/transforming-ai-langchain-text-data-game-changer.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![XXXXX](../Images/fae8bf3635873f99b3b132732050fed6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Over the past few years, Large Language Models — or LLMs for friends — **have
    taken the world of artificial intelligence by storm. **
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: With the groundbreaking release of OpenAI’s GPT-3 in 2020, we have witnessed
    a steady surge in the popularity of LLMs, which has only intensified with recent
    advancements in the field.
  prefs: []
  type: TYPE_NORMAL
- en: '**These powerful AI models have opened up new possibilities for natural language
    processing applications**, enabling developers to create more sophisticated, human-like
    interactions.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Isn’t it?***'
  prefs: []
  type: TYPE_NORMAL
- en: However, when dealing with this AI technology it is hard to scale and generate
    reliable algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Amidst this rapidly evolving landscape, **LangChain has emerged as a versatile
    framework designed to help developers harness the full potential of LLMs for a
    wide range of applications. One of the most important use cases is to deal with
    large** **amounts** **of text data.**
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive in and start harnessing the power of LLMs today!
  prefs: []
  type: TYPE_NORMAL
- en: LangChain can be used in chatbots, question-answering systems, summarization
    tools, and beyond. **However, one of the most useful - and used - applications
    of LangChain is dealing with text.**
  prefs: []
  type: TYPE_NORMAL
- en: Today’s world is flooded with data. And one of the most notorious types is text
    data.
  prefs: []
  type: TYPE_NORMAL
- en: All websites and apps are being bombed with tons and tons of words every single
    day. No human can process this amount of information…
  prefs: []
  type: TYPE_NORMAL
- en: '**But can computers?**'
  prefs: []
  type: TYPE_NORMAL
- en: LLM techniques together with LangChain are a great way to reduce the amount
    of text while maintaining the most important parts of the message. This is why
    today we will cover two basic — but really useful — use cases of LangChain to
    deal with text.
  prefs: []
  type: TYPE_NORMAL
- en: '**Summarization:** Express the most important facts about a body of text or
    chat interaction. It can reduce the amount of data while maintaining the most
    important parts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extraction:** Pull structured data from a body of text or some user query.
    It can detect and extract keywords within the text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether you’re new to the world of LLMs or looking to take your language generation
    projects to the next level, this guide will provide you with valuable insights
    and hands-on examples to unlock the full potential of LangChain to deal with text.
  prefs: []
  type: TYPE_NORMAL
- en: ⚠️ If you want to have some basic grasp, you can go check ????????
  prefs: []
  type: TYPE_NORMAL
- en: '[**LangChain 101: Build Your Own GPT-Powered Applications — KDnuggets**](/2023/04/langchain-101-build-gptpowered-applications.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Always remember that for working with OpenAI and GPT models, we need to have
    the OpenAI library installed on our local computer and have an active OpenAI key.
    If you do not know how to do that, you can go check [here](https://medium.com/@rfeers/openai-a-step-by-step-guide-to-getting-your-api-key-gpt-usage-control-artificial-intelligence-2a0917c70f3f).
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Summarization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ChatGPT together with LangChain **can summarize information quickly and in a
    very reliable way. **
  prefs: []
  type: TYPE_NORMAL
- en: LLM summarization techniques are a great way to reduce the amount of text while
    maintaining the most important parts of the message. This is why LLMs can be the
    best ally to any digital company that needs to process and analyze large volumes
    of  text data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform the following examples, the following libraries are required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 1.1\. Short text summarization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For summaries of short texts, the method is straightforward, in fact, you don’t
    need to do anything fancy other than simple prompting with instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Which basically means generating a template with an input variable.
  prefs: []
  type: TYPE_NORMAL
- en: I know you might be wondering… **what is exactly a prompt template?**
  prefs: []
  type: TYPE_NORMAL
- en: A prompt template refers to a reproducible way to generate a prompt. It contains
    a text string - a template - that can take in a set of parameters from the end
    user and generates a prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'A prompt template contains:'
  prefs: []
  type: TYPE_NORMAL
- en: '**instructions to the language model** - that allow us to standardize some
    steps for our LLM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**an input variable** -  that allows us to apply the previous instructions
    to any input text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s see this in a simple example. I can standardize a prompt that generates
    a name of a brand that produces a specific product.
  prefs: []
  type: TYPE_NORMAL
- en: '![XXXXX](../Images/1c86fcb34a81da268a6f7bb73f911bcc.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of my Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: As you can observe in the previous example, the magic of LangChain is that we
    can define a standardized prompt with a changing input variable.
  prefs: []
  type: TYPE_NORMAL
- en: The instructions to generate a name for a brand remain always the same.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The product variable works as an input that can be changed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This allows us to define versatile prompts that can be used in different scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: '*So now that we know what a prompt template is… *'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s imagine we want to define a prompt that summarizes any text using super
    easy-to-understand vocabulary. We can define a prompt template with some specific
    instructions and a text variable that changes depending on the input variable
    we define.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: So let’s try this prompt template. Using the wikipedia API, I am going to get
    the summary of the USA country and further summarize it in a really easy-to-understand
    tone.
  prefs: []
  type: TYPE_NORMAL
- en: '![XXXXX](../Images/796b446babb8aea865223f36c584ae3d.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of my Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: So now that we know how to summarize a short text… can I spice this up a bit?
  prefs: []
  type: TYPE_NORMAL
- en: '*Sure we can with… *'
  prefs: []
  type: TYPE_NORMAL
- en: 1.2\. Long text summarization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When dealing with long texts, the main problem is that we cannot communicate
    them to our AI model directly via prompt, as they contain too many tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '*And now you might be wondering… what is a token?*'
  prefs: []
  type: TYPE_NORMAL
- en: Tokens are how the model sees the input — single characters, words, parts of
    words, or segments of text. As you can observe, the definition is not really precise
    and it depends on every model. For instance, OpenAI’s GPT 1000 tokens are approximately
    750 words.
  prefs: []
  type: TYPE_NORMAL
- en: But the most important thing to learn is that our cost depends on the number
    of tokens and that we cannot send as many tokens as we want in a single prompt. 
    To have a longer text, we will repeat the same example as before but using the
    whole Wikipedia page text.
  prefs: []
  type: TYPE_NORMAL
- en: '![XXXXX](../Images/fb4c7a2f319dd269e3149f4a4aafbf76.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of my Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: If we check how long it is… it is around 17K tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Which is quite a lot to be sent directly to our API.
  prefs: []
  type: TYPE_NORMAL
- en: '*So what now?*'
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll need to split it up. This process is called **chunking** or **splitting**
    your text into smaller pieces. I usually use [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html)
    because it’s easy to control but there are a [bunch](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)
    you can try.
  prefs: []
  type: TYPE_NORMAL
- en: After using it, instead of just having a single piece of text, we get 23 pieces
    which facilitate the work of our GPT model.
  prefs: []
  type: TYPE_NORMAL
- en: Next we need to load up a chain which will make successive calls to the LLM
    for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'LangChain provides the Chain interface for such **chained** applications. We
    define a Chain very generically as a sequence of calls to components, which can
    include other chains. The base interface is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If you want to learn more about chains, you can go check directly in the [LangChain
    documentation. ](https://python.langchain.com/docs/get_started/introduction.html)
  prefs: []
  type: TYPE_NORMAL
- en: So if we repeat again the same procedure with the splitted text - called docs
    - the LLM can easily generate a summary of the whole page.
  prefs: []
  type: TYPE_NORMAL
- en: '![XXXXX](../Images/dff9ad393005d1e90ee5bf3a9f423920.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of my Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '*Useful right?*'
  prefs: []
  type: TYPE_NORMAL
- en: So now that we know how to summarize text, we can move to the second use case!
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Extraction is the process of parsing data from a piece of text. This is commonly
    used with output parsing to *structure* our data.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting key data is really useful in order to identify and parse key words
    within a text.  Common use cases are extracting a structured row from a sentence
    to insert into a database or extracting multiple rows from a long document to
    insert into a database.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s imagine we are running a digital e-commerce company and we need to process
    all reviews that are stated on our website.
  prefs: []
  type: TYPE_NORMAL
- en: '**I could go read all of them one by one… which would be crazy. **'
  prefs: []
  type: TYPE_NORMAL
- en: '*Or I can simply EXTRACT the information that I need from each of them and
    analyze all the data. *'
  prefs: []
  type: TYPE_NORMAL
- en: Sounds easy… right?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with a quite simple example. First, we need to import the following
    libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 2.1\. Extracting specific words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I can try to look for specific words within some text. In this case, I want
    to parse all fruits that are contained within a text.  Again, it is quite straightforward
    as before. We can easily define a prompt giving clear instructions to our LLM
    stating that identifies all fruits contained in a text and gives back a JSON-like
    structure containing such fruits and their corresponding colors.
  prefs: []
  type: TYPE_NORMAL
- en: '![XXXXX](../Images/1bab02cad584e793442e4d6652b3961e.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of my Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: And as we can see before, it works perfectly!
  prefs: []
  type: TYPE_NORMAL
- en: So now… let’s play a little bit more with it. While this worked this time, it’s
    not a long term reliable method for more advanced use cases. And this is where
    a fantastic LangChain concept comes into play…
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Using LangChain’s Response Schema
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LangChain’s response schema will do two main things for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generate a prompt with bonafide format instructions.** This is great because
    I don’t need to worry about the prompt engineering side, I’ll leave that up to
    LangChain!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Read the output from the LLM and turn it into a proper python object for
    me.** Which means, always generate a given structure that is useful and that my
    system can parse.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**And to do so, I just need to define what response I except from the model. **'
  prefs: []
  type: TYPE_NORMAL
- en: So let’s imagine I want to determine the products and brands that users are
    stating in their comments. I could easily perform as before with a simple prompt
    - take advantage of LangChain to generate a more reliable method.
  prefs: []
  type: TYPE_NORMAL
- en: So first I need to define a response_schema where I define every keyword I want
    to parse with a name and a description.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: After defining our parser, we generate the format of our instruction using the
    .get_format_instructions() command from LangChain and define the final prompt
    using the ChatPromptTemplate. And now it is as easy as using this output_parser
    object with any input query I can think of, and it will automatically generate
    an output with my desired keywords.
  prefs: []
  type: TYPE_NORMAL
- en: '![XXXXX](../Images/cf56304ad3a4912ba5cb6bbcd3ebaa4c.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of my Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can observe in the example below, with the input of “I run out of Yogurt
    Danone, No-brand Oat Milk and those vegan bugers made by Heura”, the LLM gives
    me the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![XXXXX](../Images/17bdd2865f3f9ccc6c3eca312c694ac3.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of my Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Main Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LangChain is a versatile Python library that helps developers harness the full
    potential of LLMs, especially for dealing with large amounts of text data. It
    excels at two main use cases for dealing with text. LLMs enable developers to
    create more sophisticated and human-like interactions in natural language processing
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summarization: **LangChain can quickly and reliably summarize information**,
    reducing the amount of text while preserving the most important parts of the message.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Extraction: **The library can parse data from a piece of text, allowing for
    structured output** and enabling tasks like inserting data into a database or
    making API calls based on extracted parameters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LangChain facilitates prompt engineering, which is a crucial technique for maximizing
    the performance of AI models like ChatGPT. With prompt engineering, developers
    can design standardized prompts that can be reused across different use cases,
    making the AI application more versatile and effective.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Overall, LangChain serves as a powerful tool to enhance AI usage, especially
    when dealing with text data, and prompt engineering is a key skill for effectively
    leveraging AI models like ChatGPT in various applications.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Josep Ferrer](https://www.linkedin.com/in/josep-ferrer-sanchez)** is an
    analytics engineer from Barcelona. He graduated in physics engineering and is
    currently working in the Data Science field applied to human mobility. He is a
    part-time content creator focused on data science and technology. You can contact
    him on [LinkedIn](https://www.linkedin.com/in/josep-ferrer-sanchez/), [Twitter](https://twitter.com/rfeers)
    or [Medium](https://medium.com/@rfeers).'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Breaking the Data Barrier: How Zero-Shot, One-Shot, and Few-Shot…](https://www.kdnuggets.com/2023/08/breaking-data-barrier-zeroshot-oneshot-fewshot-learning-transforming-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The AIoT Revolution: How AI and IoT Are Transforming Our World](https://www.kdnuggets.com/2022/07/aiot-revolution-ai-iot-transforming-world.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, July 27: The AIoT Revolution: How AI and IoT Are…](https://www.kdnuggets.com/2022/n30.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How AI is Transforming the Retail Industry](https://www.kdnuggets.com/how-ai-is-transforming-the-retail-industry)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Future-Proof Your Data Game: Top Skills Every Data Scientist Needs in 2023](https://www.kdnuggets.com/futureproof-your-data-game-top-skills-every-data-scientist-needs-in-2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Step up your Python game with Fast Python for Data Science!](https://www.kdnuggets.com/2022/06/manning-step-python-game-fast-python-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
