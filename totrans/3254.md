# 支持向量机（SVM）教程：通过示例学习SVM

> 原文：[https://www.kdnuggets.com/2017/08/support-vector-machines-learning-svms-examples.html/2](https://www.kdnuggets.com/2017/08/support-vector-machines-learning-svms-examples.html/2)

### **非线性可分数据**

我们已经了解了支持向量机如何系统地处理完全/几乎线性可分的数据。对于绝对非线性可分的数据，它是如何处理的呢？毕竟，很多现实世界的数据都属于这个类别。显然，找到一个超平面可能不再适用。这似乎很遗憾，因为支持向量机在这方面表现出色。

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速通道进入网络安全职业。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 在IT方面支持你的组织

* * *

这是一个非线性可分数据的示例（这是著名的 [XOR 数据集](http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node19.html) 的变体），用线性分类器SVM找到的结果展示如下：

![](../Images/1762042cf4625e35939e2393246648ce.png)

你会同意这看起来不太好。我们在训练数据上的准确率只有75%——这是线性分隔情况下的最佳结果。而且，更糟的是，这条线与一些数据点非常接近。最佳准确率并不理想，为了达到这个准确率，这条线几乎与几个点重叠。

我们需要做得更好。

这就是我最喜欢的支持向量机部分之一。到目前为止，我们有一种非常擅长找到超平面的技术。但我们也有非线性可分的数据。那么我们该怎么办？将数据投影到一个 *线性可分* 的空间中，并在这个空间中找到一个超平面！

我将一步一步地阐述这个想法。

我们从上图中的数据集开始，并将其投影到三维空间中，其中新的坐标是：

![](../Images/120a96bf55dc688b8f8d9b92f30b67ee.png)

这就是投影后的数据。你看到一个我们可能可以插入一个平面的位置吗？

![](../Images/dd530ca54f0cdf41d7e5c761885a511b.png)

让我们在其上运行SVM：

![](../Images/c388a96eff09336ed5e0bc472305604b.png)

bingo！我们有了完美的标签分隔！让我们将平面投影回原始二维空间，看看分隔边界是什么样的：

![](../Images/67b03248b5a3f5f2eb1e2ef030fca8ab.png)

在训练数据上达到100%的准确率 *并且* 分隔边界不会过于接近数据！耶！

原始空间中分隔边界的形状取决于投影。在投影空间中，这*总是*一个超平面。

> *记住投影数据的主要目标是利用支持向量机的超平面发现能力。*

当你将其映射回原始空间时，分隔边界不再是一条直线。这对边距和支持向量也是如此。就我们的视觉直觉而言，它们在投影空间中是合理的。

看看它们在投影空间中的样子，然后再看看在原始空间中的样子。3D 边距是分隔超平面上方和下方平面之间的区域（未着色以避免视觉混乱）。

![](../Images/2f76478a7f13efda4d2825728d0e4860.png)

投影空间中有 4 个支持向量，这似乎是合理的。它们位于确定边距的两个平面上。在原始空间中，它们仍然在边距上，但似乎数量不足。

让我们退一步分析一下发生了什么：

****1\. 我怎么知道将数据投影到什么空间？****

看起来我非常具体——在其中有一个平方根 2！

在这种情况下，我想展示高维度投影的工作原理，因此我选择了一个非常具体的投影。一般来说，这很难知道。然而，我们所知道的是，数据在投影到更高维度时，更*可能*是线性可分的，这要归功于[Cover 定理](https://en.wikipedia.org/wiki/Cover%27s_theorem)。

在实践中，我们尝试几种高维度的投影，看看哪种有效。事实上，我们可以将数据投影到*无限*维度，这通常效果很好。这值得详细探讨，接下来的部分就是关于这个的。

****2\. 那我先投影数据，然后运行 SVM 吗？****

不。为了使上述例子易于理解，我让它听起来像是我们需要先投影数据。事实上，你是让支持向量机为你进行投影。这有一些好处。首先，支持向量机使用一种叫做*核函数*的东西来进行这些投影，而且这些操作非常快速（原因我们很快会看到）。

另外，记得我在上一点提到过投影到无限维度吗？如果你自己进行数据投影，你如何表示或存储无限维度？事实证明，支持向量机在这方面非常聪明，多亏了核函数。

是时候看看核函数了。

### 进一步阅读这个话题

+   [支持向量机：一种直观的方法](https://www.kdnuggets.com/2022/08/support-vector-machines-intuitive-approach.html)

+   [支持向量机的温和介绍](https://www.kdnuggets.com/2023/07/gentle-introduction-support-vector-machines.html)

+   [语义向量搜索如何转变客户支持互动](https://www.kdnuggets.com/how-semantic-vector-search-transforms-customer-support-interactions)

+   [Python 向量数据库与向量索引：构建 LLM 应用](https://www.kdnuggets.com/2023/08/python-vector-databases-vector-indexes-architecting-llm-apps.html)

+   [挑选示例以理解机器学习模型](https://www.kdnuggets.com/2022/11/picking-examples-understand-machine-learning-model.html)

+   [使用示例进行集成学习](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)
