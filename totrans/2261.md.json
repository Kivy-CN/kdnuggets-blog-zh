["```py\nclassification_models = torchvision.models.list_models(module=torchvision.models)\n\nprint(len(classification_models), \"classification models:\", classification_models)\n```", "```py\n80 classification models: ['alexnet', 'convnext_base', 'convnext_large', 'convnext_small', 'convnext_tiny', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'efficientnet_v2_l', 'efficientnet_v2_m', 'efficientnet_v2_s', 'googlenet', 'inception_v3', 'maxvit_t', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'mobilenet_v3_large', 'mobilenet_v3_small', 'regnet_x_16gf', 'regnet_x_1_6gf', 'regnet_x_32gf', 'regnet_x_3_2gf', 'regnet_x_400mf', 'regnet_x_800mf', 'regnet_x_8gf', 'regnet_y_128gf', 'regnet_y_16gf', 'regnet_y_1_6gf', 'regnet_y_32gf', 'regnet_y_3_2gf', 'regnet_y_400mf', 'regnet_y_800mf', 'regnet_y_8gf', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext101_64x4d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'swin_b', 'swin_s', 'swin_t', 'swin_v2_b', 'swin_v2_s', 'swin_v2_t', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'vit_b_16', 'vit_b_32', 'vit_h_14', 'vit_l_16', 'vit_l_32', 'wide_resnet101_2', 'wide_resnet50_2']\n```", "```py\nvgg16 = torchvision.models.vgg16_bn(weights=None)\nresnet50 = torchvision.models.resnet50(weights=None)\nresnet152 = torchvision.models.resnet152(weights=None)\n\nprint(\"vgg16**\\n**\", vgg16.classifier)\nprint(\"resnet50**\\n**\", resnet50.fc)\nprint(\"resnet152**\\n**\", resnet152.fc)\n```", "```py\nvgg16\n Sequential(\n  (0): Linear(in_features=25088, out_features=4096, bias=True)\n  (1): ReLU(inplace=True)\n  (2): Dropout(p=0.5, inplace=False)\n  (3): Linear(in_features=4096, out_features=4096, bias=True)\n  (4): ReLU(inplace=True)\n  (5): Dropout(p=0.5, inplace=False)\n  (6): Linear(in_features=4096, out_features=1000, bias=True)\n)\nresnet50\n Linear(in_features=2048, out_features=1000, bias=True)\nresnet152\n Linear(in_features=2048, out_features=1000, bias=True) \n```", "```py\ndef __init__(self, backbone, load_pretrained):\n    super().__init__()\n    assert backbone in backbones\n    self.backbone = backbone\n    self.pretrained_model = None\n    self.classifier_layers = []\n    self.new_layers = []\n\n    if backbone == \"resnet50\":\n        if load_pretrained:\n            self.pretrained_model = torchvision.models.resnet50(\n                weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2\n            )\n        else:\n            self.pretrained_model = torchvision.models.resnet50(weights=None)\n        # end if\n\n        self.classifier_layers = [self.pretrained_model.fc]\n        # Replace the final layer with a classifier for 102 classes for the Flowers 102 dataset.\n        self.pretrained_model.fc = nn.Linear(\n            in_features=2048, out_features=102, bias=True\n        )\n        self.new_layers = [self.pretrained_model.fc]\n    elif backbone == \"resnet152\":\n        if load_pretrained:\n            self.pretrained_model = torchvision.models.resnet152(\n                weights=torchvision.models.ResNet152_Weights.IMAGENET1K_V2\n            )\n        else:\n            self.pretrained_model = torchvision.models.resnet152(weights=None)\n        # end if\n\n        self.classifier_layers = [self.pretrained_model.fc]\n        # Replace the final layer with a classifier for 102 classes for the Flowers 102 dataset.\n        self.pretrained_model.fc = nn.Linear(\n            in_features=2048, out_features=102, bias=True\n        )\n        self.new_layers = [self.pretrained_model.fc]\n    elif backbone == \"vgg16\":\n        if load_pretrained:\n            self.pretrained_model = torchvision.models.vgg16_bn(\n                weights=torchvision.models.VGG16_BN_Weights.IMAGENET1K_V1\n            )\n        else:\n            self.pretrained_model = torchvision.models.vgg16_bn(weights=None)\n        # end if\n\n        self.classifier_layers = [self.pretrained_model.classifier]\n        # Replace the final layer with a classifier for 102 classes for the Flowers 102 dataset.\n        self.pretrained_model.classifier[6] = nn.Linear(\n            in_features=4096, out_features=102, bias=True\n        )\n        self.new_layers = [self.pretrained_model.classifier[6]] \n```", "```py\ndef fine_tune(self, what: FineTuneType):\n    # The requires_grad parameter controls whether this parameter is\n    # trainable during model training.\n    m = self.pretrained_model\n    for p in m.parameters():\n        p.requires_grad = False\n    if what is FineTuneType.NEW_LAYERS:\n        for l in self.new_layers:\n            for p in l.parameters():\n                p.requires_grad = True\n    elif what is FineTuneType.CLASSIFIER:\n        for l in self.classifier_layers:\n            for p in l.parameters():\n                p.requires_grad = True\n    else:\n        for p in m.parameters():\n            p.requires_grad = True \n```", "```py\ndef get_optimizer_params(self):\n    \"\"\"This method is used only during model fine-tuning when we need to\n    set a linearly or exponentially decaying learning rate (LR) for the\n    layers in the model. We exponentially decay the learning rate as we\n    move away from the last output layer.\n    \"\"\"\n    options = []\n    if self.backbone == \"vgg16\":\n        # For vgg16, we start with a learning rate of 1e-3 for the last layer, and\n        # decay it to 1e-7 at the first conv layer. The intermediate rates are\n        # decayed linearly.\n        lr = 0.0001\n        options.append(\n            {\n                \"params\": self.pretrained_model.classifier.parameters(),\n                \"lr\": lr,\n            }\n        )\n        final_lr = lr / 1000.0\n        diff_lr = final_lr - lr\n        lr_step = diff_lr / 44.0\n        for i in range(43, -1, -1):\n            options.append(\n                {\n                    \"params\": self.pretrained_model.features[i].parameters(),\n                    \"lr\": lr + lr_step * (44 - i),\n                }\n            )\n        # end for\n    elif self.backbone in [\"resnet50\", \"resnet152\"]:\n        # For the resnet class of models, we decay the LR exponentially and reduce\n        # it to a third of the previous value at each step.\n        layers = [\"conv1\", \"bn1\", \"layer1\", \"layer2\", \"layer3\", \"layer4\", \"fc\"]\n        lr = 0.0001\n        for layer_name in reversed(layers):\n            options.append(\n                {\n                    \"params\": getattr(self.pretrained_model, layer_name).parameters(),\n                    \"lr\": lr,\n                }\n            )\n            lr = lr / 3.0\n        # end for\n    # end if\n    return options\n\n# end def \n```", "```py\noptimizer = torch.optim.Adam(fc.get_optimizer_params(), lr=1e-8)\n```"]