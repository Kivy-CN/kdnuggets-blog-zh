- en: 'Machine Learning Is Not Like Your Brain Part 5: Biological Neurons Can’t Do
    Summation of Inputs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习不像你的大脑 第五部分：生物神经元无法对输入进行求和
- en: 原文：[https://www.kdnuggets.com/2022/07/machine-learning-like-brain-part-5-biological-neurons-cant-summation-inputs.html](https://www.kdnuggets.com/2022/07/machine-learning-like-brain-part-5-biological-neurons-cant-summation-inputs.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/07/machine-learning-like-brain-part-5-biological-neurons-cant-summation-inputs.html](https://www.kdnuggets.com/2022/07/machine-learning-like-brain-part-5-biological-neurons-cant-summation-inputs.html)
- en: Fundamental to all ML systems is the idea that the artificial neuron, the perceptron,
    sums the weighted signals from the various input synapses. Admittedly, there are
    a few cases where this appears to work in a biological neuron model. For the majority
    of cases, though, it doesn’t.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 所有机器学习系统的基本思想是人工神经元，即感知器，对来自各种输入突触的加权信号进行求和。诚然，在生物神经元模型中确实有少数情况看起来能工作。然而，在大多数情况下，它不能。
- en: The biological neuron has a membrane potential or voltage which I’ll call its
    “charge.” The neuron accumulates charge from its input synapses and the various
    synapses have “weights” corresponding to the amount of charge they contribute.
    The synapse weight may be positive or negative. When the charge exceeds a threshold,
    the neuron will fire, sending a neural spike to all the neurons to which it is
    connected and resetting the internal charge. For convenience, we’ll call the resting
    charge 0 and the threshold level 1.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经元具有一个膜电位或电压，我称之为“电荷”。神经元从其输入突触中积累电荷，各个突触具有对应于它们贡献电荷量的“权重”。突触权重可以是正的也可以是负的。当电荷超过阈值时，神经元会发射神经冲动，向所有连接的神经元发送信号，并重置内部电荷。为了方便起见，我们将静息电荷称为0，阈值水平称为1。
- en: 'There are three characteristics of biological neurons which make signal summation
    difficult or impossible:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经元有三个特征，使得信号求和变得困难或不可能：
- en: After a neuron fires, it cannot fire again for about 4ms, the “refractory period.”
    During the refractory period, all incoming signals are ignored.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在神经元发射信号后，它在约4毫秒内不能再次发射，这段时间称为“绝对不应期”。在绝对不应期内，所有输入信号都被忽略。
- en: No matter the level of incoming signals, the neuron can never fire faster than
    its maximum rate.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 无论输入信号的水平如何，神经元的发射速度都不能超过其最大速率。
- en: The internal charge cannot go below 0\. If the internal charge is 0, incoming
    negative-weight signals will be ignored. You can see that ignoring any incoming
    signals will upset the accuracy of the summation.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内部电荷不能低于0。如果内部电荷为0，负权重的输入信号将被忽略。你可以看到，忽略任何输入信号都会扰乱求和的准确性。
- en: The internal charge cannot go above 1\. If the charge ever reaches 1, the neuron
    will fire, so additional incoming signals (either positive or negative) will be
    ignored during the refractory period. This and the above means that an overall
    summation can’t be performed properly if any partial sum would go below 0 or above
    1.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内部电荷不能超过1。如果电荷达到了1，神经元将发射信号，因此在绝对不应期内，额外的输入信号（无论是正的还是负的）都将被忽略。这一点加上前面提到的内容意味着，如果任何部分和低于0或高于1，总体求和将无法正确进行。
- en: Consider a simple example. Imagine that our neurons represent a value from 0-1
    in a 40ms period, represented by the number of neural spikes emitted in the period.
    To represent a value of .6, we’d have 6 spikes which might be evenly distributed
    but, more likely, are randomly distributed in the 40ms time segment. Let’s assign
    our signal with a value of .6 to “In1” in the figure and assign another with a
    value of .1 to “In2”. For the summation to be accurate, we’d like “Out” to spike
    7 times in the time segment.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个简单的例子来说明。假设我们的神经元在40毫秒的时间段内表示一个0到1之间的值，这个值由该时间段内发射的神经冲动数量表示。为了表示0.6，我们需要6个冲动，这些冲动可能均匀分布，但更可能是在40毫秒的时间段内随机分布。我们将图中的信号值0.6分配给“In1”，将另一个值0.1分配给“In2”。为了确保总和的准确性，我们希望“Out”在该时间段内发射7次冲动。
- en: Looking just at the value of 6, during the 40ms time segment, Out will be in
    its refractory period for 24ms so there is a 60% probability that our signal from
    In2 will arrive during a refractory period and be ignored. If the 6 spikes are
    evenly distributed, they occur every 6.7ms so if our in2 spike happens to sneak
    in, the following spike from In1 will arrive during the refractory period and
    be ignored. Therefore, if spikes are evenly distributed, there is no possibility
    that 6 spikes + 1 spike will ever equal anything but 6 spikes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 仅从6的值来看，在40毫秒的时间段内，Out会处于其不应期24毫秒，因此我们的信号从In2到达不应期并被忽略的概率为60%。如果6个脉冲均匀分布，它们每隔6.7毫秒发生一次，因此如果我们的in2脉冲恰好在此期间到达，来自In1的下一个脉冲将到达不应期并被忽略。因此，如果脉冲均匀分布，6个脉冲+1个脉冲永远不会等于除6个脉冲之外的任何值。
- en: If the spikes are randomly distributed, there is some possibility that a neuron
    might compute that 6+1=7, but the probability is remote and related to the distribution
    of the spikes. You *could* get a better summation with neurons if the signal values
    were always small relative to the time segment. To do this, you need to further
    reduce the number of allowed values and/or make the time segment longer. With
    a 400ms time segment you can get reasonable summations of small values like 6+1,
    but this is too slow and too limited to be useful for Machine Learning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果脉冲是随机分布的，神经元有可能计算出6+1=7，但概率较小，并且与脉冲的分布有关。如果信号值相对于时间段总是较小，你*可能*会通过神经元获得更好的加总。要做到这一点，你需要进一步减少允许的值的数量和/或延长时间段。使用400毫秒的时间段，你可以获得类似6+1的小值的合理加总，但这对于机器学习来说速度太慢且限制太多。
- en: Considering the 3rd and 4th points above, perceptrons start with a similar limitation,
    which is why there is a function, usually a sigmoid, which maps the summed inputs
    back to the 0-1 range (or -1,1). The problem is that biological neurons “clip”
    the incoming signals *before* the summation is complete, so there is no way to
    clean up with the equivalent of the sigmoid function.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到以上第3和第4点，感知机有类似的限制，这就是为什么会有一个函数，通常是sigmoid，将加总的输入映射回0-1范围（或-1,1）。问题在于生物神经元会在加总完成*之前*“剪裁”输入信号，因此没有办法用等效的sigmoid函数来清理。
- en: While the neuron cannot perform the most basic ML function, it is much better
    at recognizing which of several signals arrived first. This is how you can recognize
    the direction of sound from signals which arrive at your two ears only a fraction
    of a millisecond apart. Neurons are also very good at determining which of two
    input signals is spiking faster. This functionality is used by neurons which recognize
    boundaries in your visual field where neurons on one side of the boundary are
    firing at a different rate than those on the other. Machine learning algorithms
    tupically ignore these capabilities of the biological neuron.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管神经元无法执行最基本的机器学习功能，但它们在识别多个信号中哪个先到达方面要好得多。这就是你如何通过在两只耳朵之间仅相差毫秒的信号来识别声音的方向。神经元在确定两个输入信号中哪个脉冲更快方面也非常擅长。这种功能被用于识别视觉领域中的边界，其中边界一侧的神经元的发放速率与另一侧的不同。机器学习算法通常忽略了生物神经元的这些能力。
- en: In the same way that neurons have a limited range of values as was discussed
    in the previous article in this series, synapses are even more limited. And that
    is the topic of the next article in our nine-part series on why ML is not like
    your brain.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 就像之前文章中讨论的神经元具有有限的值范围一样，突触的限制更大。这也是我们九部分系列文章中下一个主题——为什么机器学习不像你的大脑。
- en: '![Machine Learning Is Not Like Your Brain Part 5: Biological Neurons Can’t
    Do Summation of Inputs](../Images/8c8a2cb72bbb23eac911ec8d84ed2eff.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习不像你的大脑 第5部分：生物神经元不能对输入进行加总](../Images/8c8a2cb72bbb23eac911ec8d84ed2eff.png)'
- en: 6+1=6, showing that spiking neurons don’t really perform the summation the way
    artificial perceptrons do. The neuron “timing” is just a marker firing every 40ms.
    “In1” is slightly randomized and produces 6 spikes in each time period, while
    “In2” fires only once per period. Both are connected to “Out” with synapses of
    weight 1\. By observing Out, you can see which spike lands in the refractory period
    of Out and is lost. In the left case, the spike from In2 arrives during the refractory
    period. In the right-hand case, the spike from In2 causes Out to fire, but the
    next spike from In1 arrives in the refractory period. In either case, the summation
    clearly does not work properly.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 6+1=6，表明脉冲神经元并不像人工感知器那样执行求和操作。神经元的“时序”只是每 40 毫秒发射一次标记。“In1”稍微随机化，在每个时间段内产生 6
    个脉冲，而“In2”每个周期仅发射一次。两者都通过权重为 1 的突触连接到“Out”。通过观察 Out，你可以看到哪个脉冲落在 Out 的不应期中并被丢失。在左侧情况下，In2
    的脉冲在不应期内到达。在右侧情况下，In2 的脉冲导致 Out 发射，但 In1 的下一个脉冲在不应期内到达。在任何情况下，求和显然都不能正常工作。
- en: You can [learn more about this topic in this video](https://www.youtube.com/watch?v=dIa85rxo8AU).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[这个视频中了解更多关于这个话题](https://www.youtube.com/watch?v=dIa85rxo8AU)。
- en: '**[Charles Simon](https://futureai.guru/Founder.aspx)** is a nationally recognized
    entrepreneur and software developer, and the CEO of FutureAI. Simon is the author
    of Will the Computers Revolt?: Preparing for the Future of Artificial Intelligence,
    and the developer of Brain Simulator II, an AGI research software platform. For
    more information, [visit here](https://futureai.guru/Founder.aspx).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**[查尔斯·西蒙](https://futureai.guru/Founder.aspx)** 是一位全国认可的企业家和软件开发者，未来人工智能公司的首席执行官。西蒙是《计算机会叛变吗？：为人工智能的未来做准备》的作者，也是脑模拟器
    II 的开发者，这是一个 AGI 研究软件平台。更多信息，[请访问这里](https://futureai.guru/Founder.aspx)。'
- en: More On This Topic
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Machine Learning Is Not Like Your Brain Part One: Neurons Are Slow,…](https://www.kdnuggets.com/2022/04/machine-learning-like-brain-part-one-neurons-slow-slow-slow.html)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习并不像你的大脑 第一部分：神经元慢,…](https://www.kdnuggets.com/2022/04/machine-learning-like-brain-part-one-neurons-slow-slow-slow.html)'
- en: '[Machine Learning Is Not Like Your Brain Part Two: Perceptrons vs Neurons](https://www.kdnuggets.com/2022/05/machine-learning-like-brain-part-two-perceptrons-neurons.html)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习并不像你的大脑 第二部分：感知器与神经元](https://www.kdnuggets.com/2022/05/machine-learning-like-brain-part-two-perceptrons-neurons.html)'
- en: '[Machine Learning is Not Like Your Brain Part Seven: What Neurons…](https://www.kdnuggets.com/2022/08/machine-learning-like-brain-part-seven-neurons-good.html)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习并不像你的大脑 第七部分：神经元…](https://www.kdnuggets.com/2022/08/machine-learning-like-brain-part-seven-neurons-good.html)'
- en: '[Efficiency Spells the Difference Between Biological Neurons and…](https://www.kdnuggets.com/2022/11/efficiency-spells-difference-biological-neurons-artificial-counterparts.html)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[效率在生物神经元和…之间的区别](https://www.kdnuggets.com/2022/11/efficiency-spells-difference-biological-neurons-artificial-counterparts.html)'
- en: '[Machine Learning Is Not Like Your Brain Part 3: Fundamental Architecture](https://www.kdnuggets.com/2022/06/machine-learning-like-brain-part-3-fundamental-architecture.html)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习并不像你的大脑 第三部分：基本架构](https://www.kdnuggets.com/2022/06/machine-learning-like-brain-part-3-fundamental-architecture.html)'
- en: '[Machine Learning Is Not Like Your Brain Part 4: The Neuron’s…](https://www.kdnuggets.com/2022/06/machine-learning-like-brain-part-4-neuron-limited-ability-represent-precise-values.html)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习并不像你的大脑 第四部分：神经元的…](https://www.kdnuggets.com/2022/06/machine-learning-like-brain-part-4-neuron-limited-ability-represent-precise-values.html)'
