# 机器学习的对抗性攻击是一颗定时炸弹

> 原文：[https://www.kdnuggets.com/2021/01/machine-learning-adversarial-attacks.html](https://www.kdnuggets.com/2021/01/machine-learning-adversarial-attacks.html)

[评论](#comments)

![](../Images/cf462880f32504c8511171ef0dd53996.png)

如果你关注人工智能的新闻，你可能听说过或见过修改过的图像，例如看起来对人眼普通但让AI系统行为异常的熊猫、乌龟和停车标志。这些被称为[对抗性示例或对抗性攻击](https://bdtechtalks.com/2018/12/27/deep-learning-adversarial-attacks-ai-malware/)，这些图像及其[音频](https://bdtechtalks.com/2019/04/29/ai-audio-adversarial-examples/)和文本[对应物](https://bdtechtalks.com/2019/04/02/ai-nlp-paraphrasing-adversarial-attacks/)已成为机器学习社区日益关注的焦点。

尽管关于[对抗性机器学习](https://bdtechtalks.com/2020/07/15/machine-learning-adversarial-examples/)的研究不断增多，但数据显示在实际应用中应对对抗性攻击的进展甚微。

快速扩展的机器学习应用使得技术社区必须制定一个路线图，以确保AI系统免受对抗性攻击的威胁。否则，对抗性机器学习可能会酿成灾难。

![](../Images/28f6c254c67fed2d1f983de3e635d807.png)

*AI研究人员发现，通过在停车标志上添加小的黑白贴纸，他们可以使这些标志对计算机视觉算法不可见（来源：arxiv.org）。*

### 什么使得对抗性攻击不同？

每种软件都有其独特的安全漏洞，随着软件的新趋势，新威胁也会出现。例如，随着数据库后台的网络应用程序开始取代静态网站，SQL注入攻击变得普遍。浏览器端脚本语言的广泛应用催生了跨站脚本攻击。缓冲区溢出攻击通过利用编程语言如C处理内存分配的方式，覆盖关键变量并在目标计算机上执行恶意代码。反序列化攻击利用编程语言如Java和Python在应用程序和进程之间传输信息的缺陷。最近，我们还看到[原型污染攻击](https://portswigger.net/daily-swig/prototype-pollution-the-dangerous-and-underrated-vulnerability-impacting-javascript-applications)的激增，这种攻击利用JavaScript语言中的特性导致NodeJS服务器上的异常行为。

在这方面，对抗性攻击与其他网络威胁没有什么不同。随着机器学习成为许多应用的[重要组成部分](https://bdtechtalks.com/2019/12/30/computer-vision-applications-deep-learning/)，恶意行为者将寻找植入和触发 AI 模型中恶意行为的方法。

然而，使对抗性攻击与众不同的是其性质和可能的对策。对于大多数安全漏洞，界限非常清楚。一旦发现漏洞，安全分析师可以准确记录其发生的条件，并找到导致漏洞的源代码部分。响应也相对直接。例如，SQL 注入漏洞是因为没有对用户输入进行清理。缓冲区溢出错误发生在复制字符串数组时没有设置从源到目的地的字节数限制。

在大多数情况下，对抗性攻击利用了机器学习模型中学习到的参数的特殊性。攻击者通过精细地对输入进行修改来探测目标模型，直到它产生期望的行为。例如，通过逐渐改变图像的像素值，攻击者可以使[卷积神经网络](https://bdtechtalks.com/2020/01/06/convolutional-neural-networks-cnn-convnets/)将预测结果从“乌龟”改变为“步枪”。对抗性扰动通常是一层对人眼不可察觉的噪音。

（注：在某些情况下，例如[数据中毒](https://bdtechtalks.com/2020/10/07/machine-learning-data-poisoning/)，对抗性攻击通过机器学习管道中其他组件的漏洞成为可能，例如被篡改的训练数据集。）

![](../Images/de7ad72408df5d0445d289e0be91c91b.png)

*一个神经网络认为这是一张步枪的图片。人类视觉系统绝不会犯这样的错误（来源：[LabSix](http://www.labsix.org/physical-objects-that-fool-neural-nets/)）。*

机器学习的统计特性使得发现和修补对抗性攻击变得困难。一个在某些条件下有效的对抗性攻击可能在其他条件下失效，例如角度或光照条件的变化。此外，你无法指出导致漏洞的代码行，因为它分布在构成模型的数千或数百万个参数中。

对抗性攻击的防御也有些模糊。就像你无法精确定位 AI 模型中导致对抗性漏洞的位置一样，你也无法找到修复漏洞的精确补丁。对抗性防御通常涉及统计调整或对机器学习模型架构的总体修改。

比如，一种流行的方法是对抗性训练，研究人员通过对模型进行测试，生成对抗性样本，然后在这些样本及其正确标签上重新训练模型。对抗性训练会调整模型的所有参数，使其对训练过的样本类型具有鲁棒性。但通过足够的严谨性，攻击者可以找到其他噪声模式来创建对抗性样本。

事实是，我们仍在学习如何应对对抗性机器学习。安全研究人员习惯于审查代码中的漏洞。现在，他们必须学会在包含数百万个数值参数的机器学习中找到安全漏洞。

### 对对抗性机器学习的兴趣不断增长

近年来，对抗性攻击的论文数量激增。为了追踪这一趋势，我在arXiv预印本服务器上搜索了在摘要部分提到“对抗性攻击”或“对抗性样本”的论文。在[2014年](https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=%22adversarial+attack%22&terms-0-field=abstract&terms-1-operator=OR&terms-1-term=%22adversarial+example%22&terms-1-field=abstract&classification-physics_archives=all&classification-include_cross_list=include&date-filter_by=specific_year&date-year=2014&date-from_date=&date-to_date=&date-date_type=submitted_date&abstracts=show&size=50&order=-announced_date_first)，对抗性机器学习领域的论文数量为零。[在2020年](https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=%22adversarial+attack%22&terms-0-field=abstract&terms-1-operator=OR&terms-1-term=%22adversarial+example%22&terms-1-field=abstract&classification-physics_archives=all&classification-include_cross_list=include&date-filter_by=specific_year&date-year=2020&date-from_date=&date-to_date=&date-date_type=submitted_date&abstracts=show&size=50&order=-announced_date_first)，提交到arxiv的对抗性样本和攻击的论文数量约为1,100篇。

![](../Images/2ca23e3db2f231abbe93c59246d42cda.png)

*从2014年到2020年，arXiv.org上的对抗性机器学习论文从零篇增加到一年内的1,100篇。*

对抗性攻击和防御方法也已成为著名AI会议如NeurIPS和ICLR的重点。即使是像DEF CON、Black Hat和Usenix这样的网络安全会议，也开始举办关于对抗性攻击的研讨会和报告。

这些会议上展示的研究在检测对抗性漏洞和开发可以使机器学习模型更加稳健的防御方法方面取得了巨大进展。例如，研究人员发现了保护机器学习模型免受对抗性攻击的新方法，如使用[random switching mechanisms](https://bdtechtalks.com/2019/08/20/ai-adversarial-examples-hierarchical-random-switching/)和[神经科学的见解](https://bdtechtalks.com/2020/12/07/vonenet-neurscience-inspired-deep-learning/)。

然而，值得注意的是，AI 和安全会议聚焦于前沿研究。在 AI 会议上展示的工作与组织日常进行的实际工作之间存在较大的差距。

### 对对抗性攻击的反应乏善可陈

令人担忧的是，尽管对对抗性攻击的关注日益增加，警告声也越来越大，但对实际应用中对抗性漏洞的追踪活动却非常有限。

我查阅了几个跟踪漏洞、漏洞和漏洞悬赏的来源。例如，在 NIST 国家漏洞数据库中超过 145,000 条记录中，没有关于对抗性攻击或对抗性样本的条目。搜索“机器学习”返回了五个结果。大多数是系统中包含机器学习组件的跨站脚本（XSS）和 XML 外部实体（XXE）漏洞。其中一个涉及一个漏洞，允许攻击者创建一个机器学习模型的仿制版本并获取见解，这可能是对抗性攻击的窗口。但没有关于对抗性漏洞的直接报告。搜索“深度学习”显示了一个[严重漏洞](https://nvd.nist.gov/vuln/detail/CVE-2017-5719)，记录于 2017 年 11 月。但这不是对抗性漏洞，而是深度学习系统中另一个组件的缺陷。

![](../Images/362cef7d3d7e04b914489427224ac384.png)

*国家漏洞数据库中关于对抗性攻击的信息非常少。*

我还检查了 GitHub 的 Advisory 数据库，它跟踪托管在 GitHub 上的项目的安全性和漏洞修复。搜索“对抗性攻击”、“对抗性样本”、“机器学习”和“深度学习”没有结果。搜索“Tensoflow”显示了 41 条记录，但这些大多数是有关 TensorFlow 代码库的漏洞报告。没有关于对抗性攻击或 TensorFlow 模型参数中隐藏漏洞的内容。

这值得注意，因为 GitHub 上已经托管了许多[深度学习](https://bdtechtalks.com/2019/02/15/what-is-deep-learning-neural-networks/)模型和预训练的[神经网络](https://bdtechtalks.com/2019/08/05/what-is-artificial-neural-network-ann/)。

![](../Images/1de45823b690ce5f50dfda94c7905cbe.png)

*GitHub Advisory 中没有关于对抗性攻击的记录。*

最后，我查看了HackerOne，这是许多公司用来运行漏洞奖励计划的平台。在这里，也没有报告提到对抗性攻击。

虽然这可能不是一个非常准确的评估，但这些来源中没有任何关于对抗性攻击的信息这一事实非常具有指示性。

### 对抗性攻击的威胁不断增长

对抗性漏洞深嵌在机器学习模型的许多参数中，这使得用传统的安全工具检测它们变得困难。

![](../Images/f3e9cd8cc27c1f7233a8340b6dce235b.png)

*自动化防御是另一个值得讨论的领域。涉及基于代码的漏洞时，开发者可以使用一系列防御工具。*

静态分析工具可以帮助开发者发现他们代码中的漏洞。动态测试工具在运行时检查应用程序的脆弱行为模式。编译器已经使用了许多这些技术来跟踪和修补漏洞。今天，即使是你的浏览器也配备了工具来发现和阻止可能的恶意代码。

与此同时，组织已经学会将这些工具与适当的政策结合起来，以实施安全编码实践。许多公司已采用程序和做法，严格测试应用程序以发现已知和潜在的漏洞，然后再将其公开。例如，GitHub、Google和Apple利用这些工具和其他工具来审核在其平台上上传的数百万个应用程序和项目。

但是，用于防御机器学习系统对抗性攻击的工具和程序仍处于初步阶段。这也是我们看到关于对抗性攻击的报告和警告非常少的部分原因。

与此同时，另一个令人担忧的趋势是各级开发者对深度学习模型的日益使用。十年前，只有对机器学习和深度学习算法有全面了解的人才能在他们的应用中使用这些模型。你需要知道如何建立神经网络，通过直觉和实验调整超参数，并且还需要能够训练模型的计算资源。

但如今，将预训练的神经网络集成到应用程序中非常简单。

例如，PyTorch，这是领先的Python深度学习平台之一，[提供了一个工具](https://pytorch.org/docs/stable/hub.html)，使机器学习工程师能够在GitHub上发布预训练的神经网络，并使其对开发者可用。如果你想将图像分类深度学习模型集成到应用程序中，你只需具备基础的深度学习和PyTorch知识即可。

由于GitHub没有检测和阻止对抗性漏洞的程序，恶意行为者可能会利用这些工具发布具有隐藏后门的深度学习模型，并在数千名开发者将其集成到应用程序后加以利用。

### 如何应对对抗性攻击的威胁

![](../Images/4283a29402dfb17dd75300b4deb3981a.png)

可以理解，由于对抗性攻击的统计性质，使用与应对基于代码的漏洞相同的方法来解决这些问题是困难的。但幸运的是，已有一些积极的发展可以指导未来的步骤。

[对抗性机器学习威胁矩阵](https://bdtechtalks.com/2020/10/26/adversarial-machine-learning-threat-matrix/)，由微软、IBM、Nvidia、MITRE及其他安全和人工智能公司研究人员于上个月发布，为安全研究人员提供了一个框架，以便在包括机器学习组件的软件生态系统中找到薄弱环节和潜在的对抗性漏洞。对抗性机器学习威胁矩阵遵循ATT&CK框架，这是安全研究人员熟知和信赖的格式。

另一个有用的项目是IBM的对抗性鲁棒性工具箱，这是一个开源Python库，提供了评估机器学习模型对抗性漏洞的工具，并帮助开发者加固他们的人工智能系统。

这些和其他未来开发的对抗性防御工具需要得到适当政策的支持，以确保机器学习模型的安全。像GitHub和Google Play这样的软件平台必须建立程序，并将一些这些工具整合到包含机器学习模型的应用程序的审查过程中。对抗性漏洞的漏洞赏金也可以是确保数百万用户使用的机器学习系统强健的一个好措施。

可能还需要为机器学习系统的安全制定新规。正如处理敏感操作和信息的软件被期望符合一系列标准一样，用于生物识别认证和医学成像等关键应用的机器学习算法也必须经过对抗性攻击的鲁棒性审计。

随着机器学习的广泛应用，对抗性攻击的威胁变得越来越迫近。对抗性漏洞是一颗定时炸弹。只有系统性的应对才能解除它。

[原文](https://bdtechtalks.com/2020/12/16/machine-learning-adversarial-attacks-against-machine-learning-time-bomb/)。经许可转载。

**相关：**

+   [深度学习中的对抗性样本——入门指南](https://www.kdnuggets.com/2020/11/adversarial-examples-deep-learning-primer.html)

+   [对抗性机器学习和生成对抗网络简介](https://www.kdnuggets.com/2019/10/adversarial-machine-learning-generative-adversarial-networks.html)

+   [为什么机器学习容易受到对抗性攻击以及如何修复它](https://www.kdnuggets.com/2019/06/machine-learning-adversarial-attacks.html)

* * *

## 我们的三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业道路

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持组织的 IT 工作

* * *

### 更多相关内容

+   [GPT-4 易受提示注入攻击影响，导致错误信息](https://www.kdnuggets.com/2023/05/gpt4-vulnerable-prompt-injection-attacks-causing-misinformation.html)

+   [什么是对抗性机器学习？](https://www.kdnuggets.com/2022/03/adversarial-machine-learning.html)

+   [实时 AI 和机器学习的特征存储](https://www.kdnuggets.com/2022/03/feature-stores-realtime-ai-machine-learning.html)

+   [通过集成 Jupyter 和 KNIME 减少实施时间](https://www.kdnuggets.com/2021/12/cutting-implementation-time-integrating-jupyter-knime.html)

+   [市场数据和新闻：时间序列分析](https://www.kdnuggets.com/2022/06/market-data-news-time-series-analysis.html)

+   [使用 AI 和分析引擎更快准备时间序列数据](https://www.kdnuggets.com/2021/12/piexchange-faster-way-prepare-timeseries-data-ai-analytics-engine.html)
