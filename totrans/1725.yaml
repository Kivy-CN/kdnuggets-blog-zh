- en: XGBoost, a Top Machine Learning Method on Kaggle, Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/10/xgboost-top-machine-learning-method-kaggle-explained.html](https://www.kdnuggets.com/2017/10/xgboost-top-machine-learning-method-kaggle-explained.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![XGBoost](../Images/cd6c6f40e9b8754536de6a0b3bd3a202.png)'
  prefs: []
  type: TYPE_IMG
- en: '*What is XGBoost?*'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost has become a widely used and really popular tool among Kaggle competitors
    and Data Scientists in industry, as it has been battle tested for production on
    large-scale problems. It is a highly flexible and versatile tool that can work
    through most regression, classification and ranking problems as well as user-built
    objective functions. As an open-source software, it is easily accessible and it
    may be used through different platforms and interfaces. The amazing portability
    and compatibility of the system permits its usage on all three Windows, Linux
    and OS X. It also supports training on distributed cloud platforms like AWS, Azure,
    GCE among others and it is easily connected to large-scale cloud dataflow systems
    such as Flink and Spark. Although it was built and initially used in the Command
    Line Interface (CLI) by its creator (Tianqi Chen), it can also be loaded and used
    in various languages and interfaces such as Python, C++, R, Julia, Scala and Java.
  prefs: []
  type: TYPE_NORMAL
- en: Its name stands for **eXtreme Gradient Boosting**, it was developed by Tianqi
    Chen and now is part of a wider collection of open-source libraries developed
    by the Distributed Machine Learning Community (DMLC). XGBoost is a scalable and
    accurate implementation of gradient boosting machines and it has proven to push
    the limits of computing power for boosted trees algorithms as it was built and
    developed for the sole purpose of model performance and computational speed. Specifically,
    it was engineered to exploit every bit of memory and hardware resources for tree
    boosting algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of XGBoost offers several advanced features for model tuning,
    computing environments and algorithm enhancement. It is capable of performing
    the three main forms of gradient boosting (Gradient Boosting (GB), Stochastic
    GB and Regularized GB) and it is robust enough to support fine tuning and addition
    of regularization parameters. According to Tianqi Chen, the latter is what makes
    it superior and different to other libraries.
  prefs: []
  type: TYPE_NORMAL
- en: “…xgboost used a more regularized model formalization to control over-fitting,
    which gives it better performance.”- Tianqi Chen on [Quora](https://www.quora.com/What-is-the-difference-between-the-R-gbm-gradient-boosting-machine-and-xgboost-extreme-gradient-boosting)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: System-wise, the library’s portability and flexibility allow the use of a wide
    variety of computing environments like parallelization for tree construction across
    several CPU cores; distributed computing for large models; Out-of-Core computing;
    and Cache Optimization to improve hardware usage and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm was developed to efficiently reduce computing time and allocate
    an optimal usage of memory resources. Important features of implementation include
    handling of missing values (Sparse Aware), Block Structure to support parallelization
    in tree construction and the ability to fit and boost on new data added to a trained
    model (Continued Training).
  prefs: []
  type: TYPE_NORMAL
- en: '*Why use XGBoost?*'
  prefs: []
  type: TYPE_NORMAL
- en: As we already mentioned, the key features of this library rely on *model performance
    and execution speed*. A well-structured clear benchmark done by [Szilard Pafka](http://datascience.la/benchmarking-random-forest-implementations/),
    shows how XGBoost outperforms several other well-known implementations of gradient
    tree boosting.
  prefs: []
  type: TYPE_NORMAL
- en: '![xgboost benchmarks](../Images/b7b2c95f91556bd75215178b20aacc52.png)'
  prefs: []
  type: TYPE_IMG
- en: This comparison in Figure 1 helps us grasp the power of the tool and see how
    well balanced its benefits are, i.e., it does not seem to sacrifice speed over
    accuracy or vice versa. It starts to become clear why more Kagglers are using
    it every day, it is a semi-perfect equilibrium of both performance and time-efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '*How does it work?*'
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on to the details of the algorithm, let’s set some basic definitions
    to make our life easier and get an intuitive and complete understanding of this
    popular tool.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s clarify the concept of boosting. This is an ensemble method that
    seeks to create a strong classifier (model) based on “weak” classifiers. In this
    context, weak and strong refer to a measure of how correlated are the learners
    to the actual target variable. By adding models on top of each other iteratively,
    the errors of the previous model are corrected by the next predictor, until the
    training data is accurately predicted or reproduced by the model. If you want
    to dig into boosting a bit more, check out information about a popular implementation
    called AdaBoost (Adaptive Boosting) [here](https://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/).
  prefs: []
  type: TYPE_NORMAL
- en: Now, gradient boosting also comprises an ensemble method that sequentially adds
    predictors and corrects previous models. However, instead of assigning different
    weights to the classifiers after every iteration, this method fits the new model
    to new residuals of the previous prediction and then minimizes the loss when adding
    the latest prediction. So, in the end, you are updating your model using gradient
    descent and hence the name, gradient boosting. This is supported for both regression
    and classification problems. XGBoost specifically, implements this algorithm for
    decision tree boosting with an additional custom regularization term in the objective
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '*Getting started with XGBoost*'
  prefs: []
  type: TYPE_NORMAL
- en: You may download and install XGBoost regardless of which interface you are using.
    To learn more on how to use on each specific platform please follow the instructions
    on this link. You will also find official documentation and tutorials [here](https://xgboost.readthedocs.io/en/latest/get_started/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: For further information on the source code and examples, you may visit this
    [DMLC repository on Github](https://github.com/dmlc/xgboost).
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on boosting and gradient boosting the following resources
    might be helpful:'
  prefs: []
  type: TYPE_NORMAL
- en: The official published paper by Tianqi Chen is available for download from [Arxiv](https://arxiv.org/abs/1603.02754)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The official documentation [XGBoost page](http://xgboost.readthedocs.io/en/latest/model.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Here](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf) is
    a great presentation that summarizes the math in a very intuitive way'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some [Wikipedia Articles](https://en.wikipedia.org/wiki/Gradient_boosting)
    give a good general idea of the history and the math behind the algorithms:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thanks to Jason Brownlee for the inspiration of this post, more resources on
    Boosting and XGBoost are available on [his post](https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/).
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Lessons Learned From Benchmarking Fast Machine Learning Algorithms](/2017/08/lessons-benchmarking-fast-machine-learning-algorithms.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Simple XGBoost Tutorial Using the Iris Dataset](/2017/03/simple-xgboost-tutorial-iris-dataset.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[XGBoost: Implementing the Winningest Kaggle Algorithm in Spark and Flink](/2016/03/xgboost-implementing-winningest-kaggle-algorithm-spark-flink.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
