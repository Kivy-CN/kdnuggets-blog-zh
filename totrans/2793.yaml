- en: Making sense of ensemble learning techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/03/making-sense-ensemble-learning-techniques.html](https://www.kdnuggets.com/2020/03/making-sense-ensemble-learning-techniques.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Ido Zehori](https://www.linkedin.com/in/ido-zehori/), Data Science Team
    Leader at [Bigabid](https://www.bigabid.com/)**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/733e44c46833a8497ef2f1341a1d187c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [Packt](https://hub.packtpub.com/what-is-ensemble-learning/)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: For many companies/data scientists that specialize or work with machine learning
    (ML), ensemble learning methods have become the weapons of choice. As ensemble
    learning methods combine multiple base models, together they have a greater ability
    to produce a much more accurate ML model. For example, at Bigabid we’ve been ensemble
    learning to successfully solve a variety of problems ranging from optimizing LTV
    ([Customer Lifetime Value](https://www.qualtrics.com/experience-management/customer/customer-lifetime-value/))
    to [fraud detection](https://searchsecurity.techtarget.com/definition/fraud-detection).
  prefs: []
  type: TYPE_NORMAL
- en: 'It is hard not overstate the importance of ensemble learning to the overall
    ML process, including the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)
    and the three main ensemble techniques: bagging, boosting and stacking. These
    powerful techniques should be a part of any data scientist’s tool kit, as they
    are concepts that are encountered everywhere. Plus, understanding their underlying
    mechanism is at the heart of the field of machine learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensemble Learning Methods: An Overview'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ensemble learning is an ML paradigm where numerous base models (which are often
    referred to as “weak learners”) are combined and trained to solve the same problem.
    This method is based on the theory that by correctly combining several base models
    together, these weak learners can be used as building blocks for designing more
    complex ML models. Together these ensemble models (aptly called “strong learners”)
    achieve better, more accurate results.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, a weak learner is merely a base model that alone performs rather
    poorly. In fact, its accuracy level is barely above chance, meaning that it predicts
    the outcome only slightly better than a random guess would. These weak learners
    will often be computationally simple as well. Typically, the reason base models
    don’t perform very well by themselves is because they either have a high bias
    or too much variance, which makes them weak.
  prefs: []
  type: TYPE_NORMAL
- en: This is where ensemble learning comes in. This method attempts to try to reduce
    the general error by combining several weak learners together. Think of ensemble
    models as the data science version of the expression “two heads are better than
    one.” If one model works well, then a number of models working together can do
    even better.
  prefs: []
  type: TYPE_NORMAL
- en: A Word about the Bias-Variance Tradeoff
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It’s important to understand the concept of a weak learner and why it earned
    this name in the first place, as the reason comes down to either bias or variance.
    More specifically, the prediction error of an ML model, namely the difference
    between the trained model and the ground truth, can be broken down into the sum
    of the following: the bias and the variance. For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Error due to Bias:** This is the difference between a model’s expected prediction
    and the precise value that we are aiming to predict.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error due to Variance:** This is the variance of a model prediction for a
    specified data point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a model is too simplistic and doesn’t have many parameters, then it may have
    high bias and low variance. In contrast, if a model has many parameters, then
    it may have high variance and low bias. As such, it’s necessary to find the right
    balance without underfitting or overfitting the data, as this tradeoff in complexity
    is the reason why there exists a tradeoff between variance and bias. Simply put,
    an algorithm can’t simultaneously be more complex and less complex at the same
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble Learning Techniques - Combining Weak Learners
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are three main ensemble techniques: bagging, boosting and stacking. There
    are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging:**'
  prefs: []
  type: TYPE_NORMAL
- en: Bagging attempts to incorporate similar learners on small-sample populations
    and calculates the average of all the predictions. Generally, bagging allows you
    to use different learners in different populations. By doing so, this method helps
    to reduce the variance error.
  prefs: []
  type: TYPE_NORMAL
- en: '**Boosting**'
  prefs: []
  type: TYPE_NORMAL
- en: Boosting is an iterative method that fine-tunes the weight of an observation
    according to the most recent classification. If an observation was incorrectly
    classified, this method will increase the weight of that observation in the next
    round (in which the next model will be trained) and will be less prone to misclassification.
    Similarly, if an observation was classified correctly, then it will reduce its
    weight for the next classifier. The weight represents how important the correct
    classification of the specific data point should be, as this enables the sequential
    classifiers to focus on examples previously misclassified. Generally, boosting
    reduces the bias error and forms strong predictive models, but at times they may
    overfit on the training data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stacking**'
  prefs: []
  type: TYPE_NORMAL
- en: Stacking is a clever way of combining the information provided by different
    models. With this method, a learner of any sort can be used to combine different
    learners’ outputs. The result can be a decrease in bias or variance determined
    by which combined models are used.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Promise of Ensemble Learning**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ensemble learning is about combining multiple base models to achieve a more
    effective and accurate ensemble model that features more powerful properties and
    thus, performs better. Ensemble learning methods have successfully set record
    performances on challenging datasets and are constantly a part of the winning
    submissions of [Kaggle competitions.](https://www.kaggle.com/competitions)
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that even with the three main ensemble techniques – bagging,
    boosting and stacking – variants are still possible and can be better designed
    to more effectively adapt to specific problems, such as classifications, regression,
    time-series analyses, etc. This first requires an understanding of the problem
    at hand and to be creative in approaching problem solving!
  prefs: []
  type: TYPE_NORMAL
- en: Using ensemble learning methods is a great, promising way to start approaching
    any problem.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Ido Zehori](https://www.linkedin.com/in/ido-zehori/)** is the Data
    Science Team Leader at [Bigabid](https://www.bigabid.com/), a data science company
    that has developed a second-generation DSP optimized for in-app advertising user
    acquisition & re-engagement, providing both the scale of a tier-1 DSP and the
    precision of a cutting edge DMP.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Ensemble Methods for Machine Learning: AdaBoost](/2019/09/ensemble-methods-machine-learning-adaboost.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Random Forest® — A Powerful Ensemble Learning Algorithm](/2020/01/random-forest-powerful-ensemble-learning-algorithm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Explaining Black Box Models: Ensemble and Deep Learning Using LIME and SHAP](/2020/01/explaining-black-box-models-ensemble-deep-learning-lime-shap.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[When Would Ensemble Techniques be a Good Choice?](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ensemble Learning with Examples](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Significance of Data Quality in Making a Successful Machine…](https://www.kdnuggets.com/2022/03/significance-data-quality-making-successful-machine-learning-model.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ploomber vs Kubeflow: Making MLOps Easier](https://www.kdnuggets.com/2022/02/ploomber-kubeflow-mlops-easier.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Making Intelligent Document Processing Smarter: Part 1](https://www.kdnuggets.com/2023/02/making-intelligent-document-processing-smarter-part-1.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
