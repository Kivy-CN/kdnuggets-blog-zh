- en: 'A Deep Dive into GPT Models: Evolution & Performance Comparison'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨GPT模型：演变与性能比较
- en: 原文：[https://www.kdnuggets.com/2023/05/deep-dive-gpt-models.html](https://www.kdnuggets.com/2023/05/deep-dive-gpt-models.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/05/deep-dive-gpt-models.html](https://www.kdnuggets.com/2023/05/deep-dive-gpt-models.html)
- en: By Ankit, Bhaskar & Malhar
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 作者：Ankit、Bhaskar & Malhar
- en: Over the past few years, there has been remarkable progress in the field of
    Natural Language Processing, thanks to the emergence of large language models.
    Language models are used in machine translation systems to learn how to map strings
    from one language to another. Among the family of language models, the Generative
    Pre-trained Transformer (GPT) based Model has garnered the most attention in recent
    times. Initially, the language models were rule-based systems that heavily relied
    on human input to function. However, the evolution of deep learning techniques
    has positively impacted the complexity, scale and accuracy of the tasks handled
    by these models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，得益于大型语言模型的出现，自然语言处理领域取得了显著进展。语言模型用于机器翻译系统中，以学习如何将字符串从一种语言映射到另一种语言。在语言模型家族中，基于生成预训练变换器（GPT）的模型最近受到了最多的关注。最初，语言模型是基于规则的系统，严重依赖人工输入来运行。然而，深度学习技术的发展对这些模型处理任务的复杂性、规模和准确性产生了积极影响。
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的IT工作'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In our [previous blog](https://www.sigmoid.com/blogs/gpt-3-all-you-need-to-know-about-the-ai-language-model/),
    we provided a comprehensive explanation of the various aspects of the GPT3 model,
    evaluated features offered by Open AI’s GPT-3 API and also explored the model’s
    usage and limitations. In this blog, we will shift our focus to the GPT Model
    and its foundational components. We will also look at evolution - starting from
    GPT-1 to the recently introduced GPT-4 and dive into the key improvements done
    in each generation that made the models potent over time.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的[上一篇博客](https://www.sigmoid.com/blogs/gpt-3-all-you-need-to-know-about-the-ai-language-model/)中，我们提供了对GPT-3模型各个方面的全面解释，评估了Open
    AI的GPT-3 API提供的功能，并探讨了模型的使用和局限性。在这篇博客中，我们将把重点转向GPT模型及其基础组件。我们还将回顾从GPT-1到最近推出的GPT-4的演变，并深入探讨每一代中所做的关键改进，这些改进使模型随着时间的推移变得更为强大。
- en: 1\. Understanding GPT Models
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. 了解GPT模型
- en: GPT (Generative Pre-trained Transformers) is a deep learning-based Large Language
    Model (LLM), utilizing a decoder-only architecture built on transformers. Its
    purpose is to process text data and generate text output that resembles human
    language.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: GPT（生成预训练变换器）是一个基于深度学习的大型语言模型（LLM），利用基于变换器的解码器架构。它的目的是处理文本数据并生成类似人类语言的文本输出。
- en: 'As the name suggests, there are three pillars of the model namely:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如名称所示，该模型有三个支柱，即：
- en: Generative
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成性
- en: Pre-trained
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预训练
- en: Transformers
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变换器
- en: 'Let''s explore the model through these components:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过这些组件来探索模型：
- en: '**Generative:** This feature emphasizes the model''s ability to generate text
    by comprehending and responding to a given text sample. Prior to GPT models, text
    output was generated by rearranging or extracting words from the input itself.
    The generative capability of GPT models gave them an edge over existing models,
    enabling the production of more coherent and human-like text.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成性：** 这一特性强调了模型通过理解和响应给定文本样本生成文本的能力。在GPT模型之前，文本输出是通过重新排列或提取输入中的单词来生成的。GPT模型的生成能力使其相较于现有模型具有优势，能够产生更连贯且更像人类的文本。'
- en: This generative capability is derived from the modeling objective used during
    training.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这种生成能力源自于训练过程中使用的建模目标。
- en: GPT models are trained using autoregressive language modeling, where the models
    are fed with an input sequence of words, and the model tries to find the most
    suitable next word by employing probability distributions to predict the most
    probable word or phrase.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 模型使用自回归语言建模进行训练，模型接收一个词序列作为输入，然后通过概率分布预测最可能的下一个词或短语。
- en: '**Pre-Trained**: "Pre-trained" refers to an ML model that has undergone training
    on a large dataset of examples before being deployed for a specific task. In the
    case of GPT, the model is trained on an extensive corpus of text data using an
    unsupervised learning approach. This allows the model to learn patterns and relationships
    within the data without explicit guidance.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**预训练：** “预训练”指的是一个机器学习模型在被部署到特定任务之前，已经在大量示例数据集上进行过训练。在 GPT 的情况下，该模型在大量文本数据语料库上进行训练，使用无监督学习方法。这使得模型能够在没有明确指导的情况下学习数据中的模式和关系。'
- en: In simpler terms, training the model with vast amounts of data in an unsupervised
    manner helps it understand the general features and structure of a language. Once
    learned, the model can leverage this understanding for specific tasks such as
    question answering and summarization.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，使用大量数据以无监督的方式训练模型有助于模型理解语言的普遍特征和结构。一旦学习完成，模型可以利用这些理解来执行特定任务，例如问答和摘要。
- en: '**Transformers:** A type of neural network architecture that is designed to
    handle text sequences of varying lengths. The concept of transformers gained prominence
    after the groundbreaking paper titled "Attention Is All You Need" was published
    in 2017.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**变换器：** 一种神经网络架构，旨在处理不同长度的文本序列。变换器的概念在2017年发表的开创性论文《Attention Is All You Need》之后获得了广泛关注。'
- en: GPT uses decoder-only architecture.The primary component of a transformer is
    its "self-attention mechanism," which enables the model to capture the relationship
    between each word and other words within the same sentence.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 使用仅解码器架构。变换器的主要组件是其“自注意力机制”，它使模型能够捕捉同一句子中每个词与其他词之间的关系。
- en: 'Example:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: A dog is sitting on the bank of the River Ganga.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一只狗坐在恒河的河岸上。
- en: I’ll withdraw some money from the bank.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我将从银行取一些钱。
- en: Self-attention evaluates each word in relation to other words in the sentence.
    In the first example when “bank” is evaluated in the context of “River”, the model
    learns that it refers to a river bank. Similarly, in the second example, evaluating
    "bank" with respect to the word "money" suggests a financial bank.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制在句子中评估每个词与其他词的关系。在第一个例子中，当“bank”在“River”的上下文中被评估时，模型学习到它指的是河岸。同样，在第二个例子中，将“bank”与“money”一词相关联则暗示这是一个金融银行。
- en: 2\. Evolution of GPT Models
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. GPT 模型的发展
- en: Now, let's take a closer look at the various versions of GPT Models, with a
    focus on the enhancements and additions introduced in each subsequent model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更详细地了解 GPT 模型的各种版本，重点关注每个后续模型中引入的改进和新增功能。
- en: '![A Deep Dive into GPT Models](../Images/43b75d49c63cb8a4737cd4563ffe403c.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![深入了解 GPT 模型](../Images/43b75d49c63cb8a4737cd4563ffe403c.png)'
- en: '***Slide 3 in** [GPT Models](https://docs.google.com/presentation/d/1xxyE18P-G6eZ6l34iP1-Xu2Junx6Sm4KDoaYhCFGVGk/edit#slide=id.g21eb2c7aeab_0_0)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '***第 3 幻灯片** [GPT 模型](https://docs.google.com/presentation/d/1xxyE18P-G6eZ6l34iP1-Xu2Junx6Sm4KDoaYhCFGVGk/edit#slide=id.g21eb2c7aeab_0_0)'
- en: GPT-1
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-1
- en: It is the first model of the GPT series and was trained on around 40GB of text
    data. The model achieved state-of-the-art results for modeling tasks like LAMBADA
    and demonstrated competitive performance for tasks like GLUE and SQuAD. With a
    maximum context length of 512 tokens (around 380 words), the model could retain
    information for relatively short sentences or documents per request. The model's
    impressive text generation capabilities and strong performance on standard tasks
    provided the impetus for the development of the subsequent model in the series.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 GPT 系列的第一个模型，训练时使用了约 40GB 的文本数据。该模型在建模任务如 LAMBADA 中取得了最先进的结果，并在 GLUE 和 SQuAD
    等任务中表现出竞争力。该模型的最大上下文长度为 512 个标记（约 380 个词），因此能够保留相对较短句子或文档的信息。模型出色的文本生成能力和在标准任务中的强大表现为后续模型的发展提供了动力。
- en: GPT-2
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-2
- en: Derived from the GPT-1 Model, the GPT-2 Model retains the same architectural
    features. However, it undergoes training on an even larger corpus of text data
    compared to GPT-1\. Notably, GPT-2 can accommodate double the input size, enabling
    it to process more extensive text samples. With nearly 1.5 billion parameters,
    GPT-2 exhibits a significant boost in capacity and potential for language modeling.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从 GPT-1 模型派生的 GPT-2 模型保留了相同的架构特征。然而，它在比 GPT-1 更大的文本数据语料库上进行训练。值得注意的是，GPT-2 可以处理双倍的输入大小，使其能够处理更广泛的文本样本。GPT-2
    拥有近 15 亿个参数，展现了在语言建模方面显著的能力和潜力提升。
- en: 'Here are some major improvements in GPT-2 over GPT 1:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 GPT-2 相对于 GPT-1 的一些主要改进：
- en: '**Modified Objective Training** is a technique utilized during the pre-training
    phase to enhance language models. Traditionally, models predict the next word
    in the sequence solely based on previous words, leading to potentially incoherent
    or irrelevant predictions. MO training addresses this limitation by incorporating
    additional context, such as Parts of Speech (Noun, Verb, etc.) and Subject-Object
    Identification. By leveraging this supplementary information, the model generates
    outputs that are more coherent and informative.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**修改目标训练** 是一种在预训练阶段用于增强语言模型的技术。传统上，模型仅基于前面的单词预测序列中的下一个单词，这可能导致不连贯或不相关的预测。MO
    训练通过结合额外的上下文信息（如词性（名词、动词等）和主谓宾识别）来解决这一限制。通过利用这些补充信息，模型生成的输出更加连贯和信息丰富。'
- en: '**Layer normalization** is another technique employed to improve training and
    performance. It involves normalizing the activations of each layer within the
    neural network, rather than normalizing the network''s inputs or outputs as a
    whole. This normalization mitigates the issue of Internal Covariate Shift, which
    refers to the change in the distribution of network activations caused by alterations
    in network parameters.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**层归一化** 是另一种用于提高训练和性能的技术。它涉及对神经网络中每一层的激活进行归一化，而不是对整个网络的输入或输出进行归一化。这种归一化减少了内部协变量偏移的问题，即网络激活的分布因网络参数的变化而发生的变化。'
- en: 'GPT 2 is also powered by superior **sampling** algorithms as compared to GPT
    1\. Key improvements include:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPT-2 还配备了比 GPT-1 更优的 **采样** 算法。主要改进包括：
- en: '**Top - p sampling**: Only tokens with cumulative probability mass exceeding
    a certain threshold are considered during sampling. This avoids sampling from
    low-probability tokens, resulting in more diverse and coherent text generation.'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Top-p 采样**：在采样过程中仅考虑累积概率质量超过某个阈值的令牌。这避免了从低概率令牌中采样，从而生成更具多样性和连贯性的文本。'
- en: '**Temperature scaling** of the logits (i.e., the raw output of the neural network
    before Softmax), controls the level of randomness in the generated text. Lower
    temperatures yield more conservative and predictable text, while higher temperatures
    produce more creative and unexpected text.'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**温度缩放**：控制 logits（即神经网络在 Softmax 之前的原始输出）中的随机性水平。较低的温度生成的文本更保守和可预测，而较高的温度则产生更具创意和意外的文本。'
- en: '**Unconditional sampling** (random sampling) option, which allows users to
    explore the model''s generative capabilities and can produce ingenious results.'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**无条件采样**（随机采样）选项，允许用户探索模型的生成能力，并能产生巧妙的结果。'
- en: GPT-3
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-3
- en: '| **Training Data source** | **Training Data Size** |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| **训练数据来源** | **训练数据规模** |'
- en: '| Common Crawl, BookCorpus, Wikipedia, Books, Articles, and more | over 570
    GB of text data |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| Common Crawl、BookCorpus、Wikipedia、Books、Articles 等 | 超过 570 GB 的文本数据 |'
- en: The GPT-3 Model is an evolution of the GPT-2 Model, surpassing it in several
    aspects. It was trained on a significantly larger corpus of text data and featured
    a maximum of 175 billion parameters.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 模型是对 GPT-2 模型的进化，超越了它的多个方面。它在一个显著更大的文本数据语料库上进行训练，并具有最多 1750 亿个参数。
- en: 'Along with its increased size, GPT-3 introduced several noteworthy improvements:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 随着其规模的增加，GPT-3 引入了几个显著的改进：
- en: '**GShard (**Giant-Sharded model parallelism): allows the model to be split
    across multiple accelerators. This facilitates parallel training and inference,
    particularly for large language models with billions of parameters.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GShard（** 巨型分片模型并行性）：允许模型在多个加速器之间进行分割。这有助于并行训练和推理，尤其是对于拥有数十亿参数的大型语言模型。'
- en: '**Zero-shot learning** capabilities facilitates GPT-3 to exhibit the ability
    to perform tasks for which it hadn''t been explicitly trained. This means it could
    generate text in response to novel prompts by leveraging its general understanding
    of language and the given task.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**零样本学习** 功能使 GPT-3 展现出执行未明确训练的任务的能力。这意味着它可以通过利用对语言和给定任务的一般理解，响应新的提示生成文本。'
- en: '**Few-shot learning** capabilities powers GPT-3 to quickly adapt to new tasks
    and domains with minimal training. It demonstrates an impressive ability to learn
    from a small number of examples.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**少样本学习** 功能使 GPT-3 能够快速适应新任务和领域，仅需少量训练。它展现了从少量示例中学习的惊人能力。'
- en: '**Multilingual support:** GPT-3 is proficient in generating text in ~30 languages,
    including English, Chinese, French, German, and Arabic. This broad multilingual
    support makes it a highly versatile language model for diverse applications.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多语言支持：** GPT-3 精通生成约 30 种语言的文本，包括英语、中文、法语、德语和阿拉伯语。这种广泛的多语言支持使其成为多种应用的高度通用语言模型。'
- en: '**Improved sampling:** GPT-3 uses an improved sampling algorithm that includes
    the ability to adjust the randomness in generated text, similar to GPT-2\. Additionally,
    it introduces the option of "prompted" sampling, enabling text generation based
    on user-specified prompts or context.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进的采样：** GPT-3 使用了一种改进的采样算法，包括调整生成文本随机性的能力，类似于 GPT-2。此外，它引入了“提示”采样选项，允许根据用户指定的提示或上下文生成文本。'
- en: GPT-3.5
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-3.5
- en: '| **Training Data source** | **Training Data Size** |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| **训练数据来源** | **训练数据大小** |'
- en: '| Common Crawl, BookCorpus, Wikipedia, Books, Articles, and more | > 570 GB
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Common Crawl, BookCorpus, Wikipedia, Books, Articles, and more | > 570 GB
    |'
- en: Similar to its predecessors, the GPT-3.5 series models were derived from the
    GPT-3 models.  However, the distinguishing feature of GPT-3.5 models lies in their
    adherence to specific policies based on human values, incorporated using a technique
    called Reinforcement Learning with Human Feedback (RLHF). The primary objective
    was to align the models more closely with the user's intentions, mitigate toxicity,
    and prioritize truthfulness in their generated output. This evolution signifies
    a conscious effort to enhance the ethical and responsible usage of language models
    in order to provide a safer and more reliable user experience.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与其前身类似，GPT-3.5 系列模型源自 GPT-3 模型。然而，GPT-3.5 模型的显著特点在于其依据人类价值观的具体政策，通过一种称为强化学习与人类反馈（RLHF）的技术进行整合。主要目标是使模型更贴近用户意图，减少毒性，并优先考虑生成内容的真实性。这一进化标志着在语言模型的伦理和负责任使用方面的自觉努力，以提供更安全、更可靠的用户体验。
- en: '**Improvements over GPT-3:**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**相较于 GPT-3 的改进：**'
- en: OpenAI used Reinforcement Learning from human feedback to fine-tune GPT-3 and
    enable it to follow a broad set of instructions. The RLHF technique entails training
    the model using reinforcement learning principles, wherein the model receives
    rewards or penalties based on the quality and alignment of its generated outputs
    with human evaluators. By integrating this feedback into the training process,
    the model gains the ability to learn from errors and enhance its performance,
    ultimately producing text outputs that are more natural and captivating.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 使用来自人类反馈的强化学习来微调 GPT-3，使其能够遵循广泛的指令集。RLHF 技术涉及使用强化学习原则来训练模型，其中模型根据其生成输出的质量和与人类评估者的一致性获得奖励或惩罚。通过将这种反馈整合到训练过程中，模型能够从错误中学习并提升性能，最终生成更加自然和吸引人的文本输出。
- en: GPT 4
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT 4
- en: GPT-4 represents the latest model in the GPT series introducing multimodal capabilities
    that allow it to process both text and image inputs while generating text outputs.
    It accommodates various image formats, including documents with text, photographs,
    diagrams, graphs, schematics, and screenshots.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 代表了 GPT 系列的最新模型，引入了多模态能力，允许其处理文本和图像输入，同时生成文本输出。它支持各种图像格式，包括含文本的文档、照片、图表、图示、图形和截图。
- en: While OpenAI has not disclosed technical details such as model size, architecture,
    training methodology, or model weights for GPT-4, some estimates suggest that
    it comprises nearly 1 trillion parameters. The base model of GPT-4 follows a training
    objective similar to previous GPT models, aiming to predict the next word given
    a sequence of words. The training process involved using a huge corpus of publicly
    available internet data and licensed data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管OpenAI没有披露GPT-4的技术细节，如模型大小、架构、训练方法或模型权重，但一些估计表明，它包含了近1万亿个参数。GPT-4的基础模型遵循类似于之前GPT模型的训练目标，旨在预测给定一系列词语后的下一个词。训练过程涉及使用大量公开可用的互联网数据和授权数据。
- en: GPT-4 has showcased superior performance compared to GPT-3.5 in OpenAI's internal
    adversarial factuality evaluations and public benchmarks like TruthfulQA. The
    RLHF techniques utilized in GPT-3.5 were also incorporated into GPT-4\. OpenAI
    actively seeks to enhance GPT-4 based on feedback received from ChatGPT and other
    sources.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4在OpenAI内部的对抗性事实评估和像TruthfulQA这样的公开基准测试中，表现出比GPT-3.5更出色的性能。GPT-3.5中使用的RLHF技术也被纳入了GPT-4中。OpenAI积极根据ChatGPT和其他来源的反馈来提升GPT-4。
- en: Performance Comparison of GPT Models for Standard Modeling Tasks
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT模型在标准建模任务中的性能比较
- en: Scores of GPT-1,GPT-2 and GPT-3 in standard NLP Modeling tasks LAMBDA, GLUE
    and SQuAD.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-1、GPT-2和GPT-3在标准NLP建模任务LAMBDA、GLUE和SQuAD中的分数。
- en: '| Model | GLUE | LAMBADA | SQuAD F1 | SQuAD Exact Match |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | GLUE | LAMBADA | SQuAD F1 | SQuAD准确匹配 |'
- en: '| GPT-1 | 68.4 | 48.4 | 82.0 | 74.6 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| GPT-1 | 68.4 | 48.4 | 82.0 | 74.6 |'
- en: '| GPT-2 | 84.6 | 60.1 | 89.5 | 83.0 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 | 84.6 | 60.1 | 89.5 | 83.0 |'
- en: '| GPT-3 | 93.2 | 69.6 | 92.4 | 88.8 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3 | 93.2 | 69.6 | 92.4 | 88.8 |'
- en: '| GPT-3.5 | 93.5 | 79.3 | 92.4 | 88.8 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | 93.5 | 79.3 | 92.4 | 88.8 |'
- en: '| GPT-4 | 94.2 | 82.4 | 93.6 | 90.4 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 94.2 | 82.4 | 93.6 | 90.4 |'
- en: All numbers are in Percentages. || source - BARD
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数字均以百分比表示。 || 来源 - BARD
- en: This table demonstrates the consistent improvement in results, which can be
    attributed to the aforementioned enhancements.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表格展示了结果的一致改进，这可以归因于前述的增强。
- en: GPT-3.5 and GPT-4 are tested on the newer benchmarks tests and standard examinations.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3.5和GPT-4在较新的基准测试和标准考试中进行了测试。
- en: The newer GPT models, (3.5 and 4) are tested on the tasks that require reasoning
    and domain knowledge. The models have been tested on numerous examinations which
    are known to be challenging. One such examination for which GPT-3 (ada, babbage,
    curie, davinci), GPT-3.5 , ChatGPT and GPT-4 are compared is the MBE Exam. From
    the graph we can see continuous improvement in score, with GPT-4 even beating
    the average student score.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 较新的GPT模型（3.5和4）在需要推理和领域知识的任务上进行了测试。这些模型已在众多被认为具有挑战性的考试中进行测试。一个这样的考试是MBE考试，GPT-3（ada、babbage、curie、davinci）、GPT-3.5、ChatGPT和GPT-4都进行了比较。从图表中可以看到分数的持续改进，GPT-4甚至超过了平均学生分数。
- en: 'Figure 1 illustrates the comparison of percentage of marks obtained in MBE*
    by different GPT models:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图1展示了不同GPT模型在MBE*中获得的分数百分比的比较：
- en: '![A Deep Dive into GPT Models](../Images/ba177b1c6a8a0666ac0ead25b7ea812a.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![深入了解GPT模型](../Images/ba177b1c6a8a0666ac0ead25b7ea812a.png)'
- en: '*The Multistate Bar Exam (MBE) is a challenging battery of tests designed to
    evaluate an applicant’s legal knowledge and skills, and is a precondition to practice
    law in the US.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*多州律师考试（MBE）是一项旨在评估申请人法律知识和技能的挑战性测试，是在美国从事法律工作的前提条件。'
- en: The below graphs also highlights the progress of the models and again beating
    the average students scores for different streams of legal subject areas.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表还突出了模型的进展，并且再次超过了不同法律学科领域的平均学生分数。
- en: '![A Deep Dive into GPT Models](../Images/a6aecc3fa0218f7fb877e17f9b277bf6.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![深入了解GPT模型](../Images/a6aecc3fa0218f7fb877e17f9b277bf6.png)'
- en: 'Source: [Data Science Association](https://www.datascienceassn.org/sites/default/files/GPT-4%20Passes%20the%20Bar%20Exam.pdf)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[数据科学协会](https://www.datascienceassn.org/sites/default/files/GPT-4%20Passes%20the%20Bar%20Exam.pdf)
- en: Conclusion
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: With the rise of Transformer-based Large Language Models (LLMs), the field of
    natural language processing is undergoing rapid evolution. Among the various language
    models built on this architecture, the GPT models have emerged exceptional in
    terms of output and performance. OpenAI, the organization behind GPT, has consistently
    enhanced the model on multiple fronts since the release of the first model.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 随着基于Transformer的大型语言模型（LLMs）的崛起，自然语言处理领域正在迅速演变。在基于这一架构的各种语言模型中，GPT模型在输出和性能方面表现出色。作为GPT背后的组织，OpenAI自首个模型发布以来，始终在多个方面不断提升模型。
- en: Over the course of five years, the size of the model has scaled significantly,
    expanding approximately 8,500 times from GPT-1 to GPT-4\. This remarkable progress
    can be attributed to continuous enhancements in areas such as training data size,
    data quality, data sources, training techniques, and the number of parameters.
    These factors have played a pivotal role in enabling the models to deliver outstanding
    performance across a wide range of tasks.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在五年的时间里，模型的规模显著扩大，从GPT-1到GPT-4约增加了8,500倍。这一显著进展可归因于在训练数据规模、数据质量、数据来源、训练技术以及参数数量等方面的持续改进。这些因素在使模型能够在各种任务中提供出色性能方面发挥了关键作用。
- en: '**[Ankit Mehra](https://www.linkedin.com/in/ankit-mehra-11979617b/)** is a
    Senior Data Scientist at Sigmoid. He specializes in analytics and ML-based data
    solutions.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[Ankit Mehra](https://www.linkedin.com/in/ankit-mehra-11979617b/)** 是Sigmoid的高级数据科学家。他专注于分析和基于机器学习的数据解决方案。'
- en: '**[Malhar Yadav](https://www.linkedin.com/in/malhar-yadav-772693210/)** is
    an Associate Data Scientist at Sigmoid and a coding and ML enthusiast.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[Malhar Yadav](https://www.linkedin.com/in/malhar-yadav-772693210/)** 是Sigmoid的助理数据科学家，同时也是编码和机器学习爱好者。'
- en: '**[Bhaskar Ammu](https://www.linkedin.com/in/bhaskar-ammu-04a99213/)** is a
    Senior Lead Data Scientist at Sigmoid. He specializes in designing data science
    solutions for clients, building database architectures, and managing projects
    and teams.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[Bhaskar Ammu](https://www.linkedin.com/in/bhaskar-ammu-04a99213/)** 是Sigmoid的高级数据科学领导。他专注于为客户设计数据科学解决方案，构建数据库架构，以及管理项目和团队。'
- en: More On This Topic
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解更多相关话题
- en: '[Optimizing Python Code Performance: A Deep Dive into Python Profilers](https://www.kdnuggets.com/2023/02/optimizing-python-code-performance-deep-dive-python-profilers.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化Python代码性能：深入探讨Python分析器](https://www.kdnuggets.com/2023/02/optimizing-python-code-performance-deep-dive-python-profilers.html)'
- en: '[Meet Gorilla: UC Berkeley and Microsoft’s API-Augmented LLM…](https://www.kdnuggets.com/2023/06/meet-gorilla-uc-berkeley-microsoft-apiaugmented-llm-outperforms-gpt4-chatgpt-claude.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[认识Gorilla：UC Berkeley和微软的API增强LLM…](https://www.kdnuggets.com/2023/06/meet-gorilla-uc-berkeley-microsoft-apiaugmented-llm-outperforms-gpt4-chatgpt-claude.html)'
- en: '[Unveiling Neural Magic: A Dive into Activation Functions](https://www.kdnuggets.com/unveiling-neural-magic-a-dive-into-activation-functions)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[揭开神经魔法：深入探讨激活函数](https://www.kdnuggets.com/unveiling-neural-magic-a-dive-into-activation-functions)'
- en: '[Dive into the Future with Kaggle''s AI Report 2023 – See What''s Hot](https://www.kdnuggets.com/dive-into-the-future-with-kaggle-ai-report-2023-see-what-hot)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[与Kaggle的AI报告2023一起探索未来——看看什么是热门](https://www.kdnuggets.com/dive-into-the-future-with-kaggle-ai-report-2023-see-what-hot)'
- en: '[ChatGPT vs Google Bard: A Comparison of the Technical Differences](https://www.kdnuggets.com/2023/03/chatgpt-google-bard-comparison-technical-differences.html)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatGPT与Google Bard：技术差异比较](https://www.kdnuggets.com/2023/03/chatgpt-google-bard-comparison-technical-differences.html)'
- en: '[A Comparison of Machine Learning Algorithms in Python and R](https://www.kdnuggets.com/2023/06/machine-learning-algorithms-python-r.html)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python和R中的机器学习算法比较](https://www.kdnuggets.com/2023/06/machine-learning-algorithms-python-r.html)'
