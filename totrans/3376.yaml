- en: 'Deep Learning Research Review: Reinforcement Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习研究回顾：强化学习
- en: 原文：[https://www.kdnuggets.com/2016/11/deep-learning-research-review-reinforcement-learning.html](https://www.kdnuggets.com/2016/11/deep-learning-research-review-reinforcement-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://www.kdnuggets.com/2016/11/deep-learning-research-review-reinforcement-learning.html](https://www.kdnuggets.com/2016/11/deep-learning-research-review-reinforcement-learning.html)'
- en: '![](../Images/21b4ad6273c43fa6a8bd2bd337284dbb.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21b4ad6273c43fa6a8bd2bd337284dbb.png)'
- en: '*This is the 2nd installment of a new series called Deep Learning Research
    Review. Every couple weeks or so, I’ll be summarizing and explaining research
    papers in specific subfields of deep learning. This week focuses on Reinforcement
    Learning.*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是一个名为深度学习研究回顾的新系列的第2期。每隔几周，我会总结并解释特定深度学习子领域的研究论文。本周关注强化学习。*'
- en: '[*Last time*](https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-1-Generative-Adversarial-Nets)
    *was Generative Adversarial Networks ICYMI*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[*上次*](https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-1-Generative-Adversarial-Nets)
    *是生成对抗网络 ICYMI*'
- en: '**Introduction to Reinforcement Learning**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**强化学习简介**'
- en: '**3 Categories of Machine Learning**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习的3个类别**'
- en: Before getting into the papers, let’s first talk about what **reinforcement
    learning** is. The field of machine learning can be separated into 3 main categories.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入论文之前，我们先来谈谈什么是**强化学习**。机器学习领域可以分为3个主要类别。
- en: Supervised Learning
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised Learning
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Reinforcement Learning
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 强化学习
- en: '![](../Images/7a5ff597262bfbaecade1e2614986dc5.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a5ff597262bfbaecade1e2614986dc5.png)'
- en: The first category, **supervised learning**, is the one you may be most familiar
    with. It relies on the idea of creating a function or model based on a set of
    training data, which contains inputs and their corresponding labels. Convolutional
    Neural Networks are a great example of this, as the images are the inputs and
    the outputs are the classifications of the images (dog, cat, etc).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个类别，**监督学习**，是你可能最熟悉的。它依赖于基于一组训练数据（包含输入及其对应的标签）来创建一个函数或模型。卷积神经网络就是一个很好的例子，因为图像是输入，而输出是图像的分类（狗、猫等）。
- en: '**Unsupervised learning** seeks to find some sort of structure within data
    through methods of cluster analysis. One of the most well-known ML clustering
    algorithms, K-Means, is an example of unsupervised learning.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**无监督学习** 通过聚类分析的方法在数据中寻找某种结构。最著名的机器学习聚类算法之一 K-Means 就是无监督学习的一个例子。'
- en: '**Reinforcement learning** is the task of learning what actions to take, given
    a certain situation/environment, so as to maximize a reward signal. The interesting
    difference between supervised and reinforcement learning is that this reward signal
    simply tells you whether the action (or input) that the agent takes is good or
    bad. It doesn’t tell you anything about what the *best* action is. Contrast this
    to CNNs where the corresponding label for each image input is a definite instruction
    of what the output should be for each input.  Another unique component of RL is
    that an agent’s actions will affect the subsequent data it receives. For example,
    an agent’s action of moving left instead of right means that the agent will receive
    different input from the environment at the next time step. Let’s look at an example
    to start off.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习** 是学习在给定某种情况/环境下应该采取什么行动，以最大化奖励信号的任务。监督学习和强化学习之间有趣的区别在于，奖励信号仅仅告诉你代理采取的行动（或输入）是好是坏，并不会告诉你*最佳*行动是什么。与此相对的是
    CNN，其中每个图像输入的对应标签明确指示了每个输入的输出应该是什么。强化学习的另一个独特组成部分是代理的行动将影响其后续接收到的数据。例如，代理向左移动而不是向右移动意味着代理在下一时间步骤将接收到来自环境的不同输入。让我们从一个例子开始。'
- en: '**The RL Problem**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习问题**'
- en: So, let’s first think about what have in a reinforcement learning problem. Let’s
    imagine a tiny robot in a small room. We haven’t programmed this robot to move
    or walk or take any action. It’s just standing there. This robot is our **agent**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们首先考虑在强化学习问题中有什么。假设在一个小房间里有一个小机器人。我们还没有编程这个机器人去移动或走动或采取任何行动。它只是站在那里。这个机器人就是我们的**代理**。
- en: '![](../Images/5612d33f51f345bd3f80fd3b221d4df9.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5612d33f51f345bd3f80fd3b221d4df9.png)'
- en: Like we mentioned before, reinforcement learning is all about trying to understand
    the optimal way of making decisions/actions so that we maximize some **reward** **R**.
    This reward is a feedback signal that just indicates how well the agent is doing
    at a given time step. The **action A** that an agent takes at every time step
    is a function of both the reward (signal telling the agent how well it’s currently
    doing) and the **state S**, which is a description of the environment the agent
    is in. The mapping from environment states to actions is called our **policy P**.
    The policy basically defines the agent’s way of behaving at a certain time, given
    a certain situation. Now, we also have a **value function V** which is a measure
    of how good each position is. This is different from the reward in that the reward
    signal indicates what is good in the immediate sense, while the value function
    is more indicative of how good it is to be in this state/position in the long
    run. Finally, we also have a **model M** which is the agent’s representation of
    the environment. This is the agent’s model of how it thinks that the environment
    is going to behave.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，强化学习完全是关于理解做决策/行动的最佳方式，以便我们最大化某个**奖励** **R**。这个奖励是一个反馈信号，仅仅表示代理在给定时间步的表现如何。代理在每个时间步采取的**动作
    A**是奖励（告知代理当前表现如何）的信号和**状态 S**（描述代理所处环境的情况）的函数。环境状态到动作的映射称为我们的**策略 P**。策略基本上定义了在特定情况下，代理在特定时间的行为方式。现在，我们还有一个**价值函数
    V**，这是衡量每个位置好坏的标准。这与奖励不同，因为奖励信号指示即时的好处，而价值函数更多地表示在长期来看处于这个状态/位置的好处。最后，我们还有一个**模型
    M**，它是代理对环境的表示。这是代理对环境如何表现的模型。
- en: '![](../Images/7cde482d39514ca446c63adbf7dd676b.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7cde482d39514ca446c63adbf7dd676b.png)'
- en: '**Markov Decision Process**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**马尔可夫决策过程**'
- en: So, let’s now think back to our robot (the agent) in the small room. Our **reward
    function** is dependent on what we want the agent to accomplish. Let’s say that
    we want it to move to one of the corners of the room where it will receive a reward.
    The robot will get a +25 when it reaches this point, and will get a -1 for every
    time step it takes to get there. We basically want the robot to get the corner
    as fast as possible. The **actions** the agent can take are moving north, south,
    east, or west. The agent’s **policy** can be a simple one, where the behavior
    is that the agent will always move to the location with the higher value function.
    Makes sense right? A position with a high value function = good to be in this
    position (with regards to long term reward).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们回到我们的小房间中的机器人（代理）。我们的**奖励函数**取决于我们希望代理完成的任务。假设我们希望它移动到房间的一个角落，在那里它将获得奖励。机器人到达这个点时将获得
    +25，并且每经过一个时间步就会得到 -1。我们基本上希望机器人尽快到达角落。代理可以采取的**动作**是向北、向南、向东或向西移动。代理的**策略**可以是简单的，即代理将始终移动到具有更高价值函数的位置。对吧？一个高价值函数的位置
    = 在这个位置（就长期奖励而言）很好。
- en: Now, this whole RL environment can be described with a **Markov Decision Process**.
    For those that haven’t heard the term before, an MDP is a framework for modeling
    an agent’s decision making. It contains a finite set of states (and value functions
    for those states), a finite set of actions, a policy, and a reward function. Our
    value function can be split into 2 terms.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这整个 RL 环境可以用**马尔可夫决策过程**来描述。对于那些以前没听过这个术语的人来说，MDP 是一个建模代理决策的框架。它包含一个有限的状态集（及这些状态的价值函数），一个有限的动作集，一个策略和一个奖励函数。我们的价值函数可以分为两个部分。
- en: '**State-value function V**: The expected return from being in a state S and
    following a policy π. This return is calculated by looking at summation of the
    reward at each future time step (The gamma refers to a constant discount factor,
    which means that the reward at time step 10 is weighted a little less than the
    reward at time step 1).'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**状态价值函数 V**：在状态 S 中，遵循策略 π 的预期回报。这个回报是通过查看每个未来时间步的奖励总和来计算的（gamma 指的是一个常数折扣因子，这意味着时间步
    10 的奖励的权重比时间步 1 的奖励略低）。'
- en: '![](../Images/29868060c2213b587f9057bd5e41c833.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29868060c2213b587f9057bd5e41c833.png)'
- en: '**Action-value function Q**: The expected return from being in a state S, following
    a policy π, and taking an action a (Equation will be same as above except that
    we have an additional condition that At = a).'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**动作价值函数 Q**：在状态 S 中，遵循策略 π 并采取动作 a 的预期回报（方程与上面相同，只是我们增加了一个额外条件 At = a）。'
- en: Now that we have all the components, what do we do with this MDP? Well, we want
    to solve it, of course. By solving an MDP, you’ll be able to find the optimal
    behavior (policy) that maximizes the amount of reward the agent can expect to
    get from any state in the environment.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了所有的组成部分，我们该如何处理这个 MDP 呢？当然，我们想要解决它。通过解决 MDP，你将能够找到最大化代理可以从环境中任何状态获得的奖励的最优行为（策略）。
- en: '**Solving the MDP**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决 MDP**'
- en: We can solve an MDP and get the optimum policy through the use of dynamic programming
    and specifically through the use of **policy iteration **(there is another technique
    called value iteration, but won’t go into that right now). The idea is that we
    take some initial policy π1 and evaluate the state value function for that policy.
    The way we do this is through the **Bellman expectation equation**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用动态规划和特别是**策略迭代**来解决一个 MDP（还有一种叫做价值迭代的技术，但现在不讨论）。其想法是我们从一些初始策略 π1 开始，并评估该策略的状态价值函数。我们通过**贝尔曼期望方程**来完成这个过程。
- en: '![](../Images/efc5925eb0ccb99a4fcd1e748c63df11.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efc5925eb0ccb99a4fcd1e748c63df11.png)'
- en: This equation basically says that our value function, given that we’re following
    policy π, can be decomposed into the expected return sum of the immediate reward
    Rt+1 and the value function of the successor state St+1\. If you think about it
    closely, this is equivalent to the value function definition we used in the previous
    section. Using this equation is our **policy evaluation**component. In order to
    get a better policy, we use a **policy **improvement step where we simply act
    greedily with respect to the value function. In other words, the agent takes the
    action that maximizes value.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程基本上说，给定我们遵循策略 π，我们的价值函数可以被分解为即时奖励 Rt+1 和后继状态 St+1 的价值函数的期望回报和。如果仔细想想，这等同于我们在上一节中使用的价值函数定义。使用这个方程是我们的**策略评估**组成部分。为了得到更好的策略，我们使用**策略**改进步骤，在这个步骤中我们简单地相对于价值函数采取贪婪行动。换句话说，代理采取最大化价值的行动。
- en: '![](../Images/d6dbe01172b839fa39441614dd9d0708.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6dbe01172b839fa39441614dd9d0708.png)'
- en: Now, in order to get the *optimal* policy, we repeat these 2 steps, one after
    the other, until we converge to optimal policy π*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了获得*最优*策略，我们重复这两个步骤，一个接一个，直到我们收敛到最优策略 π*。
- en: '**When You’re Not Given an MDP**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**当没有给定 MDP 时**'
- en: Policy iteration is great and all, but it only works when we have a given MDP.
    The MDP essentially tells you how the environment works, which realistically is
    not going to be given in real world scenarios. When not given an MDP, we use model
    free methods that go directly from the experience/interactions of the agents and
    the environment to the value functions and policies. We’re going to be doing the
    same steps of policy evaluation and policy improvement, just without the information
    given by the MDP.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 策略迭代非常棒，但它只在我们有一个给定的 MDP 时才有效。MDP 本质上告诉你环境是如何工作的，而在现实世界中这通常不会被直接给出。当没有给定 MDP
    时，我们使用无模型的方法，这些方法直接从代理与环境的经验/交互中得出价值函数和策略。我们将进行相同的策略评估和策略改进步骤，只是没有 MDP 提供的信息。
- en: The way we do this is instead of improving our policy by optimizing over the
    state value function, we’re going to optimize over the action value function Q.
    Remember how we decomposed the state value function into the sum of immediate
    reward and value function of the successor state? Well, we can do the same with
    our Q function.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的做法是，不是通过优化状态价值函数来改进我们的策略，而是优化行动价值函数 Q。还记得我们是如何将状态价值函数分解为即时奖励和后继状态的价值函数之和的吗？我们可以用同样的方法处理我们的
    Q 函数。
- en: '![](../Images/655495d0bbcdba93e724a2c83da5b86c.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/655495d0bbcdba93e724a2c83da5b86c.png)'
- en: Now, we’re going to go through the same process of policy evaluation and policy
    improvement, except we replace our state value function V with our action value
    function Q. Now, I’m going to skip over the details of what changes with the evaluation/improvement
    steps. To understand MDP free evaluation and improvement methods, topics such
    as Monte Carlo Learning, Temporal Difference Learning, and SARSA would require
    whole blogs just themselves (If you are interested, though, please take a listen
    to David Silver’s [Lecture 4](https://www.youtube.com/watch?v=PnHCvfgC_ZA) and [Lecture
    5](https://www.youtube.com/watch?v=0g4j2k_Ggc4)). Right now, however, I’m going
    to jump ahead to value function approximation and the methods discussed in the
    AlphaGo and Atari Papers, and hopefully that should give a taste of modern RL
    techniques. **The main takeaway is that we want to find the optimal policy π* that
    maximizes our action value function Q.**
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将按照相同的过程进行策略评估和策略改进，不过我们将状态值函数V替换为动作值函数Q。现在，我将略过评估/改进步骤中的详细变化。要理解MDP无关的评估和改进方法，如蒙特卡罗学习、时间差分学习和SARSA等话题需要专门的博客（如果你感兴趣的话，可以听听David
    Silver的[Lecture 4](https://www.youtube.com/watch?v=PnHCvfgC_ZA)和[Lecture 5](https://www.youtube.com/watch?v=0g4j2k_Ggc4)）。然而，现在我将跳到值函数近似以及AlphaGo和Atari论文中讨论的方法，希望这能让你了解现代强化学习技术。**主要的收获是我们想要找到能够最大化我们的动作值函数Q的最优策略π***。
- en: '**Value Function Approximation**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**值函数近似**'
- en: So, if you think about everything we’ve learned up until this point, we’ve treated
    our problem in a relatively simplistic way. Look at the above Q equation. We’re
    taking in a specific state S and action A, and then computing a number that basically
    tells us what the expected return is. Now let’s imagine that our agent moves 1
    millimeter to the right. This means we have a whole new state S’, and now we’re
    going to have to compute a Q value for that. In real world RL problems, there
    are millions and millions of states so it’s important that our value functions
    understand generalization in that we don’t have to store a completely separate
    value function for every possible state. The solution is to use a **Q value function
    approximation **that is able to generalize to unknown states.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你考虑到迄今为止我们学到的一切，我们以一种相对简单的方式处理了我们的问题。看看上面的Q方程。我们接受一个特定的状态S和动作A，然后计算一个基本上告诉我们预期回报的数字。现在假设我们的代理向右移动了1毫米。这意味着我们有了一个全新的状态S’，现在我们需要为此计算一个Q值。在现实世界的强化学习问题中，有成千上万的状态，因此我们的值函数必须理解泛化，以便我们不需要为每个可能的状态存储一个完全独立的值函数。解决方案是使用**Q值函数近似**，它能够对未知状态进行泛化。
- en: So, what we want is some function, let’s call is Qhat, that gives a rough approximation
    of the Q value given some state S and some action A.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们想要的是一个函数，我们称之为Qhat，它可以在给定某个状态S和某个动作A的情况下，提供Q值的粗略近似。
- en: '![](../Images/96caca4b660041c06c312793e33f7876.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96caca4b660041c06c312793e33f7876.png)'
- en: This function is going to take in S, A, and a good old weight vector W (Once
    you see that W, you already know we’re bringing in some gradient descent *). It
    is going to compute the dot product between x (which is just a feature vector
    that represents S and A) and W. The way we’re going to improve this function is
    by calculating the loss between the true Q value (let’s just assume that it’s
    given to us for now) and the output of the approximate function.*
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将接受S、A和一个老旧的权重向量W（一旦你看到W，你就知道我们引入了一些梯度下降*）。它将计算x（即表示S和A的特征向量）和W之间的点积。我们改进这个函数的方法是计算真实Q值（暂时假设它是已知的）和近似函数输出之间的损失*。
- en: '*![](../Images/61c07904b90863d6af0b7ae4b910caac.png)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*![](../Images/61c07904b90863d6af0b7ae4b910caac.png)*'
- en: After we compute the loss, we use gradient descent to find the minimum value,
    at which point we will have our optimal W vector. This idea of function approximation
    is going to be very key when taking a look at the papers a little later.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们计算损失之后，我们使用梯度下降来找到最小值，到那时我们将获得最优的W向量。函数近似的这一概念将在稍后查看的论文中非常关键。
- en: '**Just One More Thing**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**还有一件事**'
- en: Before getting to the papers, just wanted to touch on one last thing. An interesting
    discussion with the topic of reinforcement learning is that of exploration vs
    exploitation. **Exploitation** is the agent’s process of taking what it already
    knows, and then making the actions that it knows will produce the maximum reward.
    This sounds great, right? The agent will always be making the best action based
    on its current knowledge. However, there is a key phrase in that statement. *Current
    knowledge*. If the agent hasn’t explored enough of the state space, it can’t possibly
    know whether it is really taking the best possible action. This idea of taking
    actions with the main purpose of exploring the state space is called **exploration**.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入论文之前，想要提及最后一件事。有关强化学习的一个有趣讨论是探索与利用的主题。**利用**是代理利用其已知的知识，并采取它知道会产生最大奖励的行动。听起来不错，对吧？代理将始终根据其当前知识做出最佳行动。然而，这句话中有一个关键短语。*当前知识*。如果代理没有足够探索状态空间，它无法知道自己是否真的采取了最佳可能的行动。这种以探索状态空间为主要目的的行动被称为**探索**。
- en: This idea can be easily related to a real world example. Let’s say you have
    a choice of what restaurant to eat at tonight. You (acting as the agent) know
    that you like Mexican food, so in RL terms, going to a Mexican restaurant will
    be the action that maximizes your reward, or happiness/satisfaction in this case.
    However, there is also a choice of Italian food, which you’ve never had before.
    There’s a possibility that it could be better than Mexican food, or could be a
    lot worse. This tradeoff between whether to exploit an agent’s past knowledge
    vs trying something new in hope of discovering a greater reward is one of the
    major challenges in reinforcement learning (and in our daily lives tbh).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法可以很容易地联系到现实世界的例子。假设你今晚要选择去哪个餐厅用餐。你（作为代理）知道你喜欢墨西哥菜，因此从RL的角度来看，去墨西哥餐厅将是最大化你奖励的行动，即在这种情况下是幸福感/满足感。然而，还有一个意大利餐的选择，你以前从未尝试过。有可能它比墨西哥菜更好，或者可能更差。这种在利用代理过去知识与尝试新事物以期发现更大奖励之间的权衡是强化学习（以及我们日常生活中的一个主要挑战）。
- en: '**Other Resources for Learning RL**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**其他学习RL的资源**'
- en: Phew. That was a lot of info. By no means, however, was that a comprehensive
    overview of the field. If you’d like a more in-depth overview of RL, I’d strongly
    recommend these resources.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀，那真是很多信息。然而，这绝不是对该领域的全面概述。如果你想要更深入的RL概述，我强烈推荐这些资源。
- en: David Silver (from Deepmind) Reinforcement Learning [Video Lectures](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: David Silver（来自Deepmind）强化学习[视频讲座](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT)
- en: My [personal notes](https://docs.google.com/document/d/1TjmYDOxQzOQ0jd0lUiFOVQ1hNAmtwKfSAACck9dR7a8/edit?usp=sharing) from
    the RL course
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我在RL课程中的[个人笔记](https://docs.google.com/document/d/1TjmYDOxQzOQ0jd0lUiFOVQ1hNAmtwKfSAACck9dR7a8/edit?usp=sharing)
- en: Sutton and Barto’s [Reinforcement Learning Textbook](https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf) (This
    is really the holy grail if you are determined to learn the ins and outs of this
    subfield)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton和Barto的[强化学习教材](https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf)（如果你决心学习这一子领域的所有细节，这真的是终极宝典）
- en: Andrej Karpathy’s [Blog Post](http://karpathy.github.io/2016/05/31/rl/) on RL
    (Start with this one if you want to ease into RL and want to see a really well
    done practical example)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrej Karpathy的[博客文章](http://karpathy.github.io/2016/05/31/rl/)关于RL（如果你想轻松入门RL，并查看一个做得非常好的实际示例，可以从这个开始）
- en: '[UC Berkeley CS 188](http://ai.berkeley.edu/lecture_videos.html) Lectures 8-11'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[UC Berkeley CS 188](http://ai.berkeley.edu/lecture_videos.html) 讲座 8-11'
- en: '[Open AI Gym](https://gym.openai.com/): When you feel comfortable with RL,
    try creating your own agents with this reinforcement learning toolkit that Open
    AI created'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Open AI Gym](https://gym.openai.com/): 当你对RL感到舒适时，尝试使用Open AI创建的这个强化学习工具包创建自己的代理'
- en: '* * *'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的快车道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的IT'
- en: '* * *'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关话题
- en: '[HuggingFace Has Launched a Free Deep Reinforcement Learning Course](https://www.kdnuggets.com/2022/05/huggingface-launched-free-deep-reinforcement-learning-course.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HuggingFace推出了免费深度强化学习课程](https://www.kdnuggets.com/2022/05/huggingface-launched-free-deep-reinforcement-learning-course.html)'
- en: '[AI, Analytics, Machine Learning, Data Science, Deep Learning…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能、分析、机器学习、数据科学、深度学习……](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
- en: '[Hands-on Reinforcement Learning Course Part 3: SARSA](https://www.kdnuggets.com/2022/01/handson-reinforcement-learning-course-part-3-sarsa.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实践强化学习课程第3部分：SARSA](https://www.kdnuggets.com/2022/01/handson-reinforcement-learning-course-part-3-sarsa.html)'
- en: '[Hands-On Reinforcement Learning Course, Part 1](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-course-part-1.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实践强化学习课程第1部分](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-course-part-1.html)'
- en: '[Hands-On Reinforcement Learning Course, Part 2](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-part-2.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实践强化学习课程第2部分](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-part-2.html)'
- en: '[Reinforcement Learning for Newbies](https://www.kdnuggets.com/2022/05/reinforcement-learning-newbies.html)*'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[强化学习入门](https://www.kdnuggets.com/2022/05/reinforcement-learning-newbies.html)*'
