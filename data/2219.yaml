- en: 'Diving into the Pool: Unraveling the Magic of CNN Pooling Layers'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入池化：揭示CNN池化层的魔力
- en: 原文：[https://www.kdnuggets.com/diving-into-the-pool-unraveling-the-magic-of-cnn-pooling-layers](https://www.kdnuggets.com/diving-into-the-pool-unraveling-the-magic-of-cnn-pooling-layers)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/diving-into-the-pool-unraveling-the-magic-of-cnn-pooling-layers](https://www.kdnuggets.com/diving-into-the-pool-unraveling-the-magic-of-cnn-pooling-layers)
- en: Motivation
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: '* * *'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的IT需求'
- en: '* * *'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Pooling layers are common in CNN architectures used in all state-of-the-art
    deep learning models. They are prevalent in Computer Vision tasks including Classification,
    Segmentation, Object Detection, Autoencoders and many more; simply used wherever
    we find Convolutional layers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层在所有最先进的深度学习模型中使用的CNN架构中非常常见。它们在计算机视觉任务中广泛存在，包括分类、分割、目标检测、自编码器等；只要有卷积层的地方就会用到它们。
- en: In this article, we'll dig into the math that makes pooling layers work and
    learn when to use different types. We'll also figure out what makes each type
    special and how they are different from one another.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将深入探讨使池化层有效的数学原理，并学习何时使用不同类型的池化层。我们还将弄清楚每种类型的特点以及它们之间的不同之处。
- en: Why use Pooling Layers
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么使用池化层
- en: Pooling layers provide various benefits making them a common choice for CNN
    architectures. They play a critical role in managing spatial dimensions and enable
    models to learn different features from the dataset.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层提供了各种好处，使其成为CNN架构的常见选择。它们在管理空间维度方面发挥了关键作用，并使模型能够从数据集中学习不同的特征。
- en: 'Here are some benefits of using pooling layers in your models:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用池化层在模型中的一些好处：
- en: '**Dimensionality Reduction**'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**维度减少**'
- en: All pooling operations select a subsample of values from a complete convolutional
    output grid. This downsamples the outputs resulting in a decrease in parameters
    and computation for subsequent layers, which is a vital benefit of Convolutional
    architectures over fully connected models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所有池化操作从完整的卷积输出网格中选择一个子样本。这会减少输出，从而减少随后的层的参数和计算，这是卷积架构相对于全连接模型的一个重要好处。
- en: '**Translation Invariance**'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平移不变性**'
- en: Pooling layers make machine learning models invariant to small changes in input
    such as rotations, translations or augmentations. This makes the model suitable
    for basic computer vision tasks allowing it to identify similar image patterns.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层使机器学习模型对输入中的小变化（如旋转、平移或增强）具有不变性。这使得模型适用于基本的计算机视觉任务，使其能够识别类似的图像模式。
- en: Now, let us look at various pooling methods commonly used in practice.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看在实践中常用的各种池化方法。
- en: Common Example
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见示例
- en: For ease of comparison let's use a simple 2-dimensional matrix and apply different
    techniques with the same parameters.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便比较，我们使用一个简单的二维矩阵，并应用相同参数的不同技术。
- en: Pooling layers inherit the same terminology as the Convolutional Layers, and
    the concept of Kernel Size, Stride and Padding is conserved.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层继承了与卷积层相同的术语，内核大小、步幅和填充的概念得到保留。
- en: So, here we define a 2-D matrix with four rows and four columns. To use Pooling,
    we will use a Kernel size of two and stride two with no padding. Our matrix will
    look as follows.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们在这里定义一个四行四列的二维矩阵。为了使用池化，我们将使用大小为二的内核和步幅为二的设置，没有填充。我们的矩阵如下所示。
- en: '![Diving into the Pool: Unraveling the Magic of CNN Pooling Layers](../Images/74067005bf99a0a2f144582c88ad1680.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![深入池化：揭示CNN池化层的魔力](../Images/74067005bf99a0a2f144582c88ad1680.png)'
- en: Image by Author
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图片
- en: '**It is important to note that pooling is applied on a per-channel basis.**
    So the same pooling operations are repeated for each channel in a feature map.
    The number of channels remains invariant, even though the input feature map is
    downsampled.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**需要注意的是，池化是按通道进行的。** 因此，在特征图的每个通道上重复相同的池化操作。即使输入特征图被降采样，通道数量也保持不变。'
- en: Max Pooling
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大池化
- en: We iterate the kernel over the matrix and select the max value from each window.
    In the above example, we use a 2x2 kernel with stride two and iterate over the
    matrix forming four different windows, denoted by different colours.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在矩阵上遍历内核，并从每个窗口中选择最大值。在上述示例中，我们使用一个2x2的内核，步长为二，并在矩阵上遍历，形成四个不同的窗口，由不同颜色表示。
- en: '**In Max Pooling, we only retain the largest value from each window.** This
    downsamples the matrix, and we obtain a smaller 2x2 grid as our max pooling output.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**在最大池化中，我们只保留每个窗口中的最大值。** 这会对矩阵进行降采样，我们得到一个较小的2x2网格作为最大池化输出。'
- en: '![Diving into the Pool: Unraveling the Magic of CNN Pooling Layers](../Images/16229fcc4d9f0ceab1c5fc380c07017c.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![深入池化层：揭示CNN池化层的魔力](../Images/16229fcc4d9f0ceab1c5fc380c07017c.png)'
- en: Image by Author
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Benefits of Max Pooling
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最大池化的好处
- en: '**Preserve High Activation Values**'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保留高激活值**'
- en: When applied to activation outputs of a convolutional layer, we are effectively
    only capturing the higher activation values. It is useful in tasks where higher
    activations are essential, such as object detection. Effectively we are downsampling
    our matrix, but we can still preserve the critical information in our data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用于卷积层的激活输出时，我们实际上只捕捉了较高的激活值。这在高激活值至关重要的任务中很有用，例如目标检测。实际上，我们在降采样矩阵，但仍然可以保留数据中的关键信息。
- en: '**Retain Dominant Features**'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保留主要特征**'
- en: Maximum values often signify the important features in our data. When we retain
    such values, we conserve information the model considers important.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最大值通常表示我们数据中的重要特征。当我们保留这些值时，我们保存了模型认为重要的信息。
- en: '**Resistance to Noise**'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**抗噪声**'
- en: As we base our decision on a single value in a window, small variations in other
    values can be ignored, making it more robust to noise.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们基于窗口中的单个值做决策，其他值的小变化可以被忽略，使其对噪声更具鲁棒性。
- en: Drawbacks
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺点
- en: '**Possible Loss of Information**'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可能的信息丢失**'
- en: Basing our decision on the maximal value ignores the other activation values
    in the window. Discarding such information can result in possible loss of valuable
    information, irrecoverable in subsequent layers.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 基于最大值的决策忽略了窗口中的其他激活值。丢弃这些信息可能导致有价值的信息丧失，在后续层中无法恢复。
- en: '**Insensitive to Small Shifts**'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对小的位移不敏感**'
- en: In Max Pooling, small changes in the non-maximal values will be ignored. This
    insensitivity to small changes can be problematic and can bias the results.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在最大池化中，非最大值的小变化将被忽略。这种对小变化的不敏感可能是一个问题，并可能偏倚结果。
- en: '**Sensitive to High Noise**'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对高噪声敏感**'
- en: Even though small variations in values will be ignored, high noise or error
    in a single activation value can result in the selection of an outlier. This can
    alter the max pooling result significantly, causing degradation of results.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管会忽略值的小变化，但单个激活值的高噪声或错误可能导致选择一个异常值。这可能会显著改变最大池化的结果，导致结果退化。
- en: Average Pooling
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平均池化
- en: In average pooling, we similarly iterate over windows. However, **we consider
    all values in the window, take the mean and then output that as our result.**
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在平均池化中，我们以类似的方式遍历窗口。然而，**我们考虑窗口中的所有值，取平均值，然后将其作为结果输出。**
- en: '![Diving into the Pool: Unraveling the Magic of CNN Pooling Layers](../Images/b25e1d85a50e5357b1038e0d6d154e88.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![深入池化层：揭示CNN池化层的魔力](../Images/b25e1d85a50e5357b1038e0d6d154e88.png)'
- en: Image by Author
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Benefits of Average Pooling
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平均池化的好处
- en: '**Preserving Spatial Information**'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保留空间信息**'
- en: In theory, we are retaining some information from all values in the window,
    to capture the central tendency of the activation values. In effect, we lose less
    information and can persist more spatial information from the convolutional activation
    values.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论上讲，我们保留了窗口中所有值的一些信息，以捕捉激活值的集中趋势。实际上，我们丢失的信息更少，可以保留更多的卷积激活值的空间信息。
- en: '**Robust to Outliers**'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对异常值的鲁棒性**'
- en: Averaging all values makes this method more robust to outliers relative to Max
    Pooling, as a single extreme value can not significantly alter the results of
    the pooling layer.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有值进行平均使这种方法相对于最大池化更能抵抗异常值，因为单个极端值不会显著改变池化层的结果。
- en: '**Smoother Transitions**'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更平滑的过渡**'
- en: When taking the mean of values, we obtain less sharp transitions between our
    outputs. This provides a generalized representation of our data, allowing reduced
    contrast between subsequent layers.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当取平均值时，我们获得的输出之间的过渡更为平滑。这提供了数据的通用表示，减少了后续层之间的对比度。
- en: Drawbacks
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺点
- en: '**Inability to Capture Salient Features**'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无法捕捉显著特征**'
- en: All values in a window are treated equally when the Average Pooling layer is
    applied. This fails to capture the dominant features from a convolutional layer,
    which can be problematic for some problem domains.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 应用平均池化层时，窗口中的所有值被平等对待。这无法捕捉卷积层的主要特征，这在某些问题领域可能是个问题。
- en: '**Reduced Discrimination Between Features Maps**'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征图之间的区分度降低**'
- en: When all values are averaged, we can only capture the common features between
    regions. As such, we can lose the distinctions between certain features and patterns
    in an image, which is certainly a problem for tasks such as Object Detection.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当所有值被平均时，我们只能捕捉区域之间的共同特征。因此，我们可能会丧失图像中某些特征和模式之间的区别，这对于目标检测等任务确实是一个问题。
- en: Global Average Pooling
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全局平均池化
- en: '**Global Pooling is different from normal pooling layers. It has no concept
    of windows, kernel size or stride.** We consider the complete matrix as a whole
    and consider all values in the grid. In the context of the above example, we take
    the average of all values in the 4x4 matrix and get a singular value as our result.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**全局池化不同于普通池化层。它没有窗口、卷积核大小或步幅的概念。** 我们将整个矩阵视为一个整体，并考虑网格中的所有值。在上述示例中，我们对4x4矩阵中的所有值取平均，得到一个单一的值作为结果。'
- en: '![Diving into the Pool: Unraveling the Magic of CNN Pooling Layers](../Images/5729d06229649aa910f506d5b328c6cd.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![深入池化：揭示CNN池化层的魔力](../Images/5729d06229649aa910f506d5b328c6cd.png)'
- en: Image by Author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: When to Use
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用时机
- en: Global Average Pooling allows for straightforward and robust CNN architectures.
    **With the use of Global Pooling, we can implement generalizable models, that
    are applicable to input images of any size.** Global Pooling layers are directly
    used before dense layers.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 全局平均池化允许构建简单而稳健的CNN架构。**通过使用全局池化，我们可以实现通用模型，适用于任何大小的输入图像。** 全局池化层直接在密集层之前使用。
- en: The convolutional layers downsample each image, depending on kernel iterations
    and strides. However, the same convolutions applied to images of different sizes
    will result in an output of different shapes. All images are downsampled by the
    same ratio, so larger images will have larger output shapes. This can be a problem
    when passing it to Dense layers for classification, as size mismatch can cause
    runtime exceptions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层对每张图像进行降采样，具体取决于卷积核迭代次数和步幅。然而，对不同大小的图像应用相同的卷积会导致不同形状的输出。所有图像都按相同的比例降采样，因此较大的图像会有较大的输出形状。当将其传递给密集层进行分类时，尺寸不匹配可能会导致运行时异常。
- en: Without modifications in hyperparameters or model architecture, implementing
    a model applicable to all image shapes can be difficult. This problem is mitigated
    using Global Average Pooling.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有修改超参数或模型架构的情况下，实现适用于所有图像形状的模型可能会很困难。这个问题通过使用全局平均池化得以缓解。
- en: When Global Pooling is applied before Dense layers, all input sizes will be
    reduced to a size of 1x1\. So an input of either (5,5) or (50,50) will be downsampled
    to size 1x1\. They can then be flattened and sent to the Dense layers without
    worrying about size mismatches.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当全局池化应用在密集层之前时，所有输入大小都会被缩减到1x1的大小。因此，(5,5)或(50,50)的输入都会被降采样到1x1的大小。它们可以被展平并发送到密集层，而无需担心大小不匹配。
- en: Key Takeaways
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键要点
- en: We covered some fundamental pooling methods and the scenarios where each is
    applicable. It is critical to choose the one suitable for our specific tasks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了一些基本的池化方法及其适用场景。选择适合我们特定任务的方法至关重要。
- en: '**It is essential to clarify that there are no learnable parameters in pooling
    layers.** They are simply sliding windows performing basic mathematical operations.
    Pooling layers are not trainable, yet they supercharge CNN architectures allowing
    faster computation and robustness in learning input features.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**澄清一点，池化层中没有可学习的参数**。它们只是执行基本数学操作的滑动窗口。池化层不可训练，但它们能够加速CNN架构的计算，并在学习输入特征时增强鲁棒性。'
- en: '**[Muhammad Arham](https://www.linkedin.com/in/muhammad-arham-a5b1b1237/)**
    is a Deep Learning Engineer working in Computer Vision and Natural Language Processing.
    He has worked on the deployment and optimizations of several generative AI applications
    that reached the global top charts at Vyro.AI. He is interested in building and
    optimizing machine learning models for intelligent systems and believes in continual
    improvement.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**[穆罕默德·阿赫曼](https://www.linkedin.com/in/muhammad-arham-a5b1b1237/)** 是一位从事计算机视觉和自然语言处理的深度学习工程师。他曾参与多个生成式AI应用的部署和优化，这些应用在Vyro.AI上进入了全球排行榜。他对构建和优化智能系统的机器学习模型感兴趣，并相信持续改进。'
- en: More On This Topic
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Unraveling the Power of Chain-of-Thought Prompting in Large Language Models](https://www.kdnuggets.com/2023/07/power-chain-thought-prompting-large-language-models.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[揭示大型语言模型中的链式思维提示的力量](https://www.kdnuggets.com/2023/07/power-chain-thought-prompting-large-language-models.html)'
- en: '[Semantic Layers are the Missing Piece for AI-Enabled Analytics](https://www.kdnuggets.com/2024/02/cube-semantic-layers-missing-piece-ai-enabled-analytics)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[语义层是AI驱动分析的缺失环节](https://www.kdnuggets.com/2024/02/cube-semantic-layers-missing-piece-ai-enabled-analytics)'
- en: '[Unveiling Neural Magic: A Dive into Activation Functions](https://www.kdnuggets.com/unveiling-neural-magic-a-dive-into-activation-functions)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[揭示神经魔法：深入探讨激活函数](https://www.kdnuggets.com/unveiling-neural-magic-a-dive-into-activation-functions)'
- en: '[Understanding Python''s Iteration and Membership: A Guide to…](https://www.kdnuggets.com/understanding-pythons-iteration-and-membership-a-guide-to-__contains__-and-__iter__-magic-methods)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解Python的迭代和成员关系：__contains__和__iter__魔法方法指南](https://www.kdnuggets.com/understanding-pythons-iteration-and-membership-a-guide-to-__contains__-and-__iter__-magic-methods)'
- en: '[Introduction to __getitem__: A Magic Method in Python](https://www.kdnuggets.com/2023/03/introduction-getitem-magic-method-python.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[__getitem__介绍：Python中的魔法方法](https://www.kdnuggets.com/2023/03/introduction-getitem-magic-method-python.html)'
- en: '[Python f-Strings Magic: 5 Game-Changing Tricks Every Coder Needs to Know](https://www.kdnuggets.com/python-fstrings-magic-5-gamechanging-tricks-every-coder-needs-to-know)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python f-Strings魔法：每个编码者都需要知道的5个颠覆性技巧](https://www.kdnuggets.com/python-fstrings-magic-5-gamechanging-tricks-every-coder-needs-to-know)'
