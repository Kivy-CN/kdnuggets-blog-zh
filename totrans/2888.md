# 机器学习的集成方法：AdaBoost

> 原文：[https://www.kdnuggets.com/2019/09/ensemble-methods-machine-learning-adaboost.html](https://www.kdnuggets.com/2019/09/ensemble-methods-machine-learning-adaboost.html)

[comments](#comments)

**由[Valentina Alto](https://www.linkedin.com/in/valentina-alto-6a0590148/)、数据科学与商业分析硕士提供**

![Figure](../Images/26b9379c1fabe60e9ebe05ecbb777c55.png)

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持组织的IT工作

* * *

在统计学和机器学习中，集成方法使用多个学习算法来获得比任何单一算法所能获得的更好的预测性能。

组合多种算法的想法最早由计算机科学家兼教授Michael Kerns提出，他在思考*“弱学习是否等同于强学习*”时产生了这个想法。目标是将一个几乎仅比随机猜测好一点的弱算法，转变为一个强学习算法。事实证明，如果我们要求弱算法创建一大堆分类器（这些分类器都很弱），然后将它们组合起来，最终可能会形成一个更强的分类器。

**AdaBoost**，即“自适应提升”，是一种机器学习元算法，可以与许多其他类型的学习算法一起使用，以提高性能。

在这篇文章中，我将提供AdaBoost背后的数学概念，并提供一个Python实现。

### AdaBoost背后的直觉和数学

想象一下我们有一个样本，分为训练集和测试集，还有一堆分类器。每个分类器都在训练集的随机子集上进行训练（注意这些子集可以重叠——这与交叉验证不同）。然后，对于每个子集中的每个观察，AdaBoost分配一个权重，这决定了这个观察出现在训练集中的概率。权重较高的观察更可能被包括在训练集中。因此，AdaBoost倾向于给被错误分类的观察分配更高的权重，使其在下一个分类器的训练集中占据更大的比例，目的是让下一个训练出来的分类器在这些观察上表现更好。

考虑一个简单的二分类问题，其目标由“正面”或“负面”符号（表示为1和-1）表示，最终分类器的方程如下：

![](../Images/c03f7c3b8451945e633f3a7f3cf98e2e.png)

基本上，最终分类器对观测值x的输出等于T个弱分类器输出h_t(x)的加权和的符号，其中权重等于α_t。

更具体地说，α_t是分配给分类器t输出的权重（注意，这个权重与分配给观测值的权重不同，后续将讨论）。它的计算方法如下：

![](../Images/41d5e1430758a1697ef0c4497fdf9154.png)

其中ε_t是分类器t的误差项（分类错误的观测值/总观测值）。当然，低误差的分类器在加权和中会被优先考虑，因此它们的权重会更高。确实，如果我们查看与不同误差对应的alpha：

[PRE0]

![](../Images/43ba9afe4690ce8223532df31f52ab79.png)

正如你所看到的，当误差接近0时，分类器的权重呈指数增长，而当误差接近1时，权重则呈指数下降。

然后，alpha的值用于计算另一种类型的权重，即将分配给子集观测值的权重。对于第一个分类器，权重被初始化为相等，因此每个观测值的权重为1/n（其中n为子集的大小）。从第二个分类器开始，每个权重递归计算如下：

![](../Images/cac8c965366dafdd076e5fa69a729af6.png)

其中y_t是目标值（1或-1），变量w_t是一个权重向量，每个训练示例在训练集中都有一个权重。‘i’是训练示例的编号。这个方程展示了如何更新第i个训练示例的权重。

我们可以将w_t视为一个分布：这与我们一开始所说的一致，即权重代表了训练示例被选择作为训练集一部分的概率。

为了使其成为一个分布，所有这些概率之和应等于1。为确保这一点，我们通过将每个权重除以所有权重的总和Z_t来规范化权重。

让我们解释这个公式。由于我们处理的是二分类问题（-1对1），如果实际值和拟合值具有相同的符号（分类正确），则乘积y_t*h_t(x_i)为正；如果它们具有不同的符号（分类错误），则乘积为负。因此：

+   如果乘积为正且alpha大于零（强分类器），则分配给第i个观测值的权重将较小。

+   如果乘积为正且alpha小于零（弱分类器），则分配给第i个观测值的权重将较高。

+   如果乘积为负且alpha大于零（强分类器），则分配给第i个观测值的权重将较高。

+   如果产品为负且 alpha 小于零（弱分类器），则分配给第 i 个观测值的权重将很小

注意：当我说“少”和“高”时，我是指在归一化之前的指数，分别小于 1 和大于 1。实际上：

![](../Images/2905ba18f9999d808b571fddea1c10b5.png)

现在我们已经了解了 AdaBoost 的工作原理，让我们看看在 Python 中如何使用著名的鸢尾花数据集来实现它。

### 使用 Python 实现

让我们首先导入数据：

[PRE1]

AdaBoost 的默认算法是决策树，但你可以选择手动设置不同的分类器。在这里，我将使用支持向量机分类器（你可以在[这里](https://medium.com/swlh/support-vector-machine-from-scratch-ce095a47dc5c)阅读更多关于 SVM 的内容）。

[PRE2]

如你所见，我们的几乎 98% 的观测值被正确分类。

### 结论

AdaBoost 是一种容易实现的技术。它通过迭代修正弱分类器的错误来提高准确性。然而，它并非没有*缺陷*。事实上，由于它寻求减少训练误差，因此它对离群点特别敏感，存在产生过拟合模型的风险，这样的模型对新的、未标记的数据适应性差，因为它缺乏泛化能力（你可以在[这里](https://towardsdatascience.com/preventing-overfitting-regularization-5eda7d5753bc)阅读更多关于过拟合的内容）。

*最初发布于 *[*http://datasciencechalktalk.com*](https://datasciencechalktalk.com/2019/09/07/ensemble-methods-for-machine-learning-adaboost/)* 于 2019 年 9 月 7 日。*

**简介：[Valentina Alto](https://www.linkedin.com/in/valentina-alto-6a0590148/)** 是一位机器学习和统计学爱好者，目前在博科尼大学攻读数据科学硕士学位。

[原文](https://medium.com/@valentinaalto/ensemble-methods-for-machine-learning-adaboost-2ff70d5518f3)。经许可转载。

**相关内容：**

+   [直观的集成学习指南：梯度提升](/2018/07/intuitive-ensemble-learning-guide-gradient-boosting.html)

+   [集成学习：5 种主要方法](/2019/01/ensemble-learning-5-main-approaches.html)

+   [理解梯度提升机](/2019/02/understanding-gradient-boosting-machines.html)

### 更多相关话题

+   [在 Scikit-learn 中实现 Adaboost](https://www.kdnuggets.com/2022/10/implementing-adaboost-scikitlearn.html)

+   [带实例的集成学习](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)

+   [集成学习技术：Python 中随机森林的详细讲解](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)

+   [何时使用集成技术是一个好选择？](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)

+   [机器学习中的替代特征选择方法](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)

+   [k-means 聚类的质心初始化方法](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)
