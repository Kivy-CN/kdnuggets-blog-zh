# 数据科学的编程最佳实践

> 原文：[https://www.kdnuggets.com/2018/08/programming-best-practices-data-science.html](https://www.kdnuggets.com/2018/08/programming-best-practices-data-science.html)

![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)

**作者：Srini Kadamati，[Dataquest.io](https://www.dataquest.io/)**

数据科学生命周期通常包括以下组件：

+   数据检索

+   数据清洗

+   数据探索与可视化

+   统计或预测建模

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升您的数据分析水平

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持您的组织 IT

* * *

虽然这些组件对理解不同阶段有帮助，但它们并未帮助我们思考*编程*工作流程。

通常，整个数据科学生命周期最终会变成一个任意的笔记本单元的混乱集合，无论是 Jupyter Notebook 还是单一的混乱脚本。此外，大多数数据科学问题要求我们在数据检索、数据清洗、数据探索、数据可视化和统计/预测建模之间切换。

但还有更好的方法！在这篇文章中，我将介绍大多数人在进行数据科学编程工作时所切换的两种思维方式：**原型**思维方式和**生产**思维方式。

| 原型思维优先考虑： | 生产思维优先考虑： |
| --- | --- |
| 小段代码的迭代速度 | 整体流程的迭代速度 |
| 较少的抽象（直接修改代码和数据对象） | 更多的抽象（改为修改参数值） |
| 代码结构较少（模块化较少） | 代码结构较多（模块化较多） |
| 帮助您和他人理解代码和数据 | 帮助计算机自动运行代码 |

我个人使用[JupyterLab](http://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html)来进行整个过程（包括原型开发和生产化）。我建议至少在原型开发阶段使用 JupyterLab**。

### **Lending Club 数据**

为了更具体地理解原型思维与生产思维之间的区别，让我们处理一些真实的数据。我们将使用来自对等借贷网站 [Lending Club](https://www.dataquest.io/blog/programming-best-practices-for-data-science/www.lendingclub.com) 的借贷数据。与银行不同，Lending Club 不直接提供贷款。Lending Club 是一个市场，供贷方向寻求贷款的个人（如房屋维修、婚礼费用等）提供贷款。我们可以利用这些数据来构建模型，以预测某个贷款申请是否会成功。我们不会在这篇文章中深入构建机器学习预测管道，但在我们的 [机器学习项目入门课程](https://www.dataquest.io/course/machine-learning-project) 中会涵盖。

Lending Club 提供了关于已完成贷款（Lending Club 批准并找到贷方的贷款申请）和被拒绝贷款（Lending Club 拒绝的贷款申请且资金未流动）的详细历史数据。访问他们的 [数据下载页面](https://www.lendingclub.com/info/download-data.action)，并在 **DOWLNOAD LOAN DATA** 下选择 **2007-2011**。

![lendingclub](../Images/5893dcd88190c71f1a9b9914ef8fd8e0.png)

### **原型思维**

在原型思维中，我们的目标是快速迭代并尝试理解数据的一些属性和真相。创建一个新的 Jupyter 笔记本，并添加一个解释的 Markdown 单元格：

+   你对 Lending Club 进行的任何研究，以更好地了解这个平台

+   你下载的数据集的任何信息

首先，让我们将 CSV 文件读入 pandas 中。

```py
import pandas as pd
loans_2007 = pd.read_csv('LoanStats3a.csv')
loans_2007.head(2)

```

我们得到两个输出，第一个是警告。

```py
/home/srinify/anaconda3/envs/dq2/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0,1,2,3,4,7,13,18,24,25,27,28,29,30,31,32,34,36,37,38,39,40,41,42,43,44,46,47,49,50,51,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,123,124,125,126,127,128,129,130,131,132,133,134,135,136,142,143,144) have mixed types. Specify dtype option on import or set low_memory=False.
  interactivity=interactivity, compiler=compiler, result=result)

```

然后是数据框的前 5 行，我们将避免在这里显示（因为非常长）。

我们还得到了以下数据框输出：

|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | Notes offered by Prospectus (https://www.lendingclub.com/info/prospectus.action) |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 表头包括：d、member_id、loan_amnt、funded_amnt、funded_amnt_inv、term、int_rate、installment、grade、sub_grade、emp_title、emp_length、home_ownership、annual_inc、verification_status、issue_d、loan_status、pymnt_plan、url、desc、purpose、title、zip_code、addr_state、dti、delinq_2yrs、earliest_cr_line、inq_last_6mths、mths_since_last_delinq、mths_since_last_record、open_acc、pub_rec、revol_bal、revol_util、total_acc、initial_list_status、out_prncp、out_prncp_inv、total_pymnt、total_pymnt_inv、total_rec_prncp、total_rec_int、total_rec_late_fee、recoveries、collection_recovery_fee、last_pymnt_d、last_pymnt_amnt、next_pymnt_d、last_credit_pull_d、collections_12_mths_ex_med、mths_since_last_major_derog、policy_code、application_type、annual_inc_joint、dti_joint、verification_status_joint、acc_now_delinq、tot_coll_amt、tot_cur_bal、open_acc_6m、open_act_il、open_il_12m、open_il_24m、mths_since_rcnt_il、total_bal_il、il_util、open_rv_12m、open_rv_24m、max_bal_bc、all_util、total_rev_hi_lim、inq_fi、total_cu_tl、inq_last_12m、acc_open_past_24mths、avg_cur_bal、bc_open_to_buy、bc_util、chargeoff_within_12_mths、delinq_amnt、mo_sin_old_il_acct、mo_sin_old_rev_tl_op、mo_sin_rcnt_rev_tl_op、mo_sin_rcnt_tl、mort_acc、mths_since_recent_bc、mths_since_recent_bc_dlq、mths_since_recent_inq、mths_since_recent_revol_delinq、num_accts_ever_120_pd、num_actv_bc_tl、num_actv_rev_tl、num_bc_sats、num_bc_tl、num_il_tl、num_op_rev_tl、num_rev_accts、num_rev_tl_bal_gt_0、num_sats、num_tl_120dpd_2m、num_tl_30dpd、num_tl_90g_dpd_24m、num_tl_op_past_12m、pct_tl_nvr_dlq、percent_bc_gt_75、pub_rec_bankruptcies、tax_liens、tot_hi_cred_lim、total_bal_ex_mort、total_bc_limit、total_il_high_credit_limit、revol_bal_joint、sec_app_earliest_cr_line、sec_app_inq_last_6mths、sec_app_mort_acc、sec_app_open_acc、sec_app_revol_util、sec_app_open_act_il、sec_app_num_rev_accts、sec_app_chargeoff_within_12_mths、sec_app_collections_12_mths_ex_med、sec_app_mths_since_last_major_derog、hardship_flag、hardship_type、hardship_reason、hardship_status、deferral_term、hardship_amount、hardship_start_date、hardship_end_date、payment_plan_start_date、hardship_length、hardship_dpd、hardship_loan_status、orig_projected_additional_accrued_interest、hardship_payoff_balance_amount、hardship_last_payment_amount、disbursement_method、debt_settlement_flag、debt_settlement_flag_date、settlement_status、settlement_date、settlement_amount、settlement_percentage、settlement_term。 |

该警告提醒我们，如果在调用 `pandas.read_csv()` 时将 `low_memory` 参数设置为 `False`，pandas 对每列的数据类型推断将会得到改善。

第二个输出更有问题，因为DataFrame存储数据的方式存在问题。JupyterLab有内置的终端环境，我们可以打开它并使用bash命令`head`来查看原始文件的前两行：

```py
head -2 LoanStats3a.csv

```

虽然第二行包含我们预期的CSV文件中的列名，但看起来第一行在pandas尝试解析文件时会干扰DataFrame的格式：

```py
Notes offered by Prospectus (https://www.lendingclub.com/info/prospectus.action)

```

添加一个Markdown单元格，详细说明你的观察，并添加一个代码单元格，考虑这些观察。

```py
import pandas as pd
loans_2007 = pd.read_csv('LoanStats3a.csv', skiprows=1, low_memory=False)

```

从[Lending Club下载页面](https://www.lendingclub.com/info/download-data.action)读取数据字典，以了解哪些列不包含对特征有用的信息。`desc`和`url`列似乎立即符合这个标准。

```py
loans_2007 = loans_2007.drop(['desc', 'url'],axis=1)

```

下一步是删除任何有超过50%缺失行的列。使用一个单元格探索哪些列符合该标准，然后用另一个单元格实际删除这些列。

```py
loans_2007.isnull().sum()/len(loans_2007)

```

```py
loans_2007 = loans_2007.dropna(thresh=half_count, axis=1)

```

因为我们使用Jupyter notebook来记录我们的思考和代码，我们依赖环境（通过IPython内核）来跟踪状态的变化。这使我们可以自由地移动单元格，运行相同的代码多次等。

一般来说，原型思维中的代码应该关注：

+   可理解性

    +   描述我们观察和假设的Markdown单元格

    +   实际逻辑的小块代码

    +   大量的可视化和计数

+   最小抽象

    +   大多数代码不应该在函数中（应该感觉更面向对象）

假设我们花了另一个小时来探索数据并编写描述我们所做数据清理的markdown单元格。然后我们可以切换到生产思维，使代码更健壮。

### **生产思维**

在生产思维中，我们希望关注编写能够推广到更多情况的代码。在我们的例子中，我们希望我们的数据清理代码适用于Lending Club的任何数据集（来自其他时间段）。推广我们代码的最佳方式是将其转换为**数据管道**。数据管道是使用[函数式编程](https://www.dataquest.io/blog/introduction-functional-programming-python/)的原则设计的，其中数据在函数*内部*修改，然后*在函数之间*传递。

这是使用单个函数封装数据清理代码的管道的第一次迭代：

```py
import pandas as pd

def import_clean(file_list):
    frames = []
    for file in file_list:
        loans = pd.read_csv(file, skiprows=1, low_memory=False)
        loans = loans.drop(['desc', 'url'], axis=1)
        half_count = len(loans)/2
        loans = loans.dropna(thresh=half_count, axis=1)
        loans = loans.drop_duplicates()
        # Drop first group of features
        loans = loans.drop(["funded_amnt", "funded_amnt_inv", "grade", "sub_grade", "emp_title", "issue_d"], axis=1)
        # Drop second group of features
        loans = loans.drop(["zip_code", "out_prncp", "out_prncp_inv", "total_pymnt", "total_pymnt_inv", "total_rec_prncp"], axis=1)
        # Drop third group of features
        loans = loans.drop(["total_rec_int", "total_rec_late_fee", "recoveries", "collection_recovery_fee", "last_pymnt_d", "last_pymnt_amnt"], axis=1)
        frames.append(loans)
    return frames

frames = import_clean(['LoanStats3a.csv'])

```

在上面的代码中，我们**抽象**了之前的代码，封装成了一个函数。该函数的输入是一个文件名列表，输出是一个DataFrame对象的列表。

一般来说，生产思维应该关注：

+   健康的抽象

    +   代码应推广以兼容类似的数据源

    +   代码不应该过于通用，以至于难以理解

+   管道稳定性

    +   可靠性应与其运行频率匹配（每日？每周？每月？）

### **切换思维模式**

假设我们尝试对来自 Lending Club 的所有数据集运行该函数，Python 返回了错误。一些潜在的错误来源：

+   一些文件中列名称的方差

+   列中的方差因 50% 缺失值阈值而被丢弃

+   基于 pandas 类型推断的不同列类型

在这些情况下，我们实际上应该回到我们的原型笔记本上进一步调查。当我们确定希望管道更具灵活性并考虑数据中的特定变异时，我们可以将其重新纳入管道逻辑中。

这是一个例子，我们调整了函数以适应不同的丢弃阈值：

```py
import pandas as pd

def import_clean(file_list, threshold=0.5):
    frames = []
    for file in file_list:
        loans = pd.read_csv(file, skiprows=1, low_memory=False)
        loans = loans.drop(['desc', 'url'], axis=1)
        threshold_count = len(loans)*threshold
        loans = loans.dropna(thresh=half_count, axis=1)
        loans = loans.drop_duplicates()
        # Drop first group of features
        loans = loans.drop(["funded_amnt", "funded_amnt_inv", "grade", "sub_grade", "emp_title", "issue_d"], axis=1)
        # Drop second group of features
        loans = loans.drop(["zip_code", "out_prncp", "out_prncp_inv", "total_pymnt", "total_pymnt_inv", "total_rec_prncp"], axis=1)
        # Drop third group of features
        loans = loans.drop(["total_rec_int", "total_rec_late_fee", "recoveries", "collection_recovery_fee", "last_pymnt_d", "last_pymnt_amnt"], axis=1)
        frames.append(loans)
    return frames

frames = import_clean(['LoanStats3a.csv'], threshold=0.7)

```

默认值仍然是`0.5`，但如果需要，我们可以将其覆盖为`0.7`。

这里有几种方法可以使管道更具灵活性，按优先级递减：

+   使用可选、位置参数和必需参数

+   在函数中使用 if / then 语句以及布尔输入值

+   使用新的数据结构（字典、列表等）来表示特定数据集的自定义操作

这个管道可以扩展到数据科学工作流的所有阶段。这里有一些示例代码，预览了它的样子。

```py
import pandas as pd

def import_clean(file_list, threshold=0.5):
    ## Code

def visualize(df_list):
    # Find the most important features and generate pairwise scatter plots
    # Display visualizations and write to file.
    plt.savefig("scatter_plots.png")

def combine(df_list):
    # Combine dataframes and generate train and test sets
    # Drop features all dataframes don't share
    # Return both train and test dataframes
    return train,test

def train(train_df):
    # Train model
    return model

def validate(train_df, test-df):
    # K-fold cross validation
    # Return metrics dictionary
    return metrics_dict

frames = import_clean(['LoanStats3a.csv', 'LoanStats2012.csv'], threshold=0.7)
visualize(frames)
train_df, test_df = combine(frames)
model = train(train_df)
metrics = test(train_df, test_df)
print(metrics)

```

### **下一步**

如果你有兴趣深入理解并进一步实践，我推荐以下下一步：

+   学习如何将你的管道转换成一个可以作为模块或从命令行运行的独立脚本：[https://docs.python.org/3/library/**main**.html](https://docs.python.org/3/library/__main__.html)

+   学习如何使用 Luigi 构建可以在云端运行的更复杂的管道：[在 Python 和 Luigi 中构建数据管道](https://marcobonzanini.com/2015/10/24/building-data-pipelines-with-python-and-luigi/)

+   了解更多关于数据工程的内容：[Dataquest 上的数据工程帖子](https://www.dataquest.io/blog/tag/data-engineering/)

**个人简介：Srini Kadamati** 是**[Dataquest.io](https://www.dataquest.io/)** 的内容总监。

[原文](https://www.dataquest.io/blog/programming-best-practices-for-data-science/?utm_source=kdnuggets&utm_medium=crosspost&utm_content=text)。经许可转载。

**相关：**

+   [Swiftapply  – 自动高效的 pandas apply 操作](/2018/04/swiftapply-automatically-efficient-pandas-apply-operations.html)

+   [与机器学习算法相关的数据结构](/2018/01/data-structures-related-machine-learning-algorithms.html)

+   [Python中的函数式编程介绍](/2018/02/introduction-functional-programming-python.html)

### 更多相关主题

+   [是什么使 Python 成为初创企业理想的编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)

+   [停止学习数据科学以找到目标，并找到目标去…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)

+   [数据科学学习统计的最佳资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)

+   [成功数据科学家的 5 个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)

+   [每个数据科学家都应该知道的三个 R 库（即使你使用 Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)

+   [一个 90 亿美元的人工智能失败，深度剖析](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)
