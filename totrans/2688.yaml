- en: How to Explain Key Machine Learning Algorithms at an Interview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/10/explain-machine-learning-algorithms-interview.html](https://www.kdnuggets.com/2020/10/explain-machine-learning-algorithms-interview.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b598b47e42ef191873d50e7c2b2d268.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Created by [katemangostar](https://www.freepik.com).*'
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linear Regression involves finding a ‘line of best fit’ that represents a dataset
    using the least squares method. The least squares method involves finding a linear
    equation that minimizes the sum of squared residuals. A residual is equal to the
    actual minus predicted value.
  prefs: []
  type: TYPE_NORMAL
- en: To give an example, the red line is a better line of best fit than the green
    line because it is closer to the points, and thus, the residuals are smaller.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/31722631590d909854bc4f2cbf6a9192.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Image created by Author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Ridge Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ridge regression, also known as L2 Regularization, is a regression technique
    that introduces a small amount of bias to reduce overfitting. It does this by
    minimizing the sum of squared residuals **plus **a penalty, where the penalty
    is equal to lambda times the slope squared. Lambda refers to the severity of the
    penalty.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4e2ecf37713d2b405124b71e07babdbe.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Image Created by Author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Without a penalty, the line of best fit has a steeper slope, which means that
    it is more sensitive to small changes in X. By introducing a penalty, the line
    of best fit becomes less sensitive to small changes in X. This is the idea behind
    ridge regression.
  prefs: []
  type: TYPE_NORMAL
- en: Lasso Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lasso Regression, also known as L1 Regularization, is similar to Ridge regression.
    The only difference is that the penalty is calculated with the absolute value
    of the slope instead.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/45c9f92fb938f06886b655ff2f640b13.png)'
  prefs: []
  type: TYPE_IMG
- en: Logistic Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Logistic Regression is a classification technique that also finds a ‘line of
    best fit.’ However, unlike linear regression, where the line of best fit is found
    using least squares, logistic regression finds the line (logistic curve) of best
    fit using maximum likelihood. This is done because the *y* value can only be one
    or zero. [*Check out StatQuest’s video to see how the maximum likelihood is calculated*](https://www.youtube.com/watch?v=BfKanl1aSG0).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03c8e9e7ecd9bc5782ddb7cce468f5c1.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Image Created by Author.*'
  prefs: []
  type: TYPE_NORMAL
- en: K-Nearest Neighbours
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: K-Nearest Neighbours is a classification technique where a new sample is classified
    by looking at the nearest classified points, hence ‘K-nearest.’ In the example
    below, if *k=1*, then an unclassified point would be classified as a blue point.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f644d68310095ee4c2d6322640b9fcc1.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Image Created by Author.*'
  prefs: []
  type: TYPE_NORMAL
- en: If the value of *k* is too low, then it can be subject to outliers. However,
    if it’s too high, then it may overlook classes with only a few samples.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Naive Bayes Classifier is a classification technique inspired by Bayes
    Theorem, which states the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/21511ecbefdcb8581fcaa0a9ccb85cc8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Because of the naive assumption (hence the name) that variables are independent
    given the class, we can rewrite P(X|y) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8bc23603cfe8d5f4e118dacdde0a6945.png)'
  prefs: []
  type: TYPE_IMG
- en: Also, since we are solving for *y*, *P(X)* is a constant, which means that we
    can remove it from the equation and introduce a proportionality.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the probability of each value of *y* is calculated as the product of the
    conditional probability of *x[n]* given *y*.
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Support Vector Machines are a classification technique that finds an optimal
    boundary, called the hyperplane, which is used to separate different classes.
    The hyperplane is found by maximizing the margin between the classes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4106158905570c86fbbd9b369ff85aa3.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Image Created by Author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A decision tree is essentially a series of conditional statements that determine
    what path a sample takes until it reaches the bottom. They are intuitive and easy
    to build but tend not to be accurate.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/faf27433664fcaed598a28d3212809ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Random Forest
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Random Forest is an ensemble technique, meaning that it combines several models
    into one to improve its predictive power. Specifically, it builds 1000s of smaller
    decision trees using bootstrapped datasets and random subsets of variables (also
    known as bagging). With 1000s of smaller decision trees, random forests use a
    ‘majority wins’ model to determine the value of the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d70f24d4607fd5c44bb550e380f62865.png)'
  prefs: []
  type: TYPE_IMG
- en: For example, if we created one decision tree, the third one, it would predict
    0\. But if we relied on the mode of all 4 decision trees, then the predicted value
    would be 1\. This is the power of random forests.
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'AdaBoost is a boosted algorithm that is similar to Random Forests but has a
    couple of significant differences:'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than a forest of trees, AdaBoost typically makes a forest of stumps (a
    stump is a tree with only one node and two leaves).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each stump’s decision is not weighted equally in the final decision. Stumps
    with less total error (high accuracy) will have a higher say.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The order in which the stumps are created is important, as each subsequent stump
    emphasizes the importance of the samples that were incorrectly classified in the
    previous stump.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gradient Boost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gradient Boost is similar to AdaBoost in the sense that it builds multiple trees
    where each tree is built off of the previous tree. Unlike AdaBoost, which builds
    stumps, Gradient Boost builds trees with usually 8 to 32 leaves.
  prefs: []
  type: TYPE_NORMAL
- en: More importantly, Gradient Boost differs from AdaBoost in the way that the decisions
    trees are built. Gradient Boost starts with an initial prediction, usually the
    average. Then, a decision tree is built based on the residuals of the samples.
    A new prediction is made by taking the initial prediction + a learning rate times
    the outcome of the residual tree, and the process is repeated.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: XGBoost is essentially the same thing as Gradient Boost, but the main difference
    is how the residual trees are built. With XGBoost, the residual trees are built
    by calculating similarity scores between leaves and the preceding nodes to determine
    which variables are used as the roots and the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/how-to-explain-each-machine-learning-model-at-an-interview-499d82f91470).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Data Science Internship Interview Questions](https://www.kdnuggets.com/2020/08/data-science-internship-interview-questions.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Rock a Virtual Data Interview](https://www.kdnuggets.com/2020/05/pragmatic-rock-virtual-data-interview.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Data Science Interview Study Guide](https://www.kdnuggets.com/2020/01/data-science-interview-study-guide.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
