- en: An introduction to explainable AI, and why we need it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/04/introduction-explainable-ai.html](https://www.kdnuggets.com/2019/04/introduction-explainable-ai.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By [Patrick Ferris](https://www.patrickferris.me/)**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ba3c9c3cc86e9a0e62613fe62bd1f46.png)'
  prefs: []
  type: TYPE_IMG
- en: '**The Black Box - a metaphor that represents the unknown inner mechanics of
    functions like neural networks**'
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks (and all of their subtypes) are increasingly being used to build
    programs that can predict and classify in a myriad of different settings.
  prefs: []
  type: TYPE_NORMAL
- en: Examples include [machine translation](https://arxiv.org/pdf/1806.08730.pdf) using
    recurrent neural networks, and [image classification](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) using
    a convolutional neural network. Research published by Google DeepMind has sparked
    interest in [reinforcement learning](https://arxiv.org/pdf/1312.5602.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: All of these approaches have advanced many fields and produced usable models
    that can improve productivity and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: However, **we don’t really know how they work**.
  prefs: []
  type: TYPE_NORMAL
- en: 'I was fortunate enough to attend the [Knowledge Discovery and Data Mining](http://www.kdd.org/kdd2018/)(KDD)
    conference this year. Of the talks I went to, there were two main areas of research
    that seem to be on a lot of people’s minds:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, finding a meaningful representation of graph structures to feed into
    neural networks. [Oriol Vinyals](https://ai.google/research/people/OriolVinyals)from
    DeepMind gave a talk about their [Message Passing Neural Networks](https://arxiv.org/pdf/1704.01212.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second area, and the focus of this article, are explainable AI models. As
    we generate newer and more innovative applications for neural networks, the question
    of ‘How do they work?’ becomes more and more important.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Why the need for Explainable Models?**'
  prefs: []
  type: TYPE_NORMAL
- en: Neural Networks are not infallible.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the problems of overfitting and underfitting that we’ve developed many
    tools (like Dropout or increasing the data size) to counteract, neural networks
    operate in an opaque way.
  prefs: []
  type: TYPE_NORMAL
- en: We don’t really know why they make the choices they do. As models become more
    complex, the task of producing an interpretable version of the model becomes more
    difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Take, for example, the [one pixel attack](https://arxiv.org/pdf/1710.08864.pdf) (see
    here for a great [video](https://www.youtube.com/watch?v=SA4YEAWVpbk) on the paper).
    This is carried out by using a sophisticated approach which analyzes the CNNs
    and applies differential evolution (a member of the evolutionary class of algorithms).
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other optimisation strategies which restrict the objective function to
    be differentiable, this approach uses an iterative evolutionary algorithm to produce
    better solutions. Specifically, for this one pixel attack, the only information
    required was the probabilities of the class labels.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe7fb38ebc0ffaa0856815de932b0b20.png)'
  prefs: []
  type: TYPE_IMG
- en: '**From [One pixel attack for fooling deep neural networks](https://arxiv.org/pdf/1710.08864.pdf) by
    Jiawei Su et al.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The relative ease of fooling these neural networks is worrying. Beyond this
    lies a more systemic problem: trusting a neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: The best example of this is in the medical domain. Say you are building a neural
    network (or any black-box model) to help predict heart disease given a patient’s
    records.
  prefs: []
  type: TYPE_NORMAL
- en: When you train and test your model, you get a good accuracy and a convincing
    positive predictive value. You bring it to a clinician and they agree it seems
    to be a powerful model.
  prefs: []
  type: TYPE_NORMAL
- en: 'But they will be hesitant to use it because you (or the model) cannot answer
    the simple question: “Why did you predict this person as more likely to develop
    heart disease?”'
  prefs: []
  type: TYPE_NORMAL
- en: This lack of transparency is a problem for the clinician who wants to understand
    the way the model works to help them improve their service. It is also a problem
    for the patient who wants a concrete reason for this prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Ethically, is it right to tell a patient that they have a higher probability
    of a disease if your only reasoning is that “the black-box told me so”? Health
    care is as much about science as it is about empathy for the patient.
  prefs: []
  type: TYPE_NORMAL
- en: The field of explainable AI has grown in recent years, and this trend looks
    set to continue.
  prefs: []
  type: TYPE_NORMAL
- en: What follows are some of the interesting and innovative avenues researchers
    and machine learning experts are exploring in their search for models which not
    only perform well, but can tell you why they make the choices they do.
  prefs: []
  type: TYPE_NORMAL
- en: Reversed Time Attention Model (RETAIN)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The RETAIN model was developed at Georgia Institute of Technology by [Edward
    Choi et al.](https://arxiv.org/pdf/1608.05745.pdf) It was introduced to help doctors
    understand why a model was predicting patients to be at risk of heart failure.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/522f5ed8973e9aea1646ffc78ccf6faf.png)'
  prefs: []
  type: TYPE_IMG
- en: '**The RETAIN Recurrent Neural Network Model makes use of Attention Mechanisms
    to improve interpretability**'
  prefs: []
  type: TYPE_NORMAL
- en: The idea is, given a patients’ hospital visits records which also contain the
    events of the visit, they could predict the risk of heart failure.
  prefs: []
  type: TYPE_NORMAL
- en: The researchers split the input into two recurrent neural networks. This let
    them use the [attention mechanism](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/) on
    each to understand what the neural network was focusing on.
  prefs: []
  type: TYPE_NORMAL
- en: Once trained, the model could predict a patient’s risk. But it could also make
    use of the alpha and beta parameters to output which hospital visits (and which
    events within a visit) influenced its choice.
  prefs: []
  type: TYPE_NORMAL
- en: '**Local Interpretable Model-Agnostic Explanations (LIME)**'
  prefs: []
  type: TYPE_NORMAL
- en: Another approach that has become fairly common in use is [LIME](https://arxiv.org/pdf/1602.04938.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: This is a post-hoc model — it provides an explanation of a decision after it
    has been made. This means it isn’t a pure ‘glass-box’, transparent model (like
    decision trees) from start to finish.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main strengths of this approach is that it’s model agnostic. It can
    be applied to any model in order to produce explanations for its predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The key concept underlying this approach is perturbing the inputs and watching
    how doing so affects the model’s outputs. This lets us build up a picture of which
    inputs the model is focusing on and using to make its predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, imagine some kind of CNN for image classification. There are
    four main steps to using the LIME model to produce an explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with a normal image and use the black-box model to produce a probability
    distribution over the classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then perturb the input in some way. For images, this could be hiding pixels
    by coloring them grey. Now run these through the black-box model to see the how
    the probabilities for the class it originally predicted changed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use an interpretable (usually linear) model like a decision tree on this dataset
    of perturbations and probabilities to extract the key features which explain the
    changes. The model is locally weighted — meaning that we care more about the perturbations
    that are most similar to the original image we were using.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output the features (in our case, pixels) with the greatest weights as our explanation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer-wise Relevance Propagation (LRP)**'
  prefs: []
  type: TYPE_NORMAL
- en: This [approach](https://arxiv.org/pdf/1604.00825.pdf) uses the idea of relevance
    redistribution and conservation.
  prefs: []
  type: TYPE_NORMAL
- en: We start with an input (say, an image) and its probability of a classification.
    Then, work backwards to redistribute this to all of the inputs (in this case pixels).
  prefs: []
  type: TYPE_NORMAL
- en: The redistribution process is fairly simple from layer to layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25f23922a4cbb7f3daaa2aa12175a637.png)'
  prefs: []
  type: TYPE_IMG
- en: Don’t be scared — this equation is just weighting relevances based on neuron
    activation and weight connnection
  prefs: []
  type: TYPE_NORMAL
- en: 'In the above equation, each term represents the following ideas:'
  prefs: []
  type: TYPE_NORMAL
- en: x_j — the activation value for neuron*j *in layer *l*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: w_j,k — the weighting of the connection between neuron*j* in layer *l* and neuron *k* in
    layer *l + 1*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R_j — Relevance scores for each neuron in layer*l*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R_k — Relevance scores for each neuron in layer*l+1*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The epsilon is just a small value to prevent dividing by zero.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we can work our way backwards to determine the relevance of
    individual inputs. Further, we can sort these in order of relevance. This lets
    us extract a meaningful subset of inputs as our most useful or powerful in making
    a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '**What next?**'
  prefs: []
  type: TYPE_NORMAL
- en: The above methods for producing explainable models are by no means exhaustive.
    They are a sample of some of the approaches researchers have tried using to produce
    interpretable predictions from black-box models.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully this post also sheds some light onto why it is such an important area
    of research. We need to continue researching these methods, and develop new ones,
    in order for machine learning to benefit as many fields as possible — in a safe
    and trustworthy fashion.
  prefs: []
  type: TYPE_NORMAL
- en: If you find yourself wanting more papers and areas to read about, try some of
    the following.
  prefs: []
  type: TYPE_NORMAL
- en: '[DeepMind](http://proceedings.mlr.press/v80/kim18d/kim18d.pdf)’s research on
    Concept Activation Vectors, as well as the [slides](http://s.interpretable.ml/nips_interpretable_ml_2017_victoria_Krakovna.pdf)from
    Victoria Krakovna’s talk at Neural Information Processing Systems (NIPS) conference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A [paper](https://arxiv.org/pdf/1711.07373.pdf) by Dung Huk Park et al. on datasets
    for measuring explainable models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Finale Doshi-Velez](https://www.seas.harvard.edu/directory/finale) and [Been
    Kim](https://beenkim.github.io/)’s [paper](https://arxiv.org/pdf/1702.08608.pdf) on
    the field in general'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artificial intelligence should not become a powerful deity which we follow blindly.
    But neither should we forget about it and the beneficial insight it can have.
    Ideally, we will build flexible and interpretable models that can work in collaboration
    with experts and their domain knowledge to provide a brighter future for everyone.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio**: [Patrick Ferris](https://www.patrickferris.me/) is a nineteen year
    old programmer, blogger and all-round tech enthusiast, and currently Editor in
    Chief of the Hacker''s at Cambridge blog.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.freecodecamp.org/an-introduction-to-explainable-ai-and-why-we-need-it-a326417dd000).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resources:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning
    education](https://www.kdnuggets.com/education/online.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Software for Analytics, Data Science, Data Mining, and Machine Learning](https://www.kdnuggets.com/software/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[XAI – A Data Scientist’s Mouthpiece](https://www.kdnuggets.com/2019/04/xai-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Explainable AI or Halting Faulty Models ahead of Disaster](https://www.kdnuggets.com/2019/03/explainable-ai.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The AI Black Box Explanation Problem](https://www.kdnuggets.com/2019/03/ai-black-box-explanation-problem.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Explainable Forecasting and Nowcasting with State-of-the-art Deep…](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Community for Synthetic Data is Here and This is Why We Need It](https://www.kdnuggets.com/2022/04/community-synthetic-data-need.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Things You Need To Know About Data Management And Why It Matters…](https://www.kdnuggets.com/2022/05/6-things-need-know-data-management-matters-computer-vision.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[We Don''t Need Data Scientists, We Need Data Engineers](https://www.kdnuggets.com/2021/02/dont-need-data-scientists-need-data-engineers.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Closing the Gap Between Human Understanding and Machine Learning:…](https://www.kdnuggets.com/2023/06/closing-gap-human-understanding-machine-learning-explainable-ai-solution.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Why You Need To Learn Python In 2022](https://www.kdnuggets.com/2022/04/need-learn-python-2022.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
