# 这是如何在GPU上加速数据科学的

> 原文：[https://www.kdnuggets.com/2019/07/accelerate-data-science-on-gpu.html](https://www.kdnuggets.com/2019/07/accelerate-data-science-on-gpu.html)

![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)

数据科学家需要计算能力。无论你是在用Pandas处理大型数据集，还是用Numpy运行巨大的矩阵计算，你都需要一台强大的机器，以合理的时间完成任务。

在过去几年里，数据科学家常用的Python库已经很擅长利用CPU的计算能力。

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业轨道。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的IT工作

* * *

Pandas，其底层代码用C编写，可以很好地处理超过100GB的数据集。如果内存不足以容纳这样一个数据集，你可以使用方便的分块功能，一次处理一部分数据。

### GPUs与CPUs：并行处理

面对大量数据，单个CPU已经无法满足需求。

大于100GB的数据集会有大量的数据点，在*数百万*甚至*数十亿*的范围内。如此多的数据点需要处理，无论CPU多么快，都没有足够的核心来进行高效的并行处理。如果你的CPU有20个核心（这将是相当昂贵的CPU），你一次只能处理20个数据点！

CPUs在时钟速度更重要的任务中表现更好——或者你根本没有GPU实现。如果你正在执行的过程有GPU实现，并且该任务可以从并行处理中受益，那么GPU将会更加有效。

![figure-name](../Images/45f79f5c92359a10ccb30f2b8fbbef83.png) 多核系统如何更快地处理数据。对于单核系统（左侧），所有10个任务都集中在一个节点上。而在双核系统（右侧），每个节点处理5个任务，从而加倍处理速度。

深度学习已经充分利用了GPU。深度学习中的许多卷积操作是重复的，因此可以在GPU上大幅加速，甚至达到100倍。

今天的数据科学也不例外，因为许多重复操作在大型数据集上执行，使用像Pandas、Numpy和Scikit-Learn这样的库。这些操作在GPU上实现起来也并不复杂。

最终，有一个解决方案。

### GPU加速与Rapids

[Rapids](https://rapids.ai/?source=post_page---------------------------)是一个旨在通过利用GPU来加速数据科学的套件。它使用低级CUDA代码来实现快速、GPU优化的算法，同时在其上方保留了易用的Python层。

Rapids的美妙之处在于它与数据科学库的无缝集成——像Pandas数据框这样的东西可以很容易地传递到Rapids进行GPU加速。下面的图示说明了Rapids如何在保持易用的顶层界面的同时实现低级加速。

![figure-name](../Images/f511cd54476f8f41e8586d8a442db6e8.png)

Rapids利用了几个Python库：

+   [**cuDF**](https://github.com/rapidsai/cudf?source=post_page---------------------------)——Python GPU数据框。它在数据处理和操作方面几乎可以做Pandas能做的所有事情。

+   [**cuML**](https://github.com/rapidsai/cuml?source=post_page---------------------------)——Python GPU机器学习。它包含了许多Scikit-Learn的机器学习算法，格式非常相似。

+   [**cuGraph**](https://github.com/rapidsai/cuml?source=post_page---------------------------)——Python GPU图处理。它包含了许多常见的图分析算法，包括PageRank和各种相似度指标。

### 如何使用Rapids的教程

**安装**

现在你将看到如何使用Rapids！

要安装它，请访问[网站](https://rapids.ai/start.html?source=post_page---------------------------)，你会看到如何安装Rapids。你可以通过[Conda](https://anaconda.org/anaconda/conda?source=post_page---------------------------)直接在你的机器上安装，或者简单地拉取Docker容器。

在安装时，你可以设置系统规格，比如CUDA版本以及你希望安装的库。例如，我有CUDA 10.0，并且想安装所有的库，所以我的安装命令是：

[PRE0]

一旦那个命令运行完成，你就可以开始进行GPU加速的数据科学了。

**设置我们的数据**

在这个教程中，我们将通过一个修改版的[DBSCAN演示](https://github.com/rapidsai/notebooks/blob/branch-0.8/tutorials/DBSCAN_Demo_Full.ipynb?source=post_page---------------------------)来进行。我将使用[Nvidia数据科学工作站](https://towardsdatascience.com/nvidias-new-data-science-workstation-a-review-and-benchmark-e451db600551?source=post_page---------------------------)来进行测试，该工作站配备了2个GPU。

DBSCAN 是一种基于密度的 [聚类算法](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68?source=post_page---------------------------)，它可以自动分类数据组，无需用户指定组的数量。它在 [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html?source=post_page---------------------------) 中有实现。

我们将从设置所有的导入开始。用于加载数据、可视化数据和应用 ML 模型的库。

[PRE1]

*make_circles* 函数将自动创建一个复杂的数据分布，类似于两个圆圈，我们将对其应用 DBSCAN。

我们先创建一个包含 100,000 个点的数据集，并在图中可视化：

[PRE2]

![figure-name](../Images/12a054c98c39b6df2d4aecb2381e666c.png)

**CPU 上的 DBSCAN**

在 CPU 上运行 DBSCAN 使用 Scikit-Learn 很简单。我们将导入我们的算法并设置一些参数。

[PRE3]

现在我们可以通过 Scikit-Learn 的单个函数调用对圆圈数据应用 DBSCAN。在函数前加上 `%%time` 可以告诉 Jupyter Notebook 测量其运行时间。

[PRE4]

对于这 100,000 个点，运行时间为 8.31 秒。结果图如下所示。

![figure-name](../Images/d3b76a1e32d8538015ea451df118ecf2.png)在 CPU 上使用 Scikit-Learn 运行 DBSCAN 的结果

**GPU 上的 Rapids DBSCAN**

现在让我们用 Rapids 加快速度！

首先，我们将数据转换为 `pandas.DataFrame`，然后用它创建一个 `cudf.DataFrame`。Pandas 数据框可以无缝转换为 cuDF 数据框，而数据格式不会改变。

[PRE5]

然后，我们将从 cuML 导入并初始化一个 *特殊的* DBSCAN 版本，这个版本是 GPU 加速的。cuML 版本的 DBSCAN 函数格式与 Scikit-Learn 完全相同——参数、风格、函数都一样。

[PRE6]

最后，我们可以运行 GPU DBSCAN 的预测函数，同时测量运行时间。

[PRE7]

GPU 版本的运行时间为 4.22 秒——几乎是 2 倍的速度提升。结果图与 CPU 版本完全相同，因为我们使用的是相同的算法。

![figure-name](../Images/d3b76a1e32d8538015ea451df118ecf2.png)在 GPU 上使用 cuML 运行 DBSCAN 的结果

### 使用 Rapids GPU 获得超高速

我们从 Rapids 获得的速度提升取决于我们处理的数据量。一个好的经验法则是，较大的数据集将从 GPU 加速中受益。数据在 CPU 和 GPU 之间传输的开销时间——在处理较大的数据集时，这种开销时间更“值得”。

我们可以用一个简单的例子来说明这一点。

我们将创建一个包含随机数字的 Numpy 数组，并在其上应用 DBSCAN。我们将比较常规 CPU DBSCAN 和 cuML 的 GPU 版本的速度，同时增加和减少数据点的数量，观察对运行时间的影响。

下面的代码展示了这个测试：

[PRE8]

查看下面 Matplotlib 结果的图表：

![figure-name](../Images/6ec9f65915ba982f6379a5af54f22934.png)

使用 GPU 代替 CPU 时，性能提升幅度非常明显。即使在 10,000 点（最左侧），我们也能获得 4.54 倍的加速。在较高端的情况，使用 10,000,000 点时，切换到 GPU 能获得 88.04 倍的加速！

### 喜欢学习吗？

关注我在 [twitter](https://twitter.com/GeorgeSeif94?source=post_page---------------------------) 上，我会发布有关最新和最伟大的 AI、技术和科学的内容！也可以在 [LinkedIn](https://www.linkedin.com/in/georgeseif/?source=post_page---------------------------) 上与我联系！

### 推荐阅读

想了解更多数据科学的内容吗？[***Python 数据科学手册***](https://amzn.to/2GZN4Xv?source=post_page---------------------------)是学习如何用 Python 做*真实*数据科学的最佳资源！

另外，请注意，我通过 Amazon 联盟链接支持本博客，推荐优质书籍，因为分享好书对大家都有帮助！作为 Amazon 联盟会员，我从合格购买中赚取佣金。

**简介: [George Seif](https://towardsdatascience.com/@george.seif94)** 是认证的极客及 AI / 机器学习工程师。

[原文](https://towardsdatascience.com/heres-how-you-can-accelerate-your-data-science-on-gpu-4ecf99db3430)。经许可转载。

**相关：**

+   [Nvidia 新的数据科学工作站——评论与基准测试](/2019/07/nvidia-new-data-science-workstation.html)

+   [分析 Transformer 架构——第二部分：简要描述 Transformer 如何工作](/2019/07/transformer-architecture-part-2.html)

+   [XGBoost 在 GPU 上：解锁机器学习性能与生产力](/2018/12/nvidia-xgboost-gpu-machine-learning-performance-productivity.html)

### 更多相关内容

+   [构建 GPU 机器与使用 GPU 云服务](https://www.kdnuggets.com/building-a-gpu-machine-vs-using-the-gpu-cloud)

+   [通过 Uplimit 的 Metaflow 加速你的机器学习之旅……](https://www.kdnuggets.com/2023/10/uplimit-accelerate-your-machine-learning-journey-metaflow-mastery-course)

+   [如何利用分析加速业务增长？](https://www.kdnuggets.com/2022/12/analytics-accelerate-business-growth.html)

+   [想用你的数据技能解决全球问题？这里有一些信息……](https://www.kdnuggets.com/2022/04/jhu-want-data-skills-solve-global-problems.html)

+   [使用 RAPIDS cuDF 在特征工程中利用 GPU](https://www.kdnuggets.com/2023/06/rapids-cudf-leverage-gpu-feature-engineering.html)

+   [掌握 GPU：Python 中 GPU 加速数据框的初学者指南](https://www.kdnuggets.com/2023/07/mastering-gpus-beginners-guide-gpu-accelerated-dataframes-python.html)
