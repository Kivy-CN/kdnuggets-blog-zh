- en: A Visual Explanation of the Back Propagation Algorithm for Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络反向传播算法的可视化解释
- en: 原文：[https://www.kdnuggets.com/2016/06/visual-explanation-backpropagation-algorithm-neural-networks.html](https://www.kdnuggets.com/2016/06/visual-explanation-backpropagation-algorithm-neural-networks.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/06/visual-explanation-backpropagation-algorithm-neural-networks.html](https://www.kdnuggets.com/2016/06/visual-explanation-backpropagation-algorithm-neural-networks.html)
- en: Let's assume we are really into mountain climbing, and to add a little extra
    challenge, we cover eyes this time so that we can't see where we are and when
    we accomplished our "objective," that is, reaching the top of the mountain.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们真的喜欢登山，并为了增加一点额外的挑战，我们这次蒙上眼睛，以便看不到我们的位置，也不知道何时完成了我们的“目标”，即到达山顶。
- en: 'Since we can''t see the path upfront, we let our intuition guide us: assuming
    that the mountain top is the "highest" point of the mountain, we think that the
    steepest path leads us to the top most efficiently.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们看不到前方的路径，我们让直觉引导我们：假设山顶是山的“最高”点，我们认为最陡峭的路径最有效地将我们引向山顶。
- en: 'We approach this challenge by iteratively "feeling" around you and taking a
    step into the direction of the steepest ascent -- let''s call it "gradient ascent."
    But what do we do if we reach a point where we can''t ascent any further? I.e.,
    each direction leads downwards? At this point, we may have already reached the
    mountain''s top, but we could just have reached a smaller plateau ... we don''t
    know. Essentially, this is just an analogy of gradient ascent optimization (basically
    the counterpart of minimizing a cost function via gradient descent). However,
    this is not specific to backpropagation but just one way to minimize a convex
    cost function (if there is only a global minima) or non-convex cost function (which
    has local minima like the "plateaus" that let us think we reached the mountain''s
    top). Using a little visual aid, we could picture a non-convex cost function with
    only one parameter (where the blue ball is our current location) as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过迭代地“感知”周围并朝着最陡峭的上升方向迈出一步来解决这个挑战——我们称之为“梯度上升”。但如果我们到达一个无法进一步上升的点怎么办？即，每个方向都向下？此时，我们可能已经到达了山顶，但也可能只是到达了一个较小的高原……我们并不知道。从本质上讲，这只是梯度上升优化的一个类比（基本上是通过梯度下降最小化成本函数的对立面）。然而，这并非特指反向传播，而只是最小化一个凸成本函数（如果只有一个全局最小值）或非凸成本函数（具有局部最小值，如“高原”让我们以为达到了山顶）的其中一种方法。借助一点视觉辅助，我们可以将一个只有一个参数的非凸成本函数（蓝色球为我们当前位置）形象化如下：
- en: '[![Nonconvex cost](../Images/b7cb1df71cfdd2e82bb55a3beeeb8c30.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation/nonconvex-cost.png)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[![非凸成本](../Images/b7cb1df71cfdd2e82bb55a3beeeb8c30.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation/nonconvex-cost.png)'
- en: 'Now, backpropagation is just back-propagating the cost over multiple "levels"
    (or layers). E.g., if we have a multi-layer perceptron, we can picture forward
    propagation (passing the input signal through a network while multiplying it by
    the respective weights to compute an output) as follows:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，反向传播就是在多个“层级”上反向传播成本。例如，如果我们有一个多层感知器，我们可以将前向传播（将输入信号通过网络，并通过相应的权重计算输出）形象化如下：
- en: '[![Forward propagation](../Images/9c05363841f1cd37e9b476c669cb53fe.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation/forward-propagation.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[![前向传播](../Images/9c05363841f1cd37e9b476c669cb53fe.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation/forward-propagation.png)'
- en: 'And in backpropagation, we "simply" backpropagate the error (the "cost" that
    we compute by comparing the calculated output and the known, correct target output,
    which we then use to update the model parameters):'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播中，我们“简单地”反向传播误差（我们通过比较计算出的输出和已知的正确目标输出来计算的“成本”，然后用来更新模型参数）：
- en: '[![Backpropagation](../Images/026031521802271f37a0fdb30bc165f3.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation/backpropagation.png)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[![反向传播](../Images/026031521802271f37a0fdb30bc165f3.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation/backpropagation.png)'
- en: It may be some time ago since pre-calc, but it's essentially all based on the
    simple chain-rule that we use for nested functions
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是在预备微积分之后很久的事了，但它本质上都是基于我们用于嵌套函数的简单链式法则
- en: '[![Chain rule](../Images/e05c3db54124e253d0e75c7cff434527.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation/chain_rule_1.png)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[![链式法则](../Images/e05c3db54124e253d0e75c7cff434527.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation/chain_rule_1.png)'
- en: '[![Chain rule](../Images/f9c99e2855716c634e5701100e8ad8f9.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation/chain_rule_2.png)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[![链式法则](../Images/f9c99e2855716c634e5701100e8ad8f9.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation/chain_rule_2.png)'
- en: Instead of doing this "manually" we can use computational tools (called "automatic
    differentiation"), and backpropagation is basically the "reverse" mode of this
    auto-differentiation. Why reverse and not forward? Because it is computationally
    cheaper! If we'd do it forward-wise, we'd successively multiply large matrices
    for each layer until we multiply a large matrix by a vector in the output layer.
    However, if we start backwards, that is, we start by multiplying a matrix by a
    vector, we get another vector, and so forth. So, I'd say the beauty in backpropagation
    is that we are doing more efficient matrix-vector multiplications instead of matrix-matrix
    multiplications.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与其“手动”完成这些工作，不如使用计算工具（称为“自动微分”），而反向传播基本上是这种自动微分的“反向”模式。为什么是反向而不是前向？因为计算上更便宜！如果我们使用前向方式，我们将逐层相乘大矩阵，直到将一个大矩阵与输出层中的一个向量相乘。然而，如果我们从反向开始，也就是从将一个矩阵与一个向量相乘开始，我们得到另一个向量，以此类推。所以，我认为反向传播的美在于我们进行的是更高效的矩阵-向量乘法，而不是矩阵-矩阵乘法。
- en: '**Bio: [Sebastian Raschka](https://twitter.com/rasbt)** is a ''Data Scientist''
    and Machine Learning enthusiast with a big passion for Python & open source. Author
    of ''[Python Machine Learning](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning)''.
    Michigan State University.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[Sebastian Raschka](https://twitter.com/rasbt)** 是一名数据科学家和机器学习爱好者，对 Python
    和开源有着极大的热情。《[Python 机器学习](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning)》的作者。密歇根州立大学。'
- en: '[Original](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation.md).
    Reposted with permission.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation.md)。经许可转载。'
- en: '**Related:**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[When Does Deep Learning Work Better Than SVMs or Random Forests?](/2016/04/deep-learning-vs-svm-random-forest.html)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习何时比 SVM 或随机森林效果更好？](/2016/04/deep-learning-vs-svm-random-forest.html)'
- en: '[The Development of Classification as a Learning Machine](/2016/04/development-classification-learning-machine.html)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分类发展作为一种学习机器](/2016/04/development-classification-learning-machine.html)'
- en: '[Why Implement Machine Learning Algorithms From Scratch?](/2016/05/implement-machine-learning-algorithms-scratch.html)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么从头实现机器学习算法？](/2016/05/implement-machine-learning-algorithms-scratch.html)'
- en: '* * *'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速入门网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行 IT 工作'
- en: '* * *'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解更多相关内容
- en: '[Linear vs Logistic Regression: A Succinct Explanation](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性回归与逻辑回归：简明解释](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
- en: '[KDnuggets News 22:n12, March 23: Best Data Science Books for…](https://www.kdnuggets.com/2022/n12.html)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻 22:n12，3月23日：最佳数据科学书籍…](https://www.kdnuggets.com/2022/n12.html)'
- en: '[An Intuitive Explanation of Collaborative Filtering](https://www.kdnuggets.com/2022/09/intuitive-explanation-collaborative-filtering.html)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[协同过滤的直观解释](https://www.kdnuggets.com/2022/09/intuitive-explanation-collaborative-filtering.html)'
- en: '[Thought Propagation: An Analogical Approach to Complex Reasoning…](https://www.kdnuggets.com/thought-propagation-an-analogical-approach-to-complex-reasoning-with-large-language-models)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[思想传播：一种复杂推理的类比方法……](https://www.kdnuggets.com/thought-propagation-an-analogical-approach-to-complex-reasoning-with-large-language-models)'
- en: '[10 Simple Things to Try Before Neural Networks](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在使用神经网络之前尝试的10个简单方法](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[可解释的 PyTorch 神经网络](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
