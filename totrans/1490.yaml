- en: 7 Steps to Mastering Data Preparation for Machine Learning with Python — 2019
    Edition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/06/7-steps-mastering-data-preparation-python.html](https://www.kdnuggets.com/2019/06/7-steps-mastering-data-preparation-python.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: Interested in mastering data preparation with Python?
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation, cleaning, pre-processing, cleansing, wrangling. Whatever term
    you choose, they refer to a roughly related set of pre-modeling data activities
    in the machine learning, data mining, and data science communities.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Wikipedia defines [data cleansing](https://en.wikipedia.org/wiki/Data_cleansing)
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '...is the process of detecting and correcting (or removing) corrupt or inaccurate
    records from a record set, table, or database and refers to identifying incomplete,
    incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying,
    or deleting the dirty or coarse data. Data cleansing may be performed interactively
    with data wrangling tools, or as batch processing through scripting.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Data wrangling](https://en.wikipedia.org/wiki/Data_wrangling), for comparison,
    is defined by Wikipedia as:'
  prefs: []
  type: TYPE_NORMAL
- en: '...the process of manually converting or mapping data from one "raw" form into
    another format that allows for more convenient consumption of the data with the
    help of semi-automated tools. This may include further munging, data visualization,
    data aggregation, training a statistical model, as well as many other potential
    uses. Data munging as a process typically follows a set of general steps which
    begin with extracting the data in a raw form from the data source, "munging" the
    raw data using algorithms (e.g. sorting) or parsing the data into predefined data
    structures, and finally depositing the resulting content into a data sink for
    storage and future use.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Figure](../Images/2ebc5d14359a29620a0fe3466edd6938.png)'
  prefs: []
  type: TYPE_IMG
- en: Data preparation in both the [KDD Process](http://www2.cs.uregina.ca/~dbd/cs831/notes/kdd/1_kdd.html)
    (left) and the [CRISP-DM model](https://en.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining)
    (right).
  prefs: []
  type: TYPE_NORMAL
- en: I would say that it is "identifying incomplete, incorrect, inaccurate or irrelevant
    parts of the data and then replacing, modifying, or deleting the dirty or coarse
    data" in the context of "mapping data from one 'raw' form into another..." all
    the way up to "training a statistical model" which I like to think of data preparation
    as encompassing, or "everything from data sourcing right up to, but not including,
    model building." That is the vague-yet-oddly-precise definition we'll move forward
    with.
  prefs: []
  type: TYPE_NORMAL
- en: This article will [update a previous version from 2017](/2017/06/7-steps-mastering-data-preparation-python.html),
    in order to freshen up some of the materials throughout. I have tried to select
    a quality tutorial or two, along with video when appropriate, as a good representation
    of the particular lesson in each step.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that the article covers one particular set of data preparation
    techniques, and additional, or completely different, techniques may be used in
    a given circumstance, based on requirements. You should find that the prescription
    held herein is one which is both orthodox and general in approach.
  prefs: []
  type: TYPE_NORMAL
- en: Grab a snack and sit back, as we learn to master data preparation with Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Preparing for the Preparation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, let''s stress what everyone else has already told you: it could be argued
    that this data preparation phase is not a preliminary step prior to a machine
    learning task, but actually an integral component (or even a majority) of what
    a typical machine learning task would encompass. For our purposes, however, we
    will separate the data preparation from the modeling as its own regimen.'
  prefs: []
  type: TYPE_NORMAL
- en: As Python is the ecosystem in which we will be immersed, the following resources
    are a good jumping off point to ensure appropriate familiarity.
  prefs: []
  type: TYPE_NORMAL
- en: '**[10 Minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[Matplotlib Beginner''s Guide](https://matplotlib.org/users/beginner.html)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[Official seaborn tutorial](https://seaborn.pydata.org/tutorial.html)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preparation can be seen in the CRISP-DM model (though it can be reasonably
    argued that "data understanding" falls within our definition as well). We can
    also equate our data preparation with the framework of the KDD Process — specifically
    the first 3 major steps — which are **selection**, **preprocessing**, and **transformation**.
    We can break these down into finer granularity, but at a macro level, these steps
    of the KDD Process encompass what data wrangling is.
  prefs: []
  type: TYPE_NORMAL
- en: 'While readers should be able to follow this guide with few additional resources,
    for those interested in a more holistic take on Pandas (likely the most important
    data preparation library in the Python ecosystem), helpful information can be
    found in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Intro to pandas data structures](http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/)**,
    by Greg Reda'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[Modern Pandas (in 7 parts)](http://tomaugspurger.github.io/modern-1-intro.html)**,
    by Tom Augspurger'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, for some feedback on the data preparation process from 3 insiders
    — Sebastian Raschka, Clare Bernard, and Joe Boutros — read this interview on **[Data
    Preparation Tips, Tricks, and Tools: An Interview with the Insiders](/2016/10/data-preparation-tips-tricks-tools.html)**
    before moving on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Exploratory Data Analysis'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Exploratory data analysis](https://en.wikipedia.org/wiki/Exploratory_data_analysis)
    (EDA) is an integral aspect of any greater data analysis, data science, or machine
    learning project. Understanding data before working with it isn''t just a pretty
    good idea, it is a priority if you plan on accomplishing anything of consequence.
    [Andrew Andrade](https://datascienceguide.github.io/exploratory-data-analysis)
    concisely describes EDA as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of EDA is to use summary statistics and visualizations to better
    understand data, and find clues about the tendencies of the data, its quality
    and to formulate assumptions and the hypothesis of our analysis.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The basic gist is that we need to know the makeup of our data before we can
    effectively select predictive algorithms or map out the remaining steps of our
    data preparation. Throwing our dataset at the hottest algorithm and hoping for
    the best is not a strategy.
  prefs: []
  type: TYPE_NORMAL
- en: To gain some intuition, watch this video by Prof. Patrick Meyer of the University
    of Virginia which provides an overview of EDA.
  prefs: []
  type: TYPE_NORMAL
- en: Then read Andrade's article on **[Exploratory data analysis](https://datascienceguide.github.io/exploratory-data-analysis)**,
    which provides additional details on how to go about EDA, and what its practical
    benefits are.
  prefs: []
  type: TYPE_NORMAL
- en: For a Python based approach tutorial on EDA, check out the article **[Exploratory
    Data Analysis (EDA) and Data Visualization with Python](https://kite.com/blog/python/data-analysis-visualization-python)**
    by Vigneshwer Dhinakaran, which actually goes a bit beyond traditional EDA in
    my view, and will introduce you to some of the additional concepts covered later
    in this article.
  prefs: []
  type: TYPE_NORMAL
- en: A library which dramatically shortens the code you need to write to perform
    EDA is **[Pandas Profiling](https://github.com/pandas-profiling/pandas-profiling)**,
    which creates HTML reports from Pandas DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Generates profile reports from a pandas `DataFrame`. The pandas `df.describe()`
    function is great but a little basic for serious exploratory data analysis. `pandas_profiling`
    extends the pandas DataFrame with `df.profile_report()` for quick data analysis.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'You can run Pandas Profiling interactively in Jupyter notebooks with a single
    line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`df.profile_report()`'
  prefs: []
  type: TYPE_NORMAL
- en: Read the project's GitHub Readme for more information, and give it a try for
    yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Missing Values'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many strategies for dealing with missing data, none of which are applicable
    universally. Some people will say "never use instances which include empty values."
    Others will argue "never use an attribute's mean value to replace missing values."
    Conversely, you may hear more complex methods endorsed wholesale, such as "only
    first clustering a dataset into the number of known classes and then using intra-cluster
    regression to calculate missing values is valid."
  prefs: []
  type: TYPE_NORMAL
- en: Listen to none of this. "Never" and "only" and other inflexible assertions hold
    no value in the nuanced world of data finessing; different types of data and processes
    suggest different best practices for dealing with missing values. However, since
    this type of knowledge is both experience and domain based, we will focus on the
    more basic strategies which can be employed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some commonly used methods for dealing with missing values include:'
  prefs: []
  type: TYPE_NORMAL
- en: dropping instances with missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: dropping attributes with missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: imputing the attribute { mean | median | mode } for all missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: imputing the attribute missing values via linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Combination strategies may also be employed: drop any instances with more than
    2 missing values and use the mean attribute value imputation those which remain.
    Clearly the type of modeling methods being employed will have an effect on your
    decision — for example, decision trees are not amenable to missing values. Additionally,
    you could technically entertain any statistical method you could think of for
    determining missing values from the dataset, but the listed approaches are tried,
    tested, and commonly used.'
  prefs: []
  type: TYPE_NORMAL
- en: Since we are focusing on the Python ecosystem, from the Pandas user guide you
    can read more about **[Working with missing data](http://pandas.pydata.org/pandas-docs/stable/missing_data.html)**,
    as well as reference the API documentation on the **[Pandas `DataFrame` object's
    `fillna()` function](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html)**.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a lot of ways to accomplish filling missing values in a Pandas DataFrame.
    Here are a few basic examples:'
  prefs: []
  type: TYPE_NORMAL
- en: You can also watch this video from codebasics on handling missing values with
    Pandas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Outliers'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is not a tutorial on drafting a strategy to deal with outliers in your
    data when modeling; there are times when including outliers in modeling is appropriate,
    and there are times when they are not (regardless of what anyone tries to tell
    you). This is situation-dependent, and no one can make sweeping assertions as
    to whether your situation belongs in column A or column B.
  prefs: []
  type: TYPE_NORMAL
- en: 'Outliers can be the result of poor data collection, or they can be genuinely
    good, anomalous data. These are 2 different scenarios, and must be approached
    differently, and so no "one size fits all" advice is applicable here, similar
    to that of dealing with missing values. A particularly good point of insights
    from the Analysis Factor article from above is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: One option is to try a transformation. Square root and log transformations both
    pull in high numbers. This can make assumptions work better if the outlier is
    a dependent variable and can reduce the impact of a single point if the outlier
    is an independent variable.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Read this discussion, **[Outliers: To Drop or Not to Drop](http://www.theanalysisfactor.com/outliers-to-drop-or-not-to-drop/)**
    on The Analysis Factor, and the discussion **[Is it OK to remove outliers from
    data?](https://stats.stackexchange.com/questions/200534/is-it-ok-to-remove-outliers-from-data/200923)**
    on Stack Exchange, for further insight into this issue.'
  prefs: []
  type: TYPE_NORMAL
- en: You can have a look at **[Removing Outliers Using Standard Deviation with Python](https://www.kdnuggets.com/2017/02/removing-outliers-standard-deviation-python.html)**
    as a simple example of removing outliers with Python. Then read this Stack Overflow
    discussion, **[Remove Outliers in Pandas DataFrame using Percentiles](https://stackoverflow.com/questions/35827863/remove-outliers-in-pandas-dataframe-using-percentiles)**.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, the decision as to whether or not to remove outliers will be task-dependent,
    and the reasoning and decision will be much more of a concern than the technical
    approach to doing so.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: Imbalanced Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So, what if your otherwise robust dataset is made up of 2 classes: one which
    includes 95 percent of the instances, and the other which includes a mere 5 percent?
    Or worse, 99.8 vs 0.2 percent?'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recognizing and dealing with imbalance is important](../Images/c9cbb484ef13d59bff657b921d1b6624.png)'
  prefs: []
  type: TYPE_IMG
- en: If so, your dataset is imbalanced, at least as far as the classes are concerned.
    This can be problematic, in ways which I'm sure do not need to be pointed out.
    But no need to to toss the data to the side yet; there are, of course, strategies
    for dealing with this.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good explanation of why we can run into imbalanced data, and why we can do
    so in some domains much more frequently than in others (from 7 Techniques to Handle
    Imbalanced Data, below):'
  prefs: []
  type: TYPE_NORMAL
- en: Data used in these areas often have less than 1% of rare, but “interesting”
    events (e.g. fraudsters using credit cards, user clicking advertisement or corrupted
    server scanning its network). However, most machine learning algorithms do not
    work very well with imbalanced datasets. The following seven techniques can help
    you, to train a classifier to detect the abnormal class.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note that, while this may not genuinely be a data preparation task, such a dataset
    characteristic will make itself known early in the data preparation stage (the
    importance of EDA), and the validity of such data can certainly be assessed preliminarily
    during this preparation stage.
  prefs: []
  type: TYPE_NORMAL
- en: Tom Fawcett discusses this in his article **[Learning from Imbalanced Classes](/2016/08/learning-from-imbalanced-classes.html)**.
    Read it to get a better idea of the issue.
  prefs: []
  type: TYPE_NORMAL
- en: Then read this article, **[7 Techniques to Handle Imbalanced Data](/2017/06/7-techniques-handle-imbalanced-data.html)**
    by Ye Wu & Rick Radewagen, which covers techniques for handling class imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 6: Data Transformations'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Wikipedia defines [data transformation](https://en.wikipedia.org/wiki/Data_transformation_(statistics))
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: In statistics, data transformation is the application of a deterministic mathematical
    function to each point in a data set — that is, each data point zi is replaced
    with the transformed value *y[i]* = *f(z[i])*, where *f* is a function. Transforms
    are usually applied so that the data appear to more closely meet the assumptions
    of a statistical inference procedure that is to be applied, or to improve the
    interpretability or appearance of graphs.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Transforming data is one of the most important aspects of data preparation,
    requiring more finesse than some others. When missing values manifest themselves
    in data, they are generally easy to find, and can be dealt with by one of the
    common methods outlined above — or by more complex measures gained from insight
    over time in a domain. However, when and if data transformations are required
    is often not as easily identifiable, to say nothing of the type of transformation
    required.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at a few specific transformations in order to get a better handle
    on them.
  prefs: []
  type: TYPE_NORMAL
- en: First, this overview of **[Preprocessing data](http://scikit-learn.org/stable/modules/preprocessing.html)**
    from Scikit-learn's documentation gives some rationale for some of the most important
    preprocessing transformations, namely standardization, normalization, binarization,
    and a few others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Standardization and normalization are a pair of often employed data transformations
    in machine learning projects. Both are data scaling methods: standardization refers
    to scaling the data to have a mean of 0 and a standard deviation of 1; normalization
    refers to the scaling the data values to fit into a predetermined range, generally
    between 0 and 1\. Read this article by Shay Geller, **[Normalization vs Standardization — Quantitative
    analysis](https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf)**,
    to understand how the transformations work, how to perform them in the Python
    ecosystem, and gain some insight into best practice from the author.'
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding is a method for transforming categorical features to a format
    which will better work for classification and regression. Watch this video on
    one-hot encoding to gain a better understanding of how it does so, and see how
    it can be accomplished with Python tools.
  prefs: []
  type: TYPE_NORMAL
- en: Logarithmic distribution transformation is useful for transforming non-linear
    models into linear models and working with skewed data. Read this Stack Exchange
    discussion, **[When (and why) should you take the log of a distribution (of numbers)?](https://stats.stackexchange.com/questions/18844/when-and-why-should-you-take-the-log-of-a-distribution-of-numbers)**,
    for the intuition. You can also have a look at this short tutorial from Data Science
    Made Simple, **[Log and natural Logarithmic value of a column in pandas python](http://www.datasciencemadesimple.com/log-natural-logarithmic-value-column-pandas-python-2/)**,
    for a quick overview of using Numpy to accomplish the transformation in Python.
  prefs: []
  type: TYPE_NORMAL
- en: This short tutorial from Ontario Tech University, **[Introduction to Exponential
    and Logarithmic Functions](https://nool.uoit.ca/mathematics/exponential-logarithmic-functions/basics/index.php)**,
    takes a mathematical approach to explaining logarithmic and exponential transformations,
    along with visualizations, and can add to your intuition of what is happening
    to underlying data distributions when these transformations are performed. There
    are 3 pages in the tutorial, with the third having 2 videos which help drive the
    point home.
  prefs: []
  type: TYPE_NORMAL
- en: There are numerous additional standard data transformations which are regularly
    employed, depending on the data and your requirements. Experience with data preprocessing
    and preparation should provide intuition on what types of transformations are
    required in which circumstance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 7: Finishing Touches & Moving Ahead'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Alright. Your data is "clean." But what do you do with it?
  prefs: []
  type: TYPE_NORMAL
- en: If you want to go right to feeding your data into a machine learning algorithm
    in order to attempt building a model, you probably need your data in a more appropriate
    representation. In the Python ecosystem, that would generally be a Numpy ndarray
    (or matrix). This Stack Overflow discussion, **[Turning a Pandas Dataframe to
    an array and evaluate Multiple Linear Regression Model](https://stackoverflow.com/questions/28334091/turning-a-pandas-dataframe-to-an-array-and-evaluate-multiple-linear-regression-m)**,
    can give some preliminary ideas on getting there.
  prefs: []
  type: TYPE_NORMAL
- en: '![Very simple data preparation process](../Images/2b1892e803954b3888cf48f32e0c71b7.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Note that most of our data preparation was performed in a combination of Pandas
    and Numpy in the preceding text; however, Pandas sits atop Numpy, and so learning
    how to manipulate the underlying Numpy matrix directly is a useful skill. [Learn
    a bit more about that here](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html).*'
  prefs: []
  type: TYPE_NORMAL
- en: What if you aren't quite ready to model the data yet, and instead want to store
    your clean Pandas DataFrame for later use? **[Quick HDF5 with Pandas](https://dzone.com/articles/quick-hdf5-pandas)**
    by Giuseppe Vettigli will show you one such way to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have clean data in a proper representation for machine learning in
    Python, why not get right to the machine learning? First you will want to read
    **[7 Steps to Mastering Basic Machine Learning with Python — 2019 Edition](/2019/01/7-steps-mastering-basic-machine-learning-python.html)**
    to gain an introductory understanding of machine learning in the Python ecosystem.
    Follow that up with **[7 Steps to Mastering Intermediate Machine Learning with
    Python — 2019 Edition](/2019/06/7-steps-mastering-intermediate-machine-learning-python.html)**
    to enhance your knowledge (and be on the look out for an "advanced" installment
    as well).
  prefs: []
  type: TYPE_NORMAL
- en: 'For some differing viewpoints on data preparation, have a look at these:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Tidying Data in Python](/2017/01/tidying-data-python.html)**, by Jean-Nicholas
    Hould'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[Doing Data Science: A Kaggle Walkthrough Part 3 – Cleaning Data](/2016/06/doing-data-science-kaggle-walkthrough-data-cleaning.html)**,
    by Brett Romero'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[Machine Learning Workflows in Python from Scratch Part 1: Data Preparation](/2017/05/machine-learning-workflows-python-scratch-part-1.html)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that this entire discussion is also fully and intentionally skipping any
    mention of feature selection for a specific reason: it deserves far more than
    a simple few sentences in this much more broad discussion. Be on the lookout for
    a similar guide for feature selection.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[7 Steps to Mastering Basic Machine Learning with Python — 2019 Edition](/2019/01/7-steps-mastering-basic-machine-learning-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Steps to Mastering Intermediate Machine Learning with Python — 2019 Edition](/2019/06/7-steps-mastering-intermediate-machine-learning-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Steps to Mastering SQL for Data Science — 2019 Edition](/2019/05/7-steps-mastering-sql-data-science-2019-edition.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
