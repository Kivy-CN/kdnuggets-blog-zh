- en: Understanding How Neural Networks Think
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解神经网络的思维方式
- en: 原文：[https://www.kdnuggets.com/2020/07/understanding-neural-networks-think.html](https://www.kdnuggets.com/2020/07/understanding-neural-networks-think.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/07/understanding-neural-networks-think.html](https://www.kdnuggets.com/2020/07/understanding-neural-networks-think.html)
- en: '[comments](#comments)![Figure](../Images/0efaa7a2bc2407cb8427e52fe4864cf4.png)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)![图](../Images/0efaa7a2bc2407cb8427e52fe4864cf4.png)'
- en: Source: [https://distill.pub/2018/building-blocks/](https://distill.pub/2018/building-blocks/)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://distill.pub/2018/building-blocks/](https://distill.pub/2018/building-blocks/)
- en: '* * *'
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '* * *'
- en: ''
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_BQ
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前 3 个课程推荐
- en: ''
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道。'
- en: ''
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: ''
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行 IT 工作'
- en: ''
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '* * *'
- en: ''
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'I recently started a new newsletter focus on AI education. TheSequence is a
    no-BS (meaning no hype, no news etc) AI-focused newsletter that takes 5 minutes
    to read. The goal is to keep you up to date with machine learning projects, research
    papers and concepts. Please give it a try by subscribing below:'
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我最近开始了一份专注于 AI 教育的新通讯。《TheSequence》是一份无废话（即无炒作、无新闻等）的 AI 重点通讯，阅读时间仅需 5 分钟。目标是让你了解机器学习项目、研究论文和概念。请通过下面的订阅尝试一下：
- en: '[![Image](../Images/f2aed90f956dea213be7c9bbf9cd7072.png)](https://thesequence.substack.com/)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[![图片](../Images/f2aed90f956dea213be7c9bbf9cd7072.png)](https://thesequence.substack.com/)'
- en: One of the challenging elements of any deep learning solution is to understand
    the knowledge and decisions made by deep neural networks. While the interpretation
    of decisions made by a neural networks has always been difficult, the issue has
    become a nightmare with the raise of deep learning and the proliferation of large
    scale neural networks that operate with multi-dimensional datasets. Not surprisingly,
    the interpretation of neural networks has become one of the most active areas
    of research in the deep learning ecosystem.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 任何深度学习解决方案中的一个挑战性元素是理解深度神经网络做出的知识和决策。尽管神经网络决策的解释一直很困难，但随着深度学习的兴起以及操作多维数据集的大规模神经网络的普及，这一问题变得更加棘手。毫不奇怪，神经网络的解释已成为深度学习生态系统中最活跃的研究领域之一。
- en: 'Try to imagine a large neural network with hundreds of millions of neurons
    that is performing a deep learning task such as image recognition. Typically,
    you would like to understand how the network arrives to specific decisions. Most
    of the current research has focused on detecting what neurons in the network have
    been activated. Knowing that neuron-12345 fired five times is relevant but not
    incredibly useful in the scale of the entire network. The research about understanding
    decisions in neural networks has focused on three main areas: feature visualization,
    attribution and dimensionality reduction. Google, in particular, has done a lot
    of work in the feature visualization space publishing some [remarkable research
    and tools](https://distill.pub/2017/feature-visualization/). Over a year ago,
    Google researchers published a paper titled[ “The Building Blocks of Interpretability” ](https://distill.pub/2018/building-blocks/)that
    became a seminal paper in the are of machine learning interpretability. The paper
    proposes some new ideas to understand how deep neural networks make decisions.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试想象一个拥有数亿个神经元的大型神经网络，它正在执行诸如图像识别之类的深度学习任务。通常，你希望了解网络如何得出特定的决策。当前大多数研究集中在检测网络中哪些神经元被激活上。知道神经元-12345
    激活了五次是相关的，但在整个网络的规模中并不是特别有用。关于理解神经网络决策的研究主要集中在三个领域：特征可视化、归因和降维。Google，特别是在特征可视化领域做了大量工作，发布了一些[杰出的研究和工具](https://distill.pub/2017/feature-visualization/)。一年多前，Google
    研究人员发布了一篇题为[“可解释性的构建块”](https://distill.pub/2018/building-blocks/)的论文，成为了机器学习可解释性领域的开创性论文。该论文提出了一些新思路，以理解深度神经网络如何做出决策。
- en: The main insight of Google’s research is to not see the different interpretability
    techniques in isolation but as composable building blocks of larger models that
    help understand the behavior of neural networks. For instance, feature visualization
    is a very effective technique to understand the information processed by individual
    neurons but fails to correlate that insight with the overall decision made by
    the neural network. Attribution is a more solid technique to explain the relationship
    between different neurons but not so much when comes to understand the decision
    made by individual neurons. Combining those building blocks, Google has created
    an interpretability models that does not only *explains what* a neural network
    detects, but it does answer *how *the network assembles these individual pieces
    to arrive at later decisions, and *why* these decisions were made.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Google 研究的主要见解是不要将不同的可解释性技术孤立看待，而是将它们视为理解神经网络行为的大型模型的可组合构建块。例如，特征可视化是一种非常有效的技术，可以理解单个神经元处理的信息，但不能将该洞察力与神经网络做出的整体决策关联起来。归因是解释不同神经元之间关系的更可靠技术，但在理解单个神经元做出的决策时则不那么有效。通过结合这些构建块，Google
    创建了一个不仅仅*解释了神经网络检测了什么*的可解释性模型，还回答了*网络如何将这些单独的部分组装起来*以得出后续决策，以及*为何*做出这些决策。
- en: 'How does the new Google model for interpretability works specifically? Well,
    the main innovation, in my opinion, is that it analyzes the decisions made by
    different components of a neural network at different levels: individual neurons,
    connected groups of neurons and complete layers. Google also uses a novel research
    technique called matrix factorization to analyze the impact that arbitrary groups
    of neurons can have in the final decision.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Google 的新可解释性模型具体是如何工作的？在我看来，主要的创新点在于，它分析了神经网络不同组件在不同层次上的决策：个别神经元、连接的神经元组和完整的层。Google
    还使用了一种新颖的研究技术——矩阵分解，以分析任意神经元组对最终决策的影响。
- en: '![Figure](../Images/99dba3fddd2ba92aed2c6e61f61314a0.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/99dba3fddd2ba92aed2c6e61f61314a0.png)'
- en: Source: [https://distill.pub/2018/building-blocks/](https://distill.pub/2018/building-blocks/)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [https://distill.pub/2018/building-blocks/](https://distill.pub/2018/building-blocks/)'
- en: A good way to think about Google’s blocks of interpretability is as a model
    that detects insights about the decisions of a neural network at different levels
    of abstraction from the basic computation graph to the final decision.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Google 的可解释性模块视为一个模型，该模型检测神经网络在不同抽象层次上，从基础计算图到最终决策的洞察力，是一种很好的思考方式。
- en: '![Figure](../Images/ab0251e72755e0e8defcd6c54b948099.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/ab0251e72755e0e8defcd6c54b948099.png)'
- en: Source: [https://distill.pub/2018/building-blocks/](https://distill.pub/2018/building-blocks/)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [https://distill.pub/2018/building-blocks/](https://distill.pub/2018/building-blocks/)'
- en: Google research of deep neural network interpretability is not only a theoretical
    exercise. The research group accompanied the paper with the release of [Lucid](https://github.com/tensorflow/lucid),
    a neural network visualization library that allow developers to make the sort
    lucid feature visualizations that illustrate the decisions made by individual
    segments of a neural network. Google also released [colab notebooks](https://github.com/tensorflow/lucid#notebooks).
    These notebooks make it extremely easy to use Lucid to create Lucid visualization
    in an interactive environment.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌对深度神经网络可解释性的研究不仅仅是一个理论练习。研究小组在发布论文时，还推出了[Lucid](https://github.com/tensorflow/lucid)，这是一个神经网络可视化库，允许开发者创建清晰的特征可视化，以展示神经网络各个部分所做出的决策。谷歌还发布了[colab
    notebooks](https://github.com/tensorflow/lucid#notebooks)。这些笔记本使得在交互环境中使用 Lucid
    创建 Lucid 可视化变得非常容易。
- en: '[Original](https://medium.com/ai-in-plain-english/understanding-how-neural-networks-think-ca7d9c1f079).
    Reposted with permission.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/ai-in-plain-english/understanding-how-neural-networks-think-ca7d9c1f079)。经授权转载。'
- en: '**Related:**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Learning by Forgetting: Deep Neural Networks and the Jennifer Aniston Neuron](/2020/06/learning-forgetting-deep-neural-networks-jennifer-aniston.html)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过遗忘学习：深度神经网络与詹妮弗·安妮斯顿神经元](/2020/06/learning-forgetting-deep-neural-networks-jennifer-aniston.html)'
- en: '[Uber’s Ludwig is an Open Source Framework for Low-Code Machine Learning](/2020/06/uber-ludwig-open-source-framework-machine-learning.html)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Uber 的 Ludwig 是一个用于低代码机器学习的开源框架](/2020/06/uber-ludwig-open-source-framework-machine-learning.html)'
- en: '[Google Unveils TAPAS, a BERT-Based Neural Network for Querying Tables Using
    Natural Language](/2020/05/google-tapas-bert-neural-network-querying-natural-language.html)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[谷歌推出 TAPAS：一种基于 BERT 的神经网络，用于使用自然语言查询表格](/2020/05/google-tapas-bert-neural-network-querying-natural-language.html)'
- en: More On This Topic
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[10 Simple Things to Try Before Neural Networks](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在使用神经网络之前尝试的 10 个简单方法](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 PyTorch 的可解释神经网络](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
- en: '[Deep Neural Networks Don''t Lead Us Towards AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度神经网络并未将我们引向通用人工智能（AGI）](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
- en: '[Explainable Forecasting and Nowcasting with State-of-the-art Deep…](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[最先进的深度学习模型的可解释预测与即时预测](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
- en: '[Image Classification with Convolutional Neural Networks (CNNs)](https://www.kdnuggets.com/2022/05/image-classification-convolutional-neural-networks-cnns.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[卷积神经网络（CNN）图像分类](https://www.kdnuggets.com/2022/05/image-classification-convolutional-neural-networks-cnns.html)'
- en: '[A Comprehensive Survey on Trustworthy Graph Neural Networks:…](https://www.kdnuggets.com/2022/05/comprehensive-survey-trustworthy-graph-neural-networks-privacy-robustness-fairness-explainability.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于值得信赖的图神经网络的综合调查：…](https://www.kdnuggets.com/2022/05/comprehensive-survey-trustworthy-graph-neural-networks-privacy-robustness-fairness-explainability.html)'
