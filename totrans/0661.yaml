- en: NLP and Computer Vision Integrated
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/06/nlp-computer-vision-integrated.html](https://www.kdnuggets.com/2019/06/nlp-computer-vision-integrated.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By [Sciforce](https://sciforce.solutions)**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d61fa59424baa0cb2ac01f824879f7b1.png)'
  prefs: []
  type: TYPE_IMG
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Integration and interdisciplinarity are the cornerstones of modern science and
    industry. One example of recent attempts to combine everything is the integration
    of computer vision and natural language processing (NLP). Both these fields are
    one of the most actively developing machine learning research areas. Yet, until
    recently, they have been treated as separate areas without many ways to benefit
    from each other. It is now, with the expansion of multimedia, researchers have
    started exploring the possibilities of applying both approaches to achieve one
    result.
  prefs: []
  type: TYPE_NORMAL
- en: '**Multimodal world and semiotics**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most natural way for humans is to extract and analyze information from diverse
    sources. This conforms to the theory of semiotics (Greenlee 1978) — the study
    of the relations between signs and their meanings at different levels. Semiotics
    studies the relationship between signs and meaning, the formal relations between
    signs (roughly equivalent to syntax) and the way humans interpret signs depending
    on the context (pragmatics in linguistic theory). If we consider purely visual
    signs, then this leads to the conclusion that semiotics can also be approached
    by computer vision, extracting interesting signs for natural language processing
    to realize the corresponding meanings.
  prefs: []
  type: TYPE_NORMAL
- en: '**Computer Vision and its relation to NLP**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Malik summarizes Computer Vision tasks as the 3Rs (Malik et al. 2016): reconstruction,
    recognition, and reorganization.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reconstruction** refers to the estimation of a 3D scene that gave rise to
    a particular visual image by incorporating information from multiple views, shading,
    texture, or direct depth sensors. The process results in a 3D model, such as point
    clouds or depth images.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recognition** involves assigning labels to objects in the image. For 2D objects,
    examples of recognition are handwriting or face recognition, and 3D tasks tackle
    such problems as object recognition from point clouds which assists in robotic
    manipulation.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reorganization** means bottom-up vision when raw pixels are segmented into
    groups that represent the structure of an image. Low-level vision tasks include
    edge, contour, and corner detection, while high-level tasks involve semantic segmentation,
    which partially overlaps with recognition tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d16b7f1e4147b82fb8aba9eefe44fe24.png)'
  prefs: []
  type: TYPE_IMG
- en: It is recognition that is most closely connected to language because it has
    the output that can be interpreted as words. For example, objects can be represented
    by nouns, activities by verbs, and object attributes by adjectives. In this sense,
    vision and language are connected by means of semantic representations (Gardenfors
    2014; Gupta 2009).
  prefs: []
  type: TYPE_NORMAL
- en: '**Natural Language Processing and its relation to Computer Vision**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NLP tasks are more diverse as compared to Computer Vision and range from syntax,
    including morphology and compositionality, semantics as a study of meaning, including
    relations between words, phrases, sentences, and discourses, to pragmatics, a
    study of shades of meaning, at the level of natural communication.
  prefs: []
  type: TYPE_NORMAL
- en: Some complex tasks in NLP include machine translation, dialog interface, information
    extraction, and summarization.
  prefs: []
  type: TYPE_NORMAL
- en: It is believed that switching from images to words is the closest to machine
    translation. Still, such “translation” between low-level pixels or contours of
    an image and a high-level description in words or sentences — the task known as
    Bridging the Semantic Gap (Zhao and Grosky 2002) — remains a wide gap to cross.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scope of Integration of Computer Vision and NLP**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The integration of vision and language was not going smoothly in a top-down
    deliberate manner, where researchers came up with a set of principles. Integrated
    techniques were rather developed bottom-up, as some pioneers identiﬁed certain
    rather specific and narrow problems, attempted multiple solutions, and found a
    satisfactory outcome.
  prefs: []
  type: TYPE_NORMAL
- en: The new trajectory started with an understanding that most present-day files
    are multimedia, that they contain interrelated images, videos, and natural language
    texts. For example, a typical news article contains writing by a journalist and
    a photo related to the news content. Furthermore, there may be a clip video that
    contains a reporter or a snapshot of the scene where the event in the news occurred.
    Language and visual data provide two sets of information that are combined into
    a single story, making the basis for appropriate and unambiguous communication.
  prefs: []
  type: TYPE_NORMAL
- en: This understanding gave rise to multiple applications of an integrated approach
    to visual and textual content not only in working with multimedia files but also
    in the fields of robotics, visual translations, and distributional semantics.
  prefs: []
  type: TYPE_NORMAL
- en: '**Multimedia files**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The multimedia-related tasks for NLP and computer vision fall into three main
    categories: visual properties description, visual description, and visual retrieval.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0f8481069a38ddebbba9b9953aa51fef.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Visual properties description:** A step beyond classification, the descriptive
    approach summarizes object properties by assigning attributes. Such attributes
    may be both binary values for easily recognizable properties or relative attributes
    describing a property with the help of a learning-to-rank framework. The key is
    that the attributes will provide a set of contexts as a knowledge source for recognizing
    a speciﬁc object by its properties. The attribute words become an intermediate
    representation that helps bridge the semantic gap between the visual space and
    the label space.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Visual description**: In real life, the task of visual description is to
    provide image or video capturing. It is believed that sentences would provide
    a more informative description of an image than a bag of unordered words. To generate
    a sentence that would describe an image, a certain amount of low-level visual
    information should be extracted that would provide the basic information “who
    did what to whom, and where and how they did it.” From the part-of-speech perspective,
    the quadruplets of “Nouns, Verbs, Scenes, Prepositions” can represent meaning
    extracted from visual detectors. Visual modules extract objects that are either
    a subject or an object in the sentence. Then a Hidden Markov Model is used to
    decode the most probable sentence from a ﬁnite set of quadruplets along with some
    corpus-guided priors for verb and scene (preposition) predictions. The meaning
    is represented using objects (nouns), visual attributes (adjectives), and spatial
    relationships (prepositions). Then the sentence is generated with the help of
    the phrase fusion technique using web-scale n-grams for determining probabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Visual retrieval**: Content-based Image Retrieval (CBIR) is another ﬁeld
    in multimedia that utilizes language in the form of query strings or concepts.
    As a rule, images are indexed by low-level vision features like color, shape,
    and texture. CBIR systems try to annotate an image region with a word, similarly
    to semantic segmentation, so the keyword tags are close to human interpretation.
    CBIR systems use keywords to describe an image for image retrieval but visual
    attributes describe an image for image understanding. Nevertheless, visual attributes
    provide a suitable middle layer for CBIR with an adaptation to the target domain.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Robotics**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Robotics Vision**: Robots need to perceive their surroundings from more than
    one way of interaction. Similar to humans processing perceptual inputs by using
    their knowledge about things in the form of words, phrases, and sentences, robots
    also need to integrate their perceived picture with the language to obtain the
    relevant knowledge about objects, scenes, actions, or events in the real world,
    make sense of them and perform a corresponding action. For example, if an object
    is far away, a human operator may verbally request an action to reach a clearer
    viewpoint. Robotics Vision tasks relate to how a robot can perform sequences of
    actions on objects to manipulate the real-world environment using hardware sensors
    like a depth camera or motion camera and having a verbalized image of their surrounds
    to respond to verbal commands.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Situated Language**: Robots use languages to describe the physical world
    and understand their environment. Moreover, spoken language and natural gestures
    are more convenient ways of interacting with a robot for a human being, if the
    robot is trained to understand this mode of interaction. From the human point
    of view, this is a more natural way to interact. Therefore, a robot should be
    able to perceive and transform the information from its contextual perception
    into a language using semantic structures. The most well-known approach to represent
    meaning is Semantic Parsing (SP), which transforms words into logic predicates.
    SP tries to map a natural language sentence to a corresponding meaning representation
    that can be a logical form like λ-calculus using Combinatorial Categorical Grammar
    (CCG) as rules to compositionally construct a parse tree.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributional Semantics**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Early Multimodal Distributional Semantics Models**: The idea lying behind
    Distributional Semantics Models (DSM) is that words in similar contexts should
    have a similar meaning. Therefore, word meaning can be recovered from co-occurrence
    statistics between words and contexts in which they appear. This approach is believed
    to be beneficial in computer vision and natural language processing as image embedding
    and word embedding. DSMs are applied to jointly model semantics based on both
    visual features like colors, shape or texture and textual features like words.
    The common pipeline is to map visual data to words and apply distributional semantics
    models like LSA or topic models on top of them. Visual attributes can approximate
    the linguistic features for a distributional semantics model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Neural** **Multimodal Distributional Semantics Models**: Neural models have
    surpassed many traditional methods in both vision and language by learning better-distributed
    representation from the data. For instance, Multimodal Deep Boltzmann Machines
    can model joint visual and textual features better than topic models. In addition,
    neural models can model some cognitively plausible phenomena such as attention
    and memory. For attention, an image can initially give an image embedding representation
    using CNNs and RNNs. An LSTM network can be placed on top and act like a state
    machine that simultaneously generates outputs, such as image captions or looking
    at relevant regions of interest in an image one at a time. For memory, commonsense
    knowledge is integrated into visual question answering.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Future of Integration of NLP and Computer Vision**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If combined, two tasks can solve a number of long-standing problems in multiple
    fields, including:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Designing: In the sphere of designing homes, clothes, jewelry or similar items,
    the customer can explain the requirements verbally or in written form and this
    description can be automatically converted to images for better visualization.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Describing medical images: computer vision can be trained to identify subtler
    problems and see the image in more details compared to human specialists.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Converting sign language to speech or text to help hearing-impaired people and
    ensure their better integration into society.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Making a system which sees the surrounding and gives a spoken description of
    the same can be used by blind people.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Making systems which can convert spoken content in the form of some image which
    may assist to an extent to people who do not possess the ability of speaking and
    hearing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/1836bc42afd6a78f5da06c71aa0157c9.png)'
  prefs: []
  type: TYPE_IMG
- en: Yet, since the integration of vision and language is a fundamentally cognitive
    problem, research in this field should take into account cognitive sciences that
    may provide insights into how humans process visual and textual content as a whole
    and create stories based on it.
  prefs: []
  type: TYPE_NORMAL
- en: '**References:**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Gärdenfors, P. 2014\. The Geometry of Meaning: Semantics Based on Conceptual
    Spaces. MIT Press.'
  prefs: []
  type: TYPE_NORMAL
- en: Greenlee, D. 1978\. Semiotic and signiﬁcs. Int. Stud. Philos. 10 (1978), 251–254.
  prefs: []
  type: TYPE_NORMAL
- en: Gupta, A. 2009\. Beyond nouns and verbs. (2009).
  prefs: []
  type: TYPE_NORMAL
- en: 'Malik, J., Arbeláez, P., Carreira, J., Fragkiadaki, K., Girshick, R., Gkioxari,
    G., Gupta, S., Hariharan, B., Kar, A. and Tulsiani, S. 2016\. The three Rs of
    computer vision: Recognition, reconstruction and reorganization. Pattern Recogn.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Shukla, D., Desai A.A. Integrating Computer Vision and Natural Language Processing:
    Issues and Challenges. VNSGU Journal of Science and Technology Vol. 4, №1, p.
    190–196.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Wiriyathammabhum, P., Stay, D.S., Fermüller C., Aloimonos, Y. Computer Vision
    and Natural Language Processing: Recent Approaches in Multimedia and Robotics.
    ACM Computing Surveys. 49(4):1–44'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/sciforce/nlp-and-computer-vision-integrated-843558143e01).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Building a Computer Vision Model: Approaches and datasets](https://www.kdnuggets.com/2019/05/computer-vision-model-approaches-datasets.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to do Everything in Computer Vision](https://www.kdnuggets.com/2019/02/everything-computer-vision.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Your Guide to Natural Language Processing (NLP)](https://www.kdnuggets.com/2019/05/guide-natural-language-processing-nlp.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
