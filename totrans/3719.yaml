- en: The Evolution of Speech Recognition Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/10/evolution-speech-recognition-metrics.html](https://www.kdnuggets.com/2022/10/evolution-speech-recognition-metrics.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![The Evolution of Speech Recognition Metrics](../Images/42ae84473b30fd40757f23e592be8f48.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image by pch.vector](https://www.freepik.com/free-vector/tiny-people-near-phone-with-voice-assistant-screen-man-woman-using-ai-speaking-into-speaker-recording-voice-messages-digital-devices-flat-vector-illustration-technology-software-concept_28480879.htm#query=speech%20recognition&position=19&from_view=search&track=sph)
    on Freepik'
  prefs: []
  type: TYPE_NORMAL
- en: With the widespread adoption of deep learning, progress in Automatic Speech
    Recognition (ASR) accuracy has accelerated in the last few years. Researchers
    have been quick to claim human parity on specific measures. But is Speech Recognition
    really a solved problem today? If not, what we focus on measuring today is going
    to impact the direction ASR takes next.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Human Parity was the End Goal, but now we move the Goalpost!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ASR systems have for a long time focused on [word error rate (WER)](https://en.wikipedia.org/wiki/Word_error_rate)
    to measure accuracy. This metric makes sense. WER measures how many words the
    system gets wrong for every hundred words it recognizes. Because system performance
    can vary significantly depending on the scenario, audio quality, accent, etc.,
    it is hard to put a single number as the end goal. (Of course, 0% would be nice).
    So, instead, we have typically used WER to compare two systems. And as the system
    started to get a lot better, we started comparing its WER with that of humans.
    Human parity was originally thought of as a distant goal, then deep learning accelerated
    things and we ended up “achieving” that a lot sooner.
  prefs: []
  type: TYPE_NORMAL
- en: If we are Comparing Machines Against Humans, Who are we Comparing Humans Against?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Human judges are first asked to transcribe audio. To generate transcription
    references, different judges do multiple passes of listening to the audio and
    editing to get more accurate transcriptions. Transcription is considered clean
    when multiple judges reach a consensus. Human baseline WER is measured by comparing
    a new judge’s transcription to that of the reference transcription. One judge
    can still be incorrect at times, when compared to a reference, in fact, typically
    getting one out of twenty words incorrect. This is equivalent to a WER of 5%.
  prefs: []
  type: TYPE_NORMAL
- en: To claim human parity, we typically compare the system against one judge. An
    interesting aspect of this is that ASR systems do try to do multiple passes themselves.
    Then is it fair to compare against one-person performance? It is indeed fair because
    we assume that the person also had enough time to replay the audio as needed when
    transcribing, effectively doing multiple passes.
  prefs: []
  type: TYPE_NORMAL
- en: Once ASR reached human parity (En-US) for certain Speech Recognition tasks,
    it quickly became apparent that for formally written form, getting one word wrong
    out of every twenty words is still a terrible experience. One way dictation products
    have tried tackling this issue is by trying to figure out which recognized words
    are low confidence and surfacing viable alternatives for them. We can observe
    this ‘correction’ experience in many commercial products today, including Microsoft
    Office Dictation and Google Docs (Fig. 1 and Fig. 2). However, there was another
    glaring issue with WER.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Evolution of Speech Recognition Metrics](../Images/67cdee93a8bc65ebd54e004e7d1fd893.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Microsoft Word Dictation showing alternates for dictated text![The
    Evolution of Speech Recognition Metrics](../Images/205f0e2fdabddec80c9716f2e2467002.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Google Docs showing alternates for dictated text'
  prefs: []
  type: TYPE_NORMAL
- en: To keep things simple, traditionally WER wasn’t calculated on the final written
    form text. One could argue that the primary task of the ASR system was just to
    recognize the words correctly and not format entities like date, time, currency,
    email, etc. correctly. So instead of properly formatted text, the spoken form
    version was used for WER calculation. It canceled out any specific formatting
    differences, punctuation, capitalization, etc., and purely focused on the spoken
    words. This is an acceptable assumption if the use case is voice search, where
    task completion matters much more than text formatting. Searching with a voice
    could be made to work with just the ‘spoken form’. However, with a different use
    case like voice assistant, things started changing. Now the spoken form “wake
    me up at eight thirty-seven am” was much harder to handle compared to the written
    form “wake me up at 8:37 am”. The written form here is easier for the assistant
    to parse and turn into action.
  prefs: []
  type: TYPE_NORMAL
- en: Voice search and voice assistants are examples of what we call one-shot dictation
    use cases. As the ASR system became more reliable with one-shot dictation use
    cases, attention turned to dictation and conversation scenarios. These are both
    long-form speech recognition tasks. It was easy to ignore punctuation for voice
    search or voice assistant, but for any long-form dictation or conversation, not
    being able to punctuate is a dealbreaker. Since automatic punctuation models weren’t
    as good yet, one route that dictation took was to support ‘explicit punctuation’.
    You could explicitly say ‘period’ or ‘question mark’ and the system would do the
    right thing. It gave control to the users and ‘unblocked’ them to use dictation
    for writing emails or documents. Other aspects like capitalization or disfluency
    handling also started gaining importance for dictation scenarios. If we continued
    to rely on WER as the primary metric, we would be falsely painting the picture
    that our system has solved it. Our metric needed to evolve with the evolving use
    cases for Speech Recognition.
  prefs: []
  type: TYPE_NORMAL
- en: An obvious successor of Word Error Rate (WER) is Token Error Rate (TER). For
    TER, we try to consider all aspects of the written form like capitalization, punctuation,
    dysfluency, etc., and try to calculate a single metric just like WER (see Table
    1). On the same sets where ASR had reached human parity on WER when re-evaluated
    with TER, ASR lost the human parity battle again. Our goalpost has moved, but
    this time it does feel like a real goalpost because the outcome is going to be
    much closer to the written form that is widely used.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Recognition | Reference | Metric |'
  prefs: []
  type: TYPE_TB
- en: '| Spoken form | wake me pat eight thirty five a m | wake me up at eight twenty
    five a m | WER = 3/9 = 33.3% |'
  prefs: []
  type: TYPE_TB
- en: '| Written form | Wake me pat 8:35 AM. | Wake me up at 8:25 AM. | TER = 3/7
    = 42.9%*(Punctuation is counted separately)* |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: How does spoken form vs written form impact metrics computation?'
  prefs: []
  type: TYPE_NORMAL
- en: The Problem with these Aggregate Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TER is a good overall metric to look at, but it hides within all the crucial
    details. There are now many categories contributing to this one number, and it
    isn’t about just getting the words right. So, this metric by itself is less actionable.
    To make it more actionable, we need to figure out which category is contributing
    the most to it and decide to focus on improving that. There is a class imbalance
    issue as well. Less than 2% of all tokens would contain any numbers and number-related
    formats like time, currency, date, etc. Even if we get this category completely
    wrong it would still have a limited impact on TER depending on the TER baseline.
    But getting this 2% of the cases even wrong half the time would be a terrible
    experience for the users. For this reason, the TER metric alone cannot guide our
    research investments. I’d argue that we need to figure out important categories
    for our users and measure and improve more targeted metrics like category-[F1](https://en.wikipedia.org/wiki/F-score).
  prefs: []
  type: TYPE_NORMAL
- en: How to Figure Out What’s Important for our Users?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Aha! This is the most important question for ASR. WER, TER, or category-F1 are
    all metrics for scientists to validate their progress, but still can be quite
    far off from what’s important for the users. So, what is important? To answer
    this question, we’ll have to go back to why users need an ASR system in the first
    place. This, of course, depends on the scenario. Let’s take the case of dictation
    first. The sole purpose of dictation is to be able to replace typing. Don’t we
    already have a metric for that? Words per minute (WPM) is already a well-established
    metric to measure typing efficiency. I’d argue this is the perfect metric for
    Dictation. If dictation users can achieve much higher WPM by dictating than typing,
    the ASR system has done its job. Of course, WPM here takes well into account how
    users need to go back and correct things that can slow them down. There would
    be some errors that are a must-fix for users, and then there are some errors that
    are acceptable. This naturally assigns a higher weight to what matters and may
    even disagree with TER which gives equal weight to all errors. Beautiful!
  prefs: []
  type: TYPE_NORMAL
- en: Can we use the same Logic for Conversation or Meeting Transcription?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The meeting transcription case is quite different from dictation, in its objective.
    Dictation is a human2machine use-case and meeting transcription is a human2human
    use-case. WPM stops being the right metric. However, if there is a purpose to
    generating transcriptions, the right metric lies around that purpose. For instance,
    the goal of a broadcast meeting might be to generate human-readable transcriptions
    at the end, so how many edits a human annotator have to make on top of the machine-generated
    transcript becomes a metric. This is like TER, except it doesn’t matter if some
    sections are misrecognized, what matters is that the result is coherent and flows
    well.
  prefs: []
  type: TYPE_NORMAL
- en: Another purpose could be to extract actionable insights from transcriptions
    or generate summaries. These are harder to measure to connect back to recognition
    accuracy. However, human interactions can still be measured, and a task completion
    or engagement-type metric is more suitable.
  prefs: []
  type: TYPE_NORMAL
- en: The metrics we discussed so far can be categorized as ‘online’ metrics and ‘offline’
    metrics, as summarized in Table 2\. In an ideal world, they move together. I’d
    argue that while offline metrics could be a good indicator of potential improvements,
    ‘online’ metrics are the real measure of success. When improved ASR models are
    ready, it is important to first measure the right offline metrics. Shipping them
    is only half the job done. The real test is passed when these models improve the
    ‘online’ metrics for the customers.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Offline Metrics | Online Metrics |'
  prefs: []
  type: TYPE_TB
- en: '| Voice Search | Spoken-form Word Error Rate (WER) | Successful Click on relevant
    search result |'
  prefs: []
  type: TYPE_TB
- en: '| Voice Assistant | Token Error Rate (TER),Formatting F1 for time, date, phone
    numbers, etc. | Task Completion rate |'
  prefs: []
  type: TYPE_TB
- en: '| Voice Typing (Dictation) | Token Error Rate (TER),Punctuation-F1, Capitalization-F1,
    | Words per minute (WPM), Edit rate,User retention/engagement |'
  prefs: []
  type: TYPE_TB
- en: '| Meeting (Conversation) | Token Error Rate (TER),Punctuation-F1, Capitalization-F1,
    Disfluency-F1 | Transcription edit rate,User engagement with Transcription UI
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Offline and Online metrics for Speech Recognition'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '"Achieving Human Parity in Conversational Speech Recognition - arXiv." 17 Oct.
    2016, [https://arxiv.org/abs/1610.05256](https://arxiv.org/abs/1610.05256).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Word Error Rate [https://en.wikipedia.org/wiki/Word_error_rate](https://en.wikipedia.org/wiki/Word_error_rate)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: F1 score [https://en.wikipedia.org/wiki/F-score](https://en.wikipedia.org/wiki/F-score)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Words per minute (WPM) [https://en.wikipedia.org/wiki/Words_per_minute](https://en.wikipedia.org/wiki/Words_per_minute)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**[Piyush Behre](https://www.linkedin.com/in/piyushbehre/)** is a Principal
    Applied Scientist at Microsoft working on Speech Recognition/Natural Language
    Processing. He received his Bachelor of Technology degree in Computer Science
    and Engineering from the Indian Institute of Technology, Roorkee.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Build a Text-to-Speech Converter with Python in 5 Minutes](https://www.kdnuggets.com/2022/09/build-texttospeech-converter-python-5-minutes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Transfer Learning for Image Recognition and Natural Language Processing](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 IT Jobs That Are High in Demand But Don’t Get Enough Recognition](https://www.kdnuggets.com/5-it-jobs-that-are-high-in-demand-but-dont-get-enough-recognition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[From Oracle to Databases for AI: The Evolution of Data Storage](https://www.kdnuggets.com/2022/02/oracle-databases-ai-evolution-data-storage.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Analyzing the Probability of Future Success with Intelligence…](https://www.kdnuggets.com/2022/02/analyzing-probability-future-success-intelligence-node-attributes-evolution-model.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Evolution From Artificial Intelligence to Machine Learning to…](https://www.kdnuggets.com/2022/08/evolution-artificial-intelligence-machine-learning-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
