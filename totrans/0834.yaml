- en: 7 Steps to Mastering Data Cleaning and Preprocessing Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/08/7-steps-mastering-data-cleaning-preprocessing-techniques.html](https://www.kdnuggets.com/2023/08/7-steps-mastering-data-cleaning-preprocessing-techniques.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![7 Steps to Mastering Data Cleaning and Preprocessing Techniques](../Images/b1ac81474d1360b7d4a1b880bdd3551d.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration by Author. Inspired by MEME of Dr. Angshuman Ghosh
  prefs: []
  type: TYPE_NORMAL
- en: Mastering Data Cleaning and Preprocessing Techniques is  fundamental for solving
    a lot of data science projects. A simple demonstration of how important can be
    found in the [meme](https://www.quora.com/Why-are-the-career-expectations-vs-the-reality-of-being-data-scientists)
    about the expectations of a student studying data science before working, compared
    with the reality of the data scientist job.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: We tend to idealise the job position before having a concrete experience, but
    the reality is that it’s always different from what we really expect. When working
    with a real-world problem, there is no documentation of the data and the dataset
    is very dirty. First, you have to dig deep in the problem, understand what clues
    you are missing and what information you can extract.
  prefs: []
  type: TYPE_NORMAL
- en: After understanding the problem, you need to prepare the dataset for your machine
    learning model since the data in its initial condition is never enough. In this
    article, I am going to show seven steps that can help you on pre-processing and
    cleaning your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Exploratory Data Analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in a data science project is the exploratory analysis, that helps
    in understanding the problem and taking decisions in the next steps. It tends
    to be skipped, but it’s the worst error because you’ll lose a lot of time later
    to find the reason why the model gives errors or didn’t perform as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on my experience as data scientist, I would divide the exploratory analysis
    into three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Check the structure of the dataset, the statistics, the missing values, the
    duplicates, the unique values of the categorical variables
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understand the meaning and the distribution of the variables
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Study the relationships between variables
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To analyse how the dataset is organised, there are the following Pandas methods
    that can help you:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When trying to understand the variables, it’s useful to split the analysis
    into two further parts: numerical features and categorical features. First, we
    can focus on the numerical features that can be visualised through histograms
    and boxplots. After, it’s the turn for the categorical variables. In case it’s
    a binary problem, it’s better to start by looking if the classes are balanced.
    After our attention can be focused on the remaining categorical variables using
    the bar plots. In the end, we can finally check the correlation between each pair
    of numerical variables. Other useful data visualisations can be the scatter plots
    and boxplots to observe the relations between a numerical and a categorical variable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Deal with Missings'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first step, we have already investigated if there are missings in each
    variable. In case there are missing values, we need to understand how to handle
    the issue. The easiest way would be to remove the variables or the rows that contain
    NaN values, but we would prefer to avoid it because we risk losing useful information
    that can help our machine learning model on solving the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are dealing with a numerical variable, there are several approaches to
    fill it. The most popular method consists in filling the missing values with the
    mean/median of that feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Another way is to substitute the blanks with group by imputations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: It can be a better option in case there is a strong relationship between a numerical
    feature and a categorical feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the same way, we can fill the missing values of categorical based on the
    mode of that variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: Deal with Duplicates and Outliers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If there are duplicates within the dataset, it’s better to delete the duplicated
    rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: While deciding how to handle duplicates is simple, dealing with outliers can
    be challenging. You need to ask yourself “Drop or not Drop Outliers?”.
  prefs: []
  type: TYPE_NORMAL
- en: Outliers should be deleted if you are sure that they provide only noisy information.
    For example, the dataset contains two people with 200 years, while the range of
    the age is between 0 and 90\. In that case, it’s better to remove these two data
    points.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, most of the time removing outliers can lead to losing important
    information. The most efficient way is to apply the logarithm transformation to
    the numerical feature.
  prefs: []
  type: TYPE_NORMAL
- en: Another technique that I discovered during my last experience is the clipping
    method. In this technique, you choose the upper and the lower bound, that can
    be the 0.1 percentile and the 0.9 percentile. The values of the feature below
    the lower bound will be substituted with the lower bound value, while the values
    of the variable above the upper bound will be replaced with the upper bound value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 4: Encode Categorical Features'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next phase is to convert the categorical features into numerical features.
    Indeed, the machine learning model is able only to work with numbers, not strings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before going further, you should distinguish between two types of categorical
    variables: non-ordinal variables and ordinal variables.'
  prefs: []
  type: TYPE_NORMAL
- en: Examples of non-ordinal variables are the gender, the marital status, the type
    of job. So, it’s non-ordinal if the variable doesn’t follow an order, differently
    from the ordinal features. An example of ordinal variables can be the education
    with values “childhood”, “primary”, “secondary” and “tertiary", and the income
    with levels “low”, “medium” and “high”.
  prefs: []
  type: TYPE_NORMAL
- en: When we are dealing with non-ordinal variables, One-Hot Encoding is the most
    popular technique taken into account to convert these variables into numerical.
  prefs: []
  type: TYPE_NORMAL
- en: In this method, we create a new binary variable for each level of the categorical
    feature. The value of each binary variable is 1 when the name of the level coincides
    with the value of the level, 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: When the variable is ordinal, the most common technique used is the Ordinal
    Encoding, which consists in converting the unique values of the categorical variable
    into integers that follow an order. For example, the levels “low”, “Medium” and
    “High” of income will be encoded respectively as 0,1 and 2.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: There are other possible encoding techniques if you want to explore here. You
    can take a look [here](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02)
    in case you are interested in alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: Split dataset into training and test set'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It’s time to divide the dataset into three fixed subsets: the most common choice
    is to use 60% for training, 20% for validation and 20% for testing. As the quantity
    of data grows, the percentage for training increases and the percentage for validation
    and testing decreases.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to have three subsets because the training set is used to train
    the model, while the validation and the test sets can be useful to understand
    how the model is performing on new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To split the dataset, we can use the train_test_split of scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In case we are dealing with a classification problem and the classes are not
    balanced, it’s better to set up the stratify argument to be sure that there is
    the same proportion of classes in training, validation and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This stratified cross-validation also helps to ensure that there is the same
    percentage of the target variable in the three subsets and give more accurate
    performances of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 6: Feature Scaling'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are machine learning models, like Linear Regression, Logistic Regression,
    KNN, Support Vector Machine and Neural Networks, that require scaling features.
    The feature scaling only helps the variables be in the same range, without changing
    the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: There are three most popular feature scaling techniques are Normalization, Standardization
    and Robust scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization, aso called min-max scaling, consists of mapping the value of
    a variable into a range between 0 and 1\. This is possible by subtracting the
    minimum of the feature from the feature value and, then, dividing by the difference
    between the maximum and the minimum of that feature.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Another common approach is Standardization, that rescales the values of a column
    to respect the properties of a standard normal distribution, which is characterised
    by mean equal to 0 and variance equal to 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: If the feature contains outliers that cannot be removed, a preferable method
    is the Robust Scaling, that rescales the values of a feature based on robust statistics,
    the median, the first and the third quartile. The rescaled value is obtained by
    subtracting the median from the original value and, then, dividing by the Interquartile
    Range, which is the difference between the 75th and 25th quartile of the feature.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In general, it’s preferable to calculate the statistics based on the training
    set and then use them to rescale the values on both training, validation and test
    sets. This is because we suppose that we only have the training data and, later,
    we want to test our model on new data, which should have a similar distribution
    than the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 7: Deal with Imbalanced Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![7 Steps to Mastering Data Cleaning and Preprocessing Techniques](../Images/33d67baeb895e22926fa8545da187b47.png)'
  prefs: []
  type: TYPE_IMG
- en: This step is only included when we are working in a classification problem and
    we have found that the classes are imbalanced.
  prefs: []
  type: TYPE_NORMAL
- en: In case there is a slight difference between the classes, for example class
    1 contains 40% of the observations and class 2 contains the remaining 60%, we
    don’t need to apply oversampling or undersampling techniques to alter the number
    of samples in one of the classes. We can just avoid looking at accuracy since
    it’s a good measure only when the dataset is balanced and we should care only
    about evaluation measures, like precision, recall and f1-score.
  prefs: []
  type: TYPE_NORMAL
- en: But it can happen that the positive class has a very low proportion of data
    points (0.2) compared to the negative class (0.8). The machine learning may not
    perform well with the class with less observations, leading to failing on solving
    the task.
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome this issue, there are two possibilities: undersampling the majority
    class and oversampling the minority class. Undersampling consists in reducing
    the number of samples by randomly removing some data points from the majority
    class, while Oversampling increases the number of observations in the minority
    class by adding randomly data points from the less frequent class. There is the
    imblearn that allows to balance the dataset with few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: However, removing or duplicating some of the observations can be ineffective
    sometimes in improving the performance of the model. It would be better to create
    new artificial data points in the minority class. A technique proposed to solve
    this issue is SMOTE, which is known for generating synthetic records in the class
    less represented. Like KNN, the idea is to identify k nearest neighbors of observations
    belonging to the minority class, based on a particular distance, like t.  After
    a new point is generated at a random location between these k nearest neighbors.
    This process will keep creating new points until the dataset is completely balanced.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: I should highlight that these approaches should be applied only to resample
    the training set. We want that our machine model learns in a robust way and, then,
    we can apply it to make predictions on new data.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I hope you have found this comprehensive tutorial useful. It can be hard to
    start our first data science project without being aware of all these techniques.
    You can find all my code [here](https://www.kaggle.com/code/eugeniaanello/7-steps-to-mastering-preprocessing#7.-Deal-with-Imbalanced-Data).
  prefs: []
  type: TYPE_NORMAL
- en: There are surely other methods I didn’t cover in the article, but I preferred
    to focus on the most popular and known ones. Do you have other suggestions? Drop
    them in the comments if you have insightful suggestions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Useful resources:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[A Practical Guide for Exploratory Data Analysis](https://towardsdatascience.com/a-practical-guide-for-exploratory-data-analysis-5ab14d9a5f24)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Which models require normalized data?](https://www.yourdatateacher.com/2022/06/13/which-models-require-normalized-data/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Random Oversampling and Undersampling for Imbalanced Classification](https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[Eugenia Anello](https://www.linkedin.com/in/eugenia-anello/)** is currently
    a research fellow at the Department of Information Engineering of the University
    of Padova, Italy. Her research project is focused on Continual Learning combined
    with Anomaly Detection.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Learn Data Cleaning and Preprocessing for Data Science with This Free eBook](https://www.kdnuggets.com/2023/08/learn-data-cleaning-preprocessing-data-science-free-ebook.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Harnessing ChatGPT for Automated Data Cleaning and Preprocessing](https://www.kdnuggets.com/2023/08/harnessing-chatgpt-automated-data-cleaning-preprocessing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cleaning and Preprocessing Text Data in Pandas for NLP Tasks](https://www.kdnuggets.com/cleaning-and-preprocessing-text-data-in-pandas-for-nlp-tasks)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Steps to Mastering Data Cleaning with Python and Pandas](https://www.kdnuggets.com/7-steps-to-mastering-data-cleaning-with-python-and-pandas)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Collection of Guides on Mastering SQL, Python, Data Cleaning, Data…](https://www.kdnuggets.com/collection-of-guides-on-mastering-sql-python-data-cleaning-data-wrangling-and-exploratory-data-analysis)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Exploring Data Cleaning Techniques With Python](https://www.kdnuggets.com/2023/04/exploring-data-cleaning-techniques-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
