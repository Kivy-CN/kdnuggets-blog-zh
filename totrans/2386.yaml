- en: 'Data Transformation: Standardization vs Normalization'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据转换：标准化与归一化
- en: 原文：[https://www.kdnuggets.com/2020/04/data-transformation-standardization-normalization.html](https://www.kdnuggets.com/2020/04/data-transformation-standardization-normalization.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/04/data-transformation-standardization-normalization.html](https://www.kdnuggets.com/2020/04/data-transformation-standardization-normalization.html)
- en: '![Data Transformation: Standardization vs Normalization](../Images/fb62856a6313e463c04d9e9d31b5d822.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![数据转换：标准化与归一化](../Images/fb62856a6313e463c04d9e9d31b5d822.png)'
- en: Image from [365datascience](https://365datascience.com/standardization/)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [365datascience](https://365datascience.com/standardization/)
- en: 'Data transformation is one of the fundamental steps in the part of data processing.
    When I first learnt the technique of feature scaling, the terms *scale*, *standardise*,
    and *normalis*e are often being used. However, it was pretty hard to find information
    about which of them I should use and also when to use. Therefore, I’m going to
    explain the following key aspects in this article:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换是数据处理中的基本步骤之一。当我第一次学习特征缩放技术时，术语*scale*、*standardise*和*normalize*经常被使用。然而，找到关于何时使用这些术语的信息相当困难。因此，我将在本文中解释以下关键方面：
- en: the difference between Standardisation and Normalisation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化和归一化之间的区别
- en: when to use Standardisation and when to use Normalisation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 何时使用标准化，何时使用归一化
- en: how to apply feature scaling in Python
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在Python中应用特征缩放
- en: '* * *'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT工作'
- en: '* * *'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: What Does Feature Scaling Mean?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征缩放是什么意思？
- en: In practice, we often encounter different types of variables in the same dataset.
    A significant issue is that the range of the variables may differ a lot. Using
    the original scale may put more weights on the variables with a large range. In
    order to deal with this problem, we need to apply the technique of features rescaling
    to independent variables or features of data in the step of data pre-processing.
    The terms *normalisation* and *standardisation* are sometimes used interchangeably,
    but they usually refer to different things.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，我们经常在同一数据集中遇到不同类型的变量。一个重要的问题是这些变量的范围可能相差很大。使用原始尺度可能会对范围大的变量赋予更多权重。为了处理这个问题，我们需要在数据预处理的步骤中，对自变量或数据的特征应用特征重缩放技术。术语*normalisation*和*standardisation*有时可以互换使用，但它们通常指的是不同的概念。
- en: The goal of applying Feature Scaling is to make sure features are on almost
    the same scale so that each feature is equally important and make it easier to
    process by most ML algorithms
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 应用特征缩放的目标是确保特征几乎在相同的尺度上，以便每个特征同样重要，并且让大多数机器学习算法更容易处理。
- en: Example
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例
- en: This is a dataset that contains an independent variable (Purchased) and 3 dependent
    variables (Country, Age, and Salary). We can easily notice that the variables
    are not on the same scale because the range of *Age* is from 27 to 50, while the
    range of *Salary* going from 48 K to 83 K. The range of *Salary* is much wider
    than the range of *Age*. This will cause some issues in our models since a lot
    of machine learning models such as k-means clustering and nearest neighbour classification
    are based on the Euclidean Distance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个包含一个自变量（Purchased）和3个因变量（Country、Age和Salary）的数据集。我们可以很容易地发现这些变量的尺度不同，因为*Age*的范围从27到50，而*Salary*的范围从48
    K到83 K。*Salary*的范围远大于*Age*的范围。这会导致我们模型中的一些问题，因为许多机器学习模型，如k-means聚类和最近邻分类，是基于欧几里得距离的。
- en: '![Data Transformation: Standardization vs Normalization](../Images/441edcee4f7afe2b81f1a5432fe4734c.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![数据转换：标准化与归一化](../Images/441edcee4f7afe2b81f1a5432fe4734c.png)'
- en: Focusing on age and salary
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 关注年龄和薪资
- en: When we calculate the equation of Euclidean distance, the number of (x2-x1)²
    is much bigger than the number of (y2-y1)² which means the Euclidean distance
    will be dominated by the salary if we do not apply feature scaling. The difference
    in Age contributes less to the overall difference. Therefore, we should use Feature
    Scaling to bring all values to the same magnitudes and, thus, solve this issue.
    To do this, there are primarily two methods called Standardisation and Normalisation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们计算欧几里得距离的公式时，(x2-x1)²的数量远大于(y2-y1)²的数量，这意味着如果不进行特征缩放，欧几里得距离将主要由薪资决定。年龄的差异对整体差异的贡献较小。因此，我们应该使用特征缩放将所有值调整到相同的量级，从而解决这个问题。为此，主要有两种方法：标准化和归一化。
- en: '![Data Transformation: Standardization vs Normalization](../Images/6d2fa25d289bd786ae3c1249e8b60b2e.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![数据转换：标准化与归一化](../Images/6d2fa25d289bd786ae3c1249e8b60b2e.png)'
- en: Euclidean distance application.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离的应用。
- en: Standardisation
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准化
- en: 'The result of **standardization** (or **Z-score normalization**) is that the
    features will be rescaled to ensure the mean and the standard deviation to be
    0 and 1, respectively. The equation is shown below:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**标准化**（或**Z-score 归一化**）的结果是特征将被重新缩放，以确保均值和标准差分别为0和1。公式如下所示：'
- en: '![Data Transformation: Standardization vs Normalization](../Images/77643cc990cf40eb68b3a06c4a5e7904.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![数据转换：标准化与归一化](../Images/77643cc990cf40eb68b3a06c4a5e7904.png)'
- en: This technique is to re-scale features value with the distribution value between
    0 and 1 is useful for the optimization algorithms, such as gradient descent, that
    are used within machine learning algorithms that weight inputs (e.g., regression
    and neural networks). Rescaling is also used for algorithms that use distance
    measurements, for example, K-Nearest-Neighbours (KNN).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术是将特征值重新缩放到0和1之间，这对于优化算法（如梯度下降）非常有用，这些算法在机器学习中用于加权输入（例如回归和神经网络）。重新缩放也用于使用距离测量的算法，例如K-最近邻（KNN）。
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Max-Min Normalization
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大-最小归一化
- en: 'Another common approach is the so-called **Max-Min Normalization (**Min-Max
    scaling). This technique is to re-scales features with a distribution value between
    0 and 1\. For every feature, the minimum value of that feature gets transformed
    into 0, and the maximum value gets transformed into 1\. The general equation is
    shown below:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的方法是所谓的**最大-最小归一化**（**Min-Max scaling**）。该技术将特征重新缩放到0和1之间。对于每个特征，该特征的最小值被转换为0，最大值被转换为1。一般公式如下所示：
- en: '![Data Transformation: Standardization vs Normalization](../Images/6a867a912bf5ecce4b1e7d97356cc4a8.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![数据转换：标准化与归一化](../Images/6a867a912bf5ecce4b1e7d97356cc4a8.png)'
- en: The equation of Max-Min Normalization
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最大-最小归一化的公式
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Standardisation vs Max-Min Normalization
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准化与最大-最小归一化
- en: In contrast to standardisation, we will obtain smaller standard deviations through
    the process of Max-Min Normalisation. Let me illustrate more in this area using
    the above dataset.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 与标准化相比，通过最大-最小归一化过程我们会获得较小的标准差。让我利用上面的数据集更详细地说明这一点。
- en: '![Data Transformation: Standardization vs Normalization](../Images/dd75ff7d0d85b469fc746efb091a7fdf.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![数据转换：标准化与归一化](../Images/dd75ff7d0d85b469fc746efb091a7fdf.png)'
- en: After Feature scaling
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 特征缩放后
- en: '![Data Transformation: Standardization vs Normalization](../Images/84fd2bbb301990feda0d8d0fc6ce3756.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![数据转换：标准化与归一化](../Images/84fd2bbb301990feda0d8d0fc6ce3756.png)'
- en: Normal distribution and Standard Deviation of Salary
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 薪资的正态分布和标准差
- en: '![Data Transformation: Standardization vs Normalization](../Images/1a1081d694f82f1780da9f2038e87e6b.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![数据转换：标准化与归一化](../Images/1a1081d694f82f1780da9f2038e87e6b.png)'
- en: Normal distribution and Standard Deviation of Age
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 年龄的正态分布和标准差
- en: From the above graphs, we can clearly notice that applying Max-Min Nomaralisation
    in our dataset has generated smaller standard deviations (Salary and Age) than
    using Standardisation method. It implies the data are more concentrated around
    the mean if we scale data using Max-Min Nomaralisation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述图表中，我们可以明显看到，在数据集中应用最大-最小归一化生成的标准差（薪资和年龄）小于使用标准化方法。这表明如果使用最大-最小归一化缩放数据，数据将更集中在均值附近。
- en: As a result, if you have outliers in your feature (column), normalizing your
    data will scale most of the data to a small interval, which means all features
    will have the same scale but does not handle outliers well. Standardisation is
    more robust to outliers, and in many cases, it is preferable over Max-Min Normalisation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你的特征（列）中有异常值，归一化你的数据将把大部分数据缩放到一个小区间，这意味着所有特征将具有相同的尺度，但对异常值的处理不好。标准化对异常值更为稳健，在许多情况下，它优于
    Max-Min 归一化。
- en: When Feature Scaling Matters
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征缩放的重要性
- en: '![Data Transformation: Standardization vs Normalization](../Images/d54c8f6ac39f36b1ce13ee5fdf41bf8d.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![数据转换：标准化与归一化](../Images/d54c8f6ac39f36b1ce13ee5fdf41bf8d.png)'
- en: Some machine learning models are fundamentally based on distance matrix, also
    known as the distance-based classifier, for example, K-Nearest-Neighbours, SVM,
    and Neural Network. Feature scaling is extremely essential to those models, especially
    when the range of the features is very different. Otherwise, features with a large
    range will have a large influence in computing the distance.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习模型基本上是基于距离矩阵的，也称为基于距离的分类器，例如 K 最近邻、SVM 和神经网络。特征缩放对这些模型至关重要，特别是当特征的范围差异很大时。否则，范围较大的特征在计算距离时将有较大影响。
- en: Max-Min Normalisation typically allows us to transform the data with varying
    scales so that no specific dimension will dominate the statistics, and it does
    not require making a very strong assumption about the distribution of the data,
    such as k-nearest neighbours and artificial neural networks. However, [Normalisation](https://www.codecademy.com/articles/normalization) does
    not treat outliners very well. On the contrary, standardisation allows users to
    better handle the outliers and facilitate convergence for some computational algorithms
    like gradient descent. Therefore, we usually prefer standardisation over Min-Max
    Normalisation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Max-Min 归一化通常允许我们对具有不同尺度的数据进行转换，以使得没有特定维度主导统计数据，并且不需要对数据的分布做出非常强的假设，例如 k 最近邻和人工神经网络。然而，[归一化](https://www.codecademy.com/articles/normalization)
    对异常值的处理并不好。相反，标准化允许用户更好地处理异常值，并且有助于某些计算算法（如梯度下降）的收敛。因此，我们通常更倾向于使用标准化而不是 Min-Max
    归一化。
- en: '**Example: What algorithms need feature scaling**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：哪些算法需要特征缩放**'
- en: '![Data Transformation: Standardization vs Normalization](../Images/5112548c35615961899d8e16f7a1b734.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![数据转换：标准化与归一化](../Images/5112548c35615961899d8e16f7a1b734.png)'
- en: '**Note**: If an algorithm is not distance-based, feature scaling is unimportant,
    including Naive Bayes, Linear Discriminant Analysis, and Tree-Based models (gradient
    boosting, random forest, etc.).'
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意**：如果一个算法不是基于距离的，则特征缩放不重要，包括朴素贝叶斯、线性判别分析和基于树的模型（梯度提升、随机森林等）。'
- en: 'Summary: Now You Should Know'
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结：现在你应该知道
- en: the objective of using Feature Scaling
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用特征缩放的目的
- en: the difference between Standardisation and Normalisation
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化和归一化之间的区别
- en: the algorithms that need to apply Standardisation or Normalisation
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要应用标准化或归一化的算法
- en: applying feature scaling in Python
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 中应用特征缩放
- en: '**Please find the code and dataset **[**here**](https://github.com/clareyan/feasturescaling)**.**'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**请在此处找到代码和数据集**[**这里**](https://github.com/clareyan/feasturescaling)**。**'
- en: '[Original](https://towardsdatascience.com/data-transformation-standardisation-vs-normalisation-a47b2f38cec2).
    Reposted with permission.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/data-transformation-standardisation-vs-normalisation-a47b2f38cec2)。经许可转载。'
- en: '**[Clare Liu](https://www.linkedin.com/in/clareliuchungyan/)** is a Data Scientist
    at fintech (bank) industry, based in HK. Passionate in resolving mystery about
    data science and machine learning. Join me on the self-learning journey.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Clare Liu](https://www.linkedin.com/in/clareliuchungyan/)** 是一名在香港金融科技（银行）行业工作的数据科学家。热衷于解决数据科学和机器学习中的谜团。加入我，开始自学之旅吧。'
- en: More On This Topic
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Optimizing Data Storage: Exploring Data Types and Normalization in SQL](https://www.kdnuggets.com/optimizing-data-storage-exploring-data-types-and-normalization-in-sql)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化数据存储：探索 SQL 中的数据类型和归一化](https://www.kdnuggets.com/optimizing-data-storage-exploring-data-types-and-normalization-in-sql)'
- en: '[Does the Random Forest Algorithm Need Normalization?](https://www.kdnuggets.com/2022/07/random-forest-algorithm-need-normalization.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[随机森林算法需要归一化吗？](https://www.kdnuggets.com/2022/07/random-forest-algorithm-need-normalization.html)'
- en: '[Evolution in ETL: How Skipping Transformation Enhances Data Management](https://www.kdnuggets.com/evolution-in-etl-how-skipping-transformation-enhances-data-management)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ETL演变：跳过转换如何提升数据管理](https://www.kdnuggets.com/evolution-in-etl-how-skipping-transformation-enhances-data-management)'
- en: '[The Chatbot Transformation: From Failure to the Future](https://www.kdnuggets.com/2021/12/chatbot-transformation-failure-future.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[聊天机器人转型：从失败到未来](https://www.kdnuggets.com/2021/12/chatbot-transformation-failure-future.html)'
- en: '[The AI Transformation Strategy in the GenAI Era](https://www.kdnuggets.com/the-ai-transformation-strategy-in-the-genai-era)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GenAI时代的AI转型战略](https://www.kdnuggets.com/the-ai-transformation-strategy-in-the-genai-era)'
- en: '[Digital Transformation Playbook for Modern Businesses](https://www.kdnuggets.com/digital-transformation-playbook-for-modern-businesses)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[现代企业的数字化转型指南](https://www.kdnuggets.com/digital-transformation-playbook-for-modern-businesses)'
