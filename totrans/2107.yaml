- en: 'Ollama Tutorial: Running LLMs Locally Made Super Simple'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/ollama-tutorial-running-llms-locally-made-super-simple](https://www.kdnuggets.com/ollama-tutorial-running-llms-locally-made-super-simple)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![ollama-tutorial](../Images/1f45326dd049cd0966bfbd8d5988835b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Running large language models (LLMs) locally can be super helpful—whether you'd
    like to play around with LLMs or build more powerful apps using them. But configuring
    your working environment and getting LLMs to run on your machine is not trivial.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: So how do you run LLMs locally without any of the hassle? **Enter Ollama**,
    a platform that makes local development with open-source large language models
    a breeze. With Ollama, everything you need to run an LLM—model weights and all
    of the config—is packaged into a single Modelfile. **Think Docker for LLMs**.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we’ll take a look at how to get started with Ollama to run
    large language models locally. So let’s get right into the steps!
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Download Ollama to Get Started'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a first step, you should download Ollama to your machine. Ollama is supported
    on all major platforms: MacOS, Windows, and Linux.'
  prefs: []
  type: TYPE_NORMAL
- en: To download Ollama, you can either visit the [official GitHub repo](https://github.com/ollama/ollama)
    and follow the download links from there. Or visit the [official website](https://ollama.com/)
    and download the installer if you are on a Mac or a Windows machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’m on Linux: Ubuntu distro. So if you’re a Linux user like me, you can run
    the following command to run the installer script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The installation process typically takes a few minutes. During the installation
    process, any NVIDIA/AMD GPUs will be auto-detected. Make sure you have the drivers
    installed. The CPU-only mode works fine, too. But it may be much slower.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Get the Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, you can visit the [model library](https://ollama.com/library) to check
    the list of all model families currently supported. The default model downloaded
    is the one with the `latest` tag. On the page for each model, you can get more
    info such as the size and quantization used.
  prefs: []
  type: TYPE_NORMAL
- en: You can search through the list of tags to locate the model that you want to
    run. For each model family, there are typically foundational models of different
    sizes and instruction-tuned variants. I’m interested in running the Gemma 2B model
    from the [Gemma family of lightweight models](https://ai.google.dev/gemma) from
    Google DeepMind.
  prefs: []
  type: TYPE_NORMAL
- en: You can run the model using the `ollama run` command to pull and start interacting
    with the model directly. However, you can also pull the model onto your machine
    first and then run it. This is very similar to how you work with Docker images.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Gemma 2B, running the following pull command downloads the model onto your
    machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The model is of size 1.7B and the pull should take a minute or two:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ollama-pull](../Images/d1a077a8dd047ac39c1a9b9b8e445478.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Step 3: Run the Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Run the model using the `ollama run` command as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Doing so will start an Ollama REPL at which you can interact with the Gemma
    2B model. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ollama-response](../Images/8644500443780001ff507a342f3ec1f8.png)'
  prefs: []
  type: TYPE_IMG
- en: For a simple question about the Python standard library, the response seems
    pretty okay. And includes most frequently used modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Customize Model Behavior with System Prompts'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can customize LLMs by setting system prompts for a specific desired behavior
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: Set system prompt for desired behavior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save the model by giving it a name.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exit the REPL and run the model you just created.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Say you want the model to always explain concepts or answer questions in plain
    English with minimal technical jargon as possible. Here’s how you can go about
    doing it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now run the model you just created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ipe-response](../Images/5818ee682cda05d60d9c3d1b28fa2057.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Step 5: Use Ollama with Python'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Running the Ollama command-line client and interacting with LLMs locally at
    the Ollama REPL is a good start. But often you would want to use LLMs in your
    applications. You can run Ollama as a server on your machine and run cURL requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'But there are simpler ways. If you like using Python, you’d want to build LLM
    apps and here are a couple ways you can do it:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the official Ollama Python library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Ollama with LangChain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pull the models you need to use before you run the snippets in the following
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Ollama Python Library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To use the Ollama Python library you can install it using pip like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: There is an official JavaScript library too, which you can use if you prefer
    developing with JS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you install the Ollama Python library, you can import it in your Python
    application and work with large language models. Here''s the snippet for a simple
    language generation task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Using LangChain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another way to use Ollama with Python is using [LangChain](https://www.langchain.com/).
    If you have existing projects using LangChain it's easy to integrate or switch
    to Ollama.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure you have LangChain installed. If not, install it using pip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Using LLMs like this in Python apps makes it easier to switch between different
    LLMs depending on the application.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With Ollama you can run large language models locally and build LLM-powered
    apps with just a few lines of Python code. Here we explored how to interact with
    LLMs at the Ollama REPL as well as from within Python applications.
  prefs: []
  type: TYPE_NORMAL
- en: Next we'll try building an app using Ollama and Python. Until then, if you’re
    looking to dive deep into LLMs check out [7 Steps to Mastering Large Language
    Models (LLMs)](https://www.kdnuggets.com/7-steps-to-mastering-large-language-models-llms).
  prefs: []
  type: TYPE_NORMAL
- en: '**[](https://twitter.com/balawc27)**[Bala Priya C](https://www.kdnuggets.com/wp-content/uploads/bala-priya-author-image-update-230821.jpg)****
    is a developer and technical writer from India. She likes working at the intersection
    of math, programming, data science, and content creation. Her areas of interest
    and expertise include DevOps, data science, and natural language processing. She
    enjoys reading, writing, coding, and coffee! Currently, she''s working on learning
    and sharing her knowledge with the developer community by authoring tutorials,
    how-to guides, opinion pieces, and more. Bala also creates engaging resource overviews
    and coding tutorials.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[A Simple Guide to Running LlaMA 2 Locally](https://www.kdnuggets.com/a-simple-guide-to-running-llama-2-locally)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pydantic Tutorial: Data Validation in Python Made Simple](https://www.kdnuggets.com/pydantic-tutorial-data-validation-in-python-made-simple)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Easiest Way of Running Llama 3 Locally](https://www.kdnuggets.com/easiest-way-of-running-llama-3-locally)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Combining Pandas DataFrames Made Simple](https://www.kdnuggets.com/2022/09/combining-pandas-dataframes-made-simple.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Personalized AI Made Simple: Your No-Code Guide to Adapting GPTs](https://www.kdnuggets.com/personalized-ai-made-simple-your-no-code-guide-to-adapting-gpts)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning Made Simple for Data Analysts with BigQuery ML](https://www.kdnuggets.com/machine-learning-made-simple-for-data-analysts-with-bigquery-ml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
