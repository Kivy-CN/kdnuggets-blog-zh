- en: How the Lottery Ticket Hypothesis is Challenging Everything we Knew About Training
    Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 彩票票假设如何挑战我们对神经网络训练的所有认知
- en: 原文：[https://www.kdnuggets.com/2019/05/lottery-ticket-hypothesis-neural-networks.html](https://www.kdnuggets.com/2019/05/lottery-ticket-hypothesis-neural-networks.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/05/lottery-ticket-hypothesis-neural-networks.html](https://www.kdnuggets.com/2019/05/lottery-ticket-hypothesis-neural-networks.html)
- en: '[comments](#comments)![Lottery Ticket](../Images/c2df7583ac5ecd73a5de352405c466bb.png)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments) ![彩票票](../Images/c2df7583ac5ecd73a5de352405c466bb.png)'
- en: 'Source: Pixabay'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：Pixabay
- en: Training machine learning models is one of the most challenging and computationally
    expensive aspects of data science solutions in the real world. For decades, the
    artificial intelligence (AI) community has developed hundreds of techniques to
    improve the training of machine learning models under the single axiomatic assumption
    that training should cover the entire model. Recently, AI researchers from the
    Massachusetts Institute of Technology(MIT) published a paper that challenges that
    assumption and proposes a smarter and simpler way to train neural networks by
    focusing on subsets of the model. Within the AI community, the MIT thesis has
    come to be known by the catchy name of the [Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 训练机器学习模型是现实世界数据科学解决方案中最具挑战性和计算开销的方面之一。几十年来，人工智能（AI）社区已经开发了数百种技术来改进机器学习模型的训练，基于一个单一的公理假设，即训练应该覆盖整个模型。最近，来自麻省理工学院（MIT）的AI研究人员发表了一篇挑战这一假设的论文，提出了一种更聪明、更简单的方法，通过关注模型的子集来训练神经网络。在AI社区中，MIT的论文以引人注目的名字[彩票票假设](https://arxiv.org/abs/1803.03635)而闻名。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速入门网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The process of training machine learning models is one of the areas in which
    data scientists often face the compromise between theory and the constraints of
    real world solutions. More often than not, a neural network architecture that
    seems ideal for a specific problem can’t be fully implemented because the cost
    of training would be prohibited. Typically, the initial training of a neural networks
    requires large datasets and days of expensive computation usage. The result are
    very large neural network structures with connections between neuros and hidden
    layers. This structure often needs to be subjected to optimization techniques
    to remove some of the connections and adjust the size of the model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 训练机器学习模型的过程是数据科学家在理论和现实世界解决方案的约束之间常常面临妥协的领域之一。往往，一个看起来非常适合特定问题的神经网络结构无法完全实现，因为训练成本可能过高。通常，神经网络的初始训练需要大量的数据集和几天的昂贵计算使用。结果是非常大的神经网络结构，神经元和隐藏层之间有大量连接。这种结构通常需要经过优化技术来删除一些连接并调整模型的大小。
- en: One question that bothered AI researchers for decades is whether we actually
    need those large neural network structures to begin with. Obviously, if we connect
    almost every neuron in an architecture, we are likely to achieve a model that
    can perform the initial task but the cost might be prohibited. Couldn’t we start
    with smaller, leaner neural network architectures to begin with? This is the essence
    of the Lottery Ticket Hypothesis.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来困扰人工智能研究人员的一个问题是，我们是否真的需要那些大型神经网络结构。显然，如果我们连接几乎每一个神经元，可能会实现一个能够完成初始任务的模型，但成本可能会很高。我们不能从更小、更简洁的神经网络结构开始吗？这就是彩票票假设的核心。
- en: '**The Lottery Ticket Hypothesis**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**彩票票假设**'
- en: Using an analogy from the gambling world, the training of machine learning models
    is often compared to winning the lottery by buying every possible ticket. But
    if we know how winning the lottery looks like, couldn’t we be smarter about selecting
    the tickets?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用赌博世界的类比，机器学习模型的训练常常被比作通过购买每一张可能的彩票来赢得彩票。但如果我们知道中奖的彩票是什么样的，我们难道不能更聪明地选择票据吗？
- en: 'In machine learning models, training processes produced large neural network
    structures that are the equivalent to a big bag of lottery tickets. After the
    initial training, models need to undergo optimization techniques such as pruning
    that remove unnecessary weights within the network in order reduce the size of
    the model without sacrificing performance. This is the equivalent of searching
    for the winning tickets in the bag and getting rid of the rest. Very often, pruning
    techniques end up producing neural network structures that are 90% smaller than
    the original. The obvious question the is: if a network can be reduced in size,
    why do we not train this smaller architecture instead in the interest of making
    training more efficient as well? Paradoxically, practical experiences in machine
    learning solutions show that the architectures uncovered by pruning are harder
    to train from the start, reaching lower accuracy than the original networks. So
    you can buy a big bag of tickets and work your way to the winning numbers but
    the opposite process is too hard. Or so we thought ????'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习模型中，训练过程会产生大型神经网络结构，这些结构相当于一大包彩票。经过初步训练后，模型需要经过优化技术，如剪枝，以去除网络中不必要的权重，从而减少模型的大小而不牺牲性能。这相当于在袋子中寻找中奖票据并丢弃其余部分。很常见的是，剪枝技术最终会产生比原始网络小90%的神经网络结构。显然的问题是：如果一个网络可以缩小尺寸，为什么我们不直接训练这个更小的结构，以便提高训练效率呢？矛盾的是，实际机器学习解决方案中的经验表明，剪枝所揭示的架构从一开始就更难训练，准确性低于原始网络。所以你可以买一大包彩票并逐步找到中奖号码，但相反的过程却太难了。或者我们曾经这样认为????
- en: 'The main idea behind MIT’s Lottery Ticket Hypothesis is that, consistently,
    a large neural network will contain a smaller subnetwork that, if trained from
    the start, will achieve a similar accuracy than the larger structure. Specifically,
    the research paper outlines the hypothesis as following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 麻省理工学院的彩票票据假设的主要思想是，一致地，大型神经网络将包含一个较小的子网络，如果从一开始进行训练，将会实现与较大结构相似的准确度。具体而言，研究论文将假设概述如下：
- en: '***The Lottery Ticket Hypothesis:****A randomly-initialized, dense neural network
    contains a subnetwork that is initialized such that — when trained in isolation
    — it can match the test accuracy of the original network after training for at
    most the same number of iterations.*'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***彩票票据假设：***一个随机初始化的密集神经网络包含一个子网络，该子网络在隔离训练时可以匹配原始网络的测试准确率，经过最多相同数量的迭代训练。*'
- en: In the context of the paper, the small subnetwork is often referred to as the *winning
    ticket.*
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文的背景下，小型子网络通常被称为*中奖票据*。
- en: Consider a neural network in the form of *f(t, a, p) *in which *t= training
    time, a= accuracy and p= parameters*. Now consider *s* being the subset of all
    trainable neural networks from the original structure generated by pruning process.
    The Lottery Ticket Hypothesis tells us that there is *a f’(t’, a’, p’) € s* in
    a way that *t’ <= t, a’>= a and p’ <= p*. In simple terms, conventional pruning
    techniques unveiled neural network structures that are smaller and simpler to
    train than the original.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个形式为 *f(t, a, p)* 的神经网络，其中 *t=训练时间，a=准确率和p=参数*。现在考虑 *s* 为从原始结构中生成的所有可训练神经网络的子集，经过剪枝过程。彩票票据假设告诉我们，存在
    *a f’(t’, a’, p’) ∈ s*，使得 *t’ <= t, a’ >= a 和 p’ <= p*。简单来说，传统的剪枝技术揭示的神经网络结构比原始结构更小且更容易训练。
- en: '![](../Images/4e811ab9746fd2fa3834f3a5f5cd786b.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e811ab9746fd2fa3834f3a5f5cd786b.png)'
- en: 'If the Lottery Ticket Hypothesis is true, then the next obvious question is
    to find the strategy to identify the winning ticket. The process for this involves
    an iterative process of smart training and pruning which can be summarized in
    the following five steps:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果彩票票据假设成立，那么下一个显而易见的问题是找到识别中奖票据的策略。这个过程涉及到一个智能训练和剪枝的迭代过程，可以总结为以下五个步骤：
- en: Randomly initialize a neural network.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化一个神经网络。
- en: Train the network until it converges.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练网络直到其收敛。
- en: Prune a fraction of the network.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剪枝网络的一部分。
- en: To extract the winning ticket, reset the weights of the remaining portion of
    the network to their values from (1) — the initializations they received before
    training began.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了提取“中奖票”，请将网络剩余部分的权重重置为（1）中的值——即训练开始之前它们接收到的初始化值。
- en: To evaluate whether the resulting network at step (4) is indeed a winning ticket,
    train the pruned, untrained network and examine its convergence behavior and accuracy.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了评估步骤（4）中得到的网络是否确实是一个“中奖票”，请训练剪枝后的未训练网络，并检查其收敛行为和准确性。
- en: This process can be run one time or multiple. In the one-shot pruning approach,
    the network is trained once, p% of weights are pruned, and the surviving weights
    are reset. Although one-shot pruning is certainly effective, the Lottery Ticket
    Hypothesis paper showed the best results when the process was applied iteratively
    over n rounds; each round prunes p1/n% of the weights that survive the previous
    round. However, one-shot pruning often produced very solid results without the
    need of computationally expensive training.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程可以运行一次或多次。在一次剪枝方法中，网络训练一次，剪枝p% 的权重，并重置剩余的权重。尽管一次剪枝肯定有效，但彩票票假设论文显示，当该过程在n轮中迭代应用时，效果最佳；每轮剪枝p1/n%的权重，而这些权重在前一轮中幸存下来。然而，一次剪枝通常产生非常稳健的结果，而无需计算上昂贵的训练。
- en: '![](../Images/a44f079fc3b7ed4756eec0de8994568c.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a44f079fc3b7ed4756eec0de8994568c.png)'
- en: The MIT team tested the Lottery Ticket Hypothesis across a group of neural network
    architectures and the results showed that the pruning method was not only able
    to find to optimize the architecture but to find the winning ticket. Look at the
    following chart as
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 麻省理工学院团队在一组神经网络架构中测试了彩票票假设，结果显示剪枝方法不仅能够优化架构，还能找到“中奖票”。查看下图：
- en: Observe two main things in these results. The winning tickets, without the remaining
    redundancy of the wide network, train faster than the wide network. In fact, the
    skinnier they are, the faster they train (within reason). However, if you reinitialize
    the networks’ weights randomly (control), the resulting nets now train slower
    than full network. Therefore, pruning is not just about finding the right architecture,
    it’s also about identifying the ‘winning ticket’, which is a particularly luckily
    initialized subcomponent of the network.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些结果中观察两个主要方面。“中奖票”在没有宽网络的剩余冗余的情况下，比宽网络训练得更快。实际上，它们越瘦，训练得越快（在合理范围内）。然而，如果你随机重新初始化网络的权重（控制），结果网络的训练速度现在比完整网络更慢。因此，剪枝不仅仅是找到正确的架构，还包括识别“中奖票”，这是网络中特别幸运初始化的子组件。
- en: 'Based on the experimental results, the MIT team expanded their initial hypothesis
    with what they called the Lottery System Conjecture which express the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 根据实验结果，麻省理工学院团队扩展了他们最初的假设，提出了所谓的彩票系统猜想，表达了以下内容：
- en: '**The Lottery Ticket Conjecture: **Returning to our motivating question, we
    extend our hypothesis into an untested conjecture that SGD seeks out and trains
    a subset of well-initialized weights. Dense, randomly-initialized networks are
    easier to train than the sparse networks that result from pruning because there
    are more possible subnetworks from which training might recover a winning ticket.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**彩票票猜想：**回到我们的动机问题，我们将假设扩展到一个未经测试的猜想，即SGD寻找并训练一个子集的良好初始化权重。密集的、随机初始化的网络比剪枝后的稀疏网络更容易训练，因为可能的子网络更多，从中训练可能恢复一个“中奖票”。'
- en: The conjecture seems to make sense conceptually. The larger the pool of pruned
    subnetworks, the better chances to find a winning ticket.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个猜想在概念上似乎是有意义的。剪枝子网络的池子越大，找到“中奖票”的机会就越大。
- en: The Lottery Ticket Hypothesis could become one of the most important machine
    learning research papers of recent years as it challenges the conventional wisdom
    in neural network training. While pruning typically proceeds by training the original
    network, removing connections, and further fine-tuning, the Lottery Ticket Hypothesis
    tells us that optimal neural network structures can be learned from the start.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 彩票票假设可能成为近年来最重要的机器学习研究论文之一，因为它挑战了神经网络训练中的传统智慧。尽管剪枝通常通过训练原始网络、移除连接并进一步微调来进行，但彩票票假设告诉我们，最佳的神经网络结构可以从一开始就学习到。
- en: '[Original](https://www.linkedin.com/pulse/how-lottery-ticket-hypothesis-challenging-everything-we-rodriguez/).
    Reposted with permission.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://www.linkedin.com/pulse/how-lottery-ticket-hypothesis-challenging-everything-we-rodriguez/)。经许可转载。'
- en: '**Bio**: [Jesus Rodriguez](https://www.linkedin.com/in/jesusmrv/) is a Managing
    Partner at Invector Labs, crypto & angel investor, speaker, author, columnist
    at CIO.com and AI fanatic.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介**：[耶稣·罗德里格斯](https://www.linkedin.com/in/jesusmrv/)是Invector Labs的管理合伙人、加密货币和天使投资人、演讲者、作家、CIO.com专栏作家以及人工智能爱好者。'
- en: '**Resources:**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：**'
- en: '[On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning
    education](https://www.kdnuggets.com/education/online.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在线和基于网络的：分析、数据挖掘、数据科学、机器学习教育](https://www.kdnuggets.com/education/online.html)'
- en: '[Software for Analytics, Data Science, Data Mining, and Machine Learning](https://www.kdnuggets.com/software/index.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分析、数据科学、数据挖掘和机器学习的软件](https://www.kdnuggets.com/software/index.html)'
- en: '**Related:**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Building a Computer Vision Model: Approaches and datasets](https://www.kdnuggets.com/2019/05/computer-vision-model-approaches-datasets.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[构建计算机视觉模型：方法和数据集](https://www.kdnuggets.com/2019/05/computer-vision-model-approaches-datasets.html)'
- en: '[Autoencoders: Deep Learning with TensorFlow’s Eager Execution](https://www.kdnuggets.com/2019/05/autoencoders-deep-learning-with-tensorflows-eager-execution.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动编码器：使用TensorFlow的即时执行进行深度学习](https://www.kdnuggets.com/2019/05/autoencoders-deep-learning-with-tensorflows-eager-execution.html)'
- en: '[Large-Scale Evolution of Image Classifiers](https://www.kdnuggets.com/2019/05/large-scale-evolution-image-classifiers.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大规模图像分类器的演变](https://www.kdnuggets.com/2019/05/large-scale-evolution-image-classifiers.html)'
- en: More On This Topic
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[3 Things I Wish I Knew When I Started Data Science](https://www.kdnuggets.com/2023/01/3-things-wish-knew-started-data-science.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我希望在开始数据科学时知道的3件事](https://www.kdnuggets.com/2023/01/3-things-wish-knew-started-data-science.html)'
- en: '[12 Most Challenging Data Science Interview Questions](https://www.kdnuggets.com/2022/07/12-challenging-data-science-interview-questions.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12个最具挑战性的数据科学面试问题](https://www.kdnuggets.com/2022/07/12-challenging-data-science-interview-questions.html)'
- en: '[How Multimodality Makes LLM Alignment More Challenging](https://www.kdnuggets.com/how-multimodality-makes-llm-alignment-more-challenging)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多模态如何使LLM对齐变得更加困难](https://www.kdnuggets.com/how-multimodality-makes-llm-alignment-more-challenging)'
- en: '[Hypothesis Testing and A/B Testing](https://www.kdnuggets.com/hypothesis-testing-and-ab-testing)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[假设检验与A/B测试](https://www.kdnuggets.com/hypothesis-testing-and-ab-testing)'
- en: '[Hypothesis Testing Explained](https://www.kdnuggets.com/2021/09/hypothesis-testing-explained.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[假设检验解释](https://www.kdnuggets.com/2021/09/hypothesis-testing-explained.html)'
- en: '[Hypothesis Testing in Data Science](https://www.kdnuggets.com/2023/02/hypothesis-testing-data-science.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中的假设检验](https://www.kdnuggets.com/2023/02/hypothesis-testing-data-science.html)'
