["```py` ```", "```py    The SQL query is then executed from our Airflow pipeline and written to an intermediate S3 location using the following task definition.    ```", "```py    The input to the task, sql_file, determines what query to run on the database. This query will be read-in to the task and then executed against the database. The results of the query will then be written to S3 and the remote file key will be returned as an output of the task.    The screenshot below shows a sample result set of the extraction query from above. We will describe how to create a synthetic version of this dataset in the next section.   ![](../Images/1f5ce925697015f6d932eac9117b1775.png) *Query result preview.*   ## Synthesize Features using Gretel APIs        To generate a synthetic version of each feature, we must first train a synthetic model, and then run the model to generate synthetic records. Gretel has a set of Python SDKs that make it easy to integrate into Airflow tasks.    In addition to the Python Client SDKs, we’ve created a [Gretel Airflow Hook](https://github.com/gretelai/gretel-airflow-pipelines/blob/main/plugins/hooks/gretel.py) that manages Gretel API connections and secrets. After setting up a Gretel Airflow Connection, connecting to the Gretel API is as easy as    ```", "```py    For more information about how to configure Airflow connections, please refer to our Github repository [README](https://github.com/gretelai/gretel-airflow-pipelines#2-configure-airflow-connections).    The project variable in the example above can be used as the main entrypoint for training and running synthetic models using Gretel’s API. For more details, you can check out our [Python API docs](https://python.docs.gretel.ai/en/stable/projects/projects.html).    Referring back to the booking pipeline, we’ll now review the generate_synthetic_features task. This step is responsible for training the synthetic model using the features extracted in the previous task.    ```", "```py    Looking at the method signature, you will see it takes a path, data_source. This value points to the S3 features extracted in the previous step. In a later section we’ll walk through how all these inputs and outputs are wired together.    When creating the model using project.create_model_obj, the model_config param represents the synthetic model configuration used to generate the model. In this pipeline, we’re using our [default model config](https://github.com/gretelai/gretel-blueprints/blob/main/config_templates/gretel/synthetics/default.yml), but many other [configuration options](https://docs.gretel.ai/synthetics/synthetics-model-configuration) are available.    After the model has been configured, we call model.submit_cloud(). This will submit the model for training and record generation using Gretel Cloud. Calling poll(model) will block the task until the model has completed training.    Now that the model has been trained, we’ll use get_artifact_link to return a link to download the generated synthetic features.    ![](../Images/d55c92a904377264013477a9a1646316.png) *Data preview of the synthetic set of features.*     This artifact link will be used as an input to the final upload_synthetic_features step.    ## Load Synthetic Features        The original features have been extracted, and a synthetic version has been created. Now it’s time to upload the synthetic features so they can be accessed by downstream consumers. In this example, we’re going to use an S3 bucket as the final destination for the dataset.    ```", "```py    This task is pretty straightforward. The data_set input value contains a signed HTTP link to download the synthetic dataset from Gretel’s API. The task will read that file into the Airflow worker, and then use the already configured S3 hook to upload the synthetic feature file to an S3 bucket where downstream consumers or models can access it.    ## Orchestrating the Pipeline        Over the last three sections we’ve walked through all the code required to extract, synthesize and load a dataset. The last step is to tie each of these tasks together into a single Airflow pipeline.    If you’ll recall back to the beginning of this post, we briefly mentioned the concept of a DAG. Using Airflow’s TaskFlow API we can compose these three Python methods into a DAG that defines the inputs, outputs and order each step will be run.    ```", "```py    If you follow the path of these method calls, you will eventually get a graph that looks like our original feature pipeline.   ![](../Images/9776c937553639c4bd8886936d454f38.png) *Gretel synthetics pipeline on Airflow.*     If you want to run this pipeline, and see it in action, head over to the [accompanying Github repository](https://github.com/gretelai/gretel-airflow-pipelines). There you will find instructions on how to start an Airflow instance and run the pipeline end to end.    ## Wrapping things up        If you’ve made it this far, you’ve seen how Gretel can be integrated into a data pipeline built on Airflow. By combining Gretel’s developer friendly APIs, and Airflow’s powerful system of hooks and operators it’s easy to build ETL pipelines that make data more accessible and safer to use.    We also talked about a common feature engineering use case where sensitive data may not be readily accessible. By generating a synthetic version of the dataset, we reduce the risk of exposing any sensitive data, but still retain the utility of the dataset while making it quickly available to those who need it.    Thinking about the feature pipeline in more abstract terms, we now have a pattern that can be repurposed for any number of new SQL queries. By deploying a new version of the pipeline, and swapping out the initial SQL query, we can front any potentially sensitive query with a synthetic dataset that preserves customer privacy. The only line of code that needs to change is the path to the sql file. No complex data engineering required.    ## Thanks for reading        Send us an email at [hi@gretel.ai](https://gretel.ai/blog/running-gretel-on-apache-airflow#) or come join us in [Slack](https://gretel.ai/slackinvite) if you have any questions or comments. We’d love to hear how you’re using Airflow and how we can best integrate with your existing data pipelines.      **Bio: [Drew Newberry](https://www.linkedin.com/in/drew-newberry/)** is a Software Engineer at Gretel.ai.    [Original](https://gretel.ai/blog/running-gretel-on-apache-airflow). Reposted with permission.    **Related:**    *   [Prefect: How to Write and Schedule Your First ETL Pipeline with Python](/2021/08/prefect-write-schedule-etl-pipeline-python.html) *   [15 Python Snippets to Optimize your Data Science Pipeline](/2021/08/15-python-snippets-optimize-data-science-pipeline.html) *   [How to Query Your Pandas Dataframe](/2021/08/query-pandas-dataframe.html)     * * *      ## Our Top 3 Course Recommendations      ![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\\. [Google Cybersecurity Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast track to a career in cybersecurity.    ![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\\. [Google Data Analytics Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up your data analytics game    ![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\\. [Google IT Support Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support your organization in IT    * * *      ### More On This Topic    *   [5 Airflow Alternatives for Data Orchestration](https://www.kdnuggets.com/5-airflow-alternatives-for-data-orchestration) *   [6 Data Science Technologies You Need to Build Your Supply Chain Pipeline](https://www.kdnuggets.com/2022/01/6-data-science-technologies-need-build-supply-chain-pipeline.html) *   [How to Build a Scalable Data Architecture with Apache Kafka](https://www.kdnuggets.com/2023/04/build-scalable-data-architecture-apache-kafka.html) *   [High-Fidelity Synthetic Data for Data Engineers and Data Scientists Alike](https://www.kdnuggets.com/2022/tonic-high-fidelity-synthetic-data-engineers-scientists-alike.html) *   [How to Democratize AI/ML and Data Science with AI-generated Synthetic Data](https://www.kdnuggets.com/2022/11/mostly-ai-democratize-aiml-data-science-aigenerated-synthetic-data.html) *   [Data access is severely lacking in most companies, and 71% believe…](https://www.kdnuggets.com/2023/07/mostly-data-access-severely-lacking-synthetic-data-help.html) ```"]