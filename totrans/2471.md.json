["```py\ntf.keras.applications.Xception(\n    include_top=True,\n    weights=\"imagenet\",\n    input_tensor=None,\n    input_shape=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation=\"softmax\",\n)\n```", "```py\nmodel = tf.keras.Sequential([\n    embed,\n    tf.keras.layers.Dense(16, activation=\"relu\"),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n])\n```", "```py\nm = tf.keras.Sequential([\n    hub.KerasLayer(\"https://tfhub.dev/google/imagenet/inception_v1/classification/5\")\n])\nm.build([None, 224, 224, 3])  # Batch input shape.\n```", "```py\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_datasets as tfds\n```", "```py\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\ndata_augmentation = keras.Sequential(\n   [layers.RandomFlip(\"horizontal\"), layers.RandomRotation(0.1),]\n)\n```", "```py\nbase_model = keras.applications.Xception(\n   weights=\"imagenet\",  # Weights pre-trained on ImageNet.\n   input_shape=(150, 150, 3),\n   include_top=False,\n)\n\nbase_model.trainable = False\n```", "```py\n# Create a new model on top\ninputs = keras.Input(shape=(150, 150, 3))\nx = data_augmentation(inputs)  # Apply random data augmentation\n```", "```py\nbase_model.trainable = True\n```", "```py\nmodel.compile(\n   optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate\n   loss=keras.losses.BinaryCrossentropy(from_logits=True),\n   metrics=[keras.metrics.BinaryAccuracy()],\n)\nepochs = 10\nmodel.fit(train_ds, epochs=epochs, validation_data=validation_ds)\n```", "```py\n! pip install transformers\n\nfrom transformers import pipeline\n\nclassifier = pipeline('sentiment-analysis')\n\nclassifier('I am finding the article about Transfer learning very useful.')\n```", "```py\n[{'label': 'POSITIVE', 'score': 0.9968850016593933}]\n```", "```py\nfrom transformers import TrainingArguments\n```", "```py\nurl = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n\ndataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n                                 untar=True, cache_dir='.',\n                                 cache_subdir='')\n\ndataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\nos.listdir(dataset_dir)\n```", "```py\n# Embed a 1,000-word vocabulary into 5 dimensions.\n\nembedding_layer = tf.keras.layers.Embedding(1000, 5)\n```", "```py\n# text vectorization layer to split, and map strings to integers.\n\nvectorize_layer = TextVectorization(\n   standardize=custom_standardization,\n   max_tokens=vocab_size,\n   output_mode='int',\n   output_sequence_length=sequence_length)\n```", "```py\nmodel = Sequential([\n vectorize_layer,\n Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n GlobalAveragePooling1D(),\n Dense(16, activation='relu'),\n Dense(1)\n])\n```", "```py\nmodel.compile(optimizer='adam',\n             loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n             metrics=['accuracy'])\n```"]