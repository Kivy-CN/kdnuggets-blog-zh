- en: 'Introducing AI Explainability 360: A New Toolkit to Help You Understand what
    Machine Learning Models are Doing'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/08/introducing-ai-explainability-360-toolkit-understand-machine-learning-models.html](https://www.kdnuggets.com/2019/08/introducing-ai-explainability-360-toolkit-understand-machine-learning-models.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)![Figure](../Images/066fab538f9b0815ffb6cac2f09d73ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Interpretability is one of the most difficult challenges in modern machine learning
    solutions. While building sophisticated machine learning models is getting easier,
    understanding how models develop knowledge and arrive to conclusions remains a
    very difficult challenge. Typically, the more accurate the models the harder they
    are to interpret. Recently, AI researchers from IBM open sourced [AI Explainability
    360](http://aix360.mybluemix.net/), a new toolkit of state-of-the-art algorithms
    that support the interpretability and explainability of machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The release of AI Explainability 360 is the first practical implementation of
    ideas outlined in dozens of research papers in the last few years. In the same
    way traditional software applications incorporate instrumentation code to help
    its runtime monitoring, machine learning models need to add interpretability techniques
    to facilitate debugging, troubleshooting and versioning. However, interpreting
    machine learning models is a multiple of complexity higher than traditional software
    applications. For starters, we need have very little understanding of what makes
    a machine learning model interpretable.
  prefs: []
  type: TYPE_NORMAL
- en: The Building Blocks of Interpretability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A few months ago, researchers from Google published [a very comprehensive paper
    in which they outlined the core components of interpretability](https://distill.pub/2018/building-blocks/).
    The paper presents four fundamental challenges to make a model interpretable:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Understanding what Hidden Layers Do:** The bulk of the knowledge in a deep
    learning model is formed in the hidden layers. Understanding the functionality
    of the different hidden layers at a macro level is essential to be able to interpret
    a deep learning model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Understanding what Hidden Layers Do:** The bulk of the knowledge in a deep
    learning model is formed in the hidden layers. Understanding the functionality
    of the different hidden layers at a macro level is essential to be able to interpret
    a deep learning model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Understanding How Nodes are Activated:** The key to interpretability is not
    to understand the functionality of individual neurons in a network but rather
    groups of interconnected neurons that fire together in the same spatial location.
    Segmenting a network by groups of interconnected neurons will provide a simpler
    level of abstraction to understand its functionality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Understanding How Concepts are Formed:** Understanding how deep neural network
    forms individual concepts that can then be assembled into the final output is
    another key building block of interpretability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The other aspect to realize is that the interpretability of AI modes decreases
    as their sophistication increases. Do you care about obtaining the best results
    or do you care about understanding how those results were produced? That’s a essence
    of the interpretability vs. accuracy friction that is at the center of every machine
    learning scenario. Many deep learning techniques are complex in nature and, although
    they result very accurate in many scenarios, they can become incredibly difficult
    to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: To address the challenges of explainability, machine learning models need to
    start incorporating interpretable building blocks a first class citizen. That’s
    the goal of IBM’s new toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: AI Explainability 360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AI Explainability 360 is an open source toolkit for streamlining the implementation
    of interpretable machine learning models. The toolkit includes a series of interpretability
    algorithms that reflect state-of-the-art research in this topic as well as an
    intuitive user interface that helps understand machine learning models from different
    perspectives. One of the main contributions of the AI Explainability 360 is that
    it doesn’t rely on a single form of interpretation for a machine learning model.
    In the same way that humans rely on rich, expressive explanations to interpret
    a specific outcome, the explainability of machine learning models varies depending
    on personas and context involved. AI Explainability 360, produces different explanations
    for different roles such as data scientists or business stake holders.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b9aee3e41db3607a28a591d11eb0f1c.png)'
  prefs: []
  type: TYPE_IMG
- en: The explanations generated by AI Explainability 360 can be divided in two main
    groups depending on whether they are based on the data or the models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data:** Understanding the characteristics of a dataset is very often the
    shortest path to explainability. This is specially true in supervised learning
    algorithms that rely heavily on datasets in order to build relevant knowledge.
    Sometimes the features in a given dataset are meaningful to consumers, but other
    times they are entangled, i.e. multiple meaningful attributes are combined together
    in a single feature. AI Explainability 360 includes several algorithms that focus
    on data interpretability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model:** The interpretation of models is the key building block of any machine
    learning explainability. There are several ways to make a machine learning model
    comprehensible to consumers. The first distinction is direct interpretability
    vs. post hoc explanation. Directly interpretable models are model formats such
    as decision trees, Boolean rule sets, and generalized additive models, that are
    fairly easily understood by people and learned straight from the training data.
    Post hoc explanation methods first train a black box model and then build another
    explanation model on top of the black box model. The second distinction is global
    vs. local explanation. Global explanations are for entire models whereas local
    explanations are for single sample points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/144c477bc71123ff1fa2b7a32d4969ea.png)'
  prefs: []
  type: TYPE_IMG
- en: The distinction between global interpretable models, global and local post hoc
    explanations is one of the key contributions of AI Explainability 360\. Typically,
    global interpretable models are better suited for scenarios that need a complete
    and discrete interpretation paths of the models. Many of those scenarios include
    areas such as safety, reliability or compliance. Global post hoc explanations
    are useful for decision maker personas that are being supported by the machine
    learning model. Physicians, judges, and loan officers develop an overall understanding
    of how the model works, but there is necessarily a gap between the black box model
    and the explanation. Local post hoc explanations are relevant to induvial personas
    such as patients, defendants of applicants that are affected by the outcome of
    a model and need to understand its interpretation from a their very specific viewpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Developers can start using AI Explainability 360 by incorporating the interpretable
    components using the APIs included in the toolkit. Additionally, AI Explainability
    360 includes [a series of demos and tutorials](https://github.com/IBM/AIX360/blob/master/examples/README.md)
    that can help developers get started relatively quickly. Finally, the toolkit
    provides a very simple user interface that can be used to get started with the
    concepts of machine learning interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability is one of the most important building blocks of modern machine
    learning applications. AI Explainability 360 provides one of the most complete
    stacks to simplify the interpretability of machine learning programs without having
    to become an expert in this particular area of research. It is likely that we
    will see some of the ideas behind AI Explainability 360 being incorporated into
    mainstream deep learning frameworks and platforms.
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/introducing-ai-explainability-360-a-new-toolkit-to-help-you-understand-what-machine-learning-93438d734d04).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Two Major Difficulties in AI and One Applied Solution](/2019/02/ai-chess-difficulties-solution.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Key Concepts in Andrew Ng’s “Machine Learning Yearning”](/2019/08/key-concepts-andrew-ng-machine-learning-yearning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How Creating an AI Study Group Boosted My Skills and Got Me a Job](/2019/08/ai-study-group-boosted-skills-got-job.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[What certification are you adding to your toolkit in 2022?](https://www.kdnuggets.com/2022/03/sas-certification-adding-toolkit-2022.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Comprehensive Survey on Trustworthy Graph Neural Networks:…](https://www.kdnuggets.com/2022/05/comprehensive-survey-trustworthy-graph-neural-networks-privacy-robustness-fairness-explainability.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How ML Model Explainability Accelerates the AI Adoption Journey for…](https://www.kdnuggets.com/2022/07/ml-model-explainability-accelerates-ai-adoption-journey-financial-services.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introducing Healthcare-Specific Large Language Models from John Snow Labs](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introducing TPU v4: Googles Cutting Edge Supercomputer for Large…](https://www.kdnuggets.com/2023/04/introducing-tpu-v4-googles-cutting-edge-supercomputer-large-language-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introducing MPT-7B: A New Open-Source LLM](https://www.kdnuggets.com/2023/05/introducing-mpt7b-new-opensource-llm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
