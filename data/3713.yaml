- en: 'Approaches to Text Summarization: An Overview'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本总结方法概述
- en: 原文：[https://www.kdnuggets.com/2019/01/approaches-text-summarization-overview.html](https://www.kdnuggets.com/2019/01/approaches-text-summarization-overview.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/01/approaches-text-summarization-overview.html](https://www.kdnuggets.com/2019/01/approaches-text-summarization-overview.html)
- en: '![Approaches to Text Summarization: An Overview](../Images/0501fd9d406e6cac4cee064e52464f23.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![文本总结方法概述](../Images/0501fd9d406e6cac4cee064e52464f23.png)'
- en: Image By Editor
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由编辑提供
- en: 'The *bona fide* semantic understanding of human language text, exhibited by
    its effective summarization, may well be the holy grail of natural language processing
    (NLP). That statement isn''t as hyperbolic as it sounds: as true human language
    understanding definitely *is* the holy grail of NLP, and genuine effective summarization
    of said human language would necessarily entail true understanding, transitivity
    would back me up on this.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 人类语言文本的*真实*语义理解，其有效总结的表现，可能是自然语言处理（NLP）的圣杯。这个说法听起来并不夸张：因为真正的人类语言理解确实是NLP的圣杯，而真正有效的总结必然涉及真实的理解，这一点通过传递性可以得到支持。
- en: Unfortunately — or perhaps not, depending on your outlook — honest to goodness
    "understanding" of human language is not something we can currently count on for
    text summarization. However, the show must go on, and there currently exist an
    array of actual techniques for summarizing text, some of which stretch back decades.
    These techniques take different approaches to reach the same goal, and can be
    classified into a fairly narrow set of categories for pursuing their shared goal.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是——或者说不幸的是，这取决于你的观点——目前我们无法依靠真正的“理解”来进行文本总结。然而，事情必须继续进行，目前确实存在一系列实际的文本总结技术，其中一些已经存在了几十年。这些技术采取不同的方法来实现相同的目标，并且可以被归类为几个较窄的类别，以追求它们共同的目标。
- en: This article will present the main approaches to text summarization currently
    employed, as well as discuss some of their characteristics.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将介绍目前采用的主要文本总结方法，并讨论它们的一些特征。
- en: Automated Text Summarization Techniques
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化文本总结技术
- en: To be clear, when we say "automated text summarization," we are talking about
    employing machines to perform the summarization of a document or document using
    some form of heuristics or statistical methods. A summary in this case is a shortened
    piece of text which accurately captures and conveys the most important and relevant
    information contained in the document or documents we want to be summarized. As
    hinted at above, there are a number of these different tried and truly automated
    text summarization techniques that are currently in use.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 明确来说，当我们谈到“自动化文本总结”时，我们指的是利用机器通过某种形式的启发式或统计方法来进行文档或文档的总结。在这种情况下，总结是一个简短的文本，准确捕捉和传达我们希望总结的文档中最重要和相关的信息。如上所述，目前有许多经过验证的自动化文本总结技术正在使用中。
- en: 'There are a few ways of going about classifying automated text summarization
    techniques, as can be seen in Figure 1\. This article will explore these techniques
    from the point of view of summarization *output type*. In this regard, there are
    2 categories of techniques: extractive and abstraction.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以对自动化文本总结技术进行分类，如图 1 所示。本文将从总结*输出类型*的角度探讨这些技术。在这方面，技术可以分为 2 类：抽取式和抽象式。
- en: '![Approaches to Text Summarization: An Overview](../Images/40d151b32f9a8e5e1fc8740658fb6576.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![文本总结方法概述](../Images/40d151b32f9a8e5e1fc8740658fb6576.png)'
- en: '**Figure 1.** Automated text summarization approaches (source: [Kushal Chauhan,
    Jutana](https://medium.com/jatana/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1),
    modified).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.** 自动化文本总结方法（来源：[Kushal Chauhan, Jutana](https://medium.com/jatana/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1)，已修改）。'
- en: '**Extractive text summarization** methods function by identifying the important
    sentences or excerpts from the text and reproducing them verbatim as part of the
    summary. No new text is generated; only existing text is used in the summarization
    process.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**抽取式文本总结** 方法通过识别文本中的重要句子或摘录，并逐字复现这些内容作为总结的一部分。不会生成新的文本；总结过程中只使用现有文本。'
- en: '**Abstractive text summarization** methods employ more powerful natural language
    processing techniques to interpret text and generate new summary text, as opposed
    to selecting the most representative existing excerpts to perform the summarization.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**抽象文本总结**方法采用更强大的自然语言处理技术来解释文本并生成新的总结文本，而不是选择最具代表性的现有摘录进行总结。'
- en: While both are valid approaches to text summarization, it should not be difficult
    to convince you that abstractive techniques are far more difficult to implement.
    In fact, the majority of summarization processes today are extraction-based. This
    doesn't mean that abstractive methods should be discounted or ignored; on the
    contrary, research into their implementation — and true semantic understanding
    of human language in general — is a worthy pursuit, and much work is needed before
    we can confidently say that we have gained a true foothold in this endeavor.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这两种方法都是有效的文本总结方法，但要让你相信抽象技术远比实现难度更大应该不难。实际上，现在大多数总结过程都是基于抽取的。这并不意味着抽象方法应该被忽视或忽略；相反，对其实现的研究——以及对人类语言真正语义理解的研究——是值得追求的，在我们能够自信地说我们在这一领域真正取得了进展之前，还需要做大量的工作。
- en: For this reason, the rest of this article will focus on the specifics of extractive
    text summarization, and its differing implementation techniques.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本文其余部分将重点关注抽取式文本总结的具体内容及其不同的实现技术。
- en: Extractive Summarization
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抽取式总结
- en: 'Extractive summarization techniques vary, yet they all share the same basic
    tasks:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 抽取式总结技术各有不同，但它们都共享相同的基本任务：
- en: Construct an intermediate representation of the input text (text to be summarized)
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建输入文本（待总结文本）的中间表示
- en: Score the sentences based on the constructed intermediate representation
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于构建的中间表示对句子进行评分
- en: Select a summary consisting of the top *k* most important sentences
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个包含前*k*最重要句子的摘要
- en: 'Tasks 2 and 3 are straightforward enough; in *sentence scoring*, we want to
    determine how well each sentence relays important aspects of the text being summarized,
    while *sentence selection* is performed using some specific optimization approach.
    Algorithms for each of these 2 steps can vary, but they are conceptually quite
    simple: assign a score to each sentence using some metric, and then select from
    the best-scored sentences via some well-defined sentence selection method.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 任务2和任务3足够简单；在*句子评分*中，我们希望确定每个句子传达文本摘要重要方面的效果，而*句子选择*则使用一些特定的优化方法进行。每个这两个步骤的算法可能有所不同，但概念上相当简单：使用某种度量分配分数给每个句子，然后通过某种明确的句子选择方法从最高分的句子中进行选择。
- en: The first task, intermediate representation, could use further elaboration.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个任务，中间表示，可能需要进一步阐述。
- en: Intermediate Representation
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 中间表示
- en: Some sense needs to be made of natural language prior to its sentence scoring
    and selection, and creating some intermediate representation of each sentence
    serves this purpose. The 2 major categories of intermediate representation, *topic
    representation* and *indicator representation*, are briefly defined below, as
    are their sub-categories.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在对自然语言进行句子评分和选择之前，需要对其进行某种程度的理解，创建每个句子的中间表示可以实现这个目的。中间表示的两个主要类别，*主题表示*和*指标表示*，在下面简要定义了它们及其子类别。
- en: '**Topic Representation** - transformation of the text with a focus on text
    topic identification; major sub-categories of this approach are:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**主题表示** - 通过关注文本主题识别来转换文本；该方法的主要子类别包括：'
- en: frequency-driven approaches
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于频率的方法
- en: topic word approaches
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题词方法
- en: '[latent semantic analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis)
    (LSA)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[潜在语义分析](https://en.wikipedia.org/wiki/Latent_semantic_analysis)（LSA）'
- en: Bayesian topic models — [latent Dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)
    (LDA), for example
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯主题模型——例如[潜在狄利克雷分配](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)（LDA）
- en: The 2 most popular word frequency approaches are word probability and [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 两种最流行的词频方法是词概率和[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)。
- en: 'In topic words approaches, there are 2 ways to compute a sentence''s importance:
    by the number of topic signatures it contains (the number of topics the sentence
    discusses), or by the proportion of topics the sentence contains versus the number
    of topics contained in the text. As such, the first of these tends to reward longer
    sentences, while the second measures topic word density.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在主题词方法中，有两种计算句子重要性的方法：通过句子包含的主题签名的数量（句子讨论的主题数量），或者通过句子包含的主题与文本中包含的主题数量的比例。因此，第一个方法倾向于奖励较长的句子，而第二个方法则衡量主题词密度。
- en: Explanations of latent semantic analysis and Bayesian topic model approaches,
    such as LDA, are beyond the scope of this article, but can be read about in the
    links above.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在语义分析和贝叶斯主题模型方法（如LDA）的解释超出了本文的范围，但可以在上述链接中阅读。
- en: '![Approaches to Text Summarization: An Overview](../Images/d10abecad603535ec8a190e7755a7789.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![文本总结方法概述](../Images/d10abecad603535ec8a190e7755a7789.png)'
- en: '**Figure 2.** Constructing bag of words feature vectors (source: [Dipanjan
    Sarkar](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41)).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.** 构建词袋特征向量（来源：[Dipanjan Sarkar](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41)）。'
- en: '**Indicator Representation** - transformation of each sentence in the text
    into a list of features of importance; possible features include:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**指示器表示** - 将文本中的每个句子转换为一个重要性特征列表；可能的特征包括：'
- en: sentence length
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子长度。
- en: sentence position
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子位置。
- en: whether sentence contains a particular word (see Figure 2 for an example of
    such a feature extraction method, [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model))
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子是否包含特定单词（请参见图2以了解这种特征提取方法的示例，[词袋模型](https://en.wikipedia.org/wiki/Bag-of-words_model)）。
- en: whether sentence contains a particular phrase
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子是否包含特定短语。
- en: 'Using a set of features to represent and rank the text data can be performed
    using one of 2 overarching indicator representation methods: graph methods and
    machine learning methods.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一组特征来表示和排序文本数据可以通过两种总体指示器表示方法之一来完成：图方法和机器学习方法。
- en: 'Using **graph representations**:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**图表示**：
- en: we find that sub-graphs end up representing topics covered in the text
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们发现子图最终表示文本中涉及的主题。
- en: we are able to isolate important sentences in the text, given that these are
    the ones which would be connected to a greater number of other sentences (if you
    consider sentences as vertices and sentence similarity represented by edges)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们能够在文本中隔离重要句子，因为这些句子会与更多的其他句子连接（如果你将句子视为顶点，句子相似性表示为边）。
- en: we do not need to consider language-specific processing, and the same methods
    can be applied to a variety of languages
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不需要考虑特定语言的处理，并且相同的方法可以应用于各种语言。
- en: we can often find that the semantic information gained via graph-exposed sentence
    similarity enhances summarization performance beyond more simple frequency approaches
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通常发现，通过图揭示的句子相似性获得的语义信息，在总结性能上往往优于更简单的频率方法。
- en: 'With **machine learning representations**:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**机器学习表示**：
- en: the summarization problem is modeled as a classification problem
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结问题被建模为分类问题。
- en: we require labeled training data to build a classifier to classify sentences
    as summary or non-summary sentences
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要标注的训练数据来建立一个分类器，以将句子分类为摘要句子或非摘要句子。
- en: to combat the labeled data conundrum, alternatives such as semi-supervised learning
    show promise
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了应对标注数据困境，半监督学习等替代方案显示出希望。
- en: we find that certain methods which assume dependency between sentences often
    outperform other techniques
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们发现某些假设句子之间有依赖关系的方法通常比其他技术表现更好。
- en: Text summarization is an exciting sub-discipline of natural language processing.
    While a variety of approaches to extractive summarization are in use and are being
    researched daily, an understanding of the basis of the concepts above should allow
    you to have some understanding of how any of these operate, at least at a 30,000
    foot level. You should also be able to pick up recent papers or read recent implementation
    blog posts with some confidence you have the basic level of understanding necessary
    for such an undertaking.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 文本摘要是自然语言处理的一个令人兴奋的子领域。虽然多种提取式摘要的方法正在使用并每天进行研究，但对上述概念基础的理解应使你能够对这些方法有一定的了解，至少在30,000英尺的层面上。你也应该能够自信地阅读最近的论文或实施博客文章，具备进行此类工作的基本理解。
- en: 'Much of this information owes a debt of gratitude to the paper, [Text Summarization
    Techniques: A Brief Survey](https://arxiv.org/abs/1707.02268), by Mehdi Allahyari,
    Seyedamin Pouriyeh, Mehdi Assefi, Saeid Safaei, Elizabeth D. Trippe, Juan B. Gutierrez,
    and Krys Kochut.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息大部分得益于Mehdi Allahyari, Seyedamin Pouriyeh, Mehdi Assefi, Saeid Safaei, Elizabeth
    D. Trippe, Juan B. Gutierrez, 和 Krys Kochut 的论文[文本摘要技术：简要调查](https://arxiv.org/abs/1707.02268)。
- en: 'References & further reading:'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献与进一步阅读：
- en: '[Text Summarization Techniques: A Brief Survey](https://arxiv.org/abs/1707.02268),
    Mehdi Allahyari, Seyedamin Pouriyeh, Mehdi Assefi, Saeid Safaei, Elizabeth D.
    Trippe, Juan B. Gutierrez, Krys Kochut, 2017.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本摘要技术：简要调查](https://arxiv.org/abs/1707.02268)，Mehdi Allahyari, Seyedamin
    Pouriyeh, Mehdi Assefi, Saeid Safaei, Elizabeth D. Trippe, Juan B. Gutierrez,
    Krys Kochut, 2017。'
- en: '[Unsupervised Text Summarization using Sentence Embeddings](https://medium.com/jatana/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1),
    Kushal Chauhan, 2018.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用句子嵌入的无监督文本摘要](https://medium.com/jatana/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1)，Kushal
    Chauhan, 2018。'
- en: 'Also, this article focused on extractive summarization, but you can find more
    about abstractive summarization in the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，本文集中在提取式摘要，但你可以在以下内容中找到有关抽象摘要的更多信息：
- en: '[Abstractive and Extractive Text Summarization using Document Context Vector
    and Recurrent Neural Networks](https://arxiv.org/abs/1807.08000), Chandra Khatri,
    Gyanit Singh, Nish Parikh, 2018.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用文档上下文向量和递归神经网络的抽象和提取式文本摘要](https://arxiv.org/abs/1807.08000)，Chandra Khatri,
    Gyanit Singh, Nish Parikh, 2018。'
- en: '[Neural Abstractive Text Summarization with Sequence-to-Sequence Models](https://arxiv.org/abs/1812.02303),
    Tian Shi, Yaser Keneshloo, Naren Ramakrishnan, Chandan K. Reddy, 2018.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用序列到序列模型的神经抽象文本摘要](https://arxiv.org/abs/1812.02303)，Tian Shi, Yaser Keneshloo,
    Naren Ramakrishnan, Chandan K. Reddy, 2018。'
- en: '**[Matthew Mayo](https://www.linkedin.com/in/mattmayo13/)** ([**@mattmayo13**](https://twitter.com/mattmayo13))
    is a Data Scientist and the Editor-in-Chief of KDnuggets, the seminal online Data
    Science and Machine Learning resource. His interests lie in natural language processing,
    algorithm design and optimization, unsupervised learning, neural networks, and
    automated approaches to machine learning. Matthew holds a Master''s degree in
    computer science and a graduate diploma in data mining. He can be reached at editor1
    at kdnuggets[dot]com.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Matthew Mayo](https://www.linkedin.com/in/mattmayo13/)** ([**@mattmayo13**](https://twitter.com/mattmayo13))
    是一位数据科学家，也是KDnuggets的主编，KDnuggets是一个开创性的在线数据科学和机器学习资源。他的兴趣包括自然语言处理、算法设计与优化、无监督学习、神经网络以及机器学习的自动化方法。Matthew拥有计算机科学硕士学位和数据挖掘研究生文凭。他可以通过editor1
    at kdnuggets[dot]com联系。'
- en: '* * *'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT'
- en: '* * *'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Data Labeling for Machine Learning: Market Overview, Approaches, and Tools](https://www.kdnuggets.com/2021/12/data-labeling-ml-overview-and-tools.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习的数据标注：市场概览、方法和工具](https://www.kdnuggets.com/2021/12/data-labeling-ml-overview-and-tools.html)'
- en: '[Getting Started with Automated Text Summarization](https://www.kdnuggets.com/2019/11/getting-started-automated-text-summarization.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动化文本摘要入门](https://www.kdnuggets.com/2019/11/getting-started-automated-text-summarization.html)'
- en: '[Text Summarization Development: A Python Tutorial with GPT-3.5](https://www.kdnuggets.com/2023/04/text-summarization-development-python-tutorial-gpt35.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本摘要开发：使用 GPT-3.5 的 Python 教程](https://www.kdnuggets.com/2023/04/text-summarization-development-python-tutorial-gpt35.html)'
- en: '[Summarization with GPT-3](https://www.kdnuggets.com/2022/04/packt-summarization-gpt3.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 GPT-3 进行摘要生成](https://www.kdnuggets.com/2022/04/packt-summarization-gpt3.html)'
- en: '[Unlocking GPT-4 Summarization with Chain of Density Prompting](https://www.kdnuggets.com/unlocking-gpt-4-summarization-with-chain-of-density-prompting)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[解锁 GPT-4 摘要生成：密度提示链法](https://www.kdnuggets.com/unlocking-gpt-4-summarization-with-chain-of-density-prompting)'
- en: '[Extractive Summarization with LLM using BERT](https://www.kdnuggets.com/extractive-summarization-with-llm-using-bert)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 BERT 的提取式摘要生成](https://www.kdnuggets.com/extractive-summarization-with-llm-using-bert)'
