- en: 'PySpark SQL Cheat Sheet: Big Data in Python'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark SQL 备忘单：Python 中的大数据
- en: 原文：[https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html](https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html](https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html)
- en: '**By Karlijn Willems, [DataCamp](https://www.datacamp.com/).**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：Karlijn Willems，来自 [DataCamp](https://www.datacamp.com/)。**'
- en: Big Data With Python
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Python 处理大数据
- en: 'Big data is everywhere and is traditionally characterized by three V’s: Velocity,
    Variety and Volume. Big data is fast, is varied and has a huge volume. As a data
    scientist, data engineer, data architect, ... or whatever the role is that you’ll
    assume in the data science industry, you’ll definitely get in touch with big data
    sooner or later, as companies now gather an enormous amount of data across the
    board.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据无处不在，传统上由三个 V 特征来描述：速度、种类和体量。大数据是快速的、多样的且体量庞大。作为数据科学家、数据工程师、数据架构师……无论你在数据科学行业中扮演什么角色，你都会早晚接触到大数据，因为公司现在在各个领域收集了大量数据。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全领域的职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Data doesn’t always mean information, though, and that is where you, data science
    enthusiast, come in. You can make use of Apache Spark, “a fast and general engine
    for large-scale data processing” to start to tackle the challenges that big data
    poses to you and the company you’re working for.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据并不总是意味着信息，这正是你，数据科学爱好者，能够发挥作用的地方。你可以利用 Apache Spark，一个“快速且通用的大规模数据处理引擎”，来开始应对大数据给你和你所在公司带来的挑战。
- en: 'Nevertheless, doubts may always arise when you’re working with Spark and when
    they do, take a look at DataCamp’s [Apache Spark Tutorial: ML with PySpark tutorial](https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning)
    or download the [cheat sheet](https://www.datacamp.com/community/blog/pyspark-sql-cheat-sheet)
    for free!'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，在使用 Spark 时疑问总会出现，当你遇到这些问题时，查看一下 DataCamp 的 [Apache Spark 教程：使用 PySpark
    进行机器学习](https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning)
    或下载 [备忘单](https://www.datacamp.com/community/blog/pyspark-sql-cheat-sheet) 是一个好主意！
- en: In what follows, we’ll dive deeper into the structure and the contents of the
    cheat sheet.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨备忘单的结构和内容。
- en: PySpark Cheat Sheet
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PySpark 备忘单
- en: PySpark is the Spark Python API exposes the Spark programming model to Python.
    Spark SQL, then, is a module of PySpark that allows you to work with structured
    data in the form of DataFrames. This stands in contrast to RDDs, which are typically
    used to work with unstructured data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 是 Spark 的 Python API，将 Spark 编程模型暴露给 Python。Spark SQL 是 PySpark 的一个模块，允许你以
    DataFrames 的形式处理结构化数据。这与通常用于处理非结构化数据的 RDDs 相对立。
- en: '![PySpark cheat sheet](../Images/f31b5ce6452a08f76098ca13c56b14cb.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark 备忘单](../Images/f31b5ce6452a08f76098ca13c56b14cb.png)'
- en: '**Tip:** if you want to learn more about the differences between RDDs and DataFrames,
    but also about how Spark DataFrames differ from pandas DataFrames, you should
    definitely check out the [Apache Spark in Python: Beginner''s Guide](https://www.datacamp.com/community/tutorials/apache-spark-python).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示：** 如果你想了解更多关于 RDD 和 DataFrames 之间的区别，以及 Spark DataFrames 如何与 pandas DataFrames
    区别，你一定要查看一下 [Apache Spark in Python: Beginner''s Guide](https://www.datacamp.com/community/tutorials/apache-spark-python)。'
- en: Initializing SparkSession
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始化 SparkSession
- en: 'If you want to start working with Spark SQL with PySpark, you’ll need to start
    a SparkSession first: you can use this to create DataFrames, register DataFrames
    as tables, execute SQL over the tables and read parquet files. Don’t worry if
    all of this sounds very new to you - You’ll read more about this later on in this
    article!'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想开始使用PySpark的Spark SQL，你需要先启动一个SparkSession：你可以用它来创建DataFrames，注册DataFrames为表，执行SQL查询以及读取parquet文件。如果这些听起来对你来说很陌生，不用担心
    - 你将在本文后面阅读更多相关内容！
- en: You start a SparkSession by first importing it from the sql module that comes
    with the pyspark package. Next, you can initialize a variable spark, for example,
    to not only build the SparkSession, but also give the application a name, set
    the config and then use the getOrCreate() method to either get a SparkSession
    if there is already one running or to create one if that isn’t the case yet! That
    last method will come in extremely handy, especially for future reference, because
    it will prevent you from running multiple SparkSessions at the same time!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你通过首先从pyspark包附带的sql模块中导入SparkSession来开始一个SparkSession。接下来，你可以初始化一个变量spark，例如，不仅构建SparkSession，还可以为应用程序命名，设置配置，然后使用getOrCreate()方法来获取一个SparkSession（如果已经有一个正在运行），或者创建一个新的SparkSession（如果还没有的话）！这个方法将非常有用，特别是为了将来参考，因为它可以防止你同时运行多个SparkSessions！
- en: Cool, isn’t it?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 很酷，不是吗？
- en: '![PySpark cheat sheet](../Images/3c86b85673b094ab3ed9a4afa5142260.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark备忘单](../Images/3c86b85673b094ab3ed9a4afa5142260.png)'
- en: Inspecting the Data
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查数据
- en: When you have imported your data, it’s time to inspect the Spark DataFrame by
    using some of the built-in attributes and methods. This way, you’ll get to know
    your data a little bit better before you start manipulating the DataFrames.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当你导入数据后，是时候通过使用一些内置的属性和方法来检查Spark DataFrame了。这样，你可以在开始操作DataFrames之前，更好地了解你的数据。
- en: Now, you’ll probably already know most of the methods and attributes mentioned
    in this section of the cheat sheet  from working with pandas DataFrames or NumPy,
    such as dtypes, head(), describe(), count(),... There are also some methods that
    might be new to you, such as the take() or printSchema() method, or the schema
    attribute.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能已经了解了备忘单中提到的大多数方法和属性，这些都是你在使用pandas DataFrames或NumPy时熟悉的，比如dtypes、head()、describe()、count()等。还有一些方法可能对你来说是新的，例如take()或printSchema()方法，或者schema属性。
- en: '![PySpark cheat sheet](../Images/2b87b5681e4fc864b638d73298521c50.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark备忘单](../Images/2b87b5681e4fc864b638d73298521c50.png)'
- en: Nevertheless, you’ll see that the ramp-up is quite modest, as you can leverage
    your previous knowledge on data science packages maximally.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，你会发现上手过程相当平稳，因为你可以最大限度地利用你之前对数据科学包的知识。
- en: Duplicate Values
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重复值
- en: As you’re inspecting your data, you might find that there are some duplicate
    values. To remediate this, you can use the dropDuplicates() method, for example,
    to drop duplicate values in your Spark DataFrame.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当你检查数据时，你可能会发现存在一些重复值。为了解决这个问题，你可以使用dropDuplicates()方法，例如，来删除Spark DataFrame中的重复值。
- en: Queries
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查询
- en: You might remember that one of the main reasons for using Spark DataFrames in
    the first place is that you have a more structured way of dealing with your data
    - Querying is one of those examples of handling your data in a more structured
    way, whether you’re using SQL in a relational database, SQL-like language in No-SQL
    databases, the [query()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html?highlight=query#pandas.DataFrame.query)
    method on Pandas DataFrames, and so on.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，使用Spark DataFrames的主要原因之一是你可以以更结构化的方式处理数据 - 查询就是这种更结构化的数据处理方式之一，无论你是使用关系数据库中的SQL，还是No-SQL数据库中的类似SQL的语言，亦或是Pandas
    DataFrames上的[query()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html?highlight=query#pandas.DataFrame.query)方法，等等。
- en: '![PySpark cheat sheet](../Images/8a57af6b8937cff2c56c8dfbf857efe5.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark备忘单](../Images/8a57af6b8937cff2c56c8dfbf857efe5.png)'
- en: 'Now that we’re talking about Pandas DataFrames, you’ll notice that Spark DataFrames
    follow a similar principle: you’re using methods to get to know your data better.
    In this case, though, you won’t use query(). Instead, you’ll have to make use
    of other methods to get the results you want to get back: in a first instance,
    select() and show() will be your friends whenever you want to retrieve information
    from your DataFrames.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在谈论 Pandas DataFrames 时，你会发现 Spark DataFrames 遵循类似的原则：你使用方法来更好地了解你的数据。然而，在这种情况下，你不会使用
    `query()`。相反，你需要利用其他方法来获得你想要的结果：在第一次使用时，`select()` 和 `show()` 将是你检索 DataFrames
    信息时的好帮手。
- en: 'Within these methods, you can build up your query. As with standard SQL, you
    specify what columns you exactly want to get back within select(). You can do
    this with a regular string or by specifying the column name with the help of your
    DataFrame itself, as in the following example: df.lastName or df[“firstName”].'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些方法中，你可以构建你的查询。与标准 SQL 一样，你可以在 `select()` 中指定你确切想要获取的列。你可以使用普通字符串或通过 DataFrame
    本身来指定列名，如以下示例所示：`df.lastName` 或 `df[“firstName”]`。
- en: Note that the latter approach will give you some more freedom in some cases
    where you want to specify what information you exactly want to retrieve with additional
    functions such as isin() or startswith(), for example.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，后一种方法在某些情况下会给你更多的自由，例如当你想通过 `isin()` 或 `startswith()` 等附加函数来指定你确切想要检索的信息时。
- en: 'Besides select(), you can also make use of the functions module in PySpark
    SQL to specify, for example, a when clause in your query, as demonstrated in the
    table below:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `select()` 之外，你还可以利用 PySpark SQL 中的 functions 模块来指定，例如在查询中使用 `when` 子句，如下表所示：
- en: '![PySpark cheat sheet](../Images/d5234f75f8ad26d04f778d4b0bbf5d52.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark 速查表](../Images/d5234f75f8ad26d04f778d4b0bbf5d52.png)'
- en: All in all, you can see that there are a lot of possibilities for those who
    want to handle and get to know their data to a larger extent with the help of
    select(), help() and the functions of the functions module.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，你可以看到，有很多方法可以通过 `select()`、`help()` 和 functions 模块的功能来更大程度地处理和了解数据。
- en: Add, Update & Remove Columns
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加、更新和删除列
- en: You might also want to look into adding, updating or removing some columns from
    your Spark DataFrame. You can easily do this with the withColumn(), withColumnRenamed()
    and drop() methods. You’ll probably know by now that you also have a drop() method
    at your disposal when you’re working with Pandas DataFrames. You can read more
    [here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html?highlight=drop#pandas.DataFrame.drop).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还想了解如何添加、更新或删除 Spark DataFrame 中的一些列。你可以使用 `withColumn()`、`withColumnRenamed()`
    和 `drop()` 方法轻松完成此操作。你可能现在已经知道，在使用 Pandas DataFrames 时也可以使用 `drop()` 方法。你可以在 [这里](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html?highlight=drop#pandas.DataFrame.drop)
    了解更多。
- en: '![PySpark cheat sheet](../Images/751902d5d047e7357ce04767eb6020aa.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark 速查表](../Images/751902d5d047e7357ce04767eb6020aa.png)'
- en: GroupBy
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分组
- en: Again, just like with Pandas DataFrames, you might want to group by certain
    values and aggregate some values - The example below shows how you use the groupBy()
    method, in combination with count() and show() to retrieve and show your data,
    grouped by age, together with the number of people who have that certain age.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，就像在 Pandas DataFrames 中一样，你可能会想按某些值进行分组并聚合一些值 - 下面的示例展示了如何使用 `groupBy()`
    方法，并结合 `count()` 和 `show()` 来检索和显示按年龄分组的数据，以及具有特定年龄的人数。
- en: '![PySpark cheat sheet](../Images/283cb11c84f4170ae04c5534ce466532.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark 速查表](../Images/283cb11c84f4170ae04c5534ce466532.png)'
- en: More On This Topic
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[PySpark for Data Science](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PySpark 数据科学](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
- en: '[Data Validation for PySpark Applications using Pandera](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Pandera 进行 PySpark 应用程序的数据验证](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
- en: '[Data Cleaning with Python Cheat Sheet](https://www.kdnuggets.com/2023/02/data-cleaning-python-cheat-sheet.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 进行数据清洗速查表](https://www.kdnuggets.com/2023/02/data-cleaning-python-cheat-sheet.html)'
- en: '[Best Python Tools for Building Generative AI Applications Cheat Sheet](https://www.kdnuggets.com/2023/08/best-python-tools-generative-ai-cheat-sheet.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用于构建生成 AI 应用程序的最佳 Python 工具速查表](https://www.kdnuggets.com/2023/08/best-python-tools-generative-ai-cheat-sheet.html)'
- en: '[Python Control Flow Cheat Sheet](https://www.kdnuggets.com/2022/11/python-control-flow-cheatsheet.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 控制流速查表](https://www.kdnuggets.com/2022/11/python-control-flow-cheatsheet.html)'
- en: '[KDnuggets News, July 5: A Rotten Data Science Project • 10 AI…](https://www.kdnuggets.com/2023/n24.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，7月5日：一个糟糕的数据科学项目 • 10 人工智能…](https://www.kdnuggets.com/2023/n24.html)'
