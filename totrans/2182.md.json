["```py\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\n\n# Sample DataFrame\ndata = {'age': [25, 30, np.nan, 35, 40], 'salary': [50000, 60000, 55000, np.nan, 65000]}\ndf = pd.DataFrame(data)\n\n# Fill in missing ages using the mean\nmean_imputer = SimpleImputer(strategy='mean')\ndf['age'] = mean_imputer.fit_transform(df[['age']])\n\n# Fill in the missing salaries using the median\nmedian_imputer = SimpleImputer(strategy='median')\ndf['salary'] = median_imputer.fit_transform(df[['salary']])\n\nprint(df)\n```", "```py\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\n# Sample DataFrame\ndata = {'color': ['red', 'blue', 'green', 'blue', 'red']}\ndf = pd.DataFrame(data)\n\n# Implementing one-hot encoding\none_hot_encoder = OneHotEncoder()\none_hot_encoding = one_hot_encoder.fit_transform(df[['color']]).toarray()\ndf_one_hot = pd.DataFrame(one_hot_encoding, columns=one_hot_encoder.get_feature_names_out(['color']))\n\n# Implementing label encoding\nlabel_encoder = LabelEncoder()\ndf['color_label'] = label_encoder.fit_transform(df['color'])\n\nprint(df)\nprint(df_one_hot)\n```", "```py\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n# Sample DataFrame\ndata = {'age': [25, 30, 35, 40, 45], 'salary': [50000, 60000, 55000, 65000, 70000]}\ndf = pd.DataFrame(data)\n\n# Standardize data\nscaler_standard = StandardScaler()\ndf['age_standard'] = scaler_standard.fit_transform(df[['age']])\n\n# Min-Max Scaling\nscaler_minmax = MinMaxScaler()\ndf['salary_minmax'] = scaler_minmax.fit_transform(df[['salary']])\n\n# Robust Scaling\nscaler_robust = RobustScaler()\ndf['salary_robust'] = scaler_robust.fit_transform(df[['salary']])\n\nprint(df)\n```", "```py\nimport pandas as pd\nimport numpy as np\n\n# Sample DataFrame\ndata = {'x1': [1, 2, 3, 4, 5], 'x2': [10, 20, 30, 40, 50]}\ndf = pd.DataFrame(data)\n\n# Polynomial Features\ndf['x1_squared'] = df['x1'] ** 2\ndf['x1_x2_interaction'] = df['x1'] * df['x2']\n\nprint(df)\n```", "```py\nimport pandas as pd\nfrom sklearn.decomposition import PCA\n\n# Sample DataFrame\ndata = {'feature1': [2.5, 0.5, 2.2, 1.9, 3.1], 'feature2': [2.4, 0.7, 2.9, 2.2, 3.0]}\ndf = pd.DataFrame(data)\n\n# Use PCA for Dimensionality Reduction\npca = PCA(n_components=1)\ndf_pca = pca.fit_transform(df)\ndf_pca = pd.DataFrame(df_pca, columns=['principal_component'])\n\nprint(df_pca)\n```", "```py\nimport pandas as pd\nimport numpy as np\n\n# Sample DataFrame\ndate_rng = pd.date_range(start='1/1/2022', end='1/10/2022', freq='D')\ndata = {'date': date_rng, 'value': [100, 110, 105, 115, 120, 125, 130, 135, 140, 145]}\ndf = pd.DataFrame(data)\ndf.set_index('date', inplace=True)\n\n# Lag Features\ndf['value_lag1'] = df['value'].shift(1)\n\n# Rolling Statistics\ndf['value_rolling_mean'] = df['value'].rolling(window=3).mean()\n\nprint(df)\n```"]