- en: Must Read NLP Papers from the Last 12 Months
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过去12个月必读的NLP论文
- en: 原文：[https://www.kdnuggets.com/2023/03/must-read-nlp-papers-last-12-months.html](https://www.kdnuggets.com/2023/03/must-read-nlp-papers-last-12-months.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/03/must-read-nlp-papers-last-12-months.html](https://www.kdnuggets.com/2023/03/must-read-nlp-papers-last-12-months.html)
- en: '![Must Read NLP Papers from the Last 12 Months](../Images/def988fd066629d818503834d972600a.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![过去12个月必读的NLP论文](../Images/def988fd066629d818503834d972600a.png)'
- en: Photo by [Anil Sharma](https://www.pexels.com/photo/birds-perched-on-tree-branch-10952775/) on [Pexels](https://www.pexels.com/photo/birds-perched-on-tree-branch-10952775/)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Anil Sharma](https://www.pexels.com/photo/birds-perched-on-tree-branch-10952775/) 拍摄的照片，发布于[Pexels](https://www.pexels.com/photo/birds-perched-on-tree-branch-10952775/)
- en: Since the **groundbreaking release** of [BERT](https://arxiv.org/abs/1810.04805) in
    October 2018, machine learning has achieved ever greater heights through clever
    optimization and augmented compute. BERT, which stands for Bidirectional Encoder
    Representations from Transformers, introduced a new paradigm in neural network
    architecture. The **transformer** has served as a significant unlock in machine
    learning capabilities.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 自从2018年10月**突破性发布**的[BERT](https://arxiv.org/abs/1810.04805)以来，通过巧妙的优化和增强计算，机器学习达到了更高的高度。BERT，即双向编码器表示变换器，引入了神经网络架构的新范式。**变换器**在机器学习能力方面起到了重要的解锁作用。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Further advancements in the field of Natural Language Processing (NLP) have
    improved foreign language translation, enhanced no-code applications, increased
    the fluency of chatbots, and very quickly set new standards for an array of state-of-the
    art benchmarks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）领域的进一步进展改善了外语翻译，增强了无代码应用程序，提高了聊天机器人的流利度，并迅速为一系列最先进的基准设定了新的标准。
- en: 'Alongside these remarkable accomplishments, the development of large language
    models (LLMs) has not been without controversy. In the 2021 "[Stochastic Parrots](https://dl.acm.org/doi/10.1145/3442188.3445922)"
    paper, a team of researchers including machine learning engineer and ethicist
    Timnit Gebru criticized these models for:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些显著成就，大型语言模型（LLMs）的发展也不乏争议。在2021年发表的"[随机鹦鹉](https://dl.acm.org/doi/10.1145/3442188.3445922)"论文中，包括机器学习工程师和伦理学家Timnit
    Gebru在内的研究团队批评了这些模型：
- en: Levying a damning** environmental cost**
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 施加了一个令人谴责的**环境成本**
- en: '**Excluding marginalized voices** through inelegant curation of the training
    data set'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**排除边缘化声音** 通过不优雅的数据集策划'
- en: '**Plagiarizing** internet content and stealing from human writers'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**剽窃** 互联网内容和盗用人类创作者的作品'
- en: Gebru was summarily fired from her position on Google's Ethical Artificial Intelligence
    Team.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Gebru被迅速解雇了谷歌伦理人工智能团队的职位。
- en: In this writeup
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在这篇文章中
- en: We explore four NLP papers published in the past year that represent the latest
    advancements. Understanding these developments will improve your capabilities
    as a Data Scientist and put you at the forefront of this dynamic research space.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了过去一年中发表的四篇NLP论文，它们代表了最新的进展。理解这些发展将提高你作为数据科学家的能力，并使你处于这个动态研究领域的前沿。
- en: 1. [Training Compute Optimal Large Language Models](https://arxiv.org/abs/2203.15556)
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. [训练计算最优的大型语言模型](https://arxiv.org/abs/2203.15556)
- en: This paper examines the ideal model size and token count for a language model
    using the transformer architecture. It aims to answer the question of what constitutes
    the ideal number of parameters and size of dataset for a model trained under a
    predetermined compute budget.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了使用变换器架构的语言模型的理想模型大小和标记数。它旨在回答在预定计算预算下，模型的理想参数数量和数据集大小是什么。
- en: The researchers found that in prior cases, LLMs seem to have been severely undertrained.
    The authors criticize these teams for overemphasizing the scaling of compute resources
    while underemphasizing the importance of training data volume.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员发现，在以往的案例中，大型语言模型（LLMs）似乎严重训练不足。作者批评这些团队过于强调计算资源的扩展，而忽视了训练数据量的重要性。
- en: The authors concluded that for compute-optimal training, model size and the
    number of training tokens should be scaled equally. In other words,
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 作者总结认为，为了实现计算最优化训练，模型规模和训练标记的数量应该等比例扩展。换句话说，
- en: for every doubling of model size, the number of training tokens should also
    be doubled.
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 每当模型规模翻倍时，训练标记的数量也应翻倍。
- en: The research showed that a relatively small model (70B parameters) trained on
    4 times more training data could consistently beat larger models (up to 530B parameters)
    at state-of-the-art benchmark tests such as Multi-task Language Understanding
    ([MMLU](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 研究表明，一个相对较小的模型（70B参数）在训练数据量增加4倍的情况下，能够在最先进的基准测试（如多任务语言理解（[MMLU](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu)）中
    consistently 超越较大的模型（最高可达530B参数）。
- en: The enhanced training data allows the smaller model to utilize significantly
    less compute resources for inference and fine-tuning. This bodes well for downstream
    utilization.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 增强的训练数据使得较小的模型在推理和微调时可以使用显著更少的计算资源。这对下游应用具有良好的前景。
- en: '**TL;DR** — this paper shows that the prior understanding of scaling laws was
    incorrect. In fact, when trained with a properly extensive token count, smaller
    networks can be significantly better than larger ones.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结** — 本文表明，之前对扩展规律的理解是错误的。实际上，当使用足够广泛的标记数量进行训练时，较小的网络可以显著优于较大的网络。'
- en: 2. [Training Language Models to Follow Instructions with Human Feedback](https://arxiv.org/abs/2203.02155)
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. [通过人类反馈训练语言模型以遵循指令](https://arxiv.org/abs/2203.02155)
- en: Enhancing the compute provided to LLMs does not automatically improve their
    ability to interpret user intent. As a troubling consequence of this fact, LLMs
    may provide results that are untruthful or harmful.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 增强LLMs的计算能力并不会自动提升它们解读用户意图的能力。作为这个事实的一个令人担忧的结果，LLMs可能会提供不真实或有害的结果。
- en: This paper highlights a novel method for fine-tuning language models using human
    feedback to better align the output with user intent across a variety of tasks.
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本文强调了一种使用人类反馈来微调语言模型的新方法，以便在各种任务中更好地使输出与用户意图对齐。
- en: The researchers gathered a dataset starting from a collection of OpenAI API
    prompts. They then utilize the data to fine-tune GPT-3 via supervised learning.
    Then, using reinforcement learning based on user input, they generated a new dataset
    ranking model outputs. The researchers then used this data to further fine-tune
    the supervised model, resulting in a model they called InstructGPT.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员从OpenAI API提示集合开始，收集了一个数据集。他们利用这些数据通过监督学习对GPT-3进行微调。随后，基于用户输入的强化学习生成了一个新的数据集，用于对模型输出进行排名。研究人员然后使用这些数据进一步微调监督模型，最终得到一个他们称之为InstructGPT的模型。
- en: Compared to the original GPT-3, InstructGPT has 100 times fewer parameters,
    and yet it is capable of outperforming GPT-3 in human assessments.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始的GPT-3相比，InstructGPT的参数少了100倍，但它在人工评估中却能够超越GPT-3。
- en: On test data, the InstructGPT model is more likely to respond honestly and less
    likely to create harmful content. Though InstructGPT still occasionally makes
    basic errors, these findings demonstrate that fine-tuning with a human-in-the-loop
    serves as a viable route for matching language models with human intent.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据上，InstructGPT模型更有可能诚实回应，且不太可能生成有害内容。尽管InstructGPT仍偶尔会犯基本错误，这些发现表明，通过人机互动的微调是使语言模型与人类意图匹配的一条可行路线。
- en: '**TL;DR** — this paper shows that doing reinforcement learning with human feedback
    is an extremely helpful, low-resource way to make existing models more useful.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结** — 本文表明，进行人类反馈强化学习是一种极其有用的低资源方式，可以使现有模型更具实用性。'
- en: 3. [A Generalist Agent](https://www.deepmind.com/publications/a-generalist-agent)
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. [通用智能体](https://www.deepmind.com/publications/a-generalist-agent)
- en: This paper explores improvements resulting in a model capable of playing Atari,
    captioning pictures, generating text, stacking physical blocks using a robot arm,
    and much more.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了改进后的模型，这些模型能够进行Atari游戏、图像描述、文本生成、利用机器人手臂堆叠物理块等更多任务。
- en: The model, Gato, is composed of a single neural network with unchanged weights
    across assorted tasks.
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 模型Gato由一个单一的神经网络组成，在各种任务中权重保持不变。
- en: Gato resulted from scaled up behavior cloning, a form of sequence modeling challenge.
    The challenge of encoding many modalities into a single vector space of tokens
    constituted the most significant barrier the researchers faced in their efforts.
    The study makes a number of advancements in tokenization of standard vision and
    language datasets. In addition, the researchers sought novel solutions to the
    typical sequence model problem of determining context window length.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Gato源于放大的行为克隆，这是一种序列建模挑战。将多个模态编码到一个单一的标记向量空间中构成了研究人员在努力过程中面临的最大障碍。研究在标准视觉和语言数据集的标记化方面取得了一些进展。此外，研究人员还寻求了新的解决方案，以应对典型序列模型问题中的上下文窗口长度确定问题。
- en: '**TL;DR** — this paper shows that multimodal models can very well and are likely
    the future of the modeling paradigm. In contrast to previous state-of-the-art
    models that were capable of performing only in a narrow area, Gato executes a
    generalist policy capable of a variety tasks and multiple modalities.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结** — 这篇论文展示了多模态模型表现良好，并且很可能是建模范式的未来。与以前只能在狭窄领域表现的最先进模型相比，Gato执行了能够处理各种任务和多种模态的通用策略。'
- en: 4. [Large Language Models are Zero Shot Reasoners](https://arxiv.org/abs/2205.11916)
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4. [大型语言模型是零样本推理者](https://arxiv.org/abs/2205.11916)
- en: LLMs are remarkable few-shot learners using narrow, task-specific examples.
    This research paper demonstrates that LLMs are also competent zero-shot reasoners,
    particularly when prompted with the phrase, "let’s think step by step."
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在使用狭窄、特定任务的示例进行少样本学习方面非常出色。这篇研究论文表明，LLMs 也是合格的零样本推理者，特别是当用“我们逐步思考”这个短语进行提示时。
- en: Yes, you read that right.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，你没读错。
- en: Instructing an LLM to “think step by step” actually improves results enough
    to justify a paper.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 指示LLM“逐步思考”实际上能显著改善结果，足以为其撰写论文。
- en: The model created by authors Kojima et al. surpassed existing benchmarks on
    reasoning tasks, such as arithmetic (e.g., MultiArith, GSM8K, AQUA-RAT, SVAMP),
    symbolic reasoning (e.g., Last Letter, Coin Flip), and logical reasoning (e.g.,
    Date Understanding, Tracking Shuffled Objects).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 作者Kojima等人创建的模型在推理任务上超越了现有的基准，例如算术（如MultiArith、GSM8K、AQUA-RAT、SVAMP）、符号推理（如Last
    Letter、Coin Flip）和逻辑推理（如Date Understanding、Tracking Shuffled Objects）。
- en: The adaptability of this single prompt, "think step by step," over a wide range
    of reasoning tasks suggests that the zero-shot skills were previously significantly
    underutilized. Remarkably high-level, multi-task capabilities may be retrieved
    simply by employing a linguistic framing of the problem that requests a higher
    cognitive load.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这一单一提示“逐步思考”的适应性表明，零样本技能此前被显著低估了。通过采用要求更高认知负荷的语言框架来提出问题，可能会显著恢复非常高水平的多任务能力。
- en: My mind is blown.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我的脑袋被震撼了。
- en: '**TL;DR — **this paper shows that the quality of a LLM''s answer is largely
    dependent on the wording of the prompt'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结** — 这篇论文表明，LLM回答的质量在很大程度上依赖于提示的措辞。'
- en: Summary
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Machine learning has advanced significantly in the past four years. Only time
    will tell if this pace of development can be sustained.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 过去四年间，机器学习取得了显著进展。只有时间才能证明这种发展速度是否能够持续下去。
- en: These papers discuss the latest enhancements in NLP, revealing considerable
    room for continued improvement in training processes to involve larger datasets
    and human-in-the-loop reinforcement learning.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这些论文讨论了NLP的最新改进，揭示了在训练过程中涉及更大数据集和人类反馈强化学习方面还有相当大的改进空间。
- en: Recent research also explores the creation of multi-modal paradigms and enhanced
    zero-shot reasoning capabilities via simple alterations to the model’s input prompts.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究还探讨了通过对模型输入提示进行简单的调整，来创建多模态范式和增强零样本推理能力。
- en: '**[Nicole Janeway Bills](https://www.linkedin.com/in/nicole-janeway-bills/)**
    is the Community Organizer at Data Strategy Professionals. She offers a proven
    track record of training data practitioners to quickly and effectively ace the
    CDMP Exams. In her work as a Data Strategy consultant, Nicole has helped set up
    data collection, data storage, and data analytics functions. She applies best
    practices to solve clients’ most pressing challenges. Furthermore, she has worked
    as a Data Scientist and Project Manager for federal and commercial consulting
    teams. Her business experience includes natural language processing, cloud computing,
    statistical testing, pricing analysis, ETL processes, and web and application
    development.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Nicole Janeway Bills](https://www.linkedin.com/in/nicole-janeway-bills/)**
    是数据战略专业人员的社区组织者。她在培训数据从业者以快速有效地通过CDMP考试方面有着验证的成功记录。在她作为数据战略顾问的工作中，Nicole帮助建立了数据收集、数据存储和数据分析功能。她运用最佳实践来解决客户最紧迫的挑战。此外，她还曾在联邦和商业咨询团队中担任数据科学家和项目经理。她的业务经验包括自然语言处理、云计算、统计测试、定价分析、ETL过程以及网页和应用程序开发。'
- en: '[Original](https://medium.com/towards-data-science/must-read-nlp-papers-f9d38cda0b65).
    Reposted with permission.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/towards-data-science/must-read-nlp-papers-f9d38cda0b65)。经许可转载。'
- en: More On This Topic
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[KDnuggets News, April 27: A Brief Introduction to Papers With Code;…](https://www.kdnuggets.com/2022/n17.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets新闻，4月27日：关于代码论文的简要介绍；…](https://www.kdnuggets.com/2022/n17.html)'
- en: '[Top Machine Learning Papers to Read in 2023](https://www.kdnuggets.com/2023/03/top-machine-learning-papers-read-2023.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2023年必读的顶尖机器学习论文](https://www.kdnuggets.com/2023/03/top-machine-learning-papers-read-2023.html)'
- en: '[Generative Agent Research Papers You Should Read](https://www.kdnuggets.com/generative-agent-research-papers-you-should-read)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你应该阅读的生成代理研究论文](https://www.kdnuggets.com/generative-agent-research-papers-you-should-read)'
- en: '[5 Machine Learning Papers to Read in 2024](https://www.kdnuggets.com/5-machine-learning-papers-to-read-in-2024)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2024年必读的5篇机器学习论文](https://www.kdnuggets.com/5-machine-learning-papers-to-read-in-2024)'
- en: '[5 Free Data Science Books You Must Read in 2023](https://www.kdnuggets.com/2023/01/5-free-data-science-books-must-read-2023.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2023年你必须阅读的5本免费数据科学书籍](https://www.kdnuggets.com/2023/01/5-free-data-science-books-must-read-2023.html)'
- en: '[8 Innovative BERT Knowledge Distillation Papers That Have Changed…](https://www.kdnuggets.com/2022/09/eight-innovative-bert-knowledge-distillation-papers-changed-nlp-landscape.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8篇创新性的BERT知识蒸馏论文，它们改变了……](https://www.kdnuggets.com/2022/09/eight-innovative-bert-knowledge-distillation-papers-changed-nlp-landscape.html)'
