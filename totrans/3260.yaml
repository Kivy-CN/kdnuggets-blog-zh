- en: Lessons Learned From Benchmarking Fast Machine Learning Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从快速机器学习算法基准测试中学到的经验
- en: 原文：[https://www.kdnuggets.com/2017/08/lessons-benchmarking-fast-machine-learning-algorithms.html](https://www.kdnuggets.com/2017/08/lessons-benchmarking-fast-machine-learning-algorithms.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/08/lessons-benchmarking-fast-machine-learning-algorithms.html](https://www.kdnuggets.com/2017/08/lessons-benchmarking-fast-machine-learning-algorithms.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png)[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png)[评论](#comments)'
- en: '**By Miguel Fierro, Mathew Salvaris, Guolin Ke, and Tao Wu, all at Microsoft**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由微软的Miguel Fierro、Mathew Salvaris、Guolin Ke和Tao Wu撰写**。'
- en: 'Boosted decision trees are responsible for more than half of the winning solutions
    in machine learning challenges hosted at Kaggle, according to [KDnuggets](/2016/03/xgboost-implementing-winningest-kaggle-algorithm-spark-flink.html).
    In addition to superior performance, these algorithms have practical appeal as
    they require minimal tuning. In this post, we evaluate two popular tree boosting
    software packages: [XGBoost](https://github.com/dmlc/xgboost) and [LightGBM](https://github.com/Microsoft/LightGBM),
    including their GPU implementations. Our results, based on tests on six datasets,
    are summarized as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[KDnuggets](/2016/03/xgboost-implementing-winningest-kaggle-algorithm-spark-flink.html)，提升决策树在Kaggle主办的机器学习挑战中占据了超过一半的获胜解决方案。除了卓越的性能，这些算法在实际应用中也具有吸引力，因为它们只需最少的调整。在这篇文章中，我们评估了两个流行的树提升软件包：
    [XGBoost](https://github.com/dmlc/xgboost) 和 [LightGBM](https://github.com/Microsoft/LightGBM)，包括它们的GPU实现。我们基于六个数据集的测试结果总结如下：
- en: XGBoost and LightGBM achieve similar accuracy metrics.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: XGBoost和LightGBM实现了类似的准确率指标。
- en: LightGBM has lower training time than XGBoost and its histogram-based variant,
    XGBoost hist, for all test datasets, on both CPU and GPU implementations. The
    training time difference between the two libraries depends on the dataset, and
    can be as big as 25 times.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有测试数据集上，无论是CPU还是GPU实现，LightGBM的训练时间都低于XGBoost及其基于直方图的变体XGBoost hist。两个库之间的训练时间差异取决于数据集，可能大到25倍。
- en: XGBoost GPU implementation does not scale well to large datasets and ran out
    of memory in half of the tests.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: XGBoost GPU实现对大数据集的扩展性较差，在一半的测试中内存不足。
- en: XGBoost hist may be significantly slower than the original XGBoost when feature
    dimensionality is high.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当特征维度较高时，XGBoost hist可能显著慢于原始的XGBoost。
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的IT'
- en: '* * *'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: All our code is open-source and can be found in [this repo](https://github.com/Azure/fast_retraining/).
    We will explain the algorithms behind these libraries and evaluate them across
    different datasets. Do you like your machine learning to be quick? Then keep reading.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的所有代码都是开源的，可以在[this repo](https://github.com/Azure/fast_retraining/)中找到。我们将解释这些库背后的算法，并在不同的数据集上进行评估。你喜欢你的机器学习快速吗？那就继续阅读吧。
- en: The Basics of Boosted Decision Trees
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提升决策树的基础
- en: '[Gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting) is a machine
    learning technique that produces a prediction model in the form of an ensemble
    of weak classifiers, optimizing a differentiable loss function. One of the most
    popular types of gradient boosting is boosted decision trees, that internally
    is made up of an ensemble of weak [decision trees](https://en.wikipedia.org/wiki/Decision_tree_learning).
    There are two different strategies to compute the trees: level-wise and leaf-wise.
    The level-wise strategy grows the tree level by level. In this strategy, each
    node splits the data prioritizing the nodes closer to the tree root. The leaf-wise
    strategy grows the tree by splitting the data at the nodes with the highest loss
    change. Level-wise growth is usually better for smaller datasets whereas leaf-wise
    tends to overfit. Leaf-wise growth [tends to excel in larger datasets](http://researchcommons.waikato.ac.nz/handle/10289/2317) where
    it is considerably faster than level-wise growth.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[梯度提升](https://en.wikipedia.org/wiki/Gradient_boosting)是一种机器学习技术，它生成一个以弱分类器的集合形式存在的预测模型，优化可微分的损失函数。最受欢迎的梯度提升类型之一是提升决策树，它内部由一组弱[决策树](https://en.wikipedia.org/wiki/Decision_tree_learning)组成。计算树的方法有两种不同策略：按层次和按叶节点。按层次策略逐层生长树。在这种策略中，每个节点优先分裂接近树根的节点。按叶节点策略通过在具有最高损失变化的节点处分裂数据来生长树。按层次生长通常适用于较小的数据集，而按叶节点生长则容易过拟合。按叶节点生长在[大数据集上往往表现优异](http://researchcommons.waikato.ac.nz/handle/10289/2317)，其速度远快于按层次生长。'
- en: '![](../Images/12f36bb713456993f64393f64e51e30e.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12f36bb713456993f64393f64e51e30e.png)'
- en: A key challenge in training boosted decision trees is the [computational cost
    of finding the best split](https://arxiv.org/abs/1706.08359) for each leaf. Conventional
    techniques find the [exact split](https://arxiv.org/abs/1603.02754) for each leaf,
    and require scanning through all the data in each iteration. A different approach [approximates
    the split](https://arxiv.org/abs/1611.01276) by building histograms of the features.
    That way, the algorithm doesn’t need to evaluate every single value of the features
    to compute the split, but only the bins of the histogram, which are bounded. This
    approach turns out to be much more efficient for large datasets, without adversely
    affecting accuracy.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 训练提升决策树的一个关键挑战是每个叶节点的[寻找最佳分裂的计算成本](https://arxiv.org/abs/1706.08359)。传统技术找到每个叶节点的[准确分裂](https://arxiv.org/abs/1603.02754)，并需要在每次迭代中扫描所有数据。另一种方法通过构建特征的直方图来[近似分裂](https://arxiv.org/abs/1611.01276)。这样，算法无需评估每个特征的所有值来计算分裂，而只需评估直方图的区间，这些区间是有限的。这种方法对大数据集效率更高，同时不影响准确性。
- en: '[XGBoost started in 2014](http://homes.cs.washington.edu/~tqchen/2016/03/10/story-and-lessons-behind-the-evolution-of-xgboost.html),
    and it has become popular due to its use in many [winning Kaggle competition entries](https://github.com/dmlc/xgboost/blob/master/demo/README.md).
    Originally XGBoost was based on a level-wise growth algorithm, but recently has
    added an option for leaf-wise growth that implements split approximation using
    histograms. We refer to this version as XGBoost hist. LightGBM is a more recent
    arrival, started in March 2016 and open-sourced in August 2016\. It is based on
    a leaf-wise algorithm and histogram approximation, and has attracted a lot of
    attention due to its speed (Disclaimer: Guolin Ke, a co-author of this blog post,
    is a key contributor to LightGBM). Apart from multithreaded CPU implementations,
    GPU acceleration is now available on both [XGBoost](https://github.com/dmlc/xgboost/tree/master/plugin/updater_gpu) and [LightGBM](https://github.com/Microsoft/LightGBM/blob/master/docs/GPU-Tutorial.md) too.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[XGBoost 于2014年开始](http://homes.cs.washington.edu/~tqchen/2016/03/10/story-and-lessons-behind-the-evolution-of-xgboost.html)，由于在许多[获胜的
    Kaggle 竞赛中](https://github.com/dmlc/xgboost/blob/master/demo/README.md)的应用而变得流行。最初，XGBoost
    基于按层次生长算法，但最近增加了一个叶节点生长选项，利用直方图实现分裂近似。我们称这一版本为 XGBoost hist。LightGBM 是一个较新的工具，始于2016年3月，并在2016年8月开源。它基于叶节点算法和直方图近似，由于其速度引起了广泛关注（免责声明：这篇博客文章的合著者
    Guolin Ke 是 LightGBM 的主要贡献者）。除了多线程 CPU 实现外，GPU 加速现在也在[XGBoost](https://github.com/dmlc/xgboost/tree/master/plugin/updater_gpu)和[LightGBM](https://github.com/Microsoft/LightGBM/blob/master/docs/GPU-Tutorial.md)上可用。'
- en: Evaluating XGBoost and LightGBM
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估 XGBoost 和 LightGBM
- en: We performed [machine learning experiments](https://github.com/Azure/fast_retraining/) across
    six different datasets. These experiments are in the python notebooks in our [github
    repo](https://github.com/Azure/fast_retraining/). We also showed the specific [compilation
    versions of XGBoost and LightGBM](https://github.com/Azure/fast_retraining/blob/master/INSTALL.md) that
    we used and provided the steps to [install them and set up the experiments](https://github.com/Azure/fast_retraining/blob/master/INSTALL.md).
    We tried classification and regression problems with both CPU and GPU.  All experiments
    were run on an Azure NV24 VM with 24 cores, 224 GB of memory and NVIDIA M60 GPUs.
    The operating system was Ubuntu 16.04\. In all experiments, we found XGBoost and
    LightGBM had similar accuracy metrics (F1-scores are shown here), so we focused
    on training times in this blog post. The table below shows training times and
    the training time ratios between the two libraries in both CPU and GPU implementations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在六个不同的数据集上进行了[machine learning experiments](https://github.com/Azure/fast_retraining/)。这些实验的代码在我们的[github
    repo](https://github.com/Azure/fast_retraining/)中的python notebooks里。我们还展示了我们使用的具体[XGBoost和LightGBM的编译版本](https://github.com/Azure/fast_retraining/blob/master/INSTALL.md)，并提供了[安装它们和设置实验的步骤](https://github.com/Azure/fast_retraining/blob/master/INSTALL.md)。我们尝试了使用CPU和GPU的分类和回归问题。所有实验都在Azure
    NV24虚拟机上运行，该虚拟机配备了24个核心、224 GB内存和NVIDIA M60 GPU。操作系统为Ubuntu 16.04。在所有实验中，我们发现XGBoost和LightGBM的准确率指标相似（此处显示F1分数），因此我们在这篇博客中重点关注了训练时间。下表显示了两种库在CPU和GPU实现中的训练时间以及训练时间比率。
- en: '![](../Images/7669a13d7f013ffb9c8711007bf4bfa1.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7669a13d7f013ffb9c8711007bf4bfa1.png)'
- en: Learnings from the Benchmark
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从基准测试中的学习
- en: As it is usually said, no benchmark is true but some of them are useful. From
    our experiments, we found the leaf-wise implementation faster than the level-wise
    one in general. However, the CPU results for BCI and Planet Kaggle datasets, as
    well as the GPU result for BCI, show that XGBoost hist takes considerably longer
    than standard XGBoost. This is due to the large size of the datasets, as well
    as the large number of features, which causes considerable memory overhead for
    XGBoost hist.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如常说的，没有任何基准是绝对真实的，但有些基准是有用的。从我们的实验中，我们发现叶节点实现通常比层级实现更快。然而，BCI和Planet Kaggle数据集的CPU结果以及BCI的GPU结果显示，XGBoost
    hist的时间比标准XGBoost长得多。这是由于数据集的巨大规模以及大量的特征，这给XGBoost hist带来了相当大的内存开销。
- en: We also found that XGBoost GPU implementation did not scale well, as it gave
    out of memory errors for 3 larger datasets. In addition, we had to terminate XGBoost
    training on the Airline dataset after 5 hours.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还发现XGBoost的GPU实现扩展性不好，对于三个较大的数据集出现了内存溢出错误。此外，我们不得不在航空公司数据集的XGBoost训练运行了5小时后终止。
- en: Finally, between LightGBM and XGBoost, we found that LightGBM is faster for
    all tests where XGBoost and XGBoost hist finished, with the biggest difference
    of 25 times for XGBoost and 15 times for XGBoost hist, respectively.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在LightGBM和XGBoost之间，我们发现LightGBM在所有XGBoost和XGBoost hist完成的测试中都更快，其中XGBoost的最大差异为25倍，XGBoost
    hist为15倍。
- en: We wanted to investigate the effect of different data sizes and number of rounds
    in the performance of CPU vs GPU. In the next table, we show the results of using
    subsamples of the Airline dataset. When comparing the CPU and GPU training times,
    we see that the GPU version of LightGBM outperforms the CPU one when the dataset
    is large and for a high number of rounds. As expected, with small datasets, the
    additional IO overhead of copying the data between RAM and GPU memory overshadows
    the speed benefits of running the computation on GPU. Here, we did not observe
    any performance gains in using  XGBoost hist on GPU.  As a side note, the standard
    implementation of XGBoost (exact split instead of histogram based) does not benefit
    from GPU either, as compared to multi-core CPU, per this [recent paper](https://arxiv.org/abs/1706.08359).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要研究不同数据大小和轮数对CPU与GPU性能的影响。在下表中，我们展示了使用航空公司数据集子样本的结果。比较CPU和GPU训练时间时，我们发现当数据集较大且轮数较多时，LightGBM的GPU版本优于CPU版本。正如预期的那样，对于小数据集，将数据在RAM和GPU内存之间复制的额外IO开销掩盖了在GPU上运行计算的速度优势。在这里，我们没有观察到使用XGBoost
    hist在GPU上有任何性能提升。值得一提的是，XGBoost的标准实现（基于精确分裂而非直方图）与多核CPU相比也未能从GPU中获益，参见这篇[最近的论文](https://arxiv.org/abs/1706.08359)。
- en: '![](../Images/1fed194178d3ccb9ca962b4b9dc785ba.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fed194178d3ccb9ca962b4b9dc785ba.png)'
- en: Overall, we find that LightGBM is faster than XGBoost, in both CPU and GPU implementations.
    Furthermore, if XGBoost is used, we would recommend keeping a close eye on feature
    dimensionality and memory consumption. The significant speed advantage of LightGBM
    translates into the ability to do more iterations and/or quicker hyperparameter
    search, which can be very useful if you have a limited time budget for optimizing
    your model or want to experiment with different feature engineering ideas.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们发现 LightGBM 在 CPU 和 GPU 实现中都比 XGBoost 更快。此外，如果使用 XGBoost，我们建议密切关注特征维度和内存消耗。LightGBM
    显著的速度优势意味着可以进行更多迭代和/或更快的超参数搜索，如果你在优化模型时有时间限制，或希望尝试不同的特征工程思路，这将非常有用。
- en: Happy coding!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 编程愉快！
- en: Miguel, Mathew, Guolin & Tao.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 米格尔、马修、郭林和陶。
- en: '*Acknowledgment: We would like to thank Steve Deng, David Smith and Huseyin
    Yildiz from Microsoft for their assistance and feedback on this post.*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*致谢：我们感谢微软的斯蒂夫·邓、戴维·史密斯和胡塞因·伊尔迪兹对本文的帮助和反馈。*'
- en: '[Original](https://blogs.technet.microsoft.com/machinelearning/2017/07/25/lessons-learned-benchmarking-fast-machine-learning-algorithms/).
    Reposted with permission.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://blogs.technet.microsoft.com/machinelearning/2017/07/25/lessons-learned-benchmarking-fast-machine-learning-algorithms/)。经授权转载。'
- en: '**Bio: **Miguel Fierro, Data Scientist, Mathew Salvaris, Data Scientist, Guolin
    Ke, Associate Researcher, and Tao Wu, Principal Data Science Manager, all at Microsoft.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：** 米格尔·费罗，数据科学家；马修·萨尔瓦里斯，数据科学家；郭林·科，副研究员；陶吴，首席数据科学经理，均在微软工作。'
- en: '**Related:**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[A Simple XGBoost Tutorial Using the Iris Dataset](/2017/03/simple-xgboost-tutorial-iris-dataset.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用鸢尾花数据集的简单 XGBoost 教程](/2017/03/simple-xgboost-tutorial-iris-dataset.html)'
- en: '[Dask and Pandas and XGBoost: Playing nicely between distributed systems](/2017/04/dask-pandas-xgboost-playing-nicely-distributed-systems.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Dask 和 Pandas 与 XGBoost：在分布式系统中良好协作](/2017/04/dask-pandas-xgboost-playing-nicely-distributed-systems.html)'
- en: '[Decision Tree Classifiers: A Concise Technical Overview](/2016/10/decision-trees-concise-technical-overview.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[决策树分类器：简明技术概述](/2016/10/decision-trees-concise-technical-overview.html)'
- en: More On This Topic
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Best Architecture for Your Text Classification Task: Benchmarking…](https://www.kdnuggets.com/2023/04/best-architecture-text-classification-task-benchmarking-options.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本分类任务的最佳架构：基准测试…](https://www.kdnuggets.com/2023/04/best-architecture-text-classification-task-benchmarking-options.html)'
- en: '[KDnuggets News, September 28: Free Algorithms in Python Course •…](https://www.kdnuggets.com/2022/n38.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，9月28日：免费的 Python 算法课程 •…](https://www.kdnuggets.com/2022/n38.html)'
- en: '[Speed up Machine Learning with Fast Kriging (FKR)](https://www.kdnuggets.com/2022/06/vmc-speed-machine-learning-fast-kriging.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用快速克里金法（FKR）加速机器学习](https://www.kdnuggets.com/2022/06/vmc-speed-machine-learning-fast-kriging.html)'
- en: '[Simple and Fast Data Streaming for Machine Learning Projects](https://www.kdnuggets.com/2022/11/simple-fast-data-streaming-machine-learning-projects.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习项目的简单快速数据流处理](https://www.kdnuggets.com/2022/11/simple-fast-data-streaming-machine-learning-projects.html)'
- en: '[Practical Deep Learning from fast.ai is Back!](https://www.kdnuggets.com/2022/07/practical-deep-learning-fastai-2022.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实用深度学习课程来自 fast.ai 重磅回归！](https://www.kdnuggets.com/2022/07/practical-deep-learning-fastai-2022.html)'
- en: '[What I Learned From Using ChatGPT for Data Science](https://www.kdnuggets.com/what-i-learned-from-using-chatgpt-for-data-science)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我从使用 ChatGPT 进行数据科学中学到的](https://www.kdnuggets.com/what-i-learned-from-using-chatgpt-for-data-science)'
