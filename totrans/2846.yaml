- en: 'Scalable graph machine learning: a mountain we can climb?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可扩展的图机器学习：我们能攀登的高峰？
- en: 原文：[https://www.kdnuggets.com/2019/12/scalable-graph-machine-learning.html](https://www.kdnuggets.com/2019/12/scalable-graph-machine-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/12/scalable-graph-machine-learning.html](https://www.kdnuggets.com/2019/12/scalable-graph-machine-learning.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Kevin Jung](https://www.linkedin.com/in/kevin-jung-12a820106/), Software
    Engineer at CSIRO Data61**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由[Kevin Jung](https://www.linkedin.com/in/kevin-jung-12a820106/)，CSIRO Data61的软件工程师**。'
- en: Graph machine learning is still a relatively new and developing area of research
    and brings with it a bucket load of complexities and challenges. One such challenge
    that both fascinates and infuriates those of us working with graph algorithms
    is — *scalability*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 图机器学习仍然是一个相对较新且正在发展的研究领域，带来了大量的复杂性和挑战。其中一个既让我们着迷又让我们感到愤怒的挑战就是——*扩展性*。
- en: 'Two methods were positioned early on as the standard approaches to leveraging
    network information: Graph Convolutional Networks [1] (powerful neural network
    architecture for machine learning on graphs) and Node2Vec [2] (an algorithmic
    framework for representational learning on graphs). Both these methods can be
    very useful for extracting insights from highly-connected datasets.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 两种方法早期被确立为利用网络信息的标准方法：图卷积网络 [1]（用于图形的强大神经网络架构）和Node2Vec [2]（用于图形的表示学习的算法框架）。这两种方法对于从高度连接的数据集中提取洞见非常有用。
- en: But I learned first-hand that when trying to apply graph machine learning techniques
    to identify fraudulent behaviour in the bitcoin blockchain data, scalability was
    the biggest roadblock. The bitcoin blockchain graph we are using has millions
    of wallets (nodes) and billions of transactions (edges), which makes most graph
    machine learning methods infeasible.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 但我亲身体验到，当尝试应用图机器学习技术来识别比特币区块链数据中的欺诈行为时，扩展性是最大的障碍。我们使用的比特币区块链图包含数百万个钱包（节点）和数十亿个交易（边），这使得大多数图机器学习方法不可行。
- en: 'In this article, we will take a closer look at scalability for graph machine
    learning methods: what it is, what makes it difficult, and an example of a method
    that tries to tackle it head-on.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将更详细地探讨图机器学习方法的扩展性：它是什么，它为什么困难，以及一种试图直接应对这一挑战的方法示例。
- en: What is graph machine learning?
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是图机器学习？
- en: First of all, let’s make sure we’re on the same page in terms of what we mean
    by *graph machine learning.*
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们确保我们对*图机器学习*的定义达成共识。
- en: When we say ‘graph,’ we’re talking about a way of representing data as entities
    with connections between them. In mathematical terms, we call an entity a node
    or vertex, and a connection an edge. A collection of vertices **V**, together
    with a collection of edges **E**, form a graph **G = (V, E)**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说‘图’时，我们指的是一种将数据表示为具有连接的实体的方式。从数学的角度来看，我们称实体为节点或顶点，连接为边。一组顶点**V**和一组边**E**组成一个图**G
    = (V, E)**。
- en: Graph machine learning is a machine learning technique that can naturally learn
    and make predictions from graph structured data. We can think of machine learning
    as a way of learning some transformation function; **y = f(x)**, where **x** is
    a piece of data and **y** is something we want to predict.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图机器学习是一种可以自然地从图结构数据中学习并进行预测的机器学习技术。我们可以将机器学习视为学习某种转换函数；**y = f(x)**，其中**x**是数据的一部分，而**y**是我们想要预测的东西。
- en: 'Say we take the task of detecting fraudulent bitcoin addresses as an example,
    and we know the account balance for all addresses on the blockchain. A very simple
    model might learn to predict that if an address has a zero account balance, then
    it is unlikely to be fraudulent. In other words, our function **f(x)** represents
    a value close to zero (i.e. non-fraudulent) when **x** is zero:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 比如我们以检测欺诈比特币地址为例，我们知道区块链上所有地址的账户余额。一个非常简单的模型可能会学到，如果一个地址的账户余额为零，则它不太可能是欺诈性的。换句话说，当**x**为零时，我们的函数**f(x)**表示一个接近零的值（即非欺诈性）：
- en: '![](../Images/523dd5d89c692d6afe54c0f1ad191077.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/523dd5d89c692d6afe54c0f1ad191077.png)'
- en: We can agree that simply looking at the account balance of an address is not
    much to go by when trying to solve such a problem. So at this point, we could
    consider potential ways to engineer additional features that give our model more
    information about each address’ behaviour.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以同意，仅仅查看一个地址的账户余额并不足以解决这样的问题。因此，此时我们可以考虑如何工程化额外特征，以便为我们的模型提供更多关于每个地址行为的信息。
- en: 'What we already have is the rich network structure from the transactions occurring
    between payers and payees of bitcoin. By designing a model that leverages this
    information, we’d have more confidence in the results:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经拥有了来自比特币付款人和收款人之间交易的丰富网络结构。通过设计一个利用这些信息的模型，我们将对结果更有信心：
- en: '![](../Images/cc16becb65c6cbb54c78c53ad6f16b5f.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc16becb65c6cbb54c78c53ad6f16b5f.png)'
- en: We want to make predictions about addresses not only based on their account
    balance but also based on transactions made with other addresses. We can try to
    formulate **f** such that it is in the form **f(x, x’₀, x’₁, …)** where **x’ᵢ**
    are other data points in the local neighbourhood of **x** as defined by our graph
    structure.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望对地址进行预测，不仅基于其账户余额，还基于与其他地址进行的交易。我们可以尝试将**f**公式化为**f(x, x’₀, x’₁, …)**，其中**x’ᵢ**是由我们的图结构定义的**x**的局部邻域中的其他数据点。
- en: One way to achieve this is by making use of graph structure in the form of an
    adjacency matrix. Multiplying the input data by the adjacency matrix (or some
    normalisation of) has the effect of linearly combining a data point with its adjacent
    points.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一点的一种方法是利用邻接矩阵形式的图结构。将输入数据与邻接矩阵（或其某种规范化形式）相乘的效果是将数据点与其相邻点线性组合。
- en: 'Below is a representation of a graph with three nodes as an adjacency matrix
    and a set of features:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个有三个节点的图的邻接矩阵表示以及一组特征：
- en: '![](../Images/609c5e9ef65591a634a9778785dab26e.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/609c5e9ef65591a634a9778785dab26e.png)'
- en: '![](../Images/599dec6907af47bf1baa98e6dbe70d8f.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/599dec6907af47bf1baa98e6dbe70d8f.png)'
- en: 'The neighbourhood of node 0 can be aggregated as:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 节点0的邻域可以被聚合为：
- en: '![](../Images/2575eeda468d11d9a8a52229aab83834.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2575eeda468d11d9a8a52229aab83834.png)'
- en: 'This is the basic high-level principle followed by algorithms like graph convolutional
    networks. In general, the aggregation of local neighbourhood information is applied
    recursively to increase the size of the local network that is pulled together
    to make predictions about a node. (Read StellarGraph’s article [*Knowing Your
    Neighbours: Machine Learning on Graphs*](https://medium.com/stellargraph/knowing-your-neighbours-machine-learning-on-graphs-9b7c3d0d5896)
    for a more thorough introduction to these concepts).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是诸如图卷积网络等算法遵循的基本高层原理。一般而言，局部邻域信息的聚合是递归应用的，以增加汇集在一起进行节点预测的局部网络的规模。（阅读StellarGraph的文章[*了解你的邻居：图上的机器学习*](https://medium.com/stellargraph/knowing-your-neighbours-machine-learning-on-graphs-9b7c3d0d5896)以获得更彻底的概念介绍）。
- en: What is *scalable*?
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是*可扩展的*？
- en: A [scalable](https://www.yourdictionary.com/scalable) mountain is a mountain
    that people can climb. A scalable system is a system that can handle growing demands.
    A scalable graph machine learning method should be a method that handles growing
    data sizes… and it also happens to be a huge mountain to climb.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[可扩展的](https://www.yourdictionary.com/scalable)山是一个人们可以攀登的山。一个可扩展的系统是一个能够处理不断增长的需求的系统。一个可扩展的图机器学习方法应该是一个能够处理不断增长的数据规模的方法……这也恰好是一座巨大的山。
- en: Here, I’m going to argue that the basic principle of naively aggregating across
    a node’s neighbourhood is *not* scalable, and describe the problems an algorithm
    must solve in order to be considered otherwise.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我将争论简单地在节点邻域之间聚合的基本原理是*不可扩展的*，并描述算法必须解决的问题，以便被认为是可扩展的。
- en: The first problem stems from the fact that one node can be connected arbitrarily
    to many other nodes from a graph - even the entire graph.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题源于一个节点可以任意连接到图中的许多其他节点——甚至是整个图。
- en: In a more traditional deep learning pipeline, if we want to predict something
    about **x**, we only need information about **x** itself. But with the graph structure
    in mind, in order to predict something about **x** we potentially need to aggregate
    information from the entire dataset. As a dataset gets larger and larger, suddenly
    we end up aggregating terabytes of data just to make a prediction about a single
    data point. This doesn’t sound so scalable.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在更传统的深度学习管道中，如果我们想要预测关于**x**的某些内容，我们只需**x**本身的信息。但考虑到图结构，为了预测关于**x**的某些内容，我们可能需要从整个数据集中聚合信息。随着数据集的不断增大，我们突然需要聚合TB级的数据才能对单个数据点进行预测。这听起来并不那么可扩展。
- en: The explanation for the second problem involves understanding the difference
    between a transductive and an inductive algorithm.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题的解释涉及理解传导算法和归纳算法之间的区别。
- en: '**Inductive algorithms** try to discover a general rule for the world. The
    model takes the data as a basis for making predictions for unseen data.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**归纳算法**尝试发现世界的一般规则。模型以数据为基础，对未见过的数据进行预测。'
- en: '**Transductive algorithms** attempt to make better predictions for the unlabelled
    data in a dataset by not generalising a universal model.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**传导算法**试图通过不泛化一个通用模型来为数据集中的未标记数据做出更好的预测。'
- en: When we’re trying to tackle problems in the real world, we’re met with the challenge
    that the data is not static. Gigabytes of new data may present every day, and
    this is what makes scalability such an important consideration. But many graph
    machine learning methods are inherently **transductive** due to the way information
    is aggregated from the entire dataset, as opposed to just looking at a single
    instance of data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们试图解决现实世界中的问题时，我们面临着数据不是静态的挑战。每天可能会出现数以千计的新数据，这就是为什么可扩展性如此重要的原因。但许多图机器学习方法由于从整个数据集中聚合信息的方式，天生具有**传导性**，而不是仅仅查看单一的数据实例。
- en: 'Let’s take a look at a more concrete example that demonstrates this problem.
    Consider a node A in a graph that is connected to three other nodes B, C, and
    D:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个更具体的示例来演示这个问题。考虑一个在图中与三个其他节点B、C和D相连的节点A：
- en: '![](../Images/d8337b1b12761bd3abb47ad892606986.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8337b1b12761bd3abb47ad892606986.png)'
- en: 'If we weren’t applying any fancy graph methods, we would simply be learning
    a function that maps from the features of A to a more useful metric; e.g., a prediction
    we want to make about the node:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有应用任何复杂的图方法，我们将简单地学习一个从A的特征到更有用的指标的映射；例如，我们想对节点做出的预测：
- en: '![](../Images/af44019986b181a064a5434133dbcf3d.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af44019986b181a064a5434133dbcf3d.png)'
- en: 'However, as we want to make use of the graph structure, we end up taking the
    features of B, C, and D as input for the function we’re learning:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于我们希望利用图结构，我们最终将B、C和D的特征作为我们学习的函数的输入：
- en: '![](../Images/d1196f6c7adff9f06fe97f3cca37231e.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1196f6c7adff9f06fe97f3cca37231e.png)'
- en: Consider that after we’ve trained the model, a new data point arrives some time
    in the future that happens to be connected to our original node A. We ended up
    learning a function that doesn’t take this connection into account, so we are
    stuck in a situation where we’re unsure whether the model we trained is valid
    for our new set of data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，当我们训练完模型后，未来某个时刻到达了一个新的数据点，它恰好与我们原始节点A相连。我们最终学习了一个没有考虑到这一连接的函数，因此我们陷入了一个不确定我们训练的模型是否对新数据集有效的情况。
- en: '![](../Images/d0c94587f2198c7018d77a3456aa9c2f.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d0c94587f2198c7018d77a3456aa9c2f.png)'
- en: '*Node E and Edge AE are introduced, causing the Model to also bring in the
    features of E when aggregating neighbourhood information to make a new prediction
    for A.*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*节点E和边AE被引入，导致模型在聚合邻域信息以对A进行新的预测时也带入了E的特征。*'
- en: So far, our understanding of graph algorithms suggests they are generally not
    very scalable, particularly if the algorithm is transductive in nature. Next,
    we’ll explore an algorithm that attempts to tackle some of these challenges.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们对图算法的理解表明，它们通常不太具备良好的可扩展性，尤其是当算法具有传导性质时。接下来，我们将探讨一种试图解决这些挑战的算法。
- en: Introducing GraphSAGE
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引入GraphSAGE
- en: A typical way many algorithms try to tackle the scalability problem in graph
    machine learning is to incorporate some form of sampling. One particular approach
    we will discuss in this section is the method of neighbour-sampling, which was
    introduced by the GraphSAGE [3] algorithm.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 许多算法试图通过引入某种形式的采样来解决图机器学习中的可扩展性问题。在本节中我们将讨论的一种特定方法是邻居采样方法，它是由GraphSAGE [3] 算法引入的。
- en: 'The SAGE in GraphSAGE stands for Sample-and-Aggregate, which in simple terms
    means: “for each node, take a sample of nodes from its local neighbourhood, and
    aggregate their features.”'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: GraphSAGE中的SAGE代表采样和聚合，简单来说就是：“对于每个节点，从其局部邻域中取样节点，并聚合它们的特征。”
- en: The concepts of “taking a sample of its neighbours” and “aggregating features”
    sound rather vague, so let’s explore what they actually mean.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: “取样其邻居”和“聚合特征”这两个概念听起来比较模糊，因此我们来探讨一下它们实际意味着什么。
- en: GraphSAGE prescribes that we take a fixed size sample of any given node’s local
    neighbourhood. This immediately solves our first problem of needing to aggregate
    information from across the entire dataset. But what are we sacrificing by doing
    so?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: GraphSAGE 规定我们对任何给定节点的局部邻域进行固定大小的采样。这立即解决了我们需要从整个数据集中汇总信息的第一个问题。但这样做我们牺牲了什么呢？
- en: '![](../Images/5b14a60e05edd012cfea2b139f869089.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b14a60e05edd012cfea2b139f869089.png)'
- en: First and most obviously, taking a sample means we’re taking an approximation
    of what the neighbourhood actually looks like. Depending on the size of the sample
    we choose to take, it may be a good enough approximation for our purposes, but
    an approximation nonetheless.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先最明显的一点是，进行采样意味着我们正在对邻域的实际情况进行近似。根据我们选择的样本大小，它可能对我们的目的足够好，但仍然是一种近似。
- en: We give up the chance for our model to learn something from how connected a
    node is. For GraphSAGE, a node with five neighbours looks exactly the same as
    a node with 50 neighbours since we always sample the same number of neighbours
    for each node.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们放弃了模型从节点的连接程度中学习的机会。对于 GraphSAGE，拥有五个邻居的节点与拥有 50 个邻居的节点看起来完全相同，因为我们总是对每个节点采样相同数量的邻居。
- en: Finally, we end up in a world where we could make different predictions about
    a node based on which neighbours we happened to sample at the time.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终，我们可能会进入一个世界，在这个世界中，我们可以根据当时采样到的邻居做出不同的节点预测。
- en: Depending on the problem we’d like to solve and what we know about our data,
    we can try to take a guess at how these issues may affect our results and make
    a decision about whether GraphSAGE is a suitable algorithm for a particular use-case.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们希望解决的问题和对数据的了解，我们可以尝试猜测这些问题可能如何影响我们的结果，并决定 GraphSAGE 是否适合特定的用例。
- en: Aggregating features can be done in a number of different ways, but each can
    be described as a function that takes a list of features from the sampled neighbourhood
    and outputs an ‘aggregated’ feature vector.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合特征可以通过多种不同的方式完成，但每种方式都可以描述为一个函数，该函数从采样的邻域中获取特征列表并输出一个“聚合”的特征向量。
- en: 'For example, the *mean aggregator* simply takes the element-wise mean of the
    features:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，*均值聚合器*简单地对特征进行逐元素均值计算：
- en: '![](../Images/e46db48646372048f6eafb02c66cc632.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e46db48646372048f6eafb02c66cc632.png)'
- en: '*GraphSAGE mean aggregator.*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*GraphSAGE 平均聚合器。*'
- en: We can then apply a second aggregation step to combine the features of the node
    itself and its aggregated neighbours. A simple way this can be done, demonstrated
    above, is to concatenate the two feature vectors and multiply this with a set
    of trainable weights.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以应用第二个聚合步骤，将节点自身及其聚合邻居的特征结合起来。上述演示的简单方法是将两个特征向量连接起来，并用一组可训练的权重进行乘法运算。
- en: The local sampling nature of GraphSAGE gives us both the inductive algorithm
    and a mechanism to scale. We are also able to choose the aggregation method to
    give us some flexibility in the model. Though these benefits come at a cost, where
    we need to sacrifice model performance for scalability. However, for our purposes,
    the GraphSAGE algorithm provided a good approach for scaling graph machine learning
    on the bitcoin dataset.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: GraphSAGE 的局部采样特性为我们提供了归纳算法和扩展机制。我们还可以选择聚合方法，为模型提供一些灵活性。尽管这些好处有代价，我们需要牺牲模型性能来换取可扩展性。然而，就我们的目的而言，GraphSAGE
    算法为在比特币数据集上扩展图机器学习提供了一个良好的方法。
- en: Success, but not without challenges
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 成功，但不是没有挑战
- en: 'GraphSAGE presents the neighbourhood sampling approach to overcome some of
    the challenges for scalability. Specifically, it:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: GraphSAGE 提出了邻域采样的方法来克服一些扩展性挑战。具体而言，它：
- en: '**gives us a good approximation while bounding the input size for making predictions**;
    and'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**为我们提供了一个良好的近似，同时限制了进行预测时的输入大小**；并且'
- en: '**allows for an inductive algorithm*.***'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**允许使用归纳算法**。 '
- en: This is a solid breakthrough but doesn’t leave us without problems to be solved.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个实质性的突破，但并未完全解决问题。
- en: '**1\. Efficient sampling is still difficult**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 高效采样仍然困难**'
- en: In order to sample the neighbours of a node without introducing bias, you still
    need to iterate through all of them. This means although GraphSAGE does restrict
    the size of the input to the neural network, the step required to populate the
    input involves looking through the entire graph, which can be very costly.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在不引入偏差的情况下对节点的邻居进行采样，你仍然需要遍历所有邻居。这意味着虽然GraphSAGE确实限制了神经网络的输入大小，但填充输入所需的步骤涉及浏览整个图，这可能是非常昂贵的。
- en: '**2\. Even with sampling, neighbourhood aggregation still aggregates A LOT
    of data**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 即使在采样的情况下，邻域聚合仍然聚合了大量数据**'
- en: Even with a fixed neighbourhood size, applying this scheme recursively means
    that you get an exponential explosion of the neighbourhood size. For example,
    if we take 10 random neighbours each time but apply the aggregation over three
    recursive steps, this ultimately results in a neighbourhood size of 10³.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在固定的邻域大小下，递归地应用这种方案意味着邻域大小会指数级增长。例如，如果我们每次选择10个随机邻居，但在三个递归步骤上应用聚合，这最终会导致邻域大小为10³。
- en: '**3\. Distributed data introduces even more challenges for graph-based methods**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 分布式数据为基于图的方法引入了更多挑战**'
- en: Much of the big data ecosystem revolves around distributing data to enable parallelised
    workloads and provide the ability to scale out horizontally based on demand. However,
    naively distributing graph data introduces a significant problem as there is no
    guarantee that neighbourhood aggregation can be done without communication across
    the network. This leaves graph-based methods in a place where you pay the cost
    of shuffling data across the network or miss out on the value of using big data
    technologies to enable your pipeline.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据生态系统的很多部分围绕着分发数据以启用并行化的工作负载，并根据需求提供水平扩展的能力。然而，天真地分发图数据会引入一个重大问题，因为没有保证邻域聚合可以在没有网络通信的情况下完成。这使得基于图的方法处于一个需要付出在网络中传输数据成本或错过利用大数据技术来启用管道的价值的位置。
- en: There are still mountains to climb and more exploration to be done to make scalable
    graph machine learning more practical. I, for one, will be paying close attention
    to new developments in this space.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然有许多困难需要克服，以及更多探索需要进行，以使可扩展的图机器学习变得更加实用。我个人将密切关注这个领域的新进展。
- en: If you’d like to learn more about graph machine learning, feel free to download
    the open source [StellarGraph Python Library](https://github.com/stellargraph/stellargraph)
    or contact us at [stellargraph.io](http://stellargraph.io).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于图机器学习的信息，欢迎下载开源的[StellarGraph Python库](https://github.com/stellargraph/stellargraph)或通过[stellargraph.io](http://stellargraph.io)联系我们。
- en: '*This work is supported by CSIRO’s Data61, Australia’s leading digital research
    network.*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*这项工作得到了CSIRO的Data61支持，这是澳大利亚领先的数字研究网络。*'
- en: References
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Graph Convolutional Networks (GCN): Semi-Supervised Classification with Graph
    Convolutional Networks. Thomas N. Kipf, Max Welling. International Conference
    on Learning Representations (ICLR), 2017'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图卷积网络（GCN）：具有图卷积网络的半监督分类。Thomas N. Kipf, Max Welling。国际学习表征会议（ICLR），2017年
- en: 'Node2Vec: Scalable Feature Learning for Networks. A. Grover, J. Leskovec. ACM
    SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),
    2016.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Node2Vec：网络的可扩展特征学习。A. Grover, J. Leskovec。ACM SIGKDD国际知识发现与数据挖掘会议（KDD），2016年。
- en: Inductive Representation Learning on Large Graphs. W.L. Hamilton, R. Ying, and
    J. Leskovec. Neural Information Processing Systems (NIPS), 2017.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大规模图的归纳表示学习。W.L. Hamilton, R. Ying, 和 J. Leskovec。神经信息处理系统（NIPS），2017年。
- en: '[Original](https://medium.com/stellargraph/scalable-graph-machine-learning-a-mountain-we-can-climb-753dccc572f).
    Reposted with permission.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/stellargraph/scalable-graph-machine-learning-a-mountain-we-can-climb-753dccc572f)。经许可转载。'
- en: '**Bio:** [Kevin Jung](https://www.linkedin.com/in/kevin-jung-12a820106/) is
    a Software Engineer working in the [StellarGraph](https://www.stellargraph.io/)
    team at CSIRO’s Data61, Australia’s leading digital research network.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：** [Kevin Jung](https://www.linkedin.com/in/kevin-jung-12a820106/) 是CSIRO
    Data61团队中的一名软件工程师，该团队是澳大利亚领先的数字研究网络[StellarGraph](https://www.stellargraph.io/)。'
- en: '**Related:**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Can graph machine learning identify hate speech in online social networks?](https://www.kdnuggets.com/2019/09/graph-machine-learning-hate-speech-social-networks.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[图机器学习能否识别在线社交网络中的仇恨言论？](https://www.kdnuggets.com/2019/09/graph-machine-learning-hate-speech-social-networks.html)'
- en: '[Knowing Your Neighbours: Machine Learning on Graphs](https://www.kdnuggets.com/2019/08/neighbours-machine-learning-graphs.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[认识你的邻居：图上的机器学习](https://www.kdnuggets.com/2019/08/neighbours-machine-learning-graphs.html)'
- en: '[Why organizations fail in scaling AI and Machine Learning](https://www.kdnuggets.com/2019/05/why-organizations-fail-scaling-ai-machine-learning.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么组织在扩展 AI 和机器学习时失败](https://www.kdnuggets.com/2019/05/why-organizations-fail-scaling-ai-machine-learning.html)'
- en: '* * *'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的 IT 需求'
- en: '* * *'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Building a Scalable ETL with SQL + Python](https://www.kdnuggets.com/2022/04/building-scalable-etl-sql-python.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 SQL + Python 构建可扩展 ETL](https://www.kdnuggets.com/2022/04/building-scalable-etl-sql-python.html)'
- en: '[Generalized and Scalable Optimal Sparse Decision Trees(GOSDT)](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[广义和可扩展的最优稀疏决策树 (GOSDT)](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
- en: '[How to Build a Scalable Data Architecture with Apache Kafka](https://www.kdnuggets.com/2023/04/build-scalable-data-architecture-apache-kafka.html)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 Apache Kafka 构建可扩展的数据架构](https://www.kdnuggets.com/2023/04/build-scalable-data-architecture-apache-kafka.html)'
- en: '[Gradient Descent: The Mountain Trekker''s Guide to Optimization with…](https://www.kdnuggets.com/gradient-descent-the-mountain-trekker-guide-to-optimization-with-mathematics)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[梯度下降：山地旅行者的优化指南](https://www.kdnuggets.com/gradient-descent-the-mountain-trekker-guide-to-optimization-with-mathematics)'
- en: '[How Machine Learning Can Benefit Online Learning](https://www.kdnuggets.com/2022/12/machine-learning-benefit-online-learning.html)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习如何惠及在线学习](https://www.kdnuggets.com/2022/12/machine-learning-benefit-online-learning.html)'
- en: '[Make Amazing Visualizations with Python Graph Gallery](https://www.kdnuggets.com/2022/12/make-amazing-visualizations-python-graph-gallery.html)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 Python 图形库制作惊人的可视化](https://www.kdnuggets.com/2022/12/make-amazing-visualizations-python-graph-gallery.html)'
