- en: 'Exclusive: Interview with Rich Sutton, the Father of Reinforcement Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独家：对强化学习之父Rich Sutton的采访
- en: 原文：[https://www.kdnuggets.com/2017/12/interview-rich-sutton-reinforcement-learning.html](https://www.kdnuggets.com/2017/12/interview-rich-sutton-reinforcement-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/12/interview-rich-sutton-reinforcement-learning.html](https://www.kdnuggets.com/2017/12/interview-rich-sutton-reinforcement-learning.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)I
    met Rich Sutton back in 1980s, when he and I, both fresh PhDs, joined GTE Laboratories
    in Boston area. I was doing research into Intelligent Databases and he was working
    on Reinforcement Learning department, but our GTE Labs projects were far from
    real-world deployment. We frequently played chess, where we were about equal,
    but Rich was far ahead of me in Machine Learning. Rich is both a brilliant researcher
    and a very nice and modest person. He says in the interview below that the idea
    of "Reinforcement Learning" was obvious, but there is a huge distance between
    having an idea and developing it into a working, mathematically-based theory,
    which is what Rich and Andrew Barto - his PhD thesis advisor - did for Reinforcement
    Learning. RL was a major part of the recent success of AlphaGo Zero, and if Artificial
    General Intelligence (AGI) will be developed at some point, RL is likely to play
    a major role in it.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments) 我在1980年代遇见了Rich
    Sutton，那时他和我都是刚获得博士学位的新人，我们一起加入了位于波士顿地区的GTE实验室。我在研究智能数据库，而他则在强化学习部门工作，但我们的GTE实验室项目离实际应用还很远。我们经常下棋，我们大致平手，但在机器学习方面，Rich远远领先于我。Rich既是一个杰出的研究人员，也是一个非常友好和谦逊的人。他在下面的采访中提到，“强化学习”的想法很明显，但从有想法到发展成一个具有数学基础的实际理论之间还有很大的距离，而这正是Rich和Andrew
    Barto（他的博士生导师）为强化学习所做的工作。强化学习在AlphaGo Zero的成功中扮演了重要角色，如果人工通用智能（AGI）在某个时点被开发出来，强化学习可能会在其中发挥重要作用。'
- en: '[![Rich Sutton](../Images/b6150061b1b0033bbbfe58bc7a004f94.png) Rich Sutton,
    Ph.D.](https://en.wikipedia.org/wiki/Richard_S._Sutton) is currently [ professor
    of Computer Science](https://www.ualberta.ca/science/about-us/contact-us/faculty-directory/rich-sutton),
    iCORE chair at the University of Alberta, and a Distinguished Research Scientist
    at DeepMind. He is one of the founding fathers of [Reinforcement Learning](https://en.wikipedia.org/wiki/Reinforcement_learning)
    (RL), an increasingly important part of Machine Learning and AI. His significant
    contributions to RL include [temporal difference learning](https://en.wikipedia.org/wiki/Temporal_difference_learning)
    and policy gradient methods. He is the author of a widely acclaimed book (with
    Andrew Barto) [ "Reinforcement Learning, an introduction"](https://mitpress.mit.edu/books/reinforcement-learning)
    - cited over 25,000 times, with 2nd edition coming soon.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[![Rich Sutton](../Images/b6150061b1b0033bbbfe58bc7a004f94.png) 理查德·萨顿博士](https://en.wikipedia.org/wiki/Richard_S._Sutton)
    目前是 [计算机科学教授](https://www.ualberta.ca/science/about-us/contact-us/faculty-directory/rich-sutton)，担任艾伯塔大学的iCORE主席，并且是DeepMind的杰出研究科学家。他是
    [强化学习](https://en.wikipedia.org/wiki/Reinforcement_learning)（RL）的创始人之一，强化学习在机器学习和人工智能中越来越重要。他在RL领域的重要贡献包括
    [时间差分学习](https://en.wikipedia.org/wiki/Temporal_difference_learning) 和策略梯度方法。他是一本广受好评的书籍（与Andrew
    Barto合著） [《强化学习导论》](https://mitpress.mit.edu/books/reinforcement-learning) 的作者，该书被引用超过25,000次，第二版即将发布。'
- en: He received BA in Psychology from Stanford (1978) and MS (1980) and PhD (1984)
    in Computer science from U. of Massachusetts at Amherst. His doctoral dissertation
    was entitled "Temporal Credit Assignment in Reinforcement Learning", where he
    introduced actor-critic architectures and "temporal credit assignment".
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 他在斯坦福大学获得心理学学士学位（1978年），并在马萨诸塞大学阿默斯特分校获得计算机科学硕士（1980年）和博士学位（1984年）。他的博士论文题为“强化学习中的时间信用分配”，在论文中他引入了演员-评论家架构和“时间信用分配”。
- en: From 1985 to 1994 Sutton was a Principal Member of Technical Staff at GTE Laboratories.
    He then spent 3 years at UMass Amherst as a Senior Research Scientist, and after
    that 5 years at the AT&T Shannon Laboratory as Principal Technical Staff Member.
    Since 2003 he is Professor and iCORE Chair in the Dept. of Computing Science at
    the University of Alberta, where he leads the Reinforcement Learning and Artificial
    Intelligence Laboratory (RLAI). Starting June 2017, Sutton also co-leads [a new
    Alberta office of DeepMind](https://deepmind.com/blog/deepmind-office-canada-edmonton/).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 从1985年到1994年，萨顿在GTE实验室担任技术员工首席成员。随后，他在UMass Amherst担任高级研究科学家3年，之后在AT&T香农实验室担任首席技术员工5年。自2003年起，他在阿尔伯塔大学计算科学系担任教授和iCORE主席，领导强化学习与人工智能实验室（RLAI）。自2017年6月起，萨顿还共同领导了[DeepMind在阿尔伯塔的新办公室](https://deepmind.com/blog/deepmind-office-canada-edmonton/)。
- en: Rich also keeps a blog/personal page at [incompleteideas.net](http://incompleteideas.net/).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Rich还在[incompleteideas.net](http://incompleteideas.net/)上维护一个博客/个人页面。
- en: '**Gregory Piatetsky: 1\. What are the main ideas in Reinforcement Learning
    (RL) and how it is different from Supervised Learning?**'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**Gregory Piatetsky: 1\. 强化学习（RL）的主要思想是什么？它与监督学习有何不同？**'
- en: '![Reinforcement Learning](../Images/17e5d554350227c33a37e7f224412f13.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习](../Images/17e5d554350227c33a37e7f224412f13.png)'
- en: 'The typical RL scenario: an agent takes actions in an environment, which is
    interpreted into a reward and a representation of the state, which are fed back
    into the agent.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的RL场景是：一个代理在环境中采取行动，环境将这些行动解释为奖励和状态表示，这些奖励和状态表示反馈给代理。
- en: 'Source: Wikipedia**Rich Sutton:** Reinforcement learning is learning from rewards,
    by trial and error, during normal interaction with the world. This makes it very
    much like natural learning processes and unlike supervised learning, in which
    learning only happens during a special training phase in which a supervisory or
    teaching signal is available that will not be available during normal use.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：维基百科**Rich Sutton:** 强化学习是通过试错学习从奖励中获得知识，这与自然学习过程非常相似，而与监督学习不同，后者只在有监督或教学信号的特殊训练阶段进行学习，而这种信号在正常使用过程中是不可用的。
- en: For example, speech recognition is currently done by supervised learning, using
    large datasets of speech sounds and their correct transcriptions into words. The
    transcriptions are the supervisory signals that will not be available when new
    speech sounds come in to be recognized. Game playing, on the other hand, is often
    done by reinforcement learning, using the outcome of the game as a reward. Even
    when you play a new game you will see whether you win or lose, and can use this
    with reinforcement learning algorithms to improve your play. A supervised learning
    approach to game playing would instead require examples of "correct" moves, say
    from a human expert. This would be handy to have, but it is not available during
    normal play, and would limit the skill of the learned system to that of the human
    expert. In reinforcement learning you make do with less informative training information,
    with the advantage that that information is more plentiful and is not limited
    by the skill of the supervisor.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，语音识别目前通过监督学习完成，使用大量的语音样本及其正确的文字转录。这些转录是监督信号，在新的语音样本到来时无法获得。另一方面，游戏通常通过强化学习完成，使用游戏结果作为奖励。即使你玩一个新游戏，你也能看到自己是赢还是输，并可以利用强化学习算法来改进你的游戏表现。监督学习的方法则需要“正确”动作的示例，比如来自人类专家的示例。这当然是有用的，但在正常游戏中无法获得，并且会将学习系统的技能限制在专家的水平。强化学习则使用较少的信息性训练数据，这些数据更为丰富且不受监督者技能的限制。
- en: '**GP: 2\. The second edition of your classic book with Andrew Barto: "Reinforcement
    Learning, an introduction" is coming soon (when?). What are the main advances
    covered in the second edition, and can you tell us about new chapters on the intriguing
    connections between reinforcement learning and psychology (Ch. 14) and neuroscience
    (Ch. 15)?**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**GP: 2\. 与安德鲁·巴托合著的经典书籍《强化学习：导论》的第二版即将出版（何时？）。第二版中涉及了哪些主要的进展？能否告诉我们关于强化学习与心理学（第14章）和神经科学（第15章）之间有趣联系的新章节？**'
- en: '**RS:** The complete draft of the second edition is currently available'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**RS:** 第二版的完整草稿目前已经完成。'
- en: 'on the web at [![Reinforcement Learning Book](../Images/428e4836cc1a2cbe09a7d4dee494a660.png)richsutton.com](http://richsutton.com).
    Andy Barto and I are putting some final finishing touches on it: validating all
    the references, things like that. It will be printed in a physical form early
    next year.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络上可以找到[![强化学习书籍](../Images/428e4836cc1a2cbe09a7d4dee494a660.png)richsutton.com](http://richsutton.com)。Andy
    Barto和我正在做一些最后的修订：验证所有参考文献，等等。它将在明年初以纸质形式印刷。
- en: A lot has happened in reinforcement learning in the twenty years since the first
    edition. Perhaps the most important of these is the huge impact reinforcement
    learning ideas have had on neuroscience, where the now-standard theory of brain
    reward systems is that they are an instance of temporal-difference learning (one
    of the fundamental learning methods of reinforcement learning).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 自从第一版发布以来，强化学习在过去二十年中发生了很多变化。其中最重要的可能是强化学习思想对神经科学的巨大影响，现在标准的脑奖励系统理论认为它们是时间差学习的一个实例（强化学习的基本学习方法之一）。
- en: In particular, the theory now is that a primary role of the neurotransmitter
    Dopamine is to carry the temporal-difference error, also called the reward-prediction
    error. This has been a huge development with many sources, ramifications, and
    tests, and our treatment in the book can only summarize them. This and other developments
    are covered in Chapter 15, and Chapter 14 summarizes their important precursors
    in psychology.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的理论认为，多巴胺这种神经递质的主要作用是传递时间差错，也称为奖励预测误差。这是一个巨大的发展，有很多来源、影响和测试，我们在书中的处理只能做简要总结。这些和其他发展在第15章中进行了介绍，第14章总结了它们在心理学中的重要前身。
- en: Overall the second edition is at about two-thirds larger than the first. There
    are now five chapters on function approximation instead of one. There are the
    two new chapters on psychology and neuroscience. There is also a new chapters
    on the frontiers of reinforcement learning, including a section on its societal
    implications. And everything has been updated and extended throughout the book.
    For example, the new applications chapter covers Atari game playing and AlphaGo
    Zero.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，第二版比第一版大约大了三分之二。现在有五章关于函数近似的内容，而不是一章。新增了两章关于心理学和神经科学的内容。还有一章关于强化学习的前沿，包括社会影响的一部分。书中的所有内容都得到了更新和扩展。例如，新应用章节涵盖了Atari游戏玩法和AlphaGo
    Zero。
- en: '**GP: 3\. What is Deep Reinforcement Learning - how it is different from RL?**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**GP: 3\. 什么是深度强化学习——它与强化学习有何不同？**'
- en: '**RS:** Deep reinforcement learning is the combination of deep learning and
    reinforcement learning. These two kinds of learning address largely orthogonal
    issues and combine nicely. In short, reinforcement learning needs methods for
    approximating functions from data to implement all of its components - value functions,
    policies, world models, state updaters - and deep learning is the latest and most
    successful of recently developed function approximators. Our textbook covers mainly
    linear function approximators, while giving the equations for the general case.
    We cover neural networks in the applications chapter and in one section, but to
    learn fully about deep reinforcement learning one would have to complement our
    book with, say, the [Deep Learning book](http://www.deeplearningbook.org/) by
    Goodfellow, Bengio, and Courville.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**RS:** 深度强化学习是深度学习和强化学习的结合。这两种学习方法解决了大相径庭的问题，并且结合得很好。简而言之，强化学习需要从数据中近似函数的方法来实现其所有组件——价值函数、策略、世界模型、状态更新器——而深度学习是最近开发的、最成功的函数近似器。我们的教科书主要介绍线性函数近似器，并给出了一般情况的方程。我们在应用章节和一个部分中介绍了神经网络，但要完全了解深度强化学习，还需补充如[深度学习书籍](http://www.deeplearningbook.org/)
    by Goodfellow, Bengio, and Courville的内容。'
- en: '**GP: 4\. RL had great success in games, for example with AlphaGo Zero. What
    other areas you expect RL to do well?**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**GP: 4\. RL在游戏中取得了巨大成功，例如AlphaGo Zero。你期待RL在其他哪些领域表现良好？**'
- en: '**RS:** Well, of course I believe that in some sense reinforcement learning
    is the future of AI. Reinforcement learning is the best representative of the
    idea that an intelligent system must be able to learn on its own, without constant
    supervision. An AI has to be able to tell for itself if it is right or wrong.
    Only in this way can it scale to really large amounts of knowledge and general
    skill.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**RS:** 嗯，当然我相信在某种意义上强化学习是人工智能的未来。强化学习最能代表一个智能系统必须能够自主学习，没有持续监督的想法。人工智能必须能够自己判断对错。只有这样，它才能扩展到真正大量的知识和通用技能。'
- en: '**GP: 5\. Yann LeCun [commented](https://www.theverge.com/2017/10/26/16552056/a-intelligence-terminator-facebook-yann-lecun-interview)
    that AlphaGo Zero success is hard to generalize to other domains because it played
    millions of games a day, but you cannot run real-world faster than real time.
    Where RL is not currently successful (eg when the feedback is sparse) and how
    that can be fixed?**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**GP: 5\. Yann LeCun [评论](https://www.theverge.com/2017/10/26/16552056/a-intelligence-terminator-facebook-yann-lecun-interview)称AlphaGo
    Zero的成功很难推广到其他领域，因为它每天玩数百万场游戏，但你无法让现实世界的速度快于实时。当前强化学习不成功的地方（例如当反馈稀疏时）以及如何解决这个问题？**'
- en: '**RS:** As Yann would readily agree, the key is to learn from ordinary unsupervised
    data. Yann and I would also agree, I think, that in the near term this will be
    done by focusing on **"prediction learning".** Prediction learning may shortly
    become a strong buzzword. It means just what you might think. It means predicting
    what will happen, and then learning based on what actually does happen. Because
    you learn from what happens, no supervisor is needed to tell you what you should
    have predicted. But because you find out what happens just by waiting, you do
    essentially have a supervisory signal. Prediction learning is the unsupervised
    supervised learning. Prediction learning is likely to be where big advances in
    applications will occur.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**RS:** 正如Yann会立刻同意的那样，关键是从普通的无监督数据中学习。我和Yann也会同意，我认为，近期的重点将是关注**"预测学习"**。预测学习可能很快会成为一个热门词汇。它的意思正如你所想，即预测将发生什么，然后基于实际发生的事情进行学习。因为你从发生的事情中学习，所以不需要监督者告诉你应该预测什么。但因为你只是等待事情发生，你实际上有一个监督信号。预测学习就是无监督的监督学习。预测学习可能会在应用方面取得重大进展。'
- en: The only question is whether you want to see prediction learning more as an
    outgrowth of supervised learning or of reinforcement learning. Students of reinforcement
    learning know that reinforcement learning has a major subproblem known as the
    "prediction problem" and that solving it efficiently is the focus of much of the
    algorithmic work. In fact, the first paper discussing temporal-difference learning
    is entitled "Learning to predict by the methods of temporal differences".
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的问题是你是否希望将预测学习更多地看作是监督学习或强化学习的衍生物。强化学习的学生知道强化学习有一个主要的子问题，称为“预测问题”，解决这个问题的效率是算法工作的重点。实际上，第一篇讨论时序差分学习的论文题为《通过时序差分方法学习预测》。
- en: '**GP: 6\. When you worked on RL in 1980s, did you think it will achieve such
    success?**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**GP: 6\. 当你在1980年代研究强化学习时，你是否认为它会取得如此成功？**'
- en: '**RS:** RL was definitely out of fashion in the 1980s. It essentially didn''t
    exist as a scientific or engineering idea. But it was nevertheless an obvious
    idea. Obvious to psychologists and obvious to ordinary people. And so it seemed
    plain to me that it was the thing to work on and that it would eventually be recognized.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**RS:** 在1980年代，强化学习确实过时了。它基本上不存在作为一个科学或工程思想。但它仍然是一个显而易见的思想。对心理学家来说显而易见，对普通人来说也显而易见。因此，我认为它是值得研究的，并且最终会被认可。'
- en: '**GP: 7\. What are the next research directions for RL and what are you working
    on now ?**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**GP: 7\. 强化学习的下一步研究方向是什么？你现在在做什么？**'
- en: '**RS**: Beyond prediction learning, I would say that a next big step will come
    when we have systems that plan with a learned model of the world. We currently
    have excellent planning algorithms, but only when the model is provided to them,
    as seen in all the game-playing systems, in which the model is provided by the
    rules of the game (and by self-play). But we don''t have the analog of the rules
    of the game for the real world. We need the laws of physics, yes, but we also
    need to know a myriad of other things, from how to walk and see to how other people
    will respond to what we do. The Dyna system in Chapter 8 of our textbook describes
    an integrated planning and learning system, but it is limited in several ways.
    Chapter 17 sketches ways in which the limitations might be overcome. I will be
    starting from there.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**RS**：除了预测学习，我认为下一步重大进展将是当我们拥有能够以学习到的世界模型进行规划的系统时。目前，我们有出色的规划算法，但仅在模型被提供时，如所有游戏系统中所见，模型由游戏规则（以及自我对弈）提供。然而，我们没有现实世界的游戏规则的类似物。我们需要物理定律，但我们还需要知道无数其他的事情，从如何走路和看东西到其他人如何回应我们的行为。我们教科书第8章中的Dyna系统描述了一个集成的规划和学习系统，但它在几个方面有限。第17章概述了可能克服这些限制的方法。我将从那里开始。'
- en: '**GP: 8\. RL may be central to the development of Artificial General Intelligence
    (AGI). What is your opinion - will researchers develop AGI in a foreseeable future?
    If yes, will it be a great benefit to humanity or will it be an existential threat
    to humanity, as Elon Musk warns?**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**GP: 8. 强化学习可能是发展人工通用智能（AGI）的核心。你怎么看？研究人员会在可预见的未来开发出AGI吗？如果会，它会对人类带来巨大好处，还是像埃隆·马斯克警告的那样，对人类构成生存威胁？**'
- en: '**RS:** ![artificial-intelligence](../Images/fe5182fe4b645ad53ad3c5ab25b92b04.png)I
    view artificial intelligence as the attempt to understand the human mind by making
    things like it. As Feynman said, "what i cannot create, i do not understand".
    In my view, the main event is that we are about to genuinely understand minds
    for the first time. This understanding alone will have enormous consequences.
    It will be the greatest scientific achievement of our time and, really, of any
    time. It will also be the greatest achievement of the humanities of all time -
    to understand ourselves at a deep level. When viewed in this way it is impossible
    to see it as a bad thing. Challenging yes, but not bad. We will reveal what is
    true. Those who don''t want it to be true will see our work as bad, just as when
    science dispensed with notions of soul and spirit it was seen as bad by those
    who held those ideas dear. Undoubtedly some of the ideas we hold dear today will
    be similarly challenged when we understand more deeply how minds work.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**RS**：![artificial-intelligence](../Images/fe5182fe4b645ad53ad3c5ab25b92b04.png)我认为人工智能是通过创造类似人类的事物来理解人类思维的尝试。正如费曼所说，“我不能创造的，我无法理解”。在我看来，主要事件是我们即将第一次真正理解思维。这种理解本身将产生巨大影响。这将是我们时代，实际上是任何时代最大的科学成就。它也将是所有时代人文学科的最大成就——在深层次上理解我们自己。从这个角度来看，将其视为坏事是不可能的。确实具有挑战性，但不是坏事。我们将揭示真相。那些不希望这成为现实的人将把我们的工作视为坏事，就像科学摒弃了灵魂和精神的观念时，那些持有这些观念的人将其视为坏事一样。毫无疑问，我们今天所珍视的一些观念将在我们更深刻理解思维如何运作时面临类似的挑战。'
- en: For more on the impact of AI on society, I would refer your readers to the final
    section of our textbook and to my comments in this video (https://www.youtube.com/watch?v=QqLcniN2VAk)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 关于人工智能对社会影响的更多信息，我建议读者查看我们教科书的最后一部分以及我在这个视频中的评论（https://www.youtube.com/watch?v=QqLcniN2VAk）
- en: '**GP: 9\. What do you like to do when you are away from computers and smartphones?
    What recent book you read and liked?**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**GP: 9. 当你远离电脑和智能手机时喜欢做什么？最近读过并喜欢的书是什么？**'
- en: '**RS:** I am a lover of nature and a student of speculative ideas in philosophy,
    economics, and science. I recently read and enjoyed "Seveneves" by Neal Stephenson,
    "Sapiens" by Yuval Harari, and "The Creature from Jekyll Island" by G. Edward
    Griffin.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**RS**：我热爱自然，并且是哲学、经济学和科学中的思辨性思想的学生。我最近阅读并喜欢了尼尔·斯蒂芬森的《七个世界》、尤瓦尔·赫拉利的《人类简史》以及G·爱德华·格里芬的《杰基尔岛的生物》。'
- en: '**Related:**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[**Machine Learning Algorithms: Which One to Choose for Your Problem**](https://www.kdnuggets.com/2017/11/machine-learning-algorithms-choose-your-problem.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**机器学习算法：如何选择适合你问题的算法**](https://www.kdnuggets.com/2017/11/machine-learning-algorithms-choose-your-problem.html)'
- en: '[**3 different types of machine learning**](https://www.kdnuggets.com/2017/11/3-different-types-machine-learning.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**3种不同类型的机器学习**](https://www.kdnuggets.com/2017/11/3-different-types-machine-learning.html)'
- en: '[**AlphaGo Zero: The Most Significant Research Advance in AI**](https://www.kdnuggets.com/2017/10/alphago-zero-biggest-ai-advance.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**AlphaGo Zero：人工智能领域最重要的研究进展**](https://www.kdnuggets.com/2017/10/alphago-zero-biggest-ai-advance.html)'
- en: More On This Topic
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Celebrating Devart''s 26th Birthday with an Exclusive 20% Discount…](https://www.kdnuggets.com/2023/08/devart-celebrating-26th-birthday-exclusive-discount-data-connectivity-tools.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[庆祝 Devart 的第 26 个生日，独享 20% 折扣…](https://www.kdnuggets.com/2023/08/devart-celebrating-26th-birthday-exclusive-discount-data-connectivity-tools.html)'
- en: '[Data Science Interview Guide - Part 2: Interview Resources](https://www.kdnuggets.com/2022/04/data-science-interview-guide-part-2-interview-resources.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学面试指南 - 第 2 部分：面试资源](https://www.kdnuggets.com/2022/04/data-science-interview-guide-part-2-interview-resources.html)'
- en: '[Interview Kickstart Data Science Interview Course — What Makes It…](https://www.kdnuggets.com/2022/10/interview-kickstart-data-science-interview-course-makes-different.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Interview Kickstart 数据科学面试课程——有什么特别之处…](https://www.kdnuggets.com/2022/10/interview-kickstart-data-science-interview-course-makes-different.html)'
- en: '[Hands-on Reinforcement Learning Course Part 3: SARSA](https://www.kdnuggets.com/2022/01/handson-reinforcement-learning-course-part-3-sarsa.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实践强化学习课程，第 3 部分：SARSA](https://www.kdnuggets.com/2022/01/handson-reinforcement-learning-course-part-3-sarsa.html)'
- en: '[Hands-On Reinforcement Learning Course, Part 1](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-course-part-1.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实践强化学习课程，第 1 部分](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-course-part-1.html)'
- en: '[Hands-On Reinforcement Learning Course, Part 2](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-part-2.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实践强化学习课程，第 2 部分](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-part-2.html)'
