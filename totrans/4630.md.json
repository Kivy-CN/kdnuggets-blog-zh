["```py\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\n\nratings = pd.read_csv(RATING_DATA_FILE,\n sep='::',\n engine='python',\n encoding='latin-1',\n names=['userid', 'movieid', 'rating', 'timestamp'])\n\nmovies = pd.read_csv(os.path.join(MOVIELENS_DIR, MOVIE_DATA_FILE),\n sep='::',\n engine='python',\n encoding='latin-1',\n names=['movieid', 'title', 'genre']).set_index(\"movieid\")\n```", "```py\nrating_counts = ratings.groupby(\"movieid\")[\"rating\"].count().sort_values(ascending=False)\n\n# only the 500 most popular movies\npop_ratings = ratings[ratings[\"movieid\"].isin((rating_counts).index[0:500])]\npop_ratings = pop_ratings.set_index([\"movieid\", \"userid\"])\n```", "```py\nprefs = pop_ratings[\"rating\"]\n\nmean_0 = pop_ratings[\"rating\"].mean()\nprefs = prefs - mean_0\n\nmean_i = prefs.groupby(\"movieid\").mean()\nprefs = prefs - mean_i\n\nmean_u = prefs.groupby(\"userid\").mean()\nprefs = prefs - mean_u\n\npref_matrix = prefs.reset_index()[[\"userid\", \"movieid\", \"rating\"]].pivot(index=\"userid\", columns=\"movieid\", values=\"rating\")\n```", "```py\nimport tensorflow as tf\n\nfrom keras.layers import Input, Dense, Lambda\nfrom keras.models import Model, load_model as keras_load_model\nfrom keras import losses\nfrom keras.callbacks import EarlyStopping\n\nENCODING_DIM = 25\nITEM_COUNT = 500\n```", "```py\n# ~~~ build recommender ~~~ #\ninput_layer = Input(shape=(ITEM_COUNT, ))\n# compress to low dimension\nencoded = Dense(ENCODING_DIM, activation=\"linear\", use_bias=False)(input_layer)\n# blow up to large dimension\ndecoded = Dense(ITEM_COUNT, activation=\"linear\", use_bias=False)(encoded) \n\n# define subsets of the model:\n# 1\\. the recommender itself\nrecommender = Model(input_layer, decoded)\n\n# 2\\. the encoder\nencoder = Model(input_layer, encoded)\n\n# 3\\. the decoder\nencoded_input = Input(shape=(ENCODING_DIM, ))\ndecoder = Model(encoded_input, recommender.layers[-1](encoded_input))\n```", "```py\nprefs[prefs == 0]\n# movieid  userid\n# 2664     2204      0.0\n```", "```py\ndef lambda_mse(frac=0.8):\n \"\"\"\n Specialized loss function for recommender model.\n\n :param frac: Proportion of weight to give to novel ratings.\n :return: A loss function for use in a Lambda layer.\n \"\"\"\n def lossfunc(xarray):\n x_in, y_true, y_pred = xarray\n zeros = tf.zeros_like(y_true)\n\n novel_mask = tf.not_equal(x_in, y_true)\n known_mask = tf.not_equal(x_in, zeros)\n\n y_true_1 = tf.boolean_mask(y_true, novel_mask)\n y_pred_1 = tf.boolean_mask(y_pred, novel_mask)\n\n y_true_2 = tf.boolean_mask(y_true, known_mask)\n y_pred_2 = tf.boolean_mask(y_pred, known_mask)\n\n unknown_loss = losses.mean_squared_error(y_true_1, y_pred_1)\n known_loss = losses.mean_squared_error(y_true_2, y_pred_2)\n\n # remove nans\n unknown_loss = tf.where(tf.is_nan(unknown_loss), 0.0, unknown_loss)\n\n return frac*unknown_loss + (1.0 - frac)*known_loss\n return lossfunc\n```", "```py\ndef final_loss(y_true, y_pred):\n \"\"\"\n Dummy loss function for wrapper model.\n :param y_true: true value (not used, but required by Keras)\n :param y_pred: predicted value\n :return: y_pred\n \"\"\"\n return y_pred\n```", "```py\noriginal_inputs = recommender.input\ny_true_inputs = Input(shape=(ITEM_COUNT, ))\noriginal_outputs = recommender.output\n# give 80% of the weight to guessing the missings, 20% to reproducing the knowns\nloss = Lambda(lambda_mse(0.8))([original_inputs, y_true_inputs, original_outputs])\n\nwrapper_model = Model(inputs=[original_inputs, y_true_inputs], outputs=[loss])\nwrapper_model.compile(optimizer='adadelta', loss=final_loss)\n```", "```py\ndef generate(pref_matrix, batch_size=64, mask_fraction=0.2):\n \"\"\"\n Generate training triplets from this dataset.\n\n :param batch_size: Size of each training data batch.\n :param mask_fraction: Fraction of ratings in training data input to mask. 0.2 = hide 20% of input ratings.\n :param repeat: Steps between shuffles.\n :return: A generator that returns tuples of the form ([X, y], zeros) where X, y, and zeros all have\n shape[0] = batch_size. X, y are training inputs for the recommender.\n \"\"\"\n\n def select_and_mask(frac):\n def applier(row):\n row = row.copy()\n idx = np.where(row != 0)[0]\n if len(idx) > 0:\n masked = np.random.choice(idx, size=(int)(frac*len(idx)), replace=False)\n row[masked] = 0\n return row\n return applier\n\n indices = np.arange(pref_matrix.shape[0])\n batches_per_epoch = int(np.floor(len(indices)/batch_size))\n while True:\n np.random.shuffle(indices)\n\n for batch in range(0, batches_per_epoch):\n idx = indices[batch*batch_size:(batch+1)*batch_size]\n\n y = np.array(pref_matrix[idx,:])\n X = np.apply_along_axis(select_and_mask(frac=mask_fraction), axis=1, arr=y)\n\n yield [X, y], np.zeros(batch_size)\n```", "```py\n[X, y], _ = next(generate(pref_matrix.fillna(0).values))\nlen(X[X != 0])/len(y[y != 0])\n# returns 0.8040994014148377\n```", "```py\ndef fit(wrapper_model, pref_matrix, batch_size=64, mask_fraction=0.2, epochs=1, verbose=1, patience=0):\n stopper = EarlyStopping(monitor=\"loss\", min_delta=0.00001, patience=patience, verbose=verbose)\n batches_per_epoch = int(np.floor(pref_matrix.shape[0]/batch_size))\n\n generator = generate(pref_matrix, batch_size, mask_fraction)\n\n history = wrapper_model.fit_generator(\n generator,\n steps_per_epoch=batches_per_epoch,\n epochs=epochs,\n callbacks = [stopper] if patience > 0 else []\n )\n\n return history\n```", "```py\n# stop after 3 epochs with no improvement\nfit(wrapper_model, pref_matrix.fillna(0).values, batch_size=125, epochs=100, patience=3)\n# Loss of 0.6321\n```", "```py\ndef predict(ratings, recommender, mean_0, mean_i, movies):\n # add a dummy user that's seen all the movies so when we generate\n # the ratings matrix, it has the appropriate columns\n dummy_user = movies.reset_index()[[\"movieid\"]].copy()\n dummy_user[\"userid\"] = -99999\n dummy_user[\"rating\"] = 0\n dummy_user = dummy_user.set_index([\"movieid\", \"userid\"])\n\n ratings = ratings[\"rating\"]\n\n ratings = ratings - mean_0\n ratings = ratings - mean_i\n mean_u = ratings.groupby(\"userid\").mean()\n ratings = ratings - mean_u\n\n ratings = ratings.append(dummy_user[\"rating\"])\n\n pref_mat = ratings.reset_index()[[\"userid\", \"movieid\", \"rating\"]].pivot(index=\"userid\", columns=\"movieid\", values=\"rating\")\n X = pref_mat.fillna(0).values\n y = recommender.predict(X)\n\n output = pd.DataFrame(y, index=pref_mat.index, columns=pref_mat.columns)\n output = output.iloc[1:] # drop the bad user\n\n output = output.add(mean_u, axis=0)\n output = output.add(mean_i, axis=1)\n output = output.add(mean_0)\n\n return output\n```", "```py\nsample_ratings = pd.DataFrame([\n {\"userid\": 1, \"movieid\": 2858, \"rating\": 1}, # american beauty\n {\"userid\": 1, \"movieid\": 260, \"rating\": 5},  # star wars\n {\"userid\": 1, \"movieid\": 480, \"rating\": 5},  # jurassic park\n {\"userid\": 1, \"movieid\": 593, \"rating\": 2},  # silence of the lambs\n {\"userid\": 1, \"movieid\": 2396, \"rating\": 2}, # shakespeare in love\n {\"userid\": 1, \"movieid\": 1197, \"rating\": 5}  # princess bride\n]).set_index([\"movieid\", \"userid\"])\n\n# predict and print the top 10 ratings for this user\ny = predict(sample_ratings, recommender, mean_0, mean_i, movies.loc[(rating_counts).index[0:500]]).transpose()\npreds = y.sort_values(by=1, ascending=False).head(10)\n\npreds[\"title\"] = movies.loc[preds.index][\"title\"]\npreds\n```", "```py\nsample_ratings2 = pd.DataFrame([\n {\"userid\": 1, \"movieid\": 2858, \"rating\": 5}, # american beauty\n {\"userid\": 1, \"movieid\": 260, \"rating\": 1},  # star wars\n {\"userid\": 1, \"movieid\": 480, \"rating\": 1},  # jurassic park\n {\"userid\": 1, \"movieid\": 593, \"rating\": 1},  # silence of the lambs\n {\"userid\": 1, \"movieid\": 2396, \"rating\": 5}, # shakespeare in love\n {\"userid\": 1, \"movieid\": 1197, \"rating\": 5}  # princess bride\n]).set_index([\"movieid\", \"userid\"])\n\ny = predict(sample_ratings2, recommender, mean_0, mean_i, movies.loc[(rating_counts).index[0:500]]).transpose()\npreds = y.sort_values(by=1, ascending=False).head(10)\n\npreds[\"title\"] = movies.loc[preds.index][\"title\"]\npreds\n```", "```py\nstarwars = decoder.get_weights()[0][:,33]\nesb = decoder.get_weights()[0][:,144]\namericanbeauty = decoder.get_weights()[0][:,401]\n```", "```py\nnp.sqrt(((starwars - esb)**2).sum())\n# 0.209578\n\nnp.sqrt(((starwars - americanbeauty)**2).sum())\n# 0.613659\n```"]