- en: Comparing Linear and Logistic Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较线性回归和逻辑回归
- en: 原文：[https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html](https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html](https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html)
- en: Data Science interviews vary in their depth. Some interviews go really deep
    and test the candidates on their knowledge of advanced models or tricky fine-tuning.
    But many interviews are conducted at an entry level, trying to test the basic
    knowledge of the candidate. In this article we will see a question that can be
    discussed in such an interview. Even though the question is very simple, the discussion
    brings up many interesting aspects of the fundamentals of machine learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学面试的深度有所不同。有些面试会深入探讨候选人对高级模型或复杂微调的知识。而许多面试则在入门级别进行，测试候选人的基本知识。在本文中，我们将看到一个可以在这种面试中讨论的问题。尽管这个问题非常简单，但讨论涉及到许多有趣的机器学习基础方面。
- en: '* * *'
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '* * *'
- en: ''
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_BQ
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: ''
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: ''
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: ''
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的 IT'
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '* * *'
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Question: What is the difference between Linear Regression and Logistic Regression?'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问题：线性回归和逻辑回归有什么区别？
- en: There are actually many similarities between the two, starting with the fact
    that their names are very similar sounding. They both use lines as the model functions.
    Their graphs look very similar too.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，两者之间有许多相似之处，从它们的名字发音相似开始。它们都使用直线作为模型函数。它们的图形也非常相似。
- en: '![Comparing Linear and Logistic Regression](../Images/dddee014bc459c3e0923fb7a490ecce5.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![比较线性回归和逻辑回归](../Images/dddee014bc459c3e0923fb7a490ecce5.png)'
- en: Image by Author
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'But despite these similarities, they are very different in method as well as
    application. We will highlight these differences now. For comparison, we will
    use the following points that are generally considered while discussing any machine
    learning model:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些相似之处，但它们在方法和应用上却大相径庭。我们现在将重点突出这些差异。为了比较，我们将使用以下通常在讨论任何机器学习模型时考虑的要点：
- en: Hypothesis or model family
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设或模型家族
- en: Input and output
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入和输出
- en: Loss function
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数
- en: Optimization technique
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化技术
- en: Application
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用
- en: We will now compare Linear Regression (LinReg) and Logistic Regression (LogReg)
    on each of these points. Let’s start with the application, to put the discussion
    on the right track.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将对线性回归（LinReg）和逻辑回归（LogReg）在每一点上进行比较。让我们从应用开始，以便将讨论引导到正确的轨道上。
- en: Application
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用
- en: '![Comparing Linear and Logistic Regression](../Images/730b545ace7fe794761c035901f68de9.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![比较线性回归和逻辑回归](../Images/730b545ace7fe794761c035901f68de9.png)'
- en: Image by Rajashree Rajadhyax
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 Rajashree Rajadhyax 提供
- en: Linear Regression is used for estimating a quantity based on other quantities.
    As an example, imagine that as a student, you run a lemonade stand during the
    summer vacation. You want to figure out how many glasses of lemonade will be sold
    tomorrow, so that you can buy enough lemons and sugar. From your long experience
    in selling lemonade, you have figured out that the sale has a strong relationship
    with the maximum temperature in the day. So you want to use the predicted max
    temperature to predict the lemonade sale. This is a classic LinReg application,
    generally called prediction in ML literature.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归用于根据其他数量估计一个数量。例如，想象一下你作为学生，在暑假期间经营一个柠檬水摊。你想预测明天会卖出多少杯柠檬水，以便你可以购买足够的柠檬和糖。通过你在售卖柠檬水方面的长期经验，你已经发现销量与当天的最高温度有很强的关系。因此，你想利用预测的最高温度来预测柠檬水的销售。这是一个经典的
    LinReg 应用，通常在 ML 文献中称为预测。
- en: LinReg is also used to find out how a particular input affects the output. In
    the lemonade stall example, suppose you have two inputs- the maximum temperature
    and whether the day is a holiday. You want to find out which affects the sale
    more — max temperature or holiday. LinReg will be useful in identifying this.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LinReg 也用于找出特定输入如何影响输出。在柠檬水摊的例子中，假设你有两个输入——最高温度和是否是节假日。你想找出哪一个对销售的影响更大——最高温度还是节假日。LinReg
    将在识别这一点时发挥作用。
- en: LogReg is mainly used for classification. Classification is the act of categorizing
    the input into one of the many possible baskets. Classification is so central
    to human intelligence that it would not be wrong to say ‘most of the intelligence
    is classification’. A good example of classification is clinical diagnosis. Consider
    the elderly, reliable family doctor. A lady walks in and complains of incessant
    coughing. The doctor conducts various examinations to decide between many possible
    conditions. Some possible conditions are relatively harmless, like a bout of throat
    infection. But some are serious, such as tuberculosis or even lung cancer. Based
    on various factors, the doctor decides what she is suffering from and starts appropriate
    treatment. This is classification at work.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: LogReg 主要用于分类。分类是将输入归入多个可能类别之一的行为。分类对于人类智能如此重要，以至于可以说“智能大多是分类”。一个好的分类例子是临床诊断。考虑一位可靠的家庭医生。一个女士走进来，抱怨不停地咳嗽。医生进行各种检查以决定多种可能的病症。某些可能的病症相对无害，如喉咙感染。但有些则很严重，例如结核病甚至肺癌。根据各种因素，医生决定她所患的病症，并开始适当的治疗。这就是分类的实际应用。
- en: We must keep in mind that both estimation and classification are guessing tasks
    rather than computations. There is no exact or correct answer in such types of
    tasks. The guessing tasks are what machine learning systems are good at.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须记住，估计和分类都是猜测任务，而不是计算任务。这类任务没有准确或正确的答案。猜测任务是机器学习系统擅长的领域。
- en: Model Family
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型家族
- en: ML systems solve guessing problems by detecting patterns. They detect a pattern
    from the given data and then use it for performing the task such as estimation
    or classification. An important pattern that is found in natural phenomena is
    the relation pattern. In this pattern, one quantity is related to the other quantity.
    This relation can be approximated by a mathematical function in most of the cases.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统通过检测模式来解决猜测问题。它们从给定的数据中检测模式，然后利用这些模式执行任务，如估计或分类。在自然现象中发现的一个重要模式是关系模式。在这种模式下，一个量与另一个量相关。这种关系在大多数情况下可以用数学函数来近似。
- en: 'Identifying a mathematical function from the given data is called ‘learning’
    or ‘training’. There are two steps of learning:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从给定的数据中识别数学函数被称为“学习”或“训练”。学习有两个步骤：
- en: The ‘type’ of function (for example linear, exponential, polynomial) is chosen
    by a human
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 函数的“类型”（例如线性、指数、多项式）由人工选择。
- en: The learning algorithm learns the parameters (like the slope and intercept of
    a line) from the given data.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习算法从给定的数据中学习参数（如直线的斜率和截距）。
- en: So when we say that ML systems learn from data, it is only partially true. The
    first step of selecting the type of function is manual and is a part of the model
    design. The type of function is also called ‘hypothesis’ or ‘model family’.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所以当我们说机器学习系统从数据中学习时，这只是部分正确。选择函数类型的第一步是手动的，并且是模型设计的一部分。函数的类型也称为“假设”或“模型家族”。
- en: In both LinReg and LogReg, the model family is the linear function. As you know,
    a line has two parameters — slope and intercept. But this is true only if the
    function takes just one input. For most real world problems, there are more than
    one inputs. The model function for these cases is called a linear function, not
    a line. A linear function has more parameters to learn. If there are n inputs
    to the model, the linear function has n+1 parameters. As mentioned, these parameters
    are learned from the given data. For the purpose of this article, we will continue
    to assume that the function is the simple line with two parameters. The model
    function for LogReg is a little more complex. The line is there, but it is combined
    with another function. We will see this in a moment.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在LinReg和LogReg中，模型家族都是线性函数。正如你所知道的，一条线有两个参数——斜率和截距。但这仅在函数只有一个输入时才成立。对于大多数实际问题，输入不止一个。这些情况的模型函数称为线性函数，而不是线。线性函数有更多的参数需要学习。如果模型有n个输入，线性函数有n+1个参数。如前所述，这些参数是从给定的数据中学习的。为了本文的目的，我们将继续假设函数是具有两个参数的简单直线。LogReg的模型函数稍微复杂一点。直线存在，但它与另一个函数组合在一起。我们稍后会看到这一点。
- en: Inputs and Outputs
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入和输出
- en: As we said above, both LinReg and LogReg learn the parameters of the linear
    function from the given data, called the training data. What does the training
    data contain?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，LinReg和LogReg都从给定的数据中学习线性函数的参数，这些数据被称为训练数据。训练数据包含了什么？
- en: Training data is prepared by recording some real world phenomena (RWP). For
    example, the relation between the maximum day temperature and the sale of lemonade
    is a RWP. We have no visibility of the underlying relation. All we can see are
    the values of the temperature and the sale everyday. While recording the observations,
    we designate some quantities as inputs of the RWP and others as output. In the
    lemonade example, we call the max temperature as input and the sale of lemonade
    as output.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据是通过记录一些实际世界现象（RWP）来准备的。例如，最高气温与柠檬水销售之间的关系就是一个RWP。我们看不到潜在的关系。我们只能看到每天的温度值和销售情况。在记录观察时，我们将一些量指定为RWP的输入，其他量指定为输出。在柠檬水的例子中，我们称最高气温为输入，柠檬水的销售为输出。
- en: '![Comparing Linear and Logistic Regression](../Images/d7b5eefa49d29dd3bcfdc14352466e62.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![比较线性回归和逻辑回归](../Images/d7b5eefa49d29dd3bcfdc14352466e62.png)'
- en: Image by Author
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Our training data contains pairs of inputs and outputs. In this example, the
    data will have rows of everyday maximum temperature and glasses of lemonade sold.
    Such will be the input and output to LinReg.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练数据包含输入和输出的对。在这个示例中，数据将包括每日最高气温和售出的柠檬水数量。这将是LinReg的输入和输出。
- en: The task that LogReg performs is classification, so its output should be a class.
    Let’s imagine that there are two classes called 0 and 1\. The output of the model
    should then also be either 0 or 1.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: LogReg执行的任务是分类，因此它的输出应该是一个类别。假设有两个类别，称为0和1。模型的输出也应该是0或1。
- en: 'However, this method of specifying output is not very apt. See the following
    diagram:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种指定输出的方法不是很合适。请看下面的图：
- en: '![Comparing Linear and Logistic Regression](../Images/91fb36966ce902a4dda4c3a24e234a6a.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![比较线性回归和逻辑回归](../Images/91fb36966ce902a4dda4c3a24e234a6a.png)'
- en: Image by Author
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: The points in yellow belong to class 1 and the light blue ones belong to 0\.
    The line is our model function that separates the two classes. According to this
    separator, both the yellow points (a and b) belong to Class 1 . However, the membership
    of point b is much more certain than that of point a. If the model simply outputs
    0 and 1, then this fact is lost.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 黄色点属于类别1，浅蓝色点属于类别0。那条线是我们的模型函数，它将这两个类别分开。根据这个分隔线，黄色点（a和b）都属于类别1。然而，点b的隶属度比点a更确定。如果模型只是输出0和1，那么这一事实就会丢失。
- en: To correct this situation, the LogReg model produces the probability of each
    point belonging to a certain class. In the above example, the probability of point
    ‘a’ belonging to Class 1 is low, whereas that of point ‘b’ is high. Since probability
    is a number between 0 and 1, so is the output of LogReg.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了纠正这种情况，LogReg模型会产生每个点属于某个类别的概率。在上述示例中，点‘a’属于类别1的概率较低，而点‘b’的概率较高。由于概率是0到1之间的数值，LogReg的输出也是如此。
- en: 'Now see the following diagram:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在请看下面的图：
- en: '![Comparing Linear and Logistic Regression](../Images/f9ef7037add14ed255794a565ffefe95.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![比较线性回归和逻辑回归](../Images/f9ef7037add14ed255794a565ffefe95.png)'
- en: Image by Author
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: This diagram is the same as the earlier, with point c added. This point also
    belongs to Class 1 and in fact is more certain than point b. However, it would
    be wrong to increase the probability of a point in proportion to its distance
    from the line. Intuitively, once you go a certain distance away from the line,
    we are more or less certain about the membership of those points. We need not
    increase the probability further. This is in line with the nature of probabilities,
    whose maximum value can be 1.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图与之前的相同，只是添加了点 c。这个点也属于类别 1，并且实际上比点 b 更加确定。然而，将点的概率按其与直线的距离成比例地增加是错误的。直观上，一旦你离直线一定距离，我们对这些点的类别就或多或少有了确定性。我们不需要进一步增加概率。这符合概率的性质，其最大值为
    1。
- en: 'In order that the LogReg model is able to produce such output, the line function
    has to be connected to another function. This second function is called the sigmoid
    and it has the equation:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使 LogReg 模型能够产生这样的输出，线性函数必须连接到另一个函数。这个第二个函数叫做 sigmoid，其方程为：
- en: '![Comparing Linear and Logistic Regression](../Images/11042e42f0e56a3a1bc967c839326d0b.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![比较线性回归和逻辑回归](../Images/11042e42f0e56a3a1bc967c839326d0b.png)'
- en: Image by Author
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Thus the LogReg model looks like:![Comparing Linear and Logistic Regression](../Images/327b254aec1777aa616c3e6e1971be66.png)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，LogReg 模型看起来像：![比较线性回归和逻辑回归](../Images/327b254aec1777aa616c3e6e1971be66.png)
- en: Image by Author
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: The sigmoid function is also called the ‘logistic’ and is the reason for the
    name ‘Logistic Regression’.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数也称为‘逻辑斯蒂’函数，这也是‘逻辑回归’这个名字的由来。
- en: If there are more than two classes, the output of LogReg is a vector. The elements
    of the output vector are probabilities of the input being of that particular class.
    For example, if the first element of the clinical diagnosis model has the value
    0.8, it means that the model thinks there is a 80% probability of the patient
    suffering from cold.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有超过两个类别，LogReg 的输出是一个向量。输出向量的元素是输入属于特定类别的概率。例如，如果临床诊断模型的第一个元素值为 0.8，这意味着模型认为患者有
    80% 的可能性患有感冒。
- en: Loss Function
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: We saw that both LinReg and LogReg learn the parameters of the linear function
    from the training data. How do they learn these parameters?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到 LinReg 和 LogReg 都从训练数据中学习线性函数的参数。那么它们是如何学习这些参数的呢？
- en: They use a method called ‘optimization’. Optimization works by generating many
    possible solutions for the given problem. In our case, the possible solutions
    are the sets of (slope, intercept) values. We evaluate each of these solutions
    using a performance measure. The solution that proves to be best on this measure
    is finally selected.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 它们使用一种叫做‘优化’的方法。优化通过生成许多可能的解决方案来解决给定的问题。在我们的例子中，可能的解决方案是 (斜率, 截距) 的集合。我们使用性能度量来评估这些解决方案中的每一个。最终选择在这一度量下表现最好的解决方案。
- en: 'In the learning of ML models, the performance measure is sometimes called ‘loss’
    and the function which helps us to calculate it is called ‘loss function’. We
    can represent this as:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习模型的学习中，性能度量有时被称为‘损失’，而帮助我们计算损失的函数被称为‘损失函数’。我们可以将其表示为：
- en: '[PRE0]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The terms ‘loss’ and ‘loss function’ have a negative connotation, which means
    that a lower value of loss indicates a better solution. In other words, learning
    is an optimization that aims to find parameters that produce minimum loss.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ‘损失’和‘损失函数’这两个术语有负面含义，这意味着较低的损失值表示更好的解决方案。换句话说，学习是一个优化过程，旨在找到产生最小损失的参数。
- en: We will now see the common loss functions used to optimize LinReg and LogReg.
    Note that many different loss functions are used in actual practice, so we can
    discuss those which are most common.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在来看一下优化 LinReg 和 LogReg 时常用的损失函数。注意，实际操作中使用了许多不同的损失函数，因此我们可以讨论那些最常见的。
- en: 'For optimization of LinReg parameters, the most common loss function is called
    Sum of Squares Error (SSE). This function takes the following inputs:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 LinReg 参数的优化，最常见的损失函数称为平方和误差 (SSE)。这个函数接受以下输入：
- en: '1) All the training data points. For each point, we specify :'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 所有训练数据点。对于每个点，我们指定：
- en: a) the inputs, such as the maximum data temperature,
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: a) 输入，例如最大数据温度，
- en: b) the outputs, like the number of lemonade glasses sold
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: b) 输出，例如出售的柠檬水杯数
- en: 2) The linear equation with parameters
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 带参数的线性方程
- en: 'The function then calculates loss using the following formula:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后函数使用以下公式计算损失：
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The optimization measure for LogReg is defined in a very different way. In
    the SSE function, we ask the following question:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: LogReg的优化度量以非常不同的方式定义。在SSE函数中，我们问以下问题：
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In designing the measure for LogReg optimization, we ask:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计LogReg优化度量时，我们问：
- en: '[PRE3]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The output of this measure is thus a likelihood. The mathematical form of the
    measure function uses logarithms, thus giving it the name Log Likelihood (LL).
    While discussing the outputs, we saw that the LogReg function involves exponential
    terms (the terms with e ‘raised to’ z) The logarithms help to deal with these
    exponentials effectively.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 该度量的输出因此是一个似然性。度量函数的数学形式使用对数，因此得名对数似然性（LL）。在讨论输出时，我们看到LogReg函数涉及指数项（e ‘提升到’
    z的项），对数有助于有效处理这些指数项。
- en: 'It should be intuitively clear to you that optimization should maximize LL.
    Think like this: we want to find the line that makes the training data most likely.
    In practice however, we prefer a measure that can be minimized, so we just take
    the negative of the LL. We thus get the Negative Log Likelihood (NLL) loss function,
    though according to me calling it a loss function is not very correct.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该直观地理解优化应当最大化LL。这样考虑：我们想找到使训练数据最可能的线。然而在实际中，我们更倾向于使用可以最小化的度量，因此我们只是取LL的负值。我们因此得到了负对数似然性（NLL）损失函数，尽管我认为称之为损失函数并不完全正确。
- en: 'So we have the two loss functions: SSE for LinReg and NLL for LogReg. Note
    that these loss functions have many names, and you should familiarize yourself
    with the terms.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有两个损失函数：LinReg的SSE和LogReg的NLL。请注意，这些损失函数有许多名称，你应当熟悉这些术语。
- en: Summary
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Even though Linear Regression and Logistic Regression look and sound very similar,
    in reality they are quite different. LinReg is used for estimation/prediction
    and LogReg is for classification. It is true that they both use the linear function
    as their basis, but LogReg further adds the logistic function. They differ in
    the way they consume their training data and produce their model outputs. The
    two also use a very different loss function.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管线性回归和逻辑回归看起来非常相似，但实际上它们有很大不同。LinReg用于估计/预测，而LogReg用于分类。确实它们都使用线性函数作为基础，但LogReg进一步添加了逻辑函数。它们在消耗训练数据和生成模型输出的方式上有所不同。两者也使用了非常不同的损失函数。
- en: Further details can be probed. Why SSE? How is the likelihood calculated? We
    did not go into the optimization method here to avoid more mathematics. However,
    you must keep in mind that optimization of LogReg usually requires the iterative
    gradient descent method while LinReg can usually do with a quick closed form solution.
    We can discuss these and more points in another article.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的细节可以深入探讨。为什么选择SSE？如何计算似然性？我们没有在这里深入优化方法以避免更多的数学内容。然而，你必须记住，LogReg的优化通常需要迭代梯度下降方法，而LinReg通常可以使用快速的封闭形式解法。我们可以在另一篇文章中讨论这些和更多的点。
- en: '**[Devesh Rajadhyax](https://www.linkedin.com/in/deveshraj/)** have been working
    in the field of Artificial Intelligence since last eight years. Cere Labs is the
    company he founded to work on various aspects of AI. Cere Labs has created an
    AI platform called Cerescope based on Deep Learning, Machine Learning and Cognitive
    Computing. The platform has been used to build solutions for financial services,
    healthcare, retail, manufacturing and so on.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Devesh Rajadhyax](https://www.linkedin.com/in/deveshraj/)** 已在人工智能领域工作了八年。他创立了Cere
    Labs公司，致力于AI的各个方面。Cere Labs创建了一个名为Cerescope的AI平台，基于深度学习、机器学习和认知计算。该平台已经被用于金融服务、医疗保健、零售、制造等领域的解决方案建设。'
- en: '[Original](https://towardsdatascience.com/comparing-linear-and-logistic-regression-11a3e1812212).
    Reposted with permission.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/comparing-linear-and-logistic-regression-11a3e1812212)。转载授权。'
- en: More On This Topic
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Linear vs Logistic Regression: A Succinct Explanation](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性回归与逻辑回归: 简明解释](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
- en: '[KDnuggets News 22:n12, March 23: Best Data Science Books for…](https://www.kdnuggets.com/2022/n12.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻 22:n12, 3月23日: 数据科学最佳书籍…](https://www.kdnuggets.com/2022/n12.html)'
- en: '[Classification Metrics Walkthrough: Logistic Regression with…](https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分类指标演示: 逻辑回归与…](https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html)'
- en: '[An Overview of Logistic Regression](https://www.kdnuggets.com/2022/02/overview-logistic-regression.html)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[逻辑回归概述](https://www.kdnuggets.com/2022/02/overview-logistic-regression.html)'
- en: '[Logistic Regression for Classification](https://www.kdnuggets.com/2022/04/logistic-regression-classification.html)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用于分类的逻辑回归](https://www.kdnuggets.com/2022/04/logistic-regression-classification.html)'
- en: '[How Does Logistic Regression Work?](https://www.kdnuggets.com/2022/07/logistic-regression-work.html)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[逻辑回归是如何工作的？](https://www.kdnuggets.com/2022/07/logistic-regression-work.html)'
