- en: 'Topic Modeling Approaches: Top2Vec vs BERTopic'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模方法：Top2Vec 与 BERTopic
- en: 原文：[https://www.kdnuggets.com/2023/01/topic-modeling-approaches-top2vec-bertopic.html](https://www.kdnuggets.com/2023/01/topic-modeling-approaches-top2vec-bertopic.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/01/topic-modeling-approaches-top2vec-bertopic.html](https://www.kdnuggets.com/2023/01/topic-modeling-approaches-top2vec-bertopic.html)
- en: '![Topic Modeling Approaches: Top2Vec vs BERTopic](../Images/668abce1b24f95ea2750b95c939a3722.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![主题建模方法：Top2Vec 与 BERTopic](../Images/668abce1b24f95ea2750b95c939a3722.png)'
- en: Photo by [Mikechie Esparagoza](https://www.pexels.com/photo/photo-of-assorted-letter-board-quote-hanged-on-wall-1742370/)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Mikechie Esparagoza](https://www.pexels.com/photo/photo-of-assorted-letter-board-quote-hanged-on-wall-1742370/)
    拍摄
- en: Every day, we are dealing most of the time with unlabeled text and supervised
    learning algorithms cannot be used at all to extract information from the data.
    A subfield of natural language can reveal the underlying structure in large amounts
    of text. This discipline is called Topic Modeling, that is specialized in extracting
    topics from text.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 每天，我们大多数时候都在处理未标记的文本，而监督学习算法完全无法用于从数据中提取信息。自然语言的一个子领域可以揭示大量文本中的潜在结构。这一学科称为主题建模，专门用于从文本中提取主题。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您所在组织的 IT 工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In this context, conventional approaches, like Latent Dirichlet Allocation and
    Non-Negative Matrix Factorization, demonstrated to not capture well the relationships
    between words since they are based on bag-of-word.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，传统的方法，如潜在狄利克雷分配（Latent Dirichlet Allocation）和非负矩阵分解（Non-Negative Matrix
    Factorization），未能很好地捕捉词之间的关系，因为它们基于词袋模型。
- en: For this reason, we are going to focus on two promising approaches, Top2Vec
    and BERTopic, that address these drawbacks by exploiting pre-trained language
    models to generate topics. Let’s get started!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将重点关注两种有前景的方法，Top2Vec 和 BERTopic，这些方法通过利用预训练语言模型生成主题，从而解决这些缺陷。让我们开始吧！
- en: Top2Vec
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Top2Vec
- en: Top2Vec is a model capable of detecting automatically topics from the text by
    using pre-trained word vectors and creating meaningful embedded topics, documents
    and word vectors.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Top2Vec 是一种能够通过使用预训练的词向量和创建有意义的嵌入主题、文档和词向量，自动检测文本中的主题的模型。
- en: 'In this approach, the procedure to extract topics can be split into different
    steps:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，提取主题的过程可以分为不同的步骤：
- en: '**Create Semantic Embedding**: jointly embedded document and word vectors are
    created. The idea is that similar documents should be closer in the embedding
    space, while dissimilar documents should be distant between them.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建语义嵌入**：创建联合嵌入的文档和词向量。其思想是，相似的文档在嵌入空间中应更接近，而不相似的文档之间应保持距离。'
- en: '**Reduce the dimensionality of the document embedding**: The application of
    the dimensionality reduction approach is important to preserve most of the variability
    of the embedding of documents while reducing the high dimensional space. Moreover,
    it allows to identification of dense areas, in which each point represents a document
    vector. UMAP is the typical dimensionality reduction approach chosen in this step
    because it’s able to preserve the local and global structure of the high-dimensional
    data.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**减少文档嵌入的维度**：应用降维方法对于在减少高维空间的同时保留大部分文档嵌入的变异性非常重要。此外，这也允许识别密集区域，其中每个点代表一个文档向量。UMAP
    是在此步骤中选择的典型降维方法，因为它能够保留高维数据的局部和全局结构。'
- en: '**Identify clusters of documents**: HDBScan, a density-based clustering approach,
    is applied to find dense areas of similar documents. Each document is assigned
    as noise if it’s not in a dense cluster, or a label if it belongs to a dense area.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**识别文档簇**：应用基于密度的聚类方法HDBScan来寻找相似文档的密集区域。如果文档不在密集簇中，则被标记为噪声；如果属于密集区域，则被标记为一个标签。'
- en: '**Calculate centroids in the original embedding space**: The centroid is computed
    by considering the high dimensional space, instead of the reduced embedding space.
    The classic strategy consists in calculating the arithmetic mean of all the document
    vectors belonging to a dense area, obtained in the previous step with HDBSCAN.
    In this way, a topic vector is generated for each cluster.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在原始嵌入空间中计算质心**：质心通过考虑高维空间而不是缩减的嵌入空间来计算。经典策略包括计算所有属于密集区域的文档向量的算术平均值，这些区域是通过HDBSCAN在先前步骤中获得的。通过这种方式，为每个簇生成一个主题向量。'
- en: '**Find words for each topic vector**: the nearest word vectors to the document
    vector are semantically the most representative.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为每个主题向量找到词汇**：与文档向量最近的词向量在语义上是最具代表性的。'
- en: Example of Top2Vec
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Top2Vec的示例
- en: In this tutorial, we are going to analyze the negative reviews of McDonald's
    from a dataset available on [data.world](https://data.world/crowdflower/mcdonalds-review-sentiment).
    Identifying the topics from these reviews can be valuable for the multinational
    to improve the products and the organisation of this fast food chain in the USA
    locations provided by the data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将分析来自[data.world](https://data.world/crowdflower/mcdonalds-review-sentiment)的数据集中麦当劳的负面评论。从这些评论中识别主题对跨国公司来说是有价值的，以便改进产品和美国地区的快餐连锁店组织。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Topic Modeling Approaches: Top2Vec vs BERTopic](../Images/5679d764b1c931d55b32169dcd6791b3.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![主题建模方法：Top2Vec与BERTopic](../Images/5679d764b1c931d55b32169dcd6791b3.png)'
- en: In a single line of code, we are going to perform all the steps of the top2vec
    explained previously.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一行代码，我们将执行之前解释的所有Top2Vec步骤。
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The main arguments of Top2Vec are:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Top2Vec的主要论点是：
- en: 'docs_bad: is a list of strings.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'docs_bad: 是一个字符串列表。'
- en: 'universal-sentence-encoder: is the chosen pre-trained embedding model.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'universal-sentence-encoder: 是选择的预训练嵌入模型。'
- en: 'deep-learn: is a parameter that determines the quality of the produced document
    vector.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'deep-learn: 是一个决定生成文档向量质量的参数。'
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The most
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是
- en: '![Topic Modeling Approaches: Top2Vec vs BERTopic](../Images/342e839d6dad2a34bf4cf34cdd41b104.png)![Topic
    Modeling Approaches: Top2Vec vs BERTopic](../Images/28b15cea4b0bc64984e1b42d9173a0d6.png)![Topic
    Modeling Approaches: Top2Vec vs BERTopic](../Images/d79a930ee5541b0a66f0479e165d19f5.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![主题建模方法：Top2Vec与BERTopic](../Images/342e839d6dad2a34bf4cf34cdd41b104.png)![主题建模方法：Top2Vec与BERTopic](../Images/28b15cea4b0bc64984e1b42d9173a0d6.png)![主题建模方法：Top2Vec与BERTopic](../Images/d79a930ee5541b0a66f0479e165d19f5.png)'
- en: From the word clouds, we can deduce that the topic 0 is about general complaints
    about the service in McDonald, like “slow service”, “horrible service” and “order
    wrong”, while the topic 1 and 2 refer respectively to breakfast food (McMuffin,
    biscuit, egg) and coffee (iced coffee and cup coffee).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从词云中，我们可以推测主题0是关于麦当劳服务的一般投诉，如“服务慢”、“服务糟糕”和“订单错误”，而主题1和2分别指的是早餐食品（麦芬、饼干、鸡蛋）和咖啡（冰咖啡和杯咖啡）。
- en: 'Now, we try to search documents using two keywords, wrong and slow:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们尝试使用两个关键词搜索文档：错误和慢：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Output:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: BERTopic
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERTopic
- en: '*"BERTopic is a topic modeling technique that leverages transformers and c-TF-IDF
    to create dense clusters allowing for easily interpretable topics whilst keeping
    important words in the topic descriptions."*'
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“BERTopic是一种主题建模技术，它利用变换器和c-TF-IDF创建密集的簇，从而使主题易于解释，同时保留主题描述中的重要词汇。”*'
- en: As the name suggests, BERTopic utilises powerful transformer models to identify
    the topics present in the text. Another characteristic of this topic modeling
    algorithm is the use of a variant of TF-IDF, called class-based variation of TF-IDF.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如名字所示，BERTopic利用强大的变换器模型来识别文本中的主题。这个主题建模算法的另一个特点是使用TF-IDF的变体，称为基于类别的TF-IDF变体。
- en: Like Top2Vec, it doesn’t need to know the number of topics, but it automatically
    extracts the topics.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 像Top2Vec一样，它不需要知道主题的数量，而是自动提取主题。
- en: 'Moreover, similarly to Top2Vec, it is an algorithm that involves different
    phases. The first three steps are the same: creation of embedding documents, dimensionality
    reduction with UMAP and clustering with HDBScan.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，与Top2Vec类似，它是一个涉及不同阶段的算法。前三个步骤是相同的：创建嵌入文档、使用UMAP进行降维和使用HDBScan进行聚类。
- en: The successive phases begin to diverge from Top2Vec. After finding the dense
    areas with HDBSCAN, each topic is tokenized into a bag-of-words representation,
    which takes into account if the word appears in the document or not. After the
    documents belonging to a cluster are considered a unique document and TF-IDF is
    applied. So, for each topic, we identify the most relevant words, that should
    have the highest c-TF-IDF.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 随后的阶段开始与Top2Vec有所不同。在使用HDBSCAN找到密集区域后，每个主题被标记为一个词袋表示，这个表示考虑了词汇是否出现在文档中。然后，将属于同一集群的文档视为一个唯一文档，并应用TF-IDF。因此，对于每个主题，我们识别出最相关的词汇，这些词汇应该具有最高的c-TF-IDF。
- en: Example of BERTopic
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERTopic示例
- en: We repeat the analysis on the same dataset.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在相同的数据集上重复分析。
- en: 'We are going to extract the topics from the reviews using BERTopic:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用BERTopic从评论中提取主题：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Topic Modeling Approaches: Top2Vec vs BERTopic](../Images/3f49f3d932e79c03c92cc4f49904f3af.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![主题建模方法：Top2Vec与BERTopic](../Images/3f49f3d932e79c03c92cc4f49904f3af.png)'
- en: The table returned by the model provides information about the 14 topics extracted.
    Topic corresponds to the topic identifier, except for all the outliers that are
    ignored that are labeled as -1.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 模型返回的表格提供了关于提取的14个主题的信息。主题对应于主题标识符，除了被忽略的所有离群点，这些离群点标记为-1。
- en: Now, we are going to pass to the most interesting part regarding the visualization
    of our topics into interactive graphs, such as the visualization of the most relevant
    terms for each topic, the intertopic distance map, the two-dimensional representation
    of the embedding space and the topic hierarchy.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将进入最有趣的部分，即将我们的主题可视化为互动图表，例如每个主题的最相关术语的可视化、主题间距离图、嵌入空间的二维表示和主题层级。
- en: Let’s begin to show the bar charts for the top ten topics. For each topic, we
    can observe the most important words, sorted in decreasing order based on the
    c-TF-IDF score. The more a word is relevant, the more the score is higher.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始展示前十个主题的条形图。对于每个主题，我们可以观察最重要的词汇，并根据c-TF-IDF评分按降序排列。一个词汇的相关性越高，它的评分就越高。
- en: The first topic contains generic words, like location and food, topic 1 order
    and wait, topic 2 worst and service, topic 3 place and dirty, ad so on.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个主题包含通用词汇，如位置和食物，主题1为顺序和等待，主题2为最差和服务，主题3为地方和肮脏，等等。
- en: After visualizing the bar charts, it’s time to take a look at the intertopic
    distance map. We reduce the dimensionality of c-TF-IDF score into a two-dimensional
    space to visualize the topics in a plot. At the bottom, there is a slider that
    allows selecting the topic that will be coloured in red. We can notice that the
    topics are grouped in two different clusters, one with generic thematics like
    food, chicken and location, and one with different negative aspects, such as worst
    service, dirty, place and cold.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在可视化条形图之后，我们可以查看主题间距离图。我们将c-TF-IDF评分的维度降到二维空间，以便在图中可视化这些主题。底部有一个滑块，可以选择将其标记为红色的主题。我们可以注意到，主题被分成了两个不同的集群，一个包含诸如食物、鸡肉和地点等通用主题，另一个则包含不同的负面方面，如最差服务、肮脏、地方和寒冷。
- en: The next graph permits to see the relationship between the reviews and the topics.
    In particular, it can be useful to understand why a review is assigned to a specific
    topic and is aligned with the most relevant words found. For example, we can focus
    on the red cluster, corresponding to topic 2 with some words about the worst service.
    The documents within this dense area seem quite negative, like “Terrible customer
    service and even worse food”.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 下一张图表展示了评论与主题之间的关系。特别是，它可以帮助理解为何某条评论被分配到特定主题，并与最相关的词汇对齐。例如，我们可以关注红色集群，它对应于主题2，包含了一些关于最差服务的词汇。这个密集区域中的文档似乎相当负面，比如“糟糕的客户服务和更糟糕的食物”。
- en: Differences between TopVec and BERTopic
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TopVec与BERTopic的区别
- en: At first sight, these approaches have many aspects in common, like finding automatically
    the number of topics, no necessity of pre-processing in most of cases, the application
    of UMAP to reduce the dimensionality of document embeddings and, then, HDBSCAN
    is used for modelling these reduced document embeddings, but they are fundamentally
    different when looking at the way they assign the topics to the documents.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 表面上看，这些方法有许多共同点，如自动确定主题数量、大多数情况下无需预处理、应用UMAP降低文档嵌入的维度，然后使用HDBSCAN对这些降低的文档嵌入进行建模，但在将主题分配给文档的方式上，它们在根本上是不同的。
- en: Top2Vec creates topic representations by finding words located close to a cluster’s
    centroid.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Top2Vec通过寻找靠近聚类中心的单词来创建主题表示。
- en: Differently from Top2Vec, BERTopic doesn’t take into account the cluster’s centroid,
    but it considered all the documents in a cluster as a unique document and extracts
    topic representations using a class-based variation of TF-IDF.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与Top2Vec不同，BERTopic不考虑聚类中心，而是将聚类中的所有文档视为一个独特的文档，并使用基于类的TF-IDF变体提取主题表示。
- en: '| **Top2Vec** | **BERTopic** |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| **Top2Vec** | **BERTopic** |'
- en: '| The strategy to extract topics based on cluster’s centroids.  | The strategy
    to extract topics based on c-TF-IDF. |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 基于聚类中心提取主题的策略。 | 基于c-TF-IDF提取主题的策略。 |'
- en: '| It doesn’t support Dynamic Topic Modeling. | It supports Dynamic Topic Modeling.
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 不支持动态主题建模。 | 支持动态主题建模。 |'
- en: '| It builds word clouds for each topic and provides searching tools for topics,
    documents and words. | It allows for building Interactive visualization plots,
    allowing to interpretation of the extracted topics. |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 它为每个主题构建词云，并提供主题、文档和单词的搜索工具。 | 它允许构建互动可视化图，帮助解释提取的主题。 |'
- en: Endnotes
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: The Topic Modeling is a growing field of Natural Language Processing and there
    are numerous possible applications, like reviews, audio and social media posts.
    As it has been shown, this article provides an overviews of Topi2Vec and BERTopic,
    that are two promising approaches, that can help you to identify topics with few
    lines of code and interpret the results through data visualizations. If you have
    questions about these techniques or you have other suggestions about other approaches
    to detect topics, write it in the comments.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模是自然语言处理中一个不断发展的领域，应用广泛，包括评论、音频和社交媒体帖子。正如所示，本文概述了Topi2Vec和BERTopic这两种有前景的方法，它们可以帮助你用少量代码识别主题，并通过数据可视化来解释结果。如果你对这些技术有疑问，或者有其他检测主题的方法建议，请在评论中写下。
- en: '**[Eugenia Anello](https://www.linkedin.com/in/eugenia-anello/)** is currently
    a research fellow at the Department of Information Engineering of the University
    of Padova, Italy. Her research project is focused on Continual Learning combined
    with Anomaly Detection.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**[尤金妮亚·安内洛](https://www.linkedin.com/in/eugenia-anello/)** 目前是意大利帕多瓦大学信息工程系的研究员。她的研究项目专注于持续学习与异常检测的结合。'
- en: More On This Topic
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Approaches to Text Summarization: An Overview](https://www.kdnuggets.com/2019/01/approaches-text-summarization-overview.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本摘要的方法概述](https://www.kdnuggets.com/2019/01/approaches-text-summarization-overview.html)'
- en: '[Data Labeling for Machine Learning: Market Overview, Approaches, and Tools](https://www.kdnuggets.com/2021/12/data-labeling-ml-overview-and-tools.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习的数据标注：市场概况、方法和工具](https://www.kdnuggets.com/2021/12/data-labeling-ml-overview-and-tools.html)'
- en: '[A new book that will revolutionize the way your organization…](https://www.kdnuggets.com/2022/02/manning-new-book-revolutionize-way-organization-approaches-data.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一本将彻底改变你组织的…](https://www.kdnuggets.com/2022/02/manning-new-book-revolutionize-way-organization-approaches-data.html)'
- en: '[AI-Generated Sports Highlights: Different Approaches](https://www.kdnuggets.com/2022/03/aigenerated-sports-highlights-different-approaches.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AI生成的体育亮点：不同的方法](https://www.kdnuggets.com/2022/03/aigenerated-sports-highlights-different-approaches.html)'
- en: '[Machine Learning’s Sweet Spot: Pure Approaches in NLP and Document Analysis](https://www.kdnuggets.com/2022/05/machine-learning-sweet-spot-pure-approaches-nlp-document-analysis.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习的甜蜜点：NLP和文档分析中的纯方法](https://www.kdnuggets.com/2022/05/machine-learning-sweet-spot-pure-approaches-nlp-document-analysis.html)'
- en: '[3 Approaches to Data Imputation](https://www.kdnuggets.com/2022/12/3-approaches-data-imputation.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3种数据插补方法](https://www.kdnuggets.com/2022/12/3-approaches-data-imputation.html)'
