- en: 'Data Compression via Dimensionality Reduction: 3 Main Methods'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/12/data-compression-dimensionality-reduction.html](https://www.kdnuggets.com/2020/12/data-compression-dimensionality-reduction.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9897b1c6998c7f5aab989b3f5ac3a1e.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Photo by [Anna Tarazevich](https://www.pexels.com/@anntarazevich?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels) from [Pexels](https://www.pexels.com/photo/gray-and-black-metal-machine-5963136/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels).*'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Why reduce the dimensions of data?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you are in data science for quite some time, you must have heard this phrase:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Dimensionality is a curse.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is also referred to as the curse of dimensionality. You can learn more
    about this term [here](https://en.wikipedia.org/wiki/Curse_of_dimensionality).
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, some of the common disadvantages of high dimensional data are:'
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Difficulties in [clustering algorithms](https://towardsdatascience.com/the-curse-of-dimensionality-50dc6e49aa1e)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Very hard to visualize
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There might be some useless features in your data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Complex and costly models
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are other disadvantages too, which you can search and look for details
    on Google.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we are going to discuss 3 important and famous techniques,
    which will help you in reducing the dimensions of your data while maintaining
    useful features or information.
  prefs: []
  type: TYPE_NORMAL
- en: These 3 essential techniques are divided into 2 parts.
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear dimensionality reduction**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PCA (Principal Component Analysis)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LDA (Linear Discriminant Analysis)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-linear dimensionality reduction**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: KPCA (Kernel Principal Component Analysis)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will discuss the basic idea behind each technique, practical implementation
    in sklearn, and the results of each technique.
  prefs: []
  type: TYPE_NORMAL
- en: PCA (Principal Component Analysis)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Principal Component Analysis is one of the most famous data compression technique
    that is used for unsupervised data compression.
  prefs: []
  type: TYPE_NORMAL
- en: PCA helps us to identify the patterns in the dataset based on the correlation
    between them. Or simply, it is a technique for feature extraction that combines
    our input variables in such a way that we can drop the least important ones while
    retaining the important information in the dataset. PCA finds the direction of
    the maximum variance and projects the data into lower dimensions. The principal
    components of the new subspace can be interpreted as the direction of maximum
    variance given the constraint that the new feature axes are orthogonal to each
    other.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/998e450e556e6774afaf52cbf8c0d94f.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Image Credits: [Python Machine Learning repo](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch05/ch05.ipynb).*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, x1 and x2 are the original feature axes, and PC1 and PC2 are the principal
    components.
  prefs: []
  type: TYPE_NORMAL
- en: When using PCA for dimensionality reduction, we construct a transformation matrix **W**,
    which is ***d *x *k ***dimensions, that allows us to map a sample vector **x** onto
    a new k-dimensional feature subspace that has fewer dimensions than the original *d*–dimensional
    feature space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fb838787031f2de697d85a1028c5ab3c.png)'
  prefs: []
  type: TYPE_IMG
- en: Typically *k* is very less than *d*, so the first principal component has the
    maximum variance, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: PCA is highly sensitive to data scaling, so before using PCA, we have to standardize
    our features and bring them on the same scale.
  prefs: []
  type: TYPE_NORMAL
- en: PCA is simple to implement from scratch in Python, and it is given as a built-in
    function in sklearn. To check a from scratch implementation, refer to this [repo](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch05/ch05.ipynb).
    We will review the implementation in sklearn.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sklearn Code:**'
  prefs: []
  type: TYPE_NORMAL
- en: First of all, we have to import a dataset and preprocess it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/56e87c85a384463ff69a0cb3a09baf28.png)'
  prefs: []
  type: TYPE_IMG
- en: This is our dataframe with class labels (1,2,3) in 0th column and features in
    1–13 columns. Let’s assign those to **X** and **y**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s split the dataset into training and testing portions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now since we have split the dataset, let’s apply a Standard Scaler to standardize
    our features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now, all we have to do is to perform PCA on it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can pass either how much percent of variance do we want to keep or the
    number of components. For example, if we want to store 80% of the information
    on our data, we can do *pca = PCA(n_components=0.8)*, or if we want to have 4
    features in our dataset, we can do *pca = PCA(n_components=4)*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/134bf39927377d23763de3a0884777da.png)'
  prefs: []
  type: TYPE_IMG
- en: And we can see that we have jumped from 13 features to 2 features. In order
    to check how much information we are saving, we can do
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8f6e36b2be04cb61c8be2b5b6cdbfc4f.png)'
  prefs: []
  type: TYPE_IMG
- en: For visualization purposes, we did 2 features, as 2 features can be easily visualized.
    And we can see that the first feature has ~36% of the information and 2nd feature
    has ~18% of the information. So we lost 11 features while we still have 55% of
    the information, which is really cool.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s plot these 2 features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6309aff29d0d1306210507bc3c65c3d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Now you can apply any Machine Learning algorithm, and it will converge a lot
    faster than your algorithm, which was initially trained on 13 features.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if I want to retain 80% of the information, all I have to do is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now we can check the dimensions of *X_train_pca*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: And first 5 rows of the resultant dataset are
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/038547e905f755db309fb941f07bb0d5.png)'
  prefs: []
  type: TYPE_IMG
- en: And the explained variance ratio via
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3b08d381abeb51d338525491f5202e70.png)'
  prefs: []
  type: TYPE_IMG
- en: '**How to choose the number of dimensions in PCA**'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, when you have good computational power, we normally use Grid Search
    to find the optimum number of hyperparameters, and we also use it to find the
    number of principal components, which give the best performance.
  prefs: []
  type: TYPE_NORMAL
- en: If we lack good computational power, we have to choose the number of principal
    components by a tradeoff between the performance of the classifier and the computational
    efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised Data Compression via Linear Discriminant Analysis (LDA)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LDA or Linear Discriminant Analysis is one of the famous supervised data compressions.
    As the name supervised might have given you the idea, it takes into account the
    class labels that are absent in PCA. It is also a linear transformation technique,
    just like PCA**.**
  prefs: []
  type: TYPE_NORMAL
- en: LDA uses quite a similar approach to PCA*, *in the sense that we decompose the
    matrix into eigenvectors and eigenvalues, which then forms a lower-dimensional
    feature space. However, it uses a supervised approach, meaning that it takes into
    account the class labels.
  prefs: []
  type: TYPE_NORMAL
- en: Before applying LDA to our data, we assume that our data is normally distributed,
    the classes have identical covariance matrices, and the training examples are
    statistically independent of each other. It has been proved from some research
    papers that if one, or more, of these assumptions are violated slightly, LDA still
    works pretty well.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/82295939649572b0e9526ad02361abfe.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Source: [Python Machine Learning Github Repo](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch05/ch05.ipynb).*'
  prefs: []
  type: TYPE_NORMAL
- en: This graphic summarizes the concept of LDA for the 2-class problem, where circles
    are class 1 and + are class 2\. A linear discriminant LD 1 (*x*-axis) would separate
    the 2 normally distributed classes well. Whereas the linear discriminant LD 2
    captures a lot of variance in the dataset, it would fail as it would not be able
    to gather any class discrimination information.
  prefs: []
  type: TYPE_NORMAL
- en: In LDA, what we basically do is compute the within-class and between-class scatter
    matrices. Then, we compute the eigenvectors and corresponding eigenvalues for
    the scatter matrices, sort them in decreasing order, and select the top ***k ***eigenvectors
    that correspond to the ***k*** largest eigenvalues for constructing a ***d ***x ***k***-dimensional
    transformation matrix ***W***. We then project the examples in new feature subspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can compute the ***within-class scatter matrix*** using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f06bed5d4e46a7e98a3b78d3953a0f94.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where **c **is the total number of distinct classes, and ***Si ***is the individual
    scatter matrix of class ***i ***calculated via:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eaa08685135955aef33b03f51bc96380.png)'
  prefs: []
  type: TYPE_IMG
- en: where ***mi ***is the mean vector that stores the mean feature value **μm **with
    respect to the example of class i.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/244fda843a7a5a1b3c270ce122742b24.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarly, we can compute the between-class scatter matrix **S*b:***
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e37246eb170a979ab81c82b1ab33632a.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, **m** is the overall mean that is computed, including examples from all **c **classes.
  prefs: []
  type: TYPE_NORMAL
- en: This whole work is not that difficult in Python and Numpy, and a lot of people
    have already done it. If you want to look at in detailed from-scratch implementation,
    have a look at this [repo](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch05/ch05.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a look at its implementation in sklearn in Python.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '*LinearDiscriminantAnalysis*is a class implemented in sklearn’s *discriminant_analysis*
    package. You can have a look at the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis).'
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to create an LDA object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: An important thing to notice here is that in *fit_transform* function, we are
    passing the labels of the data set, and, as discussed earlier, it is a supervised
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Here we haven’t set *n_components* a parameter, so it will automatically decide
    to pick from the formula *min(n_features, n_classes — 1)*. If we implicitly provide
    any value greater than this, it will give us an error. Check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html) for
    more details.
  prefs: []
  type: TYPE_NORMAL
- en: We can check out the resultant shape via
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Since it is in 2 dimensions, we can easily plot all the classes. We will use
    the same code as above for plotting.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7ca5199dc77b85d9d5ed36861f2dfaac.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, as we have compressed the data, we can easily apply any machine learning
    algorithm to it. We can see that this data is easily linearly separable, so Logistic
    Regression would give us quite a good accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: One thing we have to note in LDA via sklearn is that we can not provide *n_components* in
    probabilities as we can do in PCA. It must be an integer, and it must fulfill
    the condition *min(n_features, n_classes — 1)*.
  prefs: []
  type: TYPE_NORMAL
- en: LDA vs PCA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One thought you might have encountered is that since LDA also keeps in mind
    the class labels as its supervised algorithm, the results in some cases show that
    PCA performs better than LDA, such as in tasks like image recognition when each
    class consists of a small number of examples (PCA Vrtsis LDA, A. M. Martinez,
    and A. C. Kak, *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    23(2)L 228-233, 2001).
  prefs: []
  type: TYPE_NORMAL
- en: Kernel PCA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed earlier, both PCA and LDA are linear dimensionality reduction techniques.
    Similarly, most machine learning algorithms make assumptions about the linear
    separability of the data to converge perfectly.
  prefs: []
  type: TYPE_NORMAL
- en: But the real-world is not always linear, and most of the time, you have to deal
    with nonlinear datasets. So, we can see that linear dimensionality reduction techniques
    might not be the best choice for these kinds of datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9fb0f7695219f669790a579b6fa26c6f.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Image Credits: [statistics4u](http://www.statistics4u.info/fundstat_eng/cc_linvsnonlin.html).*'
  prefs: []
  type: TYPE_NORMAL
- en: Using KPCA, we can transform the data that is nonlinear to a linear subspace
    on which classifiers perform really well.
  prefs: []
  type: TYPE_NORMAL
- en: The main idea behind KPCA is that we have to perform a nonlinear mapping that
    transforms the data into a higher-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we transform our *d-*dimensional data to this higher *k*-dimensional
    space, and we define a nonlinear mapping function ϕ.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dfc7cdbe585a28cbf65933e54edd493d.png)'
  prefs: []
  type: TYPE_IMG
- en: Think of ϕ as a function that maps the original *d*-dimensional dataset onto
    a larger, *k*-dimensional dataset via some nonlinear mappings.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we perform a nonlinear mapping to transform the dataset to a
    higher dimension where we use standard PCA to project the data back to a lower-dimension
    linearly separable space.
  prefs: []
  type: TYPE_NORMAL
- en: One downside of this approach is that it is very computationally expensive.
    To overcome this problem, we have to use the **kernel trick**, via which we can
    compute the similarity between two high-dimension feature vectors in the original
    feature space.
  prefs: []
  type: TYPE_NORMAL
- en: Now the mathematical details and intuition behind the kernel trick are difficult
    to explain in a written manner. I suggest you watch [this](https://www.youtube.com/watch?v=HbDHohXPLnU&t=30s&ab_channel=caltech) video
    by the California Institute of Technology via which you can develop a good understanding
    of the mathematical intuition behind KPCA and kernel trick.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to look at the implementation in Python and scipy, I recommend you
    to go through this [repo](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch05/ch05.ipynb) where
    it is implemented step by step.
  prefs: []
  type: TYPE_NORMAL
- en: We will have a look at its implementation in sklearn in Python. What we are
    going to do is to convert a nonlinear 2-D dataset to a linear 2-D dataset. Remember,
    what KPCA will do is to map it to a higher dimension and then map it to a linearly
    separable lower dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here we have imported important libraries and datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1d93d490ca9f2ac7e5091dd704b5a48d.png)'
  prefs: []
  type: TYPE_IMG
- en: And we can see that our dataset has 100 rows and 2 columns. Let us see the number
    of classes and their values in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d918d880187905fe339437ea00b51be8.png)'
  prefs: []
  type: TYPE_IMG
- en: Since we have 2 classes, we can easily plot them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0d3408d938b02e932e12978aceb3dbdc.png)'
  prefs: []
  type: TYPE_IMG
- en: Here we can see that both classes are not linearly separable. We can confirm
    it by using any linear classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db490ba5efb40d88365394a91f330dc9.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, to make it linearly separable, we will cast it to a higher dimension,
    then map it back to a lower dimension, all using KPCA.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Here we have used the *kernel='rbf' *and *gamma=15*. There are different kernel
    functions that are to be used. Some of them are *linear*, *poly*, and *rbf*. Similarly, *gamma *is
    the kernel coefficient for the rbf, poly, and sigmoid kernels. There is a detailed
    answer on stats.StackExchange about which kernel to choose that you can read [here](https://stats.stackexchange.com/questions/131142/how-to-choose-a-kernel-for-kernel-pca).
  prefs: []
  type: TYPE_NORMAL
- en: Now we simply have to transform our data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Let us plot to check whether it is Linearly separable now or not.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ccde042b4b9a559c7fae58b67bb925ad.png)'
  prefs: []
  type: TYPE_IMG
- en: And we can see that our data is now linearly separable, and we can even confirm
    it via any linear classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e448dd009ed8dfadd92f53c1e41ad108.png)'
  prefs: []
  type: TYPE_IMG
- en: In this example, we have converted a nonlinear dataset into a linear dataset
    by using KPCA. In the coming example, we will decrease the dimensions of the wine
    dataset that we used previously in PCA and LDA.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: And this will transform our previous 13-dimensional data into 2 dimensional
    linearly separable data. We can now use any linear classifier to get good decision
    boundaries and good results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/402c6857fad92a61078d5b0314c41b95.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Learning Outcomes**'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, you learned the basics of 3 dimensionality reduction techniques
    in which 2 are linear, and 1 is nonlinear or uses the kernel trick. You have also
    learned their implementation in one of the most famous Python libraries, sklearn.
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Dimensionality Reduction with Principal Component Analysis (PCA)](https://www.kdnuggets.com/2020/05/dimensionality-reduction-principal-component-analysis.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Is Dimension Reduction In Data Science?](https://www.kdnuggets.com/2019/01/dimension-reduction-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Must-Know: What is the curse of dimensionality?](https://www.kdnuggets.com/2017/04/must-know-curse-dimensionality.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Dimensionality Reduction Techniques in Data Science](https://www.kdnuggets.com/2022/09/dimensionality-reduction-techniques-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AI, Analytics, Machine Learning, Data Science, Deep Learning…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Science & Analytics Industry Main Developments in 2021 and Key…](https://www.kdnuggets.com/2021/12/developments-predictions-data-science-analytics-industry.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Main 2021 Developments and Key 2022 Trends in AI, Data Science,…](https://www.kdnuggets.com/2021/12/trends-ai-data-science-ml-technology.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Science Methods Drive Business Success](https://www.kdnuggets.com/2023/10/nwu-data-science-methods-drive-business-success)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Centroid Initialization Methods for k-means Clustering](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
