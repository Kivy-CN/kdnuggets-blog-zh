# 深入探讨GPT模型：演变与性能比较

> 原文：[https://www.kdnuggets.com/2023/05/deep-dive-gpt-models.html](https://www.kdnuggets.com/2023/05/deep-dive-gpt-models.html)

作者：Ankit、Bhaskar & Malhar

在过去几年中，得益于大型语言模型的出现，自然语言处理领域取得了显著进展。语言模型用于机器翻译系统中，以学习如何将字符串从一种语言映射到另一种语言。在语言模型家族中，基于生成预训练变换器（GPT）的模型最近受到了最多的关注。最初，语言模型是基于规则的系统，严重依赖人工输入来运行。然而，深度学习技术的发展对这些模型处理任务的复杂性、规模和准确性产生了积极影响。

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升您的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持您的组织的IT工作

* * *

在我们的[上一篇博客](https://www.sigmoid.com/blogs/gpt-3-all-you-need-to-know-about-the-ai-language-model/)中，我们提供了对GPT-3模型各个方面的全面解释，评估了Open AI的GPT-3 API提供的功能，并探讨了模型的使用和局限性。在这篇博客中，我们将把重点转向GPT模型及其基础组件。我们还将回顾从GPT-1到最近推出的GPT-4的演变，并深入探讨每一代中所做的关键改进，这些改进使模型随着时间的推移变得更为强大。

# 1. 了解GPT模型

GPT（生成预训练变换器）是一个基于深度学习的大型语言模型（LLM），利用基于变换器的解码器架构。它的目的是处理文本数据并生成类似人类语言的文本输出。

如名称所示，该模型有三个支柱，即：

1.  生成性

1.  预训练

1.  变换器

让我们通过这些组件来探索模型：

**生成性：** 这一特性强调了模型通过理解和响应给定文本样本生成文本的能力。在GPT模型之前，文本输出是通过重新排列或提取输入中的单词来生成的。GPT模型的生成能力使其相较于现有模型具有优势，能够产生更连贯且更像人类的文本。

这种生成能力源自于训练过程中使用的建模目标。

GPT 模型使用自回归语言建模进行训练，模型接收一个词序列作为输入，然后通过概率分布预测最可能的下一个词或短语。

**预训练：** “预训练”指的是一个机器学习模型在被部署到特定任务之前，已经在大量示例数据集上进行过训练。在 GPT 的情况下，该模型在大量文本数据语料库上进行训练，使用无监督学习方法。这使得模型能够在没有明确指导的情况下学习数据中的模式和关系。

简而言之，使用大量数据以无监督的方式训练模型有助于模型理解语言的普遍特征和结构。一旦学习完成，模型可以利用这些理解来执行特定任务，例如问答和摘要。

**变换器：** 一种神经网络架构，旨在处理不同长度的文本序列。变换器的概念在2017年发表的开创性论文《Attention Is All You Need》之后获得了广泛关注。

GPT 使用仅解码器架构。变换器的主要组件是其“自注意力机制”，它使模型能够捕捉同一句子中每个词与其他词之间的关系。

示例：

1.  一只狗坐在恒河的河岸上。

1.  我将从银行取一些钱。

自注意力机制在句子中评估每个词与其他词的关系。在第一个例子中，当“bank”在“River”的上下文中被评估时，模型学习到它指的是河岸。同样，在第二个例子中，将“bank”与“money”一词相关联则暗示这是一个金融银行。

# 2\. GPT 模型的发展

现在，让我们更详细地了解 GPT 模型的各种版本，重点关注每个后续模型中引入的改进和新增功能。

![深入了解 GPT 模型](../Images/43b75d49c63cb8a4737cd4563ffe403c.png)

***第 3 幻灯片** [GPT 模型](https://docs.google.com/presentation/d/1xxyE18P-G6eZ6l34iP1-Xu2Junx6Sm4KDoaYhCFGVGk/edit#slide=id.g21eb2c7aeab_0_0)

## GPT-1

这是 GPT 系列的第一个模型，训练时使用了约 40GB 的文本数据。该模型在建模任务如 LAMBADA 中取得了最先进的结果，并在 GLUE 和 SQuAD 等任务中表现出竞争力。该模型的最大上下文长度为 512 个标记（约 380 个词），因此能够保留相对较短句子或文档的信息。模型出色的文本生成能力和在标准任务中的强大表现为后续模型的发展提供了动力。

## GPT-2

从 GPT-1 模型派生的 GPT-2 模型保留了相同的架构特征。然而，它在比 GPT-1 更大的文本数据语料库上进行训练。值得注意的是，GPT-2 可以处理双倍的输入大小，使其能够处理更广泛的文本样本。GPT-2 拥有近 15 亿个参数，展现了在语言建模方面显著的能力和潜力提升。

以下是 GPT-2 相对于 GPT-1 的一些主要改进：

1.  **修改目标训练** 是一种在预训练阶段用于增强语言模型的技术。传统上，模型仅基于前面的单词预测序列中的下一个单词，这可能导致不连贯或不相关的预测。MO 训练通过结合额外的上下文信息（如词性（名词、动词等）和主谓宾识别）来解决这一限制。通过利用这些补充信息，模型生成的输出更加连贯和信息丰富。

1.  **层归一化** 是另一种用于提高训练和性能的技术。它涉及对神经网络中每一层的激活进行归一化，而不是对整个网络的输入或输出进行归一化。这种归一化减少了内部协变量偏移的问题，即网络激活的分布因网络参数的变化而发生的变化。

1.  GPT-2 还配备了比 GPT-1 更优的 **采样** 算法。主要改进包括：

    1.  **Top-p 采样**：在采样过程中仅考虑累积概率质量超过某个阈值的令牌。这避免了从低概率令牌中采样，从而生成更具多样性和连贯性的文本。

    1.  **温度缩放**：控制 logits（即神经网络在 Softmax 之前的原始输出）中的随机性水平。较低的温度生成的文本更保守和可预测，而较高的温度则产生更具创意和意外的文本。

    1.  **无条件采样**（随机采样）选项，允许用户探索模型的生成能力，并能产生巧妙的结果。

## GPT-3

| **训练数据来源** | **训练数据规模** |
| --- | --- |
| Common Crawl、BookCorpus、Wikipedia、Books、Articles 等 | 超过 570 GB 的文本数据 |

GPT-3 模型是对 GPT-2 模型的进化，超越了它的多个方面。它在一个显著更大的文本数据语料库上进行训练，并具有最多 1750 亿个参数。

随着其规模的增加，GPT-3 引入了几个显著的改进：

+   **GShard（** 巨型分片模型并行性）：允许模型在多个加速器之间进行分割。这有助于并行训练和推理，尤其是对于拥有数十亿参数的大型语言模型。

+   **零样本学习** 功能使 GPT-3 展现出执行未明确训练的任务的能力。这意味着它可以通过利用对语言和给定任务的一般理解，响应新的提示生成文本。

+   **少样本学习** 功能使 GPT-3 能够快速适应新任务和领域，仅需少量训练。它展现了从少量示例中学习的惊人能力。

+   **多语言支持：** GPT-3 精通生成约 30 种语言的文本，包括英语、中文、法语、德语和阿拉伯语。这种广泛的多语言支持使其成为多种应用的高度通用语言模型。

+   **改进的采样：** GPT-3 使用了一种改进的采样算法，包括调整生成文本随机性的能力，类似于 GPT-2。此外，它引入了“提示”采样选项，允许根据用户指定的提示或上下文生成文本。

## GPT-3.5

| **训练数据来源** | **训练数据大小** |
| --- | --- |
| Common Crawl, BookCorpus, Wikipedia, Books, Articles, and more | > 570 GB |

与其前身类似，GPT-3.5 系列模型源自 GPT-3 模型。然而，GPT-3.5 模型的显著特点在于其依据人类价值观的具体政策，通过一种称为强化学习与人类反馈（RLHF）的技术进行整合。主要目标是使模型更贴近用户意图，减少毒性，并优先考虑生成内容的真实性。这一进化标志着在语言模型的伦理和负责任使用方面的自觉努力，以提供更安全、更可靠的用户体验。

**相较于 GPT-3 的改进：**

OpenAI 使用来自人类反馈的强化学习来微调 GPT-3，使其能够遵循广泛的指令集。RLHF 技术涉及使用强化学习原则来训练模型，其中模型根据其生成输出的质量和与人类评估者的一致性获得奖励或惩罚。通过将这种反馈整合到训练过程中，模型能够从错误中学习并提升性能，最终生成更加自然和吸引人的文本输出。

## GPT 4

GPT-4 代表了 GPT 系列的最新模型，引入了多模态能力，允许其处理文本和图像输入，同时生成文本输出。它支持各种图像格式，包括含文本的文档、照片、图表、图示、图形和截图。

尽管OpenAI没有披露GPT-4的技术细节，如模型大小、架构、训练方法或模型权重，但一些估计表明，它包含了近1万亿个参数。GPT-4的基础模型遵循类似于之前GPT模型的训练目标，旨在预测给定一系列词语后的下一个词。训练过程涉及使用大量公开可用的互联网数据和授权数据。

GPT-4在OpenAI内部的对抗性事实评估和像TruthfulQA这样的公开基准测试中，表现出比GPT-3.5更出色的性能。GPT-3.5中使用的RLHF技术也被纳入了GPT-4中。OpenAI积极根据ChatGPT和其他来源的反馈来提升GPT-4。

# GPT模型在标准建模任务中的性能比较

GPT-1、GPT-2和GPT-3在标准NLP建模任务LAMBDA、GLUE和SQuAD中的分数。

| 模型 | GLUE | LAMBADA | SQuAD F1 | SQuAD准确匹配 |
| --- | --- | --- | --- | --- |
| GPT-1 | 68.4 | 48.4 | 82.0 | 74.6 |
| GPT-2 | 84.6 | 60.1 | 89.5 | 83.0 |
| GPT-3 | 93.2 | 69.6 | 92.4 | 88.8 |
| GPT-3.5 | 93.5 | 79.3 | 92.4 | 88.8 |
| GPT-4 | 94.2 | 82.4 | 93.6 | 90.4 |

所有数字均以百分比表示。 || 来源 - BARD

这个表格展示了结果的一致改进，这可以归因于前述的增强。

GPT-3.5和GPT-4在较新的基准测试和标准考试中进行了测试。

较新的GPT模型（3.5和4）在需要推理和领域知识的任务上进行了测试。这些模型已在众多被认为具有挑战性的考试中进行测试。一个这样的考试是MBE考试，GPT-3（ada、babbage、curie、davinci）、GPT-3.5、ChatGPT和GPT-4都进行了比较。从图表中可以看到分数的持续改进，GPT-4甚至超过了平均学生分数。

图1展示了不同GPT模型在MBE*中获得的分数百分比的比较：

![深入了解GPT模型](../Images/ba177b1c6a8a0666ac0ead25b7ea812a.png)

*多州律师考试（MBE）是一项旨在评估申请人法律知识和技能的挑战性测试，是在美国从事法律工作的前提条件。

下面的图表还突出了模型的进展，并且再次超过了不同法律学科领域的平均学生分数。

![深入了解GPT模型](../Images/a6aecc3fa0218f7fb877e17f9b277bf6.png)

来源：[数据科学协会](https://www.datascienceassn.org/sites/default/files/GPT-4%20Passes%20the%20Bar%20Exam.pdf)

# 结论

随着基于Transformer的大型语言模型（LLMs）的崛起，自然语言处理领域正在迅速演变。在基于这一架构的各种语言模型中，GPT模型在输出和性能方面表现出色。作为GPT背后的组织，OpenAI自首个模型发布以来，始终在多个方面不断提升模型。

在五年的时间里，模型的规模显著扩大，从GPT-1到GPT-4约增加了8,500倍。这一显著进展可归因于在训练数据规模、数据质量、数据来源、训练技术以及参数数量等方面的持续改进。这些因素在使模型能够在各种任务中提供出色性能方面发挥了关键作用。

+   **[Ankit Mehra](https://www.linkedin.com/in/ankit-mehra-11979617b/)** 是Sigmoid的高级数据科学家。他专注于分析和基于机器学习的数据解决方案。

+   **[Malhar Yadav](https://www.linkedin.com/in/malhar-yadav-772693210/)** 是Sigmoid的助理数据科学家，同时也是编码和机器学习爱好者。

+   **[Bhaskar Ammu](https://www.linkedin.com/in/bhaskar-ammu-04a99213/)** 是Sigmoid的高级数据科学领导。他专注于为客户设计数据科学解决方案，构建数据库架构，以及管理项目和团队。

### 了解更多相关话题

+   [优化Python代码性能：深入探讨Python分析器](https://www.kdnuggets.com/2023/02/optimizing-python-code-performance-deep-dive-python-profilers.html)

+   [认识Gorilla：UC Berkeley和微软的API增强LLM…](https://www.kdnuggets.com/2023/06/meet-gorilla-uc-berkeley-microsoft-apiaugmented-llm-outperforms-gpt4-chatgpt-claude.html)

+   [揭开神经魔法：深入探讨激活函数](https://www.kdnuggets.com/unveiling-neural-magic-a-dive-into-activation-functions)

+   [与Kaggle的AI报告2023一起探索未来——看看什么是热门](https://www.kdnuggets.com/dive-into-the-future-with-kaggle-ai-report-2023-see-what-hot)

+   [ChatGPT与Google Bard：技术差异比较](https://www.kdnuggets.com/2023/03/chatgpt-google-bard-comparison-technical-differences.html)

+   [Python和R中的机器学习算法比较](https://www.kdnuggets.com/2023/06/machine-learning-algorithms-python-r.html)
