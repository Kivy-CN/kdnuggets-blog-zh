- en: The 5 Clustering Algorithms Data Scientists Need to Know
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学家需要了解的5种聚类算法
- en: 原文：[https://www.kdnuggets.com/2018/06/5-clustering-algorithms-data-scientists-need-know.html](https://www.kdnuggets.com/2018/06/5-clustering-algorithms-data-scientists-need-know.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/06/5-clustering-algorithms-data-scientists-need-know.html](https://www.kdnuggets.com/2018/06/5-clustering-algorithms-data-scientists-need-know.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: Clustering is a Machine Learning technique that involves the grouping of data
    points. Given a set of data points, we can use a clustering algorithm to classify
    each data point into a specific group. In theory, data points that are in the
    same group should have similar properties and/or features, while data points in
    different groups should have highly dissimilar properties and/or features. Clustering
    is a method of unsupervised learning and is a common technique for statistical
    data analysis used in many fields.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种机器学习技术，它涉及数据点的分组。给定一组数据点，我们可以使用聚类算法将每个数据点分类到一个特定的组中。理论上，处于同一组的数据点应该具有相似的属性和/或特征，而不同组中的数据点应该具有高度不同的属性和/或特征。聚类是一种无监督学习方法，是许多领域中用于统计数据分析的常用技术。
- en: In Data Science, we can use clustering analysis to gain some valuable insights
    from our data by seeing what groups the data points fall into when we apply a
    clustering algorithm. Today, we’re going to look at 5 popular clustering algorithms
    that data scientists need to know and their pros and cons!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学中，我们可以通过聚类分析从数据中获得一些有价值的见解，观察数据点应用聚类算法时落入了哪些组。今天，我们将深入探讨数据科学家需要了解的5种流行的聚类算法及其优缺点！
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '**K-Means Clustering**'
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**K-Means 聚类**'
- en: K-Means is probably the most well know clustering algorithm. It’s taught in
    a lot of introductory data science and machine learning classes. It’s easy to
    understand and implement in code! Check out the graphic below for an illustration.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: K-Means 可能是最知名的聚类算法。它在许多入门数据科学和机器学习课程中都有教授。它易于理解和编码实现！请查看下面的图示。
- en: '![](../Images/c05b1366307b112541414098b9107b76.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c05b1366307b112541414098b9107b76.png)'
- en: K-Means Clustering
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: K-Means 聚类
- en: To begin, we first select a number of classes/groups to use and randomly initialize
    their respective center points. To figure out the number of classes to use, it’s
    good to take a quick look at the data and try to identify any distinct groupings.
    The center points are vectors of the same length as each data point vector and
    are the “X’s” in the graphic above.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开始时，我们首先选择要使用的类别/组的数量，并随机初始化它们各自的中心点。为了确定使用的类别数量，最好快速查看数据并尝试识别任何明显的分组。中心点是与每个数据点向量长度相同的向量，是上图中的“X”。
- en: Each data point is classified by computing the distance between that point and
    each group center, and then classifying the point to be in the group whose center
    is closest to it.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个数据点通过计算该点与每个组中心之间的距离来进行分类，然后将该点分类到离它最近的组中。
- en: Based on these classified points, we recompute the group center by taking the
    mean of all the vectors in the group.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于这些分类点，我们通过取组中所有向量的均值来重新计算组中心。
- en: Repeat these steps for a set number of iterations or until the group centers
    don’t change much between iterations. You can also opt to randomly initialize
    the group centers a few times, and then select the run that looks like it provided
    the best results.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复这些步骤直到达到设定的迭代次数或直到组中心在迭代之间变化不大。你也可以选择随机初始化组中心几次，然后选择看起来效果最佳的运行结果。
- en: K-Means has the advantage that it’s pretty fast, as all we’re really doing is
    computing the distances between points and group centers; very few computations!
    It thus has a linear complexity *O*(*n*).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: K-Means的一个优点是速度相当快，因为我们实际做的只是计算点和组中心之间的距离；计算非常少！因此，它具有线性复杂度 *O*(*n*)。
- en: On the other hand, K-Means has a couple of disadvantages. Firstly, you have
    to select how many groups/classes there are. This isn’t always trivial and ideally
    with a clustering algorithm we’d want it to figure those out for us because the
    point of it is to gain some insight from the data. K-means also starts with a
    random choice of cluster centers and therefore it may yield different clustering
    results on different runs of the algorithm. Thus, the results may not be repeatable
    and lack consistency. Other cluster methods are more consistent.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，K-Means有一些缺点。首先，你必须选择有多少个组/类。这并不总是简单的，理想情况下，我们希望聚类算法能为我们解决这个问题，因为它的目的是从数据中获得一些洞察。K-Means还从随机选择的簇中心开始，因此可能会在算法的不同运行中产生不同的聚类结果。因此，结果可能无法重复且缺乏一致性。其他聚类方法则更具一致性。
- en: K-Medians is another clustering algorithm related to K-Means, except instead
    of recomputing the group center points using the mean we use the median vector
    of the group. This method is less sensitive to outliers (because of using the
    Median) but is much slower for larger datasets as sorting is required on each
    iteration when computing the Median vector.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: K-Medians是另一种与K-Means相关的聚类算法，只不过它不是使用均值来重新计算组中心点，而是使用组的中位数向量。这种方法对离群点的敏感性较低（因为使用了中位数），但对于较大的数据集来说，速度要慢得多，因为在计算中位数向量时每次迭代都需要排序。
- en: '**Mean-Shift Clustering**'
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**均值漂移聚类**'
- en: Mean shift clustering is a sliding-window-based algorithm that attempts to find
    dense areas of data points. It is a centroid-based algorithm meaning that the
    goal is to locate the center points of each group/class, which works by updating
    candidates for center points to be the mean of the points within the sliding-window.
    These candidate windows are then filtered in a post-processing stage to eliminate
    near-duplicates, forming the final set of center points and their corresponding
    groups. Check out the graphic below for an illustration.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 均值漂移聚类是一种基于滑动窗口的算法，旨在找到数据点的密集区域。它是一种基于中心点的算法，意味着目标是定位每个组/类的中心点，这通过更新中心点候选为滑动窗口内点的均值来实现。这些候选窗口随后在后处理阶段进行过滤，以消除近似重复项，形成最终的中心点集合及其对应的组。请查看下方的图示。
- en: '![](../Images/86fbab1ffd1548cb70f309b04226eb64.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/86fbab1ffd1548cb70f309b04226eb64.png)'
- en: Mean-Shift Clustering for a single sliding window
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 单个滑动窗口的均值漂移聚类
- en: To explain mean-shift we will consider a set of points in two-dimensional space
    like the above illustration. We begin with a circular sliding window centered
    at a point C (randomly selected) and having radius r as the kernel. Mean shift
    is a hill climbing algorithm which involves shifting this kernel iteratively to
    a higher density region on each step until convergence.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了解释均值漂移，我们将考虑二维空间中的一组点，如上图所示。我们从一个以点C（随机选择）为中心，半径为r的圆形滑动窗口开始，作为核。均值漂移是一种爬山算法，涉及在每一步将该核迭代地移动到更高密度区域，直到收敛。
- en: At every iteration the sliding window is shifted towards regions of higher density
    by shifting the center point to the mean of the points within the window (hence
    the name). The density within the sliding window is proportional to the number
    of points inside it. Naturally, by shifting to the mean of the points in the window
    it will gradually move towards areas of higher point density.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每次迭代中，滑动窗口被向更高密度的区域移动，通过将中心点移到窗口内点的均值（因此得名）。滑动窗口内的密度与其中的点数成正比。自然地，通过向窗口中点的均值移动，它将逐渐朝着点密度更高的区域移动。
- en: We continue shifting the sliding window according to the mean until there is
    no direction at which a shift can accommodate more points inside the kernel. Check
    out the graphic above; we keep moving the circle until we no longer are increasing
    the density (i.e number of points in the window).
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们继续根据均值移动滑动窗口，直到没有方向可以使得更多点进入核。请查看上面的图示；我们继续移动圆圈，直到我们不再增加密度（即窗口中的点数）。
- en: This process of steps 1 to 3 is done with many sliding windows until all points
    lie within a window. When multiple sliding windows overlap the window containing
    the most points is preserved. The data points are then clustered according to
    the sliding window in which they reside.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤1到步骤3的过程会用许多滑动窗口完成，直到所有点都在一个窗口内。当多个滑动窗口重叠时，会保留包含最多点的窗口。然后根据点所在的滑动窗口进行聚类。
- en: An illustration of the entire process from end-to-end with all of the sliding
    windows is show below. Each black dot represents the centroid of a sliding window
    and each gray dot is a data point.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下方展示了整个过程的插图，包含所有滑动窗口。每个黑点表示一个滑动窗口的质心，每个灰点是一个数据点。
- en: '![](../Images/a8bfbaaa240000f99f521641a3e1b295.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8bfbaaa240000f99f521641a3e1b295.png)'
- en: The entire process of Mean-Shift Clustering
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Mean-Shift 聚类的整个过程
- en: In contrast to K-means clustering there is no need to select the number of clusters
    as mean-shift automatically discovers this. That’s a massive advantage. The fact
    that the cluster centers converge towards the points of maximum density is also
    quite desirable as it is quite intuitive to understand and fits well in a naturally
    data-driven sense. The drawback is that the selection of the window size/radius
    “r” can be non-trivial.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 与 K-means 聚类相比，无需选择聚类的数量，因为均值漂移会自动发现这一点。这是一个巨大的优势。聚类中心收敛到最大密度点的事实也相当理想，因为它直观易懂，并且很好地符合自然数据驱动的感觉。缺点是窗口大小/半径“r”的选择可能并不简单。
- en: '****Density-Based Spatial Clustering of Applications with Noise** (**DBSCAN**)**'
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**基于密度的空间聚类应用与噪声（DBSCAN）**'
- en: DBSCAN is a density based clustered algorithm similar to mean-shift, but with
    a couple of notable advantages. Check out another fancy graphic below and let’s
    get started!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN 是一种基于密度的聚类算法，类似于均值漂移，但具有一些显著的优势。查看下面的另一个炫酷图形，我们开始吧！
- en: '![](../Images/08bb9d6663e9642833570cea17f613bc.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08bb9d6663e9642833570cea17f613bc.png)'
- en: DBSCAN Smiley Face Clustering
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN 微笑面孔聚类
- en: DBSCAN begins with an arbitrary starting data point that has not been visited.
    The neighborhood of this point is extracted using a distance epsilon ε (All points
    which are within the ε distance are neighborhood points).
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DBSCAN 从一个未被访问的任意起始数据点开始。通过距离 ε (所有在 ε 距离内的点都是邻域点) 提取该点的邻域。
- en: If there are a sufficient number of points (according to minPoints) within this
    neighborhood then the clustering process starts and the current data point becomes
    the first point in the new cluster. Otherwise, the point will be labeled as noise
    (later this noisy point might become the part of the cluster). In both cases that
    point is marked as “visited”.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果在这个邻域内有足够数量的点（根据 minPoints），则开始聚类过程，当前数据点成为新聚类中的第一个点。否则，该点会被标记为噪声（以后这个噪声点可能会成为聚类的一部分）。在这两种情况下，该点都会被标记为“已访问”。
- en: For this first point in the new cluster, the points within its ε distance neighborhood
    also become part of the same cluster. This procedure of making all points in the
    ε neighborhood belong to the same cluster is then repeated for all of the new
    points that have been just added to the cluster group.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于新聚类中的第一个点，距离其 ε 距离邻域内的点也会成为相同的聚类的一部分。这个将 ε 邻域内的所有点归入相同聚类的过程会对刚刚添加到聚类组中的所有新点重复进行。
- en: This process of steps 2 and 3 is repeated until all points in the cluster are
    determined i.e all points within the ε neighborhood of the cluster have been visited
    and labelled.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤2和步骤3的过程会重复进行，直到确定聚类中的所有点，即所有在聚类的 ε 邻域内的点都已被访问和标记。
- en: Once we’re done with the current cluster, a new unvisited point is retrieved
    and processed, leading to the discovery of a further cluster or noise. This process
    repeats until all points are marked as visited. Since at the end of this all points
    have been visited, each point well have been marked as either belonging to a cluster
    or being noise.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦完成当前的聚类，就会检索并处理一个新的未访问点，从而发现进一步的聚类或噪声。这个过程会重复进行，直到所有点都被标记为已访问。由于最终所有点都已被访问，因此每个点都会被标记为属于某个聚类或是噪声。
- en: DBSCAN poses some great advantages over other clustering algorithms. Firstly,
    it does not require a pe-set number of clusters at all. It also identifies outliers
    as noises unlike mean-shift which simply throws them into a cluster even if the
    data point is very different. Additionally, it is able to find arbitrarily sized
    and arbitrarily shaped clusters quite well.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN相较于其他聚类算法有一些很好的优势。首先，它根本不需要预设的聚类数量。它还将离群点识别为噪声，这与均值漂移不同，均值漂移即使数据点非常不同也会将它们放入聚类中。此外，它能够很好地找到任意大小和任意形状的聚类。
- en: The main drawback of DBSCAN is that it doesn’t perform as well as others when
    the clusters are of varying density. This is because the setting of the distance
    threshold ε and minPoints for identifying the neighborhood points will vary from
    cluster to cluster when the density varies. This drawback also occurs with very
    high-dimensional data since again the distance threshold ε becomes challenging
    to estimate.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN的主要缺点是当聚类的密度变化时，它的表现不如其他方法。这是因为用于识别邻域点的距离阈值ε和minPoints的设置会随着聚类密度的变化而变化。这个缺点也出现在非常高维的数据中，因为距离阈值ε的估计变得具有挑战性。
- en: '**Expectation–Maximization (EM) Clustering using Gaussian Mixture Models (GMM)**'
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**期望最大化（EM）使用高斯混合模型（GMM）的聚类**'
- en: One of the major drawbacks of K-Means is its naive use of the mean value for
    the cluster center. We can see why this isn’t the best way of doing things by
    looking at the image below. On the left hand side it looks quite obvious to the
    human eye that there are two circular clusters with different radius’ centered
    at the same mean. K-Means can’t handle this because the mean values of the clusters
    are a very close together. K-Means also fails in cases where the clusters are
    not circular, again as a result of using the mean as cluster center.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: K均值的一个主要缺点是它对聚类中心的均值值的简单使用。通过查看下图，我们可以看到为什么这不是最好的方法。在左侧，人眼很明显地看到有两个不同半径的圆形聚类，以相同的均值为中心。K均值无法处理这种情况，因为聚类的均值非常接近。K均值在聚类不是圆形的情况下也会失败，这又是因为使用均值作为聚类中心。
- en: '![](../Images/9279d1ef196e070cd23e72769d8d278e.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9279d1ef196e070cd23e72769d8d278e.png)'
- en: Two failure cases for K-Means
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: K均值的两个失败案例
- en: 'Gaussian Mixture Models (GMMs) give us more flexibility than K-Means. With
    GMMs we assume that the data points are Gaussian distributed; this is a less restrictive
    assumption than saying they are circular by using the mean. That way, we have
    two parameters to describe the shape of the clusters: the mean and the standard
    deviation! Taking an example in two dimensions, this means that the clusters can
    take any kind of elliptical shape (since we have standard deviation in both the
    x and y directions). Thus, each Gaussian distribution is assigned to a single
    cluster.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯混合模型（GMMs）比K均值提供了更多的灵活性。使用GMM时，我们假设数据点是高斯分布的；这一假设比使用均值假设数据点为圆形要不那么严格。因此，我们有两个参数来描述聚类的形状：均值和标准差！以二维为例，这意味着聚类可以采取任何形状的椭圆（因为我们在x和y方向都有标准差）。因此，每个高斯分布被分配到一个单独的聚类中。
- en: In order to find the parameters of the Gaussian for each cluster (e.g the mean
    and standard deviation) we will use an optimization algorithm called Expectation–Maximization
    (EM). Take a look at the graphic below as an illustration of the Gaussians being
    fitted to the clusters. Then we can proceed on to the process of Expectation–Maximization
    clustering using GMMs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到每个聚类的高斯参数（如均值和标准差），我们将使用一种称为期望最大化（EM）的优化算法。请查看下面的图形，以说明高斯如何适应聚类。然后，我们可以继续进行使用GMM的期望最大化聚类过程。
- en: '![](../Images/02eb2b5a85a77e79c5f3bd66b1a676fe.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02eb2b5a85a77e79c5f3bd66b1a676fe.png)'
- en: EM Clustering using GMMs
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GMM的EM聚类
- en: We begin by selecting the number of clusters (like K-Means does) and randomly
    initializing the Gaussian distribution parameters for each cluster. One can try
    to provide a good guesstimate for the initial parameters by taking a quick look
    at the data too. Though note, as can be seen in the graphic above, this isn’t
    100% necessary as the Gaussians start our as very poor but are quickly optimized.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先选择聚类的数量（像K均值那样）并随机初始化每个聚类的高斯分布参数。可以通过快速查看数据来尝试提供一个好的初始参数估计。尽管要注意，如上图所示，这并不是100%必要，因为高斯初始时很差，但会很快得到优化。
- en: Given these Gaussian distributions for each cluster, compute the probability
    that each data point belongs to a particular cluster. The closer a point is to
    the Gaussian’s center, the more likely it belongs to that cluster. This should
    make intuitive sense since with a Gaussian distribution we are assuming that most
    of the data lies closer to the center of the cluster.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定每个簇的高斯分布，计算每个数据点属于特定簇的概率。一个点离高斯中心越近，它属于该簇的可能性就越大。这应该是直观的，因为在高斯分布中，我们假设大多数数据点更靠近簇的中心。
- en: Based on these probabilities, we compute a new set of parameters for the Gaussian
    distributions such that we maximize the probabilities of data points within the
    clusters. We compute these new parameters using a weighted sum of the data point
    positions, where the weights are the probabilities of the data point belonging
    in that particular cluster. To explain this in a visual manner we can take a look
    at the graphic above, in particular the yellow cluster as an example. The distribution
    starts off randomly on the first iteration, but we can see that most of the yellow
    points are to the right of that distribution. When we compute a sum weighted by
    the probabilities, even though there are some points near the center, most of
    them are on the right. Thus naturally the distribution’s mean is shifted closer
    to those set of points. We can also see that most of the points are “top-right
    to bottom-left”. Therefore the standard deviation changes to create an ellipse
    that is more fitted to these points, in order to maximize the sum weighted by
    the probabilities.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于这些概率，我们计算高斯分布的新参数集，以最大化簇内数据点的概率。我们使用数据点位置的加权和来计算这些新参数，其中权重是数据点属于特定簇的概率。为了直观地解释这一点，我们可以查看上面的图形，特别是黄色簇作为例子。分布在第一次迭代时是随机的，但我们可以看到大多数黄色点位于该分布的右侧。当我们计算加权和时，即使有些点靠近中心，大多数点还是在右侧。因此，分布的均值自然会向这些点的集合移动。我们还可以看到大多数点的分布是“从左上到右下”。因此，标准差发生变化，形成一个更适合这些点的椭圆，以最大化加权和。
- en: Steps 2 and 3 are repeated iteratively until convergence, where the distributions
    don’t change much from iteration to iteration.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第2步和第3步会反复进行，直到收敛为止，此时分布在每次迭代中变化不大。
- en: There are really 2 key advantages to using GMMs. Firstly GMMs are a lot more **flexible** in
    terms of **cluster covariance** than K-Means; due to the standard deviation parameter,
    the clusters can take on any ellipse shape, rather than being restricted to circles.
    K-Means is actually a special case of GMM in which each cluster’s covariance along
    all dimensions approaches 0\. Secondly, since GMMs use probabilities, they can
    have multiple clusters per data point. So if a data point is in the middle of
    two overlapping clusters, we can simply define its class by saying it belongs
    X-percent to class 1 and Y-percent to class 2\. I.e GMMs support **mixed** **membership**.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GMM有两个主要优势。首先，GMM在**簇协方差**方面比K-Means更加**灵活**；由于标准差参数，簇可以呈现任何椭圆形状，而不是局限于圆形。K-Means实际上是GMM的一个特例，其中每个簇在所有维度上的协方差接近0。其次，由于GMM使用概率，它们可以为每个数据点提供多个簇。因此，如果一个数据点位于两个重叠簇的中间，我们可以简单地定义它的类别，表示它属于类别1的概率是X%，属于类别2的概率是Y%。即GMM支持**混合**
    **成员资格**。
- en: '**Agglomerative Hierarchical Clustering**'
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**聚合层次聚类**'
- en: 'Hierarchical clustering algorithms actually fall into 2 categories: top-down
    or bottom-up. Bottom-up algorithms treat each data point as a single cluster at
    the outset and then successively merge (or *agglomerate*) pairs of clusters until
    all clusters have been merged into a single cluster that contains all data points.
    Bottom-up hierarchical clustering is therefore called *hierarchical agglomerative
    clustering* or *HAC*. This hierarchy of clusters is represented as a tree (or
    dendrogram). The root of the tree is the unique cluster that gathers all the samples,
    the leaves being the clusters with only one sample. Check out the graphic below
    for an illustration before moving on to the algorithm steps'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类算法实际上分为两类：自上而下和自下而上。自下而上的算法在开始时将每个数据点视为一个单独的簇，然后逐步合并（或*聚合*）簇对，直到所有簇合并为一个包含所有数据点的单一簇。因此，自下而上的层次聚类被称为*层次聚合聚类*或*HAC*。这种簇的层次结构表示为一棵树（或树状图）。树的根是一个唯一的簇，汇集了所有样本，叶子是仅包含一个样本的簇。请查看下面的图示，然后再继续算法步骤。
- en: '![](../Images/a3c3d0dcbe2d42f27bb36f5f6768a2a2.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3c3d0dcbe2d42f27bb36f5f6768a2a2.png)'
- en: Agglomerative Hierarchical Clustering
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合层次聚类
- en: We begin by treating each data point as a single cluster i.e if there are X
    data points in our dataset then we have X clusters. We then select a distance
    metric that measures the distance between two clusters. As an example we will
    use *average linkage *which defines the distance between two clusters to be the
    average distance between data points in the first cluster and data points in the
    second cluster.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们开始时将每个数据点视为一个单独的簇，即如果数据集中有X个数据点，那么我们就有X个簇。然后，我们选择一种度量簇间距离的距离度量。例如，我们将使用*平均连接*，它定义了两个簇之间的距离为第一个簇中的数据点和第二个簇中的数据点之间的平均距离。
- en: On each iteration we combine two clusters into one. The two clusters to be combined
    are selected as those with the smallest average linkage. I.e according to our
    selected distance metric, these two clusters have the smallest distance between
    each other and therefore are the most similar and should be combined.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每次迭代中，我们将两个簇合并为一个。被合并的两个簇是选择那些具有最小平均连接的簇。即根据我们选择的距离度量，这两个簇之间的距离最小，因此它们是最相似的，应该被合并。
- en: Step 2 is repeated until we reach the root of the tree i.e we only have one
    cluster which contains all data points. In this way we can select how many clusters
    we want in the end, simply by choosing when to stop combining the clusters i.e
    when we stop building the tree!
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤2重复进行，直到我们到达树的根部，即我们只剩下一个包含所有数据点的簇。通过这种方式，我们可以选择最终需要多少个簇，只需选择何时停止合并簇，即何时停止构建树！
- en: Hierarchical clustering does not require us to specify the number of clusters
    and we can even select which number of clusters looks best since we are building
    a tree. Additionally, the algorithm is not sensitive to the choice of distance
    metric; all of them tend to work equally well whereas with other clustering algorithms,
    the choice of distance metric is critical. A particularly good use case of hierarchical
    clustering methods is when the underlying data has a hierarchical structure and
    you want to recover the hierarchy; other clustering algorithms can’t do this.
    These advantages of hierarchical clustering come at the cost of lower efficiency,
    as it has a time complexity of *O(n³)*, unlike the linear complexity of K-Means
    and GMM.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类不需要我们指定簇的数量，我们甚至可以选择哪个簇的数量看起来最好，因为我们在构建树。此外，该算法对距离度量的选择不敏感；所有度量通常效果相同，而其他聚类算法中，距离度量的选择至关重要。层次聚类方法的一个特别好的用例是，当基础数据具有层次结构时，你希望恢复这种层次结构；其他聚类算法无法做到这一点。这些层次聚类的优点是以较低效率为代价的，因为它的时间复杂度为*O(n³)*，不同于K-Means和GMM的线性复杂度。
- en: '**Conclusion**'
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**结论**'
- en: There are your top 5 clustering algorithms that a data scientist should know!
    We’ll end off with an awesome visualization of how well these algorithms and a
    few others perform, courtesy of Scikit Learn!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是数据科学家应知道的前五种聚类算法！我们将以一个精彩的可视化结束，展示这些算法以及其他一些算法的表现，感谢Scikit Learn！
- en: '![](../Images/a3ced5dd22d1f6f912cf4f94518ed2c6.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3ced5dd22d1f6f912cf4f94518ed2c6.png)'
- en: '**Bio: [George Seif](https://towardsdatascience.com/@george.seif94)** is a
    Certified Nerd and AI / Machine Learning Engineer.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介： [George Seif](https://towardsdatascience.com/@george.seif94)** 是一名认证的极客和人工智能
    / 机器学习工程师。'
- en: '[Original](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68).
    Reposted with permission.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)。经许可转载。'
- en: '**Related:**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Comparing Clustering Techniques: A Concise Technical Overview](/2016/09/comparing-clustering-techniques-concise-technical-overview.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[比较聚类技术：简明技术概述](/2016/09/comparing-clustering-techniques-concise-technical-overview.html)'
- en: '[Must-Know: How to determine the most useful number of clusters?](/2017/05/must-know-most-useful-number-clusters.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[必知：如何确定最有用的簇数量？](/2017/05/must-know-most-useful-number-clusters.html)'
- en: '[Toward Increased k-means Clustering Efficiency with the Naive Sharding Centroid
    Initialization Method](/2017/03/naive-sharding-centroid-initialization-method.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过朴素分片质心初始化方法提高k-means聚类效率](/2017/03/naive-sharding-centroid-initialization-method.html)'
- en: More On This Topic
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成为优秀数据科学家所需的5项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个初学者数据科学家应该掌握的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2021年最佳ETL工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个数据科学家都应该知道的三个R库（即使你使用Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
- en: '[Clustering Unleashed: Understanding K-Means Clustering](https://www.kdnuggets.com/2023/07/clustering-unleashed-understanding-kmeans-clustering.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[解放聚类：理解K-Means聚类](https://www.kdnuggets.com/2023/07/clustering-unleashed-understanding-kmeans-clustering.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，找到目标再……](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
