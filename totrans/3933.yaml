- en: 'Cookiecutter Data Science: How to Organize Your Data Science Project'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2018/07/cookiecutter-data-science-organize-data-project.html](https://www.kdnuggets.com/2018/07/cookiecutter-data-science-organize-data-project.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By [DrivenData](https://www.drivendata.org/)**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/cd61bf44f3394ed05156d00b1e97ee5e.png)'
  prefs: []
  type: TYPE_IMG
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Why use this project structure?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We're not talking about bikeshedding the indentation aesthetics or pedantic
    formatting standards — ultimately, data science code quality is about correctness
    and reproducibility.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When we think about data analysis, we often think just about the resulting reports,
    insights, or visualizations. While these end products are generally the main event,
    it's easy to focus on making the products *look nice* and ignore the *quality
    of the code that generates them*. Because these end products are created programmatically, **code
    quality is still important**! And we're not talking about bikeshedding the indentation
    aesthetics or pedantic formatting standards — ultimately, data science code quality
    is about correctness and reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: It's no secret that good analyses are often the result of very scattershot and
    serendipitous explorations. Tentative experiments and rapidly testing approaches
    that might not work out are all part of the process for getting to the good stuff,
    and there is no magic bullet to turn data exploration into a simple, linear progression.
  prefs: []
  type: TYPE_NORMAL
- en: 'That being said, once started it is not a process that lends itself to thinking
    carefully about the structure of your code or project layout, so it''s best to
    start with a clean, logical structure and stick to it throughout. We think it''s
    a pretty big win all around to use a fairly standardized setup like this one.
    Here''s why:'
  prefs: []
  type: TYPE_NORMAL
- en: Other people will thank you
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Nobody sits around before creating a new Rails project to figure out where they
    want to put their views; they just run `rails new` to get a standard project skeleton
    like everybody else.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A well-defined, standard project structure means that a newcomer can begin to
    understand an analysis without digging in to extensive documentation. It also
    means that they don't necessarily have to read 100% of the code before knowing
    where to look for very specific things.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well organized code tends to be self-documenting in that the organization itself
    provides context for your code without much overhead. People will thank you for
    this because they can:'
  prefs: []
  type: TYPE_NORMAL
- en: Collaborate more easily with you on this analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn from your analysis about the process and the domain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feel confident in the conclusions at which the analysis arrives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A good example of this can be found in any of the major web development frameworks
    like Django or Ruby on Rails. Nobody sits around before creating a new Rails project
    to figure out where they want to put their views; they just run `rails new` to
    get a standard project skeleton like everybody else. Because that default project
    structure is *logical* and *reasonably standard across most projects*, it is much
    easier for somebody who has never seen a particular project to figure out where
    they would find the various moving parts.
  prefs: []
  type: TYPE_NORMAL
- en: Another great example is the [Filesystem Hierarchy Standard](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard) for
    Unix-like systems. The `/etc` directory has a very specific purpose, as does the `/tmp` folder,
    and everybody (more or less) agrees to honor that social contract. That means
    a Red Hat user and an Ubuntu user both know roughly where to look for certain
    types of files, even when using each other's system — or any other standards-compliant
    system for that matter!
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, that's how it should be when a colleague opens up your data science
    project.
  prefs: []
  type: TYPE_NORMAL
- en: You will thank you
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Ever tried to reproduce an analysis that you did a few months ago or even a
    few years ago? You may have written the code, but it''s now impossible to decipher
    whether you should use `make_figures.py.old`, `make_figures_working.py` or `new_make_figures01.py` to
    get things done. Here are some questions we''ve learned to ask with a sense of
    existential dread:'
  prefs: []
  type: TYPE_NORMAL
- en: Are we supposed to go in and join the column X to the data before we get started
    or did that come from one of the notebooks?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Come to think of it, which notebook do we have to run first before running
    the plotting code: was it "process data" or "clean data"?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where did the shapefiles get downloaded from for the geographic plots?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Et cetera, times infinity.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These types of questions are painful and are symptoms of a disorganized project.
    A good project structure encourages practices that make it easier to come back
    to old work, for example separation of concerns, abstracting analysis as a [DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph),
    and engineering best practices like version control.
  prefs: []
  type: TYPE_NORMAL
- en: Nothing here is binding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '"A foolish consistency is the hobgoblin of little minds" — Ralph Waldo Emerson
    (and [PEP 8!](https://www.python.org/dev/peps/pep-0008/#a-foolish-consistency-is-the-hobgoblin-of-little-minds))'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Disagree with a couple of the default folder names? Working on a project that's
    a little nonstandard and doesn't exactly fit with the current structure? Prefer
    to use a different package than one of the (few) defaults?
  prefs: []
  type: TYPE_NORMAL
- en: '**Go for it!** This is a lightweight structure, and is intended to be a good *starting
    point* for many projects. Or, as PEP 8 put it:'
  prefs: []
  type: TYPE_NORMAL
- en: Consistency within a project is more important. Consistency within one module
    or function is the most important. ... However, know when to be inconsistent --
    sometimes style guide recommendations just aren't applicable. When in doubt, use
    your best judgment. Look at other examples and decide what looks best. And don't
    hesitate to ask!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With this in mind, we've created a data science cookiecutter template for projects
    in Python. Your analysis doesn't have to be in Python, but the template does provide
    some Python boilerplate that you'd want to remove (in the `src` folder for example,
    and the Sphinx documentation skeleton in `docs`).
  prefs: []
  type: TYPE_NORMAL
- en: Requirements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Python 2.7 or 3.5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[cookiecutter Python package](http://cookiecutter.readthedocs.org/en/latest/installation.html) >=
    1.4.0: `pip install cookiecutter`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting a new project
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Starting a new project is as easy as running this command at the command line.
    No need to create a directory first, the cookiecutter will do it for you.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Directory structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Opinions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are some opinions implicit in the project structure that have grown out
    of our experience with what works and what doesn't when collaborating on data
    science projects. Some of the opinions are about workflows, and some of the opinions
    are about tools that make life easier. Here are some of the beliefs which this
    project is built on—if you've got thoughts, please [contribute or share them](https://drivendata.github.io/cookiecutter-data-science/#contributing).
  prefs: []
  type: TYPE_NORMAL
- en: Data is immutable
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Don't ever edit your raw data, especially not manually, and especially not in
    Excel. Don't overwrite your raw data. Don't save multiple versions of the raw
    data. Treat the data (and its format) as immutable. The code you write should
    move the raw data through a pipeline to your final analysis. You shouldn't have
    to run all of the steps every time you want to make a new figure (see [Analysis
    is a DAG](https://drivendata.github.io/cookiecutter-data-science/#analysis-is-a-dag)),
    but anyone should be able to reproduce the final products with only the code in `src` and
    the data in `data/raw`.
  prefs: []
  type: TYPE_NORMAL
- en: Also, if data is immutable, it doesn't need source control in the same way that
    code does. Therefore, ***by default, the data folder is included in the `.gitignore`
    file.*** If you have a small amount of data that rarely changes, you may want
    to include the data in the repository. Github currently warns if files are over
    50MB and rejects files over 100MB. Some other options for storing/syncing large
    data include [AWS S3](https://aws.amazon.com/s3/) with a syncing tool (e.g., [`s3cmd`](http://s3tools.org/s3cmd)), [Git
    Large File Storage](https://git-lfs.github.com/), [Git Annex](https://git-annex.branchable.com/),
    and [dat](http://dat-data.com/). Currently by default, we ask for an S3 bucket
    and use [AWS CLI](http://docs.aws.amazon.com/cli/latest/reference/s3/index.html) to
    sync data in the `data` folder with the server.
  prefs: []
  type: TYPE_NORMAL
- en: Notebooks are for exploration and communication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Notebook packages like the [Jupyter notebook](http://jupyter.org/), [Beaker
    notebook](http://beakernotebook.com/), [Zeppelin](http://zeppelin-project.org/),
    and other literate programming tools are very effective for exploratory data analysis.
    However, these tools can be less effective for reproducing an analysis. When we
    use notebooks in our work, we often subdivide the `notebooks` folder. For example, `notebooks/exploratory` contains
    initial explorations, whereas `notebooks/reports` is more polished work that can
    be exported as html to the `reports` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since notebooks are challenging objects for source control (e.g., diffs of
    the `json` are often not human-readable and merging is near impossible), we recommended
    not collaborating directly with others on Jupyter notebooks. There are two steps
    we recommend for using notebooks effectively:'
  prefs: []
  type: TYPE_NORMAL
- en: Follow a naming convention that shows the owner and the order the analysis was
    done in. We use the format `<step>-<ghuser>-<description>.ipynb` (e.g., `0.3-bull-visualize-distributions.ipynb`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Refactor the good parts. Don't write code to do the same task in multiple notebooks.
    If it's a data preprocessing task, put it in the pipeline at `src/data/make_dataset.py` and
    load data from `data/interim`. If it's useful utility code, refactor it to `src`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now by default we turn the project into a Python package (see the `setup.py` file).
    You can import your code and use it in notebooks with a cell like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Analysis is a DAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Often in an analysis you have long-running steps that preprocess data or train
    models. If these steps have been run already (and you have stored the output somewhere
    like the `data/interim` directory), you don't want to wait to rerun them every
    time. We prefer [`make`](https://www.gnu.org/software/make/) for managing steps
    that depend on each other, especially the long-running ones. Make is a common
    tool on Unix-based platforms (and [is available for Windows](https://drivendata.github.io/cookiecutter-data-science/)).
    Following the [`make` documentation](https://www.gnu.org/software/make/), [Makefile
    conventions](https://www.gnu.org/prep/standards/html_node/Makefile-Conventions.html#Makefile-Conventions),
    and [portability guide](http://www.gnu.org/savannah-checkouts/gnu/autoconf/manual/autoconf-2.69/html_node/Portable-Make.html#Portable-Make) will
    help ensure your Makefiles work effectively across systems. Here are [some](http://zmjones.com/make/) [examples](https://blog.kaggle.com/2012/10/15/make-for-data-scientists/) to [get
    started](https://web.archive.org/web/20150206054212/http://www.bioinformaticszen.com/post/decomplected-workflows-makefiles/).
    A number of data folks use `make` as their tool of choice, including [Mike Bostock](https://bost.ocks.org/mike/make/).
  prefs: []
  type: TYPE_NORMAL
- en: There are other tools for managing DAGs that are written in Python instead of
    a DSL (e.g., [Paver](http://paver.github.io/paver/#), [Luigi](http://luigi.readthedocs.org/en/stable/index.html), [Airflow](https://pythonhosted.org/airflow/cli.html), [Snakemake](https://bitbucket.org/snakemake/snakemake/wiki/Home), [Ruffus](http://www.ruffus.org.uk/),
    or [Joblib](https://pythonhosted.org/joblib/memory.html)). Feel free to use these
    if they are more appropriate for your analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Build from the environment up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step in reproducing an analysis is always reproducing the computational
    environment it was run in. You need the same tools, the same libraries, and the
    same versions to make everything play nicely together.
  prefs: []
  type: TYPE_NORMAL
- en: 'One effective approach to this is use [virtualenv](https://virtualenv.pypa.io/en/latest/) (we
    recommend [virtualenvwrapper](https://virtualenvwrapper.readthedocs.org/en/latest/) for
    managing virtualenvs). By listing all of your requirements in the repository (we
    include a `requirements.txt` file) you can easily track the packages needed to
    recreate the analysis. Here is a good workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: Run `mkvirtualenv` when creating a new project
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`pip install` the packages that your analysis needs'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run `pip freeze > requirements.txt` to pin the exact package versions used to
    recreate the analysis
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you find you need to install another package, run `pip freeze > requirements.txt` again
    and commit the changes to version control.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you have more complex requirements for recreating your environment, consider
    a virtual machine based approach such as [Docker](https://www.docker.com/) or [Vagrant](https://www.vagrantup.com/).
    Both of these tools use text-based formats (Dockerfile and Vagrantfile respectively)
    you can easily add to source control to describe how to create a virtual machine
    with the requirements you need.
  prefs: []
  type: TYPE_NORMAL
- en: Keep secrets and configuration out of version control
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You *really* don''t want to leak your AWS secret key or Postgres username and
    password on Github. Enough said — see the [Twelve Factor App](http://12factor.net/config) principles
    on this point. Here''s one way to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Store your secrets and config variables in a special file**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `.env` file in the project root folder. Thanks to the `.gitignore`,
    this file should never get committed into the version control repository. Here''s
    an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Use a package to load these variables automatically.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you look at the stub script in `src/data/make_dataset.py`, it uses a package
    called [python-dotenv](https://github.com/theskumar/python-dotenv) to load up
    all the entries in this file as environment variables so they are accessible with `os.environ.get`.
    Here''s an example snippet adapted from the `python-dotenv` documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**AWS CLI configuration**'
  prefs: []
  type: TYPE_NORMAL
- en: 'When using Amazon S3 to store data, a simple method of managing AWS access
    is to set your access keys to environment variables. However, managing mutiple
    sets of keys on a single machine (e.g. when working on multiple projects) it is
    best to use a [credentials file](https://docs.aws.amazon.com/cli/latest/userguide/cli-config-files.html),
    typically located in `~/.aws/credentials`. A typical file might look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You can add the profile name when initialising a project; assuming no applicable
    environment variables are set, the profile credentials will be used be default.
  prefs: []
  type: TYPE_NORMAL
- en: Be conservative in changing the default folder structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To keep this structure broadly applicable for many different kinds of projects,
    we think the best approach is to be liberal in changing the folders around for *your* project,
    but be conservative in changing the default structure for *all* projects.
  prefs: []
  type: TYPE_NORMAL
- en: We've created a folder-layout label specifically for issues proposing to add,
    subtract, rename, or move folders around. More generally, we've also created a needs-discussion label
    for issues that should have some careful discussion and broad support before being
    implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Contributing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Cookiecutter Data Science project is opinionated, but not afraid to be wrong.
    Best practices change, tools evolve, and lessons are learned. **The goal of this
    project is to make it easier to start, structure, and share an analysis.** [Pull
    requests](https://github.com/drivendata/cookiecutter-data-science/pulls) and [filing
    issues](https://github.com/drivendata/cookiecutter-data-science/issues) is encouraged.
    We'd love to hear what works for you, and what doesn't.
  prefs: []
  type: TYPE_NORMAL
- en: If you use the Cookiecutter Data Science project, link back to this page or [give
    us a holler](https://twitter.com/drivendataorg) and [let us know](mailto:info@drivendata.org)!
  prefs: []
  type: TYPE_NORMAL
- en: Links to related projects and references
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Project structure and reproducibility is talked about more in the R research
    community. Here are some projects and blog posts if you're working in R that may
    help you out.
  prefs: []
  type: TYPE_NORMAL
- en: '[Project Template](http://projecttemplate.net/index.html) - An R data analysis
    template'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"[Designing projects](http://nicercode.github.io/blog/2013-04-05-projects/)"
    on Nice R Code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"[My research workflow](http://www.carlboettiger.info/2012/05/06/research-workflow.html)"
    on Carlboettifer.info'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"[A Quick Guide to Organizing Computational Biology Projects](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000424)"
    in PLOS Computational Biology'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, a huge thanks to the [Cookiecutter](https://cookiecutter.readthedocs.org/en/latest/) project
    ([github](https://github.com/audreyr/cookiecutter)), which is helping us all spend
    less time thinking about and writing boilerplate and more time getting things
    done.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [DrivenData](https://www.drivendata.org/)** is a mission-driven data
    science firm that brings the powerful capabilities of data science, machine learning,
    and artificial intelligence to organizations tackling the world’s biggest challenges.
    DrivenData Labs (drivendata.co) helps mission-driven organizations harness data
    to work smarter, offer more impactful services, and use machine intelligence to
    its fullest potential. DrivenData also runs online machine learning competitions
    (drivendata.org) where a passionate, global community of data scientists build
    algorithms for social impact.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://drivendata.github.io/cookiecutter-data-science/). Reposted
    with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Data Structures Related to Machine Learning Algorithms](/2018/01/data-structures-related-machine-learning-algorithms.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python Regular Expressions Cheat Sheet](/2018/04/python-regular-expressions-cheat-sheet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introduction to Functional Programming in Python](/2018/02/introduction-functional-programming-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
