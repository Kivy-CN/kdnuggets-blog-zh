- en: 'ChatGLM-6B: A Lightweight, Open-Source ChatGPT Alternative'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ChatGLM-6B：轻量级开源ChatGPT替代品
- en: 原文：[https://www.kdnuggets.com/2023/04/chatglm6b-lightweight-opensource-chatgpt-alternative.html](https://www.kdnuggets.com/2023/04/chatglm6b-lightweight-opensource-chatgpt-alternative.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/04/chatglm6b-lightweight-opensource-chatgpt-alternative.html](https://www.kdnuggets.com/2023/04/chatglm6b-lightweight-opensource-chatgpt-alternative.html)
- en: '![ChatGLM-6B: A Lightweight, Open-Source ChatGPT Alternative](../Images/9dfdd021af6594d2e14b5f24f29d0ddb.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![ChatGLM-6B: 轻量级开源ChatGPT替代品](../Images/9dfdd021af6594d2e14b5f24f29d0ddb.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Recently we’ve all been having a super hard time catching up on the latest releases
    in the LLM space. In the last few weeks, several open-source ChatGPT alternatives
    have become popular.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，我们都很难跟上LLM领域的最新发布。在过去几周里，几个开源ChatGPT替代品变得非常受欢迎。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: And in this article we’ll learn about the **ChatGLM** series and **ChatGLM-6B**,
    an open-source and lightweight ChatGPT alternative.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将了解**ChatGLM**系列和**ChatGLM-6B**，一个开源且轻量级的ChatGPT替代品。
- en: Let's get going!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: What is ChatGLM?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是ChatGLM？
- en: Researchers at the Tsinghua University in China have worked on developing the
    ChatGLM series of models that have comparable performance to other models such
    as GPT-3 and BLOOM.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 中国清华大学的研究人员开发了ChatGLM系列模型，这些模型的性能与GPT-3和BLOOM等其他模型相当。
- en: 'ChatGLM is a bilingual large language model trained on both Chinese and English.
    Currently, the following models are available:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGLM 是一个双语大型语言模型，训练时涵盖了中文和英文。目前，以下模型已提供：
- en: 'ChatGLM-130B: an open-source LLM'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ChatGLM-130B：一个开源的LLM
- en: 'ChatGLM-100B: not open-sourced, but available through invite-only access'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ChatGLM-100B：未开源，但通过邀请制访问
- en: 'ChatGLM-6B: a lightweight open-source alternative'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ChatGLM-6B：一个轻量级的开源替代品
- en: Though these models may seem similar to the Generative Pretrained Transformer
    (GPT) group of large language models, the **General Language Model (GLM) pretraining
    framework** is what makes them different. We’ll learn more about this in the next
    section.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些模型可能与生成预训练变换器（GPT）系列的大型语言模型相似，但**通用语言模型（GLM）预训练框架**使它们有所不同。我们将在下一节中详细了解这一点。
- en: How Does ChatGLM Work?
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ChatGLM是如何工作的？
- en: In machine learning, you'd know GLMs as *generalized linear models*, but the
    GLM in ChatGLM stands for *General Language Model*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，你可能会知道GLM作为*广义线性模型*，但ChatGLM中的GLM代表*通用语言模型*。
- en: GLM Pretraining Framework
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GLM预训练框架
- en: LLM pre training has been extensively studied and is still an area of active
    research. Let’s try to understand the key differences between GLM pretraining
    and GPT-style models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: LLM预训练已经被广泛研究，并且仍然是一个活跃的研究领域。让我们试着理解GLM预训练和GPT风格模型之间的关键区别。
- en: The GPT-3 family of models use decoder-only auto regressive language modeling.
    In GLM, on the other hand, optimization of the objective is formulated as an **auto
    regressive blank infilling problem**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3系列模型使用的是仅解码器的自回归语言建模。另一方面，在GLM中，目标的优化被制定为**自回归空白填充问题**。
- en: '![ChatGLM-6B: A Lightweight, Open-Source ChatGPT Alternative](../Images/23df7f00c157d578290e442e5cae1240.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![ChatGLM-6B: 轻量级开源ChatGPT替代品](../Images/23df7f00c157d578290e442e5cae1240.png)'
- en: GLM | [Image Source](https://arxiv.org/abs/2103.10360)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: GLM | [图片来源](https://arxiv.org/abs/2103.10360)
- en: In simple terms, *auto regressive blank infilling* involves blanking out a continuous
    span of text, and then sequentially reconstructing the text this blanking. In
    addition to shorter masks, there is a longer mask that randomly removes long blanks
    of text from the end of sentences. This is done so that the model performs reasonably
    well in natural language understanding as well as generation tasks.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，*自回归空白填充*涉及将连续的文本段落留空，然后顺序地重建这些空白的文本。除了较短的掩码外，还有一个较长的掩码，它随机移除句子末尾的长文本空白。这样做的目的是让模型在自然语言理解和生成任务中表现合理。
- en: Another difference is in the type of attention used. The GPT group of large
    language models use unidirectional attention, whereas the GLM group of LLMs use
    **bidirectional attention**. Using bidirectional attention over unmasked contexts
    can capture dependencies better and can improve performance on natural language
    understanding tasks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个区别在于所使用的注意力类型。GPT 大型语言模型组使用单向注意力，而 GLM 大型语言模型组使用 **双向注意力**。使用双向注意力可以更好地捕捉依赖关系，并能提高自然语言理解任务的性能。
- en: GELU Activation
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GELU 激活
- en: In GLM, GELU (Gaussian Error Linear Units) activation is used instead of the
    ReLU activation [1].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GLM 中，使用 GELU（高斯误差线性单元）激活代替了 ReLU 激活 [1]。
- en: '![ChatGLM-6B: A Lightweight, Open-Source ChatGPT Alternative](../Images/9f6d2d8afebf440a8d9f0363b8a4db6d.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![ChatGLM-6B: 一个轻量级的开源 ChatGPT 替代品](../Images/9f6d2d8afebf440a8d9f0363b8a4db6d.png)'
- en: GELU, ReLU, and ELU Activations | [Image Source](https://arxiv.org/abs/1606.08415)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: GELU、ReLU 和 ELU 激活 | [图像来源](https://arxiv.org/abs/1606.08415)
- en: 'The GELU activation and has non-zero values for all inputs and has the following
    form [3]:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: GELU 激活对所有输入有非零值，形式如下 [3]：
- en: '![ChatGLM-6B: A Lightweight, Open-Source ChatGPT Alternative](../Images/c42f168b0c87a3a5c1a921182a2a019d.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![ChatGLM-6B: 一个轻量级的开源 ChatGPT 替代品](../Images/c42f168b0c87a3a5c1a921182a2a019d.png)'
- en: The GELU activation is found to improve performance in as compared to ReLU activations,
    though computationally more intensive than ReLU.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 与 ReLU 激活相比，GELU 激活被发现可以提高性能，尽管计算上比 ReLU 更为复杂。
- en: In the GLM series of LLMs, ChatGLM-130B which is open-source and performs as
    well as GPT-3’s Da-Vinci model. As mentioned, as of writing this article, there's
    a ChatGLM-100B version, which is restricted to invite-only access.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GLM 系列的 LLM 中，ChatGLM-130B 是一个开源模型，表现与 GPT-3 的 Da-Vinci 模型相当。如前所述，截至本文撰写时，存在一个
    ChatGLM-100B 版本，仅限邀请访问。
- en: ChatGLM-6B
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChatGLM-6B
- en: 'The following details about ChatGLM-6B to make it more accessible to end users:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 ChatGLM-6B 的以下细节使其对终端用户更为友好：
- en: Has about 6.2 billion parameters.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 约有 62 亿个参数。
- en: The model is pre-trained on 1 trillion tokens—equally from English and Chinese.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型在1万亿个标记上进行预训练——英文和中文各占一半。
- en: Subsequently, techniques such as supervised fine-tuning and reinforcement learning
    with human feedback are used.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随后，使用了如监督微调和带有人类反馈的强化学习等技术。
- en: Advantages and Limitations of ChatGLM
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ChatGLM 的优点和局限性
- en: 'Let’s wrap up our discussion by going over ChatGLM’s advantages and limitations:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回顾 ChatGLM 的优点和局限性来总结讨论：
- en: Advantages
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优点
- en: 'From being a bilingual model to an open-source model that you can run locally,
    ChatGLM-6B has the following advantages:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个双语模型到一个可以在本地运行的开源模型，ChatGLM-6B 具有以下优势：
- en: Most mainstream large language models are trained on large corpora of English
    text, and large language models for other languages are not as common. The ChatGLM
    series of LLMs are bilingual and a great choice for Chinese. The model has good
    performance in both English and Chinese.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数主流大型语言模型都在大量英文文本上进行训练，而其他语言的大型语言模型则不那么常见。ChatGLM 系列 LLM 是双语的，是中文的绝佳选择。该模型在英文和中文方面都表现良好。
- en: ChatGLM-6B is optimized for user devices. End users often have limited computing
    resources on their devices, so it becomes almost impossible to run LLMs locally—without
    access to high-performance GPUs. With [INT4 quantization](https://developer.nvidia.com/blog/int4-for-ai-inference/),
    ChatGLM-6B can run with a modest memory requirement of as low as 6GB.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ChatGLM-6B 为用户设备进行了优化。终端用户的设备通常计算资源有限，因此在没有高性能 GPU 的情况下，几乎无法在本地运行 LLM。通过 [INT4
    量化](https://developer.nvidia.com/blog/int4-for-ai-inference/)，ChatGLM-6B 可以以低至
    6GB 的内存要求运行。
- en: Performs well on a variety of tasks including summarization and single and multi-query
    chats.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在总结和单一及多查询聊天等各种任务上表现良好。
- en: Despite the substantially smaller number of parameters as compared to other
    mainstream LLMs, ChatGLM-6B supports context length of up to 2048.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管与其他主流 LLMs 相比参数数量大幅减少，ChatGLM-6B 支持最长为 2048 的上下文长度。
- en: Limitations
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: 'Next, let’s list a few limitations of ChatGLM-6B:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们列出 ChatGLM-6B 的一些限制：
- en: Though ChatGLM is a bilingual model, its performance in English is likely suboptimal.
    This can be attributed to the instructions used in training mostly being in Chinese.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管 ChatGLM 是一个双语模型，但其在英语中的表现可能不尽如人意。这可以归因于训练中使用的指令主要为中文。
- en: Because ChatGLM-6B has substantially *fewer parameters* as compared to other
    LLMs such as BLOOM, GPT-3, and ChatGLM-130B, the performance may be worse when
    the context is too long. As a result, ChatGLM-6B may give inaccurate information
    more often than models with a larger number of parameters.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 ChatGLM-6B 相比 BLOOM、GPT-3 和 ChatGLM-130B 等其他 LLMs 具有**更少的参数**，因此在上下文过长时，其性能可能会较差。因此，ChatGLM-6B
    可能比参数更多的模型更频繁地给出不准确的信息。
- en: Small language models have *limited memory capacity*. Therefore, in multi-turn
    chats, the performance of the model may degrade slightly.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小型语言模型具有*有限的内存容量*。因此，在多轮对话中，模型的性能可能会略有下降。
- en: Bias, misinformation, and toxicity are limitations of all LLMs, and ChatGLM
    is susceptible to these, too.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏见、虚假信息和毒性是所有 LLM 的限制，ChatGLM 也容易受到这些问题的影响。
- en: Conclusion
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: As a next step, run ChatGLM-6B locally or try out the demo on HuggingFace spaces.
    If you’d like to delve deeper into the working of LLMs, here's a list of [free
    courses on large language models](/2023/03/top-free-courses-large-language-models.html).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 作为下一步，运行本地的 ChatGLM-6B 或尝试 HuggingFace spaces 上的演示。如果你想要深入了解 LLM 的工作原理，这里有一份
    [大型语言模型的免费课程列表](/2023/03/top-free-courses-large-language-models.html)。
- en: References
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Z Du, Y Qian et al., [GLM: General Language Model Pretraining with Autoregressive
    Blank Infilling](https://arxiv.org/abs/2103.10360), ACL 2022'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Z Du, Y Qian 等，[GLM: 一种自回归空白填充的通用语言模型预训练](https://arxiv.org/abs/2103.10360)，ACL
    2022'
- en: '[2] A Zheng, X Liu et al., [GLM-130B - An Open Bilingual Pretrained Model](https://arxiv.org/abs/2210.02414),
    ICML 2023'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] A Zheng, X Liu 等，[GLM-130B - 一个开放的双语预训练模型](https://arxiv.org/abs/2210.02414)，ICML
    2023'
- en: '[3] D Hendryks, K Gimpel, [Gaussian Error Linear Units (GELUs)](https://arxiv.org/abs/1606.08415),
    arXiv, 2016'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] D Hendryks, K Gimpel，[高斯误差线性单元（GELUs）](https://arxiv.org/abs/1606.08415)，arXiv，2016'
- en: '[4] [ChatGLM-6B: Demo on HuggingFace Spaces](https://huggingface.co/spaces/multimodalart/ChatGLM-6B)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [ChatGLM-6B: HuggingFace Spaces 上的演示](https://huggingface.co/spaces/multimodalart/ChatGLM-6B)'
- en: '[5] [GitHub Repo](https://github.com/THUDM/ChatGLM-6B/blob/main/README_en.md)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [GitHub 仓库](https://github.com/THUDM/ChatGLM-6B/blob/main/README_en.md)'
- en: '**[Bala Priya C](https://www.linkedin.com/in/bala-priya/)** is a technical
    writer who enjoys creating long-form content. Her areas of interest include math,
    programming, and data science. She shares her learning with the developer community
    by authoring tutorials, how-to guides, and more.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Bala Priya C](https://www.linkedin.com/in/bala-priya/)** 是一位技术作家，喜欢创建长篇内容。她的兴趣领域包括数学、编程和数据科学。她通过编写教程、操作指南等方式与开发者社区分享她的学习经验。'
- en: More On This Topic
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[MiniGPT-4: A Lightweight Alternative to GPT-4 for Enhanced…](https://www.kdnuggets.com/2023/04/minigpt4-lightweight-alternative-gpt4-enhanced-visionlanguage-understanding.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MiniGPT-4: 一种轻量级的 GPT-4 替代品，用于增强…](https://www.kdnuggets.com/2023/04/minigpt4-lightweight-alternative-gpt4-enhanced-visionlanguage-understanding.html)'
- en: '[OpenChatKit: Open-Source ChatGPT Alternative](https://www.kdnuggets.com/2023/03/openchatkit-opensource-chatgpt-alternative.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenChatKit: 开源的 ChatGPT 替代品](https://www.kdnuggets.com/2023/03/openchatkit-opensource-chatgpt-alternative.html)'
- en: '[8 Open-Source Alternative to ChatGPT and Bard](https://www.kdnuggets.com/2023/04/8-opensource-alternative-chatgpt-bard.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8 个 ChatGPT 和 Bard 的开源替代品](https://www.kdnuggets.com/2023/04/8-opensource-alternative-chatgpt-bard.html)'
- en: '[Dolly 2.0: ChatGPT Open Source Alternative for Commercial Use](https://www.kdnuggets.com/2023/04/dolly-20-chatgpt-open-source-alternative-commercial.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Dolly 2.0: 用于商业用途的 ChatGPT 开源替代品](https://www.kdnuggets.com/2023/04/dolly-20-chatgpt-open-source-alternative-commercial.html)'
- en: '[Alternative Feature Selection Methods in Machine Learning](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中的替代特征选择方法](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
- en: '[HuggingChat Python API: Your No-Cost Alternative](https://www.kdnuggets.com/2023/05/huggingchat-python-api-alternative.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HuggingChat Python API: 你的免费替代品](https://www.kdnuggets.com/2023/05/huggingchat-python-api-alternative.html)'
