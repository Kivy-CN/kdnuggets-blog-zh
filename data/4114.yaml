- en: Easy Guide To Data Preprocessing In Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python数据预处理简易指南
- en: 原文：[https://www.kdnuggets.com/2020/07/easy-guide-data-preprocessing-python.html](https://www.kdnuggets.com/2020/07/easy-guide-data-preprocessing-python.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/07/easy-guide-data-preprocessing-python.html](https://www.kdnuggets.com/2020/07/easy-guide-data-preprocessing-python.html)
- en: '![Easy Guide To Data Preprocessing In Python](../Images/64a33dd50d43223ee1daee66f467fb1a.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Python数据预处理简易指南](../Images/64a33dd50d43223ee1daee66f467fb1a.png)'
- en: '[Image by rawpixel.com](https://www.freepik.com/free-photo/businesswoman-networking-using-digital-devices_15440982.htm#query=data&from_query=Data%20Preprocessing&position=0&from_view=search&track=sph)
    on Freepik'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[图片来源于rawpixel.com](https://www.freepik.com/free-photo/businesswoman-networking-using-digital-devices_15440982.htm#query=data&from_query=Data%20Preprocessing&position=0&from_view=search&track=sph)
    在Freepik上'
- en: Machine Learning is 80% preprocessing and 20% model making.
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 机器学习中80%是数据预处理，20%是模型制作。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: You must have heard this phrase if you have ever encountered a senior Kaggle
    data scientist or machine learning engineer. The fact is that this is a true phrase.
    In a real-world data science project, data preprocessing is one of the most important
    things, and it is one of the common factors of success of a model, i.e., if there
    is correct data preprocessing and feature engineering, that model is more likely
    to produce noticeably better results as compared to a model for which data is
    not well preprocessed.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾遇到过高级Kaggle数据科学家或机器学习工程师，你一定听过这个说法。事实上，这句话是正确的。在真实的数据科学项目中，数据预处理是最重要的工作之一，它也是模型成功的共同因素之一。也就是说，如果数据预处理和特征工程做得正确，该模型更可能产生显著更好的结果，而相比之下，数据没有经过良好预处理的模型效果会较差。
- en: Important Steps
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重要步骤
- en: There are 4 main important steps for the preprocessing of data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理有4个主要的关键步骤。
- en: Splitting of the data set in Training and Validation sets
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的训练和验证集拆分
- en: Taking care of Missing values
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: Taking care of Categorical Features
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理分类特征
- en: Normalization of data set
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的归一化
- en: Let’s have a look at all of these points.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这些要点。
- en: 1\. Train Test Split
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 训练测试拆分
- en: Train Test Split is one of the important steps in Machine Learning. It is very
    important because your model needs to be evaluated before it has been deployed.
    And that evaluation needs to be done on unseen data because when it is deployed,
    all incoming data is unseen.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 训练测试拆分是机器学习中一个重要的步骤。这一点非常重要，因为你的模型需要在部署之前进行评估。而这种评估需要在未见过的数据上进行，因为一旦部署，所有接收到的数据都是未见过的。
- en: The main idea behind the train test split is to convert original data set into
    2 parts
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 训练测试拆分的主要思想是将原始数据集转换为两个部分
- en: train
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: train
- en: test
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: test
- en: where train consists of training data and training labels and test consists
    of testing data and testing labels.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 其中训练集包括训练数据和训练标签，而测试集包括测试数据和测试标签。
- en: The easiest way to do it is by using *scikit-learn*, which has a built-in function *train_test_split*.
    Let’s code it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是使用*scikit-learn*，它有一个内置函数*train_test_split*。让我们编写代码。
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here we have passed-in *X* and *y* as arguments in *train_test_split*, which
    splits *X* and *y* such that there is 20% testing data and 80% training data successfully
    split between *X_train*, *X_test*, *y_train*, and *y_test*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们传入了*X*和*y*作为*train_test_split*中的参数，这样会将*X*和*y*拆分为20%的测试数据和80%的训练数据，并成功分配到*X_train*、*X_test*、*y_train*和*y_test*中。
- en: 2\. Taking Care of Missing Values
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 处理缺失值
- en: There is a famous Machine Learning phrase which you might have heard that is
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个著名的机器学习说法，你可能听过，那就是
- en: Garbage in Garbage out
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 垃圾进，垃圾出
- en: If your data set is full of NaNs and garbage values, then surely your model
    will perform garbage too. So taking care of such missing values is important.
    Let’s take a dummy data set to see how we can tackle this problem of taking care
    of garbage values. You can get that data set [here](https://docs.google.com/spreadsheets/d/1I6IQUtp_x4ZAjY8q3VdiTiTrd-w0p8zGvCn-FD_Ae70/edit?usp=sharing).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据集中充满了NaN和垃圾值，那么你的模型表现也肯定会很差。因此，处理这些缺失值非常重要。我们可以用一个示例数据集来看一下如何解决这个垃圾值问题。你可以在[这里](https://docs.google.com/spreadsheets/d/1I6IQUtp_x4ZAjY8q3VdiTiTrd-w0p8zGvCn-FD_Ae70/edit?usp=sharing)获取该数据集。
- en: Let’s see the missing values in the data set.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看数据集中的缺失值。
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Easy Guide To Data Preprocessing In Python](../Images/1a590bb31504134461b63200306a7d4d.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![Python数据预处理简易指南](../Images/1a590bb31504134461b63200306a7d4d.png)'
- en: Here we can see that we have 2 missing values in 4 columns. One approach to
    fill in missing values is to fill it with the mean of that column, which is the
    average of that column. For example, we can fill in the missing value of *Final *column
    by an average of all students in that column.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到4列中有2个缺失值。一种填补缺失值的方法是用该列的均值来填补，即该列的平均值。例如，我们可以用*Final*列中所有学生的平均值来填补缺失值。
- en: To do that, we can use **SimpleImputer **from **sklearn.impute**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们可以使用**SimpleImputer**来自**sklearn.impute**。
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This will fill all the missing values in the data frame *df* using *mean *of
    that column. We use the *fit_transform *function to do that.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用*mean*填补数据框*df*中的所有缺失值。我们使用*fit_transform*函数来做到这一点。
- en: Because it returns a numpy array, to read it, we can convert it back to the
    data frame.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它返回的是一个numpy数组，我们可以将其转换回数据框以读取。
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Easy Guide To Data Preprocessing In Python](../Images/c34f2223e45f8c13343ca5e2caf20585.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![Python数据预处理简易指南](../Images/c34f2223e45f8c13343ca5e2caf20585.png)'
- en: And now, we can see that we have filled all the missing values by mean of all
    values.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到我们已经用所有值的均值填补了所有缺失值。
- en: We can confirm it by
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方法确认
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: and the output is
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为
- en: '![Easy Guide To Data Preprocessing In Python](../Images/c64fe3b01f2d39a63e492093560d6e16.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![Python数据预处理简易指南](../Images/c64fe3b01f2d39a63e492093560d6e16.png)'
- en: We can use *mean*, *meadian*, *mode *etc. in **SimpleImputer**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在**SimpleImputer**中使用*mean*、*median*、*mode*等。
- en: If our number of rows which have missing values are less, or our data is such
    that it is not advised to fill in missing values, then we can drop the missing
    rows by using *dropna *in pandas.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有缺失值的行数较少，或者我们的数据不建议填补缺失值，那么我们可以使用*pandas*中的*dropna*来删除缺失行。
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: And here, we have dropped all the null rows in the dataframe and stored it in
    another dataframe.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们删除了数据框中的所有空行，并将其存储到另一个数据框中。
- en: Now we have 0 null rows as we have dropped them. We can confirm it as
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经删除了所有的空行，所以行数为0。我们可以确认这一点为
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Easy Guide To Data Preprocessing In Python](../Images/c64fe3b01f2d39a63e492093560d6e16.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![Python数据预处理简易指南](../Images/c64fe3b01f2d39a63e492093560d6e16.png)'
- en: 3\. Taking care of Categorical Features
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 处理分类特征
- en: We can take care of categorical features by converting them to integers. There
    are 2 common ways to do so.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将分类特征转换为整数来处理它们。有两种常见的方法来实现。
- en: Label Encoding
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签编码
- en: One Hot Encoding
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一键编码
- en: In **Label Encoder**, we can convert the Categorical values into numerical labels.
    Let’s say this is our dataset
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在**标签编码器**中，我们可以将分类值转换为数值标签。假设这是我们的数据集
- en: '![Easy Guide To Data Preprocessing In Python](../Images/6e9d07286ec00c70b75f56ae0e51760a.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![Python数据预处理简易指南](../Images/6e9d07286ec00c70b75f56ae0e51760a.png)'
- en: and using label encoder on the Country column will convert India to 1, the USA
    to 2, and China to 0\. This technique has a drawback that it gives the USA the
    highest priority due to its label is high and China the lowest priority for its
    label being 0, but still, it is helping a lot of times.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标签编码器处理Country列将把印度转换为1，美国转换为2，中国转换为0。这种技术有一个缺点，它由于标签值高而将美国优先级最高，将中国标签为0，因此中国优先级最低，但它在很多情况下仍然非常有用。
- en: Let’s code it.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始编码吧。
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Easy Guide To Data Preprocessing In Python](../Images/6f5f83dadb595d1cb4eadd6fc085ff9a.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![Python数据预处理简易指南](../Images/6f5f83dadb595d1cb4eadd6fc085ff9a.png)'
- en: Output after Label Encoder
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 标签编码器后的输出
- en: Here we have instantiated a LabelEncoder object, then used the *fit *method
    to fit it on our categorical column and then used *transform *method to apply
    it.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们实例化了一个LabelEncoder对象，然后使用*fit*方法将其拟合到我们的分类列上，接着使用*transform*方法进行应用。
- en: Note that it is not *inplace *so in order to make the change permanent, we have
    to return the value to our categorical column, i.e.,
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，它不是 *inplace* 的，因此为了使更改永久生效，我们必须将值返回到我们的分类列中，即，
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In **OneHotEncoder **we make a new column for each unique categorical value,
    and the value is 1 for that column, if in an actual data frame that value is there,
    else it is 0.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **OneHotEncoder** 中，我们为每个唯一的分类值创建一列，如果实际数据框中存在该值，则该列的值为 1，否则为 0。
- en: Let’s See it with the same example but a bit modified. We will add another categorical
    column, which is “Continent,” that has the name of the continent of the respective
    country. We can do it by
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下相同的示例，但稍作修改。我们将添加另一列分类数据，即“Continent”，它包含了相应国家的大陆名称。我们可以通过
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Easy Guide To Data Preprocessing In Python](../Images/162109627fa175aea9a13e094bc4f873.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![Python 数据预处理简易指南](../Images/162109627fa175aea9a13e094bc4f873.png)'
- en: Now because we have 2 categorical columns, which are *[['Country', 'Continent']]*,
    we can one hot encode them.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在由于我们有 2 个分类列，即 *[['Country', 'Continent']] *，我们可以对它们进行 one-hot 编码。
- en: There are 2 ways to do so.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以做到这一点。
- en: 1\. DataFrame.get_dummies
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. DataFrame.get_dummies
- en: This is a pretty common way where we use pandas built-in function *get_dummies*to
    convert categorical values in a dataframe to a one-hot vector.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种相当常见的方法，我们使用 pandas 内置的 *get_dummies* 函数将数据框中的分类值转换为 one-hot 向量。
- en: Let’s do this.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This will **return **a data frame with all the categorical values encoded in
    a one-hot vector format.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这将 **返回** 一个数据框，所有的分类值都以 one-hot 向量格式编码。
- en: '![Easy Guide To Data Preprocessing In Python](../Images/46967468158961459b8e3fb6223f4369.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![Python 数据预处理简易指南](../Images/46967468158961459b8e3fb6223f4369.png)'
- en: Here we can see that it has converted the unique values of Country columns as
    3 different columns, which are Country_China, Country_India, and Country_USA.
    Similarly, 2 unique values of Continent Column has been converted into 2 different
    columns named as Continent_Asia and Continent_North America.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们可以看到，它将 Country 列的唯一值转换为 3 列，分别是 Country_China、Country_India 和 Country_USA。类似地，Continent
    列的 2 个唯一值已被转换为 2 列，分别命名为 Continent_Asia 和 Continent_North America。
- en: Because it is not in place, we have to either store it in a data frame, i.e.,
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它不是就地操作的，我们必须将其存储在数据框中，即，
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 2\. OneHotEncoder
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. OneHotEncoder
- en: Using OneHotEncoder from Sci-Kit Learning is also a common practice. It does
    provide more flexibility and more options but is a bit difficult to use. Let’s
    see how we can do it for our dataset.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Sci-Kit Learning 的 OneHotEncoder 也是一种常见的做法。它提供了更多的灵活性和选项，但使用起来有点复杂。让我们看看如何在我们的数据集上实现它。
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, we have initialized OneHotEncoder Object and used its *fit_transform* method
    on our desired columns (column number 0 and column number 3) in the data frame.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们初始化了 OneHotEncoder 对象，并在数据框中对我们所需的列（列号 0 和列号 3）使用了它的 *fit_transform* 方法。
- en: The return type of *fit_transform* is *numpy.ndarray*, so we convert it into
    a dataframe by *pd.DataFrame* and stored it in a variable. Then, to join it in
    our original data frame, we can use *pd.concat* the function that concatenates
    2 different data frames. We have used* axis=1*, which means it has to join on
    the basis of columns instead of rows.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*fit_transform* 的返回类型是 *numpy.ndarray*，因此我们将其转换为数据框 *pd.DataFrame* 并存储在一个变量中。然后，为了将其加入到原始数据框中，我们可以使用
    *pd.concat* 函数，它用于连接两个不同的数据框。我们使用了 *axis=1*，这意味着它将基于列而不是行进行连接。'
- en: Also, remember that *pd.concat* is not *inplace*, so we have to store the returning
    dataframe somewhere.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请记住 *pd.concat* 不是 *inplace* 的，因此我们必须将返回的数据框存储在某个地方。
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The resulting dataframe is
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的 dataframe 是
- en: '![Easy Guide To Data Preprocessing In Python](../Images/293e24679c8b0ed3deebc55a1f950abf.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![Python 数据预处理简易指南](../Images/293e24679c8b0ed3deebc55a1f950abf.png)'
- en: OneHotEncoded DataFrame
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: OneHotEncoded DataFrame
- en: You can see that it is not clearly readable as compared to *pd.get_dummies*,
    but if you compare the last 5 columns that we got using *pd.get_dummies* and *OneHotEncoder*,
    then they are all equal.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，与 *pd.get_dummies* 相比，它的可读性不那么清晰，但如果你比较最后 5 列，我们使用 *pd.get_dummies* 和
    *OneHotEncoder* 得到的结果，它们是相等的。
- en: Of course, you can modify the column names as per your choice in *OneHotEncoder*,
    which you can learn as per [this](https://stackoverflow.com/questions/54570947/feature-names-from-onehotencoder) question
    from [StackOverflow](http://www.stackoverflow.com/).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以根据自己的需要在 *OneHotEncoder* 中修改列名，你可以通过 [这个](https://stackoverflow.com/questions/54570947/feature-names-from-onehotencoder)
    问题在 [StackOverflow](http://www.stackoverflow.com/) 上学习如何操作。
- en: 4\. Normalizing the Dataset
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 数据集归一化
- en: This brings us to the last part of data preprocessing, which is the normalization
    of the dataset. It is proven from certain experimentation that Machine Learning
    and Deep Learning Models perform way better on a normalized data set as compared
    to a data set that is not normalized.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这将我们带到了数据预处理的最后部分，即数据集的标准化。通过某些实验已证明，机器学习和深度学习模型在标准化数据集上的表现远远优于未标准化的数据集。
- en: The goal of normalization is to change values to a common scale without distorting
    the difference between the range of values.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化的目标是将值更改为统一的尺度，而不扭曲值范围之间的差异。
- en: There are several ways to do so. I will discuss 2 common ways to normalize a
    dataset.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以实现。我将讨论两种常见的标准化数据集的方法。
- en: Standard Scaler
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准化器
- en: '![Easy Guide To Data Preprocessing In Python](../Images/d03ad89f7be4208a47dc9a961bd6e94c.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![Python数据预处理简易指南](../Images/d03ad89f7be4208a47dc9a961bd6e94c.png)'
- en: Credit: [https://stackoverflow.com/a/50879522/10342778](https://stackoverflow.com/a/50879522/10342778)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '参考: [https://stackoverflow.com/a/50879522/10342778](https://stackoverflow.com/a/50879522/10342778)'
- en: Using this technique, we are going to have a mean of 0 and a standard deviation
    of 1 in our dataset. We can either do it normally by combining different functions
    in numpy, i.e.,
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种技术，我们的数据集将具有均值0和标准差1。我们可以通过结合numpy中的不同函数来实现这一点，即，
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: where *x* is a dataframe with all numerical indices. If we want to retain the
    values in a dataframe, then we can simply remove *.values* in front of it.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *x* 是一个包含所有数值索引的数据框。如果我们希望保留数据框中的值，那么我们可以简单地去掉 *.values*。
- en: '**Variance before StandardScaler**'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**StandardScaler之前的方差**'
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![Easy Guide To Data Preprocessing In Python](../Images/c7c2be84a25b48be8d67714b743f6ea2.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![Python数据预处理简易指南](../Images/c7c2be84a25b48be8d67714b743f6ea2.png)'
- en: var before StandardScaler.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: StandardScaler之前的方差。
- en: Here, I have used *ddof=0*, which is by default 1 in *pandas.DataFrame.var()* and
    by default 0 in *numpy.ndarray.var()*. Ddof means Delta Degrees of Freedom, which
    is the divisor used in calculations is *N - ddof*, where *N* represents the number
    of elements.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我使用了*ddof=0*，默认情况下在*pandas.DataFrame.var()*中为1，而在*numpy.ndarray.var()*中为0。Ddof表示自由度的增量，计算中使用的除数是*N
    - ddof*，其中*N*表示元素的数量。
- en: '*ddof=0* provides a maximum likelihood estimate of the variance for normally
    distributed variables.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*ddof=0* 为正态分布变量提供了方差的最大似然估计。'
- en: You can read more about *ddof=0* [here](https://stackoverflow.com/questions/62938495/difference-between-numpy-var-and-pandas-var).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[这里](https://stackoverflow.com/questions/62938495/difference-between-numpy-var-and-pandas-var)阅读更多关于*ddof=0*的内容。
- en: Variance after StandardScaler
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StandardScaler后的方差
- en: Another good way to do so by using **StandardScaler **from *sklearn.preprocessing*.
    Let’s see the code, and then see the variance.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种好的方法是使用来自*sklearn.preprocessing*的**StandardScaler**。让我们先看代码，然后再查看方差。
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Easy Guide To Data Preprocessing In Python](../Images/aaf7b5e994169b9e45d36b3bf01c92b0.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![Python数据预处理简易指南](../Images/aaf7b5e994169b9e45d36b3bf01c92b0.png)'
- en: catDf after StandardScaler.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: StandardScaler后的catDf。
- en: Here we have applied *StandardScaler *on all the numerical columns (column number
    1 to the last column (not included)), and now you can see the values of *GDP *and *Area*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们对所有数值列（从第1列到最后一列（不包括））应用了*StandardScaler*，现在你可以看到*GDP*和*Area*的值。
- en: Now we can check the *variance *of the dataset by
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过 *方差* 来检查数据集的变异性
- en: '[PRE17]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Easy Guide To Data Preprocessing In Python](../Images/22b94d91b7ae62c5c3accb73b7653e5f.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![Python数据预处理简易指南](../Images/22b94d91b7ae62c5c3accb73b7653e5f.png)'
- en: var after StandardScaler
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: StandardScaler后的方差
- en: And we can see the massive reduction of the variance of 80 and 13 to 1\. In
    real-world datasets, normally the improvement is from thousands to 1.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到方差从80和13大幅降低到1。在现实世界的数据集中，通常改善从几千降到1。
- en: Normalization
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准化
- en: According to the [official documentation of sklearn](https://scikit-learn.org/stable/modules/preprocessing.html#normalization),
    normalization is “the process of **scaling individual samples to have unit norm**.
    This process can be useful if you plan to use a quadratic form such as the dot-product
    or any other kernel to quantify the similarity of any pair of samples.”
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 根据*[sklearn官方文档](https://scikit-learn.org/stable/modules/preprocessing.html#normalization)*，标准化是“将个体样本缩放到单位范数的过程。如果你计划使用二次形式如点积或其他内核来量化任何样本对的相似性，这个过程可能会有用。”
- en: The process of using it is very simple and similar to StandaradScaler.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用它的过程非常简单，类似于StandardScaler。
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Easy Guide To Data Preprocessing In Python](../Images/9752dd2812578f78ed7274ec01979080.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![Python数据预处理简易指南](../Images/9752dd2812578f78ed7274ec01979080.png)'
- en: catDf after Normalizer
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用归一化器后的catDf
- en: There are several other ways to normalize the data, and all of them are useful
    in specific cases. You can read more about them in the official documentation [here](https://scikit-learn.org/stable/modules/preprocessing.html#normalization).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 还有几种其他的数据归一化方法，它们在特定情况下都非常有用。你可以在官方文档中了解更多信息，[点击这里](https://scikit-learn.org/stable/modules/preprocessing.html#normalization)。
- en: Learning Outcomes
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习成果
- en: Splitting the Dataset
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 划分数据集
- en: Filling in Missing values
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充缺失值
- en: Dealing with Categorical Data
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理分类数据
- en: Normalization of Dataset for improved results
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的归一化以提高结果
- en: Hopefully, all these techniques will improve your general skills as a data scientist
    or machine learning engineer and will improve your Machine Learning Models.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这些技巧能提升你作为数据科学家或机器学习工程师的一般技能，并改进你的机器学习模型。
- en: '**[Ahmad Anis](https://twitter.com/AhmadMustafaAn1)** is interested in Machine
    Learning, Deep Learning, and Computer Vision. Currently working as a Jr. Machine
    Learning engineer at Redbuffer.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Ahmad Anis](https://twitter.com/AhmadMustafaAn1)** 对机器学习、深度学习和计算机视觉感兴趣。目前在Redbuffer担任初级机器学习工程师。'
- en: More On This Topic
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[An Easy Guide to Choose the Right Machine Learning Algorithm](https://www.kdnuggets.com/2020/05/guide-choose-right-machine-learning-algorithm.html)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[选择合适的机器学习算法的简易指南](https://www.kdnuggets.com/2020/05/guide-choose-right-machine-learning-algorithm.html)'
- en: '[OpenAI API for Beginners: Your Easy-to-Follow Starter Guide](https://www.kdnuggets.com/openai-api-for-beginners-your-easy-to-follow-starter-guide)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAI API初学者指南：简单易懂的入门指南](https://www.kdnuggets.com/openai-api-for-beginners-your-easy-to-follow-starter-guide)'
- en: '[Learn Data Cleaning and Preprocessing for Data Science with This Free eBook](https://www.kdnuggets.com/2023/08/learn-data-cleaning-preprocessing-data-science-free-ebook.html)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过这本免费电子书学习数据清理和预处理，助力数据科学](https://www.kdnuggets.com/2023/08/learn-data-cleaning-preprocessing-data-science-free-ebook.html)'
- en: '[7 Steps to Mastering Data Cleaning and Preprocessing Techniques](https://www.kdnuggets.com/2023/08/7-steps-mastering-data-cleaning-preprocessing-techniques.html)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握数据清理和预处理技术的7个步骤](https://www.kdnuggets.com/2023/08/7-steps-mastering-data-cleaning-preprocessing-techniques.html)'
- en: '[Harnessing ChatGPT for Automated Data Cleaning and Preprocessing](https://www.kdnuggets.com/2023/08/harnessing-chatgpt-automated-data-cleaning-preprocessing.html)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用ChatGPT进行自动数据清理和预处理](https://www.kdnuggets.com/2023/08/harnessing-chatgpt-automated-data-cleaning-preprocessing.html)'
- en: '[Cleaning and Preprocessing Text Data in Pandas for NLP Tasks](https://www.kdnuggets.com/cleaning-and-preprocessing-text-data-in-pandas-for-nlp-tasks)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在Pandas中清理和预处理文本数据，用于NLP任务](https://www.kdnuggets.com/cleaning-and-preprocessing-text-data-in-pandas-for-nlp-tasks)'
