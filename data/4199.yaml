- en: The Evolution of Tokenization – Byte Pair Encoding in NLP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词的演变 - NLP中的字节对编码
- en: 原文：[https://www.kdnuggets.com/2021/10/evolution-tokenization-byte-pair-encoding-nlp.html](https://www.kdnuggets.com/2021/10/evolution-tokenization-byte-pair-encoding-nlp.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/10/evolution-tokenization-byte-pair-encoding-nlp.html](https://www.kdnuggets.com/2021/10/evolution-tokenization-byte-pair-encoding-nlp.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Harshit Tyagi](https://www.linkedin.com/in/tyagiharshit/), Data Science
    Instructor | Mentor | YouTuber**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Harshit Tyagi](https://www.linkedin.com/in/tyagiharshit/), 数据科学讲师 | 导师
    | YouTuber**'
- en: '![Image](../Images/7d609b3bd5f5effc62c7f83b74dbf6a6.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7d609b3bd5f5effc62c7f83b74dbf6a6.png)'
- en: NLP may have been a little late to the AI epiphany but it is doing wonders with
    organisations like Google, OpenAI releasing state-of-the-art(SOTA) language models
    like BERT and GPT-2/3 respectively.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然NLP在AI的启示上可能有些晚，但它在Google、OpenAI等组织中表现卓越，推出了如BERT和GPT-2/3等最先进的（SOTA）语言模型。
- en: GitHub Copilot and OpenAI codex are among a few very popular applications that
    are in the news. As someone who has very limited exposure to NLP, I decided to
    take up NLP as an area of research and the next few blogs/videos will be me sharing
    what I learn after dissecting some important components of NLP.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub Copilot 和 OpenAI Codex 是当前新闻中的一些非常受欢迎的应用程序。作为一个对NLP了解有限的人，我决定将NLP作为研究领域，接下来的几篇博客/视频将是我分享在分析NLP的一些重要组件后所学到的内容。
- en: 'NLP systems have three main components that help machines understand natural
    language:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: NLP系统有三个主要组件，帮助机器理解自然语言：
- en: Tokenization
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分词
- en: Embedding
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嵌入
- en: Model architectures
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型架构
- en: Top Deep Learning models like BERT, GPT-2, or GPT-3 all share the same components
    but with different architectures that distinguish one model from another.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 顶级深度学习模型如BERT、GPT-2或GPT-3都共享相同的组件，但具有不同的架构，使每个模型有所区别。
- en: In this newsletter(and [notebook](https://colab.research.google.com/drive/1QLlQx_EjlZzBPsuj_ClrEDC0l8G-JuTn?usp=sharing)),
    we are going to focus on the basics of the first component of an NLP pipeline
    which is **tokenization**. An often overlooked concept but it is a field of research
    in itself. We have come so far ahead of the traditional NLTK tokenization process.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本期通讯中（以及 [笔记本](https://colab.research.google.com/drive/1QLlQx_EjlZzBPsuj_ClrEDC0l8G-JuTn?usp=sharing)），我们将专注于NLP管道中第一个组件的基础知识，即**分词**。这是一个经常被忽视的概念，但它本身就是一个研究领域。我们已经远远超越了传统的NLTK分词过程。
- en: Though we have SOTA algorithms for tokenization, it's always a good practice
    to understand the evolution trail and learning how have we reached here.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们有最先进的分词算法，但了解其演变过程并学习我们如何达到现在的状态始终是一个好习惯。
- en: 'So, here''s what we''ll cover:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们将覆盖以下内容：
- en: What is tokenization?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是分词？
- en: Why do we need a tokenizer?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们为什么需要分词器？
- en: Types of tokenization - Word, Character, and Subword.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词的类型 - 词级、字符级和子词级。
- en: Byte Pair Encoding Algorithm - a version of which is used by most NLP models
    these days.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字节对编码算法 - 现在大多数NLP模型都使用的一个版本。
- en: 'The next part of this tutorial will dive into more advanced(or enhanced version
    of BPE) algorithms:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程的下一部分将深入探讨更先进（或增强版BPE）的算法：
- en: '**Unigram Algorithm**'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Unigram算法**'
- en: '**WordPiece - BERT transformer**'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**WordPiece - BERT变压器**'
- en: '**SentencePiece - End-to-End tokenizer system**'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SentencePiece - 端到端分词器系统**'
- en: What is Tokenization?
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是分词？
- en: Tokenization is the process of representing raw text in smaller units called
    tokens. These tokens can then be mapped with numbers to further feed to an NLP
    model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是将原始文本表示为称为“tokens”的较小单位的过程。这些tokens可以被映射为数字，以便进一步输入到NLP模型中。
- en: 'Here''s an overly simplified example of what a tokenizer does:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个过于简化的分词器功能示例：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![](../Images/f7f26c06b602d353f1881e141d340e98.png)](https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fcaa2e479-181a-4703-afb6-9796d0f74d09_229x327.png)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/f7f26c06b602d353f1881e141d340e98.png)](https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fcaa2e479-181a-4703-afb6-9796d0f74d09_229x327.png)'
- en: Here, we have simply mapped every word in the text to a numerical index. Obviously,
    this is a very simple example and we have not considered grammar, punctuations,
    compound words(like test, test-ify, test-ing, etc.).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只是简单地将文本中的每个词映射到一个数字索引。显然，这是一个非常简单的示例，我们没有考虑语法、标点符号、复合词（如test, test-ify,
    test-ing等）。
- en: Thus, we need a more technical and accurate definition of tokenization. To take
    into account every punctuation and related word, we need to start working at the
    character level.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要一个更技术化且准确的令牌化定义。为了考虑每一个标点符号和相关词汇，我们需要从字符级别开始工作。
- en: There are multiple applications of tokenization. One of the use cases comes
    from compiler design where we need to parse computer programs to convert raw characters
    into keywords of a programming language.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌化有多种应用。其中一个用例来源于编译器设计，我们需要解析计算机程序，将原始字符转换为编程语言的关键字。
- en: '**In deep learning,** tokenization is the process of converting a sequence
    of characters into a sequence of tokens which further needs to be converted into
    a sequence of numerical vectors that can be processed by a neural network.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**在深度学习中，** 令牌化是将字符序列转换为令牌序列的过程，之后还需将其转换为可以由神经网络处理的数值向量序列。'
- en: Why do we need a Tokenizer?
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么我们需要令牌化器？
- en: The need for a tokenizer has protruded from the question "How can we make machines
    read?"
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对令牌化器的需求源于“我们如何让机器阅读？”这个问题。
- en: One of the common ways of processing textual data is by defining a set of rules
    in a dictionary and then looking up that fixed dictionary of rules. But this method
    can only go so far and we want machines to learn these rules from the text that
    is read by it.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 处理文本数据的常见方法之一是定义一个规则字典，然后查找那个固定的规则字典。但这种方法只能走到一定程度，我们希望机器从阅读的文本中学习这些规则。
- en: Now, machines don't know any language, nor do they understand sound or phonetics.
    They need to be taught from scratch and in such a way that they could read any
    language possible.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，机器不懂任何语言，也不理解声音或语音学。它们需要从头开始学习，以便能够阅读任何可能的语言。
- en: Quite a task, right?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这可是一项大任务，对吧？
- en: Humans learn a language by connecting sound to the meaning and then we learn
    to read and write in that language. Machines can't do that, so they need to be
    given the most basic units of text to start processing it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 人类通过将声音与意义联系起来来学习语言，然后我们学习用这种语言阅读和写作。机器无法做到这一点，因此它们需要从最基本的文本单元开始处理。
- en: That's where tokenization comes into play. Break down the text into smaller
    units called "tokens".
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是令牌化的作用。将文本分解成称为“令牌”的更小单元。
- en: And there are different ways of tokenizing text which is what we'll learn now.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 还有不同的文本令牌化方法，这就是我们现在要学习的内容。
- en: Tokenization policies - simple ways to tokenize
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 令牌化策略 - 简单的令牌化方法
- en: 'To make the deep learning model learn from the text, we need a two-step process:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让深度学习模型从文本中学习，我们需要一个两步过程：
- en: '[![](../Images/3a952105e1bfa4e42c255f0dd40324af.png)](https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fff7fafb7-a127-4e41-a050-cb02951f3112_1391x821.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/3a952105e1bfa4e42c255f0dd40324af.png)](https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fff7fafb7-a127-4e41-a050-cb02951f3112_1391x821.jpeg)'
- en: Tokenize - decide the algorithm to be used to generate the tokens.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 令牌化 - 决定使用什么算法来生成令牌。
- en: Encode the tokens to vectors
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将令牌编码为向量
- en: As the first step suggests, we need to decide how to convert text into small
    tokens. A simple and straight forward method that most of us would propose is
    the word-based tokens, splitting the text by space.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正如第一步所示，我们需要决定如何将文本转换为小的令牌。大多数人提出的简单直接的方法是基于词的令牌，通过空格分割文本。
- en: Problems with Word tokenizer
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 字词令牌化器的问题
- en: '**the risk of missing words in the training data:** with word tokens, your
    model won''t recognize the variants of words that were not part of the data on
    which the model was trained. So, if your model has seen `foot` and `ball` in the
    training data but the final text has `football`, the model won''t be able to recognize
    the word and it will be treated with `<UNK>` token. Similarly, punctuations pose
    another problem, `let` or `let''s` will need individual tokens and it is an inefficient
    solution. This will **require a huge vocabulary** to make sure you''ve every variant
    of the word. Even if you add a **lemmatizer** to solve this problem, you''re adding
    an extra step in your processing pipeline.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据中缺词的风险：** 使用词元时，你的模型无法识别训练数据中未包含的词的变体。因此，如果你的模型在训练数据中看到了`foot`和`ball`，但最终文本中有`football`，模型将无法识别该词，并将其视为`<UNK>`标记。类似地，标点符号也会带来问题，`let`或`let''s`需要单独的标记，这是一个低效的解决方案。这将**需要一个庞大的词汇表**以确保你拥有每个词的所有变体。即使你添加了**词形还原器**来解决这个问题，你也增加了处理流程中的一个额外步骤。'
- en: '**Handling slang and abbreviations-** another problem is the use of slang and
    abbreviations in texts these days such as "FOMO", "LOL", "tl;dr" etc. What do
    we do for these words?'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理俚语和缩写：** 另一个问题是现在文本中使用的俚语和缩写，例如 "FOMO"、"LOL"、"tl;dr" 等。我们该如何处理这些词？'
- en: '**What if the language doesn''t use space for segmentation:** for a language
    like Chinese, which doesn''t use spaces for word separation, this tokenizer will
    fail completely.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**如果语言不使用空格进行分隔怎么办：** 对于像中文这样不使用空格进行单词分隔的语言，这种分词器将完全失败。'
- en: After encountering these problems, researchers looked into another approach
    which was tokenizing all the characters.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在遇到这些问题后，研究人员开始研究另一种方法，即逐字符分词。
- en: Character-based tokenization
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于字符的分词
- en: To resolve the problems associated with word-based tokenization, an alternative
    approach of character-by-character tokenization was tried.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决与基于词的分词相关的问题，尝试了一种逐字符分词的替代方法。
- en: This did solve the problem of missing words as now we are dealing with characters
    that can be encoded using ASCII or Unicode and it could generate embedding for
    any word now.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实解决了缺词的问题，因为现在我们处理的是可以使用 ASCII 或 Unicode 编码的字符，并且现在可以为任何单词生成嵌入。
- en: Every character, be it space or apostrophes or colons, can now be assigned a
    symbol to generate a sequence of vectors.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 每个字符，无论是空格、撇号还是冒号，现在都可以分配一个符号来生成向量序列。
- en: But this approach had its own cons.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 但这种方法也有其缺点。
- en: Drawbacks of character-based models
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于字符的模型的缺点
- en: '**Requirement of more compute:** character-based models will treat each character
    as tokens and more tokens mean more input computations to process each token which
    in turn requires more compute resources. For a 5-word long sentence, you may need
    to process 30 tokens instead of 5 word-based tokens.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算需求更多：** 基于字符的模型将每个字符视为词元，更多的词元意味着更多的输入计算以处理每个词元，从而需要更多的计算资源。对于一个5词长的句子，你可能需要处理30个词元，而不是5个基于词的词元。'
- en: '**Narrows down the number of NLP tasks and applications:** with long sequences
    of characters, only a certain type of neural network architecture can be used.
    This puts a limitation on the type of NLP tasks we can perform. For applications
    like Entity recognition or text classification, character-based encoding might
    turn out to be an inefficient approach.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缩小 NLP 任务和应用的数量：** 对于长字符序列，只有某种类型的神经网络架构可以使用。这限制了我们可以执行的 NLP 任务类型。对于像实体识别或文本分类这样的应用，基于字符的编码可能会变得低效。'
- en: '**Risk of learning incorrect semantics:** working with characters could generate
    incorrect spellings of words. Also, with no inherent meaning, learning with characters
    is like learning with no meaningful semantics.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习错误语义的风险：** 与字符一起工作可能会产生单词的错误拼写。此外，由于没有固有的意义，使用字符学习就像在没有有意义语义的情况下学习。'
- en: What's fascinating is that for such a seemingly simple task, mutliple algorithms
    are written to find the optimal tokenization policy.
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 有趣的是，对于这样一个看似简单的任务，已经编写了多个算法来寻找最佳的分词策略。
- en: After understanding the pros and cons of these tokenization methods, it makes
    sense to look for an approach that offers a middle route i.e. preserve the semantics
    with limited vocabulary that can generate all the words in the text on merging.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了这些分词方法的优缺点之后，寻找一种提供中间路径的方法是有意义的，即保留语义的同时使用有限的词汇表，以便在合并时生成文本中的所有单词。
- en: Subword Tokenization
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 子词分词
- en: With character-based models, we risk losing the semantic features of the word
    and with word-based tokenization, we need a very large vocabulary to encompass
    all the possible variations of every word.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于字符的模型，我们有丢失词语的语义特征的风险，而使用基于词的分词，我们需要一个非常大的词汇表来包含每个单词的所有可能变体。
- en: 'So, the goal was to develop an algorithm that could:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，目标是开发一个算法，能够：
- en: Retain the semantic features of the token i.e. information per token.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保留标记的语义特征，即每个标记的信息。
- en: tokenize without demanding a very large vocabulary with a finite set of words.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在不需要非常大词汇表的情况下进行分词，使用有限的词汇表。
- en: To solve this problem, we could think of breaking down the words based on a
    set of prefixes and suffixes. For example, we can write a rule-based system to
    identify subwords like `"##s"`, `"##ing"`, `"##ify"`, `"un##"` etc., where the
    position of double hash denotes prefix and suffixes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以考虑根据一组前缀和后缀来拆分单词。例如，我们可以编写基于规则的系统来识别子词，如`"##s"`、`"##ing"`、`"##ify"`、`"un##"`等，其中双哈希的位置表示前缀和后缀。
- en: So, a word like `"unhappily"` is tokenized using subwords like `"un##"`, `"happ"`,
    and `"##ily"`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，像`"unhappily"`这样的词通过子词`"un##"`、`"happ"`和`"##ily"`进行分词。
- en: The model only learns a few subwords and then puts them together to create other
    words. This solves the problem of memory requirement and effort to create a large
    vocabulary.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 模型只学习一些子词，然后将它们组合起来创建其他词。这解决了创建大型词汇表所需的内存要求和工作量。
- en: 'Problems with this algorithm:'
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 该算法的问题：
- en: Some of the subwords that are created as per the defined rules may never appear
    in your text to tokenize and may end up occupying extra memory.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据定义的规则创建的一些子词可能从未出现在你的文本中进行分词，并且可能占用额外的内存。
- en: Also, for every language, we'll need to define a different set of rules to create
    subwords.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，对于每种语言，我们需要定义不同的规则集来创建子词。
- en: To alleviate this problem, in practice, most modern tokenizers have a training
    phase that identifies the recurring text in the input corpus and creates new subwords
    tokens. For rare patterns, we stick to word-based tokens.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解这个问题，在实践中，大多数现代分词器都有一个训练阶段，识别输入语料库中出现的文本并创建新的子词标记。对于稀有模式，我们坚持使用基于词的标记。
- en: Another important factor that plays a vital role in this process is the size
    of the vocabulary that is set by the user. Large vocabulary size allows for more
    common words to be tokenized whereas smaller vocabulary requires more subwords
    to be created to create every word in the text without using the `<UNK>` token.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中另一个重要因素是由用户设置的词汇表的大小。较大的词汇表允许更多常见单词被分词，而较小的词汇表则需要创建更多子词来构造文本中的每个单词，而不使用`<UNK>`标记。
- en: Striking the balance for your application is key here.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，平衡你的应用程序是关键。
- en: Byte Pair Encoding(BPE)
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 字节对编码（BPE）
- en: BPE was originally a data compression algorithm that is used to find the best
    way to represent data by identifying the common byte pairs. It is now used in
    NLP to find the best representation of text using the least number of tokens.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: BPE最初是一种数据压缩算法，用于通过识别常见字节对找到表示数据的最佳方式。现在它被用于自然语言处理（NLP），以使用最少的标记找到文本的最佳表示。
- en: 'Here''s how it works:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 工作原理如下：
- en: Add an identifier(`</w>`) at the end of each word to identify the end of a word
    and then calculate the word frequency in the text.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个单词的末尾添加一个标识符（`</w>`）以标识单词的结束，然后计算文本中的单词频率。
- en: Split the word into characters and then calculate the character frequency.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将单词拆分为字符，然后计算字符频率。
- en: From the character tokens, for a predefined number of iterations, count the
    frequency of the consecutive byte pairs and merge the most frequently occurring
    byte pairing.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从字符标记开始，进行预定义次数的迭代，计算连续字节对的频率，并合并出现频率最高的字节对。
- en: Keep iterating until you have reached the iteration limit(set by you) or if
    you have reached the token limit.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 持续迭代，直到你达到迭代限制（由你设置）或达到标记限制。
- en: Let's go through each step(in code) for a sample text. For coding this, I have
    taken help from [Lei Mao's very minimalistic blog on BPE](https://leimao.github.io/blog/Byte-Pair-Encoding/).
    I encourage you to check it out!
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步（在代码中）处理一个示例文本。为此编写代码，我借鉴了[Lei Mao的BPE极简博客](https://leimao.github.io/blog/Byte-Pair-Encoding/)。我鼓励你去看看！
- en: 'Here''s our sample text:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的示例文本：
- en: '[PRE1]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![](../Images/b12c502f08d3d40df13b3a665ddeb325.png)](https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb90e5882-9f1f-4b05-be48-3e9336cf1854_283x392.png)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/b12c502f08d3d40df13b3a665ddeb325.png)](https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb90e5882-9f1f-4b05-be48-3e9336cf1854_283x392.png)'
- en: 'Step 2: Split the word into characters and then calculate the character frequency.'
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二步：将单词拆分为字符，然后计算字符频率。
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![](../Images/c1e660153c7aec3e03f4430fc9301f49.png)](https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fecbf93c5-7fd6-4a40-a63d-e504be1bf157_396x536.png)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/c1e660153c7aec3e03f4430fc9301f49.png)](https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fecbf93c5-7fd6-4a40-a63d-e504be1bf157_396x536.png)'
- en: 'Step 3: Merge the most frequently occurring consecutive byte pairing.'
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三步：合并最频繁出现的连续字节对。
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![](../Images/abcae8d892a8f69406e06c3c0bf6f78a.png)](https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fdaf21979-946b-4dfb-b090-64e591c13907_400x590.png)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/abcae8d892a8f69406e06c3c0bf6f78a.png)](https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fdaf21979-946b-4dfb-b090-64e591c13907_400x590.png)'
- en: Step 4 - Iterate a number of times to find the best(in terms of frequency) pairs
    to encode and then concatenate them to find the subwords.
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四步 - 迭代多次以找到最佳（就频率而言）对进行编码，然后将它们连接以查找子词。
- en: 'It is better at this point to turn structure our code into functions. This
    will require us to perform the following steps:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，最好将我们的代码结构化为函数。这将要求我们执行以下步骤：
- en: Find the most frequently occurring byte pairs in each iteration.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每次迭代中，找到最频繁出现的字节对。
- en: Merge these tokens.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 合并这些标记。
- en: Recalculate the character tokens frequency with the new pair encoding added.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新计算字符标记的频率，添加新的对编码。
- en: Keep doing it until there is no more pair or you reach the end of the for a
    loop.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续进行，直到没有更多对或者你到达循环的结束。
- en: For detailed code, you should **check out my [Colab notebook](https://colab.research.google.com/drive/1QLlQx_EjlZzBPsuj_ClrEDC0l8G-JuTn?usp=sharing).**
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看详细代码，你可以**查看我的 [Colab 笔记本](https://colab.research.google.com/drive/1QLlQx_EjlZzBPsuj_ClrEDC0l8G-JuTn?usp=sharing)。**
- en: 'Here’s a trimmed output of those 4 steps:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这是这四个步骤的精简输出：
- en: '[![](../Images/db0e450fa66790cc0afbb9effa4ede62.png)](https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4cb7b992-1986-4dbc-a444-da817255f80f_1295x637.png)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/db0e450fa66790cc0afbb9effa4ede62.png)](https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4cb7b992-1986-4dbc-a444-da817255f80f_1295x637.png)'
- en: So as we iterate with each best pair, we merge(concatenating) the pair and you
    can see as we recalculate the frequency, the original character token frequency
    is reduced and the new paired token frequency pops up in the token dictionary.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们迭代每个最佳对时，我们合并（连接）该对，你可以看到，当我们重新计算频率时，原始字符标记频率减少，新配对的标记频率在标记字典中出现。
- en: If you look at the number of tokens created, it first increases because we create
    new pairings but the number starts to decrease after a number of iterations.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看创建的标记数量，最初会增加，因为我们创建了新的配对，但在经过若干次迭代后，数量开始减少。
- en: Here, we started from 25 tokens, went up to 31 tokens in the 14th iteration,
    and then came down to 16 tokens in the 50th iteration. Interesting, right?
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们从 25 个标记开始，在第 14 次迭代中增加到 31 个标记，然后在第 50 次迭代中减少到 16 个标记。很有趣，对吧？
- en: Scope of improvement for the BPE algorithm
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BPE 算法的改进空间
- en: BPE algorithm is a greedy algorithm i.e. it tries to find the best pair in each
    iteration. And there are some limitations of this greedy approach.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: BPE 算法是一种贪婪算法，即它在每次迭代中尝试寻找最佳对。这个贪婪方法有一些局限性。
- en: Thus, there are pros and cons of the BPE algorithm too.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，BPE 算法也有优缺点。
- en: The final tokens will vary depending upon the number of iterations you have
    run which also causes another problem. We now can have different tokens for a
    single text and thus different embeddings.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最终标记将根据你运行的迭代次数而有所不同，这也带来了另一个问题。现在，我们可以对同一文本拥有不同的标记，从而得到不同的嵌入。
- en: To address this issue, multiple solutions were proposed but the one that stood
    out was a unigram language model that added [subword regularization(a new method
    of subword segmentation)](https://arxiv.org/pdf/1804.10959.pdf) training that
    calculates the probability for each subword token to choose the best option using
    a loss function. More on this in the upcoming blogs.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，提出了多种解决方案，但最突出的是一种添加了[subword regularization（子词正则化，一种新的子词分割方法）](https://arxiv.org/pdf/1804.10959.pdf)训练的单语语言模型，该模型通过使用损失函数计算每个子词令牌的概率，以选择最佳选项。更多内容将在即将到来的博客中介绍。
- en: Do they use BPE in BERTs or GPTs?
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 他们在BERTs或GPTs中使用BPE吗？
- en: Models like BERT or GPT-2 use some version of the BPE or the unigram model to
    tokenize the input text.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 像BERT或GPT-2这样的模型使用某种版本的BPE或单语模型来对输入文本进行标记化。
- en: BERT included a new algorithm called WordPiece which is also similar to the
    BPE but has an added layer of likelihood calculation to decide whether the merged
    token will make the final cut.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: BERT包含了一种新的算法，叫做WordPiece，这与BPE类似，但增加了一个概率计算层，以决定合并的令牌是否会被最终采纳。
- en: Summary
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: What you've learned(if at all) in this blog how a machine starts to make sense
    of language by breaking down the text into very small units.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这篇博客中学到了什么（如果有的话），即机器如何通过将文本分解为非常小的单元来开始理解语言。
- en: Now, there are many ways to break text down and thus it becomes important to
    compare one approach with another.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有很多方法可以将文本分解，因此比较不同方法变得重要。
- en: We started off by understanding tokenization by splitting the English text by
    spaces but not every language is written the same way(i.e. using spaces to denote
    segmentation) so we looked at splitting by character to generate character tokens.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从通过空格分割英文文本来理解标记化开始，但并非所有语言都是以相同的方式书写（即使用空格来表示分割），因此我们研究了通过字符分割来生成字符令牌。
- en: The problem with characters was the loss of semantic features from the tokens
    at the risk of creating incorrect word representations or embeddings.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用字符的问题在于令牌的语义特征丧失，可能导致创建不正确的词汇表示或嵌入。
- en: To get the best of both worlds, subword tokenization was introduced which was
    more promising and then we looked at the BPE algorithm to implement subword tokenization.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了兼顾两全，推出了更有前景的子词标记化方法，然后我们研究了BPE算法来实现子词标记化。
- en: More on the next steps and advanced tokenizers like WordPiece, SentencePiece
    and how to work with the HuggingFace tokenizer next week.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 下一周将详细介绍WordPiece、SentencePiece等高级标记化器的下一步，以及如何使用HuggingFace标记化器。
- en: References and Notes
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献和注释
- en: 'My post is actually an accumulation of the following papers and blogs that
    I encourage you to read:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我的文章实际上是以下论文和博客的汇总，我鼓励你阅读：
- en: '[Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf) -
    Research paper that discusses different segmentation techniques based BPE compression
    algorithm.'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[稀有词汇的神经机器翻译与子词单元](https://arxiv.org/pdf/1508.07909.pdf) - 讨论基于BPE压缩算法的不同分割技术的研究论文。'
- en: '[GitHub repo on Subword NMT(Neural Machine Translation) ](https://github.com/rsennrich/subword-nmt)-
    supporting code for the above paper.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[关于子词NMT（神经机器翻译）的GitHub代码库](https://github.com/rsennrich/subword-nmt) - 支持上述论文的代码。'
- en: '[Lei Mao’s blog on Byte Pair Encoding](https://leimao.github.io/blog/Byte-Pair-Encoding/) -
    I used the code in his blog to implement and understand BPE myself.'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Lei Mao的Byte Pair Encoding博客](https://leimao.github.io/blog/Byte-Pair-Encoding/)
    - 我使用了他博客中的代码来实现和理解BPE。'
- en: '[How Machines read](https://blog.floydhub.com/tokenization-nlp/) - a blog by
    Cathal Horan.'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[机器如何阅读](https://blog.floydhub.com/tokenization-nlp/) - Cathal Horan的博客。'
- en: If you’re looking to start in the field of data science or ML, check out my
    course on [Foundations of Data Science & ML](https://www.wiplane.com/p/foundations-for-data-science-ml).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想开始学习数据科学或机器学习领域，可以查看我的课程 [数据科学与机器学习基础](https://www.wiplane.com/p/foundations-for-data-science-ml)。
- en: If you would like to see more of such content and you are not a subscriber,
    consider subscribing to my newsletter using the button below.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想看到更多类似内容而你还不是订阅者，可以考虑使用下面的按钮订阅我的新闻通讯。
- en: '**Bio: [Harshit Tyagi](https://www.linkedin.com/in/tyagiharshit/)** is an engineer
    with amalgamated experience in web technologies and data science(aka full-stack
    data science). He has mentored over 1000 AI/Web/Data Science aspirants, and is
    designing data science and ML engineering learning tracks. Previously, Harshit
    developed data processing algorithms with research scientists at Yale, MIT, and
    UCLA.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介: [Harshit Tyagi](https://www.linkedin.com/in/tyagiharshit/)** 是一位在网页技术和数据科学（即全栈数据科学）方面拥有丰富经验的工程师。他已经辅导了超过1000名人工智能/网页/数据科学的求职者，并正在设计数据科学和机器学习工程学习路径。此前，Harshit
    与耶鲁大学、麻省理工学院和加州大学洛杉矶分校的研究科学家一起开发了数据处理算法。'
- en: '[Original](https://dswharshit.substack.com/p/the-evolution-of-tokenization-byte).
    Reposted with permission.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://dswharshit.substack.com/p/the-evolution-of-tokenization-byte)。经授权转载。'
- en: '**Related:**'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Text Preprocessing Methods for Deep Learning](/2021/09/text-preprocessing-methods-deep-learning.html)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习的文本预处理方法](/2021/09/text-preprocessing-methods-deep-learning.html)'
- en: '[15 Must-Know Python String Methods](/2021/09/15-must-know-python-string-methods.html)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15 个必须了解的 Python 字符串方法](/2021/09/15-must-know-python-string-methods.html)'
- en: '[Learning Data Science and Machine Learning: First Steps After The Roadmap](/2021/08/learn-data-science-machine-learning.html)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学和机器学习：路线图后的第一步](/2021/08/learn-data-science-machine-learning.html)'
- en: '* * *'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的捷径。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的 IT 工作'
- en: '* * *'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Encoding Categorical Features with MultiLabelBinarizer](https://www.kdnuggets.com/2023/01/encoding-categorical-features-multilabelbinarizer.html)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 MultiLabelBinarizer 对类别特征进行编码](https://www.kdnuggets.com/2023/01/encoding-categorical-features-multilabelbinarizer.html)'
- en: '[From Oracle to Databases for AI: The Evolution of Data Storage](https://www.kdnuggets.com/2022/02/oracle-databases-ai-evolution-data-storage.html)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从甲骨文到人工智能数据库：数据存储的演变](https://www.kdnuggets.com/2022/02/oracle-databases-ai-evolution-data-storage.html)'
- en: '[Analyzing the Probability of Future Success with Intelligence…](https://www.kdnuggets.com/2022/02/analyzing-probability-future-success-intelligence-node-attributes-evolution-model.html)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用智能分析未来成功的概率……](https://www.kdnuggets.com/2022/02/analyzing-probability-future-success-intelligence-node-attributes-evolution-model.html)'
- en: '[The Evolution From Artificial Intelligence to Machine Learning to…](https://www.kdnuggets.com/2022/08/evolution-artificial-intelligence-machine-learning-data-science.html)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从人工智能到机器学习的演变……](https://www.kdnuggets.com/2022/08/evolution-artificial-intelligence-machine-learning-data-science.html)'
- en: '[The Evolution of Speech Recognition Metrics](https://www.kdnuggets.com/2022/10/evolution-speech-recognition-metrics.html)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[语音识别指标的演变](https://www.kdnuggets.com/2022/10/evolution-speech-recognition-metrics.html)'
- en: '[A Deep Dive into GPT Models: Evolution & Performance Comparison](https://www.kdnuggets.com/2023/05/deep-dive-gpt-models.html)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深入探讨 GPT 模型：演变与性能比较](https://www.kdnuggets.com/2023/05/deep-dive-gpt-models.html)'
