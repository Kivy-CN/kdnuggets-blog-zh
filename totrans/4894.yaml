- en: Is Learning Rate Useful in Artificial Neural Networks?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习率在人工神经网络中有用吗？
- en: 原文：[https://www.kdnuggets.com/2018/01/learning-rate-useful-neural-network.html](https://www.kdnuggets.com/2018/01/learning-rate-useful-neural-network.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/01/learning-rate-useful-neural-network.html](https://www.kdnuggets.com/2018/01/learning-rate-useful-neural-network.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: This article will help you understand why we need the learning rate and whether
    it is useful or not for training an artificial neural network. Using a very simple
    Python code for a single layer perceptron, the learning rate value will get changed
    to catch its idea.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章将帮助你理解我们为什么需要学习率以及它是否对训练人工神经网络有用。通过使用一个非常简单的单层感知器Python代码，我们将改变学习率值以理解其概念。
- en: '**Introduction**'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: An obstacle for newbies in artificial neural networks is the learning rate.
    I was asked many times about the effect of the learning rate in the training of
    the artificial neural networks (ANNs). Why we use learning rate? What is the best
    value for the learning rate? In this article, I will try to make things simpler
    by providing an example that shows how learning rate is useful in order to train
    an ANN. I will start by explaining our example with Python code before working
    with the learning rate.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络的初学者面临的一个障碍是学习率。我被问过很多次关于学习率在训练人工神经网络（ANNs）中的作用。我们为什么要使用学习率？学习率的最佳值是什么？在这篇文章中，我将通过提供一个示例来简化问题，展示学习率在训练ANN时的作用。我将从解释我们的Python代码示例开始，然后再讨论学习率。
- en: '**Example**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例**'
- en: A very very simple example is used to get us out of complexity and allow us
    to just focus on the learning rate. A single numerical input will get applied
    to a single layer perceptron. If the input is 250 or smaller, its value will get
    returned as the output of the network. If the input is larger than 250, then it
    will be clipped to just 250\. The following table shows the 6 samples used for
    training.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常简单的示例被用来摆脱复杂性，让我们专注于学习率。一个数值输入将应用于一个单层感知器。如果输入为250或更小，则其值将作为网络的输出返回。如果输入大于250，则将被裁剪为250。下表展示了用于训练的6个样本。
- en: '![](../Images/bdcb17c10de3c2ec4e956f201ac6d85a.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdcb17c10de3c2ec4e956f201ac6d85a.png)'
- en: '**ANN Architecture**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**ANN架构**'
- en: The architecture of the ANN used is shown in the next figure. There are just
    input and output layers. The input layer has just a single neuron for our single
    input. The output layer has just a single neuron for generating the output. The
    output layer neuron is responsible for mapping the input to the correct output.
    There is also a bias applied to the output layer neuron with weight ***b*** and
    value ***+1***. There is also a weight ***W*** for the input.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的ANN架构如下一图所示。该架构仅包含输入层和输出层。输入层只有一个神经元用于我们的单一输入。输出层也只有一个神经元用于生成输出。输出层的神经元负责将输入映射到正确的输出。同时，输出层的神经元上还应用了一个偏置，其权重为***b***，值为***+1***。输入层还有一个权重***W***。
- en: '![](../Images/94a9409e811a7869b85ea922f5cc67ea.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94a9409e811a7869b85ea922f5cc67ea.png)'
- en: '**Activation Function**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**激活函数**'
- en: The equation and the graph of the activation function used in this example are
    as shown in the next figure. When the input is below or equal to 250, the output
    will be the same as the input. Otherwise, it will be clipped to 250.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例中使用的激活函数的方程和图形如下图所示。当输入小于或等于250时，输出将与输入相同。否则，输出将被裁剪为250。
- en: '![](../Images/97066b4183b4e99264208931ddd63d88.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97066b4183b4e99264208931ddd63d88.png)'
- en: '**Implementation using Python**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用Python的实现**'
- en: The Python code implementing the entire network is shown below. We will discuss
    all of it until making it easy as much as possible then focus on changing the
    learning rate to find out how it affects the network training.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 实现整个网络的Python代码如下所示。我们将讨论代码的所有部分，尽量将其简化，然后专注于改变学习率以了解其对网络训练的影响。
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Lines 17 and 18 are responsible for creating two arrays (inputs and desired_output)
    holding the training input and output data presented in the previous table. Each
    input will have an output according to the activation function used.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第17行和第18行负责创建两个数组（inputs和desired_output），用于存储前面表格中展示的训练输入和输出数据。每个输入将根据所使用的激活函数产生相应的输出。
- en: 'Line 16 creates an array of the network weights. There are just two weights:
    one for the input and another for the bias. They were randomly initialized to
    0.05 for the bias and 0.1 for the input.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 第16行创建了一个网络权重的数组。只有两个权重：一个用于输入，另一个用于偏置。它们被随机初始化为偏置0.05和输入0.1。
- en: The activation function itself is implemented using the activation_function(inpt)
    method from line 3 to 7\. It accepts a single argument which is the input and
    returns a single value which is the expected output.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数本身是通过第3到7行定义的 activation_function(inpt) 方法实现的。它接受一个参数，即输入，并返回一个值，即期望输出。
- en: 'Because there may be an error in the prediction, we need to measure that error
    to know how far we are from the correct prediction. For that reason, there is
    a method implemented from line 9 to 10 called prediction_error(desired, expected)
    that accepts two inputs: the desired and expected outputs. That method just calculates
    the absolute difference between each desired and expected outputs. The best value
    for any error is for sure 0\. This is the optimal value.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预测中可能存在误差，我们需要测量该误差以了解我们离正确预测的距离。因此，有一个从第9到10行实现的方法，称为 prediction_error(desired, expected)，它接受两个输入：期望输出和实际输出。这个方法只是计算每个期望输出和实际输出之间的绝对差异。任何误差的最佳值当然是0。这是最终值。
- en: 'What if there was a prediction error? In this case, we must make a change to
    the network. But what exactly to change? It is the network weights. For updating
    the network weights, there is a method called update_weights(weights, predicted, idx)
    defined from line 13 to 14\. It accepts three inputs: old weights, predicted output,
    and the index of the input that has a false prediction. This method applies the
    following equation to update the weights:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果出现预测误差怎么办？在这种情况下，我们必须对网络进行更改。但是具体要更改什么呢？是网络权重。为了更新网络权重，有一个方法叫做 update_weights(weights, predicted, idx)，定义在第13到14行。它接受三个输入：旧权重、预测输出和具有错误预测的输入的索引。这个方法应用以下方程来更新权重：
- en: '![](../Images/dc8b0e0c701ece674dbc601279191261.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc8b0e0c701ece674dbc601279191261.png)'
- en: The equation uses the weights of the current step (n) to generate the weights
    of the next step (n+1). This equation is what we will use for knowing how the
    learning rate affects the learning process.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程使用当前步骤（n）的权重生成下一步骤（n+1）的权重。这个方程是我们用来了解学习率如何影响学习过程的。
- en: Finally, we need to concatenate all of these together to make the network learn.
    This is done using the training_loop(inpt, weights) method defined from line 20
    to 31\. It goes into a training loop. The loop is used to map the inputs to their
    outputs with the least possible prediction error.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要将所有这些拼接在一起以使网络学习。这是通过从第20行到31行定义的 training_loop(inpt, weights) 方法完成的。它进入一个训练循环。这个循环用于将输入映射到其输出，同时使预测误差最小化。
- en: 'The loop does three operations:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 循环执行三个操作：
- en: 1.      Output Prediction.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 1.      输出预测。
- en: 2.      Error Calculation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 2.      误差计算。
- en: 3.      Updating Weights.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 3.      更新权重。
- en: After getting the idea of the example and its Python code, let us start showing
    how the learning rate is useful in order to get the best results.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了示例及其 Python 代码后，让我们开始展示学习率在获得最佳结果方面的作用。
- en: '**Learning Rate**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习率**'
- en: 'In the previously discussed example, line 13 has the weights update equation
    in which the learning rate is used. At first, let us assume that we have not used
    the learning rate completely. The equation will as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面讨论的示例中，第13行有学习率被使用的权重更新方程。首先，让我们假设我们没有完全使用学习率。方程将如下：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let us see the effect of removing the learning rate. In the iteration of the
    training loop, the network has the following inputs (b=0.05 and W=0.1, Input =
    60, and desired output=60).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看去除学习率的效果。在训练循环的迭代中，网络有以下输入（b=0.05 和 W=0.1，输入 = 60，期望输出=60）。
- en: The expected output which is the result of the activation function as in line
    25 will be activation_function(0.05(+1) + 0.1(60)). The predicted output will
    be 6.05.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 期望输出，即第25行激活函数的结果，将是 activation_function(0.05(+1) + 0.1(60))。预测输出将是6.05。
- en: In line 26, the prediction error will be calculated by getting the difference
    between the desired and the predicted output. The error will be abs(60-6.05)=53.95.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在第26行，预测误差将通过计算期望输出和预测输出之间的差异来计算。误差将是 abs(60-6.05)=53.95。
- en: Then in line 27 the weights will get updated according to the above equation.
    The new weights will be [0.05, 0.1] + (53.95)*60 = [0.05, 0.1] + 3237 = [3237.05,
    3237.1].
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在第27行，权重将根据上述方程进行更新。新的权重将是[0.05, 0.1] + (53.95)*60 = [0.05, 0.1] + 3237 =
    [3237.05, 3237.1]。
- en: It seems that the new weights are too different from the previous weights. Each
    weight got increased by 3,237 which is too large. But let us continue making the
    next prediction.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来新的权重与之前的权重差异太大。每个权重增加了3,237，这实在太大了。但让我们继续进行下一次预测。
- en: 'In the next iteration, the network will have these inputs applied: (b=3237.05
    and W=3237.1, Input = 40, and desired output=40). The expected output will be
    activation_function((3237.05 + 3237.1(40)) = 250\. The prediction error will be
    abs(40 - 250) = 210\. The error is very large. It is larger than the previous
    error. Thus we have to update the weights again. According to the above equation,
    the new weights will be [3237.05, 3237.1] + (-210)*40 = [3237.05, 3237.1] + -8400
    = [-5162.95, -5162.9].'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一次迭代中，网络将应用以下输入：（b=3237.05 和 W=3237.1，输入=40，期望输出=40）。预期输出将是activation_function((3237.05
    + 3237.1(40)) = 250。预测误差将是abs(40 - 250) = 210。误差非常大，比之前的误差还要大。因此，我们必须再次更新权重。根据上述方程，新的权重将是[3237.05,
    3237.1] + (-210)*40 = [3237.05, 3237.1] + -8400 = [-5162.95, -5162.9]。
- en: 'The next table summarizes the results of the first three iterations:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 下一张表总结了前三次迭代的结果：
- en: '![](../Images/f274b272653f8d81d3df3858f7954883.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f274b272653f8d81d3df3858f7954883.png)'
- en: As we go into more iterations, the results get worse. The magnitude of the weights
    is changing rapidly and sometimes with changing its signs. They are moving from
    very large positive value to very large negative value. How can we stop this large
    and abrupt changes in the weights? How to scale down the value by which the weights
    are updated?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 随着迭代次数的增加，结果变得更差。权重的大小迅速变化，有时甚至改变了符号。它们从非常大的正值变为非常大的负值。我们如何停止权重的这种大幅度和突变？如何缩小更新权重的值？
- en: If we looked at the value by which the weights are changing by from the previous
    table, it seems that the value is very large. This means that the network changes
    its weights with large speed. It is like someone that makes large moves within
    small times. At one time, the person is in the far east and after a very short
    time, that person will be in the far west. We just need to make it slower.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看上一张表中权重变化的值，似乎这个值非常大。这意味着网络以较大的速度更改其权重。这就像一个人在短时间内做出大幅度的移动。某个时刻，这个人在远东，而在很短的时间内，这个人会在远西。我们只需要让它变得更慢。
- en: If we are able to scale down this value to get smaller then everything will
    be alright. But how?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够将这个值缩小到更小的程度，那么一切都会变得顺利。但是怎么做呢？
- en: 'Getting back to the part of the code that generates this value, it looks that
    the update equation is what generates it. Specifically this part:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 回到生成这个值的代码部分，看来更新方程就是生成它的原因。具体来说，就是这一部分：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can scale this part by multiplying it by a small value such as 0.1\. So,
    rather than generating 3237.0 as the updated value in the first iteration, it
    will be reduced to just 323.7\. We can even scale this value to a smaller value
    by decreasing the scale value to say 0.001\. Using 0.001, the value will be just
    3.327.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将这部分乘以一个小值（例如0.1）来缩放它。因此，第一次迭代中生成的3237.0的更新值将减少到323.7。我们甚至可以将这个值缩小到更小的值，例如0.001。使用0.001时，值将仅为3.327。
- en: We can catch it now. This scaling value is the learning rate. Choosing small
    values for the learning rate makes the rate of weights update smaller and avoids
    abrupt changes. As the value gets larger as the changes are faster and as a result
    bad results.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以理解了。这一缩放值就是学习率。选择较小的学习率使权重更新的速度较小，并避免突变。随着值变大，变化速度加快，从而导致不良结果。
- en: '**But what is the best value for the learning rate?**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**但学习率的最佳值是什么？**'
- en: There is no value we can say it is the best value for the learning rate. The
    learning rate is a hyperparameter. A hyperparameter has its value determined by
    experiments. We try different values and use the value that gives best results.
    There are some ways that just helps you select values of hyperparameters.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能说哪个值是学习率的最佳值。学习率是一个超参数。超参数的值通过实验来确定。我们尝试不同的值，并使用那些给出最佳结果的值。有一些方法可以帮助你选择超参数的值。
- en: '**Testing Network**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**测试网络**'
- en: For our problem, I deduced that a value of .00001 works fine. After training
    the network with that learning rate, we can make a test. The following table shows
    the results of prediction of 4 new testing samples. It seems that results are
    now much better after using the learning rate.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的问题，我推测值为 .00001 是合适的。在使用这个学习率训练网络后，我们可以进行测试。下表显示了对 4 个新测试样本的预测结果。似乎在使用学习率后结果有了很大改善。
- en: '![](../Images/2face38a2a1266e29bdb60faf7af8beb.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2face38a2a1266e29bdb60faf7af8beb.png)'
- en: '**Bio: [Ahmed Gad](https://www.linkedin.com/in/ahmedfgad/)** received his B.Sc.
    degree with excellent with honors in information technology from the Faculty of
    Computers and Information (FCI), Menoufia University, Egypt, in July 2015\. For
    being ranked first in his faculty, he was recommended to work as a teaching assistant
    in one of the Egyptian institutes in 2015 and then in 2016 to work as a teaching
    assistant and a researcher in his faculty. His current research interests include
    deep learning, machine learning, artificial intelligence, digital signal processing,
    and computer vision.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介： [艾哈迈德·贾德](https://www.linkedin.com/in/ahmedfgad/)** 于 2015 年 7 月从埃及梅努费亚大学计算机与信息学院获得信息技术学士学位，成绩优异并获得荣誉。由于在学院中排名第一，他被推荐在
    2015 年于埃及的一所学院担任助教，随后在 2016 年继续担任助教和研究员。他当前的研究兴趣包括深度学习、机器学习、人工智能、数字信号处理和计算机视觉。'
- en: '[Original](https://www.linkedin.com/pulse/learning-rate-useful-artificial-neural-networks-ahmed-gad/).
    Reposted with permission.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://www.linkedin.com/pulse/learning-rate-useful-artificial-neural-networks-ahmed-gad/)。经许可转载。'
- en: '**Related:**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[TensorFlow: Building Feed-Forward Neural Networks Step-by-Step](/2017/10/tensorflow-building-feed-forward-neural-networks-step-by-step.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow：一步一步构建前馈神经网络](/2017/10/tensorflow-building-feed-forward-neural-networks-step-by-step.html)'
- en: '[An Overview of 3 Popular Courses on Deep Learning](/2017/10/3-popular-courses-deep-learning.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3 个受欢迎的深度学习课程概述](/2017/10/3-popular-courses-deep-learning.html)'
- en: '[5 Free Resources for Furthering Your Understanding of Deep Learning](/2017/10/5-free-resources-furthering-understanding-deep-learning.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5 个免费资源帮助你进一步理解深度学习](/2017/10/5-free-resources-furthering-understanding-deep-learning.html)'
- en: '* * *'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT'
- en: '* * *'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[The Base Rate Fallacy and its Impact on Data Science](https://www.kdnuggets.com/2023/04/base-rate-fallacy-impact-data-science.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基础率谬误及其对数据科学的影响](https://www.kdnuggets.com/2023/04/base-rate-fallacy-impact-data-science.html)'
- en: '[Increase Your Callback Rate With A LinkedIn Profile](https://www.kdnuggets.com/increase-your-callback-rate-with-a-linkedin-profile)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过 LinkedIn 个人资料提高你的回访率](https://www.kdnuggets.com/increase-your-callback-rate-with-a-linkedin-profile)'
- en: '[Neural Networks and Deep Learning: A Textbook (2nd Edition)](https://www.kdnuggets.com/2023/07/aggarwal-neural-networks-deep-learning-textbook-2nd-edition.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[神经网络与深度学习：教材（第 2 版）](https://www.kdnuggets.com/2023/07/aggarwal-neural-networks-deep-learning-textbook-2nd-edition.html)'
- en: '[10 Simple Things to Try Before Neural Networks](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[神经网络前尝试的 10 个简单步骤](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[可解释的 PyTorch 神经网络](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
- en: '[Deep Neural Networks Don''t Lead Us Towards AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度神经网络不会引导我们走向 AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
