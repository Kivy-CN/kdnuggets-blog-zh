- en: The Ultimate Guide To Different Word Embedding Techniques In NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html](https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/f2707b9df702dfb7da8ae07a7e6d5bdb.png)'
  prefs: []
  type: TYPE_IMG
- en: '* * *'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_BQ
  - PREF_H2
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '* * *'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '"You shall know a word by the company it keeps!"'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —John Rupert Firth
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Wouldn’t it be incredible if computers could start understanding Shakespeare?
    Or write fiction like J.K Rowling? This was unimaginable a few years back. Recent
    advancements in [Natural Language Processing](https://www.ibm.com/cloud/learn/natural-language-processing)
    (NLP) and [Natural Language Generation](https://www.techopedia.com/definition/33012/natural-language-generation-nlg)
    (NLG) have skyrocketed the ability of computers to better understand text-based
    content.
  prefs: []
  type: TYPE_NORMAL
- en: To understand and generate text, NLP-powered systems must be able to recognize
    words, grammar, and a whole lot of language nuances. For computers, this is easier
    said than done because they can only comprehend numbers.
  prefs: []
  type: TYPE_NORMAL
- en: To bridge the gap, NLP experts developed a technique called *word embeddings*
    that convert words into their numerical representations. Once converted, NLP algorithms
    can easily digest these learned representations to process textual information.
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings map the words as real-valued numerical vectors. It does so by
    tokenizing each word in a sequence (or sentence) and converting them into a vector
    space. Word embeddings aim to capture the semantic meaning of words in a sequence
    of text. It assigns similar numerical representations to words that have similar
    meanings.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a look at some of the most promising word embedding techniques in
    NLP.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF — Term Frequency-Inverse Document Frequency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TF-IDF is a machine learning (ML) algorithm based on a statistical measure
    of finding the relevance of words in the text. The text can be in the form of
    a document or various documents (corpus). It is a combination of two metrics:
    Term Frequency (TF) and Inverse Document Frequency (IDF).'
  prefs: []
  type: TYPE_NORMAL
- en: TF score is based on the frequency of words in a document. Words are counted
    for their number of occurrences in the documents. TF is calculated by dividing
    the number of occurrences of a word (i) by the total number (N) of words in the
    document (j).
  prefs: []
  type: TYPE_NORMAL
- en: '*TF (i) = log (frequency (i,j)) / log (N (j))*'
  prefs: []
  type: TYPE_NORMAL
- en: IDF score calculates the rarity of the words. It is important because TF gives
    more weightage to words that occur more frequently. However, words that are rarely
    used in the corpus may hold significant information. IDF captures this information.
    It can be calculated by dividing the total number (N) of documents (d) by the
    number of documents containing the word (i).
  prefs: []
  type: TYPE_NORMAL
- en: '*IDF (i) = log (N (d) / frequency (d,i))*'
  prefs: []
  type: TYPE_NORMAL
- en: The *log* is taken in the above formulas to dampen the effect of large scores
    for TF and IDF. The final TF-IDF score is calculated by multiplying TF and IDF
    scores.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF algorithm is used in solving simpler ML and NLP problems. It is better
    used for information retrieval, keyword extraction, stop words (like ‘a’, ‘the’,
    ‘are’, ‘is’) removal, and basic [text analysis](https://algoscale.com/data-analytics/text-analytics/).
    It cannot capture the semantic meaning of words in a sequence efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec — Capturing Semantic Information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developed by Tomas Mikolov and other researchers at Google in 2013, [Word2Vec](https://arxiv.org/abs/1301.3781)
    is a word embedding technique for solving advanced NLP problems. It can iterate
    over a large corpus of text to learn associations or dependencies among words.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec finds similarities among words by using the [*cosine similarity*](https://www.machinelearningplus.com/nlp/cosine-similarity/)
    metric. If the cosine angle is 1, that means words are overlapping. If the cosine
    angle is 90, that means words are independent or hold no contextual similarity.
    It assigns similar vector representations to similar words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Word2Vec offers two [neural network](https://www.ibm.com/cloud/learn/neural-networks)-based
    variants: Continuous Bag of Words (CBOW) and Skip-gram. In CBOW, the neural network
    model takes various words as input and predicts the target word that is closely
    related to the context of the input words. On the other hand, the Skip-gram architecture
    takes one word as input and predicts its closely related context words.'
  prefs: []
  type: TYPE_NORMAL
- en: CBOW is quick and finds better numerical representations for frequent words,
    while Skip Gram can efficiently represent rare words. Word2Vec models are good
    at capturing semantic relationships among words. For example, the relationship
    between a country and its capital, like Paris is the capital of France and Berlin
    is the capital of Germany. It is best suited for performing [semantic analysis](https://www.expert.ai/blog/natural-language-process-semantic-analysis-definition/),
    which has application in recommendation systems and knowledge discovery.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b27c49f170dd269f575b79015ec9c8b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'CBOW & Skip-gram architectures. Image Source: [Word2Vec paper](https://arxiv.org/abs/1301.3781).'
  prefs: []
  type: TYPE_NORMAL
- en: GloVe — Global Vectors for Word Representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developed by Jeffery Pennington and other researchers at Stanford, [GloVe](https://nlp.stanford.edu/pubs/glove.pdf)
    extends the work of Word2Vec to capture global contextual information in a text
    corpus by calculating a global [word-word co-occurrence matrix](https://medium.com/swlh/co-occurrence-matrix-9cacc5dd396e).
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec only captures the local context of words. During training, it only
    considers neighboring words to capture the context. GloVe considers the entire
    corpus and creates a large matrix that can capture the co-occurrence of words
    within the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'GloVe combines the advantages of two-word vector learning methods: matrix factorization
    like [latent semantic analysis](https://blog.marketmuse.com/glossary/latent-semantic-analysis-definition/)
    (LSA) and local context window method like Skip-gram. The GloVe technique has
    a simpler [least square](https://www.investopedia.com/terms/l/least-squares-method.asp)
    cost or error function that reduces the computational cost of training the model.
    The resulting word embeddings are different and improved.'
  prefs: []
  type: TYPE_NORMAL
- en: GloVe performs significantly better in word analogy and [named entity recognition](https://www.expert.ai/blog/entity-extraction-work/)
    problems. It is better than Word2Vec in some tasks and competes in others. However,
    both techniques are good at capturing semantic information within a corpus.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b23719ec023f20703fbec6be04b53e85.png)'
  prefs: []
  type: TYPE_IMG
- en: 'GloVe word vectors capturing words with similar semantics. Image Source: [Stanford
    GloVe](https://nlp.stanford.edu/projects/glove/).'
  prefs: []
  type: TYPE_NORMAL
- en: BERT — Bidirectional Encoder Representations from Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Introduced](https://arxiv.org/abs/1810.04805) by Google in 2019, [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)
    belongs to a class of NLP-based language algorithms known as [transformers](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html).
    BERT is a massive pre-trained deeply bidirectional encoder-based transformer model
    that comes in two variants. BERT-Base has 110 million parameters, and BERT-Large
    has 340 million parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: For generating word embeddings, BERT relies on an [*attention mechanism*](https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/).
    It generates high-quality context-aware or contextualized word embeddings. During
    the training process, embeddings are refined by passing through each BERT encoder
    layer. For each word, the attention mechanism captures word associations based
    on the words on the left and the words on the right. Word embeddings are also
    positionally encoded to keep track of the pattern or position of each word in
    a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: BERT is more advanced than any of the techniques discussed above. It creates
    better word embeddings as the model is pre-trained on massive word corpus and
    Wikipedia datasets. BERT can be improved by fine-tuning the embeddings on task-specific
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Though, BERT is most suited for language translation tasks. It has been optimized
    for many other applications and domains.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a6c52e356c9b9737bb8e3651c4d79d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Bidirectional BERT architecture. Image Source: [Google AI Blog](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Concluding Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With advancements in NLP, word embedding techniques are also improving. There
    are many NLP tasks that don’t require advanced embedding techniques. Many can
    perform equally well with simple word embedding techniques. The selection of a
    word embedding technique must be based on careful experimentations and task-specific
    requirements. Fine-tuning the word embedding models can improve the accuracy significantly.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we have given a high-level overview of various word embedding
    algorithms. Let’s summarize them below:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Word Embedding Technique** | **Main Characteristics** | **Use Cases** |'
  prefs: []
  type: TYPE_TB
- en: '| TF-IDF | Statistical method to capture the relevance of words w.r.t the corpus
    of text. It does not capture semantic word associations. | Better for information
    retrieval and keyword extraction in documents. |'
  prefs: []
  type: TYPE_TB
- en: '| Word2Vec | Neural network-based CBOW and Skip-gram architectures, better
    at capturing semantic information. | Useful in semantic analysis task. |'
  prefs: []
  type: TYPE_TB
- en: '| GloVe | Matrix factorization based on global word-word co-occurrence. It
    solves the local context limitations of Word2Vec. | Better at word analogy and
    named-entity recognition tasks. Comparable results with Word2Vec in some semantic
    analysis tasks while better in others. |'
  prefs: []
  type: TYPE_TB
- en: '| BERT | Transformer-based attention mechanism to capture high-quality contextual
    information. | Language translation, question-answering system. Deployed in Google
    Search engine to understand search queries. |'
  prefs: []
  type: TYPE_TB
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[https://www.ibm.com/cloud/learn/natural-language-processing](https://www.ibm.com/cloud/learn/natural-language-processing)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.techopedia.com/definition/33012/natural-language-generation-nlg](https://www.techopedia.com/definition/33012/natural-language-generation-nlg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.machinelearningplus.com/nlp/cosine-similarity/](https://www.machinelearningplus.com/nlp/cosine-similarity/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.ibm.com/cloud/learn/neural-networks](https://www.ibm.com/cloud/learn/neural-networks)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.expert.ai/blog/natural-language-process-semantic-analysis-definition/](https://www.expert.ai/blog/natural-language-process-semantic-analysis-definition/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://medium.com/swlh/co-occurrence-matrix-9cacc5dd396e](https://medium.com/swlh/co-occurrence-matrix-9cacc5dd396e)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://blog.marketmuse.com/glossary/latent-semantic-analysis-definition/](https://blog.marketmuse.com/glossary/latent-semantic-analysis-definition/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.investopedia.com/terms/l/least-squares-method.asp](https://www.investopedia.com/terms/l/least-squares-method.asp)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.expert.ai/blog/entity-extraction-work/](https://www.expert.ai/blog/entity-extraction-work/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/](https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**[Neeraj Agarwal](https://www.linkedin.com/in/neeagl/)** is a founder of Algoscale,
    a data consulting company covering data engineering, applied AI, data science,
    and product engineering. He has over 9 years of experience in the field and has
    helped a wide range of organizations from start-ups to Fortune 100 companies ingest
    and store enormous amounts of raw data in order to translate it into actionable
    insights for better decision-making and faster business value.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[The Range of NLP Applications in the Real World: A Different…](https://www.kdnuggets.com/2022/03/different-solution-problem-range-nlp-applications-real-world.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Different Ways to Load Data in Python](https://www.kdnuggets.com/2020/08/5-different-ways-load-data-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AI-Generated Sports Highlights: Different Approaches](https://www.kdnuggets.com/2022/03/aigenerated-sports-highlights-different-approaches.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How is Data Mining Different from Machine Learning?](https://www.kdnuggets.com/2022/06/data-mining-different-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Learn How Different Data Visualizations Work](https://www.kdnuggets.com/2022/09/datacamp-learn-different-data-visualizations-work.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Baize: An Open-Source Chat Model (But Different?)](https://www.kdnuggets.com/2023/04/baize-opensource-chat-model-different.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
