- en: 2 Things You Need to Know about Reinforcement Learning – Computational Efficiency
    and Sample Efficiency
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你需要知道的关于强化学习的2件事——计算效率和样本效率
- en: 原文：[https://www.kdnuggets.com/2020/04/2-things-reinforcement-learning.html](https://www.kdnuggets.com/2020/04/2-things-reinforcement-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/04/2-things-reinforcement-learning.html](https://www.kdnuggets.com/2020/04/2-things-reinforcement-learning.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: The High Cost of Deep Learning
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习的高成本
- en: Have you ever put on a sweater because the air conditioning was too cold? Forgotten
    to turn off the lights in another room before heading to bed? Do you commute to
    work more than 30 minutes every day just for the sake of “filling seats” at the
    office, even though everything you do at work could be done via laptop from home?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾因为空调太冷而穿上毛衣？是否忘记在上床睡觉前关掉另一间房间的灯？你每天通勤超过30分钟只是为了“填补办公室的座位”，尽管你在工作中做的一切都可以通过家里的笔记本完成？
- en: '![](../Images/c9abbeb0f5af3bd45a17a2f8e048b315.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9abbeb0f5af3bd45a17a2f8e048b315.png)'
- en: '*In the counter-intuitive trade-offs between sample and computational efficiency
    in Reinforcement Learning, choosing evolution strategies can be smarter than it
    looks.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*在强化学习中样本效率和计算效率之间的反直觉权衡中，选择进化策略可能比看起来更明智。*'
- en: Modern life is full of inefficiencies small and large, but the energy costs
    of our deep learning research/passion/enterprise are not so immediately obvious.
    With a high-powered workstation tower at your desk, the stream of hot air blown
    out by cooling fans is a constant reminder of the persistence and scale of energy
    being used. But, if you’re sending jobs to a shared [cluster](https://www.exxactcorp.com/Deep-Learning-HPC-Clusters) down
    the hall, it’s a little more difficult to intuitively appreciate the impact of
    your design decisions on your electricity bill. If you use cloud computing for
    big jobs, the energy use and pursuant consequences are even more abstracted away
    from your immediate attention. But out-of-sight, out-of-mind is not a viable policy
    for making decisions about projects that might, at the extreme end of the scale,
    may represent a greater energy expenditure than the entire product cycle of an
    automobile.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现代生活充满了大大小小的低效，但我们深度学习研究/热情/企业的能源成本并不那么明显。有了高性能的工作站塔，散热风扇吹出的热空气始终提醒着能源使用的持续性和规模。但是，如果你把作业发送到走廊里的共享[集群](https://www.exxactcorp.com/Deep-Learning-HPC-Clusters)，直观上感受到设计决策对电费的影响就更困难了。如果你为大型工作使用云计算，能源使用和随之而来的后果更是被抽象化了。但“眼不见，心不烦”并不是在极端情况下，可能代表比整车生命周期更大的能源开销的项目决策的可行政策。
- en: Energy Requirements for Training Deep Learning Models
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练深度学习模型的能源需求
- en: '[Strubell et al.](https://arxiv.org/abs/1906.02243) brought attention to the
    substantial energy requirements of training modern deep learning models in 2019\.
    Although they focused mainly on a large type of natural language processing model
    called a transformer, the considerations and conclusions discussed in their paper
    should hold roughly true for training deep models for other tasks on similar hardware
    for about the same amount of time.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[Strubell et al.](https://arxiv.org/abs/1906.02243)在2019年引起了对训练现代深度学习模型的巨大能源需求的关注。虽然他们主要关注了一种叫做变换器的大型自然语言处理模型，但他们论文中讨论的考虑和结论应该大致适用于在类似硬件上训练其他任务的深度模型，大致时间也相同。'
- en: They estimated training the large variant of [Vaswani et al.’s](https://arxiv.org/abs/1706.03762)
    transformer from scratch emits about 10% of the carbon dioxide equivalent emissions
    as a flight from New York to San Francisco, based on some assumptions about the
    energy mix published by major cloud providers, and a financial cost of just less
    than 1000 USD (assuming cloud compute). For a business-defining project, that’s
    probably a worthwhile expenditure, but when tuning and experimentation are taken
    into account, the energy bill can easily be multiplied by a factor of 10 or more.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 他们估计，从头开始训练大型的[Vaswani et al.](https://arxiv.org/abs/1706.03762)变换器大约排放了相当于从纽约到旧金山的航班约10%的二氧化碳排放，这基于主要云服务提供商发布的能源组合的一些假设，财务成本略低于1000美元（假设使用云计算）。对于一个决定业务方向的项目，这可能是值得的支出，但当考虑到调整和实验时，能源费用很容易被放大10倍或更多。
- en: Strubell and colleagues estimated that adding [Neural Architecture Search (NAS)](https://arxiv.org/abs/1901.11117) to
    NLP model development comes with a price tag in the millions and a carbon footprint
    to match. Although many of the largest models take advantage of dedicated hardware,
    like Google’s TPUs, to train, and that may decrease the energy costs by [30 to
    80X](https://cloud.google.com/blog/products/gcp/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu) and
    the price of training by a slightly lower factor, but that’s still a substantial
    cost. In the realm of reinforcement learning that we are concerned with in this
    article, the consequences of inefficient training can be a dead project, product,
    or business.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Strubell及其同事估计，将[神经架构搜索（NAS）](https://arxiv.org/abs/1901.11117)添加到NLP模型开发中需要数百万美元的费用，并且具有相应的碳足迹。虽然许多最大的模型利用专用硬件，如Google的TPU进行训练，这可能将能源成本降低[30到80倍](https://cloud.google.com/blog/products/gcp/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu)和训练价格降低一个稍低的倍数，但这仍然是一个巨大的成本。在我们关心的强化学习领域中，低效训练的后果可能是一个失败的项目、产品或业务。
- en: DeepMind Training Example Using Starcraft II
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DeepMind使用《星际争霸II》的训练示例
- en: Over a period of 44 days, [Deepmind trained](https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning) agents
    to achieve grandmaster status in all three playable races in the multiplayer real-time
    strategy game Starcraft II, beating out 99.8% of all players in the Battle.net
    rankings. OpenAI’s major game-mastery project for Dota 2 employed 10 real-time
    months (about 800 petaflop/days) of training time to [defeat world champion, human
    players](https://openai.com/blog/openai-five-defeats-dota-2-world-champions/).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在44天的时间里，[Deepmind训练](https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning)
    代理，以在多人实时战略游戏《星际争霸II》中获得三种可玩种族的宗师地位，击败了99.8%的所有玩家。在Battle.net排行榜中名列前茅。OpenAI针对Dota
    2的主要游戏掌握项目使用了10个实时月（大约800拍次/天）的训练时间来[击败世界冠军、人类玩家](https://openai.com/blog/openai-five-defeats-dota-2-world-champions/)。
- en: Due to the use of TPUs, virtual workers, etc., it’s not easy to accurately estimate
    the energy costs for such headline-grabbing game players, but estimates fall in
    the ballpark of a [12](https://medium.com/swlh/deepmind-achieved-starcraft-ii-grandmaster-level-but-at-what-cost-32891dd990e4) to [18
    million USD](https://www.aitrends.com/business/openai-establishes-for-profit-company/) cloud
    compute bill to train champion Alphastar and OpenAI Five, respectively. Clearly,
    that’s out of reach for a typical academic or industry machine learning team.
    There’s another danger arising from inefficient learning in the field of reinforcement
    learning. An agent trying to learn a moderately complex task with bad exploration
    and poor sample efficiency might never find a viable solution.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用了TPU、虚拟工人等，准确估算这些引人注目的游戏玩家的能量成本并不容易，但估算的范围在[12](https://medium.com/swlh/deepmind-achieved-starcraft-ii-grandmaster-level-but-at-what-cost-32891dd990e4)到[1800万美元](https://www.aitrends.com/business/openai-establishes-for-profit-company/)的云计算账单，分别用于训练冠军Alphastar和OpenAI
    Five。显然，这对于典型的学术或行业机器学习团队来说是不可企及的。在强化学习领域中，另一个危险是低效学习的出现。一个试图学习中等复杂任务但探索不佳且样本效率低的代理可能永远找不到可行的解决方案。
- en: Predicting Protein Structure Using Reinforcement Learning (NL)
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用强化学习预测蛋白质结构（NL）
- en: As an example, we can compare moderately complex reinforcement learning tasks
    to the challenge of [predicting protein structure](https://blog.exxactcorp.com/deepminds-protein-folding-upset/) from
    a sequence. Consider a small protein consisting of 100 amino acids linked together,
    like a chain with 100 links. Each link is a specific one out of 21 unique link
    variants, and the structure will take shape based on the angles between each link
    in turn. Assuming a simplified 9 possible configurations for the bond angles between
    each amino acid/chain link, it would take
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个例子，我们可以将中等复杂度的强化学习任务与[预测蛋白质结构](https://blog.exxactcorp.com/deepminds-protein-folding-upset/)的挑战进行比较。考虑一个由100个氨基酸链连接在一起的小型蛋白质，就像一个有100个链环的链条。每个链环是21种独特链环变体中的一种，结构将根据每个链环之间的角度形成。假设简化为9种可能的配置用于每个氨基酸/链环之间的键角，那么需要
- en: '*999 = 29,512,665,430,652,752,148,753,480,226,197,736,314,359, 272,517,043,832,886,063,884,637,676,943,433,478,020,332,709,411,004,889'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*999 = 29,512,665,430,652,752,148,753,480,226,197,736,314,359,272,517,043,832,886,063,884,637,676,943,433,478,020,332,709,411,004,889'
- en: (that rounds to a 3 followed by 94 zeros) *
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: （这大约是一个3后面跟着94个零）*
- en: iterations to see each possible structure. That 100 link protein is roughly
    analogous to a reinforcement learning environment with 100 time steps and 9 possible
    actions at each step. *Talk about a combinatorial explosion. *
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代以查看每个可能的结构。那个有100个链接的蛋白质大致相当于一个具有100个时间步和每个步骤9个可能动作的强化学习环境。*谈到组合爆炸。*
- en: Of course, a reinforcement learning agent doesn’t set out to randomly sample
    all possible game states. Instead, an agent will generate learning trajectories
    through some combination of making their best guess, directed exploration, and
    random search. But that method of generating experience, typical of what we call
    “on-policy” learning algorithms, can cut both ways. An agent that finds a local
    optimum approach to a given reinforcement learning task might just stay there
    forever, repeating the same mistakes and never solving the overall problem.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，强化学习代理并不是随机采样所有可能的游戏状态。相反，代理将通过某种组合的最佳猜测、定向探索和随机搜索来生成学习轨迹。但这种生成经验的方法，典型的“策略”学习算法，可能会有两面性。一个找到局部最优解的代理可能会一直停留在那儿，重复同样的错误，永远无法解决整体问题。
- en: Historically reinforcement learning has benefitted from trying to formulate
    the problem to resemble supervised learning wherever possible, such as sending
    students hiking with three cameras strapped to their head. The left camera generates
    an image with a training label to “turn right,” while the right camera is labeled
    “go left” and the center camera “straight ahead.”
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，强化学习一直受益于尽可能将问题形式化为类似于监督学习的方式，例如让学生背上三台相机去远足。左侧相机生成一个标记为“向右转”的图像，而右侧相机标记为“向左走”，中间相机则标记为“直行”。
- en: In the end, you would have a labeled dataset suitable for training a [quadcopter
    to navigate forest trails](https://www.youtube.com/watch?v=umRdt3zGgpU). In the
    sections below, we’ll discuss how similar concepts (e.g., imitation learning)
    and model-based RL can vastly decrease the number of training examples necessary
    for an agent to learn a task and why that is not always a good thing.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你将拥有一个适合训练[四旋翼无人机导航森林小径](https://www.youtube.com/watch?v=umRdt3zGgpU)的标记数据集。在下面的章节中，我们将讨论类似的概念（例如模仿学习）和基于模型的强化学习如何大幅减少学习任务所需的训练样本数量，以及为什么这并不总是好事。
- en: 'Doing More with Less: Reinforcement Learning Agents Learn Best from Examples'
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 少量更多：强化学习代理从示例中学习最佳
- en: Deep reinforcement learning is a branch of machine learning inspired by fields
    as diverse as animal and human cognition, optimal control, and deep learning.
    There are obvious analogs to animal behavior experiments, where an animal (agent)
    is put in a situation where it must learn to solve a problem to get a food treat
    (reward). It only takes a few examples before an animal starts to associate a
    series of actions with a reward, but deep reinforcement learning algorithms may
    need to consider [10 to 100 thousand time steps](https://youtu.be/8EcdaCk9KaQ?t=736) per
    epoch in order to make stable updates to the agent’s parameters.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习是机器学习的一个分支，灵感来源于动物和人类认知、最优控制和深度学习等多种领域。显然，它与动物行为实验有类似之处，其中一个动物（代理）被置于一个必须学习解决问题以获得食物奖励的情境中。动物只需几个例子便开始将一系列动作与奖励关联起来，但深度强化学习算法可能需要考虑[每个周期10到100千个时间步](https://youtu.be/8EcdaCk9KaQ?t=736)以便对代理的参数进行稳定更新。
- en: Most reinforcement learning environments are formulated in steps. The environment
    generates an observation, based upon which the agent decides an action that is
    applied to the environment. The environment makes an update based on its current
    state and the action chosen by the agent in what we refer to as a time step throughout
    this article. The fewer time steps, or “samples,” needed to learn, the more efficient
    the algorithm is.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数强化学习环境是以步骤形式构建的。环境生成一个观察值，基于此，代理决定一个对环境施加的动作。环境根据其当前状态和代理选择的动作进行更新，在本文中我们称之为一个时间步。学习所需的时间步或“样本”越少，算法就越高效。
- en: An important aspect of most modern reinforcement learning algorithms is that
    they are at their core, a form of trial-and-error. For an agent to learn without
    employing explicit exploration techniques, [random activity should occasionally
    do something right](https://youtu.be/8EcdaCk9KaQ?t=417), otherwise, an agent may
    interact with an environment for (essentially) forever without ever seeing a positive
    reward. In fact, proximal policy optimization, an algorithm that achieves competitive
    performance in a range of benchmarks, [completely fails](https://arxiv.org/abs/1912.01588) in
    OpenAI’s procedurally generated environment suite in hard exploration mode. We
    can imagine how this is a problem for a wide range of relevant tasks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代强化学习算法的一个重要方面是，它们本质上是一种试错法。为了让代理在没有采用显式探索技术的情况下进行学习，[随机活动偶尔应该做对一些事情](https://youtu.be/8EcdaCk9KaQ?t=417)，否则代理可能会与环境互动（基本上）永远而得不到正向奖励。事实上，近端策略优化算法，在多个基准中表现优异，[在OpenAI的程序生成环境套件的困难探索模式中完全失败](https://arxiv.org/abs/1912.01588)。我们可以想象这对广泛相关任务是一个问题。
- en: An agent learning to play a video game will probably experience a positive reward
    with random actions (aka button-mashing), and that is one reason why the Atari
    suite is a popular reinforcement learning benchmark. For more complex tasks like
    figuring out how to tie a complex knot, it’s extremely unlikely that a robot will
    come across a solution by accident. No matter how much random interaction is allowed,
    arriving at the desired outcome is just too unlikely.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一个学习玩视频游戏的代理可能会通过随机动作（即按键乱按）获得正向奖励，这也是为什么Atari套件是流行的强化学习基准之一。对于像解决复杂打结问题这样的复杂任务，机器人偶然遇到解决方案的可能性极低。无论允许多少随机互动，实现期望结果都太不可能了。
- en: As we’ve seen before in our discussion of [learning Rubik’s cube manipulation](https://blog.exxactcorp.com/timeline-review-of-openais-robotic-hand-project/),
    there are [custom knot tying robots](https://www.youtube.com/watch?v=erNi07dH5pw) built
    and programmed manually to create knots, but a reinforcement learning agent that
    learns to do so from scratch remains out of reach. It goes without saying that
    humans don’t learn knot-tying from scratch either. A toddler left alone in a room
    full of untied sneakers will probably have a hard time discovering the standard [“bunny
    rabbit” knot](https://www.youtube.com/watch?v=erNi07dH5pw) on their own. Like
    the toddler in the thought experiment, reinforcement learning agents can learn
    better from examples, even for complicated tasks like making knots.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在讨论[学习魔方操作](https://blog.exxactcorp.com/timeline-review-of-openais-robotic-hand-project/)时所见，已经有[定制的打结机器人](https://www.youtube.com/watch?v=erNi07dH5pw)被手动构建和编程以创建结，但从零开始学习打结的强化学习代理仍然无法实现。不言而喻，人类也不会从零开始学习打结。一个被独自留在充满未系鞋带的房间里的小孩，可能会很难自己发现标准的[“兔子结”](https://www.youtube.com/watch?v=erNi07dH5pw)。就像思维实验中的小孩一样，强化学习代理可以通过示例学得更好，即使是像打结这样的复杂任务。
- en: Example of Teaching Machines to Tie Knots Using Reinforcement Learning & More
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用强化学习及其他方法教机器打结的示例
- en: By combining autonomous learning of rope physics with human demonstrations,
    researchers at Berkeley were able to tackle the problem of [teaching machines
    to tie knots](https://arxiv.org/pdf/1703.02018.pdf). First, and this part may
    be simulated, the robot interacts with a rope on a table surface mostly at random,
    with the intent of learning a reasonable model of how their limited view of the
    world works. Following the agent’s autonomous learning tenure, it is shown the
    desired action of tying a simple knot. When all goes well, the agent is able to
    mimic the desired action. A mix of autonomously learning a dynamics model of the
    world (in the example above, a table surface with a rope) and human examples of
    desired behavior is a powerful combination. Imitation learning and the related
    inverse reinforcement learning represent some of the most sample-efficient approaches
    to RL out there.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将绳索物理的自主学习与人工示范相结合，伯克利的研究人员成功解决了[教机器打结](https://arxiv.org/pdf/1703.02018.pdf)的问题。首先，机器人在桌面上随机地与绳子互动，目的是学习一个合理的模型，了解它们对世界的有限视角是如何运作的。经过自主学习阶段后，机器人会被展示打简单结的期望动作。当一切顺利时，机器人能够模仿期望的动作。将自主学习的世界动态模型（如上例中的桌面和绳子）与人类示例的期望行为结合是一种强大的组合。模仿学习及相关的逆强化学习代表了目前一些最有效的强化学习方法。
- en: Knot-tying is a slightly esoteric task (and plainly outside the capabilities
    of many learning algorithms), but we can compare the sample efficiency of different
    learning algorithms applied to more standard tasks. Lecture 21, slide 39 from
    Sergey Levine’s deep RL course at Berkeley ([pdf](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-21.pdf)),
    gives us an idea of the relative sample efficiencies for various RL algorithms
    by comparing published results from the HalfCheetah task. HalfCheetah is a 2D
    locomotion task for a simulated robot vaguely resembling half a fast feline. Implementations
    of HalfCheetah are available in both the Mujoco (requires paid license) and [Bullet](https://github.com/bulletphysics/bullet3/releases) (open
    source, free) physics simulators.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 绑结是一项略显冷门的任务（显然超出了许多学习算法的能力），但我们可以比较应用于更标准任务的不同学习算法的样本效率。来自 Sergey Levine 在伯克利的深度
    RL 课程的讲座 21，第 39 张幻灯片 ([pdf](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-21.pdf))，通过比较
    HalfCheetah 任务的已发表结果，给我们提供了各种 RL 算法的相对样本效率的一个概念。HalfCheetah 是一个 2D 运动任务，模拟了一个大致类似于一半快的猫的机器人。HalfCheetah
    的实现可以在 Mujoco（需要付费许可）和 [Bullet](https://github.com/bulletphysics/bullet3/releases)（开源，免费）物理模拟器中找到。
- en: According to the slide, we expect evolution strategies to be the most sample
    inefficient and model-based/inverse reinforcement learning to be the most efficient
    with respect to the total number of time steps. The estimates from the slide are
    reproduced below, along with specific examples from the literature.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 根据幻灯片，我们预计进化策略在样本效率方面最为低效，而基于模型/逆强化学习在总时间步数方面最为高效。下面重新列出了幻灯片中的估计值，并附上了文献中的具体示例。
- en: '![](../Images/e8e671c1f2d44a829211ecaf0c78129a.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8e671c1f2d44a829211ecaf0c78129a.png)'
- en: '*The HalfCheetah robot simulated in PyBullet, where the objective is to move
    forward as quickly as possible. Although rendered in 3D, movement is constrained
    to a 2D plane. It’s like a cheetah, but half.*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*HalfCheetah 在 PyBullet 中模拟的机器人，其目标是尽可能快地向前移动。尽管以 3D 渲染，但运动受到限制在 2D 平面上。它像一只猎豹，但只有一半。*'
- en: '| **Method** | *Estimated steps from [CS 285 lec 21](http://rail.eecs.berkeley.edu/deeprlcourse/)*
    | *Example time steps to solve HalfCheetah* |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| **方法** | *来自 [CS 285 lec 21](http://rail.eecs.berkeley.edu/deeprlcourse/)*
    的*估计步骤* | *解决 HalfCheetah 的示例时间步数* |'
- en: '| **Evolution strategies** | ~1,000,000,000 | ~3,000,000 [(ES, Salimans et
    al. 2016)](https://arxiv.org/abs/1703.03864) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| **进化策略** | ~1,000,000,000 | ~3,000,000 [(ES, Salimans 等, 2016)](https://arxiv.org/abs/1703.03864)
    |'
- en: '| **Fully online actor-critic methods** | ~100,000,000 | ~100,000,000[ (Trust-A3C,
    Wang et al. 2016)](https://arxiv.org/abs/1611.01224) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **全在线演员-评论家方法** | ~100,000,000 | ~100,000,000[ (Trust-A3C, Wang 等, 2016)](https://arxiv.org/abs/1611.01224)
    |'
- en: '| **Policy gradients** | ~10,000,000 | ~1,000,000 [(PPO, Schulman et al. 2017)](https://arxiv.org/abs/1707.06347)
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| **策略梯度** | ~10,000,000 | ~1,000,000 [(PPO, Schulman 等, 2017)](https://arxiv.org/abs/1707.06347)
    |'
- en: '| **Value function (off-policy) methods** | ~1,000,000 | ~1,500,000 [(Gu et
    al. 2018)](https://arxiv.org/abs/1603.00748) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| **值函数（离线）方法** | ~1,000,000 | ~1,500,000 [(Gu 等, 2018)](https://arxiv.org/abs/1603.00748)
    |'
- en: '| (**model-based)** | ~30,000 | ~400,000 [(PE-TS, Kurtland Chua et al. 2018)](https://arxiv.org/abs/1805.12114)
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| (**基于模型**) | ~30,000 | ~400,000 [(PE-TS, Kurtland Chua 等, 2018)](https://arxiv.org/abs/1805.12114)
    |'
- en: '*Table 1: Relative sample efficiency of various RL algorithm families.*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 1: 各种 RL 算法家族的相对样本效率。*'
- en: Sample efficiency varies widely between implementations within the same class
    of algorithm, and I found that the estimates from the slide can be somewhat exaggerated
    with respect to specific examples in the literature. In particular, OpenAI’s [evolutionary
    strategies](https://arxiv.org/abs/1703.03864) paper actually reported a better
    sample efficiency than TRPO, a policy gradient method that they used as a comparison.
    Their reported solution for HalfCheetah took 3 million timesteps, a far cry from
    Levine’s estimate of 1 billion.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 样本效率在同一类算法的实现之间差异很大，我发现幻灯片中的估计值在文献中的具体示例中可能有些夸张。特别是，OpenAI 的 [进化策略](https://arxiv.org/abs/1703.03864) 论文实际上报告了比
    TRPO 更好的样本效率，TRPO 是他们用作比较的策略梯度方法。他们报告的 HalfCheetah 解决方案需要 300 万时间步数，这与 Levine
    估计的 10 亿相去甚远。
- en: On the other hand, a related [paper published by Uber AI labs](https://arxiv.org/abs/1712.06567) that
    applied genetic algorithms to Atari games found sample efficiencies that were
    around the 1 billion time steps mark, about 5 times greater than the value-function
    algorithm they compared with and in line with Levine’s estimates from the course.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，由 Uber AI 实验室发布的[相关论文](https://arxiv.org/abs/1712.06567)应用遗传算法于 Atari 游戏中发现样本效率大约在10亿时间步数左右，约为他们比较的值函数算法的5倍，与
    Levine 的课程估计一致。
- en: More is Less in Some Cases – Learning from Evolution
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在某些情况下，多即是少——从进化中学习
- en: In the example discussed above, evolutionary strategies are among the most sample-inefficient
    methods available, typically requiring at least 10 times more steps to learn a
    given task than other approaches. On the other end of the spectrum, model-based
    and imitation methods require the least amount of time steps to learn the same
    task.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述讨论的例子中，进化策略是最样本效率低下的方法之一，通常需要比其他方法多出至少10倍的步骤来学习一个给定任务。在另一端，基于模型和模仿的方法需要最少的时间步数来学习相同的任务。
- en: At first glance, it appears to be an open and shut case against the utility
    of evolution-based methods, but a funny thing happens when you optimize for compute
    instead of sample efficiency. Due to the decreased overhead of running evolutionary
    strategies, which don’t even require a backpropagation pass, they can actually
    require [less compute](https://youtu.be/tzieElmtAjs?list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&t=3949).
    They are also inherently parallelizable. Because each episode or individual agent
    in a population does not depend on the results of other agents/episodes, the learning
    algorithm becomes [embarrassingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel).
    With a fast simulator, it may take substantially less time (as measured by the
    clock on your wall) to solve a given problem.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 初看起来，这似乎是一个对进化方法实用性的明确否定，但当你优化计算而不是样本效率时，会发生有趣的事情。由于运行进化策略的开销较小，这些策略甚至不需要反向传播，它们实际上可能需要[更少的计算](https://youtu.be/tzieElmtAjs?list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&t=3949)。它们也天生具有并行性。因为一个种群中的每个回合或个体都不依赖于其他代理/回合的结果，所以学习算法变得[显然可以并行](https://en.wikipedia.org/wiki/Embarrassingly_parallel)。有了一个快速的模拟器，解决一个给定问题可能需要的时间（按墙上的时钟计算）会大大减少。
- en: '![](../Images/83f38ef1d2b6751c657e82a87a10375a.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83f38ef1d2b6751c657e82a87a10375a.png)'
- en: '*The *InvertedPendulumSwingup* task start and goal state. The green capsule
    slides along the yellow rod, with the goal of bringing the unactuated red pole
    to balance upright.*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*InvertedPendulumSwingup* 任务的起始和目标状态。绿色胶囊沿黄色杆滑动，目标是将未驱动的红色杆保持直立。'
- en: To give a rough comparison of computational efficiency, I ran several open source
    implementations of various learning algorithms on a cart-pole swing-up task InvertedPendulumSwingupBulletEnv-v0 from [PyBullet](https://pybullet.org/wordpress/).
    Code for the representative evolutionary algorithm, covariance matrix adaptation,
    is [available on Github](http://bit.ly/exxact_cma), while the other algorithms
    are all part of OpenAI’s [Spinning Up in Deep RL](https://spinningup.openai.com/) resource.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了粗略比较计算效率，我在[PyBullet](https://pybullet.org/wordpress/)的一个倒立摆摆动任务 InvertedPendulumSwingupBulletEnv-v0
    上运行了几种开源实现的各种学习算法。代表性的进化算法协方差矩阵自适应的代码[可以在 Github 上找到](http://bit.ly/exxact_cma)，而其他算法都属于
    OpenAI 的[Spinning Up in Deep RL](https://spinningup.openai.com/)资源。
- en: 'The policy architectures were all kept the same: a feed-forward dense neural
    network with two hidden layers of 16 neurons each. I ran all algorithms on a single
    core of a modest Intel i5 2.4 GHz CPU, so the results can only really be compared
    to each other, and the differences in training time are without any parallelization
    speedup. It’s worth noting that OpenAI managed to train the MujoCo humanoid in [10
    minutes on 1,440 workers ](https://openai.com/blog/evolution-strategies/)with
    evolution strategies, so parallelization speedups are real and lucrative if you
    have the CPU cores for it.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 政策架构保持不变：一个前馈密集型神经网络，具有两个每层16个神经元的隐藏层。我在一台中等配置的 Intel i5 2.4 GHz CPU 的单核心上运行了所有算法，因此结果只能互相比较，训练时间的差异没有任何并行加速。值得注意的是，OpenAI
    使用进化策略在[10分钟内用1440个工人](https://openai.com/blog/evolution-strategies/)训练了 MujoCo
    人形机器人，因此如果你有足够的 CPU 核心，平行加速是真实且有利可图的。
- en: '| *Method family* | *Algorithm* | *Time steps to solve* | *Time to solve (seconds)*
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| *方法家族* | *算法* | *解决所需时间步数* | *解决时间（秒）* |'
- en: '| **Evolution strategies** | [Covariance Matrix Adaptation](https://en.wikipedia.org/wiki/CMA-ES)[[code](http://bit.ly/exxact_cma)]
    | ~4,100,000 | ~500 s |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| **进化策略** | [协方差矩阵自适应](https://en.wikipedia.org/wiki/CMA-ES)[[代码](http://bit.ly/exxact_cma)]
    | ~4,100,000 | ~500 s |'
- en: '| **Policy gradients** | [Proximal Policy Optimization](https://spinningup.openai.com/en/latest/algorithms/ppo.html)[[code](https://github.com/openai/spinningup)]
    | ~1,800,000 | ~2000 s |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| **策略梯度** | [近端策略优化](https://spinningup.openai.com/en/latest/algorithms/ppo.html)[[代码](https://github.com/openai/spinningup)]
    | ~1,800,000 | ~2000 s |'
- en: '| **Fully online actor-critic methods** | [Soft Actor-Critic](https://spinningup.openai.com/en/latest/algorithms/sac.html)[[code](https://github.com/openai/spinningup)]
    | ~1,500,000 | ~17,000 s |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| **完全在线演员-评论家方法** | [软演员-评论家](https://spinningup.openai.com/en/latest/algorithms/sac.html)[[代码](https://github.com/openai/spinningup)]
    | ~1,500,000 | ~17,000 s |'
- en: '| **Value function (off-policy) methods** | [Twin-Delayed DDPG](https://spinningup.openai.com/en/latest/algorithms/td3.html)[[code](https://github.com/openai/spinningup)]
    | ~2,000,000 | ~17,500 s |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| **值函数（离策略）方法** | [双延迟DDPG](https://spinningup.openai.com/en/latest/algorithms/td3.html)[[代码](https://github.com/openai/spinningup)]
    | ~2,000,000 | ~17,500 s |'
- en: '*Table 2: Comparing different learning methods wall-clock time and sample efficiency. *'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*表2：比较不同学习方法的实际时间和样本效率。*'
- en: The table above gives a good idea of the relative relationship between the required
    resources for different methods. The most sample efficient algorithm (soft actor-critic)
    required about half as many steps but consumed 34 times more CPU time. Twin-delayed
    DDPG (aka TD3) and soft actor-critic were less stable than the other methods.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 上表很好地展示了不同方法所需资源的相对关系。样本效率最高的算法（软演员评论家）所需步骤约为其他方法的一半，但消耗的CPU时间是其34倍。双延迟DDPG（也称为TD3）和软演员评论家比其他方法稳定性差。
- en: We didn’t look at model-based RL algorithms when running experiments for Table
    2 due to a lack of open source tools. Research progress in model-based RL suffers
    from closed source development and a lack of standard environments, but [Wang
    et al.](https://arxiv.org/abs/1907.02057) made a heroic effort to provide benchmarks
    for many model-based algorithms. They found that most model-based methods were
    able to solve a task they called Pendulum, very similar to the task reported in
    the table above, in about 25,000 steps. Compared to policy gradient methods, training
    (wall-clock) time was about 100 to 200 times longer for most model-based methods
    they investigated.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缺乏开源工具，我们在运行表2实验时没有考虑基于模型的RL算法。基于模型的RL的研究进展受限于闭源开发和缺乏标准环境，但[Wang等](https://arxiv.org/abs/1907.02057)做出了英雄般的努力，为许多基于模型的算法提供了基准。他们发现，大多数基于模型的方法能够在大约25,000步内解决一个他们称之为Pendulum的任务，该任务与上表中报告的任务非常相似。与策略梯度方法相比，他们调查的多数基于模型的方法的训练（实际）时间大约长100到200倍。
- en: 'Although deep learning still seems to favor “fancy” approaches to reinforcement
    learning, evolution-based methods have been making a comeback. OpenAI’s “[Evolution
    Strategies as a Scalable Alternative to Reinforcement Learning](https://openai.com/blog/evolution-strategies/)”
    from 2016 was joined in 2017 by a large-scale demonstration of genetic algorithms [learning
    to play Atari games](https://eng.uber.com/deep-neuroevolution). Without all the
    fancy mathematical trappings surrounding advanced RL algorithms, evolutionary
    algorithms are conceptually straightforward: generate a population and set the
    individual to some task, keep the best performers to seed the next generation,
    and repeat until a performance threshold is met. Due to the somewhat simple concepts
    behind them and their inherent parallelization potential, you may find that evolutionary
    algorithms will yield substantial savings in what is arguably your most scarce
    resource: developer time.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习仍然偏爱“华丽”的强化学习方法，但基于进化的方法正在复兴。OpenAI的“[进化策略作为一种可扩展的强化学习替代方案](https://openai.com/blog/evolution-strategies/)”于2016年发布，2017年则有大规模的遗传算法[学习玩Atari游戏](https://eng.uber.com/deep-neuroevolution)的展示。与所有复杂的数学装饰不同，进化算法在概念上很简单：生成一个种群并设置个体任务，保留表现最好的个体用于种群繁殖，并重复直到达到性能阈值。由于其相对简单的概念和固有的并行化潜力，你可能会发现进化算法能在你的最稀缺资源——开发者时间上带来显著节省。
- en: There Are Some Caveats to Know About Compute & Sample Efficiency
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有一些需要了解的计算和样本效率的警示
- en: '![](../Images/0247a876640cfeb39f5186b8fbf18dde.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0247a876640cfeb39f5186b8fbf18dde.png)'
- en: Direct comparisons of compute and sample efficiency between different RL algorithms
    on a single task isn’t entirely fair due to the many variables which can differ
    between implementations. One algorithm may converge earlier but never reach the
    same score as a different, slower algorithm, and in general, you can bet that
    in any published RL work, the authors spent a lot more effort optimizing and experimenting
    with their new method than any of the comparison algorithms.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在单一任务上直接比较不同 RL 算法的计算和样本效率并不完全公平，因为不同实现之间存在许多变量。一个算法可能更早收敛，但从未达到与其他较慢算法相同的分数，通常，你可以打赌，在任何发布的
    RL 工作中，作者花费了比任何比较算法更多的精力来优化和实验他们的新方法。
- en: However, the examples we discussed above give us a good idea of what to expect
    from reinforcement learning agents. Model-based methods, while promising, are
    not mature compared to other options and may not be worth the additional effort
    for most tasks. Perhaps surprisingly, evolutionary algorithms perform well on
    a variety of tasks and shouldn’t be ignored because of the perception they are
    “old-fashioned” or “too simple.” When considering the value of all available resources
    and their consumption by available methods, evolutionary algorithms may indeed
    offer the best return.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们上面讨论的例子给了我们对强化学习代理的预期一个很好的概念。虽然基于模型的方法有前景，但与其他选项相比尚不成熟，对于大多数任务可能不值得额外的努力。也许令人惊讶的是，进化算法在各种任务上表现良好，不应因为被认为是“过时”或“过于简单”而被忽视。在考虑所有可用资源的价值及其被现有方法消耗的情况下，进化算法确实可能提供最佳回报。
- en: '[Original](https://blog.exxactcorp.com/2-things-you-need-to-know-about-reinforcement-learning/).
    Reposted with permission.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://blog.exxactcorp.com/2-things-you-need-to-know-about-reinforcement-learning/)。经许可转载。'
- en: '**Related:**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[We Created a Lazy AI](https://www.kdnuggets.com/2020/01/created-lazy-ai.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我们创建了一个懒惰的 AI](https://www.kdnuggets.com/2020/01/created-lazy-ai.html)'
- en: '[The Reinforcement-Learning Methods that Allow AlphaStar to Outcompete Almost
    All Human Players at StarCraft II](https://www.kdnuggets.com/2019/11/reinforcement-learning-methods-alphastar-outcompete-human-players-starcraft.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使 AlphaStar 在《星际争霸 II》中超越几乎所有人类玩家的强化学习方法](https://www.kdnuggets.com/2019/11/reinforcement-learning-methods-alphastar-outcompete-human-players-starcraft.html)'
- en: '[Three Things to Know About Reinforcement Learning](https://www.kdnuggets.com/2019/10/mathworks-reinforcement-learning.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于强化学习的三件事](https://www.kdnuggets.com/2019/10/mathworks-reinforcement-learning.html)'
- en: '* * *'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的 IT 工作'
- en: '* * *'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Calculate Computational Efficiency of Deep Learning Models with…](https://www.kdnuggets.com/2023/06/calculate-computational-efficiency-deep-learning-models-flops-macs.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[计算深度学习模型的计算效率…](https://www.kdnuggets.com/2023/06/calculate-computational-efficiency-deep-learning-models-flops-macs.html)'
- en: '[6 Things You Need To Know About Data Management And Why It Matters…](https://www.kdnuggets.com/2022/05/6-things-need-know-data-management-matters-computer-vision.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于数据管理及其重要性的 6 件事](https://www.kdnuggets.com/2022/05/6-things-need-know-data-management-matters-computer-vision.html)'
- en: '[5 Things You Need to Know When Building LLM Applications](https://www.kdnuggets.com/2023/08/5-things-need-know-building-llm-applications.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[构建 LLM 应用程序时需要知道的 5 件事](https://www.kdnuggets.com/2023/08/5-things-need-know-building-llm-applications.html)'
- en: '[7 Things You Didn''t Know You Could do with a Low Code Tool](https://www.kdnuggets.com/2022/09/7-things-didnt-know-could-low-code-tool.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你不知道的低代码工具的 7 个功能](https://www.kdnuggets.com/2022/09/7-things-didnt-know-could-low-code-tool.html)'
- en: '[KDnuggets News, April 13: Python Libraries Data Scientists Should…](https://www.kdnuggets.com/2022/n15.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，4 月 13 日：数据科学家应该了解的 Python 库](https://www.kdnuggets.com/2022/n15.html)'
- en: '[Things You Should Know When Scaling Your Web Data-Driven Product](https://www.kdnuggets.com/2023/08/things-know-scaling-web-datadriven-product.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[扩展你的网络数据驱动产品时应该了解的事项](https://www.kdnuggets.com/2023/08/things-know-scaling-web-datadriven-product.html)'
