- en: Are You Still Using Pandas to Process Big Data in 2021? Here are two better
    options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/03/pandas-big-data-better-options.html](https://www.kdnuggets.com/2021/03/pandas-big-data-better-options.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Roman Orac](https://www.linkedin.com/in/romanorac/), Data Scientist**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a59832f76b253613a4a80482402949fc.png)'
  prefs: []
  type: TYPE_IMG
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '*Photo by [NASA](https://unsplash.com/@nasa?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral).*'
  prefs: []
  type: TYPE_NORMAL
- en: 'I recently wrote two introductory articles about processing Big Data with [Dask](https://towardsdatascience.com/are-you-still-using-pandas-for-big-data-12788018ba1a) and [Vaex](https://towardsdatascience.com/how-to-process-a-dataframe-with-billions-of-rows-in-seconds-c8212580f447) —
    libraries for processing bigger than memory datasets. While writing, a question
    popped up in my mind:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Can these libraries really process bigger than memory datasets, or is it
    all just a sales slogan?***'
  prefs: []
  type: TYPE_NORMAL
- en: This intrigued meto do a practical experiment with Dask and Vaex and try to
    process a bigger than memory dataset. The dataset was so big that you cannot even
    open it with pandas.
  prefs: []
  type: TYPE_NORMAL
- en: What do I mean by Big Data?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/b05d980e9599377d490e9e4c223bf940.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Photo by [ev](https://unsplash.com/@ev?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral).*'
  prefs: []
  type: TYPE_NORMAL
- en: Big Data is a loosely defined term, which has as many definitions as there are
    hits on Google. In this article, I use the term to describe a dataset that is
    so big that we need specialized software to process it. With Big, I am referring
    to “bigger than the main memory on a single machine.”
  prefs: []
  type: TYPE_NORMAL
- en: '*Definition from Wikipedia:*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Big data is a field that treats ways to analyze, systematically extract information
    from, or otherwise deal with data sets that are too large or complex to be dealt
    with by traditional data-processing application software.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What are Dask and Vaex?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/e319d9a66acd1c1cb7cdc578de9de774.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Photo by [JESHOOTS.COM](https://unsplash.com/@jeshoots?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral).*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dask** provides advanced parallelism for analytics, enabling performance
    at scale for the tools you love. This includes numpy, pandas, and sklearn. It
    is open-source and freely available. It uses existing Python APIs and data structures
    to make it easy to switch between Dask-powered equivalents.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vaex** is a high-performance Python library for lazy Out-of-Core DataFrames
    (similar to Pandas) to visualize and explore big tabular datasets. It can calculate
    basic statistics for more than a billion rows per second. It supports multiple
    visualizations allowing interactive exploration of big data.'
  prefs: []
  type: TYPE_NORMAL
- en: Dask and Vaex Dataframes are not fully compatible with Pandas Dataframes, but
    some most common “data wrangling” operations are supported by both tools. Dask
    is more focused on scaling the code to compute clusters, while Vaex makes it easier
    to work with large datasets on a single machine.
  prefs: []
  type: TYPE_NORMAL
- en: The Experiment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/4da59230f2fce0a9b76164579c4ad52f.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Photo by [Louis Reed](https://unsplash.com/@_louisreed?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral).*'
  prefs: []
  type: TYPE_NORMAL
- en: I generated two CSV files with 1 million rows and 1000 columns. The size of
    a file was 18.18 GB, which is 36.36 GB combined. Files have random numbers from
    a Uniform distribution between 0 and 100.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b82f45e3ed2c9ba9fd4563b24b34750.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Two CSV files with random data. Photo made by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f3bb04e9acfcf77f4b3132f4998b3bf3.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Head of a file. Photo made by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: The experiment was run on a MacBook Pro with 32 GB of main memory — quite a
    beast. When testing the limits of a pandas Dataframe, I surprisingly found that
    reaching a Memory Error on such a machine is quite a challenge!
  prefs: []
  type: TYPE_NORMAL
- en: macOS starts dumping data from the main memory to SSD when the memory is running
    near its capacity. The upper limit for pandas Dataframe was 100 GB of free disk
    space on the machine.
  prefs: []
  type: TYPE_NORMAL
- en: When your Mac needs memory, it will push something that isn’t currently being
    used into a swapfile for temporary storage. When it needs access again, it will
    read the data from the swap file and back into memory.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I’ve spent some time thinking about how I should address this issue so that
    the experiment would be fair. The first idea that came to my mind was to disable
    swapping so that each library would have only the main memory available — good
    luck with that on macOS. After spending a few hours, I wasn’t able to disable
    swapping.
  prefs: []
  type: TYPE_NORMAL
- en: The second idea was to use a brute force approach. I’ve filled the SSD to its
    full capacity so that the operating system couldn’t use swap as there was no free
    space left on the device.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5594b5fa18db9949a1b52d09e74538b8.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Your disk is almost full notification during the experiment. Photo made by
    the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: This worked! pandas couldn’t read two 18 GB files, and Jupyter Kernel crashed.
  prefs: []
  type: TYPE_NORMAL
- en: If I performed this experiment again, I would create a virtual machine with
    less memory. That way, it would be easier to show the limits of these tools.
  prefs: []
  type: TYPE_NORMAL
- en: Can Dask or Vaex help us and process these large files? Which one is faster?
    Let’s find out.
  prefs: []
  type: TYPE_NORMAL
- en: Vaex vs. Dask
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/722af4abc5eb2948aa97ad5840048134.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Photo by [Frida Bredesen](https://unsplash.com/@fridooh?utm_source=medium&utm_medium=referral) on [Unsplash](https://www.kdnuggets.com/2021/02/data-science-learning-roadmap-2021.html).*'
  prefs: []
  type: TYPE_NORMAL
- en: 'When designing the experiment, I thought about basic operations when performing
    Data Analysis, like grouping, filtering, and visualizing data. I came up with
    the following operations:'
  prefs: []
  type: TYPE_NORMAL
- en: calculating 10th quantile of a column,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: adding a new column,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: filtering by column,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: grouping by column and aggregating,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: visualizing a column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All of the above operations perform a calculation using a single column, e.g.:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'So I was intrigued to try an operation, which requires all data to be processed:'
  prefs: []
  type: TYPE_NORMAL
- en: calculate the sum of all of the columns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can be achieved by breaking down the calculation into smaller chunks. E.g.,
    reading each column separately and calculating the sum, and in the last step calculating
    the overall sum. These types of computational problems are known as [Embarrassingly
    parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel) — no effort is
    required to separate the problem into separate tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Vaex
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/bf4fceb8c4267f23beeb74631c57105a.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Photo by [Photos by Lanty](https://unsplash.com/@photos_by_lanty?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral).*'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with Vaex. The experiment was designed in a way that follows best
    practices for each tool — this is using binary format HDF5 for Vaex. So we need
    to convert CSV files to HDF5 format (The Hierarchical Data Format version 5).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Vaex needed 405 seconds to covert two CSV files (36.36 GB) to two HDF5 files,
    which have 16 GB combined. Conversion from text to binary format reduced the file
    size.
  prefs: []
  type: TYPE_NORMAL
- en: '**Open HDF5 dataset with Vaex:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Vaex needed 1218 seconds to read the HDF5 files. I expected it to be faster
    as Vaex claims near-instant opening of files in binary format.
  prefs: []
  type: TYPE_NORMAL
- en: '*[From Vaex documentation](https://vaex.readthedocs.io/en/latest/example_io.html#Binary-file-formats):*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Opening such data is instantenous regardless of the file size on disk: Vaex
    will just memory-map the data instead of reading it in memory. This is the optimal
    way of working with large datasets that are larger than available RAM.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Display head with Vaex:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Vaex needed 1189 seconds to display head. I am not sure why displaying the first
    5 rows of each column took so long.
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculate 10th quantile with Vaex:**'
  prefs: []
  type: TYPE_NORMAL
- en: Note, Vaex has percentile_approx function, which calculates an approximation
    of quantile.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Vaex needed 0 seconds to calculate the approximation of the 10th quantile for
    the col1 column.
  prefs: []
  type: TYPE_NORMAL
- en: '**Add a new column with Vaex:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Vaex has a concept of virtual columns, which stores an expression as a column.
    It does not take up any memory and is computed on the fly when needed. A virtual
    column is treated just like a normal column. As expected, Vaex needed 0 seconds
    to execute the command above.
  prefs: []
  type: TYPE_NORMAL
- en: '**Filter data with Vaex:**'
  prefs: []
  type: TYPE_NORMAL
- en: Vaex has a concept of [selections](https://vaex.readthedocs.io/en/latest/tutorial.html#Selections-and-filtering),
    which I didn’t use as Dask doesn’t support selections, which would make the experiment
    unfair. The filter below is similar to filtering with pandas, except that Vaex
    does not copy the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Vaex needed 0 seconds to execute the filter above.
  prefs: []
  type: TYPE_NORMAL
- en: '**Grouping and aggregating data with Vaex:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The command below is slightly different from pandas as it combines grouping
    and aggregation. The command groups the data by col1_binary and calculate the
    mean for col3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/581272a415640561ab96338e7ae15211.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Calculating mean with Vaex. Photo made by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Vaex needed 0 seconds to execute the command above.
  prefs: []
  type: TYPE_NORMAL
- en: '**Visualize the histogram:**'
  prefs: []
  type: TYPE_NORMAL
- en: Visualization with bigger datasets is problematic as traditional tools for data
    analysis are not optimized to handle them. Let’s try if we can make a histogram
    of col3 with Vaex.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5bf52454450d54aebabc992f73988d29.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Visualizing data with Vaex. Photo made by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Vaex needed 0 seconds to display the plot, which was surprisingly fast.
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculate the sum of all columns**'
  prefs: []
  type: TYPE_NORMAL
- en: Memory is not an issue when processing a single column at a time. Let’s try
    to calculate the sum of all the numbers in the dataset with Vaex.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Vaex needed 40 seconds to calculate the sum of all columns.
  prefs: []
  type: TYPE_NORMAL
- en: Dask
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/d913642addcb5e4a15d313d4664e79b5.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Photo by [Kelly Sikkema](https://unsplash.com/@kellysikkema?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral).*'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s repeat the operations above but with Dask. The Jupyter Kernel was
    restarted before running Dask commands.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of reading CSV files directly with Dask’s read_csv function, we convert
    the CSV files to HDF5 to make the experiment fair.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Dask needed 763 seconds for conversion. Let me know in the comments if there
    is a faster way to convert the data with Dask. I tried to read the HDF5 files
    that were converted with Vaex with no luck.
  prefs: []
  type: TYPE_NORMAL
- en: '*[Best practices with Dask](https://docs.dask.org/en/latest/dataframe-best-practices.html#store-data-in-apache-parquet-format):*'
  prefs: []
  type: TYPE_NORMAL
- en: '*HDF5 is a popular choice for Pandas users with high performance needs. We
    encourage Dask DataFrame users to store and load data using Parquet instead.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Open HDF5 dataset with Dask:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Dask needed 0 seconds to open the HDF5 file. This is because I didn’t explicitly
    run the compute command, which would actually read the file.
  prefs: []
  type: TYPE_NORMAL
- en: '**Display head with Dask:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Dask needed 9 seconds to output the first 5 rows of the file.
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculate the 10th quantile with Dask:**'
  prefs: []
  type: TYPE_NORMAL
- en: Dask has a quantile function, which calculates actual quantile, not an approximation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Dask wasn’t able to calculate quantile as Juptyter Kernel crashed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Define a new column with Dask:**'
  prefs: []
  type: TYPE_NORMAL
- en: The function below uses the quantile function to define a new binary column.
    Dask wasn’t able to calculate it because it uses quantile.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**Filter data with Dask:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The command above needed 0 seconds to execute as Dask uses the delayed execution
    paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: '**Grouping and aggregating data with Dask:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Dask wasn’t able to group and aggregate the data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Visualize the histogram of col3:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Dask wasn’t able to visualize the data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculate the sum of all columns:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Dask wasn’t able to sum all the data.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The table below shows the execution times of the Vaex vs. Dask experiment. NA
    means that the tool couldn’t process the data, and Jupyter Kernel crashed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa55cde9fad098a6a6a7f56d384a39f0.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Summary of execution times in the experiment. Photo made by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/cfd642a6a0627b6902b4f66e752a8bb1.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Photo by [Joshua Golde](https://unsplash.com/@joshgmit?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral).*'
  prefs: []
  type: TYPE_NORMAL
- en: Vaex requires conversion of CSV to HDF5 format, which doesn’t bother me as you
    can go to lunch, come back, and the data will be converted. I also understand
    that in harsh conditions (like in the experiment) with little or no main memory
    reading data will take longer.
  prefs: []
  type: TYPE_NORMAL
- en: What I don’t understand is the time that Vaex needed to display the head of
    the file (1189 seconds for the first 5 rows!). Other operations in Vaex are heavily
    optimized, which enables us to do interactive data analysis on bigger than main
    memory datasets.
  prefs: []
  type: TYPE_NORMAL
- en: I kinda expected the problems with Dask as it is more optimized for compute
    clusters instead of a single machine. Dask is built on top of pandas, which means
    that operations that are slow in pandas, stay slow in Dask.
  prefs: []
  type: TYPE_NORMAL
- en: The winner of the experiment is clear. Vaex was able to process bigger than
    the main memory file on a laptop while Dask couldn’t. This experiment is specific
    as I am testing performance on a single machine, not a compute cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/are-you-still-using-pandas-to-process-big-data-in-2021-850ab26ad919).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Pandas on Steroids: End to End Data Science in Python with Dask](https://www.kdnuggets.com/2020/11/pandas-steroids-dask-python-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Why and How to Use Dask with Big Data](https://www.kdnuggets.com/2020/04/dask-big-data.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Good-bye Big Data. Hello, Massive Data!](https://www.kdnuggets.com/2020/10/sqream-massive-data.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Best Architecture for Your Text Classification Task: Benchmarking…](https://www.kdnuggets.com/2023/04/best-architecture-text-classification-task-benchmarking-options.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
