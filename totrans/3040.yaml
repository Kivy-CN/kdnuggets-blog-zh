- en: Explainable Artificial Intelligence (Part 2) – Model Interpretation Strategies
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释人工智能（第二部分）– 模型解释策略
- en: 原文：[https://www.kdnuggets.com/2018/12/explainable-ai-model-interpretation-strategies.html/2](https://www.kdnuggets.com/2018/12/explainable-ai-model-interpretation-strategies.html/2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/12/explainable-ai-model-interpretation-strategies.html/2](https://www.kdnuggets.com/2018/12/explainable-ai-model-interpretation-strategies.html/2)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](/2018/12/explainable-ai-model-interpretation-strategies.html?page=2#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](/2018/12/explainable-ai-model-interpretation-strategies.html?page=2#comments)'
- en: The Accuracy vs. Interpretability Trade-off
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准确性与可解释性的权衡
- en: There exists a typical Trade-off between Model Performance and Interpretability
    just like we have our standard Bias vs. Variance Trade-off in machine learning.
    In the industry, you will often hear that business stakeholders tend to prefer
    models which are more interpretable like linear models (linear\logistic regression)
    and trees which are intuitive, easy to validate and explain to a non-expert in
    data science. This increases the trust of people in these models since its decision
    policies are easier to understand. However, if you talk to data scientists solving
    real-world problems in the industry, they will tell you that due to the inherent
    high-dimensional and complex nature of real-world datasets, they often have to
    leverage machine learning models which might be non-linear and more complex in
    nature which are often impossible to explain using traditional methods (ensembles,
    neural networks). Thus, data scientists spend a lot of their time trying to improve
    model performance but in the process trying to [strike a balance between model
    performance and interpretability](https://www.oreilly.com/ideas/predictive-modeling-striking-a-balance-between-accuracy-and-interpretability).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型性能和可解释性之间存在一个典型的权衡，就像我们在机器学习中有标准的偏差与方差权衡一样。在行业中，你会常听到商业利益相关者倾向于选择更具可解释性的模型，如线性模型（线性回归/逻辑回归）和树模型，这些模型直观、易于验证，并且可以向非数据科学专家解释。这增加了人们对这些模型的信任，因为其决策政策更容易理解。然而，如果你与解决实际问题的数据科学家交谈，他们会告诉你，由于现实世界数据集本身的高维度和复杂性，他们往往必须利用可能是非线性和更复杂的机器学习模型，这些模型通常无法使用传统方法（如集成方法、神经网络）解释。因此，数据科学家花费大量时间在提升模型性能的同时，尝试 [在模型性能和可解释性之间取得平衡](https://www.oreilly.com/ideas/predictive-modeling-striking-a-balance-between-accuracy-and-interpretability)。
- en: '![Figure](../Images/8ae2b7a09fc9d9030ac37a23e226c14a.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/8ae2b7a09fc9d9030ac37a23e226c14a.png)'
- en: Model performance vs. interpretability (Source: [https://www.datascience.com)](https://www.datascience.com%29/)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 模型性能与可解释性（来源：[https://www.datascience.com](https://www.datascience.com)）
- en: 'The above figure shows us model decision boundaries for a customer loan approval
    problem. We can clearly see that simple, easy to interpret models with monotonic
    decision boundaries may work fine in certain scenarios but usually in real-world
    scenarios and datasets, we end up using a more complex and hard to interpret model
    having a non-monotonic decision boundary. Hence to reinforce our motivation, we
    need model interpretation such that, we are able to account for **fairness** (unbiasedness/non-discriminative),
    **accountability** (reliable results) and **transparency** (being able to query
    and validate predictive decisions) of a predictive model. Interested readers should
    check out the article, [*“Toward the Jet Age of machine learning”*](https://www.oreilly.com/ideas/toward-the-jet-age-of-machine-learning)*.* Besides
    this, I would definitely recommend readers to check out my personal favorite,
    and an article from which I adapted a lot of content in this series, [*“Interpreting
    predictive models with Skater: Unboxing model opacity”*](https://www.oreilly.com/ideas/interpreting-predictive-models-with-skater-unboxing-model-opacity) which
    talks about this in further detail.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了客户贷款审批问题的模型决策边界。我们可以清楚地看到，具有单调决策边界的简单、易于解释的模型在某些场景中可能表现良好，但通常在现实世界的场景和数据集中，我们最终会使用更复杂、难以解释的具有非单调决策边界的模型。因此，为了强化我们的动机，我们需要模型解释，以便我们能够考虑到**公平性**（无偏见/非歧视性）、**问责性**（可靠的结果）和**透明性**（能够查询和验证预测决策）。感兴趣的读者可以查看文章，[*“迈向机器学习的喷气时代”*](https://www.oreilly.com/ideas/toward-the-jet-age-of-machine-learning)*。*
    此外，我还强烈推荐读者查看我个人最喜欢的文章，很多内容都是我在本系列中参考的，[*“使用 Skater 解读预测模型：揭开模型的不透明性”*](https://www.oreilly.com/ideas/interpreting-predictive-models-with-skater-unboxing-model-opacity)，其中详细讨论了这一点。
- en: '[**Interpreting predictive models with Skater: Unboxing model opacity**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[**使用 Skater 解读预测模型：揭开模型的不透明性**](https://www.oreilly.com/ideas/interpreting-predictive-models-with-skater-unboxing-model-opacity)'
- en: '*A deep dive into model interpretation as a theoretical concept and a high-level
    overview of Skater.*www.oreilly.com](https://www.oreilly.com/ideas/interpreting-predictive-models-with-skater-unboxing-model-opacity)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*对模型解释作为理论概念的深入探讨以及 Skater 的高层次概述。* [www.oreilly.com](https://www.oreilly.com/ideas/interpreting-predictive-models-with-skater-unboxing-model-opacity)'
- en: Model Interpretation Techniques
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型解释技术
- en: There are a wide variety of new model interpretation techniques which try to
    address the limitations and challenges of traditional model interpretation techniques
    and try to combat the classic Intepretability vs. Model Performance Trade-off.
    In this section, we will take a look at some of these techniques and strategies.
    Remember, our focus will be to cover model-agnostic interpretation techniques
    since these techniques are truly going to help us on the road in our journey towards *Explainable
    AI (XAI)*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有多种新的模型解释技术，试图解决传统模型解释技术的局限性和挑战，并努力应对经典的解释性与模型性能的权衡。在本节中，我们将查看一些这些技术和策略。记住，我们的重点是涵盖模型无关的解释技术，因为这些技术确实会帮助我们在通往*可解释人工智能（XAI）*的道路上前进。
- en: '**Using Interpretable Models**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用可解释模型**'
- en: 'The easiest way to get started with model interpretation is to use models which
    are interpretable out of the box! This typically includes your regular parametric
    models like linear regression, logistic regression, tree-based models, rule-fits
    and even models like k-nearest neighbors and Naive Bayes! A way to categorize
    these models based on their major capabilities would be:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的模型解释入门方法是使用开箱即用的可解释模型！这通常包括你的常规参数模型，如线性回归、逻辑回归、基于树的模型、规则拟合，甚至像 k-近邻和朴素贝叶斯这样的模型！根据它们的主要功能来分类这些模型的一种方法是：
- en: 'Linearity: Typically we have a linear model model if the association between
    features and target is modeled linearly.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性性：如果特征和目标之间的关联是线性建模的，我们通常有一个线性模型。
- en: 'Monotonicity: A monotonic model ensures that the relationship between a feature
    and the target outcome is always in one consistent direction (increase or decrease)
    over the feature (in its entirety of its range of values).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单调性：单调模型确保特征与目标结果之间的关系在特征的整个范围内始终朝着一个一致的方向（增加或减少）。
- en: 'Interactions: You can always add interaction features, non-linearity to a model
    with manual feature engineering. Some models create it automatically also.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交互：你总是可以通过手动特征工程为模型添加交互特征、非线性。有些模型也会自动创建它。
- en: Christoph Molnar’s excellent book, [*‘Interpretable Machine Learning’*](https://christophm.github.io/interpretable-ml-book/) has
    a nice table summarizing the above aspects.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Christoph Molnar的优秀著作，[*《可解释的机器学习》*](https://christophm.github.io/interpretable-ml-book/)中有一个很好的表格，总结了上述方面。
- en: '![Figure](../Images/892cda2170b08f6c264ddb52731cb927.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/892cda2170b08f6c264ddb52731cb927.png)'
- en: 'Source: Interpretable Machine Learning, Christoph Molnar'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：可解释的机器学习，Christoph Molnar
- en: A good point to remember is that some of these models might be too simplistic
    and hence we might need to think of better ways to build and interpret more complex
    and better performing models.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的一点是，这些模型可能过于简单，因此我们可能需要考虑更好的方法来构建和解释更复杂且性能更好的模型。
- en: '**Feature Importance**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征重要性**'
- en: Feature importance is generic term for the degree to which a predictive model
    relies on a particular feature. Typically, a feature’s importance is the increase
    in the model’s prediction error after we permuted the feature’s values. Frameworks
    like Skater compute this based on an information theoretic criteria, measuring
    the entropy in the change of predictions, given a perturbation of a given feature.
    The intuition is that the more a model’s decision criteria depend on a feature,
    the more we’ll see predictions change as a function of perturbing a feature. However,
    frameworks like SHAP, use a combination of feature contributions and game theory
    to come up with SHAP values. Then, it computes the global feature importance by
    taking the average of the SHAP value magnitudes across the dataset. Following
    is a standard example of a feature importance plot from Skater on a census dataset.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性是指预测模型对特定特征的依赖程度的通用术语。通常，特征的重要性是我们扰动特征值后模型预测误差的增加。像Skater这样的框架基于信息论标准来计算这一点，测量在给定特征扰动下预测变化的熵。直观地说，模型的决策标准越依赖于某个特征，我们就会看到预测随着特征的扰动而变化。然而，像SHAP这样的框架使用特征贡献和博弈论的结合来得出SHAP值。然后，通过计算数据集中SHAP值的平均值来计算全局特征重要性。以下是Skater在一个人口普查数据集上绘制的特征重要性图的标准示例。
- en: '![](../Images/bf2d3257040d7d0f8d1024eda38c0fe7.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf2d3257040d7d0f8d1024eda38c0fe7.png)'
- en: Looks like `Age` and `Education-Num` are the top two features, where `Age` is
    reponsible for model predictions changing by an average of 14.5% on perturbing\permuting
    the `Age` feature. Hence, to summarize, the concept behind global interpretations
    of model-agnostic feature importance is really straightforward.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来`Age`和`Education-Num`是最重要的两个特征，其中`Age`在扰动/置换`Age`特征时，模型预测的变化平均为14.5%。因此，总结来说，模型无关特征重要性的全局解释背后的概念是非常简单明了的。
- en: We measure a feature’s importance by calculating the increase of the model’s
    prediction error after perturbing the feature.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过计算扰动特征后的模型预测误差的增加来衡量特征的重要性。
- en: A feature is “important” if perturbing its values increases the model error,
    because the model relied on the feature for the prediction.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果扰动特征的值会增加模型误差，那么该特征被认为是“重要的”，因为模型依赖于该特征来进行预测。
- en: A feature is “unimportant” if perturbing its values keeps the model error unchanged,
    because the model basically ignored the feature for the prediction.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果扰动特征的值不会改变模型误差，则该特征被认为是“不重要的”，因为模型在预测时基本上忽略了该特征。
- en: The permutation feature importance measurement was introduced for *Random Forests
    by Breiman (2001)*. Based on this idea, *Fisher, Rudin, and Dominici (2018)* proposed
    a model-agnostic version of the feature importance — they called it *Model Reliance*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 特征的重要性度量方法是由*Breiman (2001)*为*随机森林*提出的。基于这个想法，*Fisher, Rudin, and Dominici (2018)*提出了一种模型无关的特征重要性版本——他们称之为*模型依赖性*。
- en: '**Partial Dependence Plots**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**部分依赖图**'
- en: 'Partial Dependence describes the marginal impact of a feature on model prediction,
    holding other features in the model constant. The derivative of partial dependence
    describes the impact of a feature (analogous to a feature coefficient in a regression
    model). The partial dependence plot (PDP or PD plot) shows the marginal effect
    of a feature on the predicted outcome of a previously fit model. PDPs can show
    if the relationship between the target and a feature is linear, monotonic or more
    complex. The partial dependence plot is a global method: The method takes into
    account all instances and makes a statement about the global relationship of a
    feature with the predicted outcome. Following figures show some example PDPs.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 部分依赖性描述了在模型中保持其他特征不变的情况下，某个特征对模型预测的边际影响。部分依赖性的导数描述了特征的影响（类似于回归模型中的特征系数）。部分依赖图（PDP
    或 PD 图）展示了特征对之前拟合的模型的预测结果的边际效应。PDP 可以显示目标与特征之间的关系是线性的、单调的还是更复杂的。部分依赖图是一种全球方法：该方法考虑了所有实例，并对特征与预测结果的全球关系做出声明。以下图展示了一些示例
    PDP。
- en: '![Figure](../Images/c246d92920033cb8be70bf0c40e8aa6e.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/c246d92920033cb8be70bf0c40e8aa6e.png)'
- en: PDP for 1 feature
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 1 特征的 PDP
- en: We have leveraged Skater and SHAP to show the effect of Education Level on earning
    more money here. This is a one-way PDP showing the effect of one feature on the
    model predictions. We can also build two-way PDPs showing the effect of two features
    on model predictions. An example is illustrated in the following figure.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用 Skater 和 SHAP 显示了教育水平对收入的影响。这是一个单变量 PDP（部分依赖图），展示了一个特征对模型预测的影响。我们还可以构建双变量
    PDP，展示两个特征对模型预测的影响。以下图示例说明了这一点。
- en: '![Figure](../Images/68b2e01d5eab64a2466f093586f4ad74.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/68b2e01d5eab64a2466f093586f4ad74.png)'
- en: PDP for 2 features
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 2 特征的 PDP
- en: You can clearly see the effect and interaction of the features which influence
    the model predictions in the above PDPs. Notable middle-aged people with higher
    education levels and more working hours per week earn more money!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的 PDP 中，你可以清楚地看到影响模型预测的特征及其相互作用。显著的中年人有更高的教育水平和每周更多的工作时间，他们的收入也更高！
- en: '**Global Surrogate Models**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**全球替代模型**'
- en: We have see various ways to interpret machine learning models with feature importances,
    dependence plots, less complex models. But is there a way to build intepretable
    approximations of really complex models? Thankfully we have global surrogate models
    just for this purpose! A global surrogate model is an interpretable model that
    is trained to approximate the predictions of a black box model which can essentially
    be any model regardless of its complexity or training algorithm — this is as model-agnostic
    as it gets!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了各种解释机器学习模型的方法，如特征重要性、依赖图、较简单的模型等。但是否有方法来构建可解释的复杂模型的近似？幸运的是，我们有全球替代模型来实现这一目的！全球替代模型是一种可解释的模型，它被训练来近似黑箱模型的预测，而黑箱模型本质上可以是任何模型，不论其复杂性或训练算法如何——这就是最具模型无关性的！
- en: '![](../Images/3740743b350afb3b4cf0b5990e8a0fb8.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3740743b350afb3b4cf0b5990e8a0fb8.png)'
- en: Typically we approximate a more interpretable surrogate model based on our base
    model which is treated as a black box model. We can then draw conclusions about
    the black box model by interpreting the surrogate model. Solving machine learning
    interpretability by using more machine learning!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们会基于我们的基础模型（它被视为黑箱模型）来近似一个更具可解释性的替代模型。然后，我们可以通过解释替代模型来得出关于黑箱模型的结论。通过使用更多的机器学习来解决机器学习的可解释性问题！
- en: The purpose of (interpretable) surrogate models is to approximate the predictions
    of the underlying model as closely as possible while being interpretable. Fitting
    a surrogate model is a model-agnostic method, since it requires no information
    about the inner workings of the black box model, only the relation of input and
    predicted output is used. The choice of the base black box model type and of the
    surrogate model type is decoupled. Tree-based models are not too simplistic but
    interpretable and make a good choice for building surrogate models.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: （可解释的）替代模型的目的是尽可能接近地近似基础模型的预测，同时保持可解释性。拟合替代模型是一种模型无关的方法，因为它不需要关于黑箱模型内部工作的信息，只使用输入和预测输出之间的关系。基础黑箱模型类型和替代模型类型的选择是解耦的。基于树的模型虽然不太简单，但可解释性强，适合用于构建替代模型。
- en: Skater introduce the novel idea of using `**TreeSurrogates**` as means for explaining
    a model's learned decision policies (for inductive learning tasks), which is inspired
    by the work of Mark W. Craven described as the TREPAN algorithm. We will be covering
    the TREPAN model with examples in Part 3 of this series. For now, Christoph Molnar,
    does an excellent job in talking about the main steps involved for building surrogate
    models in his [*book*](https://christophm.github.io/interpretable-ml-book/).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Skater 引入了使用 `**TreeSurrogates**` 作为解释模型学习决策政策（用于归纳学习任务）的新颖想法，这一想法受到了 Mark W.
    Craven 描述的 TREPAN 算法的启发。我们将在本系列第3部分中通过示例介绍 TREPAN 模型。目前，Christoph Molnar 在他的 [*书籍*](https://christophm.github.io/interpretable-ml-book/)
    中对构建代理模型的主要步骤进行了很好的讲解。
- en: Choose a dataset This could be the same dataset that was used for training the
    black box model or a new dataset from the same distribution. You could even choose
    a subset of the data or a grid of points, depending on your application.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个数据集，这可以是用于训练黑箱模型的相同数据集，也可以是来自相同分布的新数据集。你甚至可以选择数据的一个子集或一个点网格，具体取决于你的应用。
- en: For the chosen dataset, get the predictions of your base black box model.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于选择的数据集，获取基准黑箱模型的预测结果。
- en: Choose an interpretable surrogate model (linear model, decision tree, …).
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个可解释的代理模型（线性模型、决策树等）。
- en: Train the interpretable model on the dataset and its predictions.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据集及其预测上训练可解释模型。
- en: Congratulations! You now have a surrogate model.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 恭喜你！你现在有了一个代理模型。
- en: Measure how well the surrogate model replicates the prediction of the black
    box model.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测量代理模型如何复制黑箱模型的预测。
- en: Interpret / visualize the surrogate model.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释/可视化代理模型。
- en: Of course this is a high level illustration and algorithms like TREPAN do a
    lot more internally but the overall workflow still remains the same.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这只是一个高层次的插图，像 TREPAN 这样的算法在内部做了很多工作，但总体工作流程仍然是相同的。
- en: '![Figure](../Images/38b4858bb32d4307a2026b892641323e.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/38b4858bb32d4307a2026b892641323e.png)'
- en: Sample illustration of a surrogate tree model
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 代理树模型的示例插图
- en: The above illustration is of a surrogate tree model approximated from a complex
    XGBoost black box model. We will be building this from scratch in Part 3 so stay
    tuned! Interestingly this model has an overall accuracy of 83% as compared to
    the XGBoost model’s accuracy which is 87%. Not bad!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的插图是从复杂的 XGBoost 黑箱模型中近似得到的代理树模型。我们将在第3部分从头开始构建这个模型，所以请继续关注！有趣的是，这个模型的整体准确率为
    83%，而 XGBoost 模型的准确率为 87%。还不错！
- en: '**Local Interpretable Model-agnostic Explanations (LIME)**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**局部可解释模型无关解释（LIME）**'
- en: LIME is a novel algorithm designed by Riberio Marco, Singh Sameer, Guestrin
    Carlos to access the behavior of the any base estimator(model) using local interpretable
    surrogate models (e.g. linear classifier/regressor). Such form of comprehensive
    evaluation helps in generating explanations which are locally faithful but may
    not align with the global behavior. Basically, LIME explanations are based on
    local surrogate models. These, surrogate models are interpretable models (like
    a linear model or decision tree) that are learned on the predictions of the original
    black box model. But instead of trying to fit a global surrogate model, LIME focuses
    on fitting local surrogate models to explain why single predictions were made.
    In fact, [***LIME***](https://github.com/marcotcr/lime) is also available as an [*open-source
    framework on GitHub*](https://github.com/marcotcr/lime) and is based on the work
    presented in [*this paper*](https://arxiv.org/abs/1602.04938).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**LIME** 是由 Riberio Marco、Singh Sameer 和 Guestrin Carlos 设计的一种新颖算法，用于使用局部可解释代理模型（例如线性分类器/回归器）访问任何基估计器（模型）的行为。这种全面评估的形式有助于生成局部忠实的解释，但可能与全局行为不一致。基本上，LIME
    解释基于局部代理模型。这些代理模型是可以解释的模型（如线性模型或决策树），在原始黑箱模型的预测上学习。但 LIME 并不试图拟合全局代理模型，而是专注于拟合局部代理模型，以解释为什么做出单一预测。实际上，[***LIME***](https://github.com/marcotcr/lime)
    也可以作为一个 [*开源框架在 GitHub 上*](https://github.com/marcotcr/lime) 获取，并基于 [*这篇论文*](https://arxiv.org/abs/1602.04938)
    的工作。'
- en: The idea is very intuitive. To start with, just try and unlearn what you have
    done so far! Forget about the training data, forget about how your model works!
    Think that your model is a black box model with some magic happening inside, where
    you can input data points and get the models predicted outcomes. You can probe
    this magic black box as often as you want with inputs and get output predictions.
    Now, you main objective is to understand why the machine learning model which
    you are treating as a magic black box, gave the outcome it produced. LIME tries
    to do this for you! It tests out what happens to you black box model’s predictions
    when you feed variations or perturbations of your dataset into the black box model.
    Typically, LIME generates a new dataset consisting of perturbed samples and the
    associated black box model’s predictions. On this dataset LIME then trains an
    interpretable model weighted by the proximity of the sampled instances to the
    instance of interest. Following is a standard high-level workflow for this.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法非常直观。首先，尝试忘记你迄今为止所做的工作！忘记训练数据，忘记你的模型是如何工作的！认为你的模型是一个黑箱模型，内部发生了一些魔法，你可以输入数据点并获得模型预测的结果。你可以随时用输入探测这个神奇的黑箱，并得到输出预测。现在，你的主要目标是理解为什么你把机器学习模型当作魔法黑箱时，它给出了所产生的结果。LIME
    试图为你做到这一点！它测试当你将数据集的变异或扰动输入到黑箱模型中时，黑箱模型的预测会发生什么。通常，LIME 生成一个由扰动样本和相关黑箱模型预测组成的新数据集。在这个数据集上，LIME
    然后训练一个加权的、可解释的（替代）模型，根据采样实例与感兴趣实例的接近度进行加权。以下是一个标准的高层次工作流程。
- en: Choose your instance of interest for which you want to have an explanation of
    the predictions of your black box model.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择你感兴趣的实例，以便对黑箱模型的预测进行解释。
- en: Perturb your dataset and get the black box predictions for these new points.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扰动你的数据集，并获取这些新点的黑箱预测。
- en: Weight the new samples by their proximity to the instance of interest.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按样本与感兴趣实例的接近度对新样本进行加权。
- en: Fit a weighted, interpretable (surrogate) model on the dataset with the variations.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在带有变异的数据集上拟合一个加权的、可解释的（替代）模型。
- en: Explain prediction by interpreting the local model.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过解释局部模型来解释预测。
- en: Following is a sample example of LIME in action in the Skater framework explaining
    why the model predicted a person will earn more than $50K.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 LIME 在 Skater 框架中应用的示例，解释为什么模型预测一个人会赚取超过 50K 美元。
- en: '![Figure](../Images/edbaa91a515df2eeeb019dbf18ca97cc.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/edbaa91a515df2eeeb019dbf18ca97cc.png)'
- en: Explaining predictions with LIME
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LIME解释预测
- en: We will be using the Census dataset to build models and explore them with LIME
    in our next article in further detail.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一篇文章中使用 Census 数据集来构建模型，并利用 LIME 进一步探讨它们的细节。
- en: '****Shapley Values and SHapley Additive exPlanations (SHAP)****'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '****Shapley 值和 SHapley Additive exPlanations (SHAP)****'
- en: '**SHAP (SHapley Additive exPlanations)** is a unified approach to explain the
    output of any machine learning model. SHAP connects game theory with local explanations,
    uniting several previous methods and representing the only possible consistent
    and locally accurate additive feature attribution method based on what they claim!
    (do check out the [SHAP NIPS paper](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions) for
    details). SHAP is an excellent model interpretation framework which is based on
    adaptation and enhancements to Shapley values which we shall explore in-depth
    in this section. Thanks once again to Christoph Molnar’s [*amazing model interpretation
    book*](https://christophm.github.io/interpretable-ml-book/) which I shall be shamelessly
    adapting into this tutorial because in my opinion that is perhaps the best way
    to understand this concept!'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**SHAP（SHapley Additive exPlanations）** 是一种统一的方法，用于解释任何机器学习模型的输出。SHAP 将博弈论与局部解释相结合，整合了几个先前的方法，并代表了基于其声明的唯一一致且局部准确的加性特征归因方法！（有关详细信息，请查看
    [SHAP NIPS 论文](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions)）。SHAP
    是一个出色的模型解释框架，基于对 Shapley 值的适应和增强，我们将在本节中深入探讨。再次感谢 Christoph Molnar 的 [*惊人模型解释书籍*](https://christophm.github.io/interpretable-ml-book/)，我将无耻地将其改编为本教程，因为在我看来，这可能是理解这一概念的最佳方式！'
- en: Typically, model predictions can be explained by assuming that each feature
    is a *‘player’* in a game where the prediction is the payout. The Shapley value — a
    method from coalitional game theory — tells us how to fairly distribute the *‘payout’* among
    the features. Let’s take an illustrative example.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，模型预测可以通过假设每个特征是一个*‘玩家’*的游戏来解释，其中预测是支付的结果。Shapley值——一种来自合作博弈论的方法——告诉我们如何公平地分配*‘支付’*给各个特征。让我们看一个说明性的例子。
- en: '![Figure](../Images/b6ab4a9b44d9da7ca1d5773a582dc0d8.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/b6ab4a9b44d9da7ca1d5773a582dc0d8.png)'
- en: Assume you trained a machine learning model to predict apartment prices. For
    a certain apartment it predicts 300,000 € and you need to explain this prediction.
    The apartment has a *size *of 50 m-sq, is located on the *2nd floor*, with a *park
    nearby*and *cats are forbidden. *The average prediction for all apartments is
    310,000€. How much did each feature value contribute to the prediction compared
    to the average prediction?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你训练了一个机器学习模型来预测公寓价格。对于某个公寓，模型预测了300,000欧元，你需要解释这个预测。该公寓的*面积*为50平方米，位于*2楼*，附近有*公园*，而且*禁止养猫*。所有公寓的平均预测价格是310,000欧元。每个特征值相对于平均预测的贡献是多少？
- en: 'The answer is easy for linear regression models: The effect of each feature
    is the weight of the feature times the feature value minus the average effect
    of all apartments: This works only because of the linearity of the model. For
    more complex models what do we do? One option is LIME which we just discussed.
    A different solution comes from cooperative game theory: ***The Shapley value***,
    coined by Shapley, is a method for assigning *payouts* to *players* depending
    on their contribution towards the total payout. Players cooperate in a *coalition* and
    obtain a certain *gain *from that cooperation.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于线性回归模型，答案很简单：每个特征的效果是特征的权重乘以特征值，减去所有公寓的平均效果：这仅仅是因为模型的线性特性。对于更复杂的模型，我们该怎么办？一个选项是我们刚刚讨论的LIME。另一个解决方案来自于合作博弈论：***Shapley值***，由Shapley提出，是一种根据特征对总支付的贡献分配*支付*给*玩家*的方法。玩家在一个*联盟*中合作，并从这种合作中获得一定的*收益*。
- en: The *‘game’* is the prediction task for a single instance of the dataset.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*‘游戏’*是数据集中单个实例的预测任务。'
- en: The *‘gain’* is the actual prediction for this instance minus the average prediction
    of all instances.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*‘收益’*是该实例的实际预测值减去所有实例的平均预测值。'
- en: The *‘players’ *are the feature values of the instance, which collaborate to
    receive the gain (= predict a certain value).
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*‘玩家’*是实例的特征值，它们合作以获得收益（即预测一个特定值）。'
- en: 'Thus, in our apartment example, the feature values `**‘park-allowed’**`, `**‘cat-forbidden’**`, `**‘area-50m-sq’**` and `**‘floor-2nd’**` worked
    together to achieve the prediction of 300,000€. Our goal is to explain the difference
    of the actual prediction (300,000€) and the average prediction (310,000€): a difference
    of -10,000€. The answer could be: The `**‘park-nearby’**` contributed 30,000€; `**‘size-50m-sq’**` contributed
    10,000€; `**‘floor-2nd’**` contributed 0€; `**‘cat-forbidden’**` contributed -50,000€.
    The contributions add up to -10,000€: the final prediction minus the average predicted
    apartment price.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在我们的公寓示例中，特征值`**‘允许公园’**`、`**‘禁止养猫’**`、`**‘面积-50平方米’**`和`**‘楼层-2楼’**`共同作用以实现300,000欧元的预测。我们的目标是解释实际预测（300,000欧元）与平均预测（310,000欧元）之间的差异：即-10,000欧元。答案可能是：`**‘公园附近’**`贡献了30,000欧元；`**‘面积-50平方米’**`贡献了10,000欧元；`**‘楼层-2楼’**`贡献了0欧元；`**‘禁止养猫’**`贡献了-50,000欧元。贡献总和为-10,000欧元：即最终预测减去平均预测的公寓价格。
- en: The Shapley value is the average marginal contribution of a feature value over
    all possible coalitions. Coalitions are basically combinations of features which
    are used to estimate the shapley value of a specific feature. Typically more the
    features, it starts increasing exponentially hence it may take a lot of time to
    compute these values for big or wide datasets. The following figure shows all
    coalitions of feature values that are needed to assess the Shapley value for `**‘cat-forbidden’**`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Shapley值是特征值在所有可能的联盟中的平均边际贡献。联盟基本上是特征的组合，用于估算特定特征的Shapley值。通常，特征越多，计算量会指数增长，因此对于大规模或宽数据集计算这些值可能需要很多时间。下图展示了评估`**‘禁止养猫’**`的Shapley值所需的所有特征值联盟。
- en: '![](../Images/f10c02704d81b7280b3b7f2844ba061f.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f10c02704d81b7280b3b7f2844ba061f.png)'
- en: The first row shows the coalition without any feature values. The 2nd, 3rd and
    4th row show different coalitions — separated by `**‘|’ **`— with increasing coalition
    size. For each of those coalitions we compute the predicted apartment price with
    and without the `**‘cat-forbidden’**` feature value and take the difference to
    get the marginal contribution. The Shapley value is the (weighted) average of
    marginal contributions. We replace the feature values of features that are not
    in a coalition with random feature values from the apartment dataset to get a
    prediction from the machine learning model. When we repeat the Shapley value for
    all feature values, we get the complete distribution of the prediction (minus
    the average) among the feature values. SHAP is an enhancement on the Shapley values.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行显示了没有任何特征值的联盟。第二行、第三行和第四行显示了不同的联盟——用`**‘|’**`分隔——联盟的大小逐渐增加。对于这些联盟中的每一个，我们计算了包含和不包含`**‘cat-forbidden’**`特征值的预测公寓价格，并取其差值来获得边际贡献。Shapley
    值是边际贡献的（加权）平均值。我们用公寓数据集中随机的特征值替换那些不在联盟中的特征值，以获取机器学习模型的预测。当我们对所有特征值重复计算 Shapley
    值时，我们得到特征值之间的预测（减去平均值）的完整分布。SHAP 是对 Shapley 值的增强。
- en: 'SHAP (SHapley Additive exPlanations) assigns each feature an importance value
    for a particular prediction. Its novel components include: the identification
    of a new class of additive feature importance measures, and theoretical results
    showing there is a unique solution in this class with a set of desirable properties.
    Typically, SHAP values try to explain the output of a model (function) as a sum
    of the effects of each feature being introduced into a conditional expectation.
    Importantly, for non-linear functions the order in which features are introduced
    matters. The SHAP values result from averaging over all possible orderings. Proofs
    from game theory show this is the only possible consistent approach. The following
    figure from the KDD 18 paper, [*Consistent Individualized Feature Attribution
    for Tree Ensembles*](https://arxiv.org/pdf/1802.03888.pdf)summarizes this in a
    nice way!'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP（SHapley Additive exPlanations）为特定预测分配每个特征一个重要性值。它的新颖组成部分包括：识别出一种新的加性特征重要性度量类别，并且理论结果表明在这个类别中存在一个具有一组期望属性的唯一解决方案。通常，SHAP
    值试图将模型（函数）的输出解释为将每个特征引入条件期望的效果之和。重要的是，对于非线性函数，引入特征的顺序是重要的。SHAP 值是对所有可能排序的平均结果。博弈论的证明表明，这是唯一可能一致的方法。以下来自
    KDD 18 论文，[*Consistent Individualized Feature Attribution for Tree Ensembles*](https://arxiv.org/pdf/1802.03888.pdf)
    的图示很好地总结了这一点！
- en: '![Figure](../Images/8c0220954319dc8c083bdc4c62b5bd59.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/8c0220954319dc8c083bdc4c62b5bd59.png)'
- en: Understanding SHAP value
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 SHAP 值
- en: Following is an illustration of using SHAP to explaining the model’s decisions
    when it’s predicting if a person’s income > $50K
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用 SHAP 解释模型在预测一个人的收入是否超过 $50K 时的决策的示例。
- en: '![Figure](../Images/056ec218a0316751c00dfd9ebb4df73c.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/056ec218a0316751c00dfd9ebb4df73c.png)'
- en: Explaining model predictions with SHAP
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SHAP 解释模型预测
- en: It is interesting to see the key drivers (features) behind the model taking
    such a decision! We will also be covering this with hands-on examples in Part
    3 of this series.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 看到模型在做出这样的决策时背后的关键驱动因素（特征）非常有趣！我们将在本系列的第三部分中通过实际示例来覆盖这一内容。
- en: Conclusion
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: This article should help you take more definitive steps on the road towards
    Explanable AI (XAI). You now know the need and importance of model interpretation.
    The issues with bias and fairness from the first article. Here we have taken a
    look at traditional techniques for model interpretation, discussed their challenges
    and limitations and also covered the classic trade-off between model interpretability
    and prediction performance. Finally, we looked at the current state-of-the-art
    model interpretation techniques and strategies including feature importances,
    PDPs, global surrogates, local surrogates and LIME, shapley values and SHAP. Like
    I have mentioned before, Let’s try and work towards human-interpretable machine
    learning and XAI to demystify machine learning for everyone and help increase
    the trust in model decisions.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 本文应帮助你在通向可解释人工智能（XAI）的道路上迈出更明确的步伐。你现在知道了模型解释的需求和重要性，第一篇文章中提到的偏见和公平性问题。在这里，我们回顾了传统的模型解释技术，讨论了它们的挑战和局限性，并且涵盖了模型可解释性与预测性能之间的经典权衡。最后，我们查看了当前最先进的模型解释技术和策略，包括特征重要性、部分依赖图、全局代理模型、局部代理模型、LIME、Shapley值和SHAP。正如我之前提到的，让我们努力朝着人类可解释的机器学习和XAI迈进，以揭开机器学习的神秘面纱，并帮助提高对模型决策的信任。
- en: What’s next?
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 下一步是什么？
- en: In Part 3 of this series, we will be looking at a comprehensive guide to building
    and interpreting machine learning models using all the new techniques we learnt
    in this article. We will be using several state-of-the-art model interpretation
    frameworks for this.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本系列的第3部分中，我们将详细介绍使用本文所学的所有新技术来构建和解释机器学习模型的全面指南。我们将使用几个最先进的模型解释框架。
- en: Hands-on guides on using the latest state-of-the-art model interpretation frameworks
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最新最先进的模型解释框架的实操指南
- en: Features, concepts and examples of using frameworks like ELI5, Skater and SHAP
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用框架如ELI5、Skater和SHAP的特点、概念和示例
- en: Explore concepts and see them in action — Feature importances, partial dependence
    plots, surrogate models, interpretation and explanations with LIME, SHAP values
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索概念并查看它们的实际应用——特征重要性、部分依赖图、代理模型、LIME、SHAP值的解释和说明
- en: Hands-on Machine Learning Model Interpretation on a supervised learning example
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个监督学习示例中进行实操的机器学习模型解释
- en: Stay tuned, this is definitely going to get more interesting and exciting!
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 敬请关注，这将变得更加有趣和激动人心！
- en: Check out [***‘Part I — The Importance of Human Interpretable Machine Learning’***](https://towardsdatascience.com/human-interpretable-machine-learning-part-1-the-need-and-importance-of-model-interpretation-2ed758f5f476)which
    covers the what and why of human interpretable machine learning and the need and
    importance of model interpretation along with its scope and criteria in case you
    haven’t!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[***‘第一部分——人类可解释机器学习的重要性’***](https://towardsdatascience.com/human-interpretable-machine-learning-part-1-the-need-and-importance-of-model-interpretation-2ed758f5f476)，它涵盖了人类可解释机器学习的什么和为什么，以及模型解释的需求和重要性，及其范围和标准，以防你还没看过！
- en: Thanks to all the wonderful folks at [DataScience.com](https://www.datascience.com/) and
    especially [*Pramit Choudhary*](https://www.linkedin.com/in/pramitc/?lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_top%3Bi7sMwalBRG69UVr8ck4n%2BA%3D%3D&licu=urn%3Ali%3Acontrol%3Ad_flagship3_search_srp_top-search_srp_result&lici=LV1csX2ATFSqPNbcebrjvQ%3D%3D) for
    building an amazing model interpretation framework, [***Skater***](https://www.datascience.com/resources/tools/skater)***,***and
    helping me out with some excellent content for this series.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢[DataScience.com](https://www.datascience.com/)的所有优秀人士，特别是[*Pramit Choudhary*](https://www.linkedin.com/in/pramitc/?lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_top%3Bi7sMwalBRG69UVr8ck4n%2BA%3D%3D&licu=urn%3Ali%3Acontrol%3Ad_flagship3_search_srp_top-search_srp_result&lici=LV1csX2ATFSqPNbcebrjvQ%3D%3D)为构建出色的模型解释框架[***Skater***](https://www.datascience.com/resources/tools/skater)而付出的努力，并为本系列提供了出色的内容。
- en: I cover a lot of examples of machine learning model interpretation in my book, [***“Practical
    Machine Learning with Python”***](https://github.com/dipanjanS/practical-machine-learning-with-python).
    The code is open-sourced for your benefit!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我在我的书[***“Python实用机器学习”***](https://github.com/dipanjanS/practical-machine-learning-with-python)中涵盖了大量的机器学习模型解释示例。代码是开源的，供你使用！
- en: Have feedback for me? Or interested in working with me on research, data science,
    artificial intelligence or even publishing an article on [***TDS***](https://towardsdatascience.com/)?
    You can reach out to me on [**LinkedIn**](https://www.linkedin.com/in/dipanzan/)**.**
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 有反馈吗？或者有兴趣与我合作研究、数据科学、人工智能，甚至在[***TDS***](https://towardsdatascience.com/)上发布文章？你可以通过[**LinkedIn**](https://www.linkedin.com/in/dipanzan/)与我联系。
- en: '**Bio: [Dipanjan Sarkar](https://www.linkedin.com/in/dipanzan)** is a Data
    Scientist @Intel, an author, a mentor @Springboard, a writer, and a sports and
    sitcom addict.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介：[Dipanjan Sarkar](https://www.linkedin.com/in/dipanzan)** 是一位@Intel的数据科学家、作者、@Springboard的导师、作家，以及体育和情景喜剧爱好者。'
- en: '[Original](https://towardsdatascience.com/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739).
    Reposted with permission.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739)。经授权转载。'
- en: '**Related:**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Human Interpretable Machine Learning (Part 1) — The Need and Importance of
    Model Interpretation](/2018/06/human-interpretable-machine-learning-need-importance-model-interpretation.html)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人类可解释的机器学习（第1部分） — 模型解释的必要性和重要性](/2018/06/human-interpretable-machine-learning-need-importance-model-interpretation.html)'
- en: '[Emotion and Sentiment Analysis: A Practitioner’s Guide to NLP](/2018/08/emotion-sentiment-analysis-practitioners-guide-nlp-5.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[情感和情绪分析：实践者的NLP指南](/2018/08/emotion-sentiment-analysis-practitioners-guide-nlp-5.html)'
- en: '[Named Entity Recognition: A Practitioner’s Guide to NLP](/2018/08/named-entity-recognition-practitioners-guide-nlp-4.html)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[命名实体识别：实践者的NLP指南](/2018/08/named-entity-recognition-practitioners-guide-nlp-4.html)'
- en: '* * *'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三名课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织进行IT工作'
- en: '* * *'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关话题
- en: '[Should You Become a Freelance Artificial Intelligence Engineer?](https://www.kdnuggets.com/2021/12/ucsd-become-freelance-artificial-intelligence-engineer.html)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你应该成为一名自由职业的人工智能工程师吗？](https://www.kdnuggets.com/2021/12/ucsd-become-freelance-artificial-intelligence-engineer.html)'
- en: '[Artificial Intelligence Project Ideas for 2022](https://www.kdnuggets.com/2022/01/artificial-intelligence-project-ideas-2022.html)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2022年人工智能项目创意](https://www.kdnuggets.com/2022/01/artificial-intelligence-project-ideas-2022.html)'
- en: '[Artificial Intelligence and the Metaverse](https://www.kdnuggets.com/2022/02/artificial-intelligence-metaverse.html)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能与元宇宙](https://www.kdnuggets.com/2022/02/artificial-intelligence-metaverse.html)'
- en: '[Uncertainty Quantification in Artificial Intelligence-based Systems](https://www.kdnuggets.com/2022/04/uncertainty-quantification-artificial-intelligencebased-systems.html)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基于人工智能的系统中的不确定性量化](https://www.kdnuggets.com/2022/04/uncertainty-quantification-artificial-intelligencebased-systems.html)'
- en: '[How Artificial Intelligence Can Transform Data Integration](https://www.kdnuggets.com/2022/04/artificial-intelligence-transform-data-integration.html)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能如何改变数据集成](https://www.kdnuggets.com/2022/04/artificial-intelligence-transform-data-integration.html)'
- en: '[Most In-demand Artificial Intelligence Skills To Learn In 2022](https://www.kdnuggets.com/2022/08/indemand-artificial-intelligence-skills-learn-2022.html)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2022年最受欢迎的人工智能技能](https://www.kdnuggets.com/2022/08/indemand-artificial-intelligence-skills-learn-2022.html)'
