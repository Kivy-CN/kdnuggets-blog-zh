- en: 'Transfer Learning Made Easy: Coding a Powerful Technique'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/11/transfer-learning-coding.html](https://www.kdnuggets.com/2019/11/transfer-learning-coding.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: Artificial Intelligence for the Average User
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Artificial intelligence (A.I.) is shaping up to be the [*most powerful and transformative
    technology to sweep the globe and touch all facets of life*](https://www.strategyand.pwc.com/uk/en/transformative-power-ai/transformative-power-artificial-intelligence.html) –
    economics, healthcare, finance, industry, socio-cultural interactions, etc. –
    in an unprecedented manner. This is even more important with developments in transfer
    learning and machine learning capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Already, we are using A.I. technologies on a daily basis, and it is impacting
    our lives and choices whether we consciously know it or not. From our Google search
    and Navigation, Netflix movie recommendations, Amazon purchase suggestions, voice
    assistants for daily tasks like Siri or Alexa, Facebook community building, medical
    diagnoses, credit score calculations, and mortgage decision making, etc., A.I.
    is only going to grow in adoption.
  prefs: []
  type: TYPE_NORMAL
- en: Most modern A.I. systems are currently being powered by a family of algorithms
    or techniques called [deep learning](https://machinelearningmastery.com/what-is-deep-learning/),
    which basically trains and builds [deep layers of neural networks](https://blog.exxactcorp.com/deep-learning-vs-machine-learning-vs-data-science-how-do-they-differ/) with
    various architectural configurations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/208231dc1b5052d184e745923aeea657.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Image source: Fjodor van Veen – asimovinstitute.org.*'
  prefs: []
  type: TYPE_NORMAL
- en: After 50+ years of ebbs and flows, the deep learning revolution has caught the
    steam and looks unstoppable – fueled by Big Data technologies, innovation in hardware,
    and algorithms. Therefore, [deep learning networks are promising to impact and
    fundamentally alter](https://elitedatascience.com/machine-learning-impact) how
    we, humans, live, work, and play for at least the next few decades to come.
  prefs: []
  type: TYPE_NORMAL
- en: '*So, we can finally see the promise of A.I. for everyone on the planet!*'
  prefs: []
  type: TYPE_NORMAL
- en: '**However... there is a catch.**'
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning is Expensive and the Focus is Narrow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning networks tend to be resource-hungry and computationally expensive.
    Unlike traditional statistical learning models (such as regression, decision trees,
    or support vector machines), they tend to contain millions of parameters and therefore
    need a lot of training data to avoid overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, deep learning models are trained with **massive amounts of high-dimensional
    raw data** such as images, unstructured text, or audio signals. Also, they employ **millions
    of vectorized computation** (e.g., matrix multiplication) over and over, to optimize
    the huge parameter set to fit the data. Moreover, they are built with a [large
    number of hyperparameters](https://towardsdatascience.com/hyperparameters-in-deep-learning-927f7b2084dd) (e.g.
    number of layers, neurons per layer, optimizer algorithm settings, etc.), and
    it often takes many weeks or months for a team of highly trained researchers to
    create a state-of-the-art model.
  prefs: []
  type: TYPE_NORMAL
- en: '*All of these lead to a great demand on the computational power needed to train
    and robust and high-performance deep learning model, optimized for a given task.*'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we can afford to train a great model after spending a huge amount
    of computational resources. **Don’t we want to re-use this model for the maximum
    number and variety of tasks and reap the benefit of our investment many times
    over**?
  prefs: []
  type: TYPE_NORMAL
- en: '*Herein lies the problem.*'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning algorithms, so far, have been **traditionally designed to work
    in isolation**. These algorithms are trained to [solve specific tasks](https://bdtechtalks.com/2018/04/23/strong-ai-vs-weak-ai-deep-learning/).
    The models, in most cases, have to be **rebuilt from scratch** once the feature-space
    distribution changes.
  prefs: []
  type: TYPE_NORMAL
- en: But, this does not make sense, especially when it is compared to how we humans
    currently utilize our limited computation speed.
  prefs: []
  type: TYPE_NORMAL
- en: Humans have an [inherent ability to transfer knowledge across tasks](https://www.nap.edu/read/9853/chapter/6#52).
    What we acquire as knowledge while learning about one task, we utilize in the
    same way to solve related tasks. If the similarity between the tasks or domains
    is high, we are able to cross-utilize our ‘learned’ knowledge better.
  prefs: []
  type: TYPE_NORMAL
- en: '*Transfer learning is the idea of overcoming the isolated learning paradigms
    and utilizing the knowledge acquired for one task to solve related ones, as applied
    to machine learning, and in particular, to the domain of deep learning.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c6555dd2aeee750bd0f47985e3600e0d.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Image source: [A Comprehensive Hands-on Guide to Transfer Learning with Real-World
    Applications in Deep Learning](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a).*'
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Learning for Deep Learning Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is a myriad of strategies to follow for the transfer learning process
    in the deep learning setting, and multiple important things to consider and engineering
    decisions to make – similarity of datasets and domains, supervised or unsupervised
    setting, how much retraining to be done, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, to put it very simply, we can assume that for transfer learning:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to take a pre-trained deep learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Re-use all or certain portions of it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply it to our new domain of interest for a particular machine learning problem–classification
    or regression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this way, we are able to avoid a large portion of the huge computational
    effort of training and optimizing a large deep learning model.
  prefs: []
  type: TYPE_NORMAL
- en: '*In the end, a trained deep learning model is just a collection of millions
    of real numbers in a particular data structure format, which can be used readily
    for prediction/inference, the task we are really interested in, as the consumers
    of the model.*'
  prefs: []
  type: TYPE_NORMAL
- en: But remember that a pre-trained model might have been trained using a particular
    classification in mind, i.e. its output vector and computation graph is suited
    for prediction of a particular task only.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, a widely used strategy in transfer learning is to:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the weights matrices of a pre-trained model except for the weights of the
    very last layers near the O/P,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hold those weights fixed, i.e. untrainable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attach new layers suitable for the task at hand, and train the model with new
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/4513886ec76fb18b9b3ca3a77712aeaa.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Fig: The transfer learning strategy for deep learning networks, as we will
    explore here.*'
  prefs: []
  type: TYPE_NORMAL
- en: This way, we don’t have to train the whole model, we get to **repurpose the
    model for our specific machine learning task**, yet can leverage the learned structures
    and patterns of the data, contained in the fixed weights, which are loaded from
    the pre-trained, optimized model.
  prefs: []
  type: TYPE_NORMAL
- en: A Hands-On Example You Can Run on Your Laptop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s get our hands dirty and build a simple code demo to demonstrate the power
    of transfer learning, shall we?
  prefs: []
  type: TYPE_NORMAL
- en: '**Why not follow a traditional approach?**'
  prefs: []
  type: TYPE_NORMAL
- en: Now, traditionally, tutorials on this topic have focused on learning from famous,
    high-performance deep learning networks, such as [VGGNet-16](https://neurohive.io/en/popular-networks/vgg16/), [ResNet-50](https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624),
    or [Inception-V3/V4](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202),
    etc. These networks were trained on the massive [ImageNet database](http://www.image-net.org/),
    and secured top places in the [annual ImageNet competition – ILSVRC](http://www.image-net.org/challenges/LSVRC/),
    thereby positioning themselves as the golden benchmark models for image classification
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: However, the issue with these networks is that they contain a large number of
    complex layers, and not easy to understand when you are starting to learn deep
    learning concepts.
  prefs: []
  type: TYPE_NORMAL
- en: '*Therefore, if you want to code up a transfer learning example from scratch,
    it may be beneficial, from a self-learning and confidence-building point of view,
    to try an independent example first. You can train a deep learning model first,
    transfer its learning to another seed network, and then show the performance on
    a standard classification task.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**What are we going to do in this tutorial?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we will demonstrate the transfer learning concept in a very
    simple setting using Python, working with the Keras package (TensorFlow backend).
    We will take the well-known CIFAR-10 dataset and do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a Keras neural net by stacking a set of classificiation_layers on top
    of a set of feature_layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the resulting network on a partial CIFAR-10 dataset, consisting of examples
    from the first 5 categories (0…4).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Freeze the feature_layers and stack a new set of fully-connected layers on top
    of them, thereby creating another conv net
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train this new conv net on the examples from the rest of the categories (5..9)
    of CIFAR-10, tuning weights only for those densely connected layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The entire code is open-source and [can be found here](https://github.com/tirthajyoti/Deep-learning-with-Python/blob/master/Notebooks/Transfer_learning_CIFAR.ipynb).
    We show only some essential parts of the code here in this article.
  prefs: []
  type: TYPE_NORMAL
- en: '**Code Along**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the necessary libraries and functions:'
  prefs: []
  type: TYPE_NORMAL
- en: from time import time
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: import keras
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: from keras.datasets import mnist,cifar10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: from keras.models import Sequential
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: from keras.layers import Dense, Dropout, Activation, Flatten
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: from keras.layers import Conv2D, MaxPooling2D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: from keras.optimizers import Adam
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: from keras import backend as K
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: import matplotlib.pyplot as plt
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: import random
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we decide some architectural choices for the deep learning models. We
    will use the convolutional neural net (CNN), as it is the most suited for image
    classification task:'
  prefs: []
  type: TYPE_NORMAL
- en: number of convolutional filters to use
  prefs:
  - PREF_OL
  - PREF_H1
  type: TYPE_NORMAL
- en: filters = 64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: size of pooling area for max pooling
  prefs:
  - PREF_OL
  - PREF_H1
  type: TYPE_NORMAL
- en: pool_size = 2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: convolution kernel size
  prefs:
  - PREF_OL
  - PREF_H1
  type: TYPE_NORMAL
- en: kernel_size = 3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we split the dataset into training and validation sets, and create two
    datasets – one with class labels below 5 and one with 5 and above. Why do we do
    that?
  prefs: []
  type: TYPE_NORMAL
- en: The whole CIFAR-10 dataset has 10 categories of images in a very small size.** We
    will have two neural networks**. One will be pre-trained, and the learning will
    be transferred to a second network. But, we will not use all the categories of
    image to train both networks. **The first one will be trained only using first
    5 categories of images and this learning will help the second network to learn
    the last 5 categories of images faster**.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2d79361e3f786d6866899a439699885b.png)'
  prefs: []
  type: TYPE_IMG
- en: '*All 10 categories of images in the CIFAR-10 dataset.*'
  prefs: []
  type: TYPE_NORMAL
- en: Here are some random images from the first 5 categories, which the first neural
    network will ‘see’ and be trained on. The categories are – **airplane, automobile,
    bird, cat,** or **deer.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37af3bf1cfccd33cb5a0ec7f18282f57.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Fig: First 5 categories of images, seen only by the first neural network.*'
  prefs: []
  type: TYPE_NORMAL
- en: But we are actually interested in building a neural net for the last 5 categories
    of images – **dog, frog, horse, sheep, **or** truck**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d2624dfcc6e5e50b7d33aa383dd7dcf.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Fig: Last 5 categories of images, seen only by the second neural network.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define two groups/types of layers: feature (convolutions) and classification
    (dense).'
  prefs: []
  type: TYPE_NORMAL
- en: Again, do not be bothered about the implementation details of these code snippets.
    You can learn the details from any standard tutorial on Keras package. The idea
    is to understand the concept.
  prefs: []
  type: TYPE_NORMAL
- en: 'The feature layers:'
  prefs: []
  type: TYPE_NORMAL
- en: feature_layers = [
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Conv2D**(filters, kernel_size,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: padding='valid',
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: input_shape=input_shape),
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Activation**(''relu''),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Conv2D**(filters, kernel_size),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Activation**(''relu''),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**MaxPooling2D**(pool_size=pool_size),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Dropout**(25),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Flatten**(),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ']'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The dense classification layer:'
  prefs: []
  type: TYPE_NORMAL
- en: classification_layers = [
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Dense**(128),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Activation**(''relu''),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Dropout**(25),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Dense**(num_classes),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Activation**(''softmax'')'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ']'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we Create the complete model by stacking together **feature_layers** and **classification_layers**.
  prefs: []
  type: TYPE_NORMAL
- en: model_1 = **Sequential**(feature_layers + classification_layers)
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We then define a function for training the model (not shown) and just train
    the model for certain number of epochs to reach a good enough performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**train_model**(model_1,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (x_train_lt5, y_train_lt5),
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (x_test_lt5, y_test_lt5), num_classes)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can show how the accuracy of the network evolved over training epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e35416277e8b1316d8d807f8a6b4cb6d.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Fig: Validation set accuracy over epochs while training the first network.*'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we **freeze feature layers and rebuild the model**.
  prefs: []
  type: TYPE_NORMAL
- en: This freezing of feature layers is at the heart of transfer learning. This allows
    re-use of the pre-trained model for classification tasks because users can just
    stack up new fully-connected layers on top of the pre-trained feature layers and
    get good performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create a fresh new model called **model_2** with the **untrainable** **feature_layers** and **trainable** **classification_layers**. We
    show the summary in the figure below:'
  prefs: []
  type: TYPE_NORMAL
- en: 'for l in feature_layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: trainable = False
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: model_2 = **Sequential**(feature_layers + classification_layers)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/0e2f59f900013b67c13f41b64fa377c1.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Fig: The model summary of the second network showing the fixed and trainable
    weights. The fixed weights are transferred directly from the first network.*'
  prefs: []
  type: TYPE_NORMAL
- en: Now we train the second model and observe **how it takes less overall time and
    still gets equal or higher performance**.
  prefs: []
  type: TYPE_NORMAL
- en: '**train_model**(model_2,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (x_train_gte5, y_train_gte5),(x_test_gte5, y_test_gte5), num_classes)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The accuracy of the second model is even higher than the first model, although
    this may not be the case all the time, and depends on the model architecture and
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f74b9408a1d84fe41383f4fb5a738b53.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Fig: Validation set accuracy over epochs while training the second network.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The time taken for training two models are shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5dec718a5302933f8930708ff6d0b97d.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Fig: Training time for the two networks.*'
  prefs: []
  type: TYPE_NORMAL
- en: What Did We Achieve?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Not only did the **model_2** train faster than **model_1**, it also started
    at a higher baseline accuracy and achieved better final accuracy for the same
    number of epochs and identical hyperparameters (learning rate, optimizer, batch
    size, etc.). And it achieved this training on images that were not seen by **model_1**.
  prefs: []
  type: TYPE_NORMAL
- en: This means that although **model_1** was trained on images of – **airplane,
    automobile, bird, cat,** or **deer** – it’s learned weights, when transferred
    to **model_2**, helped **model_2** achieve excellent performance on the classification
    of completely different categories of images – **dog, frog, horse, sheep, **or** truck**.
  prefs: []
  type: TYPE_NORMAL
- en: Isn’t that amazing? And you can now build this kind of transfer learning with
    so few lines of codes. Again, the entire code is open-source and [can be found
    here](https://github.com/tirthajyoti/Deep-learning-with-Python/blob/master/Notebooks/Transfer_learning_CIFAR.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://blog.exxactcorp.com/transfer-learning-made-easy/). Reposted
    with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Building NLP Classifiers Cheaply With Transfer Learning and Weak Supervision](https://www.kdnuggets.com/2019/03/building-nlp-classifiers-cheaply-transfer-learning-weak-supervision.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Recycling Deep Learning Models with Transfer Learning](https://www.kdnuggets.com/2015/08/recycling-deep-learning-representations-transfer-ml.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The State of Transfer Learning in NLP](https://www.kdnuggets.com/2019/09/state-transfer-learning-nlp.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[TensorFlow for Computer Vision - Transfer Learning Made Easy](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Parallel Processing in Prompt Engineering: The Skeleton-of-Thought…](https://www.kdnuggets.com/parallel-processing-in-prompt-engineering-the-skeleton-of-thought-technique)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LLaMA 3: Meta’s Most Powerful Open-Source Model Yet](https://www.kdnuggets.com/llama-3-metas-most-powerful-open-source-model-yet)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning Made Simple for Data Analysts with BigQuery ML](https://www.kdnuggets.com/machine-learning-made-simple-for-data-analysts-with-bigquery-ml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Mistake Every Data Scientist Has Made at Least Once](https://www.kdnuggets.com/2022/09/mistake-every-data-scientist-made-least.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Combining Pandas DataFrames Made Simple](https://www.kdnuggets.com/2022/09/combining-pandas-dataframes-made-simple.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
