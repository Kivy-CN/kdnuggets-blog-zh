["```py\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout\n\nparams = {\n    'dropout': 0.25,\n    'batch-size': 128,\n    'epochs': 50,\n    'layer-1-size': 128,\n    'layer-2-size': 128,\n    'initial-lr': 0.01,\n    'decay-steps': 2000,\n    'decay-rate': 0.9,\n    'optimizer': 'adamax'\n}\n\nmnist = tf.keras.datasets.mnist  \nnum_class = 10\n\n# split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# reshape and normalize the data\nx_train = x_train.reshape(60000, 784).astype(\"float32\")/255\nx_test = x_test.reshape(10000, 784).astype(\"float32\")/255\n\n# convert class vectors to binary class matrices\ny_train = to_categorical(y_train, num_class)\ny_test = to_categorical(y_test, num_class)\n```", "```py\n# Model Definition\n# Get parameters from logged hyperparameters\nmodel = Sequential([\n  Flatten(input_shape=(784, )),\n  Dense(params('layer-1-size'), activation='relu'),\n  Dense(params('layer-2-size'), activation='relu'),\n  Dropout(params('dropout')),\n  Dense(10)\n  ])\n\nlr_schedule = \n    tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=experiment.get_parameter('initial-lr'),\n    decay_steps=experiment.get_parameter('decay-steps'),\n    decay_rate=experiment.get_parameter('decay-rate')\n    )\n\nloss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n\nmodel.compile(optimizer='adamax', \n              loss=loss_fn,\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train,\n    batch_size=experiment.get_parameter('batch-size'),\n    epochs=experiment.get_parameter('epochs'),\n    validation_data=(x_test, y_test),)\n\nscore = model.evaluate(x_test, y_test)\n\n# Log Model\nmodel.save('tf-mnist-comet.h5')\n```"]