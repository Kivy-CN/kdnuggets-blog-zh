- en: 'XGBoost Explained: DIY XGBoost Library in Less Than 200 Lines of Python'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost 解释：少于200行Python代码实现DIY XGBoost库
- en: 原文：[https://www.kdnuggets.com/2021/05/xgboost-explained-diy-xgboost-library-200-lines-python.html](https://www.kdnuggets.com/2021/05/xgboost-explained-diy-xgboost-library-200-lines-python.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/05/xgboost-explained-diy-xgboost-library-200-lines-python.html](https://www.kdnuggets.com/2021/05/xgboost-explained-diy-xgboost-library-200-lines-python.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Guillaume Saupin](https://www.linkedin.com/in/guillaume-saupin-5802aa31/),
    CTO at Verteego**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Guillaume Saupin](https://www.linkedin.com/in/guillaume-saupin-5802aa31/)
    供稿，Verteego的CTO**'
- en: '![](../Images/2af80546d15b24dbb95b3e7f06ad2b1d.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2af80546d15b24dbb95b3e7f06ad2b1d.png)'
- en: Photo by [Jens Lelie](https://unsplash.com/@madebyjens?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Jens Lelie](https://unsplash.com/@madebyjens?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: XGBoost is probably one of the most widely used libraries in data science. Many
    data scientists around the world are using it. It’s a very versatile algorithm
    that can be use to perform classification, regression as well as confidence intervals
    as shown in this [article](https://towardsdatascience.com/confidence-intervals-for-xgboost-cac2955a8fde).
    But how many really understand its underlying principles?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost可能是数据科学中最广泛使用的库之一。世界各地的许多数据科学家都在使用它。这是一个非常通用的算法，可以用于分类、回归以及置信区间，如在这篇[文章](https://towardsdatascience.com/confidence-intervals-for-xgboost-cac2955a8fde)中所示。但有多少人真正理解其基本原理呢？
- en: You might think that a Machine Learning algorithm that performs as well as XGBoost
    might be using very complex and advanced mathematics. You probably imagine that
    it’s a masterpiece of software engineering.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能认为像XGBoost这样的机器学习算法会使用非常复杂和先进的数学方法。你可能会想象它是一个软件工程的杰作。
- en: And you’re partly right. The XGBoost library is a pretty complex one, but if
    you consider only the mathematical formulation of gradient boosting applied to
    decision trees, it’s not that complicated.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你说得对。XGBoost库确实很复杂，但如果仅考虑应用于决策树的梯度提升的数学公式，其实并不复杂。
- en: You will see below in detail how to train decision trees for **regression** using
    the gradient boosting method with less than 200 lines of code.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在下面详细看到如何使用梯度提升方法训练**回归**决策树，代码少于200行。
- en: Decision Tree
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树
- en: 'Before getting into the mathematical details, let’s refresh our memory regarding
    decision trees. The principle is fairly simple: associate a value to a given set
    of features traversing a binary tree. Each node of the binary tree is attached
    to a condition; leaves contain values.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入数学细节之前，让我们先回顾一下决策树。其原理相当简单：将一个给定特征集的值与遍历二叉树相关联。每个二叉树节点都附带一个条件；叶子节点包含值。
- en: If the condition is true, we continue the traversal using the left node, if
    not, we use the right one. Once a leaf is reached, we have our prediction.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果条件为真，我们继续使用左节点进行遍历，如果条件为假，则使用右节点。一旦到达叶子节点，我们就得到了预测结果。
- en: 'As often, a picture is worth a thousand words:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 正如常见的那样，一张图片胜过千言万语：
- en: '![](../Images/f681ccf5b863618aa3ba615f5f7ed042.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f681ccf5b863618aa3ba615f5f7ed042.png)'
- en: A three-level decision tree. Image by the author.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 三层决策树。图片由作者提供。
- en: The condition attached to a node can be regarded as a decision, hence the name **Decision
    Tree**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 附加到节点的条件可以视为一个决策，因此得名**决策树**。
- en: 'This kind of structure is pretty old in the history of computer science and
    has been used for decades with success. A basic implementation is given by the
    following lines:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构在计算机科学历史中相当久远，并且成功地使用了几十年。基本实现如下：
- en: Stacking multiple trees
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 堆叠多个树
- en: Even though Decision Trees have been used with some success in a number of applications,
    like expert systems (before the AI winter) it remains a very basic model that
    cannot handle the complexity usually encountered in real-world data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管决策树在一些应用中，如专家系统（在AI寒冬之前）取得了一定的成功，但它仍然是一个非常基础的模型，无法处理通常在实际数据中遇到的复杂性。
- en: We usually refer to this kind of estimators as ***weak models***.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将这种类型的估计器称为***弱模型***。
- en: To overcome this limitation, the idea emerged in the nineties to combine multiple *weak
    models* to create a *strong model: ****ensemble learning****.*
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这一局限性，九十年代出现了将多个*弱模型*结合起来创建*强模型*的想法：****集成学习****。
- en: This method can be applied to any kind of model, but as Decision Trees are simple,
    fast, generic and easily interpretable models, they are commonly used.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以应用于任何类型的模型，但由于决策树是简单、快速、通用且易于解释的模型，因此它们被广泛使用。
- en: Various strategies can be deployed to combine models. We can for instance use
    a weighted sum of each model prediction. Or even better, use a Bayesian approach
    to combine them based on learning.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 可以采用各种策略来组合模型。例如，我们可以使用每个模型预测的加权和。更好的是，使用贝叶斯方法根据学习来组合它们。
- en: 'XGBoost and all **boosting** methods use another approach: each new model tries
    to compensate for the errors of the previous one.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 和所有**提升**方法使用另一种方法：每个新模型试图弥补前一个模型的错误。
- en: Optimizing decision trees
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化决策树
- en: 'As we have seen above, making predictions using a decision tree is straightforward.
    The job is hardly more complicated when using *ensemble learning*: all we have
    to do is sum contributions of each model.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们上面所看到的，使用决策树进行预测是直接的。使用*集成学习*时，工作也不会更复杂：我们只需将每个模型的贡献相加即可。
- en: What’s really complicated is building the tree itself! How can we find the best
    condition to apply at each node of our training dataset? This is where math helps
    us. A full derivation can be found in the [XGBoost documentation](https://xgboost.readthedocs.io/en/latest/tutorials/model.html).
    Here, we will focus only on the formulas of interest for this article.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上复杂的是构建树本身！我们如何找到在训练数据集的每个节点上应用的最佳条件？这就是数学发挥作用的地方。完整的推导可以在 [XGBoost 文档](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)中找到。在这里，我们将只关注本文感兴趣的公式。
- en: 'As always in machine learning, we want to set our model parameters so that
    our model’s predictions on the training set minimize a given objective:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在机器学习中总是要做的那样，我们希望设置模型参数，使得模型在训练集上的预测最小化给定的目标：
- en: '![](../Images/1c6ea3b60063b729682ac9a687e59dd2.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c6ea3b60063b729682ac9a687e59dd2.png)'
- en: Objective formula. Formula by the author.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 目标公式。公式由作者提供。
- en: 'Please note that this objective is made of two terms:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个目标由两个项组成：
- en: One to measure the error made by the prediction. It’s the famous loss function *l(y,
    y_hat)*.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种用于衡量预测误差的方法。它是著名的损失函数*l(y, y_hat)*。
- en: The other, *omega*, to control model complexity.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个*omega*，用于控制模型复杂度。
- en: 'As stated in the XGBoost [documentation](https://xgboost.readthedocs.io/en/latest/tutorials/model.html),
    complexity is a very important part of the objective that allows us to tune the
    bias/variance trade-off. Many different functions can be used to define this regularization
    term. XGBoost use:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 XGBoost [文档](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)中所述，复杂度是目标的一个非常重要的部分，它允许我们调整偏差/方差的权衡。可以使用许多不同的函数来定义这个正则化项。XGBoost
    使用：
- en: '![](../Images/0ce54f0ee55300ffbfdaa119f4139291.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ce54f0ee55300ffbfdaa119f4139291.png)'
- en: Regularization term. Formula by the author.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化项。公式由作者提供。
- en: Here* T* is the total number of leaves, whereas *w_j* are the weights attached
    to each leaf. This means that a large weight and a large number of leaves will
    be penalized.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里* T* 是叶子的总数，而*w_j* 是附加到每个叶子的权重。这意味着大的权重和大量的叶子会受到惩罚。
- en: 'As the error is often a complex, non-linear function, we linearized it using
    second-order Taylor expansion:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于误差通常是复杂的非线性函数，我们使用二阶泰勒展开对其进行线性化：
- en: '![](../Images/30f9a52c0856023e980b2ed196a90f7a.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30f9a52c0856023e980b2ed196a90f7a.png)'
- en: Second-order expansion of the loss. Formula by the author.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 损失的二阶展开。公式由作者提供。
- en: 'where:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '![](../Images/d15672d7944ba013a374896777a30f45.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d15672d7944ba013a374896777a30f45.png)'
- en: Gaussian and hessian formulas. Formula by the author.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯和赫西恩公式。公式由作者提供。
- en: The linearization is computed with respect to the prediction term, as we want
    to estimate how the error changes when the prediction changes. Linearization is
    essential, as it will ease the minimization of the error.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 线性化是相对于预测项计算的，因为我们想估计当预测发生变化时误差如何变化。线性化是必需的，因为它将简化误差的最小化。
- en: Whatwe want to achieve with gradient boosting, is to find the optimal *delta_y_i *that
    will minimize the loss function, i.e. we want to find how to modify the existing
    tree so that the modification improves the prediction.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望通过梯度提升实现的是找到最优的*delta_y_i*，以最小化损失函数，即我们希望找到如何修改现有的树，以便这种修改能够改进预测。
- en: 'When dealing with tree models, there are two kinds of parameters to consider:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 处理树模型时，需要考虑两种参数：
- en: 'The ones defining the tree in itself: the condition for each node, the depth
    of the tree'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义树本身的部分：每个节点的条件，树的深度
- en: The values attached to each leaf of the tree. These values are the predictions
    themselves.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 附加到每个树叶子的值。这些值就是预测值本身。
- en: 'Exploring every tree configurations would be too complex, therefore gradient
    tree boosting methods only consider splitting one node into two leaves. This means
    that we have to optimize three parameters:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 探索每种树的配置将会过于复杂，因此梯度树提升方法只考虑将一个节点拆分成两个叶子。这意味着我们需要优化三个参数：
- en: 'The splitting value: on which condition do we split data'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分裂值：我们根据什么条件来分裂数据
- en: The value attached to the left leaf
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 附加到左叶子的值
- en: The value attached to the right leaf
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 附加到右叶子的值
- en: 'In the XGBoost documentation, the best value for a leaf *j* with respect to
    the objective is given by:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 XGBoost 文档中，相对于目标的叶子 *j* 的最佳值由以下公式给出：
- en: '![](../Images/30ba452986e0c2f8e110335381fe9963.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30ba452986e0c2f8e110335381fe9963.png)'
- en: Optimal leaf value with respect to objective. Formula by the author.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于目标的最佳叶子值。公式由作者提供。
- en: where *G_j *is the sum of the gradient of the training points attached to the
    node *j*, and *H_j* is the sum of the hessian of the training points attached
    to the node *j.*
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *G_j* 是附加到节点 *j* 的训练点梯度的总和，*H_j* 是附加到节点 *j* 的训练点海森矩阵的总和。
- en: 'The reduction in the objective function obtained with this optimal param is:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个最佳参数得到的目标函数减少量是：
- en: '![](../Images/92074bf0fd43651aab5b4918039c0764.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92074bf0fd43651aab5b4918039c0764.png)'
- en: Objective improvement when using optimal weight. Formula by the author.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最佳权重时的目标改进。公式由作者提供。
- en: 'Choosing the right split value is done using brut force: we compute the improvement
    for each split value, and we keep the best one.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的分裂值是使用穷举法：我们计算每个分裂值的改进，并保留最佳的一个。
- en: We now have all the required mathematical information to boost an initial tree
    performance with respect to a given objective by adding new leaves.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在拥有了所有所需的数学信息，可以通过添加新的叶子来提升初始树的性能，以达到给定的目标。
- en: Before doing it concretely, let’s take some time to understand what these formulas
    mean.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在具体实施之前，让我们花点时间理解这些公式的含义。
- en: Grokking gradient boosting
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解梯度提升
- en: Let’s try to get some insight on how weight is computed, and what *G_j* and *H_i* equal
    to. As they are respectively the gradient and hessian of the loss function with
    respect to the prediction, we have to pick a loss function.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试了解权重是如何计算的，以及 *G_j* 和 *H_i* 等于多少。由于它们分别是损失函数对预测值的梯度和海森矩阵，我们必须选择一个损失函数。
- en: 'We will focus on the squared error, which is commonly used and is the default
    objective for XGBoost:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将关注平方误差，它是常用的，并且是 XGBoost 的默认目标：
- en: '![](../Images/bd91ae15b1e00e8ed1dd2fc927f73301.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd91ae15b1e00e8ed1dd2fc927f73301.png)'
- en: Squared error loss function. Formula by the author
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 平方误差损失函数。公式由作者提供。
- en: 'It’s a pretty simple formula, whose gradient is:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当简单的公式，其梯度为：
- en: '![](../Images/9941cb63747e203b1bc9de0f40549606.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9941cb63747e203b1bc9de0f40549606.png)'
- en: The gradient of the loss function. Formula by the author.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的梯度。公式由作者提供。
- en: 'and hessian:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以及海森矩阵：
- en: '![](../Images/4821077093599e50bb41001008817d81.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4821077093599e50bb41001008817d81.png)'
- en: Hessian of the loss function. Formula by the author.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的海森矩阵。公式由作者提供。
- en: 'Hence, if we remember the formulas for the optimal weight that maximize error
    reduction:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们记得最大化误差减少的最佳权重的公式：
- en: '![](../Images/30ba452986e0c2f8e110335381fe9963.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30ba452986e0c2f8e110335381fe9963.png)'
- en: Optimal leaf value with respect to the objective. Formula by the author.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于目标的最佳叶子值。公式由作者提供。
- en: We realize that the optimal weight, *i.e. *the value that we add to the previous
    prediction is the opposite of the average error between the previous prediction
    and the real value (when regularization is disabled, i.e. *lambda = 0*). Using
    the squared loss to train Decision Trees with gradient boosting just boils down
    to updating the prediction with the average error in each new node.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们意识到，最佳权重，*即*我们在之前的预测中添加的值，正好是之前预测与实际值之间的平均误差的相反数（当正则化被禁用时，即 *lambda = 0*）。使用平方损失训练带有梯度提升的决策树最终只是将预测值与每个新节点的平均误差进行更新。
- en: We also see that *lambda* has the expected effect, that is ensuring that weights
    are not too large, as weight is inversely proportional to *lambda*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到，*lambda* 具有预期的效果，即确保权重不会过大，因为权重与 *lambda* 成反比。
- en: Training decision trees
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练决策树
- en: Now comes the easy part. Let’s say that we have an existing decision tree that
    ensures predictions with a given error. We want to reduce the error and improve
    the attached objective by splitting one node.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是简单的部分。假设我们有一个现有的决策树，它能以给定的误差进行预测。我们想通过分裂一个节点来减少误差并改善附带的目标。
- en: 'The algorithm to do so is pretty straightforward:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一点的算法非常直接：
- en: Pick a feature of interest
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个感兴趣的特征。
- en: Order data points attached to the current node using values of the selected
    feature
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据所选特征的值对当前节点附加的数据点进行排序。
- en: pick a possible split value
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个可能的分裂值。
- en: Put data points below this split value in the right node, and the one above
    in the left node
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据点低于此分裂值的放入右节点，高于此值的放入左节点。
- en: compute objective reduction for the parent node, the right node and the left
    one
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算父节点、右节点和左节点的目标减少。
- en: If the sum of the objective reduction for left and right nodes is greater than
    the one of the parent node, keep the split value as the best one
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果左节点和右节点的目标减少总和大于父节点的目标，则将分裂值保留为最佳值。
- en: Iterate for each split value
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对每个分裂值进行迭代。
- en: Use the best split value if any, and add the two new nodes
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果有最佳分裂值，则使用该值，并添加两个新节点。
- en: If no splitting improved the objective, don’t add child nodes.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果没有分裂能改善目标，则不要添加子节点。
- en: The resulting code creates a DecisionTree class, that is configured by an objective,
    a number of estimators, i.e. the number of trees, and a max depth.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的代码创建了一个DecisionTree类，该类由一个目标、一个估计器数量（即树的数量）和一个最大深度进行配置。
- en: 'As promised the code takes less than 200 lines:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如承诺，代码少于200行：
- en: The core of the training is coded in the function *_find_best_split*. It essentially
    follows the steps detailed above.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的核心代码在函数*_find_best_split*中。它基本上遵循上述详细步骤。
- en: Note that to support any kind of objective, without the pain of manually calculating
    the gradient and the hessian, we use automatic differentiation and the [jax](https://jax.readthedocs.io/en/latest/index.html) library
    to automate computations.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了支持任何目标，而无需手动计算梯度和海森矩阵，我们使用自动微分和[jax](https://jax.readthedocs.io/en/latest/index.html)库来自动化计算。
- en: Initially, we start with a tree with only one node whose leaf value leaf is
    zero. As we have mimic XGBoost, we also use a base score, that we set to be the
    mean of the value to predict.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们从一个只有一个节点的树开始，其叶子值为零。由于我们模仿XGBoost，我们还使用了一个基础分数，该分数设为待预测值的均值。
- en: Also, note that at line 126 we stop tree building if we reach the max depth
    defined when initializing the tree. We could have used other conditions like a
    minimum number of sample for each leaf or a minimum value for the new weight.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请注意，在第126行，如果达到初始化树时定义的最大深度，我们将停止树的构建。我们还可以使用其他条件，如每个叶子的最小样本数量或新权重的最小值。
- en: Another very important point is the choice of the feature used for the splitting.
    Here, for the sake of simplicity, features are selected randomly, but we could
    have been using smarter strategies, like using the feature having the greatest
    variance.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常重要的点是选择用于分裂的特征。在这里，为了简单起见，特征是随机选择的，但我们本可以使用更智能的策略，例如选择具有最大方差的特征。
- en: Conclusion
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we have seen how gradient boosting works to train decision
    trees. To further improve our understanding, we have written the minimal set of
    lines required to train an ensemble of decision trees and use them for prediction.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们了解了梯度提升如何训练决策树。为了进一步提高我们的理解，我们编写了训练决策树集合并用于预测所需的最小行数。
- en: Having a deep understanding of the algorithms we use for machine learning is
    absolutely critical. It not only helps us building better models but more importantly
    allows us to bend these models to our needs.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 深刻理解我们用于机器学习的算法至关重要。这不仅有助于我们构建更好的模型，更重要的是使我们能够根据需要调整这些模型。
- en: For instance, in the case of gradient boosting, playing with loss functions
    is an excellent way to increase precision when predicting.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在梯度提升的情况下，调整损失函数是提高预测精度的一个极好方法。
- en: '**Bio: [Guillaume Saupin](https://www.linkedin.com/in/guillaume-saupin-5802aa31/)**
    is CTO at Verteego.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介：[Guillaume Saupin](https://www.linkedin.com/in/guillaume-saupin-5802aa31/)**
    是Verteego的CTO。'
- en: '[Original](https://towardsdatascience.com/diy-xgboost-library-in-less-than-200-lines-of-python-69b6bf25e7d9).
    Reposted with permission.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/diy-xgboost-library-in-less-than-200-lines-of-python-69b6bf25e7d9)。经许可转载。'
- en: '**Related:**'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Gradient Boosted Decision Trees – A Conceptual Explanation](/2021/04/gradient-boosted-trees-conceptual-explanation.html)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[梯度提升决策树 – 概念解释](/2021/04/gradient-boosted-trees-conceptual-explanation.html)'
- en: '[XGBoost: What it is, and when to use it](/2020/12/xgboost-what-when.html)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost：它是什么，以及何时使用](/2020/12/xgboost-what-when.html)'
- en: '[Many Heads Are Better Than One: The Case For Ensemble Learning](/2019/09/ensemble-learning.html)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[三人行必有我师：集成学习的案例](/2019/09/ensemble-learning.html)'
- en: '* * *'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的 IT 部门'
- en: '* * *'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Multi-modal deep learning in less than 15 lines of code](https://www.kdnuggets.com/2023/01/predibase-multi-modal-deep-learning-less-15-lines-code.html)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[少于 15 行代码实现多模态深度学习](https://www.kdnuggets.com/2023/01/predibase-multi-modal-deep-learning-less-15-lines-code.html)'
- en: '[KDnuggets News, July 20: Machine Learning Algorithms Explained in…](https://www.kdnuggets.com/2022/n29.html)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，7 月 20 日：机器学习算法解释…](https://www.kdnuggets.com/2022/n29.html)'
- en: '[Machine Learning Algorithms Explained in Less Than 1 Minute Each](https://www.kdnuggets.com/2022/07/machine-learning-algorithms-explained-less-1-minute.html)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个机器学习算法在不到 1 分钟内解释](/2022/07/machine-learning-algorithms-explained-less-1-minute.html)'
- en: '[Become a Business Intelligence Analyst in Less Than 6 Months](https://www.kdnuggets.com/become-a-business-intelligence-analyst-in-less-than-6-months)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在不到 6 个月内成为商业智能分析师](https://www.kdnuggets.com/become-a-business-intelligence-analyst-in-less-than-6-months)'
- en: '[DIY Automated Machine Learning with Streamlit](https://www.kdnuggets.com/2021/11/diy-automated-machine-learning-app.html)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Streamlit 自制自动化机器学习](/2021/11/diy-automated-machine-learning-app.html)'
- en: '[Black Friday Deal - Master Machine Learning for Less with DataCamp](https://www.kdnuggets.com/2022/11/datacamp-black-friday-deal-master-machine-learning-less-datacamp.html)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[黑色星期五特惠 - 用 DataCamp 以更低的价格精通机器学习](https://www.kdnuggets.com/2022/11/datacamp-black-friday-deal-master-machine-learning-less-datacamp.html)'
