- en: Hyperparameter Optimization for Machine Learning Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型的超参数优化
- en: 原文：[https://www.kdnuggets.com/2020/05/hyperparameter-optimization-machine-learning-models.html](https://www.kdnuggets.com/2020/05/hyperparameter-optimization-machine-learning-models.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/05/hyperparameter-optimization-machine-learning-models.html](https://www.kdnuggets.com/2020/05/hyperparameter-optimization-machine-learning-models.html)
- en: '[comments](#comments)![Figure](../Images/e60360ab2907502e8bfd8adc536d14bf.png)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)![图示](../Images/e60360ab2907502e8bfd8adc536d14bf.png)'
- en: '[Credits](https://swisscognitive.ch/2020/01/13/machine-learning-in-adversity/)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[致谢](https://swisscognitive.ch/2020/01/13/machine-learning-in-adversity/)'
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能。'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的 IT。'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Model optimization is one of the toughest challenges in the implementation of
    machine learning solutions. Entire branches of machine learning and deep learning
    theory have been dedicated to the optimization of models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 模型优化是实现机器学习解决方案中的最大挑战之一。整个机器学习和深度学习理论的分支都致力于模型优化。
- en: Hyperparameter optimization in machine learning intends to find the hyperparameters
    of a given machine learning algorithm that deliver the best performance as measured
    on a validation set. Hyperparameters, in contrast to model parameters, are set
    by the machine learning engineer before training. The number of trees in a random
    forest is a hyperparameter while the weights in a neural network are model parameters
    learned during training. I like to think of hyperparameters as the model settings
    to be tuned so that the model can optimally solve the machine learning problem.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的超参数优化旨在找到给定机器学习算法的超参数，以在验证集上测得最佳性能。与模型参数不同，超参数由机器学习工程师在训练前设定。随机森林中的树木数量是一个超参数，而神经网络中的权重是训练过程中学习到的模型参数。我喜欢把超参数看作是模型设置，需要调整以使模型能够最佳地解决机器学习问题。
- en: 'Some examples of model hyperparameters include:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型超参数的例子包括：
- en: The `learning rate` for training a neural network.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练神经网络的`learning rate`。
- en: The `**C**` and `**????**` hyperparameters for support vector machines.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机中的`**C**`和`**????**`超参数。
- en: The `**k**` in k-nearest neighbors.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-最近邻算法中的`**k**`。
- en: Hyperparameter optimization finds a combination of hyperparameters that returns
    an optimal model which reduces a predefined loss function and in turn increases
    the accuracy on given independent data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数优化找到一个超参数组合，以返回一个最优模型，该模型减少了预定义的损失函数，从而提高了给定独立数据上的准确性。
- en: '![Figure](../Images/1a76857e56c4740636dc8f28879c8971.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/1a76857e56c4740636dc8f28879c8971.png)'
- en: '[Classification models with their respective hyperparameters.](https://www.computer.org/csdl/journal/ts/2018/06/07990590/13rRUx0gerA)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[带有相应超参数的分类模型。](https://www.computer.org/csdl/journal/ts/2018/06/07990590/13rRUx0gerA)'
- en: Hyperparameter Optimization methods
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超参数优化方法
- en: 'Hyperparameters can have a direct impact on the training of machine learning
    algorithms. Thus, to achieve maximal performance, it is important to understand
    how to optimize them. Here are some common strategies for optimizing hyperparameters:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数对机器学习算法的训练有直接影响。因此，为了实现最佳性能，了解如何优化它们是很重要的。以下是一些常见的超参数优化策略：
- en: '**1\. Manual Hyperparameter Tuning**'
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**1\. 手动超参数调整**'
- en: Traditionally, hyperparameters were tuned manually by trial and error. This **is** still
    commonly done, and experienced engineers can “guess” parameter values that will
    deliver very high accuracy for ML models. However, there isa continual search
    for better, faster, and more automatic methods to optimize hyperparameters.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，超参数是通过试错法手动调整的。这**仍然**是常见的做法，经验丰富的工程师可以“猜测”出能够为 ML 模型提供非常高准确度的参数值。然而，仍在不断寻找更好、更快、更自动化的优化超参数的方法。
- en: 2\. Grid Search
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2\. 网格搜索
- en: Grid search is arguably the most basic hyperparameter tuning method. With this
    technique, we simply build a model for each possible combination of all of the
    hyperparameter values provided, evaluating each model, and selecting the architecture
    which produces the best results.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索可以说是最基本的超参数调优方法。使用这种技术，我们只是为提供的所有超参数值的每个可能组合构建一个模型，评估每个模型，并选择产生最佳结果的架构。
- en: '![](../Images/0d65f74594b1f825f12beb47db397245.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d65f74594b1f825f12beb47db397245.png)'
- en: Grid-search does NOT only apply to one model type but can be applied across
    machine learning to calculate the best parameters to use for any given model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索不仅适用于一种模型类型，还可以应用于机器学习中的各类模型，以计算用于任何给定模型的最佳参数。
- en: 'For example, a typical soft-margin SVM classifier equipped with an RBF kernel
    has at least two hyperparameters that need to be optimized for good performance
    on unseen data: a regularization constant *C* and a kernel hyperparameter γ. Both
    parameters are continuous, so to perform grid search, one selects a finite set
    of “reasonable” values for each, let’s say'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，典型的软间隔支持向量机（SVM）分类器配备了 RBF 核函数，它有至少两个超参数需要优化以在未见数据上获得良好性能：正则化常数*C*和核超参数 γ。两个参数都是连续的，因此为了进行网格搜索，需要为每个参数选择一个有限的“合理”值，比如
- en: '![](../Images/e78be57e455f5dfa0eac00be4c135791.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e78be57e455f5dfa0eac00be4c135791.png)'
- en: Grid search then trains an SVM with each pair (*C*, γ) in the c[artesian product](https://www.wikiwand.com/en/Cartesian_product) of
    these two sets and evaluates their performance on a held-out validation set (or
    by internal cross-validation on the training set, in which case multiple SVMs
    are trained per pair). Finally, the grid search algorithm outputs the settings
    that achieved the highest score in the validation procedure.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索然后用这两个集合的笛卡尔积中的每一对（*C*, γ）训练一个 SVM，并在保留的验证集上评估它们的性能（或者通过对训练集的内部交叉验证，在这种情况下，每对会训练多个
    SVM）。最后，网格搜索算法输出在验证过程中获得的最高分的设置。
- en: '**How does it work in python?**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**它在 Python 中如何工作？**'
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here’s a python implementation of grid search using `[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)` from
    the `sklearn` library.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个使用`[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)`来自`sklearn`库的网格搜索的
    Python 实现。
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Fitting the Grid Search:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索的实现：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Methods to Run on Grid-Search:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索的方法：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We then use the best set of hyperparameter values chosen in the grid search,
    in the actual model as shown above.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用在网格搜索中选择的最佳超参数值，应用于实际模型，如上所示。
- en: One of the **drawbacks** of grid search is that when it comes to dimensionality,
    it suffers when evaluating the number of hyperparameters grows exponentially.
    However, there is no guarantee that the search will produce the perfect solution,
    as it usually finds one by aliasing around the right set.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索的**缺点**之一是当涉及到维度时，它在评估超参数数量指数增长时表现不佳。然而，没有保证搜索会产生完美的解决方案，因为它通常通过在正确的集合周围徘徊来找到一个。
- en: 3\. Random Search
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3\. 随机搜索
- en: Often some of the hyperparameters matter much more than others. Performing random
    search rather than grid search allows a much more precise discovery of good values
    for the important ones.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，有些超参数比其他超参数更为重要。进行随机搜索而不是网格搜索可以更精确地发现重要超参数的良好值。
- en: '![](../Images/8292a0cb03eeaa9f5cf4f473a2581516.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8292a0cb03eeaa9f5cf4f473a2581516.png)'
- en: Random Search sets up a grid of hyperparameter values and selects random combinations
    to train the model and score. This allows you to explicitly control the number
    of parameter combinations that are attempted. The number of search iterations
    is set based on time or resources. Scikit Learn offers the `RandomizedSearchCV` function
    for this process.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索设置了一个超参数值的网格，并选择随机组合来训练模型和评分。这允许你明确控制尝试的参数组合的数量。搜索迭代的次数根据时间或资源进行设置。Scikit
    Learn 提供了`RandomizedSearchCV`函数来执行这一过程。
- en: While it’s possible that `RandomizedSearchCV` will not find as accurate a result
    as `GridSearchCV`, it surprisingly picks the best result more often than not and
    in a *fraction* of the time it takes `GridSearchCV` would have taken. Given the
    same resources, Randomized Search can even outperform Grid Search. This can be
    visualized in the graphic below when continuous parameters are used.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`RandomizedSearchCV`可能找不到像`GridSearchCV`那么准确的结果，但它往往在比`GridSearchCV`快得多的时间内选出最佳结果。考虑到相同的资源，随机搜索甚至可能优于网格搜索。这在使用连续参数时可以在下面的图示中看到。
- en: The chances of finding the optimal parameter are comparatively higher in random
    search because of the random search pattern where the model might end up being
    trained on the optimized parameters without any aliasing. Random search works
    best for lower dimensional data since the time taken to find the right set is
    less with less number of iterations. Random search is the best parameter search
    technique when there is less number of dimensions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索的机会相对较高，因为随机搜索模式下，模型可能会在优化参数上进行训练而没有任何别名。由于维度较低，随机搜索在找到合适参数集的时间更短，因此适用于较低维度的数据。对于维度较少的情况，随机搜索是最佳的参数搜索技术。
- en: In the case of deep learning algorithms, it outperforms the grid search.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习算法的情况下，它优于网格搜索。
- en: '![Figure](../Images/b1b1600a34c9a27779b36bbb9d9a02cc.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/b1b1600a34c9a27779b36bbb9d9a02cc.png)'
- en: '[Credits](https://community.alteryx.com/t5/Data-Science-Blog/Hyperparameter-Tuning-Black-Magic/ba-p/449289)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[致谢](https://community.alteryx.com/t5/Data-Science-Blog/Hyperparameter-Tuning-Black-Magic/ba-p/449289)'
- en: In the above figure, yay that you have two parameters, with 5x6 grid search
    you check only 5 different parameter values from each of the parameters (six rows
    and five columns on the plot on the left), while with the random search you check
    14 different parameter values of each of the parameters.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，恭喜你有两个参数，通过5x6网格搜索你只检查每个参数的5种不同值（左侧图中的六行五列），而通过随机搜索你检查了每个参数的14种不同值。
- en: '**How does it work in python?**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**它在Python中如何工作？**'
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here’s a python implementation of grid search using `RandomizedSearchCV` of
    the `sklearn` library.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用`sklearn`库的`RandomizedSearchCV`进行网格搜索的Python实现。
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Fitting the Random Search:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索的拟合：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Methods to Run on Grid-Search:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索中运行的方法：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 4\. Bayesian Optimization
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4\. 贝叶斯优化
- en: The previous two methods performed individual experiments building models with
    various hyperparameter values and recording the model performance for each. Because
    each experiment was performed in isolation, it’s very easy to parallelize this
    process. However, because each experiment was performed in isolation, we’re not
    able to use the information from one experiment to improve the next experiment.
    Bayesian optimization belongs to a class of *sequential model-based optimization* (SMBO)
    algorithms that allow for one to use the results of our previous iteration to
    improve our sampling method of the next experiment.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的两个方法通过各种超参数值进行单独实验，并记录每个模型的表现。由于每个实验都是孤立进行的，因此很容易并行化这个过程。然而，由于每个实验都是孤立进行的，我们无法利用一个实验的信息来改进下一个实验。贝叶斯优化属于*基于模型的序列优化*（SMBO）算法的一类，允许我们使用先前迭代的结果来改进下一次实验的采样方法。
- en: This, in turn, limits the number of times a model needs to be trained for validation
    as solely those settings that are expected to generate a higher validation score
    are passed through for evaluation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这反过来限制了模型需要训练的次数，因为只有那些预计会产生更高验证分数的设置才会被传递以供评估。
- en: Bayesian optimization works by constructing a posterior distribution of functions
    (Gaussian process) that best describes the function you want to optimize. As the
    number of observations grows, the posterior distribution improves, and the algorithm
    becomes more certain of which regions in parameter space are worth exploring and
    which are not.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化通过构建最能描述你想优化的函数的后验分布（高斯过程）来工作。随着观察数量的增加，后验分布会改进，算法对参数空间中值得探索和不值得探索的区域变得更加确定。
- en: 'We can see this in the image below:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下面的图像中看到这一点：
- en: '![Figure](../Images/23d29016155406c3b70ff0644e409dd3.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/23d29016155406c3b70ff0644e409dd3.png)'
- en: Source: [bayesian-optimization](https://github.com/fmfn/BayesianOptimization)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[贝叶斯优化](https://github.com/fmfn/BayesianOptimization)
- en: As you iterate over and over, the algorithm balances its needs of exploration
    and exploitation taking into account what it knows about the target function.
    At each step, a Gaussian Process is fitted to the known samples (points previously
    explored), and the posterior distribution, combined with an exploration strategy
    (such as UCB (Upper Confidence Bound), or EI (Expected Improvement)), is used
    to determine the next point that should be explored.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当你一遍遍迭代时，算法会在考虑目标函数已知信息的基础上平衡探索和开发的需求。在每一步中，将为已知样本（之前探索过的点）拟合一个高斯过程，并结合探索策略（如UCB（上置信界）或EI（期望改进）），以确定应探索的下一点。
- en: Using Bayesian Optimization, we can explore the parameter space more smartly,
    and thus reduce the time required to do this process.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝叶斯优化，我们可以更智能地探索参数空间，从而减少进行此过程所需的时间。
- en: 'You can check the python implementation of Bayesian optimization below:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看下面的贝叶斯优化的Python实现：
- en: '[**thuijskens/bayesian-optimization**](https://github.com/thuijskens/bayesian-optimization/blob/master/ipython-notebooks/svm-optimization.ipynb)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[**thuijskens/bayesian-optimization**](https://github.com/thuijskens/bayesian-optimization/blob/master/ipython-notebooks/svm-optimization.ipynb)'
- en: 5\. Gradient-based Optimization
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5\. 基于梯度的优化
- en: It is specially used in the case of Neural Networks. It computes the gradient
    with respect to hyperparameters and optimizes them using the gradient descent
    algorithm.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 它特别用于神经网络的情况。它计算关于超参数的梯度，并使用梯度下降算法对其进行优化。
- en: The calculation of the gradient is the least of problems. At least in times
    of advanced [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) software.
    (**Implementing this in a general way for all sklearn-classifiers, of course,
    is not easy**).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 计算梯度问题并不是最难的，至少在高级的[自动微分](https://en.wikipedia.org/wiki/Automatic_differentiation)软件出现之后。（**当然，为所有sklearn分类器以通用方式实现这一点并不容易**）
- en: '![Figure](../Images/43a7177eaea30f7a4c2b619f758eb304.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/43a7177eaea30f7a4c2b619f758eb304.png)'
- en: '[Credits](https://www.mathworks.com/matlabcentral/fileexchange/27631-derivative-based-optimization)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[Credits](https://www.mathworks.com/matlabcentral/fileexchange/27631-derivative-based-optimization)'
- en: 'And while there are works of people who used this kind of idea, they only did
    this for some specific and well-formulated problem (e.g. SVM-tuning). Furthermore,
    there probably were a lot of assumptions because:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有一些人使用这种想法的工作，但他们只是为一些特定且明确的问题（例如SVM调优）做了这件事。此外，这可能有很多假设，因为：
- en: Why is this not a good idea?
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么这不是一个好主意？
- en: '**1\. Hyperparameter optimization is in general non-smooth**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 超参数优化通常是不平滑的**'
- en: GD really likes smooth functions as a gradient of zero is not helpful
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GD真的喜欢平滑函数，因为零梯度没有帮助。
- en: Each hyper-parameter which is defined by some discrete-set (e.g. choice of l1
    vs. l2 penalization) introduces non-smooth surfaces.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个由某些离散集合定义的超参数（例如l1与l2惩罚的选择）会引入不平滑的表面。
- en: '**2\. Hyperparameter optimization is in general non-convex**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 超参数优化通常是非凸的**'
- en: The whole convergence-theory of gradient descent assumes, that the underlying
    problem is convex.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降的整个收敛理论假设基础问题是凸的。
- en: 'Good-case: you obtain some local-minimum (can be arbitrarily bad).'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 好的情况：你能获得某个局部最小值（可以非常糟糕）。
- en: 'Worst-case: gradient descent is not even converging to some local-minimum.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最坏的情况：梯度下降甚至没有收敛到某个局部最小值。
- en: To get python implementation and more about the Gradient Descent Optimization
    algorithm [click here.](https://hackernoon.com/implementing-different-variants-of-gradient-descent-optimization-algorithm-in-python-using-numpy-809e7ab3bab4)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取Python实现和更多关于梯度下降优化算法的信息，请[点击这里](https://hackernoon.com/implementing-different-variants-of-gradient-descent-optimization-algorithm-in-python-using-numpy-809e7ab3bab4)。
- en: '**6\. Evolutionary Optimization**'
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**6\. 进化优化**'
- en: Evolutionary optimization follows a process inspired by the biological concept
    of evolution and since natural evolution is a dynamic process in a changing environment,
    they are also well suited to dynamic optimization problems.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 进化优化遵循一个受生物进化概念启发的过程，由于自然进化是在变化环境中的动态过程，因此它们也非常适合动态优化问题。
- en: '![](../Images/801484956ac7dd0a3414aecce35df824.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/801484956ac7dd0a3414aecce35df824.png)'
- en: Evolutionary algorithms are often used to find good approximate solutions that
    cannot be easily solved by other techniques. Optimization problems often don’t
    have an exact solution as it may be too time-consuming and computationally intensive
    to find an optimal solution. However, evolutionary algorithms are ideal in such
    situations as they can be used to find a near-optimal solution which is often
    sufficient.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 进化算法通常用于寻找良好的近似解决方案，这些方案无法通过其他技术轻易解决。优化问题通常没有精确的解决方案，因为找到最优解决方案可能过于耗时和计算密集。然而，在这种情况下，进化算法是理想的，因为它们可以用于找到一个接近最优的解决方案，这通常是足够的。
- en: One advantage of evolutionary algorithms is that they develop solutions free
    of any human misconceptions or biases, which means they can produce surprising
    ideas which we might never generate ourselves.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 进化算法的一个优点是它们可以生成没有任何人为误解或偏见的解决方案，这意味着它们可以产生我们可能永远无法自己想出的惊人想法。
- en: You can learn more about evolutionary algorithms [here](https://en.wikipedia.org/wiki/Evolutionary_algorithm).
    You can also check python implementation [here](https://github.com/MorvanZhou/Evolutionary-Algorithm).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[这里](https://en.wikipedia.org/wiki/Evolutionary_algorithm)了解更多关于进化算法的信息。你也可以在[这里](https://github.com/MorvanZhou/Evolutionary-Algorithm)查看
    Python 实现。
- en: As a general rule of thumb, any time you want to optimize tuning hyperparameters,
    think Grid Search and Randomized Search!
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一般来说，每当你想要优化调优超参数时，请考虑网格搜索和随机搜索！
- en: Conclusion
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we’ve learned that finding the right values for hyperparameters
    can be a frustrating task and can lead to underfitting or overfitting machine
    learning models. We saw how this hurdle can be overcome by using Grid Search &
    Randomized Search and other algorithms — which optimize tuning of hyperparameters
    to save time and eliminate the chance of overfitting or underfitting by random
    guessing.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们了解到寻找合适的超参数值可能是一项令人沮丧的任务，可能导致机器学习模型的欠拟合或过拟合。我们看到如何通过使用网格搜索、随机搜索和其他算法来克服这一难题——这些算法优化超参数的调优，以节省时间并消除通过随机猜测导致的过拟合或欠拟合的可能性。
- en: Well, this concludes this article**. **I hope you guys have enjoyed reading
    it, feel free to share your comments/thoughts/feedback in the comment section.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，这篇文章到此结束**。**希望你们喜欢阅读这篇文章，欢迎在评论区分享你的评论/想法/反馈。
- en: Thanks for reading !!!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读 !!!
- en: '**Bio: [Nagesh Singh Chauhan](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)**
    is a Data Science enthusiast. Interested in Big Data, Python, Machine Learning.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：[纳戈什·辛格·乔汉](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)**
    是一名数据科学爱好者。对大数据、Python 和机器学习感兴趣。'
- en: '[Original](https://medium.com/swlh/hyperparameter-optimization-for-machine-learning-models-12582f00ae52).
    Reposted with permission.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/swlh/hyperparameter-optimization-for-machine-learning-models-12582f00ae52)。经许可转载。'
- en: '**Related:**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[How to Do Hyperparameter Tuning on Any Python Script in 3 Easy Steps](/2020/04/hyperparameter-tuning-python.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在 3 个简单步骤中对任何 Python 脚本进行超参数调优](/2020/04/hyperparameter-tuning-python.html)'
- en: '[Coronavirus COVID-19 Genome Analysis using Biopython](/2020/04/coronavirus-covid-19-genome-analysis-biopython.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Biopython 进行冠状病毒 COVID-19 基因组分析](/2020/04/coronavirus-covid-19-genome-analysis-biopython.html)'
- en: '[Practical Hyperparameter Optimization](/2020/02/practical-hyperparameter-optimization.html)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实用超参数优化](/2020/02/practical-hyperparameter-optimization.html)'
- en: More On This Topic
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Hyperparameter Optimization: 10 Top Python Libraries](https://www.kdnuggets.com/2023/01/hyperparameter-optimization-10-top-python-libraries.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[超参数优化：10 个顶级 Python 库](https://www.kdnuggets.com/2023/01/hyperparameter-optimization-10-top-python-libraries.html)'
- en: '[Machine Learning Pipeline Optimization with TPOT](https://www.kdnuggets.com/2021/05/machine-learning-pipeline-optimization-tpot.html)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 TPOT 进行机器学习管道优化](https://www.kdnuggets.com/2021/05/machine-learning-pipeline-optimization-tpot.html)'
- en: '[Hyperparameter Tuning Using Grid Search and Random Search in Python](https://www.kdnuggets.com/2022/10/hyperparameter-tuning-grid-search-random-search-python.html)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用网格搜索和随机搜索进行超参数调优](https://www.kdnuggets.com/2022/10/hyperparameter-tuning-grid-search-random-search-python.html)'
- en: '[Hyperparameter Tuning: GridSearchCV and RandomizedSearchCV, Explained](https://www.kdnuggets.com/hyperparameter-tuning-gridsearchcv-and-randomizedsearchcv-explained)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[超参数调优：GridSearchCV 和 RandomizedSearchCV 解析](https://www.kdnuggets.com/hyperparameter-tuning-gridsearchcv-and-randomizedsearchcv-explained)'
- en: '[SQL Query Optimization Techniques](https://www.kdnuggets.com/2023/03/sql-query-optimization-techniques.html)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQL 查询优化技术](https://www.kdnuggets.com/2023/03/sql-query-optimization-techniques.html)'
- en: '[Database Optimization: Exploring Indexes in SQL](https://www.kdnuggets.com/2023/07/database-optimization-exploring-indexes-sql.html)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据库优化：探索 SQL 中的索引](https://www.kdnuggets.com/2023/07/database-optimization-exploring-indexes-sql.html)'
