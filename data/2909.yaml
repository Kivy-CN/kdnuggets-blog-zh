- en: Coding Random Forests® in 100 lines of code*
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用100行代码编写随机森林®*
- en: 原文：[https://www.kdnuggets.com/2019/08/coding-random-forests.html](https://www.kdnuggets.com/2019/08/coding-random-forests.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/08/coding-random-forests.html](https://www.kdnuggets.com/2019/08/coding-random-forests.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [STATWORX](https://www.statworx.com/)**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [STATWORX](https://www.statworx.com/) 提供**'
- en: Motivation
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动机
- en: There are dozens of machine learning algorithms out there. It is impossible
    to learn all their mechanics; however, many algorithms sprout from the most established
    algorithms, e.g. ordinary least squares, gradient boosting, support vector machines,
    tree-based algorithms and neural networks. At [STATWORX](https://www.statworx.com/) we
    discuss algorithms daily to evaluate their benefits for a specific project. In
    any case, understanding these core algorithms is key to most machine learning
    algorithms in the literature.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的机器学习算法种类繁多。要掌握所有算法的细节几乎是不可能的；然而，许多算法源自最成熟的算法，例如普通最小二乘法、梯度提升、支持向量机、基于树的算法和神经网络。在
    [STATWORX](https://www.statworx.com/) 我们每天讨论算法，以评估它们在特定项目中的好处。无论如何，理解这些核心算法是文献中大多数机器学习算法的关键。
- en: Why bother writing from scratch?
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么要从头开始编写？
- en: While I like reading machine learning research papers, the maths is sometimes
    hard to follow. That is why I like implementing the algorithms in R by myself.
    Of course, this means digging through the maths and the algorithms as well. However,
    you can challenge your understanding of the algorithm directly.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我喜欢阅读机器学习研究论文，但数学有时难以理解。这就是为什么我喜欢自己用R实现算法。当然，这意味着深入挖掘数学和算法。然而，你可以直接挑战你对算法的理解。
- en: In my last blog posts, I introduced two machine learning algorithms in 150 lines
    of R Code. You can find the other blog posts about coding [gradient boosted machines](https://www.statworx.com/blog/coding-gradient-boosted-machines-in-100-lines-of-code) and [regression
    trees](https://www.statworx.com/blog/coding-regression-trees-in-150-lines-of-code) from
    scratch on our [blog](https://www.statworx.com/blog/coding-gradient-boosted-machines-in-100-lines-of-code) or
    in the readme on my [GitHub](https://www.github.com/andrebleier/cheapml). This
    blog post is about the random forest, which is probably the most prominent machine
    learning algorithm. You have probably noticed the asterisk (*) in the title. These
    things often suggest, that there has to be something off. Like seeing a price
    for a cell phone plan in a tv commercial and while reading the fine prints you
    learn that it only applies if you have successfully climbed Mount Everest and
    you got three giraffes on your yacht. Also, yes your suspicion is justified; unfortunately,
    the 100 lines of code only apply if we don’t add the code from the regression
    tree algorithm, which is essential for a random forest. That is why I would strongly
    recommend reading the blog about [regression trees](https://www.statworx.com/blog/coding-regression-trees-in-150-lines-of-code),
    if you are not familiar with the regression tree algorithm.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我上一篇博客中，我用150行R代码介绍了两种机器学习算法。你可以在我们的 [博客](https://www.statworx.com/blog/coding-gradient-boosted-machines-in-100-lines-of-code)
    上找到关于用代码实现 [梯度提升机](https://www.statworx.com/blog/coding-gradient-boosted-machines-in-100-lines-of-code)
    和 [回归树](https://www.statworx.com/blog/coding-regression-trees-in-150-lines-of-code)
    的其他博客帖子，或者在我的 [GitHub](https://www.github.com/andrebleier/cheapml) 上的readme中找到。这篇博客文章介绍了随机森林，这可能是最著名的机器学习算法。你可能注意到了标题中的星号（*）。这些通常暗示着有些问题。就像在电视广告中看到手机计划的价格，而在阅读小字时你发现它仅适用于你成功攀登了珠穆朗玛峰，并且你的游艇上有三只长颈鹿。此外，是的，你的怀疑是合理的；不幸的是，100行代码仅适用于如果我们不添加回归树算法的代码，而回归树算法对随机森林来说是至关重要的。因此，如果你不熟悉回归树算法，我强烈建议阅读关于
    [回归树](https://www.statworx.com/blog/coding-regression-trees-in-150-lines-of-code)
    的博客。
- en: Understanding ML with simple and accessible code
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用简单和可访问的代码理解机器学习
- en: In this series we try to produce very generic code, i.e. it won’t produce a
    state-of-the-art performance. It is instead designed to be very generic and easily
    readable.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个系列中，我们尝试生成非常通用的代码，即它不会产生最先进的性能。它的设计旨在非常通用且易于阅读。
- en: 'Admittedly, there are tons of great articles out there which explain random
    forests theoretically accompanied with a hands-on example. That is not the objective
    of this blog post. If you are interested in a hands-on tutorial with all the necessary
    theory, I strongly recommend this [tutorial](https://datascienceplus.com/random-forests-in-r/).
    The objective of this blog post is to establish the theory of the algorithm by
    writing simple R code. The only thing you need to know, besides the fundamentals
    of a regression tree, is our objective: We want to estimate our real-valued target
    (`y`) with a set of real-valued features (`X`).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 毋庸置疑，有大量出色的文章理论上解释了随机森林，并配有实际操作示例。但这不是这篇博客文章的目的。如果你对包含所有必要理论的实际教程感兴趣，我强烈推荐这个 [教程](https://datascienceplus.com/random-forests-in-r/)。这篇博客文章的目的是通过编写简单的
    R 代码来建立算法的理论。除了回归树的基础知识外，你需要知道的唯一事情是我们的目标：我们希望用一组实值特征（`X`）来估计我们的实值目标（`y`）。
- en: Reduce dimensionality with Feature Importance
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用特征重要性减少维度
- en: Fortunately, we do not have to cover too much maths in this tutorial, since
    that part is already covered in the regression tree [tutorial](https://www.statworx.com/blog/coding-regression-trees-in-150-lines-of-code).
    However, there is one part, which I have added in the code since the last [blog
    post](https://www.statworx.com/blog/coding-regression-trees-in-150-lines-of-code).
    Regression trees and hence random forests, opposed to a standard OLS regression,
    can neglect unimportant features in the fitting process. This is one significant
    advantage of tree-based algorithms and is something which should be covered in
    our basic algorithm. You can find this enhancement in the new `reg_tree_imp.R` script
    on [GitHub](https://www.github.com/andrebleier/cheapml). We use this function
    to sprout trees in our forest later on.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们在这个教程中不需要覆盖太多的数学，因为那部分已经在回归树 [教程](https://www.statworx.com/blog/coding-regression-trees-in-150-lines-of-code)中涵盖了。然而，有一个部分，我在代码中加入了，因为自上一个 [博客文章](https://www.statworx.com/blog/coding-regression-trees-in-150-lines-of-code)以来发生了变化。回归树，因此随机森林，相对于标准的
    OLS 回归，可以忽略不重要的特征。这是基于树的算法的一个重要优势，也是我们基本算法中应该涵盖的内容。你可以在新的 `reg_tree_imp.R` 脚本中找到这一改进，位于 [GitHub](https://www.github.com/andrebleier/cheapml)。我们使用这个函数来在森林中生长树木。
- en: Before we jump into the random forest code, I would like to touch very briefly
    on how we can compute feature importance in a regression tree. Surely, there are
    tons of ways on how you can calculate feature importance, the following approach
    is, however, quite intuitive and straightforward.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们跳入随机森林代码之前，我想简要介绍一下如何计算回归树中的特征重要性。当然，有许多方法可以计算特征重要性，但以下方法相当直观和直接。
- en: Evaluating the goodness of a split
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估分割的好坏
- en: 'A regression tree splits the data by choosing the feature which minimizes a
    certain criterion, e.g. the squared error of our prediction. Of course, it is
    possible, that some features will never be chosen for a split, which makes calculating
    their importance very easy. However, how can we compute importance with chosen
    features? A first shot could be to count the number of splits for each feature
    and relativize it by the total number of all splits. This measure is simple and
    intuitive, but it cannot quantify how impactful the splits were, and this is something
    we can accomplish with a very simple but more sophisticated metric. This metric
    is a weighted goodness of fit. We start by defining our goodness of fit for each
    node. For instance, the mean squared error, which is defined as:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 回归树通过选择最小化某个标准的特征来分割数据，例如我们预测的平方误差。当然，有些特征可能永远不会被选择用于分割，这使得计算它们的重要性变得非常简单。然而，如何计算已选择特征的重要性呢？一个初步的方法可能是计算每个特征的分割次数，并以所有分割的总数进行相对化。这个度量简单而直观，但它不能量化分割的影响力，这可以通过一个简单但更复杂的度量来实现。这个度量是加权拟合优度。我们从定义每个节点的拟合优度开始。例如，均方误差，定义为：
- en: '![MSE_{node} = \frac{1}{n_{node}} \sum_{i = 1}^{n_{node}} (y_i - \bar{y}_{node}))^2](../Images/06b98047f3df3521f7fcf81a2166027b.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![MSE_{node} = \frac{1}{n_{node}} \sum_{i = 1}^{n_{node}} (y_i - \bar{y}_{node}))^2](../Images/06b98047f3df3521f7fcf81a2166027b.png)'
- en: This metric describes the average squared error we make when we estimate our
    target y_i with our predictor the average value in our current node \bar(y)_{node}.
    Now we can measure the improvement by splitting the data with the chosen feature
    and compare the goodness of fit of the parent node with the performance of its
    children nodes. Essentially, this is more or less the exact step we performed
    to evaluate the best splitting feature for the current node.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标描述了我们在用预测器（当前节点的平均值 \bar(y)_{node}）估计目标 y_i 时所犯的平均平方误差。现在我们可以通过选择特征来分裂数据来测量改进，并比较父节点的拟合优度与其子节点的表现。本质上，这与我们为当前节点评估最佳分裂特征时所执行的步骤基本相同。
- en: Weighting the impact of a split
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权重分裂的影响
- en: Splits at the top of the tree are more impactful as more data reaches the nodes
    at this stage of the tree. That’s why it makes sense to lay more importance on
    earlier splits by taking into account the number of observations which reached
    this node.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 树顶的分裂具有更大的影响，因为在树的这一阶段，更多的数据到达了这些节点。这就是为什么在考虑到到达该节点的观测数量时，更早的分裂显得更为重要。
- en: '![w_{node} = \frac{n_{node}}{n}](../Images/ea7138c77f3291744cc275fc151db827.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![w_{node} = \frac{n_{node}}{n}](../Images/ea7138c77f3291744cc275fc151db827.png)'
- en: 'This weight describes the number of observations in the current node measured
    by the total number of observations. Combining the results above, we can derive
    a weighted improvement of a splitting feature p in a single node as:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个权重描述了当前节点中的观测数量，相对于总观测数量。结合以上结果，我们可以推导出单个节点中特征 p 的加权改进：
- en: Quantifying improvements by splitting the data in a regression tree
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过在回归树中分裂数据来量化改进
- en: '![Improvement^{[p]}_{node} = w_{node}MSE_{node} - (w_{left}MSE_{left} + w_{right}MSE_{right})](../Images/2018c97a3834cc3069d6c190f0194edb.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![Improvement^{[p]}_{node} = w_{node}MSE_{node} - (w_{left}MSE_{left} + w_{right}MSE_{right})](../Images/2018c97a3834cc3069d6c190f0194edb.png)'
- en: This weighted improvement is calculated at every node which was split for the
    respective splitting feature p. To get better interpretability of this improvement,
    we sum up the improvements for each feature in our tree and normalize it by the
    overall improvement in our tree.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这种加权改进在每个被分裂的节点中计算，针对相应的分裂特征 p。为了更好地解释这种改进，我们将树中每个特征的改进总和，并通过树中的整体改进进行归一化。
- en: Quantifying the importance of a feature in a regression tree
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 量化回归树中特征的重要性
- en: '![Importance^{[p]} = \frac{\sum_{k=1}^{p} Improvement^{[k]}}{\sum_{i=1}^{n_{node}}
    Improvement^{[i]}}](../Images/e7110ee9243f520f45e655f6c4072946.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![Importance^{[p]} = \frac{\sum_{k=1}^{p} Improvement^{[k]}}{\sum_{i=1}^{n_{node}}
    Improvement^{[i]}}](../Images/e7110ee9243f520f45e655f6c4072946.png)'
- en: This is the final feature importance measure used within regression tree algorithm.
    Again, you can follow these steps within the [code of the regression tree](https://github.com/andrebleier/cheapml/blob/master/algorithms/reg_tree_imp.R).
    I have tagged all variables, functions and column names involved in the feature
    importance calculation with `imp_*` or `IMP_*` , that should make it a little
    easier to follow.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是回归树算法中使用的最终特征重要性度量。同样，您可以在[回归树的代码](https://github.com/andrebleier/cheapml/blob/master/algorithms/reg_tree_imp.R)中按照这些步骤进行。我已经将所有参与特征重要性计算的变量、函数和列名标记为`imp_*`或`IMP_*`，这应该会使其更易于跟踪。
- en: The Random forest Algorithm
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林算法
- en: All right, enough with this regression tree and importance – we are interested
    in the forest in this blog post. The objective of a random forest is to combine
    many regression or decision trees. Such a combination of single results is referred
    to as ensemble techniques. The idea of this technique is very simple but yet very
    powerful.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，回归树和重要性讲得差不多了——在这篇博客文章中，我们关心的是随机森林。随机森林的目标是结合多个回归树或决策树。这种单一结果的组合被称为集成技术。这个技术的理念非常简单但却非常强大。
- en: Building a regression tree orchestra
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建回归树乐团
- en: In a symphonic orchestra, different groups of instruments are combined to form
    an ensemble, which creates more powerful and diverse harmonies. Essentially, it
    is the same in machine learning, because every regression tree we sprout in random
    forest has the chance to explore the data from a different angle. Our regression
    tree orchestra has thus different views on the data, which makes the combination
    very powerful and diverse opposed to a single regression tree.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个交响乐团中，不同组的乐器组合成一个合奏，这创造了更强大且多样的和声。本质上，机器学习中的情况也相同，因为我们在随机森林中生成的每一个回归树都有机会从不同的角度探索数据。我们的回归树乐团因此对数据有不同的视角，这使得组合比单一回归树更强大和多样。
- en: Simplicity of a random forest
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林的简单性
- en: If you are not familiar with the mechanics algorithm, you probably think that
    the code gets very complicated and hard to follow. Well, to me the amazing part
    of this algorithm is how simple and yet effective it is. The coding part is not
    as challenging as you might think. Like in the other blog posts we take a look
    at the whole code first and then we go through it bit by bit.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不熟悉这个算法的机制，你可能会觉得代码变得非常复杂且难以跟随。对我来说，这个算法令人惊讶的地方在于它的简单性和有效性。编码部分并不像你想象的那么具有挑战性。就像在其他博客文章中，我们首先查看整个代码，然后逐步分析。
- en: The algorithm at one glimpse
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一窥算法
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As you have probably noticed, our algorithm can be roughly divided into two
    parts. Firstly, a function `sprout_tree()` and afterwards some lines of code which
    call this function and process its output. Let us now work through all the code
    chunk by chunk.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经注意到的，我们的算法大致可以分为两个部分。首先是一个函数 `sprout_tree()`，然后是一些调用这个函数并处理其输出的代码行。现在让我们逐块分析所有的代码。
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Sprouting a single regression tree in a forest
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在森林中生成一个单一回归树
- en: The first part of the code is the `sprout_tree()` function, which is just a
    wrapper for the regression tree function `reg_tree_imp()`, which we source as
    the first action of our code. Then we extract our target and the features from
    the formula object.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的第一部分是 `sprout_tree()` 函数，它只是回归树函数 `reg_tree_imp()` 的一个封装，我们将其作为代码的第一个动作来调用。然后我们从公式对象中提取我们的目标和特征。
- en: Creating different angles by bagging the data
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过数据的袋装创建不同的角度
- en: 'Afterwards, we bag the data, which means we are randomly sampling our data
    with the chance of replacement. Remember when I said every tree will look at our
    data from a different angle? Well, this is the part where we create the angles.
    Random sampling with replacement is just a synonym for generating weights on our
    observations. This means that a specific observation in the data set of a specific
    tree could be repeated 10 times. The next tree could, however, lose this observation
    completely. Furthermore, there is another way of creating different angles in
    our trees: Feature sampling.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们对数据进行袋装，这意味着我们随机抽取数据，并且有可能重复。还记得我说过每棵树将从不同的角度查看我们的数据吗？好吧，这就是我们创建这些角度的部分。随机抽样与替代仅仅是对我们观察结果生成权重的另一种说法。这意味着在某棵树的数据集中，一个特定的观察值可能会被重复
    10 次。然而，下一棵树可能完全丢失这个观察值。此外，还有另一种在我们的树中创建不同角度的方法：特征抽样。
- en: Solving collinearity issues between trees in a forest
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决森林中树之间的共线性问题
- en: From our complete feature set `X` we are randomly sampling a `feature_frac *
    100` percent to reduce the dimensionality. With feature sampling we can a) compute
    faster and b) capture angles on our data from supposedly weaker features as we
    decrease the value of `feature_frac` . Suppose, we have some degree of multicollinearity
    between our features. It might occur, that the regression tree will select only
    a specific feature if we use every feature in every tree.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的完整特征集 `X` 中，我们随机抽取 `feature_frac * 100` 百分比来减少维度。通过特征抽样，我们可以 a) 更快地计算和 b)
    从假设上较弱的特征捕捉数据的角度，因为我们降低了 `feature_frac` 的值。假设，我们的特征之间存在一定程度的多重共线性。如果我们在每棵树中使用所有特征，回归树可能只会选择某个特定的特征。
- en: However, supposedly features with less improvement could bare new valuable information
    for the model, but are not granted the chance. This is something you can achieve
    by lowering the dimensionality with the argument `feature_frac`. If your objective
    of the analysis is feature selection, e.g. feature importance, you might want
    to set this parameter to 80%-100% as you will get a more clear cut selection.
    Well, the rest of the function is fitting the regression tree and exporting the
    fitted values as well as the importance.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，特征的改进较少可能会带来对模型有价值的新信息，但却未获得机会。这可以通过将参数`feature_frac`降低来实现。如果分析的目标是特征选择，例如特征重要性，你可能希望将该参数设置为80%-100%，因为这样你会得到更明确的选择。好吧，函数的其余部分是拟合回归树，并导出拟合值以及重要性。
- en: Sprouting a forest
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 培育森林
- en: In the next code chunk we start applying the `sprout_tree()` function `n_trees` times
    with the help of `plyr::raply()`. This function repeatedly applies a function
    call with the same arguments and combines the results in a list. Remember, we
    do not need to change anything in the `sprout_tree()` function, since the angles
    are created randomly every time, we call the function.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的代码块中，我们开始使用`plyr::raply()`函数`n_trees`次来应用`sprout_tree()`函数。该函数重复应用具有相同参数的函数调用，并将结果合并为列表。请记住，我们无需更改`sprout_tree()`函数中的任何内容，因为每次调用函数时角度都是随机生成的。
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Afterwards, we combine the single regression tree fits a data frame. By calculating
    the row mean we are taking the average fitted value of every regression tree in
    our forest. Our last action is to calculate the feature importance of our ensemble.
    That’s the mean feature importance of a feature in all trees normalized by the
    overall mean importance of all variables.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将单个回归树的拟合结果合并到一个数据框中。通过计算行均值，我们得到了森林中每棵回归树的平均拟合值。我们的最后一步是计算集成模型的特征重要性。这是所有树中某一特征的平均特征重要性，经过所有变量总体均值的归一化。
- en: Applying the algorithm
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用算法
- en: 'Let us apply the function to see, whether the fit is indeed better compared
    to a single regression tree. Additionally, we can check out the feature importance.
    I have created a little example on [GitHub](https://www.github.com/andrebleier/cheapml),
    which you can check out. First, we simulate data with the [Xy](https://www.github.com/andrebleier/Xy) package.
    In this simulation, five linear variables were used to create our target `y`.
    To make it a little spicier, we add five irrelevant variables, which were created
    in the simulation as well. The challenge now, of course, is whether the algorithm
    will use any irrelevant feature or if the algorithm can perfectly identify the
    important features. Our formula is:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用这个函数，看看拟合是否确实比单一回归树更好。此外，我们还可以检查特征的重要性。我在[GitHub](https://www.github.com/andrebleier/cheapml)上创建了一个小例子，你可以查看。首先，我们用[Xy](https://www.github.com/andrebleier/Xy)包模拟数据。在这个模拟中，使用了五个线性变量来创建我们的目标`y`。为了增加一点难度，我们还添加了五个在模拟中创建的不相关变量。现在的挑战是，算法是否会使用任何不相关特征，或者算法能否完美识别重要特征。我们的公式是：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The power of the forest
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 森林的力量
- en: Neither the random forest nor the regression tree has selected any unnecessary
    features. However, the regression tree was only split by the two most important
    variables. Whereas, the random forest selected all five relevant features.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林和回归树都没有选择任何不必要的特征。然而，回归树仅由两个最重要的变量进行分裂。相反，随机森林选择了所有五个相关特征。
- en: '![figure-name](../Images/59136161b342dd98f09c5aa53c1e854a.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![figure-name](../Images/59136161b342dd98f09c5aa53c1e854a.png)'
- en: This does not mean that the regression tree is not able to find the right answer.
    It depends on the minimum observation size (`minsize`) of the tree. Surely the
    regression tree would eventually find all important features if we lower the minimum
    size. The random forest, however, found all five essential features with the same
    minimum size.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着回归树无法找到正确的答案。这取决于树的最小观测量(`minsize`)。如果我们降低最小尺寸，回归树肯定会最终找到所有重要特征。然而，随机森林在相同的最小尺寸下找到了所有五个关键特征。
- en: Learning strengths and weaknesses
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习优势与劣势
- en: Of course, this was only one example. I would strongly recommend playing around
    with the functions and examples by yourself because only then you can get a feel
    for the algorithms and where they shine or fail. Feel free to clone the [GitHub](https://www.github.com/andrebleier/cheapml) repository
    and play around with the examples. Simulation is always nice because you get a
    clearcut answer; however, applying the algorithms to familiar real-world data
    might be beneficial to you as well.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这只是一个例子。我强烈建议你自己动手操作函数和示例，因为只有这样你才能真正感受算法的优劣。随意克隆 [GitHub](https://www.github.com/andrebleier/cheapml)
    仓库并尝试示例。模拟总是不错的，因为你会得到明确的答案；然而，将算法应用于熟悉的现实世界数据也可能对你有益。
- en: '**Bio: [STATWORX](https://www.statworx.com/)** is a consulting company for
    data science, statistics, machine learning and artificial intelligence located
    in Frankfurt, Zurich and Vienna.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介: [STATWORX](https://www.statworx.com/)** 是一家位于法兰克福、苏黎世和维也纳的数据科学、统计学、机器学习和人工智能咨询公司。'
- en: '[Original](https://www.statworx.com/de/blog/coding-random-forests-in-100-lines-of-code/).
    Reposted with permission.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://www.statworx.com/de/blog/coding-random-forests-in-100-lines-of-code/)。经许可转载。'
- en: RANDOM FORESTS and RANDOMFORESTS are registered marks of Minitab, LLC.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: RANDOM FORESTS 和 RANDOMFORESTS 是 Minitab, LLC 的注册商标。
- en: '**Related:**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关:**'
- en: '[A Guide to Decision Trees for Machine Learning and Data Science](/2018/12/guide-decision-trees-machine-learning-data-science.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习和数据科学的决策树指南](/2018/12/guide-decision-trees-machine-learning-data-science.html)'
- en: '[Explaining Random Forest (with Python Implementation)](/2019/03/random-forest-python.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[解释随机森林（带 Python 实现）](/2019/03/random-forest-python.html)'
- en: '[Random forests explained intuitively](/2019/01/random-forests-explained-intuitively.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[随机森林直观解释](/2019/01/random-forests-explained-intuitively.html)'
- en: '* * *'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT 需求'
- en: '* * *'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Decision Trees vs Random Forests, Explained](https://www.kdnuggets.com/2022/08/decision-trees-random-forests-explained.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[决策树与随机森林的比较说明](https://www.kdnuggets.com/2022/08/decision-trees-random-forests-explained.html)'
- en: '[Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集成学习技术：在 Python 中使用随机森林的演示](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
- en: '[Multi-modal deep learning in less than 15 lines of code](https://www.kdnuggets.com/2023/01/predibase-multi-modal-deep-learning-less-15-lines-code.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[少于 15 行代码的多模态深度学习](https://www.kdnuggets.com/2023/01/predibase-multi-modal-deep-learning-less-15-lines-code.html)'
- en: '[Random Forest vs Decision Tree: Key Differences](https://www.kdnuggets.com/2022/02/random-forest-decision-tree-key-differences.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[随机森林与决策树：主要区别](https://www.kdnuggets.com/2022/02/random-forest-decision-tree-key-differences.html)'
- en: '[Does the Random Forest Algorithm Need Normalization?](https://www.kdnuggets.com/2022/07/random-forest-algorithm-need-normalization.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[随机森林算法是否需要归一化？](https://www.kdnuggets.com/2022/07/random-forest-algorithm-need-normalization.html)'
- en: '[Hyperparameter Tuning Using Grid Search and Random Search in Python](https://www.kdnuggets.com/2022/10/hyperparameter-tuning-grid-search-random-search-python.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用网格搜索和随机搜索进行超参数调整（Python）](https://www.kdnuggets.com/2022/10/hyperparameter-tuning-grid-search-random-search-python.html)'
