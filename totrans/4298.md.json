["```py\ndata =pd.DataFrame({ \n    'Name':['Ken','Jeff','John','Mike','Andrew','Ann','Sylvia','Dorothy','Emily','Loyford'], \n    'Age':[31,52,56,12,45,50,78,85,46,135], \n    'Phone':[52,79,80,75,43,125,74,44,85,45], \n    'Uni':['One','Two','Three','One','Two','Three','One','Two','Three','One'] \n})\n```", "```py\nfrom sklearn_pandas import DataFrameMapper mapper = DataFrameMapper([ \n    ('Uni', sklearn.preprocessing.LabelBinarizer()), \n    (['Age'], sklearn.preprocessing.StandardScaler()) \n])\n```", "```py\nmapper.fit_transform(data)\n```", "```py\nmapper.transformed_names_\n```", "```py\nmapper = DataFrameMapper([ \n    ('Uni', sklearn.preprocessing.LabelBinarizer()), \n    (['Age'], sklearn.preprocessing.StandardScaler()) \n],df_out=True)\n```", "```py\nimport numpy as np \nimport xarray as xr \ndata = np.random.rand(16, 4) \nmy_xarray = xr.DataArray(data)\n```", "```py\nfrom sklearn.preprocessing import StandardScaler \nXt = wrap(StandardScaler()).fit_transform(X)\n```", "```py\npipeline = Pipeline([ \n    ('pca', wrap(PCA(n_components=50), reshapes='feature')), \n    ('cls', wrap(LogisticRegression(), reshapes='feature')) \n])\n```", "```py\nfrom sklearn_xarray.model_selection \nimport CrossValidatorWrapper from sklearn.model_selection \nimport GridSearchCV, KFold \ncv = CrossValidatorWrapper(KFold()) \npipeline = Pipeline([ \n    ('pca', wrap(PCA(), reshapes='feature')), \n    ('cls', wrap(LogisticRegression(), reshapes='feature')) \n]) \ngridsearch = GridSearchCV( \n    pipeline, cv=cv, param_grid={'pca__n_components': [20, 40, 60]} \n)\n```", "```py\n$ curl https://raw.githubusercontent.com/automl/auto-sklearn/master/requirements.txt | xargs -n 1 -L 1 pip install\n```", "```py\nfrom autosklearn.classification \nimport AutoSklearnClassifier \ncls = AutoSklearnClassifier() \ncls.fit(X_train, y_train) \npredictions = cls.predict(X_test)\n```", "```py\nfrom sklearn.model_selection import train_test_split, cross_validate \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42) \nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=54) \n\ntrain, test = X_train.join(y_train), X_val.join(y_val) \nmodel, features, train, test = Auto_ViML(train,\"target\",test,verbose=2)\n```", "```py\nfrom tpot import TPOTClassifier \nfrom sklearn.datasets import load_digits \nfrom sklearn.model_selection import train_test_split \n\ndigits = load_digits() \nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, train_size=0.75, test_size=0.25, random_state=42) \ntpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42) \ntpot.fit(X_train, y_train) \nprint(tpot.score(X_test, y_test)) \ntpot.export('tpot_digits_pipeline.py')\n```", "```py\nimport featuretools as ft\n\nentities = {\n    \"customers\" : (customers_df, \"customer_id\"),\n    \"sessions\" : (sessions_df, \"session_id\", \"session_start\"),\n    \"transactions\" : (transactions_df, \"transaction_id\", \"transaction_time\")\n }\n\nrelationships = [(\"sessions\", \"session_id\", \"transactions\", \"session_id\"),\n                 (\"customers\", \"customer_id\", \"sessions\", \"customer_id\")]\n\nfeature_matrix, features_defs = ft.dfs(entities=entities,\n                                                 relationships = relationships,\n                                                 target_entity = \"customers\")\n```", "```py\n$ run_experimen experiment.cfg\n```", "```py\nfrom neptunecontrib.monitoring.sklearn import log_regressor_summary\n\nlog_regressor_summary(rfr, X_train, X_test, y_train, y_test)\n```", "```py\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt import BayesSearchCV\nregressor = BayesSearchCV(\n    GradientBoostingRegressor(),\n      {\n         'learning_rate': Real(0.1,0.3),\n         'loss': Categorical(['lad','ls','huber','quantile']),\n         'max_depth': Integer(3,6),\n      },\n    n_iter=32,\n    random_state=0,\n    verbose=1,\n    cv=5, n_jobs=-1,\n  )\nregressor.fit(X_train,y_train)\n```", "```py\nfrom evolutionary_search import EvolutionaryAlgorithmSearchCV\ncv = EvolutionaryAlgorithmSearchCV(estimator=SVC(),\n                                   params=paramgrid,\n                                   scoring=\"accuracy\",\n                                   cv=StratifiedKFold(n_splits=4),\n                                   verbose=1,\n                                   population_size=50,\n                                   gene_mutation_prob=0.10,\n                                   gene_crossover_prob=0.5,\n                                   tournament_size=3,\n                                   generations_number=5,\n                                   n_jobs=4)\ncv.fit(X, y)\n```", "```py\nfrom skl2onnx import to_onnx \nonx = to_onnx(pipeline, X_train[:1].astype(numpy.float32))\n```", "```py\nimport treelite.sklearn \nmodel = treelite.sklearn.import_model(model)\n```", "```py\nfrom dtreeviz.trees import dtreeviz \nviz = dtreeviz( \n   model, X_train, y_train, \n   feature_names=boston.feature_names, \n   fontname=\"Arial\", title_fontsize=16, \n   colors = {\"title\":\"red\"} \n)\n```", "```py\nimport eli5 \neli5.show_weights(model)\n```", "```py\nimport dabl\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_digits\nX, y = load_digits(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\nsc = dabl.SimpleClassifier().fit(X_train, y_train)\nprint(\"Accuracy score\", sc.score(X_test, y_test))\n```", "```py\nfrom skorch import NeuralNetClassifier\nnet = NeuralNetClassifier(\n    MyModule,\n    max_epochs=10,\n    lr=0.1,\n    iterator_train__shuffle=True,\n)\nnet.fit(X, y)\n```"]