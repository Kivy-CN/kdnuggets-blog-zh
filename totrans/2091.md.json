["```py\npip install tokenizers\n```", "```py\n# Import the necessary components\nfrom tokenizers import Tokenizer\nfrom transformers import BertTokenizer\n\n# Load the pre-trained BERT-base-uncased tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n```", "```py\n# Tokenize a single sentence\nencoded_input = tokenizer.encode_plus(\"This is sample text to test tokenization.\")\nprint(encoded_input)\n```", "```py\n{'input_ids': [101, 2023, 2003, 7099, 3793, 2000, 3231, 19204, 3989, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n```", "```py\ntokenizer.decode(encoded_input[\"input_ids\"])\n```", "```py\n[CLS] this is sample text to test tokenization. [SEP]\n```", "```py\ncorpus = [\n    \"Hello, how are you?\",\n    \"I am learning how to use the Hugging Face Tokenizers library.\",\n    \"Tokenization is a crucial step in NLP.\"\n]\nencoded_corpus = tokenizer.batch_encode_plus(corpus)\nprint(encoded_corpus)\n```", "```py\n{'input_ids': [[101, 7592, 1010, 2129, 2024, 2017, 1029, 102], [101, 1045, 2572, 4083, 2129, 2000, 2224, 1996, 17662, 2227, 19204, 17629, 2015, 3075, 1012, 102], [101, 19204, 3989, 2003, 1037, 10232, 3357, 1999, 17953, 2361, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n```", "```py\ntokenizer.batch_decode(encoded_corpus[\"input_ids\"])\n```", "```py\n['[CLS] hello, how are you? [SEP]',\n '[CLS] i am learning how to use the hugging face tokenizers library. [SEP]',\n '[CLS] tokenization is a crucial step in nlp. [SEP]']\n```", "```py\nencoded_corpus_padded = tokenizer.batch_encode_plus(corpus, padding=True)\nprint(encoded_corpus_padded)\n```", "```py\n{'input_ids': [[101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2572, 4083, 2129, 2000, 2224, 1996, 17662, 2227, 19204, 17629, 2015, 3075, 1012, 102], [101, 19204, 3989, 2003, 1037, 10232, 3357, 1999, 17953, 2361, 1012, 102, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]}\n```", "```py\ntokenizer.batch_decode(encoded_corpus_padded[\"input_ids\"], skip_special_tokens=False)\n```", "```py\n['[CLS] hello, how are you? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n '[CLS] i am learning how to use the hugging face tokenizers library. [SEP]',\n '[CLS] tokenization is a crucial step in nlp. [SEP] [PAD] [PAD] [PAD] [PAD]']\n```", "```py\nencoded_corpus_truncated = tokenizer.batch_encode_plus(corpus, truncation=True, max_length=5)\nprint(encoded_corpus_truncated)\n```", "```py\n{'input_ids': [[101, 7592, 1010, 2129, 102], [101, 1045, 2572, 4083, 102], [101, 19204, 3989, 2003, 102]], 'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}\n```", "```py\nfor i, sentence in enumerate(corpus):\n    print(f\"Original sentence: {sentence}\")\n    print(f\"Token IDs: {encoded_corpus_truncated['input_ids'][i]}\")\n    print(f\"Tokens: {tokenizer.convert_ids_to_tokens(encoded_corpus_truncated['input_ids'][i])}\")\n    print()\n```", "```py\nOriginal sentence: Hello, how are you?\nToken IDs: [101, 7592, 1010, 2129, 102]\nTokens: ['[CLS]', 'hello', ',', 'how', '[SEP]']\n\nOriginal sentence: I am learning how to use the Hugging Face Tokenizers library.\nToken IDs: [101, 1045, 2572, 4083, 102]\nTokens: ['[CLS]', 'i', 'am', 'learning', '[SEP]']\n\nOriginal sentence: Tokenization is a crucial step in NLP.\nToken IDs: [101, 19204, 3989, 2003, 102]\nTokens: ['[CLS]', 'token', '##ization', 'is', '[SEP]']\n```"]