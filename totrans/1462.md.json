["```py\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom vecstack import stacking\n```", "```py\nlink = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'\nnames = ['Class', 'Alcohol', 'Malic acid', 'Ash',\n         'Alcalinity of ash' ,'Magnesium', 'Total phenols',\n         'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',     'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',\n         'Proline']df = pd.read_csv(link, header=None, names=names)\ndf.sample(5)\n```", "```py\ny = df[['Class']]\nX = df.iloc[:,1:]X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n```", "```py\nmodels = [\n    KNeighborsClassifier(n_neighbors=5,\n                        n_jobs=-1),\n\n    RandomForestClassifier(random_state=0, n_jobs=-1,\n                           n_estimators=100, max_depth=3),\n\n    XGBClassifier(random_state=0, n_jobs=-1, learning_rate=0.1,\n                  n_estimators=100, max_depth=3)\n]\n```", "```py\nS_train, S_test = stacking(models,\n                           X_train, y_train, X_test,\n                           regression=False,\n\n                           mode='oof_pred_bag',\n\n                           needs_proba=False,\n\n                           save_dir=None,\n\n                           metric=accuracy_score,\n\n                           n_folds=4,\n\n                           stratified=True,\n\n                           shuffle=True,\n\n                           random_state=0,\n\n                           verbose=2)\n```", "```py\ntask:         [classification]\nn_classes:    [3]\nmetric:       [accuracy_score]\nmode:         [oof_pred_bag]\nn_models:     [4]model  0:     [KNeighborsClassifier]\n    fold  0:  [0.72972973]\n    fold  1:  [0.61111111]\n    fold  2:  [0.62857143]\n    fold  3:  [0.76470588]\n    ----\n    MEAN:     [0.68352954] + [0.06517070]\n    FULL:     [0.68309859]model  1:     [ExtraTreesClassifier]\n    fold  0:  [0.97297297]\n    fold  1:  [1.00000000]\n    fold  2:  [0.94285714]\n    fold  3:  [1.00000000]\n    ----\n    MEAN:     [0.97895753] + [0.02358296]\n    FULL:     [0.97887324]model  2:     [RandomForestClassifier]\n    fold  0:  [1.00000000]\n    fold  1:  [1.00000000]\n    fold  2:  [0.94285714]\n    fold  3:  [1.00000000]\n    ----\n    MEAN:     [0.98571429] + [0.02474358]\n    FULL:     [0.98591549]model  3:     [XGBClassifier]\n    fold  0:  [1.00000000]\n    fold  1:  [0.97222222]\n    fold  2:  [0.91428571]\n    fold  3:  [0.97058824]\n    ----\n    MEAN:     [0.96427404] + [0.03113768]\n    FULL:     [0.96478873]\n```", "```py\nmodel = XGBClassifier(random_state=0, n_jobs=-1, learning_rate=0.1,\n                      n_estimators=100, max_depth=3)\n\nmodel = model.fit(S_train, y_train)y_pred = model.predict(S_test)print('Final prediction score: [%.8f]' % accuracy_score(y_test, y_pred))Output: Final prediction score: [0.97222222]\n```"]