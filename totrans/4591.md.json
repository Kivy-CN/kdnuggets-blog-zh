["```py\npip install yellowbrick\n```", "```py\nimport pandas as pd\nfrom sklearn import datasets\nwine_data = datasets.load_wine()\ndf_wine = pd.DataFrame(wine_data.data,columns=wine_data.feature_names)\ndf_wine['target'] = pd.Series(wine_data.target)\n\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX = df_wine.drop(['target'], axis=1)\ny = df_wine['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n```", "```py\nfrom yellowbrick.features import Rank2D\nimport matplotlib.pyplot as plt\nvisualizer = Rank2D(algorithm=\"pearson\",  size=(1080, 720))\nvisualizer.fit_transform(X_train)\nvisualizer.poof()\n\n```", "```py\nfrom yellowbrick.classifier import ClassificationReport\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nvisualizer = ClassificationReport(model, size=(1080, 720))\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.poof()\n\n```", "```py\nimport eli5\neli5.show_weights(model, feature_names = X.columns.tolist())\n\n```", "```py\nfrom eli5 import show_prediction\nshow_prediction(model, X_train.iloc[1], feature_names = X.columns.tolist(), \n                show_feature_values=True)\n\n```", "```py\npip install lime\n```", "```py\nimport lime.lime_tabular\nexplainer = lime.lime_tabular.LimeTabularExplainer(X_train.values,                                            \n                 feature_names=X_train.columns.values.tolist(),                                        \n                 class_names=y_train.unique())\n\n```", "```py\npredict_fn = lambda x: model.predict_proba(x).astype(float)\n```", "```py\nexp = explainer.explain_instance(X_test.values[0], predict_fn, num_features=6)\nexp.show_in_notebook(show_all=False)\n\n```", "```py\npip install mlxtend\n```", "```py\nfrom mlxtend.plotting import plot_decision_regions\nfrom mlxtend.classifier import EnsembleVoteClassifier\nimport matplotlib.gridspec as gridspec\nimport itertoolsfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\n\n```", "```py\nX_train_ml = X_train[['proline', 'color_intensity']].values\ny_train_ml = y_train.values\n```", "```py\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[1,1,1])\nvalue=1.5\nwidth=0.75\ngs = gridspec.GridSpec(2,2)\nfig = plt.figure(figsize=(10,8))\nlabels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'Ensemble']\nfor clf, lab, grd in zip([clf1, clf2, clf3, eclf],\n                         labels,\n                         itertools.product([0, 1], repeat=2)):\n\n    clf.fit(X_train_ml, y_train_ml)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X_train_ml, y=y_train_ml, clf=clf)\n    plt.title(lab)\n\n```"]