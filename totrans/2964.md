# 关于NLP和机器学习中的文本预处理，你需要知道的一切

> 原文：[https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html](https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html)

[评论](#comments)

**作者 [Kavita Ganesan](http://kavita-ganesan.com/)，数据科学家**。

基于一些最近的讨论，我意识到文本预处理是一个严重被忽视的话题。我与几个人交谈时，他们提到他们的NLP应用结果不一致，结果才发现他们没有进行文本预处理或使用了不适合其项目的文本预处理方法。

* * *

## 我们的三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity Certificate](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析能力

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support Professional Certificate](https://www.kdnuggets.com/google-itsupport) - 支持你的组织在IT领域

* * *

鉴于此，我想阐明一下文本预处理的真正含义、不同的文本预处理方法，以及估计你可能需要多少预处理的方法。对感兴趣的朋友，我也制作了一些 [文本预处理代码片段](https://github.com/kavgan/nlp-text-mining-working-examples/tree/master/text-pre-processing) 供你尝试。现在，让我们开始吧！

### 什么是文本预处理？

对文本进行预处理，简单来说，就是将你的文本转换为对你的任务 ***可预测*** 和 ***可分析*** 的形式。这里的任务是方法和领域的组合。例如，从推文（领域）中使用TF-IDF（方法）提取顶级关键词就是一个 *任务* 的例子。

> *任务 = 方法 + 领域*

一项任务的理想预处理可能会变成另一项任务的噩梦。所以请注意：文本预处理不能直接从一个任务转移到另一个任务。

让我们举一个非常简单的例子，假设你正在尝试发现新闻数据集中常用的词汇。如果你的预处理步骤涉及去除 [停用词](http://kavita-ganesan.com/what-are-stop-words/) ，因为其他任务使用了它，那么你可能会错过一些常用词，因为你已经将其排除。所以，实际上这并不是一种通用的方法。

### 文本预处理技术的类型

有不同的方法来预处理你的文本。以下是一些你应该了解的方法，我会尽力强调每种方法的重要性。

#### 小写化

将所有文本数据小写化，虽然通常被忽视，但却是最简单和最有效的文本预处理方式之一。它适用于大多数文本挖掘和自然语言处理问题，并且在数据集不大的情况下可以显著帮助保持预期输出的一致性。

最近，我的一位博客读者训练了一个[word embedding model for similarity lookups](http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/)。他发现输入大小写的不同变体（例如‘Canada’与‘canada’）给出了不同类型的输出或根本没有输出。这可能是因为数据集中‘Canada’的出现形式混合，神经网络没有足够的证据来有效学习较不常见版本的权重。当你的数据集相当小的时候，这种问题是不可避免的，而小写化是解决稀疏性问题的一个好方法。

这里是一个示例，展示了小写化如何解决稀疏性问题，其中相同的词以不同的大小写映射到相同的小写形式：

![](../Images/2e10cb05281aa5edaddfe9d48d76c788.png)

**不同大小写的单词都映射到相同的小写形式**

另一个小写化非常有用的示例是搜索。假设你在寻找包含“usa”的文档。然而，没显示任何结果，因为“usa”被索引为**“USA”**。现在，我们应该责怪谁呢？是设置界面的用户界面设计师，还是设置搜索索引的工程师？

虽然小写化应该是标准做法，但我也遇到过需要保留大小写的情况。例如，在预测源代码文件的编程语言时。Java中的单词System与Python中的system相差甚远。将这两个词小写化会使它们变得相同，从而使分类器丧失重要的预测特征。虽然小写化*通常是有用的*，但可能并不适用于所有任务。

#### 词干提取

词干提取是将单词（例如troubled、troubles）的词形变化减少到其词根形式（例如trouble）的过程。在这种情况下，“词根”可能不是真正的词根词，而只是原始单词的标准形式。

词干提取使用一种粗糙的启发式过程，通过切除单词的结尾部分，希望能将单词正确转换为其词根形式。因此，像“trouble”、“troubled”和“troubles”这些词可能会被转换为troubl，而不是trouble，因为结尾部分被直接剪掉了（呃，多么粗糙啊！）。

词干提取有不同的算法。最常见的算法，也被证明对英语有效的是[Porters Algorithm](https://tartarus.org/martin/PorterStemmer/)。这是一个使用Porter Stemmer进行词干提取的示例：

![](../Images/0eb52986e6bd9d09ccb2d5c531905e44.png)

**词干提取的效果**

词干提取对处理稀疏性问题以及标准化词汇非常有用。我在搜索应用中尤其取得了成功。这个想法是，如果你搜索“深度学习课程”，你还希望呈现提到“深度学习***班级***”以及“深度***学习***课程”的文档，尽管后者听起来不太对劲。但你明白我们的意思了。你希望匹配单词的所有变体，以展示最相关的文档。

然而，在我以前的大多数文本分类工作中，词干提取仅在一定程度上提高了分类准确性，相比之下，使用更好设计的特征和文本丰富方法，如使用词嵌入，效果更佳。

#### 词形还原

表面上看，词形还原与词干提取非常相似，目标是去除词形变化并将单词映射到其词根形式。唯一的区别在于，词形还原尝试以正确的方式进行。它不仅仅是切掉部分，它实际上将单词转换为实际的词根。例如，单词“better”将映射到“good”。它可能会使用像[WordNet for mappings](https://www.nltk.org/_modules/nltk/stem/wordnet.html)这样的词典，或者一些特殊的[规则基础方法](https://www.semanticscholar.org/paper/A-Rule-based-Approach-to-Word-Lemmatization-Plisson-Lavrac/5319539616e81b02637b1bf90fb667ca2066cf14)。以下是使用基于WordNet的方法进行词形还原的示例：

![](../Images/b66bdaba8be78fc48941b003a6f3b76e.png)

使用WordNet的词形还原效果

根据我的经验，词形还原对搜索和文本分类的效果不比词干提取显著。实际上，取决于你选择的算法，它可能比使用非常基本的词干提取器要慢得多，而且你可能需要知道相关单词的词性，以获得正确的词形。[这篇论文](https://arxiv.org/pdf/1707.01780.pdf)发现词形还原对使用神经网络架构进行文本分类的准确性没有显著影响。

我个人会适度使用词形还原。额外的开销可能不一定值得。但你可以尝试一下，看看它对你的性能指标有什么影响。

#### 停用词移除

停用词是一组语言中常用的词。在英语中，停用词的例子包括“a”、“the”、“is”、“are”等。使用停用词的直觉是，通过从文本中去除低信息词，我们可以将注意力集中在重要的词汇上。

例如，在搜索系统的背景下，如果你的搜索查询是*“什么是文本预处理？”*，你希望搜索系统关注于呈现讨论文本预处理的文档，而不是讨论“是什么”的文档。这可以通过防止所有停用词列表中的词被分析来实现。停用词在搜索系统、文本分类应用、主题建模、主题提取等方面被广泛应用。

根据我的经验，停用词移除在搜索和主题提取系统中效果显著，但在分类系统中显示为非关键。然而，它确实有助于减少考虑的特征数量，从而使模型保持合理的大小。

这是停用词移除的一个示例。所有的停用词都被替换为一个虚拟字符，**W**：

![](../Images/7f59c74fc0fa0c2d82bc78fe11a97cda.png)

**停用词移除前后的句子**

[停用词列表](http://kavita-ganesan.com/what-are-stop-words/) 可以来自预设的集合，或者你可以为你的领域创建一个[自定义列表](http://kavita-ganesan.com/tips-for-constructing-custom-stop-word-lists/)。一些库（例如 sklearn）允许你移除在X%文档中出现的词，这也能达到停用词移除的效果。

#### 标准化

一个被高度忽视的预处理步骤是文本标准化。文本标准化是将文本转换为标准（规范）形式的过程。例如，单词“gooood”和“gud”可以转换为其规范形式“good”。另一个例子是将“stopwords”、“stop-words”和“stop words”等近似相同的词汇映射为“stopwords”。

文本标准化对于噪声较多的文本如社交媒体评论、短信和博客评论等非常重要，因为这些文本中常出现缩写、拼写错误和超出词汇表的词（oov）。[这篇论文](https://sentic.net/microtext-normalization.pdf) 表明，通过对推文使用文本标准化策略，他们能够提高情感分类的准确性约4%。

这是标准化前后词语的一个例子：

![](../Images/7dc21a06e55993824319453994949765.png)

**文本标准化的效果**

注意这些变体如何映射到相同的规范形式。

根据我的经验，文本标准化在分析[高度非结构化的临床文本](http://kavita-ganesan.com/general-supervised-approach-segmentation-clinical-texts/)时也非常有效，医生以非标准方式记笔记。我还发现它在[主题提取](https://githubengineering.com/topics/)中很有用，其中近义词和拼写差异很常见（例如 topic modelling、topic modeling、topic-modeling、topic-modelling）。

不幸的是，与词干提取和词形还原不同，文本标准化没有标准化的方法。它通常取决于任务。例如，你标准化临床文本的方法可能与标准化短信文本的方法有所不同。

一些常见的文本标准化方法包括词典映射（最简单）、统计机器翻译（SMT）和基于拼写校正的方法。[这篇有趣的文章](https://nlp.stanford.edu/courses/cs224n/2009/fp/27.pdf) 比较了基于词典的方法和SMT方法在标准化文本消息中的使用。

#### 噪声移除

噪声去除涉及移除可能干扰文本分析的字符、数字和文本片段。噪声去除是文本预处理步骤中最重要的环节之一，同时也高度依赖于领域。

例如，在推文中，噪声可能是所有特殊字符，除了标签（hashtags），因为标签表示可以表征推文的概念。噪声的问题在于它可能导致你下游任务中结果的不一致。让我们来看下面的例子：

![](../Images/b1f2de865ee7ac0fcd66bf86c4b26a02.png)

**去噪声的词干提取**

注意，上面所有的原始单词都有一些周围的噪声。如果你对这些单词进行词干提取，你会发现提取的结果并不好看。没有一个单词的词干是正确的。然而，通过在[this notebook](https://github.com/kavgan/nlp-text-mining-working-examples/blob/master/text-pre-processing/Text%20Pre-Processing%20Examples.ipynb)中进行一些清理，结果现在看起来好多了：

![](../Images/6c5455c30e466d2d29fa068c8353ed7a.png)

带有噪声去除的词干提取

噪声去除是进行文本挖掘和自然语言处理时你应该首先关注的内容之一。去除噪声的方法有很多，包括*标点符号去除*、*特殊字符去除*、*数字去除*、*HTML格式去除*、*领域特定关键词去除*（例如‘RT’表示转发）、*源代码去除*、*标题去除*等。具体取决于你所工作的领域以及对你的任务来说什么是噪声。[我笔记本中的代码片段](https://github.com/kavgan/nlp-text-mining-working-examples/tree/master/text-pre-processing)展示了如何进行一些基本的噪声去除。

#### 文本丰富化 / 增强

文本丰富化涉及用你之前没有的信息来增强原始文本数据。文本丰富化为你的原始文本提供了更多的语义，从而提高了其预测能力以及你对数据进行分析的深度。

在信息检索的例子中，扩展用户的查询以提高关键词匹配度是一种增强的形式。像“文本挖掘”这样的查询可以变成“文本文档挖掘分析”。虽然这对人类来说没有意义，但它可以帮助获取更相关的文档。

你可以非常有创意地丰富你的文本。你可以使用[**词性标注**](https://en.wikipedia.org/wiki/Part-of-speech_tagging)来获取有关文本中单词的更详细信息。

例如，在文档分类问题中，单词**book**作为**名词**的出现可能会导致与**book**作为**动词**不同的分类，因为一个用在阅读的上下文中，另一个用在预定的上下文中。[这篇文章](http://www.iapr-tc11.org/archive/icdar2011/fileup/PDF/4520a920.pdf)讨论了如何通过将名词和动词作为输入特征来改进中文文本分类。

然而，随着大量文本的出现，人们开始使用[嵌入](https://en.wikipedia.org/wiki/Word_embedding)来丰富词汇、短语和句子的含义，以用于分类、搜索、摘要和文本生成。这在基于深度学习的NLP方法中特别常见，其中[word level embedding layer](https://keras.io/layers/embeddings/)非常普遍。你可以从[预先建立的嵌入](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)开始，或者创建自己的嵌入并在下游任务中使用它。

其他丰富文本数据的方法包括[短语提取](http://kavita-ganesan.com/how-to-incorporate-phrases-into-word2vec-a-text-mining-approach/#.XHCcJ1xKg2w)，其中识别复合词为一个（即分块），[使用同义词扩展](http://aclweb.org/anthology/R09-1073)和[依赖解析](http://www.cs.virginia.edu/~kc2wc/teaching/NLP16/slides/15-DP.pdf)。

### 你是否需要全部进行？

![](../Images/8faa55aebf384c9173f88e136c256e1d.png)

其实并非必须，但如果你希望获得良好且一致的结果，确实需要做一些噪声去除。为了让你了解最基本的要求，我将其分为***必须做***、***应该做***和***任务依赖***。所有任务依赖的部分可以在决定是否需要之前进行定量或定性测试。

记住，少即是多，你要尽可能保持你的方法优雅。你添加的开销越多，当遇到问题时你需要剥离的层次就越多。

### 必须做：

+   噪声去除

+   小写化（在某些情况下可能依赖于任务）

### 应该做：

+   简单的标准化——（例如，标准化几乎相同的词汇）

### 任务依赖：

1.  高级标准化（例如，处理词汇外的词汇）

1.  停用词去除

1.  词干提取/词形还原

1.  文本丰富/扩充

因此，对于任何任务，最基本的要求是尝试将文本转换为小写并去除噪声。噪声的定义取决于你的领域（参见噪声去除部分）。你还可以进行一些基本的标准化步骤以提高一致性，然后根据需要系统地添加其他层次。

### 通用经验法则

不是所有任务都需要相同程度的预处理。对于一些任务，你可以做最小的预处理。然而，对于其他任务，数据集噪声如此之大，如果你不进行足够的预处理，结果将会是垃圾进垃圾出。

这是一个通用的经验法则。虽然并不总是适用，但在大多数情况下有效。如果你有大量在相对通用领域中撰写良好的文本，那么预处理并不是极其关键的；你可以做最基本的预处理（例如，使用所有的维基百科文本或路透社新闻文章训练一个词嵌入模型）。

然而，如果你在一个非常狭窄的领域（例如关于健康食品的推文）工作，并且数据稀疏且嘈杂，你可能会从更多的预处理层中受益，尽管你添加的每一层（例如停用词移除、词干提取、规范化）都需要经过定量或定性验证，以证明它是有意义的层。以下是一个总结了你应对文本数据进行多少预处理的表格

![](../Images/2582e830c52b2f3ca10fa22dbee429ca.png)

希望这里的想法能引导你朝着适合你项目的正确预处理步骤前进。记住，*少即是多*。我的一个朋友曾提到，他通过剔除不必要的*预处理*层，使一个大型电商搜索系统变得更高效且错误更少。

### 资源

+   [使用 NLTK 和正则表达式的基本文本预处理 Python 代码](https://github.com/kavgan/nlp-text-mining-working-examples/blob/master/text-pre-processing/Text%20Preprocessing%20Examples.ipynb)

+   [构建自定义停用词列表](http://kavita-ganesan.com/tips-for-constructing-custom-stop-word-lists/)

+   [短语提取的源代码](https://kavgan.github.io/phrase-at-scale/)

### 参考文献

+   要获取更新的论文列表，请参见 [我原来的文章](http://kavita-ganesan.com/text-preprocessing-tutorial/#Relevant-Papers)

简介: [Kavita Ganesan](http://kavita-ganesan.com/about-me/) 是一位数据科学家，专注于自然语言处理、文本挖掘、搜索和机器学习。在过去的十年中，她曾为多个技术组织工作，包括 GitHub（微软）、3M 健康信息系统和 eBay。

[原文](https://medium.freecodecamp.org/all-you-need-to-know-about-text-preprocessing-for-nlp-and-machine-learning-bc1c5765ff67)。经许可转载。

**资源:**

+   [在线和基于网络的: 分析、数据挖掘、数据科学、机器学习教育](https://www.kdnuggets.com/education/online.html)

+   [分析、数据科学、数据挖掘和机器学习的软件](https://www.kdnuggets.com/software/index.html)

**相关:**

+   [使用 PyTorch 框架入门 NLP](https://www.kdnuggets.com/2019/04/nlp-pytorch.html)

+   [通过迁移学习和弱监督便宜地构建 NLP 分类器](https://www.kdnuggets.com/2019/03/building-nlp-classifiers-cheaply-transfer-learning-weak-supervision.html)

+   [超越新闻内容: 社会背景在假新闻检测中的作用](https://www.kdnuggets.com/2019/03/beyond-news-contents-role-of-social-context-for-fake-news-detection.html)

### 更多相关主题

+   [在 Pandas 中清理和预处理文本数据以用于 NLP 任务](https://www.kdnuggets.com/cleaning-and-preprocessing-text-data-in-pandas-for-nlp-tasks)

+   [教科书是你所需的一切: 一种革命性的 AI 培训方法](https://www.kdnuggets.com/2023/07/textbooks-all-you-need-revolutionary-approach-ai-training.html)

+   [KDnuggets 新闻，4 月 13 日: 数据科学家应关注的 Python 库…](https://www.kdnuggets.com/2022/n15.html)

+   [今天所有市场营销分析和数据科学专业人士需要的5项技能](https://www.kdnuggets.com/2023/08/mads-5-skills-marketing-analytics-data-science-pros-need-today.html)

+   [机器学习中的统计学：你需要了解的内容以成为…](https://www.kdnuggets.com/2024/03/sas-statistics-machine-learning-need-know-become-certified-expert)

+   [关于数据管理你需要了解的6件事及其重要性…](https://www.kdnuggets.com/2022/05/6-things-need-know-data-management-matters-computer-vision.html)
