- en: Nearest Neighbors for Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类的最近邻
- en: 原文：[https://www.kdnuggets.com/2022/04/nearest-neighbors-classification.html](https://www.kdnuggets.com/2022/04/nearest-neighbors-classification.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/04/nearest-neighbors-classification.html](https://www.kdnuggets.com/2022/04/nearest-neighbors-classification.html)
- en: K-Nearest Neighbors
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-最近邻
- en: '**K-nearest neighbors** (KNN) is a type of supervised learning machine learning
    algorithm and is used for both regression and classification tasks.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**K-最近邻**（KNN）是一种监督学习机器学习算法，用于回归和分类任务。'
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前 3 个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT 需求'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: KNN is used to make predictions on the test data set based on the characteristics
    of the current training data points. This is done by calculating the distance
    between the test data and training data, assuming that similar things exist within
    close proximity.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 用于基于当前训练数据点的特征对测试数据集进行预测。这是通过计算测试数据与训练数据之间的距离来完成的，假设相似的事物在较近的距离内存在。
- en: The algorithm will have stored learned data, making it more effective at predicting
    and categorising new data points. When a new data point is inputted, the KNN algorithm
    will learn its characteristics/features. It will then place the new data point
    at closer proximity to the current training data points that share the same characteristics
    or features.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法将存储学习到的数据，使其在预测和分类新数据点时更为有效。当输入一个新数据点时，KNN 算法将学习其特征。然后，它会将新数据点放置在与具有相同特征或特征的当前训练数据点更接近的位置。
- en: What Is The ‘k’ in KNN?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KNN 中的 ‘k’ 是什么？
- en: The ‘K’ in KNN is a parameter that refers to the number of nearest neighbors.
    K is a positive integer and is typically small in value and is recommended to
    be an odd number.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 中的 ‘K’ 是一个参数，表示最近邻居的数量。K 是一个正整数，通常值较小，建议为奇数。
- en: In Layman's terms, the K-value creates an environment for the data points. This
    makes it easier to assign which data point belongs to which category.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通俗地说，K 值为数据点创建了一个环境。这使得分配数据点所属的类别变得更加容易。
- en: The example below shows 3 graphs. The first, the ‘Initial Data’ is a graph where
    data points are plotted and clustered into classes, and a new example to classify
    is present. In the ‘Calculate Distance’ graph, the distance from the new example
    data point to the closest trained data points is calculated. However, this still
    does not categorise the new example data point. Therefore, using k-value, essentially
    created a neighborhood where we can classify the new example data point.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例显示了 3 个图表。第一个图表，“初始数据”是一个将数据点绘制并分类到各个类别中的图表，并且有一个新的示例需要分类。在“计算距离”图表中，计算了新示例数据点到最近训练数据点的距离。然而，这仍然不能对新示例数据点进行分类。因此，使用
    k 值，本质上创建了一个邻域，我们可以在其中对新示例数据点进行分类。
- en: We would say that k=3 and the new data point will belong to Class B as there
    are more trained Class B data points with similar characteristics to the new data
    point in comparison to Class A.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会说 k=3，新数据点将属于 B 类，因为与 A 类相比，更多的训练过的 B 类数据点具有与新数据点相似的特征。
- en: '![k-nearest-neighbor](../Images/42005d40ecbc49545df65119eba77d02.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![k-最近邻](../Images/42005d40ecbc49545df65119eba77d02.png)'
- en: 'Source: [datacamp.com](https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[datacamp.com](https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn)
- en: If we increase the k-value to 7, we will see that the new data point will belong
    to Class A as there are more trained Class A data points with similar characteristics
    to the new data point in comparison to Class B.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将 k 值增加到 7，我们将看到新数据点将属于 A 类，因为与 B 类相比，更多的训练过的 A 类数据点具有与新数据点相似的特征。
- en: '![k-nearest-neighbor](../Images/7804c5e5e9412162aad3060cbce401b8.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![K 最近邻](../Images/7804c5e5e9412162aad3060cbce401b8.png)'
- en: 'Source: [datacamp.com](https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [datacamp.com](https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn)'
- en: 'The k-value is typically a small number because, as we increase the k-value,
    the error rate also increases. The below graph shows this:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: k 值通常是一个较小的数字，因为随着 k 值的增加，误差率也会增加。下图展示了这一点：
- en: '![training-error](../Images/7e7fd5c0894c6e2a155c91de8e5811f0.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![训练误差](../Images/7e7fd5c0894c6e2a155c91de8e5811f0.png)'
- en: 'Source: [analyticsvidhya](https://www.analyticsvidhya.com/wp-content/uploads/2014/10/training-error.png)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [analyticsvidhya](https://www.analyticsvidhya.com/wp-content/uploads/2014/10/training-error.png)'
- en: However, If the k-value is small then it causes a low bias but a high variance,
    leading to overfitting of the model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果 k 值较小，则会导致低偏差但高方差，导致模型过拟合。
- en: It is also recommended that the k-value is an odd number. This is because if
    we are trying to classify a new data point and we only have an even number of
    categories/classes (e.g. Class A and Class B) it can produce inaccurate outputs.
    Therefore it is highly recommended to choose a K-value with an odd number to avoid
    a tie.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 还建议 k 值是奇数。这是因为如果我们尝试对一个新数据点进行分类，而类别/类的数量只有偶数（例如，A 类和 B 类），可能会产生不准确的结果。因此，强烈建议选择一个奇数的
    K 值以避免平局。
- en: Calculating The Distance
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算距离
- en: KNN calculates the distance between data points in order to classify new data
    points. The most common methods used to calculate this distance in KNN are Euclidian,
    Manhattan, and Minkowski.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 通过计算数据点之间的距离来对新数据点进行分类。在 KNN 中，计算这种距离的最常见方法是欧几里得距离、曼哈顿距离和闵可夫斯基距离。
- en: '**Euclidean Distance** is the distance between two points using the length
    of a line between the two points. The formula for Euclidean Distance is the square
    root of the sum of the squared differences between a new data point (x) and an
    existing trained data point (y).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**欧几里得距离**是使用两点之间的线段长度来表示的距离。欧几里得距离的公式是新数据点（x）与现有已训练数据点（y）之间差异平方和的平方根。'
- en: '**Manhattan Distance** is the distance between two points is the sum of the
    absolute difference of their Cartesian coordinates. The formula for Manhattan
    Distance is the sum of the lengths between a new data point (x) and an existing
    trained data point (y) using a line segment on the coordinate axes.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**曼哈顿距离**是两点之间的距离，是它们的笛卡尔坐标的绝对差值的总和。曼哈顿距离的公式是新数据点（x）与现有已训练数据点（y）之间的长度总和，使用坐标轴上的线段。'
- en: '**Minkowski Distance** is the distance between two points in the normed vector
    space and is a generalization of the Euclidean distance and the Manhattan distance.
    In the formula for ​​Minkowski Distance when p=2, we get Euclidian distance, also
    known as L2 Distance. When p=1 we get Manhattan distance, also known as L1 distance,
    city-block distance, and LASSO.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**闵可夫斯基距离**是规范化向量空间中两点之间的距离，是欧几里得距离和曼哈顿距离的一般化。在闵可夫斯基距离的公式中，当 p=2 时，我们得到欧几里得距离，也称为
    L2 距离。当 p=1 时，我们得到曼哈顿距离，也称为 L1 距离、城市街区距离和 LASSO。'
- en: 'The image below is the formulas:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是公式：
- en: '![](../Images/3bc2167ca1bc8ba49564ffa3c8edf704.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3bc2167ca1bc8ba49564ffa3c8edf704.png)'
- en: 'The image below explains the difference between the three:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下图解释了三者之间的区别：
- en: '![XXXXX](../Images/b7aedd9bfc220a7674cd7252a7b0fdc6.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/b7aedd9bfc220a7674cd7252a7b0fdc6.png)'
- en: 'Source: [Packt Subscription](https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781785882104/6/ch06lvl1sec40/measuring-distance-or-similarity)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [Packt 订阅](https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781785882104/6/ch06lvl1sec40/measuring-distance-or-similarity)'
- en: How Does The KNN Algorithm Work?
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KNN 算法如何工作？
- en: 'Below are the steps of how a KNN algorithm works:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 KNN 算法工作的步骤：
- en: Load in your dataset
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载你的数据集
- en: Choose a k-value. An odd number is recommended to avoid a tie.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个 k 值。建议选择奇数以避免平局。
- en: Calculate the distance between the new data point and the neighboring existing
    trained data points.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算新数据点与邻近的已训练数据点之间的距离。
- en: Find the K nearest neighbor to the new data point
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到与新数据点最近的 K 个邻居
- en: 'Below is an image that gives an overview of these steps:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个概述这些步骤的图像：
- en: '![implementing-your-own-knn](../Images/d45e3a6dc623593dacc476176f823385.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![实现你自己的 KNN](../Images/d45e3a6dc623593dacc476176f823385.png)'
- en: 'Source: [kdnuggets.com](/2016/01/implementing-your-own-knn-using-python.html)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [kdnuggets.com](/2016/01/implementing-your-own-knn-using-python.html)'
- en: KNN Algorithm Classification Implementation
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KNN 算法分类实现
- en: I will go through an example of the KNN algorithm being used on a Classification
    task using the Iris dataset
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我将通过一个使用鸢尾花数据集进行分类任务的 KNN 算法示例来演示。
- en: Import libraries
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入库
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Load the Iris Dataset
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 载入鸢尾花数据集
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/28d3e9e7fc4e3581ef92d7257e8a0956.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28d3e9e7fc4e3581ef92d7257e8a0956.png)'
- en: Preprocessing the data
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: We do this to split our dataset into its attributes and labels. The X variable
    will contain the first four columns of the dataset, which we refer to as the attributes
    and the y variable will contain the last column, which we refer to as the labels.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将数据集分成属性和标签。变量 X 将包含数据集的前四列，我们称之为属性；变量 y 将包含最后一列，我们称之为标签。
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Train Test Split
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练测试分离
- en: In this step, we are going to divide the dataset into training and test splits.
    This gives us an idea of how well the algorithm learned the training data and
    how well it performs on the testing data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们将数据集分为训练集和测试集。这让我们了解算法在训练数据上的学习效果以及在测试数据上的表现。
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Feature Scaling
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征缩放
- en: Feature scaling is an important element in preprocessing the data before making
    predictions. This method is used to normalise the range of features of the data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 特征缩放是在进行预测之前对数据进行预处理的重要步骤。这种方法用于归一化数据的特征范围。
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Making predictions with KNN
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 KNN 进行预测
- en: First we need to import the KNeighborsClassifier class from the sklearn.neighbors
    library. We then choose our k-value, in this example, I have chosen 7\. Remember
    that it is highly recommended to choose an odd value to avoid ties.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要从 sklearn.neighbors 库中导入 KNeighborsClassifier 类。然后选择我们的 k 值，在这个例子中，我选择了
    7。请记住，强烈建议选择一个奇数值以避免出现平局。
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We then move onto making predictions on our test dataset.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们开始对测试数据集进行预测。
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Accuracy of the algorithm**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**算法的准确率**'
- en: With sklearn.metrics, we can classification_report to evaluate the accuracy
    of the algorithm, looking at precision, recall and f1-score.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 sklearn.metrics，我们可以使用 classification_report 评估算法的准确性，查看精确率、召回率和 F1 分数。
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This is what the output looks like:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出的样子：
- en: '![XXXXX](../Images/a6fb0d4f8939a1f9d53d75cd86ceecd5.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/a6fb0d4f8939a1f9d53d75cd86ceecd5.png)'
- en: From this, we can see that the KNN algorithm classified 30 data points with
    an average total of 95% on precision, 93% on recall, and 94% on f1-score.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里我们可以看到 KNN 算法对 30 个数据点进行了分类，精确率平均达到了 95%，召回率为 93%，F1 分数为 94%。
- en: Finding the right k-value
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找合适的 k 值
- en: In this example, I chose a k-value of 7\. However, if we wanted to check what
    the best k-value is, we can produce a graph that shows the different k-values
    and the error rate it produces.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我选择了 k 值为 7。然而，如果我们想要检查最佳的 k 值是多少，我们可以生成一个显示不同 k 值和其产生的误差率的图表。
- en: I will be looking at k-values between 1 and 30\. A loop will be executed from
    1 to 30, where during each iteration the mean error is calculated and added to
    the error list.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我将查看 k 值在 1 到 30 之间的情况。在每次迭代中，计算平均误差并将其添加到误差列表中。
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Plotting the k-value against error rate graph:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制 k 值与错误率图：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Output of graph:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图的输出：
- en: '![Output of graph](../Images/0ff8c40f751d4aa9f7c253ab3c5dba74.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图的输出](../Images/0ff8c40f751d4aa9f7c253ab3c5dba74.png)'
- en: 'Source: Author Image'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者图片
- en: From this graph, we can see that the k-values that give us a Mean Error of 0
    are predominantly between k-value 13 - 23.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个图表中，我们可以看到产生平均误差为 0 的 k 值主要在 k 值为 13 到 23 之间。
- en: Summary
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: KNN is a simple and easy algorithm to implement and can be used for both regression
    and classification tasks. The K-value is a parameter that refers to the number
    of nearest neighbors. It is recommended that the k-value is an odd number. There
    are different distance metrics you can use, but the most common ones are Euclidian,
    Manhattan, and Minkowski.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 是一个简单易用的算法，可用于回归和分类任务。K 值是一个参数，表示最近邻的数量。建议选择一个奇数作为 k 值。有不同的距离度量标准可用，但最常见的是欧氏距离、曼哈顿距离和闵可夫斯基距离。
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and Freelance Technical Writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**[尼莎·阿利亚](https://www.linkedin.com/in/nisha-arya-ahmed/)** 是一位数据科学家和自由职业技术作家。她特别感兴趣于提供数据科学职业建议或教程，以及围绕数据科学的理论知识。她还希望探索人工智能在延长人类寿命方面的不同方式。作为一个热衷学习者，她寻求拓宽自己的技术知识和写作技能，同时帮助引导他人。'
- en: More On This Topic
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关话题
- en: '[From Theory to Practice: Building a k-Nearest Neighbors Classifier](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从理论到实践：构建一个 k-最近邻分类器](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)'
- en: '[K-nearest Neighbors in Scikit-learn](https://www.kdnuggets.com/2022/07/knearest-neighbors-scikitlearn.html)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Scikit-learn 中的 K 最近邻](https://www.kdnuggets.com/2022/07/knearest-neighbors-scikitlearn.html)'
- en: '[More Performance Evaluation Metrics for Classification Problems You…](https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分类问题的更多性能评估指标](https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html)'
- en: '[Introduction to Binary Classification with PyCaret](https://www.kdnuggets.com/2021/12/introduction-binary-classification-pycaret.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 PyCaret 进行二分类介绍](https://www.kdnuggets.com/2021/12/introduction-binary-classification-pycaret.html)'
- en: '[Fine-Tuning BERT for Tweets Classification with HuggingFace](https://www.kdnuggets.com/2022/01/finetuning-bert-tweets-classification-ft-hugging-face.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 HuggingFace 对 BERT 进行微调以进行推文分类](https://www.kdnuggets.com/2022/01/finetuning-bert-tweets-classification-ft-hugging-face.html)'
- en: '[Machine Learning Algorithms for Classification](https://www.kdnuggets.com/2022/03/machine-learning-algorithms-classification.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分类的机器学习算法](https://www.kdnuggets.com/2022/03/machine-learning-algorithms-classification.html)'
