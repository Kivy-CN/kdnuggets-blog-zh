# 神经网络的快速介绍

> 原文：[https://www.kdnuggets.com/2016/11/quick-introduction-neural-networks.html/3](https://www.kdnuggets.com/2016/11/quick-introduction-neural-networks.html/3)

图 5 中展示的多层感知器（改编自 Sebastian Raschka 的[优秀的反向传播算法可视化解释](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation.md)）在输入层有两个节点（除了偏置节点之外），它们接受输入 '学习小时数' 和 '期中成绩'。它还具有一个隐藏层，包含两个节点（除了偏置节点之外）。输出层也有两个节点——上面的节点输出 '通过' 的概率，而下面的节点输出 '失败' 的概率。

在分类任务中，我们通常在多层感知器的输出层使用[Softmax 函数](http://cs231n.github.io/linear-classify/#softmax)作为激活函数，以确保输出为概率并且它们的总和为 1。Softmax 函数将任意实值分数的向量压缩为值在零与一之间的向量，这些值的总和为 1。因此，在这种情况下，

概率（通过）+ 概率（失败）= 1

**步骤 1：前向传播**

网络中的所有权重都是随机分配的。我们考虑下面图 5 中标记为 **V** 的隐藏层节点。假设从输入到该节点的连接权重为 w1、w2 和 w3（如图所示）。

网络接着将第一个训练样本作为输入（我们知道，对于输入 35 和 67，Pass 的概率是 1）。

+   网络输入 = [35, 67]

+   网络的期望输出（目标）= [1, 0]

然后，考虑节点的输出 V 可以如下计算（*****f***** 是一个激活函数，例如 sigmoid）：

V = ***f*** (1*w1 + 35*w2 + 67*w3)

同样，隐藏层中其他节点的输出也会被计算出来。隐藏层中两个节点的输出作为输入传递给输出层的两个节点。这使我们能够计算输出层两个节点的输出概率。

假设输出层两个节点的输出概率分别为 0.4 和 0.6（由于权重是随机分配的，输出也将是随机的）。我们可以看到，计算得到的概率（0.4 和 0.6）与期望的概率（分别为 1 和 0）相差甚远，因此图 5 中的网络被认为有 '不正确的输出'。

![Screen Shot 2016-08-09 at 11.52.57 PM.png](../Images/03b9ea028c3e2f38889c14e169255473.png)

###### 图 5：多层感知器中的前向传播步骤

**步骤 2：反向传播和权重更新**

我们计算输出节点的总误差，并通过反向传播将这些误差传递回网络，以计算*梯度*。然后，我们使用诸如*梯度下降*之类的优化方法来“调整”**所有**网络中的权重，旨在减少输出层的错误。下图6展示了这一过程（暂时忽略图中的数学方程式）。

假设考虑的节点的新权重是w4、w5和w6（经过反向传播和调整权重后）。

![Screen Shot 2016-08-09 at 11.53.06 PM.png](../Images/658b72bb9d8a4b1f43dc9feaf05e91ef.png)

###### 图6：多层感知器中的反向传播和权重更新步骤

如果我们现在将相同的示例再次输入网络，由于权重已经调整以最小化预测错误，网络的表现应该比之前更好。如图7所示，输出节点的错误现在减少到[0.2, -0.2]，而之前为[0.6, -0.4]。这意味着我们的网络已经学会了正确分类第一个训练示例。

![Screen Shot 2016-08-09 at 11.53.15 PM.png](../Images/b303562a02f5729aa8178e2433fb4808.png)

###### 图7：MLP网络现在在相同输入下表现更好

我们用数据集中所有其他训练示例重复这个过程。然后，我们的网络被认为已经*学习*了这些示例。

如果我们现在想预测一个学生在期中考试中获得70分且学习25小时是否能通过期末考试，我们将进行前向传播步骤，并找出通过和未通过的输出概率。

我在这里避免了数学方程式和像“梯度下降”这样的概念解释，而是尝试为算法发展直观理解。有关反向传播算法的更数学化讨论，请参阅 [此链接](http://home.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html)。

#### 多层感知器的3D可视化

Adam Harley 创建了一个 [3D 可视化](http://scs.ryerson.ca/~aharley/vis/fc/) 的多层感知器，该网络已经在 MNIST 手写数字数据库上经过训练（使用反向传播）。

网络从28 x 28的手写数字图像中接受784个数值像素作为输入（它在输入层中有784个节点对应像素）。网络在第一个隐藏层中有300个节点，在第二个隐藏层中有100个节点，在输出层中有10个节点（对应10个数字）[15]。

尽管这里描述的网络比我们在上一节讨论的网络要大得多（使用了更多隐藏层和节点），但前向传播步骤和反向传播步骤中的所有计算（在每个节点）都是以之前讨论的相同方式进行的。

图8 显示了输入为数字 '5' 时的网络。

![Screen Shot 2016-08-09 at 5.45.34 PM.png](../Images/c07f23238abc74838a1b91b796fb316c.png)

###### 图8: 可视化输入为'5'的网络

输出值高于其他节点的节点用较亮的颜色表示。在输入层中，亮节点是那些接收到较高数值像素输入的节点。注意，在输出层中，唯一的亮节点对应于数字5（其输出概率为1，比其他九个节点的输出概率0更高）。这表明MLP正确地对输入数字进行了分类。我强烈建议玩弄这个可视化并观察不同层节点之间的连接。

#### 深度神经网络

1.  [深度学习与普通机器学习有何区别？](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning.md)

1.  [神经网络与深度神经网络有什么不同？](http://stats.stackexchange.com/questions/182734/what-is-the-difference-between-a-neural-network-and-a-deep-neural-network?rq=1)

1.  [深度学习与多层感知机有何不同？](https://www.quora.com/How-is-deep-learning-different-from-multilayer-perceptron)

#### 结论

为了方便理解，我略去了本文讨论的一些重要细节。我建议查阅斯坦福大学的神经网络教程中的[第1部分](http://cs231n.github.io/neural-networks-1/)、[第2部分](http://cs231n.github.io/neural-networks-2/)、[第3部分](http://cs231n.github.io/neural-networks-3/)以及[案例研究](http://cs231n.github.io/neural-networks-case-study/)以全面了解多层感知机。

如果你有任何问题或建议，请在下面的评论中告诉我！

简介: [![ujjwal-karn-150](../Images/a2ca7f20cf4747d5e6567bb0a4741fcc.png)Ujjwal Karn](https://ujjwalkarn.me/) 具有3年的机器学习行业和研究经验，感兴趣于深度学习在语言和视觉理解中的实际应用。

#### 参考文献

1.  [人工神经元模型](https://www.willamette.edu/~gorr/classes/cs449/ann-overview.html)

1.  [神经网络第1部分：设置架构（斯坦福CNN教程）](http://cs231n.github.io/neural-networks-1/)

1.  [前馈神经网络的维基百科文章](https://en.wikipedia.org/wiki/Feedforward_neural_network)

1.  [感知机的维基百科文章](https://en.wikipedia.org/wiki/Perceptron)

1.  [单层神经网络（感知机）](http://computing.dcu.ie/~humphrys/Notes/Neural/single.neural.html)

1.  [单层感知机](http://www.cs.stir.ac.uk/courses/ITNP4B/lectures/kms/2-Perceptrons.pdf)

1.  [加权网络 – 感知机](http://page.mi.fu-berlin.de/rojas/neural/chapter/K3.pdf)

1.  [神经网络模型（监督学习）（scikit-learn文档）](http://scikit-learn.org/dev/modules/neural_networks_supervised.html)

1.  [神经网络中的隐藏层计算了什么？](http://stats.stackexchange.com/a/63163/53914)

1.  [如何选择前馈神经网络中的隐藏层数量和节点数量？](http://stats.stackexchange.com/a/1097/53914)

1.  [人工神经网络速成简介](http://ulcar.uml.edu/~iag/CS/Intro-to-ANN.html)

1.  [为什么在人工神经网络中需要偏置？我们是否需要为每一层设置单独的偏置？](http://stackoverflow.com/questions/7175099/why-the-bias-is-necessary-in-ann-should-we-have-separate-bias-for-each-layer)

1.  [基础神经网络教程 – 理论](https://takinginitiative.wordpress.com/2008/04/03/basic-neural-network-tutorial-theory/)

1.  [神经网络揭秘（视频系列）：第 1 部分，Welch 实验室 @ MLconf SF](https://www.youtube.com/watch?v=5MXp9UUkSmc)

1.  A. W. Harley, "卷积神经网络的交互式节点-链接可视化," 见 ISVC, 页码 867-877, 2015 ([链接](http://scs.ryerson.ca/~aharley/vis/harley_vis_isvc15.pdf))

[原文](https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/)。

* * *

## 我们的三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业轨道。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的 IT

* * *

### 更多相关主题

+   [快速数据科学技巧与窍门：学习 SAS](https://www.kdnuggets.com/2022/05/sas-quick-data-science-tips-tricks-learn.html)

+   [7 个 Pandas 绘图函数以快速数据可视化](https://www.kdnuggets.com/7-pandas-plotting-functions-for-quick-data-visualization)

+   [Voronoi 图的快速概述](https://www.kdnuggets.com/2022/11/quick-overview-voronoi-diagrams.html)

+   [快速指南：如何找到合适的注释专家](https://www.kdnuggets.com/2022/04/quick-guide-find-right-minds-annotation.html)

+   [使用 PyTorch 的可解释神经网络](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)

+   [深度神经网络不会引领我们走向 AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)
