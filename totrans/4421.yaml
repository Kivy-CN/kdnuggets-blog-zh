- en: Which methods should be used for solving linear regression?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/09/solving-linear-regression.html](https://www.kdnuggets.com/2020/09/solving-linear-regression.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Ahmad Bin Shafiq](https://medium.com/@ahmadbinshafiq), Machine Learning
    Student**.'
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression is a supervised machine learning algorithm. It predicts a linear
    relationship between an independent variable (y), based on the given dependant
    variables (x), such that the independent variable (y) has the **lowest cost**.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Different approaches to solve linear regression models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many different methods that we can apply to our linear regression
    model in order to make it more efficient. But we will discuss the most common
    of them here.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Least Square Method / Normal Equation Method
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adams Method
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Singular Value Decomposition (SVD)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Okay, so let’s begin…
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most common and easiest methods for beginners to solve linear regression
    problems is gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: '**How Gradient Descent works**'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's suppose we have our data plotted out in the form of a scatter graph,
    and when we apply a cost function to it, our model will make a prediction. Now
    this prediction can be very good, or it can be far away from our ideal prediction
    (meaning its cost will be high). So, in order to minimize that cost (error), we
    apply gradient descent to it.
  prefs: []
  type: TYPE_NORMAL
- en: Now, gradient descent will slowly converge our hypothesis towards a global minimum,
    where the **cost** would be lowest. In doing so, we have to manually set the value
    of **alpha, **and the slope of the hypothesis changes with respect to our alpha’s
    value. If the value of alpha is large, then it will take big steps. Otherwise,
    in the case of small alpha, our hypothesis would converge slowly and through small
    baby steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05c736b21cefb5041ec721443fcb0400.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Hypothesis converging towards a global minimum. Image from [Medium](https://medium.com/@ahmadbinshafiq/linear-regression-simplified-for-beginners-dcd3afe0b23f).*'
  prefs: []
  type: TYPE_NORMAL
- en: The Equation for Gradient Descent is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c61371274f47e7d11e00674987d93d89.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Source: [Ruder.io](https://ruder.io/optimizing-gradient-descent/index.html#batchgradientdescent).*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Implementing Gradient Descent in Python**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2134bd14e00ac4b5f024dfa631837691.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Model after Gradient Descent.*'
  prefs: []
  type: TYPE_NORMAL
- en: Here first, we have created our dataset, and then we looped over all our training
    examples in order to minimize our cost of hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros:**'
  prefs: []
  type: TYPE_NORMAL
- en: Important advantages of Gradient Descent are
  prefs: []
  type: TYPE_NORMAL
- en: Less Computational Cost as compared to SVD or ADAM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running time is O(kn²)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Works well with more number of features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons:**'
  prefs: []
  type: TYPE_NORMAL
- en: Important cons of Gradient Descent are
  prefs: []
  type: TYPE_NORMAL
- en: Need to choose some learning rate **α**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Needs many iterations to converge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be stuck in Local Minima
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If not proper Learning Rate **α**, then it might not converge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Least Square Method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The least-square method, also known as the **normal equation,** is also one
    of the most common approaches to solving linear regression models easily. But,
    this one needs to have some basic knowledge of linear algebra.
  prefs: []
  type: TYPE_NORMAL
- en: '**How the least square method works**'
  prefs: []
  type: TYPE_NORMAL
- en: In normal LSM, we solve directly for the value of our coefficient. In short,
    in one step, we reach our optical minimum point, or we can say only in one step
    we fit our hypothesis to our data with the lowest cost possible.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de26c7903434f41e202cdfdc438bfa58.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Before and after applying LSM to our dataset. Image from [Medium](https://towardsdatascience.com/complete-guide-to-linear-regression-in-python-d95175447255).*'
  prefs: []
  type: TYPE_NORMAL
- en: The equation for LSM is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46f6be2f627273f634faebbbd21e17ee.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Implementing LSM in Python**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/37cf628c5f022a6500bd42f2a2e07d28.png)'
  prefs: []
  type: TYPE_IMG
- en: Here first we have created our dataset and then minimized the cost of our hypothesis
    using the
  prefs: []
  type: TYPE_NORMAL
- en: '*b = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)*'
  prefs: []
  type: TYPE_NORMAL
- en: code, which is equivalent to our equation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Important advantages of LSM are:'
  prefs: []
  type: TYPE_NORMAL
- en: No Learning Rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No Iterations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature Scaling Not Necessary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Works really well when the Number of Features is less.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Important cons are:'
  prefs: []
  type: TYPE_NORMAL
- en: Is computationally expensive when the dataset is big.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slow when Number of Features is more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running Time is O(n³)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, your X transpose X is non-invertible, i.e., a singular matrix with
    no inverse. You can use *np.linalg.pinv* instead of *np.linalg.inv* to overcome
    this problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adam’s Method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ADAM, which stands for Adaptive Moment Estimation, is an optimization algorithm
    that is widely used in Deep Learning.
  prefs: []
  type: TYPE_NORMAL
- en: It is an iterative algorithm that works well on noisy data.
  prefs: []
  type: TYPE_NORMAL
- en: It is the combination of RMSProp and Mini-batch Gradient Descent algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to storing an exponentially decaying average of past squared gradients
    like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of
    past gradients, similar to momentum.
  prefs: []
  type: TYPE_NORMAL
- en: 'We compute the decaying averages of past and past squared gradients respectively
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d8d0ce73722f8ff946b44afce3bed82b.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Credit: [Ruder.io](https://ruder.io/optimizing-gradient-descent/index.html#adam).*'
  prefs: []
  type: TYPE_NORMAL
- en: As *mt* and *vt* are initialized as vectors of 0’s, the authors of Adam observe
    that they are biased towards zero, especially during the initial time steps, and
    especially when the decay rates are small (i.e., β1β1 and β2β2 are close to 1).
  prefs: []
  type: TYPE_NORMAL
- en: 'They counteract these biases by computing bias-corrected first and second-moment
    estimates:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b1d19391beb6059d447bedfd93aac64.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Credit: [Ruder.io](https://ruder.io/optimizing-gradient-descent/#adam).*'
  prefs: []
  type: TYPE_NORMAL
- en: 'They then update the parameters with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72bdfcb71d105944ac3fc874029e46e4.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Credit: [Ruder.io](https://ruder.io/optimizing-gradient-descent/#adam).*'
  prefs: []
  type: TYPE_NORMAL
- en: You can learn the theory behind Adam [here](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c) or [here](https://ruder.io/optimizing-gradient-descent/#adam).
  prefs: []
  type: TYPE_NORMAL
- en: '**Pseudocode for Adam** is'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7d9cefe677a4dec1b2411846d9b89eb.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Source: [Arxiv Adam](https://arxiv.org/pdf/1412.6980v9.pdf).*'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see it’s code in Pure Python.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now Let’s find the actual graph of Linear Regression and values for slope and
    intercept for our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/052dd2e3a15d0ee688af908dd00bfd1a.png)'
  prefs: []
  type: TYPE_IMG
- en: Now let us see the Linear Regression line using the Seaborn *regplot *function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/17b5a51be7715cb2d005f456aebaff9b.png)'
  prefs: []
  type: TYPE_IMG
- en: Let us code Adam Optimizer now in pure Python.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have implemented all the equations mentioned in the pseudocode above
    using an object-oriented approach and some helper functions.
  prefs: []
  type: TYPE_NORMAL
- en: Let us now set the hyperparameters for our model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: And finally, the training process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/edf2910abbf73579207f6226f5774d44.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, if we compare the *Final theta* values to the slope and intercept values,
    calculated earlier using *scipy.stats.mstat.linregress*, they are almost 99% equal
    and can be 100% equal by adjusting the hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let us plot it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/35e7578a0e22a853df15714a5103be67.png)'
  prefs: []
  type: TYPE_IMG
- en: And we can see that our plot is similar to plot obtained using *sns.regplot*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros:**'
  prefs: []
  type: TYPE_NORMAL
- en: Straightforward to implement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computationally efficient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Little memory requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invariant to diagonal rescale of the gradients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Well suited for problems that are large in terms of data and/or parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appropriate for non-stationary objectives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appropriate for problems with very noisy/or sparse gradients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyper-parameters have intuitive interpretation and typically require little
    tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons:**'
  prefs: []
  type: TYPE_NORMAL
- en: Adam and RMSProp are highly sensitive to certain values of the learning rate
    (and, sometimes, other hyper-parameters like the batch size), and they can catastrophically
    fail to converge if e.g., the learning rate is too high. (Source: [stackexchange](https://ai.stackexchange.com/questions/11455/when-should-we-use-algorithms-like-adam-as-opposed-to-sgd))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singular Value Decomposition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Singular value decomposition shortened as SVD is one of the famous and most
    widely used dimensionality reduction methods in linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: SVD is used (amongst other uses) as a preprocessing step to reduce the number
    of dimensions for our learning algorithm. SVD decomposes a matrix into a product
    of three other matrices (U, S, V).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/99d90e99fc9d0ec6c74a128cb72ae814.png)'
  prefs: []
  type: TYPE_IMG
- en: Once our matrix has been decomposed, the coefficients for our hypothesis can
    be found by calculating the pseudoinverse of the input matrix **X** and multiplying
    that by the output vector **y**. After that, we fit our hypothesis to our data,
    and that gives us the lowest cost.
  prefs: []
  type: TYPE_NORMAL
- en: '**Implementing SVD in Python**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0474bf1c816645c5a8f9d172b29df722.png)'
  prefs: []
  type: TYPE_IMG
- en: Though it is not converged very well, it is still pretty good.
  prefs: []
  type: TYPE_NORMAL
- en: Here first, we have created our dataset and then minimized the cost of our hypothesis
    using b = np.linalg.pinv(X).dot(y), which is the equation for SVD.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros:**'
  prefs: []
  type: TYPE_NORMAL
- en: Works better with higher dimensional data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good for gaussian type distributed data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Really stable and efficient for a small dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While solving linear equations for linear regression, it is more stable and
    the preferred approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons:**'
  prefs: []
  type: TYPE_NORMAL
- en: Running time is O(n³)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple risk factors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Really sensitive to outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: May get unstable with a very large dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning Outcome
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As of now, we have learned and implemented gradient descent, LSM, ADAM, and
    SVD. And now, we have a very good understanding of all of these algorithms, and
    we also know what are the pros and cons.
  prefs: []
  type: TYPE_NORMAL
- en: One thing we noticed was that the ADAM optimization algorithm was the most accurate,
    and according to the actual ADAM research paper, ADAM outperforms almost all other
    optimization algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Linear to Logistic Regression, Explained Step by Step](https://www.kdnuggets.com/2020/03/linear-logistic-regression-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Beginner’s Guide to Linear Regression in Python with Scikit-Learn](https://www.kdnuggets.com/2019/03/beginners-guide-linear-regression-python-scikit-learn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear Regression In Real Life](https://www.kdnuggets.com/2018/08/linear-regression-real-life.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[3 Reasons Why You Should Use Linear Regression Models Instead of…](https://www.kdnuggets.com/2021/08/3-reasons-linear-regression-instead-neural-networks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Which Metric Should I Use? Accuracy vs. AUC](https://www.kdnuggets.com/2022/10/metric-accuracy-auc.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Comparing Linear and Logistic Regression](https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear vs Logistic Regression: A Succinct Explanation](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News 22:n12, March 23: Best Data Science Books for…](https://www.kdnuggets.com/2022/n12.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear Regression for Data Science](https://www.kdnuggets.com/2022/07/linear-regression-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
