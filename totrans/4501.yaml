- en: 'Build an Artificial Neural Network From Scratch: Part 2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头构建人工神经网络：第2部分
- en: 原文：[https://www.kdnuggets.com/2020/03/build-artificial-neural-network-scratch-part-2.html](https://www.kdnuggets.com/2020/03/build-artificial-neural-network-scratch-part-2.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/03/build-artificial-neural-network-scratch-part-2.html](https://www.kdnuggets.com/2020/03/build-artificial-neural-network-scratch-part-2.html)
- en: '[comments](#comments)![Figure](../Images/c9b03ee546c984cf6fada48aa5830875.png)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)![图](../Images/c9b03ee546c984cf6fada48aa5830875.png)'
- en: '[Source](https://forum.novelupdates.com/threads/i-need-answers.16270/)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://forum.novelupdates.com/threads/i-need-answers.16270/)'
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的IT需求'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'In my previous article, [**Build an Artificial Neural Network(ANN) from scratch:
    Part-1**](https://towardsdatascience.com/build-an-artificial-neural-network-ann-from-scratch-part-1-a21988497962)we
    started our discussion about what are artificial neural networks; we saw how to
    create a simple neural network with one input and one output layer, from scratch
    in Python. Such a neural network is called a perceptron. However, real-world neural
    networks, capable of performing complex tasks such as image classification and
    stock market analysis, contain multiple hidden layers in addition to the input
    and output layer.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前的文章中，[**从头构建人工神经网络(ANN)：第1部分**](https://towardsdatascience.com/build-an-artificial-neural-network-ann-from-scratch-part-1-a21988497962)，我们开始讨论什么是人工神经网络；我们看到如何用Python从头创建一个简单的神经网络，具有一个输入层和一个输出层。这样的神经网络被称为感知器。然而，现实世界的神经网络能够执行复杂任务，如图像分类和股票市场分析，除了输入层和输出层，还包含多个隐藏层。
- en: In the previous article, we concluded that a Perceptron is capable of finding
    a linear decision boundary. We used the perceptron to predict whether a person
    is diabetic or not using a dummy dataset. However, **a perceptron is not capable
    of finding non-linear decision boundaries.**
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一篇文章中，我们总结了感知器能够找到线性决策边界。我们使用感知器预测一个人是否患糖尿病，使用的是一个虚拟数据集。然而，**感知器无法找到非线性决策边界。**
- en: In this article, we will develop a neural network with one input layer, one
    hidden layer, and one output layer. We will see that the neural network that we
    will develop will be capable of finding non-linear boundaries.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将开发一个具有一个输入层、一个隐藏层和一个输出层的神经网络。我们将看到我们开发的神经网络将能够找到非线性边界。
- en: Generating a dataset
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成数据集
- en: Let’s start by generating a dataset we can play with. Fortunately, [scikit-learn](http://scikit-learn.org/) has
    some useful dataset generators, so we don’t need to write the code ourselves.
    We will go with the [make_moons](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) function.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始生成一个可以玩耍的数据集。幸运的是，[scikit-learn](http://scikit-learn.org/) 提供了一些有用的数据集生成器，因此我们不需要自己编写代码。我们将使用
    [make_moons](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html)
    函数。
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/29a9ae72755b310886ff687fc841e614.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29a9ae72755b310886ff687fc841e614.png)'
- en: The dataset we generated has two classes, plotted as red and blue points. You
    can think of the blue dots as male patients and the red dots as female patients,
    with the x-axis and y-axis being medical measurements.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成的数据集有两个类别，以红色和蓝色点绘制。你可以将蓝色点视为男性患者，红色点视为女性患者，x轴和y轴表示医疗测量。
- en: Our goal is to train a Machine Learning classifier that predicts the correct
    class (male or female) given the `x` and `y`coordinates. Note that the data is
    not linearly separable, we can’t draw a straight line that separates the two classes.
    This means that linear classifiers, such as ANN without any hidden layer or even
    Logistic Regression, won’t be able to fit the data unless you hand-engineer non-linear
    features (such as polynomials) that work well for the given dataset.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是训练一个机器学习分类器，该分类器根据 `x` 和 `y` 坐标预测正确的类别（男性或女性）。请注意，数据是非线性可分的，我们无法绘制一条直线将两个类别分开。这意味着线性分类器，例如没有隐藏层的
    ANN 或者逻辑回归，将无法拟合数据，除非你手动工程化对给定数据集有效的非线性特征（例如多项式）。
- en: Neural Networks with One Hidden Layer
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单隐藏层神经网络
- en: 'Here’s our simple network:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的简单网络：
- en: '![](../Images/fdd87697154b2bd9784b0ceed74a5cf3.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fdd87697154b2bd9784b0ceed74a5cf3.png)'
- en: We have two inputs: **x1** and **x2**. There is a single hidden layer with 3
    units (nodes): **h1, h2**, and **h3**. Finally, there are two outputs: **y1** and **y2**.
    The arrows that connect them are the weights. There are two weights matrices: **w**,
    and **u**. The **w** weights connect the input layer and the hidden layer. The **u** weights
    connect the hidden layer and the output layer. We have employed the letters **w**,
    and **u**, so it is easier to follow the computation to follow. You can also see
    that we compare the outputs **y1** and **y2** with the targets **t1** and **t2**.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个输入：**x1** 和 **x2**。有一个隐藏层，包含 3 个单元（节点）：**h1, h2** 和 **h3**。最后，有两个输出：**y1**
    和 **y2**。连接它们的箭头是权重。共有两个权重矩阵：**w** 和 **u**。**w** 权重连接输入层和隐藏层。**u** 权重连接隐藏层和输出层。我们使用了字母
    **w** 和 **u**，以便更容易跟踪计算。你还可以看到，我们将输出 **y1** 和 **y2** 与目标 **t1** 和 **t2** 进行比较。
- en: 'There is one last letter we need to introduce before we can get to the computations.
    Let **a** be the linear combination prior to activation. Thus, we have:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行计算之前，我们需要引入最后一个字母。让 **a** 代表激活之前的线性组合。因此，我们有：
- en: '![](../Images/439b819aae373829b90d9174d522e237.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/439b819aae373829b90d9174d522e237.png)'
- en: and
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![](../Images/434fbfe985ff45b2fbaa8dae922c1fd7.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/434fbfe985ff45b2fbaa8dae922c1fd7.png)'
- en: Since we cannot exhaust all activation functions and all loss functions, we
    will focus on two of the most common. A **sigmoid** activation and an **L2-norm
    loss**. With this new information and the new notation, the output y is equal
    to the activated linear combination.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们无法穷举所有激活函数和所有损失函数，我们将专注于两种最常见的：**sigmoid** 激活函数和 **L2-norm 损失**。有了这些新信息和新符号，输出
    y 等于激活的线性组合。
- en: 'Therefore, for the output layer, we have:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于输出层，我们有：
- en: '![](../Images/facc4d63342964822889148b02a922a5.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/facc4d63342964822889148b02a922a5.png)'
- en: 'while for the hidden layer:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 而对于隐藏层：
- en: '![](../Images/956ac8ed8250ea3ce341d186d7f7a0ca.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/956ac8ed8250ea3ce341d186d7f7a0ca.png)'
- en: We will examine backpropagation for the output layer and the hidden layer separately,
    as the methodologies differ.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分别考察输出层和隐藏层的反向传播，因为方法有所不同。
- en: 'I would like to remind you that:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我想提醒你：
- en: '![](../Images/93e86f051d737f9fd5382cf96b06aec6.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93e86f051d737f9fd5382cf96b06aec6.png)'
- en: 'The sigmoid function is:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: sigmoid 函数是：
- en: '![](../Images/4d8f2ba13f34e96007b11946575c8542.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d8f2ba13f34e96007b11946575c8542.png)'
- en: 'and its derivative is:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其导数是：
- en: '![](../Images/c61e2215da96d344393a1cdb220e5f3f.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c61e2215da96d344393a1cdb220e5f3f.png)'
- en: Backpropagation for the output layer
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输出层的反向传播
- en: 'In order to obtain the update rule:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到更新规则：
- en: '![](../Images/6666261d5aa8814bbb88483ab35ed6f2.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6666261d5aa8814bbb88483ab35ed6f2.png)'
- en: we must calculate
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须计算
- en: '![](../Images/b4b1e6984fda749dcfb8222d7fd02cf4.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4b1e6984fda749dcfb8222d7fd02cf4.png)'
- en: 'Let’s take a single weight *u*ij . The partial derivative of the loss w.r.t. *u*ij
    equals:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑单个权重 *u*ij。损失函数关于 *u*ij 的偏导数等于：
- en: '![](../Images/fc37eeeedb738c64fc33a127448e33c0.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc37eeeedb738c64fc33a127448e33c0.png)'
- en: where i corresponds to the previous layer (input layer for this transformation)
    and j corresponds to the next layer (output layer of the transformation). The
    partial derivatives were computed simply following the chain rule.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 i 对应于前一层（本次变换的输入层），j 对应于下一层（变换的输出层）。偏导数的计算是简单地按照链式法则进行的。
- en: '![](../Images/7d6d25ad485edee669bebe7feccd898e.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d6d25ad485edee669bebe7feccd898e.png)'
- en: following the L2-norm loss derivative.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 按照 L2-norm 损失的导数。
- en: '![](../Images/759bfb3e847d8aee144b39460e27ece6.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/759bfb3e847d8aee144b39460e27ece6.png)'
- en: following the sigmoid derivative.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 按照 sigmoid 导数进行。
- en: 'Finally, the third partial derivative is simply the derivative of:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，第三个偏导数只是以下的导数：
- en: '![](../Images/434fbfe985ff45b2fbaa8dae922c1fd7.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/434fbfe985ff45b2fbaa8dae922c1fd7.png)'
- en: So,
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，
- en: '![](../Images/08850244595ca1cf164aeec7edd0adc0.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08850244595ca1cf164aeec7edd0adc0.png)'
- en: 'Replacing the partial derivatives in the expression above, we get:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 替换上述表达式中的偏导数，我们得到：
- en: '![](../Images/6c06ef7bc245aa3df9066a95aba67a9c.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c06ef7bc245aa3df9066a95aba67a9c.png)'
- en: 'Therefore, the update rule for a single weight for the output layer is given
    by:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，输出层单个权重的更新规则为：
- en: '![](../Images/8db95d12806ce2f99e5619d0ca7afd07.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8db95d12806ce2f99e5619d0ca7afd07.png)'
- en: 5\. Backpropagation of a hidden layer
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5\. 隐藏层的反向传播
- en: 'Similarly to the backpropagation of the output layer, the update rule for a
    single weight, *wij* would depend on:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于输出层的反向传播，单个权重的更新规则，*wij* 将依赖于：
- en: '![](../Images/62ad4693fc62b76dbb9d9081b404152e.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62ad4693fc62b76dbb9d9081b404152e.png)'
- en: 'following the chain rule. Taking advantage of the results we have so far for
    transformation using the sigmoid activation and the linear model, we get:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 按照链式法则。利用目前我们为使用 sigmoid 激活函数和线性模型变换得到的结果，我们得到：
- en: '![](../Images/7342f010519945bddc9f40d11aff893b.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7342f010519945bddc9f40d11aff893b.png)'
- en: and
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![](../Images/ceb75107e4dc2c12135998a034fccb5b.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ceb75107e4dc2c12135998a034fccb5b.png)'
- en: The actual problem for backpropagation comes from the term
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播的实际问题来源于术语
- en: '![](../Images/55435ed94c0cd7c31c3885d2ca12b97e.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55435ed94c0cd7c31c3885d2ca12b97e.png)'
- en: That’s due to the fact that there is no “hidden” target. You can follow the
    solution for weight *w11* below. It is advisable to keep a look on the NN diagram
    shown above, while going through the computations.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为没有“隐藏”的目标。你可以参考下面对权重 *w11* 的解决方案。在进行计算时，建议查看上面显示的神经网络图。
- en: '![](../Images/4005feb701d1e03924701879e0e8ada8.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4005feb701d1e03924701879e0e8ada8.png)'
- en: From here, we can calculate
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们可以计算
- en: '![](../Images/2cb24bcd18194bd78b7ecbd8559f79f7.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2cb24bcd18194bd78b7ecbd8559f79f7.png)'
- en: 'Which was what we wanted. The final expression is:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们想要的。最终的表达式是：
- en: '![](../Images/baa55ff6d8cc63a8edf579dafb9c0e10.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/baa55ff6d8cc63a8edf579dafb9c0e10.png)'
- en: 'The generalized form of this equation is:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程的广义形式是：
- en: '![](../Images/2adb09a0fc5b9bc6319733aa1b92767f.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2adb09a0fc5b9bc6319733aa1b92767f.png)'
- en: Backpropagation generalization
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播的广义化
- en: Using the results for backpropagation for the output layer and the hidden layer,
    we can put them together in one formula, summarizing backpropagation, in the presence
    of L2-norm loss and sigmoid activations.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用输出层和隐藏层的反向传播结果，我们可以将它们合并为一个公式，总结反向传播，考虑 L2 范数损失和 sigmoid 激活。
- en: '![](../Images/b60b742945e59628928aa6b066ab43de.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b60b742945e59628928aa6b066ab43de.png)'
- en: where for a hidden layer
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于隐藏层
- en: '![](../Images/6433c58e11c166fd2a334db235c2c67f.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6433c58e11c166fd2a334db235c2c67f.png)'
- en: Code for Neural Networks with One Hidden Layer
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带有一个隐藏层的神经网络代码
- en: Now let’s implement the neural network that we just discussed in Python from
    scratch. We will again try to classify the non-linear data that we created above.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们从头开始在 Python 中实现刚才讨论的神经网络。我们将再次尝试分类我们上面创建的非线性数据。
- en: We start by defining some useful variables and parameters for gradient descent
    like training dataset size, dimensions of input and output layers.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从定义一些用于梯度下降的有用变量和参数开始，比如训练数据集的大小、输入层和输出层的维度。
- en: '[PRE1]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Also defining the Gradient descent parameters.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 同时定义梯度下降的参数。
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'First, let’s implement the loss function we defined above. We use this to evaluate
    how well our model is doing:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们实现上述定义的损失函数。我们用它来评估模型的表现：
- en: '[PRE3]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We also implement a helper function to calculate the output of the network.
    It does forward propagation and returns the class with the highest probability.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还实现了一个辅助函数来计算网络的输出。它执行前向传播并返回概率最高的类别。
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Finally, here comes the function to train our Neural Network. It implements
    batch gradient descent using the backpropagation derivates we found above.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这里是训练我们神经网络的函数。它实现了使用上述反向传播导数的批量梯度下降。
- en: This function learns parameters for the neural network and returns the model.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数学习神经网络的参数并返回模型。
- en: '**nn_hdim**: Number of nodes in the hidden layer'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**nn_hdim**: 隐藏层中节点的数量'
- en: '**num_passes**: Number of passes through the training data for gradient descent'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**num_passes**: 梯度下降训练数据的遍历次数'
- en: '**print_loss**: If True, print the loss every 1000 iterations'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**print_loss**: 如果为 True，每1000次迭代打印一次损失'
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally the main method:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最后是主要方法：
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Print loss after every 1000 iterations:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 每 1000 次迭代后打印损失：
- en: '![](../Images/2379742845310b45ea5aaaaba1dd09c4.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2379742845310b45ea5aaaaba1dd09c4.png)'
- en: '![](../Images/d8f8aa5b2fe29ab7c66fce9b731bcfc8.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8f8aa5b2fe29ab7c66fce9b731bcfc8.png)'
- en: Classification when the number of nodes in the hidden layer is 3
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当隐藏层节点数量为 3 时的分类
- en: Now Let’s now get a sense of how varying the hidden layer size affects the result.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们了解一下隐藏层大小的变化如何影响结果。
- en: '[PRE7]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Figure](../Images/720b6b3c0ad14b2f64ca3799a648f12a.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/720b6b3c0ad14b2f64ca3799a648f12a.png)'
- en: We can see that a hidden layer of low dimensionality nicely captures the general
    trend of our data. Higher dimensionalities are prone to overfitting. They are
    “memorizing” the data as opposed to fitting the general shape.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，低维度的隐藏层很好地捕捉了数据的一般趋势。较高的维度容易导致过拟合。它们是在“记忆”数据，而不是拟合一般的形状。
- en: If we were to evaluate our model on a separate test set the model with a smaller
    hidden layer size would likely perform better due to better generalization. We
    could counteract overfitting with stronger regularization, but picking the correct
    size for the hidden layer is a much more “economical” solution.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在单独的测试集上评估我们的模型，较小隐藏层大小的模型由于更好的泛化能力可能会表现得更好。我们可以通过更强的正则化来对抗过拟合，但选择合适的隐藏层大小是更“经济”的解决方案。
- en: You can get the whole code in this GitHub repo.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这个 GitHub 仓库中获取完整的代码。
- en: '[**nageshsinghc4/Artificial-Neural-Network-from-scratch-python**](https://github.com/nageshsinghc4/Artificial-Neural-Network-from-scratch-python/tree/master)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[**nageshsinghc4/人工神经网络从零开始-python**](https://github.com/nageshsinghc4/Artificial-Neural-Network-from-scratch-python/tree/master)'
- en: Conclusion
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: So in this article, we saw how we can mathematically derive a neural network
    with one hidden layer and we also created a neural network with 1 hidden layer,
    from scratch in Python using NumPy.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这篇文章中，我们看到如何从数学上推导出一个具有一个隐藏层的神经网络，并且我们还用 NumPy 从零开始创建了一个具有 1 个隐藏层的神经网络。
- en: Well, this concludes the two-article series on building an Artificial Neural
    Network from scratch**. **I hope you guys have enjoyed reading it, feel free to
    share your comments/thoughts/feedback in the comment section.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，这就结束了关于从零开始构建人工神经网络的两篇文章系列**。** 希望你们喜欢阅读，欢迎在评论区分享你的评论/想法/反馈。
- en: '**Bio: [Nagesh Singh Chauhan](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)**
    is a Big data developer at CirrusLabs. He has over 4 years of working experience
    in various sectors like Telecom, Analytics, Sales, Data Science having specialisation
    in various Big data components.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[Nagesh Singh Chauhan](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)**
    是 CirrusLabs 的大数据开发人员。他在电信、分析、销售、数据科学等各个领域拥有超过 4 年的工作经验，并在各种大数据组件方面具有专业知识。'
- en: '[Original](https://towardsdatascience.com/build-an-artificial-neural-network-ann-from-scratch-part-2-a33c44eca56b).
    Reposted with permission.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/build-an-artificial-neural-network-ann-from-scratch-part-2-a33c44eca56b)。
    经授权转载。'
- en: '**Related:**'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Build an Artificial Neural Network From Scratch: Part 1](/2019/11/build-artificial-neural-network-scratch-part-1.html)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从零开始构建人工神经网络：第 1 部分](/2019/11/build-artificial-neural-network-scratch-part-1.html)'
- en: '[Nothing but NumPy: Understanding & Creating Neural Networks with Computational
    Graphs from Scratch](/2019/08/numpy-neural-networks-computational-graphs.html)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[只有 NumPy：从头理解和创建神经网络的计算图](/2019/08/numpy-neural-networks-computational-graphs.html)'
- en: '[How to Convert a Picture to Numbers](/2020/01/convert-picture-numbers.html)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何将图片转换为数字](/2020/01/convert-picture-numbers.html)'
- en: More On This Topic
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[How to Build and Train a Transformer Model from Scratch with…](https://www.kdnuggets.com/how-to-build-and-train-a-transformer-model-from-scratch-with-hugging-face-transformers)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何从零开始构建和训练 Transformer 模型…](https://www.kdnuggets.com/how-to-build-and-train-a-transformer-model-from-scratch-with-hugging-face-transformers)'
- en: '[Learn Deep Learning by Building 15 Neural Network Projects in 2022](https://www.kdnuggets.com/2022/01/15-neural-network-projects-build-2022.html)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过构建 15 个神经网络项目在 2022 年学习深度学习](https://www.kdnuggets.com/2022/01/15-neural-network-projects-build-2022.html)'
- en: '[Neural Network Optimization with AIMET](https://www.kdnuggets.com/2022/04/qualcomm-neural-network-optimization-aimet.html)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 AIMET 优化神经网络](https://www.kdnuggets.com/2022/04/qualcomm-neural-network-optimization-aimet.html)'
- en: '[Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 TensorFlow 和 Keras 构建和训练您的第一个神经网络](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
- en: '[Building a Convolutional Neural Network with PyTorch](https://www.kdnuggets.com/building-a-convolutional-neural-network-with-pytorch)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 PyTorch 构建卷积神经网络](https://www.kdnuggets.com/building-a-convolutional-neural-network-with-pytorch)'
- en: '[The Importance of Permutation in Neural Network Predictions](https://www.kdnuggets.com/2022/12/importance-permutation-neural-network-predictions.html)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[排列在神经网络预测中的重要性](https://www.kdnuggets.com/2022/12/importance-permutation-neural-network-predictions.html)'
