- en: 'The Machine Learning Abstracts: Decision Trees'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/08/machine-learning-abstracts-decision-trees.html](https://www.kdnuggets.com/2017/08/machine-learning-abstracts-decision-trees.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**By Narendra Nath Joshi, Carnegie Mellon.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decisions, decisions...](../Images/240702f60e87ee704fd1f8bab010cd11.png)'
  prefs: []
  type: TYPE_IMG
- en: To be or not to be, is the question? But it is, really? Or isn’t it really?
    OMG, this is addictive, isn’t it? But again, is it?
  prefs: []
  type: TYPE_NORMAL
- en: '*Decision Tree Learning* is a classic algorithm used in machine learning for
    classification and regression purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: Regression is the process of predicting a continuous value as opposed to predicting
    a discrete class label in classification
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The basic intuition behind a decision tree is to map out all possible decision
    paths in the form of a tree.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/502244b74de5d9f3c28d3244dacfd2e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A tree showing survival of passengers on the [Titanic](https://en.wikipedia.org/wiki/Titanic)
    (“sibsp” is the number of spouses or siblings aboard). The figures under the leaves
    show the probability of outcome and the percentage of observations in the leaf.
    **Source: Wikipedia**'
  prefs: []
  type: TYPE_NORMAL
- en: Each path from the root to a leaf of the tree signifies a decision process.
    Let us try to analyze and deeply understand decision trees using our Spring cleaning
    example we talked about in the [previous part about classification](https://medium.com/the-science-of-data/the-machine-learning-abstracts-part-1-classification-63c620bd3707).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d231adb32fe247a28a5c2c025d58eb4d.png)'
  prefs: []
  type: TYPE_IMG
- en: Our training data.
  prefs: []
  type: TYPE_NORMAL
- en: We inferred that the rule the classification algorithm learns is
  prefs: []
  type: TYPE_NORMAL
- en: '*KEEP = SENTIMENTAL_VALUE | (NEED_FUTURE & EXPENSIVE)*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'So, a decision tree for the current problem would be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1d64ad3977bf60f12a693e93e96ac05.png)'
  prefs: []
  type: TYPE_IMG
- en: The Decision Tree for our Spring cleaning problem
  prefs: []
  type: TYPE_NORMAL
- en: Explaining, ain’t nobody got time for dat?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s see how we go about the process of constructing a decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree construction involves splitting. In our example, each feature
    has only two possible values (**“yes”** and **“no”**). So, each feature can be
    split into two ways.
  prefs: []
  type: TYPE_NORMAL
- en: We can go about splitting every possible feature into two, but that would give
    us a very big and highly inefficient tree.
  prefs: []
  type: TYPE_NORMAL
- en: '**How do we ensure we have a reasonable and a good tree?**'
  prefs: []
  type: TYPE_NORMAL
- en: Before splitting at each level of the tree, we evaluate all features based on
    their distribution and decide which feature is the best to split. More formally,
    which feature gives us the highest ***information gain*** on splitting.
  prefs: []
  type: TYPE_NORMAL
- en: Information gain is defined as the expected amount of information that would
    be needed to specify whether a new input instance should be classified **“yes”**
    or **“no”**, given that the example reached that node from the root.
  prefs: []
  type: TYPE_NORMAL
- en: '**What do you mean “*reached that node”*?**'
  prefs: []
  type: TYPE_NORMAL
- en: All training examples do not reach all parts of the tree. For instance, consider
    item 3in our Spring cleaning example.
  prefs: []
  type: TYPE_NORMAL
- en: It is eliminated and classified as **“KEEP”** at the root of our decision tree
    itself and does not make it to lower levels of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4b05160cf61a541bc1386e312524cb2.png)'
  prefs: []
  type: TYPE_IMG
- en: There are algorithms for constructing decision trees. The one I talked about
    above is called the [ID3 algorithm](https://en.wikipedia.org/wiki/ID3_algorithm)
    which is a basic one.
  prefs: []
  type: TYPE_NORMAL
- en: There are some advanced ones too like [C4.5 algorithm](https://en.wikipedia.org/wiki/C4.5_algorithm),
    [CART](https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29)
    (Classification and Regression Tree) and [CHAID](https://en.wikipedia.org/wiki/CHAID)
    (CHi-squared Automatic Interaction Detector)
  prefs: []
  type: TYPE_NORMAL
- en: Thanks y’all, next time I shall preach a little about Unsupervised Learning
    and stuff.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Narendra Nath Joshi](http://nnjoshi.co/)** is a graduate student in
    AI and Machine Learning at Carnegie Mellon University, currently pursuing a research
    intern at Disney Research Pittsburgh. Has a keen interest in natural language,
    computer vision, and deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/the-science-of-data/the-machine-learning-abstracts-part-2-decision-trees-58c87c40a22b).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[The Machine Learning Abstracts: Classification](/2017/07/machine-learning-abstracts-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Which Machine Learning Algorithm Should I Use?](/2017/06/which-machine-learning-algorithm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Why Artificial Intelligence and Machine Learning?](/2017/06/why-artificial-intelligence-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Machine Learning from Scratch: Decision Trees](https://www.kdnuggets.com/2022/11/machine-learning-scratch-decision-trees.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Decision Trees vs Random Forests, Explained](https://www.kdnuggets.com/2022/08/decision-trees-random-forests-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Generalized and Scalable Optimal Sparse Decision Trees(GOSDT)](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Demystifying Decision Trees for the Real World](https://www.kdnuggets.com/demystifying-decision-trees-for-the-real-world)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Telling a Great Data Story: A Visualization Decision Tree](https://www.kdnuggets.com/2021/02/telling-great-data-story-visualization-decision-tree.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Random Forest vs Decision Tree: Key Differences](https://www.kdnuggets.com/2022/02/random-forest-decision-tree-key-differences.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
