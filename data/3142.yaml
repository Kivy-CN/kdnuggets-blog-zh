- en: Multiscale Methods and Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多尺度方法与机器学习
- en: 原文：[https://www.kdnuggets.com/2018/03/multiscale-methods-machine-learning.html](https://www.kdnuggets.com/2018/03/multiscale-methods-machine-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/03/multiscale-methods-machine-learning.html](https://www.kdnuggets.com/2018/03/multiscale-methods-machine-learning.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: Multiscale methods, in which a dataset is viewed and analyzed at different scales,are
    becoming more commonplace in machine learning recently and are proving to be valuable
    tools. At their core, multiscale methods capture the local geometry of neighborhoods
    defined by a series of distances between points or sets of nearest neighbors.
    This is a bit like viewing a part of a slide through a series of microscope resolutions.
    At high resolutions, very small features are captured in a small space within
    the sample. At lower resolutions, more of the slide is visible, and a person can
    investigate bigger features.Main advantages of multiscale methods include improved
    performance relative to state-of-the-art methods and dramatic reductions in necessary
    sample size to achieve these results.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 多尺度方法，即在不同尺度下查看和分析数据集，近年来在机器学习中变得越来越普遍，并且被证明是有价值的工具。它们的核心是捕捉由一系列点之间的距离或最近邻的集合定义的邻域的局部几何形状。这有点像通过一系列显微镜分辨率来观察幻灯片的一部分。在高分辨率下，非常小的特征会在样本的一个小空间内被捕捉。在低分辨率下，更多的幻灯片区域可见，人们可以研究更大的特征。多尺度方法的主要优点包括相对于最先进的方法性能的提升，以及实现这些结果所需样本量的显著减少。
- en: 'As a simple example, consider taking the mean of nearest neighbors within a
    number line or two-dimensional space. When k increases, the mean of point sets
    on the number line becomes more spread out, with farther neighbors pulling the
    mean more than nearer neighbors. Nearest neighbors that are farther away from
    an existing set of points tend to drag the mean further than added points that
    are closer to the existing set of points. In the number line example, say the
    points sit at 1, 2, 4, 5, 9, 11, and 14\. Consider the set of the left most point
    (at 1) and its 3 nearest neighbors: 2, 4, and 5\. The mean for this set is 3,
    which has 2 points with higher values and 2 points with lower values. When 9 is
    added to the set as a 4^(th) nearest neighbor of point 1, the mean jumps to 4.2,
    which is skewed towards point 9 rather than the 4 points whose values are close
    to one another. Examining the mean of point 1’s 3 nearest neighbors reveals a
    set of points with fairly similar values and variance among values. However, examining
    the 4 nearest neighbors of point 1 captures the presence of an outlier. The KNN
    mean function with a single value of k fails to capture some of these important
    local properties that emerge at different scales, and important information is
    lost.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 举一个简单的例子，考虑在数轴或二维空间中取最近邻的均值。当 k 增加时，数轴上点集的均值变得更加分散，较远的邻居对均值的影响大于较近的邻居。在数轴示例中，假设点位于
    1、2、4、5、9、11 和 14。考虑最左边点（在 1）及其 3 个最近邻：2、4 和 5。该集合的均值是 3，其中有 2 个点的值较高，2 个点的值较低。当
    9 作为点 1 的第 4 个最近邻加入集合时，均值跳到 4.2，这个均值偏向于点 9，而不是那 4 个值接近的点。检查点 1 的 3 个最近邻的均值揭示了一个具有相似值和方差的点集。然而，检查点
    1 的 4 个最近邻则捕捉到了一个离群点。具有单一 k 值的 KNN 均值函数无法捕捉这些在不同尺度下出现的重要局部属性，从而导致重要信息的丢失。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析水平'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 为你的组织提供 IT 支持'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![Multiscale methods](../Images/c4b6f227c0a5700e4497ef647a6076cf.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![多尺度方法](../Images/c4b6f227c0a5700e4497ef647a6076cf.png)'
- en: Not all multiscale methods rely on KNN-defined scales, but they do employ similar
    principles that allow the method to learn multiscale features within a dataset.
    Grouping points within a series of distances, providing a series of convolutions
    of different scales, and employing hierarchies of clusters are common choices
    in multiscale methods. A recent deep learning paper by Pelt & Sethian (2017) based
    on multiscale methods was able to learn image segmentation and image classification
    on training sets as small as 6 images with high accuracy (>90%); this algorithm
    significantly reduced tuning requirements of the deep learning framework, as well.
    Another deep learning paper (Xiao et al., 2018) blends global image characteristics
    with a series of local extractions (in convolution layers, shown below) to capture
    many different scales of image features within a single deep learning framework.
    This algorithm consistently outperformed state-of-the-art deep learning algorithms
    on several benchmark datasets, suggesting that multiscale methods can improve
    even state-of-the-art convolution networks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有的多尺度方法都依赖于KNN定义的尺度，但它们确实使用了类似的原理，使得方法可以在数据集中学习多尺度特征。将点在一系列距离内分组、提供不同尺度的卷积序列以及采用集群层次结构是多尺度方法中的常见选择。Pelt和Sethian（2017）的一篇基于多尺度方法的深度学习论文能够在多达6张图像的小训练集上以高精度（>90%）进行图像分割和图像分类；该算法显著减少了深度学习框架的调参需求。另一篇深度学习论文（Xiao等，2018）将全局图像特征与一系列局部提取（在卷积层中，见下图）结合起来，以捕捉单一深度学习框架中的多种图像特征尺度。该算法在多个基准数据集上
    consistently 优于最新的深度学习算法，表明多尺度方法可以进一步改进最先进的卷积网络。
- en: '![KNN Multiscale methods](../Images/796baf813395aac399997d6b283897e7.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![KNN 多尺度方法](../Images/796baf813395aac399997d6b283897e7.png)'
- en: Recent work on k-nearest neighbor (KNN) regression and classification ensembles
    using varied neighborhood size have shown dramatic improvement over not only the
    KNN algorithms with a single value for k but also other machine learning methods.
    The multiscale tools of topological data analysis (TDA), including persistent
    homology, have aided analysis of small datasets, images, videos, and other types
    of data, providing data scientists and machine learning researchers with a robust,
    general unsupervised learning tool. A recently-added multiscale component on the
    TDA tool Mapper improved theoretical stability of the algorithm and yielded consistent
    results.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最近关于k-近邻（KNN）回归和分类集成方法在使用不同邻域大小时显示出了显著的改进，不仅比单一k值的KNN算法表现更好，还超越了其他机器学习方法。拓扑数据分析（TDA）的多尺度工具，包括持久同调，帮助分析了小型数据集、图像、视频和其他类型的数据，为数据科学家和机器学习研究人员提供了一个强大且通用的无监督学习工具。最近添加的TDA工具Mapper上的多尺度组件改善了算法的理论稳定性，并产生了一致的结果。
- en: Alternatives to multiscale methods exist, but they often come with a cost. Rather
    than improve performance and compute times with a small training sample, many
    provide either improved performance or reduction in necessary sample size. Bayesian
    methods, the main competitor of multiscale methods in recent years, have reduced
    the necessary sample size for good performance of deep learning algorithms, but
    they come with a high computational cost that may not be appealing for online
    algorithm use or quick turnaround of analyses in industry. These methods also
    require a high degree of statistical expertise that many practitioners may not
    have. Deep learning has become a ubiquitous, general tool in recent years, but
    its power seems to lie in the asymptotic properties. Rather than level-off in
    performance when a large enough sample size is reached for the algorithm to converge,
    deep learning algorithms continue to improve with further increases in sample
    size. This is desirable when a lot of data is available, but many industry use
    cases and research problems involve small datasets or limited examples for a given
    class.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多尺度方法的替代方案，但它们通常伴随一定的成本。与使用小样本训练样本来提高性能和计算时间相比，许多方法要么提供了改进的性能，要么减少了所需的样本量。贝叶斯方法，近年来多尺度方法的主要竞争者，已减少了深度学习算法良好性能所需的样本量，但其计算成本很高，这可能不适合在线算法使用或在工业中快速分析。这些方法还需要高水平的统计专业知识，而许多实践者可能没有。近年来，深度学习已成为一种普遍的通用工具，但其力量似乎在于渐近性质。深度学习算法在样本量足够大时不会停滞不前，而是随着样本量的进一步增加而不断提高。这在数据量充足时是令人期望的，但许多工业用例和研究问题涉及的是小数据集或有限的类别样本。
- en: It seems that local geometry matters. What is relevant and important for one
    neighborhood may not be relevant or important for a larger neighborhood surrounding
    the original one, and examining multiple scales can help capture all the important
    features, rather than the neighborhood yielding the best subset of features. Given
    the success of multiscale methods on a variety of learning tasks and their enhancement
    many types of algorithms, it is likely that machine learning will see more algorithms
    incorporating multiscale subsets in the coming years.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来局部几何结构很重要。对一个邻域而言相关和重要的内容，可能对围绕原始邻域的更大邻域来说并不相关或重要，检查多个尺度可以帮助捕捉所有重要特征，而不是仅仅依赖于邻域所提供的最佳特征子集。鉴于多尺度方法在各种学习任务中的成功及其对多种算法的增强，预计未来几年机器学习将会出现更多结合多尺度子集的算法。
- en: References
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Dey, T. K., Mémoli, F., & Wang, Y. (2016). Multiscale mapper: Topological summarization
    via codomain covers. In *Proceedings of the twenty-seventh annual acm-siam symposium
    on discrete algorithms* (pp. 997-1013). Society for Industrial and Applied Mathematics.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Dey, T. K., Mémoli, F., & Wang, Y. (2016). 多尺度映射器：通过余域覆盖的拓扑总结。在*第二十七届ACM-SIAM离散算法年会论文集*（第997-1013页）。工业与应用数学学会。
- en: 'Farrelly, C. M. (2017). KNN Ensembles for Tweedie Regression: The Power of
    Multiscale Neighborhoods. *arXiv preprint arXiv:1708.02122*.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Farrelly, C. M. (2017). KNN集成方法用于Tweedie回归：多尺度邻域的力量。*arXiv预印本 arXiv:1708.02122*。
- en: Pelt, D. M., & Sethian, J. A. (2017). A mixed-scale dense convolutional neural
    network for image analysis. *Proceedings of the National Academy of Sciences*,
    201715832.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Pelt, D. M., & Sethian, J. A. (2017). 一种用于图像分析的混合尺度密集卷积神经网络。*美国国家科学院院刊*, 201715832。
- en: Xia, K., & Wei, G. W. (2014). Persistent homology analysis of protein structure,
    flexibility, and folding. *International journal for numerical methods in biomedical
    engineering*, *30*(8), 814-844.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Xia, K., & Wei, G. W. (2014). 蛋白质结构、灵活性和折叠的持久同调分析。*生物医学工程数值方法国际期刊*, *30*(8),
    814-844。
- en: 'Xiao, F., Deng, W., Peng, L., Cao, C., Hu, K., &Gao, X. (2018). MSDNN: Multi-Scale
    Deep Neural Network for Salient Object Detection. *arXiv preprint arXiv:1801.04187*.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Xiao, F., Deng, W., Peng, L., Cao, C., Hu, K., & Gao, X. (2018). MSDNN：用于显著目标检测的多尺度深度神经网络。*arXiv预印本
    arXiv:1801.04187*。
- en: '**Bio: [Colleen M. Farrelly](https://www.linkedin.com/in/colleenmfarrelly)**
    is a data scientist whose industry experience includes positions related to healthcare,
    education, biotech, marketing, and finance. Her areas of research include topology/topological
    data analysis, ensemble learning, nonparametric statistics, manifold learning,
    and explaining mathematics to lay audiences (https://www.quora.com/profile/Colleen-Farrelly-1).
    When she isn’t doing data science, she is a poet and author (https://www.amazon.com/Colleen-Farrelly/e/B07832WQG7).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[Colleen M. Farrelly](https://www.linkedin.com/in/colleenmfarrelly)**
    是一位数据科学家，拥有涉及医疗保健、教育、生物技术、营销和金融的行业经验。她的研究领域包括拓扑/拓扑数据分析、集成学习、非参数统计、流形学习以及向普通观众解释数学（https://www.quora.com/profile/Colleen-Farrelly-1）。当她不从事数据科学时，她是一位诗人和作者（https://www.amazon.com/Colleen-Farrelly/e/B07832WQG7）。'
- en: '**Related:**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[A Tour of The Top 10 Algorithms for Machine Learning Newbies](https://www.kdnuggets.com/2018/02/tour-top-10-algorithms-machine-learning-newbies.html)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习新手的十大算法巡礼](https://www.kdnuggets.com/2018/02/tour-top-10-algorithms-machine-learning-newbies.html)'
- en: '[Deep Misconceptions About Deep Learning](https://www.kdnuggets.com/2018/03/deep-misconceptions-about-deep-learning.html)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于深度学习的深层误解](https://www.kdnuggets.com/2018/03/deep-misconceptions-about-deep-learning.html)'
- en: '[How to do Machine Learning Efficiently](https://www.kdnuggets.com/2018/03/machine-learning-efficiently.html)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何高效进行机器学习](https://www.kdnuggets.com/2018/03/machine-learning-efficiently.html)'
- en: More On This Topic
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Alternative Feature Selection Methods in Machine Learning](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中的替代特征选择方法](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
- en: '[Understanding Python''s Iteration and Membership: A Guide to…](https://www.kdnuggets.com/understanding-pythons-iteration-and-membership-a-guide-to-__contains__-and-__iter__-magic-methods)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解 Python 的迭代和成员资格：__contains__ 和 __iter__ 魔法方法指南](https://www.kdnuggets.com/understanding-pythons-iteration-and-membership-a-guide-to-__contains__-and-__iter__-magic-methods)'
- en: '[Centroid Initialization Methods for k-means Clustering](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[k-means 聚类的质心初始化方法](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)'
- en: '[Python String Methods](https://www.kdnuggets.com/2022/12/python-string-methods.html)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 字符串方法](https://www.kdnuggets.com/2022/12/python-string-methods.html)'
- en: '[Data Science Methods Drive Business Success](https://www.kdnuggets.com/2023/10/nwu-data-science-methods-drive-business-success)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学方法推动业务成功](https://www.kdnuggets.com/2023/10/nwu-data-science-methods-drive-business-success)'
- en: '[11 Python Magic Methods Every Programmer Should Know](https://www.kdnuggets.com/11-python-magic-methods-every-programmer-should-know)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个程序员都应该知道的 11 个 Python 魔法方法](https://www.kdnuggets.com/11-python-magic-methods-every-programmer-should-know)'
