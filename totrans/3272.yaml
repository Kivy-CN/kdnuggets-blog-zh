- en: AI and Deep Learning, Explained Simply
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/07/ai-deep-learning-explained-simply.html/2](https://www.kdnuggets.com/2017/07/ai-deep-learning-explained-simply.html/2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Faulty automation increases (rather than kill) human jobs**. 2 days after
    I published this article, suddenly my profile was blocked. Google for “LinkedIn
    account restricted” to learn this happens to many for simply too much activity,
    even if not messaging. I had only opened, for curiosity, the profiles of the hundreds
    who clicked “I like” on this article (thanks to you all). Then, a naive rule “*x* pages
    opened within time *y*” AI decided I was an AI too, of the “web bot” kind (programs
    browsing all the pages of a site to copy its contents). It blocked me without
    any “Slow down” warning, why to warn a bot?'
  prefs: []
  type: TYPE_NORMAL
- en: '**I was not a bot, I swear I am human**. Counting “*x* pages opened within
    time *y*” it catches many bots, but also “false positives”: curious humans in
    activity peaks. This frustrated the LinkedIn staff too: who to trust, the AI or
    the user? I had to send my ID as proof, it took days. This rule-based “AI” created
    extra human support jobs to just handle unnecessary “AI” errors that humans alone
    would not do. Think at: “flag all emails from Nigeria as spam” or “flag all people
    with long beard as terrorists”. You can improve rules by adding parameters, past
    activity, etc., but never reaching the accuracy of humans or fine trained MLs.
    Beware of “automation” or “AI” claims: most it is still too simple rules, no any
    deep learning. Microsoft acquired LinkedIn for $26 billion, will surely upgrade
    this old piece to real ML. But until then, don’t browse LinkedIn too fast!'
  prefs: []
  type: TYPE_NORMAL
- en: '**If no human can predict something, often the ML can’t too.** Many people
    trained MLs with years of market price changes, but these MLs **fail to predict
    the market**. The ML will guess how things will go if the learned past factors
    and trends will keep the same. But stock and economy trends change very often,
    like at random. MLs fail when the older data gets less relevant or wrong very
    soon and often. The task or rules learned must keep the same, or at most rarely
    updated, so you can re-train. For ex. learning to drive, play poker, paint in
    a style, predict a sickness given health data, translate between languages are
    jobs for MLs: old examples will keep valid for the near future.'
  prefs: []
  type: TYPE_NORMAL
- en: '**ML can find cause-effects on data, but it can’t find what it does not exist**.
    For ex. in the weird research “Automated inference on criminality using face images”,
    the ML was trained on labeled face photos of jailed and honest guys (some of whom,
    let me add, could be criminals who was not discovered?). Authors claimed that
    the ML learned to catch new bad guys from just a face photo, but “feeling” that
    further research will refute the validity of physiognomy (racism). Really, their
    data set is biased: some white collar criminals pose as honest guys, laughing
    about that. The ML learned the only relations it could find: happy or angry mouths,
    type of collar (neck cloth). Those smiling with white collar are classified as
    honest, those sad with dark collar are rated as crooks. The ML authors tried to
    judge the people by their faces (not science! no correlation), but failed to see
    that the ML learned to judge by clothes (social status) instead. The ML amplified
    an injustice bias: street thieves in cheap clothes (perhaps with darker skin)
    are discovered and jailed more often than corrupt politicians and top level corporate
    fraudsters. This ML will send to jail all the street guys, and not a single white
    collar, if not also told that street thieves are discovered *x*% more frequently
    than white collars. If told so, again, it would take random or no decisions, this
    is not science. A lesson is: MLs do not experienced living in our world like an
    adult human. MLs can’t can’t know what’s outside the data given, including the
    “obvious”, for ex.: the more a fire is damaging, the more fire trucks are sent
    to stop it. An ML will note: the more firefighters at a fire scene, the more damage
    the day after, so the fire trucks cause fire damage. Result: the ML will send
    to jail the firefighters for arson, cause: “95% correlation”!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/53e8919e8836c68766c74e8746fe27ca.png)'
  prefs: []
  type: TYPE_IMG
- en: '*(ML can’t find correlations that do not exist, like: face with criminality.
    But this data set is biased: no smiling white collar criminals in it! ML will
    learn the bias)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**MLs can predict what humans can’t, in some cases**. For ex. Deep Patient,
    trained from 700,000 patients data by M. Sinai Hospital in New York, it can anticipate
    the onset of schizophrenia: no one knows how! Only the ML can: humans can’t learn
    to do the same by studying the ML. This is an issue: for an investment, medical,
    judicial or military decision, you may want to **know** **how the AI reached its
    conclusions, but you can’t**. You can’t know why the ML denied your loan, advised
    a judge to jail you or gave the job to someone else. Was the ML fair or unfair?
    Unbiased or biased by race, gender or else? The ML computations are visible, but
    too many to make a human-readable summary. The ML speaks like a prophet: “You
    humans can’t understand, even if I show you the math, so have faith! You tested
    my past predictions, and these were correct!”.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Humans are never fully explaining their decisions too:** We give reasonable-sounding,
    but always **incomplete, over-simplified** reasons. For ex: “We invaded Iraq due
    to its weapons of mass destruction” looked right, but there were dozens more reasons.
    This looks wrong, even when the ML is right: “We bombed that village since a reputable
    ML said they was terrorists”. It only lacks explanation. People getting almost
    always right answers from MLs will start to make up fake explanations, just for
    the public to accept the MLs predictions. Some will use MLs in secret, crediting
    the ideas to themselves.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The ML results are only as good the data you train the ML with**. In ML you
    rarely write software, that’s provided by Google (Keras, Tensorflow), Microsoft
    etc. and the algorithms are open source. ML is an unpredictable science defined
    by experimentation, not by theory. You spend most of the time preparing the data
    to train and studying the results, then doing lots of changes, mostly by guessing,
    and retrying. ML’s fed with too few or inaccurate data will give wrong results.
    Google Images incorrectly classified African Americans as gorillas, while Microsoft’s
    Tay bot learned nazi, sex and hate speech after only hours training on Twitter.
    The issue was the data, not the software.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Undesirable biases are implicit in human-generated data**: an ML trained
    on Google News associated “father is to doctor as mother is to nurse” reflecting
    gender bias. If used as is, it might prioritize male job applicants over female
    ones. A law enforcement ML could discriminate by skin color. During the Trump
    campaign, some ML may have reduced recommending “Mexican” restaurants, as a side
    effect of reading many negative posts about Mexican immigration, even if no one
    complained about Mexican food or restaurants specifically. You can’t simply copy
    data from the internet into your ML, and expect it to end up balanced. **To train
    a wise ML it’s expensive**: you need humans to review and “de-bias” what’s wrong
    or evil, but naturally happening in the media.'
  prefs: []
  type: TYPE_NORMAL
- en: '![self-driving car](../Images/1fe4b408380fa65ec825765f72c9f9a0.png)'
  prefs: []
  type: TYPE_IMG
- en: '***(Photo: James Bridle entraps a self-driving car inside an unexpected circle)***'
  prefs: []
  type: TYPE_NORMAL
- en: '**ML is limited since it lacks general intelligence and prior common sense**.
    Even merging together all the specialized MLs, or training an ML on everything,
    it will still fail at general AI tasks, for ex. at understanding language. You
    can’t talk about every topic with Siri, Alexa or Cortana like with real people:
    they’re just assistants. In 2011, IBM Watson answered faster than humans at *Jeopardy!* TV
    quiz, but confused Canada with USA. ML can produce useful summaries of long texts,
    including sentiment analysis (opinions and mood identification), but not as reliable
    as human works. Chatbots fail to understand too many questions. No current AI
    can do what’s easy for every human: to guess all the times when a customer is
    frustrated or sarcastic, and to change tone accordingly. There is no any general
    AI like in the movies. But we can get small sci-fi looking AI pieces, separately,
    that win humans at narrow (specific) tasks. What’s new is that “narrow” can include
    creative or supposedly human-only tasks: paint (styles, geometries, less likely
    if symbolic or conceptual), compose, create, guess, deceive, fake emotions, etc.
    all of which, incredibly, seem not to require general AI.'
  prefs: []
  type: TYPE_NORMAL
- en: '**No one knows how to build a general AI**. This is great: we get super-human
    specialized (narrow AI) workers, but no any Terminator or Matrix will decide on
    its own to kill us anytime soon. Unfortunately, humans can train machines to kill
    us right now, for ex. a terrorist teaching self-driving trucks to hit pedestrians.
    An AI with general intelligence it would probably self-destruct, rather than obey
    to terrorist orders.'
  prefs: []
  type: TYPE_NORMAL
- en: '**AI ethics will be hacked, reprogrammed illegally**. Current ML, being not
    general or sentient AI, will always follow the orders (training data) given by
    humans: don’t expect AI conscientious objectors. Each government will have to
    write laws detailing if a self-driving car will prefer to kill either its passenger(s)
    or pedestrian(s). For ex. two kids run suddenly in front of a car with a single
    passenger, and to avoid the kids, the car can only run in a deadly option, like
    a cliff. Polls show that the majority of people would prefer to own a car that
    kills pedestrians rather than themselves. Most people don’t think yet at these
    very rare events, but will overreact and question politicians when the first case
    it will happen, even if only once per billion cars. In countries where cars will
    be instructed to kill a single passenger to save multiple pedestrians, car owners
    will ask hackers to secretly reprogram cars to always save the passenger(s). But
    within pirated AI patches, hidden AI malware and viruses will probably be installed
    too!'
  prefs: []
  type: TYPE_NORMAL
- en: '**To teach a human it’s easy**: for most tasks, you give a dozen of examples
    and let him/her try a few times. But an ML requires thousand times more labeled
    data: only humans can learn from little data. An ML must try a million more times:
    if real world experiments are mandatory (can’t fully simulate like for chess,
    go, etc.), you’ll have to crash thousands of real cars, kill or hurt thousands
    of real human patients, etc. before to complete a training. An ML, unlike humans, **overfits**:
    it memorizes too specific detail of the training data, instead of general patterns.
    So, it fails on real tasks over never seen before data, even just a little different
    from the training data. Current ML it lacks the human general intelligence that
    models each situation and relates it to prior experience, to learn from very few
    examples or trial and errors, memorizing just what’s general, and ignoring what’s
    not relevant, avoiding to try what it can be predicted as a fail.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
