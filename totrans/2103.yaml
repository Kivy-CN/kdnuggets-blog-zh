- en: The Easiest Way of Running Llama 3 Locally
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在本地运行 Llama 3 的最简单方法
- en: 原文：[https://www.kdnuggets.com/easiest-way-of-running-llama-3-locally](https://www.kdnuggets.com/easiest-way-of-running-llama-3-locally)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/easiest-way-of-running-llama-3-locally](https://www.kdnuggets.com/easiest-way-of-running-llama-3-locally)
- en: '![Most Easiest Way of Running Llama 3 Locally](../Images/39060dd5295993a6ff5cdf6171b3a640.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![最简单的本地运行 Llama 3 的方法](../Images/39060dd5295993a6ff5cdf6171b3a640.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Running LLMs (Large Language Models) locally has become popular as it provides
    security, privacy, and more control over model outputs. In this mini tutorial,
    we learn the easiest way of downloading and using the Llama 3 model.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地运行 LLM（大型语言模型）已变得越来越流行，因为它提供了安全性、隐私性和对模型输出的更多控制。在本教程中，我们将学习下载和使用 Llama 3
    模型的最简单方法。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Llama 3 is Meta AI's latest family of LLMs. It is open-source, comes with advanced
    AI capabilities, and improves response generation compared to Gemma, Gemini, and
    Claud 3.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 3 是 Meta AI 最新的 LLM 家族。它是开源的，具有先进的 AI 能力，并且比 Gemma、Gemini 和 Claud 3 改进了响应生成。
- en: What is Ollama?
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是 Ollama？
- en: '[Ollama/ollama](https://github.com/ollama/ollama) is an open-source tool for
    using LLMs like Llama 3 on your local machine. With new research and development,
    these large language models do not require large VRam, computing, or storage.
    Instead, they are optimized for use in laptops.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ollama/ollama](https://github.com/ollama/ollama) 是一个开源工具，用于在本地计算机上使用像 Llama
    3 这样的 LLM。通过新的研究和开发，这些大型语言模型不再需要大量的 VRam、计算或存储。相反，它们经过优化，可以在笔记本电脑上使用。'
- en: There are multiple tools and frameworks available for you to use LLMs locally,
    but Ollama is the easiest to set up and use. It lets you use LLMs directly from
    a terminal or Powershell. It is fast and comes with core features that will make
    you start using it immediately.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种工具和框架可以让您在本地使用 LLM，但 Ollama 是最容易设置和使用的。它允许您直接从终端或 Powershell 使用 LLM。它快速且具有核心功能，使您能够立即开始使用。
- en: The best part of Ollama is that it integrates with all kinds of software, extensions,
    and applications. For example, you can use the CodeGPT extension in VScode and
    connect Ollama to start using Llama 3 as your AI code assistant.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Ollama 的最佳部分是它可以与各种软件、扩展和应用程序集成。例如，您可以在 VScode 中使用 CodeGPT 扩展并连接 Ollama，以将 Llama
    3 作为您的 AI 代码助手。
- en: Installing Ollama
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 Ollama
- en: Download and Install Ollama by going to the GitHub repository [Ollama/ollama](https://github.com/ollama/ollama),
    scrolling down, and clicking the download link for your operating system.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通过访问 GitHub 仓库 [Ollama/ollama](https://github.com/ollama/ollama)，向下滚动并点击适合您操作系统的下载链接来下载和安装
    Ollama。
- en: '![Download option for various operating systems of Ollama](../Images/6e8b4e26d87d3166233ef73014ddd816.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![Ollama 的各种操作系统下载选项](../Images/6e8b4e26d87d3166233ef73014ddd816.png)'
- en: Image from [ollama/ollama](https://github.com/ollama/ollama) | Download option
    for various operating systems
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [ollama/ollama](https://github.com/ollama/ollama) | 各种操作系统的下载选项
- en: After Ollama is successfully installed it will show in the system tray as shown
    below.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 成功安装 Ollama 后，它将在系统托盘中显示如下图所示。
- en: '![Ollama in system tray](../Images/2f89a1204d44f371b2c87bcdc44c667b.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![Ollama 在系统托盘中](../Images/2f89a1204d44f371b2c87bcdc44c667b.png)'
- en: Downloading and Using Llama 3
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载和使用 Llama 3
- en: To download the Llama 3 model and start using it, you have to type the following
    command in your terminal/shell.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载 Llama 3 模型并开始使用它，您需要在终端/命令行中输入以下命令。
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Depending on your internet speed, it will take almost 30 minutes to download
    the 4.7GB model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的互联网速度，下载 4.7GB 模型大约需要 30 分钟。
- en: '![PowerShell: downloading the Llama 3 using Ollama](../Images/5723d8bfd58789ad1627b12f584e983f.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![PowerShell：使用 Ollama 下载 Llama 3](../Images/5723d8bfd58789ad1627b12f584e983f.png)'
- en: Apart from the Llama 3 model, you can also install other LLMs by typing the
    commands below.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Llama 3 模型，你还可以通过输入下面的命令来安装其他 LLMs。
- en: '![Running other LLMs using Ollama](../Images/ed95b36b560e7818b171650123da7288.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Ollama 运行其他 LLMs](../Images/ed95b36b560e7818b171650123da7288.png)'
- en: Image from [ollama/ollama](https://github.com/ollama/ollama) | Running other
    LLMs using Ollama
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [ollama/ollama](https://github.com/ollama/ollama) | 使用 Ollama 运行其他 LLMs
- en: As soon as downloading is completed, you will be able to use the LLama 3 locally
    as if you are using it online.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦下载完成，你就可以像在线使用一样在本地使用 LLama 3。
- en: '**Prompt:** *"Describe a day in the life of a Data Scientist."*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示：** *“描述数据科学家的一天。”*'
- en: '![Using Llama 3 in Ollama](../Images/0988f34affd21f6b2648f83ad2ac06e2.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![在 Ollama 中使用 Llama 3](../Images/0988f34affd21f6b2648f83ad2ac06e2.png)'
- en: To demonstrate how fast the response generation is, I have attached the GIF
    of Ollama generating Python code and then explaining it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示响应生成的速度，我附上了一个 GIF，显示了 Ollama 生成 Python 代码并解释它的过程。
- en: '**Note:** If you have Nvidia GPU on your laptop and CUDA installed, Ollama
    will automatically use GPU instead of CPU to generate a response. Which is 10
    better.'
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 如果你的笔记本电脑上有 Nvidia GPU 和 CUDA 安装，Ollama 会自动使用 GPU 而不是 CPU 来生成响应。效果提高了
    10 倍。'
- en: '**Prompt:** *"Write a Python code for building the digital clock."*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示：** *“编写一个构建数字时钟的 Python 代码。”*'
- en: '![Checking the speed of Llama 3 response generation on GPU using Ollama](../Images/413943f3b41ecb21cc7e968434a65a19.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![检查使用 Ollama 在 GPU 上生成 Llama 3 响应的速度](../Images/413943f3b41ecb21cc7e968434a65a19.png)'
- en: You can exit the chat by typing `/bye` and then start again by typing `ollama
    run llama3`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过输入 `/bye` 来退出聊天，然后通过输入 `ollama run llama3` 重新开始。
- en: Final Thoughts
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最后的想法
- en: Open-source frameworks and models have made AI and LLMs accessible to everyone.
    Instead of being controlled by a few corporations, these locally run tools like
    Ollama make AI available to anyone with a laptop.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 开源框架和模型使 AI 和 LLMs 对每个人都变得可用。与少数公司控制不同，这些本地运行的工具如 Ollama 使任何拥有笔记本电脑的人都能使用 AI。
- en: Using LLMs locally provides privacy, security, and more control over response
    generation. Moreover, you don't have to pay to use any service. You can even create
    your own AI-powered coding assistant and use it in VSCode.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地使用 LLMs 提供了隐私、安全性以及对响应生成的更多控制。此外，你不需要支付任何服务费用。你甚至可以创建自己的 AI 驱动编码助手，并在 VSCode
    中使用它。
- en: If you want to learn about other applications to run LLMs locally, then you
    should read [5 Ways To Use LLMs On Your Laptop](/5-ways-to-use-llms-on-your-laptop).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解其他在本地运行 LLMs 的应用程序，那么你应该阅读 [在你的笔记本电脑上使用 LLMs 的 5 种方法](/5-ways-to-use-llms-on-your-laptop)。
- en: '[](https://www.polywork.com/kingabzpro)****[Abid Ali Awan](https://www.polywork.com/kingabzpro)****
    ([@1abidaliawan](https://www.linkedin.com/in/1abidaliawan)) is a certified data
    scientist professional who loves building machine learning models. Currently,
    he is focusing on content creation and writing technical blogs on machine learning
    and data science technologies. Abid holds a Master''s degree in technology management
    and a bachelor''s degree in telecommunication engineering. His vision is to build
    an AI product using a graph neural network for students struggling with mental
    illness.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.polywork.com/kingabzpro)****[Abid Ali Awan](https://www.polywork.com/kingabzpro)****
    ([@1abidaliawan](https://www.linkedin.com/in/1abidaliawan)) 是一位认证的数据科学专业人士，热衷于构建机器学习模型。目前，他专注于内容创作，并撰写关于机器学习和数据科学技术的技术博客。Abid
    拥有技术管理硕士学位和电信工程学士学位。他的愿景是使用图神经网络构建一款 AI 产品，帮助那些在精神健康方面挣扎的学生。'
- en: More On This Topic
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[A Simple Guide to Running LlaMA 2 Locally](https://www.kdnuggets.com/a-simple-guide-to-running-llama-2-locally)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在本地运行 LlaMA 2 的简单指南](https://www.kdnuggets.com/a-simple-guide-to-running-llama-2-locally)'
- en: '[The Easiest Way to Make Beautiful Interactive Visualizations With Pandas](https://www.kdnuggets.com/2021/12/easiest-way-make-beautiful-interactive-visualizations-pandas.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 Pandas 制作美丽交互式可视化的最简单方法](https://www.kdnuggets.com/2021/12/easiest-way-make-beautiful-interactive-visualizations-pandas.html)'
- en: '[Llama, Llama, Llama: 3 Simple Steps to Local RAG with Your Content](https://www.kdnuggets.com/3-simple-steps-to-local-rag-with-your-content)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Llama、Llama、Llama：与您的内容本地 RAG 的 3 个简单步骤](https://www.kdnuggets.com/3-simple-steps-to-local-rag-with-your-content)'
- en: '[Ollama Tutorial: Running LLMs Locally Made Super Simple](https://www.kdnuggets.com/ollama-tutorial-running-llms-locally-made-super-simple)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ollama 教程：让本地运行 LLMs 变得超级简单](https://www.kdnuggets.com/ollama-tutorial-running-llms-locally-made-super-simple)'
- en: '[Using Groq Llama 3 70B Locally: Step by Step Guide](https://www.kdnuggets.com/using-groq-llama-3-70b-locally-step-by-step-guide)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[本地使用 Groq Llama 3 70B：逐步指南](https://www.kdnuggets.com/using-groq-llama-3-70b-locally-step-by-step-guide)'
- en: '[Run an LLM Locally with LM Studio](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在本地使用 LM Studio 运行 LLM](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio)'
