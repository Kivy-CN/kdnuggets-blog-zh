["```py\n!pip install jupysql --quiet\n```", "```py\n%load_ext sql\n```", "```py\n%sql dialect://username:password@host:port/database\n```", "```py\n%sql duckdb://\n```", "```py\n%sql mysql://username:password@host:port/database\ndata = %sql SELECT * FROM mytable\n```", "```py\nfrom urllib.request import urlretrieve\n\n_ = urlretrieve(\n   \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\",\n   \"penguins.csv\",\n)\n```", "```py\nSELECT *\nFROM penguins.csv\nLIMIT 3\n```", "```py\n# Rename columns\ndf = data.rename(columns={'old_column_name': 'new_column_name'})  # Pandas\n%%sql df <<\nSELECT *, old_column_name\nAS new_column_name\nFROM data;  # JupySQL\n```", "```py\n# Remove missing values\ndata = data.dropna()  # Pandas\n%%sql df <<\nSELECT *\nFROM data\nWHERE column_name IS NOT NULL;  # JupySQL single column, can add conditions to all columns as needed.\n```", "```py\n# Convert data types\ndata['date_column'] = data['date_column'].astype('datetime64[ns]')  # Pandas\n%sql df <<\nSELECT *,\nCAST(date_column AS timestamp) AS date_column\nFROM data  # Jupysql\n```", "```py\n# Filter data\nfiltered_data = data[data['sales'] > 1000]  # Pandas\n%%sql df <<\nSELECT * FROM data\nWHERE sales > 1000;  # JupySQL\n```", "```py\n# Aggregate data\nmonthly_sales = data.groupby(['year', 'month'])['sales'].sum()  # Pandas\n%%sql df <<\nSELECT year, month,\nSUM(sales) as monthly_sales\nFROM data\nGROUP BY year, month  # JupySQL\n```", "```py\n# Combine data\nmerged_data = pd.merge(data1, data2, on='key_column')  # Pandas\n%%sql df <<\nSELECT * FROM data1\nJOIN data2\nON data1.key_column = data2.key_column;  # JupySQL\n```", "```py\n# Combine data\nmerged_data = pd.merge(data1, data2, on='key_column')  # Pandas\n%%sql df <<\nSELECT * FROM data1\nJOIN data2\nON data1.key_column = data2.key_column;  # JupySQL\n```", "```py\nSELECT *\nFROM penguins.csv\nWHERE species IS NOT NULL AND island IS NOT NULL AND bill_length_mm IS NOT NULL AND bill_depth_mm IS NOT NULL\nAND flipper_length_mm IS NOT NULL AND body_mass_g IS NOT NULL AND sex IS NOT NULL;\n```", "```py\n# Map the species column into classifiers\ntransformed_df = transformed_df.DataFrame().dropna()\ntransformed_df[\"mapped_species\"] = transformed_df.species.map(\n   {\"Adelie\": 0, \"Chinstrap\": 1, \"Gentoo\": 2}\n)\ntransformed_df.drop(\"species\", inplace=True, axis=1)\n\n# Checking our transformed data\ntransformed_df.head() \n```", "```py\n%sql postgresql://username:password@host:port/database\n%sql DROP TABLE IF EXISTS mytable;\n%sql CREATE TABLE mytable (column1 datatype1, column2 datatype2, ...);\n%sql COPY mytable FROM '/path/to/datafile.csv' DELIMITER ',' CSV HEADER;\n```", "```py\ntransformed_df.to_csv(\"transformed_data.csv\")\n```", "```py\nname: CI\n\non:\n push:\n pull_request:\n schedule:\n   - cron: '0 0 4 * *'\n\n# These permissions are needed to interact with GitHub's OIDC Token endpoint.\npermissions:\n id-token: write\n contents: read\n\njobs:\n report:\n   runs-on: ubuntu-latest\n   steps:\n   - uses: actions/checkout@v3\n   - name: Set up Python ${{ matrix.python-version }}\n     uses: conda-incubator/setup-miniconda@v2\n     with:\n       python-version: '3.10'\n       miniconda-version: latest\n       activate-environment: conda-env\n       channels: conda-forge, defaults\n\n   - name: Run notebook\n     env:\n       PLOOMBER_STATS_ENABLED: false\n       PYTHON_VERSION: '3.10'\n     shell: bash -l {0}\n     run: |\n       eval \"$(conda shell.bash hook)\"\n\n       # pip install -r requirements.txt\n       pip install jupysql pandas ploomber-engine --quiet\n       ploomber-engine --log-output posthog.ipynb report.ipynb\n\n   - uses: actions/upload-artifact@v3\n     if: always()\n     with:\n       name: Transformed_data\n       path: transformed_data.csv\n```"]