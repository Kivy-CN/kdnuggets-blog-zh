- en: Optimize Response Time of your Machine Learning API In Production
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化生产环境中机器学习API的响应时间
- en: 原文：[https://www.kdnuggets.com/2020/05/optimize-response-time-machine-learning-api-production.html](https://www.kdnuggets.com/2020/05/optimize-response-time-machine-learning-api-production.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/05/optimize-response-time-machine-learning-api-production.html](https://www.kdnuggets.com/2020/05/optimize-response-time-machine-learning-api-production.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Yannick Wolf](https://www.linkedin.com/in/yannick-wolff-457616124/),
    Lead Data Scientist @ Sicara**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由[Yannick Wolf](https://www.linkedin.com/in/yannick-wolff-457616124/)，Sicara的首席数据科学家**'
- en: This article demonstrates how building a smarter API serving Deep Learning models
    minimizes the response time.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章演示了如何通过构建一个更智能的API来服务深度学习模型，从而最小化响应时间。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: <picture>![](../Images/e8acc09138d252f613079422b466a468.png)</picture>
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/e8acc09138d252f613079422b466a468.png)</picture>
- en: 'Your team worked hard to build a Deep Learning model for a given task (let''s
    say: detecting bought products in a store thanks to Computer Vision). Good.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你的团队为给定任务（比如：通过计算机视觉检测商店内购买的产品）努力构建了一个深度学习模型。很好。
- en: 'You then developed and deployed an API that integrates this model (let''s keep
    our example: self-checkout machines would call this API). Great!'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你开发并部署了一个集成此模型的API（我们继续以自助结账机为例，这些机器会调用这个API）。太棒了！
- en: The new product is working well and you feel like all the work is done.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 新产品运行良好，你感觉所有工作都完成了。
- en: But since the manager decided to install more self-checkout machines (I really
    like this example), users have started to complain about the huge latency that
    occurs each time they are scanning a product.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 但由于经理决定安装更多自助结账机（我真的很喜欢这个例子），用户们开始抱怨每次扫描产品时出现的巨大延迟。
- en: What can you do? Buy 10x faster—and 10x more expensive—GPUs? Ask data scientists
    to try reducing the depth of the model without degrading its accuracy?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你能做什么？购买10倍更快且10倍更贵的GPU？让数据科学家尝试在不降低模型准确性的情况下减少模型的深度？
- en: Cheaper and easier solutions exist, as you will see in this article.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 存在更便宜、更简单的解决方案，正如你将在这篇文章中看到的。
- en: A basic API with a big dummy model
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个基本的API与一个大虚拟模型
- en: 'First of all, we''ll need a model with a long inference time to work with.
    Here is how I would do that with [TensorFlow 2](https://www.tensorflow.org/)''s
    Keras API (if you''re not familiar with this Deep Learning framework, just step
    over this piece of code):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要一个推理时间较长的模型来进行工作。下面是我如何使用[TensorFlow 2](https://www.tensorflow.org/)'s
    Keras API来实现这个目标的方式（如果你对这个深度学习框架不熟悉，可以跳过这段代码）：
- en: When testing the model on my [GeForce RTX 2080](https://www.nvidia.com/fr-fr/geforce/graphics-cards/rtx-2080/)
    GPU, I measured an inference time of 303 ms. That's what we can call a big model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当在我的[GeForce RTX 2080](https://www.nvidia.com/fr-fr/geforce/graphics-cards/rtx-2080/)
    GPU上测试模型时，我测得推理时间为303毫秒。这可以称为一个大模型。
- en: Now, we need a very simple API to serve our model, with only one route to ask
    for a prediction. A very standard API framework in Python is [Flask](https://www.palletsprojects.com/p/flask/).
    That's the one I chose, along with a [WSGI HTTP Server](https://www.fullstackpython.com/wsgi-servers.html)
    called [Gunicorn](https://gunicorn.org/).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要一个非常简单的API来提供我们的模型，只需要一个路由来请求预测。Python中一个非常标准的API框架是[Flask](https://www.palletsprojects.com/p/flask/)。这是我选择的框架，还有一个叫做[Gunicorn](https://gunicorn.org/)的[WSGI
    HTTP服务器](https://www.fullstackpython.com/wsgi-servers.html)。
- en: Our unique route parses the input from the request, calls the instantiated model
    on it and sends the output back to the user.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们独特的路由解析来自请求的输入，调用实例化的模型，并将输出发送回用户。
- en: 'We can run our deep learning API with the command:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令运行我们的深度学习API：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Okay, I can now send some random numbers to my API and it responds to me with
    some other random numbers. The question is: how fast?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我现在可以向我的 API 发送一些随机数字，它会用其他随机数字回应我。问题是：多快？
- en: Let's load test our API
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 让我们对我们的 API 进行负载测试
- en: We indeed want to know how fast our API responds, and especially when increasing
    the number of requests per second. Such a load test can be performed using [Locust](https://locust.io/)
    Python library.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确实想知道我们的 API 响应速度，尤其是在每秒请求数量增加时。这样的负载测试可以使用 [Locust](https://locust.io/) Python
    库进行。
- en: 'This library works with a locustfile.py which indicates which behavior to simulate:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库与一个 locustfile.py 文件一起工作，该文件指示要模拟的行为：
- en: Locust simulates several users, executing different tasks at the same time.
    In our case, we want to define only one task which is to call the /predict route
    with a random input.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Locust 模拟多个用户，同时执行不同的任务。在我们的案例中，我们只定义一个任务，即用随机输入调用 /predict 路径。
- en: We can also parametrize the wait_time, which is the time a simulated user waits
    after having received a response from the API, before sending his next request.
    To simulate the self-checkout use case, I've set this time to be a random value
    between 1 and 3 seconds.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以参数化 wait_time，即模拟用户在收到 API 响应后等待的时间，然后再发送下一个请求。为了模拟自助结账用例，我将这个时间设置为 1 到
    3 秒之间的随机值。
- en: 'The number of users is chosen through Locust''s dashboard, where all the statistics
    are shown in real-time. This dashboard can be run by calling the command:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 用户数量通过 Locust 的仪表板选择，其中所有统计数据实时显示。可以通过运行以下命令来启动此仪表板：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Locust dashboard
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Locust 仪表板
- en: 'So, how does our naive API react when we increase the number of users? As we
    could expect, quite badly:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，当我们增加用户数量时，我们的原始 API 如何反应？正如我们预期的那样，非常糟糕：
- en: with 1 user, the average response time I measured was 332 ms (slightly more
    than the isolated inference time previously measured, nothing surprising)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 1 个用户的情况下，我测量的平均响应时间为 332 毫秒（略高于之前测量的隔离推理时间，没什么意外）
- en: 'with 5 users, this average time increased a little bit: 431 ms'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 5 个用户的情况下，这个平均时间稍微增加了一些：431 毫秒
- en: with 20 users, it reached 4.1 s (more than 12 times more than with 1 user)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 20 个用户的情况下，响应时间达到了 4.1 秒（比 1 个用户多了 12 倍以上）
- en: <picture>![gunicorn-flask-tensorflow-api-response-time](../Images/e9ae12e8925c2b179622755f9d2e5010.png)</picture>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![gunicorn-flask-tensorflow-api-response-time](../Images/e9ae12e8925c2b179622755f9d2e5010.png)</picture>
- en: 'These results are not so surprising when we think of how requests are handled
    by our API. By default Gunicorn launches 2 processes: a master process listen
    to incoming requests, stores them into a [FIFO](https://en.wikipedia.org/wiki/FIFO_and_LIFO_accounting#FIFO)
    queue, and sends them successively to a worker process, each time it''s available.
    The latter manages to run your code to compute the response for each request.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑到 API 如何处理请求时，这些结果并不那么令人惊讶。默认情况下，Gunicorn 启动 2 个进程：一个主进程监听传入请求，将其存储到 [FIFO](https://en.wikipedia.org/wiki/FIFO_and_LIFO_accounting#FIFO)
    队列中，并将其依次发送到每个工作进程，每次它可用。后者负责运行你的代码，以计算每个请求的响应。
- en: Since the worker process is single-threaded, requests are handled one by one.
    If 5 requests arrive simultaneously, the 5th request will receive its response
    after 5 x 300 ms = 1500 ms, which explains the high average response time with
    20 users.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于工作进程是单线程的，请求逐个处理。如果同时到达 5 个请求，第 5 个请求将在 5 x 300 毫秒 = 1500 毫秒后收到响应，这解释了 20
    个用户情况下的高平均响应时间。
- en: The more the faster
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 越多越快
- en: 'Luckily, Gunicorn provides two ways to scale APIs by increasing:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Gunicorn 提供了通过增加以下方式来扩展 API 的两种方法：
- en: the number of threads handling requests
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理请求的线程数量
- en: the number of worker processes
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作进程的数量
- en: The multi-threading option is not the one that may help us the most since TensorFlow
    would not allow a thread to access the session graph initialized in another thread.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 多线程选项可能不是最有帮助的，因为 TensorFlow 不允许线程访问在另一个线程中初始化的会话图。
- en: On the other hand, setting a number of workers greater than one would create
    several processes, with the whole Flask app initialized for each worker. Each
    one instantiates its own TensorFlow session with its own model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，设置多个工作进程会创建多个进程，每个工作进程都初始化整个 Flask 应用程序。每个工作进程都实例化其自己的 TensorFlow 会话和模型。
- en: <picture>![multi-workers-gunicorn-flask-tensorflow-api-schema](../Images/1c4723ca5844a91abd1575e67f684d17.png)</picture>
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![multi-workers-gunicorn-flask-tensorflow-api-schema](../Images/1c4723ca5844a91abd1575e67f684d17.png)</picture>
- en: 'Let''s try this approach by running the following command:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下命令来尝试这种方法：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And here are the new performances we get with a multi-workers API:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在多工作进程API下获得的新性能：
- en: <picture>![multi-workers-gunicorn-flask-tensorflow-api-response-time](../Images/69dfec94e4ccee61dcc3858dfb78e933.png)</picture>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![多工作进程-gunicorn-flask-tensorflow-api-响应时间](../Images/69dfec94e4ccee61dcc3858dfb78e933.png)</picture>
- en: 'You can see that the effect on response time when using 5 workers instead of
    1 is drastic: the 4.1 s average for 20 users has almost been divided by 2\. By
    2 and not by 5 as you might expect, because of the wait time between requests
    — it would have been divided by 5 if the simulated users were immediately requesting
    the API again after having received a response.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，使用5个工作进程而不是1个工作进程对响应时间的影响是显著的：20个用户的4.1秒平均时间几乎减半了。是减半而不是减为5分之一，因为请求之间的等待时间——如果模拟用户在收到响应后立即再次请求API，它将被减少到5分之一。
- en: 'You may wonder why I stopped to 5 workers: why not to set-up 20 workers, to
    be able to handle all the 20 users requesting the API at the exact same time?
    The reason is that each worker is an independent process that instantiates its
    own version of the model in the GPU memory.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想为什么我停在了5个工作进程：为什么不设置20个工作进程，以便能够处理所有20个用户同时请求API的情况？原因是每个工作进程都是一个独立的进程，在GPU内存中实例化自己的模型版本。
- en: 20 workers would thus consume 20 times the size of the model just for initializing
    the weights, and more memory is then needed for inference computing. With a 2GiB
    model and an 8GiB GPU, we are a bit limited.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 20个工作进程将消耗20倍的模型大小仅用于初始化权重，推理计算还需要更多内存。对于一个2GiB的模型和一个8GiB的GPU，我们有些受限。
- en: '![Figure](../Images/781550b5a91b86e5e76a34b60c897835.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/781550b5a91b86e5e76a34b60c897835.png)'
- en: Each of the 2 workers consumes 2349MiB of GPU memory
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 每个工作进程消耗2349MiB的GPU内存
- en: It's possible to maximize the number of running workers, by fine-tuning the
    memory allocated to each of them with a TensorFlow parameter (per_process_gpu_memory_fraction
    in TensorFlow 1.x, and memory_limit in TensorFlow 2.x — I've only tested the first
    one), setting it to the minimal value which does not make predictions fail with
    an out of memory error.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过调整每个工作进程的内存分配来最大化运行的工作进程数量，使用TensorFlow参数（TensorFlow 1.x中的`per_process_gpu_memory_fraction`，TensorFlow
    2.x中的`memory_limit`——我只测试了第一个），将其设置为不会因内存不足错误而导致预测失败的最小值。
- en: As warned by the library, reducing the available memory could theoretically
    increase inference time by disabling optimization. However, I did not personally
    notice any significant change.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 根据库的警告，减少可用内存理论上可能会通过禁用优化来增加推理时间。然而，我个人并没有注意到任何显著的变化。
- en: Anyway, this will not allow us to run 20 workers, even using our 2 GPUs. So,
    let us buy 2 more? Don't panic, you have more than one string to your bow.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，这不会让我们在使用2个 GPU的情况下运行20个工作进程。那么，我们要再买2个吗？别慌，你还有更多的办法。
- en: Haste makes waste
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 欲速则不达
- en: 'Let us come back to the mono-worker case for now. If you think about how things
    happen when two or more requests arrive almost at the same time to this API, there
    is something unoptimized:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回到单工作进程的情况。如果你考虑当两个或更多请求几乎同时到达这个 API 时的情况，会发现有些地方未得到优化：
- en: '![Figure](../Images/06d468e84c3a8937dd518aa81510fca8.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/06d468e84c3a8937dd518aa81510fca8.png)'
- en: 3 requests arriving with 100 ms between each of them, the average response time
    is 1433 ms
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 3个请求间隔100毫秒到达时，平均响应时间为1433毫秒
- en: The API passes the 3 inputs through the model one by one. However, most Deep
    Learning models, thanks to the nature of the mathematical operations they are
    made of, can process several inputs at the same time without increasing inference
    time — that's what we call batch processing, and that's what we usually do during
    the training phase.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: API 按顺序将3个输入通过模型。然而，由于深度学习模型的数学操作性质，它们可以同时处理多个输入而不会增加推理时间——这就是我们所说的批量处理，也是我们在训练阶段通常做的事。
- en: So, why not processing these 3 inputs in a batch? This would imply that after
    receiving the first request, the API waits a bit before running the model, in
    order to receive the 2 next requests before processing them all in one. This would
    increase the response time for the first request but decrease it on average.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么不将这3个输入批量处理呢？这意味着在接收到第一个请求后，API 会稍等一下再运行模型，以便在处理所有请求之前先接收接下来的两个请求。这会增加第一个请求的响应时间，但平均响应时间会减少。
- en: '![Figure](../Images/c266286ba19aad8c0e6421495b0655e0.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/c266286ba19aad8c0e6421495b0655e0.png)'
- en: If the API waits 300 ms before processing requests by batch, we get an average
    response time of 600 ms
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 API 在批量处理请求之前等待 300 毫秒，我们会得到一个 600 毫秒的平均响应时间。
- en: 'In practice, we do not know when future requests will arrive, and it''s a bit
    complex to decide how long to wait for potential next requests. A quite logical
    rule to trigger processing of queued requests is to do it:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们不知道未来的请求何时会到达，因此决定等待潜在下一个请求的时间有些复杂。触发处理排队请求的一个合理规则是：
- en: if the number of queued requests has reached a maximum value, corresponding
    to the GPU memory limit or the maximum number of users (if known)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果排队请求的数量达到了一个最大值，对应于 GPU 内存限制或最大用户数（如果已知）
- en: OR
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: if the oldest request in the queue is older than a timeout value. This value
    has to be fine-tuned empirically to minimize the average response time.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果队列中最旧的请求比超时值更久。这一值需要通过经验来微调，以最小化平均响应时间。
- en: Here is a way to implement such behavior, still with a Gunicorn - Flask - TensorFlow
    stack, using a Python queue and a thread dedicated to handling requests by batch.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一种实现这种行为的方法，仍然使用 Gunicorn - Flask - TensorFlow 堆栈，使用 Python 队列和专门处理批量请求的线程。
- en: 'This new API has to be run with as many threads as the maximum number of requests
    we want to be able to process in batch:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新的 API 必须运行与我们希望能够批量处理的请求最大数量相等的线程：
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here are the very satisfying results I''ve got after a very quick fine-tuning
    of the timeout value:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我在快速微调超时值后获得的非常令人满意的结果：
- en: <picture>![batching-gunicorn-flask-tensorflow-api-response-time](../Images/5141b3adf790bc580e4c443de1d5c6ef.png)</picture>
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![batching-gunicorn-flask-tensorflow-api-response-time](../Images/5141b3adf790bc580e4c443de1d5c6ef.png)</picture>
- en: As you can see, when simulating only 1 user, this new API set-up with a timeout
    of 500 ms increases response time by 500 ms (the timeout value) and is of course
    useless. But with 20 users, the average response time has been divided by 6.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，当仅模拟 1 个用户时，这个新的 API 设置具有 500 毫秒的超时会将响应时间增加 500 毫秒（超时值），显然没有用。但对于 20
    个用户，平均响应时间已被缩短了 6 倍。
- en: Note that in order to get still better results, this batching technique can
    be used in parallel with the multi-workers one presented above.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了获得更好的结果，这种批处理技术可以与上述多工作者技术并行使用。
- en: Didn't we reinvent the wheel?
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们不是在重新发明轮子吗？
- en: As developers, it's a kind of guilty pleasure to code from scratch something
    that should very likely already be implemented by an existing tool and that's
    what we've just done all along this article.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 作为开发者，从零开始编写一个可能已经被现有工具实现的功能是一种有点罪恶的乐趣，这也是我们在整篇文章中一直在做的事情。
- en: A tool indeed exists for serving a TensorFlow model in an API. It has been developed
    by TensorFlow and it is called... [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 确实存在一个工具用于在 API 中服务 TensorFlow 模型。它由 TensorFlow 开发，叫做…… [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)。
- en: Not only does it allow to start a prediction API for the model without writing
    any other code than the model itself, but it also provides several optimizations
    that you may find interesting, like parallelism and requests batching!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 它不仅允许为模型启动一个预测 API，而不需要编写除模型本身之外的任何其他代码，还提供了你可能会感兴趣的几种优化，比如并行处理和请求批处理！
- en: 'To serve a model with TF Serving, the first thing you need is to export it
    to your disk (in the TensorFlow format, not the Keras one), which can be done
    with the following piece of code:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要用 TF Serving 服务一个模型，首先需要将其导出到磁盘（使用 TensorFlow 格式，而不是 Keras 格式），可以使用以下代码完成：
- en: Then, one of the easiest ways to start a serving API is to use the TF Serving
    [docker image](https://www.tensorflow.org/tfx/serving/docker).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，启动服务 API 的一种简单方法是使用 TF Serving [docker 镜像](https://www.tensorflow.org/tfx/serving/docker)。
- en: 'The command below will start a prediction API that you can test by requesting
    this endpoint: [http://localhost:8501/v1/models/my_model:predict](http://localhost:8501/v1/models/my_model:predict)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令将启动一个预测 API，你可以通过请求此端点来测试：[http://localhost:8501/v1/models/my_model:predict](http://localhost:8501/v1/models/my_model:predict)
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that it does not use the GPU by default. You can find in [this documentation](https://www.tensorflow.org/tfx/serving/docker#serving_with_docker_using_your_gpu)
    how to make it work.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，它默认不使用 GPU。你可以在 [此文档](https://www.tensorflow.org/tfx/serving/docker#serving_with_docker_using_your_gpu)
    中找到如何使其工作的信息。
- en: 'What about batching? You will have to write such a batching config file:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 那批处理呢？你需要编写这样的批处理配置文件：
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'It includes, in particular, the 2 parameters used in our custom implementation:
    max_batch_size and batch_timeout_micros parameter. The latter is what we called
    the timeout and has to be given in microseconds.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 它包括，特别是我们自定义实现中使用的2个参数：max_batch_size和batch_timeout_micros参数。后者是我们所说的超时，必须以微秒为单位。
- en: 'Finally, the command to run TF Serving with batching enabled is:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，启用批处理的TF Serving命令是：
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'I tested it and thanks to some black magic, I got even better results than
    with my custom batching API:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我进行了测试，感谢一些黑魔法，我得到的结果甚至比我的自定义批处理API更好：
- en: <picture>![batching-tensorflow-serving-api-response-time](../Images/01ab4df8dea678bba57bcbd7fde2432f.png)</picture>
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![batching-tensorflow-serving-api-response-time](../Images/01ab4df8dea678bba57bcbd7fde2432f.png)</picture>
- en: I hope that you will be able to use the different tips presented in this article
    to boost your own Machine Learning APIs. There are many tracks I did not explore,
    especially the optimizations on the model itself (rather than the API) like [Model
    Pruning](https://towardsdatascience.com/pruning-deep-neural-network-56cae1ec5505)
    or [Post-training Quantization](https://www.tensorflow.org/model_optimization/guide/quantization).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你能利用本文中介绍的各种技巧来提升你自己的机器学习API。有很多我没有探索的方面，特别是模型本身的优化（而非API），如[模型剪枝](https://towardsdatascience.com/pruning-deep-neural-network-56cae1ec5505)或[训练后量化](https://www.tensorflow.org/model_optimization/guide/quantization)。
- en: '**Bio: [Yannick Wolf](https://www.linkedin.com/in/yannick-wolff-457616124/)**
    is Lead Data Scientist at Sicara.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介: [Yannick Wolf](https://www.linkedin.com/in/yannick-wolff-457616124/)**
    是Sicara的首席数据科学家。'
- en: '[Original](https://www.sicara.ai/blog/optimize-response-time-api). Reposted
    with permission.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://www.sicara.ai/blog/optimize-response-time-api)。经许可转载。'
- en: '**Related:**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Software Interfaces for Machine Learning Deployment](/2020/03/software-interfaces-machine-learning-deployment.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习部署的软件接口](/2020/03/software-interfaces-machine-learning-deployment.html)'
- en: '[TensorFlow 2.0 Tutorial: Optimizing Training Time Performance](/2020/03/tensorflow-optimizing-training-time-performance.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow 2.0教程：优化训练时间性能](/2020/03/tensorflow-optimizing-training-time-performance.html)'
- en: '[How to Do Hyperparameter Tuning on Any Python Script in 3 Easy Steps](/2020/04/hyperparameter-tuning-python.html)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在3个简单步骤中对任何Python脚本进行超参数调优](/2020/04/hyperparameter-tuning-python.html)'
- en: More On This Topic
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Top 10 MLOps Tools to Optimize & Manage Machine Learning Lifecycle](https://www.kdnuggets.com/2022/10/top-10-mlops-tools-optimize-manage-machine-learning-lifecycle.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化和管理机器学习生命周期的十大MLOps工具](https://www.kdnuggets.com/2022/10/top-10-mlops-tools-optimize-manage-machine-learning-lifecycle.html)'
- en: '[Deploying Your Machine Learning Model to Production in the Cloud](https://www.kdnuggets.com/deploying-your-ml-model-to-production-in-the-cloud)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[将机器学习模型部署到云中的生产环境]((https://www.kdnuggets.com/deploying-your-ml-model-to-production-in-the-cloud)'
- en: '[How to Optimize SQL Queries for Faster Data Retrieval](https://www.kdnuggets.com/2023/06/optimize-sql-queries-faster-data-retrieval.html)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何优化SQL查询以加快数据检索速度](https://www.kdnuggets.com/2023/06/optimize-sql-queries-faster-data-retrieval.html)'
- en: '[How To Optimize Dockerfile Instructions for Faster Build Times](https://www.kdnuggets.com/how-to-optimize-dockerfile-instructions-for-faster-build-times)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何优化Dockerfile指令以加快构建时间](https://www.kdnuggets.com/how-to-optimize-dockerfile-instructions-for-faster-build-times)'
- en: '[A Full End-to-End Deployment of a Machine Learning Algorithm into a…](https://www.kdnuggets.com/2021/12/deployment-machine-learning-algorithm-live-production-environment.html)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习算法的完整端到端部署…](https://www.kdnuggets.com/2021/12/deployment-machine-learning-algorithm-live-production-environment.html)'
- en: '[Operationalizing Machine Learning from PoC to Production](https://www.kdnuggets.com/2022/05/operationalizing-machine-learning-poc-production.html)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[将机器学习从概念验证到生产环境的运作化](https://www.kdnuggets.com/2022/05/operationalizing-machine-learning-poc-production.html)'
