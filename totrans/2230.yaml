- en: 'Understanding Supervised Learning: Theory and Overview'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/understanding-supervised-learning-theory-and-overview](https://www.kdnuggets.com/understanding-supervised-learning-theory-and-overview)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Understanding Supervised Learning: Theory and Overview](../Images/e8dc88ea3399cedf36133942f5d82b33.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Supervised is a subcategory of machine learning in which the computer learns
    from the labeled dataset containing both the input as well as the correct output.
    It tries to find the mapping function that relates the input (x) to the output
    (y). You can think of it as teaching your younger brother or sister how to recognize
    different animals. You will show them some pictures (x) and tell them what each
    animal is called (y). After a certain time, they will learn the differences and
    will be able to recognize the new picture correctly. This is the basic intuition
    behind supervised learning. Before moving forward, let's take a deeper look at
    its workings.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: How Does Supervised Learning Work?
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Supervised Learning: Theory and Overview](../Images/2a4574ae6e1142a8697461a291df47ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that you want to build a model that can differentiate between apples
    and oranges based on some characteristics. We can break down the process into
    the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Collection:**  Gather a dataset with pictures of apples and oranges,
    and each image is labeled as either "apple" or "orange."'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Selection**: We have to pick the right classifier here often known
    as the right supervised machine learning algorithm for your task. It is just like
    picking the right glasses that will help you see better'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training the Model:** Now, you feed the algorithm with the labeled images
    of apples and oranges. The algorithm looks at these pictures and learns to recognize
    the differences, such as the color, shape, and size of apples and oranges.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluating & Testing:** To check if your model is working correctly, we will
    feed some unseen pictures to it and compare the predictions with the actual one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of Supervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Supervised learning can be divided into two main types:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In classification tasks, the primary objective is to assign data points to specific
    categories from a set of discrete classes. When there are only two possible outcomes,
    such as "yes" or "no," "spam" or "not spam," "accepted" or "rejected," it is referred
    to as binary classification. However, when there are more than two categories
    or classes involved, like grading students based on their marks (e.g., A, B, C,
    D, F), it becomes an example of a multi-classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For regression problems, you are trying to predict a continuous numerical value.
    For example, you might be interested in predicting your final exam scores based
    on your past performance in the class. The predicted scores can span any value
    within a specific range, typically from 0 to 100 in our case.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of Popular Supervised Learning Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we have a basic understanding of the overall process. We will explore
    the popular supervised machine learning algorithms, their usage, and how they
    work:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As the name suggests, it is used for regression tasks like predicting stock
    prices, forecasting the temperature, estimating the likelihood of disease progression,
    etc. We try to predict the target (dependent variable) using the set of labels
    (independent variables). It assumes that we have a linear relationship between
    our input features and the label. The central idea revolves around predicting
    the best-fit line for our data points by minimizing the error between our actual
    and predicted values. This line is represented by the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/5fafd0dd11eff7815530e8bc855d776c.png "Y=b_{0}+b_{1}X")'
  prefs: []
  type: TYPE_IMG
- en: Where,
  prefs: []
  type: TYPE_NORMAL
- en: '**Y**   Predicted output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**X** =  Input feature or feature matrix in multiple linear regression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**b0** = Intercept (where the line crosses the Y-axis).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**b1** =  Slope or coefficient that determines the line''s steepness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It estimates the slope of the line (weight) and its intercept(bias). This line
    can be used further to make predictions. Although it is the simplest and useful
    model for developing the baselines it is highly sensitive to outliers that may
    influence the position of the line.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Supervised Learning: Theory and Overview](../Images/0816f56ee0c1a92a0784679dac4b2d97.png)'
  prefs: []
  type: TYPE_IMG
- en: Gif on [Primo.ai](https://cdn-images-1.medium.com/max/640/1*eeIvlwkMNG1wSmj3FR6M2g.gif)
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Logistic Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although it has regression in its name, but is fundamentally used for binary
    classification problems. It predicts the probability of a positive outcome (dependent
    variable) which lies in the range of 0 to 1\. By setting a threshold (usually
    0.5), we classify data points: those with a probability greater than the threshold
    belongs to the positive class, and vice versa. Logistic regression calculates
    this probability using the sigmoid function applied to the linear combination
    of the input features which is specified as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/d72c23a2ab99505557f2743b27dd08dd.png "P(Y=1)=\frac{1}{1+e^{-(b_0+b_1X_1+b_2X_2+\ldots+b_nX_n)}}")'
  prefs: []
  type: TYPE_IMG
- en: Where,
  prefs: []
  type: TYPE_NORMAL
- en: P(Y=1) = Probability of the data point belonging to the positive class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: X1 ,... ,Xn = Input Features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: b0,....,bn = Input weights that the algorithm learns during training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This sigmoid function is in the form of S like curve that transforms any data
    point to a probability score within the range of 0-1\. You can see the below graph
    for a better understanding.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Supervised Learning: Theory and Overview](../Images/320a811842b87f33e0d8398ec5313ae5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image on [Wikipedia](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png)
  prefs: []
  type: TYPE_NORMAL
- en: A closer value to 1 indicates a higher confidence in the model in its prediction.
    Just like linear regression, it is known for its simplicity but we cannot perform
    the multi-class classification without modification to the original algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Decision Trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike the above two algorithms, decision trees can be used for both classification
    and regression tasks. It has a hierarchical structure just like the flowcharts.
    At each node, a decision about the path is made based on some feature values.
    The process continues unless we reach the last node that depicts the final decision.
    Here is some basic terminology that you must be aware of:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Root Node:** The top node containing the entire dataset is called the root
    node. We then select the best feature using some algorithm to split the dataset
    into 2 or more sub-trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Internal Nodes:** Each Internal node represents a specific feature and a
    decision rule to decide the next possible direction for a data point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leaf Nodes:** The ending nodes that represent a class label are referred
    to as leaf nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It predicts the continuous numerical values for the regression tasks. As the
    size of the dataset grows, it captures the noise leading to overfitting. This
    can be handled by pruning the decision tree. We remove branches that don't significantly
    improve the accuracy of our decisions. This helps keep our tree focused on the
    most important factors and prevents it from getting lost in the details.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Supervised Learning: Theory and Overview](../Images/349f500f34b03161b6de7c918b240f37.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Jake Hoare](https://cdn-dfnaj.nitrocdn.com/xxeFXDnBIOflfPsgwjDLywIQwPChAOzV/assets/images/optimized/rev-4a6533c/www.displayr.com/wp-content/uploads/2018/07/what-is-a-decision-tree.png)
    on Displayr
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Random Forest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Random forest can also be used for both the classification and the regression
    tasks. It is a group of decision trees working together to make the final prediction.
    You can think of it as the committee of experts making a collective decision.
    Here is how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Sampling:** Instead of taking the entire dataset at once, it takes the
    random samples via a process called bootstrapping or bagging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature Selection:** For each decision tree in a random forest, only the
    random subset of features is considered for the decision-making instead of the
    complete feature set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Voting:** For classification, each decision tree in the random forest casts
    its vote and the class with the highest votes is selected. For regression, we
    average the values obtained from all trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although it reduces the effect of overfitting caused by individual decision
    trees, but is computationally expensive. One word that you will read frequently
    in the literature is that the random forest is an ensemble learning method, which
    means it combines multiple models to improve overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Support Vector Machines (SVM)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is primarily used for classification problems but can handle regression
    tasks as well. It tries to find the best hyperplane that separates the distinct
    classes using the statistical approach, unlike the probabilistic approach of logistic
    regression. We can use the linear SVM for the linearly separable data. However,
    most of the real-world data is non-linear and we use the kernel tricks to separate
    the classes. Let''s dive deep into how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hyperplane Selection:** In binary classification, SVM finds the best hyperplane
    (2-D line) to separate the classes while maximizing the margin. Margin is the
    distance between the hyperplane and the closest data points to the hyperplane.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kernel Trick:** For linearly inseparable data, we employ a kernel trick that
    maps the original data space into a high-dimensional space where they can be separated
    linearly. Common kernels include linear, polynomial, radial basis function (RBF),
    and sigmoid kernels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Margin Maximization:** SVM also tries to improve the generalization of the
    model by increasing the maximizing margin.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification:** Once the model is trained, the predictions can be made
    based on their position relative to the hyperplane.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVM also has a parameter called C that controls the trade-off between maximizing
    the margin and keeping the classification error to a minimum. Although they can
    handle high-dimensional and non-linear data well, choosing the right kernel and
    hyperparameter is not as easy as it seems.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Supervised Learning: Theory and Overview](../Images/de4cdeab60c3720657daf47783c423d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image on [Javatpoint](https://static.javatpoint.com/tutorial/machine-learning/images/support-vector-machine-algorithm.png)
  prefs: []
  type: TYPE_NORMAL
- en: 6\. k-Nearest Neighbors (k-NN)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: K-NN is the simplest supervised learning algorithm mostly used for classification
    tasks. It doesn’t make any assumptions about the data and assigns the new data
    point a category based on its similarity with the existing ones. During the training
    phase, it keeps the entire dataset as a reference point. It then calculates the
    distance between the new data point and all the existing points using a distance
    metric (Eucilinedain distance e.g.). Based on these distances, it identifies the
    K nearest neighbors to these data points. We then count the occurrence of each
    class in the K nearest neighbors and assign the most frequently appearing class
    as the final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Supervised Learning: Theory and Overview](../Images/f378070bfb874decbeca262401c1d6ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Image on [GeeksforGeeks](https://media.geeksforgeeks.org/wp-content/uploads/20200616145419/Untitled2781.png)
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right value of K requires experimentation. Although it is robust
    to noisy data it is not suitable for high dimensional datasets and has a high
    cost associated due to the calculation of the distance from all data points.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As I conclude this article, I would encourage the readers to explore more algorithms
    and try to implement them from scratch. This will strengthen your understanding
    of how things are working under the hood. Here are some additional resources to
    help you get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Mastering Machine Learning Algorithms - Second Edition](https://www.oreilly.com/library/view/mastering-machine-learning/9781838820299/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine Learning Course - [Javatpoint](https://www.javatpoint.com/machine-learning)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine Learning Specialization - [Coursera](https://www.coursera.org/specializations/machine-learning-introduction)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[Kanwal Mehreen](https://www.linkedin.com/in/kanwal-mehreen1)** is an aspiring
    software developer with a keen interest in data science and applications of AI
    in medicine. Kanwal was selected as the Google Generation Scholar 2022 for the
    APAC region. Kanwal loves to share technical knowledge by writing articles on
    trending topics, and is passionate about improving the representation of women
    in tech industry.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Machine Learning Evaluation Metrics: Theory and Overview](https://www.kdnuggets.com/machine-learning-evaluation-metrics-theory-and-overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Statistics in Data Science: Theory and Overview](https://www.kdnuggets.com/statistics-in-data-science-theory-and-overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Primary Supervised Learning Algorithms Used in Machine Learning](https://www.kdnuggets.com/2022/06/primary-supervised-learning-algorithms-used-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, June 22: Primary Supervised Learning Algorithms…](https://www.kdnuggets.com/2022/n25.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding Machine Learning Algorithms: An In-Depth Overview](https://www.kdnuggets.com/understanding-machine-learning-algorithms-an-indepth-overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hands-On with Supervised Learning: Linear Regression](https://www.kdnuggets.com/handson-with-supervised-learning-linear-regression)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
