- en: The 8 Neural Network Architectures Machine Learning Researchers Need to Learn
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习研究人员需要了解的8种神经网络架构
- en: 原文：[https://www.kdnuggets.com/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html](https://www.kdnuggets.com/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html](https://www.kdnuggets.com/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html)
- en: '[comments](/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html?page=2#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html?page=2#comments)'
- en: '![Header image](../Images/ce396eda700c6c69f296bdb37596e40f.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![头图](../Images/ce396eda700c6c69f296bdb37596e40f.png)'
- en: '**Why do we need Machine Learning?**'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们为什么需要机器学习？**'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Machine learning is needed for tasks that are too complex for humans to code
    directly. Some tasks are so complex that it is impractical, if not impossible,
    for humans to work out all of the nuances and code for them explicitly. So instead,
    we provide a large amount of data to a machine learning algorithm and let the
    algorithm work it out by exploring that data and searching for a model that will
    achieve what the programmers have set it out to achieve.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习适用于那些人类无法直接编码的任务。有些任务复杂到人类无法全面解决所有细节并明确编码。因此，我们将大量数据提供给机器学习算法，让算法通过探索数据和寻找能够实现程序员设定目标的模型来解决问题。
- en: 'Let’s look at these 2 examples:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看这两个例子：
- en: It is very hard to write programs that solve problems like recognizing a 3-dimensional
    object from a novel viewpoint in new lighting conditions in a cluttered scene.
    We don’t know what program to write because we don’t know how it’s done in our
    brain. Even if we had a good idea about how to do it, the program might be horrendously
    complicated.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写解决诸如在混乱场景中的新光照条件下从新视角识别三维物体这样的问题的程序是非常困难的。我们不知道该写什么程序，因为我们不知道大脑是如何处理的。即使我们对如何做到这一点有一个好的想法，程序也可能复杂得令人恐惧。
- en: It is hard to write a program to compute the probability that a credit card
    transaction is fraudulent. There may not be any rules that are both simple and
    reliable. We need to combine a very large number of weak rules. Fraud is a moving
    target but the program needs to keep changing.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写程序来计算信用卡交易是否欺诈的概率是很困难的。可能没有既简单又可靠的规则。我们需要结合大量的弱规则。欺诈是一个不断变化的目标，但程序需要不断改变。
- en: 'Then comes the **Machine Learning Approach**: Instead of writing a program
    by hand for each specific task, we collect lots of examples that specify the correct
    output for a given input. A machine learning algorithm then takes these examples
    and produces a program that does the job. The program produced by the learning
    algorithm may look very different from a typical hand-written program. It may
    contain millions of numbers. If we do it right, the program works for new cases
    as well as the ones we trained it on. If the data changes the program can change
    too by training on the new data. You should note that massive amounts of computation
    are now cheaper than paying someone to write a task-specific program.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是**机器学习方法**：我们不是为每个特定任务手动编写程序，而是收集大量示例，指定给定输入的正确输出。机器学习算法然后利用这些示例生成一个能完成任务的程序。学习算法生成的程序可能与典型的手写程序大相径庭，可能包含数百万个数字。如果我们做对了，程序不仅在我们训练的情况下有效，也能在新案例中有效。如果数据发生变化，程序也可以通过在新数据上进行训练而改变。你应该注意到，大量计算现在比支付人们编写任务特定程序更便宜。
- en: 'Given that, some examples of tasks best solved by machine learning include:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，一些最适合由机器学习解决的任务包括：
- en: 'Recognizing patterns: Objects in real scenes, Facial identities or facial expressions,
    Spoken words'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模式识别：真实场景中的物体，面部身份或面部表情，语音
- en: 'Recognizing anomalies: Unusual sequences of credit card transactions, Unusual
    patterns of sensor readings in a nuclear power plant'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常检测：不寻常的信用卡交易序列，核电站传感器读数的异常模式
- en: 'Prediction: Future stock prices or currency exchange rates, Which movies will
    a person like'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测：未来的股票价格或货币汇率，某个人可能喜欢哪些电影
- en: '**What are Neural Networks?**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是神经网络？**'
- en: Neural networks are a class of models within the general machine learning literature.
    So for example, if you took a Coursera course on machine learning, neural networks
    will likely be covered. Neural networks are a specific set of algorithms that
    has revolutionized the field of machine learning. They are inspired by biological
    neural networks and the current so called deep neural networks have proven to
    work quite very well. Neural Networks are themselves general function approximations,
    that is why they can be applied to literally almost any machine learning problem
    where the problem is about learning a complex mapping from the input to the output
    space.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是广义机器学习文献中的一类模型。例如，如果你参加了Coursera上的机器学习课程，神经网络很可能会被涵盖。神经网络是一组已彻底改变机器学习领域的特定算法。它们受到生物神经网络的启发，目前所谓的深度神经网络已经证明效果非常好。神经网络本身是一般的函数近似，因此它们几乎可以应用于任何机器学习问题，只要问题涉及从输入到输出空间的复杂映射学习。
- en: 'Here are the 3 reasons to convince you to study neural computation:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有3个理由说服你学习神经计算：
- en: 'To understand how the brain actually works: It’s very big and very complicated
    and made of stuff that dies when you poke it around. So we need to use computer
    simulations.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解大脑实际如何运作：它非常庞大且复杂，由一些在你碰触时会死亡的物质构成。因此，我们需要使用计算机模拟。
- en: 'To understand a style of parallel computation inspired by neurons and their
    adaptive connections: It’s a very different style from a sequential computation.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解一种受神经元及其自适应连接启发的并行计算风格：这是一种与顺序计算截然不同的风格。
- en: 'To solve practical problems by using novel learning algorithms inspired by
    the brain: Learning algorithms can be very useful even if they are not how the
    brain actually works.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用受大脑启发的新颖学习算法来解决实际问题：即使这些学习算法的工作方式与大脑实际运作不完全相同，它们也可以非常有用。
- en: After finishing the famous [Andrew Ng’s Machine Learning Coursera course](https://github.com/khanhnamle1994/machine-learning),
    I started developing interest towards neural networks and deep learning. Thus,
    I started looking at the best online resources to learn about the topics and found [Geoffrey
    Hinton’s Neural Networks for Machine Learning course](https://github.com/khanhnamle1994/neural-nets).
    If you are a deep learning practitioner or someone who want to get into the deep
    learning/machine learning world, you should really take this course. Geoffrey
    Hinton is without a doubt a godfather of the deep learning world. And he actually
    provided something extraordinary in this course. In this blog post, I want to
    share the **8 neural network architectures** from the course that I believe any
    machine learning researchers should be familiar with to advance their work.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成了著名的[Andrew Ng的机器学习Coursera课程](https://github.com/khanhnamle1994/machine-learning)之后，我开始对神经网络和深度学习产生了兴趣。因此，我开始寻找最好的在线资源来学习这些主题，并发现了[Geoffrey
    Hinton的机器学习神经网络课程](https://github.com/khanhnamle1994/neural-nets)。如果你是深度学习从业者或希望进入深度学习/机器学习领域的人，你真的应该参加这门课程。Geoffrey
    Hinton无疑是深度学习领域的奠基人。他在这门课程中确实提供了非凡的内容。在这篇博客文章中，我想分享来自这门课程的**8种神经网络架构**，我认为任何机器学习研究人员都应该熟悉这些架构，以推动他们的工作。
- en: '![](../Images/39c93b981f680576756eed859830f9e8.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39c93b981f680576756eed859830f9e8.png)'
- en: 'Generally, these architectures can be put into 3 specific categories:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些架构可以分为3个特定类别：
- en: '**1 — Feed-Forward Neural Networks**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**1 — 前馈神经网络**'
- en: These are the commonest type of neural network in practical applications. The
    first layer is the input and the last layer is the output. If there is more than
    one hidden layer, we call them “deep” neural networks. They compute a series of
    transformations that change the similarities between cases. The activities of
    the neurons in each layer are a non-linear function of the activities in the layer
    below.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是实际应用中最常见的神经网络类型。第一层是输入层，最后一层是输出层。如果有多个隐藏层，我们称之为“深度”神经网络。它们计算一系列变换，以改变案例之间的相似性。每一层神经元的活动是下面一层活动的非线性函数。
- en: '**2 — Recurrent Networks**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**2 — 递归网络**'
- en: These have directed cycles in their connection graph. That means you can sometimes
    get back to where you started by following the arrows. They can have complicated
    dynamics and this can make them very difficult to train. They are more biologically
    realistic.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这些在其连接图中具有有向循环。这意味着你有时可以通过沿着箭头回到起点。它们可能具有复杂的动态，这使得训练它们非常困难。它们在生物学上更为真实。
- en: There is a lot of interest at present in finding efficient ways of training
    recurrent nets. Recurrent neural networks are a very natural way to model sequential
    data. They are equivalent to very deep nets with one hidden layer per time slice;
    except that they use the same weights at every time slice and they get input at
    every time slice. They have the ability to remember information in their hidden
    state for a long time but is very hard to train them to use this potential.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当前对寻找有效训练递归网络的方法非常感兴趣。递归神经网络是一种非常自然的建模序列数据的方式。它们等同于每个时间片有一个隐藏层的非常深的网络；只是它们在每个时间片使用相同的权重，并且在每个时间片获取输入。它们能够在隐藏状态中记住信息很长时间，但训练它们利用这种潜力非常困难。
- en: '**3 — Symmetrically Connected Networks**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**3 — 对称连接网络**'
- en: These are like recurrent networks, but the connections between units are symmetrical
    (they have the same weight in both directions). Symmetric networks are much easier
    to analyze than recurrent networks. They are also more restricted in what they
    can do because they obey an energy function. Symmetrically connected nets without
    hidden units are called “Hopfield Nets.” Symmetrically connected network with
    hidden units are called “Boltzmann machines.”
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类似于递归网络，但单元之间的连接是对称的（它们在两个方向上具有相同的权重）。对称网络比递归网络更容易分析。由于它们遵循一个能量函数，因此它们在功能上也更加受限。没有隐藏单元的对称连接网络称为“霍普菲尔德网络”。具有隐藏单元的对称连接网络称为“玻尔兹曼机”。
- en: 1 — Perceptrons
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1 — 感知器
- en: Considered the first generation of neural networks, **perceptrons** are simply
    computational models of a single neuron. They were popularized by [Frank Rosenblatt](https://blogs.umass.edu/comphon/2017/06/15/did-frank-rosenblatt-invent-deep-learning-in-1962/) in
    the early 1960s. They appeared to have a very powerful learning algorithm and
    lots of grand claims were made for what they could learn to do. In 1969, Minsky
    and Papers published a book called [“Perceptrons”](https://mitpress.mit.edu/books/perceptrons) that
    analyzed what they could do and showed their limitations. Many people thought
    these limitations applied to all neural network models. However, the perceptron
    learning procedure is still widely used today for tasks with enormous feature
    vectors that contain many millions of features.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 被认为是第一代神经网络的**感知器**，仅仅是单个神经元的计算模型。它们在1960年代初由[弗兰克·罗森布拉特](https://blogs.umass.edu/comphon/2017/06/15/did-frank-rosenblatt-invent-deep-learning-in-1962/)推广。它们似乎具有非常强大的学习算法，并且对它们能够学习做什么做出了许多宏伟的声明。1969年，明斯基和帕普斯出版了一本名为[《感知器》](https://mitpress.mit.edu/books/perceptrons)的书，分析了它们的功能并展示了它们的局限性。许多人认为这些局限性适用于所有神经网络模型。然而，感知器学习程序今天仍广泛用于具有包含数百万特征的大型特征向量的任务。
- en: '![](../Images/4d94d59e6e4b8eae6bd7be64cc25f945.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d94d59e6e4b8eae6bd7be64cc25f945.png)'
- en: In the standard paradigm for statistical pattern recognition, we first convert
    the raw input vector into a vector of feature activations. We then use hand-written
    programs based on common-sense to define the features. Next, we learn how to weight
    each of the feature activations to get a single scalar quantity. If this quantity
    is above some threshold, we decide that the input vector is a positive example
    of the target class.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计模式识别的标准范式中，我们首先将原始输入向量转换为特征激活向量。然后，我们使用基于常识的手写程序来定义特征。接下来，我们学习如何加权每个特征激活，以得到一个标量量。如果这个量高于某个阈值，我们决定输入向量是目标类别的正例。
- en: 'The standard Perceptron architecture follows the feed-forward model, meaning
    inputs are sent into the neuron, are processed, and result in an output. In the
    diagram below, this means the network reads bottom-up: input comes in from the
    bottom and output goes out from the top.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的感知机结构遵循前馈模型，这意味着输入被送入神经元，经过处理后产生输出。在下面的图示中，这意味着网络从下往上读取：输入从底部进入，输出从顶部出去。
- en: '![](../Images/2b2a45c42c8d6cd62cb8bcdb0870fd89.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b2a45c42c8d6cd62cb8bcdb0870fd89.png)'
- en: 'However, Perceptrons do have limitations: If you are followed to choose the
    features by hand and if you use enough features, you can do almost anything. For
    binary input vectors, we can have a separate feature unit for each of the exponentially
    many binary vectors and so we can make any possible discrimination on binary input
    vectors. But once the hand-coded features have been determined, there are very
    strong limitations on what a perceptron can learn.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，感知机确实存在限制：如果允许手动选择特征，并且使用足够的特征，你几乎可以做任何事情。对于二进制输入向量，我们可以为每一个指数级的二进制向量设置一个单独的特征单元，因此我们可以在二进制输入向量上做出任何可能的区分。但是一旦手工编码的特征确定后，感知机能够学习的内容就会受到很大的限制。
- en: This result is devastating for Perceptrons because the whole point of pattern
    recognition is to recognize patterns despite transformations like translation.
    Minsky and Papert’s “Group Invariance Theorem” says that the part of a Perceptron
    that learns cannot learn to do this if the transformations form a group. To deal
    with such transformations, a Perceptron needs to use multiple feature units to
    recognize transformations of informative sub-patterns. So the tricky part of pattern
    recognition must be solved by the hand-coded feature detectors, not the learning
    procedure.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这一结果对感知机是毁灭性的，因为模式识别的核心是能够识别尽管有像平移这样的变换。明斯基和帕珀特的“群不变性定理”指出，如果变换形成一个群，感知机的学习部分无法学会应对这些变换。为了应对这些变换，感知机需要使用多个特征单元来识别信息子模式的变换。因此，模式识别中复杂的部分必须通过手工编码的特征检测器解决，而不是学习过程。
- en: Networks without hidden units are very limited in the input-output mappings
    they can learn to model. More layers of linear units do not help. It’s still linear.
    Fixed output non-linearities are not enough. Thus, we need multiple layers of
    adaptive, non-linear hidden units. But how we train such nets? We need an efficient
    way of adapting all the weights, not just the last layer. This is hard. Learning
    the weights going into hidden units is equivalent to learning features. This is
    difficult because nobody is telling us directly what the hidden units should do.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 没有隐藏单元的网络在输入输出映射的建模能力上非常有限。更多的线性单元层并没有帮助，它仍然是线性的。固定输出的非线性不够。因此，我们需要多层自适应的非线性隐藏单元。但是我们如何训练这样的网络？我们需要一种有效的方式来调整所有的权重，而不仅仅是最后一层。这很困难。学习进入隐藏单元的权重等同于学习特征。这很困难，因为没有人直接告诉我们隐藏单元应该做什么。
- en: 2 — Convolutional Neural Networks
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2 — 卷积神经网络
- en: 'Machine Learning research has focused extensively on object detection problems
    over the time. There are various things that make it hard to recognize objects:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习研究长期以来重点关注物体检测问题。各种因素使得识别物体变得困难：
- en: 'Segmentation: Real scenes are cluttered with other objects. It’s hard to tell
    which pieces go together as parts of the same object. Parts of an object can be
    hidden behind other objects.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割：真实场景中物体常常被其他物体遮挡。很难判断哪些部分属于同一个物体。物体的部分可能被其他物体遮挡。
- en: 'Lighting: The intensities of the pixels are determined as much by the lighting
    as by the objects.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 光照：像素的强度受到光照和物体本身的共同影响。
- en: 'Deformation: Objects can deform in a variety of non-affine ways. E.g., a handwritten
    too can have a large loop or just a cusp.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变形：物体可以以各种非仿射方式变形。例如，手写的“2”可能有一个大圈或只是一个尖点。
- en: 'Affordances: Object classes are often defined by how they are used. E.g., chairs
    are things designed for sitting on so they have a wide variety of physical shapes.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 功能性：物体类别通常由它们的使用方式来定义。例如，椅子是设计用来坐的，因此它们具有多种不同的物理形状。
- en: 'Viewpoint: Changes in viewpoint cause changes in images that standard learning
    methods cannot cope with. Information hops between input dimensions (i.e. pixels)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视角：视角的变化会导致图像变化，而标准学习方法无法应对这些变化。信息在输入维度（即像素）之间跳跃。
- en: Imagine a medical database in which the age of a patient sometimes hopes to
    the input dimension that normally codes for weight! To apply machine learning
    we would first want to eliminate this dimension-hopping.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想象一个医疗数据库，其中患者的年龄有时会跳到通常编码体重的输入维度！为了应用机器学习，我们首先需要消除这种维度跳跃。
- en: '![](../Images/5b43460563dfae211f0ceeee551229b6.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b43460563dfae211f0ceeee551229b6.png)'
- en: The replicated feature approach is currently the dominant approach for neural
    networks to solve object detection problem. It uses many different copies of the
    same feature detector with different positions. It could also replicate across
    scale and orientation, which is tricky and expensive. Replication greatly reduces
    the number of free parameters to be learned. It uses several different feature
    types, each with its own map of replicated detectors. It also allows each patch
    of image to be represented in several ways.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 复制特征方法目前是解决对象检测问题的主流方法。它使用许多不同位置的相同特征检测器的副本。它还可以在尺度和方向上进行复制，这既棘手又昂贵。复制大大减少了需要学习的自由参数数量。它使用几种不同的特征类型，每种特征都有其自己的复制检测器映射。它还允许每个图像补丁以多种方式进行表示。
- en: So what does replicating the feature detectors achieve?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 那么复制特征检测器能实现什么呢？
- en: 'Equivalent activities: Replicated features do not make the neural activities
    invariant to translation. The activities of are equivariant.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等效活动：复制特征并未使神经活动对平移不变。活动是等变的。
- en: 'Invariant knowledge: If a feature is useful in some locations during training,
    detectors for that feature will be available in all locations during testing.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不变知识：如果一个特征在训练期间在某些位置有用，那么在测试期间，这个特征的检测器将在所有位置都可用。
- en: 'In 1998, Yann LeCun and his collaborators developed a really good recognizer
    for handwritten digits called [LeNet](http://yann.lecun.com/exdb/lenet/). It used
    back propagation in a feedforward net with many hidden layers, many maps of replicated
    units in each layer, pooling of the outputs of nearby replicated units, a wide
    net that can cope with several characters at once even if they overlap, and a
    clever way of training a complete system, not just a recognizer. Later it is formalized
    under the name **convolutional neural networks**. Fun fact: This net was used
    for reading ~10% of the checks in North America.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 1998年，Yann LeCun及其合作者开发了一个非常好的手写数字识别器，名为[LeNet](http://yann.lecun.com/exdb/lenet/)。它在一个前馈网络中使用反向传播，具有多个隐藏层，每层有多个复制单元映射，邻近复制单元输出的池化，一个宽网络可以同时处理几个字符，即使它们重叠，以及一种聪明的方式来训练一个完整的系统，而不仅仅是一个识别器。后来它在**卷积神经网络**的名字下被正式化。有趣的是：这个网络曾用于阅读北美约10%的支票。
- en: '![](../Images/5d555581239934e2d31a32647a7003a6.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d555581239934e2d31a32647a7003a6.png)'
- en: Convolutional Neural Networks can be used for all work related to object recognition
    from hand-written digits to 3D objects. However, recognizing real objects in color
    photographs downloaded from the web is much more complicated than recognizing
    hand-written digits. There are hundred times as many classes (1000 vs 10), hundred
    times as many pixels (256 x 256 color vs 28 x 28 gray), two-dimensional images
    of three-dimensional scenes, cluttered scenes requiring segmentation, and multiple
    objects in each image. Will the same type of convolutional neural network work?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络可以用于所有与对象识别相关的工作，从手写数字到三维物体。然而，识别从网络下载的彩色照片中的真实物体比识别手写数字要复杂得多。分类的数量多了百倍（1000对比10），像素的数量多了百倍（256
    x 256彩色对比28 x 28灰度），三维场景的二维图像，要求分割的杂乱场景，以及每张图像中的多个物体。相同类型的卷积神经网络能否有效呢？
- en: Then came the [ILSVRC-2012 competition](http://www.image-net.org/challenges/LSVRC/2012/) on **ImageNet**,
    a dataset with approximately 1.2 million high-resolution training images. Test
    images will be presented with no initial annotation (no segmentation or labels)
    and algorithms will have to produce labelings specifying what objects are present
    in the images. Some of the best existing computer vision methods were tried on
    this dataset by leading computer vision groups from Oxford, INRIA, XRCE… Typically,
    computer vision systems use complicated multi-stage systems and the early stages
    are typically hand-tuned by optimizing a few parameters.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然后来了[ILSVRC-2012竞赛](http://www.image-net.org/challenges/LSVRC/2012/)，这是一个包含约120万张高分辨率训练图像的**ImageNet**数据集。测试图像将以没有初始注释（没有分割或标签）的形式呈现，算法需要生成标注以指定图像中存在的物体。一些领先的计算机视觉团队，如牛津大学、INRIA、XRCE等，在这个数据集上尝试了现有的一些最佳计算机视觉方法。通常，计算机视觉系统使用复杂的多阶段系统，早期阶段通常通过优化一些参数进行手动调整。
- en: '![](../Images/10ef5959d0e91a8e208616591820caa9.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10ef5959d0e91a8e208616591820caa9.png)'
- en: The winner of the competition, [Alex Krizhevsky (NIPS 2012)](http://www.image-net.org/challenges/LSVRC/2012/supervision.pdf),
    developed a very deep convolutional neural net of the type pioneered by Yann LeCun.
    Its architecture includes 7 hidden layers not counting some max-pooling layers.
    The early layers were convolutional, while the last 2 layers were globally connected.
    The activation functions were rectified linear units in every hidden layer. These
    train much faster and are more expressive than logistic units. In addition to
    that, it also uses competitive normalization to suppress hidden activities when
    nearby units have stronger activities. This helps with variations in intensity.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 竞赛的获胜者，[Alex Krizhevsky (NIPS 2012)](http://www.image-net.org/challenges/LSVRC/2012/supervision.pdf)，开发了一种由Yann
    LeCun首创的非常深的卷积神经网络。这种架构包括7个隐藏层，不包括一些最大池化层。早期层是卷积层，而最后2层是全球连接层。每个隐藏层的激活函数都是修正线性单元。这些激活函数比逻辑单元训练得更快且更具表达能力。此外，它还使用了竞争性归一化来抑制在附近单元活动更强时的隐藏活动。这有助于处理强度的变化。
- en: 'There are a couple of technical tricks that significantly improve generalization
    for the neural net:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些技术技巧可以显著提高神经网络的泛化能力：
- en: 'Training on random 224 x 224 patches from the 256 x 256 images to get more
    data and using left-right reflections of the images. At test time, combining the
    opinions from 10 different patches: The four 224 x 224 corner patches plus the
    central 224 x 224 patch plus the reflections of those 5 patches.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在256 x 256图像上训练随机224 x 224的图像补丁以获取更多数据，并使用图像的左右反射。在测试时，结合来自10个不同补丁的意见：四个224
    x 224的角落补丁加上中央的224 x 224补丁以及这些5个补丁的反射。
- en: Using “dropout” to regularize the weights in the globally connected layers (which
    contain most of the parameters). Dropout means that half of the hidden units in
    a layer are randomly removed for each training example. This stops hidden units
    from relying too much on other hidden units.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用“dropout”来规范全球连接层中的权重（这些层包含大部分参数）。Dropout意味着在每个训练样本中，层中的一半隐藏单元会被随机移除。这防止了隐藏单元过度依赖其他隐藏单元。
- en: '![](../Images/adcd19bf34e1806f4d14c72e8c826e70.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adcd19bf34e1806f4d14c72e8c826e70.png)'
- en: In terms of hardware requirement, Alex uses a very efficient implementation
    of convolutional nets on 2 Nvidia GTX 580 GPUs (over 1000 fast little cores).
    The GPUs are very good for matrix-matrix multiplies and also have very high bandwidth
    to memory. This allows him to train the network in a week and makes it quick to
    combine results from 10 patches at test time. We can spread a network over many
    cores if we can communicate the states fast enough. As cores get cheaper and datasets
    get bigger, big neural nets will improve faster than old-fashioned computer vision
    systems.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在硬件需求方面，Alex在2块Nvidia GTX 580 GPU（超过1000个快速小核心）上使用了非常高效的卷积网络实现。这些GPU非常适合矩阵-矩阵乘法，并且具有非常高的内存带宽。这使得他能够在一周内训练网络，并使得在测试时快速结合来自10个补丁的结果。如果我们能够足够快速地通信状态，我们可以将网络分布在许多核心上。随着核心成本的降低和数据集的增大，大型神经网络将比传统计算机视觉系统进步更快。
- en: 3 — Recurrent Neural Network
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3 — 递归神经网络
- en: '![](../Images/165765dffd0351593bc95483a6eccae1.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/165765dffd0351593bc95483a6eccae1.png)'
- en: To understand RNNs, we need to have a brief overview of sequence modeling. When
    applying machine learning to sequences, we often want to turn an input sequence
    into an output sequence that lives in a different domain; for example, turn a
    sequence of sound pressures into a sequence of word identities. When there is
    no separate target sequence, we can get a teaching signal by trying to predict
    the next term in the input sequence. The target output sequence is the input sequence
    with an advance of 1 step. This seems much more natural than trying to predict
    one pixel in an image from the other pixels, or one patch of an image from the
    rest of the image. Predicting the next term in a sequence blurs the distinction
    between supervised and unsupervised learning. It uses methods designed for supervised
    learning, but it doesn’t require a separate teaching signal.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解RNN，我们需要对序列建模有一个简要的概述。在将机器学习应用于序列时，我们通常希望将输入序列转换为一个不同领域的输出序列；例如，将一系列声音压力转换为一系列单词身份。当没有单独的目标序列时，我们可以通过尝试预测输入序列中的下一个项来获得教学信号。目标输出序列是输入序列向前移动1步。这似乎比尝试从图像中的其他像素预测一个像素，或从图像的其余部分预测一个图像块要自然得多。预测序列中的下一个项模糊了监督学习和无监督学习之间的区别。它使用了为监督学习设计的方法，但不需要单独的教学信号。
- en: '**Memoryless models** are the standard approach to this task. In particular,
    autoregressive models can predict the next term in a sequence from a fixed number
    of previous terms using “delay taps; and feed-forward neural nets are generalized
    autoregressive models that use one or more layers of non-linear hidden units.
    However, if we give our generative model some hidden state, and if we give this
    hidden state its own internal dynamics, we get a much more interesting kind of
    model: It can store information in its hidden state for a long time. If the dynamics
    are noisy and the way they generate outputs from their hidden state is noisy,
    we can never know its exact hidden state. The best we can do is to infer a probability
    distribution over the space of hidden state vectors. This inference is only tractable
    for 2 types of hidden state model.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**无记忆模型**是解决此任务的标准方法。特别是，自回归模型可以从固定数量的前项预测序列中的下一个项，使用“延迟抽头”；前馈神经网络是广义的自回归模型，使用一个或多个层的非线性隐藏单元。然而，如果我们给我们的生成模型一些隐藏状态，并且如果我们给这个隐藏状态其自身的内部动态，我们就会得到一种更有趣的模型：它可以在其隐藏状态中存储信息很长时间。如果这些动态是嘈杂的，并且它们从隐藏状态中生成输出的方式是嘈杂的，我们就无法知道其确切的隐藏状态。我们能做的最好的是推断隐藏状态向量空间的概率分布。这种推断仅对两种类型的隐藏状态模型是可行的。'
- en: '**Recurrent Neural Networks **are very powerful, because they combine 2 properties:
    1) distributed hidden state that allows them to store a lot of information about
    the past efficiently, and 2) non-linear dynamics that allow them to update their
    hidden state in complicated ways. With enough neurons and time, RNNs can compute
    anything that can be computed by your computer. So what kinds of behavior can
    RNNs exhibit? They can oscillate, they can settle to point attractors, they can
    behave chaotically. And they could potentially learn to implement lots of small
    programs that each capture a nugget of knowledge and run in parallel, interacting
    to produce very complicated effects.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**递归神经网络**非常强大，因为它们结合了两种特性：1）分布式隐藏状态，使它们能够高效地存储大量过去的信息，2）非线性动态，使它们能够以复杂的方式更新其隐藏状态。拥有足够的神经元和时间，RNN可以计算出计算机能够计算的任何内容。那么，RNN可以表现出什么样的行为？它们可以振荡，可以收敛到点吸引子，可以表现得混乱。它们可能会学习实现许多小程序，每个程序捕获一个知识点，并并行运行，相互作用以产生非常复杂的效果。'
- en: '![](../Images/e6043e3e88d5c61a2b88f6a281218d60.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6043e3e88d5c61a2b88f6a281218d60.png)'
- en: However, the computational power of RNNs makes them very hard to train. It is
    quite difficult to train a RNN because of the exploding or vanishing gradients
    problem. As we backpropagate through many layers, what happens to the magnitude
    of the gradients? If the weights are small, the gradients shrink exponentially.
    If the weights are big, the gradients grow exponentially. Typical feed-forward
    neural nets can cope with these exponential effects because they only have a few
    hidden layers. On the other hand, in a RNN trained on long sequences, the gradients
    can easily explode or vanish. Even with good initial weights, it’s very hard to
    detect that the current target output depends on an input from many time-steps
    ago, so RNNs have difficulty dealing with long-range dependencies.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，RNN 的计算能力使得训练非常困难。由于梯度爆炸或消失问题，训练 RNN 是相当困难的。当我们通过多个层进行反向传播时，梯度的大小会发生什么变化？如果权重很小，梯度会指数级缩小。如果权重很大，梯度会指数级增长。典型的前馈神经网络可以应对这些指数效应，因为它们只有少数几个隐藏层。另一方面，在对长序列进行训练的
    RNN 中，梯度容易爆炸或消失。即使有良好的初始权重，仍然很难检测到当前目标输出依赖于很多时间步之前的输入，因此 RNN 处理长程依赖关系时困难重重。
- en: 'There are essentially 4 effective ways to learn a RNN:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，有4种有效的方式来学习 RNN：
- en: '**Long Short Term Memory**: Make the RNN out of little modules that are designed
    to remember values for a long time.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长短期记忆**：将 RNN 制作成小模块，这些模块设计用于长时间记忆值。'
- en: '**Hessian Free Optimization**: Deal with the vanishing gradients problem by
    using a fancy optimizer that can detect directions with a tiny gradient but even
    smaller curvature.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hessian 自由优化**：使用一种可以检测到极小梯度但更小曲率的高级优化器来解决梯度消失问题。'
- en: '**Echo State Networks**: Initialize the input -> hidden and hidden -> hidden
    and output -> hidden connections very carefully so that the hidden state has a
    huge reservoir of weakly coupled oscillators which can be selectively driven by
    the input.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回声状态网络**：非常小心地初始化输入 -> 隐藏层和隐藏层 -> 隐藏层以及输出 -> 隐藏层的连接，以便隐藏状态具有一个巨大的弱耦合振荡器库，这些振荡器可以通过输入进行选择性驱动。'
- en: '**Good initialization with momentum**: Initialize like in Echo State Networks,
    but then learn all of the connections using momentum.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**良好的动量初始化**：像在回声状态网络中那样初始化，然后使用动量学习所有的连接。'
- en: 4 — Long/Short Term Memory Network
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4 — 长短期记忆网络
- en: '![](../Images/d6dbe07ea1caf153d300783588d959d4.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6dbe07ea1caf153d300783588d959d4.png)'
- en: '[Hochreiter & Schmidhuber (1997)](http://www.bioinf.jku.at/publications/older/2604.pdf) solved
    the problem of getting a RNN to remember things for a long time (like hundreds
    of time steps) by building what known as **long-short term memory network. **They
    designed a memory cell using logistic and linear units with multiplicative interactions.
    Information gets into the cell whenever its “write” gate is on. The information
    stays in the cell so long as its “keep” gate is on. Information can be read from
    the cell by turning on its “read” gate.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[Hochreiter & Schmidhuber (1997)](http://www.bioinf.jku.at/publications/older/2604.pdf)
    通过构建被称为**长短期记忆网络**的模型，解决了让 RNN 记住长时间信息（如数百个时间步）的问题。他们设计了一个使用逻辑单元和线性单元的记忆单元，并具有乘法交互。当“写入”门打开时，信息进入单元。只要“保持”门打开，信息就会留在单元中。通过打开“读取”门，可以从单元中读取信息。'
- en: Reading cursive handwriting is a natural task for an RNN. The input is a sequence
    of (x, y, p) coordinates of the tip of the pen, where p indicates whether the
    pen is up or down. The output is a sequence of characters. [Graves & Schmidhuber
    (2009)](http://people.idsia.ch/~juergen/nips2009.pdf) showed that RNNs with LSTM
    are currently the best systems for reading cursive writing. In brief, they used
    a sequence of small images as input rather than pen coordinates.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读草书是一项自然的 RNN 任务。输入是一系列（x，y，p）坐标，其中 p 表示笔是否抬起或放下。输出是一系列字符。[Graves & Schmidhuber
    (2009)](http://people.idsia.ch/~juergen/nips2009.pdf) 证明了带有 LSTM 的 RNN 目前是阅读草书的最佳系统。简而言之，他们使用了一系列小图像作为输入，而不是笔坐标。
- en: '![](../Images/0de05190c7dbf55ac81c791d617a527e.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0de05190c7dbf55ac81c791d617a527e.png)'
- en: More On This Topic
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成为伟大数据科学家所需的5项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个初学者数据科学家应掌握的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2021年最佳ETL工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，并以寻找目标来学习数据科学](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学统计学的最佳资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功数据科学家的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
