- en: Why the Newest LLMs use a MoE (Mixture of Experts) Architecture
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么最新的LLM使用MoE（专家混合）架构
- en: 原文：[https://www.kdnuggets.com/why-the-newest-llms-use-a-moe-mixture-of-experts-architecture](https://www.kdnuggets.com/why-the-newest-llms-use-a-moe-mixture-of-experts-architecture)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/why-the-newest-llms-use-a-moe-mixture-of-experts-architecture](https://www.kdnuggets.com/why-the-newest-llms-use-a-moe-mixture-of-experts-architecture)
- en: '![Why the Newest LLMs use a MoE (Mixture of Experts) Architecture](../Images/04132d59d743f55cc347207e9fe0e535.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![为什么最新的LLM使用MoE（专家混合）架构](../Images/04132d59d743f55cc347207e9fe0e535.png)'
- en: Specialization Made Necessary
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专业化的必要性
- en: A hospital is overcrowded with experts and doctors each with their own specializations,
    solving unique problems. Surgeons, cardiologists, pediatricians—experts of all
    kinds join hands to provide care, often collaborating to get the patients the
    care they need. We can do the same with AI.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 医院里挤满了各类专家和医生，他们各自拥有不同的专业领域，解决各种独特的问题。外科医生、心脏科医生、儿科医生——各类专家携手合作，提供护理，常常需要合作以满足患者的需求。我们可以在AI中实现类似的做法。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三名课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全领域。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织在IT方面'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Mixture of Experts (MoE) architecture in artificial intelligence is defined
    as a mix or blend of different "expert" models working together to deal with or
    respond to complex data inputs. When it comes to AI, every expert in an MoE model
    specializes in a much larger problem—just like every doctor specializes in their
    medical field. This improves efficiency and increases system efficacy and accuracy.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能中，专家混合（MoE）架构定义为不同“专家”模型的组合，这些模型共同处理或响应复杂的数据输入。对于AI而言，每个MoE模型中的专家专注于更大的问题——就像每位医生专注于其医学领域一样。这提高了效率，增加了系统的效能和准确性。
- en: Mistral AI delivers open-source foundational LLMs that rival that of OpenAI.
    They have formally discussed the use of an MoE architecture in their Mixtral 8x7B
    model, a revolutionary breakthrough in the form of a cutting-edge Large Language
    Model (LLM). We’ll deep dive into why Mixtral by Mistral AI stands out among other
    foundational LLMs and why current LLMs now employ the MoE architecture highlighting
    its speed, size, and accuracy.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral AI 提供了与 OpenAI 相媲美的开源基础LLM。他们正式讨论了在其 Mixtral 8x7B 模型中使用 MoE 架构，这是一种突破性的前沿大型语言模型（LLM）。我们将深入探讨为什么
    Mistral AI 的 Mixtral 在其他基础LLM中脱颖而出，以及为什么当前LLM采用MoE架构，突出其速度、规模和准确性。
- en: Common Ways to Upgrade Large Language Models (LLMs)
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 升级大型语言模型（LLM）的常见方式
- en: To better understand how the MoE architecture enhances our LLMs, let’s discuss
    common methods for improving LLM efficiency. AI practitioners and developers enhance
    models by increasing parameters, adjusting the architecture, or fine-tuning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解MoE架构如何提升我们的LLM，让我们讨论一下提高LLM效率的常见方法。AI从业者和开发者通过增加参数、调整架构或微调来提升模型性能。
- en: '**Increasing Parameters:** By feeding more information and interpreting it,
    the model''s capacity to learn and represent complex patterns increases. However,
    this can lead to overfitting and hallucinations, necessitating extensive Reinforcement
    Learning from Human Feedback (RLHF).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增加参数：** 通过输入更多信息并进行解读，模型学习和表示复杂模式的能力提高。然而，这可能导致过拟合和幻觉现象，需要进行大量的来自人类反馈的强化学习（RLHF）。'
- en: '**Tweaking Architecture:** Introducing new layers or modules accommodates the
    increasing parameter counts and improves performance on specific tasks. However,
    changes to the underlying architecture are challenging to implement.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调整架构：** 引入新的层或模块以适应增加的参数数量，并提高特定任务的性能。然而，对底层架构的更改实施起来具有挑战性。'
- en: '**Fine-tuning:** Pre-trained models can be fine-tuned on specific data or through
    transfer learning, allowing existing LLMs to handle new tasks or domains without
    starting from scratch. This is the easiest method and doesn’t require significant
    changes to the model.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调：** 预训练模型可以在特定数据上或通过迁移学习进行微调，使现有的 LLM 能够处理新任务或领域，而无需从头开始。这是最简单的方法，并且不需要对模型进行重大更改。'
- en: What is the MoE Architecture?
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MoE 架构是什么？
- en: 'The Mixture of Experts (MoE) architecture is a neural network design that improves
    efficiency and performance by dynamically activating a subset of specialized networks,
    called experts, for each input. A gating network determines which experts to activate,
    leading to sparse activation and reduced computational cost. MoE architecture
    consists of two critical components: the gating network and the experts. Let''s
    break that down:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 专家混合（MoE）架构是一种神经网络设计，通过动态激活每个输入的专业网络子集（称为专家）来提高效率和性能。门控网络决定激活哪些专家，从而实现稀疏激活和降低计算成本。MoE
    架构包括两个关键组件：门控网络和专家。让我们深入了解一下：
- en: At its heart, the MoE architecture functions like an efficient traffic system,
    directing each vehicle – or in this case, data – to the best route based on real-time
    conditions and the desired destination. Each task is routed to the most suitable
    expert, or sub-model, specialized in handling that particular task. This dynamic
    routing ensures that the most capable resources are employed for each task, enhancing
    the overall efficiency and effectiveness of the model. The MoE architecture takes
    advantage of all 3 ways how to improve a model’s fidelity.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，MoE 架构像一个高效的交通系统，根据实时条件和目标目的地，将每辆车——在这种情况下，是数据——引导到最佳路线。每个任务都被路由到最适合处理该任务的专家或子模型。这种动态路由确保为每个任务分配最具能力的资源，提高了模型的整体效率和效果。MoE
    架构利用了提高模型保真度的所有 3 种方法。
- en: By implementing multiple experts, MoE inherently increases the model's
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过实现多个专家，MoE 自然地提高了模型的
- en: parameter size by adding more parameters per expert.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过增加每个专家的参数数量来增大参数规模。
- en: MoE changes the classic neural network architecture which incorporates a gated
    network to determine which experts to employ for a designated task.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MoE 改变了经典的神经网络架构，包含一个门控网络，以确定为指定任务使用哪些专家。
- en: Every AI model has some degree of fine-tuning, thus every expert in an MoE is
    fine-tuned to perform as intended for an added layer of tuning traditional models
    could not take advantage of.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个 AI 模型都有一定程度的微调，因此 MoE 中的每个专家都会经过微调，以便为传统模型无法利用的额外调优层执行预期的功能。
- en: MoE Gating Network
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MoE 门控网络
- en: The gating network acts as the decision-maker or controller within the MoE model.
    It evaluates incoming tasks and determines which expert is suited to handle them.
    This decision is typically based on learned weights, which are adjusted over time
    through training, further improving its ability to match tasks with experts. The
    gating network can employ various strategies, from probabilistic methods where
    soft assignments are tasked to multiple experts, to deterministic methods that
    route each task to a single expert.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 门控网络在 MoE 模型中充当决策者或控制器的角色。它评估传入的任务，并确定哪个专家适合处理这些任务。这个决策通常基于学习到的权重，这些权重随着训练时间的推移而调整，从而进一步提高其将任务与专家匹配的能力。门控网络可以采用各种策略，从使用概率方法将任务分配给多个专家的软分配，到将每个任务路由到单一专家的确定性方法。
- en: MoE Experts
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MoE 专家
- en: Each expert in the MoE model represents a smaller neural network, machine learning
    model, or LLM optimized for a specific subset of the problem domain. For example,
    in Mistral, different experts might specialize in understanding certain languages,
    dialects, or even types of queries. The specialization ensures each expert is
    proficient in its niche, which, when combined with the contributions of other
    experts, will lead to superior performance across a wide array of tasks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: MoE 模型中的每个专家代表一个较小的神经网络、机器学习模型或针对问题领域特定子集优化的 LLM。例如，在 Mistral 中，不同的专家可能专注于理解某些语言、方言或特定类型的查询。这种专业化确保每个专家在其小众领域中精通，当与其他专家的贡献相结合时，将在各种任务中表现出更高的性能。
- en: MoE Loss Function
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MoE 损失函数
- en: Although not considered a main component of the MoE architecture, the loss function
    plays a pivotal role in the future performance of the model, as it’s designed
    to optimize both the individual experts and the gating network.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不被视为MoE架构的主要组成部分，但损失函数在模型未来的性能中扮演着关键角色，因为它旨在优化个别专家和门控网络。
- en: It typically combines the losses computed for each expert which are weighted
    by the probability or significance assigned to them by the gating network. This
    helps to fine-tune the experts for their specific tasks while adjusting the gating
    network to improve routing accuracy.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 它通常结合了为每个专家计算的损失，并根据门控网络分配给它们的概率或重要性进行加权。这有助于为专家的特定任务进行微调，同时调整门控网络以提高路由准确性。
- en: '![MoE Mixture of Experts LLM Architecture](../Images/fb2e31286a6d2a442ab292ee207382f4.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![MoE 专家混合 LLM 架构](../Images/fb2e31286a6d2a442ab292ee207382f4.png)'
- en: The MoE Process Start to Finish
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MoE过程从开始到结束
- en: Now let’s sum up the entire process, adding more details.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们总结一下整个过程，添加更多细节。
- en: 'Here''s a summarized explanation of how the routing process works from start
    to finish:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是路由过程从开始到结束的总结说明：
- en: 'Input Processing: Initial handling of incoming data. Mainly our Prompt in the
    case of LLMs.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入处理：对输入数据进行初步处理。在LLMs的情况下，主要是我们的提示。
- en: 'Feature Extraction: Transforming raw input for analysis.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取：将原始输入转换为可分析的数据。
- en: 'Gating Network Evaluation: Assessing expert suitability via probabilities or
    weights.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 门控网络评估：通过概率或权重评估专家的适用性。
- en: 'Weighted Routing: Allocating input based on computed weights. Here, the process
    of choosing the most suitable LLM is completed. In some cases, multiple LLMs are
    chosen to answer a single input.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加权路由：根据计算的权重分配输入。在这里，选择最合适的LLM的过程完成。在某些情况下，会选择多个LLM来回答单一输入。
- en: 'Task Execution: Processing allocated input by each expert.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务执行：处理每个专家分配的输入。
- en: 'Integration of Expert Outputs: Combining individual expert results for final
    output.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专家输出的整合：将各个专家的结果合并为最终输出。
- en: 'Feedback and Adaptation: Using performance feedback to improve models.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反馈与调整：利用性能反馈来改进模型。
- en: 'Iterative Optimization: Continuous refinement of routing and model parameters.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代优化：对路由和模型参数进行持续的改进。
- en: Popular Models that Utilize an MoE Architecture
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用MoE架构的流行模型
- en: '**OpenAI’s GPT-4 and GPT-4o:** GPT-4 and GPT4o power the premium version of
    ChatGPT. These multi-modal models utilize MoE to be able to ingest different source
    mediums like images, text, and voice. It is rumored and slightly confirmed that
    GPT-4 has 8 experts each with 220 billion paramters totalling the entire model
    to over 1.7 trillion parameters.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI的GPT-4和GPT-4o：** GPT-4和GPT-4o为ChatGPT的高级版本提供支持。这些多模态模型利用MoE能够处理不同来源的媒介，如图像、文本和语音。有传闻且略微证实GPT-4拥有8个专家，每个专家拥有2200亿参数，总模型超过1.7万亿参数。'
- en: '**Mistral AI’s Mixtral 8x7b: **Mistral AI delivers very strong AI models open
    source and have said their Mixtral model is a sMoE model or sparse Mixture of
    Experts model delivered in a small package. Mixtral 8x7b has a total of 46.7 billion
    parameters but only uses 12.9B parameters per token, thus processing inputs and
    outputs at that cost. Their MoE model consistently outperforms Llama2 (70B) and
    GPT-3.5 (175B) while costing less to run.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mistral AI的Mixtral 8x7b：** Mistral AI提供了非常强大的开源AI模型，并表示他们的Mixtral模型是一种sMoE模型或稀疏专家混合模型，体积小巧。Mixtral
    8x7b总共有467亿参数，但每个token只使用129亿参数，因此以此成本处理输入和输出。他们的MoE模型在性能上 consistently 超越了Llama2（70B）和GPT-3.5（175B），而运行成本更低。'
- en: The Benefits of MoE and Why It's the Preferred Architecture
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MoE的好处以及为何它是首选架构
- en: Ultimately, the main goal of MoE architecture is to present a paradigm shift
    in how complex machine learning tasks are approached. It offers unique benefits
    and demonstrates its superiority over traditional models in several ways.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，MoE架构的主要目标是提出一种新的范式，来应对复杂的机器学习任务。它提供了独特的好处，并在多个方面展示了其优于传统模型的优势。
- en: '**Enhanced Model Scalability**'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强的模型可扩展性**'
- en: Each expert is responsible for a part of a task, therefore scaling by adding
    experts won't incur a proportional increase in computational demands.
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个专家负责任务的一部分，因此通过增加专家来扩展不会导致计算需求的同比增加。
- en: This modular approach can handle larger and more diverse datasets and facilitates
    parallel processing, speeding up operations. For instance, adding an image recognition
    model to a text-based model can integrate an additional LLM expert for interpreting
    pictures while still being able to output text. Or
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种模块化方法可以处理更大、更复杂的数据集，并促进并行处理，加快操作。例如，将图像识别模型添加到基于文本的模型中，可以集成一个额外的 LLM 专家来解释图片，同时仍能输出文本。或者
- en: Versatility allows the model to expand its capabilities across different types
    of data inputs.
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多功能性使模型能够扩展其处理不同类型数据输入的能力。
- en: '**Improved Efficiency and Flexibility**'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提高效率和灵活性**'
- en: MoE models are extremely efficient, selectively engaging only necessary experts
    for specific inputs, unlike conventional architectures that use all their parameters
    regardless.
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: MoE 模型非常高效，仅在特定输入下选择性地调用必要的专家，不像传统架构那样无论如何都使用所有参数。
- en: The architecture reduces the computational load per inference, allowing the
    model to adapt to varying data types and specialized tasks.
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该架构减少了每次推断的计算负担，使模型能够适应不同的数据类型和专门任务。
- en: '**Specialization and Accuracy:**'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专门化与准确性：**'
- en: Each expert in an MoE system can be finely tuned to specific aspects of the
    overall problem, leading to greater expertise and accuracy in those areas
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: MoE 系统中的每个专家可以针对整体问题的特定方面进行精细调节，从而在这些领域实现更高的专业性和准确性。
- en: Specialization like this is helpful in fields like medical imaging or financial
    forecasting, where precision is key
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种专门化在医学影像或金融预测等领域中非常有用，这些领域中精确性是关键。
- en: MoE can generate better results from narrow domains due to its nuanced understanding,
    detailed knowledge, and the ability to outperform generalist models on specialized
    tasks.
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: MoE 可以从狭窄领域中生成更好的结果，因为它对细微差别的理解、详细知识以及在专门任务上超越通用模型的能力。
- en: '![Employing a mixture of experts in a dynamics way increases LLM capabilities](../Images/6038e2df244ba9a66975dfe9f5e1f48b.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![以动态方式使用专家组合提高了 LLM 能力](../Images/6038e2df244ba9a66975dfe9f5e1f48b.png)'
- en: The Downsides of The MoE Architecture
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MoE 架构的缺点
- en: While MoE architecture offers significant advantages, it also comes with challenges
    that can impact its adoption and effectiveness.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 MoE 架构提供了显著的优势，但它也带来了可能影响其采用和有效性的挑战。
- en: '**Model Complexity:** Managing multiple neural network experts and a gating
    network for directing traffic makes MoE development and operational costs challenging'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型复杂性：** 管理多个神经网络专家和一个用于引导流量的门控网络，使 MoE 的开发和运营成本具有挑战性。'
- en: '**Training Stability:** Interaction between the gating network and the experts
    introduces unpredictable dynamics that hinder achieving uniform learning rates
    and require extensive hyperparameter tuning.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练稳定性：** 门控网络与专家之间的交互引入了不可预测的动态，阻碍了实现均匀学习率，并需要大量的超参数调整。'
- en: '**Imbalance:** Leaving experts idle is poor optimization for the MoE model,
    spending resources on experts that are not in use or relying on certain experts
    too much. Balancing the workload distribution and tuning an effective gate is
    crucial for a high-performing MoE AI.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不平衡：** 让专家闲置对 MoE 模型来说是糟糕的优化，浪费资源在不使用的专家上或过度依赖某些专家。平衡工作负载分配和调节有效的门控对高性能 MoE
    AI 至关重要。'
- en: It should be noted that the above drawbacks usually diminish over time as MoE
    architecture is improved.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，上述缺点通常会随着 MoE 架构的改进而逐渐减少。
- en: The Future Shaped by Specialization
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 由专门化塑造的未来
- en: Reflecting on the MoE approach and its human parallel, we see that just as specialized
    teams achieve more than a generalized workforce, specialized models outperform
    their monolithic counterparts in AI models. Prioritizing diversity and expertise
    turns the complexity of large-scale problems into manageable segments that experts
    can tackle effectively.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 反思 MoE 方法及其与人类并行的情况，我们可以看到，就像专门化团队比通才团队取得更好的成果一样，专门化模型在人工智能模型中也优于单一模型。优先考虑多样性和专业知识将大规模问题的复杂性转化为专家可以有效解决的可管理的部分。
- en: As we look to the future, consider the broader implications of specialized systems
    in advancing other technologies. The principles of MoE could influence developments
    in sectors like healthcare, finance, and autonomous systems, promoting more efficient
    and accurate solutions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，考虑到专门化系统在推动其他技术方面的广泛影响。MoE 的原则可能会影响医疗保健、金融和自主系统等领域的发展，促进更高效和更准确的解决方案。
- en: The journey of MoE is just beginning, and its continued evolution promises to
    drive further innovation in AI and beyond. As high-performance hardware continues
    to advance, this mixture of expert AIs can reside in our smartphones, capable
    of delivering even smarter experiences. But first, someone's going to need to
    train one.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: MoE 的旅程才刚刚开始，其持续演变有望推动 AI 及其他领域的进一步创新。随着高性能硬件的不断进步，这种专家 AI 的混合可以驻留在我们的智能手机中，提供更加智能的体验。但首先，需要有人来训练这些模型。
- en: '**[Kevin Vu](https://blog.exxactcorp.com/)** manages [Exxact Corp blog](https://blog.exxactcorp.com/)
    and works with many of its talented authors who write about different aspects
    of Deep Learning.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Kevin Vu](https://blog.exxactcorp.com/)** 管理着 [Exxact Corp 博客](https://blog.exxactcorp.com/)，并与许多撰写有关深度学习不同方面的才华横溢的作者合作。'
- en: More On This Topic
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Data Mesh & Its Distributed Data Architecture](https://www.kdnuggets.com/2022/02/data-mesh-distributed-data-architecture.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据网格及其分布式数据架构](https://www.kdnuggets.com/2022/02/data-mesh-distributed-data-architecture.html)'
- en: '[KDnuggets™ News 22:n07, Feb 16: How to Learn Math for Machine…](https://www.kdnuggets.com/2022/n07.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets™ 新闻 22:n07, 2月 16: 如何学习机器学习中的数学…](https://www.kdnuggets.com/2022/n07.html)'
- en: '[Data Mesh Architecture: Reimagining Data Management](https://www.kdnuggets.com/2022/05/data-mesh-architecture-reimagining-data-management.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据网格架构：重新定义数据管理](https://www.kdnuggets.com/2022/05/data-mesh-architecture-reimagining-data-management.html)'
- en: '[KDnuggets News, May 18: 5 Free Hosting Platform For Machine…](https://www.kdnuggets.com/2022/n20.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，5月 18: 5 个免费的机器学习托管平台…](https://www.kdnuggets.com/2022/n20.html)'
- en: '[How to Build a Scalable Data Architecture with Apache Kafka](https://www.kdnuggets.com/2023/04/build-scalable-data-architecture-apache-kafka.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 Apache Kafka 构建可扩展的数据架构](https://www.kdnuggets.com/2023/04/build-scalable-data-architecture-apache-kafka.html)'
- en: '[Best Architecture for Your Text Classification Task: Benchmarking…](https://www.kdnuggets.com/2023/04/best-architecture-text-classification-task-benchmarking-options.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你文本分类任务的最佳架构：基准测试…](https://www.kdnuggets.com/2023/04/best-architecture-text-classification-task-benchmarking-options.html)'
