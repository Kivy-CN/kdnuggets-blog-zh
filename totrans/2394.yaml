- en: K-nearest Neighbors in Scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/07/knearest-neighbors-scikitlearn.html](https://www.kdnuggets.com/2022/07/knearest-neighbors-scikitlearn.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![K-nearest Neighbors in Scikit-learn](../Images/617f62294b09863e60149f470d2d8903.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Nina Strehl](https://unsplash.com/@ninastrehl) via Unsplash'
  prefs: []
  type: TYPE_NORMAL
- en: K-nearest neighbors (KNN) is a type of supervised learning machine learning
    algorithm and can be used for both regression and classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: A supervised machine learning algorithm is dependent on labeled input data which
    the algorithm learns on and uses its learnt knowledge to produce accurate outputs
    when unlabeled data is inputted.
  prefs: []
  type: TYPE_NORMAL
- en: The use of KNN is to make predictions on the test data set based on the characteristics
    (labeled data) of the training data. The method used to make these predictions
    is by calculating the distance between the test data and training data, assuming
    that similar characteristics or attributes of the data points exist within close
    proximity.
  prefs: []
  type: TYPE_NORMAL
- en: It allows us to identify and assign the category of the new data whilst taking
    into consideration its characteristics based on learned data points from the training
    data. These characteristics of the new data point will be learned by the KNN algorithm
    and based on its proximity to other data points, it will be categorized.
  prefs: []
  type: TYPE_NORMAL
- en: Why is KNN a Good Algorithm?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: KNN is a good algorithm to use, specifically for classification tasks. Classification
    is a typical task that a lot of Data Scientists and Machine Learning engineers
    come across. It solves a lot of real world problems.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, algorithms such as KNN are a good and accurate choice of algorithm
    to be used for pattern classification and regression models. KNN has been known
    not to make any assumptions about the data, leading to higher accuracy than other
    classification algorithms. The algorithm is also easy to implement and interpretable.
  prefs: []
  type: TYPE_NORMAL
- en: The ‘k’ in KNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ‘K’ in KNN is a parameter that refers to the number of nearest neighbors,
    where the K-value essentially creates an environment for the data points to understand
    its similarities based on proximity. Using the K-value, we compute the distance
    between the test data points and the trained labeled points to better categorize
    the new data points.
  prefs: []
  type: TYPE_NORMAL
- en: The K-value is a positive integer which is typically small in value with a recommendation
    that it be an odd number. When the K-value is small, the error rate decreases,
    with a low bias but a high variance which leads to overfitting of the model.
  prefs: []
  type: TYPE_NORMAL
- en: How do you Calculate the Distance Between the Data Points?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'KNN is a distance-based algorithm, with the most common methods used being:'
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean and Manhattan for continuous data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hamming distance for categorical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Euclidean Distance is the mathematical distance between two points within Euclidean
    space using the length of a line between the two points. This is the most well
    known distance metric and a lot of people will remember it from school from Pythagoras
    Theorem.
  prefs: []
  type: TYPE_NORMAL
- en: Manhattan Distance is the mathematical distance between two points, which is
    the sum of the absolute difference of their Cartesian coordinates. In simple terms,
    the movement of direction between the distance can only be top, bottom and sideways.
  prefs: []
  type: TYPE_NORMAL
- en: Hamming distance compares two binary data strings and then compares these two
    string inputs to find the number of different characters in each position of the
    string.
  prefs: []
  type: TYPE_NORMAL
- en: Simple KNN Algorithm Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These are the general steps you need to take for the KNN algorithm
  prefs: []
  type: TYPE_NORMAL
- en: Load in your dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose a k-value. You should choose an odd number to avoid a tie.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the distance between the new data point and the neighboring existing trained
    data points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign the new data point to its K nearest neighbor
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using sklearn for kNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: neighbours is a package from the sklearn module which you use for nearest neighbor
    classification tasks. This can be used for both unsupervised and supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you will need to import these libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the sklearn neighbors package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'These are the parameters that can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The distance metric used here in the example is minkowski, however as mentioned
    above there are different distance metrics you can use.
  prefs: []
  type: TYPE_NORMAL
- en: If you would like to know more about these parameters, click on this [link](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).
  prefs: []
  type: TYPE_NORMAL
- en: K-nearest Neighbors in Scikit-Learn on the Iris Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Load in the Iris Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import these libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Import Iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what the dataset should look like by executing df.head():'
  prefs: []
  type: TYPE_NORMAL
- en: '![Iris dataset](../Images/b25c0e8dcb031b95eee8a7dee71ed74f.png)'
  prefs: []
  type: TYPE_IMG
- en: Preprocessing of Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step is to split the dataset based on attributes and labels. The class
    column is considered out labels and is referred to as y, where the first 4 columns
    are attributes and will be referred to as X
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Train/Test split
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using a train/test split on our dataset will help us to better understand how
    well our algorithm performs on unseen data/testing phase. It also helps with reducing
    overfitting from occurring.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Feature Scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feature scaling is an important step before executing your model to start making
    predictions. It involves rescaling the features in a common boundary so that no
    information about each of the data points is lost. Without this, your model could
    possibly make wrong predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Make Your Prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is where we will use the neighbors package from the sklearn module. As
    you can see, we have chosen our number of neighbors (K-value) as 5.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we want to make a prediction on the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating Your Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most typical metrics used for evaluating your algorithm are confusion matrix,
    precision, recall and f1 score.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![code output](../Images/4e1a093c224913a29143f850e70b107a.png)'
  prefs: []
  type: TYPE_IMG
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So we’ve come to understand that KNN is a good algorithm to use for classification
    tasks and that the neighbors package in Scikit-learn can make it all so much easier.
    It is very simple and easy to implement with a flexibility to feature/distance
    choices. It has the ability to handle multi-class cases and it can effectively
    produce accurate outputs.
  prefs: []
  type: TYPE_NORMAL
- en: But there are some things you need to take into consideration when it comes
    to KNN. Determining the k-value can be difficult, because it can be the difference
    between causing overfitting or not. It can also be a matter of trial and error
    when determining which distance metric you should use.KNN also has a high computational
    cost as we compute the distances between the new data point and the training data
    points. With this being said, the KNN algorithm slows down as the number of examples
    and variables increases
  prefs: []
  type: TYPE_NORMAL
- en: Although it is one of the oldest and well used classification algorithms out
    there, you need to consider the con as well.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and Freelance Technical Writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[From Theory to Practice: Building a k-Nearest Neighbors Classifier](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Nearest Neighbors for Classification](https://www.kdnuggets.com/2022/04/nearest-neighbors-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
