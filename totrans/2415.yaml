- en: Weak Supervision Modeling, Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弱监督建模，解释
- en: 原文：[https://www.kdnuggets.com/2022/05/weak-supervision-modeling-explained.html](https://www.kdnuggets.com/2022/05/weak-supervision-modeling-explained.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/05/weak-supervision-modeling-explained.html](https://www.kdnuggets.com/2022/05/weak-supervision-modeling-explained.html)
- en: What is Weak Supervision?
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是弱监督？
- en: '[Weak supervision](https://snorkel.ai/weak-supervision/) is a way to obtain
    labels for training data points for a machine learning model programmatically,
    by writing labeling functions. These functions label your data much faster and
    more efficiently than manual labeling, but they also create “noisy” labels, which
    you then need to model and combine in order to generate the labels that you will
    later use to train your downstream model. In other words, there is little-to-no
    actual hand labeling being done by a human, instead, the labeling is done programmatically
    and then “de-noised” via a label model.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[弱监督](https://snorkel.ai/weak-supervision/)是一种通过编写标记函数以程序化的方式为机器学习模型训练数据点获取标签的方法。这些函数比手动标记更快、更高效地标记数据，但它们也会创建“嘈杂”的标签，然后你需要对这些标签进行建模和合并，以生成稍后用于训练下游模型的标签。换句话说，几乎没有实际的人工标记，而是通过程序化的方式进行标记，然后通过标签模型进行“去噪”。'
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT需求'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'In this post, we will be focusing primarily on the middle section of this pipeline:
    the label model itself. This is the object used to model and combine the noisy
    labels.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将主要关注这个流程的中间部分：标签模型本身。这是用来建模和合并嘈杂标签的对象。
- en: '![What is Weak Supervision](../Images/6a23615bf36f74246582f0b5a310c4c3.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![What is Weak Supervision](../Images/6a23615bf36f74246582f0b5a310c4c3.png)'
- en: Creating the Label Model in Weak Supervision
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在弱监督中创建标签模型
- en: 'One way to think about de-noising is to combine noisy labels—but all of the
    labeling functions aren’t necessarily equally reliable! When you generate a label
    model, then, you are asking: how do you know which of the noisy labels produced
    by the [programmatic labeling](https://snorkel.ai/programmatic-labeling/) function
    are reliable? Essentially, you need to figure out which ones you should throw
    out or not, and then how to use them. By doing that, you can determine the “best
    guess” of what the actual label should be, and that provides you with an automatically
    constructed data set. You then train your final model using those labels.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 思考去噪的一种方法是合并嘈杂的标签——但所有的标记函数不一定都同样可靠！当你生成一个标签模型时，你在问：你如何知道由[程序化标记](https://snorkel.ai/programmatic-labeling/)函数生成的嘈杂标签中哪些是可靠的？本质上，你需要弄清楚哪些标签应该丢弃，哪些应该保留，然后如何使用它们。通过这样做，你可以确定“最佳猜测”实际标签是什么，这为你提供了一个自动构建的数据集。然后，你使用这些标签训练你的最终模型。
- en: '![what is intution](../Images/a9e17d702a09c23ef6cbb5ff28d74b99.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![what is intution](../Images/a9e17d702a09c23ef6cbb5ff28d74b99.png)'
- en: What is the intuition behind a label model? To use a simplistic analogy, imagine
    a courtroom full of witnesses. We usually refer to the “true” label for a data
    point as being a latent, unobserved variable, and that is exactly what happens
    in court. You don't know whether the defendant actually committed the crime or
    not. So, you have to use some alternative forms of learning the truth. In this
    very simplistic analogy, each witness says, “I think the defendant committed the
    crime,” or, “I do not think the defendant committed the crime.”
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 标签模型背后的直觉是什么？用一个简单的类比来说明，想象一个充满证人的法庭。我们通常将数据点的“真实”标签视为一个潜在的、未观察到的变量，这正是法庭上的情况。你不知道被告是否真正犯了罪。因此，你必须使用一些替代方法来了解真相。在这个非常简单的类比中，每个证人会说，“我认为被告犯了罪，”或者，“我不认为被告犯了罪。”
- en: The witnesses are a lot like our labeling functions. The variable of whether
    the defendant committed the crime or not is kind of like a “+1; -1” label for
    our case. One witness is +1, one witness is +1, one witness is -1, and so on.
    From there, the easiest thing to do is to go with what the majority of witnesses
    say, and this is exactly what the majority-vote aspect of the label model is doing
    as well. It says, “the majority of the labeling function said +1, so this data
    point is a +1.” You can get a lot from doing this. But there are some downsides.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 证人很像我们的标签函数。被告是否犯了罪这个变量有点像我们的“+1; -1”标签。一位证人是+1，一位证人是+1，一位证人是-1，等等。从这里开始，最简单的方法是依照多数证人的意见，这也是标签模型中多数投票方面的作用。它的意思是，“多数标签函数说是+1，所以这个数据点是+1。”这样做可以获得很多信息。但也有一些缺点。
- en: Accuracies and Correlations
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准确性与相关性
- en: '![Accuracies and Correlations](../Images/ccaf3ad01728b2c95270d7058a8ff987.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![准确性与相关性](../Images/ccaf3ad01728b2c95270d7058a8ff987.png)'
- en: There are two major downsides to majority vote, for our purposes. One is that
    not all witnesses are equally reliable. You don't want to simply throw out the
    witnesses that are not as reliable, but you do want to assign them different weights.
    The hard part is that you don't know for sure which witnesses are reliable or
    unreliable. You have to figure it out on your own. We call this process finding
    “accuracies.” We want our label model to incorporate the accuracies of the labeling
    function.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的来说，多数投票有两个主要缺点。一个是并非所有证人都是同等可靠的。你不希望仅仅抛弃那些不太可靠的证人，但你确实想给他们分配不同的权重。困难之处在于你不能确定哪些证人是可靠的还是不可靠的。你需要自己搞清楚。我们称这个过程为寻找“准确性”。我们希望我们的标签模型能结合标签函数的准确性。
- en: The second downside is something called correlations or “cliques.” Imagine if
    three of the witnesses cooperated and talked together before the trial began.
    You would not want to trust them as much as you might three independent witnesses,
    because once they coordinated, they may have come to change their minds and agree
    on a single view. Ideally, you want to then down-weight these “cliques” of witnesses.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个缺点是所谓的相关性或“圈子”。想象一下，如果三位证人在审判开始之前合作并讨论过。你不会像对待三位独立证人那样信任他们，因为一旦他们协调一致，他们可能会改变想法并达成统一观点。理想情况下，你会想要减少这些“圈子”证人的权重。
- en: Majority vote cannot help us with either of these problems. It simply gives
    equal weight to everybody, reliable or unreliable, and it gives equal weight to
    everybody even if they form a clique. But you want to include accuracies and correlations
    in your label model, which means we will have to learn about them.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 多数投票无法解决这些问题。它仅仅对每个人赋予相等的权重，无论可靠与否，即使他们形成了一个圈子也如此。但你希望在标签模型中包括准确性和相关性，这意味着我们需要学习这些内容。
- en: '![model setup](../Images/6f402313c8200e910ff63708224659a0.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![模型设置](../Images/6f402313c8200e910ff63708224659a0.png)'
- en: 'From this point forward, the math we will use is this notation: “*lambda 1*,”
    “*lambda 2*,” through “*lambda n*.” These are your labeling functions. They''re
    random variables, and they take on values like -1 or +1\. Then, there is the true
    but unobserved label “*Y*.” You don’t get to see it, but it''s there.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，我们将使用这种符号表示数学：“*lambda 1*”，“*lambda 2*”，一直到“*lambda n*”。这些是你的标签函数。它们是随机变量，取值如-1或+1。然后，是“*Y*”这个真实但未观察到的标签。你看不到它，但它确实存在。
- en: 'Very concretely, your goal is to compute this conditional probability *P(Y)*
    given ? *1 ? 2* through ? *n*. It is the probability of the label, having known
    that we have this information from the labeling functions. In other words: what
    is the probability, given all of the noisy labels that you have, that *Y* has
    value 0 or 1 or 2, and so on.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 非常具体地说，你的目标是计算在给定? *1 ? 2*到? *n*的条件概率 *P(Y)*。这是已知我们从标签函数中获得了这些信息的情况下，标签的概率。换句话说：考虑到你拥有的所有噪声标签，*Y*的值是0、1、2，等等的概率。
- en: This is already a big improvement over majority vote, because you don't have
    to just say, “I think it is +1.” Instead, you can say, “I think it is a probability
    of 0.6 of being +1 and 0.3 of being -1,” and so on. That is already a lot more
    useful for training your eventual model. But you still don’t know enough to compute
    your conditional probability. In order to do that, you need to encode all of your
    information into what we call a label model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经比多数投票有了很大的改进，因为您不需要仅仅说，“我认为它是+1。”相反，您可以说，“我认为它是+1的概率是0.6，-1的概率是0.3，”等等。这对于训练您的最终模型来说已经有了更多的用处。但您仍然不知道足够的信息来计算条件概率。为了做到这一点，您需要将所有信息编码成我们所说的标签模型。
- en: 'This is what a label model really is: it is all of the information that you
    need to compute these conditional probabilities, which then tell you, given the
    labeling functions’ output, what is the probability of every possible value of
    the true label, *Y*?'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 标签模型实际上就是这样：它包含了计算这些条件概率所需的所有信息，这些条件概率告诉您，给定标记函数的输出，真实标签的每一个可能值的概率是*Y*。
- en: What’s a Probabilistic Model?
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是概率模型？
- en: '![What’s a probabilistic model?](../Images/fe2e433b19b38a72c2dc2ef88bf34cdf.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![什么是概率模型？](../Images/fe2e433b19b38a72c2dc2ef88bf34cdf.png)'
- en: In order to compute your conditional probability, you need a probabilistic model.
    A probabilistic model is a set of parameters that can fully specify some distribution
    that you can use for calculations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算条件概率，您需要一个概率模型。概率模型是一组参数，可以完全指定您用于计算的某种分布。
- en: A really simple example is Gaussian random variables. You can think of one as
    a model just by itself if you know the mean and the variance. So, ifitell you
    the mean and the variance, you can say, for example, “the probability that *X*
    equals 3.7 is ___; the probability that *X* is between -2 and 16 is ___,” and
    so on.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常简单的例子是高斯随机变量。如果您知道均值和方差，您可以把它看作一个独立的模型。因此，如果我告诉您均值和方差，您可以说，例如，“*X*等于3.7的概率是___；*X*在-2和16之间的概率是___”，等等。
- en: 'For your label model, you want to define similar things. You want to find:
    what are the parameters that are sufficient for you to compute the conditional
    probabilities for the true label, given the values of the labeling function.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于您的标签模型，您需要定义类似的内容。您需要找出：在给定标记函数的值的情况下，哪些参数足以计算真实标签的条件概率。
- en: Random Variables
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机变量
- en: '![Random Variables](../Images/74f3f8f8872ab679ef0c8ca105caa422.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![随机变量](../Images/74f3f8f8872ab679ef0c8ca105caa422.png)'
- en: Let’s pause here and review a quick primer on random variables.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在这里暂停一下，回顾一下随机变量的简要介绍。
- en: In the classic image of the bell curve above, the mean is in the middle. That
    is the expectation of *X*. The variance is the measure of the spread—how far away
    you are from the mean, on average.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的经典钟形曲线图中，均值位于中间。这就是*X*的期望。方差是衡量分布扩散程度的指标——即您平均离均值有多远。
- en: The covariance between two variables is a measure of how related two variables
    are. When variables are independent, meaning they have no information about one
    another, their covariance is 0\. You can already think of this as defining an
    accuracy. We want to know the covariance between one labeling function and the
    true label, *Y*. If a given labeling function is really bad at guessing the *Y*,
    the covariance between these two things will be 0.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 两个变量之间的协方差是衡量两个变量相关性的指标。当变量是独立的，意味着它们没有关于彼此的信息时，它们的协方差为0。您可以将其视为定义准确度。我们想知道一个标记函数和真实标签*Y*之间的协方差。如果一个给定的标记函数在猜测*Y*时表现非常糟糕，这两个变量之间的协方差将是0。
- en: 'Finally, there is the independence property: *A* and *B* are independent if,
    and only if, *P(AB)* is *P(A)* times *P(B)*. This implies that the expectations
    for these two things you can write the product as the product of the expectations.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有独立性属性：*A*和*B*是独立的，当且仅当，*P(AB)*等于 *P(A)* 乘以 *P(B)*。这意味着这两个东西的期望值可以写成期望值的乘积。
- en: Setting Up Your Model
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置您的模型
- en: '![Setting Up Your Model](../Images/1c7fa5466791a8f1370564cbba04fa95.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![设置您的模型](../Images/1c7fa5466791a8f1370564cbba04fa95.png)'
- en: Now, let’s set up your actual label model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们设置您的实际标签模型。
- en: On a [data-centric AI](https://snorkel.ai/data-centric-ai/) approach, the label
    model involves all of these variables, the labeling functions in the true label,
    *Y*, and then of course you need to learn these parameters from data, because
    you don't know them *a priori*. In this Gaussian case, for example, you could
    compute the mean by simply taking a lot of samples, adding them up, and normalizing,
    dividing by how many samples you saw. You can think of a coin flip in the same
    way. If the probability of heads is *P*, you can estimate *P* by flipping a coin
    10,000 times, seeing how many heads come up, and dividing by 10,000\. That's an
    empirical estimate of *P*. For our purposes, you have to do a very similar procedure.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个 [数据中心化 AI](https://snorkel.ai/data-centric-ai/) 方法中，标签模型涉及所有这些变量，包括真实标签中的标记函数
    *Y*，然后你当然需要从数据中学习这些参数，因为你不知道它们 *a priori*。以这个高斯示例为例，你可以通过简单地取很多样本、加总它们并归一化来计算均值，即将其除以你看到的样本数量。你也可以用抛硬币的方式来理解。如果正面朝上的概率是
    *P*，你可以通过抛硬币 10,000 次来估计 *P*，查看有多少次正面朝上，并将其除以 10,000。这就是 *P* 的经验估计。就我们的目的而言，你必须做一个非常类似的程序。
- en: 'A brief recap: You want to improve on simple majority vote by integrating the
    accuracies of the labeling functions as well as their correlations, or “cliques.”
    To do so, you have to introduce some probabilistic model over the labeling functions
    in the true label. You need to learn what the appropriate parameters are that
    encode all this information. You need to have a way to learn these parameters.
    And then you need to find what calculation is going to give us that conditional
    probability.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 简要回顾：你想通过整合标记函数的准确性以及它们的相关性或“团体”来改进简单的多数投票。为此，你必须在真实标签中的标记函数上引入一些概率模型。你需要了解编码所有这些信息的适当参数是什么。你需要有一种学习这些参数的方法。然后，你需要找到什么计算能给我们提供那个条件概率。
- en: 'In other words: What is the probability of the true label, *Y*, given what
    we saw from the wavelength functions? You won''t ever get to see *Y.* That''s
    the whole idea behind weak supervision. You will only see samples from the labeling
    functions, and that''s the information you get to use.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说：给定我们从波长函数中看到的内容，真实标签 *Y* 的概率是多少？你永远不会看到 *Y*。这就是弱监督的整个思想。你只能看到来自标记函数的样本，那就是你可以使用的信息。
- en: '![label](../Images/ef0344ec803ba9751534e4a58404ccee.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![标签](../Images/ef0344ec803ba9751534e4a58404ccee.png)'
- en: Now, let's specify what these parameters are actually going to be.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来具体说明这些参数到底是什么。
- en: Obviously, you want things like the means, so the expectations of each of these
    labeling functions, ? i, is also the expectation of *Y*. These things are not
    very difficult to estimate. To find the expectation of ? i, look at all of the
    outputs for the labeling function, and then see how often it votes +1, how often
    it votes -1, divided by the total, and that gives you the mean.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，你想要像均值这样的东西，所以这些标记函数的期望 ? i 也是 *Y* 的期望。这些东西不难估计。要找到 ? i 的期望，查看标记函数的所有输出，然后看看它投票
    +1 的频率、投票 -1 的频率，除以总数，这样你就可以得到均值。
- en: The expectation of *Y* is harder, because you don't get to see the true labels,
    but you can get this too if you have, say, a dev set. You can get an estimate
    of the class balance from a dev set, and there are other ways to do this as well.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*Y* 的期望更难，因为你无法看到真实的标签，但如果你有一个开发集，例如，你也可以得到这个期望。你可以从开发集中获得类别平衡的估计，还有其他方法可以做到这一点。'
- en: These are the means, and if you recall from the Gaussian example above, the
    means and the variances give you everything you need. It's going to be the same
    thing here. The variances (or covariances) are going to be these accuracies and
    these correlations. The expectation of ?itimes *Y*, and then the expectations
    of this product ? i comes ? *J*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是均值，如果你从上面的高斯示例中回忆一下，均值和方差给你提供了所有你需要的信息。这儿也是一样。方差（或协方差）将是这些准确性和这些相关性。*itimes
    Y* 的期望，然后是这个乘积的期望 ? i comes ? *J*。
- en: Assume we're talking about binary variables, here. So they are all either +1
    or -1\. Of course, this is just for illustration—we can do slightly more sophisticated
    things if we have multiclass label values. If your ? i—your labeling function—is
    really accurate, then it's going to always agree with *Y*. So when *Y* is 1, it's
    going to be 1\. When *Y* is -1, it's going to be -1\. This product will always
    be 1, because 1 times 1 is 1 and -1 times -1 is also 1\. This expectation is going
    to be close to 1 all of the time. If your labeling function is really inaccurate,
    you are going to be close to 0 all of the time. This kind of parameter will tell
    us how accurate the actual labeling functions are, and that's why we can justify
    this accuracy terminology.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们讨论的是二元变量。那么它们要么是 +1，要么是 -1。 当然，这只是为了说明——如果我们有多类标签值，我们可以做稍微复杂一点的事情。如果你的 ?
    i——你的标记函数——真的很准确，那么它总是会与*Y*一致。因此，当*Y*为1时，它也会是1。当*Y*为-1时，它也会是-1。这个乘积总是为1，因为1乘1等于1，-1乘-1也等于1。这个期望值会一直接近1。如果你的标记函数真的不准确，你将会一直接近0。这种参数将告诉我们实际标记函数的准确性，这就是我们可以解释这种准确性术语的原因。
- en: Correlations are the same. How often do labeling functions agree with each other
    or how often do they disagree with each other? Then in the cases where there are
    these correlations you are going to get one correlation as well. This gives you
    all of the parameters of the model. Just like the Gaussian example, you have these
    means, and you have the covariance instead of the variance. If you know these,
    you have enough information to compute those conditional probabilities. This is
    the whole goal for the label model, and everything else follows from there.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性是一样的。标记函数之间的相符程度如何，或它们之间的不一致程度如何？然后，在存在这些相关性的情况下，你也会得到一个相关性。这为你提供了模型的所有参数。就像高斯示例一样，你有这些均值，并且你有协方差而不是方差。如果你知道这些，你就有足够的信息来计算这些条件概率。这就是标签模型的全部目标，其余的都从这里开始。
- en: 'This is all much more difficult to do when compared to the coin flip example.
    Normally, if you were computing a mean, you would view a lot of samples and just
    find their average. But you don''t know what *Y* is in this case, so you can''t
    just take samples of ?itimes *Y*. That''s the hard part about weak supervision.
    You don''t know the true label, so you cannot easily compute these kinds of empirical
    expectations. That''s the only real trick that you need to use here for the label
    model: a way to get access to this information without actually knowing what *Y*
    is.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于硬币抛掷示例，这一切要困难得多。通常，如果你在计算均值，你会查看大量样本并找出它们的平均值。但在这种情况下，你不知道*Y*是什么，所以你不能仅仅采样
    ?itimes *Y*。这就是弱监督的难点。你不知道真实标签，所以你无法轻易计算这些经验期望。这就是你在标签模型中需要使用的唯一真正技巧：一种在实际不知道*Y*是什么的情况下获取这些信息的方法。
- en: '![graphic model review](../Images/8c840db9cb3b292d3bb65252ea3c1fdf.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图形模型回顾](../Images/8c840db9cb3b292d3bb65252ea3c1fdf.png)'
- en: In this graphical model, there is a node for every random variable. The edges
    tell you something about correlations or covariances. There is a real technical
    explanation for what exactly they mean, but very roughly, if there is an edge
    between things you can think of them as correlated. If there is no edge between
    a pair of nodes, you can think of them as having some independence between them.
    Sometimes, you have to do something to the variables to find that independence,
    but that is the basic idea. In this setup, the labeling functions are always connected
    to the true label. If they were not, then there would be no relationship between
    the labeling function value and the label. You would have a “random guessing”
    labeling function, which would not be useful. You always have these edges between
    the actual value of the label and all the labeling functions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图形模型中，每个随机变量都有一个节点。边缘告诉你关于相关性或协方差的信息。确切的技术解释是存在的，但大致上，如果两个事物之间有一条边，你可以认为它们是相关的。如果两个节点之间没有边，你可以认为它们之间有某种独立性。有时，你必须对变量做一些处理才能发现这种独立性，但这是基本的思想。在这个设置中，标记函数总是与真实标签连接。如果它们没有连接，那么标记函数值和标签之间将没有关系。你将会有一个“随机猜测”的标记函数，这将没有用。你总是有这些边连接实际标签值和所有标记函数。
- en: Sometimes you have edges between pairs of labeling functions, and sometimes
    you don't. When we have that pair, it is in the case of correlations. That was
    in the courtroom analogy from above, where you have two witnesses talking to each
    other and coming up with one story. That is something we also want to model, because
    we do want to down-weight those values. And in fact implicitly when you compute
    this conditional “*P(Y)* given ___” probability, you are going to take that into
    account when you compute it.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有时标记函数对之间有边缘，有时没有。当我们有那对边缘时，是在相关性的情况下。那就像上面的法庭比喻，你有两个证人互相交谈并编造一个故事。这也是我们想建模的，因为我们确实想降低这些值的权重。实际上，当你计算这个条件“*P(Y)*
    given ___”概率时，你会在计算时考虑到这一点。
- en: '![learning parameter 1](../Images/0309fd4e2689306a2d8c272810c9aadb.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![学习参数 1](../Images/0309fd4e2689306a2d8c272810c9aadb.png)'
- en: How do you learn the parameters. There are two primary methods.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如何学习这些参数呢？主要有两种方法。
- en: The first is perhaps easiest. The only real parameter we care to know about
    are the accuracies. To learn it, you want to exploit independence specifically,
    and that's why, with this graphical model, on the left there are no edges between
    the labeling functions, only between the labeling functions and the true label.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法可能最简单。我们真正关心的唯一参数是准确性。要学习它，你需要专门利用独立性，这就是为什么在这个图形模型中，左侧标记函数之间没有边缘，只有标记函数与真实标签之间有边缘。
- en: Again, remember what the accuracy parameters are. There are these expectations
    of the product of a labeling function, and the bigger they are, the more reliable
    you can think of a given labeling function. If they were perfectly reliable, you
    would just use that information for *Y*. If they are super unreliable, you want
    to down-weight them.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 再次记住准确性参数是什么。这些是标记函数乘积的期望值，它们越大，你可以认为给定的标记函数越可靠。如果它们是完全可靠的，你会直接使用这些信息来确定 *Y*。如果它们非常不可靠，你就需要降低它们的权重。
- en: Let’s look at this really neat property of independence. The labeling functions
    themselves are not independent of one another, which is good because they are
    all trying to predict the same thing. If they were independent, they would be
    terrible. But it turns out that the products of the labeling functions and *Y*,
    often are independent. Essentially, the accuracies are independent even though
    the variables themselves are not independent.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来深入探讨这个独立性的非常有趣的特性。标记函数本身并不相互独立，这很好，因为它们都在尝试预测相同的事物。如果它们是独立的，那将非常糟糕。但事实证明，标记函数和*Y*的乘积往往是独立的。实际上，尽管变量本身不独立，但准确性是独立的。
- en: 'So, what does that mean? If you write this product on the left—this ? *1Y*,
    ? *2Y*—you can factorize this product because of independence as the product of
    two expectations (under some minor assumptions that we won’t get into). It is:
    expectation of the product *is* the product of the expectations.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 那这意味着什么？如果你在左边写这个乘积——这个 ? *1Y*，? *2Y*——你可以因独立性将这个乘积因式分解为两个期望值的乘积（在一些我们不会详细讨论的小假设下）。这是：乘积的期望值*是*期望值的乘积。
- en: '![triplets](../Images/ff76c6470a0ecc76708055669d187ebc.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![三元组](../Images/ff76c6470a0ecc76708055669d187ebc.png)'
- en: The second method for learning parameters involves using triplets. This *Y*
    was +1 or -1\. Those are the two possible values. When you multiply ? *1Y* ? *2Y*,
    you get ? *1 ? 2Y*-squared. But *Y*-squared is 1, regardless of whether *Y* is
    +1 or -1\. So this one just goes away, and you end up just seeing ? *1 ? 2*'s
    expectation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种学习参数的方法涉及使用三元组。这个 *Y* 可以是 +1 或 -1。这是两种可能的值。当你乘以 ? *1Y* ? *2Y* 时，你会得到 ? *1
    ? 2Y* 的平方。但 *Y* 的平方为 1，无论 *Y* 是 +1 还是 -1。因此，这一项就会消失，你最终只看到 ? *1 ? 2* 的期望值。
- en: 'So, we are writing the expectation of ? *1Y* times ? *2Y,* which are these
    two accuracy parameters. Their product is this correlation parameter thatican
    actually estimate. This is just the agreement or disagreement on average of a
    pair of labeling functions. If you look at all the things they have labeled, you
    can say: “they have the same label 75 percent of the time and they are different
    25 percent of the time, and that tells me this expectation of ? *1 ? 2*.” In other
    words, this is observable.idon''t really know what the accuracy parameters are
    for these two labeling functions, butido know that they multiply to be this kind
    of correlation parameter, which is knowable. We don''t know the accuracy parameters
    but we know a function of the accuracy parameters, which is their product.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们在计算*1Y*次*2Y*的期望，这两个是准确度参数。它们的乘积是这个相关性参数，实际上可以估计。这只是对一对标记函数在平均上的一致性或不一致性。如果你查看它们标记的所有内容，你可以说：“它们有75%的时间具有相同的标签，25%的时间不同，这告诉我*1
    ? 2*的期望。”换句话说，这是可观察的。我并不知道这两个标记函数的准确度参数是什么，但我知道它们的乘积是这种类型的相关性参数，这是可知的。我们不知道准确度参数，但我们知道准确度参数的一个函数，即它们的乘积。
- en: This is actually the key thing that we have to have here. Once we have this,
    we can do a little system of equations, called triplets. We can write down this
    exact thing for three labeling functions, just going two at a time. For the three
    equations above, we just did this exact trick for ? *1 ? 2*, and for ? *1 ? 3*,
    then for *? 2 ? 3*. We have this independence property between each of these pairs.
    There are three different pairs and you have three equations and you have three
    variables. What are the three variables? They are just each of these accuracy
    parameters. You know the left-hand sides for all these cases, so you can think
    of this as *A* times *B* has some value, *A* times *C* has some value, *B* times
    *C* has some value. You know those three values andineed to compute *A, B,* and
    *C* from them. You can solve this.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是我们必须拥有的关键内容。一旦我们有了这个，我们可以做一个小的方程组，称为三元组。我们可以为三个标记函数逐一写下这个确切的内容。对于上面的三个方程，我们刚刚对*1
    ? 2*做了这个确切的操作，然后是*1 ? 3*，再到*2 ? 3*。我们在这些对之间有独立性属性。共有三对不同的对，你有三个方程和三个变量。那三个变量是什么？它们就是这些准确度参数。你知道所有这些情况下的左侧值，所以你可以把它看作*A*乘以*B*有一个值，*A*乘以*C*有一个值，*B*乘以*C*有一个值。你知道这三个值，需要从中计算*A,
    B*和*C*。你可以解决这个问题。
- en: So, even though you never got to see *Y*, and even though you cannot directly
    do a sample average to get this expectation, you have still written it in terms
    that are observable. You can go into our label matrix and compute these three
    terms, solve this equation, and get our guess of this product. Thenican do this
    for every possible labeling function andiwill get all of the accuracies. As for
    correlation, you can estimate it directly. You simply average them, just like
    the coin flips. Now we have access to all the accuracies and we are done.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，即使你从未看到过*Y*，也即使你不能直接做样本平均来获得这个期望，你仍然将其写成可观察的形式。你可以进入我们的标签矩阵，计算这三个术语，解这个方程，并得到我们对这个乘积的估计。然后你可以对每一个可能的标记函数进行这操作，得到所有的准确度。至于相关性，你可以直接估计它。你只需像掷硬币一样进行平均。现在我们可以访问所有的准确度，我们就完成了。
- en: '![triplets 2](../Images/4e41615d2b4ec3d9381b41317191d991.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![triplets 2](../Images/4e41615d2b4ec3d9381b41317191d991.png)'
- en: Learning Parameters
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 学习参数
- en: Exploiting Independence
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用独立性
- en: Triplets
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三元组
- en: Covariance Matrix
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协方差矩阵
- en: 'Where to connect with Fred: [Twitter](https://twitter.com/fredsala) | [Website](https://pages.cs.wisc.edu/~fredsala/).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 与Fred联系的方式：[Twitter](https://twitter.com/fredsala) | [Website](https://pages.cs.wisc.edu/~fredsala/)。
- en: Stay in touch with Snorkel AI, follow us on [Twitter](https://twitter.com/snorkelai),
    [LinkedIn](https://www.linkedin.com/company/snorkel-ai), [Facebook](https://www.facebook.com/snorkelai),
    [Youtube](https://www.youtube.com/channel/UC6MQ2p8gZFYdTLEV8cysE6Q), or [Instagram](https://www.instagram.com/snorkel_ai/),
    and if you’re interested in joining the Snorkel team, we’re hiring! Please apply
    on our [careers page](https://snorkel.ai/careers).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 与Snorkel AI保持联系，关注我们的[Twitter](https://twitter.com/snorkelai)，[LinkedIn](https://www.linkedin.com/company/snorkel-ai)，[Facebook](https://www.facebook.com/snorkelai)，[Youtube](https://www.youtube.com/channel/UC6MQ2p8gZFYdTLEV8cysE6Q)或[Instagram](https://www.instagram.com/snorkel_ai/)，如果你有兴趣加入Snorkel团队，我们正在招聘！请在我们的[招聘页面](https://snorkel.ai/careers)申请。
- en: '**[Frederic Sala](https://pages.cs.wisc.edu/~fredsala/)** is an Assistant Professor
    in the Computer Sciences Department at the University of Wisconsin-Madison. Fred''s
    research studies the foundations of data-driven systems, with a focus on machine
    learning systems. Previously, he was a postdoctoral researcher in the Stanford
    CS department associated with the Stanford InfoLab and Stanford DAWN. He received
    his Ph.D. in electrical engineering from UCLA in December 2016\. He is the recipient
    of the NSF Graduate Research Fellowship, the outstanding Ph.D. dissertation award
    from the UCLA Department of Electrical Engineering, and the Edward K. Rice Outstanding
    Masters Student Award from the UCLA Henry Samueli School of Engineering & Applied
    Science. He received the B.S.E. degree in Electrical Engineering from the University
    of Michigan, Ann Arbor.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**[弗雷德里克·萨拉](https://pages.cs.wisc.edu/~fredsala/)** 是威斯康星大学麦迪逊分校计算机科学系的助理教授。弗雷德的研究专注于数据驱动系统的基础，特别是机器学习系统。此前，他曾在斯坦福大学计算机科学系担任博士后研究员，参与斯坦福信息实验室和斯坦福DAWN项目。他于2016年12月获得加州大学洛杉矶分校电气工程博士学位。他获得了NSF研究生奖学金、加州大学洛杉矶分校电气工程系杰出博士论文奖，以及加州大学洛杉矶分校亨利·萨缪尔工程与应用科学学院的爱德华·K·赖斯杰出硕士生奖。他在密歇根大学安娜堡分校获得了电气工程学士学位。'
- en: More On This Topic
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[N-gram Language Modeling in Natural Language Processing](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理中的N-gram语言建模](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
- en: '[A community developing a Hugging Face for customer data modeling](https://www.kdnuggets.com/2022/08/objectiv-community-developing-hugging-face-customer-data-modeling.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一个为客户数据建模开发Hugging Face的社区](https://www.kdnuggets.com/2022/08/objectiv-community-developing-hugging-face-customer-data-modeling.html)'
- en: '[Topic Modeling Approaches: Top2Vec vs BERTopic](https://www.kdnuggets.com/2023/01/topic-modeling-approaches-top2vec-bertopic.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[主题建模方法：Top2Vec与BERTopic](https://www.kdnuggets.com/2023/01/topic-modeling-approaches-top2vec-bertopic.html)'
- en: '[A List of 7 Best Data Modeling Tools for 2023](https://www.kdnuggets.com/2023/03/list-7-best-data-modeling-tools-2023.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2023年最佳数据建模工具的7个推荐](https://www.kdnuggets.com/2023/03/list-7-best-data-modeling-tools-2023.html)'
- en: '[Database Key Terms, Explained](https://www.kdnuggets.com/2016/07/database-key-terms-explained.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据库关键术语解析](https://www.kdnuggets.com/2016/07/database-key-terms-explained.html)'
- en: '[Descriptive Statistics Key Terms, Explained](https://www.kdnuggets.com/2017/05/descriptive-statistics-key-terms-explained.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[描述性统计学关键术语解析](https://www.kdnuggets.com/2017/05/descriptive-statistics-key-terms-explained.html)'
