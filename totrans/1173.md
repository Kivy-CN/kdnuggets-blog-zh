# GPU 驱动的数据科学（非深度学习）与 RAPIDS

> 原文：[https://www.kdnuggets.com/2021/08/gpu-powered-data-science-deep-learning-rapids.html](https://www.kdnuggets.com/2021/08/gpu-powered-data-science-deep-learning-rapids.html)

[评论](#comments)

![标题图片](../Images/d3c4ef08dfa0e0885360d379295720aa.png)

**图片来源**： [Pixabay](https://pixabay.com/photos/pc-hardware-geforce-radeon-6113265/) （免费图片）

## 你是否在寻找“GPU 驱动的数据科学”？

* * *

## 我们的前三名课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速入门网络安全职业

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你在 IT 领域的组织

* * *

想象一下你自己是一个数据科学家、商业分析师或物理学/经济学/神经科学的学术研究人员……

你经常进行**数据整理、清理、统计测试、可视化**。你也常常调整许多**线性模型**来拟合数据，并偶尔尝试**随机森林**。你还涉及到**聚类**大数据集。听起来够熟悉吗？

然而，鉴于你处理的数据集的性质（大多为表格和结构化数据），你不会深入学习深度学习。你更愿意将所有的硬件资源投入到你实际日常工作中，而不是花费在一些花哨的深度学习模型上。再次说说，这听起来熟悉吗？

你听说过 [GPU 系统的强大性能和闪电般的计算能力](https://www.nvidia.com/en-us/deep-learning-ai/products/solutions/)，例如来自 NVidia 的各种工业和科学应用。

你不断思考 —— “***这对我有什么好处？我如何在我的特定工作流程中利用这些强大的半导体***？”

你正在寻找 GPU 驱动的数据科学。

评估这种方法的最佳（也是最快）选择之一是使用 [**Saturn Cloud**](https://saturncloud.io/?utm_source=Tirtha&utm_medium=GPU-powered%20Data%20Science%20Article)** + **[**RAPIDS**](https://rapids.ai/)**。**让我详细解释一下……

## 在 AI/ML 传说中，GPU 主要用于深度学习

尽管在学术界和商业界广泛讨论了GPU和分布式计算在核心AI/ML任务中的使用（例如运行[1000层深度神经网络](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035)进行图像分类或[billion-parameter BERT](https://arxiv.org/abs/1909.08053)语音合成模型），但它们在常规数据科学和数据工程任务中的实用性却鲜有涉及。

尽管如此，**数据相关任务是任何AI管道中的机器学习工作负载的必要前提**，这些任务通常占据了数据科学家甚至机器学习工程师的大部分时间和智力劳动，[数据科学家或者机器学习工程师花费的时间和智力劳动占据了大多数](https://www.infoworld.com/article/3228245/the-80-20-data-science-dilemma.html)。最近，著名的AI先驱

[Andrew Ng](https://medium.com/u/592ce2a67248?source=post_page-----29f9ed8d51f3--------------------------------)谈到了[**从以模型为中心转向以数据为中心的AI**](https://www.youtube.com/watch?v=06-AZXmwHjo)工具开发。这意味着在实际AI工作负载在你的管道中执行之前，需要花费更多的时间处理原始数据和预处理数据。

所以，重要的问题是：***我们能否利用GPU和分布式计算的力量来处理常规的数据处理工作？***

![](../Images/cfe568cb012bc33d8646f653e9cad210.png)

**图片来源**：作者使用免费图片创建的拼贴 ([Pixabay](https://pixabay.com/vectors/squirrel-reading-books-surprise-304021/))

> 尽管在学术界和商业界广泛讨论了GPU和分布式计算在核心AI/ML任务中的使用，但它们在常规数据科学和数据工程任务中的实用性却鲜有涉及。

## 令人惊叹的RAPIDS生态系统

[RAPIDS软件库和API套件](https://rapids.ai/)为你——一个普通的数据科学家（而不一定是深度学习从业者）——提供了在GPU上**完整执行端到端的数据科学和分析管道**的选项和灵活性。

这个开源项目由Nvidia孵化，旨在通过构建工具来利用CUDA原语。它特别专注于**通过数据科学友好的Python语言暴露GPU并行性和高带宽内存速度特性**。

**常见的数据准备和整理任务**在RAPIDS生态系统中非常受到重视。它还大大提供了对**多节点、多GPU部署和分布式处理**的支持。在可能的情况下，它与其他库集成，使得**超出内存**（即数据集大小大于单台计算机RAM）数据处理对个别数据科学家变得简单而可及。

![](../Images/0829816c3ae29c222246fdfa91aae184.png)

**图片来源**：作者创建的拼贴

三个最突出的（且具有Python风格的）组件——对普通数据科学家特别感兴趣的——是，

+   [**CuPy**](https://docs.cupy.dev/en/stable/reference/comparison.html)：一个 CUDA 驱动的数组库，看起来和感觉都像 Numpy，同时利用各种 CUDA 库，例如 cuBLAS、cuDNN、cuRand、cuSolver、cuSPARSE、cuFFT 和 NCCL，充分发挥底层 GPU 架构的优势。

+   **CuDF**：这是一个 GPU 数据框架库，用于加载、聚合、连接、过滤和处理数据，具有 **类似 pandas 的 API**。数据工程师和数据科学家可以利用它来轻松加速他们的任务流程，使用强大的 GPU，而无需学习 CUDA 编程的细节。

+   [**CuML**](https://github.com/rapidsai/cuml)：这个库使数据科学家、分析师和研究人员能够运行传统/经典的机器学习算法及相关处理任务，充分利用 GPU 的计算能力。自然，这个库主要用于表格数据集。想象一下 Scikit-learn 以及它如何利用你 GPU 卡上的所有 CUDA 和 Tensor 核心！与此相应，在大多数情况下，cuML 的 Python API 与 Scikit-learn 相匹配。此外，它还试图通过与[**Dask**](https://docs.dask.org/en/latest/)的优雅集成提供 **多 GPU 和多节点 GPU 支持**，以充分利用真正的分布式处理/集群计算。

> ***我们能否利用 GPU 和分布式计算的力量来处理常规的数据处理任务和结构化数据的机器学习？***

## 这与使用 Apache Spark 有什么不同？

你可能会问这种 GPU 驱动的数据处理与使用 Apache Spark 有什么不同。实际上，它们之间有一些微妙的差别，只有最近，随着 Spark 3.0 的发布，GPU 才成为 Spark 工作负载的主流资源。

[**利用 GPU 和 RAPIDS 加速 Apache Spark 3.0 | NVIDIA 开发者博客**](https://developer.nvidia.com/blog/accelerating-apache-spark-3-0-with-gpus-and-rapids/)

我们没有时间或空间来讨论这种 GPU 驱动的数据科学方法与特别适合 Apache Spark 的大数据任务之间的独特差异。但如果你问自己这些问题，你可能会理解其中的微妙差别，

“*作为一个建模经济交易和投资组合管理的数据科学家，我想用 100,000 个变量来解决一个*[*线性方程组*](https://en.wikipedia.org/wiki/System_of_linear_equations)*。我应该使用纯线性代数库还是 Apache Spark*？”

“*作为图像压缩管道的一部分，我想在一个包含百万条条目的大型矩阵上使用*[*奇异值分解*](https://towardsdatascience.com/understanding-singular-value-decomposition-and-its-application-in-data-science-388a54be95d)*。Apache Spark 适合这个任务吗*？”

大问题规模并不总是意味着 Apache Spark 或 Hadoop 生态系统。大计算并不等于大数据。作为一名全面的数据科学家，你需要了解这两者，以解决各种问题。

> RAPIDS 特别关注通过 Python API **暴露 GPU 并行性和高带宽内存速度特性**。

## 我们在这篇文章中展示了什么？

### 仅提供 CuPy 和 CuML 的简洁示例

所以，在这篇文章中，我们将仅演示 CuPy 和 CuML 的简洁示例，

+   它们与相应的 Numpy 和 Scikit-learn 函数/估算器在速度上的比较

+   数据/问题规模在这个速度比较中有多重要。

### 后续文章中的 CuDF 示例

尽管类似 Pandas 数据处理的数据工程示例对许多数据科学家而言具有很高的兴趣，我们将在后续文章中涵盖 CuDF 示例。

### 我的 GPU 基础硬件平台是什么？

我正在使用一个[**Saturn Cloud**](https://saturncloud.io/?utm_source=Tirtha&utm_medium=GPU-powered%20Data%20Science%20Article) Tesla T4 GPU 实例，因为启动一个[功能齐全且加载了（数据科学和人工智能库）计算资源的云实例](https://www.saturncloud.io/s/nvidia/) 实际上只需 5 分钟时间，满足我所有的数据科学工作。**只要我每月 Jupyter Notebook 使用时间不超过 10 小时，它就是免费的**！如果你想了解更多关于他们服务的信息，

## Saturn Cloud 托管服务已推出：人人都能进行 GPU 数据科学！

[**GPU 计算是数据科学的未来。像 RAPIDS、TensorFlow 和 PyTorch 这样的包使得计算速度极快…**](https://www.saturncloud.io/blog/saturn-cloud-hosted-has-launched-gpu-data-science-for-everyone/)

除了拥有[Tesla T4 GPU](https://www.nvidia.com/en-us/data-center/tesla-t4/)，它还是一台 4 核 Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz 机器，配有 16 GB 内存和 10 GB 持久磁盘。因此，从硬件配置的角度来看，这是一种相当普通的设置（由于免费层的硬盘有限），即任何数据科学家都可能拥有这样的硬件。唯一的区别是 GPU 的存在以及以正确的方式设置所有 CUDA 和 Python 库，以确保 RAPIDS 套件顺利运行。

> 大问题规模并不总意味着 Apache Spark 或 Hadoop 生态系统。大计算并不等同于大数据。作为一个全面的数据科学家，你需要了解这两者，以应对各种问题。

### 求解线性方程组

我们创建不同规模的线性方程组，并使用 Numpy（和 CuPy）的`linalg.solve`例程通过以下代码求解，

![](../Images/da82dcb41575536341d1705d43609761.png)

而且，对于 CuPy 实现，代码只因一个字母的变化（在多个调用中）而有所不同！

![](../Images/a1bac98a78faba8d375bbea8b9339f76.png)

还要注意，我们如何从 Numpy 数组创建 CuPy 数组作为参数。

结果是显著的。CuPy 开始时速度较慢或与 Numpy 相似，但对于大规模问题（方程数量）明显超越 Numpy。

![](../Images/3eef97e201daaefe710505532167abe3.png)

### 奇异值分解

接下来，我们处理使用从正态分布中生成的随机方阵进行的奇异值分解问题。我们在这里不重复代码块，只展示结果以节省篇幅。

![](../Images/c9e56c1ab31bb1185a08a915d28e1175.png)

值得注意的是，CuPy 算法在这一问题类别中并没有表现出比 Numpy 算法显著更优的性能。也许，这是 CuPy 开发者需要改进的地方。

### 回到基础：矩阵求逆

最后，我们回到基础问题，考虑矩阵求逆（几乎所有机器学习算法中都会用到的）。结果再次显示 CuPy 算法在性能上明显优于 Numpy 包。

![](../Images/520e51bcb56b1e8fec32e0f8f078365b.png)

### 解决 K-means 聚类问题

接下来，我们考虑使用熟悉的 k-means 算法进行无监督学习中的聚类问题。在这里，我们将 CuML 函数与 Scikit-learn 包中的等效估算器进行比较。

仅供参考，这是这两个估算器之间的 API 比较。

![](../Images/facdccdbfe76522bafd1a3c7a968a4b3.png)

**图片来源**：[Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) 和 [CuML 网站](https://docs.rapids.ai/api/cuml/stable/api.html#k-means-clustering)（开源项目）

这是一个具有 10 个特征/维度的数据集的结果。

![](../Images/6e741bf16eb344f368b8968ea9a58538.png)

这是另一个具有 100 个特征的数据集的实验结果。

![](../Images/c49a3e4ddf1e024f4f98159babf87482.png)

显然，样本大小（行数）和维度（列数）在 GPU 加速表现优越上都起了作用。

### 熟悉的线性回归问题

在处理表格数据集时，谁能忽视线性回归问题的速度比较？按照之前的节奏，我们同时变化问题规模——这次同时变化样本数量和维度——并比较 CuML `LinearRegression` 估算器与 Scikit-learn 稳定版本的性能。

以下图中的 X 轴表示问题规模——从 1,000 样本/50 特征到 20,000 样本/1,000 特征。

同样，CuML 估算器在问题复杂度（样本大小和维度）增加时表现得更好。

![](../Images/6e95f098dc9fe9c215df7cef5e96d97f.png)

## 总结

我们关注了 RAPIDS 框架的两个最基本的组成部分，该框架旨在将 GPU 的强大功能带到数据分析和机器学习的日常任务中，即使数据科学家没有进行任何深度学习任务。

![](../Images/99e2cd77dfaf96b3a37ddccb0662cae7.png)

**图片来源**：作者使用免费的Pixabay图片制作（[链接-1](https://pixabay.com/photos/nvidia-graphic-card-bitcoin-gpu-5264921/)，[链接-2](https://pixabay.com/vectors/cube-hexagon-stairs-152932/)，[链接-3](https://pixabay.com/vectors/statistic-analytic-diagram-1564428/)）

我们使用了[**Saturn Cloud**](https://saturncloud.io/?utm_source=Tirtha&utm_medium=GPU-powered%20Data%20Science%20Article)**基于**[**Tesla T4的实例**](https://www.saturncloud.io/s/freehosted/)**，展示了CuPy和CuML库的一些功能，以及常用算法的性能比较。

+   并非所有RAPIDS库中的算法都显著优于其他算法，但大多数确实如此。

+   通常，随着问题复杂性（样本大小和维度）的增加，性能提升会迅速增长。

+   如果你有GPU，始终尝试RAPIDS，比较和测试是否获得了性能提升，并将其作为数据科学流程中的可靠工具。

+   代码更改非常少，几乎可以忽略不计。

**让GPU的力量为你的分析和数据科学工作流注入活力**。

你可以查看作者的[**GitHub**](https://github.com/tirthajyoti?tab=repositories)**库，获取机器学习和数据科学方面的代码、创意和资源。如果你像我一样，对AI/机器学习/数据科学充满热情，请随时[在LinkedIn上添加我](https://www.linkedin.com/in/tirthajyoti-sarkar-2127aa7/)或[在Twitter上关注我](https://twitter.com/tirthajyotiS)。

感谢Mel。

[原文](https://medium.com/dataseries/gpu-powered-data-science-not-deep-learning-with-rapids-29f9ed8d51f3)。经许可转载。

**相关内容：**

+   [如何使用NVIDIA GPU加速库](/2021/07/nvidia-gpu-accelerated-libraries.html)

+   [为什么以及如何学习“高效数据科学”？](/2021/07/learn-productive-data-science.html)

+   [不仅仅是深度学习：GPU如何加速数据科学与数据分析](/2021/07/deep-learning-gpu-accelerate-data-science-data-analytics.html)

### 更多相关主题

+   [成为一名出色数据科学家所需的5项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)

+   [每个初学者数据科学家应掌握的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)

+   [2021年最佳ETL工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)

+   [停止学习数据科学以寻找目的，并为目的而学习…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)

+   [学习数据科学统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)

+   [建立一个强大的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)
