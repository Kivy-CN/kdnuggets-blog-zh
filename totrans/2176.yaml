- en: Tuning Hyperparameters in Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整神经网络中的超参数
- en: 原文：[https://www.kdnuggets.com/tuning-hyperparameters-in-neural-networks](https://www.kdnuggets.com/tuning-hyperparameters-in-neural-networks)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/tuning-hyperparameters-in-neural-networks](https://www.kdnuggets.com/tuning-hyperparameters-in-neural-networks)
- en: '![Tuning Hyperparameters in Neural Networks](../Images/b9de72ea3c51ae95da6057db4e0a857f.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![调整神经网络中的超参数](../Images/b9de72ea3c51ae95da6057db4e0a857f.png)'
- en: Hyperparameters determine how well your neural network learns and processes
    information. Model parameters are learned during training. Unlike these parameters,
    hyperparameters must be set before the training process starts. In this article,
    we will describe the techniques for optimizing the hyperparameters in the models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数决定了您的神经网络学习和处理信息的效果。模型参数是在训练过程中学习的。与这些参数不同，超参数必须在训练过程开始之前设置。在本文中，我们将描述优化模型中超参数的技术。
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的IT需求'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Hyperparameters In Neural Networks
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络中的超参数
- en: Learning Rate
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习率
- en: The learning rate tells the model how much to change based on its errors. If
    the learning rate is high, the model learns quickly but might make mistakes. If
    the learning rate is low, the model learns slowly but more carefully. This leads
    to less errors and better accuracy.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率告诉模型基于其错误改变多少。如果学习率高，模型学习得快，但可能犯错误。如果学习率低，模型学习得慢但更仔细。这会导致较少的错误和更好的准确率。
- en: '![Tuning Hyperparameters in Neural Networks](../Images/2b104f721c61169c260d51c2f1f83c47.png)Source:
    https://www.jeremyjordan.me/nn-learning-rate/'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '![调整神经网络中的超参数](../Images/2b104f721c61169c260d51c2f1f83c47.png) 来源: https://www.jeremyjordan.me/nn-learning-rate/'
- en: There are ways of adjusting the learning rate to achieve the best results possible.
    This involves adjusting the learning rate at predefined intervals during training.
    Furthermore, optimizers like the Adam enables a self-tuning of the learning rate
    according to the execution of the training.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种调整学习率以获得最佳结果的方法。这涉及在训练过程中在预定义的时间间隔调整学习率。此外，像Adam这样的优化器使学习率能够根据训练的执行进行自我调节。
- en: Batch Size
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量大小
- en: Batch size is the number of training samples a model undergoes at a given time.
    A large batch size basically means that the model goes through more samples before
    the parameter update. It can lead to more stable learning but requires more memory.
    A smaller batch size on the other hand updates the model more frequently. In this
    case, learning can be faster but it has more variation in each update.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大小是模型在给定时间接受的训练样本数量。大批量大小意味着模型在更新参数之前处理更多的样本。这可以导致更稳定的学习，但需要更多的内存。另一方面，小批量大小会更频繁地更新模型。在这种情况下，学习可以更快，但每次更新的变化更多。
- en: The value of the batch size affects memory and processing time for learning.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大小的值影响学习的内存和处理时间。
- en: Number of Epochs
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 纪元数
- en: Epochs refers to the number of times a model goes through the entire dataset
    during training. An epoch includes several cycles where all the data batches are
    shown to the model, it learns from it, and optimizes its parameters. More epochs
    are better in learning the model but if not well observed they can result in overfitting.
    Deciding the correct number of epochs is necessary to achieve a good accuracy.
    Techniques like early stopping are commonly used to find this balance.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 纪元是指模型在训练过程中遍历整个数据集的次数。一个纪元包括多个周期，在这些周期中，所有数据批次都会展示给模型，模型从中学习，并优化其参数。更多的纪元有助于模型学习，但如果观察不当，可能导致过拟合。决定正确的纪元数对于获得良好的准确率是必要的。像早停这样的技术通常用于找到这种平衡。
- en: Activation Function
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 激活函数
- en: Activation functions decide whether a neuron should be activated or not. This
    leads to non-linearity in the model. This non-linearity is beneficial especially
    while trying to model complex interactions in the data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数决定神经元是否应该被激活。这会导致模型的非线性。这种非线性特别有益，尤其是在尝试建模数据中的复杂交互时。
- en: '![Tuning Hyperparameters in Neural Networks](../Images/bda31e9774872cf755f405bf06b7dd96.png)Source:
    https://www.researchgate.net/publication/354971308/figure/fig1/AS:1080246367457377@1634562212739/Curves-of-the-Sigmoid-Tanh-and-ReLu-activation-functions.jpg'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![神经网络中的超参数调整](../Images/bda31e9774872cf755f405bf06b7dd96.png)来源：https://www.researchgate.net/publication/354971308/figure/fig1/AS:1080246367457377@1634562212739/Curves-of-the-Sigmoid-Tanh-and-ReLu-activation-functions.jpg'
- en: Common activation functions include ReLU, Sigmoid and Tanh. ReLU makes the training
    of neural networks faster since it permits only the positive activations in neurons.
    Sigmoid is used for assigning probabilities since it outputs a value between 0
    and 1\. Tanh is advantageous especially when one does not want to use the whole
    scale which ranges from 0 to ± infinity. The selection of a right activation function
    requires careful consideration since it dictates whether the network shall be
    able to make a good prediction or not.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的激活函数包括ReLU、Sigmoid和Tanh。ReLU使神经网络的训练更快，因为它只允许神经元中的正激活。Sigmoid用于分配概率，因为它输出一个介于0和1之间的值。Tanh尤其在不想使用从0到±无穷大的整个范围时是有利的。选择合适的激活函数需要仔细考虑，因为它决定了网络是否能够做出良好的预测。
- en: Dropout
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dropout
- en: Dropout is a technique which is used to avoid overfitting of the model. It randomly
    deactivates or "drops out" some neurons by setting their outputs to zero during
    each training iteration. This process prevents neurons from relying too heavily
    on specific inputs, features, or other neurons. By discarding the result of specific
    neurons, dropout helps the network to focus on essential features in the process
    of training. Dropout is mostly implemented during training while it is disabled
    in the inference phase.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout是一种用于避免模型过拟合的技术。它通过将神经元的输出在每次训练迭代时设置为零，随机停用或“丢弃”一些神经元。这个过程防止了神经元过度依赖特定的输入、特征或其他神经元。通过丢弃特定神经元的结果，Dropout帮助网络在训练过程中专注于关键特征。Dropout通常在训练期间实现，而在推理阶段则禁用。
- en: Hyperparameter Tuning Techniques
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调整技术
- en: Manual Search
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 手动搜索
- en: This method involves trial and error of values for parameters that determine
    how the learning process of a machine learning model is done. These settings are
    adjusted one at a time to observe how it influences the model’s performance. Let's
    try to change the settings manually to get better accuracy.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法涉及对决定机器学习模型学习过程的参数值进行反复试验。这些设置一次调整一个，以观察它对模型性能的影响。让我们尝试手动更改设置以获得更好的准确性。
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Manual search is simple because you do not require any complicated algorithms
    to manually set parameters for testing. However, it has several disadvantages
    as compared to other methods. It can take a lot of time and it may not find the
    very best settings efficiently than the automated methods
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 手动搜索很简单，因为你不需要任何复杂的算法来手动设置测试参数。然而，与其他方法相比，它有几个缺点。它可能需要很长时间，而且可能没有自动化方法高效地找到最佳设置。
- en: Grid Search
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网格搜索
- en: Grid search tests many different combinations of hyperparameters to find the
    best ones. It trains the model on part of the data. After that, it checks how
    well it does with another part. Let's implement grid search using GridSearchCV
    to find the best model .
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索测试许多不同的超参数组合以找到最佳组合。它在部分数据上训练模型。之后，它检查模型在另一部分数据上的表现。我们可以使用GridSearchCV实现网格搜索，以找到最佳模型。
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Grid search is much faster than manual search. However, it is computationally
    expensive because it takes time to check every possible combination.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索比手动搜索快得多。然而，它计算开销很大，因为检查每个可能的组合需要时间。
- en: Random Search
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机搜索
- en: This technique randomly selects combinations of hyperparameters to find the
    most efficient model. For each random combination, it trains the model and checks
    how well it performs. In this way, it can quickly arrive at good settings that
    cause the model to perform better. We can implement random search using RandomizedSearchCV
    to achieve the best model on the training data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术随机选择超参数组合以找到最有效的模型。对于每个随机组合，它训练模型并检查其表现如何。通过这种方式，它可以快速找到使模型表现更好的良好设置。我们可以使用RandomizedSearchCV实现随机搜索，以在训练数据上获得最佳模型。
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Random search is normally better than the grid search since only a few number
    of hyperparameters are checked to get suitable hyperparameters settings. Nonetheless,
    it might not search the correct combination of hyperparameters particularly when
    the working hyperparameters space is large.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索通常优于网格搜索，因为只检查少量的超参数以获取合适的超参数设置。然而，当超参数空间较大时，它可能不会找到正确的超参数组合。
- en: Wrapping Up
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: We've covered some of the basic hyperparameter tuning techniques. Advanced techniques
    include Bayesian Optimization, Genetic Algorithms and Hyperband.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了一些基本的超参数调优技术。高级技术包括贝叶斯优化、遗传算法和Hyperband。
- en: '**[Jayita Gulati](https://www.linkedin.com/in/jayitagulati1998/)** is a machine
    learning enthusiast and technical writer driven by her passion for building machine
    learning models. She holds a Master''s degree in Computer Science from the University
    of Liverpool.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Jayita Gulati](https://www.linkedin.com/in/jayitagulati1998/)** 是一位对构建机器学习模型充满热情的机器学习爱好者和技术作者。她拥有利物浦大学计算机科学硕士学位。'
- en: More On This Topic
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关话题
- en: '[Tuning Random Forest Hyperparameters](https://www.kdnuggets.com/2022/08/tuning-random-forest-hyperparameters.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[调优随机森林超参数](https://www.kdnuggets.com/2022/08/tuning-random-forest-hyperparameters.html)'
- en: '[Tuning XGBoost Hyperparameters](https://www.kdnuggets.com/2022/08/tuning-xgboost-hyperparameters.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[调优 XGBoost 超参数](https://www.kdnuggets.com/2022/08/tuning-xgboost-hyperparameters.html)'
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 PyTorch 进行可解释神经网络](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
- en: '[3 Reasons Why You Should Use Linear Regression Models Instead of…](https://www.kdnuggets.com/2021/08/3-reasons-linear-regression-instead-neural-networks.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用线性回归模型而不是…的3个理由](https://www.kdnuggets.com/2021/08/3-reasons-linear-regression-instead-neural-networks.html)'
- en: '[Deep Neural Networks Don''t Lead Us Towards AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度神经网络不会引领我们走向AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
- en: '[Explainable Forecasting and Nowcasting with State-of-the-art Deep…](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[前沿的可解释预测与即时预测…](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
