- en: Introduction to PyTorch for Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2018/11/introduction-pytorch-deep-learning.html](https://www.kdnuggets.com/2018/11/introduction-pytorch-deep-learning.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '![Image](../Images/601b7698df3c9c20b0c40518801cdfa0.png)'
  prefs: []
  type: TYPE_IMG
- en: In this tutorial, you’ll get an introduction to **deep learning using the PyTorch
    framework**, and by its conclusion, you’ll be comfortable applying it to your
    deep learning models. Facebook launched PyTorch 1.0 early this year with integrations
    for [Google Cloud,](https://cloud.google.com/) [AWS](http://aws.amazon.com/),
    and [Azure Machine Learning.](https://azure.microsoft.com/en-us/services/machine-learning-studio/) In
    this tutorial, I assume that you’re already familiar with [Scikit-learn](http://scikit-learn.org/), [Pandas](https://pandas.pydata.org/), [NumPy](http://www.numpy.org/),
    and [SciPy](https://www.scipy.org/). These packages are important prerequisites
    for this tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Plan of Attack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What is [deep learning](https://heartbeat.fritz.ai/introduction-to-deep-learning-with-keras-c7c3d14e1527)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introduction to PyTorch
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why you’d prefer PyTorch to other Python Deep Learning Libraries
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PyTorch Tensors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PyTorch Autograd
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PyTorch nn Module
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PyTorch optim Package
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Custom nn Modules in PyTorch
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Putting it all Together and Further Reading
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Deep Learning?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning is a subfield of machine learning with algorithms inspired by
    the working of the human brain. These algorithms are referred to as artificial
    neural networks. Examples of these neural networks include [Convolutional Neural
    Networks](https://heartbeat.fritz.ai/a-beginners-guide-to-convolutional-neural-networks-cnn-cf26c5ee17ed) that
    are used for image classification, Artificial Neural Networks and Recurrent Neural
    Networks.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'PyTorch is a Python machine learning package based on[ Torch](https://en.wikipedia.org/wiki/Torch_%28machine_learning%29),
    which is an open-source machine learning package based on the programming language [Lua](https://en.wikipedia.org/wiki/Lua_%28programming_language%29).
    PyTorch has two main features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Tensor](https://www.kdnuggets.com/2018/05/wtf-tensor.html) computation (like
    NumPy) with strong GPU acceleration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic differentiation for building and training neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why you might prefer PyTorch to other Python deep learning libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are a few reason you might prefer PyTorch to other deep learning libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other libraries like TensorFlow where you have to first define an entire
    computational graph before you can run your model, PyTorch allows you to define
    your graph dynamically.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PyTorch is also great for deep learning research and provides maximum flexibility
    and speed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PyTorch Tensors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**PyTorch Tensors** are very similar to NumPy arrays with the addition that
    they can run on the GPU. This is important because it helps accelerate numerical
    computations, which can increase the speed of neural networks by 50 times or greater.
    In order to use PyTorch, you’ll need to head over to[https://PyTorch.org/](https://pytorch.org/) and
    install PyTorch. If you’re using Conda, you can install PyTorch by running this
    simple command:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to define a PyTorch tensor, start by importing the `torch` package.
    PyTorch allows you to define two types of[ tensors](https://pytorch.org/docs/0.3.1/tensors.html) — a
    CPU and GPU tensor. For this tutorial, I’ll assume you’re running a CPU machine,
    but I’ll also show you how to define tensors in a GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The default tensor type in PyTorch is a float tensor defined as `**torch.FloatTensor**`**. **As
    an example, you’ll create a tensor from a Python list:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re using a GPU-enabled machine, you’ll define the tensor as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also perform mathematical computations such as addition and subtraction
    using PyTorch tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also define matrices and perform matrix operations. Let’s see how you’d
    define a matrix and transpose it:'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Autograd
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PyTorch uses a technique called[ **automatic differentiation**](https://en.wikipedia.org/wiki/Automatic_differentiation) that
    numerically evaluates the derivative of a function. Automatic differentiation
    computes backward passes in neural networks. In training neural networks weights
    are randomly initialized to numbers that are near zero but not zero. **Backward
    pass** is the process by which these weights are adjusted from right to left,
    and a forward pass is the inverse (left to right).
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.autograd` is the library that supports automatic differentiation in
    PyTorch. The central class of this package is `torch.Tensor`**.** To track all
    operations on it, set `.requires_grad`as`True`**. **To compute all gradients,
    call `.backward()`**.** The gradient for this tensor will be accumulated in the `**.**grad`attribute.'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to detach a tensor from computation history, call the `**.**detach()`function.
    This will also prevent future computations on the tensor from being tracked. Another
    way to prevent history tracking is by wrapping your code with `torch.no_grad():`
  prefs: []
  type: TYPE_NORMAL
- en: The `Tensor` and `Function`classes are interconnected to build an acyclic graph
    that encodes a complete history of the computation. The `.grad_fn`attribute of
    the tensor references the `Function`that created the tensor. To compute derivatives,
    call `.backward()`on a`Tensor`*.*If the `Tensor`contains one element, you don’t
    have to specify any parameters for the `backward()`function. If the `Tensor`contains
    more than one element, specify a gradient that’s a tensor of matching shape.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, you’ll create two tensors, one with `requires_grad`as `True`and
    the other as `False`. You’ll then use these two tensors to perform addition and
    sum operations. Thereafter, you’ll compute the gradient of one of the tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Calling `**.**grad`on `b` will return nothing since you didn’t set `requires_grad`
    to `True` on it.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch nn Module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is the module for building neural networks in PyTorch. `nn` depends on `autograd` to
    define models and differentiate them. Let’s start by defining the procedure for **training
    a neural network**:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the neural network with some learnable parameters, referred to as weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate over a dataset of inputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Process input through the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare predicted results to actual values and measure the error.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Propagate gradients back into the network’s parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the weights of the network using a simple update rule:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`weight = weight — learning_rate * gradient`'
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll now use the `nn` package to create a two-layer neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explain some of the parameters used above:'
  prefs: []
  type: TYPE_NORMAL
- en: '`N` is batch size. Batch size is the number of observations after which the
    weights will be updated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`D_in` is the input dimension'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`H` is the hidden dimension'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`D_out` is the output dimension'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.randn` defines a matrix of the specified dimensions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.nn.Sequential` initializes a linear stack of layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.nn.Linear`applies a linear transformation to the incoming data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.nn.ReLU`applies the rectified linear unit function element-wise'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.nn.MSELoss`creates a criterion that measures the mean squared error
    between n elements in the input x and target y'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch optim Package
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next you’ll use the `optim` package to **define an optimizer** that will update
    the weights for you. The `optim` package abstracts the idea of an optimization
    algorithm and provides implementations of commonly used optimization algorithms
    such as AdaGrad, RMSProp and Adam. We’ll use the Adam optimizer, which is one
    of the more popular optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: The first argument this optimizer takes is the tensors, which it should update.
    In the forward pass you’ll compute the predicted y by passing x to the model.
    After that, compute and print the loss. Before running the backward pass, zero
    all the gradients for the variables that will be updated using the optimizer.
    This is done because, by default, gradients are not overwritten when `.backward()`is
    called. Thereafter, call the step function on the optimizer, and this updates
    its parameters. How you’d implement this is shown below,
  prefs: []
  type: TYPE_NORMAL
- en: Custom nn Modules in PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sometimes you’ll need to build your own custom modules. In these cases you’ll
    subclass the `nn.Module`**. **You’ll then need to define a `forward`that will
    receive input tensors and produce output tensors. How to implement a two-layer
    network using `nn.Module`is shown below*.* The model is very similar to the one
    above, but the difference is you’ll use `torch.nn.Module`to create the neural
    network. The other difference is the use of `stochastic gradient descent optimizer`instead
    of Adam. You can implement a custom nn module as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all Together and Further Reading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PyTorch allows you to implement different types of layers such as [convolutional
    layers,](https://heartbeat.fritz.ai/a-beginners-guide-to-convolutional-neural-networks-cnn-cf26c5ee17ed) [recurrent
    layers](https://pytorch.org/docs/stable/index.html), and [linear layers,](https://pytorch.org/docs/stable/nn.html) among
    others. You can learn more about PyTorch from its official[ documentation](https://pytorch.org/docs/stable/).
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Derrick Mwiti](https://derrickmwiti.com/)** is a data analyst, a writer,
    and a mentor. He is driven by delivering great results in every task, and is a
    mentor at Lapid Leaders Africa.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://heartbeat.fritz.ai/introduction-to-pytorch-for-deep-learning-5b437cea90ac).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Introduction to Deep Learning with Keras](/2018/10/introduction-deep-learning-keras.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Simple Derivatives with PyTorch](/2018/05/simple-derivatives-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PyTorch Tensor Basics](/2018/05/pytorch-tensor-basics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Introduction to Deep Learning Libraries: PyTorch and Lightning AI](https://www.kdnuggets.com/introduction-to-deep-learning-libraries-pytorch-and-lightning-ai)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Complete Free PyTorch Course for Deep Learning](https://www.kdnuggets.com/2022/10/complete-free-pytorch-course-deep-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Practical Guide to Transfer Learning using PyTorch](https://www.kdnuggets.com/2023/06/practical-guide-transfer-learning-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PyTorch or TensorFlow? Comparing popular Machine Learning frameworks](https://www.kdnuggets.com/2022/02/packt-pytorch-tensorflow-comparing-popular-machine-learning-frameworks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Start Using Natural Language Processing With PyTorch](https://www.kdnuggets.com/2022/04/start-natural-language-processing-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
