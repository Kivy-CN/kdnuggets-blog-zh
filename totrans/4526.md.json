["```py\nimport os\nimport warnings\nimport math\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 6\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split \nfrom sklearn import linear_model\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_score, recall_score,roc_curve,auc, f1_score, roc_auc_score,confusion_matrix, accuracy_score, classification_report\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom scipy import ndimage\nimport seaborn as sns\n```", "```py\ntest_data = pd.read_csv('/Users/nageshsinghchauhan/Downloads/ML/kaggle/exoplanet/exoTest.csv').fillna(0)\ntrain_data = pd.read_csv('/Users/nageshsinghchauhan/Downloads/ML/kaggle/exoplanet/exoTrain.csv').fillna(0)train_data.head()\n```", "```py\ncateg = {2: 1,1: 0}\ntrain_data.LABEL = [categ[item] for item in train_data.LABEL]\ntest_data.LABEL = [categ[item] for item in test_data.LABEL]\n```", "```py\n#Reduce memory\ndef reduce_memory(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    return dftest_data = reduce_memory(test_data)#Output\nMemory usage of dataframe is 13.91 MB\nMemory usage after optimization is: 6.25 MB\nDecreased by 55.1%\n```", "```py\nplt.figure(figsize=(6,4))\ncolors = [\"0\", \"1\"]\nsns.countplot('LABEL', data=train_data, palette=colors)\nplt.title('Class Distributions \\n (0: Not Exoplanet || 1: Exoplanet)', fontsize=14)\n```", "```py\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 13, 8\nplt.title('Distribution of flux values', fontsize=10)\nplt.xlabel('Flux values')\nplt.ylabel('Flux intensity')\nplt.plot(train_data.iloc[0,])\nplt.plot(train_data.iloc[1,])\nplt.plot(train_data.iloc[2,])\nplt.plot(train_data.iloc[3,])\nplt.show()\n```", "```py\nlabels_1=[100,200,300]\nfor i in labels_1:\n    plt.hist(train_data.iloc[i,:], bins=200)\n    plt.title(\"Gaussian Histogram\")\n    plt.xlabel(\"Flux values\")\n    plt.show()\n```", "```py\nlabels_1=[16,21,25]\nfor i in labels_1:\n    plt.hist(train_data.iloc[i,:], bins=200)\n    plt.title(\"Gaussian Histogram\")\n    plt.xlabel(\"Flux values\")\n    plt.show()\n```", "```py\nx_train = train_data.drop([\"LABEL\"],axis=1)\ny_train = train_data[\"LABEL\"]   \nx_test = test_data.drop([\"LABEL\"],axis=1)\ny_test = test_data[\"LABEL\"]\n```", "```py\nx_train = normalized = normalize(x_train)\nx_test = normalize(x_test)\n```", "```py\nx_train = filtered = ndimage.filters.gaussian_filter(x_train, sigma=10)\nx_test = ndimage.filters.gaussian_filter(x_test, sigma=10)\n```", "```py\n#Feature scaling\nstd_scaler = StandardScaler()\nx_train = scaled = std_scaler.fit_transform(x_train)\nx_test = std_scaler.fit_transform(x_test)\n```", "```py\n#Dimentioanlity reduction\nfrom sklearn.decomposition import PCA\npca = PCA() \nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\ntotal=sum(pca.explained_variance_)\nk=0\ncurrent_variance=0\nwhile current_variance/total < 0.90:\n    current_variance += pca.explained_variance_[k]\n    k=k+1\n```", "```py\n#Apply PCA with n_componenets\npca = PCA(n_components=37)\nx_train = pca.fit_transform(x_train)\nx_test = pca.transform(x_test)\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.title('Exoplanet Dataset Explained Variance')\nplt.show()\n```", "```py\n#Resampling\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))sm = SMOTE(random_state=27, ratio = 1.0)\nx_train_res, y_train_res = sm.fit_sample(x_train, y_train.ravel())print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))\n```", "```py\nBefore OverSampling, counts of label '1': 37\nBefore OverSampling, counts of label '0': 5050 \n\nAfter OverSampling, counts of label '1': 5050\nAfter OverSampling, counts of label '0': 5050\n```", "```py\ndef model(classifier,dtrain_x,dtrain_y,dtest_x,dtest_y):\n    #fit the model\n    classifier.fit(dtrain_x,dtrain_y)\n    predictions = classifier.predict(dtest_x)\n\n    #Cross validation\n    accuracies = cross_val_score(estimator = classifier, X = x_train_res, y = y_train_res, cv = 5, n_jobs = -1)\n    mean = accuracies.mean()\n    variance = accuracies.std()\n    print(\"Accuracy mean: \"+ str(mean))\n    print(\"Accuracy variance: \"+ str(variance))\n\n    #Accuracy\n    print (\"\\naccuracy_score :\",accuracy_score(dtest_y,predictions))\n\n    #Classification report\n    print (\"\\nclassification report :\\n\",(classification_report(dtest_y,predictions)))\n\n    #Confusion matrix\n    plt.figure(figsize=(13,10))\n    plt.subplot(221)\n    sns.heatmap(confusion_matrix(dtest_y,predictions),annot=True,cmap=\"viridis\",fmt = \"d\",linecolor=\"k\",linewidths=3)\n    plt.title(\"CONFUSION MATRIX\",fontsize=20)\n```", "```py\nfrom sklearn.svm import SVC\nSVM_model=SVC()\nmodel(SVM_model,x_train_res,y_train_res,x_test,y_test)\n```", "```py\n#Display feature importance   \n    df1 = pd.DataFrame.from_records(dtrain_x)     \n    tmp = pd.DataFrame({'Feature': df1.columns, 'Feature importance': classifier.feature_importances_})\n    tmp = tmp.sort_values(by='Feature importance',ascending=False)\n    plt.figure(figsize = (7,4))\n    plt.title('Features importance',fontsize=14)\n    s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n    s.set_xticklabels(s.get_xticklabels(),rotation=90)\n    plt.show()\n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier\nrf_classifier = RandomForestClassifier()\nmodel(rf_classifier,x_train_res,y_train_res,x_test,y_test)\n```", "```py\nfrom tensorflow import set_random_seed\nset_random_seed(101)\nfrom sklearn.model_selection import cross_val_score\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential # initialize neural network library\nfrom keras.layers import Dense # build our layers library\ndef build_classifier():\n    classifier = Sequential() # initialize neural network\n    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train_res.shape[1]))\n    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifierclassifier = KerasClassifier(build_fn = build_classifier, epochs = 40)\naccuracies = cross_val_score(estimator = classifier, X = x_train_res, y = y_train_res, cv = 5, n_jobs = -1)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))#Accuracy mean: 0.9186138613861387\n#Accuracy variance: 0.07308084375906461\n```"]