- en: Optimizing Your LLM for Performance and Scalability
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化你的 LLM 以提高性能和可扩展性
- en: 原文：[https://www.kdnuggets.com/optimizing-your-llm-for-performance-and-scalability](https://www.kdnuggets.com/optimizing-your-llm-for-performance-and-scalability)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/optimizing-your-llm-for-performance-and-scalability](https://www.kdnuggets.com/optimizing-your-llm-for-performance-and-scalability)
- en: '![Article Main Cover](../Images/a88cf9dfeda8c9650770b5f02f9edfa1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![文章主封面](../Images/a88cf9dfeda8c9650770b5f02f9edfa1.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Large language models or LLMs have emerged as a driving catalyst in natural
    language processing. Their use-cases range from chatbots and virtual assistants
    to content generation and translation services. However, they have become one
    of the fastest-growing fields in the tech world - and we can find them all over
    the place.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型或 LLM 已成为自然语言处理的推动力。它们的使用场景从聊天机器人和虚拟助手到内容生成和翻译服务。然而，它们已经成为科技界增长最快的领域之一——我们可以在各种场景中见到它们。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT 需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: As the need for more powerful language models grows, so does the need for effective
    optimization techniques.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对更强大语言模型需求的增长，对有效优化技术的需求也在增加。
- en: 'However,many natural questions emerge:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，许多自然的问题出现了：
- en: How to improve their knowledge?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如何提升它们的知识？
- en: How to improve their general performance?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如何提升它们的整体性能？
- en: How to scale these models up?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如何扩展这些模型？
- en: The insightful presentation titled "A Survey of Techniques for Maximizing LLM
    Performance" by John Allard and Colin Jarvis from OpenAI DevDay tried to answer
    these questions. If you missed the event, you can catch the talk on [YouTube](https://www.youtube.com/watch?v=ahnGLM-RC1Y).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI DevDay 上由 John Allard 和 Colin Jarvis 主讲的题为“最大化 LLM 性能的技术调查”的深刻演讲试图回答这些问题。如果你错过了这一活动，可以在[YouTube](https://www.youtube.com/watch?v=ahnGLM-RC1Y)上观看这场讲座。
- en: This presentation provided an excellent overview of various techniques and best
    practices for enhancing the performance of your LLM applications. This article
    aims to summarize the best techniques to improve both the performance and scalability
    of our AI-powered solutions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这场演讲提供了各种技术和最佳实践的精彩概述，以提升 LLM 应用的性能。本文旨在总结改善我们 AI 驱动的解决方案性能和可扩展性的最佳技术。
- en: Understanding the Basics
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解基础知识
- en: LLMs are sophisticated algorithms engineered to understand, analyze, and produce
    coherent and contextually appropriate text. They achieve this through extensive
    training on vast amounts of linguistic data covering diverse topics, dialects,
    and styles. Thus, they can understand how human-language works.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 是精密的算法，旨在理解、分析和生成连贯且符合上下文的文本。它们通过对大量语言数据进行广泛训练，涵盖各种主题、方言和风格，从而实现这一目标。因此，它们能够理解人类语言的运作方式。
- en: 'However, when integrating these models in complex applications, there are some
    key challenges to consider:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在将这些模型集成到复杂应用中时，需要考虑一些关键挑战：
- en: Key Challenges in Optimizing LLMs
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优化 LLM 面临的主要挑战
- en: 'LLMs Accuracy: Ensuring that LLMs output is accurate and reliable information
    without hallucinations.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 的准确性：确保 LLM 输出准确且可靠的信息，没有幻觉。
- en: 'Resource Consumption: LLMs require substantial computational resources, including
    GPU power, memory and big infrastructure.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源消耗：LLM 需要大量的计算资源，包括 GPU 能力、内存和大型基础设施。
- en: 'Latency: Real-time applications demand low latency, which can be challenging
    given the size and complexity of LLMs.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 延迟：实时应用要求低延迟，这可能由于 LLM 的规模和复杂性而具有挑战性。
- en: 'Scalability: As user demand grows, ensuring the model can handle increased
    load without degradation in performance is crucial.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展性：随着用户需求的增长，确保模型能够处理增加的负载而不降低性能至关重要。
- en: Strategies for a Better Performance
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升表现的策略
- en: The first question is about “How to improve their knowledge?”
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题是关于“如何提升其知识？”
- en: Creating a partially functional LLM demo is relatively easy, but refining it
    for production requires iterative improvements. LLMs may need help with tasks
    needing deep knowledge of specific data, systems, and processes, or precise behavior.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个部分功能的LLM演示相对简单，但将其完善以投入生产需要迭代改进。LLM可能在需要深厚专业知识、系统和流程理解或精准行为的任务中遇到困难。
- en: 'Teams use prompt engineering, retrieval augmentation, and fine-tuning to address
    this. A common mistake is to assume that this process is linear and should be
    followed in a specific order. Instead, it is more effective to approach it along
    two axes, depending on the nature of the issues:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 团队使用提示工程、检索增强和微调来应对这一问题。一个常见的错误是认为这个过程是线性的，应按特定顺序进行。相反，根据问题的性质，从两个轴来处理更为有效：
- en: 'Context Optimization: Are the problems due to the model lacking access to the
    right information or knowledge?'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上下文优化：问题是否由于模型缺乏获取正确信息或知识？
- en: 'LLM Optimization: Is the model failing to generate the correct output, such
    as being inaccurate or not adhering to a desired style or format?'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM优化：模型是否未能生成正确的输出，例如不准确或不符合期望的风格或格式？
- en: '![Understanding the context requirements of our LLMs. ](../Images/3cae5f5fa83b8fc4333fc9bdccaf50f6.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![理解我们LLM的上下文需求。](../Images/3cae5f5fa83b8fc4333fc9bdccaf50f6.png)'
- en: Image by Author
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: 'To address these challenges, three primary tools can be employed, each serving
    a unique role in the optimization process:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为应对这些挑战，可以使用三种主要工具，每种工具在优化过程中都扮演着独特的角色：
- en: Prompt Engineering
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提示工程
- en: Tailoring the prompts to guide the model’s responses. For instance, refining
    a customer service bot's prompts to ensure it consistently provides helpful and
    polite responses.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 定制提示以引导模型的响应。例如，改进客服机器人提示，以确保其始终提供有用且礼貌的回答。
- en: Retrieval-Augmented Generation (RAG)
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）
- en: Enhancing the model’s context understanding through external data. For example,
    integrating a medical chatbot with a database of the latest research papers to
    provide accurate and up-to-date medical advice.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通过外部数据提升模型的上下文理解。例如，将医疗聊天机器人与最新研究论文数据库集成，以提供准确且最新的医疗建议。
- en: Fine-Tuning
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调
- en: Modifying the base model to better suit specific tasks. Just like fine-tuning
    a legal document analysis tool using a dataset of legal texts to improve its accuracy
    in summarizing legal documents.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 修改基础模型以更好地适应特定任务。就像使用法律文本数据集对法律文档分析工具进行微调，以提高其总结法律文档的准确性。
- en: The process is highly iterative, and not every technique will work for your
    specific problem. However, many techniques are additive. When you find a solution
    that works, you can combine it with other performance improvements to achieve
    optimal results.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程高度迭代，并非所有技术都会对你的具体问题有效。然而，许多技术是可以叠加的。当你找到有效的解决方案时，可以将其与其他性能改进结合，以实现**最佳结果**。
- en: Strategies for an Optimized Performance
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**优化性能的策略**'
- en: The second question is about “How to improve their general performance?”
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题是关于“如何提升其整体性能？”
- en: After having an accurate model, a second concerning point is the Inference time.
    Inference is the process where a trained language model, like GPT-3, generates
    responses to prompts or questions in real-world applications (like a chatbot).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在拥有准确的模型后，第二个令人关切的问题是推理时间。推理是一个训练好的语言模型，如GPT-3，在现实应用中生成对提示或问题的响应的过程（例如聊天机器人）。
- en: It is a critical stage where models are put to the test, generating predictions
    and responses in practical scenarios. For big LLMs like GPT-3, the computational
    demands are enormous, making optimization during inference essential.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个关键阶段，在实际场景中对模型进行测试，生成预测和响应。对于像GPT-3这样的巨大LLM，计算需求是巨大的，因此在推理期间进行优化至关重要。
- en: Consider a model like GPT-3, which has 175 billion parameters, equivalent to
    700GB of float32 data. This size, coupled with activation requirements, necessitates
    significant RAM. This is why Running GPT-3 without optimization would require
    an extensive setup.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑像GPT-3这样的模型，它有1750亿个参数，相当于700GB的float32数据。这个规模，加上激活需求，要求显著的RAM。这就是为什么在没有优化的情况下运行GPT-3需要一个庞大的设置。
- en: 'Some techniques can be used to reduce the amount of resources required to execute
    such applications:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一些技术可以用来减少执行这些应用所需的资源量：
- en: Model Pruning
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型剪枝
- en: It involves trimming non-essential parameters, ensuring only the crucial ones
    to performance remain. This can drastically reduce the model's size without significantly
    compromising its accuracy.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及修剪非关键参数，确保只有对性能至关重要的参数保留。这可以大幅减少模型的大小，而不会显著影响其准确性。
- en: Which means a significant decrease in the computational load while still having
    the same accuracy. You can find easy-to-implement pruning code in the following
    GitHub.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着计算负载显著减少，同时保持相同的准确性。你可以在以下GitHub中找到易于实现的修剪代码。
- en: Quantization
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化
- en: It is a model compression technique that converts the weights of a LLM from
    high-precision variables to lower-precision ones. This means we can reduce the
    32-bit floating-point numbers to lower precision formats like 16-bit or 8-bit,
    which are more memory-efficient. This can drastically reduce the memory footprint
    and improve inference speed.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种模型压缩技术，将LLM的权重从高精度变量转换为低精度变量。这意味着我们可以将32位浮点数减少到16位或8位等更节省内存的格式，这可以大幅减少内存占用并提高推理速度。
- en: LLMs can be easily loaded in a quantized manner using HuggingFace and bitsandbytes.
    This allows us to execute and fine-tune LLMs in lower-power resources.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs可以通过HuggingFace和bitsandbytes以量化方式轻松加载。这使我们能够在低功耗资源上执行和微调LLMs。
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Distillation
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 蒸馏
- en: It is the process of training a smaller model (student) to mimic the performance
    of a larger model (also referred to as a teacher). This process involves training
    the student model to mimic the teacher's predictions, using a combination of the
    teacher's output logits and the true labels. By doing so, we can a achieve similar
    performance with a fraction of the resource requirement.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这是训练一个较小模型（学生）以模拟较大模型（也称为教师）性能的过程。这个过程涉及训练学生模型以模拟教师的预测，使用教师的输出logits和真实标签的组合。通过这样做，我们可以在资源需求的很小一部分中实现类似的性能。
- en: The idea is to transfer the knowledge of larger models to smaller ones with
    simpler architecture. One of the most known examples is Distilbert.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是将较大模型的知识转移到具有更简单架构的较小模型上。最著名的例子之一是Distilbert。
- en: This model is the result of mimicking the performance of Bert. It is a smaller
    version of BERT that retains 97% of its language understanding capabilities while
    being 60% faster and 40% smaller in size.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是模拟Bert性能的结果。它是BERT的一个较小版本，保留了97%的语言理解能力，同时速度快60%，体积小40%。
- en: Techniques for Scalability
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可扩展性技术
- en: The third question is about “How to scale these models up?”
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个问题是关于“如何扩展这些模型？”
- en: 'This step is often crucial. An operational system can behave very differently
    when used by a handful of users versus when it scales up to accommodate intensive
    usage. Here are some techniques to address this challenge:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步通常至关重要。当一个系统被少量用户使用时，它的行为可能与在扩展以应对密集使用时大相径庭。以下是一些应对这一挑战的技术：
- en: Load-balancing
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 负载均衡
- en: This approach distributes incoming requests efficiently, ensuring optimal use
    of computational resources and dynamic response to demand fluctuations. For instance,
    to offer a widely-used service like ChatGPT across different countries, it is
    better to deploy multiple instances of the same model.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有效分配传入请求，确保计算资源的最佳利用和对需求波动的动态响应。例如，为了在不同国家提供像ChatGPT这样的广泛使用服务，最好部署多个相同模型的实例。
- en: 'Effective load-balancing techniques include:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的负载均衡技术包括：
- en: 'Horizontal Scaling: Add more model instances to handle increased load. Use
    container orchestration platforms like Kubernetes to manage these instances across
    different nodes.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 横向扩展：添加更多模型实例以应对增加的负载。使用像Kubernetes这样的容器编排平台来管理不同节点上的这些实例。
- en: 'Vertical Scaling: Upgrade existing machine resources, such as CPU and memory.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 垂直扩展：升级现有机器资源，例如CPU和内存。
- en: Sharding
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分片
- en: Model sharding distributes segments of a model across multiple devices or nodes,
    enabling parallel processing and significantly reducing latency. Fully Sharded
    Data Parallelism (FSDP) offers the key advantage of utilizing a diverse array
    of hardware, such as GPUs, TPUs, and other specialized devices in several clusters.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 模型分片将模型的片段分布在多个设备或节点上，实现并行处理并显著减少延迟。完全分片数据并行（FSDP）提供了利用各种硬件的关键优势，如GPU、TPU和其他在多个集群中的专用设备。
- en: This flexibility allows organizations and individuals to optimize their hardware
    resources according to their specific needs and budget.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这种灵活性允许组织和个人根据他们的具体需求和预算来优化硬件资源。
- en: Caching
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缓存
- en: Implementing a caching mechanism reduces the load on your LLM by storing frequently
    accessed results, which is especially beneficial for applications with repetitive
    queries. Caching these frequent queries can significantly save computational resources
    by eliminating the need to repeatedly process the same requests over.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 实现缓存机制可以通过存储频繁访问的结果来减少对 LLM 的负载，这对于有重复查询的应用程序尤其有利。缓存这些频繁的查询可以通过消除重复处理相同请求的需要，从而显著节省计算资源。
- en: Additionally, batch processing can optimize resource usage by grouping similar
    tasks.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，批处理可以通过将类似任务分组来优化资源使用。
- en: Conclusion
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: For those building applications reliant on LLMs, the techniques discussed here
    are crucial for maximizing the potential of this transformative technology. Mastering
    and effectively applying strategies to a more accurate output of our model, optimize
    its performance, and allowing scaling up are essential steps in evolving from
    a promising prototype to a robust, production-ready model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些依赖 LLM 的应用程序来说，这里讨论的技术对于最大化这种变革性技术的潜力至关重要。掌握并有效应用策略以获得更准确的模型输出、优化其性能以及实现扩展，是从一个有前景的原型发展为一个强大的、生产就绪的模型的重要步骤。
- en: To fully understand these techniques, I highly recommend getting a deeper detail
    and starting to experiment with them in your LLM applications for optimal results.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要充分理解这些技术，我强烈推荐深入了解并开始在你的 LLM 应用程序中实验这些技术，以获得最佳结果。
- en: '**[](https://www.linkedin.com/in/josep-ferrer-sanchez/)**[Josep Ferrer](https://www.linkedin.com/in/josep-ferrer-sanchez)****
    is an analytics engineer from Barcelona. He graduated in physics engineering and
    is currently working in the data science field applied to human mobility. He is
    a part-time content creator focused on data science and technology. Josep writes
    on all things AI, covering the application of the ongoing explosion in the field.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**[](https://www.linkedin.com/in/josep-ferrer-sanchez/)**[Josep Ferrer](https://www.linkedin.com/in/josep-ferrer-sanchez)****
    是一位来自巴塞罗那的分析工程师。他毕业于物理工程专业，目前在应用于人类流动的数据科学领域工作。他是一个兼职内容创作者，专注于数据科学和技术。Josep 撰写关于人工智能的文章，涵盖了该领域的持续爆炸性发展。'
- en: More On This Topic
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Strategies for Optimizing Performance and Costs When Using Large…](https://www.kdnuggets.com/strategies-for-optimizing-performance-and-costs-when-using-large-language-models-in-the-cloud)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用大型语言模型时优化性能和成本的策略](https://www.kdnuggets.com/strategies-for-optimizing-performance-and-costs-when-using-large-language-models-in-the-cloud)'
- en: '[Optimizing Python Code Performance: A Deep Dive into Python Profilers](https://www.kdnuggets.com/2023/02/optimizing-python-code-performance-deep-dive-python-profilers.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化 Python 代码性能：深入探讨 Python 性能分析器](https://www.kdnuggets.com/2023/02/optimizing-python-code-performance-deep-dive-python-profilers.html)'
- en: '[Web LLM: Bring LLM Chatbots to the Browser](https://www.kdnuggets.com/2023/05/webllm-bring-llm-chatbots-browser.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Web LLM：将 LLM 聊天机器人带到浏览器](https://www.kdnuggets.com/2023/05/webllm-bring-llm-chatbots-browser.html)'
- en: '[Scalability Challenges & Strategies in Data Science](https://www.kdnuggets.com/scalability-challenges-strategies-in-data-science)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中的可扩展性挑战与策略](https://www.kdnuggets.com/scalability-challenges-strategies-in-data-science)'
- en: '[Optimizing Data Storage: Exploring Data Types and Normalization in SQL](https://www.kdnuggets.com/optimizing-data-storage-exploring-data-types-and-normalization-in-sql)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化数据存储：探索 SQL 中的数据类型和规范化](https://www.kdnuggets.com/optimizing-data-storage-exploring-data-types-and-normalization-in-sql)'
- en: '[Optimizing Genes with a Genetic Algorithm](https://www.kdnuggets.com/2022/04/optimizing-genes-genetic-algorithm.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用遗传算法优化基因](https://www.kdnuggets.com/2022/04/optimizing-genes-genetic-algorithm.html)'
