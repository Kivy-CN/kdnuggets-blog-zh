- en: Zero-shot Learning, Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零样本学习解释
- en: 原文：[https://www.kdnuggets.com/2022/12/zeroshot-learning-explained.html](https://www.kdnuggets.com/2022/12/zeroshot-learning-explained.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/12/zeroshot-learning-explained.html](https://www.kdnuggets.com/2022/12/zeroshot-learning-explained.html)
- en: '![Zero-shot Learning, Explained](../Images/70b20b91eb586e6dad9e5c6b7676de05.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![零样本学习解释](../Images/70b20b91eb586e6dad9e5c6b7676de05.png)'
- en: '[Bruce Warrington](https://unsplash.com/@brucebmax) via Unsplash'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[布鲁斯·沃灵顿](https://unsplash.com/@brucebmax) 通过 Unsplash'
- en: The reason why machine learning models in general are becoming smarter is due
    to their dependency on using labeled data to help them discern between two similar
    objects.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型之所以变得更智能，是因为它们依赖于使用标记数据来帮助它们区分两个相似的物体。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的IT工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: However, without these labeled datasets, you will encounter major obstacles
    when creating the most effective and trustworthy machine-learning model. Labeled
    datasets during the training phase of a model are important.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在没有这些标记数据集的情况下，你在创建最有效和可信赖的机器学习模型时会遇到重大障碍。模型训练阶段的标记数据集是重要的。
- en: Deep learning has been widely used to solve tasks such as Computer vision using
    supervised learning. However, as with many things in life, it comes with restrictions.
    Supervised classification requires a high quantity and quality of labeled training
    data in order to produce a robust model. This means that the classifying model
    cannot handle unseen classes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已被广泛用于解决计算机视觉等任务，采用监督学习。然而，就像生活中的许多事物一样，它也有其限制。监督分类需要大量高质量的标记训练数据，以产生一个强健的模型。这意味着分类模型无法处理未见过的类别。
- en: And we all know how much computational power, re-training, time, and money it
    takes to train a deep learning model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都知道训练深度学习模型需要多少计算能力、再训练、时间和金钱。
- en: But can a model still be able to discern between two objects without having
    used training data? Yes, it’s called zero-shot learning. Zero-shot learning is
    a model's ability to be able to complete a task without having received or used
    any training examples.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，一个模型是否仍然能够在没有使用训练数据的情况下区分两个物体？是的，这就是所谓的零样本学习。零样本学习是指一个模型在没有收到或使用任何训练示例的情况下完成任务的能力。
- en: Humans are naturally capable of zero-shot learning without having to put much
    effort in. Our brains already store dictionaries and allow us to differentiate
    objects by looking at their physical properties due to our current knowledge base.
    We can use this knowledge base to see the similarities and differences between
    objects and find the link between them.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 人类天生具备零样本学习的能力，不需要付出太多努力。我们的脑海中已经储存了词典，并且能够通过观察物体的物理属性来区分物体，这是由于我们现有的知识基础。我们可以利用这一知识基础来查看物体之间的相似性和差异性，并找到它们之间的联系。
- en: For example, let’s say we are trying to build a classification model on animal
    species. According to [OurWorldInData](https://ourworldindata.org/biodiversity-and-wildlife),
    there were 2.13 million species calculated in 2021\. Therefore, if we want to
    create the most effective classification model for animal species, we would need
    2.13 million different classes. Also needed will be a lot of data. High quantity
    and quality data are hard to come across.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们正在尝试建立一个动物物种的分类模型。根据 [OurWorldInData](https://ourworldindata.org/biodiversity-and-wildlife)，2021年计算出有213万种物种。因此，如果我们想为动物物种创建最有效的分类模型，我们需要213万种不同的类别。同时也需要大量数据。高质量和数量的数据很难获得。
- en: So how does zero-shot learning solve this problem?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 那么零样本学习如何解决这个问题呢？
- en: Because zero-shot learning does not require the model to have learned the training
    data and how to classify classes, it allows us to rely less on the model’s need
    for labeled data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因为零样本学习不要求模型学习训练数据和分类类别，它使我们可以减少对模型需要标注数据的依赖。
- en: Zero-shot Learning Process
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零样本学习过程
- en: The following is what your data will need to consist of in order to proceed
    with zero-shot learning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是你的数据需要包含的内容，以便进行零样本学习。
- en: Seen Classes
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 已见类别
- en: This consists of the data classes that have been previously used to train a
    model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括那些以前用于训练模型的数据类别。
- en: Unseen Classes
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 未见类别
- en: This consists of the data classes that have NOT been used to train a model and
    the new zero-shot learning model will generalize.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括那些没有用来训练模型的数据类别，新零样本学习模型将进行泛化。
- en: Auxiliary Information
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 辅助信息
- en: As the data in the unseen classes are not labeled, zero-shot learning will require
    auxiliary information in order to learn, and find correlations, links, and properties.
    This can be in the form of word embeddings, descriptions, and semantic information.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于未见类别中的数据没有标签，零样本学习将需要辅助信息以便进行学习，发现关联、链接和属性。这可以是词嵌入、描述和语义信息的形式。
- en: Zero-shot Learning Methods
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零样本学习方法
- en: 'Zero-shot learning is typically used in:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本学习通常用于：
- en: Classifier-based methods
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于分类器的方法
- en: Instance-based methods
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于实例的方法
- en: Stages
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阶段
- en: 'Zero-shot learning is used to build models for classes that do not train using
    labeled data, therefore it requires these two stages:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本学习用于为不使用标注数据进行训练的类别构建模型，因此它需要这两个阶段：
- en: 1\. Training
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1\. 训练
- en: The training stage is the process of the learning method trying to capture as
    much knowledge as possible about the qualities of the data. We can view this as
    the learning phase.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 训练阶段是学习方法尽可能多地捕捉数据特征的过程。我们可以将其视为学习阶段。
- en: 2\. Inference
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2\. 推断
- en: During the inference stage, all the learned knowledge from the training stage
    is applied and utilized in order to classify examples into a new set of classes.
    We can view this as the making predictions phase.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在推断阶段，所有从训练阶段学到的知识都会被应用和利用，以便将样本分类到新的类别集合中。我们可以将其视为预测阶段。
- en: How Does it Work?
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其工作原理是什么？
- en: 'The knowledge from the seen classes will be transferred to the unseen classes
    in a high-dimensional vector space; this is called semantic space. For example,
    in image classification the semantic space along with the image will undergo two
    steps:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 从已见类别中获得的知识将被转移到未见类别中，这发生在一个高维向量空间中；这被称为语义空间。例如，在图像分类中，语义空间以及图像将经历两个步骤：
- en: 1\. Joint embedding space
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1\. 联合嵌入空间
- en: This is where the semantic vectors and the vectors of the visual feature are
    projected to.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是语义向量和视觉特征向量被投射到的地方。
- en: 2\. Highest similarity
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2\. 最高相似度
- en: This is where features are matched against those of an unseen class.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这是特征与未见类别的特征进行匹配的地方。
- en: Zero-shot learning stages with image classification
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像分类中的零样本学习阶段
- en: To help understand the process with the two stages (training and inference),
    let’s apply them in the use of image classification.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助理解这两个阶段（训练和推断）的过程，我们将它们应用于图像分类。
- en: Training
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: '![Zero-shot Learning, Explained](../Images/506d733b30cd5456a07f167816685979.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![零样本学习，解释](../Images/506d733b30cd5456a07f167816685979.png)'
- en: '[Jari Hytönen](https://unsplash.com/@jarispics) via Unsplash'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[Jari Hytönen](https://unsplash.com/@jarispics) 通过 Unsplash'
- en: As a human being, if you were to read the text on the right in the image above,
    you would instantly assume that there are 4 kittens in a brown basket. But let’s
    say you have no idea what a ‘kitten’ is. You will assume that there is a brown
    basket with 4 things inside, which are called ‘kittens’. Once you come across
    more images that contain something that looks like a ‘kitten’, you will be able
    to differentiate a ‘kitten’ from other animals.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人类，如果你阅读上图右侧的文本，你会立即认为有4只小猫在一个棕色的篮子里。但假设你对“小猫”没有任何概念。你会假设篮子里有4样东西，被称为“小猫”。一旦你看到更多包含类似“小猫”的图像，你就能够区分“小猫”和其他动物。
- en: This is what happens when you use [Contrastive Language-Image Pretraining](https://openai.com/blog/clip/)
    (CLIP) by OpenAI for zero-shot learning in image classification. It is known as
    auxiliary information.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用 [对比语言-图像预训练](https://openai.com/blog/clip/)（CLIP）进行图像分类的零样本学习时，就会发生这种情况。这被称为辅助信息。
- en: You might be thinking, ‘well that’s just labeled data’. I understand why you
    would think that, but they are not. Auxiliary information is not labels of the
    data, they are a form of supervision to help the model learn during the training
    stage.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，“那只是标记数据”。我理解你为什么会这么想，但它们并不是。辅助信息不是数据的标签，而是一种监督方式，帮助模型在训练阶段进行学习。
- en: When a zero-shot learning model sees a sufficient amount of image-text pairings,
    it will be able to differentiate and understand phrases and how they correlate
    with certain patterns in the images. Using the CLIP technique ‘contrastive learning’,
    the zero-shot learning model has been able to accumulate a good knowledge base
    to be able to make predictions on classification tasks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个零样本学习模型看到足够数量的图像-文本配对时，它将能够区分和理解短语及其如何与图像中的某些模式相关联。通过使用CLIP技术的“对比学习”，零样本学习模型已经积累了良好的知识基础，从而能够在分类任务上进行预测。
- en: 'This is a summary of the CLIP approach where they train an image encoder and
    a text encoder together in order to predict the correct pairings of a batch of
    (image, text) training examples. Please see the image below:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是CLIP方法的总结，其中训练了一个图像编码器和一个文本编码器，以预测一批（图像，文本）训练示例的正确配对。请参见下图：
- en: '![Zero-shot Learning, Explained](../Images/7215f67e2b8545551db2af0610651bb4.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![零样本学习解析](../Images/7215f67e2b8545551db2af0610651bb4.png)'
- en: '[Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[从自然语言监督中学习可转移的视觉模型](https://arxiv.org/pdf/2103.00020.pdf)'
- en: Inference
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推断
- en: Once the model has gone through the training stage, it has a good knowledge
    base of image-text pairing and can now be used to make predictions. But before
    we can get right into making predictions, we need to set up the classification
    task by creating a list of all possible labels that the model could output.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型完成了训练阶段，它就具备了图像-文本配对的良好知识基础，并且现在可以用来进行预测。但在我们真正开始预测之前，我们需要通过创建一个可能的标签列表来设置分类任务。
- en: For example, sticking with the image classification task on animal species,
    we will need a list of all the species of animals. Each one of these labels will
    be encoded, T? to T? using the pretrained text encoder that occurred in the training
    stage.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，继续进行动物物种的图像分类任务，我们需要一个所有动物物种的列表。这些标签中的每一个都将被编码，从T?到T?，使用在训练阶段发生的预训练文本编码器。
- en: Once the labels have been encoded, we can input images through the pre-trained
    image encoder. We will use the distance metric cosine similarity to compute the
    similarities between the image encoding and each text label encoding.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦标签被编码，我们可以通过预训练的图像编码器输入图像。我们将使用距离度量余弦相似度来计算图像编码与每个文本标签编码之间的相似性。
- en: The classification of the image is done based on the label with the greatest
    similarity to the image. And that is how zero-shot learning is achieved, specifically
    in image classification.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图像的分类是基于与图像最相似的标签进行的。这就是零样本学习在图像分类中的实现方式。
- en: The Importance of Zero-shot Learning
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零样本学习的重要性
- en: Scarcity of Data
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据稀缺性
- en: As mentioned before, high quantity and quality data are hard to get your hands
    on. Unlike humans who already possess the zero-shot learning ability, machines
    require input labeled data to learn and then be able to adapt to variances that
    may naturally occur.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，高数量和高质量的数据很难获得。与已经具备零样本学习能力的人类不同，机器需要输入标记数据以进行学习，然后才能适应可能自然发生的变化。
- en: If we look at the animal species example, there were so many. And as the number
    of categories continues to grow in different domains, it will take a lot of work
    to keep up with collecting annotated data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看动物物种的例子，会发现有很多种。随着不同领域类别数量的不断增长，跟上收集标注数据的工作将会非常繁重。
- en: Due to this, zero-shot learning has become more valuable to us. More and more
    researchers are interested in automatic attribute recognition to compensate for
    the lack of available data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，零样本学习对我们变得越来越有价值。越来越多的研究人员对自动属性识别感兴趣，以弥补数据缺乏的问题。
- en: Data Labeling
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据标注
- en: Another benefit of zero-shot learning is its data labeling properties. Data
    labeling can be labor-intensive and very tedious, and due to this, it can lead
    to errors during the process. Data labeling requires experts, such as medical
    professionals who are working on a biomedical dataset, which is highly expensive
    and time-consuming.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本学习的另一个好处是其数据标注特性。数据标注可能非常耗费劳力且十分枯燥，因此在过程中可能会导致错误。数据标注需要专家，比如在生物医学数据集上工作的医疗专业人士，这既昂贵又耗时。
- en: Wrapping Up
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'Zero-shot learning is becoming more popular due to the above limitations of
    data. There are a few papers I would recommend you read if you are interested
    in its abilities:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本学习因数据的上述限制而变得越来越流行。如果你对其能力感兴趣，我推荐你阅读以下几篇论文：
- en: '[Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从自然语言监督中学习可转移的视觉模型](https://arxiv.org/pdf/2103.00020.pdf)'
- en: '[Improving Semantic Embedding Consistency by Metric Learning for Zero-Shot
    Classification](https://arxiv.org/pdf/1607.08085.pdf)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过度量学习改进零样本分类的一致性](https://arxiv.org/pdf/1607.08085.pdf)'
- en: '[Learning a Deep Embedding Model for Zero-Shot Learning](https://arxiv.org/pdf/1611.05088.pdf)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习用于零样本学习的深度嵌入模型](https://arxiv.org/pdf/1611.05088.pdf)'
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and Freelance Technical Writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** 是一位数据科学家和自由技术撰稿人。她特别关注提供数据科学职业建议或教程，以及围绕数据科学的理论知识。她还希望探索人工智能在延长人类生命方面的不同方式。她是一个热衷于学习的人，寻求拓宽她的技术知识和写作技能，同时帮助指导他人。'
- en: More On This Topic
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Machine Learning Key Terms, Explained](https://www.kdnuggets.com/2016/05/machine-learning-key-terms-explained.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习关键术语解析](https://www.kdnuggets.com/2016/05/machine-learning-key-terms-explained.html)'
- en: '[Deep Learning Key Terms, Explained](https://www.kdnuggets.com/2016/10/deep-learning-key-terms-explained.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习关键术语解析](https://www.kdnuggets.com/2016/10/deep-learning-key-terms-explained.html)'
- en: '[KDnuggets News, November 16: How LinkedIn Uses Machine Learning •…](https://www.kdnuggets.com/2022/n45.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets新闻，11月16日：LinkedIn如何使用机器学习 •…](https://www.kdnuggets.com/2022/n45.html)'
- en: '[7 Best Libraries for Machine Learning Explained](https://www.kdnuggets.com/2023/01/7-best-libraries-machine-learning-explained.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7个最佳机器学习库解析](https://www.kdnuggets.com/2023/01/7-best-libraries-machine-learning-explained.html)'
- en: '[5 Machine Learning Models Explained in 5 Minutes](https://www.kdnuggets.com/5-machine-learning-models-explained-in-5-minutes)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5种机器学习模型在5分钟内解析](https://www.kdnuggets.com/5-machine-learning-models-explained-in-5-minutes)'
- en: '[Machine Learning Algorithms Explained in Less Than 1 Minute Each](https://www.kdnuggets.com/2022/07/machine-learning-algorithms-explained-less-1-minute.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个机器学习算法在1分钟内解析](https://www.kdnuggets.com/2022/07/machine-learning-algorithms-explained-less-1-minute.html)'
