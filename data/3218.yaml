- en: Top 10 Machine Learning Algorithms for Beginners
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初学者十大机器学习算法
- en: 原文：[https://www.kdnuggets.com/2017/10/top-10-machine-learning-algorithms-beginners.html/2](https://www.kdnuggets.com/2017/10/top-10-machine-learning-algorithms-beginners.html/2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/10/top-10-machine-learning-algorithms-beginners.html/2](https://www.kdnuggets.com/2017/10/top-10-machine-learning-algorithms-beginners.html/2)
- en: 'V. Unsupervised learning algorithms:'
  id: totrans-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V. 无监督学习算法：
- en: '**6\. Apriori**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**6\. Apriori**'
- en: 'The Apriori algorithm is used in a transactional database to mine frequent
    itemsets and then generate association rules. It is popularly used in market basket
    analysis, where one checks for combinations of products that frequently co-occur
    in the database. In general, we write the association rule for ‘if a person purchases
    item X, then he purchases item Y’ as : X -> Y.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Apriori算法用于事务数据库中，以挖掘频繁项集并生成关联规则。它在市场篮子分析中被广泛使用，检查数据库中经常共同出现的产品组合。通常，我们将“如果一个人购买了X商品，那么他也会购买Y商品”的关联规则写作：X
    -> Y。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的IT需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Example: if a person purchases milk and sugar, then he is likely to purchase
    coffee powder. This could be written in the form of an association rule as: {milk,sugar}
    -> coffee powder. Association rules are generated after crossing the threshold
    for support and confidence.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：如果一个人购买了牛奶和糖，那么他很可能会购买咖啡粉。这可以写成关联规则的形式：{牛奶,糖} -> 咖啡粉。关联规则在支持度和置信度达到阈值后生成。
- en: '![](../Images/644f10b1b1ef53a08eec4a0458b93880.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/644f10b1b1ef53a08eec4a0458b93880.png)'
- en: 'Figure 5: Formulae for support, confidence and lift for the association rule
    X->Y. [Source](http://chem-eng.utoronto.ca/~datamining/dmc/association_rules.htm)The
    Support measure helps prune the number of candidate itemsets to be considered
    during frequent itemset generation. This support measure is guided by the Apriori
    principle. The Apriori principle states that if an itemset is frequent, then all
    of its subsets must also be frequent.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：关联规则X->Y的支持度、置信度和提升度公式。[来源](http://chem-eng.utoronto.ca/~datamining/dmc/association_rules.htm)支持度度量有助于减少在频繁项集生成过程中需要考虑的候选项集的数量。这个支持度度量由Apriori原理指导。Apriori原理表明，如果一个项集是频繁的，那么它的所有子集也必须是频繁的。
- en: '**7\. K-means**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**7\. K-means**'
- en: K-means is an iterative algorithm that groups similar data into clusters.It
    calculates the centroids of k clusters and assigns a data point to that cluster
    having least distance between its centroid and the data point.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: K-means是一种迭代算法，将相似的数据分组到聚类中。它计算k个聚类的中心点，并将数据点分配给与其中心点距离最小的聚类。
- en: '![](../Images/fffb3ddbea2ab8b4c211f642866fbe29.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fffb3ddbea2ab8b4c211f642866fbe29.png)'
- en: 'Figure 6: Steps of the K-means algorithm. [Source](https://www.packtpub.com/books/content/clustering-and-other-unsupervised-learning-methods)*Step
    1: k-means initialization:*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：K-means算法的步骤。[来源](https://www.packtpub.com/books/content/clustering-and-other-unsupervised-learning-methods)*步骤1：K-means初始化：*
- en: a) Choose a value of k. Here, let us take k=3.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: a) 选择一个k的值。这里，我们取k=3。
- en: b) Randomly assign each data point to any of the 3 clusters.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: b) 随机将每个数据点分配到3个聚类中的任何一个。
- en: c) Compute cluster centroid for each of the clusters. The red, blue and green
    stars denote the centroids for each of the 3 clusters.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: c) 计算每个聚类的中心点。红色、蓝色和绿色的星星分别表示3个聚类的中心点。
- en: '*Step 2: Associating each observation to a cluster:*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤2：将每个观察分配到一个聚类中：*'
- en: Reassign each point to the closest cluster centroid. Here, the upper 5 points
    got assigned to the cluster with the blue colour centroid. Follow the same procedure
    to assign points to the clusters containing the red and green colour centroid.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个点重新分配到最近的簇质心。这里，上面的5个点被分配到具有蓝色质心的簇。按照相同的程序将点分配到包含红色和绿色质心的簇中。
- en: '*Step 3: Recalculating the centroids:*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 3: 重新计算质心：*'
- en: Calculate the centroids for the new clusters. The old centroids are shown by
    gray stars while the new centroids are the red, green and blue stars.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 计算新簇的质心。旧的质心由灰色星星表示，而新的质心是红色、绿色和蓝色的星星。
- en: '*Step 4: Iterate, then exit if unchanged.*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 4: 迭代，然后如果没有变化则退出。*'
- en: Repeat steps 2-3 until there is no switching of points from one cluster to another.
    Once there is no switching for 2 consecutive steps, exit the k-means algorithm.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 重复步骤2-3，直到没有点从一个簇切换到另一个簇。一旦连续2步没有切换，退出k-means算法。
- en: '**8\. PCA**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**8\. PCA**'
- en: Principal Component Analysis (PCA) is used to make data easy to explore and
    visualize by reducing the number of variables. This is done by capturing the maximum
    variance in the data into a new co-ordinate system with axes called ‘principal
    components’. Each component is a linear combination of the original variables
    and is orthogonal to one another. Orthogonality between components indicates that
    the correlation between these components is zero.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）用于通过减少变量数量来使数据易于探索和可视化。这是通过将数据中的最大方差捕捉到一个新的坐标系统中，该系统的轴称为‘主成分’来完成的。每个成分是原始变量的线性组合，并且彼此正交。成分之间的正交性表示这些成分之间的相关性为零。
- en: The first principal component captures the direction of the maximum variability
    in the data. The second principal component captures the remaining variance in
    the data but has variables uncorrelated with the first component. Similarly, all
    successive principal components (PC3, PC4 and so on) capture the remaining variance
    while being uncorrelated with the previous component.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个主成分捕捉数据中最大变异性的方向。第二个主成分捕捉剩余的方差，但与第一个成分的变量不相关。类似地，所有后续的主成分（PC3、PC4等）捕捉剩余的方差，同时与前一个成分不相关。
- en: '![](../Images/d44c1bcdae7f89b678e8c00568b3fdc4.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d44c1bcdae7f89b678e8c00568b3fdc4.png)'
- en: 'Figure 7: The 3 original variables (genes) are reduced to 2 new variables termed
    principal components (PC''s). [Source](http://www.nlpca.org/pca_principal_component_analysis.html)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：3个原始变量（基因）被减少为2个新变量，称为主成分（PC）。[来源](http://www.nlpca.org/pca_principal_component_analysis.html)
- en: 'VI. Ensemble learning techniques:'
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI. 集成学习技术：
- en: Ensembling means combining the results of multiple learners (classifiers) for
    improved results, by voting or averaging. Voting is used during classification
    and averaging is used during regression. The idea is that ensembles of learners
    perform better than single learners.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法意味着通过投票或平均来结合多个学习者（分类器）的结果，以提高结果。分类时使用投票，回归时使用平均。其思想是，集成学习者的表现优于单一学习者。
- en: 'There are 3 types of ensembling algorithms: Bagging, Boosting and Stacking.
    We are not going to cover ‘stacking’ here, but if you’d like a detailed explanation
    of it, let me know in the comments section below, and I can write a separate blog
    on it.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有3种集成算法：袋装、提升和堆叠。我们这里不讨论‘堆叠’，但如果你想了解详细解释，请在评论区告诉我，我可以写一篇单独的博客。
- en: '**9\. Bagging with Random Forests**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**9\. 使用随机森林进行袋装**'
- en: Random Forest (multiple learners) is an improvement over bagged decision trees
    (a single learner).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林（多个学习者）比袋装决策树（单一学习者）有更好的改进。
- en: 'Bagging: The first step in bagging is to create multiple models with datasets
    created using the Bootstrap Sampling method. In Bootstrap Sampling, each generated
    trainingset is composed of random subsamples from the original dataset. Each of
    these trainingsets is of the same size as the original dataset, but some records
    repeat multiple times and some records do not appear at all. Then, the entire
    original dataset is used as the testset. Thus, if the size of the original dataset
    is N, then the size of each generated trainingset is also N, with the number of
    unique records being about (2N/3); the size of the testset is also N.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging：Bagging 的第一步是使用 Bootstrap Sampling 方法创建多个模型。在 Bootstrap Sampling 中，每个生成的训练集都是由原始数据集中的随机子样本组成的。这些训练集的大小与原始数据集相同，但一些记录会重复出现，而一些记录则根本不出现。然后，整个原始数据集被用作测试集。因此，如果原始数据集的大小是
    N，那么每个生成的训练集的大小也是 N，唯一记录的数量约为（2N/3）；测试集的大小也是 N。
- en: 'The second step in bagging is to create multiple models by using the same algorithm
    on the different generated trainingsets. In this case, let us discuss Random Forest.
    Unlike a decision tree, where each node is split on the best feature that minimizes
    error, in random forests, we choose a random selection of features for constructing
    the best split. The reason for randomness is: even with bagging, when decision
    trees choose a best feature to split on, they end up with similar structure and
    correlated predictions. But bagging after splitting on a random subset of features
    means less correlation among predictions from subtrees.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging 的第二步是使用相同的算法对不同生成的训练集创建多个模型。在这种情况下，我们讨论随机森林。与决策树不同，在决策树中，每个节点根据最小化错误的最佳特征进行分裂，而在随机森林中，我们选择特征的随机选择来构建最佳分裂。随机性的原因是：即使在
    bagging 中，当决策树选择最佳特征进行分裂时，它们最终会具有相似的结构和相关的预测。但在对特征的随机子集进行分裂后进行 bagging，这意味着子树之间的预测相关性较小。
- en: The number of features to be searched at each split point is specified as a
    parameter to the random forest algorithm.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个分裂点搜索的特征数量作为参数指定给随机森林算法。
- en: Thus, in bagging with Random Forest, each tree is constructed using a random
    sample of records and each split is constructed using a random sample of predictors.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在随机森林的 bagging 中，每棵树都是使用记录的随机样本构建的，每次分裂是使用预测变量的随机样本构建的。
- en: '**10\. Boosting with AdaBoost**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**10\. 使用 AdaBoost 进行提升**'
- en: a) Bagging is a parallel ensemble because each model is built independently.
    On the other hand, boosting is a sequential ensemble where each model is built
    based on correcting the misclassifications of the previous model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: a) Bagging 是并行集成，因为每个模型都是独立构建的。另一方面，提升是顺序集成，其中每个模型是基于纠正前一个模型的误分类来构建的。
- en: b) Bagging mostly involves ‘simple voting’, where each classifier votes to obtain
    a final outcome– one that is determined by the majority of the parallel models;
    boosting involves ‘weighted voting’, where each classifier votes to obtain a final
    outcome which is determined by the majority– but the sequential models were built
    by assigning greater weights to misclassified instances of the previous models.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: b) Bagging 主要涉及“简单投票”，即每个分类器投票以获得最终结果——该结果由平行模型的多数决定；提升涉及“加权投票”，即每个分类器投票以获得最终结果——但顺序模型是通过对前一个模型的误分类实例分配更大权重来构建的。
- en: Adaboost stands for Adaptive Boosting.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Adaboost 代表自适应提升。
- en: '![](../Images/480921b711d02ec9e399316c1876264d.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/480921b711d02ec9e399316c1876264d.png)'
- en: 'Figure 9: Adaboost for a decision tree. [Source](https://sebastianraschka.com/faq/docs/bagging-boosting-rf.html)In
    Figure 9, steps 1, 2, 3 involve a weak learner called a decision stump (a 1-level
    decision tree making a prediction based on the value of only 1 input feature;
    a decision tree with its root immediately connected to its leaves). The process
    of constructing weak learners continues until a user-defined number of weak learners
    has been constructed or until there is no further improvement while training.
    Step 4 combines the 3 decision stumps of the previous models (and thus has 3 splitting
    rules in the decision tree).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：决策树的 Adaboost。[来源](https://sebastianraschka.com/faq/docs/bagging-boosting-rf.html)在图
    9 中，步骤 1、2、3 涉及一种称为决策树桩的弱学习器（一个基于仅 1 个输入特征的值进行预测的 1 级决策树；根节点直接连接到叶节点的决策树）。构建弱学习器的过程持续进行，直到构建出用户定义数量的弱学习器或在训练过程中没有进一步的改进。步骤
    4 结合了之前模型的 3 个决策树桩（因此在决策树中有 3 个分裂规则）。
- en: '*Step 1: Start with 1 decision tree stump to make a decision on 1 input variable:*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 1：从 1 个决策树桩开始，对 1 个输入变量做出决策：*'
- en: The size of the data points show that we have applied equal weights to classify
    them as a circle or triangle. The decision stump has generated a horizontal line
    in the top half to classify these points. We can see that there are 2 circles
    incorrectly predicted as triangles. Hence, we will assign higher weights to these
    2 circles and apply another decision stump.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据点的大小显示我们已应用相等的权重来分类它们为圆圈或三角形。决策树桩在上半部分生成了一条水平线来分类这些点。我们可以看到有 2 个圆圈被错误预测为三角形。因此，我们将为这
    2 个圆圈分配更高的权重，并应用另一个决策树桩。
- en: '*Step 2: Move to another decision tree stump to make a decision on another
    input variable:*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 2：转到另一个决策树桩，对另一个输入变量做出决策：*'
- en: We observe that the size of the 2 misclassified circles from the previous step
    is larger than the remaining points. Now, the 2^(nd) decision stump will try to
    predict these 2 circles correctly.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到前一步骤中被误分类的 2 个圆圈比剩余点要大。现在，第 2 个决策树桩将尝试正确预测这 2 个圆圈。
- en: As a result of assigning higher weights, these 2 circles have been correctly
    classified by the vertical line on the left. But this has now resulted in misclassifying
    the 3 circles at the top. Hence, we will assign higher weights to these 3 circles
    at the top and apply another decision stump.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于分配了更高的权重，这 2 个圆圈已被左侧的垂直线正确分类。但这导致了顶部 3 个圆圈的误分类。因此，我们将为这 3 个圆圈分配更高的权重，并应用另一个决策树桩。
- en: '*Step 3: Train another decision tree stump to make a decision on another input
    variable.*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 3：训练另一个决策树桩，对另一个输入变量做出决策。*'
- en: The 3 misclassified circles from the previous step are larger than the rest
    of the data points. Now, a vertical line to the right has been generated to classify
    the circles and triangles.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 前一步骤中被误分类的 3 个圆圈比其他数据点要大。现在，已经生成了一个向右的垂直线来分类圆圈和三角形。
- en: '*Step 4: Combine the decision stumps:*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 4：组合决策树桩：*'
- en: We have combined the separators from the 3 previous models and observe that
    the complex rule from this model classifies data points correctly as compared
    to any of the individual weak learners.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将之前 3 个模型的分隔符结合起来，观察到这个模型的复杂规则与任何单一的弱学习器相比，可以更准确地分类数据点。
- en: 'VII. Conclusion:'
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 VII 部分 结论：
- en: 'To recap, we have learnt:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们已经学习了：
- en: 5 supervised learning techniques- Linear Regression, Logistic Regression, CART,
    Naïve Bayes, KNN.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5 种监督学习技术——线性回归、逻辑回归、CART、朴素贝叶斯、KNN。
- en: 3 unsupervised learning techniques- Apriori, K-means, PCA.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3 种无监督学习技术——Apriori、K-means、PCA。
- en: 2 ensembling techniques- Bagging with Random Forests, Boosting with XGBoost.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2 种集成技术——随机森林的袋装法，XGBoost 的提升法。
- en: In my next blog, we will learn about a technique that has become a rage at Kaggle
    competitions - the XGBoost algorithm.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的下一篇博客中，我们将学习一种在 Kaggle 比赛中变得非常流行的技术——XGBoost 算法。
- en: '**Related:**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Top Algorithms and Methods used by Data Scientists](/2016/09/poll-algorithms-used-data-scientists.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学家使用的顶级算法和方法](/2016/09/poll-algorithms-used-data-scientists.html)'
- en: '[The 10 Algorithms Machine Learning Engineers need to know](/2016/08/10-algorithms-machine-learning-engineers.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习工程师需要了解的 10 种算法](/2016/08/10-algorithms-machine-learning-engineers.html)'
- en: '[Top 10 Data Mining Algorithms, Explained](/2015/05/top-10-data-mining-algorithms-explained.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[前 10 大数据挖掘算法详解](/2015/05/top-10-data-mining-algorithms-explained.html)'
- en: More On This Topic
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[建立一个强大的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用管道编写干净的 Python 代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目的，并找到目的以…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个初学者数据科学家应掌握的 6 种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一个 $9B 人工智能失败的检视](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
