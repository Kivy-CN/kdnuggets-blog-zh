# 实时图像分割使用5行代码

> 原文：[https://www.kdnuggets.com/2021/10/real-time-image-segmentation-5-lines-code.html](https://www.kdnuggets.com/2021/10/real-time-image-segmentation-5-lines-code.html)

[评论](#comments)

**由 [Ayoola Olafenwa](https://www.linkedin.com/in/ayoola-olafenwa-003b901a9/)，机器学习工程师**

### **对实时图像分割应用的需求**

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业道路。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的IT需求

* * *

图像分割是计算机视觉的一个方面，涉及将计算机可视化的物体内容分割成不同类别以便更好地分析。图像分割在解决许多计算机视觉问题中的贡献，如医学图像分析、背景编辑、自驾车视觉和卫星图像分析，使其在计算机视觉领域中具有不可估量的价值。在计算机视觉领域中，最大的挑战之一是如何在实时应用中保持准确性与速度性能之间的平衡。在计算机视觉领域中，往往面临这样一种两难困境：解决方案要么更准确但更慢，要么较少准确但更快。

PixelLib库是一个创建以便于将对象分割功能轻松集成到图像和视频中的库，使用几行Python代码即可实现。PixelLib的前一个版本使用Tensorflow深度学习作为其后台，采用Mask R-CNN进行实例分割。Mask R-CNN是一个出色的对象分割架构，但它在实时应用中无法平衡准确性与速度性能。PixelLib提供对PyTorch后台的支持，以使用***PointRend***分割架构实现更快、更准确的图像和视频中对象的分割和提取。

***PointRend*** 由 [Alexander Kirillov等](https://arxiv.org/abs/1912.08193) 开发，用于替代Mask R-CNN进行对象的实例分割。***PointRend*** 是一个卓越的前沿神经网络，适用于实现对象分割。它生成准确的分割掩码，并以高推理速度运行，满足了对准确且实时计算机视觉应用的不断增长的需求。我将PixelLib与Detectron2的Python实现的PointRend集成在一起，该实现仅支持Linux操作系统。我对原始Detectron2 PointRend实现进行了修改，以支持Windows操作系统。用于PixelLib的PointRend实现支持Linux和Windows操作系统。

**注意：** 本文基于使用PyTorch和***PointRend***进行实例分割。如果你想学习如何使用Tensorflow和Mask R-CNN进行实例分割，请阅读这篇 [文章](https://towardsdatascience.com/image-segmentation-with-six-lines-0f-code-acb870a462e8)。

![图示](../Images/22bc7c780b4afe84799ac42f72f0cfd6.png)

[原始图像来源](https://unsplash.com/photos/6UWqw25wfLI)（左：MASK R-CNN，右：PointRend）

![图示](../Images/cb489b9285153f06bae3e2511f8f1e0f.png)

[原始图像来源](https://unsplash.com/photos/rrI02QQ9GSQ)（左：MASK R-CNN，右：PointRend）

标记为**PointRend**的图像明显比**Mask R-CNN**的分割结果更好。

### 下载与安装

**下载Python**

PixelLib PyTorch支持Python 3.7及以上版本。下载兼容的 [Python版本](https://www.python.org/)。

**安装PixelLib及其依赖项**

**安装PyTorch**

PixelLib PyTorch版本支持以下PyTorch版本（***1.6.0、1.7.1、1.8.0*** 和 ***1.90***）。不支持PyTorch ***1.7.0***，且不要使用任何低于1.6.0的PyTorch版本。安装兼容的 [PyTorch版本](https://PyTorch.org/)。

**安装Pycocotools**

[PRE0]

**安装PixelLib**

[PRE1]

**如果已安装，请使用以下命令升级到最新版本：**

[PRE2]

### **图像分割**

PixelLib使用五行Python代码进行图像和视频中的对象分割，使用PointRend模型。下载 [PointRend模型](https://github.com/ayoolaolafenwa/PixelLib/releases/download/0.2.0/pointrend_resnet50.pkl)。这是图像分割的代码。

[PRE3]

**第1-4行：** 导入了PixelLib包，并从模块***pixellib.torchbackend.instance***中导入了类***instanceSegmentation***（从PyTorch支持中导入实例分割类）。我们创建了这个类的一个实例，并最终加载了我们下载的***PointRend***模型。

**第5行：** 我们调用了函数***segmentImage***来对图像中的对象进行分割，并将以下参数添加到函数中：

+   ***Image_path：*** 这是待分割图像的路径。

+   ***Show_bbox：*** 这是一个可选参数，用于显示带有边界框的分割结果。

+   ***Output_image_name:*** 这是保存的分割图像的名称。

**分割样本图像**

![图示](../Images/a8a8466af953d96d9291e39b63072e43.png)

[原始图像来源](https://commons.wikimedia.org/wiki/File:Carspotters.jpg)

[PRE4]

**分割后的图像**

![图像](../Images/03a6cceedbc5db33a5d577d457ae424b.png)

[PRE5]

如果你运行分割代码，可能会出现上述日志。这不是错误，代码将正常工作。

[PRE6]

分割结果返回一个字典，其中包含与图像中分割的对象相关的值。打印的结果将是以下格式：

[PRE7]

**检测阈值**

PixelLib使得确定物体分割的检测阈值成为可能。

[PRE8]

**confidence：** 这是在***load_model***函数中引入的新参数，设置为***0.3***，以***30%***作为检测的阈值。我设置的检测阈值的默认值为***0.5***，可以通过***confidence***参数增加或减少。

**速度记录**

PixelLib使得实时物体分割成为可能，并增加了调整推理速度以适应实时预测的能力。使用具有4GB容量的Nvidia GPU处理单张图像的默认推理速度约为***0.26秒***。

**速度调整**

PixelLib支持速度调整，有两种速度调整模式，分别为***fast***和***rapid***模式：

**1\. 快速模式**

[PRE9]

在***load_model***函数中，我们添加了参数***detection_speed***并将其值设置为***fast***。快速模式处理单张图像的时间为***0.20秒***。

**快速模式检测的完整代码**

[PRE10]

**2\. 快速模式**

[PRE11]

在***load_model***函数中，我们添加了参数***detection_speed***并将其值设置为***rapid***。**快速**模式处理单张图像的时间为***0.15秒***。

**快速模式检测的完整代码**

[PRE12]

### **PointRend模型**

用于物体分割的PointRend模型有两种类型，分别为***resnet50变体***和***resnet101变体***。本文使用***resnet50变体***，因为它更快且准确性较高。***resnet101变体***更准确，但比***resnet50变体***慢。根据[官方报告](https://github.com/facebookresearch/detectron2/tree/main/projects/PointRend)上的Detectron2模型，***resnet50变体***在COCO上的mAP为***38.3***，而***resnet101变体***在COCO上的mAP为***40.1***。

**Resnet101的速度记录：** 默认分割速度为***0.5秒***，快速模式为***0.3秒***，而快速模式为***0.25秒***。

**Resnet101变体的代码**

[PRE13]

使用resnet101模型进行推理的代码是相同的，唯一的不同是我们在***load_model***函数中加载了***PointRend resnet101模型***。从[这里](https://github.com/ayoolaolafenwa/PixelLib/releases/download/0.2.0/pointrend_resnet101.pkl)下载resnet101模型。我们在***load_model***函数中添加了额外的参数***network_backbone***并将其值设置为***resnet101***。

**注意：** 如果你希望实现高速推理和良好的准确性，使用**PointRend** ***resnet50变体***，但如果你更关心准确性，则使用**PointRend** ***resnet101变体***。所有这些推理报告都是基于使用4GB容量的Nvidia GPU。

**图像分割中的自定义对象检测**

使用的PointRend模型是一个预训练的COCO模型，支持80类对象。PixelLib支持自定义对象检测，使得过滤检测结果并确保目标对象的分割成为可能。我们可以从支持的80类对象中进行选择，以匹配我们的目标。这些是支持的80类对象：

[PRE14]

**目标类分割的代码**

[PRE15]

函数***select_target_classes***被调用以选择要分割的目标对象。函数***segmentImage***新增了参数***segment_target_classes***，用于从目标类中进行选择，并根据这些目标类筛选检测结果。我们筛选检测结果以仅检测图像中的人。

![图像](../Images/03a6cceedbc5db33a5d577d457ae424b.png)

### **图像中的对象提取**

PixelLib使得从图像中提取和分析分割的对象成为可能。

**对象提取的代码**

[PRE16]

图像分割的代码是相同的，唯一不同的是我们添加了额外的参数***extract_segmented_objects***和***save_extracted_objects***，分别用于提取分割对象和保存提取的对象。每个分割的对象将保存为***segmented_object_index***，例如***segmented_object_1***。对象将按照提取的顺序保存。

[PRE17]

![图示](../Images/8b1a301199f1b4ca6df20a01cd384dcc.png)

注意：图像中的所有对象都已被提取，我选择只显示其中的三个。

**从边界框坐标中提取对象**

[PRE18]

我们引入了新参数***extract_from_box***，以从其边界框坐标中提取分割的对象。每个提取的对象将保存为***object_extract_index***，例如***object_extract_1***。对象将按照提取的顺序保存。

![图示](../Images/948881319af9a109839b6ad02bbf7278.png)

从边界框坐标中提取

**图像分割输出可视化**

PixelLib使得根据图像的分辨率调节其可视化成为可能。

[PRE19]

![图示](../Images/2eff7a7adc25c50baf6aac1db642d3ee.png)

[原始图像来源](https://unsplash.com/photos/UiVe5QvOhao)

可视化效果不明显，因为 ***text size*** 和 ***box thickness*** 太细。我们可以调整 ***text size***、***text thickness*** 和 ***box thickness*** 来调节可视化效果。

**更好的可视化修改**。

[PRE20]

segmentImage 函数接受了新的参数，用于调节文本和边界框的厚度。

+   ***text_size:*** 默认的文本大小是 ***0.6***，对于分辨率适中的图像来说是合适的。对于高分辨率图像，它会显得太小。我将其增加到 5。

+   ***text_thickness:*** 默认的文本厚度是 1。我将其增加到 4 以匹配图像分辨率。

+   ***box_thickness:*** 默认的框厚度是 2，我将其更改为 10 以匹配图像分辨率。

**输出更好的可视化图像**

![图像](../Images/0841dec48cb891e893d87abb233a4a2d.png)

**注意：** 根据图像的分辨率调整参数。我为分辨率为 ***5760 x 3840*** 的示例图像使用的值可能对于分辨率较低的图像来说太大。如果你的图像分辨率非常高，可以将参数值提高到比我在示例代码中设置的值更高。***text_thickness*** 和 ***box_thickness*** 参数的值必须是整数，不要用浮点数表示。***text_size*** 值可以用整数和浮点数表示。

在本文中，我们详细讨论了如何进行准确和快速的图像分割以及提取图像中的对象。我们还描述了 PixelLib 的升级，使用 PointRend 使库能够满足计算机视觉中对准确性和速度性能平衡日益增长的需求。

**注意：** [阅读完整教程](https://towardsdatascience.com/real-time-image-segmentation-using-5-lines-of-code-7c480abdb835)，教程包含如何使用 PixelLib 对一批图像、视频和实时摄像头视频进行对象分割。

**个人简介：[Ayoola Olafenwa](https://www.linkedin.com/in/ayoola-olafenwa-003b901a9/)** 是一位自学成才的程序员、技术作家和深度学习实践者。Ayoola 开发了两个开源计算机视觉项目，全球许多开发者都在使用。目前，她在 DeepQuest AI 担任机器学习工程师，负责构建和部署云中的机器学习应用。Ayoola 的专业领域包括计算机视觉和机器学习。她有机器学习系统的工作经验，使用深度学习库如 PyTorch 和 Tensorflow 在生产环境中构建和部署机器学习模型，并在 Azure 等云计算平台上使用 Docker、Pulumi 和 Kubernetes 等 DevOps 工具。Ayoola 还在使用 PyTorchMobile、TensorflowLite 和 ONNX Runtime 等高效框架，将机器学习模型部署到 Nvidia Jetson Nano 和 Raspberry PI 等边缘设备上。

**相关：**

+   [使用 5 行代码提取图像和视频中的对象](/2021/03/extraction-objects-images-videos-5-lines-code.html)

+   [使用 5 行代码更改任何图像的背景](/2020/11/change-background-image-5-lines-code.html)

+   [使用 5 行代码更改任何视频的背景](/2020/12/change-background-video-5-lines-code.html)

### 更多相关话题

+   [少于 15 行代码实现多模态深度学习](https://www.kdnuggets.com/2023/01/predibase-multi-modal-deep-learning-less-15-lines-code.html)

+   [如何使用图数据库构建实时推荐引擎](https://www.kdnuggets.com/2023/08/build-realtime-recommendation-engine-graph-databases.html)

+   [Segment Anything Model: 图像分割的基础模型](https://www.kdnuggets.com/2023/07/segment-anything-model-foundation-model-image-segmentation.html)

+   [实时 AI 和机器学习的特征存储](https://www.kdnuggets.com/2022/03/feature-stores-realtime-ai-machine-learning.html)

+   [利用 AI 实现实时翻译](https://www.kdnuggets.com/2022/07/realtime-translations-ai.html)

+   [大数据如何实时拯救生命：物联网数据分析帮助预防事故](https://www.kdnuggets.com/how-big-data-is-saving-lives-in-real-time-iov-data-analytics-helps-prevent-accidents)
