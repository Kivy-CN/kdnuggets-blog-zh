# 并行化 Python 代码

> 原文：[https://www.kdnuggets.com/2021/10/parallelizing-python-code.html](https://www.kdnuggets.com/2021/10/parallelizing-python-code.html)

[评论](#comments)

**由 [Dawid Borycki](https://www.linkedin.com/in/dawidborycki/)，生物医学研究员和软件工程师 & [Michael Galarnyk](https://www.linkedin.com/in/michaelgalarnyk/)，数据科学专业人士**

Python 非常适合用于训练机器学习模型、执行数值模拟以及快速开发概念验证解决方案，而无需设置开发工具和安装多个依赖项。在执行这些任务时，您还希望尽可能充分利用底层硬件，以获得快速结果。并行化 Python 代码可以实现这一点。然而，使用标准 CPython 实现意味着由于全局解释器锁（GIL），您无法充分利用底层硬件，因为 GIL 阻止了多个线程同时运行字节码。

本文回顾了一些常见的 Python 代码并行化选项，包括：

+   [基于进程的并行性](https://docs.python.org/3/library/multiprocessing.html)

+   专用库

+   [IPython Parallel](https://ipython.readthedocs.io/en/stable/)

+   [Ray](https://docs.ray.io/en/master/)

对于每种技术，本文列出了其一些优缺点，并展示了代码示例，以帮助您理解使用它的情况。

## 如何并行化 Python 代码

有几种常见的方法来并行化 Python 代码。您可以启动多个应用程序实例或脚本来并行执行任务。当您不需要在并行任务之间交换数据时，这种方法非常好。否则，在进程之间共享数据会在汇总数据时显著降低性能。

在同一进程中启动多个线程可以更有效地在任务之间共享数据。在这种情况下，基于线程的并行化可以将一些工作卸载到后台。然而，标准 CPython 实现的全局解释器锁（GIL）阻止了在多个线程中同时运行字节码。

以下示例函数模拟复杂计算（旨在模拟激活函数）

```py
iterations_count = round(1e7)
def complex_operation(input_index):
   print("Complex operation. Input index: {:2d}".format(input_index))
   [math.exp(i) * math.sinh(i) for i in [1] * iterations_count]
```

`complex_operation` 执行多次以更好地估计处理时间。它将长时间运行的操作分成一批较小的操作。这通过将输入值划分为几个子集，然后并行处理这些子集中的输入来实现。

这是运行 `complex_operation` 多次（输入范围为十）并使用 timebudget 包测量执行时间的代码：

```py
@timebudget
def run_complex_operations(operation, input):
   for i in input:
      operation(i) 

input = range(10)
run_complex_operations(complex_operation, input)
```

执行 [这个脚本](https://gist.github.com/mGalarnyk/8c491fbdfe6ce3e498a7f62f03fa9ca4) 后，您将获得类似于下面的输出：

![并行化博客 1](../Images/556b374ed5ce8fe0d936731ac27c8969.png)

如您所见，在本教程中使用的笔记本电脑上执行此代码大约需要 39 秒。让我们看看如何改进这一结果。

## 基于进程的并行性

第一个方法是使用基于进程的并行性。使用这种方法，可以同时启动多个进程（并发）。这样，它们可以同时执行计算。

从Python 3开始，[multiprocessing包](https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing)已预安装，并为启动并发进程提供了便捷的语法。它提供了Pool对象，自动将输入分割成子集并在多个进程间分配。

[这里是一个示例](https://gist.github.com/mGalarnyk/b5455b0454815b04363ef9994f22fbf3)，展示如何使用Pool对象来启动十个进程：

```py
import math
import numpy as np
from timebudget import timebudget
from multiprocessing import Pool

iterations_count = round(1e7)

def complex_operation(input_index):
    print("Complex operation. Input index: {:2d}\n".format(input_index))
    [math.exp(i) * math.sinh(i) for i in [1] * iterations_count]

@timebudget
def run_complex_operations(operation, input, pool):
    pool.map(operation, input)

processes_count = 10

if __name__ == '__main__':
    processes_pool = Pool(processes_count)
    run_complex_operations(complex_operation, range(10), processes_pool)
```

[代码示例](https://gist.github.com/mGalarnyk/b5455b0454815b04363ef9994f22fbf3)

每个进程同时执行复杂操作。因此，理论上，这段代码可以将总执行时间减少多达十倍。然而，下面代码的输出仅显示了约四倍的改进（上一节39秒对比本节9.4秒）。

![并行化博客 2](../Images/586ee96a1d06dc9d5c7323aed2b130ef.png)

改进没有达到十倍的原因有几个。首先，能并发运行的最大进程数量取决于系统中的CPU数量。你可以通过使用`os.cpu_count()`方法来查找你的系统有多少个CPU。

```py
import os
print('Number of CPUs in the system: {}'.format(os.cpu_count()))
```

![图](../Images/e533ca816283748e2fbec835c8a9607d.png)

本教程使用的机器有八个CPU。

改进没有更多的另一个原因是本教程中的计算相对较小。最后，需要注意的是，在并行计算时通常会有一些开销，因为想要通信的进程必须利用[进程间通信机制](https://en.wikipedia.org/wiki/Inter-process_communication)。这意味着对于非常小的任务，并行计算通常比串行计算（普通Python）更慢。如果你有兴趣了解更多关于多进程的知识，Selva Prabhakaran有一个[出色的博客](https://www.anyscale.com/blog/writing-your-first-distributed-python-application-with-ray)，它启发了本教程的这一部分。如果你想了解更多关于并行/分布式计算的权衡，[查看这个教程](https://towardsdatascience.com/writing-your-first-distributed-python-application-with-ray-4248ebc07f41)。

![并行化博客 4](../Images/0950f9777fafefce27b2135f25e3ebea.png)

## 专门的库

[许多针对NumPy等专门库的计算不受GIL影响](https://stackoverflow.com/questions/36479159/why-are-numpy-calculations-not-affected-by-the-global-interpreter-lock)，可以利用线程和其他技术并行工作。本教程的这一部分讲解了结合NumPy和多进程的好处。

为了演示朴素实现与基于 NumPy 的实现之间的差异，需要实现一个额外的函数：

```py
def complex_operation_numpy(input_index):
      print("Complex operation (numpy). Input index: {:2d}".format(input_index))

      data = np.ones(iterations_count)
      np.exp(data) * np.sinh(data)
```

代码现在使用 NumPy 的 exp 和 sinh 函数对输入序列进行计算。然后，代码使用进程池执行 complex_operation 和 complex_operation_numpy 十次，以比较它们的性能：

```py
processes_count = 10
input = range(10)

if __name__ == '__main__':
    processes_pool = Pool(processes_count)
    print(‘Without NumPy’)
    run_complex_operations(complex_operation, input, processes_pool)
    print(‘NumPy’)
    run_complex_operations(complex_operation_numpy, input, processes_pool)
```

以下输出显示了 [这个脚本](https://gist.github.com/mGalarnyk/703c53bb98aa94d66bb6c49d48ce5c09) 使用和不使用 NumPy 的性能。

![并行化博客 5](../Images/674158bdcd5d5338901af10c077dfd72.png)

NumPy 提供了性能的快速提升。在这里，NumPy 将计算时间减少到原始时间的约 10%（859 毫秒 vs 9.515 秒）。它更快的一个原因是 NumPy 的大部分处理都是向量化的。通过向量化，底层代码实际上被“并行化”，因为操作可以一次计算多个数组元素，而不是一个一个地循环遍历。如果你有兴趣了解更多，Jake Vanderplas 在 [这里](https://youtu.be/EEUXKG97YRw?t=613) 做了一个很棒的讲座。

![并行化博客 6](../Images/5202adfec876af490cac38a2ffebf8a9.png)

## IPython 并行

IPython shell 支持跨多个 IPython 实例的交互式并行和分布式计算。IPython Parallel 几乎是与 IPython 一起开发的。当 IPython 更名为 Jupyter 时，他们将 IPython Parallel 拆分成了一个独立的包。[IPython Parallel](https://ipython.org/ipython-doc/3/parallel/parallel_intro.html) 具有许多优点，但也许最大的优点是它使得并行应用程序可以交互式地开发、执行和监控。在进行并行计算时，通常会使用 ipcluster 命令开始。

```py
ipcluster start -n 10
```

最后一个参数控制要启动的引擎（节点）数量。上述命令在 [安装 ipyparallel Python 包](https://ipyparallel.readthedocs.io/en/latest/) 后变得可用。以下是一个示例输出：

![并行化博客 7](../Images/49b3a897c2a925662927af084c4d024c.png)

下一步是提供 Python 代码，以连接到 ipcluster 并启动并行作业。幸运的是，IPython 提供了一个方便的 API 来实现这一点。代码看起来像是基于 Pool 对象的进程并行：

```py
import math
import numpy as np
from timebudget import timebudget
import ipyparallel as ipp

iterations_count = round(1e7)

def complex_operation(input_index):
    print("Complex operation. Input index: {:2d}".format(input_index))

    [math.exp(i) * math.sinh(i) for i in [1] * iterations_count]

def complex_operation_numpy(input_index):
    print("Complex operation (numpy). Input index: {:2d}".format(input_index))

    data = np.ones(iterations_count)
    np.exp(data) * np.sinh(data)

@timebudget
def run_complex_operations(operation, input, pool):
    pool.map(operation, input)

client_ids = ipp.Client()
pool = client_ids[:]

input = range(10)
print('Without NumPy')
run_complex_operations(complex_operation, input, pool)
print('NumPy')
run_complex_operations(complex_operation_numpy, input, pool)
```

[代码示例](https://gist.github.com/mGalarnyk/6dab23cc6485f145d2b148fc64d34b3c)

在终端的新标签页中执行上述代码会产生如下输出：

![并行化博客 8](../Images/e475926323d5f6e832d388f481c8efc1.png)

IPython Parallel 使用和不使用 NumPy 的执行时间分别为 13.88 毫秒和 9.98 毫秒。请注意，标准输出中没有包含日志，但可以通过附加命令进行查看。

![并行化博客 9](../Images/433106b430695e82862e568760b0396d.png)

## Ray

像 IPython Parallel 一样，[Ray](https://docs.ray.io/en/master/index.html) 可以用于并行**和**分布式计算。Ray 是一个快速、简单的分布式执行框架，使得扩展应用程序和利用最先进的机器学习库变得容易。使用 Ray，你可以将顺序运行的 Python 代码转换为分布式应用程序，几乎不需要修改代码。

![图](../Images/8e1afe4eff7de3591f54c705e0083009.png)

虽然本教程简要介绍了 Ray 如何使平凡的 Python 代码并行化，但需要注意的是，Ray 及其生态系统也使得并行化现有库变得容易，比如 [scikit-learn](https://medium.com/distributed-computing-with-ray/how-to-speed-up-scikit-learn-model-training-aaf17e2d1e1)， [XGBoost](https://www.anyscale.com/blog/distributed-xgboost-training-with-ray)， [LightGBM](https://www.anyscale.com/blog/introducing-distributed-lightgbm-training-with-ray)， [PyTorch](https://medium.com/pytorch/getting-started-with-distributed-machine-learning-with-pytorch-and-ray-fd83c98fdead)，等等。

要使用 Ray，需要调用 ray.init() 来启动所有相关的 Ray 进程。默认情况下，Ray 为每个 CPU 核心创建一个工作进程。如果你想在集群上运行 Ray，则需要传入集群地址，例如 ray.init(address='insertAddressHere')。

```py
ray.init() 
```

下一步是创建一个 Ray 任务。这可以通过使用 @ray.remote 装饰器装饰普通 Python 函数来完成。这会创建一个任务，可以在你笔记本电脑的 CPU 核心（或 Ray 集群）中调度。以下是之前创建的 complex_operation_numpy 的示例：

```py
@ray.remote
def complex_operation_numpy(input_index):
   print("Complex operation (numpy). Input index: {:2d}".format(input_index))
   data = np.ones(iterations_count)
   np.exp(data) * np.sinh(data)
```

在最后一步，像这样在 Ray 运行时中执行这些函数：

```py
@timebudget
def run_complex_operations(operation, input):
   ray.get([operation.remote(i) for i in input])
```

执行 [这个脚本](https://gist.github.com/mGalarnyk/30c8672620c8655a37940be935899a57)后，你将获得类似于以下的输出：

![并行化博客 11](../Images/464768f1d47ae3ef302ee401e8f70cac.png)

Ray 的执行时间在有 NumPy 和没有 NumPy 的情况下分别为 3.382 秒和 419.98 毫秒。需要记住的是，Ray 的性能优势在执行长期任务时会更加明显，如下图所示。

![图](../Images/da5f3e3440818c336832cd06dfbab026.png)

Ray 在运行更大任务时具有更明显的好处 [(图片来源)](https://towardsdatascience.com/10x-faster-parallel-python-without-python-multiprocessing-e5017c93cce1)

如果你想了解 Ray 的语法，可以在 [这里](https://www.anyscale.com/blog/writing-your-first-distributed-python-application-with-ray) 查看入门教程。

![并行化博客 13](../Images/ff1bcf03b91977091f603a756bd3e9a3.png)

## 替代的 Python 实现

最后一个考虑是，你可以使用其他 Python 实现来应用多线程。例如，包括 .NET 的 IronPython 和 Java 的 Jython。在这种情况下，你可以利用底层框架提供的低级线程支持。如果你已经对 .NET 或 Java 的多进程能力有经验，这种方法会很有用。

## 结论

本文通过代码示例回顾了并行化 Python 的常见方法，并突出了它们的一些优缺点。我们使用简单的数值数据进行了基准测试。需要注意的是，并行化的代码通常会引入一些开销，并且并行化的好处在较大的任务中比在本教程中的短小计算中更为明显。

请记住，平行化在其他应用中可能更为强大。特别是在处理典型的 AI 任务时，你必须对模型进行重复的微调。在这种情况下，[Ray](https://github.com/ray-project/ray) 提供了最佳的支持，因为它拥有丰富的生态系统、自动扩展、容错能力和使用远程机器的能力。

**[Dawid Borycki](https://www.linkedin.com/in/dawidborycki/)** 是生物医学研究员和软件工程师，同时也是书籍作者和会议演讲者。

**[Michael Galarnyk](https://www.linkedin.com/in/michaelgalarnyk/)** 是数据科学专家，现任 Anyscale 的开发者关系工作者。

[原文](https://www.anyscale.com/blog/parallelizing-python-code)。经允许转载。

**相关：**

+   [如何加速 Scikit-Learn 模型训练](/2021/02/speed-up-scikit-learn-model-training.html)

+   [使用 Ray 编写你的第一个分布式 Python 应用程序](/2021/08/distributed-python-application-ray.html)

+   [Dask 和 Pandas：数据多到没法处理？](/2021/03/dask-pandas-data.html)

* * *

## 我们的三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织进行 IT 工作

* * *

### 更多相关话题

+   [优化 Python 代码性能：深入探讨 Python 性能分析器](https://www.kdnuggets.com/2023/02/optimizing-python-code-performance-deep-dive-python-profilers.html)

+   [作为数据科学家管理可重用的 Python 代码](https://www.kdnuggets.com/2021/06/managing-reusable-python-code-data-scientist.html)

+   [宣布 PyCaret 3.0：开源、低代码的 Python 机器学习](https://www.kdnuggets.com/2023/03/announcing-pycaret-30-opensource-lowcode-machine-learning-python.html)

+   [使用管道编写干净的 Python 代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)

+   [追踪和可视化 Python 代码执行的 3 个工具](https://www.kdnuggets.com/2021/12/3-tools-track-visualize-execution-python-code.html)

+   [使用 timeit 和 cProfile 进行 Python 代码性能分析](https://www.kdnuggets.com/profiling-python-code-using-timeit-and-cprofile)
