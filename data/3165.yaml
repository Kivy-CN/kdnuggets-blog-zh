- en: The 8 Neural Network Architectures Machine Learning Researchers Need to Learn
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习研究人员需要学习的 8 种神经网络架构
- en: 原文：[https://www.kdnuggets.com/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html/2](https://www.kdnuggets.com/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html/2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html/2](https://www.kdnuggets.com/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html/2)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html?page=2#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html?page=2#comments)'
- en: 5 — Hopfield Networks
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5 — 霍普菲尔德网络
- en: 'Recurrent networks of non-linear units are generally very hard to analyze.
    They can behave in many different ways: settle to a stable state, oscillate, or
    follow chaotic trajectories that cannot be predicted far into the future. A **Hopfield
    net** is composed of binary threshold units with recurrent connections between
    them. In 1982, [John Hopfield](http://www.pnas.org/content/79/8/2554.full.pdf) realized
    that if the connections are symmetric, there is a global energy function. Each
    binary “configuration” of the whole network has an energy; while the binary threshold
    decision rule causes the network to settle for a minimum of this energy function.
    A neat way to make use of this type of computation is to use memories as energy
    minima for the neural net. Using energy minima to represent memories gives a content-addressable
    memory. An item can be accessed by just knowing part of its content. It is robust
    against hardware damage.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性单元的递归网络通常很难分析。它们可以表现出许多不同的方式：稳定到一个稳定状态、振荡或沿着无法预测的混沌轨迹前进。一个**霍普菲尔德网络**由二进制阈值单元组成，这些单元之间有递归连接。1982年，[约翰·霍普菲尔德](http://www.pnas.org/content/79/8/2554.full.pdf)意识到，如果连接是对称的，就存在一个全局能量函数。整个网络的每个二进制“配置”都有一个能量；而二进制阈值决策规则使网络趋向于这个能量函数的最小值。利用这种计算类型的一种巧妙方式是将记忆作为神经网络的能量最小值。使用能量最小值来表示记忆提供了一种内容可寻址的记忆。只需知道其内容的一部分即可访问该项。它对硬件损坏具有鲁棒性。
- en: '![](../Images/389e610dd01eba5d37d2433e8cf8f61a.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/389e610dd01eba5d37d2433e8cf8f61a.png)'
- en: Each time we memorize a configuration, we hope to create a new energy minimum.
    But what if two nearby minima at an intermediate location? This limits the capacity
    of a Hopfield net. So how do we increase the capacity of a Hopfield net? Physicists
    love the idea that the math they already know might explain how the brain works.
    Many papers were published in physics journals about Hopfield nets and their storage
    capacity. Eventually, [Elizabeth Gardner](http://www.baginsky.de/eli/eg_portr.html)figured
    out that there was a much better storage rule that uses the full capacity of the
    weights. Instead of trying to store vectors in one shot, she cycled through the
    training set many times and used the perceptron convergence procedure to train
    each unit to have the correct state given the states of all the other units in
    that vector. Statisticians call this technique “pseudo-likelihood.”
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 每次我们记住一个配置时，我们希望创建一个新的能量最小值。但是，如果两个相邻的最小值位于一个中间位置怎么办？这限制了霍普菲尔德网络的容量。那么我们如何增加霍普菲尔德网络的容量呢？物理学家喜欢这样一个想法：他们已经知道的数学可能解释大脑的工作原理。许多论文在物理学期刊上发表，讨论霍普菲尔德网络及其存储容量。最终，[伊丽莎白·加德纳](http://www.baginsky.de/eli/eg_portr.html)发现有一个更好的存储规则，能够充分利用权重的容量。她没有试图一次性存储向量，而是多次循环遍历训练集，并使用感知机收敛过程来训练每个单元，以便在给定该向量中所有其他单元的状态的情况下具有正确的状态。统计学家称这种技术为“伪似然”。
- en: '![](../Images/a5879ce459a4610c5864a73590815152.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5879ce459a4610c5864a73590815152.png)'
- en: There is another computational role for Hopfield nets. Instead of using the
    net to store memories, we use it to construct interpretations of sensory input.
    The input is represented by the visible units, the interpretation is represented
    by the states of the hidden units, and the badness of the interpretation is represented
    by the energy.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 霍普菲尔德网络还有另一种计算作用。我们可以用它来构建对感官输入的解释，而不是用来存储记忆。输入由可见单元表示，解释由隐藏单元的状态表示，而解释的糟糕程度由能量表示。
- en: 6 — Boltzmann Machine Network
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6 — 玻尔兹曼机网络
- en: A **Boltzmann machine** is a type of stochastic recurrent neural network. It
    can be seen as the stochastic, generative counterpart of Hopfield nets. It was
    one of the first neural networks capable of learning internal representations
    and is able to represent and solve difficult combinatoric problems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**玻尔兹曼机** 是一种随机递归神经网络。它可以看作是Hopfield网的随机生成对等物。它是最早能够学习内部表示的神经网络之一，能够表示和解决困难的组合问题。'
- en: '![](../Images/b30a0c0fdfe5954adf36bd121d128e76.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b30a0c0fdfe5954adf36bd121d128e76.png)'
- en: 'The goal of learning for Boltzmann machine learning algorithm is to maximize
    the product of the probabilities that the Boltzmann machine assigns to the binary
    vectors in the training set. This is equivalent to maximizing the sum of the log
    probabilities that the Boltzmann machine assigns to the training vectors. It is
    also equivalent to maximizing the probability that we would obtain exactly the
    N training cases if we did the following: 1) Let the network settle to its stationary
    distribution N different time with no external input; and 2) Sample the visible
    vector once each time.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 玻尔兹曼机学习算法的学习目标是最大化玻尔兹曼机对训练集中的二进制向量分配的概率的乘积。这等同于最大化玻尔兹曼机对训练向量分配的对数概率之和。这也等同于最大化我们得到准确N个训练样本的概率，如果我们进行以下操作：1）让网络在没有外部输入的情况下收敛到其稳态分布N次；2）每次采样一次可见向量。
- en: An efficient mini-batch learning procedure was proposed for Boltzmann Machines
    by [Salakhutdinov and Hinton in 2012](http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Salakhutdinov 和 Hinton 在2012年提出了一种高效的小批量学习程序用于玻尔兹曼机，详见 [Salakhutdinov 和 Hinton
    在2012](http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf)。
- en: For the positive phase, first initialize the hidden probabilities at 0.5, then
    clamp a data vector on the visible units, then update all the hidden units in
    parallel until convergence using mean field updates. After the net has converged,
    record PiPj for every connected pair of units and average this over all data in
    the mini-batch.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于正向阶段，首先将隐藏单元的概率初始化为0.5，然后将数据向量固定在可见单元上，然后使用均值场更新并行更新所有隐藏单元直到收敛。在网络收敛后，记录每一对连接单元的PiPj，并对小批量中的所有数据取平均。
- en: 'For the negative phase: first keep a set of “fantasy particles.” Each particle
    has a value that is a global configuration. Then sequentially update all the units
    in each fantasy particle a few times. For every connected pair of units, average
    SiSj over all the fantasy particles.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于负向阶段：首先保持一组“虚拟粒子”。每个粒子都有一个作为全局配置的值。然后依次更新每个虚拟粒子中的所有单元几次。对于每一对连接单元，计算SiSj在所有虚拟粒子中的平均值。
- en: In a general Boltzmann machine, the stochastic updates of units need to be sequential.
    There is a special architecture that allows alternating parallel updates which
    are much more efficient (no connections within a layer, no skip-layer connections).
    This mini-batch procedure makes the updates of the Boltzmann machine more parallel.
    This is called a Deep Boltzmann Machine (DBM), a general Boltzmann machine with
    a lot of missing connections.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般的玻尔兹曼机中，单位的随机更新需要按顺序进行。有一种特殊的架构允许交替的并行更新，这种更新方式更高效（层内没有连接，层间没有跳跃连接）。这种小批量处理使玻尔兹曼机的更新更加并行。这被称为深度玻尔兹曼机（DBM），它是一个具有许多缺失连接的通用玻尔兹曼机。
- en: '![](../Images/2fb367be246b0e42e30f3f464619de56.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2fb367be246b0e42e30f3f464619de56.png)'
- en: In 2014, Salakhutdinov and Hinton came up with another update for their model,
    calling it [Restricted Boltzmann Machines](http://www.machinelearning.org/proceedings/icml2007/papers/407.pdf).
    They restrict the connectivity to make inference and learning easier (only one
    layer of hidden units and no connections between hidden units). In an RBM it only
    takes one step to reach thermal equilibrium when the visible units are clamped.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年，Salakhutdinov 和 Hinton 提出了对其模型的另一种更新，称为 [限制玻尔兹曼机](http://www.machinelearning.org/proceedings/icml2007/papers/407.pdf)。他们限制了连接性以简化推理和学习（仅有一层隐藏单元且隐藏单元之间没有连接）。在RBM中，当可见单元被固定时，只需一步即可达到热平衡。
- en: 'Another efficient mini-batch learning procedure for RBM goes like this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种高效的小批量学习程序如下：
- en: For the positive phase, first clamp a data vector on the visible units. Then
    compute the exact value of <ViHj> for all pairs of a visible and a hidden unit.
    For every connected pair of units, average <ViHj> over all data in the mini-batch.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于正向阶段，首先将数据向量固定在可见单元上。然后计算每对可见单元和隐藏单元的<ViHj>的确切值。对于每一对连接单元，计算<ViHj>在小批量数据中的平均值。
- en: For the negative phase, also keep a set of “fantasy particles.” Then update
    each fantasy particle a few times using alternating parallel updates. For every
    connected pair of units, average ViHj over all the fantasy particles.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于负阶段，也保持一组“虚拟粒子”。然后使用交替并行更新更新每个虚拟粒子几次。对于每对连接的单元，平均所有虚拟粒子的 ViHj。
- en: 7 — Deep Belief Network
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7 — 深度信念网络
- en: '![](../Images/40b880c73edf17c5116730a46ce96a27.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40b880c73edf17c5116730a46ce96a27.png)'
- en: Back-propagation is considered the standard method in artificial neural networks
    to calculate the error contribution of each neuron after a batch of data is processed.
    However, there are some major problems using back-propagation. Firstly, it requires
    labeled training data; while almost all data is unlabeled. Secondly, the learning
    time does not scale well, which means it is very slow in networks with multiple
    hidden layers. Thirdly, it can get stuck in poor local optima, so for deep nets
    they are far from optimal.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播被认为是人工神经网络中计算每个神经元在处理一批数据后错误贡献的标准方法。然而，使用反向传播存在一些主要问题。首先，它需要标记的训练数据，而几乎所有的数据都是未标记的。其次，学习时间不具备良好的扩展性，这意味着在具有多个隐藏层的网络中，它非常缓慢。第三，它可能会陷入较差的局部最优解，因此对于深度网络，它们离最优解还很远。
- en: To overcome the limitations of back-propagation, researchers have considered
    using unsupervised learning approaches. This helps keep the efficiency and simplicity
    of using a gradient method for adjusting the weights, but also use it for modeling
    the structure of the sensory input. In particular, they adjust the weights to
    maximize the probability that a generative model would have generated the sensory
    input. The question is what kind of generative model should we learn? Can it be
    an energy-based model like a Boltzmann machine? Or a causal model made of idealized
    neurons? Or a hybrid of the two?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服反向传播的局限性，研究人员考虑使用无监督学习方法。这有助于保持使用梯度方法调整权重的效率和简便性，同时还用于建模感官输入的结构。特别是，他们调整权重以最大化生成模型生成感官输入的概率。问题是我们应该学习什么样的生成模型？它可以是类似于玻尔兹曼机的基于能量的模型吗？或者是由理想化神经元组成的因果模型？还是两者的混合？
- en: '![](../Images/9309979c829915136afaa38be8b8b931.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9309979c829915136afaa38be8b8b931.png)'
- en: 'A **belief net** is a directed acyclic graph composed of stochastic variables.
    Using belief net, we get to observe some of the variables and we would like to
    solve 2 problems: 1) The inference problem: Infer the states of the unobserved
    variables, and 2) The learning problem: Adjust the interactions between variables
    to make the network more likely to generate the training data.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**信念网络** 是由随机变量组成的有向无环图。使用信念网络，我们可以观察到一些变量，并希望解决两个问题：1）推理问题：推断未观察到的变量的状态；2）学习问题：调整变量之间的交互，使网络更有可能生成训练数据。'
- en: Early graphical models used experts to define the graph structure and the conditional
    probabilities. By then, the graphs were sparsely connected; so researchers initially
    focused on doing correct inference, not on learning. For neural nets, learning
    was central and hand-writing the knowledge was not cool, because knowledge came
    from learning the training data. Neural networks did not aim for interpretability
    or sparse connectivity to make inference easy. Nevertheless, there are neural
    network versions of belief nets.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的图形模型使用专家定义图的结构和条件概率。那时，图的连接稀疏；因此，研究人员最初专注于进行正确的推理，而不是学习。对于神经网络，学习是核心，手工编写知识并不可取，因为知识来自于学习训练数据。神经网络并不以可解释性或稀疏连接为目标来简化推理。尽管如此，信念网络也有神经网络版本。
- en: 'There are two types of generative neural network composed of stochastic binary
    neurons: 1) **Energy-based**, in which we connect binary stochastic neurons using
    symmetric connections to get a Boltzmann Machine; and 2) **Causal**, in which
    we connect binary stochastic neurons in a directed acyclic graph to get a Sigmoid
    Belief Net. The descriptions of these two types go beyond the scope of this article.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的生成神经网络，由随机二进制神经元组成：1）**基于能量的**，其中我们使用对称连接连接二进制随机神经元以获得玻尔兹曼机；2）**因果**，其中我们在有向无环图中连接二进制随机神经元以获得
    sigmoid 信念网络。这两种类型的描述超出了本文的范围。
- en: 8 — Deep Auto-encoders
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8 — 深度自编码器
- en: '![](../Images/a3146c500d37f940619b432a3a854e23.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3146c500d37f940619b432a3a854e23.png)'
- en: 'Finally, let’s discuss **deep auto-encoders. **They always looked like a really
    nice way to do non-linear dimensionality reduction because of a few reasons: They
    provide flexible mappings both ways. The learning time is linear (or better) in
    the number of training cases. And the final encoding model is fairly compact and
    fast. However, it turned out to be very difficult to optimize deep auto encoders
    using back propagation. With small initial weights, the back propagated gradient
    dies. We now have a much better ways to optimize them; either use unsupervised
    layer-by-layer pre-training or just initialize the weights carefully as in Echo-State
    Nets.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们讨论一下**深度自编码器**。由于以下几个原因，它们一直被认为是进行非线性降维的一个非常好的方法：它们提供了双向灵活映射。学习时间在训练案例数量上是线性的（或更好）。最终的编码模型相当紧凑且快速。然而，使用反向传播来优化深度自编码器非常困难。由于初始权重较小，反向传播的梯度会消失。我们现在有更好的优化方法；可以使用无监督的逐层预训练，或者像在Echo-State
    Nets中那样小心初始化权重。
- en: 'For pre-training task, there are actually 3 different types of shallow auto-encoders:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预训练任务，实际上有3种不同类型的浅层自编码器：
- en: '**RBM’s as auto-encoders**: When we train an RBM with one-step contrastive
    divergence, it tries to make the reconstructions look like data. It’s like an
    auto encoder, but it’s strongly regularized by using binary activities in the
    hidden layer. When trained with maximum likelihood, RBMs are not like auto encoders.
    We can replace the stack of RBM’s used for pre-training by a stack of shallow
    auto encoders; however pre-training is not as effective (for subsequent discrimination)
    if the shallow auto encoders are regularized by penalizing the squared weights.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**RBM作为自编码器**：当我们用一步对比散度训练RBM时，它试图使重建看起来像数据。它像自编码器，但通过使用隐藏层中的二进制活动进行强正则化。当用最大似然训练时，RBM不像自编码器。我们可以用一堆浅层自编码器替代用于预训练的RBM堆叠；然而，如果浅层自编码器通过惩罚平方权重进行正则化，则预训练的效果（对于后续的辨别）不如前者。'
- en: '**Denoising auto encoders**: These add noise to the input vector by setting
    many of its components to 0 (like dropout, but for inputs). They are still required
    to reconstructing these components so they must extract features that capture
    correlations between inputs. Pre-training is very effective if we use a stack
    of denoting auto encoders. It’s as good as or better than pre-training with RBMs.
    It’s also simpler to evaluate the pre-training because we can easily compute the
    value of the objective function. It lacks the nice variational bound we get with
    RBMs, but this is only of theoretical interest.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**去噪自编码器**：这些通过将输入向量的许多组件设置为0（类似于dropout，但用于输入）来添加噪声。它们仍需重建这些组件，因此必须提取能够捕捉输入之间相关性的特征。如果我们使用一系列去噪自编码器，预训练效果非常好。它的效果与使用RBM进行预训练一样好，甚至更好。评估预训练也更简单，因为我们可以轻松计算目标函数的值。它缺乏RBM所提供的优美变分界限，但这仅仅是理论上的兴趣。'
- en: '**Contractive auto encoders**: Another way to regularize an auto encoder is
    to try to make the activities of the hidden units as insensitive as possible to
    the inputs; but they cannot just ignore the inputs because they must reconstruct
    them. We achieve this by penalizing the squared gradient of each hidden activity
    with respect to the inputs. Contractive auto encoders work very well for pre-training.
    The codes tend to have the property that only a small subset of the hidden units
    are sensitive to changes in the input.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**收缩自编码器**：另一种对自编码器进行正则化的方法是尽可能使隐藏单元的活动对输入不敏感；但它们不能忽视输入，因为它们必须重建输入。我们通过惩罚每个隐藏活动相对于输入的平方梯度来实现这一点。收缩自编码器在预训练中效果非常好。编码往往具有这样的特性：只有一小部分隐藏单元对输入的变化敏感。'
- en: '![](../Images/7ac6f888cc07b24fb8fe9dd7111e91ee.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ac6f888cc07b24fb8fe9dd7111e91ee.png)'
- en: In brief, there are now many different ways to do layer-by-layer pre-training
    of features. For datasets that do not have huge numbers of labeled cases, pre-training
    helps subsequent discriminative learning. For very large, labeled datasets, initializing
    the weights used in supervised learning by using unsupervised pre-training is
    not necessary, even for deep nets. Pre-training was the first good way to initialize
    the weights for deep nets, but now there are other ways. But if we make the nets
    much larger, we will need pre-training again!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '-   简而言之，现在有许多不同的方法可以进行逐层特征预训练。对于没有大量标记案例的数据集，预训练有助于后续的判别学习。对于非常大、标记的数据集，通过无监督预训练初始化用于监督学习的权重并非必要，即使是对于深度网络。预训练曾是初始化深度网络权重的首选方法，但现在有其他方法。不过，如果我们将网络做得更大，我们将需要再次进行预训练！'
- en: '**Last Takeaway**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**最后的收获**'
- en: Neural networks are one of the most beautiful programming paradigms ever invented.
    In the conventional approach to programming, we tell the computer what to do,
    breaking big problems up into many small, precisely defined tasks that the computer
    can easily perform. By contrast, in a neural network we don’t tell the computer
    how to solve our problem. Instead, it learns from observational data, figuring
    out its own solution to the problem at hand.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '-   神经网络是有史以来最美妙的编程范式之一。在传统编程方法中，我们告诉计算机做什么，将大问题拆分成许多小的、明确定义的任务，计算机可以轻松完成。相比之下，在神经网络中，我们不告诉计算机如何解决问题。相反，它从观察数据中学习，找出自己解决当前问题的方法。'
- en: Today, deep neural networks and deep learning achieve outstanding performance
    on many important problems in computer vision, speech recognition, and natural
    language processing. They’re being deployed on a large scale by companies such
    as Google, Microsoft, and Facebook.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '-   今天，深度神经网络和深度学习在计算机视觉、语音识别和自然语言处理等许多重要问题上表现出色。它们正在被谷歌、微软和 Facebook 等公司大规模部署。'
- en: I hope that this post helps you learn the core concepts of neural networks,
    including modern techniques for deep learning. You can get all the lecture slides,
    research papers and programming assignments I have done for Dr. Hinton’s Coursera
    course [from my GitHub repo here](https://github.com/khanhnamle1994/neural-nets).
    Good luck studying!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '-   我希望这篇文章能帮助你学习神经网络的核心概念，包括深度学习的现代技术。你可以从[我的 GitHub 仓库](https://github.com/khanhnamle1994/neural-nets)获取我为
    Dr. Hinton 的 Coursera 课程所做的所有讲义、研究论文和编程作业。祝你学习顺利！'
- en: '**Bio: [James Le](https://www.linkedin.com/in/khanhnamle94/)** is currently
    applying for Master of Science Computer Science programs in the US for the Fall
    2018 admission. His intended research will focus on Machine Learning and Data
    Mining. In the mean time, he is working as a freelance full-stack web developer.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[James Le](https://www.linkedin.com/in/khanhnamle94/)** 目前正在申请美国的计算机科学硕士项目，计划于2018年秋季入学。他的研究方向将集中在机器学习和数据挖掘方面。与此同时，他还在担任自由职业的全栈网页开发人员。'
- en: '[Original](https://towardsdatascience.com/the-8-neural-network-architectures-machine-learning-researchers-need-to-learn-11a0c96d6073).
    Reposted with permission.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/the-8-neural-network-architectures-machine-learning-researchers-need-to-learn-11a0c96d6073)。经授权转载。'
- en: '**Related:**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[The 10 Deep Learning Methods AI Practitioners Need to Apply](/2017/12/10-deep-learning-methods-ai-practitioners-need-apply.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AI 从业者需要应用的10种深度学习方法](/2017/12/10-deep-learning-methods-ai-practitioners-need-apply.html)'
- en: '[The 10 Algorithms Machine Learning Engineers Need to Know](/2016/08/10-algorithms-machine-learning-engineers.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习工程师需要了解的10种算法](/2016/08/10-algorithms-machine-learning-engineers.html)'
- en: '[The 10 Statistical Techniques Data Scientists Need to Master](/2017/11/10-statistical-techniques-data-scientists-need-master.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学家需要掌握的10种统计技术](/2017/11/10-statistical-techniques-data-scientists-need-master.html)'
- en: '* * *'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '-   我们的前三大课程推荐'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析水平'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织在IT方面'
- en: '* * *'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解更多相关话题
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成为优秀数据科学家所需的5项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个初学者数据科学家应该掌握的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2021年最佳ETL工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，寻找目标去…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学统计学的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功数据科学家的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
