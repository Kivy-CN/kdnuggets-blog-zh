- en: 'PySpark SQL Cheat Sheet: Big Data in Python'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html/2](https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html/2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Filter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Filtering your Spark DataFrames is extremely easy: you use the filter() method
    on your DataFrame. Do make sure to pass in the condition on which you’re filtering!'
  prefs: []
  type: TYPE_NORMAL
- en: '![PySpark cheat sheet](../Images/223c3cc3091b988d6c36c70103a7dfd0.png)'
  prefs: []
  type: TYPE_IMG
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Sort
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Also sorting your Spark DataFrame is easy peasy: make use of the sort() and
    orderBy() methods, in combination with collect() to retrieve your sorted DataFrame.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PySpark cheat sheet](../Images/90419b84e9129e991b74bd6504cd8123.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Remember:** while show() prints the first n rows to the console, collect()
    returns all the records as a list of [Row](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.Row).
    Make sure that you use the latter when only when you’re working on small DataFrames,
    exactly because of what you have just read: the use of this method will make sure
    that *all* records are returned and will move all records back from the executor
    to the driver.'
  prefs: []
  type: TYPE_NORMAL
- en: Missing & Replacing Values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To handle missing values, you can replace, fill or drop them - you can use the
    replace(), fill() and drop() methods to do this.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** the replace() method takes two arguments, namely, the value that
    you want to be replacing and the value with which you want to replace the original
    value.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PySpark cheat sheet](../Images/91c90e62240d7b7ad8f2856aaede2e8a.png)'
  prefs: []
  type: TYPE_IMG
- en: Repartitioning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Repartitioning is something that you’ll find yourself doing from time to time.
    You can use this method to increase or decrease the number of partitions in your
    DataFrame.  However, you might want to consider the way you’re doing this operation.
    Consider the following examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PySpark cheat sheet](../Images/db874543f832e3f5e449d323028f8813.png)'
  prefs: []
  type: TYPE_IMG
- en: You see that both the repartition() and coalesce() methods are used to repartition
    the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note** that, while repartition() creates equal-sized data partitions by means
    of a full data shuffle, coalesce() avoids this full shuffle by combining existing
    partitions.'
  prefs: []
  type: TYPE_NORMAL
- en: Running SQL Queries Programmatically
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![PySpark cheat sheet](../Images/fb906355148f1cd2844876634560e38b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Registering DataFrames as Views**'
  prefs: []
  type: TYPE_NORMAL
- en: You register Spark DataFrames as views so that you can query it using “real”
    SQL queries (and not the variants with methods which you saw earlier in this article).
  prefs: []
  type: TYPE_NORMAL
- en: The first step in querying your Spark DataFrames with “real” SQL is, of course,
    registering your DataFrames as views. When you’re doing this, it’s important to
    keep the scope of your project in mind - as the name already suggests, createTempView()
    creates a temporary view, while createGlobalTempView() creates a global temporary
    view. The latter is shared among all sessions and is kept alive until the Spark
    application terminates, while the former is session-scoped and will disappear
    if the session that creates it terminates.
  prefs: []
  type: TYPE_NORMAL
- en: Note that you can also use createOrReplaceTempView() to create a temporary view
    if there is none or replace one if there is already a view present.
  prefs: []
  type: TYPE_NORMAL
- en: '**Query Views**'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have made some views, as in the example above, where you made the
    views “people” and “customer”, you can use the sql() function on a SparkSession
    to run SQL queries programmatically - The result of this operation will be another
    DataFrame, as you can see in the examples above.
  prefs: []
  type: TYPE_NORMAL
- en: Output
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Lastly, there’s the output to consider: your analyses will always have some
    results and you’ll want to save these results to another file, a Pandas DataFrame,
    … This section will cover some of the options that you have available to do this.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PySpark cheat sheet](../Images/fe06a19dff8edd82cc35474174d8623e.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Data Structures**'
  prefs: []
  type: TYPE_NORMAL
- en: As already mentioned above, you can convert your Spark DataFrame into an RDD,
    a Pandas DataFrame, an RDD of string, … There are many possible data structures
    to which you can convert your Spark DataFrame but the three listed in the image
    above are probably the most common ones to be used.
  prefs: []
  type: TYPE_NORMAL
- en: '**Write & Save to Files**'
  prefs: []
  type: TYPE_NORMAL
- en: Writing and saving the results of your analysis is of key importance whenever
    you’re doing data science - The same holds, of course, when you’re working with
    big data. You’ll want to save your results to JSON or parquet files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember: parquet files have a columnar file format available to any project
    in the Hadoop ecosystem. It doesn’t matter which data processing framework, data
    model or programming language you’re using! Parquet files provide efficient data
    compression and encoding schemes with enhanced performance to handle complex data
    in bulk. JSON or JavaScript Object Notation files, on the other hand, have an
    open-standard file format that uses human-readable text to transmit data objects
    consisting of attribute–value pairs and array data types (or any other serializable
    value).'
  prefs: []
  type: TYPE_NORMAL
- en: Stopping SparkSession
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before you leave, don’t forget to stop your SparkSession! Remember that you
    have assigned your SparkSession to the spark variable - You’ll need this exact
    variable and the stop() method to correctly stop your SparkSession.
  prefs: []
  type: TYPE_NORMAL
- en: '![PySpark cheat sheet](../Images/20516c91f72dfe0e86f6b1a3d0c02d4f.png)'
  prefs: []
  type: TYPE_IMG
- en: This is just the beginning of your journey with Spark SQL. We hope that you
    don’t forget to keep your [cheat sheet](https://www.datacamp.com/community/blog/bokeh-cheat-sheet-python)
    handy, of course, when you continue learning more about this exciting technology,
    its applications and its possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: '[**DataCamp**](https://www.datacamp.com/) is an online interactive education
    platform that that focuses on building the best learning experience specifically
    for Data Science. Our courses on [R](https://www.datacamp.com/courses?learn=r_programming),
    [Python](https://www.datacamp.com/courses?learn=python_programming), [SQL](https://www.datacamp.com/courses/tech:sql),
    and [Data Science](https://www.datacamp.com/courses) are built around a certain
    topic, and combine video instruction with in-browser coding challenges so that
    you can learn by doing. [You can start every course for free](https://www.datacamp.com/courses),
    whenever you want, wherever you want.'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Karlijn Willems**](https://www.linkedin.com/in/karlijnwillems) is a data
    science journalist and writes for the [DataCamp community](https://www.datacamp.com/community/authors/karlijn-willems),
    focusing on data science education, the latest news and the hottest trends. She
    holds degrees in Literature and Linguistics and Information Management.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Keras Cheat Sheet: Deep Learning in Python](/2017/09/datacamp-keras-cheat-sheet-deep-learning-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pandas Cheat Sheet: Data Science and Data Wrangling in Python](/2017/01/pandas-cheat-sheet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Bokeh Cheat Sheet: Data Visualization in Python](/2017/03/bokeh-cheat-sheet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PySpark for Data Science](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Validation for PySpark Applications using Pandera](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Cleaning with Python Cheat Sheet](https://www.kdnuggets.com/2023/02/data-cleaning-python-cheat-sheet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Best Python Tools for Building Generative AI Applications Cheat Sheet](https://www.kdnuggets.com/2023/08/best-python-tools-generative-ai-cheat-sheet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python Control Flow Cheat Sheet](https://www.kdnuggets.com/2022/11/python-control-flow-cheatsheet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, July 5: A Rotten Data Science Project • 10 AI…](https://www.kdnuggets.com/2023/n24.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
