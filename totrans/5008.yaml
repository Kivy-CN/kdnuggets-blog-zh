- en: How to Rank 10% in Your First Kaggle Competition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在第一次Kaggle比赛中排名前10%
- en: 原文：[https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3)
- en: '**Model Training**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型训练**'
- en: We can improve a model’s performance by tuning its parameters. A model usually
    have many parameters, but only a few of them are significant to its performance.
    For example, the most important parameters for a random forset is the number of
    trees in the forest and the maximum number of features used in developing each
    tree. **We need to understand how models work and what impact does each parameter
    have to the model’s performance, be it accuracy, robustness or speed.**
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调整参数来提升模型的性能。一个模型通常有很多参数，但只有其中少数对其性能至关重要。例如，对于随机森林，最重要的参数是森林中的树木数量和每棵树使用的最大特征数量。**我们需要理解模型如何工作以及每个参数对模型性能的影响，无论是准确性、鲁棒性还是速度。**
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的IT组织'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Normally we would find the best set of parameters by a process called **[grid
    search](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html))**.
    Actually what it does is simply iterating through all the possible combinations
    and find the best one.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们会通过一个称为**[网格搜索](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html))**的过程找到最佳参数组合。实际上，它所做的就是遍历所有可能的组合并找到最佳的一组。
- en: By the way, random forest usually reach optimum when `max_features` is set to
    the square root of the total number of features.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，随机森林通常在`max_features`设置为特征总数的平方根时达到最佳状态。
- en: 'Here I’d like to stress some points about tuning XGB. These parameters are
    generally considered to have real impacts on its performance:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我想强调一些关于调整XGB的要点。这些参数通常被认为对其性能有实际影响：
- en: '`eta`: Step size used in updating weights. Lower `eta` means slower training
    but better convergence.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta`：用于更新权重的步长。较低的`eta`意味着训练较慢但收敛性更好。'
- en: '`num_round`: Total number of iterations.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_round`：总迭代次数。'
- en: '`subsample`: The ratio of training data used in each iteration. This is to
    combat overfitting.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subsample`：每次迭代中使用的训练数据比例。这是为了防止过拟合。'
- en: '`colsample_bytree`: The ratio of features used in each iteration. This is like `max_features` in `RandomForestClassifier`.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`colsample_bytree`：每次迭代中使用的特征比例。这类似于`RandomForestClassifier`中的`max_features`。'
- en: '`max_depth`: The maximum depth of each tree. Unlike random forest, **gradient
    boosting would eventually overfit if we do not limit its depth**.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：每棵树的最大深度。与随机森林不同，**梯度提升如果不限制其深度最终会过拟合**。'
- en: '`early_stopping_rounds`: If we don’t see an increase of validation score for
    a given number of iterations, the algorithm will stop early. This is to combat
    overfitting, too.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`early_stopping_rounds`：如果在给定的迭代次数内没有看到验证评分的提升，算法将提前停止。这也是为了防止过拟合。'
- en: 'Usual tuning steps:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通常的调整步骤：
- en: Reserve a portion of training set as the validation set.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保留一部分训练集作为验证集。
- en: Set `eta` to a relatively high value (e.g. 0.05 ~ 0.1), `num_round` to 300 ~
    500.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`eta`设置为相对较高的值（例如0.05 ~ 0.1），`num_round`设置为300 ~ 500。
- en: Use grid search to find the best combination of other parameters.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用网格搜索找到其他参数的最佳组合。
- en: Gradually lower `eta` until we reach the optimum.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逐渐降低`eta`直到我们达到最佳状态。
- en: '**Use the validation set as `watch_list` to re-train the model with the best
    parameters. Observe how score changes on validation set in each iteration. Find
    the optimal value for `early_stopping_rounds`.**'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用验证集作为`watch_list`，用最佳参数重新训练模型。观察每次迭代中验证集上的得分变化。找到`early_stopping_rounds`的最佳值。**'
- en: Finally, note that models with randomness all have a parameter like `seed` or `random_state` to
    control the random seed. **You must record this** with all other parameters when
    you get a good model. Otherwise you wouldn’t be able to reproduce it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，所有具有随机性的模型都有一个类似于`seed`或`random_state`的参数来控制随机种子。**你必须记录下这一点**以及其他所有参数，当你得到一个好的模型时。否则你将无法重现它。
- en: '**Cross Validation**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉验证**'
- en: '**[Cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))** is
    an essential step in model training. It tells us whether our model is at high
    risk of overfitting. In many competitions, public LB scores are not very reliable.
    Often when we improve the model and get a better local CV score, the LB score
    becomes worse. **It is widely believed that we should trust our CV scores under
    such situation.**Ideally we would want **CV scores obtained by different approaches
    to improve in sync with each other and with the LB score**, but this is not always
    possible.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**[交叉验证](https://en.wikipedia.org/wiki/Cross-validation_(statistics))**是模型训练中的一个关键步骤。它告诉我们模型是否有很高的过拟合风险。在许多比赛中，公共LB分数并不非常可靠。通常，当我们改进模型并获得更好的本地交叉验证得分时，LB分数会变得更差。**在这种情况下，人们普遍认为我们应该相信我们的交叉验证得分。**理想情况下，我们希望**不同方法获得的交叉验证得分与LB分数同步提升**，但这并不总是可能的。'
- en: Usually **5-fold CV** is good enough. If we use more folds, the CV score would
    become more reliable, but the training takes longer to finish as well. However,
    we shouldn’t use too many folds if our training data is limited. Otherwise we
    would have too few samples in each fold to guarantee statistical significance.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通常**5折交叉验证**足够好了。如果我们使用更多折，交叉验证得分会更可靠，但训练也需要更长时间才能完成。然而，如果我们的训练数据有限，我们不应该使用太多折。否则，每一折中的样本可能太少，无法保证统计显著性。
- en: How to do CV properly is not a trivial problem. It requires constant experiment
    and case-by-case discussion. Many Kagglers share their CV approaches (like [this
    one](https://www.kaggle.com/c/telstra-recruiting-network/forums/t/19277/what-is-your-cross-validation-method))
    after competitions when they feel that reliable CV is not easy.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如何正确进行交叉验证并不是一个简单的问题。这需要不断的实验和逐案讨论。许多Kaggle竞赛者在比赛后分享他们的交叉验证方法（比如[这个](https://www.kaggle.com/c/telstra-recruiting-network/forums/t/19277/what-is-your-cross-validation-method)），当他们觉得可靠的交叉验证并不容易时。
- en: '**Ensemble Generation**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**集成生成**'
- en: '[Ensemble Learning](https://en.wikipedia.org/wiki/Ensemble_learning) refers
    to the technique of combining different models. It **reduces both bias and variance
    of the final model** (you can find a proof [here](http://link.springer.com/chapter/10.1007%2F3-540-33019-4_19)),
    thus **increasing the score and reducing the risk of overfitting**. Recently it
    became virtually impossible to win prize without using ensemble in Kaggle competitions.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[集成学习](https://en.wikipedia.org/wiki/Ensemble_learning)指的是结合不同模型的技术。它**减少了最终模型的偏差和方差**（你可以在[这里](http://link.springer.com/chapter/10.1007%2F3-540-33019-4_19)找到证明），从而**提高得分并减少过拟合的风险**。最近，在Kaggle竞赛中几乎不可能在不使用集成学习的情况下获奖。'
- en: 'Common approaches of ensemble learning are:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习的常见方法有：
- en: '**Bagging**: Use different random subsets of training data to train each base
    model. Then all the base models vote to generate the final predictions. This is
    how random forest works.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自助法**：使用训练数据的不同随机子集来训练每个基础模型。然后所有基础模型进行投票以生成最终预测。这就是随机森林的工作原理。'
- en: '**Boosting**: Train base models iteratively, modify the weights of training
    samples according to the last iteration. This is how gradient boosted trees work.
    (Actually it’s not the whole story. Apart from boosting, GBTs try to learn the
    residuals of earlier iterations.) It performs better than bagging but is more
    prone to overfitting.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升**：迭代训练基础模型，根据上一轮迭代修改训练样本的权重。这就是梯度提升树的工作原理。（实际上，这并不是全部。除了提升，GBT还尝试学习早期迭代的残差。）它比自助法表现更好，但更容易过拟合。'
- en: '**Blending**: Use non-overlapping data to train different base models and take
    a weighted average of them to obtain the final predictions. This is easy to implement
    but uses less data.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合**：使用不重叠的数据训练不同的基础模型，并对它们取加权平均值以获得最终预测。这种方法易于实现，但数据使用较少。'
- en: '**Stacking**: To be discussed next.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**堆叠**：将稍后讨论。'
- en: 'In theory, for the ensemble to perform well, two factors matter:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，为了使集成表现良好，有两个因素需要考虑：
- en: '**Base models should be as unrelated as possibly**. This is why we tend to
    include non-tree-based models in the ensemble even though they don’t perform as
    well. The math says that the greater the diversity, and less bias in the final
    ensemble.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础模型应该尽可能不相关**。这就是为什么我们倾向于在集成中包括非树模型，即使它们的表现不如树模型。数学表明，多样性越大，最终集成的偏差越小。'
- en: '**Performance of base models shouldn’t differ to much.**'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础模型的表现不应相差太大。**'
- en: Actually we have a **trade-off** here. In practice we may end up with highly
    related models of comparable performances. Yet we ensemble them anyway because
    it usually increase the overall performance.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们在这里有一个**权衡**。在实践中，我们可能会得到性能相当的高度相关模型。然而，我们还是将它们集成在一起，因为这通常会提高整体性能。
- en: '**Stacking**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**堆叠**'
- en: 'Compared with blending, stacking makes better use of training data. Here’s
    a diagram of how it works:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 与混合相比，堆叠更好地利用了训练数据。这里是它如何工作的示意图：
- en: '[![Stacking](../Images/fb2fd03a6a8cf5851969a1254b354966.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-diagram-stacking.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[![堆叠](../Images/fb2fd03a6a8cf5851969a1254b354966.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-diagram-stacking.jpg)'
- en: '*(Taken from [Faron](https://www.kaggle.com/mmueller). Many thanks!)*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*（取自 [Faron](https://www.kaggle.com/mmueller)。非常感谢！）*'
- en: 'It’s much like cross validation. Take 5-fold stacking as an example. First
    we split the training data into 5 folds. Next we will do 5 iterations. In each
    iteration, train every base model on 4 folds and predict on the hold-out fold. **You
    have to keep the predictions on the testing data as well.** This way, in each
    iteration every base model will make predictions on 1 fold of the training data
    and all of the testing data. After 5 iterations we will obtain a matrix of shape `#(samples
    in training data) X #(base models)`. This matrix is then fed to the stacker (it’s
    just another model) in the second level. After the stacker is fitted, use the
    predictions on testing data by base models (**each base model is trained 5 times,
    therefore we have to take an average to obtain a matrix of the same shape**) as
    the input for the stacker and obtain our final predictions.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '这很像交叉验证。以5折堆叠为例。首先，我们将训练数据分成5折。接下来，我们将进行5次迭代。在每次迭代中，对4折数据训练每个基础模型，并在保留折上进行预测。**你也必须保留测试数据上的预测。**这样，在每次迭代中，每个基础模型都会对训练数据的1折和所有测试数据进行预测。经过5次迭代后，我们将得到一个形状为`#(训练数据样本数)
    X #(基础模型数)`的矩阵。这个矩阵然后被输入到第二层的堆叠器（它只是另一个模型）中。在堆叠器拟合后，使用基础模型对测试数据的预测（**每个基础模型训练5次，因此我们需要取平均值以获得相同形状的矩阵**）作为堆叠器的输入，得到最终预测。'
- en: 'Maybe it’s better to just show the codes:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 也许展示代码更好：
- en: Prize winners usually have larger and much more complicated ensembles. For beginner,
    implementing a correct 5-fold stacking is good enough.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 奖项获得者通常拥有更大且更复杂的集成。对于初学者来说，实现一个正确的5折堆叠就足够了。
- en: '***Pipeline**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '***管道***'
- en: 'We can see that the workflow for a Kaggle competition is quite complex, especially
    for model selection and ensemble. Ideally, we need a highly automated pipeline
    capable of:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，Kaggle竞赛的工作流程非常复杂，特别是在模型选择和集成方面。理想情况下，我们需要一个高度自动化的管道，能够：
- en: '**Modularized feature transformations**. We only need to write a few lines
    of codes (or better, rules / DSLs) and the new feature is added to the training
    set.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模块化特征转换**。我们只需编写几行代码（或者更好，规则/领域特定语言），新的特征就会被添加到训练集中。'
- en: '**Automated grid search**. We only need to set up models and parameter grid,
    the search will be run and the best parameters will be recorded.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化网格搜索**。我们只需设置模型和参数网格，搜索将运行，最佳参数将被记录。'
- en: '**Automated ensemble selection**. Use K best models for training the ensemble
    as soon as we put another base model into the pool.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化集成选择**。当我们将另一个基础模型加入池中时，使用最佳的K个模型来训练集成。'
- en: For beginners, the first one is not very important because the number of features
    is quite manageable; the third one is not important either because typically we
    only do several ensembles at the end of the competition. But the second one is
    good to have because**manually recording the performance and parameters of each
    model is time-consuming and error-prone**.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于初学者来说，第一个不是很重要，因为特征数量是相当可控的；第三个也不重要，因为通常我们只会在竞赛结束时做几次集成。但第二个是有用的，因为**手动记录每个模型的表现和参数既费时又容易出错**。
- en: '[Chenglong Chen](https://www.kaggle.com/chenglongchen), the winner of [Crowdflower
    Search Results Relevance](https://www.kaggle.com/c/crowdflower-search-relevance),
    once released his pipeline on [GitHub](https://github.com/ChenglongChen/Kaggle_CrowdFlower).
    It’s very complete and efficient. Yet it’s very hard to understand and extract
    all his logic to build a general framework. This is something you might want to
    do when you have plenty of time.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[程龙·陈](https://www.kaggle.com/chenglongchen)，[Crowdflower 搜索结果相关性比赛](https://www.kaggle.com/c/crowdflower-search-relevance)的获胜者，曾在[GitHub](https://github.com/ChenglongChen/Kaggle_CrowdFlower)上发布了他的管道。它非常完整和高效，但理解和提取他的所有逻辑以构建通用框架非常困难。这可能是你有足够时间时想做的事情。'
- en: More On This Topic
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[How LinkedIn Uses Machine Learning To Rank Your Feed](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LinkedIn 如何利用机器学习来排名你的动态](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)'
- en: '[How to Get Your First Job in Data Science without Any Work Experience](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在没有任何工作经验的情况下获得你的第一个数据科学工作](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)'
- en: '[Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 TensorFlow 和 Keras 构建并训练你的第一个神经网络](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
- en: '[It''s alive! Build your first robots with Python and some cheap,…](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[它活了！使用 Python 和一些便宜的组件构建你的第一个机器人，…](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
- en: '[From Zero to Hero: Create Your First ML Model with PyTorch](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从零到英雄：使用 PyTorch 创建你的第一个机器学习模型](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)'
- en: '[Deploying Your First Machine Learning Model](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[部署你的第一个机器学习模型](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)'
