- en: Image Classification with Convolutional Neural Networks (CNNs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/05/image-classification-convolutional-neural-networks-cnns.html](https://www.kdnuggets.com/2022/05/image-classification-convolutional-neural-networks-cnns.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Image Classification with Convolutional Neural Networks (CNNs)](../Images/d31df1465f041fd9f4014c1e72487e2e.png)'
  prefs: []
  type: TYPE_IMG
- en: What Is A Convolutional Neural Network (CNN)?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: A Convolutional Neural Network is a special class of neural networks that are
    built with the ability to extract unique features from image data. For instance,
    they are used in face detection and recognition because they can identify complex
    features in image data.
  prefs: []
  type: TYPE_NORMAL
- en: How Do Convolutional Neural Networks Work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like other types of neural networks, CNNs consume numerical data.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the images fed to these networks must be converted to a numerical
    representation. Since images are made up of pixels, they are converted into a
    numerical form that is passed to the CNN.
  prefs: []
  type: TYPE_NORMAL
- en: However, as we will discuss in the upcoming section, the entire numerical representation
    is not passed into the network. To understand how this works, let’s look at some
    of the steps involved in training a CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reducing the size of the numerical representation sent to the CNN is done via
    the ***convolution*** operation. This process is vital so that only features that
    are important in classifying an image are sent to the neural network. Apart from
    improving the accuracy of the network, this also ensures that minimal compute
    resources are used in training the network.
  prefs: []
  type: TYPE_NORMAL
- en: The result of the convolution operation is referred to as a **feature map**,
    **convolved feature**, or **activation map**. Applying a **feature detector**
    is what leads to a feature map. The feature detector is also known by other names
    such as **kernel** or **filter**.
  prefs: []
  type: TYPE_NORMAL
- en: The kernel is usually a 3 by 3 matrix. Performing an element-wise multiplication
    of the kernel with the input image and summing the values, outputs the feature
    map. This is done by sliding the kernel on the input image. The sliding happens
    in steps known as **strides**. The strides and the size of the kernel can be set
    manually when creating the CNN.
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolution](../Images/f1b3bab4eaa566a6a47f03c428a4caf7.png)'
  prefs: []
  type: TYPE_IMG
- en: '[A 3 by 3 convolutions operation. ](https://developers.google.com/machine-learning/practica/image-classification/convolutional-neural-networks)'
  prefs: []
  type: TYPE_NORMAL
- en: For example, given a 5 by 5 input, a kernel of 3 by 3 will output a 3 by 3 output
    feature map.
  prefs: []
  type: TYPE_NORMAL
- en: Padding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the above operations, we have seen that the size of the feature map reduces
    as part of applying the convolution operation. What if you want the size of the
    feature map to be the same size as that of the input image? That is achieved through
    padding.
  prefs: []
  type: TYPE_NORMAL
- en: Padding involves increasing the size of the input image by “padding” the images
    with zeros. As a result, applying the filter to the image leads to a feature map
    of the same size as the input image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Padding](../Images/12665ab6d814838225950e1f750f45ca.png)'
  prefs: []
  type: TYPE_IMG
- en: '[The uncolored area represents the padded area. ](https://commons.wikimedia.org/wiki/File:Convolution_arithmetic_-_Padding_strides.gif)'
  prefs: []
  type: TYPE_NORMAL
- en: Padding reduces the amount of information lost in the convolution operation.
    It also ensures that the edges of the images are factored more often in the convolution
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: When building the CNN, you will have the option to define the type of padding
    you want or no padding at all. The common options here are ***valid*** or ***same***.
    Valid means no padding will be applied while ***same*** means that padding will
    be applied so that the size of the feature map is the same as the size of the
    input image.
  prefs: []
  type: TYPE_NORMAL
- en: '![3 by3 kernel reduces a 5 by 5 input](../Images/3b544f10953e66d230c3775ff00f6337.png)'
  prefs: []
  type: TYPE_IMG
- en: '[A 3 by3 kernel reduces a 5 by 5 input to a 3 by 3 output ](https://developers.google.com/machine-learning/practica/image-classification/convolutional-neural-networks)'
  prefs: []
  type: TYPE_NORMAL
- en: Here is what the element-wise multiplication of the above feature map and filter
    would look like.
  prefs: []
  type: TYPE_NORMAL
- en: '![Element-wise multiplication of a 5 by input with a 3 by 3 filter](../Images/019f20ad536f4ec66373614bfe4c5f0a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Element-wise multiplication of a 5 by input with a 3 by 3 filter.](https://developers.google.com/machine-learning/practica/image-classification/convolutional-neural-networks)'
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Rectified Linear Unit (ReLU) transformation is applied after every convolution
    operation to ensure non-linearity. ReLU is the most popular activation function
    but there are [other activation functions](https://keras.io/api/layers/activations/)
    to choose from.
  prefs: []
  type: TYPE_NORMAL
- en: After the transformation, all values below zero are returned as zero while the
    other values are returned as they are.
  prefs: []
  type: TYPE_NORMAL
- en: '![ReLu function plot](../Images/fdf8fd39d5c14cdad49d37daee199912.png)'
  prefs: []
  type: TYPE_IMG
- en: '[ReLu function plot](https://www.researchgate.net/figure/ReLU-activation-function_fig3_319235847)'
  prefs: []
  type: TYPE_NORMAL
- en: Pooling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this operation, the size of the feature map is reduced further. There are
    various [pooling methods.](https://keras.io/api/layers/pooling_layers/)
  prefs: []
  type: TYPE_NORMAL
- en: A common technique is **max-pooling.** The size of the pooling filter is usually
    a 2 by 2 matrix. In max-pooling, the 2 by 2 filter slides over the feature map
    and picks the largest value in a given box. This operation results in a **pooled
    feature map**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Applying a 2 by 2 pooling filter to a 4 by 4 feature map](../Images/44d367354205b41010ff38d754b7142c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Applying a 2 by 2 pooling filter to a 4 by 4 feature map](https://developers.google.com/machine-learning/practica/image-classification/convolutional-neural-networks).'
  prefs: []
  type: TYPE_NORMAL
- en: Pooling forces the network to identify key features in the image irrespective
    of their location. The reduced image size also makes training the network faster.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Applying Dropout Regularization is a common practice in CNNs. This involves
    randomly dropping some nodes in layers so that they are not updated during back-propagation.
    This prevents overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Flattening
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flattening involves transforming the pooled feature map into a single column
    that is passed to the fully connected layer. This is a common practice during
    the transition from convolutional layers to fully connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The flattened feature map is then passed to a fully connected layer. There might
    be several fully connected layers depending on the problem and the network. The
    last fully connected layer is responsible for outputting the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: An activation function is used in the final layer depending on the type of problem.
    A [sigmoid activation](https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid)
    is used for binary classification, while a [softmax activation](https://keras.io/api/layers/activation_layers/softmax)
    function is used for multi-class image classification.
  prefs: []
  type: TYPE_NORMAL
- en: '![Fully connected convolutional neural network](../Images/f1b3bab4eaa566a6a47f03c428a4caf7.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Fully connected convolutional neural network](https://commons.wikimedia.org/wiki/File:Typical_cnn.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Why ConvNets Over Feed-forward Neural Nets?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having learned about CNNs, you might be wondering why we can’t use normal neural
    networks for image problems. Normal neural networks can’t extract complex features
    from images as CNNs can.
  prefs: []
  type: TYPE_NORMAL
- en: The ability of CNNs to extra features from images through the application of
    filters makes them a better fit for image problems. Also, feeding images directly
    into the feed-forward neural networks would be computationally expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Network Architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can always design your CNNs from scratch. However, you can also take advantage
    of numerous architectures that have been developed and released publicly. Some
    of these networks also come with pre-trained models that you can easily adapt
    for your use case. Some popular architectures include:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ResNet50](https://keras.io/api/applications/resnet/#resnet50-function)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[VGG19](https://keras.io/api/applications/vgg/#vgg19-function)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Xception](https://keras.io/api/applications/xception/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Inception](https://keras.io/api/applications/inceptionv3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can start using these architectures through [Keras applications](https://keras.io/api/applications/).
    For example, here is how to use VGG19:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Convolutional Neural Networks (CNN) In TensorFlow Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s now build a food classification CNN using a [food dataset](http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz).
    The dataset contains over a hundred thousand images belonging to 101 classes.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step is to download and extract the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Let’s take a look at one image from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![output](../Images/a9134fbc9b70c3c77835fb19d9f32dd4.png)'
  prefs: []
  type: TYPE_IMG
- en: Generate a tf.data.Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, load the images into a [TensorFlow dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).
    We’ll use 20% of the data for testing and the rest for training. Therefore, we
    have to create an `[ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator?authuser=1)`
    for the training and testing set.
  prefs: []
  type: TYPE_NORMAL
- en: The training set generator will also specify a couple of [image augmentation](https://www.tensorflow.org/tutorials/images/data_augmentation)
    techniques, such as zooming and flipping the images. Augmentation prevents overfitting
    in the network.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: With the generators at hand, the next step is to use them to load the food images
    from the base directory. While loading the images, we specify the target size
    of the images. All images will be resized to the specified size.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'When loading the images we also specify:'
  prefs: []
  type: TYPE_NORMAL
- en: The directory where the images are being loaded from.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The batch size, in this case 32, meaning that the images will be loaded in batches
    of 32.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The subset; whether it’s training or validation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The class mode as categorical since we have multiple classes. In the case of
    two classes this would be binary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Model definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step is to define the CNN model. The architecture of the network will
    look like the steps we discussed in the how CNNs work section. We’ll use the [Keras
    Sequential API](https://keras.io/api/models/sequential/) to define the network.
    CNNs are defined using the [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The Conv2D layer expects:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of filters to be applied, in this case, 32.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of the kernel, in this case, 3 by 3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of the input images. 200 by 200 is the size of the image and 3 indicates
    that it’s a colored image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The activation function; usually [ReLu](https://keras.io/api/layers/activation_layers/relu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the network, we apply pooling with a filter of 2 by 2 and apply a [Dropout](https://keras.io/api/layers/regularization_layers/dropout/)
    layer to prevent overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: The final layer has 101 units because there are 101 food classes. The activation
    function is [softmax](https://keras.io/api/layers/activation_layers/softmax) because
    it is a multiclass image classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the CNN model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We [compile](https://keras.io/api/models/model_training_apis) the network using
    categorical loss and accuracy because it involves multiples classes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Training the CNN model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s now train the CNN model. We apply the [EarlyStopping](https://keras.io/api/callbacks/early_stopping)
    callback in the training process so that training stops if the model doesn’t improve
    after a number of iterations. In this case 3 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The image dataset we are working with here is quite large. We therefore need
    to use GPUs to train this model. Let’s utilize free GPUs offered by [Layer](http://layer)
    to train the model. To do that, we need to bundle all the code we have developed
    above into a single function. This function should return a model. In this case,
    a [TensorFlow](http://link) model.
  prefs: []
  type: TYPE_NORMAL
- en: To use GPUs to train the model, just decorate the function with a GPU environment.
    This is specified using the [fabric decorator.](https://docs.app.layer.ai/docs/reference/fabrics)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Training the model is done by passing the training function to the `layer.run`
    function. If you wish to train the model on your local infrastructure, call the
    `train()` function normally.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![training](../Images/8ee120dd5167c4569cc8d3579c3294ef.png)'
  prefs: []
  type: TYPE_IMG
- en: Making predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the model ready, we can make predictions on a new image. This can be done
    in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Fetch the trained model from [Layer](http://layer.ai).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the image of the same size as the one used in the training images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert the image into an array.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transform the numbers in the array to be between 0 and 1 by dividing by 255\.
    The training images were of the same form.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expand the dimensions of the image to add a batch size of 1, since we are making
    predictions on a single image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Since this is a multiclass network, we’ll use the [softmax function](https://www.tensorflow.org/api_docs/python/tf/nn/softmax?authuser=1)
    to interpret the results. The function converts the [logits](https://developers.google.com/machine-learning/glossary?authuser=1#logits)
    to a probability for each class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article, we have covered CNNs at length. Specifically, we talked about:'
  prefs: []
  type: TYPE_NORMAL
- en: What are CNNs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How CNNs work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNN architectures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build a CNN for an image classification problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Google Colab notebook](https://colab.research.google.com/drive/1qfQijIFG7gva5TdehmL70NXmBfwCJEwe?usp=sharing)'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Derrick Mwiti](https://www.linkedin.com/in/mwitiderrick/)** is experienced
    in data science, machine learning, and deep learning with a keen eye for building
    machine learning communities.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[A Comprehensive Guide to Convolutional Neural Networks](https://www.kdnuggets.com/2023/06/comprehensive-guide-convolutional-neural-networks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a Convolutional Neural Network with PyTorch](https://www.kdnuggets.com/building-a-convolutional-neural-network-with-pytorch)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Guide to Train an Image Classification Model Using Tensorflow](https://www.kdnuggets.com/2022/12/guide-train-image-classification-model-tensorflow.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10 Simple Things to Try Before Neural Networks](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Neural Networks Don''t Lead Us Towards AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
