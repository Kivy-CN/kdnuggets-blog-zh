["```py\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n```", "```py\ntext = \"The rain in Spain falls mainly on the plain.\"\ndoc = nlp(text)\n\nfor token in doc:\n    print(token.text, token.lemma_, token.pos_, token.is_stop)\n```", "```py\nThe the DET True\nrain rain NOUN False\nin in ADP True\nSpain Spain PROPN False\nfalls fall VERB False\nmainly mainly ADV False\non on ADP True\nthe the DET True\nplain plain NOUN False\n. . PUNCT False\n```", "```py\nimport pandas as pd\n\ncols = (\"text\", \"lemma\", \"POS\", \"explain\", \"stopword\")\nrows = []\n\nfor t in doc:\n    row = [t.text, t.lemma_, t.pos_, spacy.explain(t.pos_), t.is_stop]\n    rows.append(row)\n\ndf = pd.DataFrame(rows, columns=cols)\n\ndf\n```", "```py\nfrom spacy import displacy\n\ndisplacy.render(doc, style=\"dep\")\n```", "```py\ntext = \"We were all out at the zoo one day, I was doing some acting, walking on the railing of the gorilla exhibit. I fell in. Everyone screamed and Tommy jumped in after me, forgetting that he had blueberries in his front pocket. The gorillas just went wild.\"\n\ndoc = nlp(text)\n\nfor sent in doc.sents:\n    print(\">\", sent)\n```", "```py\n> We were all out at the zoo one day, I was doing some acting, walking on the railing of the gorilla exhibit.\n> I fell in.\n> Everyone screamed and Tommy jumped in after me, forgetting that he had blueberries in his front pocket.\n> The gorillas just went wild.\n```", "```py\nfor sent in doc.sents:\n    print(\">\", sent.start, sent.end)\n```", "```py\n> 0 25\n> 25 29\n> 29 48\n> 48 54\n```", "```py\ndoc[48:54]\n```", "```py\nThe gorillas just went wild.\n```", "```py\ntoken = doc[51]\nprint(token.text, token.lemma_, token.pos_)\n```", "```py\nwent go VERB\n```", "```py\nimport sys\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n```", "```py\nfrom bs4 import BeautifulSoup\nimport requests\nimport traceback\n\ndef get_text (url): \n    buf = []\n\n    try:\n        soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n\n        for p in soup.find_all(\"p\"): \n            buf.append(p.get_text())\n\n        return \"\\n\".join(buf)\n    except:  \n        print(traceback.format_exc())\n        sys.exit(-1)\n```", "```py\nlic = {}\nlic[\"mit\"] = nlp(get_text(\"https://opensource.org/licenses/MIT\"))\nlic[\"asl\"] = nlp(get_text(\"https://opensource.org/licenses/Apache-2.0\"))\nlic[\"bsd\"] = nlp(get_text(\"https://opensource.org/licenses/BSD-3-Clause\"))\n\nfor sent in lic[\"bsd\"].sents: \n    print(\">\", sent)\n```", "```py\n> SPDX short identifier: BSD-3-Clause\n> Note: This license has also been called the \"New BSD License\" or  \"Modified BSD License\"\n> See also the 2-clause BSD License.\n…\n```", "```py\npairs = [\n    [\"mit\", \"asl\"], \n    [\"asl\", \"bsd\"], \n    [\"bsd\", \"mit\"]\n]\n\nfor a, b in pairs:\n    print(a, b, lic[a].similarity(lic[b]))\n```", "```py\nmit asl 0.9482039305669306\nasl bsd 0.9391555350757145\nbsd mit 0.9895838089575453\n```", "```py\ntext = \"Steve Jobs and Steve Wozniak incorporated Apple Computer on January 3, 1977, in Cupertino, California.\"\ndoc = nlp(text)\n\nfor chunk in doc.noun_chunks: \n    print(chunk.text)\n```", "```py\nSteve Jobs\nSteve Wozniak\nApple Computer\nJanuary\nCupertino\nCalifornia\n```", "```py\nfor ent in doc.ents:\n    print(ent.text, ent.label_)\n```", "```py\nSteve Jobs PERSON\nSteve Wozniak PERSON\nApple Computer ORG\nJanuary 3, 1977 DATE\nCupertino GPE\nCalifornia GPE\n```", "```py\ndisplacy.render(doc, style=\"ent\")\n```", "```py\nimport nltk\nnltk.download(\"wordnet\")\n\n[nltk_data] Downloading package wordnet to /home/ceteri/nltk_data...\n[nltk_data] Package wordnet is already up-to-date!\n```", "```py\nTrue\n```", "```py\nfrom spacy_wordnet.wordnet_annotator import WordnetAnnotator\n\nprint(\"before\", nlp.pipe_names)\n\nif \"WordnetAnnotator\" not in nlp.pipe_names:\n    nlp.add_pipe(WordnetAnnotator(nlp.lang), after=\"tagger\")\n\nprint(\"after\", nlp.pipe_names)\n```", "```py\nbefore ['tagger', 'parser', 'ner']\nafter ['tagger', 'WordnetAnnotator', 'parser', 'ner']\n```", "```py\ntoken = nlp(\"withdraw\")[0]\ntoken._.wordnet.synsets()\n```", "```py\n[Synset('withdraw.v.01'),\nSynset('retire.v.02'),\nSynset('disengage.v.01'), \nSynset('recall.v.07'), \nSynset('swallow.v.05'), \nSynset('seclude.v.01'), \nSynset('adjourn.v.02'),\nSynset('bow_out.v.02'), \nSynset('withdraw.v.09'), \nSynset('retire.v.08'), \nSynset('retreat.v.04'), \nSynset('remove.v.01')]\n```", "```py\ntoken._.wordnet.lemmas()\n```", "```py\n[Lemma('withdraw.v.01.withdraw'),\nLemma('withdraw.v.01.retreat'),\nLemma('withdraw.v.01.pull_away'), \nLemma('withdraw.v.01.draw_back'), \nLemma('withdraw.v.01.recede'),\nLemma('withdraw.v.01.pull_back'), \nLemma('withdraw.v.01.retire'),\n…\n```", "```py\ntoken._.wordnet.wordnet_domains()\n```", "```py\n['astronomy',\n'school',\n'telegraphy',\n'industry',\n'psychology',\n'ethnology',\n'ethnology',\n'administration',\n'school',\n'finance',\n'economy',\n'exchange',\n'banking',\n'commerce',\n'medicine',\n'ethnology', \n'university',\n…\n```", "```py\ndomains = [\"finance\", \"banking\"]\nsentence = nlp(\"I want to withdraw 5,000 euros.\")\n\nenriched_sent = []\n\nfor token in sentence:\n    # get synsets within the desired domains\n    synsets = token._.wordnet.wordnet_synsets_for_domain(domains)\n\n    if synsets:\n       lemmas_for_synset = []\n\n       for s in synsets:\n           # get synset variants and add to the enriched sentence\n           lemmas_for_synset.extend(s.lemma_names())\n           enriched_sent.append(\"({})\".format(\"|\".join(set(lemmas_for_synset))))\n    else:\n        enriched_sent.append(token.text)\n\nprint(\" \".join(enriched_sent))\n```", "```py\nI (require|want|need) to (draw_off|withdraw|draw|take_out) 5,000 euros .\n```", "```py\nimport scattertext as st\n\nif \"merge_entities\" not in nlp.pipe_names:\n    nlp.add_pipe(nlp.create_pipe(\"merge_entities\"))\n\nif \"merge_noun_chunks\" not in nlp.pipe_names:\n    nlp.add_pipe(nlp.create_pipe(\"merge_noun_chunks\"))\n\nconvention_df = st.SampleCorpora.ConventionData2012.get_data() \ncorpus = st.CorpusFromPandas(convention_df,\n                             category_col=\"party\",\n                             text_col=\"text\",\n                             nlp=nlp).build()\n```", "```py\nhtml = st.produce_scattertext_explorer(\n    corpus,\n    category=\"democrat\",\n    category_name=\"Democratic\",\n    not_category_name=\"Republican\",\n    width_in_pixels=1000,\n    metadata=convention_df[\"speaker\"]\n)\n```", "```py\nfrom IPython.display import IFrame\n\nfile_name = \"foo.html\"\n\nwith open(file_name, \"wb\") as f:\n     f.write(html.encode(\"utf-8\"))\n\nIFrame(src=file_name, width = 1200, height=700)\n```"]