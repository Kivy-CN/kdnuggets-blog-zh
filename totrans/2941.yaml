- en: Your Guide to Natural Language Processing (NLP)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）指南
- en: 原文：[https://www.kdnuggets.com/2019/05/guide-natural-language-processing-nlp.html](https://www.kdnuggets.com/2019/05/guide-natural-language-processing-nlp.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/05/guide-natural-language-processing-nlp.html](https://www.kdnuggets.com/2019/05/guide-natural-language-processing-nlp.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Diego Lopez Yse](https://twitter.com/LopezYse), Moody''s Operations LATAM**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：[Diego Lopez Yse](https://twitter.com/LopezYse)，穆迪拉美地区运营部**。'
- en: '![](../Images/fbc813c836c8dc6c3bb83c0cc408e48a.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbc813c836c8dc6c3bb83c0cc408e48a.png)'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的IT工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Everything we express (either verbally or in written) carries huge amounts of
    information. The topic we choose, our tone, our selection of words, everything
    adds some type of information that can be interpreted and value extracted from
    it. In theory, we can understand and even predict human behaviour using that information.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们表达的一切（无论是口头还是书面）都包含大量信息。我们选择的话题、语气、用词等，都添加了可以解读和提取价值的信息。从理论上讲，我们可以利用这些信息理解甚至预测人类行为。
- en: 'But there is a problem: one person may generate hundreds or thousands of words
    in a declaration, each sentence with its corresponding complexity. If you want
    to scale and analyze several hundreds, thousands or millions of people or declarations
    in a given geography, then the situation is unmanageable.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但存在一个问题：一个人可能会在声明中生成数百或数千个单词，每个句子都有其对应的复杂性。如果你想对特定地区的数百、数千或数百万个人或声明进行扩展和分析，那么情况将变得难以管理。
- en: Data generated from conversations, declarations or even tweets are examples
    of unstructured data. **Unstructured data** doesn’t fit neatly into the traditional
    row and column structure of relational databases, and represent the vast majority
    of data available in the actual world. It is messy and hard to manipulate. Nevertheless,
    thanks to the advances in disciplines like machine learning a big revolution is
    going on regarding this topic. Nowadays it is no longer about trying to interpret
    a text or speech based on its keywords (the old fashioned mechanical way), but
    about understanding the meaning behind those words (the cognitive way). This way
    it is possible to detect figures of speech like irony, or even perform sentiment
    analysis.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从对话、声明甚至推文中生成的数据是非结构化数据的例子。**非结构化数据**不能完全适应传统关系数据库的行列结构，代表了实际世界中绝大多数的数据。这些数据是杂乱的，难以操作。然而，感谢机器学习等学科的进步，关于这一主题正在进行一场重大变革。如今，不再是基于关键词（老式机械方法）来解读文本或语言，而是理解这些词汇背后的含义（认知方法）。这样可以检测到比喻等修辞手法，甚至进行情感分析。
- en: '***Natural Language Processing**** or NLP is a field of Artificial Intelligence
    that gives the machines the ability to read, understand and derive meaning from
    human languages.*'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***自然语言处理***或NLP是人工智能的一个领域，使机器能够读取、理解和从人类语言中推导出意义。'
- en: It is a discipline that focuses on the interaction between data science and
    human language, and is scaling to lots of industries. Today NLP is booming thanks
    to the huge improvements in the access to data and the increase in computational
    power, which are allowing practitioners to achieve meaningful results in areas
    like healthcare, media, finance and human resources, among others.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个关注数据科学与人类语言之间互动的学科，并正在扩展到许多行业。今天，得益于数据获取的巨大改进和计算能力的提高，NLP正蓬勃发展，这使得从业者能够在医疗、媒体、金融和人力资源等领域取得有意义的成果。
- en: '**Use Cases of NLP**'
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**NLP的应用案例**'
- en: In simple terms, NLP represents the automatic handling of natural human language
    like speech or text, and although the concept itself is fascinating, the real
    value behind this technology comes from the use cases.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，NLP代表了对自然人类语言（如语音或文本）的自动处理，尽管这一概念本身很吸引人，但这种技术的真正价值来自于它的应用案例。
- en: 'NLP can help you with lots of tasks and the fields of application just seem
    to increase on a daily basis. Let’s mention some examples:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: NLP可以帮助你完成许多任务，并且应用领域似乎每天都在增加。举几个例子：
- en: NLP enables the recognition and **prediction of diseases **based on electronic
    health records and patient’s own speech. This capability is being explored in
    health conditions that go from cardiovascular diseases to depression and even
    schizophrenia. For example, Amazon Comprehend Medical is a service that uses NLP
    to [extract disease conditions](https://www.thenewsminute.com/article/tech-giants-india-join-ai-bandwagon-focus-healthcare-93833),
    medications and treatment outcomes from patient notes, clinical trial reports
    and other electronic health records.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP能够基于电子健康记录和患者自身的语音**识别和预测疾病**。这一能力正在被探索用于从心血管疾病到抑郁症甚至精神分裂症的健康状况。例如，Amazon
    Comprehend Medical是一个使用NLP的服务，用于[提取疾病状况](https://www.thenewsminute.com/article/tech-giants-india-join-ai-bandwagon-focus-healthcare-93833)、药物和治疗结果，从患者笔记、临床试验报告和其他电子健康记录中提取信息。
- en: Organizations can determine what customers are saying about a service or product
    by identifying and extracting information in sources like social media. This [**sentiment
    analysis**](https://towardsdatascience.com/sentiment-analysis-concept-analysis-and-applications-6c94d6f58c17)can
    provide a lot of information about customers choices and their decision drivers.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组织可以通过识别和提取来自社交媒体等来源的信息，了解客户对服务或产品的评价。这个[**情感分析**](https://towardsdatascience.com/sentiment-analysis-concept-analysis-and-applications-6c94d6f58c17)能够提供大量关于客户选择及其决策驱动因素的信息。
- en: '[An inventor at IBM developed a **cognitive assistant**](https://www.theatlantic.com/technology/archive/2016/01/sorry-dave-afraid-i-cant-do-that/431559/)that
    works like a personalized search engine by learning all about you and then remind
    you of a name, a song, or anything you can’t remember the moment you need it to.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[IBM的一位发明家开发了一个**认知助手**](https://www.theatlantic.com/technology/archive/2016/01/sorry-dave-afraid-i-cant-do-that/431559/)，它像一个个性化的搜索引擎一样，通过学习你的所有信息来提醒你某个名字、歌曲或任何你需要但暂时记不起的东西。'
- en: Companies like Yahoo and Google filter and classify your emails with NLP by
    analyzing text in emails that flow through their servers and **stopping spam**before
    they even enter your inbox.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像Yahoo和Google这样的公司使用NLP来过滤和分类你的电子邮件，通过分析流经其服务器的电子邮件文本，**阻止垃圾邮件**在进入你的收件箱之前。
- en: To help **identifying fake news**, the [NLP Group at MIT](http://nlp.csail.mit.edu/)developed
    a new system to determine if a source is accurate or politically biased, detecting
    if a news source can be trusted or not.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了帮助**识别虚假新闻**，[MIT的NLP小组](http://nlp.csail.mit.edu/)开发了一个新系统，用于确定一个来源是否准确或政治偏见，检测新闻来源是否可信。
- en: Amazon’s Alexa and Apple’s Siri are examples of intelligent **voice driven interfaces**that
    use NLP to respond to vocal prompts and do everything like find a particular shop,
    tell us the weather forecast, suggest the best route to the office or turn on
    the lights at home.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊的Alexa和苹果的Siri是智能**语音驱动界面**的例子，它们使用NLP响应语音提示，执行如查找特定商店、告知天气预报、建议最佳路线到办公室或在家中开灯等任务。
- en: 'Having an insight into what is happening and what people are talking about
    can be very valuable to [**financial traders**](https://news.efinancialcareers.com/nl-en/331386/charles-elkan-goldman-sachs-machine-learning).
    NLP is being used to track news, reports, comments about possible mergers between
    companies, everything can be then incorporated into a trading algorithm to generate
    massive profits. Remember: buy the rumor, sell the news.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解发生了什么以及人们在谈论什么，对[**金融交易员**](https://news.efinancialcareers.com/nl-en/331386/charles-elkan-goldman-sachs-machine-learning)来说可能非常有价值。NLP被用来追踪新闻、报告、关于公司可能合并的评论，一切都可以被纳入交易算法中，以生成巨额利润。记住：买谣言，卖新闻。
- en: NLP is also being used in both the search and selection phases of [**talent
    recruitment**](https://www.forbes.com/sites/forbeshumanresourcescouncil/2018/09/27/how-ai-makes-recruiting-more-human/#7531fc116ba4),
    identifying the skills of potential hires and also spotting prospects before they
    become active on the job market.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP 还被应用于[**人才招聘**](https://www.forbes.com/sites/forbeshumanresourcescouncil/2018/09/27/how-ai-makes-recruiting-more-human/#7531fc116ba4)的搜索和筛选阶段，识别潜在候选人的技能，并在他们尚未活跃于就业市场之前发现潜在人才。
- en: Powered by IBM Watson NLP technology, [LegalMation](https://www.legalmation.com/)developed
    a platform to automate routine** litigation tasks** and help legal teams save
    time, drive down costs and shift strategic focus.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由 IBM Watson NLP 技术支持，[LegalMation](https://www.legalmation.com/) 开发了一个平台，用于自动化常规**诉讼任务**，帮助法律团队节省时间、降低成本并转移战略重点。
- en: NLP is particularly booming in the **healthcare industry**. This technology
    is improving care delivery, disease diagnosis and bringing costs down while healthcare
    organizations are going through a growing adoption of electronic health records.
    The fact that clinical documentation can be improved means that patients can be
    better understood and benefited through better healthcare. The goal should be
    to optimize their experience, and several organizations are already working on
    this.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 在**医疗行业**特别蓬勃发展。这项技术正在改善护理服务、疾病诊断并降低成本，同时医疗组织正在逐步采用电子健康记录。临床文档的改进意味着患者可以通过更好的医疗服务得到更好的理解和受益。目标应该是优化他们的体验，多个组织已经在为此努力。
- en: '![](../Images/d488f02f8a32cceea5f6df26d4df9ef6.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d488f02f8a32cceea5f6df26d4df9ef6.png)'
- en: Number of publications containing the sentence “natural language processing”
    in PubMed in the period 1978–2018\. As of 2018, PubMed comprised more than 29
    million citations for biomedical literature
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在 1978-2018 年期间，PubMed 中包含“自然语言处理”这一句子的出版物数量。截至 2018 年，PubMed 包含了超过 2900 万条生物医学文献的引用
- en: Companies like [Winterlight Labs](https://winterlightlabs.com/) are making huge
    improvements in the treatment of Alzheimer’s disease by monitoring cognitive impairment
    through speech and they can also support clinical trials and studies for a wide
    range of central nervous system disorders. Following a similar approach, Stanford
    University developed [Woebot](https://woebot.io/), a **chatbot therapist** with
    the aim of helping people with anxiety and other disorders.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 像[Winterlight Labs](https://winterlightlabs.com/)这样的公司，通过监测语言中的认知障碍在阿尔茨海默病治疗上取得了巨大进展，它们还可以支持各种中枢神经系统疾病的临床试验和研究。遵循类似的方法，斯坦福大学开发了[Woebot](https://woebot.io/)，一个**聊天机器人治疗师**，旨在帮助有焦虑症和其他障碍的人。
- en: But serious [controversy](https://www.bmj.com/content/358/bmj.j3159) is around
    the subject. A couple of years ago Microsoft demonstrated that by analyzing large
    samples of search engine queries, they could [identify internet users who were
    suffering from pancreatic cancer](https://www.nytimes.com/2016/06/08/technology/online-searches-can-identify-cancer-victims-study-finds.html) even
    before they have received a diagnosis of the disease. How would users react to
    such diagnosis? And what would happen if you were tested as a false positive?
    (meaning that you can be diagnosed with the disease even though you don’t have
    it). This recalls the case of Google Flu Trends which in 2009 was announced as
    being able to predict influenza but later on vanished due to its low accuracy
    and inability to meet its projected rates.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 但围绕这一主题存在严重的[争议](https://www.bmj.com/content/358/bmj.j3159)。几年前，微软展示了通过分析大量搜索引擎查询，他们能够[识别出正在患有胰腺癌的互联网用户](https://www.nytimes.com/2016/06/08/technology/online-searches-can-identify-cancer-victims-study-finds.html)，甚至在他们收到疾病诊断之前。用户对这种诊断会有什么反应？如果你被测试为假阳性（即你被诊断为有疾病，尽管实际上没有），会发生什么？这让人想起了谷歌流感趋势，2009
    年宣布能够预测流感，但后来由于准确性低和未能达到预期效果而消失。
- en: NLP may be the key to an effective clinical support in the future, but there
    are still many challenges to face in the short term.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 可能是未来有效临床支持的关键，但在短期内仍面临许多挑战。
- en: '**Basic NLP to impress your non-NLP friends**'
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**基础 NLP 让你的非 NLP 朋友印象深刻**'
- en: 'The main drawbacks we face these days with NLP relate to the fact that language
    is very tricky. The process of understanding and manipulating language is extremely
    complex, and for this reason it is common to use different techniques to handle
    different challenges before binding everything together. Programming languages
    like Python or R are highly used to perform these techniques, but before diving
    into code lines (that will be the topic of a different article), it’s important
    to understand the concepts beneath them. Let’s summarize and explain some of the
    most frequently used algorithms in NLP when defining the vocabulary of terms:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在面临的主要问题与自然语言处理（NLP）有关，因为语言非常复杂。理解和处理语言的过程极其复杂，因此通常使用不同的技术来处理不同的挑战，然后将一切结合起来。像
    Python 或 R 这样的编程语言被广泛用于执行这些技术，但在深入代码行（这将是另一篇文章的话题）之前，了解其背后的概念非常重要。让我们总结并解释一些在定义词汇表时最常用的
    NLP 算法：
- en: '**Bag of Words**'
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**词袋模型**'
- en: Is a commonly used model that allows you to count all words in a piece of text.
    Basically it creates an occurrence matrix for the sentence or document, disregarding
    grammar and word order. These word frequencies or occurrences are then used as
    features for training a classifier.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 是一种常用模型，允许你计算一段文本中的所有单词。基本上，它为句子或文档创建一个出现矩阵，而忽略语法和单词顺序。这些单词频率或出现次数随后被用作训练分类器的特征。
- en: 'To bring a short example I took the first sentence of the song “Across the
    Universe” from The Beatles:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供一个简短的例子，我取了披头士乐队歌曲《Across the Universe》的第一句：
- en: '*Words are flowing out like endless rain into a paper cup,*'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*单词像无尽的雨水流入纸杯中*'
- en: ''
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*They slither while they pass, they slip away across the universe*'
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*他们在穿过宇宙时蜿蜒滑动*'
- en: 'Now let’s count the words:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来计算单词数量：
- en: '![](../Images/cda94d22ba9f20d78ed2e98df51abcaa.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cda94d22ba9f20d78ed2e98df51abcaa.png)'
- en: This approach may reflect several downsides like the absence of semantic meaning
    and context, and the facts that stop words (like “the” or “a”) add noise to the
    analysis and some words are not weighted accordingly (“universe” weights less
    than the word “they”).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可能会反映出一些缺点，比如缺乏语义意义和上下文，以及停用词（如“the”或“a”）为分析增加噪声，并且一些单词的权重不一致（“universe”的权重低于“they”）。
- en: To solve this problem, one approach is to rescale the frequency of words by
    how often they appear in all texts (not just the one we are analyzing) so that
    the scores for frequent words like “the”, that are also frequent across other
    texts, get penalized. This approach to scoring is called **“Term Frequency ****—****Inverse
    Document Frequency****”** **(TFIDF)**, and improves the bag of words by weights.
    Through TFIDF frequent terms in the text are “rewarded” (like the word “they”
    in our example), but they also get “punished” if those terms are frequent in other
    texts we include in the algorithm too. On the contrary, this method highlights
    and “rewards” unique or rare terms considering all texts. Nevertheless, this approach
    still has no context nor semantics.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，一种方法是通过计算单词在所有文本中出现的频率（不仅仅是我们正在分析的文本）来重新调整单词频率，使得像“the”这样的频繁出现的单词在其他文本中也很频繁的情况下会受到惩罚。这种评分方法称为
    **“词频—逆文档频率”** **（TFIDF）**，通过权重改进了词袋模型。通过 TFIDF，文本中的频繁词汇（如我们例子中的“they”）会被“奖励”，但如果这些词汇在我们纳入算法的其他文本中也很频繁，它们也会被“惩罚”。相反，这种方法会突出显示并“奖励”唯一或稀有的词汇，考虑到所有文本。然而，这种方法仍然没有上下文和语义。
- en: '**Tokenization**'
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**分词**'
- en: 'Is the process of segmenting running text into sentences and words. In essence,
    it’s the task of cutting a text into pieces called *tokens*, and at the same time
    throwing away certain characters, such as punctuation. Following our example,
    the result of tokenization would be:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 是将连续文本分割成句子和单词的过程。从本质上讲，这是一项将文本切割成称为 *token* 的片段，同时丢弃某些字符，如标点符号。以我们的例子为例，分词的结果将是：
- en: '![](../Images/c229250b850f1aecf4712dcebddb9435.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c229250b850f1aecf4712dcebddb9435.png)'
- en: Pretty simple, right? Well, although it may seem quite basic in this case and
    also in languages like English that separate words by a blank space (called segmented
    languages) not all languages behave the same, and if you think about it, blank
    spaces alone are not sufficient enough even for English to perform proper tokenizations.
    Splitting on blank spaces may break up what should be considered as one token,
    as in the case of certain names (e.g. San Francisco or New York) or borrowed foreign
    phrases (e.g. laissez faire).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 相当简单，对吧？不过，虽然在这种情况下以及像英语这样的分隔语言中，空格分隔单词看起来很基本，但并非所有语言行为都相同。实际上，单靠空格并不足以进行准确的分词。分隔空格可能会将应视为一个词的内容拆分开来，例如某些名称（如旧金山或纽约）或外来短语（如
    laissez faire）。
- en: '**Tokenization can remove punctuation too**, easing the path to a proper word
    segmentation but also triggering possible complications. In the case of periods
    that follow abbreviation (e.g. dr.), the period following that abbreviation should
    be considered as part of the same token and not be removed.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**分词也可以去除标点符号**，从而简化正确的单词分割路径，但也可能引发潜在的复杂情况。在缩写（例如dr.）后跟随的句点应该视为同一词的一部分，而不是被移除。'
- en: The tokenization process can be particularly problematic when dealing with biomedical
    text domains which contain lots of hyphens, parentheses, and other punctuation
    marks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理包含大量连字符、括号及其他标点符号的生物医学文本领域时，分词过程可能会特别棘手。
- en: For deeper details on tokenization, you can find a great explanation in [this
    article](https://www.ibm.com/developerworks/community/blogs/nlp/entry/tokenization?lang=en).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有关分词的更详细信息，你可以在 [这篇文章](https://www.ibm.com/developerworks/community/blogs/nlp/entry/tokenization?lang=en)中找到很好的解释。
- en: '**Stop Words Removal**'
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**停用词移除**'
- en: Includes getting rid of common language articles, pronouns and prepositions
    such as “and”, “the” or “to” in English. In this process some very common words
    that appear to provide little or no value to the NLP objective are filtered and
    excluded from the text to be processed, hence removing widespread and frequent
    terms that are not informative about the corresponding text.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 包括去除常见的语言冠词、代词和介词，如英语中的“and”、“the”或“to”。在这个过程中，一些看似对NLP目标贡献不大的非常常见的词会被过滤和排除，从而去除那些在相应文本中不具有信息性的广泛而频繁的术语。
- en: Stop words can be safely ignored by carrying out a lookup in a pre-defined list
    of keywords, freeing up database space and improving processing time.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在预定义的关键词列表中查找，可以安全地忽略停用词，从而节省数据库空间并提高处理速度。
- en: '**There is no universal list of stop words**. These can be pre-selected or
    built from scratch. A potential approach is to begin by adopting pre-defined stop
    words and add words to the list later on. Nevertheless it seems that the general
    trend over the past time has been to go from the use of large standard stop word
    lists to the use of no lists at all.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**没有通用的停用词列表**。这些可以是预先选定的或从头构建的。一个潜在的方法是先采用预定义的停用词，并在以后将词汇添加到列表中。然而，似乎近年来的普遍趋势是从使用大型标准停用词列表转向完全不使用列表。'
- en: The thing is stop words removal can wipe out relevant information and modify
    the context in a given sentence. For example, if we are performing a sentiment
    analysis we might throw our algorithm off track if we remove a stop word like
    “not”. Under these conditions, you might select a minimal stop word list and add
    additional terms depending on your specific objective.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于停用词移除可能会抹去相关信息并改变给定句子的上下文。例如，如果我们正在进行情感分析，移除像“not”这样的停用词可能会使算法偏离轨道。在这种情况下，你可能需要选择一个最小的停用词列表，并根据你的具体目标添加额外的词汇。
- en: '**Stemming**'
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**词干提取**'
- en: Refers to the process of slicing the end or the beginning of words with the
    intention of removing affixes (lexical additions to the root of the word).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 指的是切割单词的前端或后端，以去除词缀（附加在词根上的词汇成分）。
- en: '*Affixes that are attached at the beginning of the word are called prefixes (e.g.
    “astro” in the word “astrobiology”) and the ones attached at the end of the word
    are called suffixes (e.g. “ful” in the word “helpful”).*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*附加在单词开头的词缀称为前缀（例如“astrobiology”中的“astro”），而附加在单词末尾的词缀称为后缀（例如“helpful”中的“ful”）。*'
- en: The problem is that affixes can create or expand new forms of the same word
    (called *inflectional* affixes), or even create new words themselves (called *derivational* affixes).
    In English, prefixes are always derivational (the affix creates a new word as
    in the example of the prefix “eco” in the word “ecosystem”), but suffixes can
    be derivational (the affix creates a new word as in the example of the suffix
    “ist” in the word “guitarist”) or inflectional (the affix creates a new form of
    word as in the example of the suffix “er” in the word “faster”).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，词缀可以创造或扩展相同词语的新形式（称为*词形变化*词缀），甚至可以创造新的词汇（称为*派生*词缀）。在英语中，前缀总是派生词缀（词缀创造一个新词，例如“生态”这个前缀在“生态系统”一词中的例子），但后缀可以是派生的（词缀创造一个新词，例如“吉他手”中的“ist”后缀的例子）或词形变化的（词缀创造一个新形式的词，例如“更快”中的“er”后缀的例子）。
- en: Ok, so how can we tell the difference and chop the right bit?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们怎么区分并选择正确的部分呢？
- en: '![](../Images/1608f7938632b5f6fc5c9937e9d17b4e.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1608f7938632b5f6fc5c9937e9d17b4e.png)'
- en: A possible approach is to consider a list of common affixes and rules (Python
    and R languages have different libraries containing affixes and methods) and perform
    stemming based on them, but of course this approach presents limitations. Since
    stemmers use algorithmics approaches, the result of the stemming process may not
    be an actual word or even change the word (and sentence) meaning. To offset this
    effect you can edit those predefined methods by adding or removing affixes and
    rules, but you must consider that you might be improving the performance in one
    area while producing a degradation in another one. Always look at the whole picture
    and test your model’s performance.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能的方法是考虑常见词缀和规则的列表（Python 和 R 语言有不同的库包含词缀和方法），并基于这些词缀和规则进行词干提取，但这种方法确实存在局限性。由于词干提取器使用算法方法，词干提取过程的结果可能不是一个实际的词，甚至可能改变词语（和句子）的含义。为了弥补这一效果，你可以通过添加或删除词缀和规则来编辑这些预定义的方法，但你必须考虑到你可能在某个领域提高了性能的同时在另一个领域产生了退化。始终要全面考虑并测试你的模型性能。
- en: So if stemming has serious limitations, why do we use it? First of all, it can
    be used to correct spelling errors from the tokens. **Stemmers are simple to use
    and run very fast** (they perform simple operations on a string), and if speed
    and performance are important in the NLP model, then stemming is certainly the
    way to go. Remember, we use it with the objective of improving our performance,
    not as a grammar exercise.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果词干提取有严重的局限性，为什么我们还要使用它呢？首先，它可以用于纠正词元中的拼写错误。**词干提取器使用简单且运行非常快速**（它们对字符串执行简单操作），如果在自然语言处理模型中速度和性能很重要，那么词干提取绝对是首选。请记住，我们使用它的目的是提高性能，而不是作为语法练习。
- en: '**Lemmatization**'
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**词形还原**'
- en: Has the objective of reducing a word to its base form and grouping together
    different forms of the same word. For example, verbs in past tense are changed
    into present (e.g. “went” is changed to “go”) and synonyms are unified (e.g. “best”
    is changed to “good”), hence standardizing words with similar meaning to their
    root. Although it seems closely related to the stemming process, lemmatization
    uses a different approach to reach the root forms of words.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其目的是将一个词还原为其基本形式，并将不同形式的相同词汇进行归类。例如，过去时的动词被转化为现在时（例如，“went”变为“go”），同义词被统一（例如，“best”变为“good”），从而将具有相似含义的词汇标准化为其词根。虽然它似乎与词干提取过程密切相关，但词形还原使用不同的方法来达到词汇的根本形式。
- en: '*Lemmatization resolves words to their dictionary form (known as **lemma**)
    for which it requires detailed dictionaries in which the algorithm can look into
    and link words to their corresponding lemmas.*'
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*词形还原将词语还原为它们的词典形式（称为**词根**），这需要详细的词典供算法查阅，并将词语与其对应的词根联系起来。*'
- en: For example, the words “*running”, “runs”* and *“ran”* are all forms of the
    word “*run”*, so “*run”* is the lemma of all the previous words.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，“*running*”、"*runs*" 和 "*ran*" 都是“*run*”这个词的不同形式，因此“*run*”是所有这些词的词根。
- en: '![](../Images/db52d4f3acf412f2fc2968068d3a82b8.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db52d4f3acf412f2fc2968068d3a82b8.png)'
- en: Lemmatization also takes into consideration the context of the word in order
    to **solve other problems like disambiguation**, which means it can discriminate
    between identical words that have different meanings depending on the specific
    context. Think about words like “bat” (which can correspond to the animal or to
    the metal/wooden club used in baseball) or “bank” (corresponding to the financial
    institution or to the land alongside a body of water). By providing a part-of-speech
    parameter to a word ( whether it is a noun, a verb, and so on) it’s possible to
    define a role for that word in the sentence and remove disambiguation.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 词形还原还考虑了词的上下文，以**解决其他问题如消歧义**，这意味着它可以区分在特定上下文中具有不同含义的相同词。例如，像“bat”（可以指动物或棒球用的金属/木制球棒）或“bank”（可以指金融机构或水体旁的土地）这样的词。通过为词提供词性参数（如名词、动词等），可以定义词在句子中的角色并消除歧义。
- en: As you might already pictured, lemmatization is a much more resource-intensive
    task than performing a stemming process. At the same time, since it requires more
    knowledge about the language structure than a stemming approach, it **demands
    more computational power **than setting up or adapting a stemming algorithm.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如你可能已经想到的，词形还原是一项比执行词干提取过程更为资源密集的任务。同时，由于它比词干提取方法需要更多的语言结构知识，因此**需要比设置或调整词干提取算法更多的计算能力**。
- en: '**Topic Modeling**'
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**主题建模**'
- en: Is as a method for uncovering hidden structures in sets of texts or documents.
    In essence it clusters texts to discover latent topics based on their contents,
    processing individual words and assigning them values based on their distribution.
    This technique is based on the assumptions that each document consists of a mixture
    of topics and that each topic consists of a set of words, which means that if
    we can spot these hidden topics we can unlock the meaning of our texts.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 是一种揭示文本或文档集合中隐藏结构的方法。实质上，它将文本进行聚类，以发现基于内容的潜在主题，处理单个词并根据其分布分配值。这种技术基于以下假设：每个文档由一系列主题组成，每个主题由一组词汇组成，这意味着如果我们能够发现这些隐藏的主题，我们就能解锁文本的意义。
- en: 'From the universe of topic modelling techniques, **Latent Dirichlet Allocation
    (LDA)** is probably the most commonly used. This relatively new algorithm (invented
    less than 20 years ago) works as an unsupervised learning method that discovers
    different topics underlying a collection of documents. In **unsupervised learning **methods
    like this one, there is no output variable to guide the learning process and data
    is explored by algorithms to find patterns. To be more specific, LDA finds groups
    of related words by:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在主题建模技术的宇宙中，**潜在狄利克雷分配（LDA）**可能是最常用的方法。这种相对较新的算法（发明于不到20年前）作为一种无监督学习方法，发现了一组文档中潜在的不同主题。在**无监督学习**方法中，没有输出变量来指导学习过程，数据由算法进行探索以发现模式。更具体地说，LDA通过以下方式发现相关词汇的群组：
- en: Assigning each word to a random topic, where the user defines the number of
    topics it wishes to uncover. You don’t define the topics themselves (you define
    just the number of topics) and the algorithm will map all documents to the topics
    in a way that words in each document are mostly captured by those imaginary topics.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个词分配给一个随机主题，用户定义希望揭示的主题数量。你不定义主题本身（只定义主题的数量），算法将以一种方式将所有文档映射到这些主题，使得每个文档中的词主要被这些虚拟主题捕捉。
- en: The algorithm goes through each word iteratively and reassigns the word to a
    topic taking into considerations the probability that the word belongs to a topic,
    and the probability that the document will be generated by a topic. These probabilities
    are calculated multiple times, until the convergence of the algorithm.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法迭代地处理每个词，并重新将词分配给某个主题，同时考虑到该词属于某个主题的概率和文档由某个主题生成的概率。这些概率会被计算多次，直到算法收敛。
- en: Unlike other clustering algorithms like [*K-means*](https://towardsdatascience.com/the-anatomy-of-k-means-c22340543397) that
    perform hard clustering (where topics are disjointed), LDA assigns each document
    to a mixture of topics, which means that each document can be described by one
    or more topics (e.g. Document 1 is described by 70% of topic A, 20% of topic B
    and 10% of topic C) and reflect more realistic results.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与像[*K-means*](https://towardsdatascience.com/the-anatomy-of-k-means-c22340543397)这样的其他聚类算法（执行硬聚类，即主题不重叠）不同，LDA将每个文档分配给多个主题的混合体，这意味着每个文档可以由一个或多个主题来描述（例如，文档1由70%的主题A、20%的主题B和10%的主题C来描述），并反映出更现实的结果。
- en: '![](../Images/99eacccff44debcd7badd73879846b96.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99eacccff44debcd7badd73879846b96.png)'
- en: Topic modeling is extremely useful for classifying texts, building recommender
    systems (e.g. to recommend you books based on your past readings) or even detecting
    trends in online publications.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模在分类文本、构建推荐系统（例如根据你过去的阅读推荐书籍）或甚至检测在线出版物中的趋势方面极其有用。
- en: '**How does the future look like?**'
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**未来会是什么样的？**'
- en: At the moment NLP is battling to detect nuances in language meaning, whether
    due to lack of context, spelling errors or dialectal differences.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，NLP正在努力检测语言含义中的细微差别，无论是由于缺乏上下文、拼写错误还是方言差异。
- en: 'On March 2016 Microsoft launched *Tay*, an Artificial Intelligence (AI) chatbot
    released on Twitter as a NLP experiment. The idea was that as more users conversed
    with Tay, the smarter it would get. Well, the result was that after 16 hours Tay
    had to be removed due to its racist and abusive comments:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年3月，微软推出了*Tay*，这是一个在Twitter上发布的人工智能（AI）聊天机器人作为NLP实验。其想法是，随着更多用户与Tay对话，它会变得越来越聪明。结果是，16小时后，Tay因其种族主义和辱骂性评论被迫下线：
- en: '![](../Images/47796f4589c4b9f14b3dffde8ac600fd.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47796f4589c4b9f14b3dffde8ac600fd.png)'
- en: '![](../Images/bdf3e47587ee620ca505d589bd9e3a5c.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdf3e47587ee620ca505d589bd9e3a5c.png)'
- en: Microsoft learnt from its own experience and some months later released [*Zo*](https://www.zo.ai/),
    its second generation English-language chatbot that won’t be caught making the
    same mistakes as its predecessor. Zo uses a combination of innovative approaches
    to recognize and generate conversation, and other companies are exploring with
    bots that can remember details specific to an individual conversation.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 微软从自身经验中汲取了教训，并在几个月后发布了[*Zo*](https://www.zo.ai/)，这是其第二代英语聊天机器人，不会犯与前任相同的错误。Zo结合了创新的方法来识别和生成对话，其他公司也在探索能够记住个别对话细节的机器人。
- en: Although the future looks extremely challenging and full of threats for NLP,
    the discipline is developing at a very fast pace (probably like never before)
    and we are likely to reach a level of advancement in the coming years that will
    make complex applications look possible.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管未来对自然语言处理（NLP）来说看起来极具挑战和威胁，但该领域正在以极快的速度发展（可能前所未有），我们很可能在未来几年内达到一种先进水平，使得复杂的应用看起来变得可能。
- en: '[Original](https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1).
    Reposted with permission.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[原始](https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1)。经许可转载。'
- en: '**Bio**: [Diego Lopez Yse](https://twitter.com/LopezYse) is an experienced
    professional with a solid international background acquired in different industries
    (biotechnology, software, consultancy, government, agriculture).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介**：[Diego Lopez Yse](https://twitter.com/LopezYse)是一位经验丰富的专业人士，在生物技术、软件、咨询、政府和农业等不同行业中积累了坚实的国际背景。'
- en: '**Resources:**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：**'
- en: '[On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning
    education](https://www.kdnuggets.com/education/online.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在线和基于网络的：分析、数据挖掘、数据科学、机器学习教育](https://www.kdnuggets.com/education/online.html)'
- en: '[Software for Analytics, Data Science, Data Mining, and Machine Learning](https://www.kdnuggets.com/software/index.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用于分析、数据科学、数据挖掘和机器学习的软件](https://www.kdnuggets.com/software/index.html)'
- en: '**Related:**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Extracting Knowledge from Knowledge Graphs Using Facebook’s Pytorch-BigGraph](https://www.kdnuggets.com/2019/05/extracting-knowledge-graphs-facebook-pytorch-biggraph.html)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Facebook的Pytorch-BigGraph从知识图谱中提取知识](https://www.kdnuggets.com/2019/05/extracting-knowledge-graphs-facebook-pytorch-biggraph.html)'
- en: '[A Complete Exploratory Data Analysis and Visualization for Text Data: Combine
    Visualization and NLP to Generate Insights](https://www.kdnuggets.com/2019/05/complete-exploratory-data-analysis-visualization-text-data.html)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本数据的完整探索性数据分析和可视化：结合可视化和NLP生成洞见](https://www.kdnuggets.com/2019/05/complete-exploratory-data-analysis-visualization-text-data.html)'
- en: '[Build Your First Chatbot Using Python & NLTK](https://www.kdnuggets.com/2019/05/build-chatbot-python-nltk.html)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Python和NLTK构建你的第一个聊天机器人](https://www.kdnuggets.com/2019/05/build-chatbot-python-nltk.html)'
- en: More On This Topic
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[N-gram Language Modeling in Natural Language Processing](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理中的N-gram语言建模](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
- en: '[A Guide to Top Natural Language Processing Libraries](https://www.kdnuggets.com/2023/04/guide-top-natural-language-processing-libraries.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[顶级自然语言处理库指南](https://www.kdnuggets.com/2023/04/guide-top-natural-language-processing-libraries.html)'
- en: '[Natural Language Processing Key Terms, Explained](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理关键术语解析](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
- en: '[Data Representation for Natural Language Processing Tasks](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理任务的数据表示](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
- en: '[Transfer Learning for Image Recognition and Natural Language Processing](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[图像识别与自然语言处理的迁移学习](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
- en: '[How to Start Using Natural Language Processing With PyTorch](https://www.kdnuggets.com/2022/04/start-natural-language-processing-pytorch.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何开始使用PyTorch进行自然语言处理](https://www.kdnuggets.com/2022/04/start-natural-language-processing-pytorch.html)'
