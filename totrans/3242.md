# K-最近邻——最懒的机器学习技术

> 原文：[https://www.kdnuggets.com/2017/09/rapidminer-k-nearest-neighbors-laziest-machine-learning-technique.html](https://www.kdnuggets.com/2017/09/rapidminer-k-nearest-neighbors-laziest-machine-learning-technique.html)

**由Rapidminer** 赞助的帖子。

K-最近邻（K-NN）是最简单的机器学习算法之一。像其他机器学习技术一样，它的灵感来源于人类推理。例如，当生活中发生重大事件时，你会记住这个经历，并将其作为未来决策的指南。

让我给你举一个掉落玻璃的场景。当玻璃正在下落时，你预测玻璃会在撞击地面时破碎。但你是怎么做到的呢？你之前从未见过这个玻璃破碎，对吧？

实际上，不是的。但你之前见过类似的玻璃掉落在地上。虽然情况可能并不完全相同，但你知道从5英尺高的地方掉落到混凝土地板上的玻璃通常会破碎，这使你对结果有很高的信心。

那么，掉落玻璃从较短的距离落在柔软的地毯上呢？你也经历过这种情况吗？我们可以看到，距离和地面表面都会对结果产生影响。

这就是K-NN算法使用的推理方式。当新的情况发生时，它会扫描所有过去的经历，并查找k个最接近的经历。这些经历（或称数据点）就是我们所谓的*k个最近邻*。

如果你有一个分类任务，例如你想预测玻璃是否会破碎，你会根据所有k个邻居的多数投票来做出预测。如果k=5，并且在你最相似的经历中有3次或更多次玻璃破碎，那么它会预测“是的，它会破碎”。

现在假设你想预测玻璃会破成多少片。在这种情况下，你想预测一个数字，我们称之为“回归”。你将k个邻居的玻璃碎片数量的平均值作为预测或评分。如果k=5，碎片数量分别是1（未破碎）、4、8、2和10，那么你的预测结果将是5。

![图片 1](../Images/b133c96278078528875b7573035a61a3.png)

我们有蓝色和橙色的数据点。对于一个新的数据点（绿色），我们可以通过查看最近邻居的类别来确定最可能的类别。在这里，决定将是“蓝色”，因为这是大多数邻居的类别。

那么为什么这个算法被称为“懒惰”的呢？这是因为当你提供训练数据时，它不会进行任何训练。在训练时，它只是存储数据集，并不会在此时进行任何计算。它也不会尝试从数据中导出更紧凑的模型来进行评分。

### 理论

所有计算都发生在评分过程中，即当你将模型应用于未见过的数据点时。你需要确定在我们的训练集中哪些k个数据点最接近我们要预测的数据点。

假设我们的数据点如下所示：

![Image 3](../Images/26b69ffabb4f16ef558ec40647ddf6b7.png)

我们有一个包含*n*行和*m+1*列的表格，其中前*m*列是我们用来预测剩余标签列（也称为“目标”）的属性。现在，假设所有属性值*x*都是数值型的，而标签值*y*是分类的，即我们有一个分类问题。

现在我们可以定义一个距离函数来计算数据点之间的距离。它应该为任何新点找到我们训练数据中最接近的数据点。如果数据是数值型的，**欧几里得距离**通常是一个不错的距离函数选择。如果我们的新数据点具有属性值*s**1*到*s**m*，我们可以通过以下公式计算点*s*到任何数据点*x**j*的距离*d(s, x**j***：

![Image 2](../Images/2b252eb7e0ab1cac7587e110fb382969.png)

距离最近的k个数据点成为我们的k个邻居。对于分类任务，我们现在使用k个邻居中所有值*y*中最频繁的那个。对于回归任务，当*y*是数值型时，我们使用k个邻居中所有值*y*的平均值。

但如果我们的属性不是数值型的，且不包含数值和分类属性呢？那你可以使用其他能够处理这种数据类型的距离度量。[这篇文章](https://dzone.com/articles/machine-learning-measuring)讨论了一些常见的选择。

顺便提一下，k=1的K-NN模型是为什么计算训练错误完全没有意义的原因。你能看出为什么吗？

### 实际应用

K-NN应该是你工具箱中的一个标准工具。它快速、易于理解，并且容易调整以适应不同类型的预测问题。

**数据准备**

使用K-NN时需要考虑一些问题。我们已经看到算法的关键部分是距离度量的定义，而**欧几里得距离**通常被使用。然而，这种距离度量将所有数据列视为相同。它在累加这些距离的平方之前，会减去每个维度的值。这意味着数据范围较大的列对距离的影响大于数据范围较小的列。

因此，你需要对数据集进行归一化，以使所有列的大致尺度相同。归一化有两种常见方法。首先，你可以将某列的所有值调整到0到1之间。或者你可以改变每列的值，使得该列在调整后具有均值为0和标准差为1。我们称这种归一化方法为[z-变换或标准分数](https://en.wikipedia.org/wiki/Standard_score)。

> 提示：每当你知道机器学习算法在使用距离度量时，你应该对数据进行归一化。另一个著名的例子是k-均值聚类。

**调整参数**

你需要调整的最重要参数是k，即用于做出分类决策的邻居数量。最小值为1，在这种情况下，你只查看每个预测的最近邻居以做出决策。理论上，你可以使用一个与训练集总量相同的k值。然而这样做没有意义，因为在这种情况下你总是会预测完整训练集的多数类别。

这里有一个好的方式来解释k的含义。小的k值表示“局部”模型，这些模型可以是非线性的，并且类别之间的决策边界会出现很多波动。如果k值增大，波动会减少，直到你几乎得到一个线性的决策边界。

![Image 4](../Images/2333f9c70bb9393a0cfa603bd82d1f56.png)

我们看到左侧的一个二维数据集。通常，右上方是红色类，左下方是蓝色类。但在两个区域内部也有一些局部群体。k值小会导致更为弯曲的决策边界。对于较大的k值，决策边界变得更平滑，这种情况下几乎是线性的。

合适的k值取决于你拥有的数据以及问题是否非线性。你应该尝试几个在1到训练数据集大小的约10%之间的值，以查看是否有值得进一步优化k的有希望的区域。

你可能还需要考虑的第二个参数是你使用的距离函数类型。对于数值数据，欧氏距离是一个不错的选择。你也可以尝试一下[曼哈顿距离](https://en.wikipedia.org/wiki/Taxicab_geometry)，它有时也会被使用。对于文本分析，[余弦距离](https://en.wikipedia.org/wiki/Cosine_similarity)也可以是一个值得尝试的好替代方案。

**内存使用情况与运行时间**

请注意，这个算法所做的只是存储完整的训练数据。因此，内存需求随着你提供的训练数据点数量线性增长。这个算法的更智能实现可能会选择以更紧凑的方式存储数据。但在最坏情况下，你仍然会面临大量的内存使用。

对于训练，运行时间已经是最好的。算法除了存储数据外没有进行任何计算，而存储是快速的。

然而，评分的运行时间可能会很长，这在机器学习领域是不常见的。所有计算都发生在模型应用期间。因此，评分运行时间与数据列数m和训练点数n线性增长。如果你需要快速评分而训练数据点数量又很大，那么K-NN不是一个好的选择。

**RapidMiner流程**

想自己构建这个机器学习模型？ [下载RapidMiner](http://go.rapidminer.com/l/32612/2017-09-05/8243cb)并使用下面的过程*。

+   训练和应用K-NN模型： [knn_training_scoring](https://ingomierswacom.files.wordpress.com/2017/05/knn_training_scoring.zip)

+   优化k值： [knn_optimize_parameter](https://ingomierswacom.files.wordpress.com/2017/05/knn_optimize_parameter.zip)

*下载Zip文件并提取其内容。结果将是一个.rmp文件，可以通过“文件”->“导入过程”在RapidMiner中加载。*

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升您的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持您的组织进行IT工作

* * *

### 相关主题更多信息

+   [从理论到实践：构建一个k-Nearest Neighbors分类器](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)

+   [分类的最近邻方法](https://www.kdnuggets.com/2022/04/nearest-neighbors-classification.html)

+   [Scikit-learn中的K-Nearest Neighbors](https://www.kdnuggets.com/2022/07/knearest-neighbors-scikitlearn.html)

+   [提示工程中的并行处理：思想框架…](https://www.kdnuggets.com/parallel-processing-in-prompt-engineering-the-skeleton-of-thought-technique)

+   [每个机器学习工程师都应该具备的5项机器学习技能…](https://www.kdnuggets.com/2023/03/5-machine-learning-skills-every-machine-learning-engineer-know-2023.html)

+   [KDnuggets新闻，12月14日：3个免费机器学习课程…](https://www.kdnuggets.com/2022/n48.html)
