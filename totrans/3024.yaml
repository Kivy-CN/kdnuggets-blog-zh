- en: A Guide to Decision Trees for Machine Learning and Data Science
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习和数据科学中的决策树指南
- en: 原文：[https://www.kdnuggets.com/2018/12/guide-decision-trees-machine-learning-data-science.html](https://www.kdnuggets.com/2018/12/guide-decision-trees-machine-learning-data-science.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/12/guide-decision-trees-machine-learning-data-science.html](https://www.kdnuggets.com/2018/12/guide-decision-trees-machine-learning-data-science.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [George Seif](https://towardsdatascience.com/@george.seif94), AI / Machine
    Learning Engineer**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [George Seif](https://towardsdatascience.com/@george.seif94)，人工智能/机器学习工程师**'
- en: Decision Trees are a class of very powerful Machine Learning model cable of
    achieving high accuracy in many tasks while being highly interpretable. What makes
    decision trees special in the realm of ML models is really their clarity of information
    representation. The “knowledge” learned by a decision tree through training is
    directly formulated into a hierarchical structure. This structure holds and displays
    the knowledge in such a way that it can easily be understood, even by non-experts.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一类非常强大的机器学习模型，能够在许多任务中实现高准确率，同时又具有很高的可解释性。在机器学习模型领域，决策树的特别之处在于其信息表示的清晰性。决策树通过训练获得的“知识”直接被形成一个层次结构。这种结构以一种易于理解的方式展示知识，即使是非专家也能轻松理解。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速入门网络安全职业'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![](../Images/247d774fc1b4a93bcba12afc67523f20.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/247d774fc1b4a93bcba12afc67523f20.png)'
- en: Decision Trees in Real-Life
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 现实生活中的决策树
- en: You’ve probably used a decision tree before to make a decision in your own life.
    Take for example the *decision* about what activity you should do this weekend.
    It might depend on whether or not you feel like going out with your friends or
    spending the weekend alone; in both cases, your decision also depends on the weather.
    If it’s sunny and your friends are available, you may want to play soccer. If
    it ends up raining you’ll go to a movie. And if your friends don’t show up at
    all, well then you like playing video games no matter what the weather is like!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能以前使用过决策树来做出生活中的决策。举个例子，比如你要决定这个周末做什么活动。你可能会考虑是否想和朋友出去还是独自在家；在这两种情况下，你的决定也取决于天气。如果天气晴朗且你的朋友有空，你可能会想去踢足球。如果下雨，你会去看电影。如果你的朋友根本没有出现，那你就会玩电子游戏，无论天气如何！
- en: '![](../Images/4476e73464585bcc9cddaf0ae303d4a1.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4476e73464585bcc9cddaf0ae303d4a1.png)'
- en: This is a clear example of a *real-life decision tree*. We’ve built a tree to
    model a set of **sequential, hierarchical decisions** that ultimately lead to
    some final result. Notice that we’ve also chosen our decisions to be quite “high-level”
    in order to keep the tree small. For example, what if we set up *many* possible
    options for the weather such as 25 degrees sunny, 25 degrees raining, 26 degrees
    sunny, 26 degrees raining, 27 degrees sunny…. etc, our tree would be huge! The
    **exact** temperature really isn’t too relevant, we just want to know whether
    it’s OK to be outside or not.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个清晰的*现实生活中的决策树*的例子。我们构建了一个树来建模一组**顺序的、层次化的决策**，这些决策最终会导致某个最终结果。请注意，我们还选择了比较“高层次”的决策，以保持树的简洁。例如，如果我们为天气设置了*许多*可能的选项，如25度晴天、25度下雨、26度晴天、26度下雨、27度晴天……等等，我们的树会非常庞大！**准确的**温度其实并不是很重要，我们只想知道是否可以在户外活动。
- en: The concept is the same for decision trees in Machine Learning. We want to build
    a tree with a set of hierarchical decisions which eventually give us a final result,
    i.e our classification or regression prediction. The decisions will be selected
    such that the tree is as small as possible while aiming for high classification
    / regression accuracy.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的决策树概念是相同的。我们希望构建一个具有一组层次决策的树，这些决策最终给出一个最终结果，即我们的分类或回归预测。决策将被选择，以使树尽可能小，同时追求高分类/回归准确性。
- en: Decision Trees in Machine Learning
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习中的决策树
- en: 'Decision Tree models are created using 2 steps: Induction and Pruning. Induction
    is where we actually build the tree i.e set all of the hierarchical decision boundaries
    based on our data. Because of the nature of training decision trees they can be
    prone to major overfitting. Pruning is the process of removing the unnecessary
    structure from a decision tree, effectively reducing the complexity to combat
    overfitting with the added bonus of making it even easier to interpret.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树模型是通过两个步骤创建的：归纳和剪枝。归纳是我们实际构建树的地方，即基于我们的数据设置所有层次决策边界。由于训练决策树的性质，它们可能容易出现严重的过拟合。剪枝是从决策树中移除不必要结构的过程，有效地减少复杂性，以应对过拟合，并使其更易于解释。
- en: '**Induction**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**归纳**'
- en: 'From a high level, decision tree induction goes through 4 main steps to build
    the tree:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，决策树归纳经历4个主要步骤来构建树：
- en: Begin with your training dataset, which should have some feature variables and
    classification or regression output.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从你的训练数据集开始，该数据集应该具有一些特征变量和分类或回归输出。
- en: Determine the “best feature” in the dataset to split the data on; more on how
    we define “best feature” later
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定数据集中“最佳特征”来分割数据；稍后会详细说明我们如何定义“最佳特征”
- en: Split the data into subsets that contain the possible values for this best feature.
    This splitting basically defines a node on the tree i.e each node is a splitting
    point based on a certain feature from our data.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分割成包含最佳特征可能值的子集。这种分裂基本上定义了树上的一个节点，即每个节点是基于我们数据的某个特征的分裂点。
- en: Recursively generate new tree nodes by using the subset of data created from
    step 3\. We keep splitting until we reach a point where we have optimised, by
    some measure, maximum accuracy while minimising the number of splits / nodes.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用第3步创建的数据子集递归生成新的树节点。我们不断分裂，直到达到一个点，在这个点上，我们通过某种度量优化了最大准确度，同时最小化了分裂/节点的数量。
- en: Step 1 is easy, just grab your dataset!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步很简单，只需获取你的数据集！
- en: For step 2, the selection of which feature to use and the specific split is
    commonly chosen using a greedy algorithm to minimise a cost function. If we think
    about it for a second, performing a split when building a decision tree is equivalent
    to dividing up the feature space. We will iteratively try out different split
    points and then at the end select the one that has the lowest cost. Of course
    we can do a couple of smart things like only splitting within the range of values
    in our dataset. This will keep us from wasting computations on testing out split
    points that are trivially poor.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二步，选择使用哪个特征以及具体的分裂点通常使用贪心算法来最小化成本函数。如果我们稍微思考一下，构建决策树时的分裂相当于划分特征空间。我们将反复尝试不同的分裂点，然后在最后选择成本最低的那个。当然，我们可以做一些聪明的事情，比如仅在数据集中值的范围内进行分裂。这将避免在测试明显差的分裂点上浪费计算。
- en: 'For a regression tree, we can use a simple squared error as our cost function:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归树，我们可以使用一个简单的平方误差作为我们的成本函数：
- en: '![](../Images/9e737360b5111722e071106141110538.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e737360b5111722e071106141110538.png)'
- en: Where Y is our ground truth and Y-hat is our predicted value; we sum over all
    the samples in our dataset to get the total error. For a classification, we use
    the *Gini Index Function:*
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 Y 是我们的实际值，Y-hat 是我们的预测值；我们对数据集中的所有样本求和，以得到总误差。对于分类，我们使用*基尼指数函数*：
- en: '![](../Images/cbd57496eddd6d3825f64340081d9c88.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cbd57496eddd6d3825f64340081d9c88.png)'
- en: Where pk are the proportion of training instances of class k in a particular
    prediction node. A node should *ideally *have an error value of zero, which means
    that each split outputs a single class 100% of the time. This is exactly what
    we want because then we know, once we get to that particular decision node, what
    exactly our output will be whether we are on one side of the decision boundary
    or the other.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 pk 是某一预测节点中类别 k 的训练实例比例。一个节点*理想情况下*应该具有零误差值，这意味着每次分裂都能 100% 输出单一类别。这正是我们想要的，因为一旦我们到达那个特定的决策节点，我们就会知道，无论我们在决策边界的一侧还是另一侧，我们的输出到底是什么。
- en: This concept of having a single class per-split across our dataset is known
    as*information gain*. Check out the example below.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这种在数据集中每次分裂都有一个单一类别的概念称为*信息增益*。查看下面的示例。
- en: '![](../Images/a52e214cfcc8a71a438dc70e9007a719.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a52e214cfcc8a71a438dc70e9007a719.png)'
- en: If we were to choose a split where each output has a mix of classes depending
    on the input data, then we really haven’t *gained *any information at all; we
    don’t know any better whether or not a particular node i.e feature has any influence
    in classifying our data! On the otherhand, if our split has a high percentage
    of each class for each output, then we have *gained* the information that splitting
    in that particular way on that particular feature variable gives us a particular
    output!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择的分裂使每个输出根据输入数据混合了不同的类别，那么我们实际上并没有*获得*任何信息；我们不能更好地知道特定节点即特征在分类数据时是否有影响！另一方面，如果我们的分裂使每个输出具有高比例的每个类别，那么我们*获得*了这样的信息：在特定特征变量上以这种特定方式进行分裂可以给我们一个特定的输出！
- en: Now we could of course keep splitting and splitting and splitting until our
    tree has thousands of branches….. but that’s not really such a good idea! Our
    decision tree would be huge, slow, and overfitted to our training dataset. Thus,
    we will set some predefined stopping criterion to halt the construction of the
    tree.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们可以不断地分裂，直到我们的树有成千上万的分支……但这并不是一个好主意！我们的决策树将会非常庞大，速度缓慢，并且过拟合于我们的训练数据集。因此，我们将设置一些预定义的停止标准来停止树的构建。
- en: The most common stopping method is to use a minimum count on the number of training
    examples assigned to each leaf node. If the count is less than some minimum value
    then the split is not accepted and the node is taken as a final leaf node. If
    all of our leafs nodes become final, the training stops. A smaller minimum count
    will give you finer splits and potentially more information, but is also prone
    to overfitting on your training data. Too large of a min count and you might stop
    to early. As such, the min value is usually set based on the dataset, depending
    on how many examples are expected to be in each class.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的停止方法是对分配给每个叶节点的训练示例数量使用最小计数。如果数量小于某个最小值，则该分裂不被接受，节点被视为最终叶节点。如果所有的叶节点都变成最终节点，则训练停止。较小的最小计数会产生更细的分裂，并可能提供更多的信息，但也容易对训练数据进行过拟合。最小计数过大可能会过早停止。因此，最小值通常根据数据集设置，具体取决于每个类别中预计的示例数量。
- en: '**Pruning**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**剪枝**'
- en: Because of the nature of training decision trees they can be prone to major
    overfitting. Setting the correct value for minimum number of instances per node
    can be challenging. Most of the time, we might just go with a safe bet and make
    that minimum quite small, resulting in there being many splits and a very large,
    complex tree. The key is that many of these splits will end up being redundant
    and unnecessary to increasing the accuracy of our model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练决策树的性质，它们可能容易出现过拟合。为每个节点设置正确的最小实例数量可能具有挑战性。大多数时候，我们可能会选择一个安全的选择，将最小值设得非常小，导致分裂过多，树变得非常大且复杂。关键在于，这些分裂中的许多将最终是冗余的，不会增加我们模型的准确性。
- en: Tree pruning is a technique that leverages this splitting redundancy to remove
    i.e *prune* the unnecessary splits in our tree. From a high-level, pruning compresses
    part of the tree from strict and rigid decision boundaries into ones that are
    more smooth and generalise better, effectively reducing the tree complexity. The
    complexity of a decision tree is defined as the number of splits in the tree.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 树剪枝是一种利用分裂冗余来移除，即*剪枝*我们树中的不必要分裂的技术。从高层次来看，剪枝将部分树从严格和僵硬的决策边界压缩成更平滑和更具泛化性的边界，从而有效减少树的复杂性。决策树的复杂性定义为树中的分裂数量。
- en: A simple yet highly effective pruning method is to go through each node in the
    tree and evaluate the effect of removing it on the cost function. If it doesn’t
    change much, then prune away!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单而有效的剪枝方法是遍历树中的每个节点，评估移除它对成本函数的影响。如果变化不大，则进行剪枝！
- en: An Example in Scikit Learn
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Scikit Learn中的一个示例
- en: Decision trees for both classification and regression are super easy to use
    in Scikit Learn with a built in class! We’ll first load in our dataset and initialise
    our decision tree for classification. Running training is then a simple one-liner!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit Learn中分类和回归的决策树非常易于使用，提供了内置的类！我们首先加载数据集并初始化分类决策树。然后运行训练只需一行代码！
- en: Scikit Learn also allows us to visualise our tree using the graphviz library.
    It comes with a few options that will help in visualising the decision nodes and
    splits that the model learned which is super useful for understanding how it all
    works! Below we will colour the nodes based on the feature names and display the
    class and feature information of each node.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit Learn还允许我们使用graphviz库可视化我们的树。它提供了一些选项，可以帮助可视化模型学习到的决策节点和分裂，这对于理解其工作原理非常有用！下面我们将根据特征名称对节点进行着色，并显示每个节点的类别和特征信息。
- en: '![](../Images/ecaf20a53098ea1fa03240e432bd03e3.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ecaf20a53098ea1fa03240e432bd03e3.png)'
- en: 'There are several parameters that you can set for your decision tree model
    in Scikit Learn too. Here are a few of the more interesting ones to play around
    with to try and get some better results:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scikit Learn中，你还可以为决策树模型设置多个参数。以下是一些更有趣的参数，可以尝试以获得更好的结果：
- en: '**max_depth:** The max depth of the tree where we will stop splitting the nodes.
    This is similar to controlling the maximum number of layers in a deep neural network.
    Lower will make your model faster but not as accurate; higher can give you accuracy
    but risks overfitting and may be slow.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_depth：** 决定停止分裂节点的树的最大深度。这类似于控制深度神经网络中的最大层数。较低的深度会使模型更快但准确度较低；较高的深度可以提高准确度但风险过拟合，可能会变慢。'
- en: '**min_samples_split: **The minimum number of samples required to split a node.
    We discussed this aspect of decision trees above and how setting it to a higher
    value would help mitigate overfitting.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**min_samples_split：** 分裂节点所需的最小样本数量。我们在上面讨论过决策树的这个方面，设置较高的值有助于减少过拟合。'
- en: '**max_features: **The number of features to consider when looking for the best
    split. Higher means potentially better results with the tradeoff of training taking
    longer.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_features：** 查找最佳分裂时要考虑的特征数量。较高的值意味着可能得到更好的结果，但训练时间会更长。'
- en: '**min_impurity_split: **Threshold for early stopping in tree growth. A node
    will split if its impurity is above the threshold. This can be used to tradeoff
    combating overfitting (high value, small tree) vs high accuracy (low value, big
    tree).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**min_impurity_split：** 决定树生长的早期停止阈值。如果节点的杂质高于阈值，则会进行分裂。这可以用来权衡抗过拟合（高值，小树）与高准确率（低值，大树）。'
- en: '**presort:** Whether to presort the data to speed up the finding of best splits
    in fitting. If we sort our data on each feature beforehand, our training algorithm
    will have a much easier time finding good values to split on.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预排序：** 是否预排序数据以加快寻找最佳分裂点的速度。如果我们提前对每个特征进行排序，我们的训练算法将更容易找到合适的分裂值。'
- en: Tips for Practically Applying Decision Trees
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实际应用决策树的技巧
- en: 'Here are a few of the pro and cons of decision trees that can help you decide
    on whether or not it’s the right model for your problem, as well as some tips
    as to how you can effectively apply them:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是决策树的一些优缺点，可以帮助你决定它是否适合你的问题，并提供一些有效应用它们的技巧：
- en: '**Pros**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**'
- en: '**Easy to understand and interpret.** At each node, we are able to see *exactly* what
    decision our model is making. In practice we’ll be able to fully understand where
    our accuracies and errors are coming from, what type of data the model would do
    well with, and how the output is influenced by the values of the features. Scikit
    learn’s visualisation tool is a fantastic option for visualising and understanding
    decision trees.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易于理解和解释。** 在每个节点上，我们能够*准确地*看到模型做出的决策。在实践中，我们将能够完全理解我们的准确率和错误来源，模型擅长处理的数据类型，以及特征值如何影响输出。Scikit
    learn的可视化工具是可视化和理解决策树的绝佳选择。'
- en: '**Require very little data preparation. **Many ML models may require heavy
    data pre-processing such as normalization and may require complex regularisation
    schemes. Decision trees on the other hand work quite well out of the box after
    tweaking a few of the parameters.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**需要的 数据准备很少。** 许多机器学习模型可能需要大量的数据预处理，如归一化，并可能需要复杂的正则化方案。相比之下，决策树在调整少量参数后，开箱即用效果很好。'
- en: '**The cost of using the tree for inference is logarithmic in the number of
    data points used to train the tree. **That’s a huge plus since it means that having
    more data won’t necessarily make a huge dent in our inference speed.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用树进行推理的成本与训练树所用的数据点数量的对数成正比。** 这是一大优势，因为这意味着增加数据不会显著影响我们的推理速度。'
- en: '**Cons**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**'
- en: Overfitting is quite common with decision trees simply due to the nature of
    their training. It’s often recommended to perform some type of dimensionality
    reduction such as [PCA](https://towardsdatascience.com/principal-component-analysis-your-tutorial-and-code-9719d3d3f376) so
    that the tree doesn’t have to learn splits on so many features
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树由于其训练性质，过拟合非常常见。通常建议进行一些类型的降维，例如 [PCA](https://towardsdatascience.com/principal-component-analysis-your-tutorial-and-code-9719d3d3f376)，以便树不必在如此多的特征上学习分裂。
- en: For similar reasons as the case of overfitting, decision trees are also vulnerable
    to becoming biased to the classes that have a majority in the dataset. It’s always
    a good idea to do some kind of [class balancing](https://towardsdatascience.com/handling-imbalanced-datasets-in-deep-learning-f48407a0e758) such
    as class weights, sampling, or a specialised loss function.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于与过拟合类似的原因，决策树也容易对数据集中占多数的类别产生偏倚。通常，进行一些类型的 [class balancing](https://towardsdatascience.com/handling-imbalanced-datasets-in-deep-learning-f48407a0e758) 如类别权重、采样或专门的损失函数是一个好主意。
- en: Like to learn?
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 想要学习？
- en: Follow me on[ twitter](https://twitter.com/GeorgeSeif94) where I post all about
    the latest and greatest AI, Technology, and Science!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在[twitter](https://twitter.com/GeorgeSeif94)上关注我，了解最新最棒的人工智能、技术和科学！
- en: '**Bio: [George Seif](https://towardsdatascience.com/@george.seif94)** is a
    Certified Nerd and AI / Machine Learning Engineer.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[George Seif](https://towardsdatascience.com/@george.seif94)** 是一位认证的极客和人工智能/机器学习工程师。'
- en: '[Original](https://towardsdatascience.com/a-guide-to-decision-trees-for-machine-learning-and-data-science-fe2607241956).
    Reposted with permission.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/a-guide-to-decision-trees-for-machine-learning-and-data-science-fe2607241956)。经许可转载。'
- en: '**Related:**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[5 Quick and Easy Data Visualizations in Python with Code](/2018/07/5-quick-easy-data-visualizations-python-code.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python中的5种快速简易数据可视化方法（附代码）](/2018/07/5-quick-easy-data-visualizations-python-code.html)'
- en: '[The 5 Clustering Algorithms Data Scientists Need to Know](/2018/06/5-clustering-algorithms-data-scientists-need-know.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学家需要了解的5种聚类算法](/2018/06/5-clustering-algorithms-data-scientists-need-know.html)'
- en: '[Get a 2–6x Speed-up on Your Data Pre-processing with Python](/2018/10/get-speed-up-data-pre-processing-python.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Python在数据预处理上获得2–6倍的速度提升](/2018/10/get-speed-up-data-pre-processing-python.html)'
- en: More On This Topic
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目的，并找到目的去…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学统计学的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功数据科学家的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个数据科学家都应了解的三个R库（即使你使用Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分析一个90亿美元的人工智能失败案例](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[是什么让Python成为初创企业的理想编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
