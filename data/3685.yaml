- en: 'Best Architecture for Your Text Classification Task: Benchmarking Your Options'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 适合你文本分类任务的最佳架构：对比你的选择
- en: 原文：[https://www.kdnuggets.com/2023/04/best-architecture-text-classification-task-benchmarking-options.html](https://www.kdnuggets.com/2023/04/best-architecture-text-classification-task-benchmarking-options.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/04/best-architecture-text-classification-task-benchmarking-options.html](https://www.kdnuggets.com/2023/04/best-architecture-text-classification-task-benchmarking-options.html)
- en: '![Best Architecture for Your Text Classification Task: Benchmarking Your Options](../Images/3e79e4edb32b66e049a0bca8dd0d75ac.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![适合你文本分类任务的最佳架构：对比你的选择](../Images/3e79e4edb32b66e049a0bca8dd0d75ac.png)'
- en: Image by Editor
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于编辑
- en: In [our previous article](https://medium.com/toloka/choosing-the-best-architecture-for-your-text-classification-task-aee30ecc7870),
    we covered a variety of approaches to building a text classification model based
    on what modern NLP currently has to offer.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在[我们之前的文章](https://medium.com/toloka/choosing-the-best-architecture-for-your-text-classification-task-aee30ecc7870)中，我们介绍了基于现代自然语言处理技术的文本分类模型的各种方法。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: With old-school TF-IDF approaches, pre-trained embedding models, and transformers
    of various shapes and sizes to choose from, we wanted to give some practical advice
    based on our own experience. Which models are best suited for different situations?
    What are some use cases you can find in your own line of work?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择传统的TF-IDF方法、预训练嵌入模型和各种形状和大小的变换器时，我们希望根据自己的经验提供一些实用建议。哪些模型最适合不同的情况？你自己工作领域中可以找到哪些使用案例？
- en: To add extra flavor, we want to show you a real-life example of benchmarks for
    those different approaches and compare them using a dataset we chose for this
    quick follow-up article.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加趣味性，我们想展示一个实际的基准测试示例，并使用我们为这篇快速跟进文章选择的数据集进行比较。
- en: Describing the Dataset and Task
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 描述数据集和任务
- en: To illustrate our ideas, we chose [The Twitter Financial News](https://huggingface.co/datasets/zeroshot/twitter-financial-news-topic),
    an English-language dataset containing an annotated corpus of finance-related
    tweets. It’s commonly used to build finance-related content classification models
    that sort tweets into a number of topics.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明我们的观点，我们选择了[Twitter金融新闻](https://huggingface.co/datasets/zeroshot/twitter-financial-news-topic)，这是一个包含金融相关推文的英文数据集，并且附有注释。它通常用于构建金融相关内容分类模型，将推文分类到多个主题中。
- en: It’s a medium-sized dataset, which is perfect for us to illustrate how different
    models perform. Also fairly diverse, the size allows us to train and evaluate
    models relatively quickly.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个中等规模的数据集，非常适合用来展示不同模型的表现。同时也相当多样化，这个规模让我们可以相对快速地训练和评估模型。
- en: What’s interesting about this domain is that financial language is usually crisp
    and laconic. There are plenty of proper nouns describing brands, terms, and related
    entities, and the models need to learn to distinguish them from common nouns with
    completely different meanings. Intuitively, fine-tuning pre-trained generic-language
    models in this domain should boost overall performance and accuracy.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这个领域有趣的地方在于金融语言通常简练且简洁。存在大量描述品牌、术语和相关实体的专有名词，模型需要学会将它们与含义完全不同的普通名词区分开来。直观地，微调预训练的通用语言模型在这个领域应该能提升整体性能和准确性。
- en: The dataset consists of around 21,000 items. Not too small, it’s also not too
    large, making it perfect for showing off the advantages and disadvantages of each
    model and approach. Let’s come back to this once we have the results.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含大约21,000条记录。既不太小也不太大，非常适合展示每种模型和方法的优缺点。等我们得到结果后再来回顾一下。
- en: And finally, the dataset has 20 classes. It’s no common classification task,
    where you have to distinguish between a handful of sentiment classes and emotional
    tones. There’s an imbalance too. With a 60x+ difference between the most and least
    frequent classes, some approaches can be expected to underperform.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，数据集有20个类别。这不是一个常见的分类任务，你需要区分少数几个情感类别和情感色调。还有一个不平衡的问题。最频繁和最少见的类别之间有60倍以上的差异，一些方法可能表现不佳。
- en: Let’s see how different models will perform in our benchmarking test.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看不同模型在我们的基准测试中的表现如何。
- en: Describing the Approach
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 描述方法
- en: Based on our previous article, FastText, BERT, RoBERTa (with second-stage tuning),
    and GPT-3 are our choices for assessing their performance and efficiency. The
    dataset was split into training and test sets with 16,500 and 4500 items, respectively.
    After the models were trained on the former, their performance and efficiency
    (inference time) were measured on the latter.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们之前的文章，FastText、BERT、RoBERTa（经过二阶段调整）和GPT-3是我们用来评估其性能和效率的选择。数据集被拆分为训练集和测试集，分别包含16,500和4,500个项目。在对前者进行模型训练后，我们在后者上测量了模型的性能和效率（推理时间）。
- en: To train a FastText model, we used the [fastText library](https://fasttext.cc/)
    with the corresponding command line tool. We prepared the dataset by inserting
    labels into texts with the proper prefix, ran the fasttext supervised command
    to train a classifier, and waited a couple minutes to produce the model on a CPU-only
    machine. The next command, fasttext predict, gave us predictions for the test
    set and model performance.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练一个FastText模型，我们使用了[fastText库](https://fasttext.cc/)及相应的命令行工具。我们通过在文本中插入带有正确前缀的标签来准备数据集，运行fasttext监督命令以训练分类器，并在仅有CPU的机器上等待几分钟以生成模型。接下来的命令fasttext
    predict给出了测试集的预测结果和模型性能。
- en: 'As for transformers, we chose three slightly different models to compare: BERT
    (more formal, best-base-uncased), RoBERTa-large, and an adapted version of the
    latter tuned for sentiment classification on a couple finance-related datasets
    (check it out on the [HuggingFace website](https://huggingface.co/Jean-Baptiste/roberta-large-financial-news-sentiment-en)).
    The transformers library stood in for our experiments, though it entails writing
    some code to actually run training and evaluation procedures. A single machine
    with the A100 GPU handled training, which took 20–28 minutes until early stopping
    conditions were met for each model. The trained models were stored in a MLFlow
    registry.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于变换器模型，我们选择了三种稍有不同的模型进行比较：BERT（更正式，best-base-uncased）、RoBERTa-large，以及后者的一个改编版本，该版本针对几个金融相关数据集的情感分类进行了调整（在[HuggingFace
    网站](https://huggingface.co/Jean-Baptiste/roberta-large-financial-news-sentiment-en)查看）。我们在实验中使用了变换器库，尽管这需要编写一些代码来实际运行训练和评估程序。一台配备A100
    GPU的单机进行了训练，直到每个模型满足早期停止条件，这一过程耗时20到28分钟。训练好的模型被存储在MLFlow注册表中。
- en: To train a classifier based on the GPT-3 model, we referred to the [official
    documentation](https://platform.openai.com/docs/guides/fine-tuning/advanced-usage)
    on the OpenAI website and used the corresponding command line tool to submit data
    for training, track its progress, and make predictions for the test set (more
    formally, completions, a better term for generative models). Since the work itself
    happened on OpenAI’s servers, we didn’t use any particular hardware. It only took
    a regular laptop to create a cloud-based model. We trained two GPT-3 variations,
    Ada and Babbage, to see if they would perform differently. It takes 40–50 minutes
    to train a classifier in our scenario.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练基于GPT-3模型的分类器，我们参考了[官方文档](https://platform.openai.com/docs/guides/fine-tuning/advanced-usage)并使用相应的命令行工具提交数据进行训练，跟踪其进展，并为测试集进行预测（更正式地说，生成模型的“补全”）。由于工作本身发生在OpenAI的服务器上，我们没有使用任何特定的硬件。只需一台普通的笔记本电脑即可创建基于云的模型。我们训练了两个GPT-3变体，Ada和Babbage，以查看它们是否表现不同。在我们的场景中，训练一个分类器需要40到50分钟。
- en: Once training was complete, we evaluated all the models on the test set to build
    classification metrics. We chose macro average F1 and weighted average F1 to compare
    them, as that let us estimate both precision and recall in addition to seeing
    if dataset imbalance influenced the metrics. The models were compared on their
    inference speed in milliseconds per item with a batch size of one. For the RoBERTa
    model, we also include an ONNX-optimized version as well as inference using an
    A100 GPU accelerator. Measuring the average response time from our tuned Babbage
    model gave us the GPT-3 speed (note that OpenAI applies some rate limiters, so
    the actual speed might be lower or higher depending on your terms of use).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们在测试集上评估了所有模型以建立分类指标。我们选择了宏平均F1和加权平均F1来进行比较，因为这让我们可以估计精度和召回率，并查看数据集不平衡是否影响了指标。我们还比较了模型的推理速度，单位为每个项目的毫秒，批处理大小为一。在RoBERTa模型中，我们还包括了一个ONNX优化版本以及使用A100
    GPU加速器的推理。从我们调优后的Babbage模型测得的平均响应时间给出了GPT-3的速度（注意，OpenAI应用了一些速率限制器，因此实际速度可能会因使用条款而有所不同）。
- en: Results
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: How did the training work out? We arranged the results in a couple tables to
    show you the end product and the effect we observed.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 训练效果如何？我们将结果整理在几个表格中，以展示最终产品以及我们观察到的效果。
- en: '![Best Architecture for Your Text Classification Task: Benchmarking Your Options](../Images/57e188fbc17b567867c404d4dce9a49b.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![文本分类任务的最佳架构：对比不同选项](../Images/57e188fbc17b567867c404d4dce9a49b.png)'
- en: Photo by Author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作者照片
- en: What caught our eye first is that fasttext lagged far behind. With that said,
    it took minimal resources in terms of computation, time, and training, and it
    gave us a low bar benchmark.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先引起我们注意的是fasttext的表现远远落后。尽管如此，它在计算、时间和训练方面所需的资源极少，为我们提供了一个低标准的基准。
- en: How about the transformers? As expected, RoBERTa delivered better results than
    BERT, which is easy to attribute to the size advantage it had. It’s also generally
    better with domain-specific classification tasks. To be fair, we specifically
    selected a large RoBERTa architecture for this comparison, and the base RoBERTa
    model might have performed similarly to BERT despite differences in the underlying
    corpus and training methodology.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器的表现如何？正如预期的那样，RoBERTa的结果优于BERT，这可以很容易归因于其规模优势。RoBERTa在特定领域的分类任务中通常表现更好。公平起见，我们为这次比较特别选择了一个大型的RoBERTa架构，而基础的RoBERTa模型可能与BERT的表现相当，尽管在底层语料库和训练方法上有所不同。
- en: The tangible gap between the F1 metrics for BERT and RoBERTa could also have
    been caused by the fact that we’re dealing with a fairly large number of classes.
    The dataset has imbalances that larger models tend to capture better. But that’s
    just our suspicion and proving it would require more experimentation. You can
    also see that the domain-pretrained RoBERTa offered a tiny accuracy boost, though
    it’s insignificant. It’s hard to say if the pre-trained domain-tuned model was
    actually worthwhile for our experiment.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: BERT和RoBERTa之间F1指标的明显差距可能也与我们处理的类别数量相当大有关。数据集存在的不平衡往往会被较大的模型更好地捕捉。但这只是我们的猜测，验证这一点需要更多的实验。你还可以看到，领域预训练的RoBERTa提供了一个微小的准确性提升，但这一提升并不显著。很难说预训练的领域调整模型是否对我们的实验真正有价值。
- en: Next comes GPT-3\. We selected the Ada and Babbage models for a fair comparison
    with BERT and RoBERTa-large since they have excellent parameter sizes that grow
    gradually (from 165 million parameters in BERT and 355 million in RoBERTa-large
    to 2.7 billion in Ada and 6.7 billion in Babbage) and can show whether the model
    size really gives a proportional performance boost. Surprisingly, Ada and Babbage
    both deliver almost the same metrics, and they actually lose to RoBERTa even without
    domain-specific pre-training. But there’s a reason for that. Remember that GPT-3
    API-accessible models actually give users a generative inference interface, so
    they try to predict a token that would classify each example in the classification
    task. RoBERTa and other models from transformers, on the other hand, have the
    last layers of their architecture configured correctly for classification. Imagine
    a proper logit or softmax at the end that returns the likelihood of all the classes
    for any data item you pass to it. While the huge GPT-3 would be sufficient to
    tackle classification for one of 20 classes by generating the right token class,
    it’s overkill here. Let’s just not forget that the GPT-3 model is fine-tuned and
    accessed with just three lines of code, unlike RoBERTa, which takes work to roll
    out on your architecture.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是 GPT-3。我们选择了 Ada 和 Babbage 模型，以便与 BERT 和 RoBERTa-large 进行公平比较，因为它们具有出色的参数规模，这些参数规模逐步增长（从
    BERT 的 1.65 亿参数和 RoBERTa-large 的 3.55 亿参数到 Ada 的 27 亿参数和 Babbage 的 67 亿参数），并可以展示模型大小是否确实带来了比例性能提升。令人惊讶的是，Ada
    和 Babbage 提供的指标几乎相同，而且即使没有领域特定的预训练，它们实际上也不如 RoBERTa。然而，这其中有原因。请记住，GPT-3 API 可访问的模型实际上为用户提供了生成性推理接口，因此它们试图预测一个标记来对分类任务中的每个示例进行分类。另一方面，RoBERTa
    和其他变换器模型的架构的最后一层已正确配置用于分类。想象一下，在最后的 Logit 或 Softmax 中，返回你传递给它的任何数据项的所有类别的可能性。虽然巨大的
    GPT-3 足以通过生成正确的标记类别来处理 20 个类别中的一个，但在这里显得有些过剩。我们不要忘记，GPT-3 模型经过微调，并且只需三行代码即可访问，而
    RoBERTa 则需要在你的架构上进行工作。
- en: '![Best Architecture for Your Text Classification Task: Benchmarking Your Options](../Images/95d4282c06347d0e5b7f0cda64d34f8b.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![你文本分类任务的最佳架构：对你的选项进行基准测试](../Images/95d4282c06347d0e5b7f0cda64d34f8b.png)'
- en: Photo by Author
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的照片
- en: Let’s now finally compare the models and their inference setups in terms of
    their request execution speed. Since we’re not just training them for performance
    and accuracy, we need to take into account how fast they return us their inference
    for new data. We clocked the online synchronous requests to the models and tried
    to understand the best niche for each.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们最终比较这些模型及其推理设置在请求执行速度方面的表现。由于我们不仅仅是在训练它们以提高性能和准确性，我们还需要考虑它们为新数据返回推理结果的速度。我们记录了对模型的在线同步请求，并尝试了解每个模型的最佳定位。
- en: The winner here is fasttext. Its accuracy, however, forces us to keep moving
    down the list.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的赢家是 fasttext。然而，它的准确性迫使我们继续往下看。
- en: Between the RoBERTa and GPT-3 setups, we can see that GPT-3 is relatively fast
    despite being the largest, especially given that its response time includes two-sided
    network communication to their API endpoint. The actual inference here is small.
    That’s obviously good, especially since this is a pretty simple solution to set
    up, fine-tune, and implement model calls for. While it can be expensive, especially
    if you plan on sending a lot of data frequently, the cost-benefit decision is
    up to you.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RoBERTa 和 GPT-3 设置之间，我们可以看到，尽管 GPT-3 是最大的，但它的速度相对较快，特别是考虑到它的响应时间包括了到 API 端点的双向网络通信。这里的实际推理很小。这显然是好的，尤其是因为这是一个相对简单的解决方案，可以设置、微调并实现模型调用。虽然它可能很昂贵，特别是如果你计划频繁发送大量数据，但成本效益的决策还是取决于你自己。
- en: The GPU-hosted version is the winner among the RoBERTa setups. The GPUs add
    a huge performance boost to inference computations, though hosting your model
    server on GPU machines might price the project out of your budget. Rolling out
    a GPU-based model server can also be tricky, especially if you haven’t done it
    before.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 托管版本在 RoBERTa 设置中是赢家。GPU 大大提升了推理计算的性能，但将模型服务器托管在 GPU 机器上可能会超出你的预算。推出基于 GPU
    的模型服务器也可能很棘手，特别是如果你之前没有做过这种事情的话。
- en: You also need to remember that despite these benchmarks are all being fast in
    terms of returning the results of your model requests, you shouldn’t forget to
    do some planning and break down how you plan to use the models in your project.
    Real-time inference or asynchronous batch requests? Accessed over the internet
    or within your local network? Do you have overhead for your business logic operations
    on top of the model response? All that can add much more time overhead to each
    request over the actual model inference calculation itself.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要记住，尽管这些基准测试在返回模型请求结果方面都很快速，但你不应忘记进行一些规划，细化你计划如何在项目中使用这些模型。实时推理还是异步批量请求？通过互联网访问还是在本地网络内？你是否在模型响应之上还有业务逻辑操作的开销？所有这些都会在每次请求中增加比实际模型推理计算本身更多的时间开销。
- en: Conclusions and Follow-up Ideas
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论与后续想法
- en: What have we learned? We tried to demonstrate a real-life example of the balance
    between the difficulty of running various models, their resulting accuracy metrics,
    and their response speed when they are ready to be used. Obviously, figuring out
    what to use when and how given your project is a challenge. But we hope to leave
    you with some guidance?—?there’s no silver bullet when it comes to GPT models.
    We all have to count our money as well, especially when it comes to machine learning.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '我们学到了什么？我们尝试展示一个实际例子，平衡运行各种模型的难度、其准确性指标以及它们准备使用时的响应速度。显然，根据你的项目确定何时使用什么以及如何使用是一个挑战。但我们希望能给你一些指导？——
    在GPT模型方面没有万灵药。尤其是在机器学习中，我们都必须精打细算。 '
- en: Here at Toloka, we’re working hard on a platform that will enable users to train,
    deploy, and use a transformer like RoBERTa with the same three API calls as GPT-3
    API.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在Toloka，我们正在努力开发一个平台，使用户能够使用与GPT-3 API相同的三个API调用来训练、部署和使用像RoBERTa这样的转换器。
- en: In our next article, we’ll run a couple more experiments on how to mitigate
    the effects of disbalanced datasets and upsample or downsample classes for balance.
    Our suspicion is that the GPT-3 generative approach will perform better than RoBERTa-large.
    We’ll also discuss how these results might change if we take on a much smaller
    dataset, and we point out exactly when and where GPT-3+ models will outperform
    all the others in classification tasks. Stay tuned and check out more of our work
    over at the Toloka ML team’s blog.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的下一篇文章中，我们将进行更多的实验，研究如何缓解数据集不平衡的影响，并进行上采样或下采样以实现平衡。我们怀疑GPT-3生成模型的表现会优于RoBERTa-large。我们还将讨论如果使用一个更小的数据集，这些结果可能会如何变化，并指出GPT-3+模型在分类任务中何时何地会超越其他模型。敬请关注，并查看我们在Toloka
    ML团队博客上的更多工作。
- en: '**[Aleksandr Makarov](https://www.linkedin.com/in/makaroval)** is a senior
    product manager in Toloka.ai leading the product development of Toloka.ai ML platform,
    a former healthtech entrepreneur and co-founder of Droice Labs'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**[亚历山大·马卡罗夫](https://www.linkedin.com/in/makaroval)** 是Toloka.ai的高级产品经理，负责Toloka.ai
    ML平台的产品开发，曾是健康科技创业者和Droice Labs的共同创始人。'
- en: More On This Topic
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[What is Text Classification?](https://www.kdnuggets.com/2022/07/text-classification.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是文本分类？](https://www.kdnuggets.com/2022/07/text-classification.html)'
- en: '[Machine Learning Is Not Like Your Brain Part 3: Fundamental Architecture](https://www.kdnuggets.com/2022/06/machine-learning-like-brain-part-3-fundamental-architecture.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习不像你的大脑 第3部分：基本架构](https://www.kdnuggets.com/2022/06/machine-learning-like-brain-part-3-fundamental-architecture.html)'
- en: '[Data Mesh & Its Distributed Data Architecture](https://www.kdnuggets.com/2022/02/data-mesh-distributed-data-architecture.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据网格及其分布式数据架构](https://www.kdnuggets.com/2022/02/data-mesh-distributed-data-architecture.html)'
- en: '[KDnuggets™ News 22:n07, Feb 16: How to Learn Math for Machine…](https://www.kdnuggets.com/2022/n07.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets™ 新闻 22:n07，2月16日：如何为机器学习学习数学…](https://www.kdnuggets.com/2022/n07.html)'
- en: '[Data Mesh Architecture: Reimagining Data Management](https://www.kdnuggets.com/2022/05/data-mesh-architecture-reimagining-data-management.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据网格架构：重新定义数据管理](https://www.kdnuggets.com/2022/05/data-mesh-architecture-reimagining-data-management.html)'
- en: '[KDnuggets News, May 18: 5 Free Hosting Platform For Machine…](https://www.kdnuggets.com/2022/n20.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets新闻，5月18日：5个免费的机器学习托管平台…](https://www.kdnuggets.com/2022/n20.html)'
