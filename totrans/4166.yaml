- en: Hands-On Reinforcement Learning Course, Part 1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-course-part-1.html](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-course-part-1.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Pau Labarta Bajo](https://www.linkedin.com/in/pau-labarta-bajo-4432074b/),
    mathematician and data scientist**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf56880d1f13250ab6cd5d4f84b891dd.png)'
  prefs: []
  type: TYPE_IMG
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: This first part covers the bare minimum concept and theory you need to embark
    on this journey from the fundamentals to cutting edge reinforcement learning (RL),
    step-by-step, with coding examples and tutorials in Python. In each following
    chapter, we will solve a different problem with increasing difficulty.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, the most complex RL problems involve a mixture of reinforcement
    learning algorithms, optimization, and Deep Learning. You do not need to know
    deep learning (DL) to follow along with this course. I will give you enough context
    to get you familiar with DL philosophy and understand how it becomes a crucial
    ingredient in modern reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: In this first lesson, we will cover the fundamentals of reinforcement learning
    with examples, 0 maths, and a bit of Python.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. What is a Reinforcement Learning problem?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement Learning (RL) is an area of Machine Learning (ML) concerned with
    learning problems where
  prefs: []
  type: TYPE_NORMAL
- en: An intelligent **agent** needs to learn, through trial and error, how to take
    **actions** inside and **environment** in order to maximize a **cumulative reward**.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Reinforcement Learning is the kind of machine learning closest to how humans
    and animals learn.
  prefs: []
  type: TYPE_NORMAL
- en: What is an agent? And an environment? What are exactly these actions the agent
    can take? And the reward? Why do you say cumulative reward?
  prefs: []
  type: TYPE_NORMAL
- en: If you are asking yourself these questions, you are on the right track.
  prefs: []
  type: TYPE_NORMAL
- en: The definition I just gave introduces a bunch of terms that you might not be
    familiar with. In fact, they are ambiguous on purpose. This generality is what
    makes RL applicable to a wide range of seemingly different learning problems.
    This is the philosophy behind mathematical modeling, which stays at the roots
    of RL.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at a few learning problems, and see how they use the RL lens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 1: Learning to walk'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a father of a baby who recently started walking, I cannot stop asking myself,
    *how did he learn that?*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/51cf73933a411b4e9a38ce484f8debad.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Kai and Pau.*'
  prefs: []
  type: TYPE_NORMAL
- en: As a Machine Learning engineer, I fantasize about understanding and replicating
    that incredible learning curve with software and hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try to model this learning problem using the RL ingredients:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **agent** is my son, Kai. And he wants to stand up and walk. His muscles
    are strong enough at this point in time to have a chance at it. The learning problem
    for him is: how to sequentially adjust his body position, including several angles
    on his legs, waist, back, and arms to balance his body and not fall.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/dd0f67c240d923b637e48b116e93f60f.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Sai hi to Kai!*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **environment** is the physical world surrounding him, including the laws
    of physics. The most important of which is gravity. Without gravity, the learning-to-walk
    problem would drastically change and even become irrelevant: why would you want
    to walk in a world where you can simply fly? Another important law in this learning
    problem is Newton’s third law, which in plain words tells that if you fall on
    the floor, the floor is going to hit you back with the same strength. Ouch!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **actions** are all the updates in these body angles that determine his
    body position and speed as he starts chasing things around. Sure he can do other
    things at the same time, like imitating the sound of a cow, but these are probably
    not helping him accomplish his goal. We ignore these actions in our framework.
    Adding unnecessary actions does not change the modeling step, but it makes the
    problem harder to solve later on.An important (and obvious) remark is that Kai
    does not need to learn the physics of Newton to stand up and walk. He will learn
    through observing the **state** of the environment, taking action, and collecting
    feedback from this environment. He does not need to learn a model of the environment
    to achieve his goal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **reward** he receives is a stimulus coming from the brain that makes him
    happy or makes him feel pain. There is the negative reward he experiences when
    falling on the floor, which is physical pain, maybe followed by frustration. On
    the other side, there are several things that contribute positively to his happiness,
    like the happiness of getting to places faster or the external stimulus that comes
    from my wife Jagoda and me when we say “good job!” or “bravo!” to each attempt
    and marginal improvement he shows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***A little bit more about rewards***'
  prefs: []
  type: TYPE_NORMAL
- en: The reward is a signal to Kai that what he has been doing is good or bad for
    his learning. As he takes new actions and experiences pain or happiness, he starts
    to adjust his behavior to collect more positive feedback and less negative feedback.
    In other words, he learns
  prefs: []
  type: TYPE_NORMAL
- en: Some actions might seem very appealing for the baby at the beginning, like trying
    to run to get a boost of excitement. However, he soon learns that in some (or
    most) cases, he ends up falling on his face and experiencing an extended period
    of pain and tears. This is why intelligent agents maximize **cumulative reward**
    and not marginal reward. They trade short-term rewards with long-term ones. An
    action that would give immediate reward, but put my body in a position about to
    fall, is not an optimal one.
  prefs: []
  type: TYPE_NORMAL
- en: Great happiness followed by greater pain is not a recipe for long-term well-being.
    This is something that babies often learn easier than we grown-ups.
  prefs: []
  type: TYPE_NORMAL
- en: The frequency and intensity of the rewards are key for helping the agent learn.
    Very infrequent (sparse) feedback means harder learning. Think about it, if you
    do not know if what you do is good or bad, how can you learn? This is one of the
    main reasons why some RL problems are harder than others.
  prefs: []
  type: TYPE_NORMAL
- en: Reward shaping is a tough modeling decision for many real-world RL problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 2: Learning to play Monopoly like a PRO'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a kid, I spent a lot of time playing Monopoly with friends and relatives.
    Well, who hasn’t? It is an exciting game that combines luck (you roll the dices)
    and strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Monopoly is a real-estate board game for two to eight players. You roll two
    dices to move around the board, buying and trading properties, and developing
    them with houses and hotels. You collect rent from your opponents, with the goal
    being to drive them into bankruptcy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d40f5aef98ab5e32e4c6f8789a3d163d.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Photo by [Suzy Hazelwood](https://www.pexels.com/@suzyhazelwood?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)
    from [Pexels](https://www.pexels.com/photo/miniature-toy-car-on-monopoly-board-game-1422673/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels).*'
  prefs: []
  type: TYPE_NORMAL
- en: If you were so into this game that you wanted to find intelligent ways to play
    it, you could use some reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: What would the 4 RL ingredients be?
  prefs: []
  type: TYPE_NORMAL
- en: The **agent** is you, the one who wants to win at Monopoly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Your **actions** are the ones you see on this screenshot below:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/87e5b54f45b2fdbc051d542e1e31ba9a.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Action space in Monopoly. Credits to [aleph aseffa](https://github.com/aleph-aseffa/monopoly).*'
  prefs: []
  type: TYPE_NORMAL
- en: The **environment** is the current state of the game, including the list of
    properties, positions, and cash amount each player has. There is also the strategy
    of your opponent, which is something you cannot predict and lies outside of your
    control.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And the **reward** is 0, except in your last move, where it is +1 if you win
    the game and -1 if you go bankrupt. This reward formulation makes sense but makes
    the problem hard to solve. As we said above, a more sparse reward means a harder
    solution. Because of this, there are [other ways](http://doc.gold.ac.uk/aisb50/AISB50-S02/AISB50-S2-Bailis-paper.pdf)
    to model the reward, making them noisier but less sparse.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you play against another person in Monopoly, you do not know how she or
    he will play. What you can do is play against yourself. As you learn to play better,
    your opponent does too (because it is you), forcing you to level up your game
    to keep on winning. You see the positive feedback loop.
  prefs: []
  type: TYPE_NORMAL
- en: This trick is called self-play. It gives us a path to bootstrap intelligence
    without using the external advice of an expert player. Self-play is the main difference
    between [AlphaGo](https://deepmind.com/research/case-studies/alphago-the-story-so-far)
    and [AlphaGo Zero](https://deepmind.com/blog/article/alphago-zero-starting-scratch),
    the two models developed by DeepMind that play the game of Go better than any
    human.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 3: Learning to drive'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a matter of decades (maybe less), machines will drive our cars, trucks, and
    buses.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/81c12d2c3292ee118314a5ac123dc958.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Photo by [Ruiyang Zhang](https://www.pexels.com/@ruiyang-zhang-915467?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)
    from [Pexels](https://www.pexels.com/photo/time-lapse-photo-of-cars-in-asphalt-road-3717291/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels).*'
  prefs: []
  type: TYPE_NORMAL
- en: But, how?
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning to drive a car is not easy. The goal of the driver is clear: to get
    from point A to point B comfortably for her and any passengers on board.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many external aspects to the driver that make driving challenging,
    including:'
  prefs: []
  type: TYPE_NORMAL
- en: other drivers behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: traffic signs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pedestrian behaviors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pavement conditions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: weather conditions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: …even fuel optimization (who wants to spend extra on this?)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How would we approach this problem with reinforcement learning?
  prefs: []
  type: TYPE_NORMAL
- en: The **agent** is the driver who wants to get from A to B comfortably.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **state** of the environment the driver observes has lots of things, including
    the position, speed, and acceleration of the car, all other cars, passengers,
    road conditions, or traffic signs. Transforming such a big vector of inputs into
    an appropriate action is challenging, as you can imagine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **actions** are basically three: the direction of the steering wheel, throttle
    intensity, and break intensity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **reward** after each action is a weighted sum of the different aspects
    you need to balance when driving. A decrease in distance to point B brings a positive
    reward, while an increase in a negative one. To ensure no collisions, getting
    too close (or even colliding) with another car or even a pedestrian should have
    a very big negative reward. Also, in order to encourage smooth driving, sharp
    changes in speed or direction contribute to a negative reward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After these 3 examples, I hope the following representation of RL elements
    and how they play together makes sense:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/408c751b47786479e65c3f9eeaa7bd41.png)'
  prefs: []
  type: TYPE_IMG
- en: '*RL in a nutshell. Credits to [Wikipedia](https://commons.wikimedia.org/wiki/File:Markov_diagram_v2.svg).*'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how to formulate an RL problem, we need to solve it.
  prefs: []
  type: TYPE_NORMAL
- en: But how?
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Policies and value functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Policies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The agent picks the action she thinks is the best based on the current state
    of the environment. This is the agent’s strategy, commonly referred to as the
    agent’s **policy**.
  prefs: []
  type: TYPE_NORMAL
- en: A **policy** is a learned mapping from states to actions.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Solving a reinforcement learning problem means finding the best possible policy.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Policies are **deterministic** when they map each state to one action,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3e2a88738239d078ff1f68eb33c54f5.png)'
  prefs: []
  type: TYPE_IMG
- en: or **stochastic** when they map each state to a probability distribution over
    all possible actions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b69a22cb9601f896870bca518585a078.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Stochastic* is a word you often read and hear in Machine Learning, and it
    essentially means *uncertain* or *random*. In environments with high uncertainty,
    like Monopoly where you are rolling dices, stochastic policies are better than
    deterministic ones.'
  prefs: []
  type: TYPE_NORMAL
- en: There exist several methods to actually compute this optimal policy. These are
    called **policy optimization methods**.
  prefs: []
  type: TYPE_NORMAL
- en: Value functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes, depending on the problem, instead of directly trying to find the
    optimal policy, one can try to find the **value function** associated with that
    optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: But, what is a value function? And before that, what does value mean in this
    context?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The **value** is a number associated with each state **s** of the environment
    that estimates how good it is for the agent to be in state **s**.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is the cumulative reward the agent collects when starting at state **s**
    and choosing actions according to policy **π**.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A value function is a learned mapping from states to values.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The value function of a policy is commonly denoted as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e2bf7a482c2cee4f6c7ce5e173a6ce91.png)'
  prefs: []
  type: TYPE_IMG
- en: Value functions can also map pairs of (action, state) to values. In this case,
    they are called *q-value* functions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46a02e26123c6cea5223829cc42ecf5f.png)'
  prefs: []
  type: TYPE_IMG
- en: The optimal value function (or q-value function) satisfies a mathematical equation,
    called the **Bellman equation**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af9b227173562522eba577013e197e8b.png)'
  prefs: []
  type: TYPE_IMG
- en: This equation is useful because it can be transformed into an iterative procedure
    to find the optimal value function.
  prefs: []
  type: TYPE_NORMAL
- en: '*But, why are value functions useful?* Because you can infer an optimal policy
    from an optimal q-value function.'
  prefs: []
  type: TYPE_NORMAL
- en: '*How?* The optimal policy is the one where at each state **s** the agent chooses
    the action **a** that maximizes the q-value function.'
  prefs: []
  type: TYPE_NORMAL
- en: So, you can jump from optimal policies to optimal q-functions and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: There are several RL algorithms that focus on finding optimal q-value functions.
    These are called **Q-learning methods**.
  prefs: []
  type: TYPE_NORMAL
- en: The zoology of reinforcement learning algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are lots of different RL algorithms. Some try to directly find optimal
    policies, and others q-value functions, and others both at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: The zoology of RL algorithms is diverse and a bit intimidating.
  prefs: []
  type: TYPE_NORMAL
- en: There is no *one-size-fits-all* when it comes to RL algorithms. You need to
    experiment with a few of them each time you solve an RL problem and see what works
    for your case.
  prefs: []
  type: TYPE_NORMAL
- en: As you follow along this course, you will implement several of these algorithms
    and gain an insight into what works best in each situation.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. How to generate training data?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement learning agents are VERY data-hungry.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f517210521bf3ccb89c9b4435d074b6d.png)'
  prefs: []
  type: TYPE_IMG
- en: '*[Photo by Karsten Winegeart](https://unsplash.com/photos/pQVecS8pBNY)*'
  prefs: []
  type: TYPE_NORMAL
- en: To solve RL problems, you need a lot of data.
  prefs: []
  type: TYPE_NORMAL
- en: A way to overcome this hurdle is by using **simulated environments**. Writing
    the engine that simulates the environment usually requires more work than solving
    the RL problem. Also, changes between different engine implementations can render
    comparisons between algorithms meaningless.
  prefs: []
  type: TYPE_NORMAL
- en: This is why guys at OpenAI released the [Gym toolkit](https://gym.openai.com/)
    back in 2016\. OpenAIs’s gym offers a standardized API for a collection of environments
    for different problems, including
  prefs: []
  type: TYPE_NORMAL
- en: the classic Atari games,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: robotic arms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: or landing on the Moon (well, a simplified one)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are proprietary environments, too, like [MuJoCo](https://www.endtoend.ai/envs/gym/mujoco/)
    ([recently bought by DeepMind](https://venturebeat.com/2021/10/18/deepmind-acquires-and-open-sources-robotics-simulator-mujoco/)).
    MuJoCo is an environment where you can solve continuous control tasks in 3D, like
    learning to walk.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Gym also defines a standard API to build environments, allowing third
    parties (like you) to create and make your environments available to others.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in self-driving cars, then you should check out CARLA,
    the most popular open urban driving simulator.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Python boilerplate code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You might be thinking:'
  prefs: []
  type: TYPE_NORMAL
- en: '*What we covered so far is interesting, but how do I actually write all this
    in Python?*'
  prefs: []
  type: TYPE_NORMAL
- en: And I completely agree with you
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how all this looks like in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Did you find something unclear in this code?
  prefs: []
  type: TYPE_NORMAL
- en: What about line 23? What is this epsilon?
  prefs: []
  type: TYPE_NORMAL
- en: Don’t panic. I didn’t mention this before, but I won’t leave you without an
    explanation.
  prefs: []
  type: TYPE_NORMAL
- en: Epsilon is a key parameter to ensure our agent explores the environment enough
    before drawing definite conclusions on what is the best action to take in each
    state.
  prefs: []
  type: TYPE_NORMAL
- en: It is a value between 0 and 1, and it represents the probability the agent chooses
    a random action instead of what she thinks is the best one.
  prefs: []
  type: TYPE_NORMAL
- en: This tradeoff between exploring new strategies vs. sticking to already known
    ones is called the **exploration-exploitation problem**. This is a key ingredient
    in RL problems and something that distinguishes RL problems from supervised machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Technically speaking, we want the agent to find the global optimum, not a local
    one.
  prefs: []
  type: TYPE_NORMAL
- en: It is good practice to start your training with a large value (e.g., 50%) and
    progressively decrease after each episode. This way, the agent explores a lot
    at the beginning and less as it perfects its strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Recap and homework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The key takeaways for this 1st part are:'
  prefs: []
  type: TYPE_NORMAL
- en: Every RL problem has an agent (or agents), environment, actions, states, and
    rewards.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent sequentially takes actions with the goal of maximizing total rewards.
    For that, it needs to find the optimal policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Value functions are useful as they give us an alternative path to find the optimal
    policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, you need to try different RL algorithms for your problem and see
    what works best.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RL agents need a lot of training data to learn. OpenAI gym is a great tool to
    re-use and create your environments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploration vs. exploitation is necessary when training RL agents to ensure
    the agent does not get stuck in local optimums.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A course without a bit of homework would not be a course.
  prefs: []
  type: TYPE_NORMAL
- en: I want you to pick a real-world problem that interests you and that you could
    model and solve using reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Define what are the agent(s), actions, states, and rewards.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to send me an e-mail at [plabartabajo@gmail.com](mailto:plabartabajo@gmail.com)
    with your problem, and I can give you feedback.
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](http://datamachines.xyz/2021/11/17/hands-on-reinforcement-learning-course-part-1/).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Hands-on Reinforcement Learning Course Part 3: SARSA](https://www.kdnuggets.com/2022/01/handson-reinforcement-learning-course-part-3-sarsa.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hands-On Reinforcement Learning Course, Part 2](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-part-2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[HuggingFace Has Launched a Free Deep Reinforcement Learning Course](https://www.kdnuggets.com/2022/05/huggingface-launched-free-deep-reinforcement-learning-course.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hands-On with Supervised Learning: Linear Regression](https://www.kdnuggets.com/handson-with-supervised-learning-linear-regression)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hands-On with Unsupervised Learning: K-Means Clustering](https://www.kdnuggets.com/handson-with-unsupervised-learning-kmeans-clustering)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Generative AI with Large Language Models: Hands-On Training](https://www.kdnuggets.com/2023/07/generative-ai-large-language-models-handson-training.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
