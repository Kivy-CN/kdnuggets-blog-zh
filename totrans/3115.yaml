- en: Deep learning scaling is predictable, empirically
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的扩展是可预测的，经验上
- en: 原文：[https://www.kdnuggets.com/2018/05/deep-learning-scaling-predictable-empirically.html](https://www.kdnuggets.com/2018/05/deep-learning-scaling-predictable-empirically.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/05/deep-learning-scaling-predictable-empirically.html](https://www.kdnuggets.com/2018/05/deep-learning-scaling-predictable-empirically.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '[Deep learning scaling is predictable, empirically](https://arxiv.org/abs/1712.00409) Hestness
    et al., *arXiv, Dec.2017*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[深度学习的扩展是可预测的，经验上](https://arxiv.org/abs/1712.00409) Hestness等人，*arXiv，2017年12月*'
- en: '*With thanks to Nathan Benaich for highlighting this paper in his excellent [summary
    of the AI world in 1Q18](https://www.getrevue.co/profile/nathanbenaich/issues/your-guide-to-ai-in-q1-2018-by-nathan-ai-100379)*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢Nathan Benaich在他的优秀[2018年第一季度AI世界总结](https://www.getrevue.co/profile/nathanbenaich/issues/your-guide-to-ai-in-q1-2018-by-nathan-ai-100379)中突出了这篇论文*'
- en: 'This is a really wonderful study with far-reaching implications that could
    even impact company strategies in some cases. It starts with a simple question:
    “how can we improve the state of the art in deep learning?” We have three main
    lines of attack:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一项非常出色的研究，具有深远的影响，甚至可能在某些情况下影响公司战略。研究始于一个简单的问题：“我们如何改进深度学习的最新技术？”我们有三条主要攻击线：
- en: We can search for improved *model architectures*.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以寻找改进的*模型架构*。
- en: We can *scale computation*.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以*扩展计算*。
- en: We can create *larger training data sets*.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以创建*更大的训练数据集*。
- en: As DL application domains grow, we would like a deeper understanding of the
    relationships between training set size, computational scale, and model accuracy
    improvements to advance the state-of-the-art.
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 随着DL应用领域的增长，我们希望更深入地理解训练集规模、计算规模和模型准确性改进之间的关系，以推动技术的前沿。
- en: 'Finding better model architectures often depends on ‘unreliable epiphany,’
    and as the results show, has limited impact compared to increasing the amount
    of data available. We’ve known this for some time of course, including from the
    2009 Google paper, [‘The unreasonable effectiveness of data](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf).’
    The results from today’s paper help us to quantify the data advantage across a
    range of deep learning applications. The key to understanding is captured in the
    following equation:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找更好的模型架构通常依赖于“不可依赖的顿悟”，正如结果所示，与增加可用数据量相比，影响有限。当然，我们已经知道这一点，包括从2009年的Google论文中，[‘数据的非理性有效性](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf)’。今天的论文结果帮助我们量化了在各种深度学习应用中的数据优势。理解的关键在于以下方程：
- en: '![\displaystyle \epsilon(m) \sim \alpha m^{\beta_g} + \gamma](../Images/fdc3d545505be6fa9d5f9b5ca22b8968.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![\displaystyle \epsilon(m) \sim \alpha m^{\beta_g} + \gamma](../Images/fdc3d545505be6fa9d5f9b5ca22b8968.png)'
- en: Which says that the generalisation error ![\epsilon](../Images/a97aa34ff2e2a0e8c415f52f0e96e2e2.png) as
    a function of the amount of training data ![m](../Images/152a02f7d270706469057ec5bc183e60.png),
    follows a *power-law* with exponent ![\beta_g](../Images/dd7a8a5ee65d76f35eff545a1f01f234.png).
    Empirically, ![\beta_g](../Images/4e84cad13f39dbf8713190b80b67cf2c.png) usually
    seems to be in the range -0.07 and -0.35\. With error of course, lower is better,
    hence the negative exponents. We normally plot power-laws on log-log graphs, where
    they result in straight lines with the gradient indicating the exponent.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明一般化误差![\epsilon](../Images/a97aa34ff2e2a0e8c415f52f0e96e2e2.png)作为训练数据量![m](../Images/152a02f7d270706469057ec5bc183e60.png)的函数，遵循具有指数![\beta_g](../Images/dd7a8a5ee65d76f35eff545a1f01f234.png)的*幂律*。经验上，![\beta_g](../Images/4e84cad13f39dbf8713190b80b67cf2c.png)通常在-0.07到-0.35之间。误差当然是，较低更好，因此为负指数。我们通常在对数-对数图上绘制幂律，其中它们结果为直线，梯度表示指数。
- en: Making *better models* can move the y-intercept down (until we reach the irreducible
    error level), but doesn’t seem to impact the power law coefficient. On the other
    hand, *more data* puts us on an power law path of improvement.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 制作*更好的模型*可以将y截距降低（直到达到不可约错误水平），但似乎不会影响幂律系数。另一方面，*更多的数据*将使我们沿着改进的幂律路径前进。
- en: '![](../Images/fd2828eb8b5f8b54ff75d2a9596763f7.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd2828eb8b5f8b54ff75d2a9596763f7.png)'
- en: The three learning zones
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 三个学习区域
- en: 'The learning curves for real applications can be broken down into three regions:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 实际应用的学习曲线可以分为三个区域：
- en: '![](../Images/a498412270207a1e2a0024c1231235b8.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a498412270207a1e2a0024c1231235b8.png)'
- en: The **small data region** is where models struggle to learn from insufficient
    data and models can only perform as well as ‘best’ or ‘random’ guessing.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小数据区域**是模型在数据不足的情况下挣扎学习的地方，模型的表现只能和“最佳”或“随机”猜测一样好。'
- en: The middle region is the **power-law region**, where the power-law exponent
    defines the steepness of the curve (slope on a log-log scale). The exponent is
    an *indicator of the difficulty for models to represent the data generating function*.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中间区域是**幂律区域**，在该区域，幂律指数定义了曲线的陡峭程度（对数对数尺度上的斜率）。这个指数是*模型表示数据生成函数的难度指标*。
- en: “*Results in this paper indicate that the power-law exponent is unlikely to
    be easily predicted with prior theory and **probably dependent on aspects of the
    problem domain or data distribution.***”
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “*本文的结果表明，幂律指数不容易通过先验理论来预测，并且**可能依赖于问题领域或数据分布的某些方面。***”
- en: The **irreducible error region** is the non-zero lower-bound error past which
    models will be unable to improve. With sufficiently large training sets, models
    saturate in this region.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不可减少误差区域**是指模型无法改进的非零下界误差。对于足够大的训练集，模型在这个区域会达到饱和。'
- en: On model size
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于模型大小
- en: We expect the number of model parameters to fit a data set should follow ![s(m)
    \sim \alpha m^{\beta_p}](../Images/24d8f00443ba3e4ce18240cfff2d93e0.png) where ![s(m)](../Images/2203762900df00c7411cf70a847bfdda.png) is
    the required model size to fin a training set of size ![m](../Images/152a02f7d270706469057ec5bc183e60.png),
    and ![\beta_p \in [0.5,1]](../Images/c5436a709bbb30c80b8049944cde8cd7.png).
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们预期模型参数的数量与数据集的适配应遵循![s(m) \sim \alpha m^{\beta_p}](../Images/24d8f00443ba3e4ce18240cfff2d93e0.png)，其中![s(m)](../Images/2203762900df00c7411cf70a847bfdda.png)是适应大小为![m](../Images/152a02f7d270706469057ec5bc183e60.png)的训练集所需的模型大小，而![\beta_p
    \in [0.5,1]](../Images/c5436a709bbb30c80b8049944cde8cd7.png)。
- en: Best-fit models grow sublinearly in training shard size. Higher values of ![\beta_p](../Images/c98dfbc3e9d18694997557bb9e9c1e2b.png)indicate
    models that make less effective use of extra parameters on larger data sets. Despite
    model size scaling differences though, “*for a given model architecture, we can
    accurately predict the model size that will best fit increasingly larger data
    sets.*”
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳拟合模型在训练分片大小上以次线性增长。较高的![\beta_p](../Images/c98dfbc3e9d18694997557bb9e9c1e2b.png)值表明模型在较大的数据集上对额外参数的使用效果较差。尽管模型大小扩展存在差异，“*对于给定的模型架构，我们可以准确预测能够最好地适应越来越大数据集的模型大小。*”
- en: Implications of the power-law
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 幂律的影响
- en: Predictable learning curves and model size scaling indicate some significant
    implications on how DL could proceed. For machine learning practitioners and researchers,
    predictable scaling can aid model and optimization debugging and iteration time,
    and offer a way to estimate the most impactful next steps to improve model accuracy.
    Operationally, predictable curves can guid decision making about whether or how
    to grow data sets or computation.
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 可预测的学习曲线和模型大小扩展表明，深度学习可能会有一些重要的影响。对于机器学习从业者和研究人员而言，可预测的扩展可以帮助模型和优化调试以及迭代时间，并提供估计提高模型准确性的最有影响力的下一步的方法。在操作上，可预测的曲线可以指导是否以及如何增长数据集或计算。
- en: 'One interesting consequence is that *model exploration* can be done on smaller
    data sets (and hence faster / cheaper). The data set needs to be large enough
    to show accuracy in the power-law region of the curve. Then the most promising
    models can be scaled to larger data sets to ensure proportional accuracy gains.
    This works because growing training sets and models is likely to result in the
    same relative gains across models. When building a company we look for product-market
    fit before scaling the business. In deep learning it seems the analogy is to look
    for *model-problem fit* before scaling, a search then scale strategy:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的结果是，*模型探索*可以在较小的数据集上进行（因此更快/更便宜）。数据集需要足够大，以显示曲线的幂律区域的准确性。然后，可以将最有前景的模型扩展到更大的数据集上，以确保相应的准确性提升。这是因为扩展训练集和模型很可能会在模型之间带来相同的相对增益。在建立公司时，我们在扩展业务之前会寻找产品市场匹配。在深度学习中，似乎类比为在扩展之前寻找*模型问题匹配*，即先搜索再扩展的策略：
- en: '![](../Images/a1f261ca02c750e1ec53a7d399af0955.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1f261ca02c750e1ec53a7d399af0955.png)'
- en: 'If you need to make a business decision about the return on investment, or
    likely accuracy improvement, from investing in the collection of more data, the
    power-law can help you predict returns:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要对投资更多数据收集的投资回报率或可能的准确性提升做出商业决策，幂律可以帮助你预测回报：
- en: '![](../Images/957ad0a1cf564181bfeeadc305e9c9e9.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/957ad0a1cf564181bfeeadc305e9c9e9.png)'
- en: 'Conversely, if generalisation error within the power-law region drifts from
    the power-law predictions, it’s a clue that increasing model size might help,
    or a more extensive hyperparameter search (i.e., more compute), might help:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果在幂律区域内的泛化误差偏离幂律预测，这表明增加模型规模可能会有所帮助，或者更广泛的超参数搜索（即更多的计算）可能会有所帮助：
- en: '![](../Images/241cec1bf74a0100d141caba41031176.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/241cec1bf74a0100d141caba41031176.png)'
- en: …predictable learning and model size curves may offer a way to project the compute
    requirements to reach a particular accuracy level.
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: …可预测的学习和模型规模曲线可能提供了一种预测达到特定准确度水平的计算需求的方法。
- en: Can you beat the power law?
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你能超越幂律吗？
- en: Model architecture improvements (e.g., increasing model depth) seem only to
    shift learning curves down, but not improve the power-law exponent.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构改进（例如，增加模型深度）似乎仅仅是将学习曲线向下移动，但不会改善幂律指数。
- en: We have yet to find factors that affect the power-law exponent. To beat the
    power-law as we increase data set size, models would need to learn more concepts
    with successively less data. In other words, models must successively extract
    more marginal information from each additional training sample.
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们尚未找到影响幂律指数的因素。要在增加数据集规模时超越幂律，模型需要用逐渐减少的数据学习更多的概念。换句话说，模型必须从每个额外的训练样本中提取更多的边际信息。
- en: If we *can* find ways to improve the power-law exponent though, then the potential
    accuracy improvements in some problem domains are ‘immense.’
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们*能够*找到改进幂律指数的方法，那么某些问题领域的潜在准确性提升是“巨大的”。
- en: The empirical data
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实证数据
- en: The empirical data to back all this up was collected by testing various training
    data sizes (in powers of two) with state-of-the-art deep learning models in a
    number of different domains.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 支持这些的实证数据是通过在多个不同领域使用最新深度学习模型测试各种训练数据大小（以2的幂次方）收集的。
- en: Here are the neural machine translation learning curves. On the right you can
    see results for the best-fit model at each training set size. As training set
    sizes grow, the empirical error tends away from the power-law trend, and a more
    exhaustive hyperparameter search would be needed to bring it back in line.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是神经机器翻译的学习曲线。右侧可以看到每个训练集大小的最佳拟合模型的结果。随着训练集大小的增加，实证误差偏离幂律趋势，可能需要更全面的超参数搜索来将其重新对齐。
- en: '![](../Images/162e60cbdae4d8249de4838b19dd9cba.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/162e60cbdae4d8249de4838b19dd9cba.png)'
- en: The next domain is word language models. A variety of model architectures are
    used, and although they differ appreciably, they all show the same learning curve
    profile as characterised by the power-law exponent.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个领域是词语言模型。虽然使用了各种模型架构，它们的差异很大，但都表现出相同的学习曲线特征，由幂律指数表征。
- en: '![](../Images/a62fa308c7a0a853d9170cc7826d1761.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a62fa308c7a0a853d9170cc7826d1761.png)'
- en: 'And here are the results for character language models:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这是字符语言模型的结果：
- en: '![](../Images/c583cb656be9cbf924c71afd78ff0b29.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c583cb656be9cbf924c71afd78ff0b29.png)'
- en: 'With image classification, we see the ‘small data region’ appear on the plots
    when there is insufficient data to learn a good classifier:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像分类中，当数据不足以学习出良好的分类器时，我们可以在图中看到“数据稀少区域”出现：
- en: '![](../Images/ab3deb189ed0a1f1f331582c99ad0b29.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab3deb189ed0a1f1f331582c99ad0b29.png)'
- en: 'Finally, here’s speech recognition:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这里是语音识别：
- en: '![](../Images/2ad30be4abd103ec4c201536b32f984f.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ad30be4abd103ec4c201536b32f984f.png)'
- en: 'We empirically validate that DL model accuracy improves as a power-law as we
    grow training sets for state-of-the-art (SOTA) model architectures in four machine
    learning domains: machine translation, language modeling, image processing, and
    speech recognition. These power-law learning curves exist across all tested domains,
    model architectures, optimizers, and loss functions.'
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们实证验证了，随着在四个机器学习领域（机器翻译、语言建模、图像处理和语音识别）中为最先进（SOTA）模型架构增加训练集，DL模型的准确性按幂律增长。这些幂律学习曲线在所有测试领域、模型架构、优化器和损失函数中都存在。
- en: '[Original](https://blog.acolyer.org/2018/03/28/deep-learning-scaling-is-predictable-empirically/).
    Reposted with permission.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://blog.acolyer.org/2018/03/28/deep-learning-scaling-is-predictable-empirically/)。转载经许可。'
- en: '**Related:**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[How to Make AI More Accessible](/2018/04/make-ai-more-accessible.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何让AI更易获得](/2018/04/make-ai-more-accessible.html)'
- en: '[Are High Level APIs Dumbing Down Machine Learning?](/2018/04/high-level-apis-dumbing-down-machine-learning.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[高级API是否让机器学习变得更简单？](/2018/04/high-level-apis-dumbing-down-machine-learning.html)'
- en: '[Derivation of Convolutional Neural Network from Fully Connected Network Step-By-Step](/2018/04/derivation-convolutional-neural-network-fully-connected-step-by-step.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[逐步推导卷积神经网络与全连接网络的关系](/2018/04/derivation-convolutional-neural-network-fully-connected-step-by-step.html)'
- en: '* * *'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速入门网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT 工作'
- en: '* * *'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Scaling Data Management Through Apache Gobblin](https://www.kdnuggets.com/2023/01/scaling-data-management-apache-gobblin.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过 Apache Gobblin 扩展数据管理](https://www.kdnuggets.com/2023/01/scaling-data-management-apache-gobblin.html)'
- en: '[Data Scaling with Python](https://www.kdnuggets.com/2023/07/data-scaling-python.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 进行数据扩展](https://www.kdnuggets.com/2023/07/data-scaling-python.html)'
- en: '[Things You Should Know When Scaling Your Web Data-Driven Product](https://www.kdnuggets.com/2023/08/things-know-scaling-web-datadriven-product.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[扩展你的网络数据驱动产品时需要知道的事情](https://www.kdnuggets.com/2023/08/things-know-scaling-web-datadriven-product.html)'
- en: '[A Solid Plan for Learning Data Science, Machine Learning, and Deep Learning](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学、机器学习和深度学习的扎实计划](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
- en: '[AI, Analytics, Machine Learning, Data Science, Deep Learning…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能、分析、机器学习、数据科学、深度学习……](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
- en: '[15 Free Machine Learning and Deep Learning Books](https://www.kdnuggets.com/2022/10/15-free-machine-learning-deep-learning-books.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15 本免费的机器学习和深度学习书籍](https://www.kdnuggets.com/2022/10/15-free-machine-learning-deep-learning-books.html)'
