- en: 3 Main Approaches to Machine Learning Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型的三种主要方法
- en: 原文：[https://www.kdnuggets.com/2019/06/main-approaches-machine-learning-models.html](https://www.kdnuggets.com/2019/06/main-approaches-machine-learning-models.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/06/main-approaches-machine-learning-models.html](https://www.kdnuggets.com/2019/06/main-approaches-machine-learning-models.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: In September 2018, I published a blog about my [forthcoming book on The Mathematical
    Foundations of Data Science](https://www.kdnuggets.com/2018/09/learning-mathematics-machine-learning.html). 
    The central question we address is: ***How can we bridge the gap between mathematics
    needed for Artificial Intelligence (Deep Learning and Machine learning) with that
    taught in high schools (up to ages 17/18)?***
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在2018年9月，我发布了一篇关于我的[即将出版的《数据科学的数学基础》一书](https://www.kdnuggets.com/2018/09/learning-mathematics-machine-learning.html)的博客。我们讨论的核心问题是：***我们如何弥合人工智能（深度学习和机器学习）所需的数学与高中所教授的数学（至17/18岁）之间的差距？***
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业领域。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 为你的组织提供 IT 支持'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In this post, we present a chapter from this book called “A Taxonomy of Machine
    Learning Models.” The book is now available for an early bird discount released
    as chapters. **If you are interested in getting early discounted copies, please
    contact ajit.jaokar at feynlabs.ai.**
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们介绍了这本书中的一章，名为“机器学习模型的分类”。这本书现在已经发布，并以章节形式提供早鸟折扣。如果你有兴趣获得早期折扣版，请联系 ajit.jaokar
    at feynlabs.ai。
- en: 1\. A Taxonomy of Machine Learning Models
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1\. 机器学习模型的分类
- en: There is no simple way to classify machine learning algorithms. In this section,
    we present a taxonomy of machine learning models adapted from the book *Machine
    Learning* by [Peter Flach](https://www.amazon.co.uk/Machine-Learning-Science-Algorithms-Sense/dp/1107422221/ref=sr_1_1?crid=2XR5TAVPN0D1E&keywords=peter+flach&qid=1554128279&s=gateway&sprefix=peter+flach%2Caps%2C153&sr=8-1).
    While the structure for classifying algorithms is based on the book, the explanation
    presented below is created by us.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 没有简单的方法来分类机器学习算法。在这一部分，我们呈现了从*彼得·弗拉赫*的《机器学习》一书中改编的机器学习模型分类法。虽然分类算法的结构基于该书，但下面的解释是我们创建的。
- en: For a given problem, the collection of all possible outcomes represents the
    **sample space or instance space**.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定问题，所有可能结果的集合表示**样本空间或实例空间**。
- en: 'The basic idea for creating a taxonomy of algorithms is that we divide the
    instance space by using one of three ways:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 创建算法分类法的基本理念是通过以下三种方式之一划分实例空间：
- en: Using a Logical expression.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用逻辑表达式。
- en: Using the Geometry of the instance space.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用实例空间的几何。
- en: Using Probability to classify the instance space.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用概率来分类实例空间。
- en: The outcome of the transformation of the instance space by a machine learning
    algorithm using the above techniques should be exhaustive (cover all possible
    outcomes) and mutually exclusive (non-overlapping).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述技术转换实例空间的机器学习算法的结果应该是详尽的（涵盖所有可能的结果）和互斥的（不重叠）。
- en: 2\. Logical models
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2\. 逻辑模型
- en: 2.1 Logical models - Tree models and Rule models
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1 逻辑模型 - 树模型和规则模型
- en: '**Logical models** use a logical expression to divide the instance space into
    segments and hence construct grouping models. A **logical expression** is an expression
    that returns a Boolean value, i.e., a True or False outcome. Once the data is
    grouped using a logical expression, the data is divided into homogeneous groupings
    for the problem we are trying to solve.  For example, for a classiﬁcation problem,
    all the instances in the group belong to one class.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**逻辑模型**使用逻辑表达式将实例空间划分为多个段，从而构建分组模型。**逻辑表达式**是返回布尔值的表达式，即True或False结果。一旦使用逻辑表达式对数据进行分组，数据将被划分为解决问题的同质分组。例如，对于分类问题，组中的所有实例都属于同一类。'
- en: 'There are mainly two kinds of logical models: **Tree models** and **Rule models**.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑模型主要有两种类型：**树模型**和**规则模型**。
- en: Rule models consist of a collection of implications or IF-THEN rules. For tree-based
    models, the ‘if-part’ deﬁnes a segment and the ‘then-part’ deﬁnes the behaviour
    of the model for this segment. Rule models follow the same reasoning.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 规则模型由一组蕴涵或IF-THEN规则组成。对于树模型，“if部分”定义了一个段，“then部分”定义了该段的模型行为。规则模型遵循相同的推理。
- en: 'Tree models can be seen as a particular type of rule model where the if-parts
    of the rules are organised in a tree structure. Both Tree models and Rule models
    use the same approach to supervised learning.  The approach can be summarised
    in two strategies: we could first find the body of the rule (the concept) that
    covers a sufficiently homogeneous set of examples and then find a label to represent
    the body. Alternately, we could approach it from the other direction, i.e., first
    select a class we want to learn and then find rules that cover examples of the
    class.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 树模型可以看作是规则模型的一种特殊类型，其中规则的if部分以树结构组织。树模型和规则模型都采用相同的监督学习方法。该方法可以总结为两种策略：我们可以首先找到覆盖足够同质实例的规则主体（概念），然后找到代表该主体的标签。或者，我们可以从另一个方向入手，即首先选择一个我们想要学习的类别，然后找到覆盖该类别实例的规则。
- en: 'A simple tree-based model is shown below. The tree shows survival numbers of
    passengers on the Titanic ("sibsp" is the number of spouses or siblings aboard).
    The values under the leaves show the probability of survival and the percentage
    of observations in the leaf. The model can be summarised as: Your chances of survival
    were good if you were (i) a female or (ii) a male younger than 9.5 years with
    less than 2.5 siblings.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个简单的树模型。树模型显示了泰坦尼克号乘客的生存数量（“sibsp”是船上配偶或兄弟姐妹的数量）。叶子下的值显示了生存的概率以及叶子的观察百分比。该模型可以总结为：如果你是（i）女性，或（ii）年龄小于9.5岁且兄弟姐妹少于2.5个的男性，那么你的生存机会很好。
- en: '![](../Images/85e50c23e3bb1eb1dcaef01fa2dbb9aa.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85e50c23e3bb1eb1dcaef01fa2dbb9aa.png)'
- en: ([Image source](https://en.wikipedia.org/wiki/Decision_tree_learning#/media/File:CART_tree_titanic_survivors.png).)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ([图片来源](https://en.wikipedia.org/wiki/Decision_tree_learning#/media/File:CART_tree_titanic_survivors.png).)
- en: 2.2 Logical models and Concept learning
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2 逻辑模型与概念学习
- en: To understand logical models further, we need to understand the idea of **Concept
    Learning**. Concept Learning involves learning logical expressions or concepts
    from examples. The idea of Concept Learning fits in well with the idea of Machine
    learning, i.e., inferring a general function from specific training examples.
    Concept learning forms the basis of both tree-based and rule-based models.  More
    formally, Concept Learning involves acquiring the definition of a general category
    from a given set of positive and negative training examples of the category. A
    Formal Definition for Concept Learning is “***The inferring of a Boolean-valued
    function from training examples of its input and output.” ***In concept learning,
    we only learn a description for the positive class and label everything that doesn’t
    satisfy that description as negative.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要进一步理解逻辑模型，我们需要了解**概念学习**的概念。概念学习涉及从实例中学习逻辑表达式或概念。概念学习的思想与机器学习的理念非常契合，即从具体的训练实例中推断出一般函数。概念学习是树模型和规则模型的基础。更正式地说，概念学习包括从给定的一组正负训练实例中获得一般类别的定义。概念学习的正式定义是“***从输入和输出的训练实例中推断一个布尔值函数。***”在概念学习中，我们仅为正类学习描述，并将所有不满足该描述的标记为负类。
- en: The following example explains this idea in more detail.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例详细解释了这一思想。
- en: '![](../Images/d828ab1972e27be4bb02766ea6cb01ca.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d828ab1972e27be4bb02766ea6cb01ca.png)'
- en: A [Concept Learning](https://web.cs.hacettepe.edu.tr/~ilyas/Courses/BIL712/lec01-conceptLearning.pdf)
    Task called “Enjoy Sport” as shown above is defined by a set of data from some
    example days. Each data is described by six attributes. The task is to learn to
    predict the value of Enjoy Sport for an arbitrary day based on the values of its
    attribute values. The problem can be represented by a **series of hypotheses**.
    Each hypothesis is described by a conjunction of constraints on the attributes.
    The training data represents a set of positive and negative examples of the target
    function. In the example above, each hypothesis is a vector of six constraints,
    specifying the values of the six attributes –  Sky, AirTemp, Humidity, Wind, Water,
    and Forecast. The training phase involves learning the set of days (as a conjunction
    of attributes) for which Enjoy Sport = yes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 [概念学习](https://web.cs.hacettepe.edu.tr/~ilyas/Courses/BIL712/lec01-conceptLearning.pdf)
    任务，称为“享受运动”，如上所示，由一些示例日期的数据集定义。每个数据由六个属性描述。任务是学习基于属性值预测“享受运动”的值。问题可以用 **一系列假设**
    来表示。每个假设由对属性的约束的结合描述。训练数据代表目标函数的正例和负例。在上面的例子中，每个假设是一个包含六个约束的向量，指定了六个属性的值——天空、空气温度、湿度、风、水和预报。训练阶段涉及学习一组日期（作为属性的结合），其中“享受运动”=
    是。
- en: 'Thus, the problem can be formulated as:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，问题可以表述为：
- en: 'Given instances X  which represent a set of all possible days, each described
    by the attributes:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定实例 X，表示所有可能的日期集合，每个日期由以下属性描述：
- en: 'Sky – (values: Sunny, Cloudy, Rainy),'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天空 – （值：晴天，多云，雨天），
- en: 'AirTemp – (values: Warm, Cold),'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空气温度 – （值：温暖，寒冷），
- en: 'Humidity – (values: Normal, High),'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 湿度 – （值：正常，高），
- en: 'Wind – (values: Strong, Weak),'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风 – （值：强，弱），
- en: 'Water – (values: Warm, Cold),'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水 – （值：温暖，寒冷），
- en: 'Forecast – (values: Same, Change).'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预报 – （值：相同，变化）。
- en: Try to identify a function that can predict the target variable Enjoy Sport
    as yes/no, i.e., 1 or 0.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试识别一个可以预测目标变量“享受运动”是“是”/“否”，即 1 或 0 的函数。
- en: 2.3 Concept learning as a search problem and as Inductive Learning
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3 概念学习作为搜索问题和归纳学习
- en: 'We can also formulate Concept Learning as a **search problem**. We can think
    of  Concept learning as searching through a set of predefined space of potential
    hypotheses to identify a hypothesis that best fits the training examples.  Concept
    learning is also an example of **Inductive Learning**. Inductive learning, also
    known as discovery learning, is a process where the learner discovers rules by
    observing examples. Inductive learning is different from deductive learning, where
    students are given rules that they then need to apply. Inductive learning is based
    on the **inductive learning hypothesis.** The Inductive Learning Hypothesis postulates
    that: Any hypothesis found to approximate the target function well over a sufficiently
    large set of training examples is expected to approximate the target function
    well over other unobserved examples.  This idea is the fundamental assumption
    of inductive learning.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将概念学习表述为 **搜索问题**。我们可以将概念学习看作是在预定义的潜在假设空间中搜索，以识别最符合训练示例的假设。概念学习也是 **归纳学习**
    的一个例子。归纳学习，也称为发现学习，是一种通过观察示例发现规则的过程。归纳学习不同于演绎学习，演绎学习是学生获得规则后需要应用的。归纳学习基于 **归纳学习假设**。归纳学习假设认为：任何在足够大的训练示例集上能很好近似目标函数的假设，预计在其他未观察到的示例上也能很好近似目标函数。这一思想是归纳学习的基本假设。
- en: To summarise, in this section, we saw the first class of algorithms where we
    divided the instance space based on a logical expression. We also discussed how
    logical models are based on the theory of concept learning – which in turn – can
    be formulated as an inductive learning or a search problem.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，在本节中，我们看到了第一类算法，在这些算法中，我们基于逻辑表达式划分了实例空间。我们还讨论了逻辑模型如何基于概念学习理论——而概念学习理论又可以表述为归纳学习或搜索问题。
- en: 3\. Geometric models
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3. 几何模型
- en: In the previous section, we have seen that with logical models, such as decision
    trees, a logical expression is used to partition the instance space. Two instances
    are similar when they end up in the same logical segment. In this section, we
    consider models that define similarity by considering the geometry of the instance
    space.  In Geometric models, features could be described as points in two dimensions
    (*x-* and *y*-axis) or a three-dimensional space (*x*, *y,* and *z*). Even when
    features are not intrinsically geometric, they could be modelled in a geometric
    manner (for example, temperature as a function of time can be modelled in two
    axes). In geometric models, there are two ways we could impose similarity.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们看到使用逻辑模型，如决策树，使用逻辑表达式来划分实例空间。当两个实例最终落在相同的逻辑片段中时，它们是相似的。在本节中，我们考虑通过考虑实例空间的几何来定义相似性的模型。在几何模型中，特征可以描述为二维（*x-*
    和 *y* 轴）或三维空间（*x*、*y* 和 *z*）。即使特征本质上不是几何的，也可以以几何方式建模（例如，时间函数的温度可以在两个轴上建模）。在几何模型中，我们可以通过两种方式强加相似性。
- en: We could use geometric concepts like **lines or planes to segment (classify)**
    the instance space. These are called **Linear models**.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用几何概念，如**直线或平面进行分割（分类）**实例空间。这些称为**线性模型**。
- en: Alternatively, we can use the geometric notion of distance to represent similarity.
    In this case, if two points are close together, they have similar values for features
    and thus can be classed as similar. We call such models as **Distance-based models**.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，我们可以使用几何上的距离概念来表示相似性。在这种情况下，如果两个点接近，它们在特征上具有相似的值，因此可以被归类为相似。我们称这种模型为**基于距离的模型**。
- en: 3.1 Linear models
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1 线性模型
- en: Linear models are relatively simple. In this case, the function is represented
    as a linear combination of its inputs. Thus, if *x*[1] and *x*[2] are two scalars
    or vectors of the same dimension and *a* and *b* are arbitrary scalars, then *ax*[1]
    + *bx*[2] represents a linear combination of *x*[1] and *x*[2]. In the simplest
    case where *f*(*x*) represents a straight line, we have an equation of the form
    *f* (*x*) = *mx* + *c* where *c* represents the intercept and *m* represents the
    slope.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型相对简单。在这种情况下，函数被表示为其输入的线性组合。因此，如果 *x*[1] 和 *x*[2] 是两个相同维度的标量或向量，*a* 和 *b*
    是任意标量，那么 *ax*[1] + *bx*[2] 表示 *x*[1] 和 *x*[2] 的线性组合。在最简单的情况下，当 *f*(*x*) 表示一条直线时，我们有一个形式为
    *f*(*x*) = *mx* + *c* 的方程，其中 *c* 代表截距，*m* 代表斜率。
- en: '![](../Images/5d556dcaa6505f6f20801865dff203e4.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d556dcaa6505f6f20801865dff203e4.png)'
- en: ([Image source](https://en.wikipedia.org/wiki/Linear_regression).)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ([图片来源](https://en.wikipedia.org/wiki/Linear_regression).)
- en: Linear models are **parametric**, which means that they have a ﬁxed form with
    a small number of numeric parameters that need to be learned from data. For example,
    in *f* (*x*) = *mx* + *c*, *m* and *c* are the parameters that we are trying to
    learn from the data. This technique is different from tree or rule models, where
    the structure of the model (e.g., which features to use in the tree, and where)
    is [not ﬁxed in advance](https://www.quora.com/Why-is-a-decision-tree-considered-a-non-parametric-model).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型是**参数化的**，这意味着它们有一个固定的形式，具有少量需要从数据中学习的数值参数。例如，在 *f* (*x*) = *mx* + *c* 中，*m*
    和 *c* 是我们试图从数据中学习的参数。这种技术不同于树或规则模型，其中模型的结构（例如，树中使用哪些特征以及位置）[不是预先固定的](https://www.quora.com/Why-is-a-decision-tree-considered-a-non-parametric-model)。
- en: Linear models are **stable**, i.e., small variations in the training data have
    only a limited impact on the learned model. In contrast, **tree models tend to
    vary more with the training data**, as the choice of a different split at the
    root of the tree typically means that the rest of the tree is different as well. 
    As a result of having relatively few parameters, Linear models have **low variance
    and high bias**. This implies that **Linear models are less likely to overfit the
    training data** than some other models. However, they are more likely to underfit.
    For example, if we want to learn the boundaries between countries based on labelled
    data, then linear models are not likely to give a good approximation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型是**稳定**的，即训练数据的微小变化对学习到的模型的影响有限。相比之下，**树模型通常会随着训练数据的变化而变化更多**，因为树的根部选择不同的分割通常意味着树的其余部分也会不同。
    由于参数相对较少，线性模型具有**低方差和高偏差**。这意味着**线性模型比其他一些模型更不容易过拟合训练数据**。然而，它们更容易欠拟合。例如，如果我们想根据标记数据学习国家之间的边界，那么线性模型可能不会给出一个好的近似。
- en: In this section, we could also use algorithms that include **kernel methods**,
    such as support vector machine (SVM). Kernel methods use the kernel function to
    transform data into another dimension where easier separation can be achieved
    for the data, such as using a hyperplane for SVM.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们还可以使用包括**核方法**的算法，例如支持向量机（SVM）。核方法使用核函数将数据转换到另一个维度，在该维度上可以更容易地进行数据分离，例如使用超平面进行SVM。
- en: 3.2 Distance-based models
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2 基于距离的模型
- en: '**Distance-based models** are the second class of Geometric models. Like Linear
    models, distance-based models are based on the geometry of data. As the name implies,
    distance-based models work on the concept of distance.  In the context of Machine
    learning, the concept of distance is not based on merely the physical distance
    between two points. Instead, we could think of the distance between two points
    considering the **mode of transport** between two points. Travelling between two
    cities by plane covers less distance physically than by train because a plane
    is unrestricted. Similarly, in chess, the concept of distance depends on the piece
    used – for example, a Bishop can move diagonally.   Thus, depending on the entity
    and the mode of travel, the concept of distance can be experienced differently.
    The distance metrics commonly used are **Euclidean**, **Minkowski**, **Manhattan**,
    and **Mahalanobis**.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于距离的模型**是几何模型的第二类。与线性模型一样，基于距离的模型基于数据的几何。顾名思义，基于距离的模型依赖于距离的概念。在机器学习的背景下，距离的概念不仅仅是两点之间的物理距离。我们可以考虑两点之间的**运输方式**。通过飞机旅行的城市之间的距离在物理上比火车旅行的距离要少，因为飞机没有限制。类似地，在国际象棋中，距离的概念依赖于使用的棋子——例如，象可以对角移动。因此，根据实体和旅行方式，距离的概念可以有不同的体验。常用的距离度量包括**欧几里得**、**闵可夫斯基**、**曼哈顿**和**马氏**。'
- en: '![](../Images/26c20eb8d04972bcdaf9fd0b32166a34.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26c20eb8d04972bcdaf9fd0b32166a34.png)'
- en: ([Image source](http://www.ieee.ma/uaesb/pdf/distances-in-classification.pdf).)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ([图片来源](http://www.ieee.ma/uaesb/pdf/distances-in-classification.pdf).)
- en: Distance is applied through the concept of **neighbours and exemplars**. Neighbours
    are points in proximity with respect to the distance measure expressed through
    exemplars. Exemplars are either **centroids** that ﬁnd a centre of mass according
    to a chosen distance metric or **medoids** that ﬁnd the most centrally located
    data point. The most commonly used centroid is the arithmetic mean, which minimises
    squared Euclidean distance to all other points.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 距离通过**邻居和样本**的概念应用。邻居是相对于通过样本表示的距离度量的邻近点。样本是找到根据所选距离度量的质量中心的**中心点**或找到最中央数据点的**代表元**。最常用的中心点是算术均值，它最小化了与所有其他点的平方欧几里得距离。
- en: 'Notes:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：
- en: 'The **centroid** represents the geometric centre of a plane figure, i.e., the
    arithmetic mean position of all the points in the figure from the centroid point.
    This definition extends to any object in *n*-dimensional space: its centroid is
    the mean position of all the points.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中心点**代表平面图形的几何中心，即图形中所有点相对于中心点的算术平均位置。这个定义扩展到任意*n*维空间中的对象：其中心点是所有点的平均位置。'
- en: '**Medoids** are similar in concept to means or centroids. Medoids are most
    commonly used on data when a mean or centroid cannot be defined. They are used
    in contexts where the centroid is not representative of the dataset, such as in
    image data.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代表元**在概念上类似于均值或中心点。代表元通常用于无法定义均值或中心点的数据场景。它们在中心点无法代表数据集的情况下使用，例如在图像数据中。'
- en: Examples of distance-based models include the **nearest-neighbour** models,
    which use the training data as exemplars – for example, in classification. The
    **K-means clustering** algorithm also uses exemplars to create clusters of similar
    data points.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基于距离的模型的例子包括**最近邻**模型，它们使用训练数据作为样本，例如在分类中。**K均值聚类**算法也使用样本来创建相似数据点的簇。
- en: 4\. Probabilistic models
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4\. 概率模型
- en: The third family of machine learning algorithms is the probabilistic models.
    We have seen before that the k-nearest neighbour algorithm uses the idea of distance
    (e.g., Euclidian distance) to classify entities, and logical models use a logical
    expression to partition the instance space. In this section, we see how the **probabilistic
    models use the idea of probability to classify new entities.**
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法的第三类是概率模型。我们之前已经看到，k-近邻算法使用距离（例如，欧几里得距离）的理念来分类实体，而逻辑模型使用逻辑表达式来划分实例空间。在本节中，我们将看到**概率模型如何使用概率的概念来分类新实体**。
- en: 'Probabilistic models see features and target variables as random variables.
    The process of modelling represents and **manipulates the level of uncertainty**
    with respect to these variables. There are two types of probabilistic models:
    **Predictive and Generative**. Predictive probability models use the idea of a
    **conditional probability** distribution *P* (*Y* |*X*) from which *Y* can be
    predicted from *X*.  Generative models estimate the **joint distribution** *P*
    (*Y*, *X*).  Once we know the joint distribution for the generative models, we
    can derive any conditional or marginal distribution involving the same variables.
    Thus, the generative model is capable of creating new data points and their labels,
    knowing the joint probability distribution. The joint distribution looks for a
    relationship between two variables. Once this relationship is inferred, it is
    possible to infer new data points.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 概率模型将特征和目标变量视为随机变量。建模过程表示和**操控这些变量的不确定性**。概率模型有两种类型：**预测型和生成型**。预测型概率模型使用**条件概率**分布
    *P* (*Y* |*X*) 从中可以从 *X* 预测 *Y*。生成型模型估计**联合分布** *P* (*Y*, *X*)。一旦我们知道生成模型的联合分布，我们可以推导出涉及相同变量的任何条件或边际分布。因此，生成模型能够创建新的数据点及其标签，知道联合概率分布。联合分布寻找两个变量之间的关系。一旦推断出这种关系，就可以推断出新的数据点。
- en: '**Naïve Bayes** is an example of a probabilistic classifier.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯** 是一种概率分类器的例子。'
- en: The goal of any probabilistic classifier is given a set of features (*x*_0 through
    *x*_n) and a set of classes (*c*_0 through *c*_k), we aim to determine the probability
    of the features occurring in each class, and to return the most likely class.
    Therefore, for each class, we need to calculate *P*(*c*_i | x_0, …, x_n).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 任何概率分类器的目标是，给定一组特征（*x*_0 到 *x*_n）和一组类别（*c*_0 到 *c*_k），我们旨在确定特征在每个类别中出现的概率，并返回最可能的类别。因此，对于每个类别，我们需要计算
    *P*(*c*_i | x_0, …, x_n)。
- en: We can do this using the **Bayes rule** defined as
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用**贝叶斯规则**来实现这一点
- en: '![](../Images/b93638ae72ebb1dfd9def2e9dfb5165f.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b93638ae72ebb1dfd9def2e9dfb5165f.png)'
- en: The Naïve Bayes algorithm is based on the idea of **Conditional Probability. 
    Conditional probability is based on finding the** probability that something will
    happen, *given that something else* has already happened. The task of the algorithm
    then is to look at the evidence and to determine the likelihood of a specific
    class and assign a label accordingly to each entity.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯算法基于**条件概率**的理念。条件概率是基于找到**某事发生的**概率，*前提是其他事情* 已经发生。算法的任务是查看证据并确定特定类别的可能性，并相应地为每个实体分配标签。
- en: Conclusion
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: The above discussion presents a way to classify algorithms based on their mathematical
    foundations. While the discussion is simplified, it provides a comprehensive way
    to explore algorithms from first principles. **If you are interested in getting
    early discounted copies, please contact ajit.jaokar at feynlabs.ai.**
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 上述讨论提供了一种基于数学基础的分类算法的方法。虽然讨论简化了，但它提供了一种从基本原理深入探讨算法的全面方式。**如果你对获得早期折扣副本感兴趣，请联系
    ajit.jaokar at feynlabs.ai。**
- en: '**Related:**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Decision Trees — An Intuitive Introduction](https://www.kdnuggets.com/2019/02/decision-trees-introduction.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[决策树——直观介绍](https://www.kdnuggets.com/2019/02/decision-trees-introduction.html)'
- en: '[A Beginner’s Guide to Linear Regression in Python with Scikit-Learn](https://www.kdnuggets.com/2019/03/beginners-guide-linear-regression-python-scikit-learn.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Scikit-Learn 的 Python 线性回归入门指南](https://www.kdnuggets.com/2019/03/beginners-guide-linear-regression-python-scikit-learn.html)'
- en: '[Naive Bayes from Scratch using Python only – No Fancy Frameworks](https://www.kdnuggets.com/2018/10/naive-bayes-from-scratch-python.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[仅使用 Python 从头实现朴素贝叶斯——无复杂框架](https://www.kdnuggets.com/2018/10/naive-bayes-from-scratch-python.html)'
- en: More On This Topic
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[AI, Analytics, Machine Learning, Data Science, Deep Learning…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能、分析、机器学习、数据科学、深度学习…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
- en: '[Data Labeling for Machine Learning: Market Overview, Approaches, and Tools](https://www.kdnuggets.com/2021/12/data-labeling-ml-overview-and-tools.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习的数据标注：市场概况、方法与工具](https://www.kdnuggets.com/2021/12/data-labeling-ml-overview-and-tools.html)'
- en: '[Machine Learning’s Sweet Spot: Pure Approaches in NLP and Document Analysis](https://www.kdnuggets.com/2022/05/machine-learning-sweet-spot-pure-approaches-nlp-document-analysis.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习的甜蜜点：自然语言处理与文档分析中的纯方法](https://www.kdnuggets.com/2022/05/machine-learning-sweet-spot-pure-approaches-nlp-document-analysis.html)'
- en: '[Automated Machine Learning with Python: A Comparison of Different…](https://www.kdnuggets.com/2023/03/automated-machine-learning-python-comparison-different-approaches.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Python的自动化机器学习：不同方法的比较…](https://www.kdnuggets.com/2023/03/automated-machine-learning-python-comparison-different-approaches.html)'
- en: '[Main 2021 Developments and Key 2022 Trends in AI, Data Science,…](https://www.kdnuggets.com/2021/12/trends-ai-data-science-ml-technology.html)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2021年的主要发展及2022年的关键趋势：人工智能、数据科学…](https://www.kdnuggets.com/2021/12/trends-ai-data-science-ml-technology.html)'
- en: '[Approaches to Text Summarization: An Overview](https://www.kdnuggets.com/2019/01/approaches-text-summarization-overview.html)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本总结的方法：概述](https://www.kdnuggets.com/2019/01/approaches-text-summarization-overview.html)'
