# ChatGPT 在做什么以及它为何有效？

> 原文：[https://www.kdnuggets.com/2023/04/chatgpt-work.html](https://www.kdnuggets.com/2023/04/chatgpt-work.html)

![ChatGPT 在做什么 …… 以及它为何有效？](../Images/40e094be6eac621ec1e3eefb9ef1f33f.png)

# 它只是一次添加一个词

* * *

## 我们的前 3 个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业的快车道。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析能力

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你所在组织的 IT

* * *

[ChatGPT](https://chat.openai.com/) 能够自动生成即使在表面上也像是人类编写的文本的能力是令人惊叹的，也出乎意料。但是它是如何做到的呢？为什么它能有效地工作？我的目的在于粗略勾勒一下 ChatGPT 内部的运作机制——然后探讨它为何能如此成功地生成我们认为有意义的文本。我一开始就要说明，我将重点关注整体情况——虽然我会提到一些工程细节，但不会深入探讨。（我将要讲述的本质同样适用于其他当前的“大型语言模型” [LLMs]，而不仅仅是 ChatGPT。）

首先需要解释的是，ChatGPT 始终基本上试图做的是生成对目前文本的“合理延续”，其中“合理”的意思是“在看到人们在数十亿个网页上写的内容后，可能会期望某人写出的内容。”

所以假设我们有一段文本“*AI 最棒的地方在于它的能力*”。想象一下扫描数十亿页人类编写的文本（比如在网上和数字化的书籍中），并找出所有出现该文本的实例——然后查看接下来哪个词出现的频率。ChatGPT 实际上做的就是类似的事情，只不过（正如我将要解释的那样）它并不直接查看文字；它寻找在某种意义上“意义匹配”的内容。但最终的结果是，它生成了一个可能接下来的词的排名列表，以及相应的“概率”：

![ChatGPT 在做什么 …… 以及它为何有效？](../Images/a01f472455503e32a1bc337adc319aee.png)

令人惊讶的是，当 ChatGPT 写作时，它本质上在做的就是一次又一次地问“鉴于目前的文本，接下来的词应该是什么？”——每次添加一个词。（更准确地说，正如我将解释的，它是添加一个“标记”，这可能只是一个词的一部分，这就是它有时能够“创造新词”的原因。）

但是，好吧，每一步它会得到一个带有概率的词汇列表。那么，它应该实际选择哪个词来添加到它正在写的文章中（或其他内容）呢？人们可能认为应该选择“排名最高”的词（即被分配了最高“概率”的词）。但这时一点巫术开始悄然出现。因为出于某种原因——也许有一天我们会有科学风格的理解——如果我们总是选择排名最高的词，通常会得到非常“平淡”的文章，似乎从未“展现出任何创造力”（有时甚至逐字重复）。但如果有时（随机地）选择较低排名的词，我们就会得到一篇“更有趣”的文章。

这里的随机性意味着如果我们多次使用相同的提示，可能会得到不同的文章。而且，按照巫术的想法，有一个所谓的“温度”参数，决定了较低排名的词汇出现的频率。在文章生成中，结果显示“温度”设置为0.8似乎是最好的。（值得强调的是，这里并没有使用“理论”；这只是实践中发现有效的方法。比如“温度”概念的存在是因为使用了[来自统计物理学的指数分布](https://writings.stephenwolfram.com/2023/02/computational-foundations-for-the-second-law-of-thermodynamics/#textbook-thermodynamics)，但并没有“物理”上的联系——至少我们现在还不知道。）

在继续之前，我应该解释一下，为了阐述的目的，我主要不会使用[ChatGPT中的完整系统](https://openai.com/blog/chatgpt/)，而是通常使用一个更简单的[GPT-2系统](https://resources.wolframcloud.com/NeuralNetRepository/resources/GPT2-Transformer-Trained-on-WebText-Data/)，它有一个很好的特点，就是足够小可以在标准桌面计算机上运行。因此，我展示的几乎所有内容都可以包括可以立即在你电脑上运行的明确[Wolfram Language](https://www.wolfram.com/language/)代码。（点击任何图片可以复制其背后的代码。）

例如，下面是如何获取上述概率表的。首先，我们必须[检索底层的“语言模型”神经网络](https://resources.wolframcloud.com/NeuralNetRepository)：

![ChatGPT 在做什么……它为何有效？](../Images/653dad073b60fc901f968e8832731421.png)

稍后，我们将查看这个神经网络内部的工作原理。但现在我们可以将这个“网络模型”作为黑箱应用于到目前为止的文本，并请求模型所说的按概率排名前5的词汇：

![ChatGPT 在做什么……它为何有效？](../Images/9fb206410b79442ce68747faf4e25bf6.png)

这将结果转换为一个明确格式的“[数据集](https://www.wolfram.com/language/elementary-introduction/2nd-ed/45-datasets.html)”：

![ChatGPT 在做什么……以及它为何有效？](../Images/655b791e66ec07400f4902c7f259d508.png)

如果一个人重复“应用模型”——每一步都添加具有最高概率的单词（在此代码中指定为模型的“决策”）——会发生什么：

![ChatGPT 在做什么……以及它为何有效？](../Images/473fe1bb8ed8ed96f79141519bee64a3.png)

如果继续下去会发生什么？在这种（“零温度”）情况下，生成的文本很快变得相当混乱和重复：

![ChatGPT 在做什么……以及它为何有效？](../Images/b11d15bda3f9caa824039614e42cee63.png)

但如果不是总是选择“最高”单词，而是有时随机选择“非最高”单词（“随机性”对应于“温度”0.8）会怎么样？同样可以生成文本：

![ChatGPT 在做什么……以及它为何有效？](../Images/71051856346e681af42a5b513ebf56d6.png)

每次这样做时，会做出不同的随机选择，文本也会不同——如下这 5 个示例：

![ChatGPT 在做什么……以及它为何有效？](../Images/4779a5bc344605f10f00bf3b931c26e0.png)

值得指出的是，即使在第一步，有很多可能的“下一个单词”可供选择（在温度0.8下），尽管它们的概率很快就会下降（是的，这个对数-对数图上的直线对应于一种 *n*^(–1) [“幂律”衰减，这在语言的一般统计中非常有特点](https://www.wolframscience.com/nks/notes-8-8--zipfs-law/)）：

![ChatGPT 在做什么……以及它为何有效？](../Images/0b3e82f2cd0f66c6cbd3c09107e46ed9.png)

那么如果继续下去会发生什么？这是一个随机示例。它比最高概率（零温度）情况更好，但仍然有点怪异：

![ChatGPT 在做什么……以及它为何有效？](../Images/fdddb35464fab22cba81d5087a40187f.png)

这是用 [最简单的 GPT-2 模型](https://resources.wolframcloud.com/NeuralNetRepository/resources/GPT2-Transformer-Trained-on-WebText-Data/) （来自 2019 年）完成的。使用更新的 [更大的 GPT-3 模型](https://platform.openai.com/docs/model-index-for-researchers)，结果会更好。这是用相同的“提示”生成的最高概率（零温度）文本，但使用的是最大的 GPT-3 模型：

![ChatGPT 在做什么……以及它为何有效？](../Images/e7dfa34ec6789205c82ef3a798e36f71.png)

这是一个“温度 0.8”的随机示例：

![ChatGPT 在做什么……以及它为何有效？](../Images/759db263b5f007f689cdfb7593076c33.png)

# 概率从哪里来？

好吧，ChatGPT 总是基于概率选择下一个单词。但这些概率来自哪里？让我们从一个更简单的问题开始。我们来考虑逐字生成英语文本。我们如何计算每个字母的概率？

我们可以做的一个非常简单的事情是，取一段英语文本样本，计算其中不同字母的出现频率。例如，[这个工具统计了维基百科文章](https://www.wolfram.com/language/elementary-introduction/2nd-ed/34-associations.html#i-8)中“cats”一词的字母：

![ChatGPT在做什么……以及它为什么有效？](../Images/c46cb5e0569986ebba1d77f5d33b9593.png)

这对“dogs”也做了相同的处理：

![ChatGPT在做什么……以及它为什么有效？](../Images/1cf6df5eed08359b9f34acbbc5b91a07.png)

结果相似，但不完全相同（“o”在“dogs”文章中无疑更常见，因为它出现在“dog”这个单词中）。不过，如果我们取一个足够大的英语文本样本，我们可以预期最终会得到至少相当一致的结果：

![ChatGPT在做什么……以及它为什么有效？](../Images/81ed5202f4787ff4e5a8ece66d4cdc10.png)

这是如果我们仅仅根据这些概率生成字母序列的一个样本：

![ChatGPT在做什么……以及它为什么有效？](../Images/41bd4cc72ebb4434789de278f1250b42.png)

我们可以通过添加空格，将其划分成“单词”，就像这些空格是具有一定概率的字母一样：

![ChatGPT在做什么……以及它为什么有效？](../Images/76490be0c51692a3db2bef08e344c736.png)

我们可以通过强制“单词长度”的分布与英语中的实际情况相符，稍微改善一下“单词”的生成效果：

![ChatGPT在做什么……以及它为什么有效？](../Images/87fc4042cbbffeb370d678bb80669947.png)

我们这里没有生成任何“实际单词”，但结果看起来稍微好一些。不过，要进一步改进，我们需要做的不仅仅是随机挑选每个字母。例如，我们知道如果有一个“q”，下一个字母基本上必须是“u”。

这是单个字母概率的图示：

![ChatGPT在做什么……以及它为什么有效？](../Images/e0f7bb740422e6c5d82924a476635b53.png)

这里是一个图示，显示了典型英语文本中字母对（“2-grams”）的概率。可能的首字母显示在页面的横向，第二个字母显示在页面的纵向：

![ChatGPT在做什么……以及它为什么有效？](../Images/dd7cc7a8032a4d5eaac1d72eab99e25c.png)

我们在这里看到，例如，“q”列是空白的（零概率），除了在“u”行。好，现在我们不再一次生成一个字母，而是一次看两个字母，利用这些“2-gram”概率生成“单词”。下面是结果的一个样本，其中包含一些“实际单词”：

![ChatGPT在做什么……以及它为什么有效？](../Images/0372a97c531920b036ac620da8ecd741.png)

通过足够多的英语文本，我们不仅可以对单个字母或字母对（2-grams）的概率进行相当好的估计，还可以对更长的字母序列进行估计。如果我们用逐渐长的*n*-gram概率生成“随机单词”，我们会发现它们变得越来越“逼真”：

![ChatGPT 在做什么……以及为什么它有效？](../Images/6294df51134133db7fa94ff07136ca9e.png)

但让我们现在假设——或多或少像 ChatGPT 所做的那样——我们处理的是整个单词，而不是字母。英语中大约有 40,000 个[常用单词](https://reference.wolfram.com/language/ref/WordList.html)。通过查看大量英语文本（比如几百万本书，总共几百亿个单词），我们可以获得[每个单词的常见程度估算](https://reference.wolfram.com/language/ref/WordFrequencyData.html)。利用这些数据，我们可以开始生成“句子”，其中每个单词是独立随机挑选的，出现的概率与其在语料库中的概率相同。以下是我们得到的一个样本：

![ChatGPT 在做什么……以及为什么它有效？](../Images/7aed2b73c3c1de01bb7fad0096ba95ab.png)

不出所料，这是荒谬的。那么我们该如何做得更好？就像处理字母一样，我们可以开始考虑不仅是单词的概率，还有单词对或更长的*n*-gram 的概率。对于对，这里有 5 个示例，所有案例都从“cat”这个词开始：

![ChatGPT 在做什么……以及为什么它有效？](../Images/96261d9e45f08b5d89f2b9676c62fa04.png)

它看起来略微“更有意义”。我们可以想象，如果我们能够使用足够长的*n*-gram，我们基本上会“得到一个 ChatGPT”——即我们会得到一种生成具有“正确整体论文概率”的长篇单词序列的东西。但问题是：甚至没有接近足够的英语文本来推断这些概率。

在[网络爬虫](https://commoncrawl.org/)中可能有几百亿个单词；在已数字化的书籍中可能还有另一百亿个单词。但有了 40,000 个常用单词，即使是可能的 2-gram 的数量也已达到 16 亿——而可能的 3-gram 的数量是 60 万亿。因此，我们无法仅凭现有文本估计所有这些的概率。当我们处理“20 个单词的论文片段”时，可能性数量已经比宇宙中的粒子还多，从某种意义上说，它们永远不可能全部被写下来。

那我们可以做什么？大思路是制作一个模型，让我们估计序列应该出现的概率——即使我们从未在我们查看的文本语料库中明确见过这些序列。ChatGPT 的核心正是一个所谓的“大型语言模型”（LLM），它被构建用来很好地估计这些概率。

# 什么是模型？

比如你想知道（如[伽利略在 1500 年代末所做的](https://archive.org/details/bub_gb_49d42xp-USMC/page/404/mode/2up)）从比萨斜塔的每个楼层掉下的大炮弹需要多长时间才能落地。你可以在每种情况下进行测量，并制作一个结果表。或者你可以做理论科学的本质：建立一个模型，提供一些计算答案的程序，而不仅仅是测量和记住每种情况。

假设我们有（稍微理想化的）数据，表示大炮弹从不同楼层落下所需的时间：

![ChatGPT 在做什么 …… 为什么它有效？](../Images/fadb9d99539bb5a4efc1e8f053dbd7b2.png)

我们如何确定从一个没有明确数据的楼层落下需要多长时间？在这个特定情况下，我们可以使用已知的物理定律来计算。但假设我们只有数据，而不知道支配这些数据的基本定律。然后我们可能会做出数学猜测，比如我们应该使用直线作为模型：

![ChatGPT 在做什么 …… 为什么它有效？](../Images/80849ff233d06906ca43e332eb9043d6.png)

我们可以选择不同的直线。但这是与我们给定的数据平均最接近的一条直线。通过这条直线，我们可以估计任何楼层的落下时间。

我们是如何知道在这里尝试使用直线的？在某种程度上我们并不知道。这只是一些数学上简单的东西，我们习惯于许多测量的数据会被数学上简单的东西很好地拟合。我们可以尝试更复杂的数学模型——比如 *a* + *b* *x* + *c* *x*²——这样在这种情况下，我们会做得更好：

![ChatGPT 在做什么 …… 为什么它有效？](../Images/040642c53aef28d034c7fc8f6daed728.png)

但事情可能会搞得很糟糕。比如这里是我们用 *a* + *b*/*x* + *c* sin(*x*) 所能做到的[最好结果](https://reference.wolfram.com/language/ref/FindFit.html)：

![ChatGPT 在做什么 …… 为什么它有效？](../Images/c785045d470c2661d0c21894bb0f62c9.png)

需要理解的是，从来没有“没有模型的模型”。你使用的任何模型都有某种特定的基本结构——然后是一组“可以调整的旋钮”（即可以设置的参数）来拟合你的数据。在 ChatGPT 的情况下，使用了很多这样的“旋钮”——实际上是 1750 亿个。

但值得注意的是，ChatGPT 的基本结构——即使是“仅仅”这么多参数——足以使模型计算下一个单词的概率“足够好”，以生成合理的长篇文本。

# 类似人类任务的模型

我们上面给出的例子涉及为数值数据创建一个模型，这些数据基本上来自简单物理学——我们已经知道几个世纪了“简单数学适用”。但对于 ChatGPT，我们需要创建一个人类大脑产生的人类语言文本的模型。对于这样的东西，我们目前（至少还）没有类似“简单数学”的东西。那么这样的模型可能是什么样的呢？

在讨论语言之前，让我们讨论另一个类似人类的任务：识别图像。作为一个简单的例子，让我们考虑数字图像（是的，这这是一个 [经典的机器学习例子](https://resources.wolframcloud.com/NeuralNetRepository/resources/050b1a0a-f43a-4c28-b7e0-72607a918467/)）：

![ChatGPT 在做什么……以及为什么它有效？](../Images/aadbf8369c6c49fd89d0ddc5ec69607d.png)

我们可以做的一件事是为每个数字获取一组样本图像：

![ChatGPT 在做什么……以及为什么它有效？](../Images/bed352ddbc537362a96431ee723f3f11.png)

然后，为了找出我们输入的图像是否对应于某个特定的数字，我们可以与我们拥有的样本进行逐像素比较。但作为人类，我们显然能做得更好——因为即使这些数字例如是手写的，并且有各种修改和扭曲，我们仍然能识别出来：

![ChatGPT 在做什么……以及为什么它有效？](../Images/be796159b61ef630d3c35b906c792dc3.png)

当我们为上述数值数据创建模型时，我们能够接受一个数值 *x*，然后计算 *a + b x* 对于特定的 *a* 和 *b*。所以，如果我们将这里每个像素的灰度值视为某个变量 *x[i]*，是否存在一个所有这些变量的函数——当被评估时——能告诉我们图像的数字是什么？事实证明，构造这样的函数是可能的。不过，这并不令人惊讶，它并不是特别简单。一个典型的例子可能涉及到大约五十万次数学运算。

但最终结果是，如果我们将图像的像素值集合输入到这个函数中，输出的将是指定我们图像中的数字的编号。稍后，我们将探讨如何构造这样的函数以及神经网络的概念。但现在我们先将函数视为一个黑箱，我们输入，例如手写数字的图像（作为像素值数组），然后输出这些图像所对应的数字：

![ChatGPT 在做什么……以及为什么它有效？](../Images/22f48528d1c298a84d7e4672448f5725.png)

但这里到底发生了什么？假设我们逐渐模糊一个数字。在一段时间内，我们的函数仍然“识别”它，此时为“2”。但很快它就“失效”了，开始给出“错误”的结果：

![ChatGPT 在做什么……以及为什么它有效？](../Images/b5e80068d393cb74d88e87184419980e.png)

但为什么我们说这是“错误”的结果呢？在这种情况下，我们知道我们是通过模糊“2”的图像得到的所有图像。但如果我们的目标是制作一个能模拟人类识别图像的模型，真正的问题是如果人类面对这些模糊图像之一而不知道其来源时会做什么。

我们有一个“好的模型”是指如果我们从函数中得到的结果通常与人类的判断一致。而且有一个非平凡的科学事实是，对于像这样的图像识别任务，我们现在基本上知道如何构建执行这些任务的函数。

我们能“数学证明”它们有效吗？其实不能。因为要做到这一点，我们需要对我们人类的行为有一个数学理论。拿“2”的图像来改变几个像素。我们可能会想，如果只有几个像素“错位”，我们仍然应该把这个图像视为“2”。但这种错位的范围应该有多大呢？这是一个关于[人类视觉感知](https://www.wolframscience.com/nks/chap-10--processes-of-perception-and-analysis#sect-10-7--visual-perception)的问题。没错，答案对于蜜蜂或章鱼无疑会有所不同——并且对于假设的外星生物来说可能会[完全不同](https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad/#alien-views-of-the-ruliad)。

# 神经网络

好的，那么我们常见的任务模型比如[图像识别](https://writings.stephenwolfram.com/2015/05/wolfram-language-artificial-intelligence-the-image-identification-project/)到底是如何工作的呢？目前最流行—也是最成功的—方法是使用[神经网络](https://reference.wolfram.com/language/guide/NeuralNetworks.html)。神经网络在1940年代就以与今天相似的形式被发明，[接近今天的使用形式](https://www.wolframscience.com/nks/notes-10-12--history-of-ideas-about-thinking/)，可以被视为对[大脑工作方式的简单理想化](https://www.wolframscience.com/nks/notes-10-12--the-brain/)。

在人类的大脑中大约有1000亿个神经元（神经细胞），每个神经元能够产生每秒高达千次的电脉冲。神经元以复杂的网络形式连接在一起，每个神经元拥有树状分支，使其能够将电信号传递给数千个其他神经元。在粗略的估计中，任何特定的神经元在某一时刻是否产生电脉冲，取决于它从其他神经元接收到的脉冲——不同的连接以不同的“权重”贡献。

当我们“看到图像”时，发生的情况是图像中的光子落在我们眼睛后面的（“光感受器”）细胞上，这些细胞会在神经细胞中产生电信号。这些神经细胞连接到其他神经细胞，最终信号通过一系列神经元层。在这个过程中我们“识别”图像，最终“形成”我们正在“看到一个2”的“思想”（最终可能会做出类似说出“two”这个词的行为）。

上一节中的“黑箱”函数是这样一个神经网络的“数学化”版本。它恰好有 11 层（尽管只有 4 层“核心层”）：

![ChatGPT 在做什么……以及为什么它有效？](../Images/a389bf50e95c69fbbe6d2fd6e04a8b78.png)

这个神经网络没有什么特别的“理论推导”；它只是某种在[1998 年构建的工程](https://resources.wolframcloud.com/NeuralNetRepository/resources/LeNet-Trained-on-MNIST-Data/)的一部分，并发现它有效。（当然，这与我们可能描述我们的大脑是通过生物进化过程产生的差别不大。）

好吧，那么这样的神经网络是如何“识别事物”的？关键在于[吸引子](https://www.wolframscience.com/nks/chap-6--starting-from-randomness#sect-6-7--the-notion-of-attractors)的概念。假设我们有 1 和 2 的手写图像：

![ChatGPT 在做什么……以及为什么它有效？](../Images/606ae39478e636e568156b724156abd9.png)

我们希望所有的 1 都“被吸引到一个地方”，所有的 2 都“被吸引到另一个地方”。换句话说，如果一张图像某种程度上是“[更接近 1](https://www.wolframscience.com/nks/notes-10-12--memory-analogs-with-numerical-data/)”而不是 2，我们希望它最终出现在“1 的地方”，反之亦然。

作为一个直接的类比，假设我们在平面上有一些由点表示的位置（在现实生活中，它们可能是咖啡店的位置）。然后我们可以想象，从平面上的任何一点开始，我们总是希望到达最近的点（即，我们总是去最近的咖啡店）。我们可以通过将平面划分为由理想化的“集水区”分隔的区域（“吸引子盆地”）来表示这一点：

![ChatGPT 在做什么……以及为什么它有效？](../Images/f85154147f9a0f46f796feceed09efb8.png)

我们可以把这看作是实施一种“识别任务”，其中我们不是做类似于识别给定图像“最像哪个数字”的事情——而是我们只是直接查看给定点最接近哪个点。（我们在这里展示的“Voronoi 图”设置将二维欧几里得空间中的点分开；数字识别任务可以被认为是做类似的事情——但在一个由每个图像的所有像素灰度级组成的 784 维空间中。）

那么我们如何让神经网络“进行识别任务”？让我们考虑这个非常简单的案例：

![ChatGPT 在做什么……以及为什么它有效？](../Images/9c9c0a3ae63add58dfd8e024f90aa1ea.png)

我们的目标是接受一个对应于位置 {*x*,*y*} 的“输入”——然后将其“识别”为它最接近的三个点中的任意一个。换句话说，我们希望神经网络计算一个 {*x*,*y*} 的函数，如：

![ChatGPT 在做什么……以及为什么它有效？](../Images/47603118013344a2ad7dea9fd994e160.png)

那么我们如何用神经网络实现这一点呢？最终，神经网络是一个理想化的“神经元”集合—通常以层的形式排列—一个简单的例子是：

![ChatGPT 在做什么 … 为什么它能有效工作？](../Images/c6838ede31b4480c30248b8a9b0f4d27.png)

每个“神经元”实际上被设置为评估一个简单的数值函数。要“使用”这个网络，我们只需在顶部输入数字（如我们的坐标 *x* 和 *y*），然后让每一层的神经元“评估它们的函数”并将结果向前传递，通过网络—最终在底部产生最终结果：

![ChatGPT 在做什么 … 为什么它能有效工作？](../Images/0af36f6927be2f5f23582bc1b6df7f2f.png)

在传统的（生物启发的）设置中，每个神经元实际上具有一组来自前一层神经元的“输入连接”，每个连接被分配一个特定的“权重”（可以是正数或负数）。给定神经元的值通过将“前一层神经元”的值与其对应的权重相乘，然后将这些值相加并加上一个常数来确定—最后应用一个“阈值”（或“激活”）函数。用数学术语来说，如果一个神经元有输入 *x* = {*x*[1], *x*[2] …}，那么我们计算 *f*[*w* . *x* + *b*]，其中权重 *w* 和常数 *b* 通常对网络中的每个神经元选择不同；函数 *f* 通常是相同的。

计算 *w* . *x* + *b* 只是矩阵乘法和加法的问题。“激活函数” *f* 引入了非线性（并最终导致了非平凡行为）。各种激活函数通常被使用；这里我们将只使用 `*[Ramp](http://reference.wolfram.com/language/ref/Ramp.html)`（或 ReLU）：

![ChatGPT 在做什么 … 为什么它能有效工作？](../Images/921952f50d4bb8e342905709b1ef31ee.png)

对于我们希望神经网络执行的每个任务（或者等效地，对于我们希望它评估的每个整体函数），我们将有不同的权重选择。（并且—正如我们稍后讨论的—这些权重通常通过使用我们期望输出的示例来“训练”神经网络来确定。）

最终，每个神经网络只对应某个整体数学函数—尽管写出来可能很麻烦。对于上面的例子，它会是：

![ChatGPT 在做什么 … 为什么它能有效工作？](../Images/0913a4e2f7773cccc3923479b19e123d.png)

ChatGPT 的神经网络也只是对应于这样一个数学函数—但实际上具有数十亿项。

但让我们回到单个神经元。以下是一个具有两个输入（表示坐标 *x* 和 *y*）的神经元在不同权重和常数选择下可以计算的函数的一些示例（以及 `*[Ramp](https://reference.wolfram.com/language/ref/Ramp.html)` 作为激活函数）：

![ChatGPT 在做什么 … 为什么它能有效工作？](../Images/cfdb414f02f66c4a89cf761e94a02104.png)

那么，前面提到的更大网络呢？好吧，这是它计算的结果：

![ChatGPT在做什么...以及它为什么有效？](../Images/7db4b6788b968aaab311059718f189dc.png)

这并不完全“正确”，但接近于我们上面展示的“最近点”函数。

让我们看看其他一些神经网络会发生什么。在每种情况下，正如我们稍后会解释的，我们使用机器学习来寻找最佳权重选择。然后我们在这里展示了具有这些权重的神经网络计算结果：

![ChatGPT在做什么...以及它为什么有效？](../Images/92fe2eab80e186279babfacccc24746e.png)

更大的网络通常更擅长于逼近我们所期望的函数。在“每个吸引子盆地的中间”，我们通常会得到我们想要的确切答案。但[在边界处](https://www.wolframscience.com/nks/notes-10-12--memory-analogs-with-numerical-data/)——那里神经网络“很难做出决定”——情况可能会更加混乱。

对于这个简单的数学风格的“识别任务”，什么是“正确答案”很清楚。但在识别手写数字的问题中，这就不那么清楚了。如果有人把“2”写得像“7”一样糟糕，怎么办？尽管如此，我们还是可以询问神经网络如何区分数字——这会给出一些指示：

![ChatGPT在做什么...以及它为什么有效？](../Images/b421e4f25461ca98deb3f95d14ba1172.png)

我们能否“数学上”说明网络如何进行区分？不完全能。它只是“做神经网络所做的事”。但结果表明，这通常与我们人类做出的区分非常一致。

让我们举一个更复杂的例子。假设我们有猫和狗的图像。我们有一个[经过训练以区分它们的神经网络](https://writings.stephenwolfram.com/2015/05/wolfram-language-artificial-intelligence-the-image-identification-project/)。它在一些示例中可能会这样做：

![ChatGPT在做什么...以及它为什么有效？](../Images/8a48b2a02f545a59fa531e867edd0c3f.png)

现在，什么是“正确答案”变得更加模糊了。比如一只穿着猫装的狗？等等。无论给它什么输入，神经网络都会生成一个答案，并且在某种程度上与人类可能的方式相一致。正如我上面所说的，这不是我们可以“从第一原理推导出来的事实”。这只是经过经验发现的真实情况，至少在某些领域如此。但这也是神经网络有用的关键原因之一：它们以某种方式捕捉了“类人”的做事方式。

给自己看一张猫的图片，然后问“为什么那是一只猫？”。也许你会开始说“嗯，我看到了它的尖耳朵”等等。但很难解释你是如何识别出这是一只猫的。只是你的大脑以某种方式弄明白了。但对于大脑来说，当前还没有办法（至少还没有）“进入内部”查看它是如何搞明白的。那对于（人工）神经网络呢？好吧，当你展示一张猫的图片时，查看每个“神经元”在做什么是直接的。但即使是获得基本的可视化通常也很困难。

在我们用于上述“最近点”问题的最终网络中有17个神经元。在识别手写数字的网络中有2190个。在我们用于识别猫和狗的网络中有60,650个。通常，直观地理解60,650维的空间是相当困难的。但由于这是一个处理图像的网络，它的许多神经网络层被组织成数组，就像它查看的像素数组一样。

如果我们拿一张典型的猫的图像

![ChatGPT 在做什么……以及为什么有效？](../Images/48debc63638967d033e74445202cd209.png)

那么我们可以通过一系列衍生图像来表示第一层的神经元状态——其中许多我们可以很容易地解释为“没有背景的猫”或“猫的轮廓”等：

![ChatGPT 在做什么……以及为什么有效？](../Images/079c74a49df21a11196656f036408ee7.png)

到第十层时，解释发生了什么变得更加困难：

![ChatGPT 在做什么……以及为什么有效？](../Images/fec7745128553f8413a3d72001e6ef8f.png)

但一般来说，我们可以说神经网络在“提取某些特征”（也许尖耳朵在其中），并利用这些特征来确定图像的内容。但这些特征是否是我们能给出名称的——如“尖耳朵”？大多数情况下不是。

我们的大脑是否使用了类似的特征？大多数情况下我们并不知晓。但值得注意的是，我们在这里展示的神经网络的前几层似乎能提取图像中的某些方面（如物体的边缘），这些特征似乎与我们知道大脑视觉处理的第一层提取的特征类似。

但假设我们想要一个关于神经网络“猫识别”的“理论”。我们可以说：“看，这个特定的网络可以做到这一点”——这立即给了我们一些“问题有多难”的感觉（例如，需要多少神经元或层）。但至少现在我们还没有办法“给出叙述性描述”网络正在做什么。也许是因为它确实在计算上是不可简化的，除了通过明确追踪每一步之外，没有通用的方法来了解它的功能。或者可能是我们还没有“弄清楚科学”，也没有识别出“自然法则”，以便总结正在发生的事情。

当我们谈论使用 ChatGPT 生成语言时，也会遇到类似的问题。而且同样不清楚是否有方法来“总结它在做什么”。但语言的丰富性和细节（以及我们对其的经验）可能使我们能够比图像更进一步。

# 机器学习和神经网络的训练

到目前为止，我们讨论了“已经知道”如何执行特定任务的神经网络。但神经网络如此有用的原因（也许大脑也是如此）是，它们不仅原则上可以执行各种任务，而且可以通过逐步“从例子中训练”来完成这些任务。

当我们制作一个神经网络来区分猫和狗时，我们不需要编写一个程序（比如）明确地找到胡须；相反，我们只需展示大量猫和狗的示例，然后让网络“机器学习”如何区分它们。

关键点在于训练后的网络从其展示的特定例子中“泛化”。正如我们上面所见，网络不仅仅是识别了它所展示的猫图像的特定像素模式，而是神经网络以某种我们认为的“通用猫性”来区分图像。

那么神经网络训练实际上是如何工作的呢？本质上，我们始终尝试找到使神经网络成功重现我们给出的示例的权重。然后，我们依赖神经网络以“合理”的方式在这些示例之间“插值”（或“泛化”）。

让我们看一个比上面最近点问题更简单的问题。让我们尝试让神经网络学习以下函数：

![ChatGPT 在做什么...以及它为什么有效？](../Images/84503a9852ed9faf0efa04d8e95c9f40.png)

对于这个任务，我们需要一个只有一个输入和一个输出的网络，如下所示：

![ChatGPT 在做什么...以及它为什么有效？](../Images/3eff9d5b04d8c0a9a7d40c1d3edefbb5.png)

那么我们应该使用什么权重等呢？对于每一组可能的权重，神经网络将计算某个函数。例如，以下是它对一些随机选择的权重集的处理：

![ChatGPT 在做什么...以及它为什么有效？](../Images/160d886acc18ab092acf72859c181396.png)

是的，我们可以清楚地看到在这些情况下，网络都未能接近重现我们想要的函数。那么我们如何找到能够重现该函数的权重呢？

基本思想是提供大量“输入 ? 输出”示例以“学习”，然后尝试找到能够重现这些示例的权重。以下是随着示例逐渐增多而得到的结果：

![ChatGPT 在做什么...以及它为什么有效？](../Images/1ee7e31b826edc7cfa95a25af4e6d3d4.png)

在这种“训练”的每个阶段，网络中的权重都会逐步调整——我们看到最终得到一个成功重现我们想要的函数的网络。那么我们如何调整权重呢？基本的想法是在每个阶段查看“我们离得到我们想要的函数有多远”——然后以这种方式更新权重，以便更接近。

为了找出“我们还差多远”，我们计算通常称为“损失函数”（有时称为“成本函数”）的东西。在这里，我们使用一个简单的（L2）损失函数，它只是我们得到的值与真实值之间差异的平方和。我们看到的是，随着训练过程的进展，损失函数逐渐减少（遵循不同任务的特定“学习曲线”）——直到我们达到一个点，在这个点上网络（至少在很大程度上）成功地重现了我们想要的函数：

![What Is ChatGPT Doing … and Why Does It Work?](../Images/7344ea079de84e03e13cf8617caaca45.png)

好的，最后一个关键部分是如何调整权重以减少损失函数。如我们所说，损失函数给出的是我们得到的值与真实值之间的“距离”。但“我们得到的值”是在每个阶段由当前版本的神经网络及其中的权重决定的。现在假设这些权重是变量——比如说 *w[i]*。我们希望找出如何调整这些变量的值，以最小化依赖于它们的损失。

例如，假设（在对实际中使用的典型神经网络进行极简化的情况下）我们只有两个权重 *w*[1] 和 *w*[2]。那么我们可能会有一个损失函数，它作为 *w*[1] 和 *w*[2] 的函数看起来像这样：

![What Is ChatGPT Doing … and Why Does It Work?](../Images/bfc3ff42bf6424919675bfac3617deb7.png)

数值分析提供了多种技术来寻找这种情况的最小值。但一种典型的方法是逐步沿着从我们之前的 *w*[1] 和 *w*[2] 开始的最陡下降路径前进：

![What Is ChatGPT Doing … and Why Does It Work?](../Images/cb6641f8354a2e4177b4f87bc9416d51.png)

就像水流下山一样，唯一可以保证的是这个过程将最终到达表面的某个局部最小值（“一个山间湖泊”）；它可能不会到达最终的全局最小值。

在“权重景观”上找到最陡下降路径并不明显是可行的。但微积分提供了帮助。正如我们之前提到的，可以将神经网络始终看作是计算一个依赖于其输入和权重的数学函数。但现在考虑对这些权重进行微分。事实证明，微积分的链式法则实际上使我们可以“解开”神经网络中各层的操作。而结果是，我们可以——至少在某种局部近似下——“逆转”神经网络的操作，并逐步找到能最小化与输出相关联的损失的权重。

上面的图片展示了我们可能需要在仅有2个权重的非现实简单情况下进行的最小化。但是即使有更多的权重（ChatGPT 使用1750亿个），依然可以进行最小化，至少在某种程度的近似下是可能的。事实上，大约在2011年左右发生的“深度学习”重大突破与发现相关：在某种意义上，当涉及到许多权重时，进行（至少是近似的）最小化可能比涉及较少权重时更容易。

换句话说——有些违反直觉的是——用神经网络解决更复杂的问题可能比解决简单问题更容易。其大致原因似乎是，当存在大量“权重变量”时，会有一个高维空间，其中有“许多不同的方向”可以引导我们到达最小值——而对于较少的变量，更容易陷入局部最小值（“山间湖泊”），没有“方向可以脱离”。

值得指出的是，在典型情况下，有许多不同的权重组合都能使神经网络表现出几乎相同的性能。而在实际的神经网络训练中，通常会做出许多随机选择——这会导致“不同但等效的解决方案”，比如这些：

![ChatGPT 在做什么……以及它为何有效？](../Images/67f955d878ffa4dfcd4e1080b2616da8.png)

但每个这样的“不同解决方案”将至少有略微不同的行为。如果我们要求进行例如在给定训练样本区域之外的“外推”，我们可能会得到截然不同的结果：

![ChatGPT 在做什么……以及它为何有效？](../Images/081f18341af1ac29009f6cb8e5f0c62b.png)

那么这些结果中哪一个是“正确”的？实际上没有办法确定。它们都与“观察数据”一致。但它们都对应于不同的“固有”方式来“考虑”在“框外”该做什么。有些可能对我们人类来说显得“更合理”。

# 神经网络训练的实践与经验

尤其是在过去十年中，神经网络训练的艺术取得了许多进展。而且，是的，这基本上是一门艺术。有时——尤其是事后看来——我们可以看到至少一点“科学解释”的闪光点。但大多数情况是通过反复试验发现的，加入了逐渐积累的想法和技巧，从而构建了关于如何使用神经网络的丰富经验。

有几个关键部分。首先，需要考虑在特定任务中应使用什么样的神经网络架构。接着，关键问题是如何获取用于训练神经网络的数据。而且，越来越多的时候，人们不再从头开始训练一个网络：新的网络可以直接融入另一个已经训练好的网络，或者至少可以利用那个网络生成更多的训练示例。

有人可能认为每种特定类型的任务都需要不同的神经网络架构。但发现相同的架构往往对看似完全不同的任务也有效。在某种程度上，这让人想起了[通用计算的理念](https://www.wolframscience.com/nks/chap-11--the-notion-of-computation#sect-11-3--the-phenomenon-of-universality)（以及我的[计算等效原则](https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence/)），但正如我稍后讨论的，我认为这更多反映了我们通常尝试让神经网络完成的任务是“类人”的，而神经网络可以捕捉到相当普遍的“类人过程”。

在早期的神经网络研究中，通常有一种观念是“让神经网络尽可能少做事情”。例如，在[语音转文本](https://reference.wolfram.com/language/ref/SpeechRecognize.html)的过程中，人们认为应该首先分析语音的音频，将其分解为音素等。但是，事实证明——至少对于“类人任务”——通常更好的是直接在“端到端问题”上训练神经网络，让它“发现”必要的中间特征、编码等。

也曾有人认为应该将复杂的个体组件引入神经网络中，以便使其“明确实现特定的算法思想”。但再一次，这通常被证明并不值得；相反，更好的做法是处理非常简单的组件，让它们“自我组织”（尽管通常以我们无法理解的方式）来实现（推测上）那些算法思想的等效结果。

这并不是说没有与神经网络相关的“结构化想法”。例如，拥有[具有局部连接的 2D 神经元数组](https://reference.wolfram.com/language/ref/ConvolutionLayer.html)在图像处理的早期阶段似乎非常有用。而拥有专注于“回顾序列”的连接模式似乎也很有用——正如我们稍后将看到的——在处理类似人类语言的任务时，例如在 ChatGPT 中。

但神经网络的一个重要特性是——像计算机一样——它们最终只处理数据。而当前的神经网络——通过当前的神经网络训练方法——[特别处理数字数组](https://reference.wolfram.com/language/guide/NetEncoderDecoder.html)。但在处理过程中，这些数组可以被完全重新排列和重塑。举个例子，[我们用于识别数字的网络](https://resources.wolframcloud.com/NeuralNetRepository/resources/LeNet-Trained-on-MNIST-Data/) 从一个 2D “图像般”的数组开始，很快“变厚”成多个通道，但随后[“浓缩”成一个 1D 数组](https://reference.wolfram.com/language/ref/AggregationLayer.html)，最终包含表示不同可能输出数字的元素。

![ChatGPT 在做什么 … 以及为什么它有效？](../Images/30a02d69fb132e7f6dd982e924bd9029.png)

不过，好吧，怎么判断一个神经网络在特定任务中需要多大呢？这有点像艺术。在某种程度上，关键是知道“任务有多难”。但对于类似人类的任务，这通常很难估计。是的，可能有一种系统化的方式，通过计算机非常“机械”地完成任务。但很难知道是否存在所谓的技巧或捷径，可以使任务至少在“类似人类的水平”上变得容易得多。可能需要[列举一个巨大的游戏树](https://writings.stephenwolfram.com/2022/06/games-and-puzzles-as-multicomputational-systems/)才能“机械地”玩某个游戏；但可能存在一种更简单的（“启发式的”）方法来实现“人类水平的游戏”。

当处理微小的神经网络和简单任务时，有时可以明确看到“从这里无法到达那里”。例如，以下是似乎在前一节任务中用几个小型神经网络能做到的最佳效果：

![ChatGPT 在做什么 … 以及为什么它有效？](../Images/9c79fd5d16eafe3771d123feee336497.png)

我们看到的是，如果网络太小，它就无法复现我们想要的功能。但在某个大小以上，它没有问题——至少如果你为其训练足够长时间，并提供足够的示例。顺便提一下，这些图片展示了一个神经网络的知识点：如果中间有一个“挤压”部分，迫使所有内容通过一个较小的中间神经元数目，通常可以用一个较小的网络来应对。（也值得一提的是，“无中间层”——或称为“[感知机](https://en.wikipedia.org/wiki/Perceptron)”——网络只能学习本质上是线性的函数，但只要有一个中间层，就[原则上总是可能](https://en.wikipedia.org/wiki/Universal_approximation_theorem)近似任何函数，只要有足够的神经元，尽管为了使其可行地训练，通常会有某种[正则化或归一化](https://reference.wolfram.com/language/ref/BatchNormalizationLayer.html)。

好的，假设一个人已经决定使用某种神经网络架构。接下来就是获取训练网络所需数据的问题。许多关于神经网络——以及机器学习总体上的实际挑战——都集中在获取或准备必要的训练数据上。在许多情况下（“监督学习”），人们希望得到输入和期望输出的明确示例。因此，例如，一个人可能希望得到标记了内容的图像或其他属性。也许需要明确地进行标记——通常需要付出很大努力。但是，往往会发现可以利用已经完成的工作，或将其作为某种代理。例如，可以使用网络上为图像提供的 alt 标签。或者，在另一个领域中，可以使用为视频创建的字幕。又或者——对于语言翻译训练——可以使用存在于不同语言中的网页或其他文档的平行版本。

你需要向神经网络展示多少数据才能训练它完成特定任务？再次从基本原理估计很难。通过使用“迁移学习”将已在其他网络中学习到的重要特征列表“转移”过来，需求量可以大大减少。但通常神经网络需要“看到大量示例”才能有效训练。而且至少对于某些任务来说，神经网络的一个重要特性是示例可以非常重复。实际上，将所有的示例反复展示给神经网络是一种标准策略。在每一轮“训练”（或“时期”）中，神经网络将处于至少稍微不同的状态，并且以某种方式“提醒它”一个特定示例有助于它“记住那个示例”。（是的，也许这类似于人类记忆中重复的有效性。）

但通常，仅仅重复相同的示例并不够。还需要向神经网络展示示例的变化。神经网络的一个特性是这些“数据增强”变化不需要复杂就能有用。仅通过基本的图像处理稍微修改图像，就能使它们对神经网络训练来说“几乎全新”。同样，当用完实际视频等来训练自动驾驶汽车的数据时，可以继续从运行模型视频游戏环境中的模拟中获取数据，而无需实际世界场景的详细信息。

像ChatGPT这样的模型怎么样？好在它具有“无监督学习”的优点，使得获取训练示例变得容易。回想一下，ChatGPT的基本任务是弄清楚如何继续给定的文本。因此，要获得“训练示例”，只需获取一段文本，将其末尾遮蔽，然后将其作为“训练输入”——“输出”则是完整的、未遮蔽的文本。我们稍后会详细讨论，但要点是——与学习图像中的内容不同——不需要“显式标记”；ChatGPT实际上可以直接从所提供的文本示例中学习。

好的，那么神经网络中的实际学习过程如何呢？归根结底，这一切都是关于确定哪些权重能够最好地捕捉所给定的训练示例。这里有各种详细的选择和“超参数设置”（之所以这样称呼，是因为权重可以被视作“参数”），可以用来调整这些操作的方式。有不同的[损失函数选择](https://reference.wolfram.com/language/ref/CrossEntropyLossLayer.html)（平方和、绝对值和等）。有不同的损失最小化方法（每一步在权重空间中移动多远等）。然后还有类似于每次估计损失时应该展示多大的“批次”问题。是的，可以应用机器学习（例如我们在 Wolfram 语言中所做的）来自动化机器学习——并自动设置超参数等。

但最终，整个训练过程的特征是观察损失值如何逐渐减少（如在这个[Wolfram 语言小型训练进度监控器](https://reference.wolfram.com/language/ref/NetTrain.html)中所示）：

![ChatGPT 在做什么 … 以及为什么它有效？](../Images/64f8e0fcafd50ae49a95a13150a960bb.png)

通常情况下，所见的是损失值在一段时间内减少，但最终会在某个常数值上趋于平稳。如果这个值足够小，那么训练可以被视为成功；否则，这可能是一个信号，表明需要尝试改变网络架构。

能否确定“学习曲线”平稳所需的时间？就像许多其他事情一样，似乎存在近似的[幂律缩放关系](https://arxiv.org/pdf/2001.08361.pdf)，这些关系依赖于神经网络的规模和所使用的数据量。但总体结论是，训练神经网络很困难——需要大量计算资源。作为实际问题，大部分努力都花在对数字数组进行操作上，而这正是 GPU 擅长的——这也是神经网络训练通常受限于 GPU 可用性的原因。

未来是否会有根本更好的方法来训练神经网络——或一般地做神经网络所做的事情？我认为几乎可以肯定。神经网络的基本思想是用大量简单（本质上相同）组件创建一个灵活的“计算结构”——并使这个“结构”可以逐步修改以从示例中学习。在当前的神经网络中，基本上是使用微积分的思想——应用于实数——来进行这种逐步修改。但越来越明显的是，高精度数字并不重要；即使使用当前的方法，8 位或更少的精度也可能足够。

与[像细胞自动机这样的计算系统](https://www.wolframscience.com/nks/chap-2--the-crucial-experiment#sect-2-1--how-do-simple-programs-behave)基本上在许多独立的比特上并行操作一样，如何进行这种增量修改一直不清楚，[如何进行这种增量修改](https://content.wolfram.com/uploads/sites/34/2020/07/approaches-complexity-engineering.pdf)也没有理由认为它是不可能的。事实上，类似于“[2012年深度学习突破](https://en.wikipedia.org/wiki/AlexNet)”，这种增量修改在更复杂的情况下可能比在简单情况下更容易实现。

神经网络—或许有点像大脑—设置为具有基本固定的神经元网络，修改的是它们之间连接的强度（“权重”）。（或许在至少年轻的大脑中，可能还会有大量全新连接的增长。）但尽管这对生物学来说可能是一个方便的设置，但目前还不清楚这是否接近实现我们所需功能的最佳方式。涉及渐进网络重写的事物（或许类似于我们的[物理项目](https://www.wolframphysics.org/)）最终可能会更好。

但即使在现有的神经网络框架内，目前也存在一个关键限制：目前的神经网络训练基本上是顺序的，每批样本的效果会被反向传播以更新权重。事实上，即便考虑到GPU，目前的大多数神经网络在训练过程中大部分时间都是“闲置”的，只有一部分在更新。从某种意义上说，这是因为我们目前的计算机往往有与CPU（或GPU）分开的内存。但在大脑中，这显然是不同的——每个“记忆元素”（即神经元）也可能是一个活动的计算元素。如果我们能将未来的计算机硬件设置成这种方式，可能会使训练变得更加高效。

# “肯定一个足够大的网络可以做任何事情！”

像ChatGPT这样的能力看起来如此令人印象深刻，以至于有人可能会想，如果能“持续进行”并训练越来越大的神经网络，那么它们最终能够“做一切”。如果关注的是那些容易被立即人类思维所接触到的事物，这可能确实是这样。但过去几百年的科学教训是，有些东西可以通过形式化过程来解决，但不容易被立即人类思维所接触。

非平凡的数学就是一个很好的例子。但一般来说，这实际上是计算的问题。最终的问题是 [计算不可简约性](https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence#sect-12-6--computational-irreducibility)现象。有些计算你可能认为需要许多步骤才能完成，但实际上可以“简化”成相当直接的东西。但计算不可简约性的发现意味着这并不总是有效。相反，有些过程——可能像下面这个——在计算每一步时都必须追踪。

![ChatGPT 在做什么……以及为什么它有效？](../Images/9dacf5e1226a8558506d90ecbc434358.png)

我们通常用大脑做的事情大概是特别选择以避免计算不可简约性的。要在大脑中进行数学运算需要特别的努力。实际上，仅凭大脑几乎不可能“思考”任何非平凡程序的操作步骤。

但当然我们有计算机。借助计算机，我们可以轻松地完成长时间、计算不可简约的任务。关键点在于，对于这些任务一般没有捷径。

是的，我们可以记住许多特定计算系统中发生的具体例子。也许我们甚至可以看到一些（“计算可简约”）的模式，这些模式可以让我们进行一些泛化。但关键是计算不可简约性意味着我们永远无法保证意外不会发生——只有通过明确的计算才能知道在任何特定情况下实际发生了什么。

最终，学习能力与计算不可简约性之间存在根本性的紧张关系。学习实际上涉及到通过利用规律性来 [压缩数据](https://www.wolframscience.com/nks/chap-10--processes-of-perception-and-analysis/)。但计算不可简约性意味着最终可能存在规律性的限制。

实际上，我们可以设想将小型计算设备——如细胞自动机或图灵机——构建到可训练系统如神经网络中。的确，这些设备可以作为神经网络的良好“工具”——就像 [Wolfram|Alpha 可以作为 ChatGPT 的一个好工具](https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/)。但计算不可简约性意味着你不能期望“进入”这些设备并让它们进行学习。

换句话说，能力和可训练性之间存在一种**最终的权衡**：你希望系统更好地“实际利用”其计算能力，它就会表现出更强的计算不可约性，训练起来也会更加困难。而系统本质上越可训练，它能进行复杂计算的能力就越低。

（以当前的ChatGPT为例，情况实际上更加极端，因为生成每个输出的神经网络是纯粹的“前馈”网络，没有循环，因此无法进行任何具有非平凡“控制流”的计算。）

当然，人们可能会想知道进行不可约计算是否真的重要。确实，在人类历史的大部分时间里，这并不特别重要。但我们现代的技术世界是建立在至少数学计算的工程基础上——并且越来越多地是更一般的计算。如果我们观察自然界，它是[充满了不可约计算](https://www.wolframscience.com/nks/chap-8--implications-for-everyday-systems/)——我们正在慢慢理解如何模拟这些计算并将其用于我们的技术目的。

是的，神经网络确实能够注意到自然界中的某些规律，这些规律我们也可以用“未经帮助的人类思维”轻易发现。但如果我们想解决数学或计算科学领域的问题，神经网络就无法做到——除非它有效地“将普通的”计算系统作为工具。

但所有这些可能存在令人困惑的地方。过去有很多任务——包括写作——我们曾假设这些任务对计算机来说“根本太难”。现在看到ChatGPT等完成这些任务，我们会突然认为计算机变得强大得多——特别是超越了它们本来已经能做的事情（如逐步计算像细胞自动机这样的计算系统的行为）。

但这不是正确的结论。计算不可约的过程仍然是计算不可约的，对计算机来说仍然是根本困难的——即使计算机可以轻松计算它们的单个步骤。我们应该得出的结论是，像写作这样的任务——我们人类能做，但我们认为计算机做不到的——实际上在某种意义上比我们想象的计算上更容易。

换句话说，神经网络能够成功写作的原因是写作被证明是一个比我们想象的“计算上浅层”的问题。从某种意义上说，这使我们更接近于“拥有一种理论”来解释我们人类如何完成像写作这样的任务，或一般性地处理语言。

如果你拥有一个足够大的神经网络，那么，是的，你可能能够做任何人类能够做的事情。但你不会捕捉到自然界通常能做的事情——或者我们从自然界中打造的工具能做的事情。正是这些工具——无论是实用的还是概念性的——使我们在最近几个世纪能够超越“纯粹无辅助的人类思维”所能接触的边界，并为人类目的捕捉到更多物理和计算宇宙中的内容。

# 嵌入的概念

神经网络——至少按照它们目前的设置——从根本上是基于数字的。因此，如果我们要使用它们处理像文本这样的内容，我们需要一种[用数字表示文本的方法](https://reference.wolfram.com/language/guide/NetEncoderDecoder.html)。当然，我们可以从（本质上就像ChatGPT做的那样）给字典中的每个单词分配一个数字开始。但有一个重要的概念——例如对ChatGPT来说至关重要——超越了这一点。那就是“嵌入”的概念。可以将嵌入视为通过一组数字来尝试表示某物的“本质”的一种方式——其特点是“相近的事物”由相近的数字表示。

例如，我们可以将词嵌入视为尝试[在一种“意义空间”中布置单词](https://reference.wolfram.com/language/ref/FeatureSpacePlot.html)，其中在意义上“接近”的单词在嵌入中会彼此接近。实际使用的嵌入——比如在ChatGPT中——往往涉及大量的数字列表。但如果我们将其投影到二维，我们可以展示嵌入如何布置单词的示例：

![ChatGPT在做什么……以及它为何有效？](../Images/19ba90e45db7a9f6316acd975f81b6a5.png)

是的，我们看到的确实非常好地捕捉了典型的日常印象。但我们如何构建这样的嵌入呢？大致的想法是查看大量的文本（这里是来自网络的50亿个单词），然后看看不同单词出现的“环境”有多么相似。例如，“鳄鱼”和“短吻鳄”在其他相似的句子中几乎可以互换出现，这意味着它们在嵌入中会被放置得很接近。但“萝卜”和“鹰”不会在其他相似的句子中出现，因此它们在嵌入中会被放置得很远。

那么，如何使用神经网络实际实现这样的东西呢？让我们从谈论图像的嵌入开始。我们想找到一种方法，通过数字列表来表征图像，以便“我们认为相似的图像”被分配相似的数字列表。

我们如何判断是否应该“将图像视为相似”？如果我们的图像是手写数字，我们可能会“认为两个图像相似”如果它们是相同的数字。早些时候，我们讨论了一个训练用于识别手写数字的神经网络。我们可以将这个神经网络看作是在最终输出时将图像分到 10 个不同的类别中，每个数字一个类别。

但如果我们在做出最终的“这是‘4’”决定之前“截取”神经网络内部发生的事情呢？我们可能会期望神经网络内部存在一些数字，这些数字将图像特征描述为“主要像 4 但有一点像 2”或类似的东西。其思路是提取这些数字，用作嵌入的元素。

这里的概念是。与其直接尝试表征“什么图像接近什么其他图像”，我们不如考虑一个明确的任务（在这种情况下是数字识别），对于这个任务我们可以获取明确的训练数据——然后利用在执行这个任务时神经网络隐式地做出类似“接近度决策”的事实。因此，我们不必明确讨论“图像的接近度”，我们只需讨论图像表示的具体数字，然后“让神经网络”隐式地确定这对“图像接近度”意味着什么。

那么，这对数字识别网络的详细工作原理是什么呢？我们可以将网络视为由 11 层连续层组成，我们可以像这样用图标来总结（激活函数显示为单独的层）：

![ChatGPT 在做什么 …… 它为什么有效？](../Images/155e255d5396f59e4dfbacd7e7827907.png)

一开始，我们将实际图像输入到第一层，这些图像由二维像素值数组表示。到最后一层时，我们得到一个包含 10 个值的数组，我们可以理解为“网络对图像对应于数字 0 到 9 的每一个数字的确定程度”。

输入图像 ![ChatGPT 在做什么 …… 它为什么有效？](../Images/0b7f4d12ba95197fa48689c4d6b84281.png)，最后一层神经元的值为：

![ChatGPT 在做什么 …… 它为什么有效？](../Images/f8b8dad703dc517751657ef39ef1de26.png)

换句话说，神经网络在这一点上对这张图像是 4“非常确定”——为了实际得到输出“4”，我们只需找出具有最大值的神经元的位置。

但如果我们往前看一步呢？网络中的最后一个操作是所谓的 [softmax](https://reference.wolfram.com/language/ref/SoftmaxLayer.html)，它试图“强制确定性”。但在应用之前，神经元的值是：

![ChatGPT 在做什么 …… 它为什么有效？](../Images/9641e2162f3b18ab9528305e9c1a3de8.png)

代表“4”的神经元仍然具有最高的数值。但其他神经元的数值中也包含信息。我们可以预期，这些数字列表在某种意义上可以用来表征图像的“本质”，从而提供一种我们可以用作嵌入的方法。例如，这里的每个“4”都有一个稍微不同的“签名”（或“特征嵌入”），与“8”有很大的不同：

![ChatGPT在做什么……以及它为何有效？](../Images/52cb7dcd2340bf29bbc3d1f0b181cf6c.png)

在这里，我们基本上使用10个数字来表征我们的图像。但通常最好使用更多的数字。例如，在我们的数字识别网络中，我们可以通过访问前一层得到一个500个数字的数组。这可能是一个合理的“图像嵌入”数组。

如果我们想对手写数字的“图像空间”进行明确的可视化，我们需要“降维”，实际上是将我们得到的500维向量投影到，例如，3D空间中：

![ChatGPT在做什么……以及它为何有效？](../Images/8ea7c9126783b5b895abea564b9a99ca.png)

我们刚刚讨论了如何基于通过确定图像是否（根据我们的训练集）对应于相同的手写数字来创建图像的表征（从而进行嵌入）。如果我们有一个训练集来识别每个图像属于5000种常见对象（如猫、狗、椅子等）中的哪一种，我们可以以类似的方式处理图像。通过这种方式，我们可以制作一个由我们识别的常见对象“锚定”的图像嵌入，然后根据神经网络的行为“在此基础上进行泛化”。关键是，只要这种行为与我们人类感知和解释图像的方式一致，这将最终成为一个“对我们来说正确”的嵌入，并且在进行“类似人类判断”的任务时实用。

好的，那么我们如何按照相同的方法为词语找到嵌入？关键是从一个我们可以轻松进行训练的词语任务开始。标准的这种任务是“词语预测”。假设我们给定了“the ___ cat”。根据大规模文本语料库（例如，网络上的文本内容），不同词汇填补空白的概率是多少？或者，给定“___ black ___”，不同“旁边词”的概率是多少？

我们如何为神经网络设置这个问题？最终我们必须用数字来表述一切。一种方法是为英语中大约50,000个常见单词中的每一个分配一个唯一的数字。因此，例如，“the”可能是914，而“cat”（前面有一个空格）可能是3542。（这些是GPT-2实际使用的数字。）因此，对于“the ___ cat”问题，我们的输入可能是{914, 3542}。输出应该是什么样的？它应该是一个包含50,000个左右数字的列表，这些数字有效地给出每个可能的“填空”单词的概率。再一次，为了找到嵌入，我们希望“拦截”神经网络的“内部”部分，正好在它“得出结论”之前——然后拿到那里的数字列表，我们可以将其视为“表征每个单词”。

好吧，那么这些特征是什么样的？在过去的10年里，已经开发了一系列不同的系统（[word2vec](https://resources.wolframcloud.com/NeuralNetRepository/resources/ConceptNet-Numberbatch-Word-Vectors-V17.06/)，[GloVe](https://resources.wolframcloud.com/NeuralNetRepository/search/?i=GloVe)，[BERT](https://resources.wolframcloud.com/NeuralNetRepository/search/?i=BERT)，[GPT](https://resources.wolframcloud.com/NeuralNetRepository/resources/GPT2-Transformer-Trained-on-WebText-Data/)，……），每种系统基于不同的神经网络方法。但最终，它们都将单词转化为数百到数千个数字的列表。

在原始形式下，这些“嵌入向量”是相当不具信息量的。例如，以下是GPT-2为三个特定单词生成的原始嵌入向量：

![ChatGPT在做什么……以及它为什么有效？](../Images/35d2556a678dd22a373a05b9d4258cdc.png)

如果我们测量这些向量之间的距离，就可以发现单词之间的“接近度”。稍后我们将更详细地讨论我们可能考虑的这种嵌入的“认知”意义。但现在主要的点是，我们有一种有用的方法将单词转换为“神经网络友好的”数字集合。

但实际上，我们可以更进一步，不仅仅通过数字集合来表征单词；我们还可以对单词序列，甚至整块文本进行类似处理。在ChatGPT内部，就是这样处理这些事物的。它获取到目前为止的文本，并生成一个嵌入向量来表示它。然后，它的目标是找到可能接下来出现的不同单词的概率。它将其答案表示为一个数字列表，这些数字本质上给出每个大约50,000个可能单词的概率。

（严格来说，ChatGPT 不处理单词，而是[处理“标记”](https://platform.openai.com/tokenizer)——方便的语言单元，可以是整个单词，也可以只是像“pre”或“ing”或“ized”这样的片段。处理标记使 ChatGPT 更容易处理稀有、复合和非英语单词，并且有时，无论好坏，能发明新词。）

# ChatGPT 内部

好的，我们终于可以讨论 ChatGPT 的内部结构了。是的，最终，它是一个巨大的神经网络——目前是所谓的 GPT-3 网络的一个版本，拥有 1750 亿个权重。从很多方面来看，这个神经网络与我们讨论的其他网络非常相似。但它是一个特别为处理语言而设置的神经网络。而它最显著的特征是一个叫做“transformer”的神经网络架构。

在我们上面讨论的第一个神经网络中，每一层的每个神经元基本上都与前一层的每个神经元连接（至少有一定权重）。但这种完全连接的网络（可能）在处理具有特定已知结构的数据时是过度的。因此，例如，在处理图像的早期阶段，通常使用所谓的[卷积神经网络](https://reference.wolfram.com/language/ref/ConvolutionLayer.html)（“convnets”），其中神经元有效地布置在类似于图像中像素的网格上，并且只与网格上附近的神经元连接。

transformer 的理念是对组成文本的标记序列做一些至少有些相似的事情。但与其仅仅定义序列中可以存在连接的固定区域，transformers 引入了“[注意力](https://reference.wolfram.com/language/ref/AttentionLayer.html)”的概念——即“更关注”序列中的某些部分。也许有一天，启动一个通用的神经网络并通过训练进行所有自定义是有意义的。但至少到目前为止，在实践中，“模块化”事物似乎至关重要——就像 transformers 所做的那样，也可能是我们的大脑所做的那样。

好的，那么 ChatGPT（或者说，它所基于的 GPT-3 网络）实际做了什么？请记住，它的整体目标是基于它从训练中看到的内容，以一种“合理”的方式继续文本（训练包括查看来自网络等的数十亿页文本）。所以在任何给定时刻，它有一定量的文本——其目标是提出一个合适的下一个标记选择。

它在三个基本阶段进行操作。首先，它获取与目前文本对应的令牌序列，并找到一个表示这些令牌的嵌入（即一个数字数组）。然后，它对这个嵌入进行操作——以“标准神经网络方式”，值在网络的连续层中“传递”——生成一个新的嵌入（即一个新的数字数组）。接着，它取出这个数组的最后部分，从中生成一个约50,000个值的数组，这些值转化为不同可能下一个令牌的概率。（是的，恰好使用的令牌数量大致与英语中的常见单词数量相同，尽管只有约3000个令牌是完整单词，其余的是片段。）

一个关键点是，这个管道的每个部分都是由神经网络实现的，其权重由网络的端到端训练确定。换句话说，实际上除了整体架构外，没有什么是“明确设计”的；一切都是从训练数据中“学习”得来的。

然而，架构设置中还有很多细节——反映了各种经验和神经网络的经验法则。即使这确实是深入细节，我认为谈论一些这些细节是有用的，特别是为了了解构建像ChatGPT这样的东西所涉及的内容。

首先是嵌入模块。这里是GPT-2的Wolfram语言示意图：

![ChatGPT在做什么……以及为什么有效？](../Images/49d67ab9fc98b4a84415bfb2d1863161.png)

输入是一个*[n]*令牌的[向量](https://reference.wolfram.com/language/ref/netencoder/SubwordTokens.html)（如前一节所示，以从1到约50,000的整数表示）。这些令牌中的每一个都通过一个[单层神经网络](https://reference.wolfram.com/language/ref/EmbeddingLayer.html)转换为一个嵌入向量（GPT-2的长度为768，ChatGPT的GPT-3为12,288）。与此同时，还有一条“次要路径”，它接收[令牌位置的（整数）序列](https://reference.wolfram.com/language/ref/SequenceIndicesLayer.html)，并从这些整数创建另一个嵌入向量。最后，将令牌值和令牌位置的嵌入向量[相加](https://reference.wolfram.com/language/ref/ThreadingLayer.html)，生成来自嵌入模块的最终嵌入向量序列。

为什么要将令牌值和令牌位置嵌入向量相加？我认为这并没有特别的科学依据。只是尝试过各种不同的方法，这个方法似乎有效。而且神经网络的一个经验法则是——在某种意义上——只要设置“大致正确”，通常可以通过足够的训练来调整细节，而无需真正“在工程层面理解”神经网络如何配置。

嵌入模块的作用是对字符串 *hello hello hello hello hello hello hello hello hello hello bye bye bye bye bye bye bye bye bye bye* 进行处理：

![What Is ChatGPT Doing … and Why Does It Work?](../Images/6976274b6e5fd9f1a2cf9be082588d99.png)

每个令牌的嵌入向量的元素显示在页面的下方，而页面的横向则首先看到一系列的“*hello*”嵌入，然后是一系列的“*bye*”嵌入。上面的第二个数组是位置嵌入——其略显随机的结构正是“恰好被学习到”的（在这种情况下是GPT-2中的结果）。

好的，嵌入模块之后是变换器的“主要事件”：一系列所谓的“注意力块”（GPT-2为12个，ChatGPT的GPT-3为96个）。这一切都非常复杂——并且让人联想到典型的大型难以理解的工程系统，或者说生物系统。无论如何，这里是单个“注意力块”（对于GPT-2）的示意图：

![What Is ChatGPT Doing … and Why Does It Work?](../Images/5880d8eb03b792edbba01242ca3df735.png)

在每个这样的注意力块中，有一组“注意力头”（GPT-2为12个，ChatGPT的GPT-3为96个）——每个头独立地对嵌入向量中不同的值块进行操作。（而且，我们确实不知道为什么将嵌入向量拆分开来的具体理由，或其不同部分的“意义”是什么；这只是被“发现有效”的事情之一。）

好的，那么注意力头究竟做了什么呢？基本上，它们是一种“回顾”令牌序列（即已生成的文本）并以对找出下一个令牌有用的形式“打包过去”的方法。[在上面的第一部分](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/#its-just-adding-one-word-at-a-time)我们讨论了如何使用2-gram概率根据其直接前驱选择单词。变换器中的“注意力”机制使得对更早的单词也能进行“关注”——因此有可能捕捉到例如动词如何指向句子中许多词之前出现的名词的方式。

在更详细的层面上，注意力头所做的是以某些权重重新组合与不同令牌关联的嵌入向量中的块。例如，第一注意力块（在GPT-2中）的12个注意力头对于上面的“*hello*，*bye*”字符串具有以下（“回顾到令牌序列的开始”）的“重新组合权重”模式：

![What Is ChatGPT Doing … and Why Does It Work?](../Images/0ed1f66667ccd079906ba727e8970f4d.png)

在经过注意力头处理后，得到的“重新加权嵌入向量”（GPT-2 的长度为 768，ChatGPT 的 GPT-3 的长度为 12,288）会传递通过一个标准的[“全连接”神经网络层](https://reference.wolfram.com/language/ref/LinearLayer.html)。很难把握这个层在做什么。但这里有一个 768×768 权重矩阵的图（这是 GPT-2 的）：

![ChatGPT 在做什么 … 为什么它有效？](../Images/92f6cdc193fe50f6f58d09bc614652bd.png)

通过 64×64 移动平均，一些（随机游走般的）结构开始显现：

![ChatGPT 在做什么 … 为什么它有效？](../Images/79d32a691a66755f5eadc56549356a56.png)

是什么决定了这个结构？最终，它可能是某种对人类语言特征的“神经网络编码”。但截至目前，这些特征是什么仍然不太清楚。实际上，我们是在“揭开 ChatGPT 的大脑”（或者至少是 GPT-2 的大脑），发现里面确实很复杂，而且我们还不理解它——尽管最终它能够生成可识别的人类语言。

好的，那么在经过一个注意力块后，我们得到了一个新的嵌入向量——然后这个向量会依次通过额外的注意力块（GPT-2 总共 12 个；GPT-3 总共 96 个）。每个注意力块都有其独特的“注意力”和“全连接”权重模式。这里是 GPT-2 的“hello, bye”输入的注意力权重序列，第一个注意力头：

![ChatGPT 在做什么 … 为什么它有效？](../Images/a6d7060fa0b5812a8d862f08b10d98d8.png)

这里是（移动平均后的）“矩阵”，用于全连接层：

![ChatGPT 在做什么 … 为什么它有效？](../Images/d67b0f3522db33ccee532bd5ed7fdca1.png)

有趣的是，尽管这些不同注意力块中的“权重矩阵”看起来相似，但权重的大小分布可能会有所不同（并且不总是高斯分布）：

![ChatGPT 在做什么 … 为什么它有效？](../Images/4b8838c0720ceb977c46b209b4bddf59.png)

那么在经过所有这些注意力块后，变压器的净效果是什么？本质上，它是将序列的原始嵌入集合转变为最终集合。而 ChatGPT 的具体工作方式是从这个集合中取出最后一个嵌入，并“解码”它以生成一个关于下一个标记应该是什么的概率列表。

所以这就是 ChatGPT 的大致内部结构。它可能看起来很复杂（尤其是因为它有许多不可避免的、略显任意的“工程选择”），但实际上，涉及的最终元素非常简单。因为最终我们处理的只是一个由“人工神经元”组成的神经网络，每个神经元进行的简单操作是接收一组数值输入，然后与某些权重进行组合。

ChatGPT 的原始输入是一个数字数组（到目前为止的令牌嵌入向量），当 ChatGPT “运行”以产生新令牌时，实际上是这些数字在神经网络的层中“波动”，每个神经元“完成其任务”并将结果传递给下一层的神经元。没有循环或“返回”。一切都只是“前馈”通过网络。

这与典型的计算系统——如[Turing machine](https://www.wolframscience.com/nks/chap-3--the-world-of-simple-/%20programs/#sect-3-4--turing-machines)——非常不同，在那种系统中，结果会被同样的计算元素重复“再处理”。在这里——至少在生成给定的输出令牌时——每个计算元素（即神经元）只使用一次。

但从某种意义上说，即使在 ChatGPT 中，仍然存在一个“外部循环”会重用计算元素。因为当 ChatGPT 生成新令牌时，它总是“读取”（即作为输入）之前的整个令牌序列，包括 ChatGPT 自己之前“写”的令牌。我们可以认为这种设置意味着 ChatGPT 至少在最外层涉及了一个“反馈循环”，尽管每次迭代都明确地显示为生成文本中的一个令牌。

但让我们回到 ChatGPT 的核心：被重复用于生成每个令牌的神经网络。在某种程度上，它非常简单：一整套相同的人工神经元。网络的一些部分只是由（“[全连接](https://reference.wolfram.com/language/ref/LinearLayer.html)）”的神经元层组成，其中每一层上的每个神经元都与上一层的每个神经元连接（具有某些权重）。但特别是通过其变压器架构，ChatGPT 有更多结构的部分，其中只有不同层上的特定神经元是连接的。（当然，人们仍然可以说“所有神经元都是连接的”——但有些只是权重为零。）

此外，ChatGPT 中的神经网络有一些方面并不完全被认为是由“同质”层组成。例如——如上面的标志性总结所示——在注意力块内部，有些地方会“制作多个副本” 的输入数据，每个副本通过不同的“处理路径”，可能涉及不同数量的层，然后再进行组合。但虽然这可能是对发生情况的方便表示，但原则上总是可以考虑“密集填充”层，只是某些权重为零。

如果查看 ChatGPT 最长的路径，大约有 400 个（核心）层涉及其中——在某种程度上，这并不是一个巨大的数字。但有数百万个神经元——总共有 1750 亿个连接，因此有 1750 亿个权重。而且需要意识到的是，每当 ChatGPT 生成一个新标记时，它必须进行涉及每一个权重的计算。在实现上，这些计算可以按“层”组织成高度并行的数组操作，这些操作可以方便地在 GPU 上完成。但是，对于每一个生成的标记，仍然需要进行 1750 亿次计算（最终可能更多）——因此，是的，生成一段长文本可能需要一些时间，这是不令人惊讶的。

但最终，值得注意的是，所有这些操作——虽然它们单独看起来非常简单——能够共同完成如此出色的“类似人类”的文本生成。这一点需要再次强调的是（至少就我们目前所知），没有“终极理论上的理由”说明这样的事情会有效。实际上，正如我们将讨论的，我认为我们必须将其视为一种——可能令人惊讶的——科学发现：在像 ChatGPT 这样的神经网络中，确实可以捕捉到人脑在生成语言时所做的本质。

# ChatGPT 的训练

好的，我们现在已经概述了 ChatGPT 设置后的工作原理。但它是如何被设置的呢？那些 1750 亿个神经网络权重是如何确定的？基本上，它们是通过大规模训练的结果，这些训练基于由人类编写的大量文本——包括网络上的内容、书籍等。正如我们所说，即使有了所有这些训练数据，神经网络是否能成功生成“类似人类”的文本并不明显。而且，再次强调，要实现这一点似乎需要详细的工程技术。但是，ChatGPT 的重大惊喜和发现是这完全是可能的。实际上，一个具有“仅仅” 1750 亿个权重的神经网络可以生成一个“合理的模型”来模拟人类所写的文本。

在现代，存在大量以数字形式存在的人类书写文本。公共网页至少有数十亿个人书写的页面，总共可能有一万亿个词。如果包括非公开网页，这些数字可能大约大100倍。迄今为止，已经提供了超过500万本数字化的书籍（在曾经出版的约1亿本中），另外提供了大约1000亿个词。更不用提来自视频等的语音文本。（作为个人比较， [我一生出版的材料总量](https://www.stephenwolfram.com/publications/) 略低于300万字，过去的 [30年里我写了](https://writings.stephenwolfram.com/2012/03/the-personal-analytics-of-my-life/) 大约1500万字的邮件，总共打字大约5000万字——在过去几年中，我在 [直播中讲了超过1000万字](https://www.stephenwolfram.com/livestreams) 。是的，我会从所有这些内容中训练一个机器人。）

好吧，鉴于所有这些数据，如何从中训练一个神经网络？基本过程与我们在上述简单示例中讨论的非常类似。你呈现一批示例，然后调整网络中的权重，以最小化网络在这些示例上的误差（“损失”）。关于“反向传播”中花费的主要部分在于，每次进行时，网络中的每个权重通常会至少改变一点，而需要处理的权重数量非常多。（实际的“反向计算”通常只是比前向计算难度增加一个小的常数因子。）

使用现代 GPU 硬件，从成千上万的示例中并行计算结果是非常直接的。但在实际更新神经网络的权重时，目前的方法基本上需要逐批进行。（而且，是的，这可能正是实际大脑——结合了计算和记忆元素——在架构上至少暂时具有优势的地方。）

即使在我们之前讨论的看似简单的学习数值函数的情况下，我们发现我们经常需要使用数百万个示例来成功训练一个网络，至少从零开始。那么，这意味着我们需要多少个示例来训练一个“类人语言”模型？似乎没有任何基本的“理论”方法来确定。但在实践中，ChatGPT 是在数百亿个词的文本上成功训练的。

有些文本被重复喂入了几次，有些仅被喂入了一次。但它“从它看到的文本中获得了它需要的东西”。但考虑到这个学习文本的量，应该需要多大的网络才能“学得好”？再次地，我们尚无一个基本的理论方法来说明。最终—正如我们将在下文进一步讨论的—人类语言及其通常所说内容可能存在一定的“总算法内容”。但下一个问题是神经网络在基于这些算法内容实施模型时的效率如何。我们仍然不知道—尽管 ChatGPT 的成功表明它在效率上是相当合理的。

最终，我们可以简单地指出，ChatGPT 是通过几个亿个权重来完成它的工作的—这个数量与它所接受的训练数据的总词数（或标记数）相当。从某种程度上来说，这或许令人惊讶（虽然在 ChatGPT 的较小类似模型中也有观察到这种情况），即“网络的规模”与“训练数据的规模”如此相似。毕竟，肯定不是“ChatGPT 内部”直接存储了这些来自网络和书籍等的文本。因为实际上，ChatGPT 内部是一些数字—精度略低于 10 位数字—这些数字是所有这些文本的某种分布式编码。

换句话说，我们可以问人类语言的“有效信息内容”是什么，以及通常所说的内容是什么。有原始的语言示例语料库。然后是 ChatGPT 神经网络中的表示。这个表示很可能远非“算法上最小”的表示（如我们将在下文讨论的）。但这是一个神经网络可以直接使用的表示。在这个表示中，似乎对训练数据的“压缩”相当有限；平均而言，基本上只需少于一个神经网络权重就能承载一个训练数据词的“信息内容”。

当我们运行 ChatGPT 生成文本时，基本上每个权重都需要被使用一次。所以如果有 *n* 个权重，我们就需要大约 *n* 步计算—尽管在实际中，许多计算步骤通常可以在 GPU 中并行完成。但如果我们需要大约 *n* 个训练数据词来设置这些权重，那么从我们以上所说的内容可以得出结论，我们需要大约 *n*² 步计算来完成网络的训练—这就是为什么，使用当前的方法，需要谈论亿级别的训练投入。

# 基础训练之外

训练 ChatGPT 的大部分工作是“展示”大量来自网络、书籍等的现有文本。但事实证明，还有另一个—显然相当重要—部分。

一旦它完成了来自原始文本语料库的“原始训练”，ChatGPT 内部的神经网络就准备开始生成自己的文本，继续从提示中进行。但虽然这些结果可能看起来合理，它们往往——特别是对于较长的文本——会以往往相当非人类的方式“偏离”。这不是一种可以通过对文本进行传统统计轻易检测到的情况。但这是实际阅读文本的人容易注意到的。

一个 [ChatGPT 构建中的关键思想](https://openai.com/blog/instruction-following/) 是在像网络这样的“被动阅读”之后再进行另一步骤：让实际的人类与 ChatGPT 积极互动，查看它生成的内容，并实际上对其“如何成为一个优秀聊天机器人”提供反馈。但是神经网络如何利用这些反馈？第一步是让人类对神经网络的结果进行评分。但随后构建了另一个神经网络模型，试图预测这些评分。现在，这个预测模型可以运行——本质上就像一个损失函数——在原始网络上，从而使得该网络可以通过已给予的人类反馈进行“调整”。实际结果似乎对系统成功生成“类人”输出有很大影响。

总体来说，令人感兴趣的是，“最初训练过的”网络似乎只需很少的“刺激”就能在特定方向上有用地发展。人们可能会认为，要使网络表现得像是“学到了新东西”，必须进入并运行训练算法，调整权重等等。

但事实并非如此。相反，似乎基本上只需一次告诉 ChatGPT 某些事情——作为你提供的提示的一部分——然后它就可以在生成文本时成功利用你告诉它的内容。我认为，这种有效性的事实是理解 ChatGPT “真实做了什么”以及它如何与人类语言和思维结构相关的重要线索。

这确实有点像人类：至少在它接受了所有那些预训练后，你可以只告诉它一次它就能“记住”——至少“足够长的时间”来使用这些信息生成一段文本。那么，在这种情况下发生了什么？可能是“你告诉它的一切已经在某个地方”——你只是引导它到正确的位置。但这似乎不太可信。相反，更可能的情况是，确实，元素已经在里面，但细节是由类似于“这些元素之间的轨迹”的东西定义的，而这就是你在告诉它某些东西时所引入的。

确实，就像对人类一样，如果你告诉它一些奇怪且出乎意料的内容，这些内容完全不符合它所知道的框架，它似乎无法成功“整合”这些内容。它只能在基本上沿用其已有的框架的情况下“整合”这些内容。

还值得再强调一点，神经网络不可避免地存在“算法限制”，即它可以“捕捉”到的内容。告诉它一些“浅层”的规则，例如“这个去那个”，神经网络很可能能够很好地表示和再现这些规则——事实上，它从语言中“已经知道”的内容会给它一个立刻可以遵循的模式。但如果尝试给它一些实际的“深层”计算规则，这些规则涉及许多可能的计算不可约步骤，它就无法完成。 （记住，在每一步，它总是只是在其网络中“向前传递数据”，除非通过生成新标记来循环。）

当然，网络可以学习特定的“不可约”计算的答案。但一旦出现组合数目的可能性，任何这种“表查找风格”的方法都将不起作用。因此，是的，就像人类一样，神经网络也需要“伸出”并使用实际的计算工具。（而且，是的，[Wolfram|Alpha](https://www.wolframalpha.com/) 和 [Wolfram Language](https://www.wolfram.com/language/) 是 [独特适用](https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/) 的，因为它们被构建为“讨论世界中的事物”，就像语言模型神经网络一样。）

# 什么真正让 ChatGPT 工作？

人类语言——以及生成它的思维过程——总是看起来代表了一种复杂性的巅峰。确实，人类的大脑——虽然只有大约 1000 亿个神经元（可能还有 100 万亿个连接）——能够负责这一点，这似乎有些令人惊讶。也许，有人可能会想象，大脑的某些东西比其神经网络更复杂——比如某种尚未发现的物理层次。但现在，凭借 ChatGPT，我们获得了一个重要的新信息：我们知道一个纯粹的人工神经网络，其连接数量与大脑中的神经元数量相当，能够非常出色地生成类似人类的语言。

是的，这仍然是一个庞大而复杂的系统——其神经网络权重的数量与目前世界上可用的文本单词数量大致相当。但在某种程度上，仍然很难相信语言的所有丰富性以及它可以讨论的内容都可以被封装在这样一个有限的系统中。部分原因无疑是反映了普遍存在的现象（这一现象首次在[规则 30 的示例](https://www.wolframscience.com/nks/chap-2--the-crucial-experiment#sect-2-1--how-do-simple-programs-behave)中显现出来），即计算过程实际上可以极大地放大系统的表观复杂性，即使其底层规则很简单。但实际上，正如我们上面讨论的那样，ChatGPT 使用的神经网络往往被特别构造，以限制这一现象的影响——以及与之相关的计算不可简化性——以便使其训练更为可及。

那么，ChatGPT 如何在语言上取得如此大的进展呢？基本的回答是，我认为语言在根本上比它看起来的要简单。这意味着，即使是具有最终简单的神经网络结构的 ChatGPT，也能够成功地“捕捉到”人类语言及其背后的思维。此外，在其训练中，ChatGPT 不知怎么地“隐性地发现”了使这一切成为可能的语言（和思维）规律。

我认为，ChatGPT 的成功为我们提供了一个基本且重要的科学证据：它表明我们可以期待发现一些重大的新“语言规律”——实际上是“思维规律”。在 ChatGPT 中——尽管它是作为一个神经网络构建的——这些规律充其量只是隐含的。但如果我们能够以某种方式使这些规律显性化，就有可能以更直接、更高效——以及更透明的方式——做 ChatGPT 所做的事情。

那么，这些规律可能是什么样的呢？最终它们必须给我们一些关于语言——以及我们用它说的话——如何组合的指示。稍后我们将讨论如何“窥视 ChatGPT 内部”可能会给我们一些关于这方面的提示，以及从构建计算语言中得到的知识如何暗示前进的道路。但首先，让我们讨论两个早已为人知的“语言规律”示例——以及它们与 ChatGPT 操作的关系。

首先是语言的语法。语言不仅仅是随机的词汇组合。相反，存在（相当）明确的[语法规则](https://www.wolframscience.com/nks/notes-10-12--computer-and-human-languages/)，规定了不同类型的词如何组合在一起：例如在英语中，名词可以被形容词修饰并由动词跟随，但通常两个名词不能紧挨在一起。这种语法结构可以（至少大致上）通过一套规则来捕捉，这些规则定义了如何将[“解析树”组合在一起](https://reference.wolfram.com/language/ref/TextStructure.html)：

![What Is ChatGPT Doing … and Why Does It Work?](../Images/1505a76a3c53904023b06a5f75bf396e.png)

ChatGPT 并没有明确的“知识”关于这些规则。但它在训练过程中以某种方式“发现”了这些规则——然后似乎能够很好地遵循它们。那么这如何运作呢？从“大图景”层面来看，还不清楚。但为了获得一些见解，也许可以看看一个更简单的例子。

考虑一个由（’和）’组成的“语言”，其[语法规定](https://www.wolframscience.com/nks/notes-7-9--nested-lists/) 括号应始终保持平衡，如通过解析树表示：

![What Is ChatGPT Doing … and Why Does It Work?](../Images/c12a678e99094187cf531bfe227cb655.png)

我们可以训练一个神经网络来生成“语法正确”的括号序列吗？处理序列的神经网络有多种方法，但我们使用与 ChatGPT 相同的变换器网络。给定一个简单的变换器网络，我们可以开始用语法正确的括号序列作为训练样本。一个微妙之处（实际上也出现在 ChatGPT 的语言生成中）是，除了我们的“内容标记”（这里是“（” 和 “）”）外，我们还必须包括一个“结束”标记，用于表示输出不应该继续（即对于 ChatGPT 来说，这表示“故事的结尾”）。

如果我们设置一个只有一个具有 8 个头的注意力块和长度为 128 的特征向量的变换器网络（ChatGPT 也使用长度为 128 的特征向量，但有 96 个注意力块，每个具有 96 个头），那么似乎无法让它学到很多关于括号语言的知识。但有了 2 个注意力块后，学习过程似乎会收敛——至少在给出大约 1000 万个样本后（并且，正如变换器网络中常见的那样，显示更多的样本似乎只是降低了其性能）。

因此，使用这个网络，我们可以模拟 ChatGPT 的功能，并请求预测下一个标记应该是什么——在一个括号序列中：

![What Is ChatGPT Doing … and Why Does It Work?](../Images/b1db22972c6053372dac09f3075745f3.png)

在第一个例子中，网络“相当确定”序列不能在这里结束——这是好的，因为如果在这里结束，括号将会不平衡。然而，在第二个例子中，网络“正确地识别”序列可以在这里结束，尽管它也“指出”可以“重新开始”，放下一个“(”，推测会跟一个“)” 。但，哎呀，即使拥有大约40万个经过辛苦训练的权重，它也表示有15%的概率下一个标记是“）”——这不对，因为那会导致括号不平衡。

如果我们要求网络对逐渐增长的括号序列进行最高概率的补全，我们会得到以下结果：

![ChatGPT在做什么……以及为什么它有效？](../Images/bb3bf9e34391cc8fcef23843f9a12f5e.png)

是的，对于一定长度的序列，网络表现得很好。但是，超出这个长度后，它就开始出现问题。这是一种在“精确”情况下看到的典型现象，无论是神经网络还是一般的机器学习。神经网络可以解决那些人类“可以一眼看懂”的问题。但是，对于那些需要进行“更算法化”的操作（例如，明确计算括号是否匹配）的问题，神经网络往往会变得“计算上过于浅显”，难以可靠地处理。（顺便提一下，即使是当前版本的ChatGPT，在处理长序列的括号匹配时也会遇到困难。）

那么，这对像ChatGPT这样的系统以及像英语这样的语言的语法意味着什么？括号语言是“简洁的”——更像是“算法化的故事”。但在英语中，基于局部词汇选择和其他提示来“猜测”语法上会适合的内容更为现实。是的，神经网络在这方面要更好——尽管它可能会遗漏一些“形式上正确”的情况，人类也可能会遗漏。但关键点是，语言中存在整体的语法结构——所有的规则性都意味着——在某种程度上限制了“神经网络需要学习的内容”。一个关键的“自然科学”观察是，像ChatGPT中的变换器架构似乎成功地学习了那种嵌套树状的语法结构，这种结构似乎存在（至少在某种程度上）于所有人类语言中。

语法为语言提供了一种约束。然而，显然还有更多的约束。像“Inquisitive electrons eat blue theories for fish”这样的句子在语法上是正确的，但通常不被认为是正常的表达，如果ChatGPT生成了这样的句子，也不会被认为是成功的——因为按照这些词的正常含义，这句话基本上是毫无意义的。

但是否有一种通用的方法来判断句子是否有意义？对于这个问题没有传统的总体理论。但可以认为ChatGPT在用来自网络等的数十亿个（可能有意义的）句子进行训练后，隐含地“发展了一个理论”。

这个理论可能是什么样的呢？嗯，有一个微小的角落基本上已经被了解了两千年，那就是[逻辑](https://writings.stephenwolfram.com/2018/11/logic-explainability-and-the-future-of-understanding/)。毫无疑问，在亚里士多德发现它的三段论形式中，逻辑基本上是一种说法，即遵循某些模式的句子是合理的，而其他的则不是。因此，例如，说“所有X都是Y。这不是Y，所以它不是X”是合理的（比如“所有鱼都是蓝色的。这不是蓝色的，所以它不是鱼。”）。正如可以有些异想天开地想象亚里士多德通过大量修辞例子“机器学习风格”地发现了三段论逻辑一样，也可以想象在训练ChatGPT时，它能够通过查看大量网络上的文本等来“发现三段论逻辑”。（是的，虽然因此可以期望ChatGPT生成的文本包含基于三段论逻辑的“正确推论”，但当涉及到更复杂的形式逻辑时，情况就大相径庭了——我认为可以预期它在这里会失败，原因与括号匹配失败的原因类似。）

但超越狭义的逻辑示例，关于如何系统地构造（或识别）即使是看似有意义的文本，还能说些什么呢？是的，有像[疯狂填词](https://en.wikipedia.org/wiki/Mad_Libs)这样的东西，使用了非常具体的“短语模板”。但不知怎么的，ChatGPT隐含地有一种更为通用的方式来做到这一点。也许对于如何做到这一点，没有比“当你有1750亿个神经网络权重时它会发生”更好的说法了。但我强烈怀疑有一个更简单、更强的故事。

# 意义空间和语义运动定律

我们在上面讨论过，在ChatGPT内部，任何一段文本实际上都被表示为一个数字数组，我们可以将其视为某种“语言特征空间”中的一个点的坐标。因此，当ChatGPT继续一段文本时，这相当于在语言特征空间中描绘出一条轨迹。但现在我们可以问，是什么使这条轨迹对应于我们认为有意义的文本。也许存在某种“语义运动定律”，定义或至少约束语言特征空间中的点在保持“有意义性”时的移动方式？

那么这种语言特征空间是什么样的呢？这里有一个例子，展示了如果我们将这种特征空间投射到二维平面上，单个词（这里是常见名词）可能会如何分布：

![ChatGPT在做什么…以及为什么它有效？](../Images/42217e14abaafd544033c4704c165d7a.png)

上面我们看到的另一个例子基于代表植物和动物的词。但在这两种情况下的要点是，“语义相似的词”被放置在一起。

作为另一个例子，以下是不同词性对应的词的布局方式：

![ChatGPT 在做什么 … 以及它为什么有效？](../Images/07aea0f92037fd1ea81e987046ecc150.png)

当然，一个给定的词通常并不只有“一种意义”（或必然只对应一种词性）。通过查看包含一个词的句子在特征空间中的布局，通常可以“区分”不同的意义——就像这里的“crane”（鸟或机器？）一词的例子：

![ChatGPT 在做什么 … 以及它为什么有效？](../Images/dcbef2380577de81112276aa2822306b.png)

好的，所以我们至少可以合理地认为，我们可以将这个特征空间视为在此空间中将“语义相近的词”放得较近。但我们可以在这个空间中识别出什么额外的结构？例如，有没有某种“平行传输”的概念可以反映空间的“平坦性”？一种获取这种信息的方法是查看类比：

![ChatGPT 在做什么 … 以及它为什么有效？](../Images/e16bc0dfc4612deb45d2386b40e82a4c.png)

是的，即使我们投影到 2D，通常也会有至少一个“平坦性的提示”，尽管这并不是普遍存在的。

那么关于轨迹呢？我们可以查看 ChatGPT 在特征空间中跟随的提示轨迹——然后我们可以看到 ChatGPT 如何继续这一轨迹：

![ChatGPT 在做什么 … 以及它为什么有效？](../Images/03a94ecdae7ea41cb58042bac1f2b4a3.png)

这里显然没有“几何上显而易见”的运动定律。这并不令人惊讶；我们完全预期这会是一个[复杂得多的故事](https://writings.stephenwolfram.com/2021/09/multicomputation-a-fourth-paradigm-for-theoretical-science/#linguistics)。例如，即使发现了“语义运动定律”，它最自然的表述形式（或实际上的“变量”）也远非显而易见。

在上图中，我们展示了“轨迹”中的几个步骤——在每一步我们都选择 ChatGPT 认为最可能的词（“零温度”情况）。但我们也可以询问在给定点上什么词可以“接下来”出现，概率是多少：

![ChatGPT 在做什么 … 以及它为什么有效？](../Images/f04d59acdc721fb254cfeb0f0a8c7134.png)

在这种情况下，我们看到的是一个高概率词的“扇形”，似乎在特征空间中朝着一个或多或少明确的方向延展。如果我们进一步探索会发生什么？以下是我们“沿着”轨迹移动时出现的连续“扇形”：

![ChatGPT 在做什么 … 以及它为什么有效？](../Images/0134dc0356b767df2c1d3c809f03e2af.png)

这是一个 3D 表示，总共有 40 步：

![ChatGPT 在做什么 … 以及它为什么有效？](../Images/aec46fd9f424935d26a2d998424bfb0f.png)

是的，这看起来像一团糟——并没有特别鼓励人们相信可以通过经验研究“ChatGPT内部发生了什么”来识别“类似数学物理的” “语义运动法则”。但也许我们只是看错了“变量”（或错误的坐标系），如果我们只看对的，我们就会立即看到ChatGPT正在做一些“数学物理简单”的事情，比如遵循测地线。但截至目前，我们还没有准备好“从其内部行为中经验性解码”ChatGPT“发现”的关于人类语言如何“组合”的信息。

# 语义语法与计算语言的力量

产生“有意义的人类语言”需要什么？过去，我们可能认为这只能由人脑完成。但现在我们知道，这可以由ChatGPT的神经网络相当体面地完成。尽管如此，也许这就是我们能走的最远的地方，可能没有更简单的或更易于理解的东西能够奏效。但我强烈怀疑的是，ChatGPT的成功隐含地揭示了一个重要的“科学”事实：有意义的人类语言实际上比我们知道的要有更多的结构和简单性——最终可能会有一些相当简单的规则来描述这种语言如何组合在一起。

正如我们之前提到的，句法语法为不同词类的词如何在自然语言中组合提供了规则。但要处理意义，我们需要更进一步。一个版本的做法是，不仅考虑语言的句法语法，还要考虑语义语法。

在语法的层面上，我们识别名词和动词等事物。但在语义层面上，我们需要“更精细的划分”。例如，我们可能会识别“移动”的概念，以及一个“在位置上保持其身份独立”的“物体”的概念。这些“语义概念”都有无尽的具体例子。但就我们的语义语法而言，我们会有一些基本的规则，大致说明“物体”可以“移动”。关于这一切如何运作，还有很多要说的（[我以前提到过一些](https://writings.stephenwolfram.com/2016/10/computational-law-symbolic-discourse-and-the-ai-constitution/)）。但我在这里只会简单提几句，指出一些可能的前进方向。

值得一提的是，即使一个句子根据语义语法是完全正确的，也不意味着它已经在实践中实现（或甚至可能实现）。“大象旅行到月球”无疑会“通过”我们的语义语法，但它确实还没有在我们的实际世界中实现（至少目前还没有）——尽管它绝对是虚构世界的公平素材。

当我们开始讨论“语义语法”时，我们很快会问“它底下是什么？”它假设了什么“世界模型”？句法语法实际上只是关于如何从单词构建语言。但语义语法必然涉及某种“世界模型”——这是一种“骨架”，语言可以在其上叠加。

直到最近，我们可能会认为（人类）语言将是描述我们“世界模型”的唯一通用方式。早在几个世纪前，就开始对特定类型的事物进行形式化，特别是基于数学的。但现在有了一种更为通用的形式化方法：[计算语言](https://writings.stephenwolfram.com/2019/05/what-weve-built-is-a-computational-language-and-thats-very-important/)。

是的，这就是我在四十多年时间里（现在体现于[Wolfram 语言](https://www.wolfram.com/language/)）的大项目：开发一个精确的符号表示，可以尽可能广泛地谈论世界上的事物，以及我们关心的抽象事物。例如，我们有[cities](https://reference.wolfram.com/language/ref/entity/City.html)和[molecules](https://reference.wolfram.com/language/guide/MolecularStructureAndComputation.html)以及[images](https://reference.wolfram.com/language/guide/ImageRepresentation.html)和[neural networks](https://reference.wolfram.com/language/guide/NeuralNetworkConstruction.html)的符号表示，并且我们已经内置了关于如何计算这些事物的知识。

经过几十年的工作，我们已经涵盖了很多领域。但在过去，我们没有特别处理“[日常话语](https://writings.stephenwolfram.com/2016/10/computational-law-symbolic-discourse-and-the-ai-constitution/)”。在“我买了两磅苹果”中，我们可以[轻松表示](https://reference.wolfram.com/language/ref/entity/Food.html)（并对其进行营养和其他计算）“两磅苹果”。但我们还（尚未）拥有“我买了”的符号表示。

这一切都与语义语法的概念有关——以及拥有一个通用符号“构建工具”的目标，这将为我们提供关于什么可以组合在一起的规则，从而决定我们可能转化为人类语言的“流”。

但假设我们有了这种“符号话语语言”。我们会怎么处理它呢？我们可以先从生成“局部有意义的文本”开始。但**最终**我们可能会想要更多“全球有意义”的结果——这意味着“计算”实际存在或发生在世界上的更多内容（或者也许在某个一致的虚构世界中）。

目前在 Wolfram Language 中，我们拥有大量关于各种事物的内置计算知识。但对于一个完整的符号话语语言，我们还需要构建关于世界上普遍事物的额外“计算体系”：例如，如果一个物体从 A 移动到 B，再从 B 移动到 C，那么它就从 A 移动到了 C，等等。

有了符号话语语言，我们可以用它来做“独立陈述”。但我们也可以用它来提出关于世界的问题，“Wolfram|Alpha 风格”。或者我们可以用它来陈述我们“希望实现的”事情，通常是通过某种外部驱动机制实现的。或者我们可以用它来做断言——也许是关于实际世界，或者关于我们考虑的某个特定世界，无论是虚构的还是其他的。

人类语言在根本上是不精确的，尤其是因为它没有“绑定”到特定的计算实现，其含义基本上只是由使用者之间的“社会契约”定义的。但计算语言本质上具有一定的基本精确性——因为最终它所指定的内容总是可以“在计算机上明确执行”。人类语言通常可以容忍一定的模糊性。（例如，当我们说“行星”时，是否包括系外行星？）但在计算语言中，我们必须对所做的所有区分保持精确和清晰。

在构造计算语言的名称时，利用普通人类语言通常很方便。但它们在计算语言中的含义必然是精确的——而且可能覆盖或不覆盖典型人类语言使用中的某些特定含义。

如何确定适合一般符号话语语言的基本“本体论”？这并不容易。这或许也是为什么自从两千多年前亚里士多德的原始开端以来，这方面的研究几乎没有进展。但今天我们知道了很多关于如何以计算方式思考世界的知识（并且拥有来自我们的 [物理学项目](https://www.wolframphysics.org/) 和 [ruliad 概念](https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad/) 的“基础形而上学”也很有帮助）。

那么，这些在 ChatGPT 的背景下意味着什么呢？通过训练，ChatGPT 实质上“拼凑”出了一定量的语义语法（相当令人印象深刻）。但它的成功让我们有理由认为，构建一种更完整的计算语言形式是可行的。而且，与我们迄今为止对 ChatGPT 内部结构的了解不同，我们可以期望设计一种计算语言，使其对人类容易理解。

当我们谈论语义语法时，我们可以类比于三段论逻辑。一开始，三段论逻辑本质上是关于用人类语言表达的陈述的规则集合。但（是的，两千年后）[当形式逻辑被发展](https://writings.stephenwolfram.com/2018/11/logic-explainability-and-the-future-of-understanding/#the-history)时，三段论逻辑的基本构造现在可以用来构建巨大的“形式塔”，包括例如现代数字电路的操作。因此，我们可以预期，更一般的语义语法也会如此。一开始，它可能只能处理简单的模式，例如以文本形式表达。但一旦构建了整个计算语言框架，我们可以期待它能够用来建立“广义语义逻辑”的高塔，使我们能够以精确和形式化的方式处理各种以前无法接触的事物，只能通过人类语言以“地面层级”处理，带有所有的模糊性。

我们可以把计算语言和语义语法的构建看作是一种在表示事物时的终极压缩。因为它让我们能够讨论可能性的本质，而不需要处理普通人类语言中存在的所有“表达方式”。我们可以将 ChatGPT 的巨大优势视为类似的东西：因为它也在某种程度上“钻透”到了能够“以语义上有意义的方式”组合语言的地步，而不必担心不同的表达方式。

那么如果我们将 ChatGPT 应用到底层计算语言上会发生什么呢？计算语言可以描述什么是可能的。但可以进一步补充的是一种“什么是流行”的感觉——例如，基于读取互联网上的所有内容。但在底层——操作计算语言意味着像 ChatGPT 这样的系统具有即时和根本的访问权限，能够使用潜在不可简化的计算的终极工具。这使得它不仅可以“生成合理的文本”，而且可以期望解决任何可以解决的内容，以确定该文本是否实际上对世界做出了“正确”的陈述——或者它应该讨论的内容。

# 那么… ChatGPT 在做什么，为什么它能有效？

ChatGPT 的基本概念在某种程度上相当简单。从网络、书籍等来源的大量人类创作文本开始。然后训练一个神经网络生成“像这样的”文本。特别是，让它能够从一个“提示”开始，然后继续生成“像它所训练的那样”的文本。

正如我们所看到的，ChatGPT 的实际神经网络由非常简单的元素构成——尽管有数十亿个。而神经网络的基本操作也非常简单，本质上是将从其迄今生成的文本中派生的输入“通过其元素”处理一次（没有任何循环等），以生成每一个新词（或词的一部分）。

但令人惊讶且出乎意料的是，这个过程可以生成成功“类似”于网络、书籍等上的文本。而且，它不仅是连贯的人类语言，还会“表达”符合其提示的内容，利用它所“阅读”的内容。它并不总是说出“全球上合理”的话（或对应正确的计算）——因为（例如，没有[访问 Wolfram|Alpha 的“计算超能力”](https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/)）它只是在根据训练材料中“听起来对的”东西说话。

ChatGPT 的具体工程设计使其非常引人注目。但最终（至少在它能够使用外部工具之前）ChatGPT 只是从其积累的“传统智慧统计”中“抽取出一些连贯的文本线索”。但结果的类人性令人惊叹。正如我所讨论的，这暗示了一个至少在科学上非常重要的事实：人类语言（及其背后的思维模式）在结构上比我们想象的要简单和“更具规律性”。ChatGPT 已经隐含地发现了这一点。但我们可以通过语义语法、计算语言等方式显式地揭示它。

ChatGPT 在生成文本时的表现非常令人印象深刻——而且结果通常非常接近我们人类会产生的内容。这是否意味着 ChatGPT 像大脑一样工作？它的基础人工神经网络结构最终是基于对大脑的理想化模型。并且，似乎我们人类生成语言时，许多方面与之类似。

在训练（即学习）大脑和当前计算机的不同“硬件”时（以及也许一些尚未开发的算法思想），ChatGPT 被迫使用一种可能与大脑相当不同（在某些方面效率更低）的策略。还有其他一些问题：即使在典型的算法计算中，ChatGPT 也没有内部“循环”或“在数据上重新计算”。这不可避免地限制了它的计算能力——即使与当前计算机相比，也确实与大脑相比。

目前还不清楚如何“修复”这个问题，同时仍保持合理的训练效率。但是，这样做将可能让未来的ChatGPT能够做更多“类似大脑的事情”。当然，大脑确实有很多做得不那么好的事情—特别是那些涉及到不可简化计算的任务。对于这些任务，大脑和像ChatGPT这样的系统必须寻求“外部工具”—例如[Wolfram语言](https://www.wolfram.com/language/)。

但现在看到ChatGPT已经能够做到的事情令人兴奋。在某种程度上，它是一个很好的例子，展示了大量简单计算元素可以做出非凡和意想不到的事情这一基本科学事实。但它也提供了我们在两千年来最好的动机，以更好地理解人类语言这一人类状况核心特征的基本特性和原则，以及背后的思维过程。

# 感谢

我已经跟踪神经网络的发展约43年，在此期间我与许多人进行了互动。其中—有些是很久以前的，有些是最近的，还有一些是跨越多年—包括：Giulio Alessandrini, Dario Amodei, Etienne Bernard, Taliesin Beynon, Sebastian Bodenstein, Greg Brockman, Jack Cowan, Pedro Domingos, Jesse Galef, Roger Germundsson, Robert Hecht-Nielsen, Geoff Hinton, John Hopfield, Yann LeCun, Jerry Lettvin, Jerome Louradour, Marvin Minsky, Eric Mjolsness, Cayden Pierce, Tomaso Poggio, Matteo Salvarezza, Terry Sejnowski, Oliver Selfridge, Gordon Shaw, Jonas Sjöberg, Ilya Sutskever, Gerry Tesauro 和 Timothee Verdier。特别感谢Giulio Alessandrini和Brad Klee对本文的帮助。

**[斯蒂芬·沃尔弗拉姆](https://www.linkedin.com/in/stephenwolfram/)** 是Mathematica、Wolfram|Alpha 和Wolfram语言的创始人；Wolfram物理学项目的发起者；《新科学》及其他书籍的作者；Wolfram Research的创始人兼首席执行官。

[原文](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)。经许可转载。

### 更多相关内容

+   [K-Means聚类是什么？它的算法是如何工作的？](https://www.kdnuggets.com/2023/05/kmeans-clustering-algorithm-work.html)

+   [逻辑回归是如何工作的？](https://www.kdnuggets.com/2022/07/logistic-regression-work.html)

+   [数据血缘是什么？为什么它很重要？](https://www.kdnuggets.com/what-is-data-lineage-and-why-does-it-matter)

+   [机器学习为什么没有为我的业务创造价值？](https://www.kdnuggets.com/2021/12/machine-learning-produce-value-business.html)

+   [ChatGPT是否有可能成为新的国际象棋超级大满贯大师？](https://www.kdnuggets.com/does-chatgpt-have-the-potential-to-become-a-new-chess-super-grandmaster)

+   [忘掉 ChatGPT，这款全新的 AI 助手远远领先，将永远改变你的工作方式](https://www.kdnuggets.com/2023/08/forget-chatgpt-new-ai-assistant-leagues-ahead-change-way-work-forever.html)
