- en: Understanding How Neural Networks Think
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/07/understanding-neural-networks-think.html](https://www.kdnuggets.com/2020/07/understanding-neural-networks-think.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)![Figure](../Images/0efaa7a2bc2407cb8427e52fe4864cf4.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Source: [https://distill.pub/2018/building-blocks/](https://distill.pub/2018/building-blocks/)
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_BQ
  - PREF_H2
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '* * *'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'I recently started a new newsletter focus on AI education. TheSequence is a
    no-BS (meaning no hype, no news etc) AI-focused newsletter that takes 5 minutes
    to read. The goal is to keep you up to date with machine learning projects, research
    papers and concepts. Please give it a try by subscribing below:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[![Image](../Images/f2aed90f956dea213be7c9bbf9cd7072.png)](https://thesequence.substack.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: One of the challenging elements of any deep learning solution is to understand
    the knowledge and decisions made by deep neural networks. While the interpretation
    of decisions made by a neural networks has always been difficult, the issue has
    become a nightmare with the raise of deep learning and the proliferation of large
    scale neural networks that operate with multi-dimensional datasets. Not surprisingly,
    the interpretation of neural networks has become one of the most active areas
    of research in the deep learning ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Try to imagine a large neural network with hundreds of millions of neurons
    that is performing a deep learning task such as image recognition. Typically,
    you would like to understand how the network arrives to specific decisions. Most
    of the current research has focused on detecting what neurons in the network have
    been activated. Knowing that neuron-12345 fired five times is relevant but not
    incredibly useful in the scale of the entire network. The research about understanding
    decisions in neural networks has focused on three main areas: feature visualization,
    attribution and dimensionality reduction. Google, in particular, has done a lot
    of work in the feature visualization space publishing some [remarkable research
    and tools](https://distill.pub/2017/feature-visualization/). Over a year ago,
    Google researchers published a paper titled[ “The Building Blocks of Interpretability” ](https://distill.pub/2018/building-blocks/)that
    became a seminal paper in the are of machine learning interpretability. The paper
    proposes some new ideas to understand how deep neural networks make decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: The main insight of Google’s research is to not see the different interpretability
    techniques in isolation but as composable building blocks of larger models that
    help understand the behavior of neural networks. For instance, feature visualization
    is a very effective technique to understand the information processed by individual
    neurons but fails to correlate that insight with the overall decision made by
    the neural network. Attribution is a more solid technique to explain the relationship
    between different neurons but not so much when comes to understand the decision
    made by individual neurons. Combining those building blocks, Google has created
    an interpretability models that does not only *explains what* a neural network
    detects, but it does answer *how *the network assembles these individual pieces
    to arrive at later decisions, and *why* these decisions were made.
  prefs: []
  type: TYPE_NORMAL
- en: 'How does the new Google model for interpretability works specifically? Well,
    the main innovation, in my opinion, is that it analyzes the decisions made by
    different components of a neural network at different levels: individual neurons,
    connected groups of neurons and complete layers. Google also uses a novel research
    technique called matrix factorization to analyze the impact that arbitrary groups
    of neurons can have in the final decision.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/99dba3fddd2ba92aed2c6e61f61314a0.png)'
  prefs: []
  type: TYPE_IMG
- en: Source: [https://distill.pub/2018/building-blocks/](https://distill.pub/2018/building-blocks/)
  prefs: []
  type: TYPE_NORMAL
- en: A good way to think about Google’s blocks of interpretability is as a model
    that detects insights about the decisions of a neural network at different levels
    of abstraction from the basic computation graph to the final decision.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/ab0251e72755e0e8defcd6c54b948099.png)'
  prefs: []
  type: TYPE_IMG
- en: Source: [https://distill.pub/2018/building-blocks/](https://distill.pub/2018/building-blocks/)
  prefs: []
  type: TYPE_NORMAL
- en: Google research of deep neural network interpretability is not only a theoretical
    exercise. The research group accompanied the paper with the release of [Lucid](https://github.com/tensorflow/lucid),
    a neural network visualization library that allow developers to make the sort
    lucid feature visualizations that illustrate the decisions made by individual
    segments of a neural network. Google also released [colab notebooks](https://github.com/tensorflow/lucid#notebooks).
    These notebooks make it extremely easy to use Lucid to create Lucid visualization
    in an interactive environment.
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/ai-in-plain-english/understanding-how-neural-networks-think-ca7d9c1f079).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Learning by Forgetting: Deep Neural Networks and the Jennifer Aniston Neuron](/2020/06/learning-forgetting-deep-neural-networks-jennifer-aniston.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Uber’s Ludwig is an Open Source Framework for Low-Code Machine Learning](/2020/06/uber-ludwig-open-source-framework-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Google Unveils TAPAS, a BERT-Based Neural Network for Querying Tables Using
    Natural Language](/2020/05/google-tapas-bert-neural-network-querying-natural-language.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[10 Simple Things to Try Before Neural Networks](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Neural Networks Don''t Lead Us Towards AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Explainable Forecasting and Nowcasting with State-of-the-art Deep…](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Image Classification with Convolutional Neural Networks (CNNs)](https://www.kdnuggets.com/2022/05/image-classification-convolutional-neural-networks-cnns.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Comprehensive Survey on Trustworthy Graph Neural Networks:…](https://www.kdnuggets.com/2022/05/comprehensive-survey-trustworthy-graph-neural-networks-privacy-robustness-fairness-explainability.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
