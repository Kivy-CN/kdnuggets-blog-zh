- en: Top 10 Must-Know Machine Learning Algorithms for Data Scientists – Part 1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学家必知的前 10 种机器学习算法 – 第 1 部分
- en: 原文：[https://www.kdnuggets.com/2021/04/top-10-must-know-machine-learning-algorithms-data-scientists-1.html](https://www.kdnuggets.com/2021/04/top-10-must-know-machine-learning-algorithms-data-scientists-1.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/04/top-10-must-know-machine-learning-algorithms-data-scientists-1.html](https://www.kdnuggets.com/2021/04/top-10-must-know-machine-learning-algorithms-data-scientists-1.html)
- en: '[comments](#comments)![Simple ML](../Images/a5441b69289f9fd2607606e638f57b99.png)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments) ![简单机器学习](../Images/a5441b69289f9fd2607606e638f57b99.png)'
- en: 'Image source: [A Comprehensive Survey on Graph Neural Networks](https://arxiv.org/abs/1901.00596v4)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[图神经网络的全面调查](https://arxiv.org/abs/1901.00596v4)
- en: Wading through the vast array of information for data science newcomers on [machine
    learning algorithms](https://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html)
    can be a difficult and time-consuming process. Figuring out which algorithms are
    widely used and which are simply novel or interesting is not just an academic
    exercise; determining where to concentrate your time and focus during the early
    days of study can determine the difference between getting your career off to
    a quick start and experiencing an extended ground delay.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据科学新手来说，梳理大量关于[机器学习算法](https://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html)的信息可能是一个困难且耗时的过程。弄清楚哪些算法被广泛使用，哪些算法只是新颖或有趣的，并不仅仅是一个学术性练习；在学习初期确定集中时间和精力的地方，可以决定你的职业是快速起步还是经历较长的延迟。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的快车道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的 IT 工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: How, exactly, does one discriminate between immediately useful algorithms worthy
    of attention and, well, not so much? Determining how to come up with an objective
    list of machine learning algorithms of authority is inherently difficult, but
    it seems that going directly to practitioners for their feedback may be the optimal
    approach. Such a process presents a whole host of difficulties of its own, as
    could be easily imagined, and results of such surveys are few and far between.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如何准确区分值得关注的立即有用的算法与那些不那么有用的算法？确定如何制定一个权威的机器学习算法列表本质上是困难的，但直接向从业者征求反馈可能是最佳的方法。这样的过程本身带来了各种困难，正如可以想象的那样，类似的调查结果也少之又少。
- en: KDnuggets, however, [conducted such a survey](https://www.kdnuggets.com/2019/04/top-data-science-machine-learning-methods-2018-2019.html)
    within the last few years, asking respondents "Which Data Science / Machine Learning
    methods and algorithms did you use in 2018/2019 for a real-world application?"
    Of course, as alluded to above, such surveys are hindered by self-selection, lack
    of verifiability of participants, trust in the actual responses, etc., but this
    survey represents the most recent, most far-reaching, and ultimately the best
    source we have available to us.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，KDnuggets [在最近几年进行了这样的调查](https://www.kdnuggets.com/2019/04/top-data-science-machine-learning-methods-2018-2019.html)，询问受访者“你在
    2018/2019 年间在实际应用中使用了哪些数据科学/机器学习方法和算法？” 当然，如上所述，这些调查受到自我选择、参与者的可验证性、对实际响应的信任等的限制，但这项调查代表了我们目前拥有的最新、最广泛且最优质的来源。
- en: And, so, this is the source we will use to identify the top 10 machine learning
    algorithms being used, and, as such, the top 10 must-know algorithms for data
    scientists. The first 5 of these top 10 must-know algorithms are introduced below,
    with a brief overview of what the algorithms are and how they work. We will followup
    with part 2 in the coming weeks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这就是我们将用于识别当前使用的前10个机器学习算法的来源，作为数据科学家必须了解的前10个算法。这些前10个必须了解的算法中的前5个将在下面介绍，并简要概述这些算法及其工作原理。接下来的几周，我们将发布第2部分。
- en: 'Note the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意以下几点：
- en: we have skipped over entries which do not map to machine learning algorithms
    (e.g. "visualization", "descriptive statistics", "text analytics")
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们跳过了与机器学习算法不相关的条目（例如“可视化”，“描述性统计”，“文本分析”）
- en: we have treated the entry "regression" separately as both "linear regression"
    and "logistic regression"
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将“回归”条目单独处理为“线性回归”和“逻辑回归”
- en: we have swapped the entry "ensemble methods" for "bagging", a particular ensemble
    method, as a few other ensemble methods are also individually represented in this
    list
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将“集成方法”条目替换为“bagging”，一种特定的集成方法，因为此列表中还有其他一些集成方法也单独表示
- en: we have skipped over any neural network entries, as these techniques combine
    architecture with a variety of different algorithms to achieve their goals, aspects
    which are beyond the scope of this discussion
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们跳过了任何神经网络条目，因为这些技术结合了架构和各种不同的算法来实现其目标，这些方面超出了本讨论的范围
- en: 1\. Linear Regression
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1\. 线性回归
- en: Regression is a time-tested manner for approximating relationships among a given
    collection of data, and the recipient of unhelpful naming via [unfortunate circumstances](https://en.wikipedia.org/wiki/Regression_analysis#History).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 回归是一种经过时间考验的近似给定数据集合之间关系的方法，并因[不幸的情况](https://en.wikipedia.org/wiki/Regression_analysis#History)而得到了不太合适的命名。
- en: Linear regression is a simple algebraic tool which attempts to find the “best”
    (straight, for the purposes of this discussion) line fitting 2 or more attributes,
    with one attribute (**simple** linear regression), or a combination of several
    (**multiple** linear regression), being used to predict another, the class attribute.
    A set of training instances is used to compute the linear model, with one attribute,
    or a set of attributes, being plotted against another. The model then attempts
    to identify where new instances would lie on the regression line, given a particular
    class attribute.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是一种简单的代数工具，试图找到适合2个或多个属性的“最佳”（为本讨论目的所用的直线）直线，其中一个属性（**简单**线性回归）或多个属性的组合（**多元**线性回归）用于预测另一个，即类属性。利用一组训练实例来计算线性模型，其中一个属性或一组属性与另一个属性进行绘制。然后，该模型尝试确定在给定特定类属性的情况下，新实例将位于回归线上何处。
- en: 'The relationship between predictor and response variables (*x* and *y*, respectively)
    can be expressed via the following equation, with which everyone reading this
    is undoubtedly familiar: ![Equation](../Images/ef82f76aa240383ae202e5777644a284.png).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 预测变量和响应变量（分别为*x*和*y*）之间的关系可以通过以下方程式表达，所有阅读此文的人无疑都很熟悉：![方程式](../Images/ef82f76aa240383ae202e5777644a284.png)。
- en: '*m* and *b* are the regression coefficients, and denote line slope and y-intercept,
    respectively. I suggest you check your elementary school algebra notes if you
    are having trouble recalling :)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*m*和*b*是回归系数，分别表示直线的斜率和y截距。如果你难以回忆，建议你查看小学代数笔记 :)'
- en: 'The equation for multiple linear regression is generalized for *n* attributes
    is:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有*n*属性的多元线性回归，方程式为：
- en: '![Equation](../Images/2228a2c2a643f05fb98f4fdefcbe888c.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![方程式](../Images/2228a2c2a643f05fb98f4fdefcbe888c.png)'
- en: 2\. Decision Trees
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2\. 决策树
- en: 'In machine learning, decision trees have been used for decades as effective
    and easily understandable data classifiers (contrast that with the numerous blackbox
    classifiers in existence). Over the years, a number of decision tree algorithms
    have resulted from research, 3 of the most important, influential, and well-used
    being:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，决策树作为有效且易于理解的数据分类器已经使用了几十年（与现存的众多黑箱分类器相比）。多年来，研究产生了许多决策树算法，其中最重要、最有影响力和使用最广泛的3种是：
- en: Iterative Dichotimiser 3 (ID3) - Ross Quinlan's precursor to the C4.5
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代二分法3（ID3） - Ross Quinlan的C4.5的前身
- en: Iterative Dichotimiser 3 (ID3) - Ross Quinlan's precursor to the C4.5
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代二分法3（ID3） - Ross Quinlan的C4.5的前身
- en: C4.5 - one of the most popular classifiers of all time, also from Quinlan
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C4.5 - 所有时间中最受欢迎的分类器之一，也是Quinlan的作品
- en: CART - independently invented around the same time as C4.5, also still very
    popular
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CART - 与C4.5几乎同时独立发明的，至今仍然非常受欢迎
- en: ID3, C4.5, and CART all adopt a top-down, recursive, divide-and-conquer approach
    to decision tree induction.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ID3、C4.5和CART都采用自上而下、递归、分治的方法进行决策树归纳。
- en: Over the years, C4.5 has become a benchmark against which the performance of
    newer classification algorithms are often measured. Quinlan’s original implementation
    contains proprietary code; however, various open-source versions have been coded
    over the years, including the (at one time very popular) Weka Machine Learning
    Toolkit's J48 decision tree algorithm.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，C4.5已成为衡量新分类算法性能的基准。昆兰的原始实现包含专有代码；然而，随着时间的推移，已经编写了各种开源版本，包括（曾经非常受欢迎的）Weka机器学习工具包中的J48决策树算法。
- en: 'The model- (or tree-) building aspect of decision tree classification algorithms
    are composed of 2 main tasks: tree induction and tree pruning. **Tree induction**
    is the task of taking a set of pre-classified instances as input, deciding which
    attributes are best to split on, splitting the dataset, and recursing on the resulting
    split datasets until all training instances are categorized. **Tree pruning**
    involves reducing the size of decision trees by eliminating branches which are
    redundant or non-essential to the classification process.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树分类算法的模型（或树）构建方面包括两个主要任务：树归纳和树剪枝。**树归纳**是将一组预分类实例作为输入，决定最适合分裂的属性，分裂数据集，并在结果分裂的数据集上递归，直到所有训练实例都被分类。**树剪枝**涉及通过消除冗余或对分类过程非必要的分支来减少决策树的大小。
- en: While building our tree, the goal is to split on the attributes which create
    the purest child nodes possible, which would keep to a minimum the number of splits
    that would need to be made in order to classify all instances in our dataset.
    This purity is generally measured by one of a number of different attribute selection
    measures.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建我们的树时，目标是对那些能够创建最纯净子节点的属性进行分裂，这将使得为分类数据集中所有实例所需的分裂次数最小化。这种纯度通常通过多种不同的属性选择度量方法来衡量。
- en: There are 3 prominent attribute selection measures for decision tree induction,
    each paired with one of the 3 prominent decision tree classifiers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有3种显著的属性选择度量方法用于决策树归纳，每种都与3种显著的决策树分类器之一配对。
- en: Information gain - used in the ID3 algorithm
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息增益 - 用于ID3算法
- en: Gain ratio - used in the C4.5 algorithm
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增益率 - 用于C4.5算法
- en: Gini index - used in the CART algorithm
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基尼指数 - 用于CART算法
- en: In ID3, purity is measured by the concept of information gain, based on the
    work of Claude Shannon, which relates to how much would need to be known about
    a previously-unseen instance in order for it to be properly classified. In practice,
    this is measured by comparing entropy, or the amount of information needed to
    classify a single instance of a current dataset partition, to the amount of information
    to classify a single instance if the current dataset partition were to be further
    partitioned on a given attribute.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在ID3中，纯度是通过信息增益的概念来测量的，这基于克劳德·香农的工作，涉及到对一个之前未见的实例进行适当分类所需知道多少信息。实际上，这是通过比较当前数据集分区中单个实例的熵，或分类所需的信息量，与在给定属性上进一步划分当前数据集分区后对单个实例进行分类所需的信息量来进行测量的。
- en: One of the most important takeaways from this discussion should be that decision
    tree is a classification strategy as opposed to some single, well-defined classification
    algorithm. While we have had a brief look at 3 separate decision tree algorithm
    implementations, there are a variety of ways to configure different aspects of
    each. Indeed, any algorithm which seeks to classify data, and takes a top-down,
    recursive, divide-and-conquer approach to crafting a tree-based graph for subsequent
    instance classification, regardless of any other particulars (including attribution
    split selection methods and optional tree-pruning approach) would be considered
    a decision tree.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从这次讨论中最重要的收获之一应该是，决策树是一种分类策略，而不是某种单一、明确的分类算法。虽然我们简要了解了3种不同的决策树算法实现，但每种算法的不同方面有多种配置方式。确实，任何旨在对数据进行分类，并采取自上而下、递归、分治方法来构建一个树状图以供后续实例分类的算法，不论其他细节（包括属性分裂选择方法和可选的树剪枝方法），都被视为决策树。
- en: 3\. *k*-means Clustering
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3\. *k*-均值聚类
- en: '*k*-means is a simple, yet often effective, approach to clustering. Traditionally,
    *k* data points from a given dataset are randomly chosen as cluster centers, or
    centroids, and all training instances are plotted and added to the closest cluster.
    After all instances have been added to clusters, the centroids, representing the
    mean of the instances of each cluster are re-calculated, with these re-calculated
    centroids becoming the new centers of their respective clusters.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-均值是一种简单但常常有效的聚类方法。传统上，从给定数据集中随机选择*k*个数据点作为集群中心或质心，并将所有训练实例绘制并添加到最接近的集群中。在所有实例都被添加到集群后，代表每个集群实例均值的质心将重新计算，这些重新计算的质心将成为各自集群的新中心。'
- en: At this point, all cluster membership is reset, and all instances of the training
    set are re-plotted and re-added to their closest, possibly re-centered, cluster.
    This iterative process continues until there is no change to the centroids or
    their membership, and the clusters are considered settled.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，所有集群成员资格被重置，训练集中的所有实例被重新绘制并重新添加到其最近的、可能重新中心化的集群中。这个迭代过程持续进行，直到质心或其成员资格没有变化，集群被认为是稳定的。
- en: 'Convergence is achieved once the re-calculated centroids match the previous
    iteration’s centroids, or are within some preset margin. The measure of distance
    is generally Euclidean in *k*-means, which, given 2 points in the form of (*x,
    y*), can be represented as:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦重新计算的质心与上一轮迭代的质心匹配，或在某个预设的范围内，就达到了收敛。在*k*-均值中，距离的度量通常是欧几里得距离，对于形式为（*x, y*）的两个点，可以表示为：
- en: '![Image](../Images/6caecd241af5ba356579714b862cb7ae.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6caecd241af5ba356579714b862cb7ae.png)'
- en: Of technical note, especially in the era of parallel computing, iterative clustering
    in *k*-means is serial in nature; however, the distance calculations within an
    iteration need not be. Therefore, for sets of a significant size, distance calculations
    are a worthy target for parallelization in the *k*-means clustering algorithm.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 技术上值得注意的是，尤其是在并行计算时代，*k*-均值中的迭代聚类是串行的；然而，迭代中的距离计算不一定是串行的。因此，对于大规模的数据集，距离计算是*k*-均值聚类算法中值得并行化的目标。
- en: Also, while we have just described a particular method of clustering using cluster
    mean values and Euclidean distance, it should not be difficult to imagine a variety
    of possible other methods using, say, cluster median values or any number of other
    distance metrics (cosine, Manhattan, Chebyshev, etc.)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，虽然我们刚刚描述了使用集群均值和欧几里得距离的特定聚类方法，但可以想象，还有许多其他可能的方法，例如，使用集群中位数值或任何其他距离度量（余弦、曼哈顿、切比雪夫等）。
- en: 4\. Bagging
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4\. 袋装法
- en: In a given scenario, it may prove more useful than not to to chain or group
    classifiers together, using the techniques of voting, weighting, or combination
    to pursue the most accurate classifier possible. Ensemble learners are classifiers
    which provide this functionality in a variety of ways. Bagging is an example of
    an ensemble learner.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在特定场景中，将分类器串联或分组在一起，使用投票、加权或组合技术以追求最准确的分类器，可能比不这样做更有用。集成学习器是提供这种功能的分类器。袋装法就是一个集成学习器的例子。
- en: 'Bagging operates by simple concept: build a number of models, observe the results
    of these models, and settle on the majority result. I recently had an issue with
    the rear axle assembly in my car: I wasn''t sold on the diagnosis of the dealership,
    and so I took it to 2 other garages, both of which agreed the issue was something
    different than the dealership suggested. Voila. Bagging in action.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装法的基本概念很简单：构建多个模型，观察这些模型的结果，并选择多数结果。我最近遇到了汽车后轴组件的问题：我对经销商的诊断不满意，因此我将其带到另外2个车库，两者都认为问题与经销商建议的不同。瞧，这就是袋装法的实际应用。
- en: I only visited 3 garages in my example, but you could imagine that accuracy
    would likely increase if I had visited tens or hundreds of garages, especially
    if my car's problem was one of a more complex nature. This holds true for bagging,
    and the bagged classifier often is significantly more accurate than single constituent
    classifiers. Also note that the type of constituent classifier used are inconsequential;
    the resulting model can be made up of any single classifier type.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我在示例中只访问了3个车库，但你可以想象，如果我访问了数十个或数百个车库，准确性可能会提高，特别是当我的汽车问题较为复杂时。这对于袋装法也适用，袋装分类器通常比单一的组成分类器准确得多。同时，请注意，所使用的组成分类器类型无关紧要；最终的模型可以由任何单一分类器类型组成。
- en: Bagging is short for *bootstrap aggregation*, so named because it takes a number
    of samples from the dataset, with each sample set being regarded as a bootstrap
    sample. The results of these bootstrap samples are then aggregated.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging是*自助聚合*的缩写，因为它从数据集中提取多个样本，每个样本集被视为自助样本。然后将这些自助样本的结果进行聚合。
- en: 5\. Support Vector Machines
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5\. 支持向量机
- en: As mentioned, Support Vector Machines (SVMs) are a particular classification
    strategy. SMVs work by transforming the training dataset into a higher dimension,
    which is then inspected for the optimal separation boundary, or boundaries, between
    classes. In SVMs, these boundaries are referred to as hyperplanes, which are identified
    by locating support vectors, or the instances that most essentially define classes,
    and their margins, which are the lines parallel to the hyperplane defined by the
    shortest distance between a hyperplane and its support vectors. Consequently,
    SVMs are able to classify both linear and nonlinear data.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，支持向量机（SVM）是一种特殊的分类策略。SVM通过将训练数据集转换到更高维度来工作，然后检查该高维度数据中类之间的最佳分隔边界或边界。在SVM中，这些边界被称为超平面，它们通过定位支持向量（即最本质地定义类的实例）及其边距（即平行于超平面、由超平面与其支持向量之间的最短距离定义的线）来确定。因此，SVM能够对线性和非线性数据进行分类。
- en: The grand idea with SVMs is that, with a high enough number of dimensions, a
    hyperplane separating a particular class from all others can always be found,
    thereby delineating dataset member classes. When repeated a sufficient number
    of times, enough hyperplanes can be generated to separate all classes in *n*-dimensional
    space. Importantly, SVMs look not just for any separating hyperplane but the maximum-margin
    hyperplane, being that which resides equidistance from respective class support
    vectors.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: SVM的宏伟思想是，通过足够高的维度，可以始终找到一个超平面，将特定类别与所有其他类别分开，从而划分数据集成员类别。当重复足够多次时，可以生成足够的超平面来分隔*n*维空间中的所有类别。重要的是，SVM不仅寻找任何分隔超平面，而是寻找最大间隔超平面，即与各自类别的支持向量等距的超平面。
- en: Central to SVMs is the kernel trick, which enables comparison between original
    data and potential higher dimensionality feature space transformations of this
    data in order to determine if such transformation facilitates the separation of
    data classes, which would put us in a position where hyperplanes could separate
    classes. The kernel trick is essential as it can make potentially otherwise intractable
    transformation computations feasible, with these comparisons of original and high-dimensional
    feature space date enabled by pairwise similarity comparisons.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: SVM的核心是核技巧，它可以比较原始数据和可能的高维特征空间转换，以确定这种转换是否有助于数据类的分隔，从而使超平面能够分隔类。核技巧至关重要，因为它可以使潜在的难以处理的转换计算变得可行，这些比较通过成对的相似性比较来实现。
- en: When data is linearly-separable, there are many separating lines that could
    be chosen. Such a hyperplane can be expressed as
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据是线性可分时，可以选择许多分隔线。这样的超平面可以表示为
- en: '![Image](../Images/e8afe37e7dc375b373dea975c6278eba.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/e8afe37e7dc375b373dea975c6278eba.png)'
- en: where *W* is a vector of weights, b is a scalar bias, and X are the training
    data (of the form (*x[1], x[2]*, ...)). If our bias, *b*, is thought of as an
    additional weight, the equation can be expressed as
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *W* 是一个权重向量，b 是一个标量偏置，X 是训练数据（形式为 (*x[1], x[2]*, ...）。如果将我们的偏置 *b* 看作是一个额外的权重，那么该方程可以表示为
- en: '![Image](../Images/a94de6f6481000d3f547e2a04a772acb.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/a94de6f6481000d3f547e2a04a772acb.png)'
- en: which can, in turn, be rewritten as a pair of linear inequalities, solving to
    greater or less than zero, either of which satisfied indicate that a particular
    point lies above or below the hyperplane, respectively. Finding the maximum-margin
    hyperplane, or the hyperplane that resides equidistance from the support vectors,
    is done by combining the linear inequalities into a single equation and transforming
    them into a constrained quadratic optimization problem, using a Lagrangian formulation
    and solving using Karush-Kuhn-Tucker conditions, which is beyond what we will
    discuss here.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以进一步被重写为一对线性不等式，解为大于或小于零，其中任何一个满足条件的情况表示一个特定点位于超平面上方或下方。找到最大间隔超平面，或者说与支持向量等距的超平面，通过将线性不等式合并为一个方程，并将其转换为约束二次优化问题，使用拉格朗日公式并通过Karush-Kuhn-Tucker条件解决，这是我们在这里不讨论的内容。
- en: So there you have 5 must-know algorithms for data scientists, with *must-know*
    being defined as the most utilized in practice, and the most utilized informed
    by the results of a recent KDnuggets survey. We will followup with the second
    part of this list in the coming weeks. Until then, we hope you find this list
    and simple explanations helpful, and that you are able to branch out to learn
    more about each of these form here.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这里有5种数据科学家必须了解的算法，其中*必须了解*的定义为在实践中最常用的算法，而最常用的算法则基于最近KDnuggets调查的结果。我们将在接下来的几周内跟进此列表的第二部分。在此之前，我们希望你能发现这个列表和简单的解释对你有帮助，并且你能够从中扩展，进一步了解每一种算法。
- en: '**Related**:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关**：'
- en: '[How to Explain Key Machine Learning Algorithms at an Interview](/2020/10/explain-machine-learning-algorithms-interview.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在面试中解释关键机器学习算法](/2020/10/explain-machine-learning-algorithms-interview.html)'
- en: '[The 10 Algorithms Machine Learning Engineers Need to Know](/2016/08/10-algorithms-machine-learning-engineers.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习工程师需要知道的10种算法](/2016/08/10-algorithms-machine-learning-engineers.html)'
- en: '[Top 10 Machine Learning Algorithms for Beginners](/2017/10/top-10-machine-learning-algorithms-beginners.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[初学者的十大机器学习算法](/2017/10/top-10-machine-learning-algorithms-beginners.html)'
- en: More On This Topic
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[KDnuggets News, June 22: Primary Supervised Learning Algorithms…](https://www.kdnuggets.com/2022/n25.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets新闻，6月22日：主要的监督学习算法…](https://www.kdnuggets.com/2022/n25.html)'
- en: '[20 Questions (with Answers) to Detect Fake Data Scientists: ChatGPT…](https://www.kdnuggets.com/2023/01/20-questions-detect-fake-data-scientists-chatgpt-1.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[检测虚假数据科学家的20个问题（附答案）：ChatGPT…](https://www.kdnuggets.com/2023/01/20-questions-detect-fake-data-scientists-chatgpt-1.html)'
- en: '[20 Questions (with Answers) to Detect Fake Data Scientists: ChatGPT…](https://www.kdnuggets.com/2023/02/20-questions-detect-fake-data-scientists-chatgpt-2.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[检测虚假数据科学家的20个问题（附答案）：ChatGPT…](https://www.kdnuggets.com/2023/02/20-questions-detect-fake-data-scientists-chatgpt-2.html)'
- en: '[A Beginner''s Guide to the Top 10 Machine Learning Algorithms](https://www.kdnuggets.com/a-beginner-guide-to-the-top-10-machine-learning-algorithms)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[初学者指南：十大机器学习算法](https://www.kdnuggets.com/a-beginner-guide-to-the-top-10-machine-learning-algorithms)'
- en: '[Primary Supervised Learning Algorithms Used in Machine Learning](https://www.kdnuggets.com/2022/06/primary-supervised-learning-algorithms-used-machine-learning.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中使用的主要监督学习算法](https://www.kdnuggets.com/2022/06/primary-supervised-learning-algorithms-used-machine-learning.html)'
- en: '[Essential Machine Learning Algorithms: A Beginner''s Guide](https://www.kdnuggets.com/2021/05/essential-machine-learning-algorithms-beginners.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基础机器学习算法：初学者指南](https://www.kdnuggets.com/2021/05/essential-machine-learning-algorithms-beginners.html)'
