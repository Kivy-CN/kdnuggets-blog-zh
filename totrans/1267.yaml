- en: 'Essential Math for Data Science: Information Theory'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/01/essential-math-data-science-information-theory.html](https://www.kdnuggets.com/2021/01/essential-math-data-science-information-theory.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)[![Image](../Images/e6b504358bea8f63e781d7dbce4b4afc.png)](https://www.essentialmathfordatascience.com/?utm_source=kdnuggets&utm_medium=blog&utm_campaign=kdnuggets_scalars_vectors)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'The field of *information theory* studies the quantification of information
    in signals. In the context of machine learning, some of these concepts are used
    to characterize or compare probability distributions. The ability to quantify
    information is also used in the decision tree algorithm, to select the variables
    associated with the maximum information gain. The concepts of entropy and cross-entropy
    are also important in machine learning because they lead to a widely used loss
    function in classification tasks: the cross-entropy loss or log loss.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Shannon Information
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Intuition**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step to understanding information theory is to consider the concept
    of quantity of information associated with a random variable. In information theory,
    this quantity of information is denoted as II and is called the *Shannon information*, *information
    content*, *self-information*, or *surprisal*. The main idea is that likely events
    convey less information than unlikely events (which are thus more *surprising*).
    For instance, if a friend from Los Angeles, California tells you: “It is sunny
    today”, this is less informative than if she tells you: “It is raining today”.
    For this reason, is can be helpful to think of the Shannon information as the
    amount of surprise associated with an outcome. You’ll also see in this section
    why it is also a quantity of information, and why likely events are associated
    with less information.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '**Units of Information**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Common units to quantity information are the *nat* and the *bit*. These quantities
    are based on logarithm functions. The word *nat*, short for *natural unit of information* is
    based on the natural logarithm, while the bit, short for “binary digit”, is based
    on base-two logarithms. The bit is thus a rescaled version of the nat. The following
    sections will mainly use the bit and base-two logarithms in formulas, but replacing
    it with the natural logarithm would just change the unit from bits to nats.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Bits represent variables that can take two different states (0 or 1). For instance,
    1 bit is needed to encode the outcome of a coin flip. If you flip two coins, you’ll
    need at least two bits to encode the result. For instance, 00 for HH, 01 for HT,
    10 for TH and 11 for TT. You could use other codes, such as 0 for HH, 100 for
    HT, 101 for TH and 111 for TT. However, this code uses a larger number of bits
    in average (considering that the probability distribution of the four events is
    uniform, as you’ll see)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 比特表示可以有两种不同状态（0 或 1）的变量。例如，1 比特用于编码一次掷硬币的结果。如果你掷两次硬币，你至少需要两个比特来编码结果。例如，00 代表
    HH，01 代表 HT，10 代表 TH，11 代表 TT。你可以使用其他编码，比如 0 代表 HH，100 代表 HT，101 代表 TH，111 代表
    TT。然而，这种编码在平均情况下使用的比特数量更多（假设四种事件的概率分布是均匀的，正如你将看到的）。
- en: Let’s take an example to see what a bit describes. Erica sends you a message
    containing the result of three coin flips, encoding ‘heads’ as 0 and ‘tails’ as
    1\. There are 8 possible sequences, such as 001, 101, etc. When you receive a
    message of one bit, it divides your uncertainty by a factor of 2\. For instance,
    if the first bit tells you that the first roll was ‘heads’, the remaining possible
    sequences are 000, 001, 010, and 011\. There are only 4 possible sequences instead
    of 8\. Similarly, receiving a message of two bits will divide your uncertainty
    by a factor of 2222; a message of three bits, by a factor of 2323, and so on.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个例子来看一下一个比特描述了什么。Erica 发送给你一个消息，包含三次硬币掷出的结果，用 0 编码“正面”，用 1 编码“反面”。共有 8 种可能的序列，例如
    001、101 等。当你收到一个比特的消息时，它将你的不确定性减少了一半。例如，如果第一个比特告诉你第一次掷硬币是“正面”，那么剩下的可能序列是 000、001、010
    和 011。这样，可能的序列只有 4 种，而不是 8 种。同样，收到两个比特的消息将把你的不确定性减少到 4；收到三个比特的消息，将把不确定性减少到 8，依此类推。
- en: Note that we talk about “useful information”, but it is possible that the message
    is redundant and convey less information with the same number of bits.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们谈论的是“有用的信息”，但可能消息是冗余的，并且在相同的比特数下传达的信息更少。
- en: '**Example**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例**'
- en: Let’s say that we want to transmit the result of a sequence of eight tosses.
    You’ll allocate one bit per toss. You thus need eight bits to encode the sequence.
    The sequence might be for instance “00110110”, corresponding to HHTTHTTH(four
    “heads” and four “tails”).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想传输一系列八次投掷的结果。你将为每次投掷分配一个比特。因此，你需要八个比特来编码这个序列。这个序列可能是“00110110”，对应于 HHTTHTTH（四个“正面”和四个“反面”）。
- en: 'However, let’s say that the coin is biased: the chance to get “tails” is only
    1 over 8\. You can find a better way to encode the sequence. One option is to
    encode the index of the outcomes “tails”: it will take more than one bit, but
    ‘tails’ occurs only for a small proportion of the trials. With this strategy,
    you allocate more bits to rare outcomes.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，假设这个硬币是偏置的：得到“反面”的概率只有 1/8。你可以找到一种更好的方式来编码这个序列。一种选择是编码“反面”结果的索引：这将需要多个比特，但“反面”仅在少数试验中出现。通过这种策略，你可以将更多比特分配给稀有结果。
- en: 'This example illustrates that more predictable information can be compressed:
    a biased coin sequence can be encoded with a smaller amount of information than
    a fair coin. This means that Shannon information depends on the probability of
    the event.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子说明了更可预测的信息可以被压缩：一个偏置的硬币序列可以用比公平硬币序列更少的信息进行编码。这意味着香农信息依赖于事件的概率。
- en: '**Mathematical Description**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**数学描述**'
- en: Shannon information encodes this idea and converts the probability that an event
    will occur into the associated quantity of information. Its characteristics are
    that, as you saw, likely events are less informative than unlikely events and
    also that information from different events is additive (if the events are independent).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 香农信息编码了这个想法，并将事件发生的概率转化为相关的信息量。它的特点是，如你所见，可能事件的资讯少于不可能事件的信息，并且不同事件的信息是可加性的（如果事件是独立的）。
- en: 'Mathematically, the function *I(x)* is the information of the event *X=x* that
    takes the outcome as input and returns the quantity of information. It is a monotonically
    decreasing function of the probability (that is, a function that never increases
    when the probability increases). Shannon information is described as:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，函数*I(x)* 是事件 *X=x* 的信息，它以结果作为输入，返回信息的数量。它是概率的单调递减函数（即，当概率增加时，函数值从不增加）。香农信息可以描述为：
- en: '![Equation](../Images/fd55e7d2ac8f63e589fdb3a404b72031.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![方程式](../Images/fd55e7d2ac8f63e589fdb3a404b72031.png)'
- en: The result is a lower bound on the number of bits, that is, the minimum amount
    of bits needed to encode a sequence with an optimal encoding.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'The logarithm of a product is equal to the sum of the elements: ![Equation](../Images/5377a4150756f11be3e984841fce238b.png).
    This property is useful to encode the additive property of the Shannon information.
    The probability of occurrence of two events is their individual probabilities
    multiplied together (because they are independent, as you saw in [Essential Math
    for Data Science](https://bit.ly/3njFklS)):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/5ea29c31029510d5ecdebfd8225fb711.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: This means that the information corresponding to the probability of occurrence
    of two events *P(x,y)* equals the information corresponding to *P(x)* added to
    the information corresponding to *P(y)*. The information of independent events
    add together.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot this function for a range of probability between 0 and 1 to see
    the shape of the curve:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Figure](../Images/2ae17b840d7be726e113f81d3473127c.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1: The quantity of information is given by the negative logarithm of
    the probability.*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in Figure 1, the negative logarithm function encodes the idea
    that a very unlikely event (probability around 0) is associated with a large quantity
    of information and a likely event (probability around 1) is associated with a
    quantity of information around 0.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Since you used a base-two logarithm `np.log2()`, the information *I(x)* is measured
    in *bits*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Entropy
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You saw that Shannon information gives the amount of information associated
    with a single probability. You can also calculate the amount of information of
    a discrete distribution with the *Shannon entropy*, also called *information entropy*,
    or simply *entropy*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Consider for instance a biased coin, where you have a probability of 0.8 of
    getting ‘heads’.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is your distribution: you have a probability of 0.8 of getting ‘heads’
    and a probability of *1 − 0.8 = 0.2* of getting ‘tails’.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These probabilities are respectively associated with a Shannon information
    of:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Equation](../Images/73a405551777ec222441120448d774c4.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: and
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/c08c196af95ea74c7b9ce92f2acb466e.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: 'Landing ‘heads’ is associated with an information around 0.32 and landing ‘tails’
    to 2.32\. However, you don’t have the same number of ‘heads’ and ‘tails’ in average,
    so you must weight the Shannon information of each probability with the probability
    itself. For instance, if you want to transmit a message with the results of, say,
    100 trials, you’ll need around 20 times the amount of information corresponding
    to ‘tails’ and 80 times the amount of information corresponding to ‘heads’. You
    get:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Equation](../Images/cc39e9187d2c55ba6e924101a1997506.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: and
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/50d392d9bee729268e62964131585acf.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: 'The sum of these expressions gives you:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Equation](../Images/db046008bb43208d8c491754cd730c6a.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: The average number of bits required to describe a series of events from this
    distribution is 0.72 bits.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, you can consider the entropy as a summary of the information
    associated with the probabilities of the discrete distribution:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: You calculate the Shannon information of each probability of your distribution.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You weight the Shannon information with the corresponding probability.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You sum the weighted results.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Mathematical Formulation**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'The entropy is the expectation of the information with respect to the probability
    distribution. Remember from [Essential Math for Data Science](https://bit.ly/3njFklS) that
    the expectation is the mean value you’ll get if you draw a large number of samples
    from the distribution:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/f1c1585d04229c82239e2e4538c0f3bc.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: with the random variable *X* having *n* possible outcomes, *x[i]* being the *i*th
    possible outcome corresponding to a probability of *P(x[i])*. The expected value
    of the information of a distribution corresponds to the average of the information
    you’ll get.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the formula of the expectation and the Shannon information, the entropy
    of the random variable *X* is defined as:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/175b64d27a5282aee38051690df3cb20.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: The entropy gives you the average quantity of information that you need to encode
    the states of the random variable *X*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Note that the input of the function *H(X)* is the random variable *X* while *I(x)* denotes
    the Shannon information of the event *X=x*. You can also refer to the entropy
    of the random variable *X* which is distributed with respect to *P(x)* as *H(P)*.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '**Illustration**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take an example: as illustrated in Figure 2 in the bottom panel, you
    have a discrete distribution with four possible outcomes, associated with probabilities
    0.4, 0.4, 0.1, and 0.1, respectively. As you saw previously, the information is
    obtained by log transforming the probabilities (top panel). This is the last part
    of the entropy formula: *log2P(x)*.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/ecdaafee5cb8e80c770cf60f20c8bdd8.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2: Illustration of the entropy as the weighted sum of the Shannon information.*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Each of these transformed probabilities is weighted by the corresponding raw
    probability. If an outcome occurs frequently, it will give more weight into the
    entropy of the distribution. This means that a low probability (like 0.1 in Figure
    2) gives a large amount of information (3.32 bits) but has less influence on the
    final result. A larger probability (like 0.4 in Figure 2) is associated with less
    information (1.32 bits as shown in Figure 2) but has more weight.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary Entropy Function**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: In the example of a biased coin, you calculated the entropy of a Bernoulli process
    (more details about the Bernoulli distribution in [Essential Math for Data Science](https://bit.ly/3njFklS)).
    In this special case, the entropy is called the *binary entropy function*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: To characterize the binary entropy function, you’ll calculate the entropy of
    a biased coin described by various probability distributions (from heavily biased
    in favor of “tails” to heavily biased in favor of “heads”).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by creating a function to calculate the entropy of a distribution
    that takes an array with the probabilities as input and returns the corresponding
    entropy:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can also use `entropy()` from `scipy.stats`, where you can specify the base
    of the logarithm used to calculate the entropy. Here, I have used the base-two
    logarithm.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take the example of a fair coin, with a probability of 0.5 of landing
    ‘heads’. The distribution is thus 0.5 and 1 − 0.5 = 0.5\. Let’s use the function
    we just defined to calculate the corresponding entropy is:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The function calculates the sum of `P * np.log2(P)` over each element of the
    array you use as input. Using an array as input As you saw in the previous section,
    you can expect a lower entropy for biased coin. Let’s plot the entropy for various
    coin biases, from a coin landing only as ‘tails’ to a coin landing only as ‘heads’:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Figure](../Images/aae2e343d1abfe59210c0290a25e6b00.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3: Entropy as a function of the probability to land “heads”.*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3 shows that the entropy increases until you reach the more uncertain
    condition: that is, when the probability of landing ‘heads’ equals the probability
    of landing ‘tails’.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '**Differential Entropy**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: The entropy of a continuous distribution is called *differential entropy*. It
    is an extension of the entropy for discrete distribution, but it doesn’t satisfy
    the same requirements. The issue is that values have probability tending to zero
    with continuous distributions, and encoding this would require a number of bits
    tending to infinity.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'It is defined as:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/b083a67b3e20d937ac2bad40e81ff2de.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: Differential entropy can be negative. The reason is that, as you saw in [Essential
    Math for Data Science](https://bit.ly/3njFklS), continuous distributions are not
    probabilities but probability densities, meaning that they don’t satisfy the requirements
    of probabilities. For instance, they are not constrained to be lower than 1\.
    This has the consequence that *p(x)* can take positive values larger than 1 and *log2p(x)* can
    take positive values (leading to negative values because of the negative sign).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Cross Entropy
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The concept of entropy can be used to compare two probability distributions:
    this is called the *cross entropy* between two distributions, which measures how
    much they differ.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to calculate the information associated with the probabilities of
    a distribution *Q(x)*, but instead of weighting according to *Q(x)* as with the
    entropy, you weight according to the other distribution *P(x)*. Note that you
    compare two distributions concerning the same random variable *X*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: You can also consider cross-entropy as the expected quantity of information
    of events drawn from *P(x)* when you use *Q(x)* to encode them.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'This is mathematically expressed as:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/6c038f4a404d3dc7c1d1390a0f2c42fd.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: Let’s see how it works.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/8d3bb7ddd0e728ab031ab27ec6c29bb8.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4: Illustration of the cross entropy as the Shannon information of *Q(x)* weighted
    according to the distribution of *P(x)*.*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4 shows two different situations to illustrate the cross entropy. In
    the left, you have two identical distributions *P(x)* (in blue) and *Q(x)* (in
    red). Their cross entropy is equal to the entropy because the information of *Q(x)* is
    weighted according to the distribution of *P(x)*, which is similar to *Q(x)*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: However, in the right panel, *P(x)* and *Q(x)* are different. This results in
    a larger cross entropy, because probabilities associated with a large quantity
    of information have a small weight, while probabilities associated with a small
    quantity of information have large weights.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: The cross entropy can’t be smaller than the entropy. Still in the right panel,
    you can see that, when the probability *Q(x)* is larger than *P(x)* (and thus
    associated with a lower amount of information), it is counterbalanced by the low
    weights (resulting in low weights and low information). These low weights will
    be compensated with larger weights in other probabilities from the distribution
    (resulting in large weights and large information).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Note also that both distributions *P(x)* and *Q(x)* must have the same *support* (that
    is the same set of values that the random variable can take associated with positive
    probabilities).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, the cross entropy is minimum when the distributions are identical.
    As you’ll see in [0.1.4](https://hadrienj.github.io/posts/Essential-Math-information_theory/#sec:ch12_section_kullback_leibler_divergence),
    this property makes the cross entropy a useful metric. Note also that the result
    is different according to the distribution you choose as a reference: *H(P,Q)
    ≠ H(Q,P)*.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '**Cross Entropy as a Loss Function**'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, cross entropy is used as a loss function called the *cross
    entropy loss* (also called the *log loss*, or the *logistic loss*, because it
    is used in logistic regression).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/ec9bac288843cb9f238f77e33842fbf7.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5: Cross entropy can be used to compare the true distribution (probability
    of 1 for the correct class and 0 otherwise) and the distribution estimated by
    the model.*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Say you want to build a model that classifies three different bird species from
    audio samples. As illustrated in Figure 5, the audio samples are converted in
    features (here spectrograms) and the possible classes (the three different birds)
    are *one-hot encoded*, that is, encoded as 1 for the correct class and 0 otherwise.
    Furthermore, the machine learning model outputs probabilities for each class.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: To learn how to classify the birds, the model needs to compare the estimated
    distribution *Q(x)* (given by the model) and the true distribution *P(x)*. The
    cross entropy loss is computed as the cross entropy between *P(x)* and *Q(x)*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5 shows that the true class corresponding the sample you consider in
    this example is “European Green Woodpecker”. The model outputs a probability distribution
    and you’ll compute the cross entropy loss associated with this estimation. Figure
    6 shows both distributions.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/930fad886c9a9c232fbeec17a0e06f0c.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6: Comparison of the true distribution *P(x)* and the estimated distribution *Q(x)*.*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s manually calculate the cross entropy between these two distributions:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/82b182d56293446bad0e1e9ce19444cd.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: The natural logarithm is used in the cross-entropy loss instead of the base-two
    logarithm, but the principle is the same. In addition, note the use of *H(P,Q)* instead
    of *H(Q,P)* because the reference is the distribution *P*.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Since you one-hot encoded the classes (1 for the true class and 0 otherwise),
    the cross entropy is simply the negative logarithm of the estimated probability
    for the true class.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary Classification: Log Loss**'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'In machine learning, the cross entropy is widely used as a loss for binary
    classification: the log loss.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Since the classification is binary, the only possible outcomes are *y* (the
    true label corresponds to the first class) and *1−y* (the true label corresponds
    to the second class). Similarly, you have the estimated probability of the first
    class *y^* and the estimated probability of the second class *1−y^*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'From the formula of the cross entropy, *∑x* corresponds here to the sum over
    the two possible outcomes (*y* and *1−y*). You have:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/96f46583f71303d20eed4e2126b5c458.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: which is the formula of the log loss.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Kullback-Leibler Divergence (KL Divergence)
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You saw that the cross entropy is a value which depends on the similarity of
    two distributions, with the smaller cross entropy value corresponding to identical
    distributions. You can use this property to calculate the *divergence* between
    two distributions: you compare their cross entropy with the situation where the
    distributions are identical. This divergence is called the *Kullback-Leibler divergence* (or
    simply the *KL divergence*), or the *relative entropy*.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, the KL divergence is the supplemental amount of information associated
    with the encoding of the distribution *Q(x)* compared to the true distribution *P(x)*.
    It tells you how different the two distributions are.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the KL divergence between two distributions *P(x)* and *Q(x)*,
    denoted as *DKL(P||Q)*, is expressed as the difference between the cross entropy
    of *P(x)* and *Q(x)* and the entropy of *P(x)*:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/dfc5ed69b7928089bd2e65221771093b.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
- en: 'Replacing with the expressions of the cross entropy and the entropy, you get:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/d3c5d4d17b8f3f50b5db64937c389c65.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: The KL divergence is always non-negative. Since the entropy *H(P)* is identical
    to the cross entropy *H(P,P)*, and because the smallest cross entropy is between
    identical distributions *(H(P,P)H(P,P))*, *H(P,Q)* is necessarily larger than *H(P)*.
    In addition, the KL divergence is equal to zero when the two distributions are
    identical.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: However, the cross entropy is not symmetrical. Comparing a distribution *P(x)* to
    a distribution *Q(x)* can be different than comparing a distribution *Q(x)* to *P(x)* –
    which implies that you can’t consider the KL divergence to be a distance.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Hadrien Jean](https://hadrienj.github.io/)** is a machine learning
    scientist. He owns a Ph.D in cognitive science from the Ecole Normale Superieure,
    Paris, where he did research on auditory perception using behavioral and electrophysiological
    data. He previously worked in industry where he built deep learning pipelines
    for speech processing. At the corner of data science and environment, he works
    on projects about biodiversity assessement using deep learning applied to audio
    recordings. He also periodically creates content and teaches at Le Wagon (data
    science Bootcamp), and writes articles in his blog ([hadrienj.github.io](http://hadrienj.github.io)).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://hadrienj.github.io/posts/Essential-Math-information_theory/).
    Reposted with permission.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[Essential Math for Data Science: The Poisson Distribution](/2020/12/introduction-poisson-distribution-data-science.html)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Essential Math for Data Science: Probability Density and Probability Mass
    Functions](/2020/12/essential-math-data-science-probability-density-probability-mass-functions.html)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Essential Math for Data Science: Integrals And Area Under The Curve](/2020/11/essential-math-data-science-integrals-area-under-curve.html)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
