- en: Natural Language Processing Library for Apache Spark – free to use
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/11/natural-language-processing-library-apache-spark.html](https://www.kdnuggets.com/2017/11/natural-language-processing-library-apache-spark.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By [David Talby](https://twitter.com/davidtalby?lang=en), CTO Usermind**.'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark is a general-purpose cluster computing framework, with native support
    for distributed SQL, streaming, graph processing, and machine learning. Now, the
    Spark ecosystem also has an [Spark Natural Language Processing library](https://nlp.johnsnowlabs.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Get it on [GitHub](https://github.com/johnsnowlabs/spark-nlp) or begin with
    the [quickstart tutorial](https://nlp.johnsnowlabs.com/quickstart.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The John Snow Labs NLP Library is under the Apache 2.0 license, written in
    Scala with no dependencies on other NLP or ML libraries. It natively extends the [Spark
    ML Pipeline API](https://blog.insightdatascience.com/spark-pipelines-elegant-yet-powerful-7be93afcdd42).
    You will benefit from:'
  prefs: []
  type: TYPE_NORMAL
- en: Unmatched runtime performance, since processing is done directly on Spark DataFrames
    without any copying and taking full advantage of Spark’s caching, execution planning
    and optimized binary data format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Frictionless reuse of existing Spark libraries, including distributed topic
    modelling, word embeddings, n-gram calculation, string distance calculations and
    more.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Higher productivity by using a unified API across the Natural Language Understanding,
    Machine Learning & Deep Learning parts of a data science pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: “With John Snow Labs NLP, we’re delivering on the promise to enable customers
    to take advantage of the latest open source technology and academic breakthroughs
    in data science, all within a high performance, enterprise-grade code base.”,
    said the founding team. In addition, “John Snow Labs NLP encompasses a wide range
    of highly efficient Natural Language Understanding tools for text mining, question
    answering, chat bots, fact extraction, topic modelling or Search, running at a
    scale and performance that has not been available to date.”
  prefs: []
  type: TYPE_NORMAL
- en: 'The framework provides the concepts of annotators, and comes out of the box
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stemmer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lemmatizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entity Extractor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Date Extractor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part of Speech Tagger
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Named Entity Recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentence boundary detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spell checker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, given the tight integration with Spark ML, there is a lot more
    you can use right away when building your NLP pipelines. This includes word embeddings,
    topic modeling, stop word removal, a variety of feature engineering functions
    (tf-idf, n-grams, similarity metrics, …) and using NLP annotations as features
    in machine learning workflows. If you’re not familiar with these terms, this [guide
    to understanding NLP tasks](https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/) is
    a good start.
  prefs: []
  type: TYPE_NORMAL
- en: '![High Performance NLP Spark](../Images/90627fd777f2f745bd23d3810e3cfd11.png)'
  prefs: []
  type: TYPE_IMG
- en: Our virtual team has been building commercial software that heavily depends
    on natural language understanding for several years now. As such, we have hands-on
    experience with [spaCy](https://spacy.io/), [CoreNLP](https://stanfordnlp.github.io/CoreNLP/), [OpenNLP](https://opennlp.apache.org/), [Mallet](https://mallet.cs.umass.edu/), [GATE](https://gate.ac.uk/), [Weka](https://www.cs.waikato.ac.nz/ml/weka/), [UIMA](https://uima.apache.org/), [nltk](https://www.nltk.org/), [gensim](https://radimrehurek.com/gensim/), [Negex](https://blulab.chpc.utah.edu/content/contextnegex), [word2vec](https://code.google.com/archive/p/word2vec/), [GloVe](https://nlp.stanford.edu/projects/glove/),
    and a few others.
  prefs: []
  type: TYPE_NORMAL
- en: We are big fans, and the many places where we’ve imitated these libraries are
    intended as the sincere form of flattery that they are. But we’ve also banged
    our heads too many times against their limitations – when we’ve had to deliver
    scalable, high-performance, high-accuracy software for real production use.
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance**'
  prefs: []
  type: TYPE_NORMAL
- en: The first of three top-level requirements we tackled is runtime performance.
    You’d think this was largely a solved problem with the advent of [spaCy and its
    public benchmarks](https://spacy.io/docs/api/) which reflect a well thought-out
    and masterfully implemented set of tradeoffs. However, when building Spark applications
    on top of it, you’d still get unreasonably subpar throughput.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand why, consider that an NLP pipeline is always just a part of a
    bigger data processing pipeline: For example, question answering involves loading
    training, data, transforming it, applying NLP annotators, building features, training
    the value extraction models, evaluating the results (train/test split or cross-validation),
    and hyper-parameter estimation.'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting your data processing framework (Spark) from your NLP frameworks means
    that most of your processing time gets spent serializing and copying strings.
  prefs: []
  type: TYPE_NORMAL
- en: 'A great parallel is [TensorFrames](https://github.com/databricks/tensorframes) –
    which greatly improves the performance of running TensorFlow workflows on Spark
    data frames. This image is credited to [Tim Hunter’s excellent TensorFrames overview](https://www.slideshare.net/databricks/tensorframes-google-tensorflow-on-apache-spark):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Spark Communication](../Images/65ff80686b4d84caff0c9d06344ba82f.png)'
  prefs: []
  type: TYPE_IMG
- en: Both Spark and TensorFlow are optimized to the extreme for performance and scale.
    However, since DataFrames live in the JVM and TensorFlow runs in a Python process,
    any integration between the two frameworks means that every object has to be serialized,
    go through inter-process communication (!) in both ways, and copied at least twice
    in memory. TensorFrames public benchmarks report a 4x speedup by just copying
    the data within the JVM process (and much more when using GPUs).
  prefs: []
  type: TYPE_NORMAL
- en: 'We see the same issue when using spaCy with Spark: Spark is highly optimized
    for loading & transforming data, but running an NLP pipeline requires copying
    all the data outside the Tungsten optimized format, serializing it, pushing it
    to a Python process, running the NLP pipeline (this bit is lightning fast), and
    then re-serializing the results back to the JVM process. This naturally kills
    any performance benefits you would get from Spark’s caching or execution planner,
    requires at least twice the memory, and doesn’t improve with scaling. Using CoreNLP
    eliminates the copying to another process, but still requires copying all text
    from the data frames and copying the results back in.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So our first order of business is to perform the analysis directly on the optimized
    data frames, as Spark ML already does (credit: [ML Pipelines introduction post
    by Databricks](https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Spark Pipeline Model](../Images/bbf4f7ab8883466cc9278044acac0b2f.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Ecosystem**'
  prefs: []
  type: TYPE_NORMAL
- en: Our second core requirement was frictionless reuse of existing Spark libraries.
    Part of it is our own pet peeve – why does every NLP library out there have to
    build its own topic modeling and word embedding implementations? The other part
    is pragmatic – we’re a small team under tight deadlines and need to make the most
    of what’s already there.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we started thinking about a Spark NLP library, we first asked Databricks
    to point us to whoever is already building one. When the answer came there there
    isn’t one, the next ask was to help us make sure the design and API of the library
    fully meet Spark ML’s API guidelines. The result of this collaboration is that
    the library is a seamless extension of Spark ML, so that for example you can build
    this kind of pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this code, the document assembler, tokenizer, and stemmer come from the Spark
    NLP library – the com.jsl.nlp.* package. The TF hasher, IDF and labelDeIndex all
    come from MLlib’sorg.apache.spark.ml.feature.* package. The dtree stage is a spark.ml.classification.DecisionTreeClassifier.
  prefs: []
  type: TYPE_NORMAL
- en: All these stages run within one pipeline that is configurable, serializable
    and testable in the exact same way. They also run on a data frame without any
    copying of data (unlike [spark-corenlp](https://spark-packages.org/package/databricks/spark-corenlp)),
    enjoying Spark’s signature in-memory optimizations, parallelism and distributed
    scale out.
  prefs: []
  type: TYPE_NORMAL
- en: What this means is the John Snow Labs NLP library comes with fully distributed,
    heavily tested and optimized [topic modeling](https://medium.com/zero-gravity-labs/lda-topic-modeling-in-spark-mllib-febe84b9432), [word
    embedding](https://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec),
    n-gram generation, and cosine similarity out of the box. We didn’t have to build
    them though – they come with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Most importantly, it means that your NLP and ML pipelines are now unified. The
    above code sample is typical, in the sense that it’s not “just” an NLP pipeline
    – NLP is used to generate features which are then used to train a decision tree.
    This is typical of question answering tasks. A more complex example would also
    apply named entity recognition, filtered by POS tags and coreference resolution;
    train a random forest, taking into account both NLP-based features and structured
    features from other sources; and use grid search for hyper-parameter optimization.
    Being able to use a unified API pays dividends whenever you need to test, reproduce,
    serialize or publish such a pipeline – even beyond the performance and reuse benefits.
  prefs: []
  type: TYPE_NORMAL
- en: '**Enterprise Grade**'
  prefs: []
  type: TYPE_NORMAL
- en: Our third core requirement is delivering a mission-critical, enterprise-grade
    NLP library. We make our living building production software. Many of the most
    popular NLP packages today have academic roots – which shows in design trade-offs
    that favor ease of prototyping over runtime performance, breadth of options over
    simple minimalist API’s, and downplaying of scalability, error handling, frugal
    memory consumption and code reuse.
  prefs: []
  type: TYPE_NORMAL
- en: 'The John Snow Labs NLP library is written in Scala. It includes Scala and Python
    APIs for use from Spark. It has no dependency on any other NLP or ML library.
    For each type of annotator, we do an academic literature review to find the state
    of the art, have a team discussion and decide which algorithm(s) to implement.
    Implementations are evaluated on three criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy** – there’s no point in a great framework, if it has sub-par algorithms
    or models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance** – runtime should be on par or better than any public benchmark.
    No one should have to give up accuracy because annotators don’t run fast enough
    to handle a streaming use case, or don’t scale well in a cluster setting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trainability or Configurability** – NLP is an inherently domain-specific
    problem. Different grammars and vocabularies are used in social media posts vs.
    academic papers vs. SEC filings vs. electronic medical records vs. newspaper articles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The library is already in use in enterprise projects – which means that the
    first level of bugs, refactoring, unexpected bottlenecks and serialization issues
    have been resolved. Unit test coverage and [reference documentation](https://nlp.johnsnowlabs.com/components.html) are
    at a level that made us comfortable to make the code open source.
  prefs: []
  type: TYPE_NORMAL
- en: '[John Snow Labs](https://www.johnsnowlabs.com/) is the company leading and
    sponsoring the development of the Spark NLP library. The company provides commercial
    support, indemnification and consulting for it. This provides the library with
    long-term financial backing, a funded active development team, and a growing stream
    of real-world projects that drives robustness and roadmap prioritization.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Getting involved**'
  prefs: []
  type: TYPE_NORMAL
- en: If you need NLP for your current project, head to the [John Snow Labs NLP for
    Apache Spark homepage](https://nlp.johnsnowlabs.com/) or quickstart guide and
    give it a try. Prebuilt maven central (Scala) and pip install (Python) versions
    are available. Send us questions or feedback to nlp@johnsnowlabs.com or via [Twitter](https://twitter.com/johnsnowlabs),
    [LinkedIn](https://www.linkedin.com/company/10349856/) or [Facebook](https://www.facebook.com/JohnSnowLabsInc).
  prefs: []
  type: TYPE_NORMAL
- en: '[Let us know](https://ida-johnsnowlabs.youcanbook.me) what functionality you
    need next.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the requests we’re getting, and are looking for more feedback
    to design and prioritize:'
  prefs: []
  type: TYPE_NORMAL
- en: Provide a SparkR client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide “Spark-free” Java and Scala versions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a state of the art annotator for coreference resolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a state of the art annotators for polarity detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a state of the art annotator for temporal reasoning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publish sample applications for common use cases such as question answering,
    text summarization or information retrieval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train and publish models for new domains or languages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publish reproducible, peer reviewed accuracy and performance benchmarks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’d like to extend or contribute to the library, start by cloning the [John
    Snow Labs NLP for Spark GitHub](https://www.github.com/johnsnowlabs/spark-nlp) repository.
    We use pull requests and GitHub’s issue tracker to manage code changes, bugs and
    features. The library is still in its early days and we highly appreciate contribution
    and feedback of any kind.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [David Talby](https://www.linkedin.com/in/davidtalby/)** is a Consulting
    CTO at Usermind, specializing in applying big data and data science in healthcare.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related**'
  prefs: []
  type: TYPE_NORMAL
- en: '[**[eBook] A Gentle Introduction to Apache Spark(tm)**](https://www.kdnuggets.com/2017/11/databricks-ebook-gentle-introduction-apache-spark.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Building a Wikipedia Text Corpus for Natural Language Processing**](https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Search Millions of Documents for Thousands of Keywords in a Flash**](https://www.kdnuggets.com/2017/09/search-millions-documents-thousands-keywords.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Introducing the Testing Library for Natural Language Processing](https://www.kdnuggets.com/2023/04/introducing-testing-library-natural-language-processing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[N-gram Language Modeling in Natural Language Processing](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Free Books on Natural Language Processing to Read in 2023](https://www.kdnuggets.com/2023/06/5-free-books-natural-language-processing-read-2023.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25 Free Books to Master SQL, Python, Data Science, Machine…](https://www.kdnuggets.com/25-free-books-to-master-sql-python-data-science-machine-learning-and-natural-language-processing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Free Courses to Master Natural Language Processing](https://www.kdnuggets.com/5-free-courses-to-master-natural-language-processing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Transfer Learning for Image Recognition and Natural Language Processing](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
