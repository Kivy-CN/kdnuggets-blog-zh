- en: Feature Selection – All You Ever Wanted To Know
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择 – 你想知道的一切
- en: 原文：[https://www.kdnuggets.com/2021/06/feature-selection-overview.html](https://www.kdnuggets.com/2021/06/feature-selection-overview.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/06/feature-selection-overview.html](https://www.kdnuggets.com/2021/06/feature-selection-overview.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Danny Butvinik](https://www.linkedin.com/in/danny-butvinik-5a6357212/),
    Chief Data Scientist at NICE Actimize**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者 [Danny Butvinik](https://www.linkedin.com/in/danny-butvinik-5a6357212/)，NICE
    Actimize 的首席数据科学家**'
- en: '![](../Images/d08fe77a346ebb0542891d31e27c5d24.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d08fe77a346ebb0542891d31e27c5d24.png)'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析水平'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 在 IT 方面支持你的组织'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*source: TnT Woodcraft.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*来源：TnT 木工。*'
- en: Feature selection, as a dimensionality reduction technique, aims to choose a
    small subset of the relevant features from the original features by removing irrelevant,
    redundant, or noisy features. Feature selection usually can lead to better learning
    performance, higher learning accuracy, lower computational cost, and better model
    interpretability. This article focuses on the feature selection process and provides
    a comprehensive and structured overview of feature selection types, methodologies,
    and techniques both from the data and algorithm perspectives.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择作为一种降维技术，旨在通过去除无关、冗余或噪声特征，从原始特征中选择一个相关特征的小子集。特征选择通常可以带来更好的学习性能、更高的学习准确性、更低的计算成本和更好的模型可解释性。本文重点介绍了特征选择过程，并提供了从数据和算法角度对特征选择类型、方法和技术的全面结构化概述。
- en: '![](../Images/90ef63aa643bd74e6672d89e00e26b50.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90ef63aa643bd74e6672d89e00e26b50.png)'
- en: '*Figure 1: High-level taxonomy for feature selection.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1：特征选择的高级分类。*'
- en: This article considers the feature selection process. The problem is important
    because a large number of features in a dataset, comparable to or higher than
    the number of samples, leads to model overfitting, which in turn leads to poor
    results on the validation datasets. Additionally, constructing models from datasets
    with many features is more computationally demanding.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章考虑了特征选择过程。这个问题很重要，因为数据集中大量的特征数量， comparable to 或高于样本数量，会导致模型过拟合，从而在验证数据集上结果不佳。此外，从具有许多特征的数据集中构建模型计算上更为繁重。
- en: A strongly relevant feature is always necessary for an optimal feature subset,
    and it cannot be removed without affecting the original conditional target distribution.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 强相关特征对于最优特征子集始终是必要的，它不能被移除而不影响原始条件目标分布。
- en: '**The aim of feature selection is to maximize relevance and minimize redundancy.**'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**特征选择的目的是最大化相关性并最小化冗余。**'
- en: Feature selection methods can be used in data pre-processing to achieve efficient
    data reduction. This is useful for finding accurate data models. Since an exhaustive
    search for an optimal feature subset is infeasible in most cases, many search
    strategies have been proposed in the literature. The usual applications of feature
    selection are in classification, clustering, and regression tasks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择方法可以用于数据预处理，以实现高效的数据减少。这对于找到准确的数据模型非常有用。由于对最优特征子集进行穷举搜索在大多数情况下不可行，因此文献中提出了许多搜索策略。特征选择的通常应用包括分类、聚类和回归任务。
- en: What Is Feature Selection
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是特征选择
- en: All machine learning workflows depend on feature engineering, which comprises
    feature extraction and feature selection that are fundamental building blocks
    of modern machine learning pipelines. Despite the fact that feature extraction
    and feature selection processes share some overlap, often, these terms are erroneously
    equated. Feature extraction is the process of using domain knowledge to extract
    new variables from raw data that make machine learning algorithms work. The feature
    selection process is based on selecting the most consistent, relevant, and non-redundant
    features.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所有机器学习工作流都依赖于特征工程，其中包括特征提取和特征选择，这些是现代机器学习管道的基本构建块。尽管特征提取和特征选择过程有一些重叠，但这些术语通常被错误地等同起来。特征提取是利用领域知识从原始数据中提取新的变量，使机器学习算法能够正常工作。特征选择过程则基于选择最一致、最相关和不冗余的特征。
- en: 'The objectives of feature selection techniques include:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择技术的目标包括：
- en: simplification of models to make them easier to interpret by researchers/users
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简化模型，使研究人员/用户更容易解释
- en: shorter training times
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更短的训练时间
- en: avoiding the curse of dimensionality
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免维度诅咒
- en: enhanced generalization by reducing overfitting (formally, reduction of variance)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过减少过拟合（正式地说，是减少方差）来增强泛化能力
- en: Dataset size reduction is more important nowadays because of the plethora of
    developed analysis methods that are at the researcher's disposal, while the size
    of an average dataset keeps growing both with respect to the number of features
    and samples.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于研究人员可以使用大量已开发的分析方法，数据集大小缩减现在变得更加重要，同时平均数据集的大小在特征和样本数量上都在不断增长。
- en: '![](../Images/df1a4f4160873ed067e0e6229dad5fd8.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df1a4f4160873ed067e0e6229dad5fd8.png)'
- en: '*Figure 2: Illustration of feature selection and data size reduction in tabular
    data.*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2：表格数据中特征选择和数据大小缩减的说明。*'
- en: What Makes Some Feature Representations Better Than Others?
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么使一些特征表示优于其他表示？
- en: 'Regardless of the technological approach to feature representation, there is
    a common question that haunts data scientists in most machine learning workflows:
    What makes some feature representations better than others?'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 不论特征表示的技术方法如何，有一个共同的问题困扰着大多数机器学习工作流中的数据科学家：是什么使一些特征表示优于其他表示？
- en: This might seem like an insane question considering modern machine learning
    problems are using hundreds of thousands or even millions of features that are
    impossible to interpret by domain experts.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是一个疯狂的问题，因为现代机器学习问题使用了数十万甚至百万个特征，而这些特征对于领域专家来说几乎无法解释。
- en: 'While there is no trivial answer to our target questions, there are some general
    principles that we can follow. In general, there are three key desired properties
    in feature representations:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对我们目标问题没有简单的答案，但我们可以遵循一些通用原则。一般来说，特征表示中有三个关键的期望属性：
- en: disentangling of causal factors
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潜在因子的解缠
- en: easy to model
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 易于建模
- en: works well with regularization strategies
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与正则化策略配合良好
- en: Fundamentally, solid representations include features that correspond to the
    underlying causes of the observed data. More specifically, this thesis links the
    quality of representations to structures in which different features and directions
    correspond to different causes in the underlying dataset so that the representation
    is able to disentangle one cause from another.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，良好的表示包括与观察数据的潜在原因相对应的特征。更具体地说，本论文将表示的质量与不同特征和方向对应于潜在数据集中不同原因的结构相关联，以便表示能够将一种原因与另一种原因区分开来。
- en: Another leading indicator of good representation is the simplicity of modeling.
    For a given machine learning problem/dataset, we can find many representations
    that separate the underlying causal factors, but they could be brutally hard to
    model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个良好表示的领先指标是建模的简洁性。对于给定的机器学习问题/数据集，我们可以找到许多将潜在因子分开的表示，但这些表示可能非常难以建模。
- en: Supervised Feature Selection Methods
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有监督特征选择方法
- en: Supervised feature selection methods are classified into four types, based on
    the interaction with the learning model, such as the Filter, Wrapper, Hybrid,
    and Embedded Methods.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督的特征选择方法根据与学习模型的交互分为四类，如过滤器、包装器、混合方法和嵌入方法。
- en: '![](../Images/a4bdec7cac389a3e230c291c01ca8fb2.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4bdec7cac389a3e230c291c01ca8fb2.png)'
- en: '*Figure 3: Extended taxonomy of supervised feature selection methods and techniques.*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3：有监督特征选择方法和技术的扩展分类。*'
- en: '**Filter Methodology**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**过滤方法**'
- en: In the Filter method, features are selected based on statistical measures. It
    is independent of the learning algorithm and requires less computational time.
    Information gain, chi-square test, Fisher score, correlation coefficient, and
    variance threshold are some of the statistical measures used to understand the
    importance of the features.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在过滤方法中，特征是基于统计测量选择的。它独立于学习算法，并且需要较少的计算时间。信息增益、卡方检验、费舍尔得分、相关系数和方差阈值是一些用于了解特征重要性的统计测量方法。
- en: The Filter methodology uses the selected metric to identify irrelevant attributes
    and also filter out redundant columns from the models. It gives the option of
    isolating selected measures that enrich a model. The columns are ranked following
    the calculation of the feature scores.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤方法使用选择的度量来识别无关的属性，并从模型中过滤掉冗余的列。它提供了隔离选定度量以丰富模型的选项。列按照特征分数的计算结果进行排名。
- en: '**Wrapper Methodology**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**包裹方法**'
- en: The Wrapper methodology considers the selection of feature sets as a search
    problem, where different combinations are prepared, evaluated, and compared to
    other combinations. A predictive model is used to evaluate a combination of features
    and assign model performance scores.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 包裹方法将特征集的选择视为一个搜索问题，其中不同的组合被准备、评估，并与其他组合进行比较。使用预测模型来评估特征组合，并分配模型性能分数。
- en: The performance of the Wrapper method depends on the classifier. The best subset
    of features is selected based on the results of the classifier.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 包裹方法的性能依赖于分类器。最佳特征子集是基于分类器的结果选择的。
- en: '**Hybrid Methodology**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**混合方法**'
- en: The process of creating hybrid feature selection methods depends on what you
    choose to combine. The main priority is to select the methods you’re going to
    use, then follow their processes. The idea here is to use these ranking methods
    to generate a feature ranking list in the first step, then use the top *k* features
    from this list to perform wrapper methods. With that, we can reduce the feature
    space of our dataset using these filter-based rangers in order to improve the
    time complexity of the wrapper methods.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 创建混合特征选择方法的过程取决于你选择的组合方式。主要的优先事项是选择你要使用的方法，然后按照这些方法的流程进行。这里的思路是首先使用这些排名方法生成特征排名列表，然后使用该列表中的前
    *k* 个特征来执行包裹方法。通过这种方式，我们可以使用这些基于过滤的排名方法来减少数据集的特征空间，从而提高包裹方法的时间复杂度。
- en: '**Embedded Methodology**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**嵌入方法**'
- en: In embedded techniques, the feature selection algorithm is integrated as part
    of the learning algorithm. The most typical embedded technique is the decision
    tree algorithm. Decision tree algorithms select a feature in each recursive step
    of the tree growth process and divide the sample set into smaller subsets.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌入技术中，特征选择算法作为学习算法的一部分进行集成。最典型的嵌入技术是决策树算法。决策树算法在树生长过程的每个递归步骤中选择一个特征，并将样本集划分为更小的子集。
- en: Unsupervised Feature Selection Methods
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无监督特征选择方法
- en: Due to the scarcity of readily available labels, unsupervised feature selection
    (UFS) methods are widely adopted in the analysis of high-dimensional data. However,
    most of the existing UFS methods primarily focus on the significance of features
    in maintaining the data structure while ignoring the redundancy among features.
    Moreover, the determination of the proper number of features is another challenge.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可用标签的稀缺，无监督特征选择（UFS）方法在高维数据分析中被广泛采用。然而，大多数现有的UFS方法主要关注于特征在维持数据结构中的重要性，而忽略了特征之间的冗余。此外，确定合适的特征数量也是一个挑战。
- en: Unsupervised feature selection methods are classified into four types, based
    on the interaction with the learning model, such as the Filter, Wrapper, and Hybrid
    methods.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督特征选择方法根据与学习模型的交互被分类为四种类型，如过滤、包裹和混合方法。
- en: '![](../Images/f8d7eabb6532e166dd603a06f126e52f.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8d7eabb6532e166dd603a06f126e52f.png)'
- en: '*Figure 4: Extended taxonomy of unsupervised feature selection methods and
    techniques.*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4：无监督特征选择方法和技术的扩展分类。*'
- en: '**Filter Methodology**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**过滤方法**'
- en: Unsupervised feature selection methods based on the filter approach can be categorized
    as univariate and multivariate. Univariate methods, aka ranking-based unsupervised
    feature selection methods, use certain criteria to evaluate each feature to get
    an ordered ranking list of features, where the final feature subset is selected
    according to this order. Such methods can effectively identify and remove irrelevant
    features, but they are unable to remove redundant ones since they do not take
    into account possible dependencies among features. On the other hand, multivariate
    filter methods evaluate the relevance of the features jointly rather than individually.
    Multivariate methods can handle redundant and irrelevant features. Thus, in many
    cases, the accuracy reached by learning algorithms using the subset of features
    selected by multivariate methods is better than the one achieved by using univariate
    methods.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 基于筛选方法的无监督特征选择方法可以分为单变量和多变量两类。单变量方法，即基于排名的无监督特征选择方法，使用某些标准来评估每个特征，以获得特征的有序排名列表，然后根据这个顺序选择最终的特征子集。这些方法可以有效地识别和去除不相关的特征，但由于不考虑特征间可能的依赖关系，它们无法去除冗余特征。另一方面，多变量筛选方法联合评估特征的相关性，而不是逐一评估。多变量方法可以处理冗余和不相关的特征。因此，在许多情况下，使用多变量方法选择的特征子集所达到的学习算法准确性优于使用单变量方法的准确性。
- en: '**Wrapper Methodology**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**包装方法论**'
- en: Unsupervised feature selection methods based on the wrapper approach can be
    divided into three broad categories
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 基于包装方法的无监督特征选择方法可以分为三个大类。
- en: 'according to the feature search strategy: sequential, bio-inspired, and iterative.
    In sequential methodology, features are added or removed sequentially. Methods
    based on sequential search are easy to implement and fast.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 根据特征搜索策略：顺序、受生物启发的和迭代的。在顺序方法论中，特征是逐步添加或移除的。基于顺序搜索的方法容易实现且快速。
- en: On the other hand, a bio-inspired methodology tries to incorporate randomness
    into the search process, aiming to escape local optima.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，生物启发的方法论试图在搜索过程中引入随机性，旨在逃避局部最优。
- en: Iterative methods address the unsupervised feature selection problem by casting
    it as an estimation problem and thus avoiding a combinatorial search.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代方法将无监督特征选择问题视为估计问题，从而避免了组合搜索。
- en: Wrapper methods evaluate feature subsets using the results of a specific clustering
    algorithm. Methods developed under this approach are characterized by finding
    feature subsets that contribute to improving the quality of the results of the
    clustering algorithm used for the selection. However, the main disadvantage of
    wrapper methods is that they usually have a high computational cost, and they
    are limited to be used in conjunction with a particular clustering algorithm.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 包装方法通过特定的聚类算法的结果来评估特征子集。根据这种方法开发的技术的特点是寻找能改善所使用的聚类算法结果的特征子集。然而，包装方法的主要缺点是通常具有较高的计算成本，并且限于与特定的聚类算法一起使用。
- en: '**Hybrid Methodology**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**混合方法论**'
- en: Hybrid methods try to exploit the qualities of both approaches, filter and wrapper,
    trying to have a good compromise between efficiency (computational effort) and
    effectiveness (quality in the associated objective task when using the selected
    features).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 混合方法试图利用筛选和包装两种方法的优点，试图在效率（计算工作量）和有效性（使用所选特征时在相关目标任务中的质量）之间取得良好的平衡。
- en: 'In order to take advantage of the filter and wrapper approaches, hybrid methods
    include a filter stage where the features are ranked or selected by applying a
    measure based on the intrinsic properties of the data. While, in a wrapper stage,
    certain feature subsets are evaluated for finding the best one through a specific
    clustering algorithm. We can distinguish two types of hybrid methods: methods
    based on ranking and methods not based on the ranking of features.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用筛选和包装方法，混合方法包括一个筛选阶段，在此阶段，通过应用基于数据固有属性的度量来对特征进行排名或选择。而在包装阶段，评估某些特征子集，通过特定的聚类算法找到最佳子集。我们可以区分两种类型的混合方法：基于排名的方法和不基于特征排名的方法。
- en: Characteristics of Feature Selection Algorithms
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征选择算法的特点
- en: The purpose of a feature selection algorithms is to identify relevant features
    according to a definition of relevance. However, the notion of relevance in machine
    learning has not yet been rigorously defined on a common agreement. A primary
    definition of relevance is the notion of being relevant with respect to an objective.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择算法的目的是根据相关性的定义识别相关特征。然而，机器学习中的相关性概念尚未在共同协议下被严格定义。相关性的主要定义是相对于目标的相关性。
- en: 'There are several considerations in the literature to characterize feature
    selection algorithms. In view of these, it is possible to describe this characterization
    as a search problem in the hypothesis space as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中有几种考虑因素用于描述特征选择算法。考虑到这些因素，可以将这种描述视为假设空间中的搜索问题，如下所示：
- en: 'Search Organization: general strategy with which the space of hypothesis is
    explored.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索组织：探索假设空间的一般策略。
- en: 'Generation of Successors: mechanism by which possible variants (successor candidates)
    of the current hypothesis are proposed.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后继生成：提出当前假设的可能变体（后继候选）的机制。
- en: 'Evaluation Measure: function by which successor candidates are evaluated, allowing
    to compare different hypotheses to guide the search process.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估度量：用于评估后继候选的函数，允许比较不同的假设以指导搜索过程。
- en: '![](../Images/3134c03dd70e7be58833b7dfb14c2038.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3134c03dd70e7be58833b7dfb14c2038.png)'
- en: '*Figure 5: Characterization of a feature selection algorithm.*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5：特征选择算法的描述。*'
- en: Feature Representation in Financial Crime Domain
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 金融犯罪领域的特征表示
- en: Some of the NICE Actimize research in data science and machine learning is within
    feature selection and feature representation. In application areas such as fraud
    detection, these tasks are made more complicated by the diverse, high-dimensional,
    sparse, and mixed type data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: NICE Actimize 在数据科学和机器学习领域的研究包括特征选择和特征表示。在诸如欺诈检测等应用领域，这些任务由于数据的多样性、高维度、稀疏性和混合类型变得更加复杂。
- en: Without relying on domain knowledge, selecting the right set of features from
    data of high-dimensionality for inducing an accurate classification model is a
    tough computational challenge.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有依赖领域知识的情况下，从高维数据中选择正确的特征集以引导准确的分类模型是一个艰巨的计算挑战。
- en: Unfortunately, in data mining and financial crime domains, some data are described
    by a long array of features. It takes seemingly forever to use brute force in
    exhaustively trying every possible combination of features, and stochastic optimization
    may be a solution.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在数据挖掘和金融犯罪领域，一些数据由长数组特征描述。使用暴力破解方法穷举尝试所有可能的特征组合似乎永无止境，随机优化可能是一个解决方案。
- en: Thus, the time and context-specific nature of the financial data requires domain
    expertise to properly engineer the features while minimizing any potential information
    loss. In addition, there is no industry standard of metrics in the financial crime
    domain. That makes the process of developing feature extraction and feature selection
    extremely difficult, especially when defining an objective function for a machine
    learning model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，金融数据的时间和上下文特定性质需要领域专业知识来正确地工程化特征，同时最小化潜在的信息丢失。此外，金融犯罪领域没有行业标准的度量标准。这使得特征提取和特征选择的过程极为困难，尤其是在为机器学习模型定义目标函数时。
- en: The financial crime feature space vectors cannot be projected into a geometric
    plan because they won’t hold any underlying logic. The question is, how should
    one define the distance between two financial transactions (two high-dimensional,
    mixed type vectors)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 金融犯罪特征空间向量不能被投影到几何平面上，因为它们不会保持任何基本逻辑。问题是，如何定义两个金融交易（两个高维混合类型向量）之间的距离？
- en: 'I invite you to take that challenge and think about:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我邀请你接受这个挑战并思考：
- en: How would you define proper metrics between two sparse, heterogeneous feature
    vectors (or tensors) having different cardinalities?
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何定义具有不同基数的两个稀疏异构特征向量（或张量）之间的适当度量？
- en: What mechanism would guarantee the verification for valid features, where a
    valid feature is a feature that has great importance and represents domain logic?
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么机制可以保证验证有效特征，其中有效特征是具有重要性并且能表示领域逻辑的特征？
- en: References
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: Jovic et al., (2015) A Review of Feature Selection Methods with Applications
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jovic 等，(2015) 《特征选择方法及其应用综述》
- en: Dadaneh et al., (2016) Unsupervised probabilistic feature selection using ant
    colony optimization.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dadaneh 等人（2016）使用蚁群优化的无监督概率特征选择。
- en: Mohana (2016) A survey on feature selection stability measures
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mohana（2016）特征选择稳定性测量的调查
- en: Chandrashekar (2014) A survey on feature selection methods
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Chandrashekar（2014）特征选择方法的调查
- en: Kamkar, et al., (2015) Exploiting Feature Relationships Towards Stable Feature
    Selection.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kamkar 等人（2015）利用特征关系实现稳定的特征选择。
- en: Guo (2018) Dependence guided unsupervised feature selection.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Guo (2018) 依赖引导的无监督特征选择。
- en: Zhou, et al., (2015) A stable feature selection algorithm.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhou 等人（2015）稳定的特征选择算法。
- en: Yu (2004) Efficient Feature Selection via Analysis of Relevance and Redundancy
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Yu（2004）通过相关性和冗余分析进行高效特征选择
- en: Fernandez et al., (2020) Review of Unsupervised Feature Selection Methods
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Fernandez 等人（2020）无监督特征选择方法的综述
- en: Li, et al., (2017) Recent advances in feature selection and its applications
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Li 等人（2017）特征选择及其应用的最新进展
- en: Zhao and Huan Liu. (2007) Spectral feature selection for supervised and unsupervised
    learning
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhao 和 Huan Liu（2007）监督和无监督学习的光谱特征选择
- en: '**Related:**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[This Data Visualization is the First Step for Effective Feature Selection](https://www.kdnuggets.com/2021/06/data-visualization-feature-selection.html)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[这项数据可视化是有效特征选择的第一步](https://www.kdnuggets.com/2021/06/data-visualization-feature-selection.html)'
- en: '[Why Automated Feature Selection Has Its Risks](https://www.kdnuggets.com/2021/04/automated-feature-selection-risks.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为何自动特征选择存在风险](https://www.kdnuggets.com/2021/04/automated-feature-selection-risks.html)'
- en: '[Feature Ranking with Recursive Feature Elimination in Scikit-Learn](https://www.kdnuggets.com/2020/10/feature-ranking-recursive-feature-elimination-scikit-learn.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Scikit-Learn 中使用递归特征消除进行特征排序](https://www.kdnuggets.com/2020/10/feature-ranking-recursive-feature-elimination-scikit-learn.html)'
- en: More On This Topic
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Everything You’ve Ever Wanted to Know About Machine Learning](https://www.kdnuggets.com/2022/09/everything-youve-ever-wanted-to-know-about-machine-learning.html)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于机器学习你想知道的一切](https://www.kdnuggets.com/2022/09/everything-youve-ever-wanted-to-know-about-machine-learning.html)'
- en: '[KDnuggets News, September 14: Free Python for Data Science Course •…](https://www.kdnuggets.com/2022/n36.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，9月14日：免费数据科学 Python 课程 •…](https://www.kdnuggets.com/2022/n36.html)'
- en: '[StarCoder: The Coding Assistant That You Always Wanted](https://www.kdnuggets.com/2023/05/starcoder-coding-assistant-always-wanted.html)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[StarCoder: 你一直想要的编码助手](https://www.kdnuggets.com/2023/05/starcoder-coding-assistant-always-wanted.html)'
- en: '[Alternative Feature Selection Methods in Machine Learning](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中的替代特征选择方法](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
- en: '[Advanced Feature Selection Techniques for Machine Learning Models](https://www.kdnuggets.com/2023/06/advanced-feature-selection-techniques-machine-learning-models.html)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习模型的高级特征选择技术](https://www.kdnuggets.com/2023/06/advanced-feature-selection-techniques-machine-learning-models.html)'
- en: '[Feature Selection: Where Science Meets Art](https://www.kdnuggets.com/2021/12/feature-selection-science-meets-art.html)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[特征选择：科学与艺术的结合](https://www.kdnuggets.com/2021/12/feature-selection-science-meets-art.html)'
