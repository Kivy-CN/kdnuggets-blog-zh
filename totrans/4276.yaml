- en: Essential Linear Algebra for Data Science and Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学和机器学习的基本线性代数
- en: 原文：[https://www.kdnuggets.com/2021/05/essential-linear-algebra-data-science-machine-learning.html](https://www.kdnuggets.com/2021/05/essential-linear-algebra-data-science-machine-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/05/essential-linear-algebra-data-science-machine-learning.html](https://www.kdnuggets.com/2021/05/essential-linear-algebra-data-science-machine-learning.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '![](../Images/b8c614d6b4ed0ff4641960dcd8149bfd.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8c614d6b4ed0ff4641960dcd8149bfd.png)'
- en: '*Image by Benjamin O. Tayo.*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片来源：Benjamin O. Tayo.*'
- en: 'Linear Algebra is a branch of mathematics that is extremely useful in data
    science and machine learning. Linear algebra is the most important math skill in
    machine learning. Most machine learning models can be expressed in matrix form.
    A dataset itself is often represented as a matrix. Linear algebra is used in data
    preprocessing, data transformation, and model evaluation. Here are the topics
    you need to be familiar with:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数是数学的一个分支，在数据科学和机器学习中非常有用。线性代数是机器学习中最重要的数学技能。大多数机器学习模型可以用矩阵形式表示。数据集本身通常被表示为矩阵。线性代数用于数据预处理、数据转换和模型评估。你需要熟悉以下主题：
- en: Vectors
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量
- en: Matrices
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵
- en: Transpose of a matrix
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵的转置
- en: Inverse of a matrix
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵的逆
- en: Determinant of a matrix
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵的行列式
- en: Trace of a matrix
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵的迹
- en: Dot product
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点积
- en: Eigenvalues
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征值
- en: Eigenvectors
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征向量
- en: In this article, we illustrate the application of linear algebra in data science
    and machine learning using the tech stocks dataset, which can be found [here](https://github.com/bot13956/datasets/blob/master/tech-stocks-04-2021.csv).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们使用科技股票数据集来说明线性代数在数据科学和机器学习中的应用，数据集可以在 [这里](https://github.com/bot13956/datasets/blob/master/tech-stocks-04-2021.csv)
    找到。
- en: 1\. Linear Algebra for Data Preprocessing
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1\. 数据预处理中的线性代数
- en: We begin by illustrating how linear algebra is used in data preprocessing.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先说明线性代数在数据预处理中的应用。
- en: '**1.1 Import necessary libraries for linear algebra**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.1 导入线性代数所需的库**'
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**1.2 Read dataset and display features**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.2 读取数据集并显示特征**'
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/3f8596b4af9ce4d5551b74bde933e2a0.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f8596b4af9ce4d5551b74bde933e2a0.png)'
- en: '* **Table** **1**. Stock prices for selected stock prices for the first 16
    days in April 2021.*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '* **表 1**. 2021 年 4 月前 16 天的选定股票价格表.*'
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The ***data.shape*** function enables us to know the size of our dataset. In
    this case, the dataset has 5 features (date, AAPL, TSLA, GOOGL, and AMZN), and
    each feature has 11 observations. *Date* refers to the trading days in April 2021
    (up to April 16). AAPL, TSLA, GOOGL, and AMZN are the closing stock prices for
    Apple, Tesla, Google, and Amazon, respectively.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '***data.shape*** 函数使我们能够知道数据集的大小。在这种情况下，数据集有 5 个特征（日期、AAPL、TSLA、GOOGL 和 AMZN），每个特征有
    11 个观测值。*日期* 指的是 2021 年 4 月的交易日（截至 4 月 16 日）。AAPL、TSLA、GOOGL 和 AMZN 分别是苹果、特斯拉、谷歌和亚马逊的收盘股价。'
- en: '**1.3 Data visualization**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.3 数据可视化**'
- en: 'To perform data visualization, we would need to define ***column matrices***
    for the features to be visualized:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行数据可视化，我们需要定义 ***列矩阵*** 以便可视化特征：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/90e285abeb88fd588a8cf16e1d47e81b.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90e285abeb88fd588a8cf16e1d47e81b.png)'
- en: '***Figure** **1**. Tesla stock price for first 16 days in April 2021.*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '***图** **1**. 2021 年 4 月前 16 天的特斯拉股票价格.*'
- en: 2\. Covariance Matrix
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2\. 协方差矩阵
- en: 'The ***covariance matrix*** is one of the most important matrices in data science
    and machine learning. It provides information about co-movement (correlation)
    between features. Suppose we have a features matrix with *4* features and *n *observations
    as shown in **Table 2**:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '***协方差矩阵*** 是数据科学和机器学习中最重要的矩阵之一。它提供了特征之间共同运动（相关性）的信息。假设我们有一个特征矩阵，其中包含 *4* 个特征和
    *n* 个观测值，如 **表 2** 所示：'
- en: '![](../Images/a4aa34b2531b17268d1cb8c0401300f7.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4aa34b2531b17268d1cb8c0401300f7.png)'
- en: '***Table 2**. Features matrix with 4 variables and n observations.*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '***表 2**. 具有 4 个变量和 n 个观测值的特征矩阵.*'
- en: 'To visualize the correlations between the features, we can generate a scatter
    pairplot:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化特征之间的相关性，我们可以生成散点对比图：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/464fc9beacbdf3dbcaba2e5f4d869f6e.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/464fc9beacbdf3dbcaba2e5f4d869f6e.png)'
- en: '***Figure 2**. Scatter pairplot for selected tech stocks.*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '***图 2**. 选定科技股票的散点对比图.*'
- en: 'To quantify the degree of correlation between features (multicollinearity),
    we can compute the covariance matrix using this equation:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化特征之间的相关程度（多重共线性），我们可以使用以下公式计算协方差矩阵：
- en: '![](../Images/578f937c46c21a5f644a76465f132eed.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/578f937c46c21a5f644a76465f132eed.png)'
- en: where  and  are the mean and standard deviation of feature , respectively. This
    equation indicates that when features are standardized, the covariance matrix
    is simply the ***dot product*** between features.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中和分别是特征的均值和标准差。这个方程表明，当特征标准化时，协方差矩阵仅仅是特征之间的***点积***。
- en: 'In matrix form, the covariance matrix can be expressed as a 4 x 4 real and
    symmetric matrix:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性形式下，协方差矩阵可以表示为一个4 x 4的实对称矩阵：
- en: '![](../Images/d6ceeaef5f815f6bc570010b312b9afd.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6ceeaef5f815f6bc570010b312b9afd.png)'
- en: 'This matrix can be diagonalized by performing a ***unitary transformation***,
    also referred to as Principal Component Analysis (PCA) transformation, to obtain
    the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵可以通过执行***单位ary变换***，也称为主成分分析（PCA）变换，来对角化，得到以下结果：
- en: '![](../Images/23459b77a76392254d9bc68316d712e7.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23459b77a76392254d9bc68316d712e7.png)'
- en: Since the ***trace of a matrix*** remains invariant under a unitary transformation,
    we observe that the sum of the eigenvalues of the diagonal matrix is equal to
    the total variance contained in features X[1], X[2], X[3], and X[4].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于***矩阵的迹值***在单位ary变换下保持不变，我们观察到对角矩阵的特征值之和等于特征X[1]、X[2]、X[3]和X[4]中包含的总方差。
- en: '**2.1 Computing the covariance matrix for tech stocks**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.1 计算技术股票的协方差矩阵**'
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that this uses the ***transpose*** of the standardized matrix.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这使用了标准化矩阵的***转置***。
- en: '**2.2 Visualization of covariance matrix**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.2 协方差矩阵的可视化**'
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/9edc8225e01955301f05cdb7716b6073.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9edc8225e01955301f05cdb7716b6073.png)'
- en: '***Figure 3**. Covariance matrix plot for selected tech stocks.*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '***图3**。选定技术股票的协方差矩阵图。*'
- en: We observe from Figure 3 that AAPL correlates strongly with GOOGL and AMZN,
    and weakly with TSLA. TSLA correlates generally weakly with AAPL, GOOGL and AMZN,
    while AAPL, GOOGL, and AMZN correlate strongly among each other.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 从图3中我们观察到，AAPL与GOOGL和AMZN有较强的相关性，而与TSLA的相关性较弱。TSLA一般与AAPL、GOOGL和AMZN的相关性较弱，而AAPL、GOOGL和AMZN之间的相关性较强。
- en: '**2.3 Compute eigenvalues of the covariance matrix**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.3 计算协方差矩阵的特征值**'
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We observe that the trace of the covariance matrix is equal to the sum of the
    eigenvalues as expected.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，协方差矩阵的迹值等于特征值的总和，这与预期一致。
- en: '**2.4 Compute the cumulative variance**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.4 计算累积方差**'
- en: 'Since the trace of a matrix remains invariant under a unitary transformation,
    we observe that the sum of the eigenvalues of the diagonal matrix is equal to
    the total variance contained in features X[1], X[2], X[3], and X[4]. Hence, we
    can define the following quantities:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于矩阵的迹值在单位ary变换下保持不变，我们观察到对角矩阵的特征值之和等于特征X[1]、X[2]、X[3]和X[4]中包含的总方差。因此，我们可以定义以下量：
- en: '** ![](../Images/570a08d35b51f049cb7756615db848e7.png)**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](../Images/570a08d35b51f049cb7756615db848e7.png)**'
- en: Notice that when *p* = 4, the cumulative variance becomes equal to 1 as expected.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当*p*=4时，累积方差如预期变为1。
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We observe from the cumulative variance (***cum_var***) that 85% of the variance
    is contained in the first eigenvalue and 11% in the second. This means when PCA
    is implemented, only the first two principal components could be used, as 97%
    of the total variance is contributed by these 2 components. This can essentially
    reduce the dimensionally of the feature space from 4 to 2 when PCA is implemented.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从累积方差（***cum_var***）中观察到，85%的方差包含在第一个特征值中，11%包含在第二个特征值中。这意味着当PCA实施时，只能使用前两个主成分，因为这两个主成分贡献了97%的总方差。这实际上可以将特征空间的维度从4减少到2。
- en: 3\. Linear Regression Matrix
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3. 线性回归矩阵
- en: Suppose we have a dataset that has 4 predictor features and *n* observations,
    as shown below.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个数据集，包含4个预测特征和*n*个观测值，如下所示。
- en: '![](../Images/d4c31adfc233ea53bc3f14a4d758bd4c.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4c31adfc233ea53bc3f14a4d758bd4c.png)'
- en: '***Table 3**. Features matrix with 4 variables and n observations. Column 5
    is the target variable (y).*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '***表3**。包含4个变量和n个观测值的特征矩阵。第5列是目标变量（y）。*'
- en: We would like to build a multi-regression model for predicting the *y* values
    (column 5). Our model can thus be expressed in the form
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望建立一个多重回归模型来预测*y*值（第5列）。因此，我们的模型可以表示为以下形式
- en: '![](../Images/980b0eef242c79df297da7769f205d77.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/980b0eef242c79df297da7769f205d77.png)'
- en: In matrix form, this equation can be written as
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性形式下，这个方程可以写作
- en: '![](../Images/418c2a47995456c4c75ca9f523838474.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/418c2a47995456c4c75ca9f523838474.png)'
- en: where **X** is the ( n x 4) features matrix, **w** is the (4 x 1) matrix representing
    the regression coefficients to be determined, and **y** is the (n x 1) matrix
    containing the n observations of the target variable y.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中**X**是(n x 4)特征矩阵，**w**是(4 x 1)矩阵，表示待确定的回归系数，而**y**是(n x 1)矩阵，包含目标变量y的n个观测值。
- en: Note that **X** is a rectangular matrix, so we can’t solve the equation above
    by taking the inverse of **X**.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意**X**是一个矩形矩阵，因此我们不能通过取**X**的逆来解上述方程。
- en: To convert **X** into a square matrix, we multiple the left-hand side and right-hand
    side of our equation by the ***transpose***of **X**, that is
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将**X**转换为方阵，我们将方程的左侧和右侧都乘以**X**的***转置***，即
- en: '![](../Images/771cf589405282fb9fb9555ce2c32559.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/771cf589405282fb9fb9555ce2c32559.png)'
- en: This equation can also be expressed as
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程也可以表达为
- en: '![](../Images/688e45bebf95be9023c1f7498567013c.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/688e45bebf95be9023c1f7498567013c.png)'
- en: where
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![](../Images/7f084abfc4ac0ac2d3bf3d7b5f993eba.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f084abfc4ac0ac2d3bf3d7b5f993eba.png)'
- en: is the (4×4) regression matrix. Clearly, we observe that **R** is a real and
    symmetric matrix. Note that in linear algebra, the transpose of the product of
    two matrices obeys the following relationship
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 是(4×4)回归矩阵。显然，我们观察到**R**是一个实对称矩阵。注意，在线性代数中，两个矩阵的乘积的转置遵循以下关系
- en: '![](../Images/66b2af9229c447994a4bcf012c0d2ea5.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66b2af9229c447994a4bcf012c0d2ea5.png)'
- en: Now that we’ve reduced our regression problem and expressed it in terms of the
    (4×4) real, symmetric, and invertible regression matrix **R**, it is straightforward
    to show that the exact solution of the regression equation is then
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将回归问题简化，并将其表示为(4×4)实对称且可逆的回归矩阵**R**，很容易展示回归方程的精确解为
- en: '![](../Images/aaf18c0cbb6dd290099f5bb849faecc5.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aaf18c0cbb6dd290099f5bb849faecc5.png)'
- en: 'Examples of regression analysis for predicting continuous and discrete variables
    are given in the following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 预测连续和离散变量的回归分析示例如下：
- en: '[Linear Regression Basics for Absolute Beginners](https://pub.towardsai.net/linear-regression-basics-for-absolute-beginners-68ed9ff980ae)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[绝对初学者的线性回归基础](https://pub.towardsai.net/linear-regression-basics-for-absolute-beginners-68ed9ff980ae)'
- en: '[Building a Perceptron Classifier Using the Least Squares Method](https://github.com/bot13956/perceptron_classifier)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[使用最小二乘法构建感知机分类器](https://github.com/bot13956/perceptron_classifier)'
- en: 4\. Linear Discriminant Analysis Matrix
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4\. 线性判别分析矩阵
- en: 'Another example of a real and symmetric matrix in data science is the Linear
    Discriminant Analysis (LDA) matrix. This matrix can be expressed in the form:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学中的另一个实对称矩阵示例是线性判别分析（LDA）矩阵。这个矩阵可以表示为：
- en: '![](../Images/a8c40240eba3d7f0904f655ced9cd506.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8c40240eba3d7f0904f655ced9cd506.png)'
- en: where **S[W]** is the within-feature scatter matrix, and **S[B ]**is the between-feature
    scatter matrix. Since both matrices **S[W] **and** S[B] **are real and symmetric,
    it follows that **L** is also real and symmetric. The diagonalization of **L** produces
    a feature subspace that optimizes class separability and reduces dimensionality.
    Hence LDA is a supervised algorithm, while PCA is not.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 其中**S[W]**是特征内部散布矩阵，**S[B]**是特征间散布矩阵。由于矩阵**S[W]**和**S[B]**都是实对称的，因此**L**也是实对称的。对**L**的对角化产生了一个优化类别可分离性并减少维度的特征子空间。因此，LDA是一个监督算法，而PCA不是。
- en: 'For more details about the implementation of LDA, please see the following
    references:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 关于LDA的实施的更多细节，请参见以下参考文献：
- en: '[Machine Learning: Dimensionality Reduction via Linear Discriminant Analysis](https://medium.com/towards-artificial-intelligence/machine-learning-dimensionality-reduction-via-linear-discriminant-analysis-cc96b49d2757)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[机器学习：通过线性判别分析进行降维](https://medium.com/towards-artificial-intelligence/machine-learning-dimensionality-reduction-via-linear-discriminant-analysis-cc96b49d2757)'
- en: '[GitHub repository for LDA implementation using Iris dataset](https://github.com/bot13956/linear-discriminant-analysis-iris-dataset)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[使用鸢尾花数据集实现LDA的GitHub存储库](https://github.com/bot13956/linear-discriminant-analysis-iris-dataset)'
- en: '[Python Machine Learning by Sebastian Raschka, 3rd Edition (Chapter 5)](https://github.com/rasbt/python-machine-learning-book-3rd-edition)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sebastian Raschka的《Python机器学习》第三版（第5章）](https://github.com/rasbt/python-machine-learning-book-3rd-edition)'
- en: Summary
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: In summary, we’ve discussed several applications of linear algebra in data science
    and machine learning. Using the tech stocks dataset, we illustrated important
    concepts such as the size of a matrix, column matrices, square matrices, covariance
    matrix, transpose of a matrix, eigenvalues, dot products, etc. Linear algebra
    is an essential tool in data science and machine learning. Thus, beginners interested
    in data science must familiarize themselves with essential concepts in linear
    algebra.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们讨论了线性代数在数据科学和机器学习中的几种应用。通过使用技术股票数据集，我们阐明了矩阵的大小、列矩阵、方阵、协方差矩阵、矩阵的转置、特征值、点积等重要概念。线性代数是数据科学和机器学习中的一个重要工具。因此，初学者如果对数据科学感兴趣，必须熟悉线性代数中的基本概念。
- en: '**Related:**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[How To Overcome The Fear of Math and Learn Math For Data Science](https://www.kdnuggets.com/2021/03/overcome-fear-learn-math-data-science.html)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何克服对数学的恐惧并学习数据科学中的数学](https://www.kdnuggets.com/2021/03/overcome-fear-learn-math-data-science.html)'
- en: '[Essential Math for Data Science: Introduction to Matrices and the Matrix Product](https://www.kdnuggets.com/2021/02/essential-math-data-science-matrices-matrix-product.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学的基本数学：矩阵和矩阵乘积简介](https://www.kdnuggets.com/2021/02/essential-math-data-science-matrices-matrix-product.html)'
- en: '[Matrix Decomposition Decoded](https://www.kdnuggets.com/2020/12/matrix-decomposition-decoded.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[矩阵分解解码](https://www.kdnuggets.com/2020/12/matrix-decomposition-decoded.html)'
- en: '* * *'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的IT工作'
- en: '* * *'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解更多相关内容
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[建立一个稳固的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用管道编写干净的Python代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成为优秀数据科学家所需的5项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个初学者数据科学家应掌握的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2021年最佳ETL工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
- en: '[Top 3 Free Resources to Learn Linear Algebra for Machine Learning](https://www.kdnuggets.com/2022/03/top-3-free-resources-learn-linear-algebra-machine-learning.html)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习机器学习线性代数的3个免费资源](https://www.kdnuggets.com/2022/03/top-3-free-resources-learn-linear-algebra-machine-learning.html)'
