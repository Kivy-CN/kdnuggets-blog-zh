- en: Model Experiments, Tracking and Registration using MLflow on Databricks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/01/model-experiments-tracking-registration-mlflow-databricks.html](https://www.kdnuggets.com/2021/01/model-experiments-tracking-registration-mlflow-databricks.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Dash Desai](https://www.linkedin.com/in/dash-desai/), Director of Platform
    and Technical Evangelism at StreamSets**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Learn how StreamSets, [a modern data integration platform for DataOps](https://streamsets.com/),
    can help expedite operations at some of the most crucial stages of Machine Learning
    Lifecycle and MLOps.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Data Acquisition And Preparation
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Machine learning models are only as good as the quality of data and the size
    of datasets used to train the models. [Data has shown](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#282426e76f63) that
    data scientists spend around 80% of their time on preparing and managing data
    for analysis and 57% of the data scientists regard cleaning and organizing data
    as the least enjoyable part of their work. This further validates the idea of 
    MLOps and the need for collaboration between data scientists and data engineers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: During this crucial phase of data acquisition and preparation, data scientists
    identify what types of (trusted) datasets are needed to train models and work
    closely with data engineers to acquire data from viable data sources.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: How Can StreamSets Help
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some of the common data sources for acquiring datasets for data science projects
    include: Amazon S3, Microsoft Azure Blob Storage, Google Cloud Storage, Kafka,
    Hadoop, on-prem and cloud data warehouses. StreamSets DataOps Platform provides
    easy-to-use GUI for building smart data pipelines for streaming and batch dataflows
    for fast data ingestion of large amounts of data from distributed systems–including
    all of the common sources mentioned above.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect of the data ingestion process is the storage–in some cases, companies
    may already have a data lake or a data warehouse and in some cases they may need
    to build one. StreamSets DataOps Platform is capable of connecting to existing
    data lakes and data warehouses (on-prem or in the cloud) and also has built-in
    capabilities of creating new ones.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![Modern Data Integration for DataOps](../Images/12bff326545c71bc6f41afffdea07c7f.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: 'As part of building these data pipelines, data engineers can also perform some
    of the key transformations needed by data scientists. Some of the common transformations
    required during data preparation include: data type conversion for fields/columns/features,
    renaming fields/columns/features, joining datasets, merging datasets, repartitioning,
    dataset data format conversion (for example, JSON to Parquet for efficient downstream
    analysis in Apache Spark), etc. All of these transformations and many more are
    readily supported by StreamSets DataOps Platform.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建这些数据管道的过程中，数据工程师还可以执行数据科学家所需的一些关键转换。数据准备过程中常见的一些转换包括：字段/列/特征的数据类型转换、字段/列/特征的重命名、数据集的连接、数据集的合并、重分区、数据集数据格式转换（例如，为了在
    Apache Spark 中进行高效的下游分析，将 JSON 转换为 Parquet）等。所有这些转换以及更多的操作都可以通过 StreamSets DataOps
    平台轻松完成。
- en: '*Imp Note: Extensive and thorough feature engineering tasks and in depth analysis
    of features, their correlation with the target variable, feature importances,
    etc. is best suited for and better performed on interactive tools, such as, Databricks
    Notebook, Jupyter, RStudio, and ML platforms.*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*重要提示：广泛而彻底的特征工程任务以及对特征、特征与目标变量之间的相关性、特征重要性等的深入分析，最适合在交互式工具上进行，如 Databricks
    Notebook、Jupyter、RStudio 和 ML 平台。*'
- en: Model Experiments, Tracking, And Registration
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型实验、跟踪和注册
- en: Experimentation is a big precursor to model development where data scientists
    take sufficient subsets of trusted datasets and create several models in a rapid,
    iterative manner.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 实验是模型开发的重要前置环节，在这一过程中，数据科学家从可信的数据集中取出足够的子集，并以快速、迭代的方式创建多个模型。
- en: Without proper industry standards, data scientists have to rely on manual tracking
    of models, inputs, hyperparameters, outputs and any other such artifacts throughout
    the model experimentation and development process. This results in very long model
    deployment/release cycles, which effectively prevents organizations from adapting
    to dynamic changes, gaining competitive advantage, and in some cases staying in
    compliance with changing governance and regulations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有适当的行业标准的情况下，数据科学家必须依赖手动跟踪模型、输入、超参数、输出和任何其他相关工件的过程。这导致了非常长的模型部署/发布周期，实际上阻碍了组织适应动态变化、获得竞争优势，并且在某些情况下难以保持对不断变化的治理和法规的合规性。
- en: How Can StreamSets Help
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: StreamSets 如何提供帮助
- en: Using StreamSets Transformer, [a Spark ETL engine](https://streamsets.com/products/dataops-platform/transformer-etl/),
    it’s easy to integrate with [MLflow](https://mlflow.org/) using its PySpark or
    Scala APIs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 StreamSets Transformer，[一个 Spark ETL 引擎](https://streamsets.com/products/dataops-platform/transformer-etl/)，可以轻松地通过其
    PySpark 或 Scala API 与 [MLflow](https://mlflow.org/) 集成。
- en: This MLflow integration allows for tracking and versioning of model training
    code, data, config, hyperparameters as well as register and manage models in a
    central repository in MLflow from Transformer. This is critical for retraining
    models and/or for reproducing experiments.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这项 MLflow 集成允许跟踪和版本控制模型训练代码、数据、配置、超参数，并且可以在 MLflow 中的中央存储库中注册和管理模型，从 Transformer
    中进行。这对重新训练模型和/或重现实验至关重要。
- en: When using [MLflow on Databricks](https://docs.databricks.com/applications/mlflow/index.html),
    this creates a powerful and seamless solution because [Transformer can run on
    Databricks](https://streamsets.com/solutions/streamsets-for-databricks/) clusters
    and Databricks comes bundled with MLflow server.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当使用 [Databricks 上的 MLflow](https://docs.databricks.com/applications/mlflow/index.html)
    时，这会创建一个强大而无缝的解决方案，因为 [Transformer 可以在 Databricks](https://streamsets.com/solutions/streamsets-for-databricks/)
    集群上运行，并且 Databricks 已捆绑 MLflow 服务器。
- en: End-to-end Use Case
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 端到端用例
- en: Let’s walk through an end-to-end scenario where we’ll ingest data from a cloud
    object storage (for example, Amazon S3), perform necessary transformations, and
    train a regression model. The dataset consists of a set of houses with features
    like number of bedrooms, bathrooms, square footage, etc. and the price it was
    sold at.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个端到端的场景来演示，我们将从云对象存储（例如，Amazon S3）中获取数据，执行必要的转换，并训练一个回归模型。数据集由一组房屋组成，包含卧室数量、浴室数量、建筑面积等特征，以及房屋出售时的价格。
- en: Apart from tracking, versioning, and registering models in MLflow with every
    run we’d also like the pipeline to automatically promote models from “*staging*”
    to “*production*” provided they meet a certain set of conditions. For example,
    if *r2 >= ${r2Threshold} or rmse <= ${rmseThreshold},* then the model needs to
    be promoted to “Production” on MLflow server on Databricks. This can be one of
    the requirements and part of the specification given by the data scientists to
    the data engineering team responsible for deploying the models.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline Overview
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The StreamSets Transformer pipeline shown below is designed to load training
    data from [Amazon S3](https://streamsets.com/documentation/transformer/latest/help/transformer/Origins/AmazonS3.html#concept_gww_1kw_shb),
    perform transformations like [remove](https://streamsets.com/documentation/transformer/latest/help/transformer/Processors/FieldRemover.html#concept_svw_dxf_fhb) row
    id, rename target column “*mdev*” to “*label*” (which is required by SparkMLlib),
    train **Gradient Boosted Regression** model using [PySpark](https://streamsets.com/documentation/transformer/latest/help/transformer/Processors/PySpark.html#concept_gqm_4hn_ygb) processor
    and archive the training data in [Amazon S3](https://streamsets.com/documentation/transformer/latest/help/transformer/Destinations/AmazonS3-D.html#concept_ymh_534_d3b).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: More importantly, the pipeline also integrates with [MLflow on Databricks](https://docs.databricks.com/applications/mlflow/index.html) to
    track and version model training code including hyperparameters, model evaluation
    metrics, and register models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '**![Model Experiments, Tracking and Registration using MLflow on StreamSets
    and Databricks](../Images/b18bd6ccd1ef4da94c8ad40ce57f779c.png)**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Training And Experimentation**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Here’s the code snippet of interest in [PySpark Processor](https://streamsets.com/documentation/transformer/latest/help/transformer/Processors/PySpark.html#concept_gqm_4hn_ygb) —
    this is part of the pipeline that trains the Gradient Boosted Regression model
    and tracks everything in MLflow including promoting models from “*staging*” to
    “*production*” based on certain conditions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Show All ▼
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Tracking in MLflow On Databricks**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Here are the model training runs from the Transformer pipeline tracked in MLflow.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '**![Model Experiments, Tracking and Registration using MLflow on StreamSets
    and Databricks](../Images/621550444f5f5e9f78ea27f94aeda536.png)**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Versioning in MLflow On Databricks**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Here are the model versions registered from the Transformer pipeline.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '**![Model Experiments, Tracking and Registration using MLflow on StreamSets
    and Databricks](../Images/71af7b1d9949d7e5d7bde0d90f25de9f.png)**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Comparison in MLflow On Databricks**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a side-by-side comparison of two selected models created from the Transformer
    pipeline.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '**![Model Experiments, Tracking and Registration using MLflow on StreamSets
    and Databricks](../Images/a9f6c3ab266eb42837024a2c6d8e62c6.png)**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Model Retraining
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, a very common requirement is to automate the process of retraining the
    model as and when more data becomes available–especially if the model hasn’t yet
    met the evaluation criteria. For example, accuracy can be one of the metrics on
    how a particular model is evaluated. This type of automation can be implemented
    by setting up an [orchestrator pipeline](https://streamsets.com/documentation/datacollector/latest/help/datacollector/UserGuide/Orchestration_Pipelines/OrchestrationPipelines_Title.html#Orchestrators_Title) as
    shown below.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: The orchestrator pipeline is designed to continuously run and “wait” for training
    dataset  files to be uploaded on Amazon S3\. As soon as a training dataset is
    uploaded, this pipeline triggers/starts the model (re)training job described earlier.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![Model Experiments, Tracking and Registration using MLflow on StreamSets and
    Databricks](../Images/303066fe0a00df998d4f07c69006e900.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: Also note that there are two hyperparameters ***maxIter*** and ***numberOfCVFolds*** passed
    in the pipeline so there’s no need to hard code them, and can be dynamically passed
    into the pipeline during model retraining and experimentation. The StreamSets
    DataOps Platform also provides ways to check the status of jobs that are currently
    running so that actions can be taken based on the status as shown above in the
    pipeline.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Sample Pipelines
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you’re interested in additional technical details and sample pipelines,
    please reach out to me: dash at streamsets dot com or [@iamontheinet](https://twitter.com/iamontheinet).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Get Started With Your Own Model Experiments
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: StreamSets DataOps Platform is not a machine learning platform, but it does
    provide important capabilities and extensibility that can help and expedite operations
    at some of the most crucial stages of the ML Lifecycle and MLOps.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Learn more about [StreamSets For Databricks](https://streamsets.com/products/cloud/streamsets-for-databricks-marketplace/) available
    on AWS Marketplace and Microsoft Azure Marketplace.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Dash Desai](https://www.linkedin.com/in/dash-desai/)** is Director
    of Platform and Technical Evangelism at StreamSets, and has 18+ years of hands-on
    software and data engineering background. With recent experience in Big Data,
    Data Science, and Machine Learning, Dash applies his technical skills to help
    build solutions that solve business problems and surface trends that shape markets
    in new ways. Dash has worked for global enterprises and tech startups in agile
    environments as an engineer and a solutions architect. As a Platform and Technical
    Evangelist, he is passionate about evaluating new ideas to help articulate how
    technology can address a given business problem. He also enjoys writing technical
    blog posts, hands-on tutorials, and conducting technical workshops.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://streamsets.com/blog/model-experiments-tracking-and-registration-using-mlflow-on-databricks/).
    Reposted with permission.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://streamsets.com/blog/model-experiments-tracking-and-registration-using-mlflow-on-databricks/)。已获授权转载。'
- en: '**Related:**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[MLOps Is Changing How Machine Learning Models Are Developed](/2020/12/mlops-changing-machine-learning-developed.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MLOps正在改变机器学习模型的开发方式](https://www.kdnuggets.com/2020/12/mlops-changing-machine-learning-developed.html)'
- en: '[Production Machine Learning Monitoring: Outliers, Drift, Explainers & Statistical
    Performance](/2020/12/production-machine-learning-monitoring-outliers-drift-explainers-statistical-performance.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[生产机器学习监控：异常值、漂移、解释器和统计性能](https://www.kdnuggets.com/2020/12/production-machine-learning-monitoring-outliers-drift-explainers-statistical-performance.html)'
- en: '[Managing Machine Learning Cycles: Five Learnings from comparing Data Science
    Experimentation/ Collaboration Tools](/2020/01/managing-machine-learning-cycles.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[管理机器学习周期：从比较数据科学实验/协作工具中获得的五个经验](https://www.kdnuggets.com/2020/01/managing-machine-learning-cycles.html)'
- en: More On This Topic
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多关于此话题
- en: '[Versioning Machine Learning Experiments vs Tracking Them](https://www.kdnuggets.com/2021/12/versioning-machine-learning-experiments-tracking.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[版本控制机器学习实验与跟踪](https://www.kdnuggets.com/2021/12/versioning-machine-learning-experiments-tracking.html)'
- en: '[How to Package and Distribute Machine Learning Models with MLFlow](https://www.kdnuggets.com/2022/08/package-distribute-machine-learning-models-mlflow.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用MLFlow打包和分发机器学习模型](https://www.kdnuggets.com/2022/08/package-distribute-machine-learning-models-mlflow.html)'
- en: '[Developing an Open Standard for Analytics Tracking](https://www.kdnuggets.com/2022/07/developing-open-standard-analytics-tracking.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[开发分析跟踪的开放标准](https://www.kdnuggets.com/2022/07/developing-open-standard-analytics-tracking.html)'
- en: '[7 Best Tools for Machine Learning Experiment Tracking](https://www.kdnuggets.com/2023/02/7-best-tools-machine-learning-experiment-tracking.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7个最佳机器学习实验跟踪工具](https://www.kdnuggets.com/2023/02/7-best-tools-machine-learning-experiment-tracking.html)'
- en: '[Optimizing Data Analytics: Integrating GitHub Copilot in Databricks](https://www.kdnuggets.com/optimizing-data-analytics-integrating-github-copilot-in-databricks)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化数据分析：在Databricks中集成GitHub Copilot](https://www.kdnuggets.com/optimizing-data-analytics-integrating-github-copilot-in-databricks)'
- en: '[How to Design Experiments for Data Collection](https://www.kdnuggets.com/2022/04/design-experiments-data-collection.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何设计数据收集实验](https://www.kdnuggets.com/2022/04/design-experiments-data-collection.html)'
