- en: 'Machine Learning from Scratch: Decision Trees'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/11/machine-learning-scratch-decision-trees.html](https://www.kdnuggets.com/2022/11/machine-learning-scratch-decision-trees.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![v](../Images/36ac427f5a02fcb2b5f248c0eecb8ace.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Pexel](https://www.pexels.com/photo/leafless-tree-on-grass-field-1102909/)
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are one of the simplest non-linear supervised algorithms in the
    machine learning world. As the name suggests they are used for making decisions
    in ML terms we call it classification (although they can be used for regression
    as well).
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The decision trees have a unidirectional tree structure i.e. at every node the
    algorithm makes a decision to split into child nodes based on certain stopping
    criteria. Most commonly DTs use entropy, information gain, Gini index, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few known algorithms in DTs such as ID3, C4.5, CART, C5.0, CHAID,
    QUEST, CRUISE. In this article, we would discuss the simplest and most ancient
    one: ID3.'
  prefs: []
  type: TYPE_NORMAL
- en: '**ID3**, or Iternative Dichotomizer, was the first of three Decision Tree implementations
    developed by Ross Quinlan.'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm builds a tree in a top-down fashion, starting from a set of rows/objects
    and a specification of features. At each node of the tree, one feature is tested
    based on minimizing entropy or maximizing information gain to split the object
    set. This process is continued until the set in a given node is homogeneous (i.e.
    node contains objects of the same category). The algorithm uses a greedy search.
    It selects a test using the information gain criterion, and then never explores
    the possibility of alternate choices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cons:'
  prefs: []
  type: TYPE_NORMAL
- en: The model may be over-fitted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only works on categorical features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does not handle missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low speed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doesn’t support pruning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doesn’t support boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So many disadvantages of one algorithm, why are we even discussing this?
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer: it’s simple and is great to develop the intuition for tree algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s Understand with an Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most popular nursery rhyme where rain decides whether Johny/Arthur
    would play outside is our sample today. The only change is that not just rain
    but any bad weather impacts the child’s play and we would use a DT to predict
    his presence outside.
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine Learning from scratch: Decision Trees](../Images/372d0781819ea8688ab43a50782ecf20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Wikipedia'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine Learning from scratch: Decision Trees](../Images/2c58f39b5313bb05eaab4d9000b15892.png)'
  prefs: []
  type: TYPE_IMG
- en: ‘Temperature’, ‘WindSpeed’, ‘Outlook’, and ‘Humidity’ are the features, and
    ‘Play’ is the target variable. Only categorical data and no missing values mean
    we can use ID3.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through the splitting criteria before jumping onto the algorithm itself.
    For simplicity, we will discuss each criterion only for the binary classification
    case.
  prefs: []
  type: TYPE_NORMAL
- en: '**Entropy:** This is used to calculate the heterogeneity of a sample and its
    value lies between 0 and 1\. If the sample is homogeneous the entropy is 0 and
    if the sample has the same proportion of all the classes it has an entropy of
    1.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Where `p` and `q` are respective proportion of the 2 classes in the sample.
  prefs: []
  type: TYPE_NORMAL
- en: This can also be written as: `S = -(p * log₂p + (1-p)* log₂(1-p))`
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine Learning from scratch: Decision Trees](../Images/a7189f79ab54740aa38045d1954de6fd.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Information Gain:** It is the difference in entropy of a node and the average
    entropy for all the values of a child node. The feature which gives maximum Information
    Gain is chosen for the split.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s Understand this with our Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The entropy of root node: (9 — Yes and 5 — No)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'There are 4 possible ways to split the root node. (‘Temperature’, ‘WindSpeed’,
    ‘Outlook’, and ‘Humidity’). Thus we compute the weighted average entropy of the
    child node if we choose any one of the above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Where Hot, Mild, and Cool represent the proportion of the 3 values in the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here Entropy for each value is computed by filtering the sample using the value
    of the feature and then using the formula for entropy. For example, E(Temperature=Hot)
    is computed by filtering the original sample where the Temperature is Hot (in
    this case we have an equal number of Yes and No which means Entropy equals 1).
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine Learning from scratch: Decision Trees](../Images/2faaecf786f9881367c444e38303a93b.png)'
  prefs: []
  type: TYPE_IMG
- en: We compute the Information Gain of splitting on Temperature by subtracting the
    Average Entropy for Temperature from the root nodes Entropy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Likewise, we compute the Gain from all the four features and choose the 1 with
    maximum Gain.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Outlook has the maximum Information Gain, thus we would split the root node
    on Outlook and each for the child nodes represents the sample filtered by one
    of the values of Outlook i.e. Sunny, Overcast, and Rainy.
  prefs: []
  type: TYPE_NORMAL
- en: Now we would repeat the same process by treating the new nodes formed as root
    nodes with filtered samples and compute entropies for each and testing the splits
    by computing Average Entropy for each further split and subtracting that from
    the current node Entropy to get the Information Gain. Please note that ID3 doesn’t
    allow using already used feature for splitting child nodes. Thus each feature
    can only be used once in a tree for the split.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the final tree formed by all the splits:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine Learning from scratch: Decision Trees](../Images/40ba6d18ea1dde3426503c175f884f32.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple implementation with Python code can be found [here](https://www.kaggle.com/ankitmalik/decision-trees-from-scratch-id3)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I tried my best to explain the ID3 working but I know you might have questions.
    Please let me know in the comments and I would be happy to take them all.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: '**[Ankit Malik](https://www.linkedin.com/in/ankitmalikds/)** is building scalable
    AI/ML solutions across multiple domains like Marketing, Supply Chain, Retail,
    Advertising, and Process Automation. He has worked on both ends of the spectrum
    from leading data science projects in Fortune 500 companies to being the founding
    member of a data science incubator in multiple start-ups. He has pioneered various
    innovative products and services and is a believer in servant leadership.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://ai.plainenglish.io/ml-from-scratch-decision-trees-da68cdaa13bc).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Decision Trees vs Random Forests, Explained](https://www.kdnuggets.com/2022/08/decision-trees-random-forests-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Generalized and Scalable Optimal Sparse Decision Trees(GOSDT)](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Demystifying Decision Trees for the Real World](https://www.kdnuggets.com/demystifying-decision-trees-for-the-real-world)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear Regression from Scratch with NumPy](https://www.kdnuggets.com/linear-regression-from-scratch-with-numpy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Build and Train a Transformer Model from Scratch with…](https://www.kdnuggets.com/how-to-build-and-train-a-transformer-model-from-scratch-with-hugging-face-transformers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Decision Tree Algorithm, Explained](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
