# 梯度提升决策树——概念性解释

> 原文：[https://www.kdnuggets.com/2021/04/gradient-boosted-trees-conceptual-explanation.html](https://www.kdnuggets.com/2021/04/gradient-boosted-trees-conceptual-explanation.html)

[评论](#comments)

梯度提升决策树已被证明优于其他模型。这是因为提升涉及实现多个模型并汇总它们的结果。

梯度提升模型最近因在 Kaggle 机器学习竞赛中的表现而变得流行。

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业轨道。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织IT需求

* * *

在本文中，我们将深入探讨梯度提升决策树的全部内容。

### 梯度提升

在 [梯度提升](https://en.wikipedia.org/wiki/Gradient_boosting#:~:text=Gradient%20boosting%20is%20a%20machine,prediction%20models%2C%20typically%20decision%20trees.) 中，使用一组弱学习器来提升机器学习模型的性能。这些弱学习器通常是决策树。它们的组合输出结果更佳。

在回归的情况下，最终结果是从所有弱学习器的平均值中生成的。在分类的情况下，最终结果可以计算为弱学习器中投票最多的类别。

在梯度提升中，弱学习器按顺序工作。每个模型尝试改进上一个模型的错误。这与并行方式在数据子集上拟合多个模型的袋装技术不同。这些子集通常是随机抽取的。袋装技术的一个很好的例子是随机森林®。

**提升过程如下**：

+   使用数据构建初始模型，

+   对整个数据集进行预测，

+   使用预测值和实际值计算误差，

+   为错误的预测分配更多的权重，

+   创建另一个模型，尝试修正上一模型的错误，

+   使用新模型对整个数据集进行预测，

+   创建多个模型，每个模型旨在纠正前一个模型生成的错误，

+   通过对所有模型的均值加权来获得最终模型。

### 机器学习中的提升算法

让我们来看看机器学习中的提升算法。

### AdaBoost

AdaBoost 将一系列弱学习器拟合到数据上。然后，它对错误预测赋予更多的权重，对正确预测赋予较少的权重。这样，算法就更加关注那些难以预测的观察结果。最终结果通过分类中的多数投票或回归中的平均值获得。

你可以使用 Scikit-learn 实现这个算法。可以传递 `n_estimators` 参数来指示所需的弱学习器数量。你可以使用 `learning_rate` 参数控制每个弱学习器的贡献。

该算法默认使用决策树作为基础估计器。可以调整基础估计器和决策树的参数，以提高模型的性能。默认情况下，AdaBoost 中的决策树只有一个分裂。

**使用 AdaBoost 进行分类**

你可以使用 Scikit-learn 中的 `AdaBoostClassifier` 来实现用于分类问题的 AdaBoost 模型。正如下面所示，基础估计器的参数可以根据你的需要进行调整。分类器还接受你想要的估计器数量。这是你模型所需的决策树数量。

[PRE0]

**使用 AdaBoost 进行回归**

将 AdaBoost 应用于回归问题类似于分类过程，只是有一些外观上的变化。首先，你需要导入 `AdaBoostRegressor`。然后，对于基础估计器，你可以使用 `DecisionTreeRegressor`。和之前一样，你可以调整决策树回归器的参数。

[PRE1]

### Scikit-learn 梯度提升估计器

梯度提升与 AdaBoost 不同，因为损失函数优化是通过梯度下降来完成的。和 AdaBoost 一样，它也使用决策树作为弱学习器，并且它按顺序拟合这些树。当添加后续树时，通过梯度下降最小化损失。

在 Scikit-learn 的实现中，你可以指定树木的数量。这是一个需要仔细考虑的参数，因为指定过多的树木可能会导致过拟合。另一方面，指定的树木数量过少可能会导致欠拟合。

该算法允许你指定学习率。这决定了模型学习的速度。较低的学习率通常需要更多的树木，这意味着需要更多的训练时间。

现在让我们来看看在 Scikit-learn 中梯度提升树的实现。

**使用 Scikit-learn 梯度提升估计器进行分类**

这通过 `GradientBoostingClassifier` 实现。该算法预期的一些参数包括：

+   `loss` 定义了要优化的损失函数

+   `learning_rate` 决定了每棵树的贡献

+   `n_estimators` 决定了决策树的数量

+   `max_depth` 是每个估计器的最大深度

[PRE2]

拟合分类器后，你可以使用 `feature_importances_` 属性来获得特征的重要性。这通常被称为基尼重要性。

[PRE3]

![梯度提升决策树特征重要性图](../Images/9ad0e30669cda02fa3b7eb4daa2ad64b.png)

值越高，特征越重要。获得的数组中的值将总和为 1。

注意：基于不纯度的重要性并不总是准确的，特别是当特征过多时。在这种情况下，你应该考虑使用 [基于置换的重要性](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance)。

**使用 Scikit-learn 梯度提升估计器进行回归**

Scikit-learn 梯度提升估计器可以使用 `GradientBoostingRegressor` 实现回归。它接受的参数类似于分类模型：

+   损失函数，

+   估计器的数量，

+   树的最大深度，

+   学习率…

…只是举几个例子。

[PRE4]

与分类模型类似，你也可以获得回归算法的特征重要性。

[PRE5]

### XGBoost

[XGBoost](https://neptune.ai/blog/how-to-organize-your-xgboost-machine-learning-ml-model-development-process) 是一个支持 Java、Python、Java 和 C++、R 以及 Julia 的梯度提升库。它还使用了一个弱决策树的集合。

这是一个通过并行计算进行树学习的线性模型。该算法还提供了进行交叉验证和显示特征重要性的功能。该模型的主要特点是：

+   接受稀疏输入用于树提升器和线性提升器，

+   支持自定义评估和目标函数，

+   `Dmatrix`，其优化的数据结构提高了性能。

让我们来看一下如何在 Python 中应用 XGBoost。该算法接受的参数包括：

+   `objective` 用于定义任务类型，比如回归或分类；

+   `colsample_bytree` 构建每棵树时的列子样本比例。子样本化在每次迭代中发生。这个值通常在 0 和 1 之间；

+   `learning_rate` 决定了模型学习的速度；

+   `max_depth` 指示每棵树的最大深度。树的数量越多，模型的复杂度越高，过拟合的可能性也越大；

+   `alpha` 是 [L1 正则化](https://en.wikipedia.org/wiki/Regularization_(mathematics)) 权重；

+   `n_estimators` 是要拟合的决策树的数量。

**XGBoost 分类**

在导入算法后，你定义希望使用的参数。由于这是一个分类问题，所以使用了 `binary: logistic` 目标函数。下一步是使用 `XGBClassifier` 并展开定义的参数。你可以调整这些参数，直到获得对你的问题最优的参数。

[PRE6]

**使用 XGBoost 进行回归**

在回归中，使用的是 `XGBRegressor`。在这种情况下，目标函数将是 `reg:squarederror`。

[PRE7]

XGBoost 模型还允许你通过 `feature_importances_` 属性获取特征的重要性。

[PRE8]

![回归器特征导入](../Images/5fcc6666d29f1ad8c2ae319bd0f2c8e8.png)

你可以使用 Matplotlib 轻松可视化它们。这是通过 XGBoost 的 `plot_importance` 函数完成的。

[PRE9]

![梯度提升特征重要性](../Images/516c097cd9cb492a799b06e9c75c25d4.png)

`save_model` 函数可用于保存你的模型。然后你可以将这个模型发送到你的模型注册中心。

[PRE10]

查看 Neptune 文档关于与 [XGBoost](https://docs.neptune.ai/essentials/integrations/machine-learning-frameworks/xgboost) 和 [matplotlib](https://docs.neptune.ai/essentials/integrations/visualization-libraries/matplotlib) 的集成。

### LightGBM

[LightGBM](https://neptune.ai/blog/how-to-organize-your-lightgbm-ml-model-development-process-examples-of-best-practices) 与其他梯度提升框架不同，因为它使用叶子优先的树生长算法。叶子优先的树生长算法已知比深度优先算法收敛更快。然而，它们更容易过拟合。

![按叶子生长的树](../Images/3cb421772a8f89c5b77b76605f11036d.png)[*来源*](https://lightgbm.readthedocs.io/en/latest/Features.html?highlight=dart#other-features)

该算法是 [基于直方图](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html) 的，因此它将连续值放入离散箱中。这可以导致更快的训练和高效的内存利用。

该算法的其他显著特性包括：

+   支持 GPU 训练，

+   原生支持类别特征，

+   处理大规模数据的能力，

+   默认处理缺失值。

让我们来看看该算法的一些主要参数：

+   `max_depth` 每棵树的最大深度；

+   `objective` 默认为回归；

+   `learning_rate` 提升学习率；

+   `n_estimators` 拟合的决策树数量；

+   `device_type` 指你是在 CPU 还是 GPU 上工作。

**使用 LightGBM 进行分类**

通过将 `binary` 设置为目标，可以训练一个二分类模型。如果是多分类问题，则使用 `multiclass` 目标。

数据集还被转换为 LightGBM 的 `Dataset` 格式。然后使用 `train` 函数训练模型。你也可以通过 `valid_sets` 参数传递验证数据集。

[PRE11]

**使用 LightGBM 进行回归**

对于使用 LightGBM 进行回归，你只需将目标更改为 `regression`。默认的提升类型是梯度提升决策树。

如果你愿意，你可以将其更改为随机森林算法，`dart` —— Dropouts meet Multiple Additive Regression Trees，或 `goss` —— Gradient-based One-Side Sampling。

[PRE12]

你也可以使用 LightGBM 绘制模型的特征重要性。

[PRE13]

![lgb.plot_importance](../Images/98b83d78b753c41c28f904a42ece95f7.png)

LightGBM 还内置了一个保存模型的功能。这个功能是 `save_model`。

[PRE14]

### CatBoost

[CatBoost](https://github.com/catboost) 是 Yandex 开发的深度优先梯度提升库。该算法使用无知决策树生长平衡树。

它在树的每个级别使用相同的特征进行左右拆分。

例如在下面的图像中，你可以看到 `297,value>0.5` 是在那个级别使用的。

![梯度提升 CatBoost](../Images/c464403bbb3b7dff15de9e6324cf0ac7.png)

[CatBoost](https://catboost.ai/news/catboost-enables-fast-gradient-boosting-on-decision-trees-using-gpus) 的其他显著特性包括：

+   原生支持类别特征，

+   支持在多个 GPU 上进行训练，

+   使用默认参数可获得良好的性能，

+   通过 CatBoost 的模型应用程序实现快速预测，

+   原生处理缺失值，

+   支持回归和分类问题。

现在让我们提及一些 CatBoost 的训练参数：

+   `loss_function` 分类或回归时使用的损失函数；

+   `eval_metric` 模型的评估指标；

+   `n_estimators` 决策树的最大数量；

+   `learning_rate` 决定模型学习的快慢；

+   `depth` 每棵树的最大深度；

+   `ignored_features` 决定训练过程中应该忽略的特征；

+   `nan_mode` 处理缺失值的方法；

+   `cat_features` 一个包含类别列的数组；

+   `text_features` 用于声明基于文本的列。

**使用 CatBoost 进行分类**

对于分类问题，使用`CatBoostClassifier`。在训练过程中设置 `plot=True` 将可视化模型。

[PRE15]

![CatBoostClassifier](../Images/cb4a66e7e6430508805ac7c92fea9abd.png)

**使用 CatBoost 进行回归**

在回归的情况下，使用 `CatBoostRegressor`。

[PRE16]

你还可以使用 `feature_importances_` 来获取特征按重要性排名。

[PRE17]

![model.feature_importances_](../Images/1f751bb52e8a8b3b02715d3037fa158f.png)

该算法还提供了交叉验证支持。这是通过使用 `cv` 函数并传递所需参数来完成的。

传递 `plot=”True”` 将可视化交叉验证过程。`cv` 函数期望数据集为 CatBoost 的 `Pool` 格式。

[PRE18]

你还可以使用 CatBoost 进行网格搜索。通过 `grid_search` 函数完成此操作。搜索后，CatBoost 在最佳参数上进行训练。

在此过程之前，你不应已拟合模型。传递 `plot=True` 参数将可视化网格搜索过程。

[PRE19]

CatBoost 还允许你可视化模型中的单棵树。使用 `plot_tree` 函数并传递要可视化的树的索引来实现。

[PRE20]

![使用 CatBoost 进行回归](../Images/6cd1b451f8b3d417236c758f41979021.png)

### 梯度提升树的优点

有几个理由可以考虑使用梯度提升树算法：

+   与其他模型相比通常更准确，

+   在较大数据集上训练更快，

+   大多数算法支持处理分类特征，

+   其中一些算法本身就能处理缺失值。

### 梯度提升树的缺点

现在，让我们讨论使用梯度提升树时面临的一些挑战：

+   易于过拟合：可以通过应用L1和L2正则化惩罚来解决这个问题。你也可以尝试较低的学习率；

+   模型可能计算开销大，训练时间长，尤其是在CPU上；

+   最终模型难以解释。

### 结论

在这篇文章中，我们探讨了如何在机器学习问题中实现梯度提升决策树。我们还介绍了各种基于提升的算法，你可以立即开始使用。

我们特别涵盖了：

+   什么是梯度提升，

+   梯度提升的工作原理，

+   各种类型的梯度提升算法，

+   如何使用梯度提升算法解决回归和分类问题，

+   梯度提升树的优点，

+   梯度提升树的缺点，

…还有更多内容。

你已准备好开始提升你的机器学习模型。

### 资源

+   [TensorFlow中的梯度提升](https://www.tensorflow.org/tutorials/estimator/boosted_trees_model_understanding)

+   [基于直方图的梯度提升](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html)

+   [分类笔记本](https://colab.research.google.com/drive/1O6ChgoMcnEdr4opf2d1Qgoltw_VMYzzn?usp=sharing)

+   [回归笔记本](https://colab.research.google.com/drive/1LE0Hj0axWfjL7DqWP04NX_tb6gzLpk-t?usp=sharing)

**简介： [德里克·穆伊提](https://www.linkedin.com/in/mwitiderrick/)** 是一位对知识分享充满热情的数据科学家。他通过Heartbeat、Towards Data Science、Datacamp、Neptune AI、KDnuggets等博客积极贡献于数据科学社区。他的内容在互联网上的浏览量已超过一百万次。德里克还是一位作者和在线讲师。他还与各种机构合作，实施数据科学解决方案以及提升其员工技能。你可以查看他的 [Python完整数据科学与机器学习训练营课程](https://www.udemy.com/course/data-science-bootcamp-in-python/?referralCode=9F6DFBC3F92C44E8C7F4)。

[原文](https://neptune.ai/blog/gradient-boosted-decision-trees-guide)。经授权转载。

**相关：**

+   [LightGBM：一种高效的梯度提升决策树](/2020/06/lightgbm-gradient-boosting-decision-tree.html)

+   [最佳机器学习框架与Scikit-learn扩展](/2021/03/best-machine-learning-frameworks-extensions-scikit-learn.html)

+   [使用CatBoost进行快速梯度提升](/2020/10/fast-gradient-boosting-catboost.html)

### 更多相关话题

+   [从头开始学习机器学习：决策树](https://www.kdnuggets.com/2022/11/machine-learning-scratch-decision-trees.html)

+   [决策树与随机森林，解释](https://www.kdnuggets.com/2022/08/decision-trees-random-forests-explained.html)

+   [广义和可扩展的最优稀疏决策树（GOSDT）](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)

+   [揭示决策树在现实世界中的应用](https://www.kdnuggets.com/demystifying-decision-trees-for-the-real-world)

+   [线性回归与逻辑回归：简明解释](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)

+   [KDnuggets 新闻 22:n12, 3月 23: 最佳数据科学书籍…](https://www.kdnuggets.com/2022/n12.html)
