- en: 'Unveiling Neural Magic: A Dive into Activation Functions'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭开神经魔法：深入激活函数
- en: 原文：[https://www.kdnuggets.com/unveiling-neural-magic-a-dive-into-activation-functions](https://www.kdnuggets.com/unveiling-neural-magic-a-dive-into-activation-functions)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/unveiling-neural-magic-a-dive-into-activation-functions](https://www.kdnuggets.com/unveiling-neural-magic-a-dive-into-activation-functions)
- en: '![Unveiling Neural Magic: A Dive into Activation Functions](../Images/23c0c292d492c32ac44d2f06244752bb.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![揭开神经魔法：深入激活函数](../Images/23c0c292d492c32ac44d2f06244752bb.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Why Use Activation Functions
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么使用激活函数
- en: Deep Learning and Neural Networks consist of interconnected nodes, where data
    is passed sequentially through each hidden layer. However, the composition of
    linear functions is inevitably still a linear function. Activation functions become
    important when we need to learn complex and non-linear patterns within our data.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习和神经网络由互连的节点组成，其中数据顺序地通过每个隐藏层传递。然而，线性函数的组合不可避免地仍然是线性函数。当我们需要在数据中学习复杂和非线性的模式时，激活函数变得重要。
- en: 'The two major benefits of using activation functions are:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 使用激活函数的两个主要好处是：
- en: Introduces Non-Linearity
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入非线性
- en: Linear relationships are rare in real-world scenarios. Most real-world scenarios
    are complex and follow a variety of different trends. Learning such patterns is
    impossible with linear algorithms like Linear and Logistic Regression. Activation
    functions add non-linearity to the model, allowing it to learn complex patterns
    and variance in the data. This enables deep learning models to perform complicated
    tasks including the image and language domains.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 线性关系在现实世界中很少见。大多数现实世界场景是复杂的，并遵循各种不同的趋势。使用线性算法如线性和逻辑回归无法学习这样的模式。激活函数为模型添加了非线性，使其能够学习数据中的复杂模式和变异。这使得深度学习模型能够执行复杂的任务，包括图像和语言领域。
- en: Allow Deep Neural Layers
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 允许深层神经网络
- en: As mentioned above, *when we sequentially apply multiple linear functions, the
    output is still a linear combination of the inputs.* Introducing non-linear functions
    between each layer allows them to learn different features of the input data.
    Without activation functions, having a deeply connected neural network architecture
    will be the same as using basic Linear or Logistic Regression algorithms.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，*当我们顺序应用多个线性函数时，输出仍然是输入的线性组合。* 在每一层之间引入非线性函数可以让它们学习输入数据的不同特征。如果没有激活函数，深度连接的神经网络架构将与使用基本的线性或逻辑回归算法没有区别。
- en: Activation functions allow deep learning architectures to learn complex patterns,
    making them more powerful than simple Machine Learning algorithms.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数使深度学习架构能够学习复杂的模式，使其比简单的机器学习算法更强大。
- en: Let’s look at some of the most common activation functions used in deep learning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看一些在深度学习中常用的激活函数。
- en: Sigmoid
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sigmoid
- en: Commonly used in binary classification tasks, the Sigmoid function maps real-numbered
    values between 0 and 1.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在二分类任务中常用的Sigmoid函数将实数值映射到0和1之间。
- en: '![Unveiling Neural Magic: A Dive into Activation Functions](../Images/80d0e061ecdc2efbcc950004c2c7b5e2.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![揭开神经魔法：深入激活函数](../Images/80d0e061ecdc2efbcc950004c2c7b5e2.png)'
- en: 'The above equation looks as below:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程如下所示：
- en: '![Unveiling Neural Magic: A Dive into Activation Functions](../Images/f27c6ca6f7630bd3a5537f65e246b73e.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![揭开神经魔法：深入激活函数](../Images/f27c6ca6f7630bd3a5537f65e246b73e.png)'
- en: Image by [Hvidberrrg](https://hvidberrrg.github.io/deep_learning/activation_functions/sigmoid_function_and_derivative.html)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Hvidberrrg](https://hvidberrrg.github.io/deep_learning/activation_functions/sigmoid_function_and_derivative.html)提供
- en: '**The Sigmoid function is primarily used in the output layer for binary classification
    tasks where the target label is either 0 or 1.** This naturally makes Sigmoid
    preferable for such tasks, as the output is restricted between this range. For
    highly positive values that approach infinity, the sigmoid function maps them
    close to 1\. On the opposite end, it maps values approaching negative infinity
    to 0\. All real-valued numbers between these are mapped in the range 0 to 1 in
    an S-shaped trend.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sigmoid函数主要用于二分类任务的输出层，其中目标标签是0或1。** 这使得Sigmoid函数在这种任务中更受青睐，因为其输出限制在这个范围内。对于接近无穷大的高值，sigmoid函数将其映射到接近1。相反，它将接近负无穷大的值映射到0。在这些值之间的所有实数值都映射到0到1的范围内，呈S形趋势。'
- en: Shortcomings
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺点
- en: Saturation Points
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 饱和点
- en: The sigmoid function poses problems for the gradient descent algorithm during
    backpropagation. Except for values close to the center of the S-shaped curve,
    the gradient is extremely close to zero causing problems for training. Close to
    the asymptotes, it can lead to vanishing gradient problems as small gradients
    can significantly slow down convergence.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数在反向传播过程中对梯度下降算法会带来问题。除了接近S形曲线中心的值，梯度非常接近零，这会导致训练问题。在渐近线附近，它可能会导致梯度消失问题，因为小梯度可能显著减慢收敛速度。
- en: Not Zero-Centered
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非零中心
- en: I*t is empirically proven that having a zero-centered non-linear function ensures
    that the mean activation value is close to 0*. Having such normalized values ensures
    faster convergence of gradient descent towards the minima. Although not necessary,
    having zero-centered activation allows faster training. The Sigmoid function is
    centered at 0.5 when the input is 0\. This is one of the drawbacks of using Sigmoid
    in hidden layers.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*经实验证明，具有零中心的非线性函数可以确保均值激活值接近0*。具有这种归一化值可以确保梯度下降更快地收敛到最小值。虽然不是必要的，但具有零中心的激活可以加速训练。当输入为0时，Sigmoid函数的中心在0.5。这是使用Sigmoid在隐藏层中的一个缺点。'
- en: Tanh
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tanh
- en: The hyperbolic tangent function is an improvement over the Sigmoid function.
    Instead of the [0,1] range, the TanH function maps real-valued numbers between
    -1 and 1.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲正切函数是对Sigmoid函数的改进。与[0,1]范围不同，TanH函数将实数值映射到-1和1之间。
- en: '![Unveiling Neural Magic: A Dive into Activation Functions](../Images/0de1d4ed4910a0cb10282f1672de595c.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![揭示神经魔法：深入激活函数](../Images/0de1d4ed4910a0cb10282f1672de595c.png)'
- en: 'The Tanh function looks as below:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: TanH函数如下所示：
- en: '![Unveiling Neural Magic: A Dive into Activation Functions](../Images/26fa67c0aba4c8db7b99e4515bb868ef.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![揭示神经魔法：深入激活函数](../Images/26fa67c0aba4c8db7b99e4515bb868ef.png)'
- en: Image by [Wolfram](https://mathworld.wolfram.com/HyperbolicTangent.html)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Wolfram](https://mathworld.wolfram.com/HyperbolicTangent.html)提供
- en: The TanH function follows the same S-shaped curve as the Sigmoid, but it is
    now zero-centered. This allows faster convergence during training as it improves
    on one of the shortcomings of the Sigmoid function. **This makes it more suitable
    for use in hidden layers in a neural network architecture.**
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: TanH函数遵循与Sigmoid相同的S形曲线，但现在是零中心的。这允许在训练过程中更快的收敛，因为它改进了Sigmoid函数的一个缺点。**这使得它在神经网络架构中的隐藏层中更为适用。**
- en: Shortcomings
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺点
- en: Saturation Points
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 饱和点
- en: The TanH function follows the same S-shaped curve as the Sigmoid, but it is
    now zero-centered. This allows faster convergence during training improving upon
    the Sigmoid function. This makes it more suitable for use in hidden layers in
    a neural network architecture.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: TanH函数遵循与Sigmoid相同的S形曲线，但现在是零中心的。这允许在训练过程中更快的收敛，改进了Sigmoid函数的表现。这使得它在神经网络架构中的隐藏层中更为适用。
- en: Computational Expense
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算开销
- en: Although not a major concern in the modern day, the exponential calculation
    is more expensive than other common alternatives available.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现代计算中并非主要问题，但指数计算比其他常见的替代方法更为昂贵。
- en: ReLU
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ReLU
- en: The most commonly used activation function in practice, Rectified Linear Unit
    Activation (ReLU) is the most simple yet most effective possible non-linear function.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 实践中最常用的激活函数，修正线性单元激活（ReLU），是最简单但最有效的非线性函数。
- en: '![Unveiling Neural Magic: A Dive into Activation Functions](../Images/eb93e3d80f0aa40eb31cd9cbf24a7386.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![揭示神经魔法：深入激活函数](../Images/eb93e3d80f0aa40eb31cd9cbf24a7386.png)'
- en: 'It conserves all non-negative values and clamps all negative values to 0\.
    Visualized, the ReLU functions look as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 它保留所有非负值并将所有负值钳制为0。可视化时，ReLU函数如下所示：
- en: '![Unveiling Neural Magic: A Dive into Activation Functions](../Images/04eed6624e7ff79531aa963d323f5b54.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![揭示神经魔法：深入激活函数](../Images/04eed6624e7ff79531aa963d323f5b54.png)'
- en: Image by [**Michiel Straat**](https://michielstraat.com/)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[**Michiel Straat**](https://michielstraat.com/)提供
- en: Shortcomings
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺点
- en: Dying ReLU
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 死亡ReLU
- en: The gradient flattens at one end of the graph. All negative values have zero
    gradients, so half of the neurons may have minimal contribution to training.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度在图的一端趋于平坦。所有负值的梯度为零，因此一半的神经元可能对训练贡献很小。
- en: Unbounded Activation
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无界激活
- en: On the right-hand side of the graph, there is no limit on the possible gradient.
    This can lead to an exploding gradient problem if the gradient values are too
    high. This issue is normally corrected by Gradient Clipping and Weight Initialization
    techniques.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在图的右侧，可能的梯度没有限制。如果梯度值过高，这可能导致梯度爆炸问题。通常通过梯度裁剪和权重初始化技术来纠正这个问题。
- en: Not Zero-Centered
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不是零中心
- en: Similar to Sigmoid, the ReLU activation function is also not zero-centered.
    Likewise, this causes problems with convergence and can slow down training.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 Sigmoid，ReLU 激活函数也不是零中心的。同样，这会导致收敛问题并可能减慢训练速度。
- en: '**Despite all shortcomings, it is the default choice for all hidden layers
    in neural network architectures and is empirically proven to be highly efficient
    in practice.**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**尽管存在所有缺陷，但它仍然是神经网络架构中所有隐藏层的默认选择，并且在实践中被经验验证为高效。**'
- en: Key Takeaways
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键要点
- en: Now that we know about the three most common activation functions, how do we
    know what is the best possible choice for our scenario?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们了解了三种最常见的激活函数，我们如何知道什么是我们场景中最佳的选择呢？
- en: Although it highly depends on the data distribution and specific problem statement,
    there are still some basic starting points that are widely used in practice.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这在很大程度上依赖于数据分布和具体问题陈述，但在实践中仍有一些基本的起点被广泛使用。
- en: Sigmoid is only suitable for output activations of binary problems when target
    labels are either 0 or 1.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid 只适用于目标标签为 0 或 1 的二分类问题的输出激活。
- en: Tanh is now majorly replaced by the ReLU and similar functions. However, it
    is still used in hidden layers for RNNs.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanh 现在主要被 ReLU 和类似函数取代。然而，它仍然在 RNN 的隐藏层中使用。
- en: In all other scenarios, ReLU is the default choice for hidden layers in deep
    learning architectures.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有其他场景中，ReLU 是深度学习架构中隐藏层的默认选择。
- en: '**[Muhammad Arham](https://www.linkedin.com/in/muhammad-arham-a5b1b1237/)**
    is a Deep Learning Engineer working in Computer Vision and Natural Language Processing.
    He has worked on the deployment and optimizations of several generative AI applications
    that reached the global top charts at Vyro.AI. He is interested in building and
    optimizing machine learning models for intelligent systems and believes in continual
    improvement.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**[穆罕默德·阿赫姆](https://www.linkedin.com/in/muhammad-arham-a5b1b1237/)** 是一位从事计算机视觉和自然语言处理的深度学习工程师。他曾参与多个生成式
    AI 应用的部署和优化，这些应用在 Vyro.AI 达到了全球排行榜的前列。他对构建和优化智能系统的机器学习模型充满兴趣，并相信持续改进。'
- en: '* * *'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT'
- en: '* * *'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[How Activation Functions Work in Deep Learning](https://www.kdnuggets.com/2022/06/activation-functions-work-deep-learning.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习中的激活函数如何工作](https://www.kdnuggets.com/2022/06/activation-functions-work-deep-learning.html)'
- en: '[Diving into the Pool: Unraveling the Magic of CNN Pooling Layers](https://www.kdnuggets.com/diving-into-the-pool-unraveling-the-magic-of-cnn-pooling-layers)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深入了解池化层：揭示 CNN 池化层的魔力](https://www.kdnuggets.com/diving-into-the-pool-unraveling-the-magic-of-cnn-pooling-layers)'
- en: '[Optimizing Python Code Performance: A Deep Dive into Python Profilers](https://www.kdnuggets.com/2023/02/optimizing-python-code-performance-deep-dive-python-profilers.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化 Python 代码性能：深入了解 Python 性能分析器](https://www.kdnuggets.com/2023/02/optimizing-python-code-performance-deep-dive-python-profilers.html)'
- en: '[A Deep Dive into GPT Models: Evolution & Performance Comparison](https://www.kdnuggets.com/2023/05/deep-dive-gpt-models.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深入探讨 GPT 模型：演变与性能比较](https://www.kdnuggets.com/2023/05/deep-dive-gpt-models.html)'
- en: '[Dive into the Future with Kaggle''s AI Report 2023 – See What''s Hot](https://www.kdnuggets.com/dive-into-the-future-with-kaggle-ai-report-2023-see-what-hot)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深入了解Kaggle的AI报告2023 – 了解最新趋势](https://www.kdnuggets.com/dive-into-the-future-with-kaggle-ai-report-2023-see-what-hot)'
- en: '[Unveiling the Potential of CTGAN: Harnessing Generative AI for…](https://www.kdnuggets.com/2023/04/unveiling-potential-ctgan-harnessing-generative-ai-synthetic-data.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[揭示CTGAN的潜力：利用生成式AI生成合成数据](https://www.kdnuggets.com/2023/04/unveiling-potential-ctgan-harnessing-generative-ai-synthetic-data.html)'
