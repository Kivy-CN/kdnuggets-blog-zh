- en: Bayesian Hyperparameter Optimization with tune-sklearn in PyCaret
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyCaret中的tune-sklearn进行贝叶斯超参数优化
- en: 原文：[https://www.kdnuggets.com/2021/03/bayesian-hyperparameter-optimization-tune-sklearn-pycaret.html](https://www.kdnuggets.com/2021/03/bayesian-hyperparameter-optimization-tune-sklearn-pycaret.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/03/bayesian-hyperparameter-optimization-tune-sklearn-pycaret.html](https://www.kdnuggets.com/2021/03/bayesian-hyperparameter-optimization-tune-sklearn-pycaret.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Antoni Baum](https://www.linkedin.com/in/yard1/), Core Contributor to
    PyCaret and Contributor to Ray Tune**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由[Antoni Baum](https://www.linkedin.com/in/yard1/)，PyCaret的核心贡献者及Ray Tune的贡献者**'
- en: '![Image for post](../Images/6d8d95a8fe51f5d25a1b7858b0b0be43.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![帖子图片](../Images/6d8d95a8fe51f5d25a1b7858b0b0be43.png)'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织IT需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Here’s a situation every [PyCaret](https://github.com/pycaret/pycaret) user
    is familiar with: after selecting a promising model or two from `compare_models()`,
    it’s time to tune its hyperparameters to squeeze out all of the model’s potential
    with `tune_model()`.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个每个[PyCaret](https://github.com/pycaret/pycaret)用户都熟悉的情况：在从`compare_models()`中选择了一个或两个有前景的模型后，是时候使用`tune_model()`调整其超参数，以充分发挥模型的潜力了。
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: (If you would like to learn more about PyCaret — an open-source, low-code machine
    learning library in Python, [this guide](https://pycaret.org/guide/) is a good
    place to start.)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: (如果你想了解更多关于PyCaret的信息——这是一个开源的低代码Python机器学习库，[这个指南](https://pycaret.org/guide/)是一个很好的起点。)
- en: By default, `tune_model()` uses the tried and tested `RandomizedSearchCV`from
    scikit-learn. However, not everyone knows about the various advanced options `tune_model()`provides.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`tune_model()`使用的是经过验证的`RandomizedSearchCV`，来自scikit-learn。然而，并非所有人都知道`tune_model()`提供的各种高级选项。
- en: In this post, I will show you how easy it is to use other state-of-the-art algorithms
    with PyCaret thanks to [tune-sklearn](https://github.com/ray-project/tune-sklearn/),
    a drop-in replacement for scikit-learn’s model selection module with cutting edge
    hyperparameter tuning techniques. I’ll also report results from a series of benchmarks,
    showing how tune-sklearn is able to easily improve classification model performance.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将向你展示如何利用[tune-sklearn](https://github.com/ray-project/tune-sklearn/)，这是一个用于scikit-learn模型选择模块的替代品，提供了前沿的超参数调优技术，来轻松使用其他最先进的算法。我还将报告一系列基准测试结果，展示tune-sklearn如何轻松提升分类模型的性能。
- en: Random search vs Bayesian optimization
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机搜索与贝叶斯优化
- en: Hyperparameter optimization algorithms can vary greatly in efficiency.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数优化算法的效率差异可能很大。
- en: 'Random search has been a machine learning staple and for a good reason: it’s
    easy to implement, understand and gives good results in reasonable time. However,
    as the name implies, it is completely random — a lot of time can be spent on evaluating
    bad configurations. Considering that the amount of iterations is limited, it’d
    make sense for the optimization algorithm to focus on configurations that it considers
    promising by taking into account already evaluated configurations.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索一直是机器学习的主流方法，这有充分的理由：它易于实现和理解，并且在合理的时间内能给出良好的结果。然而，正如名称所示，它是完全随机的——大量时间可能花在评估不良配置上。考虑到迭代次数是有限的，优化算法专注于它认为有前景的配置，并考虑到已评估的配置，似乎更有意义。
- en: That is, in essence, the idea of Bayesian optimization (BO). BO algorithms keep
    track of all evaluations and use the data to construct a “surrogate probability
    model”, which can be evaluated a lot faster than a ML model. The more configurations
    have been evaluated, the more informed the algorithm becomes, and the closer the
    surrogate model becomes to the actual objective function. That way the algorithm
    can make an informed choice about which configurations to evaluate next, instead
    of merely sampling random ones. If you would like to learn more about Bayesian
    optimization, check out this [excellent article by Will Koehrsen](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，这就是贝叶斯优化（BO）的核心思想。BO 算法跟踪所有评估，并利用数据构建一个“替代概率模型”，该模型的评估速度比 ML 模型快得多。评估的配置越多，算法的知识越丰富，替代模型就越接近实际的目标函数。这样，算法可以做出明智的选择，决定接下来评估哪些配置，而不仅仅是随机采样。如果你想了解更多关于贝叶斯优化的知识，可以查看[Will
    Koehrsen 的这篇精彩文章](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f)。
- en: Fortunately, PyCaret has built-in [wrappers](https://pycaret.readthedocs.io/en/latest/api/regression.html#pycaret.regression.tune_model) for
    several optimization libraries, and in this article, we’ll be focusing on [tune-sklearn](https://github.com/ray-project/tune-sklearn).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，PyCaret 内置了多个优化库的[封装器](https://pycaret.readthedocs.io/en/latest/api/regression.html#pycaret.regression.tune_model)，在本文中，我们将重点关注[tune-sklearn](https://github.com/ray-project/tune-sklearn)。
- en: '**tune-sklearn in PyCaret**'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**PyCaret 中的 tune-sklearn**'
- en: '[tune-sklearn](https://docs.ray.io/en/master/tune/api_docs/sklearn.html) is
    a drop-in replacement for scikit-learn’s model selection module. tune-sklearn
    provides a scikit-learn based unified API that gives you access to various popular
    state of the art optimization algorithms and libraries, including Optuna and scikit-optimize.
    This unified API allows you to toggle between many different hyperparameter optimization
    libraries with just a single parameter.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[tune-sklearn](https://docs.ray.io/en/master/tune/api_docs/sklearn.html)是 scikit-learn
    模型选择模块的替代品。tune-sklearn 提供了一个基于 scikit-learn 的统一 API，让你可以访问各种流行的先进优化算法和库，包括 Optuna
    和 scikit-optimize。这个统一的 API 允许你通过一个参数在多种超参数优化库之间切换。'
- en: tune-sklearn is powered by [Ray Tune](https://docs.ray.io/en/latest/tune/index.html),
    a Python library for experiment execution and hyperparameter tuning at any scale.
    This means that you can scale out your tuning across multiple machines without
    changing your code.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: tune-sklearn 由[Ray Tune](https://docs.ray.io/en/latest/tune/index.html)驱动，它是一个用于实验执行和超参数调优的
    Python 库，支持任何规模的调优。这意味着你可以在多台机器上扩展调优，而无需更改代码。
- en: To make things even simpler, as of version 2.2.0, tune-sklearn has been integrated
    into PyCaret. You can simply do `pip install "pycaret[full]" `and all of the optional
    dependencies will be taken care of.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更简单，从版本 2.2.0 开始，tune-sklearn 已集成到 PyCaret 中。你只需执行`pip install "pycaret[full]"`，所有可选依赖项将自动处理。
- en: '![Image for post](../Images/76a744240ca14371222bc358c86aa1d9.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![文章图片](../Images/76a744240ca14371222bc358c86aa1d9.png)'
- en: How it all works together
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如何协同工作
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Just by adding two arguments to `tune_model()`you can switch from random search
    to tune-sklearn powered Bayesian optimization through [Hyperopt](http://hyperopt.github.io/hyperopt/) or [Optuna](https://optuna.org/).
    Remember that PyCaret has built-in search spaces for all of the included models,
    but you can always pass your own, if you wish.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 只需向`tune_model()`添加两个参数，你就可以通过[Hyperopt](http://hyperopt.github.io/hyperopt/)或[Optuna](https://optuna.org/)将随机搜索切换为
    tune-sklearn 驱动的贝叶斯优化。请记住，PyCaret 内置了所有包含模型的搜索空间，但你始终可以传递自己的搜索空间（如果你愿意的话）。
- en: But how well do they compare to random search?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，它们与随机搜索相比效果如何呢？
- en: '**A simple experiment**'
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**一个简单的实验**'
- en: In order to see how Bayesian optimization stacks up against random search, I
    have conducted a very simple experiment. Using the [Kaggle House Prices dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/datasets),
    I have created two popular regression models using PyCaret — [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) and [Elastic
    Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html?highlight=elasticnet#sklearn.linear_model.ElasticNet).
    Then, I tuned both of them using scikit-learn’s Random Search and tune-sklearn’s
    Hyperopt and Optuna Searchers (20 iterations for all, minimizing RMSLE). The process
    was repeated three times with different seeds and the results averaged. Below
    is an abridged version of the code — you can find the full code [here](https://gist.github.com/Yard1/97dd054f0a5b154ffdc08df7899ba893).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较贝叶斯优化与随机搜索的效果，我进行了一个非常简单的实验。使用 [Kaggle 房价数据集](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/datasets)，我创建了两个流行的回归模型，分别是
    [随机森林](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)
    和 [弹性网络](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html?highlight=elasticnet#sklearn.linear_model.ElasticNet)。然后，我使用
    scikit-learn 的随机搜索以及 tune-sklearn 的 Hyperopt 和 Optuna 搜索器对它们进行了调优（所有方法进行 20 次迭代，最小化
    RMSLE）。该过程重复三次，使用不同的种子，并对结果进行了平均。下面是代码的简化版本——你可以在 [这里](https://gist.github.com/Yard1/97dd054f0a5b154ffdc08df7899ba893)
    找到完整代码。
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Isn’t it great how easy PyCaret makes things? Anyway, here are the RMSLE scores
    I have obtained on my machine:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: PyCaret 让事情变得如此简单，难道不是很棒吗？无论如何，这里是我在我的机器上获得的 RMSLE 分数：
- en: Experiment’s RMSLE scores
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 实验的 RMSLE 分数
- en: 'And to put it into perspective, here’s the percentage improvement over random
    search:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，下面是相较于随机搜索的百分比改进：
- en: Percentage improvement over random search
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于随机搜索的百分比改进
- en: All of that using the same number of iterations in comparable time. Remember
    that given the stochastic nature of the process, your mileage may vary. If your
    improvement is not noticeable, try increasing the number of iterations (`n_iter`)from
    the default 10\. 20–30 is usually a sensible choice.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些操作都使用了相同的迭代次数，在相近的时间内完成。请记住，由于过程的随机性，你的效果可能会有所不同。如果你的改进不明显，可以尝试将迭代次数（`n_iter`）从默认的
    10 增加到 20–30，这通常是一个明智的选择。
- en: What’s great about Ray is that you can effortlessly scale beyond a single machine
    to a cluster of tens, hundreds or more nodes. While PyCaret doesn’t support full
    Ray integration yet, it is possible to initialize a [Ray cluster](https://docs.ray.io/en/master/cluster/index.html) before
    tuning — and tune-sklearn will automatically use it.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 的优点在于你可以轻松地将计算从单台机器扩展到由数十、数百或更多节点组成的集群。虽然 PyCaret 目前尚不完全支持 Ray 集成，但可以在调整之前初始化一个
    [Ray 集群](https://docs.ray.io/en/master/cluster/index.html)——并且 tune-sklearn 将自动使用它。
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Provided all of the necessary configuration is in place ( `RAY_ADDRESS`environment
    variable), nothing more is needed in order to leverage the power of Ray’s distributed
    computing for hyperparameter tuning. Because hyperparameter optimization is usually
    the most performance-intensive part of creating an ML model, distributed tuning
    with Ray can save you a lot of time.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 只要所有必要的配置就位（`RAY_ADDRESS` 环境变量），就无需其他操作即可利用 Ray 的分布式计算进行超参数调整。由于超参数优化通常是创建 ML
    模型中性能最密集的部分，使用 Ray 进行分布式调整可以为你节省大量时间。
- en: Conclusion
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: In order to speed up hyperparameter optimization in PyCaret, all you need to
    do is install the required libraries and change two arguments in `tune_model() `—
    and thanks to built-in tune-sklearn support, you can easily leverage Ray’s distributed
    computing to scale up beyond your local machine.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加快 PyCaret 中的超参数优化，你只需安装所需的库并更改 `tune_model()` 中的两个参数——多亏了内置的 tune-sklearn
    支持，你可以轻松利用 Ray 的分布式计算，超越本地机器的限制。
- en: Make sure to check out the documentation for [PyCaret](https://pycaret.readthedocs.io/), [Ray
    Tune](https://docs.ray.io/en/latest/tune/index.html) and [tune-sklearn](https://docs.ray.io/en/latest/tune/api_docs/sklearn.html) as
    well as the GitHub repositories of [PyCaret](https://github.com/pycaret/pycaret) and [tune-sklearn](https://github.com/ray-project/tune-sklearn).
    Finally, if you have any questions or want to connect with the community, join [PyCaret’s
    Slack](https://pycaret.slack.com/) and [Ray’s Discourse](https://discuss.ray.io/).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 请务必查看[PyCaret](https://pycaret.readthedocs.io/)、[Ray Tune](https://docs.ray.io/en/latest/tune/index.html)和[tune-sklearn](https://docs.ray.io/en/latest/tune/api_docs/sklearn.html)的文档，以及[PyCaret](https://github.com/pycaret/pycaret)和[tune-sklearn](https://github.com/ray-project/tune-sklearn)的GitHub仓库。最后，如果你有任何问题或想与社区联系，请加入[PyCaret的Slack](https://pycaret.slack.com/)和[Ray的Discourse](https://discuss.ray.io/)。
- en: '*Thanks to Richard Liaw and Moez Ali for proofreading and advice.*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢Richard Liaw和Moez Ali的校对和建议。*'
- en: '**Bio: [Antoni Baum](https://www.linkedin.com/in/yard1/)** is a Computer Science
    and Econometrics MSc student, as well as a core contributor to PyCaret and contributor
    to Ray Tune.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介: [Antoni Baum](https://www.linkedin.com/in/yard1/)** 是计算机科学与计量经济学硕士生，同时也是PyCaret的核心贡献者和Ray
    Tune的贡献者。'
- en: '[Original](https://medium.com/distributed-computing-with-ray/bayesian-hyperparameter-optimization-with-tune-sklearn-in-pycaret-a33b1592662f).
    Reposted with permission.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/distributed-computing-with-ray/bayesian-hyperparameter-optimization-with-tune-sklearn-in-pycaret-a33b1592662f)。经许可转载。'
- en: '**Related:**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关:**'
- en: '[Algorithms for Advanced Hyper-Parameter Optimization/Tuning](/2020/11/algorithms-for-advanced-hyper-parameter-optimization-tuning.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[高级超参数优化/调优算法](/2020/11/algorithms-for-advanced-hyper-parameter-optimization-tuning.html)'
- en: '[5 Tools for Effortless Data Science](/2021/01/5-tools-effortless-data-science.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[轻松的数据科学的5种工具](/2021/01/5-tools-effortless-data-science.html)'
- en: '[5 Things You Are Doing Wrong in PyCaret](/2020/11/5-things-doing-wrong-pycaret.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你在PyCaret中做错的5件事](/2020/11/5-things-doing-wrong-pycaret.html)'
- en: More On This Topic
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Hyperparameter Optimization: 10 Top Python Libraries](https://www.kdnuggets.com/2023/01/hyperparameter-optimization-10-top-python-libraries.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[超参数优化：10个顶级Python库](https://www.kdnuggets.com/2023/01/hyperparameter-optimization-10-top-python-libraries.html)'
- en: '[Introduction to Binary Classification with PyCaret](https://www.kdnuggets.com/2021/12/introduction-binary-classification-pycaret.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用PyCaret进行二分类简介](https://www.kdnuggets.com/2021/12/introduction-binary-classification-pycaret.html)'
- en: '[Introduction to Clustering in Python with PyCaret](https://www.kdnuggets.com/2021/12/introduction-clustering-python-pycaret.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用PyCaret进行Python聚类简介](https://www.kdnuggets.com/2021/12/introduction-clustering-python-pycaret.html)'
- en: '[Announcing PyCaret 3.0: Open-source, Low-code Machine Learning in Python](https://www.kdnuggets.com/2023/03/announcing-pycaret-30-opensource-lowcode-machine-learning-python.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[宣布PyCaret 3.0：Python中的开源、低代码机器学习](https://www.kdnuggets.com/2023/03/announcing-pycaret-30-opensource-lowcode-machine-learning-python.html)'
- en: '[Getting Started with PyCaret](https://www.kdnuggets.com/2022/11/getting-started-pycaret.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[开始使用PyCaret](https://www.kdnuggets.com/2022/11/getting-started-pycaret.html)'
- en: '[Hyperparameter Tuning Using Grid Search and Random Search in Python](https://www.kdnuggets.com/2022/10/hyperparameter-tuning-grid-search-random-search-python.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用网格搜索和随机搜索进行Python中的超参数调优](https://www.kdnuggets.com/2022/10/hyperparameter-tuning-grid-search-random-search-python.html)'
