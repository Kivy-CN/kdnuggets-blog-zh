- en: 'GPT-4: 8 Models in One; The Secret is Out'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/08/gpt4-8-models-one-secret.html](https://www.kdnuggets.com/2023/08/gpt4-8-models-one-secret.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The GPT4 model has been THE groundbreaking model so far, available to the general
    public either for free or through their commercial portal (for public beta use).
    It has worked wonders in igniting new project ideas and use-cases for many entrepreneurs
    but the secrecy about the number of parameters and the model was killing all enthusiasts
    who were betting on the first 1 trillion parameter model to 100 trillion parameter
    claims!
  prefs: []
  type: TYPE_NORMAL
- en: The cat is out of the bag
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well, the cat is out of the bag (Sort of). On June 20th, [George Hotz](https://twitter.com/swyx/status/1671272883379908608),
    founder of self-driving startup Comma.ai leaked that GPT-4 isn’t a single monolithic
    dense model (like GPT-3 and GPT-3.5) but a mixture of 8 x 220-billion-parameter
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Later that day, [Soumith Chintala](https://twitter.com/soumithchintala/status/1671267150101721090),
    co-founder of PyTorch at Meta, reaffirmed the leak.
  prefs: []
  type: TYPE_NORMAL
- en: Just the day before, [Mikhail Parakhin](https://twitter.com/MParakhin/status/1670666605427298304),
    Microsoft Bing AI lead, had also hinted at this.
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT 4: Not a Monolith'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What do all the tweets mean? The GPT-4 is not a single large model but a union/ensemble
    of 8 smaller models sharing the expertise. Each of these models is rumored to
    be 220 Billion parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '![GPT-4: 8 Models in One; The Secret is Out](../Images/adc4c3c626cdea18f09ee3feca386139.png)'
  prefs: []
  type: TYPE_IMG
- en: The methodology is called a mixture of experts' model paradigms (linked below).
    It's a well-known methodology also called as hydra of model. It reminds me of
    Indian mythology I will go with Ravana.
  prefs: []
  type: TYPE_NORMAL
- en: Please take it with a grain of salt that it is not official news but significantly
    high-ranking members in the AI community have spoken/hinted towards it. Microsoft
    is yet to confirm any of these.
  prefs: []
  type: TYPE_NORMAL
- en: What is a Mixture of Experts paradigm?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have spoken about the mixture of experts, let's take a little bit
    of a dive into what that thing is. The Mixture of Experts is an ensemble learning
    technique developed specifically for neural networks. It differs a bit from the
    general ensemble technique from the conventional machine learning modeling (that
    form is a generalized form). So you can consider that the Mixture of Experts in
    LLMs is a special case for ensemble methods.
  prefs: []
  type: TYPE_NORMAL
- en: In short, in this method, a task is divided into subtasks, and experts for each
    subtask are used to solve the models. It is a way to divide and conquer approach
    while creating decision trees. One could also consider it as meta-learning on
    top of the expert models for each separate task.
  prefs: []
  type: TYPE_NORMAL
- en: A smaller and better model can be trained for each sub-task or problem type.
    A meta-model learns to use which model is better at predicting a particular task.
    Meta learner/model acts as a traffic cop. The sub-tasks may or may not overlap,
    which means that a combination of the outputs can be merged together to come up
    with the final output.
  prefs: []
  type: TYPE_NORMAL
- en: '***For the concept-descriptions from MOE to Pooling, all credits to the great
    blog by Jason Brownlee (***[***https://machinelearningmastery.com/mixture-of-experts/***](https://machinelearningmastery.com/mixture-of-experts/)***).
    If you like what you read below, please please subscribe to Jason’s blog and buy
    a book or two to support his amazing work!***'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mixture of experts**, MoE or ME for short, is an ensemble learning technique
    that implements the idea of training experts on subtasks of a predictive modeling
    problem.'
  prefs: []
  type: TYPE_NORMAL
- en: '*In the neural network community, several researchers have examined the decomposition
    methodology. […] Mixture–of–Experts (ME) methodology that decomposes the input
    space, such that each expert examines a different part of the space. […] A gating
    network is responsible for combining the various experts.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Page 73, [Pattern Classification Using Ensemble Methods](https://amzn.to/2zxc0F7),
    2010.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four elements to the approach, they are:'
  prefs: []
  type: TYPE_NORMAL
- en: Division of a task into subtasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop an expert for each subtask.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a gating model to decide which expert to use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pool predictions and gating model output to make a prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The figure below, taken from Page 94 of the 2012 book “[Ensemble Methods](https://amzn.to/2XZzrjG),”
    provides a helpful overview of the architectural elements of the method.
  prefs: []
  type: TYPE_NORMAL
- en: How Do 8 Smaller Models in GPT4 Work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The secret “Model of Experts” is out, let's understand why GPT4 is so good!
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ithinkbot.com
  prefs: []
  type: TYPE_NORMAL
- en: '![GPT-4: 8 Models in One; The Secret is Out](../Images/43649f70f4c867fef5175e0c10255984.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of a Mixture of Experts Model with Expert Members and a Gating Network
  prefs: []
  type: TYPE_NORMAL
- en: 'Taken from: Ensemble Methods'
  prefs: []
  type: TYPE_NORMAL
- en: Subtasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step is to divide the predictive modeling problem into subtasks. This
    often involves using domain knowledge. For example, an image could be divided
    into separate elements such as background, foreground, objects, colors, lines,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '*… ME works in a divide-and-conquer strategy where a complex task is broken
    up into several simpler and smaller subtasks, and individual learners (called
    experts) are trained for different subtasks.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Page 94, [Ensemble Methods](https://amzn.to/2XZzrjG), 2012.
  prefs: []
  type: TYPE_NORMAL
- en: For those problems where the division of the task into subtasks is not obvious,
    a simpler and more generic approach could be used. For example, one could imagine
    an approach that divides the input feature space by groups of columns or separates
    examples in the feature space based on distance measures, inliers, and outliers
    for a standard distribution, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: '*… in ME, a key problem is how to find the natural division of the task and
    then derive the overall solution from sub-solutions.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Page 94, [Ensemble Methods](https://amzn.to/2XZzrjG), 2012.
  prefs: []
  type: TYPE_NORMAL
- en: Expert Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, an expert is designed for each subtask.
  prefs: []
  type: TYPE_NORMAL
- en: The mixture of experts approach was initially developed and explored within
    the field of artificial neural networks, so traditionally, experts themselves
    are neural network models used to predict a numerical value in the case of regression
    or a class label in the case of classification.
  prefs: []
  type: TYPE_NORMAL
- en: '*It should be clear that we can “plug in” any model for the expert. For example,
    we can use neural networks to represent both the gating functions and the experts.
    The result is known as a mixture density network.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '— Page 344, [Machine Learning: A Probabilistic Perspective](https://amzn.to/2YrVLmp),
    2012.'
  prefs: []
  type: TYPE_NORMAL
- en: Experts each receive the same input pattern (row) and make a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Gating Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A model is used to interpret the predictions made by each expert and to aid
    in deciding which expert to trust for a given input. This is called the gating
    model, or the gating network, given that it is traditionally a neural network
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The gating network takes as input the input pattern that was provided to the
    expert models and outputs the contribution that each expert should have in making
    a prediction for the input.
  prefs: []
  type: TYPE_NORMAL
- en: '*… the weights determined by the gating network are dynamically assigned based
    on the given input, as the MoE effectively learns which portion of the feature
    space is learned by each ensemble member*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Page 16, [Ensemble Machine Learning](https://amzn.to/2C7syo5), 2012.
  prefs: []
  type: TYPE_NORMAL
- en: The gating network is key to the approach and effectively, the model learns
    to choose the type subtask for a given input and, in turn, the expert to trust
    to make a strong prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '*Mixture-of-experts can also be seen as a classifier selection algorithm, where
    individual classifiers are trained to become experts in some portion of the feature
    space.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Page 16, [Ensemble Machine Learning](https://amzn.to/2C7syo5), 2012.
  prefs: []
  type: TYPE_NORMAL
- en: When neural network models are used, the gating network and the experts are
    trained together such that the gating network learns when to trust each expert
    to make a prediction. This training procedure was traditionally implemented using [expectation
    maximization](https://machinelearningmastery.com/expectation-maximization-em-algorithm/) (EM).
    The gating network might have a softmax output that gives a probability-like confidence
    score for each expert.
  prefs: []
  type: TYPE_NORMAL
- en: '*In general, the training procedure tries to achieve two goals: for given experts,
    to find the optimal gating function; for a given gating function, to train the
    experts on the distribution specified by the gating function.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Page 95, [Ensemble Methods](https://amzn.to/2XZzrjG), 2012.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling Method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, the mixture of expert models must make a prediction, and this is achieved
    using a pooling or aggregation mechanism. This might be as simple as selecting
    the expert with the largest output or confidence provided by the gating network.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, a weighted sum prediction could be made that explicitly combines
    the predictions made by each expert and the confidence estimated by the gating
    network. You might imagine other approaches to making effective use of the predictions
    and gating network output.
  prefs: []
  type: TYPE_NORMAL
- en: '*The pooling/combining system may then choose a single classifier with the
    highest weight, or calculate a weighted sum of the classifier outputs for each
    class, and pick the class that receives the highest weighted sum.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Page 16, [Ensemble Machine Learning](https://amzn.to/2C7syo5), 2012.
  prefs: []
  type: TYPE_NORMAL
- en: Switch Routing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We should also briefly discuss the switch routing approach differs from the
    MoE paper. I am bringing it up as it seems like Microsoft has used a switch routing
    than a Model of Experts to save some computational complexity, but I am happy
    to be proven wrong. When there are more than one expert's models, they may have
    a non-trivial gradient for the routing function (which model to use when). This
    decision boundary is controlled by the switch layer.
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of the switch layer are threefold.
  prefs: []
  type: TYPE_NORMAL
- en: Routing computation is reduced if the token is being routed only to a single
    expert model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The batch size (expert capacity) can be at least halved since a single token
    goes to a single model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The routing implementation is simplified and communications are reduced.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The overlap of the same token to more than 1 expert model is called as the Capacity
    factor. Following is a conceptual depiction of how routing with different expert
    capacity factors works
  prefs: []
  type: TYPE_NORMAL
- en: '![GPT-4: 8 Models in One; The Secret is Out](../Images/6bf5e35af339d7ad184aaf6f25b171e6.png)illustration
    of token routing dynamics. Each expert processes a fixed batch-size'
  prefs: []
  type: TYPE_NORMAL
- en: of tokens modulated by the capacity factor. Each token is routed to the expert
  prefs: []
  type: TYPE_NORMAL
- en: with the highest router probability, but each expert has a fixed batch size
    of
  prefs: []
  type: TYPE_NORMAL
- en: (total tokens/num experts) × capacity factor. If the tokens are unevenly dis-
  prefs: []
  type: TYPE_NORMAL
- en: patched, then certain experts will overflow (denoted by dotted red lines), resulting
  prefs: []
  type: TYPE_NORMAL
- en: in these tokens not being processed by this layer. A larger capacity factor
    allevi-
  prefs: []
  type: TYPE_NORMAL
- en: ates this overflow issue but also increases computation and communication costs
  prefs: []
  type: TYPE_NORMAL
- en: (depicted by padded white/empty slots). (source [https://arxiv.org/pdf/2101.03961.pdf](https://arxiv.org/pdf/2101.03961.pdf))
  prefs: []
  type: TYPE_NORMAL
- en: When compared with the MoE, findings from the MoE and Switch paper suggest that
  prefs: []
  type: TYPE_NORMAL
- en: Switch transformers outperform carefully tuned dense models and MoE transformers
    on a speed-quality basis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Switch transformers have a smaller compute futprint than MoE
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Switch transformers perform better at lower capacity factors (1–1.25).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![GPT-4: 8 Models in One; The Secret is Out](../Images/e0bbf8c21655eb52300d42b844e5d205.png)'
  prefs: []
  type: TYPE_IMG
- en: Concluding thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Two caveats, first, that this is all coming from hearsay, and second, my understanding
    of these concepts is fairly feeble, so I urge readers to take it with a boulder
    of salt.
  prefs: []
  type: TYPE_NORMAL
- en: But what did Microsoft achieve by keeping this architecture hidden? Well, they
    created a buzz, and suspense around it. This might have helped them to craft their
    narratives better. They kept innovation to themselves and avoided others catching
    up to them sooner. The whole idea was likely a usual Microsoft gameplan of thwarting
    competition while they invest 10B into a company.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 performance is great, but it was not an innovative or breakthrough design.
    It was an amazingly clever implementation of the methods developed by engineers
    and researchers topped up by an enterprise/capitalist deployment. OpenAI has neither
    denied or agreed to these claims ([https://thealgorithmicbridge.substack.com/p/gpt-4s-secret-has-been-revealed](https://thealgorithmicbridge.substack.com/p/gpt-4s-secret-has-been-revealed)),
    which makes me think that this architecture for GPT-4 is more than likely the
    reality (which is great!). Just not cool! We all want to know and learn.
  prefs: []
  type: TYPE_NORMAL
- en: A huge credit goes to [Alberto Romero](https://medium.com/u/7ba6be8a3022?source=post_page-----e3d16fd1eee0--------------------------------)
    for bringing this news to the surface and investigating it further by reaching
    out to OpenAI (who did not respond as per the last update. I saw his article on
    Linkedin but the same has been published on Medium too.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Dr. Mandar Karhade, MD. PhD.](https://www.linkedin.com/in/mandarkarhade/)**
    Sr. Director of Advanced Analytics and Data Strategy @Avalere Health. Mandar is
    an experienced Physician Scientist working on the cutting edge implementations
    of the AI to the Life Sciences and Health Care industry for 10+ years. Mandar
    is also part of AFDO/RAPS helping to regulate implantations of AI to the Healthcare.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/towards-artificial-intelligence/gpt-4-8-models-in-one-the-secret-is-out-e3d16fd1eee0).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[HuggingGPT: The Secret Weapon to Solve Complex AI Tasks](https://www.kdnuggets.com/2023/05/hugginggpt-secret-weapon-solve-complex-ai-tasks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Getting Started with LLMOps: The Secret Sauce Behind Seamless Interactions](https://www.kdnuggets.com/getting-started-with-llmops-the-secret-sauce-behind-seamless-interactions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Not-so-Sexy SQL Concepts to Make You Stand Out](https://www.kdnuggets.com/2022/02/not-so-sexy-sql-concepts-stand-out.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Map out your journey towards SAS Certification](https://www.kdnuggets.com/2022/11/sas-map-journey-towards-sas-certification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Don''t Miss Out! Enroll in FREE Courses Before 2023 Ends](https://www.kdnuggets.com/dont-miss-out-enroll-in-free-courses-before-2023-ends)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3 New Prompt Engineering Resources to Check Out](https://www.kdnuggets.com/3-new-prompt-engineering-resources)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
