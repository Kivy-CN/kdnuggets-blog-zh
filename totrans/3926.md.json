["```py\ndef fizzbuzz(n):  # if the number is divisible by 3 as well as by 5 and returns # \"FizzBuzz\"\n    if n % 3 == 0 and n % 5 == 0:\n        return 'FizzBuzz'# If the first condition is not satisfied then it checks if it is # divisible by 3 and return \"Fizz\"\n    elif n % 3 == 0:\n        return 'Fizz'# If both of the above tests are not satisfying, then it will check # whether it is divisible by 5 and return \"Buzz\"\n    elif n % 5 == 0:\n        return 'Buzz'# If all the conditions above do not satisfy then it returns \"Other\"\n    else:\n        return 'Other'\n\n```", "```py\n# Placeholders are the type of variables nodes where data can be \n# fed from outside when we actually run the model\n\n#Placeholder for input data\ninputTensor  = tf.placeholder(tf.float32, [None, 10]) \n# Placeholder for output data  \noutputTensor = tf.placeholder(tf.float32, [None, 4])    \n# The number of neurons which 1st hidden neurons will have i.e. 1000\nNUM_HIDDEN_NEURONS_LAYER_1 = 1000          \n\n# Learning rate, which will be later used to optimize for optmizer function\n# Learning rate defines at what rate the optimizwer function will move towards minima per iteration\n# Less than optimum learning rate will slow down the process where as higher learning rate will have chances of \n# skipping the minima all together. So it will never converge rather it will keep on going back and forth.\nLEARNING_RATE = 0.05                     \n\n# Initializing the weights to Normal Distribution\n# The weights will keep on adjusting towards optimum values in each iteration. Conceptually, weights determine the \n# discrimination factor of a particular variable in the newural network. More the weight, more will be it's \n# contribution in determining the solution.\ndef init_weights(shape):\n    return tf.Variable(tf.random_normal(shape,stddev=0.01))\n\n# Initializing the input to hidden layer weights\n# We will need a total of 10(input layer number) * 100(number of hidden layers)\ninput_hidden_weights  = init_weights([10, NUM_HIDDEN_NEURONS_LAYER_1])\n\n# Initializing the hidden to output layer weights\n# In this case we will need 100(number of hidden neurons layers) * 4(output neurons, we have only 4 categories)\nhidden_output_weights = init_weights([NUM_HIDDEN_NEURONS_LAYER_1, 4])\n\n# Computing values at the hidden layer\n# Matrix multiplication is done and then rectifier neural network activation function is used \n# for regularization of the resulting multiplication\nhidden_layer = tf.nn.relu(tf.matmul(inputTensor, input_hidden_weights))\n\n# Computing values at the output layer\n# Matrix multiplication of hidden layer and output weights it done. \noutput_layer = tf.matmul(hidden_layer, hidden_output_weights)\n\n# Defining Error Function\n# Error function computes the difference between actual output and model output.\n# Here we are calculating the error in the output as compared to the output label\nerror_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_layer, labels=outputTensor))\n\n# Defining Learning Algorithm and Training Parameters\n# We are using Gradient Descent function to optimize the error or to reach the minima.\ntraining = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(error_function)\n\n# Prediction Function\nprediction = tf.argmax(output_layer, 1)\n\n```", "```py\nNUM_OF_EPOCHS = 5000\nBATCH_SIZE = 128\n\ntraining_accuracy = []\n\nwith tf.Session() as sess:\n\n    # Set Global Variables ?\n    # We had only defined the model previously. To run the model, all the variables need to be initialized and run.\n    # Actual computation can only start after the initialization.\n    tf.global_variables_initializer().run()\n\n    for epoch in tqdm_notebook(range(NUM_OF_EPOCHS)):\n\n        #Shuffle the Training Dataset at each epoch\n        #Shuffling is done to have even more randmized data, which adds to the generalization of model even more.\n        p = np.random.permutation(range(len(processedTrainingData)))\n        processedTrainingData  = processedTrainingData[p]\n        processedTrainingLabel = processedTrainingLabel[p]\n\n        # Start batch training\n        # With batch size of 128, there will be total of 900/128 runs in each epoch where 900 is the total \n        # training data.\n        for start in range(0, len(processedTrainingData), BATCH_SIZE):\n            end = start + BATCH_SIZE\n            sess.run(training, feed_dict={inputTensor: processedTrainingData[start:end], \n                                          outputTensor: processedTrainingLabel[start:end]})\n        # Training accuracy for an epoch\n        # We are checking here the accuracy of model after each epoch\n        training_accuracy.append(np.mean(np.argmax(processedTrainingLabel, axis=1) ==\n                             sess.run(prediction, feed_dict={inputTensor: processedTrainingData,\n                                                             outputTensor: processedTrainingLabel})))\n        # Testing\n        predictedTestLabel = sess.run(prediction, feed_dict={inputTensor: processedTestingData})\n\n```", "```py\nwrong   = 0\nright   = 0\n\npredictedTestLabelList = []\n\n#Comparing the predicted value with the actual label in training data\nfor i,j in zip(processedTestingLabel,predictedTestLabel):\n    predictedTestLabelList.append(decodeLabel(j))\n\n    if np.argmax(i) == j:\n        right = right + 1\n    else:\n        wrong = wrong + 1\n\nprint(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n\nprint(\"Testing Accuracy: \" + str(right/(right+wrong)*100))\n\n```", "```py\nErrors: 2  Correct :98\nTesting Accuracy: 98.0 %\n\n```"]