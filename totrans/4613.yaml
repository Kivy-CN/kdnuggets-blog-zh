- en: Pytorch Cheat Sheet for Beginners and Udacity Deep Learning Nanodegree
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初学者和 Udacity 深度学习纳米学位的 PyTorch 备忘单
- en: 原文：[https://www.kdnuggets.com/2019/08/pytorch-cheat-sheet-beginners.html](https://www.kdnuggets.com/2019/08/pytorch-cheat-sheet-beginners.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/08/pytorch-cheat-sheet-beginners.html](https://www.kdnuggets.com/2019/08/pytorch-cheat-sheet-beginners.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Uniqtech](https://theoptips.github.io/uniqtech_1/)**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Uniqtech](https://theoptips.github.io/uniqtech_1/) 提供**'
- en: Getting started with Pytorch using a cohesive, top down approach cheatsheet.
    This cheatsheet should be easier to digest than the official documentation and
    should be a transitional tool to get students and beginners to get started reading
    documentations soon. This article is being improved continuously. It is frequently
    updated and will remain under construction until it is significantly improved.
    Your feedback is appreciated hi@uniqtech.co and mistakes, typos will be promptly
    corrected.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一致的自上而下的方法入门 Pytorch 的备忘单。这份备忘单应该比官方文档更容易消化，并且应作为一个过渡工具，帮助学生和初学者尽快开始阅读文档。本文正在不断改进，频繁更新，并将保持在建设中，直到显著改善。您的反馈非常感谢
    hi@uniqtech.co，错误和拼写错误将会被及时纠正。
- en: 'Big news: we got published on Medium Machine Learning and Data Science homepage.
    Please clap ← and comment to show your support. This cheatsheet below is primarily
    narrative. A PDF JPEG version of a detailed cheatsheet will be released soon,
    posted in this article. Updated June 18, 2019 to make this cheat sheet / tutorial
    more cohesive, we will insert code snippets from a medal winning Kaggle kernel
    to illustrate important Pytorch concepts — [Malaria Detection with Pytorch, an
    image classification, computer vision Kaggle kernel](https://www.kaggle.com/devilsknight/malaria-detection-with-pytorch) [see
    Source 3 below] by author [devilsknight](https://www.kaggle.com/devilsknight)and
    vishnu aka [qwertypsv](https://www.kaggle.com/qwertypsv).'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 大新闻：我们在 Medium 机器学习和数据科学主页上发表了文章。请点击 ← 并评论以表示支持。下面的备忘单主要是叙述性的。详细备忘单的 PDF JPEG
    版本将很快发布，并将发布在本文中。更新于 2019 年 6 月 18 日，为了使本备忘单/教程更具连贯性，我们将插入来自获奖 Kaggle 内核的代码片段，以说明重要的
    PyTorch 概念 — [使用 PyTorch 进行的疟疾检测，一个图像分类计算机视觉 Kaggle 内核](https://www.kaggle.com/devilsknight/malaria-detection-with-pytorch)
    [见 Source 3 下] 作者 [devilsknight](https://www.kaggle.com/devilsknight) 和 vishnu
    aka [qwertypsv](https://www.kaggle.com/qwertypsv)。
- en: '![Figure](../Images/b6bf45cdb45371fd00627cbdd0ae2c1f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/b6bf45cdb45371fd00627cbdd0ae2c1f.png)'
- en: pytorch cheatsheet for beginners by uniqtech
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: uniqtech 提供的初学者 PyTorch 备忘单
- en: Pytorch Defined in Its Own Words
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pytorch 用自己的话定义
- en: '**Pytorch is “An open source deep learning platform that provides a seamless
    path from research prototyping to production deployment.”**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pytorch 是“一个开源深度学习平台，提供从研究原型到生产部署的无缝路径。”**'
- en: 'According to Facebook Research [Source 1], PyTorch is a Python package that
    provides two high-level features:'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 根据 Facebook 研究 [Source 1]，PyTorch 是一个提供两个高级特性的 Python 包：
- en: Tensor computation (like NumPy) with strong GPU acceleration
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 具有强大 GPU 加速的张量计算（如 NumPy）
- en: Deep neural networks built on a tape-based autograd system
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 基于带状自动微分系统构建的深度神经网络
- en: You can reuse your favorite Python packages such as NumPy, SciPy and Cython
    to extend PyTorch when needed.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以在需要时重用你最喜欢的 Python 包，如 NumPy、SciPy 和 Cython，以扩展 PyTorch。
- en: 'Soumith Chintala, Facebook Research Engineer and creator of Pytorch gave some
    interesting facts about Pytorch: autograd used to be written in python, but the
    majority (of code) was changed to C++ (for production readiness). He thinks interesting
    Pytorch 1.0 features are hybrid front end, parsing model for production, using
    Jit compiler to get models production ready for example. Source: Chintala’s interview
    with Udacity learning.'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Soumith Chintala，Facebook 研究工程师和 PyTorch 的创造者，提供了一些关于 PyTorch 的有趣事实：自动微分曾经是用
    Python 编写的，但大多数（代码）已改为 C++（以适应生产）。他认为有趣的 PyTorch 1.0 特性包括混合前端、生产模型解析、使用 Jit 编译器使模型适合生产等。来源：Chintala
    在 Udacity 学习中的访谈。
- en: Key Features
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主要特点
- en: '**Component | Description** [Source 2]'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**组件 | 描述** [Source 2]'
- en: '[**torch**](https://pytorch.org/docs/stable/torch.html): a Tensor library like
    NumPy, with strong GPU support'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[**torch**](https://pytorch.org/docs/stable/torch.html)：一个类似于 NumPy 的张量库，具有强大的
    GPU 支持'
- en: '[**torch.autograd**](https://pytorch.org/docs/stable/autograd.html)** : **a
    tape-based automatic differentiation library that supports all differentiable
    Tensor operations in torch'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[**torch.autograd**](https://pytorch.org/docs/stable/autograd.html)**：** 一个基于带状的自动微分库，支持
    torch 中所有可微分的张量操作'
- en: '[**torch.jit**](https://pytorch.org/docs/stable/jit.html) : a compilation stack
    (TorchScript) to create serializable and optimizable models from PyTorch code'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[**torch.jit**](https://pytorch.org/docs/stable/jit.html)：一个编译堆栈（TorchScript），用于从
    PyTorch 代码创建可序列化和可优化的模型。'
- en: '[**torch.nn**](https://pytorch.org/docs/stable/nn.html): a neural networks
    library deeply integrated with autograd designed for maximum flexibility'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[**torch.nn**](https://pytorch.org/docs/stable/nn.html)：一个深度集成自动求导的神经网络库，设计为最大灵活性。'
- en: '[**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html):
    Python multiprocessing, but with magical memory sharing of torch Tensors across
    processes. Useful for data loading and Hogwild training'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html)：Python
    多进程，但具有跨进程共享 torch 张量的神奇内存功能。适用于数据加载和 Hogwild 训练。'
- en: '[**torch.utils**](https://pytorch.org/docs/stable/data.html): DataLoader and
    other utility functions for convenience'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[**torch.utils**](https://pytorch.org/docs/stable/data.html)：DataLoader 和其他实用函数，方便使用。'
- en: 'Key features: Hybrid Front-End, Distributed Training, Python-First, Tools &
    Libraries.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 主要特性：混合前端、分布式训练、Python 优先、工具和库。
- en: These features are elegantly illustrated with side-by-side code example on the [features
    page](https://pytorch.org/features)!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些功能通过 [功能页面](https://pytorch.org/features) 上的并排代码示例得到了优雅的展示！
- en: '![Figure](../Images/da94f943b753a23621a94585147571a6.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/da94f943b753a23621a94585147571a6.png)'
- en: The features page on pytorch documentation shows elegant code sample to illustrated
    each feature. Also note Python 3 short hand for dot product such as “@”
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch 文档中的功能页面展示了优雅的代码示例以说明每个功能。还需注意 Python 3 的点积简写，如“@”。
- en: Hybrid Front-End allows switching between eager mode and (computation) graph
    mode. Tensorflow used to be graph mode only, which was considered fast and efficient
    but very hard to modify, prototype and research. This gap is closing since Tensorflow
    now also offers eager mode (no more `session run`.)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 混合前端允许在即时模式和（计算）图模式之间切换。Tensorflow 曾经仅支持图模式，这被认为是快速和高效的，但很难修改、原型设计和研究。由于 Tensorflow
    现在也提供即时模式（不再需要`session run`），这个差距正在缩小。
- en: 'Distributed Training: supports GPU, CPU and easy switching between the two.
    (Tensorflow supports TPU in addition. Its own Tensor Processing Unit.)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练：支持 GPU、CPU 和两者之间的轻松切换。（Tensorflow 还支持 TPU，即 Tensor 处理单元。）
- en: 'Python-First: built for python developers. Easily create neural network, run
    deep learning in Pytorch. Tools & Libraries include robust computer vision libraries
    (convolutional neural networks and pretrained models), NLP and more.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Python 优先：为 Python 开发者打造。轻松创建神经网络，在 Pytorch 中运行深度学习。工具和库包括强大的计算机视觉库（卷积神经网络和预训练模型）、自然语言处理等。
- en: Pytorch also includes great features like **torch.tensor** instantiation and
    computation, model, validation, scoring, Pytorch feature to auto calculate gradient
    using autograd which also does all the backpropagation for you, transfer learning
    ready preloaded models and datasets (read our super short effective article on
    transfer learning), and let’s not forget GPU using CUDA.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch 还包括像 **torch.tensor** 实例化和计算、模型、验证、评分、自动计算梯度的 Pytorch 特性（使用 autograd，它也会为你执行所有的反向传播）、迁移学习准备好的预加载模型和数据集（阅读我们关于迁移学习的超短有效文章），还有使用
    CUDA 的 GPU。
- en: When Should You Use Pytorch
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 何时使用 Pytorch
- en: AWS google GCP GPU supports Pytorch as first class citizen
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: AWS、Google 和 GCP 的 GPU 支持 Pytorch 作为一流公民。
- en: Pytorch added production and cloud partner support for 1.0 for AWS, Google Cloud
    Platform, Microsoft Azure. You can now use Pytorch for any deep learning tasks
    including computer vision and NLP, even in production.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch 为 1.0 版本新增了生产和云合作伙伴支持，包括 AWS、Google Cloud Platform 和 Microsoft Azure。现在你可以在任何深度学习任务中使用
    Pytorch，包括计算机视觉和自然语言处理，甚至是在生产环境中。
- en: Because it is so easy to use and pythonic to Senior Data Scientist [Stefan Otte](https://sotte.github.io/) said
    “if you want to have fun, use pytorch”.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因为它易于使用且具有 Python 风格，高级数据科学家 [Stefan Otte](https://sotte.github.io/) 说：“如果你想要有趣的体验，使用
    pytorch”。
- en: Pytorch is also backed by Facebook AI research so if you want to work for Facebook
    data and ML, you should know Pytorch. If you are great with Python and want to
    be an open source contributor Pytorch is also the way to go.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch 也得到了 Facebook AI 研究的支持，因此如果你想在 Facebook 从事数据和机器学习工作，你应该了解 Pytorch。如果你擅长
    Python 并且想成为开源贡献者，Pytorch 也是一个不错的选择。
- en: '**Transfer Learning**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**迁移学习**'
- en: Transfer learning use models to predict the type of the dataset that it wasn’t
    trained on. It can significantly improve training time and accuracy. It can also
    help with the situation where available training data is limited. Pytorch has
    a page dedicated to pre-trained models and its performance across industry standard
    benchmark datasets. Read more in our transfer learning with Pytorch article.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习使用模型来预测未曾训练的数据集的类型。它可以显著提高训练时间和准确性。它还可以帮助处理可用训练数据有限的情况。Pytorch 有一个专门页面，介绍了预训练模型及其在行业标准基准数据集上的性能。请在我们的
    Pytorch 迁移学习文章中阅读更多内容。
- en: '![](../Images/998ce78c4b9cc073011acd533b33cd36.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/998ce78c4b9cc073011acd533b33cd36.png)'
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[Read about all the available models on Pytorch documentation](https://pytorch.org/docs/stable/torchvision/models.html).
    Note the top-1-error, top-5-error i.e. the performance of the models are also
    available.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[阅读 Pytorch 文档中所有可用模型](https://pytorch.org/docs/stable/torchvision/models.html)。请注意，模型的
    top-1-error 和 top-5-error，即模型的性能也可以查看。'
- en: '**Data Science, Academic Research | Presentation Using Jupyter Notebook**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据科学，学术研究 | 使用 Jupyter Notebook 进行演示**'
- en: Since Python has a huge developer community, it is easier to find Python talent
    among students and researchers to transition into Data Science and academic research,
    even writing production code using Pytorch. Eliminate the need to learn another
    language. Many data analysts and scientists are already familiar with Jupyter
    Notebook, on which Pytorch operates perfectly. Read more in our deploying Pytorch
    model to Amazon Web Service SageMaker.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Python 拥有庞大的开发者社区，因此在学生和研究人员中更容易找到转向数据科学和学术研究的 Python 人才，甚至使用 Pytorch 编写生产代码。这消除了学习另一种语言的需求。许多数据分析师和科学家已经熟悉
    Jupyter Notebook，Pytorch 在其上运行得非常完美。请在我们的部署 Pytorch 模型到 Amazon Web Service SageMaker
    中阅读更多内容。
- en: '**Pytorch is a Deep Learning Framework**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pytorch 是一个深度学习框架**'
- en: 'Pytorch is a deep learning framework just like Tensorflow, which means: for
    traditional machine learning models, use another tool for now. Scikit-learn a
    Pythonic deep learning framework with extremely easy-to-use API. The documentation
    is quite good, each page has an example with code snippets at the bottom. Check
    it out. Did you know many Kaggle users including masters still use sklearn train_test_split()
    to split and scaler to pre-process data, sklearn Gradient Boosting Tree or Support
    Vector Machine to benchmark performance, and the top notch high-performance XGBoost
    is notably missing.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch 是一个深度学习框架，就像 Tensorflow 一样，这意味着：对于传统的机器学习模型，目前请使用其他工具。Scikit-learn 是一个
    Python 风格的深度学习框架，具有极易使用的 API。文档非常好，每页底部都有代码片段的示例。去看看。你知道吗，许多 Kaggle 用户包括大师们仍然使用
    sklearn 的 train_test_split() 来拆分和 scaler 来预处理数据，sklearn 的 Gradient Boosting Tree
    或支持向量机来基准测试性能，而顶级高性能的 XGBoost 却显著缺失。
- en: '**Tensorflow.js** and **Tensorflow.lite** gives Tensorflow wings in the browser
    and on mobile devices. Apple just announced CreateML for Swift June 2019\. Mobile
    support is not native yet in Pytorch. Do not dispair. Scroll down to read about [ONNX
    an exchange format that is supported by almost of all of the popular frameworks](http://onnx.ai/supported-tools). [Pytorch
    also has a tutorial on moving a model to mobile, though this road is still bit
    detoured compared to Tensorflow](https://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**Tensorflow.js** 和 **Tensorflow.lite** 让 Tensorflow 在浏览器和移动设备上展翅飞翔。苹果刚刚在 2019
    年 6 月宣布了 Swift 的 CreateML。移动支持在 Pytorch 中尚未原生实现。不要气馁。向下滚动以阅读关于 [ONNX 的内容，它几乎被所有流行框架支持的交换格式](http://onnx.ai/supported-tools)。[Pytorch
    也有一个关于将模型移动到移动设备的教程，尽管这条路与 Tensorflow 相比仍有些绕行](https://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html)。'
- en: Installation Pytorch
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 Pytorch
- en: '![](../Images/b4233f6a8a1afcd239dbe879eb37cfd2.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4233f6a8a1afcd239dbe879eb37cfd2.png)'
- en: For installation tips use the official Pytorch documentation first. [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/) The
    above screenshot is an example of available installations.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于安装技巧，首先使用官方的 Pytorch 文档。[https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)
    以上截图是可用安装的示例。
- en: Using Anaconda to install Pytorch is a great start across all systems including
    and Windows. We were able to install Pytorch with Anaconda on a gaming computer
    and start to use its CUDA GPU feature right away. [Read our Anaconda Cheatsheet
    here](https://medium.com/data-science-bootcamp/anaconda-miniconda-cheatsheet-for-data-scientists-2c1be12f56db).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Hello World in Pytorch is as easy as launching a Google Colab (yes, right on
    Google’s turf), and `import torch` , [check out this shared view only notebook](https://colab.research.google.com/drive/1uK7BnOWj_-MXI4a5h4I9SLMrqhPnG9Qu).
    Modern hosted data science notebooks like Kaggle Kernel and Google Colab all come
    with Pytorch pre-intalled.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'Look Ma: deep learning with no server!'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Prefer Jupyter Notebook based tutorials instead? Getting started using the Udacity
    Intro to Pytorch repo, found at the bottom of this article.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Prefer other installation methods? Binaries, from source, and docker image,
    see Source 2.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '**Code Snippets from Source 3**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`import torch` is needed for core pytorch tasks. You also see torch neural
    nets module `nn`, optimizer `optim` and computer vision module `torchvision`,
    data transformer pipeline `transforms` , `datasets` and existing `models` being
    imported. In this case the Kaggle team also used a `SubsetRandomSampler`, you
    will see in a later snippet how it feeds into the data transformation and loading
    pipeline.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Data Transformation
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Code Snippets from Source 3**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Note that the train, test and validation transformers are similar but different.
    To augment data, training data is randomly rotated, resized and cropped, even
    vertically flipped (in this case a flipped Malaria cell does not negatively affect
    classification results). Because test and validation data should mimic real world
    data, no random noise or flipping is introduced. Just as it is, with center crop.
    Note that the size must match constantly. Deep Learning is a lot of matrix multiplication.
    Dimension size always matters. In image classification tasks, we typically want
    to normalize images according to the pre-trained model or existing dataset we
    will be using.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Why the normalization? And what are those strange numbers? Mean and standard
    deviation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Definitely don’t forget ToTensor to transform all to a pytorch tensor.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Why? Because Pytorch expects it. Read this thread by Chimtala smth creator of
    Pytorch [Source 5].
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: input image is first loaded to range [0, 1] and then this normalization is applied
    to RGB image as described here .. torch vision — Datasets, Transforms and Models
    specific to Computer Vision
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*All pre-trained models expect input images normalized in the same way, i.e.
    mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected
    to be at least 224.*'
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*The images have to be loaded in to a range of [0, 1] and then normalized using
    mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]*'
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*An example of such normalization can be found in the imagenet example here
    [Source 4]*'
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Loading Data Using Train and Test Loaders
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用训练和测试加载器加载数据
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Code Snippet from Source 3**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**源 3 的代码片段**'
- en: '`datasets.ImageFolder` and the `torch.utils.data.DataLoader` work together
    to load train, valid and test data separately, based on batch_size, sampling after
    data transformation in the previous section. Each dataset has its own loader.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`datasets.ImageFolder` 和 `torch.utils.data.DataLoader` 一起工作，以根据 batch_size
    和之前部分中的数据变换来分别加载训练、验证和测试数据。每个数据集都有自己的加载器。'
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Pytorch Model in a Nutshell
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pytorch 模型概述
- en: Using Sequential is one easy way to quickly define a model. A named ordered
    dictionary holds all the layers that are encapsulated in `nn.Sequential`, which
    is then stored in the `model.classifier` variable. This is a quick way to define
    the bare bone of a model but not necessarily the most Pythonic. It helps us illustrate
    a Python model is consisted of fully connected Linear Layers with shape specified
    in (row, col) tuples. ReLU activation layers, Dropout with 20% probability and
    an output Softmax function or LogSoftmax function. Don’t worry about that now.
    All you need to know is that Softmax is usually the last layer of a Deep Learning
    model of **multi-class classification** tasks. The famous ImageNet dataset has
    1000+ classes, so the output of Softmax has at least 1000+ components.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `Sequential` 是快速定义模型的一种简单方法。一个命名的有序字典包含了所有被封装在 `nn.Sequential` 中的层，然后将其存储在
    `model.classifier` 变量中。这是一种快速定义模型基本结构的方法，但不一定是最 Pythonic 的。它帮助我们说明一个 Python 模型由全连接的线性层组成，形状由
    (row, col) 元组指定。ReLU 激活层、20% 概率的 Dropout 和一个输出的 Softmax 或 LogSoftmax 函数。现在不用担心这些。你只需要知道
    Softmax 通常是**多分类任务**深度学习模型的最后一层。著名的 ImageNet 数据集有 1000 多个类别，因此 Softmax 的输出至少有
    1000 个以上的组件。
- en: A collection of fully connected layers with ReLU activation in between some
    dropouts and at last, another fully connected linear layer which feeds into a
    Softmax activation is very typical of a vanilla Deep Learning Neural Network.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一个由全连接层组成的集合，间隔着 ReLU 激活层和一些 Dropout，最后是另一个全连接线性层，这个层连接到一个 Softmax 激活层，是一个典型的普通深度学习神经网络结构。
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In Pytorch it is easy to view the structure of your model just use `print(model_name)`.
    More on that later.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pytorch 中，查看模型结构非常简单，只需使用 `print(model_name)`。稍后会详细介绍。
- en: '**Code Snippet in Source 3**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**源 3 的代码片段**'
- en: As previously mentioned, in transfer learning in the why pytorch section, we
    can use a pretrained model such as resnet50\. Turning gradient off for all layers
    except the last newly added, fully connected layer.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在 pytorch 部分的迁移学习中，我们可以使用如 resnet50 的预训练模型。将所有层的梯度关闭，除了最后新增的全连接层。
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Note that the last layer is 2048 by 2 because we are classifying just two classes:
    true or false, malaria or not.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意最后一层是 2048 x 2，因为我们只是分类两个类别：真或假，疟疾或非疟疾。
- en: The model variable returns a massive ResNet model structure with our customized
    last layer.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 模型变量返回一个大型的 ResNet 模型结构，并包含我们自定义的最后一层。
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the code snippet below, we omitted a lot of details to make the model architecture
    fit on the screen. If you visit the source 3 kernel link, you will see quite a
    few bottlenecks and sequential layers.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，我们省略了很多细节，以便将模型架构适配到屏幕上。如果你访问源 3 的 kernel 链接，你会看到相当多的瓶颈和顺序层。
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here we showed that the last customized layer is indeed 2048 by 2\. Note that
    the layer before the average pooling and relu is 2048, hence 2048 by 2.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了最后的自定义层确实是 2048 x 2。注意，在平均池化和 relu 之前的层是 2048，因此是 2048 x 2。
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Pytorch Training Loop
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pytorch 训练循环
- en: The training loop is perhaps the most characteristic of Pytorch as a deep learning
    framework. In Sklearn can make it go away with `fit` and in Tensorflow with `transform`.
    In Pytorch this part is much more involved.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环可能是 Pytorch 作为深度学习框架的最具特点的部分。在 Sklearn 中可以用 `fit` 来处理，在 Tensorflow 中可以用
    `transform`。在 Pytorch 中，这部分要复杂得多。
- en: '`model.train()` tells your model that you are training the model. So effectively
    layers like dropout, batchnorm etc. which behave different on the train and test
    procedures know what is going on and hence can behave accordingly.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`model.train()` 告诉你的模型你正在训练它。因此，像 dropout、batchnorm 等层在训练和测试过程中表现不同的情况下会知道发生了什么，并能相应地调整行为。'
- en: 'More details: It sets the mode to train (see [source code](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.train)).
    You can call either model.eval() or model.train(mode=False) to tell that you are
    testing. It is somewhat intuitive to expect `train`function to train model but
    it does not do that. It just sets the mode.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 更多细节：它将模式设置为训练模式（见 [源代码](https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.train)）。你可以调用
    `model.eval()` 或 `model.train(mode=False)` 来表明你正在进行测试。期望 `train` 函数来训练模型是有些直观的，但实际上它并不会这么做。它只是设置模式。
- en: '`model.eval()` will notify all your layers that you are in eval mode, that
    way, batchnorm or dropout layers will work in eval model instead of training mode.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`model.eval()` 会通知所有层你处于评估模式，这样，batchnorm 或 dropout 层将在评估模式而不是训练模式下工作。'
- en: '`torch.no_grad()` impacts the autograd engine and deactivate it. It will reduce
    memory usage and speed up computations but you won’t be able to backprop (which
    you don’t want in an eval script). — albanD'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.no_grad()` 影响自动求导引擎并将其停用。它将减少内存使用并加快计算速度，但你将无法进行反向传播（在评估脚本中你不希望进行反向传播）。—
    albanD'
- en: '**Code Snippet Training Loop Source 3**'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码片段训练循环源 3**'
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Check CUDA availability, set criterion to CrossEntropyLoss(), and optimizer
    to Stochastic Gradient Descent for example before starting to train. Note that
    the learning rate starts very small at about `lr=0.001`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练之前，检查 CUDA 可用性，将准则设置为 CrossEntropyLoss()，将优化器设置为 Stochastic Gradient Descent。例如，注意学习率从非常小的
    `lr=0.001` 开始。
- en: Note that the training loop typically takes in number of epochs, model architecture
    optimizer and criterion. The loss has to be zero’ed out first at the start of
    every loop! The big loop iterates through the number of epochs. In each epoch,
    we put the model into training model `model.train()`, move model and data to CUDA
    if needed, `optimizer.zero_grad()` zero out the gradient before starting, predict
    output, use criterion to calculate loss, `loss.backward` to calculate the delta
    weight change based on optimizer, `optimizer.step()` to take one backward step
    to update weight. When validating, turns the model to eval model `model.eval()`.
    Save the model if validation loss has decreased, keep track of the lowest validation
    loss.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，训练循环通常包括周期数、模型架构优化器和准则。损失值必须在每个循环开始时先归零！大循环遍历周期数。在每个周期中，我们将模型设置为训练模式 `model.train()`，如有必要，将模型和数据移到
    CUDA，`optimizer.zero_grad()` 在开始之前将梯度归零，预测输出，使用准则计算损失，`loss.backward` 基于优化器计算权重变化，`optimizer.step()`
    进行一次反向传播步骤以更新权重。在验证时，将模型切换到评估模式 `model.eval()`。如果验证损失有所减少，则保存模型，跟踪最低验证损失。
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Developer Tools
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 开发工具
- en: '[Train on more than one GPU](https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在多个 GPU 上训练](https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html)'
- en: 'Training Loops:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环：
- en: Train and Forward
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和前向传播
- en: '**Free GPU in the Cloud**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**云端免费 GPU**'
- en: Thanks you new cloud technology like Google Colab. Once you set up the notebook,
    you can continue train and monitor on your mobile devices! Cloud choices including
    Google Colab, Kaggle, AWS. Local choices including your own laptop and your gaming
    computer.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢像 Google Colab 这样的新云技术。一旦你设置好笔记本，你可以在移动设备上继续训练和监控！ 云端选择包括 Google Colab、Kaggle
    和 AWS。本地选择包括你的个人笔记本电脑和游戏电脑。
- en: We repurposed an msi NVIDIA GTX 1060 previously for Assassin’s Creed Origin
    :D If you want to know more let us know.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前将一台 MSI NVIDIA GTX 1060 重新用于《刺客信条：起源》 :D 如果你想了解更多，请告诉我们。
- en: Be able to train on a GPU locally has been a major advantage. We were able to
    iterate through parameter tuning combinations fast without interuption. Google
    Colab has a 12-hour timeout as well as a 12GB quota limit. If you are an advanced
    user, be sure to avoid constantly downloading the dataset, instead store it in
    your Google Drive. After deleting your model in Google Drive, be sure to empty
    trash to actually delete it.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 能够在本地 GPU 上进行训练是一个重大优势。我们能够快速进行参数调优组合的迭代而不会中断。Google Colab 有 12 小时的超时限制和 12GB
    的配额限制。如果你是高级用户，务必避免不断下载数据集，而是将其存储在 Google Drive 中。删除 Google Drive 中的模型后，请务必清空回收站以实际删除它。
- en: Regardless of where the model is trained, if the training loss has gone down
    a lot near zero, but the validation loss does not decrease (there’s no test dataset),
    you may want to watch out your model is overfitting and it may be memorizing training
    data. Halt and tune your parameters. Even if you achieve 99% accuracy your model
    may not generalize, hence it’s a possibility that it cannot be used else where.
    This paragraph of information is especially relevant for Udacity students doing
    scholarship challenges and nanodegrees. Be very suspicious of 99% accuracy, but
    do a brief dance to celebrate first.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 无论模型在哪里训练，如果训练损失接近零但验证损失没有减少（没有测试数据集），你可能要注意你的模型是否过拟合，可能在记忆训练数据。暂停并调整你的参数。即使你达到了
    99% 的准确率，你的模型也可能不具备泛化能力，因此可能无法在其他地方使用。特别是对于 Udacity 的奖学金挑战和纳米学位的学生，这段信息尤其相关。对
    99% 的准确率要保持高度怀疑，但可以先庆祝一下。
- en: Forwarding
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向前传播
- en: Pytorch forward pass will actually calculate `y = wx + b` before that we are
    just writing placeholders.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch 向前传播实际上会计算 `y = wx + b`，在此之前我们只是写了占位符。
- en: Useful Pytorch Libraries and Modules and Installations.
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有用的 Pytorch 库和模块及其安装。
- en: '[PRE14]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The Torchvision module is quite powerful. It has image processing and data processing
    codes, Convolutional Neural Nets (CNN), and other pretrained models like ResNet
    and VGG.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Torchvision 模块非常强大。它包含图像处理和数据处理代码、卷积神经网络（CNN）以及其他预训练模型，如 ResNet 和 VGG。
- en: Transformation
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换
- en: '`transforms.ToTensor()` convert data array to Pytorch tensor. Advantage include
    easy to use in CUDA, GPU training.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transforms.ToTensor()` 将数据数组转换为 Pytorch 张量。优点包括在 CUDA 和 GPU 训练中易于使用。'
- en: Developers can [build transformation and transformation pipelines](https://pytorch.org/docs/stable/torchvision/transforms.html) in
    Pytorch. Pipeline is a fancy way to say transformations that are chained together,
    and performed sequentially.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发人员可以在 Pytorch 中 [构建转换和转换管道](https://pytorch.org/docs/stable/torchvision/transforms.html)。管道是指将多个转换串联在一起并顺序执行的一种方式。
- en: Pytorch Convolutional Neural Networks (CNN)
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pytorch 卷积神经网络（CNN）
- en: This section is under construction… check back soon.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 本节正在建设中… 请稍后再查看。
- en: Use Convolutional Neural Networks to complete computer vision, image recognition
    tasks. The transformation and data augmentation APIs are very important, especially
    when training data is limited. Alternatively one can also use many of the preloaded
    model architectures — read our transfer learning section.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使用卷积神经网络完成计算机视觉、图像识别任务。转换和数据增强 API 非常重要，特别是在训练数据有限时。或者，也可以使用许多预加载的模型架构 —— 阅读我们的转移学习部分。
- en: '[PRE15]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Maxpooling layer discards detailed spatial information contained in the original
    image.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化层会丢弃原始图像中包含的详细空间信息。
- en: Inspect Pytorch Models
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查 Pytorch 模型
- en: First init the model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 首先初始化模型。
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In Pytorch, use `print(model_name)` to print out the model and architecture
    of the model. You can easily see what the model is all about.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pytorch 中，使用 `print(model_name)` 来打印模型及其架构。你可以轻松地了解模型的详细信息。
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that each layer is named (numbered and can be queried by this index).We
    used ellipsis to omit model details so that the VGG model can fit on the screen.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，每一层都有名称（编号，并且可以通过这个索引进行查询）。我们使用省略号来省略模型细节，以便 VGG 模型能够适应屏幕。
- en: 'Pro tip: inspecting the model architecture is a must. Transfer learning, in
    a nutshell, is about modifying the last few classifier layers. [Read more in our
    transfer learning article](https://medium.com/data-science-bootcamp/transfer-learning-with-pytorch-code-snippet-load-a-pretrained-model-900374950004).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 专业提示：检查模型架构是必须的。转移学习，简而言之，就是修改最后几个分类器层。[在我们的转移学习文章中阅读更多](https://medium.com/data-science-bootcamp/transfer-learning-with-pytorch-code-snippet-load-a-pretrained-model-900374950004)。
- en: 'Pro tip: for Tensorflow use `keras model.summary()` to review the entire model
    architecture. It even outputs number of parameters and dimensions.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 专业提示：对于 Tensorflow，使用 `keras model.summary()` 来查看整个模型架构。它甚至会输出参数数量和维度。
- en: Advanced Features
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级特性
- en: Pytorch Versions
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch 版本
- en: For Udacity projects, not all nanodegrees have moved to Pytorch 1.0 It is important
    to use the right version for the right project. You may also need to change the
    Kernel in Jupyter Notebook to use corresponding version of Python. Udacity projects
    have moved onto Python3\. Not all projects in the real world has migrated to Python
    3\. But it’s about time to move on from Python 2.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Udacity项目，并非所有的纳米学位都已经迁移到Pytorch 1.0。使用正确版本的Pytorch对相应的项目至关重要。你可能还需要在Jupyter
    Notebook中更改内核，以使用相应版本的Python。Udacity项目已经迁移到Python3。并非所有现实世界中的项目都已迁移到Python 3，但现在是时候从Python
    2迁移过来了。
- en: '**Using CUDA**'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用CUDA**'
- en: Though we decided to put CUDA in the advanced section, but the reality is CUDA
    is so easy to use. Just use it… today! With Anaconda, Pytorch, and CUDA, we were
    able to turn a gaming computer with an NVDIA graphics card into a home deep learning
    powerhouse. No configuration needed! It just works. The framework just works on
    a windows machine! (It is an msi NVIDIA GTX 1060 previously for Assassin’s Creed
    Origin :D If you want to know more let us know.)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们决定将CUDA放在高级部分，但现实是CUDA非常易于使用。今天就使用它吧！通过Anaconda、Pytorch和CUDA，我们将一台配备NVIDIA显卡的游戏电脑变成了家庭深度学习的强大机器。无需配置！它直接工作。这个框架在Windows机器上就能运行！（这是之前用于《刺客信条：起源》的MSI
    NVIDIA GTX 1060 :D 如果你想了解更多，请告诉我们。）
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: One common error of using CUDA with Pytorch is not moving both the model and
    the data to CUDA. And when needed move both of them back to CPU. Generally your
    model and data should always live in the same space.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CUDA与Pytorch的一个常见错误是不将模型和数据同时移动到CUDA上。当需要时将它们都移回CPU。一般来说，你的模型和数据应该始终位于同一个空间。
- en: '**Deploying Pytorch in Production**: There are two methods of turning existing
    Pytorch models to production ready deployment *trace and scripting*. Tracing does
    not support complex models with control flow in the code. Scripting supports Pytorch
    codes with control flow but supports only a limited number of Python modules.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**在生产环境中部署Pytorch**：将现有的Pytorch模型转换为生产就绪的部署有两种方法——*追踪和脚本*。追踪不支持具有代码控制流的复杂模型。脚本支持具有控制流的Pytorch代码，但仅支持有限数量的Python模块。'
- en: '**Choosing the best Softmax result**: in multi-class classification, the activation
    Softmax function is often used. Pytorch has a dedicated function to extract top
    results — the most likely class from Softmax output. `torch.topk(input, k, dim)` returns
    the top probability.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**选择最佳的Softmax结果**：在多类别分类中，通常使用激活函数Softmax。Pytorch有一个专门的函数来提取最顶级的结果——从Softmax输出中最可能的类别。`torch.topk(input,
    k, dim)`返回顶级概率。'
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**Consume Pytorch Models on Other Platforms**'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**在其他平台上使用Pytorch模型**'
- en: '[PRE20]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: “Export models in the standard ONNX (Open Neural Network Exchange) format for
    direct access to ONNX-compatible platforms, runtimes, visualizers, and more.”
    — Pytorch 1.0 Documentation
  id: totrans-146
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “以标准ONNX（开放神经网络交换）格式导出模型，以便直接访问ONNX兼容的平台、运行时、可视化工具等。” — Pytorch 1.0文档
- en: '**More on Pytorch Transfer Learning**'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**更多关于Pytorch迁移学习的内容**'
- en: To use an existing model is equivalent to freeze some of its layers and parameters
    and not train those. Turn off training autograd by setting `require_grad` to False.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用现有模型相当于冻结其某些层和参数，并且不对这些层和参数进行训练。通过将`require_grad`设置为False来关闭训练自动求导。
- en: '[PRE21]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Hyperparameter Tuning**'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**超参数调整**'
- en: In addition to using the right optimizer and adjusting learning rate according.
    You can use the learning rate scheduler to dynamically adjust your learning rate.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用正确的优化器和调整学习率之外，你还可以使用学习率调度器动态调整学习率。
- en: '[PRE22]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[Read more about scheduler here.](https://discuss.pytorch.org/t/how-to-use-torch-optim-lr-scheduler-exponentiallr/12444)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在这里阅读更多关于调度器的信息。](https://discuss.pytorch.org/t/how-to-use-torch-optim-lr-scheduler-exponentiallr/12444)'
- en: '**Model and Check Point Saving**'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型和检查点保存**'
- en: Save and Load Model Checkpoint
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保存和加载模型检查点
- en: 'Pro tip: Did you know you can save and load models locally and in google drive?
    This way you don’t have to start from scratch every time. For example, if you
    already trained 5 epochs. You can save the weights and train another 5 epochs.
    Now you did 10 epochs total! Very convenient. The free GPU resources time out
    and get erased very often. Remember incremental training is possible.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 专业提示：你知道可以在本地和Google Drive中保存和加载模型吗？这样你不必每次都从头开始。例如，如果你已经训练了5个周期，你可以保存权重，然后再训练另外5个周期。现在你总共训练了10个周期！非常方便。免费的GPU资源经常超时并被擦除。记住，增量训练是可能的。
- en: You can also save a checkpoint and load it locally. You may see both extension `.pt` and `.pth` .
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以保存一个检查点并在本地加载它。你可能会看到`.pt`和`.pth`扩展名。
- en: '[PRE23]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Code Snippet from Source 3**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**来自源3的代码片段**'
- en: '[PRE24]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note you must save any checkpoint on Google Colab to your Google Drive, else
    your data may be erased every 12 hours or sooner. Though the GPU access is free,
    the storage is temporary on Google Colab.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你必须将任何检查点保存到 Google Colab 的 Google Drive 中，否则你的数据可能每 12 小时或更早被删除。虽然 GPU
    访问是免费的，但 Google Colab 上的存储是临时的。
- en: Making Predictions with Pytorch
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Pytorch 进行预测
- en: 'Here’s what we don’t like about Pytorch. Making predictions with Pytorch seems
    to be a bit patched together. Writing your own training loop seems easy-to-customize,
    though it is harder to write than Tensorflow, it makes sense to trade a bit of
    discomfort for customization. But prediction is strange which some functions that
    appear to be hacked together. See these code snippets below to see what we mean:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们不喜欢 Pytorch 的地方。使用 Pytorch 进行预测似乎有点拼凑。尽管编写自己的训练循环看似容易自定义，但比 Tensorflow
    更难编写，权衡一下舒适度和自定义性是值得的。然而，预测过程很奇怪，有些函数看起来像是被拼凑在一起的。看看下面的代码片段以了解我们的意思：
- en: '**Code Snippets from Source 3:**'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**来源 3 的代码片段：**'
- en: '[PRE25]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`output.data.max`, `np.sum(np.squeeze())` , `cpu().numpy()`, `.unsqueeze(0)`,
    `torch.argmax` …WTF?! It’s super frustrating.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`output.data.max`，`np.sum(np.squeeze())`，`cpu().numpy()`，`.unsqueeze(0)`，`torch.argmax`
    …… WTF？！这真的很让人沮丧。'
- en: 'The important take away is: we are working with logits converted to probabilities
    here, and there are tensors that need to be turned into matrices and extra brackets
    removed. We need to know which one is most likely class using `max` or `argmax`.
    We need to move tensors back to CPU so `cpu()` and tensor needs to be turned into
    ndarray for ease of computation so `numpy()`.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的要点是：我们在这里处理的是转换为概率的 logits，还有需要转化为矩阵的张量和去除多余的括号。我们需要使用 `max` 或 `argmax` 来确定最可能的类别。我们需要将张量移回
    CPU，因此使用 `cpu()`，并将张量转化为 ndarray 以便计算，使用 `numpy()`。
- en: Pytorch is a deep learning framework, and deep learning is frequently about
    matrix math, which is about getting the dimensions right, so squeeze and unsqueeze
    have to be used to make dimensions match.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch 是一个深度学习框架，而深度学习常涉及矩阵运算，主要是调整维度，因此需要使用 squeeze 和 unsqueeze 来使维度匹配。
- en: '[PRE26]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This part of the cheatsheet will save you a lot of headache and you are welcome!
    `np.squeeze()` removed the extra set of `[]` and an extra dimension out of `[[1,2,3]]`
    of shape `(1, 3)`. Now it is just `(3,)`. A
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分备忘单将为你节省很多麻烦，不客气！`np.squeeze()` 去除了 `[[1,2,3]]` 中额外的 `[]` 和多余的维度，原本的形状是 `(1,
    3)`，现在变成了 `(3,)`。
- en: And `unsqueeze` made `[1]` shape of `(1,)` to `[[1]]` shape `(1,1).`
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 而 `unsqueeze` 将 `[1]` 的形状 `(1,)` 变为 `[[1]]` 的形状 `(1,1)`。
- en: Further Reading
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深入阅读
- en: '[Pytorch Data Science Nanodegree Deep Learning Intro to Pytorch notebooks and
    tutorials by Udacity](https://github.com/udacity/DSND_Term1/tree/master/lessons/DeepLearning/new-intro-to-pytorch).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Pytorch 数据科学纳米学位深度学习简介及 Udacity 的 Pytorch 笔记和教程](https://github.com/udacity/DSND_Term1/tree/master/lessons/DeepLearning/new-intro-to-pytorch)。'
- en: '[Transfer Learning with Pytorch — Our super short effective article](https://medium.com/data-science-bootcamp/transfer-learning-with-pytorch-code-snippet-load-a-pretrained-model-900374950004)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Pytorch 进行迁移学习 — 我们超短而有效的文章](https://medium.com/data-science-bootcamp/transfer-learning-with-pytorch-code-snippet-load-a-pretrained-model-900374950004)'
- en: Source 1: [https://research.fb.com/downloads/pytorch/](https://research.fb.com/downloads/pytorch/)
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '来源 1: [https://research.fb.com/downloads/pytorch/](https://research.fb.com/downloads/pytorch/)'
- en: 'Source 2: Pytorch Github main [https://github.com/pytorch/pytorch#getting-started](https://github.com/pytorch/pytorch#getting-started) also
    makes a great cheat sheet.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '来源 2: Pytorch Github 主页面 [https://github.com/pytorch/pytorch#getting-started](https://github.com/pytorch/pytorch#getting-started)
    也提供了很好的备忘单。'
- en: Extremely awesome forum including many active core contributors, supplements
    the documentations [https://discuss.pytorch.org/](https://discuss.pytorch.org/)
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极其棒的论坛，包括许多活跃的核心贡献者，补充了文档 [https://discuss.pytorch.org/](https://discuss.pytorch.org/)
- en: 'Source 3: Malaria Detection with Pytorch kaggle Kernel [https://www.kaggle.com/devilsknight/malaria-detection-with-pytorch](https://www.kaggle.com/devilsknight/malaria-detection-with-pytorch)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '来源 3: 使用 Pytorch 的疟疾检测 kaggle 内核 [https://www.kaggle.com/devilsknight/malaria-detection-with-pytorch](https://www.kaggle.com/devilsknight/malaria-detection-with-pytorch)'
- en: Source 4: [https://github.com/pytorch/examples/blob/97304e232807082c2e7b54c597615dc0ad8f6173/imagenet/main.py#L197-L198](https://github.com/pytorch/examples/blob/97304e232807082c2e7b54c597615dc0ad8f6173/imagenet/main.py#L197-L198)
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '来源 4: [https://github.com/pytorch/examples/blob/97304e232807082c2e7b54c597615dc0ad8f6173/imagenet/main.py#L197-L198](https://github.com/pytorch/examples/blob/97304e232807082c2e7b54c597615dc0ad8f6173/imagenet/main.py#L197-L198)'
- en: 'Source 5: Pytorch community forum Pytorch image transform normalization [https://discuss.pytorch.org/t/whats-the-range-of-the-input-value-desired-to-use-pretrained-resnet152-and-vgg19/1683](https://discuss.pytorch.org/t/whats-the-range-of-the-input-value-desired-to-use-pretrained-resnet152-and-vgg19/1683)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '来源 5: Pytorch 社区论坛 Pytorch 图像变换标准化 [https://discuss.pytorch.org/t/whats-the-range-of-the-input-value-desired-to-use-pretrained-resnet152-and-vgg19/1683](https://discuss.pytorch.org/t/whats-the-range-of-the-input-value-desired-to-use-pretrained-resnet152-and-vgg19/1683)'
- en: '**[Uniqtech](https://theoptips.github.io/uniqtech_1/)**: We write beginner
    friendly cheatsheets like this all the time. Follow our profile and our most popular
    Data Science Bootcamp publication. Check out our one page article on Transfer
    Learning in Pytorch, Pytorch on Amazon SageMaker, and Anaconda Cheatsheet for
    Data Science. We are primarily on Medium, a community we love and find strong
    affinity in. Medium treats its writers well, and has a phenomenal reader community.
    If you would like to find out about our upcoming Data Science Bootcamp course
    releasing Fall 2019, scholarship for high quality articles, or want to write for
    us, contribute feedback please email us [hi@uniqtech.co](mailto:hi@uniqtech.co).
    Thank you Medium community!'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Uniqtech](https://theoptips.github.io/uniqtech_1/)**: 我们经常编写像这样的适合初学者的 Cheat
    Sheets。关注我们的个人资料以及我们最受欢迎的数据科学训练营出版物。查看我们关于 Pytorch 中迁移学习、Amazon SageMaker 上的 Pytorch
    和数据科学的 Anaconda Cheat Sheet 的单页文章。我们主要在 Medium 上活跃，这是一个我们喜爱并有强烈认同感的社区。Medium 对其作者很好，并拥有一个极好的读者社区。如果您想了解我们即将推出的数据科学训练营课程（2024
    年秋季发布）、高质量文章的奖学金，或者想为我们写作、提供反馈，请通过 [hi@uniqtech.co](mailto:hi@uniqtech.co) 发送电子邮件给我们。感谢
    Medium 社区！'
- en: Key author and contributor [Sun](https://medium.com/u/16c164f26caa?source=post_page---------------------------).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 主要作者和贡献者 [Sun](https://medium.com/u/16c164f26caa?source=post_page---------------------------).
- en: '[Original](https://medium.com/@uniqtech/pytorch-cheat-sheet-for-beginners-and-udacity-deep-learning-nanodegree-5aadc827de82).
    Reposted with permission.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/@uniqtech/pytorch-cheat-sheet-for-beginners-and-udacity-deep-learning-nanodegree-5aadc827de82).
    经许可转载。'
- en: '**Related:**'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Deep Learning Cheat Sheets](/2018/11/deep-learning-cheat-sheets.html)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习 Cheat Sheets](/2018/11/deep-learning-cheat-sheets.html)'
- en: '[Getting started with NLP using the PyTorch framework](/2019/04/nlp-pytorch.html)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 PyTorch 框架进行 NLP 入门](/2019/04/nlp-pytorch.html)'
- en: '[3 More Google Colab Environment Management Tips](/2019/01/more-google-colab-environment-management-tips.html)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3 个更多的 Google Colab 环境管理技巧](/2019/01/more-google-colab-environment-management-tips.html)'
- en: More On This Topic
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多主题
- en: '[Streamlit for Machine Learning Cheat Sheet](https://www.kdnuggets.com/2023/01/streamlit-machine-learning-cheat-sheet.html)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Streamlit 机器学习 Cheat Sheet](https://www.kdnuggets.com/2023/01/streamlit-machine-learning-cheat-sheet.html)'
- en: '[Machine Learning with ChatGPT Cheat Sheet](https://www.kdnuggets.com/2023/05/machine-learning-chatgpt-cheat-sheet.html)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[与 ChatGPT 结合的机器学习 Cheat Sheet](https://www.kdnuggets.com/2023/05/machine-learning-chatgpt-cheat-sheet.html)'
- en: '[Scikit-learn for Machine Learning Cheat Sheet](https://www.kdnuggets.com/2022/12/scikit-learn-machine-learning-cheatsheet.html)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Scikit-learn 机器学习 Cheat Sheet](https://www.kdnuggets.com/2022/12/scikit-learn-machine-learning-cheatsheet.html)'
- en: '[The KDnuggets 2023 Cheat Sheet Collection](https://www.kdnuggets.com/the-kdnuggets-2023-cheat-sheet-collection)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 2023 Cheat Sheet 集合](https://www.kdnuggets.com/the-kdnuggets-2023-cheat-sheet-collection)'
- en: '[Data Cleaning with Python Cheat Sheet](https://www.kdnuggets.com/2023/02/data-cleaning-python-cheat-sheet.html)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 数据清洗 Cheat Sheet](https://www.kdnuggets.com/2023/02/data-cleaning-python-cheat-sheet.html)'
- en: '[The ChatGPT Cheat Sheet](https://www.kdnuggets.com/2023/01/chatgpt-cheat-sheet.html)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatGPT Cheat Sheet](https://www.kdnuggets.com/2023/01/chatgpt-cheat-sheet.html)'
