- en: 7 Steps to Mastering Large Language Model Fine-tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/7-steps-to-mastering-large-language-model-fine-tuning](https://www.kdnuggets.com/7-steps-to-mastering-large-language-model-fine-tuning)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![7 Steps to Mastering Large Language Model Fine-tuning](../Images/13b32ea8eeb1fe6ceb3783aa8acd200e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Over the recent year and a half, the landscape of natural language processing
    (NLP) has seen a remarkable evolution, mostly thanks to the rise of Large Language
    Models (LLMs) like OpenAI’s GPT family.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: These powerful models have revolutionized our approach to handling natural language
    tasks, offering unprecedented capabilities in translation, sentiment analysis,
    and automated text generation. Their ability to understand and generate human-like
    text has opened up possibilities once thought unattainable.
  prefs: []
  type: TYPE_NORMAL
- en: However, despite their impressive capabilities, the journey to train these models
    is full of challenges, such as the significant time and financial investments
    required.
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to the critical role of fine-tuning LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: By refining these pre-trained models to better suit specific applications or
    domains, we can significantly enhance their performance on particular tasks. This
    step not only elevates their quality but also extends their utility across a wide
    array of sectors.
  prefs: []
  type: TYPE_NORMAL
- en: This guide aims to break down this process into 7 simple steps to get any LLM
    fine-tuned for a specific task.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Pre-trained Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs are a specialized category of ML algorithms designed to predict the next
    word in a sequence based on the context provided by the preceding words. These
    models are built upon the Transformers architecture, a breakthrough in machine
    learning techniques and first explained in Google’s [All you need is attention](https://arxiv.org/abs/1706.03762)
    article.
  prefs: []
  type: TYPE_NORMAL
- en: Models like GPT (Generative Pre-trained Transformer) are examples of pre-trained
    language models that have been exposed to large volumes of textual data. This
    extensive training allows them to capture the underlying rules of language usage,
    including how words are combined to form coherent sentences.
  prefs: []
  type: TYPE_NORMAL
- en: '![7 Steps to Mastering Large Language Model Fine-tuning](../Images/3bb692738fdcc3d7882e4f855e864edd.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: A key strength of these models lies in their ability to not only understand
    natural language but also to produce text that closely mimics human writing based
    on the inputs they are given.
  prefs: []
  type: TYPE_NORMAL
- en: So what’s the best of this?
  prefs: []
  type: TYPE_NORMAL
- en: These models are already open to the masses using APIs.
  prefs: []
  type: TYPE_NORMAL
- en: What is Fine-tuning, and Why is it Important?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fine-tuning is the process of picking a pre-trained model and improving it with
    further training on a domain-specific dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Most LLM models have very good natural language skills and generic knowledge
    performance but fail in specific task-oriented problems. The fine-tuning process
    offers an approach to improve model performance for specific problems while lowering
    computation expenses without the necessity of building them from the ground up.
  prefs: []
  type: TYPE_NORMAL
- en: '![7 Steps to Mastering Large Language Model Fine-tuning](../Images/3aca95c4978bdad858e24830287394ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: To put it simply, Fine-tuning tailors the model to have a better performance
    for specific tasks, making it more effective and versatile in real-world applications.
    This process is essential for improving an existing model for a particular task
    or domain.
  prefs: []
  type: TYPE_NORMAL
- en: A Step-by-Step Guide to Fine-tuning a LLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s exemplify this concept by fine-tuning a real model in only 7 steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Having our concrete objective clear'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine we want to infer the sentiment of any text and decide to try GPT-2 for
    such a task.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’m pretty sure there’s no surprise that we will soon enough detect it is quite
    bad at doing so. Then, one natural question that comes to mind is:'
  prefs: []
  type: TYPE_NORMAL
- en: Can we do something to improve its performance?
  prefs: []
  type: TYPE_NORMAL
- en: And of course, the answer is that we can!
  prefs: []
  type: TYPE_NORMAL
- en: Taking advantage of fine-tuning by training our pre-trained GPT-2 model from
    the Hugging Face Hub with a dataset containing tweets and their corresponding
    sentiments so the performance improves.
  prefs: []
  type: TYPE_NORMAL
- en: So our ultimate goal is **to have a model that is good at inferring the sentiment
    out of text. **
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Choose a pre-trained model and a dataset'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The second step is to pick what model to take as a base model. In our case,
    we already picked the model: GPT-2\. So we are going to perform some simple fine-tuning
    to it.'
  prefs: []
  type: TYPE_NORMAL
- en: '![7 Steps to Mastering Large Language Model Fine-tuning](../Images/bb120b72862dd720822dd420560db0f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of Hugging Face Datasets Hub. Selecting OpenAI’s GPT2 model.
  prefs: []
  type: TYPE_NORMAL
- en: Always keep in mind to select a model that fits your task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Load the data to use'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have both our model and our main task, we need some data to work
    with.
  prefs: []
  type: TYPE_NORMAL
- en: But no worries, Hugging Face has everything arranged!
  prefs: []
  type: TYPE_NORMAL
- en: This is where their dataset library kicks in.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we will take advantage of the Hugging Face dataset library
    to import a dataset with tweets labeled with their corresponding sentiment (Positive,
    Neutral or Negative).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The data looks like follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![7 Steps to Mastering Large Language Model Fine-tuning](../Images/289efc50c9aa54035a8390b86c86070f.png)'
  prefs: []
  type: TYPE_IMG
- en: The data set to be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Tokenizer'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we have both our model and the dataset to fine-tune it. So the following
    natural step is to load a tokenizer. As LLMs work with tokens (and not with words!!),
    we require a tokenizer to send the data to our model.
  prefs: []
  type: TYPE_NORMAL
- en: We can easily perform this by taking advantage of the map method to tokenize
    the whole dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**BONUS:** To improve our processing performance, two smaller subsets are generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The training set:** To fine-tune our model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The testing set:**  To evaluate it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 5: Initialize our base model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once we have the dataset to be used, we load our model and specify the number
    of expected labels. From the Tweet’s sentiment dataset, you can know there are
    three possible labels:'
  prefs: []
  type: TYPE_NORMAL
- en: 0 or Negative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 or Neutral
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2  or Positive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 6: Evaluate method'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Transformers library provides a class called “Trainer” that optimizes both
    the training and the evaluation of our model. Therefore, before the actual training
    is begun, we need to define a function to evaluate the fine-tuned model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 7: Fine-tune using the Trainer Method'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final step is fine-tuning the model. To do so, we set up the training arguments
    together with the evaluation strategy and execute the Trainer object.
  prefs: []
  type: TYPE_NORMAL
- en: To execute the Trainer object we just use the train() command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Once our model has been fine-tuned, we use the test set to evaluate its performance.
    The trainer object already contains an optimized evaluate() method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This is a basic process to perform a fine-tuning of any LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Also, remember that the process of fine-tuning a LLM is highly computationally
    demanding, so your local computer may not have enough power to perform it.
  prefs: []
  type: TYPE_NORMAL
- en: Main Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Today, fine-tuning pre-trained large language models like GPT for specific tasks
    is crucial to enhancing LLMs performance in specific domains. It allows us to
    take advantage of their natural language power while improving their efficiency
    and the potential for customization, making the process accessible and cost-effective.
  prefs: []
  type: TYPE_NORMAL
- en: Following these simple 7 steps —from selecting the right model and dataset to
    training and evaluating the fine-tuned model— we can achieve a superior model
    performance in specific domains.
  prefs: []
  type: TYPE_NORMAL
- en: For those who want to check the full code, it is available in my l[arge language
    models GitHub repo. ](https://github.com/rfeers/large-language-models/blob/main/7%20Steps%20to%20Fine-Tune%20LLMs.ipynb)
  prefs: []
  type: TYPE_NORMAL
- en: '**[](https://www.linkedin.com/in/josep-ferrer-sanchez/)**[Josep Ferrer](https://www.linkedin.com/in/josep-ferrer-sanchez)****
    is an analytics engineer from Barcelona. He graduated in physics engineering and
    is currently working in the data science field applied to human mobility. He is
    a part-time content creator focused on data science and technology. Josep writes
    on all things AI, covering the application of the ongoing explosion in the field.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[7 Steps to Mastering Large Language Models (LLMs)](https://www.kdnuggets.com/7-steps-to-mastering-large-language-models-llms)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[RAG vs Finetuning: Which Is the Best Tool to Boost Your LLM Application?](https://www.kdnuggets.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Ultimate Open-Source Large Language Model Ecosystem](https://www.kdnuggets.com/2023/05/ultimate-opensource-large-language-model-ecosystem.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introduction to NExT-GPT: Any-to-Any Multimodal Large Language Model](https://www.kdnuggets.com/introduction-to-nextgpt-anytoany-multimodal-large-language-model)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Exploring the Zephyr 7B: A Comprehensive Guide to the Latest Large…](https://www.kdnuggets.com/exploring-the-zephyr-7b-a-comprehensive-guide-to-the-latest-large-language-model)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Free Mastery Course: Become a Large Language Model Expert](https://www.kdnuggets.com/ree-mastery-course-become-a-large-language-model-expert)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
