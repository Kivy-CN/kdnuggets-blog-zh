- en: Programming Best Practices For Data Science
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2018/08/programming-best-practices-data-science.html](https://www.kdnuggets.com/2018/08/programming-best-practices-data-science.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: '**By Srini Kadamati, [Dataquest.io](https://www.dataquest.io/)**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'The data science life cycle is generally comprised of the following components:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: data retrieval
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: data cleaning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: data exploration and visualization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: statistical or predictive modeling
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: While these components are helpful for understanding the different phases, they
    don't help us think about our *programming* workflow.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Often, the entire data science life cycle ends up as an arbitrary mess of notebook
    cells in *either*a Jupyter Notebook or a single messy script. In addition, most
    data science problems require us to switch between data retrieval, data cleaning,
    data exploration, data visualization, and statistical / predictive modeling.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'But there''s a better way! In this post, I''ll go over the two mindsets most
    people switch between when doing programming work specifically for data science:
    the **prototype** mindset and the **production** mindset.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '| Prototype mindset prioritizes: | Production mindset prioritizes: |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
- en: '| iteration speed on small pieces of code | iteration speed on the full pipeline
    |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
- en: '| less abstraction (directly modifying code and data objects) | more abstraction
    (modifying parameter values instead) |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
- en: '| less structure to the code (less modularity) | more structure to the code
    (more modularity) |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
- en: '| helping you and others understand the code and data | helping a computer
    run code automatically |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
- en: I personally use [JupyterLab](http://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html) for
    the entire process (both prototyping and productionizing). I recommend using JupyterLab **at
    least for prototyping**.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '**Lending Club data**'
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To help more concretely understand the difference between the prototyping and
    the production mindset, let's work with some real data. We'll work with lending
    data from the peer-to-peer lending site, [Lending Club](https://www.dataquest.io/blog/programming-best-practices-for-data-science/www.lendingclub.com).
    Unlike a bank, Lending Club doesn't lend money itself. Lending Club is instead
    a marketplace for lenders to lend money to individuals who are seeking loans for
    a variety of reasons (home repairs, wedding costs, etc.). We can use this data
    to build models that will predict if a given loan application will be successful
    or not. We won't dive into building a machine learning pipeline for making predictions
    in this post, but we cover it in our [Machine Learning Project Walkthrough Course](https://www.dataquest.io/course/machine-learning-project).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Lending Club offers detailed, historical data on both completed loans (loan
    applications that were approved by Lending Club and they found lenders) and declined
    loans (loan application was declined by Lending Club and money never changed hands).
    Navigate to their [data download page](https://www.lendingclub.com/info/download-data.action) and
    select **2007-2011** under **DOWLNOAD LOAN DATA**.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![lendingclub](../Images/5893dcd88190c71f1a9b9914ef8fd8e0.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: '**Prototype mindset**'
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the prototype mindset, we''re interested in quickly iterating and trying
    to understand some properties and truths about the data. Create a new Jupyter
    notebook and add a Markdown cell that explains:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Any research you did on Lending Club to better understand the platform
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any information on the data set you downloaded
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First things first, let's read the CSV file into pandas.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We get two pieces of output, first a warning.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then the first 5 rows of the dataframe, which we'll avoid showing here (because
    it's quite long).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'We also got the following dataframe output:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
    Notes offered by Prospectus (https://www.lendingclub.com/info/prospectus.action)
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '| d | member_id | loan_amnt | funded_amnt | funded_amnt_inv | term | int_rate
    | installment | grade | sub_grade | emp_title | emp_length | home_ownership |
    annual_inc | verification_status | issue_d | loan_status | pymnt_plan | url |
    desc | purpose | title | zip_code | addr_state | dti | delinq_2yrs | earliest_cr_line
    | inq_last_6mths | mths_since_last_delinq | mths_since_last_record | open_acc
    | pub_rec | revol_bal | revol_util | total_acc | initial_list_status | out_prncp
    | out_prncp_inv | total_pymnt | total_pymnt_inv | total_rec_prncp | total_rec_int
    | total_rec_late_fee | recoveries | collection_recovery_fee | last_pymnt_d | last_pymnt_amnt
    | next_pymnt_d | last_credit_pull_d | collections_12_mths_ex_med | mths_since_last_major_derog
    | policy_code | application_type | annual_inc_joint | dti_joint | verification_status_joint
    | acc_now_delinq | tot_coll_amt | tot_cur_bal | open_acc_6m | open_act_il | open_il_12m
    | open_il_24m | mths_since_rcnt_il | total_bal_il | il_util | open_rv_12m | open_rv_24m
    | max_bal_bc | all_util | total_rev_hi_lim | inq_fi | total_cu_tl | inq_last_12m
    | acc_open_past_24mths | avg_cur_bal | bc_open_to_buy | bc_util | chargeoff_within_12_mths
    | delinq_amnt | mo_sin_old_il_acct | mo_sin_old_rev_tl_op | mo_sin_rcnt_rev_tl_op
    | mo_sin_rcnt_tl | mort_acc | mths_since_recent_bc | mths_since_recent_bc_dlq
    | mths_since_recent_inq | mths_since_recent_revol_delinq | num_accts_ever_120_pd
    | num_actv_bc_tl | num_actv_rev_tl | num_bc_sats | num_bc_tl | num_il_tl | num_op_rev_tl
    | num_rev_accts | num_rev_tl_bal_gt_0 | num_sats | num_tl_120dpd_2m | num_tl_30dpd
    | num_tl_90g_dpd_24m | num_tl_op_past_12m | pct_tl_nvr_dlq | percent_bc_gt_75
    | pub_rec_bankruptcies | tax_liens | tot_hi_cred_lim | total_bal_ex_mort | total_bc_limit
    | total_il_high_credit_limit | revol_bal_joint | sec_app_earliest_cr_line | sec_app_inq_last_6mths
    | sec_app_mort_acc | sec_app_open_acc | sec_app_revol_util | sec_app_open_act_il
    | sec_app_num_rev_accts | sec_app_chargeoff_within_12_mths | sec_app_collections_12_mths_ex_med
    | sec_app_mths_since_last_major_derog | hardship_flag | hardship_type | hardship_reason
    | hardship_status | deferral_term | hardship_amount | hardship_start_date | hardship_end_date
    | payment_plan_start_date | hardship_length | hardship_dpd | hardship_loan_status
    | orig_projected_additional_accrued_interest | hardship_payoff_balance_amount
    | hardship_last_payment_amount | disbursement_method | debt_settlement_flag |
    debt_settlement_flag_date | settlement_status | settlement_date | settlement_amount
    | settlement_percentage | settlement_term |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: The warning lets us know that the type inferencing of pandas for each column
    would be improved if we set the `low_memory` parameter to `False` when calling `pandas.read_csv()`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'The second output is more problematic because the way the DataFrame is storing
    the data has issues. JupyterLab has a terminal environment built in, so we can
    open it and use the bash command `head` to observe the first two lines of the
    raw file:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'While the second line contains the column names as we expect in a CSV file,
    it looks like the first line is throwing off the formatting of the DataFrame when
    pandas tries to parse the file:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Add a Markdown cell that details your observations and add a code cell that
    factors in the observations.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Read the data dictionary from the [Lending Club download page](https://www.lendingclub.com/info/download-data.action) to
    understand which columns don't contain useful information for features. The `desc` and `url` columns
    seem to immediately fit this criteria.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The next step is to drop any columns with more than 50% missing rows. Use one
    cell to explore which columns meet that criteria, and another to actually drop
    the columns.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Because we're using a Jupyter notebook to track our thoughts and our code, we're
    relying on the environment (via the IPython kernel) to keep track of changes of
    the state. This frees us up to be freeform, move cells around, run the same code
    multiple times, etc.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, code in the prototyping mindset should focus on:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Understandability
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Markdown cells to describe our observations and assumptions
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Small pieces of code for the actual logic
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Lots of visualizations and counts
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimal abstractions
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most code shouldn't be in functions (should feel more object-oriented)
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's say we spent another hour exploring the data and writing markdown cells
    that describe the data cleaning we did. We can then switch over to the production
    mindset and make the code more robust.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '**Production mindset**'
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the production mindset, we want to focus on writing code that will generalize
    to more situations. In our case, we want our data cleaning code to work for any
    of the data sets from Lending Club (from other time periods). The best way to
    generalize our code is to turn it into a **data pipeline**. A data pipeline is
    designed using principles from [functional programming](https://www.dataquest.io/blog/introduction-functional-programming-python/),
    where data is modified *within* functions and then passed *between* functions.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a first iteration of this pipeline using a single function to encapsulate
    data cleaning code:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the code above, we **abstracted** the code from earlier into a single function.
    The input to this function is a list of filenames and the output is a list of
    DataFrame objects.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, the production mindset should focus on:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Healthy abstractions
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code should generalize to be compatible with similar data sources
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Code shouldn't be so general that it becomes cumbersome to understand
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: pipeline stability
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reliability should match how frequently its run (daily? weekly? monthly?)
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Switching between mindsets**'
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s say we tried to run the function for all of the data sets from Lending
    Club and Python returned errors. Some potential sources for errors:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Variance in column names in some of the files
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variance in columns being dropped because of the 50% missing value threshold
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different column types based on pandas type inference for that file
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In those cases, we should actually switch back to our prototype notebook and
    investigate further. When we've determined that we want our pipeline to be more
    flexible and account for specific variations in the data, we can re-incorporate
    that back into the pipeline logic.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example where we adapted the function to accommodate for a different
    drop threshold value:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The default value is still `0.5`, but we can over-ride it to `0.7` if we want.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few ways to make the pipeline more flexible, in decreasing priority:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Use optional, positional, and required arguments
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use if / then statements along with Boolean input values within the functions
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use new data structures (dictionaries, lists, etc.) to represent custom actions
    for specific datasets
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This pipeline can scale to all phases of the data science workflow. Here's some
    skeleton code that previews how this looks.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Next steps**'
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you''re interested in deepening your understanding and practicing further,
    I recommend the following next steps:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to turn your pipeline into a standalone script that can be run as
    a module or from the command line: [https://docs.python.org/3/library/**main**.html](https://docs.python.org/3/library/__main__.html)
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn how to use Luigi to build more complex pipelines that can be run in the
    cloud: [Building Data Pipelines in Python and Luigi](https://marcobonzanini.com/2015/10/24/building-data-pipelines-with-python-and-luigi/)
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn more about data engineering: [Data Engineering Posts on Dataquest](https://www.dataquest.io/blog/tag/data-engineering/)
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bio: Srini Kadamati** is Director of Content at **[Dataquest.io](https://www.dataquest.io/)**.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://www.dataquest.io/blog/programming-best-practices-for-data-science/?utm_source=kdnuggets&utm_medium=crosspost&utm_content=text).
    Reposted with permission.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[Swiftapply  – Automatically efficient pandas apply operations](/2018/04/swiftapply-automatically-efficient-pandas-apply-operations.html)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Structures Related to Machine Learning Algorithms](/2018/01/data-structures-related-machine-learning-algorithms.html)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introduction to Functional Programming in Python](/2018/02/introduction-functional-programming-python.html)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功数据科学家的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个数据科学家都应该知道的三个R库（即使你使用Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一个90亿美元的AI失败，探究](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
