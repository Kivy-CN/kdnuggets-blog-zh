- en: Easy Guide To Data Preprocessing In Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/07/easy-guide-data-preprocessing-python.html](https://www.kdnuggets.com/2020/07/easy-guide-data-preprocessing-python.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Easy Guide To Data Preprocessing In Python](../Images/64a33dd50d43223ee1daee66f467fb1a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image by rawpixel.com](https://www.freepik.com/free-photo/businesswoman-networking-using-digital-devices_15440982.htm#query=data&from_query=Data%20Preprocessing&position=0&from_view=search&track=sph)
    on Freepik'
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning is 80% preprocessing and 20% model making.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: You must have heard this phrase if you have ever encountered a senior Kaggle
    data scientist or machine learning engineer. The fact is that this is a true phrase.
    In a real-world data science project, data preprocessing is one of the most important
    things, and it is one of the common factors of success of a model, i.e., if there
    is correct data preprocessing and feature engineering, that model is more likely
    to produce noticeably better results as compared to a model for which data is
    not well preprocessed.
  prefs: []
  type: TYPE_NORMAL
- en: Important Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are 4 main important steps for the preprocessing of data.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting of the data set in Training and Validation sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking care of Missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking care of Categorical Features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization of data set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s have a look at all of these points.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Train Test Split
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Train Test Split is one of the important steps in Machine Learning. It is very
    important because your model needs to be evaluated before it has been deployed.
    And that evaluation needs to be done on unseen data because when it is deployed,
    all incoming data is unseen.
  prefs: []
  type: TYPE_NORMAL
- en: The main idea behind the train test split is to convert original data set into
    2 parts
  prefs: []
  type: TYPE_NORMAL
- en: train
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: where train consists of training data and training labels and test consists
    of testing data and testing labels.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to do it is by using *scikit-learn*, which has a built-in function *train_test_split*.
    Let’s code it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here we have passed-in *X* and *y* as arguments in *train_test_split*, which
    splits *X* and *y* such that there is 20% testing data and 80% training data successfully
    split between *X_train*, *X_test*, *y_train*, and *y_test*.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Taking Care of Missing Values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a famous Machine Learning phrase which you might have heard that is
  prefs: []
  type: TYPE_NORMAL
- en: Garbage in Garbage out
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If your data set is full of NaNs and garbage values, then surely your model
    will perform garbage too. So taking care of such missing values is important.
    Let’s take a dummy data set to see how we can tackle this problem of taking care
    of garbage values. You can get that data set [here](https://docs.google.com/spreadsheets/d/1I6IQUtp_x4ZAjY8q3VdiTiTrd-w0p8zGvCn-FD_Ae70/edit?usp=sharing).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see the missing values in the data set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![Easy Guide To Data Preprocessing In Python](../Images/1a590bb31504134461b63200306a7d4d.png)'
  prefs: []
  type: TYPE_IMG
- en: Here we can see that we have 2 missing values in 4 columns. One approach to
    fill in missing values is to fill it with the mean of that column, which is the
    average of that column. For example, we can fill in the missing value of *Final *column
    by an average of all students in that column.
  prefs: []
  type: TYPE_NORMAL
- en: To do that, we can use **SimpleImputer **from **sklearn.impute**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This will fill all the missing values in the data frame *df* using *mean *of
    that column. We use the *fit_transform *function to do that.
  prefs: []
  type: TYPE_NORMAL
- en: Because it returns a numpy array, to read it, we can convert it back to the
    data frame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![Easy Guide To Data Preprocessing In Python](../Images/c34f2223e45f8c13343ca5e2caf20585.png)'
  prefs: []
  type: TYPE_IMG
- en: And now, we can see that we have filled all the missing values by mean of all
    values.
  prefs: []
  type: TYPE_NORMAL
- en: We can confirm it by
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: and the output is
  prefs: []
  type: TYPE_NORMAL
- en: '![Easy Guide To Data Preprocessing In Python](../Images/c64fe3b01f2d39a63e492093560d6e16.png)'
  prefs: []
  type: TYPE_IMG
- en: We can use *mean*, *meadian*, *mode *etc. in **SimpleImputer**.
  prefs: []
  type: TYPE_NORMAL
- en: If our number of rows which have missing values are less, or our data is such
    that it is not advised to fill in missing values, then we can drop the missing
    rows by using *dropna *in pandas.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: And here, we have dropped all the null rows in the dataframe and stored it in
    another dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have 0 null rows as we have dropped them. We can confirm it as
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Easy Guide To Data Preprocessing In Python](../Images/c64fe3b01f2d39a63e492093560d6e16.png)'
  prefs: []
  type: TYPE_IMG
- en: 3\. Taking care of Categorical Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can take care of categorical features by converting them to integers. There
    are 2 common ways to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Label Encoding
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One Hot Encoding
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In **Label Encoder**, we can convert the Categorical values into numerical labels.
    Let’s say this is our dataset
  prefs: []
  type: TYPE_NORMAL
- en: '![Easy Guide To Data Preprocessing In Python](../Images/6e9d07286ec00c70b75f56ae0e51760a.png)'
  prefs: []
  type: TYPE_IMG
- en: and using label encoder on the Country column will convert India to 1, the USA
    to 2, and China to 0\. This technique has a drawback that it gives the USA the
    highest priority due to its label is high and China the lowest priority for its
    label being 0, but still, it is helping a lot of times.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s code it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![Easy Guide To Data Preprocessing In Python](../Images/6f5f83dadb595d1cb4eadd6fc085ff9a.png)'
  prefs: []
  type: TYPE_IMG
- en: Output after Label Encoder
  prefs: []
  type: TYPE_NORMAL
- en: Here we have instantiated a LabelEncoder object, then used the *fit *method
    to fit it on our categorical column and then used *transform *method to apply
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Note that it is not *inplace *so in order to make the change permanent, we have
    to return the value to our categorical column, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In **OneHotEncoder **we make a new column for each unique categorical value,
    and the value is 1 for that column, if in an actual data frame that value is there,
    else it is 0.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s See it with the same example but a bit modified. We will add another categorical
    column, which is “Continent,” that has the name of the continent of the respective
    country. We can do it by
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Easy Guide To Data Preprocessing In Python](../Images/162109627fa175aea9a13e094bc4f873.png)'
  prefs: []
  type: TYPE_IMG
- en: Now because we have 2 categorical columns, which are *[['Country', 'Continent']]*,
    we can one hot encode them.
  prefs: []
  type: TYPE_NORMAL
- en: There are 2 ways to do so.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. DataFrame.get_dummies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a pretty common way where we use pandas built-in function *get_dummies*to
    convert categorical values in a dataframe to a one-hot vector.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s do this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This will **return **a data frame with all the categorical values encoded in
    a one-hot vector format.
  prefs: []
  type: TYPE_NORMAL
- en: '![Easy Guide To Data Preprocessing In Python](../Images/46967468158961459b8e3fb6223f4369.png)'
  prefs: []
  type: TYPE_IMG
- en: Here we can see that it has converted the unique values of Country columns as
    3 different columns, which are Country_China, Country_India, and Country_USA.
    Similarly, 2 unique values of Continent Column has been converted into 2 different
    columns named as Continent_Asia and Continent_North America.
  prefs: []
  type: TYPE_NORMAL
- en: Because it is not in place, we have to either store it in a data frame, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 2\. OneHotEncoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using OneHotEncoder from Sci-Kit Learning is also a common practice. It does
    provide more flexibility and more options but is a bit difficult to use. Let’s
    see how we can do it for our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have initialized OneHotEncoder Object and used its *fit_transform* method
    on our desired columns (column number 0 and column number 3) in the data frame.
  prefs: []
  type: TYPE_NORMAL
- en: The return type of *fit_transform* is *numpy.ndarray*, so we convert it into
    a dataframe by *pd.DataFrame* and stored it in a variable. Then, to join it in
    our original data frame, we can use *pd.concat* the function that concatenates
    2 different data frames. We have used* axis=1*, which means it has to join on
    the basis of columns instead of rows.
  prefs: []
  type: TYPE_NORMAL
- en: Also, remember that *pd.concat* is not *inplace*, so we have to store the returning
    dataframe somewhere.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The resulting dataframe is
  prefs: []
  type: TYPE_NORMAL
- en: '![Easy Guide To Data Preprocessing In Python](../Images/293e24679c8b0ed3deebc55a1f950abf.png)'
  prefs: []
  type: TYPE_IMG
- en: OneHotEncoded DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: You can see that it is not clearly readable as compared to *pd.get_dummies*,
    but if you compare the last 5 columns that we got using *pd.get_dummies* and *OneHotEncoder*,
    then they are all equal.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you can modify the column names as per your choice in *OneHotEncoder*,
    which you can learn as per [this](https://stackoverflow.com/questions/54570947/feature-names-from-onehotencoder) question
    from [StackOverflow](http://www.stackoverflow.com/).
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Normalizing the Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This brings us to the last part of data preprocessing, which is the normalization
    of the dataset. It is proven from certain experimentation that Machine Learning
    and Deep Learning Models perform way better on a normalized data set as compared
    to a data set that is not normalized.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of normalization is to change values to a common scale without distorting
    the difference between the range of values.
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways to do so. I will discuss 2 common ways to normalize a
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Standard Scaler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Easy Guide To Data Preprocessing In Python](../Images/d03ad89f7be4208a47dc9a961bd6e94c.png)'
  prefs: []
  type: TYPE_IMG
- en: Credit: [https://stackoverflow.com/a/50879522/10342778](https://stackoverflow.com/a/50879522/10342778)
  prefs: []
  type: TYPE_NORMAL
- en: Using this technique, we are going to have a mean of 0 and a standard deviation
    of 1 in our dataset. We can either do it normally by combining different functions
    in numpy, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: where *x* is a dataframe with all numerical indices. If we want to retain the
    values in a dataframe, then we can simply remove *.values* in front of it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Variance before StandardScaler**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![Easy Guide To Data Preprocessing In Python](../Images/c7c2be84a25b48be8d67714b743f6ea2.png)'
  prefs: []
  type: TYPE_IMG
- en: var before StandardScaler.
  prefs: []
  type: TYPE_NORMAL
- en: Here, I have used *ddof=0*, which is by default 1 in *pandas.DataFrame.var()* and
    by default 0 in *numpy.ndarray.var()*. Ddof means Delta Degrees of Freedom, which
    is the divisor used in calculations is *N - ddof*, where *N* represents the number
    of elements.
  prefs: []
  type: TYPE_NORMAL
- en: '*ddof=0* provides a maximum likelihood estimate of the variance for normally
    distributed variables.'
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about *ddof=0* [here](https://stackoverflow.com/questions/62938495/difference-between-numpy-var-and-pandas-var).
  prefs: []
  type: TYPE_NORMAL
- en: Variance after StandardScaler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another good way to do so by using **StandardScaler **from *sklearn.preprocessing*.
    Let’s see the code, and then see the variance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![Easy Guide To Data Preprocessing In Python](../Images/aaf7b5e994169b9e45d36b3bf01c92b0.png)'
  prefs: []
  type: TYPE_IMG
- en: catDf after StandardScaler.
  prefs: []
  type: TYPE_NORMAL
- en: Here we have applied *StandardScaler *on all the numerical columns (column number
    1 to the last column (not included)), and now you can see the values of *GDP *and *Area*.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can check the *variance *of the dataset by
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![Easy Guide To Data Preprocessing In Python](../Images/22b94d91b7ae62c5c3accb73b7653e5f.png)'
  prefs: []
  type: TYPE_IMG
- en: var after StandardScaler
  prefs: []
  type: TYPE_NORMAL
- en: And we can see the massive reduction of the variance of 80 and 13 to 1\. In
    real-world datasets, normally the improvement is from thousands to 1.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: According to the [official documentation of sklearn](https://scikit-learn.org/stable/modules/preprocessing.html#normalization),
    normalization is “the process of **scaling individual samples to have unit norm**.
    This process can be useful if you plan to use a quadratic form such as the dot-product
    or any other kernel to quantify the similarity of any pair of samples.”
  prefs: []
  type: TYPE_NORMAL
- en: The process of using it is very simple and similar to StandaradScaler.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![Easy Guide To Data Preprocessing In Python](../Images/9752dd2812578f78ed7274ec01979080.png)'
  prefs: []
  type: TYPE_IMG
- en: catDf after Normalizer
  prefs: []
  type: TYPE_NORMAL
- en: There are several other ways to normalize the data, and all of them are useful
    in specific cases. You can read more about them in the official documentation [here](https://scikit-learn.org/stable/modules/preprocessing.html#normalization).
  prefs: []
  type: TYPE_NORMAL
- en: Learning Outcomes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Splitting the Dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filling in Missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with Categorical Data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization of Dataset for improved results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hopefully, all these techniques will improve your general skills as a data scientist
    or machine learning engineer and will improve your Machine Learning Models.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Ahmad Anis](https://twitter.com/AhmadMustafaAn1)** is interested in Machine
    Learning, Deep Learning, and Computer Vision. Currently working as a Jr. Machine
    Learning engineer at Redbuffer.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[An Easy Guide to Choose the Right Machine Learning Algorithm](https://www.kdnuggets.com/2020/05/guide-choose-right-machine-learning-algorithm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenAI API for Beginners: Your Easy-to-Follow Starter Guide](https://www.kdnuggets.com/openai-api-for-beginners-your-easy-to-follow-starter-guide)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Learn Data Cleaning and Preprocessing for Data Science with This Free eBook](https://www.kdnuggets.com/2023/08/learn-data-cleaning-preprocessing-data-science-free-ebook.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Steps to Mastering Data Cleaning and Preprocessing Techniques](https://www.kdnuggets.com/2023/08/7-steps-mastering-data-cleaning-preprocessing-techniques.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Harnessing ChatGPT for Automated Data Cleaning and Preprocessing](https://www.kdnuggets.com/2023/08/harnessing-chatgpt-automated-data-cleaning-preprocessing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cleaning and Preprocessing Text Data in Pandas for NLP Tasks](https://www.kdnuggets.com/cleaning-and-preprocessing-text-data-in-pandas-for-nlp-tasks)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
