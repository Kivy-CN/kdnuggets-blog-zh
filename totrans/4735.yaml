- en: Solve any Image Classification Problem Quickly and Easily
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2018/12/solve-image-classification-problem-quickly-easily.html/2](https://www.kdnuggets.com/2018/12/solve-image-classification-problem-quickly-easily.html/2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](/2018/12/solve-image-classification-problem-quickly-easily.html?page=2#comments)'
  prefs: []
  type: TYPE_IMG
- en: 6\. Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this example, we will see **how each of these classifiers can be implemented
    in a transfer learning solution for image classification**. According to Rawat
    and Wang (2017), ‘*comparing the performance of different classifiers on top of
    deep convolutional neural networks still requires further investigation and thus
    makes for an interesting research direction*’. So it will be interesting to see
    how each classifier performs in a standard image classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the full code of this example on [my GitHub page](https://github.com/pmarcelino/blog/blob/master/dogs_cats/dogs_cats.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '**6.1\. Prepare data**'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we will use a smaller version of the original dataset. This
    will allow us to run the models faster, which is great for people who have limited
    computational power (like me).
  prefs: []
  type: TYPE_NORMAL
- en: To build a smaller version of the dataset, we can adapt the code provided by
    Chollet (2017) as shown in Code 1.
  prefs: []
  type: TYPE_NORMAL
- en: Code 1\. Create a smaller dataset for Dogs vs. Cats.
  prefs: []
  type: TYPE_NORMAL
- en: '**6.2\. Extract features from the convolutional base**'
  prefs: []
  type: TYPE_NORMAL
- en: The convolutional base will be used to extract features. These features will
    feed the classifiers that we want to train so that we can identify if images have
    dogs or cats.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, the code provided by Chollet (2017) is adapted. Code 2 shows the
    code used.
  prefs: []
  type: TYPE_NORMAL
- en: Code 2\. Extract features from convolutional base.
  prefs: []
  type: TYPE_NORMAL
- en: '**6.3\. Classifiers**'
  prefs: []
  type: TYPE_NORMAL
- en: '**6.3.1\. Fully-connected layers**'
  prefs: []
  type: TYPE_NORMAL
- en: The first solution that we present is based on fully-connected layers. This
    classifier adds a stack of fully-connected layers that is fed by the features
    extracted from the convolutional base.
  prefs: []
  type: TYPE_NORMAL
- en: To keep it simple (and fast), we will use the solution proposed by Chollet (2018)
    with slight modifications. In particular, we will use the Adam optimizer instead
    of the RMSProp because [Stanford says so](http://cs231n.github.io/neural-networks-3/#adam) (what
    a beautiful *argumentum ad verecundiam*).
  prefs: []
  type: TYPE_NORMAL
- en: Code 3 shows the code used, while Figures 5 and 6 present the learning curves.
  prefs: []
  type: TYPE_NORMAL
- en: Code 3\. Fully connected layers solution.![](../Images/37a3b1336eb12b75a2b53aea7edf9958.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5\. Accuracy of the fully connected layers solution.![](../Images/a624ba4e5278d22f95690936d9dab636.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6\. Loss of the fully connected layers solution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Brief discussion of results:**'
  prefs: []
  type: TYPE_NORMAL
- en: Validation accuracy is around 0.85, which is encouraging given the size of the
    dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model strongly overfits. There’s a big gap between the training and the
    validation curves.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since we already used dropout, we should increase the size of the dataset to
    improve the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**6.3.2\. Global average pooling**'
  prefs: []
  type: TYPE_NORMAL
- en: The difference between this case and the previous one is that, instead of adding
    a stack of fully-connected layers, we will add a global average pooling layer
    and feed its output into a sigmoid activated layer.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we are talking about a sigmoid activated layer instead of a softmax
    one, which is what is recommended by Lin et al. (2013). We are changing to the
    sigmoid activation because in Keras, to perform binary classification, you should
    use *sigmoid* activation and *binary_crossentropy* as the loss (Chollet 2017).
    Therefore, it was necessary to do this small modification to the original proposal
    of Lin et al. (2013).
  prefs: []
  type: TYPE_NORMAL
- en: Code 4 shows the code to build the classifier. Figure 7 and 8 show the resulting
    learning curves.
  prefs: []
  type: TYPE_NORMAL
- en: Code 4\. Global average pooling solution.![](../Images/10de3d9dcec03af404861a8879c0accb.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7\. Accuracy of the global average pooling solution.![](../Images/a5d7d4e1dfd9f9804655064ae61f6cb7.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8\. Loss of the global average pooling solution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Brief discussion of results:**'
  prefs: []
  type: TYPE_NORMAL
- en: Validation accuracy is similar to the one resulting from the fully-connected
    layers solution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model doesn’t overfit as much as in the previous case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The loss function is still decreasing when the model stops training. Probably,
    it is possible to improve the model by increasing the number of epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**6.3.3 Linear support vector machines**'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we will train a linear support vector machines (SVM) classifier
    on the features extracted by the convolutional base.
  prefs: []
  type: TYPE_NORMAL
- en: To train this classifier, a traditional machine learning approach is preferable.
    Consequently, we will use k-fold cross-validation to estimate the error of the
    classifier. Since k-fold cross-validation will be used, we can concatenate the
    train and the validation sets to enlarge our training data (we keep the test set
    untouched, as we did in the previous cases). Code 5 shows how data was concatenated.
  prefs: []
  type: TYPE_NORMAL
- en: Code 5\. Data concatenation.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we must be aware that the SVM classifier has one hyperparameter. This
    hyperparameter is the penalty parameter C of the error term. To optimize the choice
    of this hyperparameter, we will use exhaustive grid search. Code 6 presents the
    code used to build this classifier, while Figure 9 illustrates the learning curves.
  prefs: []
  type: TYPE_NORMAL
- en: Code 6\. Linear SVM solution.![](../Images/1636b5411bb359e6864f8a0c3784e7b9.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9\. Accuracy of the linear SVM solution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Brief discussion of results:**'
  prefs: []
  type: TYPE_NORMAL
- en: Model’s accuracy is around 0.86, which is similar to the accuracy of the previous
    solutions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Overfitting is around the corner. Moreover, the training accuracy is always
    1.0, which is not usual and can be interpreted as a sign of overfitting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The accuracy of the model should increase with the number of training samples.
    However, that doesn’t seem to happen. This may be due to overfitting. It would
    be interesting to see how the model reacts when the dataset increases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 7\. Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this article, we:'
  prefs: []
  type: TYPE_NORMAL
- en: Presented the concepts of transfer learning, convolutional neural networks,
    and pre-trained models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defined the basic fine-tuning strategies to repurpose a pre-trained model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Described a structured approach to decide which fine-tuning strategy should
    be used, based on the size and similarity of the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduced three different classifiers that can be used on top of the features
    extracted from the convolutional base.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provided a end-to-end example on image classification for each of the three
    classifiers presented in this article.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I hope that you feel motivated to start developing your deep learning projects
    on computer vision. This is a great field of study and new exciting findings are
    coming out everyday.
  prefs: []
  type: TYPE_NORMAL
- en: I’d be glad to help you, so let me know if you have any questions or improvement
    suggestions!
  prefs: []
  type: TYPE_NORMAL
- en: 8\. References
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 1\. Bengio, Y., 2009\. Learning deep architectures for AI. Foundations and trends
    in Machine Learning, 2(1), pp.1–127.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Canziani, A., Paszke, A. and Culurciello, E., 2016\. An analysis of deep
    neural network models for practical applications. arXiv preprint arXiv:1605.07678.
  prefs: []
  type: TYPE_NORMAL
- en: '[3\. Chollet, F., 2015\. Keras.](https://keras.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: 4. [Chollet, F., 2017\. Deep learning with python. Manning Publications Co..](https://amzn.to/2CeEySF)
  prefs: []
  type: TYPE_NORMAL
- en: '5\. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K. and Fei-Fei, L., 2009,
    June. Imagenet: A large-scale hierarchical image database. In Computer Vision
    and Pattern Recognition, 2009\. CVPR 2009\. IEEE Conference on (pp. 248–255).
    Ieee.'
  prefs: []
  type: TYPE_NORMAL
- en: 6\. He, K., Zhang, X., Ren, S. and Sun, J., 2016\. Deep residual learning for
    image recognition. In Proceedings of the IEEE conference on computer vision and
    pattern recognition (pp. 770–778).
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Krizhevsky, A., Sutskever, I. and Hinton, G.E., 2012\. Imagenet classification
    with deep convolutional neural networks. In Advances in neural information processing
    systems (pp. 1097–1105).
  prefs: []
  type: TYPE_NORMAL
- en: 8\. LeCun, Y., Bengio, Y. and Hinton, G., 2015\. Deep learning. nature, 521(7553),
    p.436.
  prefs: []
  type: TYPE_NORMAL
- en: 9\. Lin, M., Chen, Q. and Yan, S., 2013\. Network in network. arXiv preprint
    arXiv:1312.4400.
  prefs: []
  type: TYPE_NORMAL
- en: 10\. Pan, S.J. and Yang, Q., 2010\. A survey on transfer learning. IEEE Transactions
    on knowledge and data engineering, 22(10), pp.1345–1359.
  prefs: []
  type: TYPE_NORMAL
- en: '11\. Rawat, W. and Wang, Z., 2017\. Deep convolutional neural networks for
    image classification: A comprehensive review. Neural computation, 29(9), pp.2352–2449.'
  prefs: []
  type: TYPE_NORMAL
- en: 12\. Simonyan, K. and Zisserman, A., 2014\. Very deep convolutional networks
    for large-scale image recognition. arXiv preprint arXiv:1409.1556.
  prefs: []
  type: TYPE_NORMAL
- en: 13\. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. and Wojna, Z., 2016\.
    Rethinking the inception architecture for computer vision. In Proceedings of the
    IEEE conference on computer vision and pattern recognition (pp. 2818–2826).
  prefs: []
  type: TYPE_NORMAL
- en: 14\. Tang, Y., 2013\. Deep learning using linear support vector machines. arXiv
    preprint arXiv:1306.0239.
  prefs: []
  type: TYPE_NORMAL
- en: '15\. Voulodimos, A., Doulamis, N., Doulamis, A. and Protopapadakis, E., 2018\.
    Deep learning for computer vision: A brief review. Computational intelligence
    and neuroscience, 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: 16\. Yosinski, J., Clune, J., Bengio, Y. and Lipson, H., 2014\. How transferable
    are features in deep neural networks?. In Advances in neural information processing
    systems (pp. 3320–3328).
  prefs: []
  type: TYPE_NORMAL
- en: 17\. Zeiler, M.D. and Fergus, R., 2014, September. Visualizing and understanding
    convolutional networks. In European conference on computer vision (pp. 818–833).
    Springer, Cham.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Thanks to [João Coelho](https://www.linkedin.com/in/joaopcoelho/) for reading
    drafts of this.
  prefs: []
  type: TYPE_NORMAL
- en: '*You can find more about me and my projects at *[*pmarcelino.com*](http://www.pmarcelino.com/)*.
    Also, you can sign up for my *[*newsletter*](http://pmarcelino.com/subscribe)* to
    receive my latest updates on Humans, Machines, and Science.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Pedro Marcelino](https://www.linkedin.com/in/pmarcelino/)** is interested
    in all aspects of machine learning and data analysis. His focus is on data mining
    & quality, exploratory analysis & visualization, and predictive/prescriptive analytics,
    but includes testing and benchmarking of different machine learning approaches
    on real-life problems.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Latest Trends in Computer Vision Technology and Applications](/2018/11/trends-computer-vision-technology-applications.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building an Image Classifier Running on Raspberry Pi](/2018/10/building-image-classifier-running-raspberry-pi.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Analyze a Soccer (Football) Game Using Tensorflow Object Detection and OpenCV](/2018/07/analyze-soccer-game-using-tensorflow-object-detection-opencv.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Introduction to NExT-GPT: Any-to-Any Multimodal Large Language Model](https://www.kdnuggets.com/introduction-to-nextgpt-anytoany-multimodal-large-language-model)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning Is Not Like Your Brain Part 6: The Importance of…](https://www.kdnuggets.com/2022/08/machine-learning-like-brain-part-6-importance-precise-synapse-weights-ability-set-quickly.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Scrape Images Easily from Websites in A No-Coding Way](https://www.kdnuggets.com/2022/06/octoparse-scrape-images-easily-websites-nocoding-way.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Explore LLMs Easily on Your Laptop with openplayground](https://www.kdnuggets.com/2023/04/explore-llms-easily-laptop-openplayground.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Easily Integrate LLMs into Your Scikit-learn Workflow with Scikit-LLM](https://www.kdnuggets.com/easily-integrate-llms-into-your-scikit-learn-workflow-with-scikit-llm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Image Classification with Convolutional Neural Networks (CNNs)](https://www.kdnuggets.com/2022/05/image-classification-convolutional-neural-networks-cnns.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
