- en: How To Deal With Imbalanced Classification, Without Re-balancing the Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何处理不平衡分类问题，而无需重新平衡数据
- en: 原文：[https://www.kdnuggets.com/2021/09/imbalanced-classification-without-re-balancing-data.html](https://www.kdnuggets.com/2021/09/imbalanced-classification-without-re-balancing-data.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/09/imbalanced-classification-without-re-balancing-data.html](https://www.kdnuggets.com/2021/09/imbalanced-classification-without-re-balancing-data.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [David B Rosen (PhD)](http://linkedin.com/in/rosen1), Lead Data Scientist
    for Automated Credit Approval at IBM Global Financing**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：[David B Rosen (PhD)](http://linkedin.com/in/rosen1)，IBM全球融资自动化信用审批首席数据科学家**'
- en: '![Balance](../Images/c5ddc8819fafe531c409cba3345bf412.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![平衡](../Images/c5ddc8819fafe531c409cba3345bf412.png)'
- en: Photo by [Elena Mozhvilo](https://unsplash.com/@miracleday?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Elena Mozhvilo](https://unsplash.com/@miracleday?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In machine learning, when building a classification model with data having far
    more instances of one class than another, the initial default classifier is often
    unsatisfactory because it classifies almost every case as the majority class.
    Many articles show you how you could use oversampling (e.g. *SMOTE*) or sometimes
    undersampling or simply class-based sample weighting to retrain the model on “rebalanced”
    data, but this isn’t always necessary. Here we aim instead to show how much you
    can do **without **balancing the data or retraining the model.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，当用数据构建分类模型时，如果某一类别的实例远多于另一类别，初始默认分类器通常是不令人满意的，因为它几乎将所有情况都分类为多数类。许多文章展示了如何使用过采样（例如*SMOTE*）或有时是欠采样，或者简单地使用基于类别的样本加权来重新训练模型以“重新平衡”数据，但这并非总是必要的。在这里，我们的目标是展示在**不**平衡数据或重新训练模型的情况下，你可以做到多少。
- en: We do this by simply adjusting the the threshold for which we say “Class 1”
    when the model’s predicted probability of Class 1 is above it in two-class classification,
    rather than naïvely using the default classification rule which chooses which
    ever class is predicted to be most probable (probability threshold of 0.5). We
    will see how this gives you the flexibility to make any desired trade-off between
    false positive and false negative classifications while avoiding problems created
    by rebalancing the data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过简单地调整“类1”阈值来实现这一点，当模型预测的“类1”概率高于该阈值时，而不是天真地使用默认分类规则（即选择预测最可能的类别，概率阈值为0.5）。我们将看到这如何让你灵活地在假阳性和假阴性分类之间进行所需的权衡，同时避免由于重新平衡数据而产生的问题。
- en: We will use the [credit card fraud identification data set](https://www.kaggle.com/mlg-ulb/creditcardfraud) from
    Kaggle to illustrate. Each row of the data set represents a credit card transaction,
    with the target variable Class==0 indicating a *legitimate *transaction and Class==1
    indicating that the transaction turned out to be a *fraud*. There are 284,807
    transactions, of which only 492 (0.173%) are frauds — very imbalanced indeed.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自Kaggle的[信用卡欺诈识别数据集](https://www.kaggle.com/mlg-ulb/creditcardfraud)进行说明。数据集的每一行代表一次信用卡交易，其中目标变量Class==0表示*合法*交易，而Class==1表示交易被认定为*欺诈*。总共有284,807笔交易，其中仅有492笔（0.173%）为欺诈——确实非常不平衡。
- en: We will use a *gradient boosting* classifier because these often give good results.
    Specifically Scikit-Learn’s new HistGradientBoostingClassifier because it is much
    faster than their original GradientBoostingClassifier when the data set is relatively
    large like this one.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个*梯度提升*分类器，因为这些分类器通常能取得较好的结果。具体来说，我们使用Scikit-Learn的新HistGradientBoostingClassifier，因为当数据集较大时，它比原始的GradientBoostingClassifier快得多。
- en: First let’s import some libraries and read in the data set.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入一些库并读取数据集。
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/eccd064eb51ea23747f429410affd65d.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eccd064eb51ea23747f429410affd65d.png)'
- en: V1 through V28 (from a principal components analysis) and the transaction Amount
    are the features, which are all numeric and there is no missing data. Because
    we are only using a tree-based classifier, we don’t need to standardize or normalize
    the features.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: V1至V28（来自主成分分析）和交易金额是特征，这些特征都是数值型的，并且没有缺失数据。因为我们只使用树基分类器，所以我们不需要对特征进行标准化或归一化。
- en: We will now train the model after splitting the data into train and test sets.
    This took about half a minute on my laptop. We use the n_iter_no_change to stop
    the training early if the performance on a validation subset starts to deteriorate
    due to overfitting. I separately did a little bit of hyperparameter tuning to
    choose the learning_rate and max_leaf_nodes, but this is not the focus of the
    present article.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将数据拆分为训练集和测试集后训练模型。这在我的笔记本电脑上花费了大约半分钟的时间。我们使用n_iter_no_change来提前停止训练，如果在验证子集上的表现因过拟合开始恶化。我单独进行了少量超参数调优，以选择learning_rate和max_leaf_nodes，但这不是本文的重点。
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now we apply this model to the test data as the default *hard-classifier*, predicting
    0 or 1 for each transaction. We are implicitly applying decision threshold 0.5
    to the model’s continuous *probability prediction* as a *soft-classifier*. When
    the probability prediction is over 0.5 we say “1” and when it is under 0.5 we
    say “0”.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将此模型应用于测试数据，作为默认的*硬分类器*，对每个交易进行0或1的预测。我们实际上将决策阈值0.5应用于模型的连续*概率预测*，作为一个*软分类器*。当概率预测超过0.5时，我们标记为“1”，当低于0.5时，我们标记为“0”。
- en: We also print the *confusion matrix* for the result, considering Class 1 (fraud)
    to be the *“positive”* class, by convention because it is the rarer class. The
    confusion matrix shows the number of True Negatives, False Positives, False Negatives,
    and True Positives. The *normalized confusion matrix* rates (e.g. FNR = False
    Negative Rate) are included as percentages in parentheses.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还打印了*混淆矩阵*以显示结果，将第1类（欺诈）视为*“正类”*，这是惯例，因为它是较少见的类别。混淆矩阵显示了真正例、假正例、假负例和真负例的数量。*标准化混淆矩阵*率（例如FNR
    = 假负率）作为百分比包括在括号中。
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/157bf7b1aa1f0e18a2f6c8ea6e4ea776.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/157bf7b1aa1f0e18a2f6c8ea6e4ea776.png)'
- en: We see that the Recall for Class 1 (a.k.a. Sensitivity or True Positive Rate
    shown as TPR above) is only 71.62%, meaning that 71.62% of the true frauds are
    correctly identified as frauds and thus denied. So 28.38% of the true frauds are
    unfortunately approved as if legitimate.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，第1类（即上面显示的**敏感性**或**真正例率**TPR）的召回率仅为71.62%，这意味着71.62%的真实欺诈行为被正确识别为欺诈并因此被拒绝。因此，28.38%的真实欺诈行为不幸被错误地批准为合法交易。
- en: Especially with imbalanced data (or generally any time false positives and false
    negatives may have different consequences), it’s important not to limit ourselves
    to using the default implicit classification decision threshold of 0.5, as we
    did above by using “.predict( )”. We want to improve the Recall of class 1 (the
    TPR) to reduce our fraud losses (reduce false negatives). To do this, we can reduce
    the threshold for which we say “Class 1” when we predict a probability above the
    threshold. This way we say “Class 1” for a wider range of predicted probabilities.
    Such strategies are known as *threshold-moving*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是在数据不平衡的情况下（或一般来说，当假正例和假负例可能有不同后果时），重要的是不要局限于使用默认的隐式分类决策阈值0.5，如我们上面使用“.predict(
    )”时所做的那样。我们希望提高第1类的召回率（TPR），以减少欺诈损失（减少假负例）。为此，我们可以降低阈值，当我们预测的概率高于该阈值时，我们标记为“第1类”。这样，我们可以在更广泛的预测概率范围内标记为“第1类”。这种策略称为*阈值移动*。
- en: Ultimately it is a business decision to what extent we want to reduce these
    False Negatives since as a trade-off the number of False Positives (true legitimate
    transactions rejected as frauds) will inevitably increase as we adjust the threshold
    that we apply to the model’s probability prediction (obtained from “.predict_proba(
    )” instead of “.predict( )”).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，决定减少这些假负例的程度是一个商业决策，因为作为权衡，假正例（真正的合法交易被错误地拒绝为欺诈）的数量会随着我们调整模型的概率预测阈值（从“.predict_proba(
    )”而不是“.predict( )”中获得）而不可避免地增加。
- en: To elucidate this trade-off and help us choose a threshold, we plot the False
    Positive Rate and False Negative Rate against the Threshold. This is a variant
    of the *Receiver Operating Characteristic* (ROC) curve, but emphasizing the threshold.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了阐明这一权衡并帮助我们选择阈值，我们绘制了假正例率和假负例率与阈值的关系图。这是*接收者操作特征*（ROC）曲线的一种变体，但重点在于阈值。
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/cdf0a2753ad38eda66fd6cafb9f96985.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cdf0a2753ad38eda66fd6cafb9f96985.png)'
- en: Although there exist some rules of thumb or proposed metrics for choosing the
    optimal threshold, ultimately it depends solely on the cost to the business of
    false negatives vs. false positives. For example, looking at the plot above, we
    might choose to apply a threshold of 0.00035 (vertical green line has been added)
    as follows.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在一些选择最佳阈值的经验法则或建议指标，但最终完全依赖于假阴性与假阳性对业务的成本。例如，查看上面的图，我们可能会选择应用阈值0.00035（已添加垂直绿色线）如以下所示。
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/d645cf70743eedce3a20585a85ed6f5f.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d645cf70743eedce3a20585a85ed6f5f.png)'
- en: We have reduced our False Negative Rate from 28.38% down to 9.46% (i.e. identified
    and denied 90.54% of our true frauds as our new Recall or Sensitivity or True
    Positive Rate or TPR), while our False Positive Rate (FPR) has increased from
    0.01% to 5.75% (i.e. still approved 94.25% of our legitimate transactions). It
    might well be worth the trade-off to us of denying about 6% of the legitimate
    transactions as the price we pay in order to approve only less than 10% of the
    fraudulent transactions, down from a very costly 28% of the frauds when we were
    using the default hard-classifier with an implicit classification decision threshold
    of 0.5.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已将假阴性率从28.38%降低到9.46%（即识别并拒绝了90.54%的真实欺诈行为，作为我们新的召回率或敏感度或真正阳性率或TPR），同时假阳性率（FPR）从0.01%增加到5.75%（即仍批准了94.25%的合法交易）。对于我们来说，拒绝约6%的合法交易作为仅批准不到10%的欺诈交易的代价可能是值得的，这一代价相比使用默认硬分类器（隐含分类决策阈值为0.5）时非常昂贵的28%的欺诈交易。
- en: Reasons not to balance your imbalanced data
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不平衡数据的平衡理由
- en: One reason to avoid “balancing” your imbalanced training data is that such methods
    bias/distort the resulting trained model’s probability predictions so that these
    become *miscalibrated*, by systematically increasing the model’s predicted probabilities
    of the original minority class, and are thus reduced to being merely relative *ordinal
    discriminant scores *or *decision functions *or *confidence scores* rather than
    being potentially accurate predicted class probabilities in the original (“imbalanced”)
    train and test set and future data that the classifier may make predictions on.
    In the event that such rebalancing for training *is *truly needed, but numerically-accurate
    probability predictions are still desired, one would then have to *recalibrate* the
    predicted probabilities to a data set having the original/imbalanced class proportions.
    Alternatively you could apply a correction to the predicted probabilities from
    the balanced model — see [Balancing is Unbalancing](https://towardsdatascience.com/balancing-is-unbalancing-5f517936f626) and [this
    reply](https://medium.com/@aliosia/let-me-explain-with-an-example-assume-p-y-0-1-22a6fde4bfd3) by
    its author.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 避免“平衡”不平衡训练数据的一个原因是，这些方法会偏向/扭曲训练模型的概率预测，使这些预测变得*误校准*，通过系统性地提高模型对原始少数类的预测概率，因此被简化为仅仅是相对的*序数判别分数*或*决策函数*或*置信度分数*，而不是在原始（“不平衡”）训练和测试集以及分类器可能对其进行预测的未来数据中潜在准确的预测类别概率。如果确实需要这种训练*重新平衡*，但仍希望获得数值准确的概率预测，则必须*重新校准*预测概率至具有原始/不平衡类别比例的数据集。或者，您可以对平衡模型的预测概率应用修正
    — 请参见[平衡即不平衡](https://towardsdatascience.com/balancing-is-unbalancing-5f517936f626)和[此回复](https://medium.com/@aliosia/let-me-explain-with-an-example-assume-p-y-0-1-22a6fde4bfd3)。
- en: Another problem with balancing your data by oversampling (as opposed to class-dependent
    instance weighting which doesn’t have this problem) is that it biases naïve cross-validation,
    potentially leading to excessive overfitting that is not detected in the cross-validation.
    In cross-validation, each time the data gets split into a “fold” subset, there
    may be instances in one fold that are duplicates of, or were generated from, instances
    in another fold. Thus the folds are not truly independent as cross-validation
    assumes — there is data “bleed” or “leakage”. For example see [Cross-Validation
    for Imbalanced Datasets](https://medium.com/lumiata/cross-validation-for-imbalanced-datasets-9d203ba47e8) which
    describes how you could re-implement cross-validation correctly for this situation.
    However, in scikit-learn, at least for the case of oversampling by instance duplication
    (not necessarily SMOTE), this can alternatively be worked around by using *model_selection.GroupKFold* for
    cross-validation, which groups the instances according to a selected group identifier
    that has the same value for all duplicates of a given instance — see [my reply](https://dabruro.medium.com/to-panos-v-s-question-how-to-implement-this-with-a-gridsearchcv-though-2ab2437784c3) to
    a response to the aforelinked article.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通过过采样（与依赖类实例加权不同，这种方法没有这个问题）来平衡数据的另一个问题是它会使天真的交叉验证产生偏差，可能导致在交叉验证中无法检测到的过度拟合。在交叉验证中，每次数据被划分为一个“折”子集时，一个折中的实例可能是另一个折中实例的重复或生成的。因此，折并不完全独立，如交叉验证所假设的——存在数据“泄漏”或“渗漏”。例如，请参见[不平衡数据集的交叉验证](https://medium.com/lumiata/cross-validation-for-imbalanced-datasets-9d203ba47e8)，该文描述了如何在这种情况下正确地重新实现交叉验证。然而，在scikit-learn中，至少对于通过实例复制（不一定是SMOTE）进行的过采样情况，这可以通过使用*model_selection.GroupKFold*进行交叉验证来解决，该方法根据一个选定的组标识符对实例进行分组，该标识符对给定实例的所有重复项具有相同的值——请参见[我的回复](https://dabruro.medium.com/to-panos-v-s-question-how-to-implement-this-with-a-gridsearchcv-though-2ab2437784c3)以了解对上述文章的回应。
- en: Conclusion
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Instead of naïvely or implicitly applying a default threshold of 0.5, or immediately
    re-training using re-balanced training data, we can try using the original model
    (trained on the original “imbalanced” data set) and simply plot the trade-off
    between false positives and false negatives to choose a threshold that may produce
    a desirable business result.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试使用原始模型（在原始的“失衡”数据集上训练），并简单绘制假阳性和假阴性之间的权衡，来选择一个可能产生理想业务结果的阈值，而不是天真地或隐式地应用默认的0.5阈值，或立即使用重新平衡的训练数据进行重新训练。
- en: '***Please leave a response if you have questions or comments.***'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '***如果您有任何问题或意见，请留下回复。***'
- en: 'Edit: Why does rebalancing your training data “work” even if the only real
    “problem” was that the default 0.5 threshold wasn’t appropriate?'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编辑：为什么重新平衡训练数据即使唯一的实际“问题”是默认的0.5阈值不合适，也能“有效”？
- en: 'The average probability prediction produced by your model will approximate
    the proportion of training instances that are class 1, because this is the average
    actual value of the target class variable (which has values of 0 and 1). The same
    is true in regression: the average predicted value of the target variable is expected
    to approximate the average actual value of the target variable. When the data
    is highly imbalanced and class 1 is the minority class, this average probability
    prediction will be much less than 0.5 and the vast majority of predictions of
    the probability of class 1 will be less than 0.5 and therefore classified as class
    0 (majority class). If you rebalance the training data, the average predicted
    probability increases to 0.5 and so then many instances will be *above* a default
    threshold of 0.5 as well as many below — the predicted classes will be more balanced.
    So instead of reducing the threshold so that more often the probability predictions
    are above it and give class 1 (minority class), rebalancing increases the the
    predicted probabilities so that more often the probability predictions will be
    above the default threshold of 0.5 and give class 1.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 您的模型产生的平均概率预测将近似于训练实例中属于类别 1 的比例，因为这是目标类别变量的实际平均值（其值为 0 和 1）。回归中也是如此：目标变量的平均预测值预计将近似于目标变量的平均实际值。当数据严重失衡且类别
    1 是少数类时，这种平均概率预测将远低于 0.5，而且绝大多数类别 1 的概率预测将低于 0.5，因此被归类为类别 0（多数类）。如果您重新平衡训练数据，平均预测概率将增加到
    0.5，因此许多实例将会*高于*默认阈值 0.5，同时也会有许多低于该阈值的预测——预测类别将更为平衡。因此，与其降低阈值以使概率预测更常高于该值并给出类别
    1（少数类），重新平衡会增加预测概率，使得概率预测更常高于默认阈值 0.5 并给出类别 1。
- en: If you want to get similar (not identical) results to those of rebalancing,
    without actually rebalancing or reweighting the data, you could try simply setting
    the threshold equal to the average or median value of the model’s predicted probability
    of class 1\. But of course this won’t necessarily be the threshold that provides
    the optimal balance between false positives and false negatives for your particular
    business problem, nor will rebalancing your data and using a threshold of 0.5.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要获得类似（但不完全相同）的结果，而不实际重新平衡或重新加权数据，您可以尝试将阈值设置为模型预测类别 1 的平均值或中位数。但当然，这并不一定是为您的特定业务问题提供最佳假阳性和假阴性平衡的阈值，也不会通过重新平衡数据和使用
    0.5 的阈值来实现。
- en: It is possible that there are situations and models where rebalancing the data
    will materially improve the model beyond merely moving the average predicted probability
    to equal the default threshold of 0.5\. But the mere fact that the model chooses
    the majority class the vast majority of the time when using the default threshold
    of 0.5 does not in itself support a claim that rebalancing the data will accomplish
    anything beyond making the average probability prediction equal to the threshold.
    If you want the average probability prediction to be equal to the threshold, you
    could simply set the threshold equal to the average probability prediction, without
    modifying or reweighting your training data in order to distort the probability
    predictions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 可能存在一些情况和模型，其中重新平衡数据将实质性地改进模型，而不仅仅是将平均预测概率移至默认阈值 0.5。但是，仅仅因为模型在使用默认阈值 0.5 时大多数时间选择了多数类，并不能支持重新平衡数据将实现超出使平均概率预测等于阈值的任何主张。如果您希望平均概率预测等于阈值，您可以简单地将阈值设置为平均概率预测，而不必修改或重新加权训练数据以扭曲概率预测。
- en: 'Edit: Note about confidence scores vs. probability predictions'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编辑：关于置信度评分与概率预测的说明
- en: If your classifier doesn’t have a predict_proba method, e.g. support vector
    classifiers, you can just as well use its decision_function method in its place,
    producing an *ordinal discriminant score *or *confidence score* model output which
    can be thresholded in the same way even if it is *not* interpretable as a *probability
    prediction* between 0 and 1\. Depending on how a particular classifier model calculates
    its confidence scores for the two classes, it *might *sometimes be necessary,
    instead of applying the threshold directly to the confidence score of class 1
    (which we did above as the predicted probability of class 1 because the predicted
    probability of class 0 is simply one minus that), to alternatively apply the threshold
    to the *difference *between the confidence score for class 0 and that for class
    1, with the default threshold being 0 for the case where the cost of a false positive
    is assumed to be the same as that of a false negative. This article assumes a
    two-class classification problem.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的分类器没有 `predict_proba` 方法，例如支持向量分类器，你也可以使用它的 `decision_function` 方法来代替，生成一个*序数判别分数*或*置信度分数*模型输出，即使它*不能*解释为0到1之间的*概率预测*，也可以以相同的方式进行阈值处理。根据特定分类器模型如何计算两个类别的置信度分数，有时*可能*需要，而不是将阈值直接应用于类别1的置信度分数（如上所述，因为类别0的预测概率仅为类别1的概率减去1），而是将阈值应用于类别0和类别1之间的*差异*，默认阈值为0，假设假阳性的成本与假阴性的成本相同。本文假设是一个二分类问题。
- en: '**Bio: [David B Rosen (PhD)](http://linkedin.com/in/rosen1)** is Lead Data
    Scientist for Automated Credit Approval at IBM Global Financing. Find more of
    David''s writing at [dabruro.medium.com](https://dabruro.medium.com/).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介: [David B Rosen (PhD)](http://linkedin.com/in/rosen1)** 是IBM全球融资部门自动化信用审批的首席数据科学家。可以在
    [dabruro.medium.com](https://dabruro.medium.com/) 找到更多David的文章。'
- en: '[Original](https://towardsdatascience.com/how-to-deal-with-imbalanced-classification-without-re-balancing-the-data-8a3c02353fe3).
    Reposted with permission.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/how-to-deal-with-imbalanced-classification-without-re-balancing-the-data-8a3c02353fe3)。经许可转载。'
- en: '**Related:**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关:**'
- en: '[The 5 Most Useful Techniques to Handle Imbalanced Datasets](/2020/01/5-most-useful-techniques-handle-imbalanced-datasets.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[处理不平衡数据集的5种最有用的技术](/2020/01/5-most-useful-techniques-handle-imbalanced-datasets.html)'
- en: '[Dealing with Imbalanced Data in Machine Learning](/2020/10/imbalanced-data-machine-learning.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[处理机器学习中的不平衡数据](/2020/10/imbalanced-data-machine-learning.html)'
- en: '[Resampling Imbalanced Data and Its Limits](/2020/12/resampling-imbalanced-data-limits.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[重采样不平衡数据及其局限性](/2020/12/resampling-imbalanced-data-limits.html)'
- en: '* * *'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的信息技术'
- en: '* * *'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[KDnuggets News, August 31: The Complete Data Science Study Roadmap…](https://www.kdnuggets.com/2022/n35.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，8月31日: 完整的数据科学学习路线图…](https://www.kdnuggets.com/2022/n35.html)'
- en: '[7 Techniques to Handle Imbalanced Data](https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[处理不平衡数据的7种技术](https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html)'
- en: '[Overcoming Imbalanced Data Challenges in Real-World Scenarios](https://www.kdnuggets.com/2023/07/overcoming-imbalanced-data-challenges-realworld-scenarios.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[克服实际场景中的不平衡数据挑战](https://www.kdnuggets.com/2023/07/overcoming-imbalanced-data-challenges-realworld-scenarios.html)'
- en: '[Unsupervised Disentangled Representation Learning in Class…](https://www.kdnuggets.com/2023/01/unsupervised-disentangled-representation-learning-class-imbalanced-dataset-elastic-infogan.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[无监督解耦表示学习中的类别…](https://www.kdnuggets.com/2023/01/unsupervised-disentangled-representation-learning-class-imbalanced-dataset-elastic-infogan.html)'
- en: '[How to Deal with Categorical Data for Machine Learning](https://www.kdnuggets.com/2021/05/deal-with-categorical-data-machine-learning.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何处理机器学习中的分类数据](https://www.kdnuggets.com/2021/05/deal-with-categorical-data-machine-learning.html)'
- en: '[5 Ways to Deal with the Lack of Data in Machine Learning](https://www.kdnuggets.com/2019/06/5-ways-lack-data-machine-learning.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[应对机器学习中数据不足的5种方法](https://www.kdnuggets.com/2019/06/5-ways-lack-data-machine-learning.html)'
