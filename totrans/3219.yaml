- en: Random Forests®, Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/10/random-forests-explained.html](https://www.kdnuggets.com/2017/10/random-forests-explained.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/32bd7c1f79cf7bfb80a751e5590bb162.png)'
  prefs: []
  type: TYPE_IMG
- en: In this post, we will give an overview of a very popular ensemble method called
    Random Forests(®). We first discuss the fundamental components of this ensemble
    learning algorithm - Decision Trees - and then the underlying algorithm and training
    procedures. We will also discuss some variations and advantages of this tool,
    and finally provide resources for you to get started with this powerful tool.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '*Decision Trees in a Nutshell*'
  prefs: []
  type: TYPE_NORMAL
- en: A decision tree is a Machine Learning algorithm capable of fitting complex datasets
    and performing both classification and regression tasks. The idea behind a tree
    is to search for a pair of variable-value within the training set and split it
    in such a way that will generate the "best" two child subsets. The goal is to
    create branches and leafs based on an optimal splitting criteria, a process called
    tree growing. Specifically, at every branch or node, a conditional statement classifies
    the data point based on a fixed threshold in a specific variable, therefore splitting
    the data. To make predictions, every new instance starts in the root node (top
    of the tree) and moves along the branches until it reaches a leaf node where no
    further branching is possible.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm used to train a tree is called CART(®) (Classification And Regression
    Tree). As we already mentioned, the algorithm seeks the best feature–value pair
    to create nodes and branches. After each split, this task is performed recursively
    until the maximum depth of the tree is reached or an optimal tree is found. Depending
    on the task, the algorithm may use a different metric (Gini impurity, information
    gain or mean square error) to measure the quality of the split. It is important
    to mention that due to the greedy nature of the CART algorithm, finding an optimal
    tree is not guaranteed and usually, a reasonably good estimation will suffice.
  prefs: []
  type: TYPE_NORMAL
- en: Trees have a high risk of overfitting the training data as well as becoming
    computationally complex if they are not constrained and regularized properly during
    the growing stage. This overfitting implies a low bias, high variance trade-off
    in the model. Therefore, in order to deal with this problem, we use Ensemble Learning,
    an approach that allows us to correct this overlearning habit and hopefully, arrive
    at better, stronger results.
  prefs: []
  type: TYPE_NORMAL
- en: '*What is an Ensemble Method?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'An ensemble method or ensemble learning algorithm consists of aggregating multiple
    outputs made by a diverse set of predictors to obtain better results. Formally,
    based on a set of “weak” learners we are trying to use a “strong” learner for
    our model. Therefore, the purpose of using ensemble methods is: to average out
    the outcome of individual predictions by diversifying the set of predictors, thus
    lowering the variance, to arrive at a powerful prediction model that reduces overfitting
    our training set.'
  prefs: []
  type: TYPE_NORMAL
- en: In our case, a Random Forest (strong learner) is built as an ensemble of Decision
    Trees (weak learners) to perform different tasks such as regression and classification.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b5e97066ca2c2499491ffcb8bd2221c.png)'
  prefs: []
  type: TYPE_IMG
- en: '*How are Random Forests trained?*'
  prefs: []
  type: TYPE_NORMAL
- en: Random Forests are trained via the bagging method. Bagging or Bootstrap Aggregating,
    consists of randomly sampling subsets of the training data, fitting a model to
    these smaller data sets, and aggregating the predictions. This method allows several
    instances to be used repeatedly for the training stage given that we are sampling
    with replacement. Tree bagging consists of sampling subsets of the training set,
    fitting a Decision Tree to each, and aggregating their result.
  prefs: []
  type: TYPE_NORMAL
- en: The Random Forest method introduces more randomness and diversity by applying
    the bagging method to the feature space. That is, instead of searching greedily
    for the best predictors to create branches, it randomly samples elements of the
    predictor space, thus adding more diversity and reducing the variance of the trees
    at the cost of equal or higher bias. This process is also known as “feature bagging”
    and it is this powerful method what leads to a more robust model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see now how to make predictions with Random Forests. Remember that in
    a Decision Tree a new instance goes from the root node to the bottom until it
    is classified in a leaf node. In the Random Forests algorithm, each new data point
    goes through the same process, but now it visits all the different trees in the
    ensemble, which are were grown using random samples of both training data and
    features. Depending on the task at hand, the functions used for aggregation will
    differ. For Classification problems, it uses the mode or most frequent class predicted
    by the individual trees (also known as a majority vote), whereas for Regression
    tasks, it uses the average prediction of each tree.
  prefs: []
  type: TYPE_NORMAL
- en: Although this is a powerful and accurate method used in Machine Learning, you
    should always cross-validate your model as there may be overfitting. Also, despite
    its robustness, the Random Forest algorithm is slow, as it has to grow many trees
    during training stage and as we already know, this is a greedy process.
  prefs: []
  type: TYPE_NORMAL
- en: '*Variations*'
  prefs: []
  type: TYPE_NORMAL
- en: As we already specified, a Random Forest uses sampled subsets of both the training
    data and the feature space, which result in high diversity and randomness as well
    as low variance. Now, we can go a step further and introduce a bit more variety
    by not only looking for a random predictor but also considering random thresholds
    for each of these variables. Therefore, instead of looking for the optimal pair
    of feature and threshold for the splitting, it uses random samples of both to
    create the different branches and nodes, thus further trading variance for bias.
    This ensemble is also known as Extremely Randomized Trees or Extra-Trees. This
    model also trades more bias for a lower variance but it is faster to train as
    it is not looking for an optimum, like the case of Random Forests.
  prefs: []
  type: TYPE_NORMAL
- en: '*Additional Properties*'
  prefs: []
  type: TYPE_NORMAL
- en: One other important attribute of Random Forests is that they are very useful
    when trying to determine feature or variable importance. Because important features
    tend to be at the top of each tree and unimportant variables are located near
    the bottom, one can measure the average depth at which this
  prefs: []
  type: TYPE_NORMAL
- en: '*Further Information*'
  prefs: []
  type: TYPE_NORMAL
- en: You may find more information about Random Forests in [this Wikipedia article](https://en.wikipedia.org/wiki/Random_forest).
    The Johns Hopkins University Coursera's course on [Practical Machine Learning](https://www.coursera.org/learn/practical-machine-learning/lecture/XKsl6/random-forests)
    has a nice intuitive approach with applications in R. For a Python implementation,
    [follow the code](https://github.com/ageron/handson-ml/blob/master/07_ensemble_learning_and_random_forests.ipynb)
    by Aurélien Géron from [his book](https://shop.oreilly.com/product/0636920052289.do)
    "Hands-On Machine Learning with Scikit-Learn and TensorFlow".
  prefs: []
  type: TYPE_NORMAL
- en: Random Forests™ is a trademark of Leo Breiman and Adele Cutler and is licensed
    exclusively to [Salford Systems](https://www.salford-systems.com/products/randomforests)
    for the commercial release of the software. CART(®) is a registered trademark
    of California Statistical Software, Inc. and is exclusively licensed to [Salford
    Systems](https://www.salford-systems.com/products/cart).
  prefs: []
  type: TYPE_NORMAL
- en: '*Related Content*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Random Forest in Python](/2016/12/random-forests-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding Machine Learning Algorithms](/2017/10/understanding-machine-learning-algorithms.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Getting Up Close and Personal with Algorithms](/2017/03/dataiku-top-algorithms.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Decision Trees vs Random Forests, Explained](https://www.kdnuggets.com/2022/08/decision-trees-random-forests-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
