["```py\nimport dalex as dx\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\ndata = dx.datasets.load_titanic()\nle = preprocessing.LabelEncoder()\nfor feature in ['gender', 'class', 'embarked']:\n\tdata[feature] = le.fit_transform(data[feature])\n\nX = data.drop(columns='survived')\ny = data.survived\n\nclassifier = RandomForestClassifier()\nclassifier.fit(X, y)\n\nexp = dx.Explainer(classifier, X, y, label = \"Titanic Random Forest\")\n```", "```py\nobservation = pd.DataFrame({'gender': ['male'],\n                   \t    'age': [25],\n                   \t    'class': ['1st'],\n                   \t    'embarked': ['Southampton'],\n                       \t    'fare': [72],\n                   \t    'sibsp': [0],\n                   \t    'parch': 0},\n                  \t    index = ['John'])\n\n# Variable influence plots - Break Down & SHAP\nbd = exp.predict_parts(observation , type='break_down')\nbd_inter = exp.predict_parts(observation, type='break_down_interactions')\nbd.plot(bd_inter)\n\nshap = exp.predict_parts(observation, type = 'shap', B = 10)\nshap.plot(max_vars=5)\n\n# Ceteris Paribus plots\ncp = exp.predict_profile(observation)\ncp.plot(variable_type = \"numerical\")\ncp.plot(variable_type = \"categorical\")\n```", "```py\n# Variable importance\n\nvi = exp.model_parts()\nvi.plot(max_vars=5)\n\n# Partial and Accumulated Dependence Profiles\n\npdp_num = exp.model_profile(type = 'partial')\nale_num = exp.model_profile(type = 'accumulated')\n\npdp_num.plot(ale_num)\n\npdp_cat = exp.model_profile(type = 'partial', \nvariable_type='categorical',\nvariables = [\"gender\",\"class\"])\nale_cat = exp.model_profile(type = 'accumulated',\n          variable_type='categorical',\n          variables = [\"gender\",\"class\"])\n\nale_cat.plot(pdp_cat)\n```", "```py\nimport neptune\nfrom neptunecontrib.api import *\nfrom neptunecontrib.versioning.data import *\n\nneptune.init('YOU/YOUR_PROJECT')\n\nneptune.create_experiment(\n          params={'lr': 0.01, 'depth': 30, 'epoch_nr': 10}, # parameters\n          upload_source_files=['**/*.py', # scripts\n                               'requirements.yaml']) # environment\nlog_data_version('/path/to/dataset') # data version\n#\n# your training logic\n#\nneptune.log_metric('test_auc', 0.82) # metrics\nlog_chart('ROC curve', fig) # performance charts\nlog_pickle('model.pkl', clf) # model file\n```", "```py\nfrom neptunecontrib.api import log_local_explanations\n\nobservation = pd.DataFrame({'gender': ['male'],\n                   \t    'age': [25],\n                   \t    'class': ['1st'],\n                   \t    'embarked': ['Southampton'],\n                       \t    'fare': [72],\n                   \t    'sibsp': [0],\n                   \t    'parch': 0},\n                  \t    index = ['John'])\n\nlog_local_explanations(expl, observation)\n```", "```py\nfrom neptunecontrib.api import log_global_explanations\n\nlog_global_explanations(expl, categorical_features=[\"gender\", \"class\"])\n```", "```py\nfrom neptunecontrib.api import log_explainer\n\nlog_explainer('explainer.pkl', expl)\n```", "```py\nimport neptune\nfrom neptunecontrib.api import get_pickle\n\nproject = neptune.init(api_token='ANONYMOUS',\n                       project_qualified_name='shared/dalex-integration')\nexperiment = project.get_experiments(id='DAL-68')[0]\nexplainer = get_pickle(filename='explainer.pkl', experiment=experiment)\n```", "```py\nexperiments =project.get_experiments(id=['DAL-68','DAL-69','DAL-70','DAL-71'])\n\nshaps = []\nfor exp in experiments:\n\tauc_score = exp.get_numeric_channels_values('auc')['auc'].tolist()[0]\n\tlabel = f'{exp.id} | AUC: {auc_score:.3f}'\n\n\texplainer_ = get_pickle(filename='explainer.pkl', experiment=exp)\n\n\tsh = explainer_.predict_parts(new_observation, type='shap', B = 10)\n\tsh.result.label = label\n\tshaps.append(sh)\n\nshaps[0].plot(shaps[1:])\n```"]